{"cell_type":{"9b0190a7":"code","1226011e":"code","a28c4335":"code","00c4ee64":"code","40b98964":"code","d850e3b3":"code","78b9bd08":"code","f8065234":"code","1804b823":"code","ac9057a7":"code","3ea6ed1c":"code","cd27cf7d":"code","9c4e301d":"code","ecd5a7e3":"code","b4c8e51a":"code","f3f19364":"code","0cda3dde":"code","6358be3f":"code","6a5abae0":"code","2b3ad87c":"code","828f094c":"code","c6db4740":"code","0da9e42d":"code","c9f255fe":"code","6641ed8f":"code","e4a2592e":"code","f35397e0":"code","63019ab4":"code","03a0a243":"code","2c34828f":"code","b0f59a9e":"code","b9dde606":"code","5c67278f":"code","b24b9ff0":"markdown","07cd335d":"markdown","83027ae0":"markdown","55f1370f":"markdown","24e76e31":"markdown","46e060c6":"markdown","9c0aae41":"markdown","6f4577d4":"markdown","c80423f9":"markdown"},"source":{"9b0190a7":"import pandas as pd\nimport numpy as np","1226011e":"test = pd.read_csv(\"..\/input\/big-mart-sales-prediction\/test_AbJTz2l.csv\")\n\ntrain = pd.read_csv(\"..\/input\/big-mart-sales-prediction\/train_v9rqX0R.csv\")\n","a28c4335":"train['source']='train'\ntest['source']='test'\ndf=pd.concat([train,test],ignore_index=True)\nprint (train.shape,test.shape,df.shape)","00c4ee64":"df.apply(lambda x:sum(x.isnull()))","40b98964":"df.describe()","d850e3b3":"df.apply(lambda x:len(x.unique()))","78b9bd08":"#Filter categorical variables\ncategorical_columns=[x for x in df.dtypes.index if df.dtypes[x]=='object']\n#Exclude ID cols and source:\ncategorical_columns=[x for x in categorical_columns if x not in ['Item_Identifier','Outlet_Identifier','source']]\n#Print frequency of categories\nfor col in categorical_columns:\n    print ('\\nFrequency of Categories for varible %s'%col)\n    print (df[col].value_counts())","f8065234":"#Determine the average weight per item:\nitem_avg=df.pivot_table(values='Item_Weight', index='Item_Identifier')\n\n#Get a boolean variable specifying missing Item_Weight values\nmiss=df['Item_Weight'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('Orignal #missing: %d'% sum(miss))\ndf.loc[miss,'Item_Weight']=df.loc[miss,'Item_Identifier'].apply(lambda x: item_avg.loc[x])\nprint (sum(df['Item_Weight'].isnull()))\n","1804b823":"#Import mode function:\nfrom scipy.stats import mode\n\n#Determing the mode for each\noutlet_size_mode=df.pivot_table(values='Outlet_Size', columns='Outlet_Type',aggfunc=(lambda x:mode(x).mode[0]) )\nprint ('Mode for each Outlet_Type:')\nprint (outlet_size_mode)\n\n#Get a boolean variable specifying missing Item_Weight values\nmiss_bool = df['Outlet_Size'].isnull() \n\n#Impute data and check #missing values before and after imputation to confirm\nprint ('\\nOrignal #missing: %d'% sum(miss_bool))\ndf.loc[miss_bool,'Outlet_Size'] = df.loc[miss_bool,'Outlet_Type'].apply(lambda x: outlet_size_mode[x])\nprint (sum(df['Outlet_Size'].isnull()))\n","ac9057a7":"df.pivot_table(values=\"Item_Outlet_Sales\",index=\"Outlet_Type\")","3ea6ed1c":"#Determine avg visibility of product (as some has min 0)\navg=df.pivot_table(values=\"Item_Visibility\",index=\"Item_Identifier\")\n\n#impute 0 with avg visibilty of that product\nmiss=(df[\"Item_Visibility\"]==0)\n\nprint ('Number of 0 values initially: %d'%sum(miss))\ndf.loc[miss,'Item_Visibility']=df.loc[miss,'Item_Identifier'].apply(lambda x:avg.loc[x])\nprint (sum(df['Item_Visibility'] == 0))","cd27cf7d":"#Determine another variable with means ratio\ndf['Item_Visibility_MeanRatio']=df.apply(lambda x: x['Item_Visibility']\/avg.loc[x['Item_Identifier']], axis=1)\nprint (df['Item_Visibility_MeanRatio'].describe())","9c4e301d":"#Get the first two characters of ID:\ndf['Item_Type_Combined']=df['Item_Identifier'].apply(lambda x: x[0:2])\n#Rename them to more intuitive categories:\ndf['Item_Type_Combined']=df['Item_Type_Combined'].map({'FD':'Food','NC':'Non-Consumable','DR':'Drinks'})\ndf['Item_Type_Combined'].value_counts()","ecd5a7e3":"#years\ndf[\"Outlet_Year\"]=2013-df[\"Outlet_Establishment_Year\"]\ndf[\"Outlet_Year\"].describe()","b4c8e51a":"#Change categories of low fat:\nprint ('Original Categories:')\nprint (df['Item_Fat_Content'].value_counts())\n\nprint ('\\nModified Categories:')\ndf['Item_Fat_Content']=df['Item_Fat_Content'].replace({'LF':'Low Fat','reg':'Regular','low fat':'Low Fat'})\nprint (df['Item_Fat_Content'].value_counts())","f3f19364":"#Mark non-consumables as separate category in low_fat:\ndf.loc[df['Item_Type_Combined']==\"Non-Consumable\",'Item_Fat_Content']=\"Non-Edible\"\ndf['Item_Fat_Content'].value_counts()","0cda3dde":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n#New variable for outlet\ndf['Outlet']=le.fit_transform(df['Outlet_Identifier'])\ndf['Item_Fat_Content']=le.fit_transform(df['Item_Fat_Content'].astype('bool'))\nvar_mod=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Item_Type_Combined','Outlet_Type','Outlet']\nle=LabelEncoder()\nfor i in var_mod:\n    df[i]=le.fit_transform(df[i])","6358be3f":"#One Hot Coding:\ndf=pd.get_dummies(df,columns=['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type_Combined','Outlet'])","6a5abae0":"df.dtypes","2b3ad87c":"df.drop(['Item_Type','Outlet_Establishment_Year'],axis=1,inplace=True)\n\n","828f094c":"train = df.loc[df['source']==\"train\"]\ntest = df.loc[df['source']==\"test\"]","c6db4740":"test.drop(['Item_Outlet_Sales','source'],axis=1,inplace=True)","0da9e42d":"train.to_csv(\"train_modified.csv\",index=False)\ntest.to_csv(\"test_modified.csv\",index=False)","c9f255fe":"train.drop(['source'],axis=1,inplace=True)","6641ed8f":"train.dtypes","e4a2592e":"target = 'Item_Outlet_Sales'\nIDcol = ['Item_Identifier','Outlet_Identifier']\n","f35397e0":"#Mean based:\nmean_sales = train['Item_Outlet_Sales'].mean()\n\n#Define a dataframe with IDs for submission:\nbase1 = test[['Item_Identifier','Outlet_Identifier']]\nbase1['Item_Outlet_Sales'] = mean_sales\n\n#Export submission file\nbase1.to_csv(\"alg0.csv\",index=False)","63019ab4":"\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\ndef modelfit(alg, dtrain, dtest, predictors, target, IDcol, filename):\n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain[target])\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n\n    #Perform cross-validation:\n    cv_score=cross_val_score(alg, dtrain[predictors], dtrain[target], cv=20, scoring='neg_mean_squared_error')\n    cv_score=np.sqrt(np.abs(cv_score))\n    \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"RMSE : %.4g\" % np.sqrt(metrics.mean_squared_error(dtrain[target].values, dtrain_predictions)))\n    print (\"CV Score : Mean - %.4g | Std - %.4g | Min - %.4g | Max - %.4g\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    \n    #Predict on testing data:\n    dtest[target] = alg.predict(dtest[predictors])\n    \n    #Export submission file:\n    IDcol.append(target)\n    submission = pd.DataFrame({ x: dtest[x] for x in IDcol})\n    submission.to_csv(filename, index=False)","03a0a243":"from sklearn.linear_model import LinearRegression,Ridge,Lasso\npredictor=[x for x in train.columns if x not in [target]+IDcol]\nl=LinearRegression(normalize=True)\nmodelfit(l,train,test,predictor,target, IDcol, 'alg1.csv')\ncoef1 = pd.Series(l.coef_, predictor).sort_values()\ncoef1.plot(kind='bar', title='Model Coefficients')","2c34828f":"#Ridge Regression\npredictors = [x for x in train.columns if x not in [target]+IDcol]\nalg2 = Ridge(alpha=0.05,normalize=True)\nmodelfit(alg2, train, test, predictors, target, IDcol, 'alg2.csv')\ncoef2 = pd.Series(alg2.coef_, predictors).sort_values()\ncoef2.plot(kind='bar', title='Model Coefficients')","b0f59a9e":"#Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor\npredictors=[x for x in train.columns if x not in [target]+IDcol]\nalg3 = DecisionTreeRegressor(max_depth=15, min_samples_leaf=100)\nmodelfit(alg3, train, test, predictors, target, IDcol, 'alg3.csv')\ncoef3 = pd.Series(alg3.feature_importances_, predictors).sort_values(ascending=False)\ncoef3.plot(kind='bar', title='Feature Importances')","b9dde606":"# Decision tree with top 4 variables\npredictors=[\"Item_MRP\",\"Outlet_Type_0\",\"Outlet_5\",\"Outlet_Type_3\"]\nalg4 = DecisionTreeRegressor(max_depth=8, min_samples_leaf=150)\nmodelfit(alg4, train, test, predictors, target, IDcol, 'alg4.csv')\ncoef4 = pd.Series(alg4.feature_importances_, predictors).sort_values(ascending=False)\ncoef4.plot(kind='bar', title='Feature Importances')","5c67278f":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\npredictors = [x for x in train.columns if x not in [target]+IDcol]\nalg5 = RandomForestRegressor(n_estimators=200,max_depth=5, min_samples_leaf=100,n_jobs=4)\nmodelfit(alg5, train, test, predictors, target, IDcol, 'alg5.csv')\ncoef5 = pd.Series(alg5.feature_importances_, predictors).sort_values(ascending=False)\ncoef5.plot(kind='bar', title='Feature Importances')","b24b9ff0":"## Data Cleaning\n### Imputting Missing Values","07cd335d":"## Data Exploration","83027ae0":"### Missing Values","55f1370f":"# Big Mart Sales Prediction(Regression)\n Retail is another industry which extensively uses analytics to optimize business processes. Tasks like product placement, inventory management, customized offers, product bundling, etc. are being smartly handled using data science techniques. As the name suggests, this data comprises of transaction records of a sales store. This is a regression problem. Predict the sales of store\n","24e76e31":"## Conclusion\nWe started with making some hypothesis about the data without looking at it. Then we moved on to data exploration where we found out some nuances in the data which required remediation. Next, we performed data cleaning and feature engineering, where we imputed missing values and solved other irregularities, made new features and also made the data model-friendly by one-hot-coding. Finally we made regression, decision tree and random forest model and got a glimpse of how to tune them for better results.","46e060c6":"## We will explore the problem in following stages:\n### 1.Hypothesis Generation \u2013 understanding the problem better by brainstorming possible factors that can impact the outcome\n### 2.Data Exploration \u2013 looking at categorical and continuous feature summaries and making inferences about the data.\n### 3.Data Cleaning \u2013 imputing missing values in the data and checking for outliers\n### 4.Feature Engineering \u2013 modifying existing variables and creating new ones for analysis\n### 5.Model Building \u2013 making predictive models on the data ","9c0aae41":"## Feature Engineering","6f4577d4":"## Model Building","c80423f9":"## Linear Regression"}}