{"cell_type":{"5c9040fd":"code","6e367e00":"code","da155ba3":"code","b6f98ca9":"code","fddb3244":"code","06f0bd0a":"code","969b29f5":"code","24f0f99d":"code","bb53f143":"code","06d02b3a":"code","eb9aae94":"code","68415261":"code","4ce7db52":"code","dac784b4":"code","29ca26be":"code","4b4a32f5":"code","d72a3c89":"code","35524e28":"code","a1cce039":"code","007df0cc":"markdown","57d8cc27":"markdown","ac5bb70a":"markdown","b558efaa":"markdown","8cd70958":"markdown","226a2075":"markdown","8eea97a9":"markdown","c70039fd":"markdown","7b9f8232":"markdown","9b4f938a":"markdown","f7bab188":"markdown","e926c0c7":"markdown","2fa0d60e":"markdown","b572e186":"markdown"},"source":{"5c9040fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # visualiztion\n%matplotlib inline\nimport seaborn as sns #sub libraray of matplotlib for visualizatoion\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e367e00":"# read data\ndata=pd.read_csv(\"..\/input\/diabetes\/diabetes.csv\")\ndata.head()","da155ba3":"data.Outcome.value_counts().plot(kind=\"bar\")\nprint(data.Outcome.value_counts())","b6f98ca9":"# let's see the distribution of all variable except outcom\nfig, ax = plt.subplots(4,2, figsize=(20,20))\nsns.distplot(data.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(data.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(data.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(data.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(data.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(data.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(data.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(data.BMI, bins = 20, ax=ax[3,1]) ","fddb3244":"sns.pairplot(data,hue=\"Outcome\")","06f0bd0a":"cor=data.corr()","969b29f5":"plt.figure(figsize=(10,6))\nsns.heatmap(cor,annot=True,cmap=\"RdYlGn\")","24f0f99d":"data.head()","bb53f143":"x=data.iloc[:,:-1]\nx.head()","06d02b3a":"y=data.Outcome\ny.head()","eb9aae94":"from sklearn.model_selection import train_test_split","68415261":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=0)\nprint(\"75% train data\" ,x_train.shape)\nprint(\"25% test data\" ,x_test.shape)","4ce7db52":"from sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings(\"ignore\") \nfrom sklearn import metrics\n#Model\nL_reg = LogisticRegression()\n\n#fit the model\nL_reg.fit(x_train, y_train)\n\n#prediction\ny_pred = L_reg.predict(x_test)\n\n#Accuracy\nprint(\"Accuracy \", metrics.accuracy_score(y_test,y_pred))\n\n","dac784b4":"# Plot confusion matrix\ncm=metrics.confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True,fmt=\"g\")","29ca26be":"from sklearn.tree import DecisionTreeClassifier\n\nD_tree = DecisionTreeClassifier(random_state=0)\n\n#fit the model\nD_tree.fit(x_train, y_train)\n\n#prediction\ny_pred_dt = D_tree.predict(x_test)\n\n#Accuracy\nprint(\"Accuracy \", metrics.accuracy_score(y_test,y_pred_dt))\n","4b4a32f5":"#plot confusion matrix\ncm = metrics.confusion_matrix(y_test,y_pred_dt)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","d72a3c89":"from sklearn.ensemble import RandomForestClassifier\n\nRf_model = RandomForestClassifier(random_state=0)\n\n#fit the model\nRf_model.fit(x_train, y_train)\n\n#prediction\ny_pred_rf = Rf_model.predict(x_test)\n\n#Accuracy\nprint(\"Accuracy \", metrics.accuracy_score(y_test,y_pred_rf))\n","35524e28":"#plot confusion matrix\ncm = metrics.confusion_matrix(y_test,y_pred_rf)\nsns.heatmap(cm, annot=True, fmt='g')\nplt.show()","a1cce039":"# check all model accuracy\n\nprint(\"Accuracy Logistic Reg \", metrics.accuracy_score(y_test,y_pred))\nprint(\"Accuracy Decission tree \", metrics.accuracy_score(y_test,y_pred_dt))\nprint(\"Accuracy Random Forest \", metrics.accuracy_score(y_test,y_pred_rf))","007df0cc":"### Predictive Modeling with different-different models","57d8cc27":"##### split the data into train and test with 0.75% and 0.25% respectively","ac5bb70a":"# Correlation between features\n\n**A correlation could be positive, meaning both variables move in the same direction, or negative, meaning that when one variable\u2019s value increases, the other variables\u2019 values decrease. Correlation can also be neural or zero, meaning that the variables are unrelated.**\n\nstrong positive corrleation = near to +1\n\nstrong negative correlation = near to -1","b558efaa":"## Task\n\n**We will try to build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?**","8cd70958":"## Random Forerst\n\n**Random forests is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.**","226a2075":"# Data\n\n**The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on**.\n\n**Pregnancies**: No. of times pregnant\n\n**Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n**BloodPressure**: Diastolic blood pressure (mm Hg)\n\n**SkinThickness**: skin fold thickness (mm)\n\n**Insulin**: 2-Hour serum insulin (mu U\/ml)\n\n**BMI**: Body mass index\n\n**DiabetesPedigreeFunction**: Diabetes pedigree function\n\n**Age**: Age (years)\n\n**Outcome**: outcome (0 or 1)","8eea97a9":"### Pair Plots\n\n**Pair plot is used to understand the relationship between two variables. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our dataset.**","c70039fd":"**This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.**","7b9f8232":"### devide the data into independent(X) and dependent(Y) variable","9b4f938a":"### Decision Tree\n\n**Decision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.**","f7bab188":"## Logistic regression==>","e926c0c7":"##### Data Visualization","2fa0d60e":"### Indian Diabetes ==>","b572e186":"### If You find this notebook useful, PLEASE UPVOTE ^"}}