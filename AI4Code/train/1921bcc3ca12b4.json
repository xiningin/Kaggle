{"cell_type":{"b08d12a0":"code","c2b65434":"code","88a4e97a":"code","0263d547":"code","ae6c1d17":"code","0e99c997":"code","ac260ad7":"code","3c8d66d8":"code","4c08e885":"code","5598d34b":"code","afb504a5":"code","0b9c1136":"code","72efcc89":"code","da656959":"code","5009ccb2":"code","7141f6e2":"code","e0fe3d23":"code","35edb16e":"code","20f539f1":"code","5f659d27":"code","d8377581":"code","9ea40c26":"code","1bc8a17e":"code","b6371e1a":"code","13d66cb6":"code","b1d89fbc":"markdown","48615305":"markdown","3ac11b04":"markdown","dc725d4e":"markdown","63d19fb9":"markdown","8775f914":"markdown","6b77fae4":"markdown","f62a0210":"markdown","035a7025":"markdown","d36f4ee7":"markdown","d9008d58":"markdown","21f394a6":"markdown","4bfe4940":"markdown","c618148e":"markdown","889d8be8":"markdown","4a80f065":"markdown","1deaac67":"markdown","9eed044a":"markdown","126a3250":"markdown","23efa88d":"markdown","6adfcc3f":"markdown","d28f15f4":"markdown","03729f3c":"markdown","7399629b":"markdown","b2ec8332":"markdown","5fd9acae":"markdown","f6fc1c8d":"markdown","90b1b808":"markdown","5d61ac4e":"markdown","25c966de":"markdown","4c686254":"markdown","7fe4c1b2":"markdown","f86c0a64":"markdown","9e3cdfc8":"markdown","9fe206e3":"markdown","5a17ed50":"markdown","c91b2bfc":"markdown","ea94af18":"markdown","30abfa6f":"markdown","c9030dc5":"markdown"},"source":{"b08d12a0":"#%pip install -U pip\n%pip install tensorflow==2.0\n%pip install tfp-nightly","c2b65434":"import tensorflow as tf\nimport tensorflow_probability as tfp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline\n#random seed as the birthday of my granp which is in the hospital fighting with cancer\n#be strong Valdomiro!\nnp.random.seed(10171927)\ntf.random.set_seed(10171927)\n\n#to see how long the notebook lasts to run\nstart = time.time()","88a4e97a":"print('TensorFlow version (expected = 2.0.0):', tf.__version__)\nprint('TensorFlow Probability version (expected = 0.9.0-dev20190912):', tfp.__version__)","0263d547":"#We first load our data, and seek the distribution for its labels\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\n#Create our labels array\nY_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n#Visualize the distribution \ng = sns.countplot(Y_train)","ae6c1d17":"X_train = X_train \/ 255.0\nX_train = X_train.values.reshape(-1,28,28,1)\nY_train = tf.keras.utils.to_categorical(Y_train, num_classes = 10)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.3, random_state=42)","0e99c997":"#We are using 1026, as it is my birthday\nidx = 1026\nplt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\nprint(\"True label of the test sample {}: {}\".format(idx, np.argmax(Y_train[idx], axis=-1)))","ac260ad7":"def build_cnn(input_shape):\n    \n    ##model building\n    model = tf.keras.models.Sequential()\n    #convolutional layer with rectified linear unit activation\n    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                     activation='relu',\n                     input_shape=input_shape))\n    #32 convolution filters used each of size 3x3\n    #again\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n    #64 convolution filters used each of size 3x3\n    #choose the best features via pooling\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    #randomly turn neurons on and off to improve convergence\n    model.add(tf.keras.layers.Dropout(0.25))\n    #flatten since too many dimensions, we only want a classification output\n    model.add(tf.keras.layers.Flatten())\n    #fully connected to get all relevant data\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    #one more dropout for convergence' sake :) \n    model.add(tf.keras.layers.Dropout(0.5))\n    #output a softmax to squash the matrix into output probabilities\n    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n    \n    return model","3c8d66d8":"#here we build the network, instance an optizmier and compile it\ncnn = build_cnn(X_train.shape[1:])\noptimizer = tf.keras.optimizers.Adam(lr=0.01)\ncnn.compile(loss=tf.keras.losses.categorical_crossentropy,\n              metrics=['accuracy'], optimizer=optimizer)","4c08e885":"history = cnn.fit(X_train, Y_train, epochs=10, validation_split=0.1)","5598d34b":"#plotting accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","afb504a5":"evaluation = cnn.evaluate(X_val, Y_val, verbose=2)\nprint(evaluation)","0b9c1136":"pred_for_idx = cnn(X_train[idx:idx+1, :, :, :])","72efcc89":"idx = 1026\nplt.imshow(X_train[idx, :, :, 0], cmap='gist_gray')\nprint(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_idx[0]), axis=-1))","da656959":"noise = np.random.random((28,28,1))\npred_for_noise = cnn(np.array([noise]))\nplt.imshow(noise[:, :, 0], cmap='gist_gray')\nprint(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(pred_for_noise[0]), axis=-1))\n","5009ccb2":"def build_bayesian_bcnn_model(input_shape):\n    \n    \"\"\"\n    Here we use tf.keras.Model to use our graph as a Neural Network:\n    We select our input node as the net input, and the last node as our output (predict node).\n    Note that our model won't be compiled, as we are usign TF2.0 and will optimize it with\n    a custom @tf.function for loss and a @tf.function for train_step\n    Our input parameter is just the input shape, a tuple, for the input layer\n    \"\"\"\n    \n    model_in = tf.keras.layers.Input(shape=input_shape)\n    conv_1 = tfp.python.layers.Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2)\n    x = conv_1(model_in)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n    conv_2 = tfp.python.layers.Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2)\n    x = conv_2(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n    x = tf.keras.layers.Flatten()(x)\n    dense_1 = tfp.python.layers.DenseFlipout(512, activation='relu')\n    x = dense_1(x)\n    dense_2 = tfp.python.layers.DenseFlipout(10, activation=None)\n    model_out = dense_2(x)  # logits\n    model = tf.keras.Model(model_in, model_out)\n    return model","7141f6e2":"\"\"\"\nthis is our loss function: a sum of KL Divergence and Softmax crossentropy\nWe use the @tf.function annotation becuase of TF2.0, and need no placeholders\nwe get each loss and return its mean\n\"\"\"\n\n@tf.function\ndef elbo_loss(labels, logits):\n    loss_en = tf.nn.softmax_cross_entropy_with_logits(labels, logits)\n    loss_kl = tf.keras.losses.KLD(labels, logits)\n    loss = tf.reduce_mean(tf.add(loss_en, loss_kl))\n    return loss","e0fe3d23":"\"\"\"\nthis is our train step with tf2.0, very ellegant:\nWe do our flow of the tensors over the model recording its gradientes\nThen, our gradient tape to give us a list of the gradients of each parameter in relation of the loss\nwe dan ask our previously instanced optimizer to apply those gradients to the variable\nIt is cool to see that it works even with TensorFlow probability- probabilistic layers parameters\n\"\"\"\n@tf.function\ndef train_step(images, labels):\n    with tf.GradientTape() as tape:\n        logits = bcnn(X_train)\n        loss = elbo_loss(labels, logits)\n    gradients = tape.gradient(loss, bcnn.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, bcnn.trainable_variables))\n    return loss\n\ndef accuracy(preds, labels):\n    return np.mean(np.argmax(preds, axis=1) == np.argmax(labels, axis=1))","35edb16e":"bcnn = build_bayesian_bcnn_model(X_train.shape[1:])\noptimizer = tf.keras.optimizers.Adam(lr=0.01)","20f539f1":"\"\"\"\nin our train step we can see that it lasts more tha na normal CNN to converge\non the other side, we can have the confidence interval for our predictions, which are \nwonderful in terms of taking sensitive predictions\n\"\"\"\ntimes = []\naccs = []\nval_accs = []\nlosses = []\nval_losses = []\nfor i in range(20):\n    tic = time.time()\n    loss = train_step(X_train, Y_train)\n    preds = bcnn(X_train)\n    acc = accuracy(preds, Y_train)\n    accs.append(acc)\n    losses.append(loss)\n    \n    val_preds = bcnn(X_val)\n    val_loss = elbo_loss(Y_val, val_preds)\n    val_acc = accuracy(Y_val, val_preds)\n    \n    val_accs.append(val_acc)\n    val_losses.append(val_loss)\n    tac = time.time()\n    train_time = tac-tic\n    times.append(train_time)\n    \n    print(\"Epoch: {}: loss = {:7.3f} , accuracy = {:7.3f}, val_loss = {:7.3f}, val_acc={:7.3f} time: {:7.3f}\".format(i, loss, acc, val_loss, val_acc, train_time))","5f659d27":"#plotting accuracy\nplt.plot(np.array(accs), label=\"acc\")\nplt.plot(np.array(val_accs), label=\"val_acc\")\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.show()\n\n# Plot training & validation loss values\nplt.plot(np.array(losses), label=\"loss\")\nplt.plot(np.array(val_losses), label=\"val_loss\")\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","d8377581":"def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n    bins = np.logspace(-n_bins, 0, n_bins+1)\n    fig, ax = plt.subplots(n_subplot_rows, n_class \/\/ n_subplot_rows + 1, figsize=figsize)\n    for i in range(n_subplot_rows):\n        for j in range(n_class \/\/ n_subplot_rows + 1):\n            idx = i * (n_class \/\/ n_subplot_rows + 1) + j\n            if idx < n_class:\n                ax[i, j].hist(y_pred[idx], bins)\n                ax[i, j].set_xscale('log')\n                ax[i, j].set_ylim([0, n_mc_run])\n                ax[i, j].title.set_text(\"{} (median probability: {:.2f}) ({})\".format(str(idx),\n                                                                               np.median(y_pred[idx]),\n                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n            else:\n                ax[i, j].axis('off')\n    plt.show()","9ea40c26":"n_mc_run = 50\nmed_prob_thres = 0.35\n\ny_pred_logits_list = [bcnn(X_val) for _ in range(n_mc_run)]  # a list of predicted logits\ny_pred_prob_all = np.concatenate([tf.nn.softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\ny_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\ny_pred = np.array(y_pred)\n\nidx_valid = [any(y) for y in y_pred]\nprint('Number of recognizable samples:', sum(idx_valid))\n\nidx_invalid = [not any(y) for y in y_pred]\nprint('Unrecognizable samples:', np.where(idx_invalid)[0])\n\nprint('Test accuracy on MNIST (recognizable samples):',\n      sum(np.equal(np.argmax(Y_val[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) \/ len(Y_val[idx_valid]))\n\nprint('Test accuracy on MNIST (unrecognizable samples):',\n      sum(np.equal(np.argmax(Y_val[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) \/ len(Y_val[idx_invalid]))","1bc8a17e":"class_nmr = 10\nplt.imshow(X_val[0, :, :, 0], cmap='gist_gray')\nprint(\"True label of the test sample {}: {}\".format(0, np.argmax(Y_val[0], axis=-1)))\n\nplot_pred_hist(y_pred_prob_all[0], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)","b6371e1a":"class_nmr = 10\ninvalids = 0\nfor idx in np.where(idx_invalid)[0]:\n    plt.imshow(X_val[idx, :, :, 0], cmap='gist_gray')\n    print(\"True label of the test sample {}: {}\".format(idx, np.argmax(Y_val[idx], axis=-1)))\n\n    plot_pred_hist(y_pred_prob_all[idx], class_nmr, n_mc_run, med_prob_thres=med_prob_thres)\n\n    if any(y_pred[idx]):\n        print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n    else:\n        print(\"I don't know!\")\n    invalids += 1\n    if invalids > 5:\n        break","13d66cb6":"end = time.time()\nruntime = (end - start) \/ 60\nprint(\"This notebook ran in {:7.3f} minutes\".format(runtime))","b1d89fbc":"## 6. Conclusion ","48615305":"## 3.2 Predicting on real data and on noise: there is always a prediction\n\nAs we are going to see, our neural network will predict outputs, as we see:\n\nFor real data:","3ac11b04":"## 4.3 Implementing the ELBO Loss function","dc725d4e":"## 5.4 Visualizing non-predicted values\n    ","63d19fb9":"## 1. Introduction\n","8775f914":"## 2.1 Load, normalize and reshape the data","6b77fae4":"## 4.1 Notes on the implementation\n\nIn order to do our implementation, we are will implement the following functions:\nThe model will be a keras one, but won't be compiled, as we are wusing TF 2.0 features\n* build_bcnn_model: builds a Bayesian Convnet for our MNIST classification;\n* elbo_loss: tf.function that calculates elbo_loss for our network\n* train_step: tf.function that trains our bcnn model\n* accuracy: gets the accuracy of the predictions for plotting","f62a0210":"## 7. References and material if you want to know more\n* [Gluon BNN tutorial](https:\/\/gluon.mxnet.io\/chapter18_variational-methods-and-uncertainty\/bayes-by-backprop.html)\n\n* [Weight Uncertainty in Neural Networks](https:\/\/arxiv.org\/pdf\/1505.05424.pdf)\n\n* [TensorFlow 2.0 quickstart tutorial](https:\/\/www.tensorflow.org\/tutorials\/quickstart\/)\n\n* [TensorFlow Probability main page and tutorials](https:\/\/www.tensorflow.org\/probability)\n\n* [An excelent TFP tutorial from Zhulingchen](https:\/\/github.com\/zhulingchen\/tfp-tutorial\/)\n\n* [My github repo with the implementetion (in case you want to run it on your local machine)](https:\/\/github.com\/piEsposito\/easy-bnn-with-tf2.0)\n","035a7025":"## 3. Ilustrating the problem with a deterministic CNN","d36f4ee7":"We normalize and reshape our data so we have a train set with shape (N, 28, 28, 1), being N the number of images. We also set our Y_train to categorical and divide our data into a train and test dataset.","d9008d58":"## 5.2 Gathering the prediction distribution\n\nOn the snippet below, we just apply the Monte Carlo technique to sample and predict labels 30 times for each image on the validation set. We then store those predictions on lists to plot the distribution and gather insights.","21f394a6":"As we see, we have very good metrics and it did converge very fastly","4bfe4940":"Pi Esposito, AI Software Development Intern @ Intel Corporation","c618148e":"#### 1.2 The Bayesian approach for Neural Networks\n\nIn order to address that issue, it was created a Bayesian Approach to Neural Networks. It consists on building networks if layer which we will call **proabilistic layers**. This lets gather uncertainity on our predictions, as we will see.","889d8be8":"As we are going to work with some non \"Kaggle default\" packages, you may want to uncomment those pip install for gathering the packages used on this notebook.","4a80f065":"## 3.1 Implementing and training the deterministic model\n\nWe will, below, implement a deterministic model for predicting on the MNIST dataset using Keras interface, as there will be no custom function or module.","1deaac67":"## 5.3 Visualizing valid predictions\n    ","9eed044a":"## 1.3 ELBO loss function \n\n\nAs we are working with Probability distribution, we will use a combination of the KL divergence, which is a mathematical representation of the information loss on representing the labels of the train features-label pair with the feedforward representation with the sparse categorical crossentropy, as is usually done on classification NN modelling.\n\nAs TensorFlow lets us calculate each of this losses, won\u00b4t have to code it by hand. If you want to know more about the derivatives and the math, I'm putting a link for the original paper on the end of this kernel.\n\nBy the way, we call this function the ELBO loss functon, which stants for expected lower bound.\nIt is defined as:\n\n(The KL Divergence of our predicted distribution relative to the labels plus the crossentropy of our predicted relative to the labels - being Q the predicted probabilities or logits):[](http:\/\/)\n\n\\begin{equation*}\n* \\mathcal{L}(\\mathcal{D}, \\mathbf{\\theta}) = \\text{KL}[q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})\\ ||\\ P(\\mathbf{w})] - \\mathbb{E}_{q(\\mathbf{w}\\ |\\ \\mathbf{\\theta})}[\\log P(\\mathcal{D}\\ |\\ \\mathbf{w})]\n\\end{equation*}","126a3250":"## 1.1 The \"need\" for uncertainity gathering on predictions\n\nNeural Networks do work by performing feedforward and outputing logits, which can be used as predictions or interpreted as wished. \n\nDespite being very accurate and able to model (with the right hyperparameters) any kind of mathematical function, NNs do operate with any kind of data (if encoded in a tensor according to its input shape). For illustration purposes, it means that, on the case of a image-classifier CNN, once I've encoded an image as a tensor as the same of its input, I'm able to try to predict its label even if my image has nothing to see with my classificator purpose. It also means that, even when I'm trying to predict a very noisy image (which can almos be seen as noise), the network will output a deterministic prediction for it (** see point 3. for illustration**). \n\nWe understand that it would be nice to know how certain our network is of its prediction. For example, in an autonomous car, it should be mandatory to have threshold on which, even if it does not recognize a pedestrian, depending on the uncertanity of its prediction, the car acts as if there was one. This goes for many examples on sensible decisions on which life is involved. \n\nWe also understand that, in some financial cases, (e.g., neural-network consulting stock trader bot) it would also be nice to understand if there is a situation where it is not sure on how to behave and thus stop trading in order to avoid money loss.\n\n**In short, the point is that it would be very beneficial for Deep Learning guided decision process if the neural network could know if it is not trained to understand the data fed on it and output something which can be interpreted as \"I'm interpreting this as noise\".**[](http:\/\/)\n\n","23efa88d":"## Bayesian Neural Networks: variational inference with epistemic uncertainity, i.e., the Neural Networks who can say \"I don't know\"","6adfcc3f":"As we can see, its almos uniformely distributed, so we're not doing any augmentation","d28f15f4":"### Accuracy and loss evolution:\n\nSome metrics to illustrate its evolution. We can see from the logs above that our BNN is generalizing very well on the validation set. On the other side, it lasts much more to train.","03729f3c":"## 4.5 Wrap-up and train the BNN\n\nWe will se that our BNN takes mroe time to start converging","7399629b":"## 4. Implementing a Bayesian Neural Network with TensorFlow 2.0","b2ec8332":"Here we just visualize some well predicted values.","5fd9acae":"On this snippet we do visualize the noisy images which the network is not able to predict on properly.","f6fc1c8d":"## 2. Data preparation","90b1b808":"* **1. Introduction**\n    * 1.1 The \"need\" for uncertainity gathering on predictions\n    * 1.2 The Bayesian approach for Neural Networks\n    * 1.3 ELBO loss function\n    * 1.4 Notes on TensorFlow 2.0 and TensorFlow Probability \n* **2. Data preparation**\n    * 2.1 Load, normalize and reshape data\n    * 2.2 Visualizing the data\n* **3. Ilustrating the problem with a deterministic CNN**\n    * 3.1 Implementing and training the deterministic model\n    * 3.2 Predicting on real data and on noise: there is always a prediction\n* **4. Implementing a Bayesian Neural Network with TensorFlow 2.0**\n    * 4.1 Notes on the implementation\n    * 4.2 Implementing the Bayesian model with TFP Layers\n    * 4.3 Implementing the ELBO Loss function\n    * 4.4 Implementing the train function\n    * 4.5 Wrap-up and train the BNN\n* **5. Predicting with Monte Carlo methods and epistemic uncertainity gathering**\n    * 5.1 Notes on the prediction method\n    * 5.2 Gathering the prediction distribution\n    * 5.3 Visualizing valid predictions\n    * 5.4 Visualizing non-predicted values (noise)\n* **6. Conclusion ** \n* **7. References **","5d61ac4e":"* **Objective:** \n    This kernel aims to **(i)** justify the **need for Neural Network models which are able to output the uncertainity for its predictions**, i.e., let the Data Scientist know if his model is not able to perform a reliable prediction on the data which he tried to predict the label; and **(ii)** **present the Bayesian approach to Neural Networks** as a way to supply this need.\n    \n    \n* **Note:** In order to ease the explanation of the mathematical part of the BNNs (without any theorethical loss), Neural Networks will be interpreted not as a maximum likelihood estimation for the training data (statistical interpretation), but as and optimized function to model the train data in function of its labels.  \n\n\n* **Packages used**:\n    * TensorFlow 2.0\n    * TensorFlow Probability (for probabilistic NN layers)\n    * Matplotlib\n    * Numpy\n    * Pandas\n    * Seaborn\n    * Sklearn train_test_split function\n    * time","25c966de":"#### Probabilistic layers\n\nProbabilistic layers are neural network layers on which the weights are not deterministic. They are actually **random variables (normally distributed) sampled differently on each feedforward step of the network**. When the layer is being built, for each weight, it randomly starts **mean and standard deviation, which are the trainable parameters**. \n\n**When the layer is called (i.e., on each feedforward step), it samples a weight from the distribution and runs normally.**\n\n**On the backpropagation step, for each trainable random variable, the gradients are the partial derivative from the Loss in relative the mean and standard deviation**.\n\nThis layer-sampling step means that, if we predict many times on the same input, we will have different weights sampled (according to the distribution). Thus, noisy images as prone to have more \"uniform-like\" distribution, being more sensible to the feature extraction layer changes (as they have no features at all), while real clean images will be less sensible to layer changing and ar prone to keep the same prediction (its label).\n\nTo illustrate the feedforward step, we see a  bayesian network on the right side. We see that the bayesian one has a different probability distribution for each weight.\n\n![](https:\/\/raw.githubusercontent.com\/zackchase\/mxnet-the-straight-dope\/master\/img\/bbb_nn_bayes.png)\n[Credits for Gluon](https:\/\/gluon.mxnet.io\/chapter18_variational-methods-and-uncertainty\/bayes-by-backprop.html)\n\n#### Uncertanity gathering\n\nTo gather uncertanity on our predictions, we use Monte Carlo methods and predict it N times, saving each sampled prediction. We then get the prediction distribution and, using a threshold, decide that, if the mode of the prediction represent less then the threshold, then the network should not decide at all and we do not consider the prediction. Our \"reliability\" of the predicition the % of the total predictions that the mode represent. \n\nReal data tend to have the mode representing almost 100%, while the more noisy it is, the more it uniform distribution it will have.\n","4c686254":"As a conclusion, we found that \n1.  Bayesian Neural Networks may be more reliable than the deterministic ones;\n2.  For sensible decision taking, it may be the more \"ethical approach\", due to the possibility of avoiding taking risky decision which the model is not trained to predict;\n3.  Bayesian Networks are more prone to avoid overfitting, due to the weights uncertainity;\n4.  On the other side, Bayesian Networks do last more to train and to start having some good accuracy on which we can rely on.","7fe4c1b2":"Above we have the illustration of the problem of noise-like data on deterministic Neural Networks: it gains a label and is processed as that, but it does not represent anything. To address this issue, we're implementing the probabilistic network. ","f86c0a64":"## 4.4 Implementing the train function","9e3cdfc8":"## 4.2 Implementing the Bayesian model with TFP Layers","9fe206e3":"### Plot prediction histogram function:\n\nThe function below plots the histograms for each label probabilities prediction and will be used to visualize our data. This function was mostly from Zhullingchen GitHub (with minor changes).\n\nHere is the link of the repo, which I strongly recomend you give a look: [TFP tutorial from Zhulingchen](https:\/\/github.com\/zhulingchen\/tfp-tutorial\/)","5a17ed50":"## 1.4 Notes on TensorFlow 2.0 and TensorFlow probability\n\nFirst, some useful links:\n\n* [TensorFlow 2.0 quickstart tutorial](https:\/\/www.tensorflow.org\/tutorials\/quickstart\/)\n\n* [TensorFlow Probability main page and tutorials](https:\/\/www.tensorflow.org\/probability)\n\n#### TensorFlow 2.0\n##### All the features explained here will be shown during the implementations\n* TensorFlow 2.0 has ended tf.Session(), feed_dict and sess.run. We now just have to call the node of the graph to get its value. TensorFlow 2.0 has also ended the need for placeholders, giving us a much more intuitive way to flow tensors thorugh graphs:\n\n* We define it as a funcion with the inputs a parameters. We do the operations and return the nodes we want to. The only thing we have to do is put @tf.function decorator above the definition. We can calculate losses and optimize models with tf.functions in a much easier way.\n\n* For gradient appliying, we have gradientTape which gets the derivative of our last operation relative to all the nodes conected to it, and a powerful and easy way to apply those gradients with the optimizer.\n\n\n\n#### TensorFlow Probability\n\n* The probabilistic counterpart os TensorFlow has, on its features, some probabilistic counterparts for layers, so we don't have to bother in implementing probabilistic layers.\n\n* The nightly version of TFP works well with all the TensorFlow 2.0 features, and so it will be very easy to write custom losses and manually optimizing our Bayesian Network on the future.","c91b2bfc":"## 5. Predicting with Monte Carlo methods and epistemic uncertainity gathering","ea94af18":"## 5.1 Notes on the prediction method\n\nWe will now use our trained Bayesian Neural Network to predict on labeled real values and noisy ones. Our approach will consist in sampling 30 predictions for each value, then see the prediction distribution. The neural network will be set to \"I don't know\" if it cant predict, with confidence, any value. To predict a value, using the montecarlo method, the median of the labels predicted probability must surpass 0.35.\n\nWe want to show that, the more the sampled networks predict on the mode prediction, the more reliable the prediction will be. On the other side, \"confuse\" and uncertain predictions will have the predictions distributed on more labels, approximating to a uniform distribution.\n\nIn order to to that, we are implementing functions to gather and plot those distributions.\n    ","30abfa6f":"## 2.2 Visualize the data\n\nIf we want to visualize an image from our dataset, we gather its index and use matplotlib this way:","c9030dc5":"For noise generated data:"}}