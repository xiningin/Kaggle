{"cell_type":{"512129fb":"code","7e3a7392":"code","04c0063a":"code","a5c22c21":"code","5d63efc9":"code","1954994e":"code","ae59f530":"code","c6733d33":"code","597279ad":"code","65e609ff":"code","f37cad7d":"code","99897795":"code","2ff896c9":"code","f1501faf":"code","412ac7c9":"code","79882d06":"code","934a0152":"code","528dd6a0":"code","3c5a10ba":"code","5b6c0474":"code","809e8222":"code","4137b788":"code","fd1fac7f":"code","772e254e":"code","f3fb0302":"code","cd4c2b46":"code","65def27b":"code","2107a68f":"code","01663f7e":"code","777ac162":"markdown","c5a9b74e":"markdown","18559ee1":"markdown","4ac5bedf":"markdown","06804201":"markdown","19fe1e00":"markdown","411c5edb":"markdown","9bf4d526":"markdown","43ffdb5a":"markdown","7e5c4c9c":"markdown","48d53ef8":"markdown","9cd16b3a":"markdown","b6a5a482":"markdown","82e5648d":"markdown","c8b683a5":"markdown","b46776b6":"markdown"},"source":{"512129fb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras import layers","7e3a7392":"df_train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ndf_test = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')","04c0063a":"df_train.head(7) # check data (first 7 rows)","a5c22c21":"df_test.head(7) # check data (first 7 rows)","5d63efc9":"df_train.shape # check data","1954994e":"df_test.shape # check data","ae59f530":"def plot_stats(df, column, ax, color, angle):\n    \"\"\" PLOT STATS OF DIFFERENT COLUMNS \"\"\"\n    count_classes = df[column].value_counts()\n    ax = sns.barplot(x=count_classes.index, y=count_classes, ax=ax, palette=color)\n    ax.set_title(column.upper(), fontsize=18)\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(angle)","c6733d33":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\nfig.autofmt_xdate()\nplot_stats(df_train, \"Sentiment\", axes, \"Reds\", 45)\nplt.show()","597279ad":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15,5))\nfig.autofmt_xdate()\nplot_stats(df_test, \"Sentiment\", axes, \"Blues\", 45)\nplt.show()","65e609ff":"def number_of_characters(df, label, ax, color):\n    \"\"\" COUNT NUMBER OF CHARACTERS \"\"\"\n    tweet_len = df[df['Sentiment']==label]['OriginalTweet'].str.len()\n    ax.hist(tweet_len, color=color)\n    ax.set_title(label.upper(), fontsize=18)","f37cad7d":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25,7))\nfig.autofmt_xdate()\nfig.suptitle('NUMBER OF CHARACTERS IN EACH CLASS', fontsize=20)\nnumber_of_characters(df_train, \"Negative\", axes[0], '#c31717')\nnumber_of_characters(df_train, \"Positive\", axes[1], '#c36d17')\nnumber_of_characters(df_train, \"Neutral\", axes[2], '#d9b814')\nnumber_of_characters(df_train, \"Extremely Positive\", axes[3], '#80d914')\nnumber_of_characters(df_train, \"Extremely Negative\", axes[4], '#14d9a1')\nplt.show()","99897795":"def number_of_words(df, label, ax, color):\n    \"\"\" COUNT NUMBER OF WORDS \"\"\"\n    tweet_len = df[df['Sentiment']==label]['OriginalTweet'].str.split().map(lambda x: len(x))\n    ax.hist(tweet_len, color=color)\n    ax.set_title(label.upper(), fontsize=18)","2ff896c9":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(25,7))\nfig.autofmt_xdate()\nfig.suptitle('NUMBER OF WORDS IN EACH CLASS', fontsize=20)\nnumber_of_words(df_train, \"Negative\", axes[0], '#14d9d2')\nnumber_of_words(df_train, \"Positive\", axes[1], '#1483d9')\nnumber_of_words(df_train, \"Neutral\", axes[2], '#1417d9')\nnumber_of_words(df_train, \"Extremely Positive\", axes[3], '#7d14d9')\nnumber_of_words(df_train, \"Extremely Negative\", axes[4], '#d914be')\nplt.show()","f1501faf":"def avg_word_length(df, label, ax, color):\n    \"\"\" THE AVERAGE WORD LENGTH \"\"\"\n    word = df[df['Sentiment']==label]['OriginalTweet'].str.split().apply(lambda x : [len(i) for i in x])\n    sns.distplot(word.map(lambda x: np.mean(x)), ax=ax, color=color)","412ac7c9":"fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(30,7))\nfig.autofmt_xdate()\nfig.suptitle('THE AVERAGE WORD LENGTH IN EACH CLASS', fontsize=20)\navg_word_length(df_train, \"Negative\", axes[0], '#39ad5a')\navg_word_length(df_train, \"Positive\", axes[1], '#e3d219')\navg_word_length(df_train, \"Neutral\", axes[2], '#e37419')\navg_word_length(df_train, \"Extremely Positive\", axes[3], '#e34519')\navg_word_length(df_train, \"Extremely Negative\", axes[4], '#e31970')\nplt.show()","79882d06":"y = df_train[\"Sentiment\"].map({\"Negative\":0,\n                               \"Positive\": 1,\n                               \"Neutral\": 2,\n                               \"Extremely Positive\": 3,\n                               \"Extremely Negative\": 4\n                               })\n\ny_test = df_test[\"Sentiment\"].map({\"Negative\":0,\n                                   \"Positive\": 1,\n                                   \"Neutral\": 2,\n                                   \"Extremely Positive\": 3,\n                                   \"Extremely Negative\": 4\n                                                })\n","934a0152":"# get the number of classes\nn_classes = df_train[\"Sentiment\"].nunique()\nn_classes","528dd6a0":"# extract text data from dataframe \nX = df_train['OriginalTweet'].to_numpy()\nX_test = df_test['OriginalTweet'].to_numpy()","3c5a10ba":"# TextVectorization transforms a batch of strings into either a list of token indices or a dense representation\ntext_vectorizer = TextVectorization(max_tokens=10000, \n                                    standardize=\"lower_and_strip_punctuation\", \n                                    output_sequence_length=15)","5b6c0474":"# adapt dataset\ntext_vectorizer.adapt(X)","809e8222":"# example\ntext_vectorizer([\"Computer vision and deep learning\"])","4137b788":"# the get_vocabulary() function provides the vocabulary to build a metadata file with one token per line\nwords = text_vectorizer.get_vocabulary()\n# The vocabulary contains the padding token ('') and OOV token ('[UNK]') as well as the passed tokens\nwords[:10] # check data","fd1fac7f":"# turns positive integers (indexes) into dense vectors of fixed size\nembedding = Embedding(input_dim=10000, output_dim=128, input_length=15, name = 'embeding_1')\nembedding","772e254e":"# example\nsample_embed = embedding(text_vectorizer([\"Computer vision and deep learning\"]))\nsample_embed","f3fb0302":"def calculate_results(y_true, y_pred):\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    model_results = {\"accuracy\": model_accuracy,\n                     \"precision\": model_precision,\n                     \"recall\": model_recall,\n                     \"f1\": model_f1}\n    return model_results","cd4c2b46":"def plot_NN_history(model_history, suptitle):\n    # plot data\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n    fig.suptitle(suptitle, fontsize=18)\n    \n    axes[0].plot(model_history.history['accuracy'], label='train accuracy', color='g', axes=axes[0])\n    axes[0].plot(model_history.history['val_accuracy'], label='val accuracy', color='r', axes=axes[0])\n    axes[0].set_title(\"Model Accuracy\", fontsize=16) \n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(model_history.history['loss'], label='train loss', color='g', axes=axes[1])\n    axes[1].plot(model_history.history['val_loss'], label='val loss', color='r', axes=axes[1])\n    axes[1].set_title(\"Model Loss\", fontsize=16) \n    axes[1].legend(loc='upper left')\n\n    plt.show()","65def27b":"def run_model(inputs, outputs, name, epochs, NN_name, suptitle):\n    \"\"\" GENERAL FUNCTION FOR RUNNING NEURAL NETWORK MODELS\"\"\"\n    \n    # create model\n    model = tf.keras.Model(inputs, outputs, name=name)\n    # compile model\n    model.compile(loss='sparse_categorical_crossentropy', \n                  optimizer=tf.keras.optimizers.Adam(), \n                  metrics=[\"accuracy\"])\n    # check model\n    model.summary()\n    print()\n    # train model\n    print(\"...training model...\")\n    model_history = model.fit(X, \n                              y, \n                              epochs=epochs, \n                              validation_data=(X_test, y_test),\n                              verbose=True)\n    print()\n    # check on test data\n    print(\"...evaluating model...\")\n    model.evaluate(X_test, y_test)\n    print()\n    \n    # check shape\n    print(\"y_test.shape = \", y_test.shape)\n    print()\n    \n    # get the probabilities\n    y_prob = model.predict(X_test)\n    # get the classes\n    y_hat = y_prob.argmax(axis=-1) \n    # see the test labels\n    print(\"y_test =\\n\", y_test)\n    print()\n    # check results\n    res = calculate_results(y_test, y_hat)\n    res = pd.DataFrame([res])\n    res.insert(0, \"model\", NN_name)\n    # visualize NN history\n    plot_NN_history(model_history, suptitle)\n    return model, res","2107a68f":"# setting inputs and outputs of NN\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.LSTM(64)(x)\noutputs = layers.Dense(units = n_classes, activation = 'softmax')(x)","01663f7e":"# run our model\nmodel, res = run_model(inputs, outputs, \"model_LSTM\", 5, \"LSTM\", \"LSTM TRAIN HISTORY\")\nres # print the accuracy metrics results","777ac162":"Let's check the **average word length**. ","c5a9b74e":"# 3. Visualize data\nLet's build some interesting plots.","18559ee1":"# 5. TextVectorization\nTextVectorization transforms a batch of strings into either a list of token indices or a dense representation","4ac5bedf":"# 1. Import libraries","06804201":"Let's count **the number of words** in each class.","19fe1e00":"# 4. Prepare data\nHere we convert text labels to **numeric labels** (e.g. \"Sentiment\" = {0,1,..4}).","411c5edb":"# Coronavirus tweets classification\nHello everyone! In this new notebook we are going to classify coronavirus tweets.\n#### References\nFor this notebook I would like to say thank you some authors for their notebooks that have inspired me to write own notebook:\n1. [AMAN MIGLANI. COVID 19 Tweets EDA & Viz](https:\/\/www.kaggle.com\/datatattle\/covid-19-tweets-eda-viz)\n","9bf4d526":"# 6. Embedding\nEmbedding turns positive integers (indexes) into dense vectors of fixed size","43ffdb5a":"Let's build some distribution plots. The first one plots **the number of characters** in each class (train dataset).","7e5c4c9c":"Let's check **how many records each class have** (train dataset).","48d53ef8":"# 7.1. LSTM\nLong short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition and anomaly detection in network traffic or IDSs (intrusion detection systems).","9cd16b3a":"After that, we count unique values to get the **number of classes**.","b6a5a482":"# 8. Conclusion\nThank you for reading my new article! Hope, you liked it and it was interesting for you! There are some more my articles:\n\n* First Part [Coronavirus tweets classification | NLP | GRU](https:\/\/www.kaggle.com\/maricinnamon\/coronavirus-tweets-classification-nlp-gru)\n\n*Other Notebooks:*\n* [Contradictory, My Dear Watson | nlp | tensorflow](https:\/\/www.kaggle.com\/maricinnamon\/contradictory-my-dear-watson-nlp-tensorflow)\n* [SMS spam with NBC | NLP | sklearn](https:\/\/www.kaggle.com\/maricinnamon\/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https:\/\/www.kaggle.com\/maricinnamon\/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https:\/\/www.kaggle.com\/maricinnamon\/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https:\/\/www.kaggle.com\/maricinnamon\/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https:\/\/www.kaggle.com\/maricinnamon\/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https:\/\/www.kaggle.com\/maricinnamon\/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https:\/\/www.kaggle.com\/maricinnamon\/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https:\/\/www.kaggle.com\/maricinnamon\/retail-trade-report-department-stores-lstm)","82e5648d":"# 2. Read data","c8b683a5":"Let's check **how many records each class have** (test dataset).","b46776b6":"# 7. Train our models"}}