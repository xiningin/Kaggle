{"cell_type":{"b02386ee":"code","d82cf1ea":"code","68d1f888":"code","ae2956dd":"code","4f2bfdbe":"code","d5b60f1d":"code","3a049e40":"code","6e242cd0":"code","f81bd830":"code","de6c7529":"code","31732913":"code","a8777e8a":"code","d60149ff":"code","3519be51":"code","59196de7":"code","d14ce1b2":"code","cc0879b0":"code","b18b3ca1":"code","391570df":"code","8f194fb0":"code","b0c90a0b":"code","4aec62e3":"code","1beca79e":"code","ef87627a":"code","792a017f":"code","b4376323":"code","831f3341":"code","dc0934e3":"code","f82fe964":"code","0c52d39f":"code","a733cefb":"code","b545f1ac":"code","c1231a46":"code","54af23e2":"code","1d05c34b":"markdown","0a870e6f":"markdown","f86cf201":"markdown","c9a50b08":"markdown","21cce901":"markdown","c5ce6c5f":"markdown","688c65e4":"markdown","343724ea":"markdown","a45511b0":"markdown","a6978794":"markdown"},"source":{"b02386ee":"PAIRS_LIST        = [\"EURUSD\", \"USDJPY\", \"EURJPY\"] #, \"USDCHF\", \"EURCHF\"] #, \"AUDUSD\" ]\nPREDICTING_PAIR   = \"EURUSD\" \nPREDICTING_COLUMN = \"close\"\nLOOK_BACK         = 30 # 20 * 15 six hour\nSPLIT             = 0.95 # data split ration for training and testing","d82cf1ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","68d1f888":"# install talib\n%cd \/kaggle\/working\n%rm -rf temp\n!mkdir temp\n%cd .\/temp\n!wget http:\/\/prdownloads.sourceforge.net\/ta-lib\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz\n%cd .\/ta-lib\n!.\/configure --prefix=\/usr\n!make\n!make install\n!pip install Ta-Lib\n%cd \/kaggle\/working\n!rm -rf temp","ae2956dd":"# import libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Input, LSTM, Dense, Flatten\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import TensorBoard\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA, KernelPCA\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Used TA-Lib for creating additional features. More on this later.\nfrom talib.abstract import *\nfrom talib import MA_Type\n\nimport datetime","4f2bfdbe":"# loading data\ndata = {}\n\nfor pair_name in PAIRS_LIST:\n    data[pair_name] = pd.read_csv(\"\/kaggle\/input\/forex-top-currency-pairs-20002020\/\"+pair_name+\"-2000-2020-15m.csv\")\n\n# normalize data shape and format\ndef norm_data_shape_format(df):\n    orig_cols = [\"DATE_TIME\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\"]\n    cols_name = [\"timestamp\", \"open\", \"high\", \"low\", \"close\"]\n    df.rename(columns=dict(zip(orig_cols, cols_name)), inplace=True)\n    df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)\n    df.set_index(\"timestamp\", inplace=True)\n    df = df.reindex(columns=cols_name[1:])\n    return df.astype(float)\n    \nfor key in data:\n    data[key] = norm_data_shape_format(data[key])","d5b60f1d":"data[\"EURUSD\"].head()","3a049e40":"def extract_features(df):\n    df['hour'] = df.index.hour\n    df['day']  = df.index.weekday\n    df['week'] = df.index.week\n    # df['volume'] = pd.to_numeric(df['volume'])\n    df['close']  = pd.to_numeric(df['close'])\n    df['open']   = pd.to_numeric(df['open'])\n    # df['momentum']   = df['volume'] * (df['open'] - df['close'])\n    df['avg_price']  = (df['low'] + df['high'])\/2\n    df['range']      = df['high'] - df['low']\n    df['ohlc_price'] = (df['low'] + df['high'] + df['open'] + df['close'])\/4\n    df['oc_diff']      = df['open'] - df['close']\n    # df['spread_open']  = df['ask_open'] - df['bid_open']\n    # df['spread_close'] = df['ask_close'] - df['bid_close']\n    inputs = {\n        'open'   : df['open'].values,\n        'high'   : df['high'].values,\n        'low'    : df['low'].values,\n        'close'  : df['close'].values,\n        'volume' : np.zeros(df['close'].shape[0]) # for sake of using TA lib\n    }\n    df['ema'] = MA(inputs, timeperiod=15, matype=MA_Type.T3)\n    df['bear_power'] = df['low'] - df['ema']\n    df['bull_power'] = df['high'] - df['ema']\n    # Since computing EMA leave some of the rows empty, we want to remove them. (EMA is a lagging indicator)\n    df.dropna(inplace=True)\n    # Add 1D PCA vector as a feature as well. This helped increasing the accuracy by adding more variance to the feature set\n    pca_input = df.drop('close', axis=1).copy()\n    pca_features = pca_input.columns.tolist()\n    pca = PCA(n_components=1)\n    df['pca'] = pca.fit_transform(pca_input.values.astype('float32'))\n\ncolumns_order = [\"open\", \"high\", \"low\", \"close\", \"hour\", \"day\", \"week\", \"avg_price\", \"range\", \"ohlc_price\", \"oc_diff\", \"ema\", \"bear_power\", \"bull_power\", \"pca\"]    ","6e242cd0":"for key in data:\n    extract_features(data[key])","f81bd830":"data[\"EURUSD\"].head()","de6c7529":"# plt.plot(data['EURUSD'][2900:5150][\"close\"])\n# plt.plot(data['EURUSD'][2900:5150][\"ema\"])\n# plt.plot(data['EURUSD'][1900:2150][\"bull_power\"])\n# plt.plot(data['EURUSD'][1900:2150][\"bear_power\"])\n# plt.show()","31732913":"# sort and rename column names\ndef sort_and_rename(df, suffix):\n    cols_name = [c + suffix for c in columns_order]    \n    df.rename(columns=dict(zip(columns_order,cols_name)), inplace=True)\n    df = df.reindex(columns=cols_name, copy=False)\n    return df\n    \nall_columns = [] # to save the correct order of data\n\nfor key in PAIRS_LIST:\n    data[key] = sort_and_rename(data[key], \"_\" + key)\n    all_columns += list(data[key].columns) # to save the correct order of data","a8777e8a":"# merge \nmerged_data = pd.DataFrame(data[PAIRS_LIST[0]])\nfor key in PAIRS_LIST[1:]:\n    merged_data = merged_data.merge(data[key], how=\"inner\", left_index=True, right_index=True)\n    \nmerged_data = merged_data.reindex(columns=all_columns)\n\n# drop duplicate columns\nfor key in PAIRS_LIST[1:]:\n    merged_data.drop(columns=[\n        \"hour_\"+key,\n        \"day_\" +key,\n        \"week_\"+key,\n    ],inplace=True)\n    \nmerged_data.head()","d60149ff":"# seeing correlation between columns\ncorr = merged_data.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nf, ax = plt.subplots(figsize=(15, 15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, ax=ax)","3519be51":"merged_data.head()","59196de7":"def create_dataset(df, look_back=10):\n    dataX, dataY = [], []\n    for i in range(len(df)-look_back-1):\n        a = df[i:(i+look_back)]\n        dataX.append(a)\n        dataY.append(df[i + look_back])\n    return np.array(dataX), np.array(dataY)","d14ce1b2":"# Scale reshape and group the data\n\ntarget_column_name = PREDICTING_COLUMN + \"_\" + PREDICTING_PAIR\n\n# Create scalers\nscaler = MinMaxScaler()\nscaled = pd.DataFrame(scaler.fit_transform(merged_data), columns=merged_data.columns)\n\nx_scaler = MinMaxScaler(feature_range=(0, 1))\nx_scaler = x_scaler.fit(merged_data.values.astype('float32'))\ny_scaler = MinMaxScaler(feature_range=(0, 1))\ny_scaler = y_scaler.fit(merged_data[target_column_name].values.astype('float32').reshape(-1,1))\n\n# Create dataset\ntarget_index = scaled.columns.tolist().index(target_column_name)\ndataset = scaled.values.astype('float32')\n\nX, y = create_dataset(dataset, look_back=LOOK_BACK)\ny = y[:,target_index]\n\ntrain_size = int(len(X) * SPLIT)\ntrainX = X[:train_size]\ntrainY = y[:train_size]\ntestX = X[train_size:]\ntestY = y[train_size:]","cc0879b0":"print(\"all data shape:\", X.shape)\nprint(\"train data shape:\", trainX.shape)\nprint(\"test data shape:\", testX.shape)","b18b3ca1":"def create_model():\n    model = Sequential()\n    model.add(LSTM(20, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n    model.add(LSTM(20, return_sequences=True))\n    model.add(LSTM(10, return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(4, return_sequences=False))\n    model.add(Dense(4, kernel_initializer='uniform', activation='relu'))\n    model.add(Dense(1, kernel_initializer='uniform', activation='relu'))\n    \n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae', 'mse'])\n    print(model.summary())\n    \n    return model","391570df":"model = create_model()","8f194fb0":"# Save the best weight during training.\nfrom keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor='val_mse', verbose=1, save_best_only=True, mode='min')\n\n# Monitor the trianing progress via TensorBoard\n# log_dir = \"logs\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n# tensorboard = TensorBoard(log_dir=log_dir)\n\n# Callbacks\ncallbacks_list = [checkpoint] # , tensorboard]\n\n# Fit\nhistory = model.fit(trainX, trainY, epochs=200, batch_size=512, verbose=1, callbacks=callbacks_list, validation_split=0.1)","b0c90a0b":"def visualize_history():\n    epoch = len(history.history['loss'])\n    for k in list(history.history.keys()):\n        if 'val' not in k:\n            plt.figure(figsize=(40,10))\n            plt.plot(history.history[k])\n            plt.plot(history.history['val_' + k])\n            plt.title(k)\n            plt.ylabel(k)\n            plt.xlabel('epoch')\n            plt.legend(['train', 'test'], loc='upper left')\n            plt.show()","4aec62e3":"visualize_history()","1beca79e":"# To improve the weights towards the global optimal, I retrained the model with LearningRateScheduler added\n\nfrom keras.callbacks import LearningRateScheduler\nimport keras.backend as K\ndef scheduler(epoch):\n    if epoch%10==0 and epoch!=0:\n        lr = K.get_value(model.optimizer.lr)\n        K.set_value(model.optimizer.lr, lr*.9)\n        print(\"lr changed to {}\".format(lr*.9))\n    return K.get_value(model.optimizer.lr)\nlr_decay = LearningRateScheduler(scheduler)\ncallbacks_list = [checkpoint, lr_decay] # , tensorboard]\nhistory = model.fit(trainX, trainY, epochs=3, batch_size=1024, callbacks=callbacks_list, validation_split=0.1)","ef87627a":"visualize_history()","792a017f":"model.load_weights(\"weights.best.hdf5\") # load best validation ","b4376323":"pred = model.predict(testX)","831f3341":"from sklearn.metrics import mean_absolute_error \n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['actual'] = testY\npredictions = predictions.astype(float)\n\npredictions.plot(figsize=(20,10))\nplt.show()\n\npredictions['diff'] = predictions['predicted'] - predictions['actual']\nplt.figure(figsize=(10,10))\nsns.distplot(predictions['diff']);\nplt.title('Distribution of differences between actual and prediction')\nplt.show()\n\nprint(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['actual'].values))\nprint(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['actual'].values))\npredictions['diff'].describe()","dc0934e3":"pred = model.predict(testX)","f82fe964":"pred = y_scaler.inverse_transform(pred)\nclose = y_scaler.inverse_transform(np.reshape(testY, (testY.shape[0], 1)))\n\npredictions = pd.DataFrame()\npredictions['predicted'] = pd.Series(np.reshape(pred, (pred.shape[0])))\npredictions['close'] = pd.Series(np.reshape(close, (close.shape[0])))","0c52d39f":"p = merged_data[-pred.shape[0]:].copy()\npredictions.index = p.index\npredictions = predictions.astype(float)\npredictions = predictions.merge(p[['low_'+PREDICTING_PAIR, 'high_'+PREDICTING_PAIR]], right_index=True, left_index=True)","a733cefb":"zoom = 200\n\nax = predictions[:zoom].plot(y='close', c='red', figsize=(40,10))\nax = predictions[:zoom].plot(y='predicted', c='blue', figsize=(40,10), ax=ax)\nindex = [str(item) for item in predictions[:zoom].index]\nplt.fill_between(x=index, y1='low_'+PREDICTING_PAIR, y2='high_'+PREDICTING_PAIR, data=p[:zoom], alpha=0.4)\nplt.title('Prediction vs Actual (low and high as blue region)')\nplt.show()","b545f1ac":"predictions['diff'] = predictions['predicted'] - predictions['close']\nplt.figure(figsize=(10,10))\nsns.distplot(predictions['diff']);\nplt.title('Distribution of differences between actual and prediction ')\nplt.show()","c1231a46":"g = sns.jointplot(\"diff\", \"predicted\", data=predictions, kind=\"kde\", space=0)\nplt.title('Distributtion of error and price')\nplt.show()","54af23e2":"print(\"MSE : \", mean_squared_error(predictions['predicted'].values, predictions['close'].values))\nprint(\"MAE : \", mean_absolute_error(predictions['predicted'].values, predictions['close'].values))\npredictions['diff'].describe()","1d05c34b":"# Merge All Pairs","0a870e6f":"# Shaping Data","f86cf201":"# EURUSD 15 Minutes Prediction Using Several Currency Pairs Feed Simultaneously\n\nThis notebook is base on this article \n[medium.com\/daveyungookim](https:\/\/medium.com\/@daveyungookim\/forex-usdcad-predict-price-every-15-minutes-116554a424b5)\n","c9a50b08":"# A Little Exploration","21cce901":"# Installing and loading Dependencies","c5ce6c5f":"# Compare the unscaled values and see if the prediction falls within the Low and High","688c65e4":"# Plot Scaled Predictions vs Scaled Actual Price","343724ea":"# Testing\nOnce the training was complete, I\u2019ve loaded the best weights discovered to my model and checked if the prediction worked as intended.","a45511b0":"# Loading Data","a6978794":"   # Define Constants"}}