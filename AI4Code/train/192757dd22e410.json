{"cell_type":{"2cf10132":"code","96fd6f68":"code","64e98b29":"code","29d3e1b7":"code","cc421c73":"code","3afe3efa":"code","7059b598":"code","e9bd7b27":"code","73edb8a1":"code","9a9ffc73":"code","61a6ffde":"code","e33cc86f":"code","bfdf1bac":"markdown","d5fec945":"markdown","66a3bdfd":"markdown","fbe387b8":"markdown","fefeb25c":"markdown","b890883e":"markdown","c10f617d":"markdown","244dfa4d":"markdown","53c03b69":"markdown","081ddbf7":"markdown","4103722b":"markdown"},"source":{"2cf10132":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb","96fd6f68":"train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")\n\n# I'll remove the ID column of both train and test, cause they are of no use here\ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace =True)\n\n# Check\nprint('train columns', train.columns)\nprint('test columns', test.columns)","64e98b29":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.2, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)\n\n# Standard scaling\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nx_train = pd.DataFrame(ss.fit_transform(x_train))\nx_test = pd.DataFrame(ss.fit_transform(x_test))","29d3e1b7":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA()\npca_train = pca.fit_transform(x_train)","cc421c73":"explained_variance = pca.explained_variance_ratio_","3afe3efa":"plt.plot(explained_variance)\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')\nplt.title(\"Scree Plot\");","7059b598":"plt.plot(np.cumsum(explained_variance))\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.title(\"Counting PC's\");","e9bd7b27":"\"\"\"\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Initantiate model with 500 decision trees\nrf = RandomForestRegressor(n_estimators=500, max_depth = 20, \n                           min_samples_split = 50, min_samples_leaf = 10,\n                           verbose = 1)\n\n# Train the model on training data\nrf.fit(pca_train, y_train)\n# Use the forest's predict method on the test data\ny_pred = rf.predict(pca_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')\n\n\"\"\"","73edb8a1":"# Splitting\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split( train.iloc[: ,:-1], train.iloc[:,-1], \n                                                    test_size=0.2, random_state=2)\nprint(\"X-train shape\", x_train.shape, \"\\nY-test shape\" ,y_test.shape)","9a9ffc73":"model = lgb.LGBMRegressor(num_iteration = 800,  learning_rate=0.03, n_estimators=150, \n                           max_depth = 5, random_state = 10 )\nmodel.fit(x_train, y_train)","61a6ffde":"# Use the forest's predict method on the test data\ny_pred = model.predict(x_test)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n# Print out the mean absolute error (mae)\nprint('Base line Mean Absolute Error', round(np.mean(errors), 4), 'degrees.')\n# Print score top check overfiting\nprint(\"train Score -\", model.score(x_train,y_train))\nprint(\"test Score -\", model.score(x_test,y_test))","e33cc86f":"submission['loss'] = model.predict(test)\nsubmission.to_csv('submission.csv', index = False)","bfdf1bac":"# Approach PCA","d5fec945":"### Inference\nWell the above images are disheartening, i was expecting plots that could help me getting the exact number of components. But the data lacked collinearity to a great extent and thus PCA will not be that helpful i am assuming. \n* Still I performed the Random forest on colab with the PCA data, I am not  running the code here, since it will a long time. I'll add the error value and the run time. ","66a3bdfd":"This notebook is a continuation of TPS-Aug-Random Forest(https:\/\/www.kaggle.com\/sabyasachi96\/tps-aug-random-forest). \n* Recap: Well in the last notebook, the results were not that promising and also the runtime of Random Forest was too long. \n    * Error = 6.1751\n    * Run time = 170 min\n* My thoughts were, since the dataset is too huge a PCA can help improve the Random Forest predictions, but things didn't turn out the way i wanted to. \n","fbe387b8":"# Model With Lgbm","fefeb25c":"Now according to literature, i'll get maximum variance explained by first few Principle components. Thus haven't put any cap on the number of PC's. I'll know how many PC's needed by plotting cumulative variance ","b890883e":"Well the error value reduced from 6.17 to 6.16, not too great, but something atleast. \n* P.s. Do suggest other methods for a beginner to try out in such datasets or cases.","c10f617d":"The above parameters were not randomly picked, rather its an outcome of multiple trial and errors. Since it's my first time with LGBM, next time onwards i'll be using a automated way like a loop or something else. Please suggest if you have anything. ","244dfa4d":"Error ~ 6.245;  Runtime - 122 min","53c03b69":"# Inference From the last notebook\n\nWell the features were too less correlated, both positively and negatively with each other and the target.\n* conducted Random forest with error 6.17 but long runtime","081ddbf7":"P.S. I have changed the test size from 0.4 to 0.2, just to make the prediction better","4103722b":"With the help of PCA i'll reduce the dimensions and then check their correlations before modelling and predicting"}}