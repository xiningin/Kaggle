{"cell_type":{"9e75ce3b":"code","e50ad89a":"code","7ec28d83":"code","a6c69bae":"code","5a10c39a":"code","c7162303":"code","8a70d143":"code","0294ecf8":"code","7a787ed1":"code","9481bafb":"code","b308a6c6":"code","f417dc8d":"code","bf740b15":"code","2e71390a":"code","8432b9d1":"code","8db2c25b":"markdown","382b2c3a":"markdown","945232d1":"markdown","90fde31a":"markdown","3ce999a8":"markdown","471b1d94":"markdown","f2bfa4f1":"markdown"},"source":{"9e75ce3b":"import numpy as np \nimport pandas as pd\nimport re","e50ad89a":"airbnb=pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\nairbnb.head()","7ec28d83":"import spacy\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\n\n# Downloading existing universal models\nnltk.download('popular')\nnlp = spacy.load(\"en_core_web_sm\") # deal with English documents\n# link: https:\/\/spacy.io\/models\/en","a6c69bae":"def stripTitle(doc):\n    return re.sub('[^A-Za-z0-9]+', ' ', doc) # \u5c06\u975e A-Z a-z 0-9 \u7684\u5b57\u7b26\u8f6c\u6362\u6210\u4e3a\u7a7a\u683c","5a10c39a":"# deal with each listing statement -> we get a vector\ndef generateAverageVector(test_string):\n    test_string = stripTitle(test_string)\n    summed_vectors = np.zeros(96) # spacy \u4f7f\u7528\u4e00\u4e2a 96 * 1\u7684vector\u8868\u793a\u4e0d\u540c\u7c7b\u522b\u7684\u8bcd\u6c47\n    word_count = 0\n    # nlp(test_string) returns a Doc object \n    # Link explains what Doc object contains: https:\/\/spacy.io\/api\/doc\n    for token in nlp(test_string): \n        if token.has_vector:\n            summed_vectors = np.add(summed_vectors, token.vector)\n            word_count = word_count + 1\n    return np.true_divide(summed_vectors, word_count)","c7162303":"stop_words = set(stopwords.words('english'))\nnouns = ['NN', 'NNS', 'NNP', 'NNPS', 'PRP']\nverbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\nadjv = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n\n# NLP pipeline = tokenization -> tagging\ndef filterTitle(title, POS): # POS is list of parts of speech wanted included in the filtered in title\n    # Word tokenizers is used to find the words and punctuation in a string \n    wordsList = nltk.word_tokenize(title) \n    \n    # removing stop words from wordList  (\u53bb\u9664\u505c\u7528\u8bcd)\n    wordsList = [w for w in wordsList if not w in stop_words] \n\n    #  Using a Tagger. Which is part-of-speech tagger or POS-tagger.\n    # varibale tagged will be like key-value pair, key is token and value would be the tagging\n    # [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n    tagged = nltk.pos_tag(wordsList)\n    \n    # \u5982\u679c\u8fd9\u4e2atoken\u7684\u8bcd\u7c7b\u578b\u662f\u6211\u4eec\u60f3\u8981\u7684(\u5728POS\u4e2d)\uff0c\u6211\u4eec\u5c31\u5c06\u5176\u7559\u4e0b\n    filtered = [word[0] for word in tagged if word[1] in POS]\n    filteredtitle = ' '.join(filtered[:10])\n    return filteredtitle # return filtered title","8a70d143":"def stripTitle(doc):\n      strip_punc = re.sub(r'[^\\w\\s]','',doc) # \\w\u8868\u793a\u4efb\u610f\u4e00\u4e2a\u5b57\u6bcd\u6216\u6570\u5b57\u6216\u4e0b\u5212\u7ebf \\s\u5339\u914d\u4efb\u4f55\u7a7a\u767d\u5b57\u7b26\n      return re.sub(r'\\w*\\d\\w*', '', strip_punc).strip()","0294ecf8":"# titles is list of Airbnb titles (strings)\ndef generate_ltsm_input(titles):  \n    lengths = [len(title.split()) for title in titles]\n    plen = 10\n    titlevecs = np.zeros(shape=(1, plen, 96))  # baseline shape\n    # print(titlevecs)\n    for title in titles:\n        title = stripTitle(title)\n        summed_vectors = np.zeros(96)\n        title = filterTitle(title, adjv)  # input nouns, verbs, or adjv \n        tokens = nlp(title)\n        for count in range(plen):  # loop through tokens until end, then fill with null tokens\n            if count < len(tokens) and tokens[count].has_vector:\n                summed_vectors = np.vstack((summed_vectors, tokens[count].vector))\n            else:\n                summed_vectors = np.vstack((summed_vectors, np.zeros(96)))\n        titlevecs = np.vstack((titlevecs, summed_vectors[1:][:].reshape(1, plen, 96)))  # add title to the stack of titles\n    return titlevecs[1:][:][:]","7a787ed1":"brooklyn_high = pd.read_csv('..\/input\/nyc-split\/Brooklyn_high.csv', encoding = \"ISO-8859-1\")\nbrooklyn_low = pd.read_csv('..\/input\/nyc-split\/Brooklyn_low.csv', encoding = \"ISO-8859-1\")\n\nmanhattan_high = pd.read_csv('..\/input\/nyc-split\/Manhattan_high.csv', encoding = \"ISO-8859-1\")\nmanhattan_low = pd.read_csv('..\/input\/nyc-split\/Manhattan_low.csv', encoding = \"ISO-8859-1\")\n\nqueens_high = pd.read_csv('..\/input\/nyc-split\/Queens_high.csv', encoding = \"ISO-8859-1\")\nqueens_low = pd.read_csv('..\/input\/nyc-split\/Queens_low.csv', encoding = \"ISO-8859-1\")\n\nbronx_high = pd.read_csv('..\/input\/nyc-split\/Bronx_high.csv', encoding = \"ISO-8859-1\")\nbronx_low = pd.read_csv('..\/input\/nyc-split\/Bronx_low.csv', encoding = \"ISO-8859-1\")\n\nstaten_high = pd.read_csv('..\/input\/nyc-split\/StatenIsland_high.csv', encoding = \"ISO-8859-1\")\nstaten_low = pd.read_csv('..\/input\/nyc-split\/StatenIsland_low.csv', encoding = \"ISO-8859-1\")\n\ndatasets = [brooklyn_high, brooklyn_low, manhattan_high, manhattan_low, queens_high, queen_slow, \n            bronx_high, bronx_low, staten_high, staten_low]\n\n# put the dataset you want to use here\ndataset = brooklyn_high  \ntitles = [title for title in dataset['name']]\nprint(len(titles))\ncombined_data = generate_ltsm_input(titles)  # dataset\nprint(combined_data.shape)","9481bafb":"def convert_categorical(dataset):  # dataset must be sorted\n    base = dataset.min()\n    diff = dataset.max() - base\n    summed_vectors = np.zeros(5)\n    for iteration in range(len(dataset)):\n        entry = np.zeros(5)\n        entry[int(iteration \/ (len(dataset) \/ 5))] = 1\n        summed_vectors = np.vstack((summed_vectors, entry))\n    return summed_vectors","b308a6c6":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nX_df = combined_data\nY_df = dataset['reviews_per_month']\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.1, random_state=99)\nprint(Y_df.shape)","f417dc8d":"from keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split","bf740b15":"# put the dataset you want to use here\ndef eval_lstm(dataset):\n    titles = [title for title in dataset['name']]\n    print(len(titles))\n    combined_data = generate_ltsm_input(titles) \n    X_df = combined_data\n    Y_df = dataset['reviews_per_month']\n    X_train, X_test, Y_train, Y_test = train_test_split(X_df, Y_df, test_size=0.25, random_state=99)  \n    model = Sequential()\n    model.add(LSTM(100, input_shape=(X_df.shape[1], 96)))\n    model.add(Dropout(0.2))\n    model.add(Dense(Y_df.shape[1], activation='sigmoid'))\n    model.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    # Fit model in batches\n    model.fit(X_train, Y_train, nb_epoch=100, batch_size=100)\n    return model","2e71390a":"brooklyn_high = pd.read_csv('..\/input\/nyc-split\/Brooklyn_high.csv', encoding = \"ISO-8859-1\")\nbrooklynhigh_model = eval_lstm(brooklyn_high)\nbrooklynhigh_model.predict(X_test[1:])","8432b9d1":"def optimal_replacement(title):\n    bestscore = 0\n    besttitle = \"\"\n    individual = title.split()\n    for index in range(len(individual)):\n        word = individual[index]\n        doc = nlp(word)\n        # freq = doc[:]._.s2v_freq\n        # vector = doc[:]._.s2v_vec\n        if not doc[:]._.in_s2v:\n            individual[index] = word\n            continue\n        most_similar = doc[:]._.s2v_most_similar(10)\n        for similar in most_similar:\n            replacement = similar[0][0]\n            individual[index] = replacement\n            testtitle = ' '.join(individual)\n            score = RNN\/LTSM(testtitle)  # TODO: replace with LSTM run\n            if score > bestscore:\n                bestscore = score\n                besttitle = testtitle\n        individual[index] = word\n    return besttitle, bestscore","8db2c25b":"#### Code for Creating Training Data\n1. Loop through dataset(s) for titles.\n2. Generate title vectors for each title and create X_train.\n3. Normalize average reviews and create Y_train.","382b2c3a":"#### Create LSTM input\nThe method cleans up each title, and stacks them to create a n x 96 matrix of word vectors for each title, with n being the number of prespecified words in a title.","945232d1":"#### Code for Testing Title Replacements Based on Best Possible Word\n1. Loop through each word in given title.\n2. For each word, generate list of 10 best\/closest words\n3. Replace current word with each closest word and score the new title based on RNN\n4. Return best title based on best score","90fde31a":"#### Code for Cleaning up Titles and Averaging the Vectors\n1. Remove all punctuation, numbers in titles\n2. Tokenize titles by space\n3. Import word2vec\/glove vectors and average them","3ce999a8":"#### Code for Training Keras Model","471b1d94":"#### Code for POS Filtering\n1.Separate title into POS using NLTK\n\n2.Remove words in list of POS\n\n3.Return filtered title","f2bfa4f1":"#### Creating Categorical Output\nThe following method determines what are the best discrete bins to put each continuous output value into."}}