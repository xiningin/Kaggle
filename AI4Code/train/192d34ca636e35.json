{"cell_type":{"0cae3767":"code","795553da":"code","2fa87b11":"code","7b92b4a7":"code","bf01b92a":"code","a0e787a2":"code","371f5f39":"code","60794db3":"code","ed8d858e":"code","67acc8e8":"code","44637976":"code","9ea99635":"code","a0b9593f":"code","83f423ef":"code","366a0a5e":"code","02ea22d6":"code","bedcbf27":"code","d71eb59c":"code","cb35daa8":"code","a39ae294":"code","9070e334":"code","feb65082":"code","1556a3c7":"code","6e9c0ea8":"code","c3236fd2":"markdown","c52d4c72":"markdown","c35612e5":"markdown","7d6b6f56":"markdown","457d21cc":"markdown","17488b2e":"markdown","b15e8f3d":"markdown","06311ebe":"markdown","14e50f66":"markdown","d1b8e715":"markdown","d722bcd4":"markdown","7d776089":"markdown","2273b54f":"markdown","f2fd5ed6":"markdown"},"source":{"0cae3767":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, SpatialDropout1D, Dense, LSTM, \\\nBidirectional, Lambda, Conv1D, MaxPooling1D, GRU,GlobalMaxPooling1D,GlobalAveragePooling1D, concatenate\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import *\nimport numpy as np\nimport pandas as pd\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras import Sequential","795553da":"class CyclicLR(Callback):\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())","2fa87b11":"data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","7b92b4a7":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(data['text'].values)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","bf01b92a":"embeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt','r',encoding='utf-8')\nfor line in f:\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray([float(val) for val in values[1:]])\n    embeddings_index[word] = coefs\nf.close()\nprint('\\nFound %s word vectors.' % len(embeddings_index))\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint('Found %s word vectors.' % len(embedding_matrix))\ndel embedding_vector,embeddings_index","a0e787a2":"X_train,X_test, y_train, y_test = train_test_split(pad_text,data['target'].values,\n                                                   test_size=0.33,shuffle=True,random_state=124, stratify=data['target'])","371f5f39":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\n\nhidden_states = Bidirectional(LSTM(units=300, return_sequences=True))(text_embed)\nglobal_max_pooling = Lambda(lambda x: K.max(x, axis=1))  # GlobalMaxPooling1D didn't support masking\nsentence_embed = global_max_pooling(hidden_states)\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nBiLSTM = Model(input_text, output)\nBiLSTM.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = BiLSTM.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","60794db3":"val_preds = BiLSTM.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","ed8d858e":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\nconv_layer = Conv1D(300, kernel_size=3, padding=\"valid\", activation='relu')(text_embed)\nconv_max_pool = MaxPooling1D(pool_size=2)(conv_layer)\n\ngru_layer = Bidirectional(GRU(300, return_sequences=True))(conv_max_pool)\nsentence_embed = GlobalMaxPooling1D()(gru_layer)\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nCNNRNN = Model(input_text, output)\nCNNRNN.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = CNNRNN.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","67acc8e8":"val_preds = CNNRNN.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","44637976":"input_text = Input(shape=(60,),dtype='int64')\n\nembedding_layer = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],\n                            weights=[embedding_matrix],\n                            trainable=False, mask_zero=True)(input_text)\ntext_embed = SpatialDropout1D(0.4)(embedding_layer)\ngru_layer = Bidirectional(GRU(300, return_sequences=True))(text_embed)\n\nconv_layer = Conv1D(64, kernel_size=2, padding=\"valid\", kernel_initializer=\"he_uniform\")(gru_layer)\n\navg_pool = GlobalAveragePooling1D()(conv_layer)\nmax_pool = GlobalMaxPooling1D()(conv_layer)\nsentence_embed = concatenate([avg_pool, max_pool])\n\ndense_layer = Dense(256, activation='relu')(sentence_embed)\noutput = Dense(1, activation='sigmoid')(dense_layer)\n\nRNNCNN = Model(input_text, output)\n\nRNNCNN.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=Adam(6e-6))\n\nlr_callback = CyclicLR()\nhistory = RNNCNN.fit(X_train,y_train, batch_size=5, epochs=3,\n                       validation_data=(X_test,y_test), callbacks=[lr_callback])","9ea99635":"val_preds = RNNCNN.predict(X_test)\nval_preds = np.round(val_preds).astype(int)\nprint(classification_report(y_test,val_preds,target_names = ['Not Relevant', 'Relevant']))","a0b9593f":"train,_, y_train, _ = train_test_split(data['text'].values,data['target'].values,\n                                                   test_size=0.2,shuffle=True,random_state=124, stratify=data['target'])","83f423ef":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\n# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n# tokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(train)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","366a0a5e":"bilstm=BiLSTM.predict(pad_text)\ncr = CNNRNN.predict(pad_text)\nrc = RNNCNN.predict(pad_text)","02ea22d6":"prediction = pd.DataFrame({\"BiLSTM\":bilstm.flatten(),\"CR\":cr.flatten(),\"RC\":rc.flatten(),\"target\":y_train})","bedcbf27":"## Yeah this network is simple as f***, but still scores 80%.....\nclf = Sequential([\n    Dense(3,activation = 'relu'),\n    Dense(1,activation= 'sigmoid')\n])\nclf.compile(loss = 'binary_crossentropy',optimizer = Adam(3e-5),metrics = ['acc'])","d71eb59c":"history =clf.fit(prediction.loc[:,['BiLSTM','CR','RC']].values,prediction.iloc[:,-1],validation_split= 0.1,batch_size = 5,epochs = 32)","cb35daa8":"test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","a39ae294":"MAX_SEQUENCE_LENGTH = 60\nMAX_NB_WORDS = 30000\nEMBEDDING_DIM = 300\n# tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n# tokenizer.fit_on_texts(data['text'].values)\nsequences = tokenizer.texts_to_sequences(test.text.values)\n\n# word_index = tokenizer.word_index\n# print('Found %s unique tokens.' % len(word_index))\n\npad_text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)","9070e334":"bilstm_test = BiLSTM.predict(pad_text).flatten()\ncr_test = CNNRNN.predict(pad_text).flatten()\nrc_test = RNNCNN.predict(pad_text).flatten()","feb65082":"test_predictions = clf.predict(np.stack([bilstm_test,cr_test,rc_test],axis=-1)).flatten()\ntest_predictions = np.round(test_predictions).astype(int)","1556a3c7":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission['target'] = test_predictions","6e9c0ea8":"submission.to_csv(\"\/kaggle\/working\/submission.csv\",index = False)","c3236fd2":"# Loading the embeddings","c52d4c72":"### Evaluation","c35612e5":"## Data Processing","7d6b6f56":"# \u091a\u0932\u094b \u092d\u0935\u093f\u0937\u094d\u092f\u0935\u093e\u0923\u0940 \u0915\u0930\u0924\u0947 \u0939\u0948\u0902","457d21cc":"# BiLSTM Network\nAs you could see I have applied a Global max pooling on the output(*hidden states*) from the Bidirection-LSTM. ","17488b2e":"## RNNCNN\nThis network uses output from 1D convolution and applies average pooling and Max pooling and concatenates the output from both pooling into single.","b15e8f3d":"## CNNRNN\nI have used CNN to work on finding relevant data in the sequence, whereas for recurrent operations I have used Bidirection GRU, which is again max pooled.","06311ebe":"## Cyclic Learning Rate {class}","14e50f66":"I just can't stop myself from using pandas, they are really fantastic","d1b8e715":"# Bi-LSTM, CNNRNN, RNNCNN","d722bcd4":"## Let stack the output to a linear regressor","7d776089":"### Evaluation","2273b54f":"This notebook consists of network model made using Tensorflow and traditional recurrent nn algorithms. <br>\nThe data it is trained on is from [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started) and along with that I have used GloVe for embedding matrix<br>\nI have also used a method to make learning faster by using Cyclic learning rate method. The model use _*triangular*_ policy to deal with the training.\nIf you find this kernel helpful enough then please  **Upvote**","f2fd5ed6":"## Libraries"}}