{"cell_type":{"ee82575a":"code","c0158a2a":"code","f2c407d2":"code","5c56b018":"code","8f22d26a":"code","449faa5c":"code","9a48bc77":"code","4c28793a":"code","95518893":"code","1597202f":"code","547a3ef8":"code","fe84fc96":"code","4f0df316":"code","ed3721f8":"code","5c4032ab":"code","78039f04":"code","2ebb3d8a":"code","2b251d32":"code","6c1ccd51":"code","0b40217c":"code","0a0b961b":"code","6be996de":"markdown","f0b5f4ef":"markdown","cb55d96d":"markdown","50a66b4d":"markdown","122714b4":"markdown","da75ff53":"markdown","6b46899d":"markdown","61326a69":"markdown","11a867f4":"markdown","90fb670a":"markdown"},"source":{"ee82575a":"!nvidia-smi","c0158a2a":"!pip install  resnest > \/dev\/null","f2c407d2":"# for TPU\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py > \/dev\/null\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev > \/dev\/null","5c56b018":"import torch_xla\nimport torch_xla.core.xla_model as xm","8f22d26a":"import numpy as np\nimport librosa as lb\nimport soundfile as sf\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom  torch.utils.data import Dataset, DataLoader\n\nfrom tqdm.notebook import tqdm\n\nfrom resnest.torch import resnest50\n\nimport time","449faa5c":"# Data Loader\nNUM_CLASSES = 24\nSR = 32_000\nDURATION =  10\nSTRIDE = 5\n\n\n# Neural Net\nTEST_BATCH_SIZE = 30\nTEST_NUM_WORKERS = 2\n\nUSE_PRE_COMPUTED_MFCC = True\n\n# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEVICE = xm.xla_device() # TPU\ntorch.set_default_tensor_type('torch.FloatTensor')\n\nTEST_AUDIO_ROOT = Path(\"..\/input\/rfcx-species-audio-detection\/test\")\n\nTEST_MFCC_ROOT = \"..\/input\/kkiller-rfcx-test-mfcc-1-0400\/test_mfcc_d10_s10_sr32000\"","9a48bc77":"class MelSpecComputer:\n    def __init__(self, sr, n_mels, fmin, fmax):\n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax\n\n    def __call__(self, y):\n\n        melspec = lb.feature.melspectrogram(\n            y, sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax,\n        )\n\n        melspec = lb.power_to_db(melspec).astype(np.float32)\n        return melspec","4c28793a":"def mono_to_color(X, eps=1e-6, mean=None, std=None):\n    X = np.stack([X, X, X], axis=-1)\n\n    # Standardize\n    mean = mean or X.mean()\n    std = std or X.std()\n    X = (X - mean) \/ (std + eps)\n\n    # Normalize to [0, 255]\n    _min, _max = X.min(), X.max()\n\n    if (_max - _min) > eps:\n        V = np.clip(X, _min, _max)\n        V = 255 * (V - _min) \/ (_max - _min)\n        V = V.astype(np.uint8)\n    else:\n        V = np.zeros_like(X, dtype=np.uint8)\n\n    return V\n\n\ndef normalize(image, mean=None, std=None):\n    image = image \/ 255.0\n    if mean is not None and std is not None:\n        image = (image - mean) \/ std\n    return np.moveaxis(image, 2, 0).astype(np.float32)\n\n\ndef crop_or_pad(y, length, sr, is_train=True):\n    if len(y) < length:\n        y = np.concatenate([y, np.zeros(length - len(y))])\n    elif len(y) > length:\n        if not is_train:\n            start = 0\n        else:\n            start = np.random.randint(len(y) - length)\n\n        y = y[start:start + length]\n\n    y = y.astype(np.float32, copy=False)\n\n    return y","95518893":"class RFCXDataset(Dataset):\n\n    def __init__(self, data, sr, n_mels=128, fmin=0, fmax=None, num_classes=NUM_CLASSES, duration=DURATION, stride=STRIDE, root=None):\n\n        self.data = data\n        \n        self.sr = sr\n        self.n_mels = n_mels\n        self.fmin = fmin\n        self.fmax = fmax or self.sr\/\/2\n\n\n        self.num_classes = num_classes\n        self.duration = duration\n        self.stride = stride\n        self.audio_length = self.duration*self.sr\n        \n        self.root =  root or TEST_AUDIO_ROOT\n\n        self.mel_spec_computer = MelSpecComputer(sr=self.sr, n_mels=self.n_mels, fmin=self.fmin, fmax=self.fmax)\n        \n        self.res_type = \"kaiser_best\"\n\n\n    def __len__(self):\n        return len(self.data)\n    \n    def load(self, record):\n        y, _ = lb.load(self.root.joinpath(record).with_suffix(\".flac\").as_posix(), sr=self.sr, res_type=self.res_type)\n        return y\n    \n    def load2(self, record):\n        y, orig_sr = sf.read(self.root.joinpath(record).with_suffix(\".flac\").as_posix())\n        y = lb.resample(y, orig_sr=orig_sr, target_sr=self.sr, res_type=self.res_type)\n        return y\n    \n    def read_index(self, idx):\n        d = self.data.iloc[idx]\n        record = d[\"recording_id\"]\n        \n        y = self.load2(record)\n        \n        window = self.duration*self.sr\n        stride = self.stride*self.sr\n            \n        y = np.stack([y[i:i+window] for i in range(0, 60*self.sr+stride-window, stride)])\n\n        return y\n            \n    def process(self, y):\n        melspec = self.mel_spec_computer(y) \n        image = mono_to_color(melspec)\n        image = normalize(image, mean=None, std=None)\n        return image\n\n    def __getitem__(self, idx):\n\n        y = self.read_index(idx)\n        \n        image = np.stack([self.process(_y) for _y in y])\n\n        return image","1597202f":"class SimpleRFCXDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        \n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        record_id_path = Path(row.mfcc_root).joinpath(row.recording_id).with_suffix(\".npy\")\n        image = np.load(record_id_path)\n        return image\n    \n    def __len__(self):\n        return len(self.data)","547a3ef8":"%%time\n\ndata = pd.DataFrame({\n    \"recording_id\": [path.stem for path in Path(TEST_AUDIO_ROOT).glob(\"*.flac\")],\n})\ndata[\"mfcc_root\"] = TEST_MFCC_ROOT\nprint(data.shape)\ndata.head()","fe84fc96":"TEST_MFCC_ROOTs = [\n    \"..\/input\/kkiller-rfcx-test-mfcc-0000-0400\/test_mfcc_d10_s2_sr32000_0000_0400\",\n    \"..\/input\/kkiller-rfcx-test-mfcc-0400-0800\/test_mfcc_d10_s2_sr32000_0400_0800\",\n    \"..\/input\/kkiller-rfcx-test-mfcc-0800-1200\/test_mfcc_d10_s2_sr32000_0800_1200\",\n    \"..\/input\/kkiller-rfcx-test-mfcc-1200-1600\/test_mfcc_d10_s2_sr32000_1200_1600\",\n    \"..\/input\/kkiller-rfcx-test-mfcc-1600-2000\/test_mfcc_d10_s2_sr32000_1600_2000\",\n]","4f0df316":"mfccs = []\nfor mfcc_root in TEST_MFCC_ROOTs:\n    mfccs += [(mfcc.stem, mfcc.parent.as_posix()) for mfcc in Path(mfcc_root).glob(\"*.npy\")]\nmfccs = pd.DataFrame(mfccs, columns = [\"recording_id\", 'mfcc_root'])\n\ndata = data[[\"recording_id\"]].merge(mfccs, on=\"recording_id\")\nprint(data.shape)\ndata.head()","ed3721f8":"ds = RFCXDataset(data=data, sr=SR)","5c4032ab":"%%time\n\nx = ds[1]\nprint(x.shape)","78039f04":"test_data = SimpleRFCXDataset(data) if (USE_PRE_COMPUTED_MFCC and TEST_MFCC_ROOT) else RFCXDataset(data=data, sr=SR)\ntest_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS)","2ebb3d8a":"def load_net(checkpoint_path):\n    net = resnest50(pretrained=True)#.to(DEVICE)\n    n_features = net.fc.in_features\n    net.fc = torch.nn.Linear(n_features, NUM_CLASSES)\n    dummy_device = torch.device(\"cpu\")\n    net.load_state_dict(torch.load(checkpoint_path, map_location=dummy_device))\n    net = net.to(DEVICE)\n    net = net.eval()\n    return net","2b251d32":"checkpoint_paths = [\n    \"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50\/rfcx_resnest_50_fold0.pth\",\n    \"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50\/rfcx_resnest_50_fold1.pth\",\n    \"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50\/rfcx_resnest_50_fold2.pth\",\n    \"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50\/rfcx_resnest_50_fold3.pth\",\n    \"..\/input\/kkiller-rfcx-species-detection-public-checkpoints\/rfcx_resnest50\/rfcx_resnest_50_fold4.pth\",\n]\n\nnets = [\n        load_net(checkpoint_path) for checkpoint_path in checkpoint_paths\n]","6c1ccd51":"preds = []\n# net.eval()\nwith torch.no_grad():\n    for xb in  tqdm(test_loader):\n        bsize, nframes = xb.shape[:2]\n        xb = xb.to(DEVICE).view(bsize*nframes, *xb.shape[2:])\n\n        pred = 0.\n        for net in nets:\n            o = net(xb)\n            o = torch.sigmoid(o)\n            o = o.view(bsize, nframes, *o.shape[1:]).max(1).values\n            o = o.detach().cpu().numpy()\n\n            pred += o\n        \n        pred \/= len(nets)\n        \n        preds.append(pred)\npreds = np.vstack(preds)\npreds.shape","0b40217c":"sub = pd.DataFrame(preds, columns=[f\"s{i}\" for i in range(24)])\nsub[\"recording_id\"] = data[\"recording_id\"].values[:len(sub)]\nsub = sub[[\"recording_id\"] + [f\"s{i}\" for i in range(24)]]\nprint(sub.shape)\nsub.head()","0a0b961b":"sub.to_csv(\"submission.csv\", index=False)","6be996de":"# Pre-computed MFCCs\n> In order to make inference crazingly faster (**< 10 mins on GPU**), I've created [this precomputed MFCC  dataset](https:\/\/www.kaggle.com\/kneroma\/kkiller-rfcx-test-mfcc-1-0400). Don't mind using it in your pipelines :) ","f0b5f4ef":"* The inference is based on these [resnest50 weights](https:\/\/www.kaggle.com\/kneroma\/kkiller-rfcx-species-detection-public-checkpoints). Please, don't forget upvoting the dataset to make it more visible for others\n* The inference pipeline is optimized as much as I can in order to reduce execution time\n* I'm using the pytorch native multi-worker data loading framework\n* You can try increasing the **DURATION** hyperparam, but it can lead to higher execution time\n* Reducing the **STRIDE** hyperparam may give better result, but  it can lead to higher execution time","cb55d96d":"* The training is based on *intelligent* crops of **10 s**, with just a training set less than **500 Mo** !!! \n* I used a stratified KFold, with n_splits = 5\n* I found the learning rate scheduler very important\n* Higher learning rates seem to give me better results\n* For the cross validation metric, I was sticked on F1 score which seems to be more robust than the comps' metric","50a66b4d":"# Configs","122714b4":"<h2><font color=\"blue\">If you find this work useful, please don't forget upvoting :)<\/font><\/h2>","da75ff53":"Hi Kagglers. Days back at the beginning of this comp', I've released a [dummy  kernel](https:\/\/www.kaggle.com\/kneroma\/inference-resnest-rfcx-audio-detection) that performs poorly because of the simple training pipeline and the poor data cropping strategy. I've updated a lot of things and I'm publishing a novel version which is more robust and has a clever training pipeline.\n\nThe goal of this work is to show that we can get a decent score with just a single **ResneSt50** architecture with no TTA of any fancy inference augmentations. As a great fan of the **open-source** philosophy, I will be releasing as much as I can, including my datasets, weights and tips. Unfortunately, my training is currently very dirty and will be hard to release. But, I will be hopefully cleaning and releasing it soon or later.","6b46899d":"# Training","61326a69":"# Data","11a867f4":"# Inference","90fb670a":"# Inference"}}