{"cell_type":{"49ac1db9":"code","093149b4":"code","48a464de":"code","7a247e56":"code","b351ed16":"code","4cc17d1e":"code","54179bd3":"code","f1527c35":"code","3b2d5388":"code","286dfc36":"code","9913f83d":"code","c81d7a81":"code","89f135be":"code","7d9559b3":"code","89d87429":"code","c6f465ee":"code","abcc7773":"code","e724c227":"code","6368b781":"code","24586718":"code","3cfb9e24":"code","6af289de":"code","c54566b6":"code","ecb382b2":"code","4d49979a":"code","7591f2b0":"code","3edaac69":"code","85218bf2":"code","7438bcca":"code","60174163":"code","d0dab7f3":"code","937b6a15":"code","f8d98c3e":"code","93f61a94":"code","b0e7601c":"code","e3e2b43d":"code","5a03e6db":"code","4ea10f93":"code","e374eb36":"code","70a6dbdf":"code","1dc0ecd5":"code","6e1b0d3d":"code","90f9d0f0":"code","c74abfd3":"code","ae1859b2":"code","4051a5bd":"code","5e8f97c4":"code","8a7d3139":"code","b468839e":"code","252c72be":"code","5f1977c0":"code","83c71f3b":"code","7777508e":"code","1cb43a6a":"code","6c40e2a0":"code","fd20c4ea":"code","12415309":"code","9204d2cd":"code","72848b1a":"code","3cc3e907":"code","ac024aa3":"code","c54f0309":"code","62f987f9":"code","82803b05":"code","8c561f49":"code","f2d1d6ef":"code","f8769b58":"code","e6f939c0":"code","ba2361f7":"code","94b3ea61":"code","33686c75":"code","bf3043f2":"code","7bdd41f8":"code","e8b331f4":"code","bf7fc199":"code","2862ccb4":"code","2b084872":"code","0dc5e768":"code","7275cc18":"code","e7a2c697":"code","f4a80d90":"code","9b7967ef":"code","2d772fda":"code","44bd9731":"code","b68b771a":"code","f2cca370":"code","9012be66":"code","a73947cc":"code","698b0078":"code","c19d434b":"code","6c89a8a9":"code","0eed3897":"code","90bb3a93":"code","fa8d5965":"code","a523cfb2":"code","3a9bcb5f":"code","a3fdc642":"code","96054e9c":"code","b23761e7":"code","5f69b1c3":"code","3f74db07":"code","646c23ea":"code","36c56eaa":"code","ef0444ad":"code","a636054f":"code","466e9d33":"code","d42e7d94":"code","801e05ba":"code","60ebe8bc":"code","938666f4":"code","b65c2637":"code","f9e64a48":"code","862e41f1":"code","d79c1922":"code","2b2db1cf":"code","0044971a":"code","897290a2":"code","9de9fe8e":"code","ac831e23":"code","30f14e37":"code","e61467af":"code","c68de6a5":"code","a349f240":"code","266b73a2":"code","6af251af":"code","cc54fb46":"code","88d4483a":"code","2d2475bf":"code","987f022c":"code","5a3763b4":"code","97f56ea2":"code","23dd7077":"code","6ac45c77":"code","452592cb":"code","ec94a2bf":"code","7bb113dc":"code","6a5905c8":"code","e5c88f38":"code","e1b6d0c9":"code","a4c16a80":"code","5d7c2f3f":"code","43d71497":"code","aa9a94f1":"code","c7cd5764":"code","1388ddd1":"code","9b0fbc31":"code","f3fcab41":"code","6edc2648":"code","cc4cba04":"code","010f73ff":"markdown","926263f9":"markdown","148bfedc":"markdown","b28ddaa3":"markdown","0801311b":"markdown","ce1fed95":"markdown","a7f32cfc":"markdown","aea00100":"markdown","d53d1a12":"markdown","26d1532b":"markdown","75b5a9f1":"markdown","cd9c2e95":"markdown","46ac10fd":"markdown","8f411f3e":"markdown","ec6f8ebe":"markdown","e51f34c5":"markdown","ac5355f9":"markdown","4d184526":"markdown","e306adb6":"markdown","5dc54e76":"markdown","624d9917":"markdown","1e233bc6":"markdown","6e289661":"markdown","df01dc47":"markdown","1df83c5c":"markdown","6a532fad":"markdown","a08cca40":"markdown","4b1e6408":"markdown","4e7d25e5":"markdown","f9243a70":"markdown","575b2cda":"markdown","c2c2f5a1":"markdown","dfe9e69d":"markdown","e7c415e0":"markdown","ac86a7b5":"markdown","945ac216":"markdown","1138bbd7":"markdown","2950badc":"markdown","19bb9448":"markdown","b693aa96":"markdown","884eb86f":"markdown","42e33167":"markdown","8f4fe295":"markdown","7bdf4d93":"markdown","a95c23f3":"markdown","5ca36d7e":"markdown","b32fe9f0":"markdown","958a4104":"markdown","fdda0fc3":"markdown","cde97a98":"markdown","4d3c370f":"markdown","6f43f084":"markdown","676bd286":"markdown","6926496c":"markdown","2ac3562e":"markdown","b97981cd":"markdown","ec0214eb":"markdown","4fc109a0":"markdown","8c029fbb":"markdown","99090913":"markdown","6805a604":"markdown","ed1c7606":"markdown","34b71994":"markdown","e066b098":"markdown","af8b109a":"markdown","5c487239":"markdown","4c78c313":"markdown"},"source":{"49ac1db9":"! pip install googletrans","093149b4":"! pip install strsim","48a464de":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom googletrans import Translator\nimport re\nfrom tqdm import tqdm_notebook\nimport gc\nfrom itertools import product","7a247e56":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    for f in float_cols:\n        df.loc[:,f] = pd.to_numeric(df[f], downcast='float')\n    \n    for i in int_cols:\n        df.loc[:,i] = pd.to_numeric(df[i], downcast='integer')\n    \n    return df","b351ed16":"train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsubmission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","4cc17d1e":"train.head()","54179bd3":"test.head()","f1527c35":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ncounts = train['shop_id'].value_counts(normalize=True).sort_values(ascending=False)\nsns.barplot(x = counts.index, y=counts, order=counts.index)\nplt.title(\"Number of transactions by shop ID (normalized)\")\n\nplt.subplot2grid((3,3), (1,0))\nsns.distplot(train.item_id)\nplt.title(\"Item ID histogram\")\n\nplt.subplot2grid((3,3), (1,1))\nsns.distplot(train.item_price)\nplt.title(\"Item price histogram\")\n\nplt.subplot2grid((3,3), (1,2))\nsns.distplot(train.item_cnt_day)\nplt.title(\"Item count day histogram\")\n\nplt.subplot2grid((3,3), (2,0), colspan=3)\ncounts = train['date_block_num'].value_counts(normalize=True).sort_values(ascending=False)\nsns.barplot(x=counts.index, y=counts, order=counts.index)\nplt.title(\"Number of transactions per date block num\");","3b2d5388":"train['item_id'].value_counts(ascending=False)[:5]","286dfc36":"items.loc[items['item_id']==20949]","9913f83d":"translator = Translator()\ntranslator.translate(items.loc[items['item_id']==20949].item_name.values[0]).text","c81d7a81":"test[test.item_id==20949].head()","89f135be":"train.item_cnt_day.sort_values(ascending=False)[:10]","7d9559b3":"train[train.item_cnt_day>2000]","89d87429":"items[items.item_id==11373]","c6f465ee":"translator.translate(items[items.item_id==11373].item_name.values[0]).text","abcc7773":"train[(train.item_id==11373)&(train.item_cnt_day<2000)]['item_cnt_day'].median()","e724c227":"train = train[train.item_cnt_day < 2000]","6368b781":"train[train.duplicated(subset=['date', 'shop_id', 'item_id'], keep=False)]","24586718":"train[train.duplicated(keep=False)]","3cfb9e24":"print(train.shape)\ntrain = train[~train.duplicated()]\nprint(train.shape)","6af289de":"train.item_price.sort_values(ascending=False)[:10]","c54566b6":"train[train.item_price>300000]","ecb382b2":"items[items.item_id==6066]","4d49979a":"translator.translate(items[items.item_id==6066].item_name.values[0]).text","7591f2b0":"train[train.item_id==6066]","3edaac69":"print(train.shape)\ntrain = train[train.item_price<300000]\nprint(train.shape)","85218bf2":"train[train.item_price <= 0]","7438bcca":"train[train.item_id==2973].head()","60174163":"median_price_item_2973 = train[(train.item_id==2973)&(train.date_block_num==4)&(train.shop_id==32)&(train.item_price>0)]['item_price'].median()\ntrain.loc[484683,'item_price'] = median_price_item_2973","d0dab7f3":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ncounts = train['shop_id'].value_counts(normalize=True).sort_values(ascending=False)\nsns.barplot(x = counts.index, y=counts, order=counts.index)\nplt.title(\"Number of transactions by shop ID (normalized)\")\n\nplt.subplot2grid((3,3), (1,0))\nsns.distplot(train.item_id)\nplt.title(\"Item ID histogram\")\n\nplt.subplot2grid((3,3), (1,1))\nsns.distplot(train.item_price)\nplt.title(\"Item price histogram\")\n\nplt.subplot2grid((3,3), (1,2))\nsns.distplot(train.item_cnt_day)\nplt.title(\"Item count day histogram\")\n\nplt.subplot2grid((3,3), (2,0), colspan=3)\ncounts = train['date_block_num'].value_counts(normalize=True).sort_values(ascending=False)\nsns.barplot(x=counts.index, y=counts, order=counts.index)\nplt.title(\"Number of transactions per date block num\");","937b6a15":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ncounts = test['shop_id'].value_counts(normalize=True).sort_values(ascending=False)\nsns.barplot(x = counts.index, y=counts, order=counts.index)\nplt.title(\"Number of transactions by shop ID (normalized)\")\n\nplt.subplot2grid((3,3), (1,0))\nsns.distplot(test.item_id)\nplt.title(\"Item ID histogram\");","f8d98c3e":"print(\"Number of item in test set: {}\".format(len(test.item_id.unique())))\nitem_not_in_train_set = test[~test.item_id.isin(train.item_id.unique())].item_id.sort_values().unique()\nprint(\"Number of item in test set with no transaction in train: {}\".format(len(item_not_in_train_set)))","93f61a94":"item_not_in_train_set[:10]","b0e7601c":"items[items.item_id.isin(range(198, 210))]","e3e2b43d":"items.loc[204,'item_name']","5a03e6db":"del counts\ndel item_not_in_train_set\ngc.collect()","4ea10f93":"shops_not_in_train = test[~test.shop_id.isin(train.shop_id.unique())].shop_id.unique()\nprint(\"Number of shops in test with no transaction in train: {}\".format(len(shops_not_in_train)))","e374eb36":"shops.shop_name[:5]","70a6dbdf":"shop_splitter = re.compile(r'(\\w+)\\s(.*)')\nshop_names = shops.shop_name.apply(lambda x: shop_splitter.search(x).groups())\nshop_names_df = pd.DataFrame(shop_names.values.tolist(), columns=['city', 'extracted_name'])\nshop_names_df.head()","1dc0ecd5":"shops = pd.concat([shops, shop_names_df], axis=1)\nshops.head()","6e1b0d3d":"shops.loc[:,'is_city'] = shops.city.apply(lambda x :0 if x in ['\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f', '\u043c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439'] else 1)","90f9d0f0":"shops.shop_name.unique()","c74abfd3":"def shop_sub_type(x):\n    if x[0] == 0:\n        return 'non_city'\n    else:\n        if '\u0422\u0426' in x[1]:\n            return '\u0422\u0426'\n        elif '\u0422\u0420\u0426' in x[1]:\n            return '\u0422\u0420\u0426'\n        elif '\u0422\u0420\u041a' in x[1]:\n            return '\u0422\u0420\u041a'\n        elif '\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435' in x[1]:\n            return '\u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435'\n        else:\n            return 'other'","ae1859b2":"shops.loc[:,'shop_sub_type'] = shops[['is_city', 'extracted_name']].apply(shop_sub_type, axis=1)\nshops.head()","4051a5bd":"shops[shops.shop_name.duplicated(keep=False)]","5e8f97c4":"del shop_names_df, shops_not_in_train\ngc.collect()","8a7d3139":"from similarity.normalized_levenshtein import NormalizedLevenshtein","b468839e":"unique_shop_id = shops.shop_id.unique()\nsimilarity_grid = np.zeros(shape=(len(unique_shop_id), len(unique_shop_id)))","252c72be":"norm_lev = NormalizedLevenshtein()\n\nfor i in unique_shop_id:\n    for j in unique_shop_id:\n        distance = norm_lev.similarity(shops[shops.shop_id==i].shop_name.values[0], shops[shops.shop_id==j].shop_name.values[0])\n        similarity_grid[i,j] = distance","5f1977c0":"fig, ax = plt.subplots(figsize=(10,8))\nmask = similarity_grid < 0.6\nsns.heatmap(similarity_grid, ax=ax, mask=mask, cmap = sns.color_palette('Blues'))\nax.set_facecolor(\"grey\")","83c71f3b":"indices = zip(*np.triu_indices_from(similarity_grid))","7777508e":"similar_stores = []\n\nfor c in indices:\n    i, j = c[0], c[1]\n    if i != j and similarity_grid[i,j]>0.6:\n        similar_stores.append([i,j, similarity_grid[i,j]])\nsimilar_stores = pd.DataFrame(similar_stores, columns=['i','j','similarity'])\nsimilar_stores.sort_values(by='similarity',ascending=False, inplace=True)\nsimilar_stores","1cb43a6a":"shops[shops.shop_id.isin([10,11])].shop_name","6c40e2a0":"train.loc[train.shop_id==10, 'shop_id'] = 11\ntest.loc[test.shop_id==10, 'shop_id'] = 11","fd20c4ea":"shops[shops.shop_id.isin([23,24])].shop_name","12415309":"shops[shops.shop_id.isin([30,31])].shop_name","9204d2cd":"shops[shops.shop_id.isin([0,57])].shop_name","72848b1a":"train.loc[train.shop_id==57, 'shop_id'] = 0\ntest.loc[test.shop_id==57, 'shop_id'] = 0","3cc3e907":"shops[shops.shop_id.isin([1,58])].shop_name","ac024aa3":"train.loc[train.shop_id==58, 'shop_id'] = 1\ntest.loc[test.shop_id==58, 'shop_id'] = 1","c54f0309":"shops[shops.shop_id.isin([39,40])].shop_name","62f987f9":"shops[shops.shop_id.isin([38,54])].shop_name","82803b05":"del similar_stores, similarity_grid\ngc.collect()","8c561f49":"categories.item_category_name.head()","f2d1d6ef":"split_names = categories.item_category_name.apply(lambda x: [x.strip() for x in x.split(' - ')])","f8769b58":"new_categories = np.chararray((len(categories), 2), itemsize=33, unicode=True)\nnew_categories[:] = 'None'\n\n# Add categories with a for loop\nfor i, c_list in enumerate(split_names):\n    for j, c_value in enumerate(c_list):\n        new_categories[i,j] = c_value","e6f939c0":"new_categories_df = pd.DataFrame(new_categories, columns=['category', 'sub_category'])\ncategories = categories.join(new_categories_df)\n\n# If sub_category is None replace it with category\ncategories.loc[:,'sub_category'] = categories[['category', 'sub_category']].apply(lambda x: x[0] if x[1]=='None' else x[1], axis=1)\n\ncategories.head()","ba2361f7":"del split_names, new_categories, new_categories_df\ngc.collect()","94b3ea61":"items[items.item_name.str.contains('FIFA 13')]","33686c75":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity","bf3043f2":"# Please notice I had to change the default token_pattern to also tokenize 1-character word\n# (e.g. Far Cry 3 and Far Cry 2)\nvectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w*\\\\b')\nvectorized_names = vectorizer.fit_transform(items.item_name.values)","7bdd41f8":"# Calculate cosine similarity grid\ncosine_similarity_grid = cosine_similarity(vectorized_names)","e8b331f4":"# Let's print out the most similar names (excluding same names)\nindices = zip(*np.triu_indices_from(cosine_similarity_grid))\nsimilar_items = []\n\nfor c in tqdm_notebook(indices):\n    i, j = c[0], c[1]\n    if i != j and cosine_similarity_grid[i,j]>0.9:\n        similar_items.append([i,j, cosine_similarity_grid[i,j]])\nsimilar_items = pd.DataFrame(similar_items, columns=['i','j','similarity'])\nsimilar_items.sort_values(by='similarity',ascending=False, inplace=True)","bf7fc199":"similar_items[similar_items.similarity==1].shape","2862ccb4":"similar_items[similar_items.similarity==1].tail()","2b084872":"items[items.item_id.isin([8642, 8643, 8632, 8633])]","0dc5e768":"items[items.item_id.isin([9048, 9049, 18126, 18127])]","7275cc18":"duplicated_items = similar_items[similar_items.similarity==1].copy()\nfor c in duplicated_items.columns:\n    duplicated_items.loc[:,c] = pd.to_numeric(duplicated_items[c], downcast='integer')","e7a2c697":"for _, r in tqdm_notebook(duplicated_items.iterrows()):\n    train.loc[train.item_id==r[1], 'item_id'] = r[0]\n    test.loc[test.item_id==r[1], 'item_id'] = r[0]","f4a80d90":"train[train.item_id==9049]","9b7967ef":"del duplicated_items\ngc.collect()","2d772fda":"similar_items[(similar_items.similarity>0.99)&(similar_items.similarity<1)]","44bd9731":"items[items.item_id.isin([4199, 4200, 10479, 10480, 14431, 14432])]","b68b771a":"# Take a sample\nsample_idx = np.random.choice(np.arange(22170), size=250, replace=False)\nsample = cosine_similarity_grid[sample_idx].copy()","f2cca370":"from sklearn.manifold import TSNE\nsim_embed = TSNE().fit_transform(sample)\nx, y = zip(*sim_embed)\nplt.scatter(x,y);","9012be66":"del sim_embed, sample\ngc.collect()","a73947cc":"from torch import topk\nimport torch","698b0078":"cosine_similarity_grid_torch = torch.from_numpy(cosine_similarity_grid)","c19d434b":"topk_values, topk_indices = topk(cosine_similarity_grid_torch, 4)\ntopk_values, topk_indices = topk_values.numpy(), topk_indices.numpy()","6c89a8a9":"def add_index(i, n, topk_values, topk_indices, threshold=0.6):\n    \n    val, ind = topk_values[i], topk_indices[i]\n    \n    if val[n] > threshold:\n        return ind[n]\n    else:\n        return i","0eed3897":"# Create similar 1 column\nitems.loc[:,'similar_1'] = items.item_id.apply(add_index, n=1, topk_values=topk_values, topk_indices=topk_indices)","90bb3a93":"def add_index_mul(i, n, topk_values, topk_indices, threshold=0.6):\n    \n    item_id, similar_prev = i[0], i[1]\n    \n    val, ind = topk_values[item_id], topk_indices[item_id]\n    \n    if val[n] > threshold:\n        return ind[n]\n    else:\n        return similar_prev","fa8d5965":"# Create similar 2 columns\nitems.loc[:,'similar_2'] = items[['item_id', 'similar_1']].apply(add_index_mul, n=2, topk_values=topk_values, topk_indices=topk_indices, axis=1)\n\n# Create similar 3 columns\nitems.loc[:,'similar_3'] = items[['item_id', 'similar_2']].apply(add_index_mul, n=3, topk_values=topk_values, topk_indices=topk_indices, axis=1)","a523cfb2":"# Let's check out the FIFA example again\nitems[items.item_name.str.contains('FIFA 14')]","3a9bcb5f":"del similar_items, cosine_similarity_grid, topk_values, topk_indices, cosine_similarity_grid_torch\ndel vectorized_names\ngc.collect()","a3fdc642":"items = items.drop('item_name', axis=1)","96054e9c":"def frequency_encode(series):\n    return series.value_counts(normalize=True)","b23761e7":"#Add shop_id and item_id combinationa\ntrain.loc[:,'shop_and_item'] = train.shop_id.astype(str) + '-' + train.item_id.astype(str)\ntrain.head()","5f69b1c3":"from sklearn.preprocessing import LabelEncoder","3f74db07":"# Create all possible shop and item combinations\nshop_and_item = pd.Series(list(product(shops.shop_id.values, items.item_id.values)))\nshop_and_item = pd.DataFrame(shop_and_item.apply(lambda x: str(x[0]) + '-' + str(x[1])), columns=['shop_and_item'])\n\n# Label-encode them\nshop_and_item_encoder = LabelEncoder()\nshop_and_item_encoder.fit(shop_and_item.shop_and_item)\n\n# Transform\nshop_and_item.loc[:,'shop_and_item'] = shop_and_item_encoder.transform(shop_and_item.shop_and_item)\ntrain.loc[:,'shop_and_item'] = shop_and_item_encoder.transform(train.shop_and_item)","646c23ea":"# Create frequency encodings in items dataframe\nitems.loc[:,'item_id_freq_encod'] = items.item_id.map(frequency_encode(train.item_id))","36c56eaa":"# Frequency encode shop_id\nshops.loc[:,'shop_id_freq_encod'] = shops.shop_id.map(frequency_encode(train.shop_id))","ef0444ad":"# Add shops details\ntrain = train.merge(shops[['shop_id', 'city', 'shop_sub_type']], how='left', on=['shop_id'])\n\n# Add category id\ntrain = train.merge(items[['item_id', 'item_category_id']], how='left', on=['item_id'])\n\n# Add category information\ntrain = train.merge(categories[['item_category_id', 'category', 'sub_category']], how='left', on=['item_category_id'])","a636054f":"# Add city freq encoding\nshops.loc[:,'city_freq_encod'] = shops.city.map(frequency_encode(shops.city))\n\n# Add category_id freq encoding\ncategories.loc[:,'item_category_id_freq_encod'] = categories.item_category_id.map(frequency_encode(train.item_category_id))\n\n# Add category freq encoding\ncategories.loc[:,'category_freq_encod'] = categories.category.map(frequency_encode(train.category))\n\n# Add sub_category freq encoding\ncategories.loc[:,'sub_category_freq_encod'] = categories.sub_category.map(frequency_encode(train.sub_category))\n\n# Add shop_item freq encoding\nshop_and_item.loc[:,'shop_and_item_freq_encod'] = shop_and_item.shop_and_item.map(frequency_encode(train.shop_and_item))","466e9d33":"# Fill na\nitems = items.fillna(0)\ncategories = categories.fillna(0)\nshops = shops.fillna(0)\nshop_and_item = shop_and_item.fillna(0)","d42e7d94":"# Add oldest transaction (don't have to fill NA here yet)\nitems.loc[:,'oldest_date_block_num'] = items.item_id.map(train.groupby('item_id')['date_block_num'].min())","801e05ba":"# Dump modified files\nitems.to_hdf('processed_items.hdf5', key='df')\ncategories.to_hdf('processed_categories.hdf5', key='df')\nshops.to_hdf('processed_shops.hdf5', key='df')\nshop_and_item.to_hdf('shop_and_item.hdf5', key='df')\ntrain.to_hdf('processed_train.hdf5', key='df')\ntest.to_hdf('processed_test.hdf5', key='df')","60ebe8bc":"grid = []\nfor block_num in tqdm_notebook(train.date_block_num.unique()):\n    cur_shops = train[train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = train[train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(cur_shops, cur_items, [block_num])), dtype='int32'))","938666f4":"# Create dataframe from grid\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols, dtype=np.int32)\ngrid.sort_values(by=['date_block_num', 'shop_id', 'item_id'], inplace=True)\ngrid.reset_index(inplace=True, drop=True)","b65c2637":"grid.head()","f9e64a48":"# Add item_cnt_month (not target)\nitem_cnt_df = train.groupby(['date_block_num', 'shop_id', 'item_id'])['item_cnt_day'].sum().rename('item_cnt_month').reset_index()\nitem_cnt_df.head()","862e41f1":"# Clip values\nitem_cnt_df.loc[:,'item_cnt_month'] = item_cnt_df.item_cnt_month.clip(0,20)","d79c1922":"# Merge item_cnt_month into grid (NaN values fill them with 0)\ngrid = grid.merge(item_cnt_df, how='left', on=['date_block_num', 'shop_id', 'item_id']).fillna(0)","2b2db1cf":"del item_cnt_df\ngc.collect()","0044971a":"grid.head()","897290a2":"test.loc[:,'date_block_num'] = 34\ntest.loc[:,'item_cnt_month'] = 0","9de9fe8e":"grid = grid.append(test.drop('ID', axis=1))\ngrid.loc[:,'item_cnt_month'] = grid.item_cnt_month.astype(int)","ac831e23":"# Add shop and item\ngrid.loc[:,'shop_and_item'] = grid.shop_id.astype(str) + '-' + grid.item_id.astype(str)\ngrid.loc[:,'shop_and_item'] = shop_and_item_encoder.transform(grid.shop_and_item)","30f14e37":"grid.head()","e61467af":"del train\ndel test\ngc.collect()","c68de6a5":"def generate_lag(grid, months, lag_column):\n    for month in months:\n        # Speed up by grabbing only the useful bits\n        \n        grid_shift = grid[['date_block_num', 'shop_id', 'item_id', lag_column]].copy()\n        grid_shift.columns = ['date_block_num', 'shop_id', 'item_id', lag_column+'_lag_'+ str(month)]\n        grid_shift['date_block_num'] += month\n        grid = pd.merge(grid, grid_shift, on=['date_block_num', 'shop_id', 'item_id'], how='left')\n    return grid","a349f240":"grid = downcast_dtypes(grid)","266b73a2":"# Lag item counts\n%time\ngrid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'item_cnt_month')","6af251af":"# Fill na with zero (later remember to select only date_block_num greater or equal to 12)\ngrid = grid.fillna(0)","cc54fb46":"for c in grid.columns[5:]:\n    grid.loc[:,c] = pd.to_numeric(grid[c], downcast='integer')","88d4483a":"grid = downcast_dtypes(grid)","2d2475bf":"from sklearn.preprocessing import LabelEncoder","987f022c":"# Fix shops\nencoder = LabelEncoder()\nshops.loc[:,'city'] = encoder.fit_transform(shops.city)\nshops.loc[:,'shop_sub_type'] = encoder.fit_transform(shops.shop_sub_type)","5a3763b4":"# Fix categories\nencoder = LabelEncoder()\ncategories.loc[:,'category'] = encoder.fit_transform(categories.category)\ncategories.loc[:,'sub_category'] = encoder.fit_transform(categories.sub_category)","97f56ea2":"# Drop some columns\ncategories.drop(columns=['item_category_name'], inplace=True)\nshops.drop(columns=['shop_name', 'extracted_name'], inplace=True)","23dd7077":"# Add all to grid\ngrid = grid.merge(items, how='left', on=['item_id'])\ndel items\ngc.collect()\ngrid = downcast_dtypes(grid)","6ac45c77":"grid = grid.merge(categories, how='left', on=['item_category_id'])\ndel categories\ngc.collect()\ngrid = downcast_dtypes(grid)","452592cb":"grid = grid.merge(shops, how='left', on=['shop_id'])\ndel shops\ngc.collect()\ngrid = downcast_dtypes(grid)","ec94a2bf":"grid = grid.merge(shop_and_item, how='left', on=['shop_and_item'])\ndel shop_and_item\ngc.collect()\ngrid.drop(columns=['shop_and_item'])\ngrid = downcast_dtypes(grid)","7bb113dc":"grid = downcast_dtypes(grid)","6a5905c8":"del shop_names, shop_splitter, x, y\ngc.collect()","e5c88f38":"grid.to_hdf('grid.hdf5', key='df')","e1b6d0c9":"# # Mean item_id\n# mean_id = grid.groupby(['date_block_num', 'item_id'])['item_cnt_month'].mean().rename('item_month_mean').reset_index()\n# grid = grid.merge(mean_id, how='left', on=['date_block_num', 'item_id'])\n\n# # Delete mean_id\n# del mean_id\n# gc.collect()\n\n# # Create lags\n# grid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'item_month_mean')\n\n# # We need to drop item_month_mean otherwise that would be a massive leakage for our model\n# # Item month mean is basically the average target value for the product.\n# grid.drop(columns=['item_month_mean'], inplace=True)","a4c16a80":"# grid = downcast_dtypes(grid)","5d7c2f3f":"# # Mean shop_id (should capture the activity of a shop)\n# mean_shop_id = grid.groupby(['date_block_num', 'shop_id'])['item_cnt_month'].mean().rename('shop_month_mean').reset_index()\n# grid = grid.merge(mean_shop_id, how='left', on=['date_block_num', 'shop_id'])\n\n# # Delete mean_id\n# del mean_shop_id\n# gc.collect()\n\n# # Create lags\n# grid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'shop_month_mean')\n\n# # We need to drop shop_month_mean otherwise that would be a massive leakage for our model\n# grid.drop(columns=['shop_month_mean'], inplace=True)","43d71497":"# grid = downcast_dtypes(grid)","aa9a94f1":"# # Mean city\n# mean_city_id = grid.groupby(['date_block_num', 'city'])['item_cnt_month'].mean().rename('city_month_mean').reset_index()\n# grid = grid.merge(mean_city_id, how='left', on=['date_block_num', 'city'])\n\n# # Delete mean_id\n# del mean_city_id\n# gc.collect()\n\n# # Create lags\n# grid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'city_month_mean')\n\n# # We need to drop city_month_mean otherwise that would be a massive leakage for our model\n# grid.drop(columns=['city_month_mean'], inplace=True)","c7cd5764":"# grid = downcast_dtypes(grid)","1388ddd1":"# # Mean category_id\n# mean_category_id = grid.groupby(['date_block_num', 'item_category_id'])['item_cnt_month'].mean().rename('category_id_month_mean').reset_index()\n# grid = grid.merge(mean_category_id, how='left', on=['date_block_num', 'item_category_id'])\n\n# # Delete mean_id\n# del mean_category_id\n# gc.collect()\n\n# # Create lags\n# grid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'category_id_month_mean')\n\n# # We need to drop category_id_month_mean otherwise that would be a massive leakage for our model\n# grid.drop(columns=['category_id_month_mean'], inplace=True)","9b0fbc31":"# grid = downcast_dtypes(grid)","f3fcab41":"# # Mean category\n# # Mean category_id\n# mean_category = grid.groupby(['date_block_num', 'category'])['item_cnt_month'].mean().rename('category_month_mean').reset_index()\n# grid = grid.merge(mean_category, how='left', on=['date_block_num', 'category'])\n\n# # Delete mean_id\n# del mean_category\n# gc.collect()\n\n# # Create lags\n# grid = generate_lag(grid, [1,2,3,4,5,6,11,12], 'category_month_mean')\n\n# # We need to drop category_id_month_mean otherwise that would be a massive leakage for our model\n# grid.drop(columns=['category_month_mean'], inplace=True)","6edc2648":"# grid = downcast_dtypes(grid)","cc4cba04":"# Mean sub-category","010f73ff":"## Read data","926263f9":"These are very similar stores by name. Let's check the names.","148bfedc":"Very interesting! There seems to be a hidden categorisation of items according to their names. I have tried a couple of clustering methods (namely DBSCAN and Affinity Propagation), but it took a long time and memory to train them. I will revert back to them in case I am not satisfied with my solution.  \nAnother way to categorise each product is to check for the set with the closest similarity levels. Let me find out a smart way to do this.","b28ddaa3":"## Shops\nFirst of all, let me see if we are requested to predict sales for shops in the test set which have never recorded a transaction in the train set.","0801311b":"Interesting. There are 363 products in the test which have not any transaction associated to them. Let's check them out. In particular, in a previous notebook I have worked on, I found out that item_id seem to have been assigned at a later stage when creating the dataset for the competition.  \nI take an example, FIFA video games. Consecutive editions of FIFA have growing number of item_id (e.g. FIFA 2013 may be item_id 10000 and FIFA 2014 may be item_id 10005). Let's check neighbouring item_id for the test set products with no transaction in the train set.","ce1fed95":"# Percentage change and positive item count for similar products (across various months)","a7f32cfc":"Let's now see if we have shops with the same shop_name but different shop_id.","aea00100":"This is weird! Not only we have duplicated transactions, but you can see that price also changes. It also seems that this issue affect a relatively small set of items. Probably I should look at duplicates by looking at all possible values in columns (i.e. two transactions for the same product, shop and date, but different price - for whateer reason it is - should not be considered duplicated).","d53d1a12":"Some Google search shows this shops are indeed different. Let's check the last one.","26d1532b":"This is very intersting indeed! Product with item_id 204 is a Cleopatra audio-book. As you can see items around it - in terms of item_id - are also audiobooks (for which we have recorded transactions). For an unseen product, we may let our behave in two ways:\n\n* predict something close to zero, i.e. the product has never sold anything and that is what we should assume;\n* assuming that the product is new, we can predict the average number of items sold by products of the same type during their first launch.  \n  \nThe second method is quite tricky. Either we create a more complicated validation set where we remove some products for which we have data, but we assume they are new (although that could be biased) or, for products which are new, we actually create some sort of \"manual predictions\" (like the one explained above). This could be an interesting strategy indeed.","75b5a9f1":"The first thing we can see is that we have a much more uniform distribution of shops for which we are reuqested to come up with predictions. We already know that some of the shops have a much lower number of transactions (and thus, possibly, item counts). Encoding shops using frequency encoding (or mean encoding) should help us signal these differences.  \n  \nSimilarly, while we know that items with item_id around 5000 are the ones transacted the most (in the train set), here we have a relatively more uniform representation of all possible item ids. Again, by encoding item id using frequency or mean encoding we should be able to signal this.  \n  \nLet me check if I have some items and item-shop_id combination in the test set which do not have any transaction in the train set.","cd9c2e95":"Let's now look at the shop names.","46ac10fd":"Ok, this one is a different city. Let's stop here.","8f411f3e":"## Item_id analysis\nLet's now analyse item_ids. Let me take an example I have analysed in a another notebook, i.e. the FIFA 13 videogame.","ec6f8ebe":"### Duplicated\nBecause the field is called `item_cnt_day`, I expect to be no duplicated entries for date, shop_id and item_id combination. In other words, for each product, shop and day we should have no more than one transaction per day. Let's check this out.","e51f34c5":"Here I want to find out the top 3 most similar product (by product name) to a given item. I want to keep a cosine similarity threshold (0.65) to only retain similar names if their cosine similarity with the original name is higher (or equal to the threshold). If the closest items have cosine similarity lower than 0.6, then I will just return NaN (and fill it with min, max and mean of the original product).\nTo do this, I will leverage pytorch top k function.","ac5355f9":"# Product age","4d184526":"It seems that this item has actually a normal price. Let's replace the transaction with negative price with the median price (same product, same shop, same date block num).","e306adb6":"There are 64 pairs for which cosine similarity is 1. This is quite weird. Let me expect a couple of them.","5dc54e76":"Again, very similar. Let's replace them.","624d9917":"Ok, this is a sensibly lower number. I would only keep one of the duplicated transactions then.","1e233bc6":"As we can see, we have to predict the `date_block_num` 34 (i.e. November 2015) volume of items sold for each item_id and shop_id combination presented in the test set.","6e289661":"Let's examine the city more in details. Some values are indeed city, but three city names are actually other categories. In particular, '\u0412\u044b\u0435\u0437\u0434\u043d\u0430\u044f', '\u043c\u0430\u0433\u0430\u0437\u0438\u043d', '\u0426\u0438\u0444\u0440\u043e\u0432\u043e\u0439'. It would then be good to signal this with a boolean variable.","df01dc47":"# Target and lagged (count and mean)\n## Create Grid","1df83c5c":"Wow! It seems that actually some product are considered differently only because lower\/upper case differences. These items should definitely be categorised as the same. Let's check some others.","6a532fad":"## Outliers\n### Item ID","a08cca40":"It seems that product 20949 has been transacated 31,340 times in the entire dataset. This seems a bit weird. Let's find out.","4b1e6408":"Bingo, these two shops are indeed the same - there is just one character different. It would then make sense to categorise any transaction from shop number 10 as it was registered in shop number 11 (similaryly in the test set).","4e7d25e5":"Although I do not speak Russian, you can see that there are some words that repeat mostly all the times, in particular:\n\n* \u0422\u0426;\n* \u0422\u0420\u0426;\n* \u0422\u0420\u041a;\n* \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435;\n* other.  \n  \nLet's add this information. When we will run EDA we may find this is not useful, but let's add it for now.","f9243a70":"These are not really similar shops (probably the name of the area is close).","575b2cda":"I now have to add missing information to the train set, including city, shop_sub_type, category_id, category, sub_category. This will require some ad-hoc merging.","c2c2f5a1":"### Mean encodings\nHere I will create mean encodings by:\n* item_id\n* shop id\n* city,\n* category_id,\n* category,\n* sub category.","dfe9e69d":"Ok, not a problem. Let's focus on the shop names now.","e7c415e0":"Let's now check if we have some transactions where the price was negative (in theory we should assume only positive prices).","ac86a7b5":"Not really. It then makes sense to remove this row.","945ac216":"A-ha! It seems this product has been sold to 522 people, and possible the price is not the price of the single product, but the value of the entire transaction. Let met check if there are other transactions.","1138bbd7":"Let's now check items with a very high similarity (e.g. > 0.99), but lower than 1.","2950badc":"Only one transaction where the item was negative. Let's check if we have other transactions for the same product.","19bb9448":"As you can see, median day count for this item is 4. We could then remove this entry.","b693aa96":"## Frequency encoding\nFrequency encoding of:\n* item_id,\n* shop_id,\n* city,\n* category_id,\n* category,\n* sub category and\n* shop-item combinations.","884eb86f":"## Categories analysis\nI have noticed that categories include some hierarchical information (separated by a hyphen).","42e33167":"Let's take a sample of products in the similarity grid, and calculate t-SNE values. Let's plot this into a scatter plot.","8f4fe295":"## Lag sales\nHere I am using the same formula as in https:\/\/www.kaggle.com\/sarthakbatra\/predicting-sales-tutorial","7bdf4d93":"### Append test set","a95c23f3":"Translation is wrong here, but this items turns out to be a plastic bag... (only one item in that category). In addition, this tricy item is indeed present in the test set.","5ca36d7e":"The translation does not help here, but let's check out the median number of items sold for the same product in the dataframe (excluding this anomalous transaction).","b32fe9f0":"Here, we also have a few transactions with a very high number of items sold. As you can see above, for example, there was one transaction in which more than 2,000 items were sold. Let's check this out.","958a4104":"### Item cnt day","fdda0fc3":"Let me highlight the key points here:\n\n1. Clearly a few shops are responsible for a lot of transactions (namely shop 31, 25, 54 and 28);\n2. Some item IDs seem to have a higher than average number of transactions. In addition, the histogram is quite smooth, possibly suggesting that close item_ids are indeed related to similar products.\n3. Item price and item count clear show some outlier values (the x-axis of the respective histograms is very wide, while most of the values are concentrated on a tiny interval).\n4. A bigger number transactions were recorded in date block number 11 and 23, i.e. December 2013 and December 2014. This may suggest some sort of seasonality effect in the data.","cde97a98":"## Mean encodings\n### Label Encode categorical features","4d3c370f":"### Prices","6f43f084":"## Test set\nLet's now have a look at the test set.","676bd286":"Same stuff! I would use a brute-force approach here and replace each duplicated item with the first id appearing in the dataframe above.","6926496c":"As you can see, many of these items are very very similar. It may make sense to start thinking about clustering my item_ids according to their name similarity. To do that, I will use a an algorithm that does not require to fix the number of clusters.","2ac3562e":"Well, this one looks pretty much the same. Let's replace it.","b97981cd":"Let's have a look at some of them.","ec0214eb":"## Value counts\nLet's simply print how many entries (in the train dataset) we have by some relevant categories. In particular, I will plot the value counts shop_id, category_id and date block num. I will also plot the histograms for item_id, price and item_count.","4fc109a0":"## Features to add\n- use frequency encoding (item_id, shop_id, item_category_id, ...). This should be useful to identify outliers.","8c029fbb":"Let's now reprint the distribution of the train set.","99090913":"In other words, signalling somehow that this number is usually selling a lot would be a good thing. I will use frequency encoding to capture such patterns.","6805a604":"After some translation, these are two equally-named shops, but positioned in different areas of the same shopping center. I don't really know whether one specialises in some specific products and the other doesn't, but I will keep them as separate. I am not really sure whether the sales of one of the shops can influence the other though.","ed1c7606":"This seems quite interesting. We have a couple of shops with very similar names. Let's check them out.","34b71994":"Let's use a slightly better approach. Let's evaluate the similarity between product names and plot that into a sort of grid plot.","e066b098":"Ok, there seems to be one transaction with an insanely high price. Let's check it out.","af8b109a":"As you can see, this video game was released in various versions and for various gaming platforms (e.g. PC, PS3, Xbox, etc.). What is interesting is that the item_id of these products are consecutive. This is precious information, because we can say that if FIFA 13 is selling well on Xbox, we have no doubt to believe that it will also sell good on other consoles. Mean-encoding may be misleading, because some consoles are more popular than others, but other simple variables (e.g. min item count on other consoles greater than 0, average item count on other consoles greater than 5, etc.) could be a good approach. Please notice that in this case I would use a leave-one-out approach.  \n  \nCalculating word similarity as before would take too much time. I would transform product names using TfIdf vectorisation (on the entire names set). I will then use built-in cosine similarity function in from sklearn to calculate a metric of similarities.","5c487239":"The first thing I have noticed (also this is quite visible in most notebooks and discussions in the competition), is that the name of the shops contain quite a lot of information. In particular, the first word in the shop is the city where the shop is in. Let's extract the shop city with the fucntion below.","4c78c313":"# Predict Future Sales (EDA)\nPlease notice that most of this EDA has been inspired by Sarhak Batra's post (https:\/\/www.kaggle.com\/sarthakbatra\/predicting-sales-tutorial) - KUDOS to him"}}