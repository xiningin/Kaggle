{"cell_type":{"2fa06fbb":"code","41b999d5":"code","acb7a741":"code","e6dee78c":"code","08865e32":"code","82e9eb11":"code","8236bd56":"code","6aac8d85":"code","0354304e":"code","1df37d64":"code","bbf236c6":"code","94f491ae":"code","b818895b":"code","99d3ec8c":"code","f022e578":"code","f07a1f4e":"code","6941a4bc":"code","58c82740":"code","8392bc4e":"code","906c89ce":"code","2b1e50d7":"code","bac906f0":"code","e11df395":"code","edd704f8":"code","90f3d11d":"code","e3ab9e4a":"code","6124c285":"code","17f47baa":"code","74e63822":"code","6f1f7dea":"code","9b97483c":"code","1baa009a":"code","54349168":"code","f1e9da6d":"code","fbcdc40f":"code","9b355f4c":"code","62892018":"code","ac7fed5d":"code","d44a9f03":"code","dce99356":"code","aea5278d":"code","fafbfd97":"code","a1425aff":"code","758dcdf2":"code","31e3eddc":"code","d1623099":"code","23bb8a68":"code","3452d46e":"code","78325784":"code","e555b06e":"code","9d649fa9":"code","d43f0a96":"code","f4699230":"code","a77ec2a3":"code","31a20032":"code","cbf37d5a":"code","afd0e6ae":"code","648a6714":"code","a202026f":"code","754cff72":"code","5df48747":"code","60ae6743":"code","e4c7f5bc":"code","788a8926":"code","2661bf1f":"code","ceeb715c":"code","d756199f":"code","f6cdc30c":"code","50df7aa9":"code","0868dab5":"code","0e723719":"code","bb5b26e9":"code","01e1a6a2":"code","c4c0596d":"code","54ed8653":"code","bcec669e":"code","f0b470fc":"code","d1606703":"code","34016dce":"code","de42a649":"code","0704675b":"code","7a9681cc":"code","195c849d":"code","50a2ff48":"code","b7db5856":"code","db67a84d":"markdown","5d59a4f6":"markdown","94dd5661":"markdown","8536d845":"markdown","7e3a8db7":"markdown","2fdca92e":"markdown","991bc488":"markdown","023f762c":"markdown","eb3bc812":"markdown","333fa400":"markdown","a0cea4d5":"markdown","1b5fd8cd":"markdown","96e905e8":"markdown","a08eee37":"markdown","7b4a4e73":"markdown","bc4018e2":"markdown","93a8bb7f":"markdown","88d14e9a":"markdown","60a5f99c":"markdown","f7816e7d":"markdown","d39f6f74":"markdown","89f1ef6e":"markdown","4e28f495":"markdown","f9028850":"markdown","3c306715":"markdown","9b0adc1b":"markdown","28427b9c":"markdown","18f06838":"markdown","2ee9c3cb":"markdown","6fa001df":"markdown","d2e005f6":"markdown","bac30219":"markdown","11532883":"markdown","59b7635d":"markdown","507a3f10":"markdown","f4621078":"markdown","ae0fd478":"markdown","521cffef":"markdown","22e1442f":"markdown","cd5bd3aa":"markdown","6beb4caa":"markdown","86b961a7":"markdown","360f41c7":"markdown","2eca72ec":"markdown","67028455":"markdown"},"source":{"2fa06fbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","41b999d5":"# Project libraries\nimport pandas as pd\nimport os\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","acb7a741":"# Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')","e6dee78c":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain = train_data.copy()\ntest = test_data.copy()","08865e32":"train.head()","82e9eb11":"print(train.shape,test.shape)","8236bd56":"train.isnull().sum()","6aac8d85":"test.isnull().sum()","0354304e":"train.drop(['PassengerId'],axis=1,inplace=True)\ntest.drop(['PassengerId'],axis=1,inplace=True)\npred = train_data['Survived']","1df37d64":"#no. of passengers and cabon crew  survived in 891 total\nsurvived =train['Survived'].sum()\nNotsur = 891-survived\nprint(survived,Notsur)","bbf236c6":"\nimport matplotlib.pyplot as plt\n\n# The slices will be ordered and plotted counter-clockwise.\nlabels1 = 'Survived',train['Survived'].sum()\nlabels2 = 'Not Survived', 891-train['Survived'].sum()\nsizes = [train['Survived'].sum(), (891-train['Survived'].sum())]\n\ncolors = ['red', 'blue']\nexplode = (0, 0)  # explode a slice if required\n\nplt.pie(sizes,explode=explode, labels=(labels1,labels2), colors=colors,\n        autopct='%1.1f%%', shadow=True)\n        \n#draw a circle at the center of pie to make it look like a donut\ncentre_circle = plt.Circle((0,0),0.7,color='black', fc='white',linewidth=1.25)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.show()  \n\n","94f491ae":"sns.countplot(x= 'Sex',data=train , hue ='Survived')","b818895b":"sex1 = pd.get_dummies(train['Sex'])\nsex2 = pd.get_dummies(test['Sex'])\n\ntrain.drop(columns = ['Sex'],axis = 1 , inplace = True)\ntest.drop(columns = ['Sex'],axis = 1 , inplace = True)\n","99d3ec8c":"sex1","f022e578":"list(train.columns)","f07a1f4e":"train","6941a4bc":"train = pd.concat([train,sex1],axis=1)\ntest = pd.concat([test,sex2],axis=1)","58c82740":"train","8392bc4e":"train.isnull().sum()","906c89ce":"sns.countplot(x ='Pclass',data =train,hue ='Survived')","2b1e50d7":"plt.figure(figsize=(10, 9))\nsns.boxplot(x='Pclass',y='Age',data=train_data,palette='rainbow')","bac906f0":"train_data.Age[train_data.Pclass]","e11df395":"plt.figure(figsize=(16, 5))\nfor y in [1,2,3]:\n    x=train_data.Age[train_data.Pclass ==y]\n    sns.distplot(x,hist =False,kde =True,label=True)\n\nplt.title(\"Age vs Pclass\")\nplt.legend((\"1st\",\"2nd\",\"3rd\"))\nplt.xlabel(\"Age\")\nplt.ylabel(\"Density\")","edd704f8":"train.isnull().sum()","90f3d11d":"train['Age'].describe()","e3ab9e4a":"train[\"Age\"].fillna(train['Age'].describe().loc[['50%']][0], inplace = True) \ntest[\"Age\"].fillna(test['Age'].describe().loc[['50%']][0], inplace = True) ","6124c285":"train.isnull().sum()","17f47baa":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train[\"Fare\"])\nplt.ylabel('Density')\nplt.title(\"Fare distribution for all the people\")","74e63822":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train.Fare[train.Survived ==1],color = 'green')\nplt.ylabel('Density')\nplt.title(\"Fare distribution for all the people Who Survived\")","6f1f7dea":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(train.Fare[train.Survived ==0],color = 'red')\nplt.ylabel('Density')\nplt.title(\"Fare distribution for all the people Who Did'nt Survived\")","9b97483c":"test.isnull().sum()","1baa009a":"test[\"Fare\"].fillna(test['Fare'].describe().loc[['50%']][0], inplace = True) ","54349168":"plt.figure\nsns.countplot(x='Embarked',data = train ,hue = 'Survived' )","f1e9da6d":"Embarked_total =pd.get_dummies(train['Embarked'])\nEmbarked_total.sum()","fbcdc40f":"train[\"Embarked\"].fillna('S', inplace = True) \ntest[\"Embarked\"].fillna('S', inplace = True) ","9b355f4c":"embark1 = pd.get_dummies(train['Embarked'])\nembark2 = pd.get_dummies(test['Embarked'])\n\ntrain.drop(['Embarked'],axis=1,inplace=True)\ntest.drop(['Embarked'],axis=1,inplace=True)\n\ntrain = pd.concat([train,embark1],axis=1)\ntest = pd.concat([test,embark2],axis=1)","62892018":"train","ac7fed5d":"test","d44a9f03":"plt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"SibSp\", data=train ,color = \"Orange\" )\n\nplt.figure(figsize=(14, 6))\nax = sns.countplot(y=\"Survived\", hue=\"Parch\", data=train , color = \"Green\" )\n\nplt.show()","dce99356":"def fam(x):\n    if  (x['SibSp'] + x['Parch'])  > 0:\n        return 1\n    else:\n        return 0\n\ntrain['Family'] = train.apply(fam, axis = 1)\ntest['Family'] = test.apply(fam, axis = 1)","aea5278d":"train = train.drop(['SibSp','Parch'],axis=1)\ntest = test.drop(['SibSp','Parch'],axis=1)","fafbfd97":"plt.figure(figsize=(6, 6))\nsns.set_style('whitegrid')\nsns.countplot(x='Family',data=train,hue='Survived')\nplt.xlabel((\"0 Without Family\",\"1 With Family\"))\nplt.legend((\"Not Survived\",\"Survived\"))","a1425aff":"train","758dcdf2":"cab =pd.get_dummies(train['Cabin'])\ncab","31e3eddc":"train[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train['Cabin'] ])\ntest[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test['Cabin'] ])","d1623099":"sns.countplot(x='Cabin',data =train,hue = 'Survived')","23bb8a68":"plt.figure(figsize=(8, 5))\ng = sns.catplot(y=\"Survived\",x=\"Cabin\",data=train,kind=\"bar\",order=['A','B','C','D','E','F','G','X'])","3452d46e":"train[\"Cabin\"] = train[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntrain[\"Cabin\"] = train[\"Cabin\"].astype(int)\ntest[\"Cabin\"] = test[\"Cabin\"].map({\"X\":0, \"A\":1, \"B\" : 2 , \"C\":3, \"D\":4, \"E\":5, \"F\":6, \"G\":7,\"T\":0})\ntest[\"Cabin\"] = test[\"Cabin\"].astype(int)","78325784":"train","e555b06e":"#txt = \"Braund, Mr. Owen Harris\"\n#x = txt.split(\",\")[1].split(\".\")[0].strip()\n#print(x)\n\n\ntrain_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in train[\"Name\"]]\ntrain[\"Title\"] = pd.Series(train_title)\ntest_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in test[\"Name\"]]\ntest[\"Title\"] = pd.Series(test_title)","9d649fa9":"train","d43f0a96":"plt.figure(figsize=(18,5))\nsns.countplot(x =\"Title\",data =train,hue=\"Survived\")","f4699230":"train[\"Title\"] = train[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain[\"Title\"] = train[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntrain[\"Title\"] = train[\"Title\"].astype(int)\ntest[\"Title\"] = test[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest[\"Title\"] = test[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntest[\"Title\"] = test[\"Title\"].astype(int)","a77ec2a3":"train","31a20032":"ticket_list = train[\"Ticket\"].tolist()\nticket_list","cbf37d5a":"Ticket1 = []\nfor i in list(train.Ticket):\n    if not i.isdigit() :\n        Ticket1.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket1.append(\"X\")\ntrain[\"Ticket\"] = Ticket1\n\nTicket2 = []\nfor j in list(test.Ticket):\n    if not j.isdigit() :\n        Ticket2.append(j.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket2.append(\"X\")\ntest[\"Ticket\"] = Ticket2","afd0e6ae":"train[\"Ticket\"].unique()","648a6714":"test[\"Ticket\"].unique()","a202026f":"union=np.union1d(train[\"Ticket\"], test[\"Ticket\"])\nunion","754cff72":"train","5df48747":"train= pd.get_dummies(train, columns = [\"Ticket\"], prefix=\"T\")\ntest = pd.get_dummies(test, columns = [\"Ticket\"], prefix=\"T\")","60ae6743":"train = train.drop(['T_SP','T_SOP','T_Fa','T_LINE','T_SWPP','T_SCOW','T_PPP','T_AS','T_CASOTON'],axis = 1)\ntest = test.drop(['T_SCA3','T_STONOQ','T_AQ4','T_A','T_LP','T_AQ3'],axis = 1)","e4c7f5bc":"train","788a8926":"train.columns.tolist()","2661bf1f":"test.columns.tolist()","ceeb715c":"train.drop(['Survived'],axis=1,inplace=True)","d756199f":"train","f6cdc30c":"train.drop(['Name'],axis=1,inplace=True)\ntest.drop(['Name'],axis=1,inplace=True)","50df7aa9":"print(train.isnull().sum())\nprint(\"Number of columns are :\",train.isnull().sum().count())","0868dab5":"print(test.isnull().sum())\nprint(\"Number of columns are :\",test.isnull().sum().count())","0e723719":"test","bb5b26e9":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\ntrain2 = sc.fit_transform(train)\ntest2 = sc.transform(test)","01e1a6a2":"from sklearn.pipeline import Pipeline\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nimport xgboost as xgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, cross_val_score\n\nKFold_Score = pd.DataFrame()\nclassifiers = ['Linear SVM', 'Radial SVM', 'LogisticRegression', \n               'RandomForestClassifier', 'AdaBoostClassifier', \n               'XGBoostClassifier', 'KNeighborsClassifier','GradientBoostingClassifier']\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          xgb.XGBClassifier(n_estimators=100),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)\n         ]\nj = 0\nfor i in models:\n    model = i\n    cv = KFold(n_splits=5, random_state=0, shuffle=True)\n    KFold_Score[classifiers[j]] = (cross_val_score(model, train, np.ravel(pred), scoring = 'accuracy', cv=cv))\n    j = j+1","c4c0596d":"mean = pd.DataFrame(KFold_Score.mean(), index= classifiers)\nKFold_Score = pd.concat([KFold_Score,mean.T])\nKFold_Score.index=['Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Mean']\nKFold_Score.T.sort_values(by=['Mean'], ascending = False)","54ed8653":"col_name1 = list(train.columns)\ncol_name2 = list(test.columns)","bcec669e":"col_name1[0],col_name1[2] = col_name1[2],col_name1[0]\ncol_name2[0],col_name2[2] = col_name2[2],col_name2[0]","f0b470fc":"train_new = train[col_name1]\ntest_new = test[col_name2]","d1606703":"train_new = train_new.drop(['Cabin'],axis = 1)\ntest_new = test_new.drop(['Cabin'],axis = 1)","34016dce":"sc = StandardScaler()\ntrain3 = sc.fit_transform(train_new)\ntest3 = sc.transform(test_new)","de42a649":"rfc = RandomForestClassifier(random_state=0)","0704675b":"param_grid = { \n    'n_estimators': [ 200,300],\n    'max_features': ['auto', 'sqrt'],\n    'max_depth' : [6,7,8],\n    'criterion' :['gini', 'entropy']\n}","7a9681cc":"from sklearn.model_selection import GridSearchCV\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(train3,pred )\nCV_rfc.best_params_","195c849d":"rfc1=RandomForestClassifier(random_state=0, n_estimators= 200, criterion = 'gini',max_features = 'auto',max_depth = 8)\nrfc1.fit(train3, pred)","50a2ff48":"pred3= rfc1.predict(test3)\nprint(pred3)","b7db5856":"pred_test = pred3\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_test})\noutput.to_csv('.\/submission.csv', index=False)","db67a84d":"Using K-Folds Cross validation to evaluate the prformance of our models","5d59a4f6":"Here we will be combining the SibSp and Parch column into one column and determining whether the passenger has a family or not.","94dd5661":"5.Fare\n","8536d845":"Here is a tricky part. The training set and test set have a few tickets which are unique to themselves.","7e3a8db7":"These are the following unique tickets which are dropped","2fdca92e":"6.\nSibSp and Parch","991bc488":"1.Survival Rate of 891(train)","023f762c":"Applying RandomForestClassifier with hyperparameter tuning on our new training set.\n\n\nNote: I have performed numerous permutations with various hyperparameters but the given following are the ones which gave me the best result.","eb3bc812":"In this dataset we have a \"Name\" column mentioning the name of every passenger. These names also have a title along with them which can be useful.","333fa400":"2.Sex","a0cea4d5":"Here we can see that a passenger having no family had a lesser chance of survival.","1b5fd8cd":"Here we can see that people from higher class(1 being the highest) had a better chance of survival","96e905e8":"In the above feature preprocessing, the values that I have used for filling missing values were chosen after experimenting with different values. I took these values as they gave me the best result. Median values are best suited for missing values in most of the Machine-Learning models.","a08eee37":"Here \".get_dummies()\" will convert this column and make 2 dummy columns of male and female. This is done in order to convert the categorical data into numerical.\n","7b4a4e73":"Here \".get_dummies()\" will convert this column and make 3 dummy columns of C,Q,S. This is done in order to convert the categorical data into numerical.\n\nAdding 3 columns C,Q,S\n* S - Southampton\n* C - Cherbourg\n* Q - Queenstown\n\n","bc4018e2":"This is our final training set after preprocessing","93a8bb7f":"These above tickets are common in both the sets","88d14e9a":"This is the most important section of this project. Here, the ultimate goal is to find an optimal combination of hyperparameters that minimizes a predefined loss function to give better results","60a5f99c":"Note : This column has not helped me much because much of the data was missing specificly 687 data was missing outoff 891","f7816e7d":"As you Can See there is Missing Values in Ages Which Will be Filled By Median","d39f6f74":"4. Age","89f1ef6e":"Note: These 2 columns were not giving any valuable information\/trend that could have helped in getting accurate prediction, hence they were combined","4e28f495":"Here RandomForestClassifier is giving the highest accuracy.\n","f9028850":"### Features","3c306715":"3.PClass","9b0adc1b":"9. Tickit","28427b9c":" In this section we will be training various models using different classifiers. Out of them all, we will be choosing the best classifier to give us the most accurate prediction","18f06838":"# Feature Scaling with Standardization","2ee9c3cb":"This column holds the embarkation records for all the passengers.\u00b6\nThey stand for:\n* S - Southampton\n* C - Cherbourg\n* Q - Queenstown","6fa001df":"\n# Model Training","d2e005f6":"Important Note:\nAs I had mentioned earlier in feature preprocessing, the \"Cabin\" column does not help us in getting a better prediction and here is why:-\n\n","bac30219":"In an actual sinking emergency (just like the one here) all the passengers irrespective of their cabin would gather around at the port side and starboard side of the ship for evacuation. Similar thing must have happened with Titanic. Therefore it does not matter which cabin you are occupying.","11532883":"Considering this I droped the \"Cabin\" column.\n","59b7635d":"There will be visualization of data using different graphs and preprocessing on the training and test set.","507a3f10":"So basically here I have Removed Sex columns From Train and test Dataset and now concating sex1 &sex2 in train and test","f4621078":"\"Fare\" column tells us about the amount of money paid by the passengers. Here we can see that passengers had a greater probability of surviving if they had payed more|","ae0fd478":"# Hyperparameter Tuning","521cffef":"5.Embarked","22e1442f":"In order to maintain the same number of columns I had to tamper the test set and had to remove these unique tickets. Same was done with training set","cd5bd3aa":"8. Name and Titles","6beb4caa":"Here we have our titles mapped with numeric values","86b961a7":"wee have Used S because it is most common and appears many time(mode)","360f41c7":"Women and children had the first preference, along with them there were people who belonged to the higher class. This data is covered under the \"Sex\" , \"Pclass\" and \"Age\" column.","2eca72ec":"7.Cabin","67028455":"SibSp tells us about the passengers' siblings and spouse.\n\n\n\nParch tells us about the passengers' parents and children."}}