{"cell_type":{"0e006c9a":"code","f6085fcf":"code","a489f4db":"code","76d7d19c":"code","fc001267":"code","1813fc88":"code","71828420":"code","52afc994":"code","ed7de3f9":"code","29e9b2e6":"code","c72af494":"code","24a27d82":"code","cd93a0ed":"code","823bc322":"code","558edc88":"code","d0cb204f":"code","aac4153b":"markdown","28c6fa6a":"markdown","b4cbd55a":"markdown","221cf1b4":"markdown","8e344665":"markdown","f1e7071c":"markdown","0862dc29":"markdown","ede96a70":"markdown","83ab491c":"markdown","8157b63b":"markdown"},"source":{"0e006c9a":"train_file_path = '\/kaggle\/input\/titanic\/train.csv'\ntest_file_path = '\/kaggle\/input\/titanic\/test.csv'","f6085fcf":"import numpy as np \nimport pandas as pd","a489f4db":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)","76d7d19c":"train_df.info()","fc001267":"import re\n#get the list of titles which are atleast more than 2 from the combined training and testing set of names\ndef getValidTitles(train_df):    \n    all_names = list(train_df.Name.values)\n    all_names = \" \".join(n for n in all_names)\n    all_titles = re.findall(r'([A-Za-z]+)\\.\\W', all_names)\n    title_counts = [(x,1) for x in all_titles]\n    title_df = pd.DataFrame(title_counts, columns = ['Title', 'Count'])\n    title_df = title_df.groupby('Title').sum()\n    common_titles = title_df[title_df.Count > 2].index.values\n    return common_titles\n\nVALID_TITLES = getValidTitles(train_df)","1813fc88":"def getTitle(name, title_scope):\n    matches = re.findall(r'([A-Za-z]+)\\.\\W', name)\n    if len(matches) == 0:\n        title = 'NA'\n    else:\n        if matches[0] in title_scope:\n            title = matches[0] #first matching pattern in the list\n        else:\n            title = 'NA'\n    return title\n\ndef getIsIndividual(row):\n    if pd.isna(row.SibSp) or pd.isna(row.Parch):\n        individual = 1\n    elif (row.SibSp + row.Parch) > 0:\n        individual = 0\n    else:\n        individual = 1\n    return individual\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass FeatureTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, common_titles):\n       self.feature_list =  ['Pclass', 'Sex', 'Age', 'Fare', 'Title', 'Individual']\n       self.title_scope = common_titles\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X['Title'] = X['Name'].apply(lambda name: getTitle(name, self.title_scope))\n        X['Individual'] = X.apply(lambda row: getIsIndividual(row), axis=1)\n        return X[self.feature_list]","71828420":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\nfeature_transformer = FeatureTransformer(VALID_TITLES)\nnum_pipeline = Pipeline(steps = [('imputer', SimpleImputer(strategy='mean')), ('minmax_scale', MinMaxScaler())])\ncat_pipeline = Pipeline(steps = [('encode_PClass', OrdinalEncoder())])\ncolumn_transformer = ColumnTransformer(transformers=[('num_transformer', num_pipeline, ['Age', 'Fare']),\n                                                    ('cat_transformer', cat_pipeline, ['Pclass', 'Sex', 'Title', 'Individual'])])\n\ndata_pipeline = Pipeline(steps = [('add_select_features', feature_transformer), \n                                  ('column_transformations', column_transformer)], verbose=True)\n","52afc994":"X = data_pipeline.fit_transform(train_df) #use fit transform on the training data\nX_test = data_pipeline.transform(test_df) #use transform on the test data\ny = train_df['Survived'].values","ed7de3f9":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","29e9b2e6":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, recall_score\n\nparam_grid = [{'n_estimators': [50, 100, 200, 300], 'max_depth': [10,20,30], 'bootstrap': [True, False]}]\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, scoring='accuracy', cv=3, verbose=True)\ngrid_search.fit(X_train, y_train)","c72af494":"grid_search.best_params_","24a27d82":"model = grid_search.best_estimator_","cd93a0ed":"y_train_hat = model.predict(X_train)\ny_train_hat_prob = model.predict_proba(X_train)[:,1]\naccuracy = accuracy_score(y_train, y_train_hat,)\nauc = roc_auc_score(y_train, y_train_hat_prob)\nprint(\"Model performance on the training set:\\nAccuracy: {:4f}\\nAUC: {:4f}\".format(accuracy, auc))\ny_val_hat = model.predict(X_val)\ny_val_hat_prob = model.predict_proba(X_val)[:,1]\naccuracy = accuracy_score(y_val, y_val_hat,)\nauc = roc_auc_score(y_val, y_val_hat_prob)\nprint(\"Model performance on the hold out set:\\nAccuracy: {:4f}\\nAUC: {:4f}\".format(accuracy, auc))","823bc322":"y_test_hat = model.predict(X_test)","558edc88":"sub_df = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': y_test_hat})","d0cb204f":"sub_df.to_csv('submit_final.csv', index=False)","aac4153b":"Now use the defined class and get the processed train and test set.  \n\nI create a pipeline as follows: \n - First step is to use the Feature transformer class which adds additional features and keeps only the required features.  \n - Next the numerical pipeline is defined which imputes the missing values with mean, followed by minmax scaling (scaling is not required for RF)\n - Next the categorical features are encoded\n - A column transformer defines which columns are to be passed to numerical pipeline and categorical pipeline\n - Final pipeline sequentially joins these Feature transformer and column transformer\n\nThe list of features that I get after passing through the pipeline are:\n1. Pclass: Encoded\n2. Sex: Encoded\n3. Age: Missing values are imputed with mean and then minmax scaled (actually scaling is not required for RF, but i kept it because I wanted a common pipeline to try other models like Logistic Regression)\n4. Fare: Missing values are imputed with mean and then minmax scaled\n5. Title: Encoded\n6. Individual: 1 (if SibSp + Parch = 0) else 0\n\n**Note: These features gave the best result on the submission set. Initially I had also used **Embarked** feature, but the accuracy was low. So I removed this feature.** ","28c6fa6a":"Train a RF classifier using grid search with 3 fold cross validation","b4cbd55a":"predictions on the test set","221cf1b4":"After reading the discussion forums, extracting the **Title** from the name made sense. \n\nI use the following method to get a list of titles from the training set. I have seen some notebooks where the names from test set where also used to get the list of titles. But I do not use any information from test set because it would result in information leak.","8e344665":"Read the training and testing dataset","f1e7071c":"Out of the available features in the raw data, not all features make sense to be used. \n\nLets us define some functions to add additional features like **Title** and whether a person boarded as an **Individual**.\nFurther let us define a class to add these features. This is useful for creating a pipeline with scikit-learn pipeline and column transformers.","0862dc29":"Now let us transform the raw data using the pipeline","ede96a70":"Define the file paths of training and testing data sets","83ab491c":"Split the train set further into a hold-out set (20%)\n\n\n(Initially I used 5% and 10%, but the submisison accuracy was low because of overfitting. Finally using 20% hold-out set from the training data helped in getting a generalized model) ","8157b63b":"Evalute the model"}}