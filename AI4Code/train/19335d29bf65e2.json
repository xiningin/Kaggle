{"cell_type":{"58f7bde8":"code","baae34c0":"code","79b3c12a":"code","79c5cdcb":"code","238b9b22":"code","75df7fbd":"code","ab3250f1":"code","764c2af7":"code","31e8f022":"code","f8019c7d":"code","c571c16c":"code","6496d35e":"code","b5b14988":"code","3eb8aa30":"code","2c7a3f72":"markdown","885ff31b":"markdown","1c4f9d42":"markdown","112827ce":"markdown","b4e24ba3":"markdown","a1c57f3f":"markdown","8f52d28c":"markdown","bbb2220b":"markdown","2e614fe7":"markdown","9469a90c":"markdown","e2734a9e":"markdown","fa5765f1":"markdown","0676667b":"markdown","3c5de500":"markdown","d0c9f844":"markdown","b520c773":"markdown"},"source":{"58f7bde8":"import numpy as np\nimport pandas as pd\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_path = dirname +'\/'+ filename\n\n# Read file\ndf = pd.read_csv(file_path)\n\ndf.head()","baae34c0":"df.isnull().sum()","79b3c12a":"df = df.drop(\n        ['job_id', 'title', 'location', 'department', 'salary_range', 'company_profile', 'description', 'requirements',\n         'benefits'], axis=1).sort_index()","79c5cdcb":"y = df['fraudulent']\n\ny","238b9b22":"X = df.drop(['fraudulent'], axis=1)\n\nX.head()","75df7fbd":"# Different features for my model\nnumerical_features = ['telecommuting', 'has_company_logo', 'has_questions']\nlabel_features = ['employment_type', 'required_experience', 'required_education', 'industry', 'function']\n","ab3250f1":"for feature in label_features:\n    X[feature].replace(np.nan, X[feature].mode()[0], regex=True, inplace=True)\n    \nX.head()","764c2af7":"'''\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median'))])\n\n    categorical_transformer = Pipeline(steps=[\n        ('cat_imputer', OneHotEncoder())])\n\n    preprocessing = ColumnTransformer(transformers=[\n        ('numerical', numeric_transformer, numerical_features),\n        ('categorical', categorical_transformer, label_features)\n    ])\n\n    log_reg = Pipeline(steps=[\n        ('preprocessing', preprocessing),\n        ('scaler', StandardScaler(with_mean=False)),\n        ('log', LogisticRegression())\n    ])\n    '''","31e8f022":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import mean_absolute_error\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA","f8019c7d":"imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nimputer.fit_transform(X[numerical_features])\n\nc_t = make_column_transformer((OneHotEncoder(), label_features), remainder='passthrough')\n\nbig_X = c_t.fit_transform(X).toarray()\n\nsc = StandardScaler()\n\nbig_X = sc.fit_transform(big_X)\n\npca = PCA()\n\nbig_X = pca.fit_transform(big_X)\n\nrus = RandomUnderSampler()\n\nundersampled_x, y = rus.fit_resample(big_X,y)\n\nx_train, x_test, y_train, y_test = train_test_split(undersampled_x, y, test_size=0.2, random_state=0)\n\nlog_reg = LogisticRegression()\n\nlog_reg.fit(x_train, y_train)\n\ny_pred = log_reg.predict(x_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(f'Prediction score: {log_reg.score(x_test, y_test) * 100:.2f}%')\nprint(f'MAE from Logistic Regression: {mean_absolute_error(y_test, y_pred) * 100:.2f}%')","c571c16c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","6496d35e":"knn = KNeighborsClassifier()\n\ngrid = GridSearchCV(knn, param_grid={'n_neighbors':range(1,31)}, scoring='accuracy')\n\ngrid.fit(undersampled_x,y)\n\nfor i in range(0, len(grid.cv_results_['mean_test_score'])):\n    print('N_Neighbors {}: {} '.format(i+1, grid.cv_results_['mean_test_score'][i]*100))","b5b14988":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","3eb8aa30":"rf = RandomForestClassifier(bootstrap=True)\n\nrf.fit(undersampled_x,y)\n\n# Cross validation of 5 folds\nscore = cross_val_score(rf, undersampled_x, y)\n\nprint(f'Prediction score: {np.mean(score) * 100:.2f}%')","2c7a3f72":"# Doing some Preprocessing","885ff31b":"# K-Nearest Neighbors with Grid Search and Cross Validation\nTo optimize K, I used GridSearchCV neighbors 1-30 and used cross validation kfold=5 to reduce variablity.\n\nImport what we need:","1c4f9d42":"So far, this is the best scoring model I have created for this problem. Before cross validation, I attempted random forests and scored 92%. I was suspicious of this result, so I added CV and my scores shot down to ~82% on average. While this seems \"bad\" in a way,\ncompared to the other models it is not! This model does not shoot down under 80%, while my other models' scores vary from 76%-83%.  \nI suspect that the nature of the Random Forest being able to generalize more data is the reason why it scores higher than the others. What do you think about this claim?","112827ce":"Going through the dataframe, I notice some missing values and some useless columns that won't help with predicting the target.\n\nRemove useless columns:\n","b4e24ba3":"Manually doing what I intended for the pipeline to do:","a1c57f3f":"# Random Forests Classification with Cross Validation\nLet's import what we need:","8f52d28c":"OK scores. I used GridSearch and Cross Validation to help my model scores out a bit. There wasn't much variance though.","bbb2220b":"I was going to have a beautifully organized pipeline going with the plan of imputing all the missing numericals and\none hot encoding all my categoricals.","2e614fe7":"Overall, I think the undersampling hurt my scores significantly. Does anyone have any suggestions? \n\nThanks for reading and give me your thoughts!","9469a90c":"Here is my pipeline stuff if you are curious","e2734a9e":"Now let's scale X and to prepare for fitting","fa5765f1":"Personally, I really wanted to get a system going to clean up the location column (because I was thinking of one hot encoding the country, state, and city, but\nI was not sure if that would help the model or not. ","0676667b":"Set my target variable","3c5de500":"As stated before, we saw some nan's in columns that consisted of strings, so this is how I imputed them:\n- Sidenote, what methods do you guys prefer for imputering? I looked at sklearn_pandas.CategoricalImputer, but it didn't work. So I did it manually","d0c9f844":"# Logistic Regression","b520c773":"# Real of Fake Job Descriptions using Several Models\n\nThanks for taking the time to look at my kernel. I am relatively new to ML and data science, so all criticism is welcome.\n\nCurrently I only have a Logistic Regression Model, but I plan to add more!\nI tried to build a pipeline to do all my tasks, but I couldn't get it to work (so I commented it out). It should still provide the benefit of pipelines\nin terms of readability so you can better understand what I did."}}