{"cell_type":{"a8869b75":"code","e745fb23":"code","409fe7c6":"code","2379f7be":"code","c8094069":"code","4afc66d3":"code","30df9e9b":"code","dfc78138":"code","3668a0dc":"code","7a36def5":"code","1bb23554":"code","7772df06":"code","78ff3e43":"code","cb6f704f":"code","394714a7":"code","f7baa05c":"code","4544607c":"code","ced82cb9":"code","2833a7f0":"code","ec3200b7":"code","ed9a0bac":"code","36f00c08":"code","a28243cd":"code","da81dc48":"code","9196b2fb":"code","cc82d922":"code","df1a552f":"markdown","9f9faa2e":"markdown","e4db8f38":"markdown","a7f2939f":"markdown","da41539c":"markdown","42fe4162":"markdown","1a36fb15":"markdown","e569a54d":"markdown"},"source":{"a8869b75":"import numpy as np\nimport pandas as pd\n\nimport re\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","e745fb23":"data = pd.read_csv('..\/input\/amazon-top-50-bestselling-books-2009-2019\/bestsellers with categories.csv')","409fe7c6":"data","2379f7be":"data.info()","c8094069":"stop_words = stopwords.words('english')\n\ndef process_name(name):\n    name = re.sub(r'\\d+', ' ', name)\n    name = name.split()\n    name = \" \".join([word for word in name if word not in stop_words])\n    return name","4afc66d3":"names = data['Name'].apply(process_name)","30df9e9b":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(names)\n\nvocab_length = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary length:\", vocab_length)","dfc78138":"names = tokenizer.texts_to_sequences(names)","3668a0dc":"max_seq_length = np.max(list(map(lambda name: len(name), names)))\n\nprint(\"Max sequence length:\", max_seq_length)","7a36def5":"names = pad_sequences(names, maxlen=max_seq_length, padding='post')","1bb23554":"names","7772df06":"data = data.drop('Name', axis=1)","78ff3e43":"data","cb6f704f":"genre_mapping = {'Non Fiction': 0, 'Fiction': 1}\n\ndata['Genre'] = data['Genre'].replace(genre_mapping)","394714a7":"print(\"Number of unique authors:\", len(data['Author'].unique()))","f7baa05c":"def onehot_encode(df, column, prefix):\n    df = df.copy()\n    dummies = pd.get_dummies(df[column], prefix=prefix)\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop(column, axis=1)\n    return df","4544607c":"data = onehot_encode(data, 'Author', 'auth')","ced82cb9":"data","2833a7f0":"y = data['Genre'].copy()\nX = data.drop('Genre', axis=1).copy()","ec3200b7":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)","ed9a0bac":"names_train, names_test, X_train, X_test, y_train, y_test = train_test_split(names, X, y, train_size=0.7, random_state=100)","36f00c08":"names.shape","a28243cd":"X.shape","da81dc48":"embedding_dim = 64\n\n# Name features\nname_input = tf.keras.Input(shape=(20,), name=\"name_input\")\n\nembedding = tf.keras.layers.Embedding(\n    input_dim=vocab_length,\n    output_dim=embedding_dim,\n    input_length=max_seq_length,\n    name=\"name_embedding\"\n)(name_input)\n\nname_flatten = tf.keras.layers.Flatten(name=\"name_flatten\")(embedding)\n\n\n# Other features\nother_input = tf.keras.Input(shape=(252,), name=\"other_input\")\n\nhidden_1 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_1\")(other_input)\nhidden_2 = tf.keras.layers.Dense(256, activation='relu', name=\"other_dense_2\")(hidden_1)\n\n# Concatenate and output\nconcat = tf.keras.layers.concatenate([name_flatten, hidden_2], name=\"concatenate\")\n\noutputs = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(concat)\n\n\nmodel = tf.keras.Model(inputs=[name_input, other_input], outputs=outputs)\n\n\nprint(model.summary())\n\ntf.keras.utils.plot_model(model)","9196b2fb":"model.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=[\n        'accuracy',\n        tf.keras.metrics.AUC(name='auc')\n    ]\n)\n\n\nbatch_size = 32\nepochs = 100\n\nhistory = model.fit(\n    [names_train, X_train],\n    y_train,\n    validation_split=0.12,\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","cc82d922":"model.evaluate([names_test, X_test], y_test)","df1a552f":"# Encoding Other Features","9f9faa2e":"# Results","e4db8f38":"# Preprocessing","a7f2939f":"# Getting Started","da41539c":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/FIY53JthQD0","42fe4162":"# Splitting\/Scaling","1a36fb15":"# Task for Today  \n\n***\n\n## Bestseller Genre Prediction  \n\nGiven *data about Amazon's Top 50 best selling books from 2009-2019*, let's try to predict the **genre** of a given book.  \n  \nWe will use a TensorFlow ANN with two inputs to make our predictions.","e569a54d":"# Modeling\/Training"}}