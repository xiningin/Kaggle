{"cell_type":{"0d8f0d54":"code","dcf15588":"code","ff14a78a":"code","fcac6847":"code","393d7456":"code","3bd83f99":"code","1f9ecdf3":"code","3bc45b71":"code","695d6e9b":"code","7dec736d":"code","aa8bee5a":"markdown","457263a5":"markdown","f3bcdb30":"markdown","d202e8bc":"markdown","f6a81e1e":"markdown","b116182b":"markdown"},"source":{"0d8f0d54":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"..\/input\/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dcf15588":"# !rm -r pascal_voc","ff14a78a":"!mkdir -p pascal_voc\/model","fcac6847":"import torch\nimport cv2\nimport os\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\n\n\nDATASET = '..\/input\/pascal-voc-yolo-works-with-albumentations\/PASCAL_VOC'\nDEVICE = 'cuda'\n# seed_everything()  # If you want deterministic behavior\nNUM_WORKERS = 8\nBATCH_SIZE = 16\nIMAGE_SIZE = 416\nNUM_CLASSES = 20\nLEARNING_RATE = 1e-5\nWEIGHT_DECAY = 1e-4\nNUM_EPOCHS = 61\nCONF_THRESHOLD = 0.4\nMAP_IOU_THRESH = 0.5\nNMS_IOU_THRESH = 0.45\nconfig_S = [IMAGE_SIZE \/\/ 32, IMAGE_SIZE \/\/ 16, IMAGE_SIZE \/\/ 8]\nPIN_MEMORY = True\nLOAD_MODEL = True\nSAVE_MODEL = True\nCHECKPOINT_FILE = \"pascal_voc\/model\/\"\nLOAD_FILE = \"..\/input\/yolov3-for-pascal-voc\/pascal_voc\/model\/checkpoint2.pth.tar\"\nIMG_DIR = DATASET + \"\/images\/\"\nLABEL_DIR = DATASET + \"\/labels\/\"\n\nANCHORS = [\n    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n]  # Note these have been rescaled to be between [0, 1]\n\n\nscale = 1.1\ntrain_transforms = A.Compose(\n    [\n        A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),\n        A.PadIfNeeded(\n            min_height=int(IMAGE_SIZE * scale),\n            min_width=int(IMAGE_SIZE * scale),\n            border_mode=cv2.BORDER_CONSTANT,\n        ),\n        A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),\n        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),\n        A.OneOf(\n            [\n                A.ShiftScaleRotate(\n                    rotate_limit=10, p=0.4, border_mode=cv2.BORDER_CONSTANT\n                ),\n                A.IAAAffine(shear=10, p=0.4, mode=\"constant\"),\n            ],\n            p=1.0,\n        ),\n        A.HorizontalFlip(p=0.5),\n        A.Blur(p=0.1),\n        A.CLAHE(p=0.1),\n        A.Posterize(p=0.1),\n        A.ToGray(p=0.1),\n        A.ChannelShuffle(p=0.05),\n        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[],),\n)\ntest_transforms = A.Compose(\n    [\n        A.LongestMaxSize(max_size=IMAGE_SIZE),\n        A.PadIfNeeded(\n            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT\n        ),\n        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n        ToTensorV2(),\n    ],\n    bbox_params=A.BboxParams(format=\"yolo\", min_visibility=0.4, label_fields=[]),\n)\n\nPASCAL_CLASSES = [\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",\n    \"chair\",\n    \"cow\",\n    \"diningtable\",\n    \"dog\",\n    \"horse\",\n    \"motorbike\",\n    \"person\",\n    \"pottedplant\",\n    \"sheep\",\n    \"sofa\",\n    \"train\",\n    \"tvmonitor\"\n]","393d7456":"import matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport numpy as np\nimport os\nimport random\nimport torch\nimport glob\nimport time\n\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom collections import Counter\n\ndef iou_width_height(boxes1, boxes2):\n    \"\"\"\n    Parameters:\n        boxes1 (tensor): width and height of the first bouding boxes\n        boxes2 (tensor): width and height of the second bounding boxes\n    Returns:\n        tensor: Intersection over union of the corresponding boxes\n    \"\"\"\n    intersection = torch.min(boxes1[..., 0], boxes2[..., 0]) * torch.min(boxes1[..., 1], boxes2[..., 1])\n    union = boxes1[..., 0] * boxes1[..., 1] + boxes2[..., 0] * boxes2[..., 1] - intersection\n    return intersection \/ union\n\n\ndef intersection_over_union(boxes_preds, boxes_labels, box_format='midpoint'):\n    \"\"\"\n    This function calculates intersection over union (iou) given pred boxes\n    and target boxes.\n    Parameters:\n        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n        box_format (str): midpoint\/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n    Returns:\n        tensor: Intersection over union for all examples\n    \"\"\"\n\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] \/ 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] \/ 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] \/ 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] \/ 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] \/ 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] \/ 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] \/ 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] \/ 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection \/ (box1_area + box2_area - intersection + 1e-6)\n\n\ndef non_max_suppression(bboxes, iou_threshold, threshold, box_format='corners'):\n    \"\"\"\n    Does Non Max Suppression given bboxes\n    Parameters:\n        bboxes (list): list of lists containing all bboxes with each bboxes\n        specified as [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): threshold where predicted bboxes is correct\n        threshold (float): threshold to remove predicted bboxes (independent of IoU)\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n    Returns:\n        list: bboxes after performing NMS given a specific IoU threshold\n    \"\"\"\n\n    assert type(bboxes) == list\n    \n    time_limit = 10.0\n    max_det = 300 # maximum of detection per image\n#     max_nms = 30000 # maximum of boxes into torchvision.ops.nms()\n    \n    t = time.time()\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n        \n        bboxes = [\n            box for box in bboxes if int(box[0]) != int(chosen_box[0])\n                or intersection_over_union(torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format) < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n        \n        if (time.time() - t) > time_limit:\n            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n            print(len(bboxes_after_nms))\n            break  # time limit exceeded\n            \n    if len(bboxes_after_nms) > max_det:\n        bboxes_after_nme = bboxes_after_nms[:max_det]\n\n    return bboxes_after_nms\n\n\ndef mean_average_precision(pred_boxes, true_boxes, iou_threshold=.5, box_format='midpoint', num_classes=20):\n    \"\"\"\n    This function calculates mean average precision (mAP)\n    Parameters:\n        pred_boxes (list): list of lists containing all bboxes with each bboxes\n        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): Similar as pred_boxes except all the correct ones\n        iou_threshold (float): threshold where predicted bboxes is correct\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n        num_classes (int): number of classes\n    Returns:\n        float: mAP value across all classes given a specific IoU threshold\n    \"\"\"\n    \n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    eps = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n\n        # if none exists for this class then we safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format,)\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # True positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if iou is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum \/ (total_true_bboxes + eps)\n        precisions = TP_cumsum \/ (TP_cumsum + FP_cumsum + eps)\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) \/ len(average_precisions)\n\n\ndef plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes in the image\"\"\"\n    cmap = plt.get_cmap(\"tab20b\")\n    class_labels = PASCAL_CLASSES\n    colors = [cmap(i) for i in np.linspace(0, 1, len(class_labels))]\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # create figures and axes\n    fig, ax = plt.subplots(1)\n    # display the image\n    ax.imshow(im)\n\n    # box[0] is x midpoint, box[2] is width\n    # box[1] is y midpoint, box[3] is height\n\n    # create a Rectangle patch\n    for box in boxes:\n        assert len(box) == 6, \"box should contain class pred, confidence, x, y, width, height\"\n        class_pred = box[0]\n        conf = box[1]\n        box = box[2:]\n        upper_left_x = box[0] - box[2] \/ 2\n        upper_left_y = box[1] - box[3] \/ 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=2,\n            edgecolor=colors[int(class_pred)],\n            facecolor='none',\n        )\n        # add the patch to the Axes\n        ax.add_patch(rect)\n        plt.text(\n            upper_left_x,\n            upper_left_y,\n            s = class_labels[int(class_pred)] + ': ' + str(conf),\n            color='white',\n            verticalalignment='top',\n            bbox={'color': colors[int(class_pred)], 'pad': 0},\n        )\n\n    plt.show()\n\n\ndef get_evaluation_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    anchors,\n    threshold,\n    box_format='midpoint',\n    device='cuda'\n):\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n    all_pred_boxes = []\n    all_true_boxes = []\n    for batch_idx, (x, labels) in enumerate(tqdm(loader)):\n        x = x.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        bboxes = [[] for _ in range(batch_size)]\n        for i in range(3):\n            S = predictions[i].shape[2]\n            anchor = torch.tensor([*anchors[i]]).to(device) * S\n            boxes_scale_i = cells_to_bboxes(\n                predictions[i], anchor, S=S, is_preds=True\n            )\n            for idx, (box) in enumerate(boxes_scale_i):\n                bboxes[idx] += box\n\n        # We just want one bbox for each label, not for each scale\n        true_bboxes = cells_to_bboxes(\n            labels[2], anchor, S=S, is_preds=False\n        )\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes\n            \n\n\n\ndef cells_to_bboxes(predictions, anchors, S, is_preds=True):\n    \"\"\"\n    Scales the predictions coming from the model to\n    be relative to the entire image such that they for example later\n    can be plotted or.\n    INPUT:\n        predictions: tensor of size (N, 3, S, S, num_classes+5)\n        anchors: the anchors used for the predictions\n        S: the number of cells the image is divided in on the width (and height)\n        is_preds: whether the input is predictions or the true bounding boxes\n    OUTPUT:\n        converted_bboxes: the converted boxes of sizes (N, num_anchors, S, S, 1+5) with class index,\n                        object score, bounding box coordinates\n    \"\"\"\n    BATCH_SIZE = predictions.shape[0]\n    num_anchors = len(anchors)\n    box_predictions = predictions[..., 1:5]\n    if is_preds:\n        anchors = anchors.reshape(1, len(anchors), 1, 1, 2)\n        box_predictions[..., 0:2] = torch.sigmoid(box_predictions[..., 0:2])\n        box_predictions[..., 2:] = torch.exp(box_predictions[..., 2:]) * anchors\n        scores = torch.sigmoid(predictions[..., 0:1])\n        best_class = torch.argmax(predictions[..., 5:], dim=-1).unsqueeze(-1)\n    else:\n        scores = predictions[..., 0:1]\n        best_class = predictions[..., 5:6]\n\n    cell_indices = (\n        torch.arange(S).repeat((predictions.shape[0], 3, S, 1)).unsqueeze(-1).to(predictions.device)\n    )\n    x = 1 \/ S * (box_predictions[..., 0:1] + cell_indices)\n    y = 1 \/ S * (box_predictions[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n    w_h = 1 \/ S * box_predictions[..., 2:4]\n    converted_bboxes = torch.cat((best_class, scores, x, y, w_h), dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n    return converted_bboxes.tolist()\n\n\ndef check_class_accuracy(model, loader, threshold):\n    model.eval()\n    tot_class_preds, correct_class = 0, 0\n    tot_noobj, correct_noobj = 0, 0\n    tot_obj, correct_obj = 0, 0\n\n    for idx, (x, y) in enumerate(tqdm(loader)):\n        if idx == 100:\n            break\n        x = x.to(DEVICE)\n        with torch.no_grad():\n            out = model(x)\n\n        for i in range(3):\n            y[i] = y[i].to(DEVICE)\n            obj = y[i][..., 0] == 1\n            noobj = y[i][..., 0] == 0\n\n            correct_class = torch.sum(\n                torch.argmax(out[i][..., 5:][obj], dim=-1) == y[i][..., 5][obj]\n            )\n            tot_class_preds += torch.sum(obj)\n\n            obj_preds = torch.sigmoid(out[i][..., 0]) > threshold\n            correct_obj += torch.sum(obj_preds[obj] == y[i][..., 0][obj])\n            tot_obj += torch.sum(obj)\n            correct_noobj += torch.sum(obj_preds[noobj] == y[i][..., 0][noobj])\n            tot_noobj += torch.sum(noobj)\n\n    print(f\"Class accuracy is: {(correct_class\/(tot_class_preds+1e-16))*100:2f}%\")\n    print(f\"No obj accuracy is: {(correct_noobj\/(tot_noobj+1e-16))*100:2f}%\")\n    print(f\"Obj accuracy is: {(correct_obj\/(tot_obj+1e-16))*100:2f}%\")\n    model.train()\n\n\ndef save_checkpoint(model, optimizer, filename=CHECKPOINT_FILE):\n    print('==> Saving checkpoint')\n    files = glob.glob(filename + '*', recursive=True)\n    count = len(files)\n    filesave = filename + 'checkpoint' + str(count) + '.pth.tar'\n    checkpoint = {\n        'state_dict': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n    }\n    torch.save(checkpoint, filesave)\n\n\ndef load_checkpoint(model, optimizer, lr, filename=LOAD_FILE):\n    print('==> Loading checkpoint')\n#     files = glob.glob(filename + '*', recursive=True)\n#     count = len(files) - 1\n#     fileload = filename + 'pascal_voc\/model\/checkpoint' + str(count) + '.pth.tar'\n#     print(f'Found {count + 1} checkpoint file. Loading {fileload}')\n    checkpoint = torch.load(filename, map_location=DEVICE)\n    model.load_state_dict(checkpoint['state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n\n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n\ndef get_loaders(train_csv_path, test_csv_path):\n\n    train_dataset = YOLODataset(\n        train_csv_path,\n        transform=train_transforms,\n        S=[IMAGE_SIZE\/\/32, IMAGE_SIZE\/\/16, IMAGE_SIZE\/\/8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    test_dataset = YOLODataset(\n        test_csv_path,\n        transform=test_transforms,\n        S=[IMAGE_SIZE\/\/32, IMAGE_SIZE\/\/16, IMAGE_SIZE\/\/8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    train_loader = DataLoader(\n        dataset=train_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        shuffle=True,\n        drop_last=False,\n    )\n    test_loader = DataLoader(\n        dataset=test_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        shuffle=False,\n        drop_last=False,\n    )\n    train_eval_dataset = YOLODataset(\n        train_csv_path,\n        transform=test_transforms,\n        S=[IMAGE_SIZE\/\/32, IMAGE_SIZE\/\/16, IMAGE_SIZE\/\/8],\n        img_dir=IMG_DIR,\n        label_dir=LABEL_DIR,\n        anchors=ANCHORS,\n    )\n    train_eval_loader = DataLoader(\n        dataset=train_eval_dataset,\n        batch_size=BATCH_SIZE,\n        num_workers=NUM_WORKERS,\n        pin_memory=PIN_MEMORY,\n        shuffle=False,\n        drop_last=False,\n    )\n    \n    return train_loader, test_loader, train_eval_loader\n\n\ndef seed_everything(seed=21032001):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","3bd83f99":"import os\nimport pandas as pd\nimport torch\nimport numpy as np\n\nfrom PIL import Image, ImageFile\nfrom torch.utils.data import Dataset, DataLoader\n\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nclass YOLODataset(Dataset):\n    def __init__(\n        self,\n        csv_file,\n        img_dir, label_dir,\n        anchors,\n        image_size=416,\n        S=[13, 26, 52],\n        C=20,\n        transform=None\n    ):\n        self.annotations = pd.read_csv(csv_file)\n        self.img_dir = img_dir\n        self.label_dir = label_dir\n        self.transform = transform\n        self.S = S\n        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])  # for all 3 scales\n        self.num_anchors = self.anchors.shape[0]\n        self.num_anchors_per_scale = self.num_anchors \/\/ 3\n        self.C = C\n        self.ignore_iou_thresh = .5\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1])\n        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=' ', ndmin=2), 4, axis=1).tolist()\n        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n        image = np.array(Image.open(img_path).convert('RGB'))\n\n        if self.transform:\n            augmentations = self.transform(image=image, bboxes=bboxes)\n            image = augmentations['image']\n            bboxes = augmentations['bboxes']\n\n        targets = [torch.zeros((self.num_anchors\/\/3, S, S, 6)) for S in self.S]  # [p_o, x, y, w ,h, c]\n\n        for box in bboxes:\n            iou_anchors = iou_width_height(torch.tensor(box[2:4]), self.anchors)\n            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n            x, y, width, height, class_label = box\n            has_anchor = [False] * 3\n\n            for anchor_idx in anchor_indices:\n                scale_idx = anchor_idx \/\/ self.num_anchors_per_scale  # 0, 1, 2\n                anchor_on_scale = anchor_idx % self.num_anchors_per_scale  # 0, 1, 2\n                S = self.S[scale_idx]\n                i, j = int(S*y), int(S*x)\n                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n\n                if not anchor_taken and not has_anchor[scale_idx]:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n                    x_cell, y_cell = S*x - j, S*y - i  # both are between [0, 1]\n                    width_cell, height_cell = (width*S, height*S)\n                    box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n                    has_anchor[scale_idx] = True\n\n                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1  # ignore this prediction\n\n        return image, tuple(targets)\n    \n    \ndef test():\n    anchors = ANCHORS\n    transform = test_transforms\n\n    dataset = YOLODataset(\n        \"..\/input\/pascal-voc-yolo-works-with-albumentations\/PASCAL_VOC\/train.csv\",\n        \"..\/input\/pascal-voc-yolo-works-with-albumentations\/PASCAL_VOC\/images\",\n        \"..\/input\/pascal-voc-yolo-works-with-albumentations\/PASCAL_VOC\/labels\",\n        S=[13, 26, 52],\n        anchors=anchors,\n        transform=transform,\n    )\n    S = [13, 26, 52]\n    scaled_anchors = torch.tensor(anchors) \/ (1 \/ torch.tensor(S).unsqueeze(1).unsqueeze(2).repeat(1, 3, 2))\n    loader = DataLoader(dataset=dataset, batch_size=1, shuffle=True)\n    for x, y in loader:\n        boxes = []\n\n        for i in range(y[0].shape[1]):\n            anchor = scaled_anchors[i]\n            print(anchor.shape)\n            print(y[i].shape)\n            boxes += cells_to_bboxes(\n                y[i], anchors=anchor, S=y[i].shape[2], is_preds=False\n            )[0]\n\n        boxes = non_max_suppression(boxes, iou_threshold=1, threshold=.7, box_format='midpoint')\n        print(boxes)\n        plot_image(x[0].permute(1, 2, 0).to('cpu'), boxes)\n        break","1f9ecdf3":"test()","3bc45b71":"import torch\nimport torch.nn as nn\n\n# tuple: (out_channels, kernel_size, stride)\n# Dict : {'B': 'residual block', 'S': 'scale prediction', 'U': 'up-sampling'}\nconfig = [\n    (32, 3, 1),\n    (64, 3, 2),\n    ['B', 1],\n    (128, 3, 2),\n    ['B', 2],\n    (256, 3, 2),\n    ['B', 8],\n    (512, 3, 2),\n    ['B', 8],\n    (1024, 3, 2),\n    ['B', 4],  # To this point is Darknet-53\n    (512, 1, 1),\n    (1024, 3, 1),\n    'S',\n    (256, 1, 1),\n    'U',\n    (256, 1, 1),\n    (512, 3, 1),\n    'S',\n    (128, 1, 1),\n    'U',\n    (128, 1, 1),\n    (256, 3, 1),\n    'S'\n]\n\n\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.leaky = nn.LeakyReLU(.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        if self.use_bn_act:\n            return self.leaky(self.bn(self.conv(x)))\n        else:\n            return self.conv(x)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for _ in range(num_repeats):\n            self.layers += [\n                nn.Sequential(\n                    CNNBlock(channels, channels\/\/2, kernel_size=1),\n                    CNNBlock(channels\/\/2, channels, kernel_size=3, padding=1)\n                )\n            ]\n\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            x = layer(x) + x if self.use_residual else layer(x)\n\n        return x\n\n\nclass ScalePrediciton(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.pred = nn.Sequential(\n            CNNBlock(in_channels, 2*in_channels, kernel_size=3, padding=1),\n            CNNBlock(2*in_channels, (num_classes + 5)*3, bn_act=False, kernel_size=1)\n        )\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        # N x 3 x 13 x 13 x 5+num_classes\n        return self.pred(x).reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3]).permute(0, 1, 3, 4, 2)\n\n\nclass YOLOv3(nn.Module):\n    def __init__(self, in_channels=3, num_classes=20):\n        super().__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.layers = self._create_conv_layers()\n\n    def forward(self, x):\n        outputs = []\n        route_connections = []\n\n        for layer in self.layers:\n            if isinstance(layer, ScalePrediciton):\n                outputs.append(layer(x))\n                continue\n\n            x = layer(x)\n            \n            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n                route_connections.append(x)\n\n            elif isinstance(layer, nn.Upsample):\n                x = torch.cat([x, route_connections[-1]], dim=1)\n                route_connections.pop()\n\n        return outputs\n\n    def _create_conv_layers(self):\n        layers = nn.ModuleList()\n        in_channels = self.in_channels\n\n        for module in config:\n            if isinstance(module, tuple):\n                out_channels, kernel_size, stride = module\n                layers.append(\n                    CNNBlock(\n                        in_channels, \n                        out_channels, \n                        kernel_size=kernel_size, \n                        stride=stride, \n                        padding = 1 if kernel_size == 3 else 0\n                    )\n                )\n                in_channels = out_channels\n\n            elif isinstance(module, list):\n                num_repeats = module[1]\n                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats))\n\n            elif isinstance(module, str):\n                if module == 'S':\n                    layers += [\n                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n                        CNNBlock(in_channels, in_channels\/\/2, kernel_size=1),\n                        ScalePrediciton(in_channels\/\/2, num_classes=self.num_classes)\n                    ]\n                    in_channels = in_channels\/\/2\n                \n                elif module == 'U':\n                    layers.append(nn.Upsample(scale_factor=2))\n                    in_channels = in_channels * 3\n                    \n        return layers","695d6e9b":"import torch\nimport torch.nn as nn\n\n\nclass YoloLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.bce = nn.BCEWithLogitsLoss()\n        self.entropy = nn.CrossEntropyLoss()\n        self.sigmoid = nn.Sigmoid()\n\n        # Constants\n        self.lambda_class = 1\n        self.lambda_noobj = 10\n        self.lambda_obj = 1\n        self.lambda_box = 10\n\n    def forward(self, predictions, target, anchors):\n        obj = target[..., 0] == 1\n        noobj = target[..., 0] == 0\n\n        # No obj loss\n        no_object_loss = self.bce((predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]))\n\n        # Object loss\n        anchors = anchors.reshape(1, 3, 1, 1, 2)  # p_w * exp(t_w)\n        box_preds = torch.cat([self.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n        ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n        object_loss = self.bce((predictions[..., 0:1][obj]), (ious * target[..., 0:1][obj]))\n\n        # Box coordinate Loss\n        predictions[..., 1:3] = self.sigmoid(predictions[..., 1:3])  # x, y to be between [0, 1]\n        target[..., 3:5] = torch.log((1e-16 + target[..., 3:5] \/ anchors))\n        box_loss = self.mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n\n        # Class loss\n        class_loss = self.entropy((predictions[..., 5:][obj]), (target[..., 5][obj].long()))\n\n        return (\n            self.lambda_noobj * no_object_loss\n            + self.lambda_obj * object_loss\n            + self.lambda_box * box_loss\n            + self.lambda_class * class_loss\n        )","7dec736d":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom tqdm import tqdm\n\ntorch.backends.cudnn.benchmark = True\n\n\ndef train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors):\n    loop = tqdm(train_loader, leave=True)\n    losses=[]\n\n    for batch_idx, (x, y) in enumerate(loop):\n        x = x.to(DEVICE)\n        y0, y1, y2 = (\n            y[0].to(DEVICE),\n            y[1].to(DEVICE),\n            y[2].to(DEVICE)\n        )\n\n        with torch.cuda.amp.autocast():\n            out = model(x)\n            loss = (\n                loss_fn(out[0], y0, scaled_anchors[0])\n                + loss_fn(out[1], y1, scaled_anchors[1])\n                + loss_fn(out[2], y2, scaled_anchors[2])\n            )\n\n        losses.append(loss.item())\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update progress bar\n        mean_loss = sum(losses) \/ len(losses)\n        loop.set_postfix(loss=mean_loss)\n\n\ndef main():\n    model = YOLOv3(num_classes=NUM_CLASSES).to(DEVICE)\n    optimizer = optim.Adam(\n        model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n    )\n\n    loss_fn = YoloLoss()\n    scaler = torch.cuda.amp.GradScaler()\n\n    train_loader, test_loader, train_eval_dataset = get_loaders(\n        train_csv_path=DATASET+'\/train.csv',\n        test_csv_path=DATASET+'\/test.csv'\n    )\n\n    if LOAD_MODEL:\n        load_checkpoint(\n            model, optimizer, LEARNING_RATE, LOAD_FILE,\n        )\n\n    scaled_anchors = (\n        torch.tensor(ANCHORS)\n        * torch.tensor(config_S).unsqueeze(1).unsqueeze(2).repeat(1, 3, 2)\n    ).to(DEVICE)\n\n    for epoch in range(NUM_EPOCHS):\n        train_fn(train_loader, model, optimizer, loss_fn, scaler, scaled_anchors)\n\n        if (epoch + 1) % 10 == 0 and epoch > 0:\n            if SAVE_MODEL:\n                save_checkpoint(model, optimizer)\n                \n            print('On Test Loader')\n            check_class_accuracy(model, test_loader, CONF_THRESHOLD) \n\n            pred_boxes, true_boxes = get_evaluation_bboxes(\n                test_loader,\n                model,\n                iou_threshold=NMS_IOU_THRESH,\n                anchors=ANCHORS,\n                threshold=CONF_THRESHOLD,\n            )\n            mapval = mean_average_precision(\n                pred_boxes,\n                true_boxes,\n                iou_threshold=MAP_IOU_THRESH,\n                box_format='midpoint',\n                num_classes=NUM_CLASSES,\n            )\n            print(f'MAP: {mapval.item()}')\n            \nmain()","aa8bee5a":"# Config\n","457263a5":"# Dataset\n","f3bcdb30":"# Utils","d202e8bc":"# Model\n","f6a81e1e":"# Loss\n","b116182b":"# Train"}}