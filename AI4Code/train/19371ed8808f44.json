{"cell_type":{"32d9bd91":"code","80348c60":"code","288b4c86":"code","5239ef3a":"code","d0f6df6d":"code","d6b36372":"code","f763a45f":"code","273687a8":"code","1c79afcb":"code","305704bd":"code","777b6288":"code","923d207b":"code","71debe7f":"code","45326e33":"code","30ed87e4":"code","6638bd91":"code","5bf59d22":"code","2fe4bc00":"code","143514b5":"code","22e5aaa9":"code","82d3b528":"code","c0bf23ca":"code","0a0ac666":"code","be49a8fc":"code","c4e3b597":"code","03c93d41":"code","54816314":"code","1806a88d":"code","d070a864":"code","2df0fe13":"markdown","cef70738":"markdown","a78607a3":"markdown","87fbefe2":"markdown","dd28e977":"markdown","3d893b26":"markdown","659a5dd9":"markdown","02bd2f8c":"markdown","24e632ee":"markdown","18ba2196":"markdown","e7e514f7":"markdown","89c4a8db":"markdown","79880836":"markdown","1393f5eb":"markdown","adc36514":"markdown","86e60188":"markdown","059b4786":"markdown","fd5d36b2":"markdown","9719ebeb":"markdown","9741196a":"markdown","67d44004":"markdown","d477f78d":"markdown","c6e07174":"markdown","ab72c216":"markdown","7e106b66":"markdown","0a42b7d3":"markdown","7791c05a":"markdown"},"source":{"32d9bd91":"import os, re, io\nimport pandas as pd\nimport numpy as np\nimport requests\n\n## stopwords\nfrom gensim.parsing.preprocessing import remove_stopwords\n## lemma functionality provide by NLTK\nfrom nltk.stem import WordNetLemmatizer\n## make sure you downloaded model for lemmatization\n#nltk.download('wordnet')\nfrom nltk import word_tokenize\n## make sure you downloaded model for tokenization\n#nltk.download('punkt')\nimport spacy\nnlp = spacy.load('en')\n\n## TF_IDF for BOW\nfrom gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary\n## cosine similarity\nfrom sklearn.metrics.pairwise import cosine_similarity","80348c60":"## The data is taken from https:\/\/www.un.org\/sites\/un2.un.org\/files\/new_dhmosh_covid-19_faq.pdf\n## it has FAQ based question and answering for COVID-19\n\ndef download_pdf_url(dataset_url, file_name):\n    response = requests.get(dataset_url)\n    pdf_content_output = None\n    with io.BytesIO(response.content) as open_pdf_file:\n        with open(file_name,'w') as obj:\n            obj.write(str(open_pdf_file))","288b4c86":"dataset_url = 'https:\/\/www.un.org\/sites\/un2.un.org\/files\/new_dhmosh_covid-19_faq.pdf'\n## download pdf from URL and save the pdf file\ndownload_pdf_url(dataset_url, 'new_dhmosh_covid-19_faq.pdf')","5239ef3a":"## QA will be stored as .csv file\ndef extract_QA_from_text_file(INPUT_DIR, text_file_name):\n    output_file_name = 'covid_19faq.csv'\n    with open(os.path.join(INPUT_DIR, text_file_name), 'r', encoding='latin') as obj:\n        text = obj.read()\n\n    text = text.strip()\n    ## extract the question by following pattern\n    pattern = '\\n+\\s*\\d+[.](.*?)\\?'\n    question_pattern = re.compile(pattern,re.MULTILINE|re.IGNORECASE|re.DOTALL)  \n    matched_QA_positions = [(m.start(0),m.end(0)) for m in question_pattern.finditer(text)]\n    print(f\"Available no of question is {len(matched_QA_positions)}\")\n    ## store question and answer pair\n    questions = {}\n    ## iterate every matched QA \n    for index in range(len(matched_QA_positions)):\n        ## get the start and end position\n        faq_start_pos = matched_QA_positions[index][0]\n        faq_end_pos = matched_QA_positions[index][1]\n        \n        if index == len(matched_QA_positions) - 1:\n            next_faq_start_pos = -1\n        else:\n            next_faq_start_pos = matched_QA_positions[index+1][0]\n\n        ## get the question from start and end position from original text      \n        question = text[faq_start_pos:faq_end_pos]\n        if next_faq_start_pos == -1:\n            answer = text[faq_end_pos:]\n        else:\n            answer = text[faq_end_pos:next_faq_start_pos]\n        ## replace multiple new lines to space in questions and answers\n        question = re.sub(\"\\n+\",\" \",question.strip())\n        answer = re.sub(\"\\n+\",\" \",answer.strip())\n        questions[question] = answer\n        \n    ## create dataframe from key-value pair\n    faq_df = pd.DataFrame.from_dict(questions, orient='index', columns=[\"answers\"])\n    faq_df[\"questions\"] = faq_df.index\n    faq_df.reset_index(inplace=True)  \n    faq_df[[\"questions\", \"answers\"]].to_csv(os.path.join(INPUT_DIR, output_file_name),index = False)\n    print(f\"COVID QA file {output_file_name} created\")","d0f6df6d":"## Converted PDF to .txt file using pdftools in R\n## create a question-answer pair in csv\n\n#extract_QA_from_text_file(INPUT_DIR, 'new_dhmosh_covid-19_faq.txt')\n\n","d6b36372":"QA_df = pd.read_excel(os.path.join(\"..\/input\/covid19-frequent-asked-questions\", \"COVID19_FAQ.xlsx\"))\nQA_df.head(10)","f763a45f":"## Data Preprocessing\nclass TextPreprocessor():\n    def __init__(self, data_df, column_name=None):\n        self.data_df = data_df  \n        if not column_name and type(colum_name) == str:\n            raise Exception(\"column name is mandatory. Make sure type is string format\")\n        self.column = column_name\n        self.convert_lowercase()    \n        self.applied_stopword = False\n        self.processed_column_name = f\"processed_{self.column}\"\n        \n    def convert_lowercase(self):\n        ## fill empty values into empty\n        self.data_df.fillna('',inplace=True)\n        ## reduce all the columns to lowercase\n        self.data_df = self.data_df.apply(lambda column: column.astype(str).str.lower(), axis=0)    \n\n    def remove_question_no(self):\n        ## remove question no        \n        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'^\\d+[.]',' ', row))    \n        \n    def remove_symbols(self):\n        ## remove unwanted character          \n        self.data_df[self.column] = self.data_df[self.column].apply(lambda row: re.sub(r'[^A-Za-z0-9\\s]', ' ', row))    \n\n    def remove_stopwords(self):\n        ## remove stopwords and create a new column \n        for idx, question in enumerate(self.data_df[self.column]):      \n            self.data_df.loc[idx, self.processed_column_name] = remove_stopwords(question)        \n\n    def apply_lemmatization(self, perform_stopword):\n        ## get the root words to reduce inflection of words \n        lemmatizer = WordNetLemmatizer()    \n        ## get the column name to perform lemma operation whether stopwords removed text or not\n        if perform_stopword:\n            column_name = self.processed_column_name\n        else:\n            column_name = self.column\n        ## iterate every question, perform tokenize and lemma\n        for idx, question in enumerate(self.data_df[column_name]):\n\n            lemmatized_sentence = []\n            ## use spacy for lemmatization\n            doc = nlp(question.strip())\n            for word in doc:       \n                lemmatized_sentence.append(word.lemma_)      \n                ## update to the same column\n                self.data_df.loc[idx, self.processed_column_name] = \" \".join(lemmatized_sentence)\n\n    def process(self, perform_stopword = True):\n        self.remove_question_no()\n        self.remove_symbols()\n        if perform_stopword:\n            self.remove_stopwords()\n        self.apply_lemmatization(perform_stopword)    \n        return self.data_df","273687a8":"## pre-process training question data\ntext_preprocessor = TextPreprocessor(QA_df.copy(), column_name=\"questions\")\nprocessed_QA_df = text_preprocessor.process(perform_stopword=True)\nprocessed_QA_df.head(10)","1c79afcb":"class TF_IDF():\n    def __init__(self):\n        self.dictionary = None    \n        self.model = None\n        self.bow_corpus = None\n\n    def create_tf_idf_model(self, data_df, column_name):\n        ## create sentence token list\n        sentence_token_list = [sentence.split(\" \") for sentence in data_df[column_name]]\n\n        ## dataset vocabulary\n        self.dictionary = Dictionary(sentence_token_list) \n\n        ## bow representation of dataset\n        self.bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]\n\n        ## compute TF-IDF score for corpus\n        self.model = TfidfModel(self.bow_corpus)\n\n        ## representation of question and respective TF-IDF value\n        print(f\"First 10 question representation of TF-IDF vector\")\n        for index, sentence in enumerate(data_df[column_name]):\n            if index <= 10:\n                print(f\"{sentence} {self.model[self.bow_corpus[index]]}\")\n            else:\n                break\n\n    def get_vector_for_test_set(self, test_df, column_name):\n        ## store tf-idf vector\n        testset_tf_idf_vector = []\n        sentence_token_list = [sentence.split(\" \") for sentence in test_df[column_name]]\n        test_bow_corpus = [self.dictionary.doc2bow(sentence_tokens) for sentence_tokens in sentence_token_list]   \n        for test_sentence in test_bow_corpus:\n            testset_tf_idf_vector.append(self.model[test_sentence])      \n\n        return testset_tf_idf_vector\n\n    def get_training_QA_vectors(self):\n        QA_vectors = []\n        for sentence_vector in self.bow_corpus:\n            QA_vectors.append(self.model[sentence_vector])      \n        return QA_vectors\n\n    def get_train_vocabulary(self):\n        vocab = []\n        for index in self.dictionary:\n            vocab.append(self.dictionary[index])\n        return vocab","305704bd":"class Embeddings():\n    def __init__(self, model_path):\n        self.model_path = model_path\n        self.model = None\n        self.__load_model__()\n        \n    def __load_model__(self):\n        #word_vectors = api.load(\"glove-wiki-gigaword-100\")  \n        model_name = 'glove-twitter-25' #'word2vec-google-news-50' #'glove-twitter-25'  \n        if not os.path.exists(self.model_path+ model_name):\n            print(\"Downloading model\")\n            self.model = api.load(model_name)\n            self.model.save(self.model_path+ model_name)\n        else:\n            print(\"Loading model from Drive\")\n            self.model = KeyedVectors.load(self.model_path+ model_name)\n        \n    def get_oov_from_model(self, document_vocabulary):\n        ## the below words are not available in our pre-trained model model_name\n        print(\"The below words are not found in our pre-trained model\")\n        words = []\n        for word in set(document_vocabulary):  \n            if word not in self.model:\n                words.append(word)\n        print(words)  \n\n    def get_sentence_embeddings(self, data_df, column_name):\n        sentence_embeddings_list = []\n        for sentence in data_df[column_name]:      \n            sentence_embeddings = np.repeat(0, self.model.vector_size)\n            try:\n                tokens = sentence.split(\" \")\n                ## get the word embedding\n                for word in tokens:\n                    if word in self.model:\n                        word_embedding = self.model[word]\n                    else:\n                        word_embedding = np.repeat(0, self.model.vector_size)          \n                    sentence_embeddings = sentence_embeddings + word_embedding\n                ## take the average for sentence embeddings\n                #sentence_embeddings = sentence_embeddings \/ len(tokens)\n                sentence_embeddings_list.append(sentence_embeddings.reshape(1, -1))\n            except Exception as e:\n                print(e)\n            \n        return sentence_embeddings_list","777b6288":"!pip install bert-embedding","923d207b":"from bert_embedding import BertEmbedding","71debe7f":"## get bert embeddings\ndef get_bert_embeddings(sentences):\n    bert_embedding = BertEmbedding()\n    return bert_embedding(sentences)","45326e33":"tf_idf = TF_IDF()\ntf_idf.create_tf_idf_model(processed_QA_df, \"processed_questions\")\n## get the tf-idf reprentation \nquestion_QA_vectors = tf_idf.get_training_QA_vectors()","30ed87e4":"## Get the document vocabulary list from TF-IDF\ndocument_vocabulary = tf_idf.get_train_vocabulary()","6638bd91":"## Now, Let's try building embedding based\nimport gensim.downloader as api\nfrom gensim.models import KeyedVectors","5bf59d22":"## create Embedding object\nembedding = Embeddings(\"\")\n## look for out of vocabulary COVID QA dataset - pretrained model\nembedding.get_oov_from_model(document_vocabulary)\n## get the sentence embedding for COVID QA dataset\nquestion_QA_embeddings = embedding.get_sentence_embeddings(processed_QA_df, \"processed_questions\")","2fe4bc00":"question_QA_bert_embeddings_list = get_bert_embeddings(processed_QA_df[\"questions\"].to_list())","143514b5":"%matplotlib inline\nimport matplotlib.pyplot as plt\n \nfrom sklearn.manifold import TSNE","22e5aaa9":"def display_closestwords_tsnescatterplot(model, word):\n    \n    arr = np.empty((0,25), dtype='f')\n    word_labels = [word]\n\n    # get close words\n    close_words = model.similar_by_word(word)\n    \n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    plt.figure(figsize=(20,10))\n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n    plt.show()","82d3b528":"display_closestwords_tsnescatterplot(embedding.model, 'prevent')","c0bf23ca":"## helps to retrieve similar question based of input vectors\/embeddings for test query\ndef retrieveSimilarFAQ(train_question_vectors, test_question_vectors, train_QA_df, train_column_name, test_QA_df, test_column_name):\n    similar_question_index = []\n    for test_index, test_vector in enumerate(test_question_vectors):\n        sim, sim_Q_index = -1, -1\n        for train_index, train_vector in enumerate(train_question_vectors):\n            sim_score = cosine_similarity(train_vector, test_vector)[0][0]\n            \n            if sim < sim_score:\n                sim = sim_score\n                sim_Q_index = train_index\n\n        print(\"######\")\n        print(f\"Query Question: {test_QA_df[test_column_name].iloc[test_index]}\")    \n        print(f\"Retrieved Question: {train_QA_df[train_column_name].iloc[sim_Q_index]}\")\n        print(\"######\")","0a0ac666":"test_query_string = [\"how does covid-19 spread?\", \n                     \"What are the symptoms of COVID-19?\",\n                \"Should I wear a mask to protect myself from covid-19\",              \n                \"Is there a vaccine for COVID-19\",\n                \"can the virus transmit through air?\",\n                \"can the virus spread through air?\"]\n\ntest_QA_df = pd.DataFrame(test_query_string, columns=[\"test_questions\"])              \n## pre-process testing QA data\ntext_preprocessor = TextPreprocessor(test_QA_df, column_name=\"test_questions\")\nquery_QA_df = text_preprocessor.process(perform_stopword=True)","be49a8fc":"## TF-IDF vector represetation\nquery_QA_vectors = tf_idf.get_vector_for_test_set(query_QA_df, \"processed_test_questions\")\nquery_QA_df.head()\n      ","c4e3b597":"retrieveSimilarFAQ(question_QA_vectors, query_QA_vectors, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")","03c93d41":"## get the sentence embedding for COVID QA query\nquery_QA_embeddings = embedding.get_sentence_embeddings(query_QA_df, \"processed_test_questions\")\n\nretrieveSimilarFAQ(question_QA_embeddings, query_QA_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")","54816314":"query_QA_bert_embeddings_list = get_bert_embeddings(test_QA_df[\"test_questions\"].to_list())","1806a88d":"## store QA bert embeddings in list\nquestion_QA_bert_embeddings = []\nfor embeddings in question_QA_bert_embeddings_list:\n    question_QA_bert_embeddings.append(embeddings[1])\n\n## store query string bert embeddings in list\nquery_QA_bert_embeddings = []\nfor embeddings in query_QA_bert_embeddings_list:\n    query_QA_bert_embeddings.append(embeddings[1])","d070a864":"retrieveSimilarFAQ(question_QA_bert_embeddings, query_QA_bert_embeddings, processed_QA_df, \"questions\", query_QA_df, \"test_questions\")","2df0fe13":"<strong>Version 1.0:<\/strong> Updated the dataset version","cef70738":"#### Embeddings (Glove)","a78607a3":"<a id=\"top\"> <\/a>\n## Table of Content\n\n1. [Data collection \u2014 COVID-19 FAQ data](#1)\n2. [Data preparation](#2)\n3. [Data pre-processing](#3)\n4. [Techniques for Question representations](#4)\n\n   4.1. [TF_IDF Representation](#4.1)\n   \n   4.2. [Embedding Representation](#4.2)\n   \n   4.3. [BERT Embeddings](#4.3)\n   \n   4.4. [Visualizing Embedding](#4.4)\n   \n5. [Evaluate with test query](#5)","87fbefe2":"Prepare the data in the desired format to solve the problem is the next task. As, we need to extract every question and respective answers from an unstructured document and store it structured file. The best and simple way you could extract information from a text file is by doing parsing. Parsing can help to retrieve specific information on the following assumptions.\n\n#### Assumptions:\n\n- It has to follow some patterns in raw text.\n- Should extract the same structure from the PDF.\n- It should not contain any noise in the text.\n\nIn the pipeline, It involves the below tasks.\n\n1. It converts the PDF file to a text file using the R module. This module maintains the same structure of PDF in the text file. I found this module works very well as compared to other modules in python. The content of the structure will help us parse the QA very well. The PDF to text conversion process performed externally. (This process not included in the code snippet)\n2.  Pattern matching is very efficient for retrieving structured information from raw text. We will parse questions and answers using the python module. (as shown in the below code snippet)\n3. Store COVID-19 FAQ question and answer as structured (.csv) file.\n\nI have used pdftools - R module for converting PDF to TXT file which is not covered in the below utility code.\n","dd28e977":"Let's visualize Glove (twitter-25) embeddings.","3d893b26":"### Test with Embeddings (glove-twitter-25)","659a5dd9":"We achieved the best results through embedding and BERT embedding representation. Drawbacks of TF-IDF cannot semantically represent the question.\n\nThis notebook illustrated the basic version question similarity using *cosine distance* of different feature representation. The exploration of question similarity is published as article in [*Towards Data Science*](https:\/\/towardsdatascience.com\/covid-19-faq-bot-everything-you-need-to-know-about-qna-similarity-35a730f63fa1) publication as well.","02bd2f8c":"Utility for evaluating user test query.\n\nWe represented COVID-19 FAQ questions to multiple representations. Let\u2019s evaluate each representation for the user new query. \n> How are we finding similarities of FAQ questions with new user queries?\n\nOne of the best techniques to find a similarity score is **Cosine Similarity**. We will use cosine similarity for comparing each representation now. How to calculate cosine similarity as below code snippet,","24e632ee":"## <font color=\"#007bff\"><b>3. Preprocessing Techniques<\/b><\/font><br><a id=\"3\"><\/a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC<\/a>\n\nNext step, We will not use the data as it is. Preprocessing is another very important step to fine-tune the dataset.\n\n1. Remove unwanted characters\n2. Remove Question number\n3. Remove stopwords\n4. Lemmatization - to reduce inflection of words and minimize the word ambiguity.\n\nWhy I chosen lemmatization over stemming? Lemmatization is powerful operation as it takes into consideration of morphological analysis of the word. \n\n**Example:** bicycles or bicycles are converted to bicyles. But, stemming algorithm works by predefined rules to remove prefix or suffix of the word.\n","18ba2196":"### <font color=\"#007bff\"><b>4.2 Word Embedding<\/b><\/font><br><a id=\"4.2\"><\/a>\n\n*GloVe* is an unsupervised learning algorithm for obtaining vector representations for words. It trained on the global word-word co-occurrence matrix. I downloaded a pre-trained word vector from Glove for our analysis. The code snippets for generating word embedding representation as below code snippet,","e7e514f7":"## <font color=\"#007bff\"><b>5. Evaluate with test query<\/b><\/font><br><a id=\"5\"><\/a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC<\/a>","89c4a8db":"<center><font size=\"6\" color=\"blue\">COVID-19 FAQ BOT<\/font><br><font size=\"-1\">Automatic question answer retrival system<\/font><\/center><br>\n\nCOVID-19 has become a viral topic in the world in recent months. Today, as COVID-19 is strongly showing, that prevention is better than cure. Most of them could realize this phrase very well with the current situation. Yes, People are trying to prevent themselves from others to stop spreading the virus. Indeed, Social distancing and wearing N-95 masks widely followed by people.\n\n![QA](https:\/\/miro.medium.com\/max\/650\/1*IcudFkhNprvU_SOAQAxgxA.png)\n\nPeople have their hypotheses and perception of every topic. Many people have common questions that represented as Frequently asked questions. In today\u2019s scenario, COVID-19 cases increases quickly. Everyone seeking to know about the exact truth of the COVID-19 virus. Also, most of them have common questions like below,\n\n- How does the virus spread?\n- What are the symptoms?\n- Is there a vaccine?\n- How long does the virus survive on surfaces?\n\n## Objective:\nTo build question similarity for COVID-19 Frequently asked questions. Need to identify new user question is how similar to existing questions with cosine distance.","79880836":"## <font color=\"#007bff\"><b>2. Data preparation<\/b><\/font><br><a id=\"2\"><\/a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC<\/a>","1393f5eb":"#### TF-IDF Computation","adc36514":"\n### \ud83d\ude4f Thanks for reading the kernal. Please upvote if you liked it and leave your comments if any.\n","86e60188":"#### BERT Embeddings","059b4786":"## <font color=\"#007bff\"><b>1. Data Collection Process<\/b><\/font><br><a id=\"1\"><\/a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC<\/a>\n\nIt is very important process in Data Science. We should collect the data from various places and combine them for our analysis. The dataset has been taken it from the below site.\n- https:\/\/www.un.org\/sites\/un2.un.org\/files\/new_dhmosh_covid-19_faq.pdf\n\nIn this notebook, Will explore Question Similarity for COVID-19\n","fd5d36b2":"### <font color=\"#007bff\"><b>4.4 Visualizing Embedding<\/b><\/font><br><a id=\"4.4\"><\/a>","9719ebeb":"Let's see how close other words related to **prevent** in glove-twitter-25 embeddings.\n\nObservations:\n- It is interesting to see, how close the words like avoid, eliminate, preventing","9741196a":"### Test with TF-IDF computation","67d44004":"### Use above techniques for our analysis","d477f78d":"## <font color=\"#007bff\"><b>4. Techniques for Question representations<\/b><\/font><br><a id=\"4\"><\/a>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to TOC\">Go to TOC<\/a>\n\nIn this section will be discussing on multiple ways of representing FAQ questions.\n\n1. TF-IDF\n2. Word Embedding\n3. BERT Embedding\n\n### <font color=\"#007bff\"><b>4.1 TF_IDF Representation<\/b><\/font><br><a id=\"4.1\"><\/a>\n\nThe first approach we will use for semantic similarity is leveraging Bag of Words (BOW). TF-IDF transforms the text into meaningful numbers. The technique is a widely used feature extraction in NLP applications. TF (Term Frequency) measures the no of times that words appear in a document. IDF (Inverse Document Frequency) measures low value for words that has high frequency across all the documents.","c6e07174":"### Test with BERT Embeddings","ab72c216":"Let's create sample few question for testing purpose.","7e106b66":"### <font color=\"#007bff\"><b>4.3 BERT Embedding<\/b><\/font><br><a id=\"4.3\"><\/a>\n\n*BERT* is a transformer-based model attempts to use the context of words to get embedding. BERT broke several records in NLP tasks. \n\nThe following search query is an excellent way to understand BERT. \n> \u201c2019 Brazil traveler to the USA need a visa\u201d. \n\nWe observe that the relationship of the word \u201cto\u201d to other words in the sentence are important to decode the meaning semantically. Returning information about USA citizens traveling to Brazil is not relevant since we are talking about Brazil citizens traveling to the USA. BERT can handle this well.","0a42b7d3":"#### Download dataset from source\n\nThe COVID-19 FAQ data available in online (https:\/\/www.un.org\/sites\/un2.un.org\/files\/new_dhmosh_covid-19_faq.pdf). We will be taking this from online for our FAQ question answering mechanism.\n","7791c05a":"The COVID-19 FAQ data after prepared looks like below,"}}