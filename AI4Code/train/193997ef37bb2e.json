{"cell_type":{"1820a14e":"code","ec2fe36c":"code","31174c04":"code","c85217c0":"code","20afee29":"code","dc804fdc":"code","91a3dfea":"code","121feca9":"code","d04eb520":"code","83cc4036":"code","5956c1fb":"code","b09b3d0f":"code","70ee2cdd":"code","dbb019a2":"code","78aa7053":"code","7bcedbe7":"code","c716f202":"code","8b402b30":"code","fbbe943c":"code","832ea609":"markdown","aecf8da3":"markdown","0ab186dc":"markdown","4bc2b5c1":"markdown","3c4d0a5f":"markdown","08568b09":"markdown","df869f2a":"markdown","ec5276da":"markdown","d168a466":"markdown","d948599e":"markdown","7e67acf9":"markdown","b352ebff":"markdown","c1833571":"markdown"},"source":{"1820a14e":"import numpy as np \nimport pandas as pd\npd.set_option('display.max_rows', 800)\npd.set_option('display.max_columns', 500)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV \n\nimport warnings\nwarnings.filterwarnings('ignore')","ec2fe36c":"df = pd.read_csv(\"..\/input\/melbourne-housing-market\/MELBOURNE_HOUSE_PRICES_LESS.csv\")\ndf.head()","31174c04":"df.info()","c85217c0":"df.describe()  ","20afee29":"df.head()","dc804fdc":"num_col = df.select_dtypes(include=np.number).columns\nprint(\"Numerical columns: \\n\",num_col)\n\ncat_col = df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical columns: \\n\",cat_col)","91a3dfea":"df.drop([\"Address\",\"Date\",\"Postcode\"], axis=1,inplace=True)\ndf.dropna(inplace=True)\n\n# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \n# Encode labels in column . \n\ndf['Suburb']= label_encoder.fit_transform(df['Suburb'])\ndf['Type']= label_encoder.fit_transform(df['Type'])\ndf['Method']= label_encoder.fit_transform(df['Method'])\ndf['SellerG']= label_encoder.fit_transform(df['SellerG'])\ndf['Regionname']= label_encoder.fit_transform(df['Regionname'])\ndf['CouncilArea']= label_encoder.fit_transform(df['CouncilArea'])\n  \ndf.head()\n","121feca9":"# Let's check the distribution of y variable\nplt.figure(figsize=(10,10), dpi= 80)\nsns.boxplot(df['Price'])\nplt.title('Price')\nplt.show()","d04eb520":"plt.figure(figsize=(8,8))\nplt.title('Price Distribution Plot')\nsns.distplot(df['Price'])","83cc4036":"num_col = df.select_dtypes(include=np.number).columns\nprint(\"Numerical columns: \\n\",num_col)\n\ncat_col = df.select_dtypes(exclude=np.number).columns\nprint(\"Categorical columns: \\n\",cat_col)","5956c1fb":"# Let's check the multicollinearity of features by checking the correlation matric\n\nplt.figure(figsize=(15,15))\np=sns.heatmap(df[num_col].corr(), annot=True,cmap='RdYlGn',center=0)","b09b3d0f":"# Train test split\nX = df.drop(['Price'], axis = 1)\ny = df['Price']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=500)","70ee2cdd":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","dbb019a2":"gbr = GradientBoostingRegressor(learning_rate = 0.05, random_state = 100)\ngbr.fit(X_train,y_train)\ny_pred = gbr.predict(X_test)\n\nprint(\"r2 score : \",r2_score(y_test,y_pred))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,y_pred))","78aa7053":"gbr = GradientBoostingRegressor(learning_rate = 0.1, random_state = 100)\ngbr.fit(X_train,y_train)\ny_pred = gbr.predict(X_test)\n\nprint(\"r2 score : \",r2_score(y_test,y_pred))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,y_pred))","7bcedbe7":"gbr = GradientBoostingRegressor(random_state = 100)\n\n# defining parameter range \nparam_grid={'n_estimators':[100,200], \n            'learning_rate': [0.15,0.2,0.3,0.5],\n            'max_depth':[2,3,5], \n            'min_samples_leaf':[1,3,5]}   \n  \ngrid = GridSearchCV(gbr, param_grid, refit = True, verbose = 3, n_jobs = -1) \n  \n# fitting the model for grid search \ngrid.fit(X_train, y_train)","c716f202":"# Best parameter after hyper parameter tuning \nprint(grid.best_params_) \n  \n# Moel Parameters \nprint(grid.best_estimator_)","8b402b30":"# Prediction using best parameters\ngrid_predictions = grid.predict(X_test) \n  \nprint(\"r2 score : \",r2_score(y_test,grid_predictions))\nprint(\"MAPE     : \",mean_absolute_percentage_error(y_test,grid_predictions))","fbbe943c":"#You can still decrease the mape by trying out different values for estimators ,learning depth and other factors,\n#but be mindful that trying out of more values means it will lead to pressure on your RAM and the process will take a lot of time\n#Maybe hours as well and your computer might get hanged in between, so do it only if you have powerful gpu and good ram.","832ea609":"**Summary :**\n\nMAPE has improved as compared to the earlier model, let's try to tune the parameter using gridsearch","aecf8da3":"## 2. Load Data","0ab186dc":"**Summary :**\n\nMAPE is quite higher, so let's try to tune the parameter again","4bc2b5c1":"## 5. Exploratory Data Analysis","3c4d0a5f":"## 4. Data Pre-processing","08568b09":"## 1. Import Libraries","df869f2a":"## Gradient Boosting Regression\n\n- learning_rate = 0.1","ec5276da":"## Gradient Boosting Regression\n\n- learning_rate = 0.05","d168a466":"1. Convert Business Problem to Data Science Problem\n2. Load Data\n3. Understand the Data\n4. Data Preprocessing\n5. Exploratory Data Analysis\n6. Model Building\n7. Predictions and Evaluations\n8. Hyperparameter Tuning","d948599e":"# Gradient Boosting Regression","7e67acf9":"## 3. Understanding the data","b352ebff":"## Grid Search\n","c1833571":"## 6. Model Building"}}