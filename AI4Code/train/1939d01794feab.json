{"cell_type":{"90bfa8f7":"code","1ba4247b":"code","9216e951":"code","bd3b985c":"code","e4b4e067":"code","8e4ecadf":"code","2d8827be":"code","6b9c8d80":"code","3c598527":"code","1ef02a4c":"code","75386fb4":"code","73f82fd6":"code","682f437e":"code","d69594a0":"code","fff0ed01":"code","d8575820":"code","bd909579":"code","81bc9274":"code","8abe4d29":"code","26a638ed":"code","c3e75ae3":"code","c9a120bc":"code","d005fa87":"code","1dd2076b":"code","33924d2b":"code","1a18164d":"code","a6d61dc1":"code","b98b122a":"code","a44679bb":"code","73090511":"code","146c24df":"code","84214e65":"code","dabeb4d9":"code","35f8b466":"code","2b3be45a":"code","26d22458":"code","48a3a970":"code","e9018d3d":"code","ea6160df":"code","84faed03":"code","801b4868":"code","d298562b":"code","a2a27447":"markdown","d172651e":"markdown","30d43469":"markdown","7b0d472a":"markdown","41d6d985":"markdown","961cc855":"markdown","88c88bbf":"markdown","f0f77a46":"markdown","d929b554":"markdown","3b1b01d4":"markdown","86fef524":"markdown","4d46e829":"markdown","ea7c2512":"markdown","6bc03f46":"markdown","03eca4c6":"markdown","9916a4d6":"markdown","1a2382e1":"markdown","8d0e9dfa":"markdown","2ecfdfcb":"markdown","6259e10c":"markdown","ecc4b809":"markdown","3a518599":"markdown","2ce12007":"markdown","e0891ba8":"markdown","d4a8d32b":"markdown","ad7a7a46":"markdown","bd87d382":"markdown","b55b94eb":"markdown","590f6a64":"markdown","05092b10":"markdown","2cc95249":"markdown","8882a29f":"markdown","531ae267":"markdown","d290a1d7":"markdown","0671ae76":"markdown"},"source":{"90bfa8f7":"\nimport pandas as pd\nadd = \"..\/input\/1429_1.csv\"\ndf = pd.read_csv(add)\ndf1_oasis = df.iloc[2816:3482,]\ndf2_fire_16gb = df.iloc[14448:15527,]\ndf3_paperwhite_4gb = df.iloc[17216:20392,]\ndf4_voyage = df.iloc[20392:20972,]\ndf5_paperwhite = df.iloc[20989:21019,]\n#df.head(5)","1ba4247b":"print(df1_oasis.shape)\nprint(df2_fire_16gb.shape)\nprint(df3_paperwhite_4gb.shape)\nprint(df4_voyage.shape)\nprint(df5_paperwhite.shape)","9216e951":"df1_oasis.to_csv('Oasis.csv')\ndf2_fire_16gb.to_csv('Fire.csv')\ndf4_voyage.to_csv('Voyage.csv')","bd3b985c":"frames = [df3_paperwhite_4gb,df5_paperwhite]\ndf4_voyage.to_csv('Voyage.csv')\nkp = pd.concat(frames)\nprint(kp.head(5))\nprint(kp.tail(5))\nkp = kp.reset_index()\nprint(kp.columns)\nprint(kp['reviews.rating'].describe())\nkp.columns = ['Index','ID','Name','ASINS','Brand','Categories','Keys','Manufacturer','ReviewDate','ReviewDateAdded','ReviewDateSeen','PurchasedOnReview','RecommendStatus','ReviewID','ReviewHelpful','Rating','SourceURL','Comments','Title','UserCity','UserProvince','Username']\nkp.columns\nkp.head(5)\nprint(kp.columns.nunique())\nkp = kp.drop(['ReviewID' , 'UserCity' , 'UserProvince','PurchasedOnReview'],axis = 1)\nprint(kp.columns.nunique())\nprint(kp.Rating.value_counts())\nkp.Rating.value_counts()\n","e4b4e067":"kp.RecommendStatus.nunique()\nimport matplotlib.pyplot as plt\nkp.hist(column = 'Rating', by = 'RecommendStatus', color = 'Red')\nplt.show()\nprint(kp.info())","8e4ecadf":"#slice for rating 5\n# slice for recommended\n#slice for comments","2d8827be":"kp['Categories'] = 'Tablets'\nkp['Name'] = 'Amazon Kindle Paperwhite'\nprint(kp.head(5))\nprint(kp.ReviewHelpful.value_counts())\n","6b9c8d80":"pd.DataFrame(kp[(kp.Rating==5)&(kp.RecommendStatus==False)]['Comments'])","3c598527":"print(kp.Username.nunique())\nprint(kp.shape)\nsum(kp['Username'].value_counts()>1)","1ef02a4c":"len(kp['Username'].value_counts()>1)","75386fb4":"kp.head(2)\nkp = kp.drop('Keys',axis = 1)\nprint(kp.columns.nunique())\nkp =kp.reset_index()\nprint(kp.head(2))","73f82fd6":"kp.ReviewDate = pd.to_datetime(kp['ReviewDate'], dayfirst= True)\nkp.ReviewDateAdded =pd.to_datetime(kp.ReviewDateAdded , dayfirst= True)\n#kp.ReviewDateSeen = pd.to_datetime(kp.ReviewDateSeen, dayfirst = True)","682f437e":"kp['ReviewDateSeen'] = kp['ReviewDateSeen'].str.split(',',expand = True).apply(lambda x:x.str.strip())\nkp.ReviewDateSeen = pd.to_datetime(kp.ReviewDateSeen,dayfirst= True)   \nprint(kp.head(4))","d69594a0":"import numpy as np\npromoters = sum(kp.Rating==5)\npassive = sum(kp.Rating == 4)\ndetractors = sum(np.logical_and(kp.Rating >= 1, kp.Rating <=3))\nrespondents = promoters+passive+detractors\nNPS_P = ((promoters - detractors)\/respondents )*100\nprint(NPS_P)","fff0ed01":"print(kp.tail(2))","d8575820":"kp.plot(x = 'ReviewDate',y = 'Rating', kind = 'line',  figsize=(10,10))","bd909579":"review_date = kp.ReviewDate\nrating = kp.Rating\ndf_dr = pd.concat([review_date,rating],axis = 1)\nprint(df_dr.tail(5))\nprint(df_dr.shape)","81bc9274":"df_dr = df_dr.groupby(['ReviewDate','Rating']).size().unstack(fill_value = 0)\nprint(df_dr.loc['2017-02-04'])\n","8abe4d29":"print(df_dr.head(5))","26a638ed":"df_dr.columns = ['A','B','C','Passive','Promoters']\ndf_dr['Detractors'] = df_dr['A'] + df_dr['B'] + df_dr['C']\ndf_dr.head(5)","c3e75ae3":"df_dr = df_dr.drop(labels = ['A','B','C'],axis = 1)\nprint(df_dr.head(5))","c9a120bc":"df_dr['NPS'] = (df_dr['Promoters'] - df_dr['Detractors']) * 100 \/ (df_dr['Passive'] + df_dr['Promoters'] + df_dr['Detractors'])\nprint(df_dr.head(5))","d005fa87":"df_dr = df_dr.reset_index()\ndf_dr.plot( x = 'ReviewDate', y = 'NPS',kind = 'line', figsize=(10,10))\n","1dd2076b":"df_dr.shape","33924d2b":"kp.Name.nunique()\nkp.head(2)\n    ","1a18164d":"data =  kp.drop(['Index','ID','Name','ASINS','Brand','Categories','Manufacturer','ReviewDateAdded','ReviewDateSeen','SourceURL'], axis = 1)\n# Cleaned Dataset Now becomes\ndata = data.reset_index()\ndata.head(5)\n","a6d61dc1":"data = data.drop(['ReviewDate'], axis = 1)\ndata.columns","b98b122a":"def status(data):\n    if(data == True):\n        data = \"Recommend\"\n        return data\n    else:\n        data = \"Not Recommend\"\n        return data\n    \ndata['RecommendStatus'] = pd.DataFrame(data['RecommendStatus'].apply(lambda x : status(x)))\ndata.head(5)\n    ","a44679bb":"dsa = data\ndsa['feedback'] = dsa['Comments'] + dsa['Title']\ndsa = dsa.drop(['Comments','Title'], axis = 1)\n","73090511":"dsa.head(5)","146c24df":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\ndef polar_score(text):\n    score = sid.polarity_scores(text)\n    x = score['compound']\n    return x\n\n\ndsa['Compound_Score'] = dsa['feedback'].apply(lambda x : polar_score(x))","84214e65":"dsa.head(5)\n","dabeb4d9":"dsa['length'] = dsa['feedback'].apply(lambda x: len(x) - x.count(\" \"))\ndsa.head(2)","35f8b466":"import numpy as np\nfrom matplotlib import pyplot\n%matplotlib inline","2b3be45a":"bins = np.linspace(0,200,40)\npyplot.hist(dsa[dsa['RecommendStatus'] == 'Not Recommend']['length'],bins,alpha  = 0.5,normed = True, label = 'Not Recommend')\npyplot.hist(dsa[dsa['RecommendStatus'] == 'Recommend']['length'],bins,alpha = 0.5,normed = True, label = 'Recommend')\npyplot.legend(loc = 'upper right')\npyplot.show()","26d22458":"import string\nimport nltk\nimport re\nstopword =  nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()","48a3a970":"def clean(text):\n    no_punct = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+',no_punct)\n    text_stem = ([ps.stem(word) for word in tokens if word not in stopword])\n    return text_stem","e9018d3d":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf_idf = TfidfVectorizer(analyzer= clean)\nXtf_idfVector = tf_idf.fit_transform(dsa['feedback'])","ea6160df":"import pandas as pd\n\nXfeatures_data = pd.concat([dsa['Compound_Score'], dsa['length'], pd.DataFrame(Xtf_idfVector.toarray())], axis = 1)\nXfeatures_data.head(5)","84faed03":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(Xfeatures_data, dsa['RecommendStatus'], test_size = 0.2)\n\nrf = RandomForestClassifier(n_estimators= 50, max_depth= 20, n_jobs= -1)\nrf_model = rf.fit(X_train,y_train)\nsorted(zip(rf.feature_importances_,X_train.columns), reverse = True)[0:10]","801b4868":"def compute(n_est, depth):\n    rf = RandomForestClassifier(n_estimators= n_est, max_depth= depth)\n    rf_model = rf.fit(X_train, y_train)\n    y_pred  = rf_model.predict(X_test)\n    precision,recall,fscore,support  = score(y_test,y_pred, pos_label= 'Recommend', average = 'binary')\n    print('Est: {}\\ Depth: {}\\ Precision: {}\\ Recall: {}\\ Accuracy: {}'.format(n_est, depth, round(precision,3), round(recall,3), (y_pred == y_test).sum()\/ len(y_pred)))","d298562b":"for n_est in [10,30,50,70]:\n    for depth in [20,40,60,80,90]:\n        compute(n_est,depth)\n    ","a2a27447":"#### Ideally people who'll Not Recommend the product, would have a lot to say against the features of the product","d172651e":"# 4. Likert Scale Analysis\n\n### Net Promoter Score","30d43469":"Let's see the comments of people have given a 5 star rating and have still Not Recommended the Product","7b0d472a":"We now have names of people who provided more than one comment we now need to figure out the dates when people with THESE NAMES provided their comments. Select dates on which these people added reviews, sort by Username ","41d6d985":"#### Above graph shows that our original hypothesis was correct","961cc855":"#### Applying Grid Search to change hyper parameters and then applying RF\n","88c88bbf":"# 2. Analysis of Data\n\n    User Rating Count Distribution\n    Rating          No            Percentage\n    5                   2564         79.97% \n    4                   545           16.99%\n    3                   60             1.87%\n    2                   22             0.686%\n    1                   15             0.467%\n\n\n","f0f77a46":"# 7. Understanding the  Features added","d929b554":"#### Columns to Remove : Index - ID - Name - ASINS - Brand - ReviewDateAdded - ReviewDateSeen - SourceURL","3b1b01d4":"Promters had a NPS of 80.324.\n\n\nThis is the overall NPS of the product, however, let's visualize how the rating of KindlePaperWhite has changed over time","86fef524":"#### Exploring individual Dataframes","4d46e829":"![alt text](https:\/\/i.imgflip.com\/2defis.jpg)","ea7c2512":"#### Changing RecommendStatus from True\/False to Recommend\/Not Recommend","6bc03f46":"#### We finally have the dataframe we would be applying Machine Learning to","03eca4c6":"#### Feature for Compound Score (using Sentiment Analysis)","9916a4d6":"# 3. Transforming Date Time\n    - Parse ReviewDate to [Date and Time]\n    - Parse ReviewDateAdded to [Date and Time]\n    - Parse ReviewDateSeen to [Date and Time]","1a2382e1":"#### New DataFrame having all the required features , the label we want to predict and the tf_idf vectorizer","8d0e9dfa":"We focus our attention on Kindle Paperwhite (df3 and df5), while saving other dataframes for later use","2ecfdfcb":"So we  now have rating distribution by date, let's now calculate the sum of ratings for 1,2 and 3 for each date,and finally add a new column to the df, while deleting 1,2,3","6259e10c":"### Approach","ecc4b809":"This project focusses on the following areas :\n    \n   - Analysis of the dataset\n   - Understanding of the User's Rating Distribution\n   - Predict Recommend Status based on the subjective review provided by the user","3a518599":"1. Clean the Dataset\n    - Clean Column names \n    - Clean Categories\n    - Clean Keys\n2. Analysis of Data\n3. Transforming Date Time\n    - Parse ReviewDate to [Date and Time]\n    - Parse ReviewDateAdded to [Date and Time]\n    - Parse ReviewDateSeen to [Date and Time]\n4. Likert Scale Analysis : \n    - 5 Point NPS Breakdown\n    - Ratings from 0-3  :  Detractors\n    - Ratings from 4  : Passive\n    - Ratings from 5  : Promoter\n5. Feature Engineering\n6. Apply NLTK - Sentiment Analysis to find Compound Score\n7. Understanding the Fearures Added\n8. Using TF-IDF and Random Forest to predict Recommendation Status","2ce12007":"###  Pivot Table for Promoter Score by Date","e0891ba8":"From the above chart we can see that there are people with 4 and 5 rating who have still not recommended the product\nLet's try to explore the review provided by these 1o individuals","d4a8d32b":"# Conclusion :  \n\nFeature Engineering played a key role in boosting the model's performance matrix. The length of the text\nand calculation of compound_score using sentiment analysis served as a basis to strike a balance between Precision & Recall (0.975 vs 1.0) and further made the model robust enough to predict user's recommend status to 97.5%\n\nThis concludes our Analysis of the Kindle Paperwhite.","ad7a7a46":"#### Feature for Text Length","bd87d382":"# Kindle PaperWhite ","b55b94eb":"So our original assumption about Compound Score being a major indicator in classifcation values was correct","590f6a64":" Let's see the Unique User Names \/ Shape of the Dataset and the the number of unique Usernames","05092b10":"All dates are different, so we hae to calculate NPS for interval of 2 months Plot a line chart for the same","2cc95249":"### NOTES : \nUnderstanding the Rating distribution\n\n\nThere are total of 2890 unique users , however the total ratings received are 3206, so that means some people are giving their ratings more than once, so we need to figure out if the extra 316 ratings are from people who have already provided ratings, and if yes are they from the same date and how many products\n\nthere are 188 people who had given more than 1 rating","8882a29f":"# 1. Cleaning the Dataset\n","531ae267":"#### Combining df3 and df5 for Kindle Paperwhite Edition Data Only","d290a1d7":"# 8. Using TF-IDF and Random Forest to predict Recommendation Status","0671ae76":"# 5. Feature Engineering  \n# 6. Sentiment Analysis - NLTK to find Compound Score"}}