{"cell_type":{"1063781d":"code","bbd713b4":"code","449b7ea3":"code","b52a7d20":"code","41ed36b1":"code","b61c159e":"code","d01d6042":"code","c072fb62":"code","1458fc09":"code","a05c847d":"code","8fff24d2":"code","b07aa568":"code","7e96eda1":"code","9c69c825":"code","37a194b6":"code","5af61bd6":"markdown","914b9842":"markdown","d5d38b9f":"markdown","4a5e9083":"markdown","4fd90498":"markdown","cbdd21cf":"markdown","1f9a205a":"markdown","10e873ac":"markdown","fd8089ca":"markdown","3c399fe8":"markdown"},"source":{"1063781d":"from gensim.matutils import sparse2full \nfrom gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\nimport sqlite3 as sql\nimport pandas as pd\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","bbd713b4":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results as a dataframe\n    '''\n    with sql.connect(db) as conn:\n        df = pd.read_sql_query(select, conn)\n    df.columns = [str(col).lower() for col in df.columns]\n    return df","449b7ea3":"select = '''select * from articles limit 5'''\ndf = get_query(select)\ndf","b52a7d20":"def get_article_text(article_id):\n    '''\n    1. Construct select statement\n    2. Retrieve all section_texts associated with article_id\n    3. Join section_texts into a single string (article_text)\n    4. Return article_text\n    \n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where article_id=%d''' % article_id\n    df_article = get_query(select)\n    doc = '\\n'.join(df_article['section_text'].tolist())\n    return doc","41ed36b1":"article_text = get_article_text(0)\n# Just print the first 1000 characters for the sake of brevity\nprint(article_text[:1000])","b61c159e":"def tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens","d01d6042":"tokens = tokenize(article_text)\nprint(tokens[:5])","c072fb62":"dictionary = Dictionary([tokens])","1458fc09":"def get_article_tokens(article_id):\n    '''\n    1. Construct select statement\n    2. Retrieve all section_texts associated with article_id\n    3. Join section_texts into a single string (article_text)\n    4. Tokenize article_text\n    5. Return list of tokens\n    \n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where article_id=%d''' % article_id\n    df_article = get_query(select)\n    doc = '\\n'.join(df_article['section_text'].tolist())\n    tokens = tokenize(doc)\n    return tokens","a05c847d":"# First, we need to grab all article_ids from the database\nselect = '''select distinct article_id from articles'''\narticle_ids = get_query(select)\narticle_ids = article_ids['article_id'].tolist()","8fff24d2":"start = time.time()\n# Grab a random sample of 10K articles and read into memory\nsample_ids = np.random.choice(article_ids, size=10000, replace=False)\ndocs = []\nfor sample_id in tqdm(sample_ids):\n    docs.append(get_article_tokens(sample_id))\n# Train dictionary\ndictionary = Dictionary(docs)\nend = time.time()\nprint('Time to train dictionary from in-memory sample: %0.2fs' % (end - start))","b07aa568":"start = time.time()\n# Grab a random sample of 10K articles and set up a generator\nsample_ids = np.random.choice(article_ids, size=10000, replace=False)\ndocs = (get_article_tokens(sample_id) for sample_id in sample_ids)\n# Train dictionary\ndictionary = Dictionary(docs)\nend = time.time()\nprint('Time to train dictionary from generator: %0.2fs' % (end - start))","7e96eda1":"class Corpus():\n    def __init__(self, article_ids, db):\n        self.article_ids = article_ids\n        self.db = db\n        self.len = len(article_ids)\n\n    def __iter__(self):\n        article_ids_shuffled = np.random.choice(self.article_ids, self.len, replace=False)\n        with sql.connect(db) as conn:\n            for article_id in article_ids_shuffled:\n                select = '''select section_text from articles where article_id=%d''' % article_id\n                df_article = self.get_query(select, conn)\n                doc = '\\n'.join(df_article['section_text'].tolist())\n                tokens = self.tokenize(doc)\n                yield tokens\n\n    def __len__(self):\n        return self.len\n        \n    def get_query(self, select, conn):\n        df = pd.read_sql_query(select, conn)\n        df.columns = [str(col).lower() for col in df.columns]\n        return df\n        \n    def tokenize(self, text, lower=True):\n        text = re.sub(\"'\", \"\", text)\n        if lower:\n            tokens = re.findall('''[a-z_]+''', text.lower())\n        else:\n            tokens = re.findall('''[A-Za-z_]''', text)\n        return tokens","9c69c825":"docs = Corpus(article_ids[:10000], db)\ndictionary = Dictionary(docs)","37a194b6":"dictionary = Dictionary.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20.dict')","5af61bd6":"# Tutorial: Dictionary\nThis is a basic guide to efficiently training a Dictionary on the English Wikipedia dump using Gensim.","914b9842":"As we can see, we have four fields: article_id, title, section_title, and section_text. Each wikipedia article is broken into sections, and to reconstitute them, we'll need to grab all section_text associated with a single article_id and combine them. Let's create a function to do just that.","d5d38b9f":"Let's start by creating a helper function to pull data from the database and examine some output.","4a5e9083":"Now that we have a way to get tokens, let's create a dictionary. Once imported, all you have to do to create a dictionary is to instantiate an object and feed it an interator\/iterable. Gensim assuming a nested list of lists structure, where each item in the sublist is a string token.","4fd90498":"So if we want to train a dictionary on the entire dataset, we can simple create an iterable of all article_ids and feed it into a Dictionary.\n\n`dictionary = Dictionary(Corpus(article_ids))`\n\nFeel free to try it, but I've already pre-trained a Dictionary on this dataset, so we'll just load it into memory and explore how to work with it.","cbdd21cf":"For extended functionality, it's actually better to define an *iterable* when working with Gensim. Here is a simple template.","1f9a205a":"Let's start by loading all documents (from a random sample) into memory, then building a dictionary.","10e873ac":"That's all well and good, but we want more than just one document in our corpus! Let's create a wrapper function to combine the retrieval and tokenization step, then try a few ways of creating a dictionary.","fd8089ca":"Now that we can get retrieve data from the database, let's start building a dictionary. Dictionaries in Gensim are built on top of the high-performance [containers](https:\/\/docs.python.org\/3.6\/library\/collections.html) module found in base python. Essentially, we want to assign an integer ID to each unique word, and keep track of how many times that word comes up across a collection of documents (i.e., corpus). Let's create a function to split our article text into tokens.","3c399fe8":"Now let's try it *from a generator*. Instead of loading all documents into memory before training the dictionary, we will tokenize them on the fly."}}