{"cell_type":{"c2ec6c9b":"code","c36a0dfd":"code","fc050f29":"code","8eab4f8d":"code","74176ac2":"code","453af447":"code","4cdf4a5d":"code","8cf6b6f9":"code","eb7ff724":"markdown","8b165573":"markdown","b5f9a40d":"markdown","6ebe42a8":"markdown","9dc1e789":"markdown","10aa042a":"markdown","b7f1a124":"markdown","b3765d94":"markdown","c554bcb7":"markdown"},"source":{"c2ec6c9b":"# Import NumPy and initialize Variable x and target y\n\nimport numpy as np\n\nx = np.array([1,3,5])\ny = np.array([5,12,18])","c36a0dfd":"w1 =((x*y).mean() - x.mean()*y.mean())\/((x**2).mean() - (x.mean())**2)\nw1","fc050f29":"w0 = y.mean() - w1*x.mean()\nw0","8eab4f8d":"# Step 1: Initializing weights to 0, \n# a (learning parameter) to 0.04, \n# and an array MSE for storing MSE at each iteration\n\nW0_new = 0\nW1_new = 0\na = 0.04\nMSE = np.array([])","74176ac2":"for iteration in range(1,11):\n    \n    y_pred = np.array([])                        # The Predicted target\n    error = np.array([])                         # The errors per iterations : (Y\u0302-Y)\n    error_x = np.array([])                       # The (Y\u0302-Y).X term for update rule\n    \n    W0 = W0_new                                  # Step 1 and 4: Initializing new weights\/Assigning the updated weights\n    W1 = W1_new\n\n    # Step 2:    \n    \n    for i in x:                                  # Iterating X row by row for calculating the Y\u0302 and error\n        y_pred = np.append(y_pred,(W0 + W1*i))   # Y\u0302 = W0 + W*X \n\n    error = np.append(error,y_pred-y)            # Calculating the error for each sample \n    error_x = np.append(error_x, error*x)        # Calculating the (Y\u0302-Y).X term for update rule\n    MSE_val = (error**2).mean()                  # Calculating the MSE    \n    MSE = np.append(MSE,MSE_val)\n    \n    # Step 3:   \n\n    W0_new = W0 - a*np.sum(error)               # Calculating the updated W0   \n    W1_new = W1 - a*np.sum(error_x)             # Calculating the updated W1","453af447":"print('W0 by gradient descent= ',W0_new)\nprint('W1 by gradient descent= ',W1_new)","4cdf4a5d":"print('y_pred: ',y_pred)\nprint('error:  ',error)","8cf6b6f9":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(MSE,'b-o')\nplt.title('Mean Square error per iteration')\nplt.xlabel('Iterations')\nplt.ylabel('MSE value')\nplt.show()","eb7ff724":"Gradient Descent operation is completed, now let's see how well did it do and how near the weights are to OLS solution above","8b165573":"> **Optimization** is at the heart of **Machine Learning**. Whichever model you create, whatever problem you solve, whatever type of data you have, you will always be optimizing the solution.\n\nWhen it comes to **regression**, we find the **optimum weights** of the regression line\/curve.\n\nTo find these weights, the **Ordinary Least Squares** method is the best way to find the optimum points.\n\nHowever, OLS cannot be extended to bigger models and dataset.\n\nTherefore, we approximate the optima by iterating towards it. This is called **Gradient Descent**, as we are *descending* towards the global minima. \n\nAnd gradient is nothing but the slope of the loss curve.","b5f9a40d":"This was it, you can feel free to tinker with it and suggest improvements!","6ebe42a8":"Now, let's plot its MSE curve and see how it decreases with iterations","9dc1e789":"Now, let's check what the **OLS solution** finds the values of the weights:","10aa042a":"In this notebook, also my first on Kaggle, I take a dummy data to show how the Gradient Descent works, **all in pure NumPy!**","b7f1a124":"Hmm, not bad.\nLet's also check the predictions and the errors","b3765d94":"Now, let's implement Gradient Descent in NumPy from Scratch!","c554bcb7":"Here, w1 is the coefficient of x and w0 is the intercept or the bias."}}