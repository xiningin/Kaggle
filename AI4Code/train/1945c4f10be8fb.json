{"cell_type":{"03ffad39":"code","3d221751":"code","3bb840c7":"code","26797139":"markdown"},"source":{"03ffad39":"\"\"\"Greedy agent that chooses machine based on maximum expected payout\n\nUses a trained decision tree model to consider the other player's movements\nin the expected payout.\n\nSee my other kernel for methodology for generating training data:\nhttps:\/\/www.kaggle.com\/lebroschar\/generate-training-data\n\n\"\"\"\nimport pickle\nimport base64\nimport random\nimport numpy as np\nimport pandas as pd\nimport sklearn.tree as skt\n\n# Parameters\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = '\/kaggle\/input\/sample-training-data\/training_data_201223.parquet'\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\n\ndef make_model():\n    \"\"\"Builds a decision tree model based on stored trainingd data\"\"\"\n    data = pd.read_parquet(DATA_FILE)\n    model = skt.DecisionTreeRegressor(min_samples_leaf=40)\n    model.fit(data[TRAIN_FEATS], data[TARGET_COL])\n    return model\n\n# Train model and save as a sav file.\nmodel = make_model()\nfilename = 'model.sav'\npickle.dump(model, open(filename, 'wb'))","3d221751":"%%writefile main.py\n\n\"\"\"Greedy agent that chooses machine based on maximum expected payout\n\nUses a trained decision tree model to consider the other player's movements\nin the expected payout.\n\nSee my other kernel for methodology for generating training data:\nhttps:\/\/www.kaggle.com\/lebroschar\/generate-training-data\n\n\"\"\"\nimport pickle\nimport base64\nimport random\nimport numpy as np\nimport pandas as pd\nimport sklearn.tree as skt\nimport sys\nimport os\n\nimport random, os, datetime, math\nfrom collections import defaultdict\n\n# Below is needed to submit tar.gz file to Kaggle.\nsys.path.append(\"\/kaggle_simulations\/agent\")\nworking_dir = \"\/kaggle_simulations\/agent\"\npath_to_model = os.path.join(working_dir,\"model.sav\")\n\n# Parameters\nFUDGE_FACTOR = 0.99\nVERBOSE = False\nDATA_FILE = '\/kaggle\/input\/sample-training-data\/training_data_201223.parquet'\nTRAIN_FEATS = ['round_num', 'n_pulls_self', 'n_success_self', 'n_pulls_opp']\nTARGET_COL = 'payout'\nfilename = 'model.sav'\n\n\nclass GreedyStrategy:\n    \"\"\"Implements strategy to maximize expected value\n\n    - Tracks estimated likelihood of payout ratio for each machine\n    - Tracks number of pulls on each machine\n    - Chooses machine based on maximum expected value\n    \n    \n    \"\"\"\n    def __init__(self, name, agent_num, n_machines):\n        \"\"\"Initialize and train decision tree model\n\n        Args:\n           name (str):   Name for the agent\n           agent_num (int):   Assigned player number\n           n_machines (int):   number of machines in the game\n        \n        \"\"\"\n        # Record inputs\n        self.name = name\n        self.agent_num = agent_num\n        self.n_machines = n_machines\n        \n        # Initialize distributions for all machines\n        self.n_pulls_self = np.array([0 for _ in range(n_machines)])\n        self.n_success_self = np.array([0. for _ in range(n_machines)])\n        self.n_pulls_opp = np.array([0 for _ in range(n_machines)])\n\n        # Track other players moves\n        self.opp_moves = []\n        \n        # Track winnings\n        self.last_reward_count = 0\n\n        # Load model from other file\n        self.model = pickle.load(open(path_to_model, 'rb'))\n        \n        # Predict expected reward\n        features = np.zeros((self.n_machines, 4))\n        features[:, 0] = len(self.opp_moves)\n        features[:, 1] = self.n_pulls_self\n        features[:, 2] = self.n_success_self\n        features[:, 3] = self.n_pulls_opp\n        self.predicts = self.model.predict(features)\n        \n\n    def __call__(self):\n        \"\"\"Choose machine based on maximum expected payout\n\n        Returns:\n           <result> (int):  index of machine to pull\n        \n        \"\"\"\n        # Otherwise, use best available\n        est_return = self.predicts\n        max_return = np.max(est_return)\n        result = np.random.choice(np.where(\n            est_return >= FUDGE_FACTOR * max_return)[0])\n        \n        if VERBOSE:\n            print('  - Chose machine %i with expected return of %3.2f' % (\n                int(result), est_return[result]))\n\n        return int(result)\n    \n        \n    def updateDist(self, curr_total_reward, last_m_indices):\n        \"\"\"Updates estimated distribution of payouts\"\"\"\n        # Compute last reward\n        last_reward = curr_total_reward - self.last_reward_count\n        self.last_reward_count = curr_total_reward\n        if VERBOSE:\n            print('Last reward: %i' % last_reward)\n\n        if len(last_m_indices) == 2:\n            # Update number of pulls for both machines\n            m_index = last_m_indices[self.agent_num]\n            opp_index = last_m_indices[(self.agent_num + 1) % 2]\n            self.n_pulls_self[m_index] += 1\n            self.n_pulls_opp[opp_index] += 1\n\n            # Update number of successes\n            self.n_success_self[m_index] += last_reward\n            \n            # Update opponent activity\n            self.opp_moves.append(opp_index)\n\n            # Update predictions for chosen machines\n            self.predicts[[opp_index, m_index]] = self.model.predict([\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[opp_index],\n                    self.n_success_self[opp_index],\n                    self.n_pulls_opp[opp_index]\n                ],\n                [\n                    len(self.opp_moves),\n                    self.n_pulls_self[m_index],\n                    self.n_success_self[m_index],\n                    self.n_pulls_opp[m_index]\n                ]])\n            \n\ntotal_reward = 0\nbandit_dict = {}\nmy_action_list = []\nop_action_list = []\nop_continue_cnt_dict = defaultdict(int)\n\ndef agent(observation, configuration):\n    global total_reward, bandit_dict, curr_agent\n    \n    if observation.step == 0:\n        # Initialize agent\n        curr_agent = GreedyStrategy('Mr. Agent %i' % observation['agentIndex'],\n            observation['agentIndex'],\n            configuration['banditCount'])\n    \n    # Update payout ratio distribution with:\n    curr_agent.updateDist(observation['reward'], observation['lastActions'])\n    \n    #pull vegas\n    my_pull = random.randrange(configuration['banditCount'])\n    if observation['step'] == 0:\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if last_reward > 0:\n            bandit_dict[my_last_action]['win'] += 1\n        else:\n            bandit_dict[my_last_action]['loss'] += 1\n        bandit_dict[op_last_action]['opp'] += 1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = curr_agent()\n                else:\n                    my_pull = curr_agent()\n            else:\n                my_pull = curr_agent()\n    \n    return my_pull","3bb840c7":"# Make a tar.gz file for submission\n!tar cvfz main.py.tar.gz main.py model.sav","26797139":"# Credits\n\nCredits for these notebooks:\n* https:\/\/www.kaggle.com\/a763337092\/pull-vegas-slot-machines-add-weaken-rate-continue5\n* https:\/\/www.kaggle.com\/lebroschar\/1000-greedy-decision-tree-model\n* https:\/\/www.kaggle.com\/iwatatakuya\/how-to-submit-greedy-decision-tree-model"}}