{"cell_type":{"1c7942f5":"code","3b1ac8e1":"code","6a115658":"code","f5269281":"code","49122524":"code","ac9991fe":"code","101e60a4":"code","95a4a84c":"code","41aa0e41":"code","f10f85d4":"code","6a298359":"code","3a7f69f2":"code","f1214146":"code","e3ac8397":"code","236810f1":"code","c9cea4d0":"code","5502aac6":"code","a557d1dd":"code","9ba6cac5":"code","23d0b5d6":"code","1a1deb85":"code","b817716e":"code","b3c6ed4d":"code","d44bb891":"code","e849c1b6":"code","5ce3941d":"code","46332837":"code","408cc32a":"code","e3269373":"code","9d75a802":"code","ea14936b":"code","e7af800a":"code","96ba6ca3":"code","ec76db85":"code","04881a99":"code","77b83e8b":"code","867cc7df":"code","8872dddc":"code","2499ab5d":"code","42a07f6a":"code","8af11480":"code","65b95483":"code","8f2a66b0":"code","e7a032ad":"code","0c83f1b3":"code","8f97a8bb":"markdown","89c528ef":"markdown","6ba59f3a":"markdown","a45a2266":"markdown","ba953b2c":"markdown","486a9e54":"markdown","c28bd67d":"markdown","424e7d19":"markdown","71412deb":"markdown","0721b27a":"markdown","44c1ed24":"markdown","10796ed4":"markdown","d2308198":"markdown","d9417e28":"markdown"},"source":{"1c7942f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\n\nplt.style.use('fivethirtyeight')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b1ac8e1":"df = pd.read_csv(\"\/kaggle\/input\/adoptable-dogs\/ShelterDogs.csv\")\ndf","6a115658":"df.isnull().sum()","f5269281":"df.info()","49122524":"#Most of the features are categorical\ndf.describe(include=[object]).T","ac9991fe":"#The variable is returned without outliers. It is the only kolichesvennaya, therefore, emissions will not be processed\ndf.describe()","101e60a4":"# The target variable is full of omissions. Let's assume that most NaNs are 'no'\ndf['likes_people'].value_counts()","95a4a84c":"df['date_found'] = pd.to_datetime(df['date_found'])\ndf['adoptable_from'] = pd.to_datetime(df['adoptable_from'])\ndf['posted'] = pd.to_datetime(df['posted'])","41aa0e41":"#There are such objects\ndf.loc[(df[\"likes_people\"].isnull()) & (df['likes_children'] == 'yes')].head()","f10f85d4":"#Making a replacement for the rest of the objects\ndf.loc[(df[\"likes_people\"].isnull()) & (df['get_along_males'] == 'yes'), 'likes_people'] = 'yes'\ndf.loc[(df[\"likes_people\"].isnull()) & (df['get_along_females'] == 'yes'), 'likes_people'] = 'yes'\ndf.loc[(df[\"likes_people\"].isnull()) & (df['likes_children'] == 'yes'), 'likes_people'] = 'yes'","6a298359":"#We were able to recover a couple of hundred missing values. Let the remaining Nan be 'no'\ndf.isnull().sum()","3a7f69f2":"#The shelter may not always know where the dogs were kept. But the 'keep_in' column itself may be of value, \n#therefore, we will leave it.\ndf_fill_likes_people = df.copy()\ndf_fill_likes_people['likes_people'] = df['likes_people'].fillna('no')\ndf_fill_likes_people['keep_in'] = df['keep_in'].fillna('unknow')\ndf_fill_likes_people.drop(['housebroken', 'get_along_cats', 'get_along_females', 'get_along_males', 'likes_children'], axis=1, inplace=True)","f1214146":"df_fill_likes_people['keep_in'].value_counts()","e3ac8397":"df_fill_likes_people","236810f1":"#I will divide the dogs by whether there is a name or not. Perhaps the presence of the name played a role in the' socialization ' of the dog\n#It will be interesting to check whether castration has affected the positive attitude of the dog to the person.\ndf_fill_likes_people['neutered'] = df_fill_likes_people['neutered'].fillna('unknow')\ndf_fill_likes_people['name'] = df_fill_likes_people['name'].fillna(\"no\")\n\ndef name(x):\n    if x == 'no':\n        return 1\n    else:\n        return 0\n    \ndf_fill_likes_people['name'] = df_fill_likes_people['name'].apply(name)\ndf_fill_likes_people['name']","c9cea4d0":"df_fill_likes_people.isnull().sum()","5502aac6":"df_fill_likes_people['likes_people'].value_counts()","a557d1dd":"#The most popular name for a dog and what is the most common name for dogs that love people\ndef bar_likes(column):\n    fig, ax = plt.subplots(1, 2)\n    ax1, ax2 = ax.flatten()\n    fig.set_size_inches(20, 6)\n    fig.autofmt_xdate()\n\n    names_counts = df_fill_likes_people[column].value_counts().head(10)\n    ax1.bar(names_counts.index, names_counts, color='#76A3DE')\n    ax1.set_title('The most popular {}'.format(column))\n\n    names_likes_people = df_fill_likes_people.loc[df_fill_likes_people['likes_people'] == 'yes', column].value_counts().head(10)\n    ax2.bar(names_likes_people.index, names_likes_people, color='#FAB464')\n    ax2.set_title('The most popular {} among likes people'.format(column))","9ba6cac5":"#The most popular breed of dog and which breed is most common in dogs that love people\n#The difference in a couple of names is mostly accidental\u0412 \u043e\u0431\u0449\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043d\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0435\u0442\u0441\u044f, \u0442.\u043a. \u0438\u043c\u0435\u043d\u0430 \u0432 \u043e\u0434\u043d\u043e\u0439 \u0442\u0430\u0431\u043b\u0438\u0446\u0435 \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442\nbar_likes('breed')","23d0b5d6":"#The most popular coat color in dogs\n#No significant difference is observed\nbar_likes('color')","1a1deb85":"#The most popular dog size\n#and is there a statistical difference between them \ndf_fill_likes_people.groupby(['size', 'likes_people'])['likes_people'].count()","b817716e":"## Dogs whose place of residence is unknown are the most angry. Probably homeless.\ndf_fill_likes_people.groupby(['keep_in', 'likes_people'])['likes_people'].count()","b3c6ed4d":"# Dependence on gender is unknown. I will check it statistically\ndf_fill_likes_people.groupby(['sex', 'likes_people'])['likes_people'].count()","d44bb891":"#Who is more peaceful, female or male? Is this statistically significant difference?\ndf_fill_likes_people.groupby(['sex', 'likes_people'])['likes_people'].count()\nA = df_fill_likes_people[df_fill_likes_people['sex'] == 'male']['likes_people'].map({'yes': 1, 'no': 0})\nB = df_fill_likes_people[df_fill_likes_people['sex'] == 'female']['likes_people'].map({'yes': 1, 'no': 0})","e849c1b6":"#\u041a\u0442\u043e \u043c\u0438\u0440\u043e\u043b\u044e\u0431\u0438\u0432\u0435\u0435 \u0441\u0430\u043c\u043a\u0430 \u0438\u043b\u0438 \u0441\u0430\u043c\u0435\u0446? \u0415\u0441\u043b\u0438 \u0434\u0430, \u0442\u043e \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043b\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u0430\u044f \u044d\u0442\u043e \u0440\u0430\u0437\u043d\u0438\u0446\u0430?\ndef A_B_Test(A, B):\n    \n    ntA = stats.shapiro(A)[1] < 0.05\n    ntB = stats.shapiro(B)[1] < 0.05\n    \n    if (ntA == False) & (ntB == False):\n        leveneTest = stats.levene(A, B)[1] < 0.05\n        \n        if leveneTest == False:\n            print('ttest, with equal_var')\n            ttest = stats.ttest_ind(A, B, equal_var=True)[1]\n            return ttest < 0.05\n        else:\n            print('ttest, without equal_var')\n            ttest = stats.ttest_ind(A, B, equal_var=False)[1]\n            return ttest < 0.05\n    else:\n        print(\"mannwhitneyuy: \")\n        mannwhitneyu = stats.mannwhitneyu(A, B)[1] \n        return mannwhitneyu < 0.05\n    \nif A_B_Test(A, B) == True:\n    print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u0437\u043d\u0430\u0447\u0438\u043c\u044b')\nelse:\n    print('\u0420\u0430\u0437\u043b\u0438\u0447\u0438\u044f \u0441\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u041d\u0415 \u0437\u043d\u0430\u0447\u0438\u043c\u044b')","5ce3941d":"#The curse of dimension breathes in the back.\ny = df_fill_likes_people['likes_people'].map({'yes': 1, 'no': 0})\ndf_to_dummie = df_fill_likes_people.drop(['ID', 'date_found', \n                                        'adoptable_from', 'posted', 'likes_people'], axis=1)\nto_dummie = df_to_dummie.select_dtypes('object').columns\ndf_dummie = pd.get_dummies(df_to_dummie, columns=to_dummie, drop_first=True)\ndf_dummie","46332837":"#We solve it.\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf_dummie['age'] = scaler.fit_transform(np.array(df_dummie['age']).reshape(-1, 1))\ndecomp = PCA(311)\ndecomp.fit(df_dummie)","408cc32a":"#Approximately 50 features can be left\nplt.plot(np.cumsum(decomp.explained_variance_ratio_), '*--');","e3269373":"df_pca = PCA(50).fit_transform(df_dummie) ","9d75a802":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df_pca, y,\n                                                    test_size=0.25)\nX_train","ea14936b":"from sklearn.linear_model import LogisticRegressionCV\nlog_model = LogisticRegressionCV(cv=3)\nlog_model.fit(X_train, y_train)\ny_pred_log = log_model.predict(X_test)","e7af800a":"from sklearn.metrics import classification_report\nprint(classification_report(y_pred_log, y_test))","96ba6ca3":"log_model.score(X_train, y_train)","ec76db85":"#As for me, the result is worthy. The model can be used to determine the probability of belonging to a class\nlog_model.score(X_test, y_test)","04881a99":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","77b83e8b":"n_estimator = [10, 50, 100, 1000]\nmax_depth = [2, 5, 7, 15, 30]\nhyperparameters = dict(n_estimators=n_estimator,\n                       max_depth= max_depth)\nforest = RandomForestClassifier()\ngridsearch = GridSearchCV(forest, hyperparameters, cv=5, verbose=1)\nbest_model = gridsearch.fit(X_train, y_train)","867cc7df":"y_pred = best_model.predict(X_test)\nprint(classification_report(y_pred, y_test))","8872dddc":"from catboost import CatBoostClassifier\ncat_model = CatBoostClassifier(verbose=0)\ngrid = {'learning_rate': [0.03, 0.1],\n        'depth': [4, 6, 10],\n        'l2_leaf_reg': [1, 3, 5]}\n\ngrid_search_result = cat_model.grid_search(grid, X=X_train, y=y_train, cv=3, plot=True, verbose=0)","2499ab5d":"#CatBoost copes better than others. I use it to get probabilities\ncat_predict = CatBoostClassifier(depth=4, l2_leaf_reg=3, learning_rate=0.03)\ncat_predict.fit(X_train, y_train, verbose=0)\ncat_pred = cat_predict.predict(X_test) \nprint(classification_report(cat_pred, y_test))","42a07f6a":"pribabylity = cat_predict.predict_proba(df_pca)\npribabylity[:, 1]","8af11480":"df_fill_likes_people['prob_likes'] = pribabylity[:, 1]","65b95483":"#I'm coding some features to simplify visualization\nbin_labels_5 = [1, 2, 3 ,4, 5, 6, 7, 8, 9, 10]\ndf_fill_likes_people['labels'] = pd.qcut(df_fill_likes_people['prob_likes'],\n                              q = [.0, .1, .2, .3, .4, .5, .6, .7, .8, .9, 1.],\n                              labels=bin_labels_5)\nage = ['Puppy', 'Young', 'Middle_age', 'Old']\ndf_fill_likes_people['age_interval'] = pd.qcut(df_fill_likes_people['age'], \n                                             q = [.0, .1, .5, .7, 1],\n                                             labels=age)","8f2a66b0":"df_fill_likes_people['age_interval']","e7a032ad":"df_fill_likes_people","0c83f1b3":"df_fill_likes_people.to_csv(\"dogs_clean.csv\")\ncat_predict.save_model('dogs_proba', format=\"cbm\")","8f97a8bb":"** The purpose of NoteBook** is to assign a probability of friendliness to each pet for subsequent visualization in PowerBI.","89c528ef":"### CatBoost","6ba59f3a":"## And let's go to dummie and then PCA","a45a2266":"### LogisticRegression","ba953b2c":"How to determine the peacefulness of the future pet by gender, color and breed? Perhaps there is such a dependence. To determine this dependence, I will go through the following steps:\n1. Deleting columns with too large gaps, while trying to restore some missing values in the target variables\n2. I will analyze the dependencies on the target variable. \n3. Processing unnecessary signs in the case of multicollinearity.\n4. I will build machine learning models to determine the probability that a pet is friendly to a person\n5. I will build a dashboard that will help you find a good friend.","486a9e54":"## Feature Eingineering ","c28bd67d":"## Uploading data and promo pages of general summary statistics","424e7d19":"## I get the probabilities and save the model","71412deb":"If the dog likes at least a child, a man or a woman, this should be reflected as \"yes\"in the likes_people column. If not, then I change Nan to 'no' in the corresponding lines\u0415\u0441\u043b\u0438 \u0441\u043e\u0431\u0430\u043a\u0430 \u043b\u044e\u0431\u0438\u0442 \u0445\u043e\u0442\u044f\u0431\u044b \u0440\u0435\u0431\u0451\u043d\u043a\u0430 \u0438\u043b\u0438 \u043c\u0443\u0436\u0447\u0438\u043d \u0438\u043b\u0438 \u0436\u0435\u043d\u0449\u0438\u043d, \u0432 \u0441\u0442\u043e\u043b\u0431\u0446e likes_people \u044d\u0442\u043e \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c \u043e\u0442\u0440\u0430\u0436\u0435\u043d\u043e \u043a\u0430\u043a \"yes\". \u0415\u0441\u043b\u0438 \u043d\u0435\u0442, \u0442\u043e \u043c\u0435\u043d\u044f\u044e Nan \u043d\u0430 yes \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0432\u0443\u044e\u0449\u0438\u0445 \u0441\u0442\u0440\u043e\u043a\u0430\u0445","0721b27a":"## Data analysis","44c1ed24":"### RandomForest","10796ed4":"## Model","d2308198":"The advantage in the direction of \"yes\" by 3 times is quite logical. Dogs are more peaceful thanks to breeding. Now let's check the dependence of the target variable on the other features","d9417e28":"## Restoring the missing values to the target variable"}}