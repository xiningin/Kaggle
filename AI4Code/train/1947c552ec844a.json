{"cell_type":{"dc6fa45b":"code","d49c71ea":"code","262d6886":"code","0e2cf5ae":"code","56cd05b2":"code","2f9e56e8":"code","c4d4279f":"code","24d281bd":"code","c4e45a61":"code","76371d08":"markdown","393eea5a":"markdown","9b72d2b5":"markdown","f469e856":"markdown","e14ab331":"markdown"},"source":{"dc6fa45b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pylab as pl\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d49c71ea":"df = pd.read_csv(\"\/kaggle\/input\/fuelconsumption\/FuelConsumptionCo2.csv\")\n\n# take a look at the dataset\ndf.head()","262d6886":"#selecting relavent data\ncdf = df[['ENGINESIZE', 'CO2EMISSIONS']]\ncdf.head(5)","0e2cf5ae":"sns.scatterplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)","56cd05b2":"msk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]","2f9e56e8":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\ntrain_x = np.asanyarray(train[['ENGINESIZE']])\ntrain_y = np.asanyarray(train[['CO2EMISSIONS']])\n\ntest_x = np.asanyarray(test[['ENGINESIZE']])\ntest_y = np.asanyarray(test[['CO2EMISSIONS']])\n\n\npoly = PolynomialFeatures(degree=2)\ntrain_x_poly = poly.fit_transform(train_x)\ntrain_x_poly","c4d4279f":"clf = linear_model.LinearRegression()\ntrain_y_ = clf.fit(train_x_poly, train_y)\n# The coefficients\nprint ('Coefficients: ', clf.coef_)\nprint ('Intercept: ',clf.intercept_)","24d281bd":"axes = sns.scatterplot(x = \"ENGINESIZE\", y = \"CO2EMISSIONS\", data = cdf, ci = False)\nXX = np.arange(0.0, 10.0, 0.1)\nyy = clf.intercept_[0]+ clf.coef_[0][1]*XX+ clf.coef_[0][2]*np.power(XX, 2)\nplt.plot(XX, yy, '-r' )\nplt.xlabel(\"Engine size\")\nplt.ylabel(\"Emission\")","c4e45a61":"from sklearn.metrics import r2_score\n\ntest_x_poly = poly.fit_transform(test_x)\ntest_y_ = clf.predict(test_x_poly)\n\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(test_y_ - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_ - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y_ , test_y) )","76371d08":"### Evaluation","393eea5a":"It looks like feature sets for multiple linear regression analysis, right? Yes. It Does. \nIndeed, Polynomial regression is a special case of linear regression, with the main idea of how do you select your features. Just consider replacing the  $x$ with $x_1$, $x_1^2$ with $x_2$, and so on. Then the degree 2 equation would be turn into:\n\n$y = b + \\theta_1  x_1 + \\theta_2 x_2$\n\nNow, we can deal with it as 'linear regression' problem. Therefore, this polynomial regression is considered to be a special case of traditional\u00a0multiple linear regression. So, you can use the same mechanism as linear regression to solve such a problems. \n\n\n\nso we can use __LinearRegression()__ function to solve it:","9b72d2b5":"For without sklearn have a look at the 2nd half of here:\nhttps:\/\/github.com\/Benlau93\/Machine-Learning-by-Andrew-Ng-in-Python\/blob\/master\/Bias_Vs_Variance\/ML_BiasVsVariance.ipynb","f469e856":"Sometimes, the trend of data is not really linear, and looks curvy. In this case we can use Polynomial regression methods. In fact, many different regressions exist that can be used to fit whatever the dataset looks like, such as quadratic, cubic, and so on, and it can go on and on to infinite degrees.\n\nIn essence, we can call all of these, polynomial regression, where the relationship between the\u00a0independent variable\u00a0x\u00a0and the\u00a0dependent variable\u00a0y\u00a0is modeled as an\u00a0nth degree\u00a0polynomial\u00a0in\u00a0x. Lets say you want to have a polynomial regression (let's make 2 degree polynomial):\n\n\n$y = b + \\theta_1  x + \\theta_2 x^2$\n\nNow, the question is: how we can fit our data on this equation while we have only x values, such as __Engine Size__? \nWell, we can create a few additional features: 1, $x$, and $x^2$.\n\n\n\n__PloynomialFeatures()__ function in Scikit-learn library, drives a new feature sets from the original feature set. That is, a matrix will be generated consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, lets say the original feature set has only one feature, _ENGINESIZE_. Now, if we select the degree of the polynomial to be 2, then it generates 3 features, degree=0, degree=1 and degree=2: ","e14ab331":"**fit_transform** takes our x values, and output a list of our data raised from power of 0 to power of 2 (since we set the degree of our polynomial to 2).\n\n$\n\\begin{bmatrix}\n    v_1\\\\\n    v_2\\\\\n    \\vdots\\\\\n    v_n\n\\end{bmatrix}\n$\n$\\longrightarrow$\n$\n\\begin{bmatrix}\n    [ 1 & v_1 & v_1^2]\\\\\n    [ 1 & v_2 & v_2^2]\\\\\n    \\vdots & \\vdots & \\vdots\\\\\n    [ 1 & v_n & v_n^2]\n\\end{bmatrix}\n$\n\nin our example\n\n$\n\\begin{bmatrix}\n    2.\\\\\n    2.4\\\\\n    1.5\\\\\n    \\vdots\n\\end{bmatrix}\n$\n$\\longrightarrow$\n$\n\\begin{bmatrix}\n    [ 1 & 2. & 4.]\\\\\n    [ 1 & 2.4 & 5.76]\\\\\n    [ 1 & 1.5 & 2.25]\\\\\n    \\vdots & \\vdots & \\vdots\\\\\n\\end{bmatrix}\n$"}}