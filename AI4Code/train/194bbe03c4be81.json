{"cell_type":{"a65bd852":"code","2cabc838":"code","c7130ffa":"code","5b18d3e3":"code","6d313306":"code","55f0befa":"code","6a45a50d":"code","318afa57":"code","aa3c4c34":"code","848368a6":"code","10ba6d9a":"code","daf47c7b":"code","a6f9166f":"code","ebd64397":"code","d623563b":"code","19b10970":"code","7a1b47c8":"code","cadec9ed":"code","198e5781":"code","a0eb6f6e":"code","11cd9af7":"code","60b3e12d":"code","c130d35e":"code","2073a4db":"code","0afb0ce3":"code","9a7144b5":"code","1052f2c3":"markdown","264e6c4a":"markdown"},"source":{"a65bd852":"# Python \u22653.5 is required\nimport sys\n\n# Common imports\nimport numpy as np\nimport os\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)","2cabc838":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = iris[\"target\"]","c7130ffa":"setosa_or_versicolor = (y == 0) | (y == 1)\nX = X[setosa_or_versicolor]\ny = y[setosa_or_versicolor]\n","5b18d3e3":"# SVM Classifier model\nsvm_clf = SVC(kernel=\"linear\", C=float(\"inf\"))\nsvm_clf.fit(X, y)","6d313306":"# Bad models\nx0 = np.linspace(0, 5.5, 200)\npred_1 = 5*x0 - 20\npred_2 = x0 - 1.8\npred_3 = 0.1 * x0 + 0.5","55f0befa":"\ndef plot_svc_decision_boundary(svm_clf, xmin, xmax):\n    w = svm_clf.coef_[0]\n    b = svm_clf.intercept_[0]\n\n    # At the decision boundary, w0*x0 + w1*x1 + b = 0\n    # => x1 = -w0\/w1 * x0 - b\/w1\n    x0 = np.linspace(xmin, xmax, 200)\n    decision_boundary = -w[0]\/w[1] * x0 - b\/w[1]\n\n    margin = 1\/w[1]\n    gutter_up = decision_boundary + margin\n    gutter_down = decision_boundary - margin\n\n    svs = svm_clf.support_vectors_\n    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n    plt.plot(x0, gutter_down, \"k--\", linewidth=2)","6a45a50d":"fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(x0, pred_1, \"g--\", linewidth=2)\nplt.plot(x0, pred_2, \"m-\", linewidth=2)\nplt.plot(x0, pred_3, \"r-\", linewidth=2)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris versicolor\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris setosa\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\n\nplt.sca(axes[1])\nplot_svc_decision_boundary(svm_clf, 0, 5.5)\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.axis([0, 5.5, 0, 2])\nplt.show()","318afa57":"Xs = np.array([[1, 50], [5, 20], [3, 80], [5, 60]]).astype(np.float64)\nys = np.array([0, 0, 1, 1])\nsvm_clf = SVC(kernel=\"linear\", C=100)\nsvm_clf.fit(Xs, ys)\n\nplt.figure(figsize=(9,2.7))\nplt.subplot(121)\nplt.plot(Xs[:, 0][ys==1], Xs[:, 1][ys==1], \"bo\")\nplt.plot(Xs[:, 0][ys==0], Xs[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf, 0, 6)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.ylabel(\"$x_1$\u00a0\u00a0\u00a0\u00a0\", fontsize=20, rotation=0)\nplt.title(\"Unscaled\", fontsize=16)\nplt.axis([0, 6, 0, 90])\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(Xs)\nsvm_clf.fit(X_scaled, ys)\n\nplt.subplot(122)\nplt.plot(X_scaled[:, 0][ys==1], X_scaled[:, 1][ys==1], \"bo\")\nplt.plot(X_scaled[:, 0][ys==0], X_scaled[:, 1][ys==0], \"ms\")\nplot_svc_decision_boundary(svm_clf, -2, 2)\nplt.xlabel(\"$x_0$\", fontsize=20)\nplt.ylabel(\"$x'_1$  \", fontsize=20, rotation=0)\nplt.title(\"Scaled\", fontsize=16)\nplt.axis([-2, 2, -2, 2])\nplt.show()","aa3c4c34":"X_outliers = np.array([[3.4, 1.3], [3.2, 0.8]])\ny_outliers = np.array([0, 0])\nXo1 = np.concatenate([X, X_outliers[:1]], axis=0)\nyo1 = np.concatenate([y, y_outliers[:1]], axis=0)\nXo2 = np.concatenate([X, X_outliers[1:]], axis=0)\nyo2 = np.concatenate([y, y_outliers[1:]], axis=0)\n\nsvm_clf2 = SVC(kernel=\"linear\", C=10**9)\nsvm_clf2.fit(Xo2, yo2)\n\nfig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(Xo1[:, 0][yo1==1], Xo1[:, 1][yo1==1], \"bs\")\nplt.plot(Xo1[:, 0][yo1==0], Xo1[:, 1][yo1==0], \"yo\")\nplt.text(0.3, 1.0, \"Impossible!\", fontsize=24, color=\"red\")\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[0][0], X_outliers[0][1]),\n             xytext=(2.5, 1.7),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\n\nplt.sca(axes[1])\nplt.plot(Xo2[:, 0][yo2==1], Xo2[:, 1][yo2==1], \"bs\")\nplt.plot(Xo2[:, 0][yo2==0], Xo2[:, 1][yo2==0], \"yo\")\nplot_svc_decision_boundary(svm_clf2, 0, 5.5)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.annotate(\"Outlier\",\n             xy=(X_outliers[1][0], X_outliers[1][1]),\n             xytext=(3.2, 0.08),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=16,\n            )\nplt.axis([0, 5.5, 0, 2])\nplt.show()","848368a6":"iris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)]  # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n\nsvm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n    ])\n\nsvm_clf.fit(X, y)","10ba6d9a":"svm_clf.predict([[5.5, 1.7]])","daf47c7b":"scaler = StandardScaler()\nsvm_clf1 = LinearSVC(C=1, loss=\"hinge\", random_state=42)\nsvm_clf2 = LinearSVC(C=100, loss=\"hinge\", random_state=42)\n\nscaled_svm_clf1 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf1),\n    ])\nscaled_svm_clf2 = Pipeline([\n        (\"scaler\", scaler),\n        (\"linear_svc\", svm_clf2),\n    ])\n\nscaled_svm_clf1.fit(X, y)\nscaled_svm_clf2.fit(X, y)","a6f9166f":"# Convert to unscaled parameters\nb1 = svm_clf1.decision_function([-scaler.mean_ \/ scaler.scale_])\nb2 = svm_clf2.decision_function([-scaler.mean_ \/ scaler.scale_])\nw1 = svm_clf1.coef_[0] \/ scaler.scale_\nw2 = svm_clf2.coef_[0] \/ scaler.scale_\nsvm_clf1.intercept_ = np.array([b1])\nsvm_clf2.intercept_ = np.array([b2])\nsvm_clf1.coef_ = np.array([w1])\nsvm_clf2.coef_ = np.array([w2])\n\n# Find support vectors (LinearSVC does not do this automatically)\nt = y * 2 - 1\nsupport_vectors_idx1 = (t * (X.dot(w1) + b1) < 1).ravel()\nsupport_vectors_idx2 = (t * (X.dot(w2) + b2) < 1).ravel()\nsvm_clf1.support_vectors_ = X[support_vectors_idx1]\nsvm_clf2.support_vectors_ = X[support_vectors_idx2]","ebd64397":"fig, axes = plt.subplots(ncols=2, figsize=(10,2.7), sharey=True)\n\nplt.sca(axes[0])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\", label=\"Iris virginica\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\", label=\"Iris versicolor\")\nplot_svc_decision_boundary(svm_clf1, 4, 5.9)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.ylabel(\"Petal width\", fontsize=14)\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf1.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\n\nplt.sca(axes[1])\nplt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\nplt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\nplot_svc_decision_boundary(svm_clf2, 4, 5.99)\nplt.xlabel(\"Petal length\", fontsize=14)\nplt.title(\"$C = {}$\".format(svm_clf2.C), fontsize=16)\nplt.axis([4, 5.9, 0.8, 2.8])\nplt.show()","d623563b":"X1D = np.linspace(-4, 4, 9).reshape(-1, 1)\nX2D = np.c_[X1D, X1D**2]\ny = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10, 3))\n\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.plot(X1D[:, 0][y==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][y==1], np.zeros(5), \"g^\")\nplt.gca().get_yaxis().set_ticks([])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.2, 0.2])\n\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(X2D[:, 0][y==0], X2D[:, 1][y==0], \"bs\")\nplt.plot(X2D[:, 0][y==1], X2D[:, 1][y==1], \"g^\")\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"$x_2$\u00a0\u00a0\", fontsize=20, rotation=0)\nplt.gca().get_yaxis().set_ticks([0, 4, 8, 12, 16])\nplt.plot([-4.5, 4.5], [6.5, 6.5], \"r--\", linewidth=3)\nplt.axis([-4.5, 4.5, -1, 17])\n\nplt.subplots_adjust(right=1)\n\nplt.show()","19b10970":"from sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\ndef plot_dataset(X, y, axes):\n    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n    plt.axis(axes)\n    plt.grid(True, which='both')\n    plt.xlabel(r\"$x_1$\", fontsize=20)\n    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\nplt.show()","7a1b47c8":"polynomial_svm_clf = Pipeline([\n        (\"poly_features\", PolynomialFeatures(degree=3)),\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", random_state=42))\n    ])\n\npolynomial_svm_clf.fit(X, y)","cadec9ed":"def plot_predictions(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid(x0s, x1s)\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict(X).reshape(x0.shape)\n    y_decision = clf.decision_function(X).reshape(x0.shape)\n    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n\nplot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\nplot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n\nplt.show()","198e5781":"def gaussian_rbf(x, landmark, gamma):\n    return np.exp(-gamma * np.linalg.norm(x - landmark, axis=1)**2)\n\ngamma = 0.3\n\nx1s = np.linspace(-4.5, 4.5, 200).reshape(-1, 1)\nx2s = gaussian_rbf(x1s, -2, gamma)\nx3s = gaussian_rbf(x1s, 1, gamma)\n\nXK = np.c_[gaussian_rbf(X1D, -2, gamma), gaussian_rbf(X1D, 1, gamma)]\nyk = np.array([0, 0, 1, 1, 1, 1, 1, 0, 0])\n\nplt.figure(figsize=(10.5, 4))\n\nplt.subplot(121)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.scatter(x=[-2, 1], y=[0, 0], s=150, alpha=0.5, c=\"red\")\nplt.plot(X1D[:, 0][yk==0], np.zeros(4), \"bs\")\nplt.plot(X1D[:, 0][yk==1], np.zeros(5), \"g^\")\nplt.plot(x1s, x2s, \"g--\")\nplt.plot(x1s, x3s, \"b:\")\nplt.gca().get_yaxis().set_ticks([0, 0.25, 0.5, 0.75, 1])\nplt.xlabel(r\"$x_1$\", fontsize=20)\nplt.ylabel(r\"Similarity\", fontsize=14)\nplt.annotate(r'$\\mathbf{x}$',\n             xy=(X1D[3, 0], 0),\n             xytext=(-0.5, 0.20),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.text(-2, 0.9, \"$x_2$\", ha=\"center\", fontsize=20)\nplt.text(1, 0.9, \"$x_3$\", ha=\"center\", fontsize=20)\nplt.axis([-4.5, 4.5, -0.1, 1.1])\n\nplt.subplot(122)\nplt.grid(True, which='both')\nplt.axhline(y=0, color='k')\nplt.axvline(x=0, color='k')\nplt.plot(XK[:, 0][yk==0], XK[:, 1][yk==0], \"bs\")\nplt.plot(XK[:, 0][yk==1], XK[:, 1][yk==1], \"g^\")\nplt.xlabel(r\"$x_2$\", fontsize=20)\nplt.ylabel(r\"$x_3$\u00a0\u00a0\", fontsize=20, rotation=0)\nplt.annotate(r'$\\phi\\left(\\mathbf{x}\\right)$',\n             xy=(XK[3, 0], XK[3, 1]),\n             xytext=(0.65, 0.50),\n             ha=\"center\",\n             arrowprops=dict(facecolor='black', shrink=0.1),\n             fontsize=18,\n            )\nplt.plot([-0.1, 1.1], [0.57, -0.1], \"r--\", linewidth=3)\nplt.axis([-0.1, 1.1, -0.1, 1.1])\n    \nplt.subplots_adjust(right=1)\nplt.show()","a0eb6f6e":"x1_example = X1D[3, 0]\nfor landmark in (-2, 1):\n    k = gaussian_rbf(np.array([[x1_example]]), np.array([[landmark]]), gamma)\n    print(\"Phi({}, {}) = {}\".format(x1_example, landmark, k))","11cd9af7":"rbf_kernel_svm_clf = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001))\n    ])\nrbf_kernel_svm_clf.fit(X, y)","60b3e12d":"from sklearn.svm import SVC\n\ngamma1, gamma2 = 0.1, 5\nC1, C2 = 0.001, 1000\nhyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n\nsvm_clfs = []\nfor gamma, C in hyperparams:\n    rbf_kernel_svm_clf = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n        ])\n    rbf_kernel_svm_clf.fit(X, y)\n    svm_clfs.append(rbf_kernel_svm_clf)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n\nfor i, svm_clf in enumerate(svm_clfs):\n    plt.sca(axes[i \/\/ 2, i % 2])\n    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n    gamma, C = hyperparams[i]\n    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n    if i in (0, 1):\n        plt.xlabel(\"\")\n    if i in (1, 3):\n        plt.ylabel(\"\")\n\n\nplt.show()","c130d35e":"np.random.seed(42)\nm = 50\nX = 2 * np.random.rand(m, 1)\ny = (4 + 3 * X + np.random.randn(m, 1)).ravel()","2073a4db":"from sklearn.svm import LinearSVR\n\nsvm_reg = LinearSVR(epsilon=1.5, random_state=42)\nsvm_reg.fit(X, y)","0afb0ce3":"svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\nsvm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\nsvm_reg1.fit(X, y)\nsvm_reg2.fit(X, y)\n\ndef find_support_vectors(svm_reg, X, y):\n    y_pred = svm_reg.predict(X)\n    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n    return np.argwhere(off_margin)\n\nsvm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\nsvm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n\neps_x1 = 1\neps_y_pred = svm_reg1.predict([[eps_x1]])","9a7144b5":"def plot_svm_regression(svm_reg, X, y, axes):\n    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n    y_pred = svm_reg.predict(x1s)\n    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n    plt.plot(X, y, \"bo\")\n    plt.xlabel(r\"$x_1$\", fontsize=18)\n    plt.legend(loc=\"upper left\", fontsize=18)\n    plt.axis(axes)\n\nfig, axes = plt.subplots(ncols=2, figsize=(9, 4), sharey=True)\nplt.sca(axes[0])\nplot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\nplt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n#plt.plot([eps_x1, eps_x1], [eps_y_pred, eps_y_pred - svm_reg1.epsilon], \"k-\", linewidth=2)\nplt.annotate(\n        '', xy=(eps_x1, eps_y_pred), xycoords='data',\n        xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n        textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5}\n    )\nplt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\nplt.sca(axes[1])\nplot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\nplt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\n\nplt.show()","1052f2c3":"### Regression","264e6c4a":"### Non-linear classification"}}