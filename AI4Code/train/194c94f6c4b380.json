{"cell_type":{"26bceb87":"code","3507ecd5":"code","64bbe962":"code","610513f6":"code","1b347a2d":"code","b32d14ff":"code","5439a94c":"code","4a565770":"code","8c55022a":"code","ac5c8459":"code","7218c794":"code","2282d030":"code","8821ccd2":"code","e5f25bb8":"code","129b830f":"code","7c90cd48":"code","41c8f5b7":"code","cc70a42f":"code","ef323eeb":"code","a85f938e":"code","de4b8ec6":"code","6804ae04":"code","d0c343e4":"code","c46d604e":"code","16f22fe8":"code","f55eda03":"code","dd5948ff":"code","79fdbaf5":"code","ac75c6c1":"code","7d36f4cc":"code","573938c2":"code","887e6725":"code","7c9571c0":"code","dc70cec3":"code","cbb4923a":"code","cdc30022":"code","b7f0b1db":"code","95d3fc45":"code","5b24d50a":"code","a9ab3c62":"code","75290422":"code","8fe5b876":"code","f8f18d51":"code","d8cf7998":"code","69e1c7ae":"code","8524df92":"code","f16faa01":"code","0dfdbfc6":"code","a5bf1447":"code","13296f4f":"code","1801ff7b":"code","4e20d0e8":"code","d1a86929":"code","cb045696":"code","226f83fc":"code","899bf416":"code","2b10785e":"code","53673ced":"code","b5a019e6":"code","6588e131":"code","e376453c":"code","95dca8db":"code","a5519ee8":"code","f79464cb":"code","d9c26978":"code","4921774f":"code","da603ace":"code","a2c9e991":"code","fd7a9ffd":"code","e6335ac9":"code","8a90b269":"code","5c402f57":"code","ff9dc137":"code","24420d27":"code","fc6206e5":"code","9bc80a4a":"code","40dc1e52":"code","0b62f462":"code","bff38f6c":"code","e0096985":"code","d7319b65":"code","c494c89e":"code","1bd483e3":"code","b711574d":"code","2b1ec0ee":"code","c1d5a824":"code","36df2fe6":"code","9b3b6cd9":"code","dd35b214":"markdown","fe8e6736":"markdown","1ea598b7":"markdown","6710ffff":"markdown","e9faafcb":"markdown","2a92f3e3":"markdown","1e05e3e6":"markdown","aeaef3a1":"markdown","a3cba090":"markdown","3b728452":"markdown","93e4460d":"markdown","9de26f1d":"markdown","8c13776d":"markdown","36594a62":"markdown","310f49f2":"markdown","05aa8ea4":"markdown","d025afbd":"markdown","a70e8005":"markdown","6579c9fc":"markdown","68d20120":"markdown","f51f22f4":"markdown","1b6f946c":"markdown","e150c6e4":"markdown"},"source":{"26bceb87":"# Data preprocessing and linear algebra\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy.stats import norm, skew, boxcox_normmax\nfrom scipy.special import boxcox1p\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom graphviz import Source\nfrom IPython.display import SVG, display, HTML\nstyle = \"<style>svg{width: 70% !important; height: 60% !important;} <\/style>\"\n\n# Tools for model stecking, cross-validation, error calculation\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\n# Machine Learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, Lasso, RidgeCV\nimport sklearn.linear_model as linear_model\nfrom sklearn.svm import SVR\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor","3507ecd5":"### Data load ###","64bbe962":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","610513f6":"### Data exploration ###","1b347a2d":"train.head()","b32d14ff":"test.head()","5439a94c":"train.shape","4a565770":"test.shape","8c55022a":"# Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']","ac5c8459":"# Now we can drop it\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","7218c794":"# Exploring number of unique observations in Target variable \nlen(train.SalePrice.unique())","2282d030":"### NB\n# It has 663 unique values. Difference between this competition and well-known Titanic is that previous \n# had only 2 unique values in target variable: 0 and 1.\n###","8821ccd2":"# Draw histogram and boxplot of SalePrice distr\n\n### NB\n# In descriptive statistics, a box plot or boxplot is a method for graphically depicting groups of \n# numerical data through their quartiles.\n###\n\n# If you prefer simple histogram.\n# sns.distplot(train['SalePrice']);\n\nf,ax = plt.subplots(1,2,figsize=(16,6))\nsns.distplot(train['SalePrice'],fit=norm,ax=ax[0])\nsns.boxplot(train['SalePrice'])\nplt.show()\n\n# Calculate and print skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))\nprint(\"--------------------------------------\")\nprint(train['SalePrice'].describe())","e5f25bb8":"# Draw probability plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","129b830f":"### Data Visualization ###","7c90cd48":"# Check correlation between target variable and independent variables","41c8f5b7":"# Draw correlation matrix\nplt.figure(figsize=(16,14))\nsns.heatmap(train.corr(),annot=False)","cc70a42f":"# Draw SalePrice' correlation matrix (zoomed heatmap)\n#saleprice correlation matrix\ncorrmat = train.corr()\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","ef323eeb":"# Order variables by correaltion with target variable:\ncorr = train.select_dtypes(include='number').corr()\nplt.figure(figsize=(16,6))\ncorr['SalePrice'].sort_values(ascending=False)[1:].plot(kind='bar')\nplt.tight_layout()","a85f938e":"# Analyse 'GrLivArea'\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0])\nplt.scatter(train['GrLivArea'],train['SalePrice'])\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()","de4b8ec6":"# We can see at the bottom right two with extremely large 'GrLivArea' that are of a low price. \n# These values are huge oultliers. Therefore, we can safely delete them.\n\ntrain.drop(train[train['GrLivArea']>4500].index,axis=0,inplace=True)","6804ae04":"# Check again after delete\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['GrLivArea'],ax=ax[0])\nplt.scatter(train['GrLivArea'],train['SalePrice'])\nplt.xlabel('GrLiveArea')\nplt.ylabel('SalePrice')\nplt.show()","d0c343e4":"# Analyse 'OverallQual'\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.boxplot(train['OverallQual'],ax=ax[0])\nplt.scatter(train['OverallQual'],train['SalePrice'])\nplt.xlabel('OverallQual')\nplt.ylabel('SalePrice')\nplt.show()","c46d604e":"# Lets study relationsheeps between variables\n\n# As we mentioned above while our first look at datasets, data in columns has different types: there are\n# both numeric and string.\n\n# Find numeric features\nnumeric_cols = train.select_dtypes(exclude='object').columns\nnumeric_cols_length = len(numeric_cols)  \n\nfig, axs = plt.subplots(ncols=2, nrows=0, figsize=(12, 120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\n\n# Skip 'Id' and 'SalePrice' features\nfor i in range(1,numeric_cols_length-1):\n    feature = numeric_cols[i]\n    plt.subplot(numeric_cols_length, 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', data=train)\n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n           \nplt.show()","16f22fe8":"f,ax = plt.subplots(1,3,figsize=(16,4))\nsns.scatterplot('GrLivArea','TotRmsAbvGrd',data=train,ax=ax[0])\nsns.scatterplot('TotalBsmtSF','1stFlrSF',data=train,ax = ax[1])\nsns.scatterplot('GarageCars','GarageArea',data=train,ax = ax[2])\nplt.show()","f55eda03":"f,ax = plt.subplots(1,3,figsize=(16,4))\nsns.scatterplot(x='TotalBsmtSF', y='SalePrice',data=train,ax=ax[0])\nsns.scatterplot(x='LotArea', y='SalePrice',data=train,ax=ax[1])\nsns.scatterplot(x='OverallQual', y='SalePrice',data=train,ax=ax[2])\nplt.show()","dd5948ff":"# After removing the two outliers, we see that skewness is reduced. \n# But of course still Saleprice is not normally distributed.\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.distplot(train['SalePrice'],ax=ax[0],fit=norm)\nstats.probplot(train['SalePrice'],plot=plt)\nplt.show()\n\n# Calculate and print skewness and kurtosis\nprint(\"Skewness: {}\".format(train['SalePrice'].skew()))\nprint(\"Kurtosis: {}\".format(train['SalePrice'].kurt()))","79fdbaf5":"# We need to normalize the distribution.\n# Lets follow a well known approach - Log-transformation [5].\n\ny = np.log1p(train['SalePrice'])\n\nf,ax = plt.subplots(1,2,figsize=(16,4))\nsns.distplot(y,fit=norm,ax=ax[0])\nstats.probplot(y,plot=plt)\nplt.show()\n\n# Calculate and print skewness and kurtosis\nprint(\"Skewness: {}\".format(y.skew()))\nprint(\"Kurtosis: {}\".format(y.kurt()))","ac75c6c1":"# Move on. To remove overfitted features lets use iterative approach suggested in [3]\ndef remove_overfit_features(df,weight):\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > weight:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit","7d36f4cc":"# Call a function with our train data as argument\noverfitted_features = remove_overfit_features(train,99)\ntrain.drop(overfitted_features,inplace=True,axis=1)\ntest.drop(overfitted_features,inplace=True,axis=1)","573938c2":"# Lets form training subset 'y' used in ML fit later\ntrain_labels = y\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","887e6725":"train.head()","7c9571c0":"# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset.\n\n### NB\n# This approach used in [2] and [3]. Of course, as some guys mensioned in comments, doing so may cause\n# data leakage (as well as applying Box Cox on combined data).\n###","dc70cec3":"# ... so we combine them.\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)","cbb4923a":"all_features.shape","cdc30022":"# Check missing data","b7f0b1db":"# Count missing data\ntotal = all_features.isnull().sum().sort_values(ascending=False)\npercent = (all_features.isnull().sum()\/all_features.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","95d3fc45":"missing_data.head(40)","5b24d50a":"# Visualize missing data\nsns.heatmap(train.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","a9ab3c62":"# And in test data\nsns.heatmap(test.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","75290422":"# Then visualise combined data\n\n### NB\n# If we do it later after feature engneering, we can check if there are left some missings\n###\nsns.heatmap(all_features.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","8fe5b876":"### Data preprocessing ###","f8f18d51":"# Count amount of missing data before we begin to fill them\nprint(\"Total No. of missing value {} before imputation\".format(sum(all_features.isnull().sum())))","d8cf7998":"# Begin fill them according to [3]\n# In [3] author uses 'Timber' value to fill 'Electrical'. Since there is only one cell with missing value\n# it's not critically important, so lets just left [3] original style\n\ndef fill_missing_values():\n \n    fillSaleType = all_features[all_features['SaleCondition'] == 'Normal']['SaleType'].mode()[0]\n    all_features['SaleType'].fillna(fillSaleType,inplace=True)\n\n    fillElectrical = all_features[all_features['Neighborhood']=='Timber']['Electrical'].mode()[0]\n    all_features['Electrical'].fillna(fillElectrical,inplace=True)\n\n    exterior1_neighbor = all_features[all_features['Exterior1st'].isnull()]['Neighborhood'].values[0]\n    fillExterior1 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior1st'].fillna(fillExterior1,inplace=True)\n\n    exterior2_neighbor = all_features[all_features['Exterior2nd'].isnull()]['Neighborhood'].values[0]\n    fillExterior2 = all_features[all_features['Neighborhood'] == exterior1_neighbor]['Exterior1st'].mode()[0]\n    all_features['Exterior2nd'].fillna(fillExterior2,inplace=True)\n\n    bsmtNeigh = all_features[all_features['BsmtFinSF1'].isnull()]['Neighborhood'].values[0]\n    fillBsmtFinSf1 = all_features[all_features['Neighborhood'] == bsmtNeigh]['BsmtFinSF1'].mode()[0]\n    all_features['BsmtFinSF1'].fillna(fillBsmtFinSf1,inplace=True)\n\n    kitchen_grade = all_features[all_features['KitchenQual'].isnull()]['KitchenAbvGr'].values[0]\n    fillKitchenQual = all_features[all_features['KitchenAbvGr'] == kitchen_grade]['KitchenQual'].mode()[0]\n    all_features['KitchenQual'].fillna(fillKitchenQual,inplace=True)\n    \n    # Groupby MSSubClass and fill in missing value by 0 MSZoning of all theMSSuClass\n    all_features['MSZoning'] = all_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: \n                                                                                        x.fillna(x.mode()[0]))\n    \n    # Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\n    all_features['LotFrontage'] = all_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: \n                                                                                        x.fillna(x.median()))\n    \n    \n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2','PoolQC']:\n        all_features[col] = all_features[col].fillna('None')\n    \n    categorical_cols =  all_features.select_dtypes(include='object').columns\n    all_features[categorical_cols] = all_features[categorical_cols].fillna('None')\n    \n    numeric_cols = all_features.select_dtypes(include='number').columns\n    all_features[numeric_cols] = all_features[numeric_cols].fillna(0)\n    \n    all_features['Shed'] = np.where(all_features['MiscFeature']=='Shed', 1, 0)\n    \n    # GarageYrBlt: missing values are for building which has no Garage. Imputing 0 makes \n    # huge difference with other buildings, imputing mean doesn't make sense since there is no Garage. \n    # We can drop it.\n    all_features.drop(['GarageYrBlt','MiscFeature'],inplace=True,axis=1)\n    \n    all_features['QualitySF'] = all_features['GrLivArea'] * all_features['OverallQual']\n","69e1c7ae":"# Call a function\nfill_missing_values()","8524df92":"# Check after imputing\nprint(\"Total No. of missing value {} after imputation\".format(sum(all_features.isnull().sum())))","f16faa01":"# And finally drop 'PoolQC'\nall_features = all_features.drop(['PoolQC',], axis=1)","0dfdbfc6":"# Fixing skewed features","a5bf1447":"# Start with convertion of numeric features to string format\nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)","13296f4f":"# And continue with filling the skewed features\nnumeric = all_features.select_dtypes(include='number').columns\nskew_features = all_features[numeric].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_features[skew_features > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_features.head(10)","1801ff7b":"# Normalize skewed features using BoxCox transform [4]\nfor i in skew_index:\n    all_features[i] = boxcox1p(all_features[i], boxcox_normmax(all_features[i] + 1))","4e20d0e8":"all_features['YearsSinceRemodel'] = all_features['YrSold'].astype(int) - all_features['YearRemodAdd'].astype(int)\nall_features['Total_Home_Quality'] = all_features['OverallQual'] + all_features['OverallCond']\n\nall_features['TotalSF'] = all_features['TotalBsmtSF'] + all_features['1stFlrSF'] + all_features['2ndFlrSF']\nall_features['YrBltAndRemod'] = all_features['YearBuilt'] + all_features['YearRemodAdd']\nall_features['BsmtFinType1_Unf'] = 1*(all_features['BsmtFinType1'] == 'Unf')\nall_features['Total_sqr_footage'] = (all_features['BsmtFinSF1'] + all_features['BsmtFinSF2'] +\n                                 all_features['1stFlrSF'] + all_features['2ndFlrSF'])\nall_features['Total_Bathrooms'] = (all_features['FullBath'] + (0.5 * all_features['HalfBath']) +\n                               all_features['BsmtFullBath'] + (0.5 * all_features['BsmtHalfBath']))\nall_features['Total_porch_sf'] = (all_features['OpenPorchSF'] + all_features['3SsnPorch'] +\n                              all_features['EnclosedPorch'] + all_features['ScreenPorch'] +\n                              all_features['WoodDeckSF'])\n\nall_features['TotalBsmtSF'] = all_features['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['2ndFlrSF'] = all_features['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\nall_features['GarageArea'] = all_features['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\nall_features['BsmtFinSF1'] = all_features['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)","d1a86929":"def booleanFeatures(columns):\n    for col in columns:\n        all_features[col+\"_bool\"] = all_features[col].apply(lambda x: 1 if x > 0 else 0)\nbooleanFeatures(['GarageArea','TotalBsmtSF','2ndFlrSF','Fireplaces','WoodDeckSF',\n                 'OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch'])  ","cb045696":"def logs(columns):\n    for col in columns:\n        all_features[col+\"_log\"] = np.log(1.01+all_features[col])  \n\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                 'TotalBsmtSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                 'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                 'TotRmsAbvGrd','Fireplaces','GarageCars','WoodDeckSF','OpenPorchSF',\n                 'EnclosedPorch','3SsnPorch','ScreenPorch','MiscVal','YearRemodAdd','TotalSF']\n\nlogs(log_features)","226f83fc":"def squares(columns):\n    for col in columns:\n        all_features[col+\"_sq\"] =  all_features[col] * all_features[col]\n\nsquared_features = ['GarageCars_log','YearRemodAdd', 'LotFrontage_log', \n                    'TotalBsmtSF_log', '2ndFlrSF_log', 'GrLivArea_log' ]\n\nsquares(squared_features)","899bf416":"# After new featres created we can pass to work with dummy variables.\n# We have to turn them into int format since ML algorhytms don't accept strings\n\n# Get dummies\nall_features = pd.get_dummies(all_features).reset_index(drop=True)","2b10785e":"all_features.head()","53673ced":"# So now after all we are ready to form our 'X' subset used in ML fit\nX = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\n\noutliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ntrain_labels = train_labels.drop(y.index[outliers])","b5a019e6":"### Machine Learning ###","6588e131":"# Split dataset into k consecutive folds (without shuffling by default).\nkf = KFold(n_splits=12, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, train_labels, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","e376453c":"# Define basic models","95dca8db":"### NB\n# We will use RobustScaler method in case of models wwhich are very sensitive to  otliers. Make then more robust.\n###","a5519ee8":"# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))","f79464cb":"# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))","d9c26978":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","4921774f":"# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)","da603ace":"# Lasso Regressor\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","a2c9e991":"# StackingCVRegressor \nstackReg = StackingCVRegressor(regressors=(lasso, svr, ridge, lightgbm),\n                                meta_regressor=gbr,\n                                use_features_in_secondary=True,random_state=42)","fd7a9ffd":"# Base models score","e6335ac9":"model_score = {}\n\nscore = cv_rmse(lightgbm)\nlgb_model_full_data = lightgbm.fit(X, train_labels)\nprint(\"lightgbm: {:.4f}\".format(score.mean()))\nmodel_score['lgb'] = score.mean()","8a90b269":"score = cv_rmse(lasso)\nlasso_model_full_data = lasso.fit(X, train_labels)\nprint(\"lasso: {:.4f}\".format(score.mean()))\nmodel_score['lasso'] = score.mean()","5c402f57":"score = cv_rmse(svr)\nsvr_model_full_data = svr.fit(X, train_labels)\nprint(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nmodel_score['svr'] = score.mean()","ff9dc137":"score = cv_rmse(ridge)\nridge_model_full_data = ridge.fit(X, train_labels)\nprint(\"ridge: {:.4f}\".format(score.mean()))\nmodel_score['ridge'] =  score.mean()","24420d27":"score = cv_rmse(gbr)\ngbr_model_full_data = gbr.fit(X, train_labels)\nprint(\"gbr: {:.4f}\".format(score.mean()))\nmodel_score['gbr'] =  score.mean()","fc6206e5":"# Stack models\nstack_reg_model = stackReg.fit(np.array(X), np.array(train_labels))","9bc80a4a":"# Make blended meta-model\ndef blended_predictions(X,weight):\n    return ((weight[0] * ridge_model_full_data.predict(X)) + \\\n            (weight[1] * svr_model_full_data.predict(X)) + \\\n            (weight[2] * gbr_model_full_data.predict(X)) + \\\n            (weight[3] * lasso_model_full_data.predict(X)) + \\\n            (weight[4] * lgb_model_full_data.predict(X)) + \\\n            (weight[5] * stack_reg_model.predict(np.array(X))))","40dc1e52":"# According to competition rules evaluation metod is RMSLE:\n# \"Submissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted \n# value and the logarithm of the observed sales price. (Taking logs means that errors in predicting \n# expensive houses and cheap houses will affect the result equally.)\"","0b62f462":"### NB\n# Root Mean Squared Error (RMSE) and Root Mean Squared Logarithmic Error (RMSLE) \n# both are the techniques to find out the difference between the values predicted by your machine \n# learning model and the actual values.\n###","bff38f6c":"# Blended model predictions\nblended_score = rmsle(train_labels, blended_predictions(X,[0.15,0.2,0.1,0.15,0.1,0.3]))\nprint(\"blended score: {:.4f}\".format(blended_score))\nmodel_score['blended_model'] =  blended_score","e0096985":"### NB\n# Weignhts are empirycally choosen according to errors rate by using this logic: the higher weights is given to model with\n# less error. Inspired by [3] approach.\n###","d7319b65":"pd.Series(model_score).sort_values(ascending=True)","c494c89e":"# Predictions","1bd483e3":"# Read sample submission csv\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\n# Add our predictions\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_test,[0.15,0.2,0.1,0.15,0.1,0.3])))","b711574d":"sample = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","2b1ec0ee":"# Write result to csv\nsubmission.to_csv('my_submission.csv', index=False)","c1d5a824":"# And finally, let\u2019s take a tired but satisfied look at our result\nmy_submission_check = pd.read_csv('my_submission.csv')","36df2fe6":"my_submission_check","9b3b6cd9":"# Literature\n# [1] https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python \n# [2] https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/comments\n# [3] https:\/\/www.kaggle.com\/johnwill225\/extensive-exploratory-data-analysis\/notebook\n# [4] https:\/\/en.wikipedia.org\/wiki\/Power_transform#Box%E2%80%93Cox_transformation\n# [5] https:\/\/en.wikipedia.org\/wiki\/Data_transformation_(statistics)","dd35b214":"What we can denote here:\n1. 'MSSubClass', 'MoSold', 'YrSold' - a category and description have same meaning.\n2. 'OverallQual', 'OverallCond' - are ordered value (i.e. scores or ratings).\n3. 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'Fireplaces', 'BedroomAbvGr', 'KitchenAvbGr' - are values.","fe8e6736":"According to this we can outline a few points:\n1. Target variable is not normally distributed.\n2. Its distribution curve is rightly skewed.\n3. Average sale price is 180921 USD.\n4. Median of sale price is 163000 USD. It is lower than Mean value.\n5. There are few outliers at the upper anr right corners.","1ea598b7":"We see a few interesting points.\n1. White squares are very strong correlated vars: 'TotalBsmtSF' and '1stFlrSF' variables, and 'GarageYrBlt', 'GarageCars', 'GarageArea'.\n2. 'SalePrice' correlations with 'GrLivArea', 'TotalBsmtSF', and 'OverallQual'.","6710ffff":"First of all we should split data into train an test splits in order to use cross-validation.  \nLets use sklearn's KFold cross-validator.","e9faafcb":"Now lets check out strong correlated vars and look that changed after we removed 2 outliers before","2a92f3e3":"In my opinion there are two points.  \n1. If we follow that notebooks and try predictiong by them, we can see quite well results. So in case if we do not extremally need 100% result, we can deal with it.\n2. Authors in [2, 3] use such approach to keep exactly same transforms on both data. If we work separately on train and test, we will have problem in fitting (on test) process: because of difference in missings algorhytms will not fit.  \nIn [1] author did absolutely perfect work on feature engineering, but unfortunately he didn't show way to correctly work on test dataset to keep it in same format and be able to fit on it.","1e05e3e6":"Selection of models and their tuning is a separate interesting section of Machine Learning. \nSo, for our task you can experiment a lot, selecting models and their combinations. \nBut based on the fact that our goal is to obtain an acceptable result, we will not go deep\ninto this point, but simply use 4 well-known regression methods the combined average prediction \nof which gives a very acceptable result, as we will see later. Also we will not do deep hypermarameter tuning except a few things.","aeaef3a1":"As we can see our Log-transformation did well and curve shape now looks better.","a3cba090":"We see that:\n1. 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n2. 'GarageCars' and 'GarageArea' are in first white square. They are very correlated. Its logical:\nthe number of cars that fit into the garage is a consequence of the garage area. We can use just one of them.\nLets keep 'GarageCars' since its correlation with 'SalePrice' is higher.\n3. 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers in second white square.\nLets keep just one of them 'TotalBsmtSF' (its no matter bacause they both have 0,61 correlation with Target var)\n4. 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again.","3b728452":"Looking on datatsets we can first see that 'Id' column is unnesessary for prediction.  \nThe second thing we can see: data has different format and many missing values, wherein\ntrain and test datasets have different amount of missings. This is quite an important point, because it determines the further way of working with data.","93e4460d":"We will use a Stacked model concept which is well described in [2].\nMain idea: to build several different models, train them, and then average their prediction in one meta-model.","9de26f1d":"Fix in mind amounts of columns and raws","8c13776d":"In order to clarify our dataset, we can create some new, but logically clear features which can\nhelp ML algorhytms.  \nFeature creation is done accordin to [3]. We'll follow authors idea about exp(6) rather than (6.5)\nin 'TotalBsmtSF' creation. It seems again not significant.","36594a62":"Let's analyse how the other independent variables correleated with the Target variable.","310f49f2":"Lets start with taking first look at our data","05aa8ea4":"Two main questions here are [1]:\n1. How prevalent is the missing data?\n2. Is missing data random or does it have a pattern?","d025afbd":"We didn't drop two outliers in right upper corner because they seem to be in linear (or exponential) trend.\nSo most probably they are not 'bad' outliers.","a70e8005":"Our decade is a time of big data, I believe this is known to everyone on this platform. But also, this is a time of open learning. Knowledge now grows like a pyramid, each next brick of which rests on the previous ones. The quintessence of such a paradigm is the Kaggle platform: I think that each of us here is studying machine learning, analyzing, disassembling and repeating notebooks of other participants. \n\nI believe that \"learning by doing\" is the most appropriate way to learn something, and more importantly, put it into practice. This notebook follows this approach. It relies on notebooks [1,2,3], whose authors have done a really great job for what I want to thank them and honestly upwote them.\n\nThere is no limit to perfection. Discussed here house price prediction problem, like any other, could be solved in a more elegant and precise way. But for educational purposes, our approach is still sufficient.\n\nLet us keep in mind that the main steps of any machine learning task are as follows.\n0. Analysis of the task.\n1. Downloading and extraction of data.\n2. Data exploration: understanding patterns, visualization, analysis.\n3. Data preprocessing.\n4. Construction and validation of the model.\n5. Prediction and uploading the result.\n\nWe will follow these steps in our notebook.\nA few comments.\n1. A number of points are left unchanged in the form in which they are proposed by authors [1,2,3]. This is done in order to maintain consistency, which in my personal experience is very useful for those who study the tasks and solutions of colleagues. At the same time, I tried to comment on a number of points on which I had questions or I chose a slightly different path from my colleagues. For the same reason, I decided to make the colors of graphs and charts identical to those works.\n2. Some notes are highlighted by a comment with the code \"Nota Bene, NB\". In such footnotes, as a rule, I provided a brief theoretical description of a particular function or statistical term. Nevertheless, I tried to be brief and in some cases simply indicated a link to Wikipedia.\n\nWell, let's start, and let's keep in mind: every great path begins with a small step.","6579c9fc":"[1] uses the following approach.\nWe'll consider that when more than 15% of the data is missing, we should delete the corresponding variable.\nAccording to this 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu' should be deleted.\n'GarageX' variables should also be deleted because they have just 5% data and we saw on corr matix that 'GarageCars' totally describes garage.\nThe same logic applies to 'BsmtX' variables.\nAlso will not lose information if we delete 'MasVnrArea' and 'MasVnrType' because hey have a strong correlation with 'YearBuilt' and 'OverallQual'.\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, \nwe'll delete this observation and keep the variable.\n\nThis is very logical way, but as we mensioned, author used only train data, so in order to get our prediction job done, we will follow [3] in question of work with missing data.","68d20120":"Very powerfull way to study variables and their relationship is graphical. We will draw some plots to investigate our target variable and independet variables having strong correlation with it.","f51f22f4":"Move on. From competition description we understand our Target variable. It is 'SalePrice'. Lets have a look on it.","1b6f946c":"First, lets outline our task.  \n\"It is your job to predict the sales price for each house. \nFor each Id in the test set, you must predict the value of the SalePrice variable.\"","e150c6e4":"To understand which columns need imputing and if so - with what value, lets work around data description.\nFollow [2] logic.  \n\nPoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value\n(+99%) and majority of houses have no Pool at all in general.  \nMiscFeature : data description says NA means \"no misc feature\"  \nAlley : data description says NA means \"no alley access\"  \nFence : data description says NA means \"no fence\"  \nFireplaceQu : data description says NA means \"no fireplace\"  \nLotFrontage : Since the area of each street connected to the house property most likely have a similar area   \nto other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood  \nGarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None  \nGarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)  \nBsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero \nfor having no basement  \nBsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related   \nfeatures, NaN means that there is no basement  \nMasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for \nthe area and None for the type  \nMSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill \nin missing values with 'RL'  \nUtilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA  \nSince the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. \nWe can then safely remove it.  \nFunctional : data description says NA means typical  \nElectrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value  \nKitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for \nthe missing value in KitchenQual.  \nExterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute \nin the most common string  \nSaleType : Fill in again with most frequent which is \"WD\"  \nMSSubClass : Na most likely means No building class. We can replace missing values with None  "}}