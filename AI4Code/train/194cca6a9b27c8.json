{"cell_type":{"6cecc53f":"code","1bdb7284":"code","2d98baaf":"code","1fcb742b":"code","25857b39":"code","dd2c9c7b":"code","168174b9":"code","f9ee3f9f":"code","eddce59a":"code","691e90cb":"code","234a80d4":"code","79a121ec":"code","8df61b02":"code","11ff0046":"code","708e8c57":"code","0fc940cb":"code","1dbec9e0":"code","bdfe0bf5":"code","0c113893":"code","ce9547e5":"code","a661703e":"code","1abae08c":"code","a9fa9962":"code","dde6efe2":"code","19b70d2a":"code","bf53e1b2":"code","136d2bbe":"code","966cb2e3":"code","acdfccee":"code","5510c28b":"markdown","d4a6ca9c":"markdown","11996c9a":"markdown","6a393838":"markdown","1e8ef659":"markdown","af506414":"markdown","4dcd88b0":"markdown","d7b926a9":"markdown","e1be08c8":"markdown","fcabc269":"markdown","58685f49":"markdown","e18a1ca4":"markdown","3701e10f":"markdown","d925372d":"markdown","7715f26c":"markdown","b0bf800b":"markdown","bb6fbc52":"markdown","06541989":"markdown","f05d6832":"markdown","25db6e7c":"markdown","e0a0d3bd":"markdown","4875ab73":"markdown","72e95b1d":"markdown","032b4a25":"markdown","5629491f":"markdown"},"source":{"6cecc53f":"import os\nfrom pathlib import Path\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, models, Model\n\nDOWNLOAD_ROOT = \"http:\/\/vision.stanford.edu\/aditya86\/ImageNetDogs\/\"\nFILENAME = \"images.tar\"\nfilepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\ndata_dir = Path(filepath).parent \/ \"Images\"\ndata_dir","1bdb7284":"class_names = os.listdir(data_dir)\nn_classes = len(os.listdir(data_dir))\n\nn_images = 0\nfor i in range(n_classes):\n    n_images += len(os.listdir(data_dir \/ class_names[i]))\nprint(\"Number of images: \", n_images)\nprint(\"Number of classes: \", n_classes)","2d98baaf":"def filepaths(data_dir, test_ratio=0.2, validation_ratio=0.2):\n    test_ratio = test_ratio\n    validation_ration = validation_ratio\n\n    train_filepaths = []\n    valid_filepaths = []\n    test_filepaths = []\n\n    path = os.path.join(\"\/root\/.keras\/datasets\/\", \"Images\/\" )\n    for i in range(n_classes):\n        img_per_cat = os.listdir(data_dir \/ class_names[i])\n        img_per_cat = np.array([os.path.join(class_names[i], s) for s in img_per_cat])\n        \n        total_size = len(img_per_cat) # ex. 157\n        test_size = int(total_size * test_ratio) # ex. 31\n        validation_size = int(total_size * validation_ratio) # ex. 31\n        train_size = total_size - test_size - validation_size # ex. 95\n\n        rnd_indices = np.random.permutation(total_size)\n\n        train_filepaths.append(img_per_cat[rnd_indices[:train_size]])\n        valid_filepaths.append(img_per_cat[rnd_indices[train_size:-test_size]])\n        test_filepaths.append(img_per_cat[rnd_indices[-test_size:]])\n\n    train_filepaths = np.array([path + s for s in np.hstack(train_filepaths)])\n    valid_filepaths = np.array([path + s for s in np.hstack(valid_filepaths)])\n    test_filepaths = np.array([path + s for s in np.hstack(test_filepaths)])\n\n    return (train_filepaths, valid_filepaths, test_filepaths)\n\n(train_filepaths, valid_filepaths, test_filepaths) = filepaths(data_dir)\ntrain_filepaths.shape, valid_filepaths.shape, test_filepaths.shape","1fcb742b":"# In the last part I said that we will put a Normalization layer within the model\n# to simplify deployment but this slow down training too much so this is why I put \n# the Rescaling layer in tf.data\n\nnorm_layer = layers.experimental.preprocessing.Rescaling(1.\/255)\n\ndef get_label(file_path):\n    \"\"\"Takes the label from the file path\"\"\"\n    parts = tf.strings.split(file_path, os.path.sep)\n    one_hot = parts[-2] == class_names\n    one_hot = tf.cast(one_hot, tf.float32)\n    return tf.argmax(one_hot)\n\ndef preprocess_xception(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [224, 224])\n    final_img = keras.applications.xception.preprocess_input(img)\n    return img, label\n\ndef preprocess_mobilenet(file_path):\n    label = get_label(file_path)\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [224, 224])\n    final_img = keras.applications.mobilenet_v2.preprocess_input(img)\n    return img, label\n\ndef create_dataset(filepaths, preprocess, batch_size=32, buffer_size=1000):\n    ds = tf.data.Dataset.list_files(filepaths, seed=42)\n    ds = ds.map(preprocess, num_parallel_calls=5)\n    ds = ds.map(lambda x, y: (norm_layer(x), y), num_parallel_calls=5)\n    ds = ds.shuffle(buffer_size, seed=42)\n    ds = ds.batch(batch_size)\n    return ds.prefetch(1)","25857b39":"train_set_xception = create_dataset(train_filepaths, preprocess_xception) \nvalid_set_xception = create_dataset(valid_filepaths, preprocess_xception)\ntest_set_xception = create_dataset(test_filepaths, preprocess_xception)","dd2c9c7b":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nbase_model_xception = tf.keras.applications.Xception(weights=\"imagenet\", include_top=False)\n\n# Create a new model on top\navg = layers.GlobalAveragePooling2D()(base_model_xception.output)\ndense = layers.Dense(128, activation='relu')(avg)\noutput = layers.Dense(n_classes, activation=\"softmax\")(dense)\nxception_model = Model(inputs=base_model_xception.input, outputs=output)\n\n# Freeze the weights of the base model\nfor layer in base_model_xception.layers:\n    layer.trainable = False\n\n# Compile the model and start training\nxception_model.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n              loss=keras.losses.SparseCategoricalCrossentropy(),\n              metrics=[\"accuracy\"])\nxception_model.summary()","168174b9":"import os \n\nroot_logdir = os.path.join(os.curdir, \"logs\")\n\ndef get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n    return os.path.join(root_logdir, run_id)\n\nrun_logdir = get_run_logdir()\n\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=5,\n                                                  restore_best_weights=True)\nepochs = 40","f9ee3f9f":"history_xception = xception_model.fit(train_set_xception, epochs=epochs, \n                                      validation_data=valid_set_xception, \n                                      callbacks=[early_stopping_cb, tensorboard_cb])\n# Let's save the model before we do fine-tuning\nxception_model.save(\"xception_without_fine_tuning.h5\")","eddce59a":"def plot_learning_curves(history):\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(20, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(accuracy, label='Training Accuracy')\n    plt.plot(val_accuracy, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('epoch')\n    plt.grid(True)\n\n    plt.subplot(1, 2, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.grid(True)\n    plt.show()\n    \nplot_learning_curves(history_xception)","691e90cb":"train_set_mobilenet = create_dataset(train_filepaths, preprocess_mobilenet) \nvalid_set_mobilenet = create_dataset(valid_filepaths, preprocess_mobilenet)\ntest_set_mobilenet = create_dataset(test_filepaths, preprocess_mobilenet)","234a80d4":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nbase_model_mobilenet = tf.keras.applications.MobileNetV2(\n                                    weights=\"imagenet\", include_top=False)\n\n# Create a new model on top\navg = layers.GlobalAveragePooling2D()(base_model_mobilenet.output)\ndense1 = layers.Dense(128, activation='relu')(avg)\ndense2 = layers.Dense(64, activation='relu')(dense1)\noutput = layers.Dense(n_classes, activation=\"softmax\")(dense2)\nmobilenet_model = Model(inputs=base_model_mobilenet.input, outputs=output)\n\n# Freeze the weights of the base model\nfor layer in base_model_mobilenet.layers:\n    layer.trainable = False\n\n# Compile the model and start training\nmobilenet_model.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n              loss=keras.losses.SparseCategoricalCrossentropy(),\n              metrics=[\"accuracy\"])\nmobilenet_model.summary()","79a121ec":"history_mobilenet = mobilenet_model.fit(train_set_mobilenet, epochs=epochs, \n                                        validation_data=valid_set_mobilenet, \n                                        callbacks=[early_stopping_cb, tensorboard_cb])","8df61b02":"plot_learning_curves(history_mobilenet)","11ff0046":"data_augmentation = keras.Sequential([\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.2),\n    layers.experimental.preprocessing.RandomContrast(0.2),\n    layers.experimental.preprocessing.RandomFlip('horizontal'),\n])","708e8c57":"plt.figure(figsize=(15, 10))\nfor images, labels in train_set_mobilenet.take(1):\n    for i in range(9):\n        augmented_image = data_augmentation(images)\n        plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_image[0])\n        plt.title(\"Class: \" + class_names[labels[0]][10:])\n        plt.axis(\"off\")","0fc940cb":"keras.backend.clear_session()\ntf.random.set_seed(42)\nnp.random.seed(42)\n\nbase_model_mobilenet = tf.keras.applications.MobileNetV2(\n                                    weights=\"imagenet\", include_top=False)\n\n# Create a new model on top\ninputs = keras.Input(shape=(224, 224, 3))\nx = data_augmentation(inputs)\nx = base_model_mobilenet(x, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512, activation='relu')(x)\nx = layers.Dropout(0.3)(x)\noutputs = layers.Dense(n_classes, activation=\"softmax\")(x)\nmobilenet_model = Model(inputs=inputs, outputs=outputs)\n\n# Freeze the weights of the base model\nfor layer in base_model_mobilenet.layers:\n    layer.trainable = False\n\n# Compile the model and start training\nmobilenet_model.compile(optimizer=keras.optimizers.Adam(lr=1e-4),\n              loss=keras.losses.SparseCategoricalCrossentropy(),\n              metrics=[\"accuracy\"])\nmobilenet_model.summary()","1dbec9e0":"history_mobilenet = mobilenet_model.fit(train_set_mobilenet, epochs=epochs, \n                                        validation_data=valid_set_mobilenet, \n                                        callbacks=[early_stopping_cb, tensorboard_cb])\n# Let's save the model before we do fine-tuning\nmobilenet_model.save(\"mobilenet_without_fine_tuning.h5\")","bdfe0bf5":"plot_learning_curves(history_mobilenet)","0c113893":"print(len(base_model_mobilenet.layers))\n\nbase_model_mobilenet.trainable = True\n# Freeze the first 100 layers\nfor layer in base_model_mobilenet.layers[:100]:\n  layer.trainable =  False","ce9547e5":"# Compile the model and start training\nmobilenet_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), # low learning rate\n              loss=keras.losses.SparseCategoricalCrossentropy(),\n              metrics=[\"accuracy\"])\n\nhistory_mobilenet = mobilenet_model.fit(train_set_mobilenet, epochs=epochs, \n                                        validation_data=valid_set_mobilenet, \n                                        callbacks=[early_stopping_cb, tensorboard_cb])","a661703e":"# Evaluate the final model on the test set\nfinal_model = keras.models.load_model(\"mobilenet_without_fine_tuning.h5\")\nloss, accuracy = final_model.evaluate(test_set_mobilenet)\nprint('Test accuracy :', np.round((accuracy * 100), 2), '%')","1abae08c":"y_true = []\nX_test = []\nfor images, labels in test_set_mobilenet:\n    for X, y in zip(images, labels):\n        y_true.append(y)\n        X_test.append(X)\ny_true = np.array(y_true)\nX_test = np.array(X_test)\nprint(y_true, \"shape:\", y_true.shape)","a9fa9962":"y_pred = final_model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_pred.shape","dde6efe2":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd\n\nclass_names = [class_[10:] for class_ in class_names] \n\nconf_mx = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(conf_mx, index=[i for i in class_names],\n                      columns=[i for i in class_names])\nplt.figure(figsize=(30,30))\nsn.heatmap(df_cm, annot=True)","19b70d2a":"from sklearn.metrics import classification_report\nfrom collections import defaultdict\nimport pandas as pd\n\npd.set_option('display.max_rows', 500) # display the whole dataframe\ndict_report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\ndf = pd.DataFrame(dict_report).T\ndf = df.drop(\"accuracy\").drop(\"macro avg\").drop(\"weighted avg\")\ndf.sort_values(\"f1-score\")","bf53e1b2":"def plot_confused_classes(class_name):\n    class_index = class_names.index(class_name)\n    confusion_class = conf_mx[class_index] # shape (120,)\n    classes = []\n    freqencies = []\n    for index, freq in enumerate(confusion_class):\n        if freq != 0: # the model predict that the image belongs to this class\n            classes.append(class_names[index])\n            freqencies.append(freq)\n    \n    print(classes, freqencies)\n    colors = ['g' if name == class_name else 'gray' for name in classes]\n    plt.bar(classes, freqencies, color=colors)\n    plt.xticks(rotation=75)\n    plt.show()\n\nplot_confused_classes('Eskimo_dog')","136d2bbe":"plot_confused_classes('Siberian_husky')","966cb2e3":"plot_confused_classes('miniature_poodle')","acdfccee":"plot_confused_classes('English_foxhound')","5510c28b":"We can see that there is a noticeable difference in accuracy between training and validation accuracy: the model is overfitting. Before we try to regularize the model let's try another model and see if it performs better than this one.","d4a6ca9c":"# Data augmentation and Dropout","11996c9a":"We can see that the validation metrics are better than the training metrics, the main cause is that layers like `BatchNormalization` and `Dropout` affect only the training accuracy.","6a393838":"The code below was copied from: [https:\/\/stackoverflow.com\/questions\/35572000\/how-can-i-plot-a-confusion-matrix](https:\/\/stackoverflow.com\/questions\/35572000\/how-can-i-plot-a-confusion-matrix)","1e8ef659":"To fight overfitting, we will use data augmentation and dropout. Dropout is a technique that consist of randomly ignore units at every training step. Data augmentation generate additional training data from existing ones by applying random transformations. Data augmentation can be applied in two ways:\n* Using Keras preprocessing layers\n    \n     There are two options to use the preprocessing layers:\n     * 1 Make the preprocessing layers part of the model\n     * 2 Apply the preprocessing layers directly to the dataset\n* Using tf.image\n\nHere we are going to make the preprocessing layers part of the model. Note that data augmentation and dropout layers are inactive at inference time.","af506414":"Let's visualize an image after applying the above random transformations:","4dcd88b0":"# Learning curves","d7b926a9":"Both models are overfitting the dataset but the MobileNet model is much more overfitting than the Xception model, on the other hand, MobileNet is much faster than Xception. Since I don't have too much computing power I will use the MobileNet model to try fine-tuning and regularization.","e1be08c8":"We can see that the classes the model struggled to classify are: \"Eskimo dog\", \"Siberian husky\", \"Miniature poodle\", and \"English foxhound\".\n\nLet's write a helper function that shows which classes the above classes (with low f1-score) have been confused with:","fcabc269":"# Model 1: Xception","58685f49":"Let's build a table to compare the 2 models:\n\n <table style=\"width:50%; border: 1px solid black; border-collapse: collapse;\">\n  <tr>\n    <th>Model<\/th>\n    <th>accuracy<\/th>\n    <th>val_accuracy<\/th>\n    <th>time per epoch<\/th>\n  <\/tr>\n  <tr>\n    <td>Xception<\/td>\n    <td>92%<\/td>\n    <td>81%<\/td>\n    <td>45s<\/td>\n  <\/tr>\n  <tr>\n    <td>MobileNetv2<\/td>\n    <td>93%<\/td>\n    <td>75%<\/td>\n    <td>28s<\/td>\n  <\/tr>\n<\/table> ","e18a1ca4":"# Learning curves","3701e10f":"We can see that most images were classified correctly, since most of them are on the diagonal. The classes \"Eskimo Dog\", \"Miniature Poodle\", \"Siberian Husky\" and some others look slightly darker than the other classes, which means that there are fewer images of them or the model does not perform well on these images. For example, the model confused a lot of \"Eskimo dog\" images with the \"Siberian husky\" (the 1st class) which is understandable since the two races are very similar.","d925372d":"In the last part, we tried to classify our images of dogs according to their breeds by building a neural network from scratch: the model was too simple for the task.\n\nIn this part, we are going to classify our images by using transfer learning from a pretrained network. Transfer learning is a technique that consists of using a neural network that was already trained on similar task that we want to tackle.\n\nHere, we are going to focus on training and fine tuning the model not on the preprocessing step.\n\n**References:**\n\n* [Transfer learning and fine-tuning](https:\/\/www.tensorflow.org\/guide\/keras\/transfer_learning#setup) guide from tensorflow.\n* [Transfer learning and fine-tuning](https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning#data_preprocessing) tutorial from tensorflow.\n* [Working with preprocessing layers](https:\/\/www.tensorflow.org\/guide\/keras\/preprocessing_layers#preprocessing_data_before_the_model_or_inside_the_model) tutorial from tensorflow.\n* Chapter14: Deep computer vision using convolutional neural networks from the book hands on machine learning.\n* [This](https:\/\/github.com\/Djinny\/Formation-Data-Scientist\/blob\/master\/Reconnaissance%20d'image%20sur%20120%20classes\/%C3%89tat%20de%20l'Art%20-%20Deep%20Learning.ipynb) notebook.","7715f26c":"Let's try to use another metric: the F1 score. A high F1 score means that both precision and recall are high, which is what we want for our task.","b0bf800b":"# Model 1: MobilNetv2\n\nCreate the datasets:","bb6fbc52":"# Import libraries","06541989":"# Error analysis\n\nNow that we have our final model, let's try to analyse the type of errors it makes.\n\nLet's first plot the confusion matrix:","f05d6832":"# Learning curves","25db6e7c":"# Fine tuning\n\nTo increase the performance of the model, let's try to unfreeze the top layers of the base model and retrain the whole model with a low learning rate. ","e0a0d3bd":"Fine-tuning seems not helped at all and the model start overfitting, so let's load and use the model without fine-tuning.","4875ab73":"Create the model:","72e95b1d":"By analysing the errors the model makes, we can say that there are several ways to improve it:\n* Gather more training data for classes the model struggeled to classify.\n* Get images with better quality.\n* Increase the size of the dataset.","032b4a25":"Let's first use the pretrained Xception model that was trained on the ImageNet dataset.\n\nThe steps that we are going to follow to implement transfer learning are:\n\n* We load the pretrained model excluding the top layers\n* Freeze the weights of the pretrained layers\n* Add a new model on top of the frozen layers\n* Train the new model on our dataset","5629491f":"# Load using tf.data"}}