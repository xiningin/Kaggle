{"cell_type":{"28bcf861":"code","885ad609":"code","e2a063e7":"code","c833ea13":"code","c7606429":"code","63f8ae64":"code","19c9f03d":"code","bbd26646":"code","4f62d77c":"code","ff0a059c":"code","891d5e2c":"code","38ac5c7f":"code","b1b8174e":"code","941660f8":"code","8403a4d6":"code","a7237bd9":"code","7289e097":"code","f4e4ddd2":"code","b944f78a":"code","93635f0f":"code","17a0fb03":"code","1b867be3":"code","4764eebc":"code","b5f87b5f":"markdown","417f2306":"markdown","dd503823":"markdown","bb6e13dd":"markdown","e2489a9f":"markdown","c374e566":"markdown","ebe46c43":"markdown","8d253742":"markdown","7a926d6d":"markdown","72adc611":"markdown","16e6c46a":"markdown","84d65850":"markdown","f6e016ba":"markdown","7d562c42":"markdown","61173d47":"markdown","42254c37":"markdown"},"source":{"28bcf861":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","885ad609":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head()","e2a063e7":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.head()","c833ea13":"train.info()\nprint('_'*45)\ntest.info()","c7606429":"train_age_missing_ratio = '{:.2%}'.format((891-714)\/819)\ntrain_cabin_missing_ratio = '{:.2%}'.format((891-204)\/819)\ntrain_embarked_missing_ratio = '{:.2%}'.format((891-889)\/819)\nprint('In training set:')\nprint('Cabin has',train_cabin_missing_ratio,'data points missing')\nprint('Age has',train_age_missing_ratio,'data points missing')\nprint('Embarked has',train_embarked_missing_ratio,'data points missing')","63f8ae64":"age_missing_ratio = '{:.2%}'.format((418-332)\/418)\ncabin_missing_ratio = '{:.2%}'.format((418-91)\/418)\nembarked_missing_ratio = '{:.2%}'.format((418-418)\/418)\nprint('In test set:')\nprint('Cabin has',cabin_missing_ratio,'data points missing')\nprint('Age has',age_missing_ratio,'data points missing')\nprint('Embarked has',embarked_missing_ratio,'data points missing')","19c9f03d":"print('Training set is','{:.2%}'.format(891\/2224),'of entire passenger group\\n')\n\nprint('A. Analysis of distribution of Categorical Data:\\n')\nprint('   Survival rate within training set is','{:.2%}'.format(train.Survived.value_counts()[1]\/train.shape[0]),'. Survival rate of entire passenger group is','{:.2%}'.format((2224-1502)\/2224))\nprint('\\n   Men represent','{:.2%}'.format((train.Sex.value_counts()[0])\/train.shape[0]),'. Women represent','{:.2%}'.format(1-(train.Sex.value_counts()[0])\/train.shape[0]),'.')\nprint('\\n   Passenger class 3 has','{:.2%}'.format(train.Pclass.value_counts()[3]\/train.shape[0]),'. Passenger class 2 has','{:.2%}'.format(train.Pclass.value_counts()[2]\/train.shape[0]), '. Passenger class 1 has', '{:.2%}'.format(train.Pclass.value_counts()[1]\/train.shape[0]),'.')\nprint('\\n   Passengers that embarked from S = ','{:.2%}'.format(train.Embarked.value_counts()['S']\/train.shape[0]),'. Passengers that embarked from C = ','{:.2%}'.format(train.Embarked.value_counts()['C']\/train.shape[0]),'. Passengers that embarked from Q = ','{:.2%}'.format(train.Embarked.value_counts()['Q']\/train.shape[0]))\nprint('\\nB. Analysis of distribution of Numerical Data:')\nprint('\\n   Age >=',(train.Age.describe()['75%']).round(0),': 25% of training set.','Age between',(train.Age.describe()['25%']).round(0),'and',(train.Age.describe()['75%']).round(0),': 50% of training set','. Age less than 20.0: 25% of training set.')\nprint('\\n   Very few people paid extremly high price for tickets, while majority (75%) paid less than 31.')","bbd26646":"Pclass_analysis = train[['Pclass','Survived']].groupby(['Pclass']).mean().round(2).sort_values(by = 'Survived', ascending = False)\nSex_analysis = train[['Sex','Survived']].groupby(['Sex']).mean().round(2).sort_values(by = 'Survived', ascending = False)\nEmbarked_analysis = train[['Embarked','Survived']].groupby(['Embarked']).mean().round(2).sort_values(by = 'Survived', ascending = False)\nSibSp_analysis = train[['SibSp','Survived']].groupby(['SibSp']).mean().round(2).sort_values(by = 'Survived', ascending = False)\nParch_analysis = train[['Parch','Survived']].groupby(['Parch']).mean().round(2).sort_values(by = 'Survived', ascending = False)\nprint(Pclass_analysis)\nprint('-'*20)\nprint(Sex_analysis)\nprint('-'*20)\nprint(Embarked_analysis)\nprint('-'*20)\nprint(SibSp_analysis)\nprint('-'*20)\nprint(Parch_analysis)","4f62d77c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nage_visualization = sns.FacetGrid(train, col = 'Survived')\nage_visualization.map_dataframe(sns.histplot,x = 'Age')\nfare_visualization = sns.FacetGrid(train, col = 'Survived')\nfare_visualization.map_dataframe(sns.histplot,x = 'Fare')","ff0a059c":"merge = [train,test]\nfor i in merge:\n    i['Title'] = i.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nfor i in merge:\n    i['Title'] = i['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    i['Title'] = i['Title'].replace('Mlle', 'Miss')\n    i['Title'] = i['Title'].replace('Ms', 'Miss')\n    i['Title'] = i['Title'].replace('Mme', 'Mrs')\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","891d5e2c":"for i in merge:\n    i['Family_size'] = i['SibSp']+i['Parch']+1\ntrain[['Family_size','Survived']].groupby(['Family_size']).mean().sort_values(by = 'Survived', ascending = False)","38ac5c7f":"for i in merge:\n    i['Is_alone'] = 0\n    i.loc[i['Family_size'] == 1, 'Is_alone'] = 1\ntrain[['Is_alone','Survived']].groupby(['Is_alone']).mean().sort_values(by = 'Survived', ascending = False)","b1b8174e":"for i in merge:\n    i.drop(['Cabin','Ticket','Name','PassengerId','Family_size','SibSp','Parch'], axis = 1, inplace = True)","941660f8":"from sklearn.preprocessing import OneHotEncoder\nenc = OneHotEncoder()\nenc_df = pd.DataFrame(enc.fit_transform(train[['Title']]).toarray())\ntrain = train.join(enc_df)\ntrain.rename(columns = {0:'Master',1:'Miss',2:'Mr',3:'Mrs',4:'Rare'}, inplace = True)\ntrain.drop(['Title','Mr'], axis = 1, inplace = True)\n# test set\nenc_df1 = pd.DataFrame(enc.fit_transform(test[['Title']]).toarray())\ntest = test.join(enc_df1)\ntest.rename(columns = {0:'Master',1:'Miss',2:'Mr',3:'Mrs',4:'Rare'}, inplace = True)\ntest.drop(['Title','Mr'], axis = 1, inplace = True)","8403a4d6":"# training set\nencsex_df = pd.DataFrame(enc.fit_transform(train[['Sex']]).toarray())\ntrain = train.join(encsex_df)\ntrain.rename(columns = {0:'Female',1:'Male'}, inplace = True)\ntrain = train.drop(['Sex','Female'], axis = 1)\ntrain.head()\n# test set\nencsex_df1 = pd.DataFrame(enc.fit_transform(test[['Sex']]).toarray())\ntest = test.join(encsex_df1)\ntest.rename(columns = {0:'Female',1:'Male'}, inplace = True)\ntest = test.drop(['Sex','Female'], axis = 1)","a7237bd9":"freq_embarked = train.Embarked.dropna().mode()[0]\ntrain['Embarked'] = train['Embarked'].fillna(freq_embarked)\ntest['Embarked'] = test['Embarked'].fillna(freq_embarked)","7289e097":"# training set\nencemb_df = pd.DataFrame(enc.fit_transform(train[['Embarked']]).toarray())\ntrain = train.join(encemb_df)\ntrain.rename(columns = {0:'Embarked_C',1:'Embarked_Q',2:'Embarked_S'}, inplace = True)\ntrain = train.drop(['Embarked','Embarked_C'],axis = 1)\n# test set\nencemb_df1 = pd.DataFrame(enc.fit_transform(test[['Embarked']]).toarray())\ntest = test.join(encemb_df1)\ntest.rename(columns = {0:'Embarked_C',1:'Embarked_Q',2:'Embarked_S'}, inplace = True)\ntest = test.drop(['Embarked','Embarked_C'],axis = 1)","f4e4ddd2":"test['Fare'].fillna(test['Fare'].dropna().median(), inplace=True)","b944f78a":"# quartiles\ntrain['FareBand'] = pd.qcut(train['Fare'],4)\ntrain.FareBand.value_counts()\n# update merge\nmerge = [train,test]\nfor i in merge:\n    i.loc[ i['Fare'] <= 7.91, 'Fare'] = 0\n    i.loc[(i['Fare'] > 7.91) & (i['Fare'] <= 14.454), 'Fare'] = 1\n    i.loc[(i['Fare'] > 14.454) & (i['Fare'] <= 31), 'Fare']   = 2\n    i.loc[ i['Fare'] > 31, 'Fare'] = 3\n    i['Fare'] = i['Fare'].astype(int)\ntrain = train.drop(['FareBand'], axis=1)","93635f0f":"guess_ages = np.zeros((2,3))\nmerge = [train, test]\nfor dataset in merge:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Male'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Male == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain.head()","17a0fb03":"train['AgeBand'] = pd.cut(train['Age'], 5)\ntrain[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\nfor i in merge:    \n    i.loc[ i['Age'] <= 16, 'Age'] = 0\n    i.loc[(i['Age'] > 16) & (i['Age'] <= 32), 'Age'] = 1\n    i.loc[(i['Age'] > 32) & (i['Age'] <= 48), 'Age'] = 2\n    i.loc[(i['Age'] > 48) & (i['Age'] <= 64), 'Age'] = 3\n    i.loc[ i['Age'] > 64, 'Age']\ntrain.head()\ntrain = train.drop(['AgeBand'], axis = 1)","1b867be3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nx_train,x_test,y_train,y_test = train_test_split(X_train,Y_train,train_size = 0.8)\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\nlogreg.score(x_test,y_test)","4764eebc":"test1 = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\npred_y = logreg.predict(test)\npred_y = pred_y.astype(int)\npred_df = pd.DataFrame({'PassengerId':test1['PassengerId'].array,'Survived':pred_y})\npred_df.shape","b5f87b5f":"### 1.4.3 Embarked Transformation\nThe first step is to fill the null values with mode within training set.\n\nOne-hot-encoder for this column too.","417f2306":"### 1.2.2 Find out if each feature contains null values\n#### Most features do not have null values, except for 3 features that contain null values: Age, Cabin, Embarked. \nCabin has a lot of(80%) missing values, might consider dropping this feature;\n\nAge has some(20%) missing values, we need to fill N\/A;\n\nEmbarked has very little(~0.2%) missing values, fill N\/A;","dd503823":"# 2. Fit into Model\n## 2.1 Logistic Regression (without hyperparameter tuning or PCA)","bb6e13dd":"### 1.4.2 Sex Transformation\nUse one-hot-encoder here as well. Notice that we drop Female column because we treat female as the reference group.","e2489a9f":"Not let's look at SipSp and Parch. We want to create a new feature using these two features that has a clearer correlation with survival.","c374e566":"# 3. Submit Prediction","ebe46c43":"### 1.3.4 Drop Useless Features\nWe determined that Ticket, Cabin are useless features. Drop them.","8d253742":"# 1. Data Preprocessing\n## 1.1 Import training set and test set","7a926d6d":"### 1.4.5 Age Transformation\nWe first fill null values\n\nThen we convert age into ordinal values","72adc611":"### 1.3.3 Create New Features\nBefore dropping Name column, we suspect there's a correlation between title(Mr. Mrs., etc.) and survival. Therefore, try extract titles from Name column and test correlations.","16e6c46a":"## 1.3 Feature Selection\n### 1.3.1 Correlation Analysis of Categorical Features\nWe can test some correlation assumptions here: we suspect that passenger class, sex, SibSp, Parch and embarked are correlated with survival.\n#### Pclass: more premium passenger class correlates with higher survival rate. Include Pclass as a feature.\n#### Sex: women had significantly higher survival rate than men. Include Sex as a feature.\n#### SibSp: In general, passengers with less siblings or spouses were more likley to survive, but we also witness when it's 0 survival rate is less than at 1\/2. Consider creating a new feature by combining with Parch.\n#### Parch: In general, passengers with less Parents or children on board were more likely to survive, but we also witness when it's 0 survival rate is less than at 1\/2.\n#### Emarked: passengers emarked at C had much higher survival rate, while passengers embarked at Q had 5% higher survival rate than passengers embarked at S. Include Embarked as a feature.","84d65850":"### 1.2.3 Evaluate representativeness of our training set data\n#### Consider:\n1. What's the ratio of training set data to the entire population? \n\nAnswer: 40.06%. Thus we have decent amount of training data. Check.\n\n2. Within training set, what's the survival rate comparing with population survival rate?\n\nAnswer: Training set survival rate = 38.38%, while population survival rate = 32.46%. Data is not skewed, check.\n\n3. Within training set, what's the ratio of men and women? We want decent amount of data from both men and women.\n\nAnswer: 64.76% men, and 35.24% women. Check.\n\n4. What's the distribution of passenger class within training set? We want training set to have decent amount of data across all PClasses.\n\nAnswer: class 3: 55.11%; class 2: 20.65%; class 1: 24.24%. Check.\n\n5. What's the distribution of embarked class within training set? We want decent amount of data across all embarked gates too.\n\nAnswer: S: 72.28%; C: 18.86%. Q: 8.64%. Check.\n\n6. What's the distribution of age within training set? We want every age group to be represented in training set.\n\nAnswer: Within training set: 25% of people are older than 38, 50% of people are 20-38, 25% of people younger than 20. Check.\n\n7. What's the distribution of fare prices within training set? We want different fare price groups to be represented here.\n\nAnswer: Within training set: extremly high fare passengers are recorded, while plenty of data exist for low fare and medium fare passengers. \n#### Conclusion: The training set is representative of population.","f6e016ba":"## 1.4 Feature Transformation\nNow we have all the features we need. The final step before feeding into machine learning models is transformation of these features.\n### 1.4.1 Title Transformation\nWe want to use one-hot-encoder here to transform this categorical data. Here we treat Mr as the dropped dummy variable.","7d562c42":"### 1.3.2 Correlation Analysis of Numerical Features\nTo understand correlations of numerical data with survival, a common approach is to visualize by plotting histograms.\n\nWe suspect that age and Fare should have correlations with survival rate.\n\n#### Age: children and elder people were more likely to survive, quadratic correlation with survival rate. Include Age as a feature.\n#### Fare: people that paid more for tickets have much higher chance of survival. Include Fare as a feature.","61173d47":"## 1.2 Examine Dataset\n### 1.2.1 Analyze data type of each feature\n#### Categorical: nominal, ordinal, ratio, or interval-based?\nFeatures that contain Categorical data: Survived, Sex, Embarked\n\nFeatures that contain Ordinal data: Pclass\n#### Numerical: discrete, continuous, or time-series?\nFeatures that contain Continuous data: Age, Fare\n\nFeatures that contain Discrete data: SibSp, Parch","42254c37":"### 1.4.4 Fare Transformation\nWe first fill null values in test set\n\nThen we convert fare into ordinal values"}}