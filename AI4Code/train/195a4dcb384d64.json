{"cell_type":{"3e90bba3":"code","68af66f8":"code","d6946085":"code","d8b506f9":"code","99e7381f":"code","0739b582":"code","4c76ead0":"code","22c44e7a":"code","0017454e":"code","97f3202b":"code","947aeede":"code","91177a68":"code","afc60d4d":"code","74371c18":"code","f832c7f6":"code","d7126dd5":"code","f42d6332":"code","df6a46b3":"code","2fedeb5d":"code","1c1eba22":"code","f6bb5f08":"code","63486be9":"code","3af2b077":"code","84f67e34":"code","019cea67":"code","0c69c4d8":"code","55983aa1":"code","bfe617aa":"code","557e49cc":"code","590b1e39":"code","e7e61c5d":"code","5f390c16":"code","0310ee34":"code","a4a98eb3":"code","0f4d74d7":"code","828bd02d":"code","2f7374a0":"code","389747b1":"code","8e9f9323":"code","a2bc38d3":"code","1d36cea6":"markdown","c26b1558":"markdown","e79e6b7c":"markdown","60dd9092":"markdown","49f39cc7":"markdown","9b4ccc30":"markdown"},"source":{"3e90bba3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#loading libraries\n\nimport os\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve\nfrom sklearn.metrics import recall_score, classification_report, auc, roc_curve\nfrom sklearn.metrics import precision_recall_fscore_support, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n\nfrom keras import losses\nfrom keras.utils import to_categorical\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model, Sequential \nfrom keras.optimizers import Adam\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.callbacks import Callback\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom scipy import stats\nimport tensorflow as tf\nimport pickle\n\nfrom pylab import rcParams","68af66f8":"#Loading train data\ntrain=pd.read_csv(\"..\/input\/train.csv\")","d6946085":"#Data\ntrain.head()","d8b506f9":"#Shape of data\ntrain.shape","99e7381f":"#Exploring missing values\ntrain.isnull().sum()[train.isnull().sum() !=0]","0739b582":"#Exploring missing values\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])\/59381)*100\nplt.figure(figsize=(20,8))\nsns.barplot(y=train_missing['features'],x=train_missing['missing_count_percentage'])\ntrain_missing","4c76ead0":"#checking data types\ntrain.dtypes.unique()","22c44e7a":"#Outliers detection\ntrain.describe()","0017454e":"#Responce variable\naixs1 = plt.subplots(1,1,figsize=(10,5))\nsns.countplot(x='Response',data=train)","97f3202b":"#Categorical codes\ntrain['Product_Info_2'] = train['Product_Info_2'].astype('category').cat.codes","947aeede":"# missing values\ntrain_missing","91177a68":"#dropping columns containing missing values more than 80%\ntrain = train.drop(['Medical_History_10','Medical_History_24','Medical_History_32'], axis=1)","afc60d4d":"#missing values AGAIN\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])\/59381)*100\ntrain_missing","74371c18":"#Mean Imputation fro continous variables\nContinuos = ['Employment_Info_1','Employment_Info_4', 'Employment_Info_6', 'Insurance_History_5',\n                    'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']\ntrain[Continuos] = train[Continuos].fillna(train[Continuos].mean())","f832c7f6":"#Mode Imputation fro continous variables\nCategorical = ['Medical_History_1', 'Medical_History_15']\ntrain[Categorical] = train[Categorical].apply(lambda x:x.fillna(x.value_counts().index[0]))","d7126dd5":"#Missing values again\ntrain_missing= train.isnull().sum()[train.isnull().sum() !=0]\ntrain_missing=pd.DataFrame(train_missing.reset_index())\ntrain_missing.rename(columns={'index':'features',0:'missing_count'},inplace=True)\ntrain_missing['missing_count_percentage']=((train_missing['missing_count'])\/59381)*100\ntrain_missing","f42d6332":"#train data\ntrain.head()","df6a46b3":"#Dataset split\ntrain_data, test_data = train_test_split(train, test_size = 0.15)\nprint(train_data.shape)\nprint(test_data.shape)","2fedeb5d":"#traindata\ntrain_data.head()","1c1eba22":"#traindata\ntest_data.head()","f6bb5f08":"#Predictor and responce variables\ntrain_x = train_data.drop(['Id', 'Response'], axis=1)\ntrain_y = train_data['Response']\ntest_x = test_data.drop(['Id', 'Response'], axis=1)\ntest_y = test_data['Response']\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n\n","63486be9":"#train responce\ntrain_y.head()","3af2b077":"#test responce\ntest_y.head()","84f67e34":"#converting to responce categorical class labels(0-7)\ntrain_y = train_y-1\ntrain_y = to_categorical(train_y, num_classes= 8)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","019cea67":"#Function for normalization\ndef normalization(data):\n    return (data - data.min())\/(data.max() - data.min())","0c69c4d8":"#normalizing data\ntrain_x = normalization(train_x)\ntest_x = normalization(test_x)","55983aa1":"#traindata\ntrain_x.head()","bfe617aa":"#testdata\ntest_x.head()","557e49cc":"#Train and test data shapes\nprint(train_x.shape)\nprint(test_x.shape)","590b1e39":"#assigning static parameter\nnb_epoch = 20\nbatch_size = 512\ninput_dim = train_x.shape[1]\nhidden_dim1 = 64 \nhidden_dim2 = 32\nhidden_dim3 = 16\nlearning_rate = 1e-7","e7e61c5d":"#Function for auto encoder to get and fit model\ndef get_fit_encoder(xs_train,xs_cv,test_x):\n    input_layer = Input(shape=(input_dim, ))\n    encoder = Dense(input_dim, activation=\"relu\",activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n    \n    encoder = Dense(hidden_dim1, activation=\"relu\")(encoder)\n    encoder = Dense(hidden_dim2, activation=\"relu\")(encoder)\n    encoder = Dense(hidden_dim3, activation=\"relu\", name=\"encoder\")(encoder)\n    \n    decoder = Dense(hidden_dim3, activation=\"relu\")(encoder)\n    decoder = Dense(hidden_dim2, activation='relu')(decoder)\n    decoder = Dense(hidden_dim1, activation='relu')(decoder)\n    \n    decoder = Dense(input_dim, activation='relu')(decoder)\n    decoder = Dense(input_dim, activation='sigmoid')(decoder)\n    autoencoder = Model(inputs=input_layer, outputs=decoder)\n    #autoencoder.summary()\n    autoencoder.compile(optimizer='adam',\n                        loss='binary_crossentropy')\n    \n    history = autoencoder.fit(x=xs_train, y=xs_train,\n                          epochs=nb_epoch,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          validation_data=(xs_cv, xs_cv),\n                          verbose=1)\n    encoder = Model(autoencoder.input, autoencoder.get_layer('encoder').output)\n    x_auto_train= encoder.predict(xs_train)\n    x_auto_cv= encoder.predict(xs_cv)\n    x_auto_test= encoder.predict(test_x)\n    return x_auto_train,x_auto_cv,x_auto_test\n    ","5f390c16":"#Function for Neural network to get and fit model\ndef get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv):\n    classifier = Sequential()\n    classifier.add(Dense(output_dim = input_dim , init = 'uniform', activation = 'relu', input_dim = 16))\n    classifier.add(Dense(output_dim = 16 , init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 8 , init = 'uniform', activation = 'relu'))\n    classifier.add(Dense(output_dim = 8, init = 'uniform', activation = 'softmax'))\n    classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n    cp = ModelCheckpoint(filepath=\"autoencoder_data.h5\",\n                         save_best_only=True,\n                         verbose=0)\n    tb = TensorBoard(log_dir='.\/logs',\n                     histogram_freq=0,\n                     write_graph=True,\n                     write_images=True)\n    history = classifier.fit(xs_encoder_train, ys_train,\n                             batch_size=batch_size ,\n                             epochs=nb_epoch ,\n                             shuffle=True,\n                             validation_data=(xs_encoder_cv,ys_cv),\n                             verbose=1,\n                            callbacks=[cp, tb]).history\n    y_pred_NN = classifier.predict(xs_encoder_test, batch_size=batch_size, verbose=1)\n    y_pred_NN = np.argmax(y_pred_NN,axis = 1) + 1\n    return y_pred_NN","0310ee34":"#Function for State of art model to get and fit model\ndef get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x):\n    model1 = RandomForestClassifier()\n    \n    inside_train_y = np.argmax(y_sampletrain, axis = 1) + 1   \n    \n    model1.fit(x_sampletrain, inside_train_y)\n    \n    y_pred1 = model1.predict(test_x) \n    return y_pred1","a4a98eb3":"#function for model evaluation\ndef model_evaluation (test_y,y_pred_NN,y_pred1):\n   \n    accuracy_NN = accuracy_score(test_y,y_pred_NN)\n    F1_score_NN=f1_score(test_y, y_pred_NN,average='weighted')\n    Precision_NN=precision_score(test_y, y_pred_NN,average='weighted')\n    Recall_score_NN=recall_score(test_y, y_pred_NN,average='weighted')\n    \n    accuracy_SOAM1 = accuracy_score(test_y, y_pred1)\n    F1_score_SOAM1=f1_score(test_y, y_pred1,average='weighted')\n    Precision_SOAM1=precision_score(test_y, y_pred1,average='weighted')\n    Recall_score_SOAM1=recall_score(test_y, y_pred1,average='weighted')\n    \n    \n    print(\"Classification score for NN:\", classification_report(test_y,y_pred_NN))\n    print(\"Classification score for SOAM1:\", classification_report(test_y, y_pred1))\n       \n    return accuracy_NN,F1_score_NN,Precision_NN,Recall_score_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_score_SOAM1\n    ","0f4d74d7":"#Function to pass sample data to autoencoder and neural network functions\ndef data_sampling(train_x, train_y, test_x, test_y):\n    accuracy_list_NN= []\n    F1_score_list_NN=[]\n    Precision_list_NN=[]\n    Recall_list_NN=[]\n    \n    accuracy_list_SOAM1= []\n    F1_score_list_SOAM1=[]\n    Precision_list_SOAM1=[]\n    Recall_list_SOAM1=[]\n    \n    \n    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6,0.7, 0.8, 0.9, 0.99]:\n        print(\"data sample {}\".format(i*100))\n        x_sampletrain, _, y_sampletrain, _ = train_test_split(train_x, train_y, stratify= train_y, train_size=i)\n        xs_train, xs_cv, ys_train, ys_cv = train_test_split(x_sampletrain, y_sampletrain, stratify=y_sampletrain, train_size=0.9)\n        xs_train.shape, xs_cv.shape, ys_train.shape, ys_cv.shape\n        xs_encoder_train,xs_encoder_cv,xs_encoder_test=get_fit_encoder(xs_train,xs_cv,test_x)\n        y_pred_NN=get_fit_neuralnetwork(xs_encoder_train,xs_encoder_cv,xs_encoder_test,ys_train,ys_cv)\n        \n        y_pred1=get_fit_SOA_Models(x_sampletrain,y_sampletrain,test_x)\n        \n        accuracy_NN,F1_score_NN,Precision_NN,Recall_NN,accuracy_SOAM1,F1_score_SOAM1,Precision_SOAM1,Recall_SOAM1=model_evaluation(test_y,y_pred_NN,y_pred1)\n        \n        \n        accuracy_list_NN.append(accuracy_NN)\n        F1_score_list_NN.append(F1_score_NN)\n        Precision_list_NN.append(Precision_NN)\n        Recall_list_NN.append(Recall_NN)\n        \n        accuracy_list_SOAM1.append(accuracy_SOAM1)\n        F1_score_list_SOAM1.append(F1_score_SOAM1)\n        Precision_list_SOAM1.append(Precision_SOAM1)\n        Recall_list_SOAM1.append(Recall_SOAM1)\n        \n        \n    return accuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1\n","828bd02d":"#main code to run all functions to reach objective\naccuracy_list_NN,F1_score_list_NN,Precision_list_NN,Recall_list_NN,accuracy_list_SOAM1,F1_score_list_SOAM1,Precision_list_SOAM1,Recall_list_SOAM1=data_sampling(train_x, train_y, test_x, test_y)","2f7374a0":"#Evalution output for Neural network\naccuracy_list_NN","389747b1":"#Evalution output for SOAM network\naccuracy_list_SOAM1","8e9f9323":"#Saving output to a file\nwith open('Accuracy_NN.txt', 'w') as f:\n    print(accuracy_list_NN, file=f)\nwith open('Accuracy_SOAM.txt', 'w') as f:\n    print(accuracy_list_SOAM1, file=f)\n\nwith open('F1_score_NN.txt', 'w') as f:\n    print(F1_score_list_NN, file=f)\nwith open('F1_score_SOAM1.txt', 'w') as f:\n    print(F1_score_list_SOAM1, file=f)\n\nwith open('Precision_Score_NN.txt', 'w') as f:\n    print(Precision_list_NN, file=f)\nwith open('Precision_Score_SOAM1.txt', 'w') as f:\n    print(Precision_list_SOAM1, file=f)\n    \nwith open('Recall_Recall_NN.txt', 'w') as f:\n    print(Recall_list_NN, file=f)\nwith open('Recall_Recall_SOAM1.txt', 'w') as f:\n    print(Recall_list_SOAM1, file=f)","a2bc38d3":"#output comparitive visualization\nfrom matplotlib.pyplot import figure\nplt.figure(figsize=(15, 5))\nplt.plot(accuracy_list_NN,label='NN')\nplt.plot(accuracy_list_SOAM1,label='SOAM1')\n#plt.plot([10,20], accuracy_list_SOAM3,label='SOAM2')\nplt.legend(loc='lower right')\nplt.xlabel(\"data fraction\")\nplt.ylabel(\"Accuaracy Value\")\n","1d36cea6":"Data Exploration","c26b1558":"Missing Value Treatment","e79e6b7c":"2)Normalization","60dd9092":"Modelling\n\n1)Dataset split","49f39cc7":"Data PreProcessing","9b4ccc30":"\n3)Models and evaluation"}}