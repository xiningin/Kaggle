{"cell_type":{"872afc4e":"code","894ead3e":"code","704413e3":"code","a2f1ea0f":"code","50ff04a3":"code","ecc0ca7b":"code","da0654cf":"code","885e3f4d":"code","18575a20":"code","7bbb20bc":"code","a0c9c7ab":"code","e194feb7":"code","2a396618":"code","3458e11f":"code","e33380bf":"code","3fae8fca":"code","ae1135be":"code","8b0c902a":"code","dd2f692a":"code","7ba8f947":"code","ec824cae":"code","45c5975d":"code","4eeb8add":"code","98593ee2":"code","561fa4c0":"code","b46edb5d":"code","3d1b4626":"code","5302b841":"code","703d8d25":"code","3f4de0ee":"code","28afe25a":"code","944afe35":"code","376247b9":"code","080ca3d6":"code","3d37d74c":"code","1c6c64f8":"code","13b9301c":"code","2b0d5738":"code","dbb21f93":"code","9a210572":"code","8cf13df2":"code","2ac68303":"code","e14b9725":"code","746bd6d0":"code","ef636b05":"code","174f04ef":"code","6681f3d4":"code","5a54f104":"code","eba789d1":"code","a0ad2dc9":"code","cf64104c":"code","228b3246":"code","27b27469":"code","70b1d185":"code","7a5c7178":"code","2623ef98":"code","96b5eb68":"markdown","b973b1b7":"markdown","971b7de8":"markdown","a5dc9fd8":"markdown","5840a4ea":"markdown","d00f0bac":"markdown","e83b4e52":"markdown","b3f5738d":"markdown","5267abdd":"markdown","47046ef1":"markdown","2226298d":"markdown","897d4944":"markdown","ee2374cb":"markdown","40ed734e":"markdown","e0b416e5":"markdown","ab857a0d":"markdown","c03132b9":"markdown","18a74da9":"markdown","36ea00ae":"markdown","cc84e4d9":"markdown","ad183a6e":"markdown","a97ae2ea":"markdown","1ab84e8d":"markdown"},"source":{"872afc4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","894ead3e":"# Import main libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Statistics\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\n\n# Preprocessing","704413e3":"# Import our dataset\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv\")","a2f1ea0f":"train.head()","50ff04a3":"# Create a list of numerical and categoricals columns\nnum_feat = [col for col in train.columns if col.startswith(\"cont\")]\ncat_feat = [col for col in train.columns if col.startswith(\"cat\")]","ecc0ca7b":"missingno.matrix(train[num_feat], figsize=(18,4))","da0654cf":"# Save id column for submission and drop it from train\/test dataset\nid_col = test['id']\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","885e3f4d":"# Describe continues features\ntrain[num_feat].describe().T","18575a20":"num_feat.append(\"target\")","7bbb20bc":"corr = train[num_feat].corr()\n\nfig = plt.figure(figsize=(14,9))\n\nsns.heatmap(corr,cmap='coolwarm', annot=True, cbar=False)","a0c9c7ab":"target_ser = corr['target']\ntarget_ser = target_ser.drop('target').sort_values(ascending=False)\n\nplt.figure(figsize=(10,5))\nsns.barplot(x=target_ser.index, y=target_ser.values, palette='cool')\nplt.title(\"Correlation numeric features\");","e194feb7":"num_feat.remove(\"target\")","2a396618":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(8,2, i+1)\n    sns.kdeplot(x=train[col], color='b', shade=True)\n    plt.grid()\n    plt.tight_layout()\n    \nfig.show()","3458e11f":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(8,2, i+1)\n    sns.scatterplot(x=col, y=\"target\", data=train, alpha=0.3)\n    plt.tight_layout()\n    \nfig.show()","e33380bf":"#Discretization of continues variables\n\n#from sklearn.preprocessing import KBinsDiscretizer\n\n#est = KBinsDiscretizer(n_bins=100, encode='ordinal', strategy='quantile')\n\n#train.loc[:,num_feat] = est.fit_transform(train.loc[:,num_feat])","3fae8fca":"train[num_feat].kurtosis()","ae1135be":"train[num_feat].skew()","8b0c902a":"def signaltonoise(a, axis=0, ddof=0):\n    a = np.asanyarray(a)\n    m = a.mean(axis)\n    sd = a.std(axis=axis, ddof=ddof)\n    return np.where(sd == 0, 0, m\/sd)\n\nfor col in num_feat:\n    print(f\"Column '{col}' signal-to-noise ratio:  {signaltonoise(train[col]):2f}\")","dd2f692a":"plt\nsns.displot(train['target'])\nplt.title(f\"skewness: {train['target'].skew()}\");","7ba8f947":"for col in cat_feat:\n    \n    set_diff = set(train[col].unique()) - set(test[col].unique())\n    print(f\"Train and Test Dataset column: {col} has different of unuque value: {set_diff}\")","ec824cae":"train['cat6'].value_counts()","45c5975d":"test['cat6'].value_counts()","4eeb8add":"fig = plt.figure(figsize=(18,30))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8,3, i+1)\n    sns.countplot(x=train[col], palette='mako_r')\n\nfig.show()","98593ee2":"# Check distribution of categorical features against target feature.\nfig = plt.figure(figsize=(18, 30))\n\nfor i, col in enumerate(cat_feat):\n    plt.subplot(8, 2, i+1)\n    sns.boxenplot(x=col, y='target', data=train)\nfig.show()","561fa4c0":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nfor col in cat_feat:\n    train[col] = le.fit_transform(train[col])\n    test[col] = le.transform(test[col])","b46edb5d":"# Test for skewness again\ntest.skew()","3d1b4626":"train.drop('cat6', axis=1, inplace=True)\ntest.drop('cat6', axis=1, inplace=True)\ncat_feat.remove('cat6')","5302b841":"#def get_dummies(df):\n    #dummy_df = pd.get_dummies(df[cat_feat], drop_first=True)\n    #df.drop(cat_feat, axis=1, inplace=True)\n    #new_df = pd.concat([dummy_df, df], axis=1)\n    #return new_df\n\n#train = get_dummies(train)\n#test = get_dummies(test)","703d8d25":"from sklearn.model_selection import train_test_split","3f4de0ee":"all_cols = [col for col in train.columns if col.startswith(\"c\")]","28afe25a":"X = train.iloc[:,:-1].values\ny = train.iloc[:, -1].values","944afe35":"X_train, X_val, y_train, y_val = train_test_split(X, y,\n                                                  test_size=0.1, random_state=45)","376247b9":"# We need to normalize the dataset for tensorflow\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler","080ca3d6":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val= scaler.transform(X_val)","3d37d74c":"X_train.shape","1c6c64f8":"# Create a simple model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nmodel = Sequential()\n\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\nmodel.add(Dense(23, activation=\"relu\"))\n\n\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mae')","13b9301c":"model.fit(x=X_train, y=y_train, validation_data=(X_val, y_val) , epochs=10, batch_size=128)","2b0d5738":"loss_df = pd.DataFrame(model.history.history)\nloss_df.plot();","dbb21f93":"from sklearn.metrics import mean_squared_error","9a210572":"base_mae = model.evaluate(X_val, y_val, verbose=0)\nprint(f\"Base Model MAE: {base_mae}\")","8cf13df2":"mean_y = np.mean(y)\noff = 100 * (base_mae \/ mean_y)\nprint(f\"We are off about {off:.2f}%\")","2ac68303":"# model.evaluate(X_train, y_train, verbose=0)","e14b9725":"base_pred = model.predict(X_val)","746bd6d0":"base_rmse = np.sqrt(mean_squared_error(y_val, base_pred))\nprint(f\"Base model RMSE: {base_rmse}\")","ef636b05":"from tensorflow.keras.layers import Dropout\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import Adam, RMSprop, SGD","174f04ef":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(54, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer='adam', loss=\"mae\")\n    \n    return model\n\n\ndef evaluate_model(model):\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(f\"RMSE: {rmse}\")","6681f3d4":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n\nann_model = create_ann()\n\nann_model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=128, epochs=100, callbacks=[early_stop])","5a54f104":"loss_df = pd.DataFrame(ann_model.history.history).head()\nloss_df.plot();","eba789d1":"evaluate_model(ann_model)","a0ad2dc9":"from sklearn.model_selection import KFold","cf64104c":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add hidden layer\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.5))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer=RMSprop(lr=0.001), loss=\"mean_squared_error\")\n    \n    return model","228b3246":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\ny_pred_list = []\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_val = sc.transform(X_val)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    ann_model = create_ann()\n    ann_model.fit(X_train, \n                  y_train, \n                  validation_data=(X_val, y_val), \n                  batch_size=128, epochs=30, \n                  callbacks=[early_stop])\n    \n    \n    y_pred_list.append(ann_model.predict(X_val))\n    \n    \n    oof[test_idx] = np.mean(y_pred_list, axis=0).reshape(len(X_val),)\n    score = np.sqrt(mean_squared_error(y_val, oof[test_idx]))\n    score_list.append(score)\n    print(f\"RMSE fold -{fold} : {score}\")\n    fold +=1\n    \nprint(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","27b27469":"print(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","70b1d185":"sc = StandardScaler()\nscaled_X = sc.fit_transform(X)\nscaled_test = sc.transform(test)\n\nann_model.fit(scaled_X, y)\nann_y_pred = ann_model.predict(scaled_test)","7a5c7178":"sub_k = pd.DataFrame({\"id\": id_col,\n                      \"target\": ann_y_pred.reshape(-1)})","2623ef98":"sub_k.to_csv(\"sub_tbs_feb_ann.csv\", index=False)","96b5eb68":"I am going to remove column 'cat6' as it is skewed the most.","b973b1b7":"### Submmit to Kaggle","971b7de8":"Normally, when converting categorical features we would be able to distinguish between nominal and ordinal but not in this case. I've notice that some of Kagglers've chosen method appropriate for ordinal transformation. I think the only reason for that would be to reduce dimensionality of the dataset as it is already quite big for computation power of out pc. For that point I will use LabelEncoder from sklearn.","a5dc9fd8":"#### Traget column","5840a4ea":"### Modeling ANN","d00f0bac":"# Introduction","e83b4e52":"### Categorical features","b3f5738d":"Discretization didn't help to improve my base model.","5267abdd":"**Model evaluation**","47046ef1":"Our target column distribution is bimodial, and is skewed to the left.","2226298d":"Now let's use KFold to make sure we train our model on different samples and take a mean of it.","897d4944":"There is a different in column 'cat6' and that need to be remembered when we onhotencode or using get_dummies method.","ee2374cb":"Ok, so this is our base line. There is a lot to do in order to bit my best score from previous notebook  (0.84364). Well we see, after all we doing it for fun and to learn something new.","40ed734e":"There is some correlation between features but interestingly none of them is correlated with the target. As we can see on the plot below correlation between feature 'cont9' and the target doesn't exist.","e0b416e5":"This is my second notebook with February Tabular-Playgroud-Series. In the first one I've learnt how to use lightgbm algorithm. https:\/\/www.kaggle.com\/godzill22\/tbs-feb-2021-with-lightgbm. In this notebook I want to learn how to use neural network to solve regression model and whether ANN can improve my score.","ab857a0d":"### Numerical Features","c03132b9":"## Exploratory Data Analysis first.","18a74da9":"### LabelEncode categorical features","36ea00ae":"### Preparing the dataset for machine learning","cc84e4d9":"#### Skewness and Kurtosis","ad183a6e":"In most cases the distribution of continues features seems to be multimodial, except feature \"cont6\".","a97ae2ea":"My score on submission was 0.87081 and as we compete in this competition in 4th and 5th decimal point this score isn't good at all.\nSo dear Kagglers can someone point me to the right direction and tell me what else can be done to improve this neural network?","1ab84e8d":"Creating dummy variable didn't improve the model, as a matter of fact it worsen the model."}}