{"cell_type":{"598fae03":"code","432a217b":"code","d5806b19":"code","dcc017be":"code","909b8b12":"code","a1cc3cc1":"code","75bdb090":"code","beff91f5":"code","552604bb":"code","8586933b":"code","f2ac1f1e":"code","5e539cae":"code","d94efe97":"code","87142e83":"code","becf96d5":"code","c5b20cf8":"code","9de36ca8":"code","a5443706":"code","6bd640e3":"code","2b71ea7a":"code","65a77b5b":"code","88300649":"code","78b34597":"code","d6f1234e":"code","6652094f":"code","fd71cb40":"code","f2ebd47e":"code","c3506cad":"code","40ae54b1":"code","928b06cf":"code","98259ec9":"code","b0243b9e":"code","d314e689":"code","9b01f753":"code","faebbd18":"code","f663479d":"code","b918cb71":"code","ef6fad48":"code","a880a5de":"code","0dd29a25":"code","a6f1180d":"code","63c12f69":"code","6299f451":"code","b601fb2a":"code","5a4291ac":"code","89a61f26":"code","4dabb0ec":"code","347100da":"code","613a986e":"code","c7dc672f":"code","d35f04f6":"code","a6109cfd":"code","f3b9fa9d":"code","e751a8e0":"code","293cf409":"code","8bef8808":"code","41ef670b":"code","99f6e473":"code","0fdf8684":"code","d86b2cec":"code","a55d25a4":"code","b2baa423":"markdown","cd9ac010":"markdown"},"source":{"598fae03":"import spacy\n\n#spacy.prefer_gpu()","432a217b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d5806b19":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()\n\nuse_cuda = True","dcc017be":"# Reading the data\nBASE_PATH = '\/kaggle\/input\/tweet-sentiment-extraction\/'\n\ntrain_df = pd.read_csv(BASE_PATH + 'train.csv')\ntest_df = pd.read_csv( BASE_PATH + 'test.csv')\nsubmission_df = pd.read_csv( BASE_PATH + 'sample_submission.csv')","909b8b12":"# Checking the shape of train and test data\nprint(train_df.shape)\nprint(test_df.shape)","a1cc3cc1":"# Checking Missing value in the training set\nprint(train_df.isnull().sum())\n# Checking Missing Value in the testing set\nprint(test_df.isnull().sum())","75bdb090":"# Droping the row with missing values\ntrain_df.dropna(axis = 0, how ='any',inplace=True)","beff91f5":"# Positive tweet\nprint(\"Positive Tweet example :\",train_df[train_df['sentiment']=='positive']['text'].values[0])\n#negative_text\nprint(\"Negative Tweet example :\",train_df[train_df['sentiment']=='negative']['text'].values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",train_df[train_df['sentiment']=='neutral']['text'].values[0])","552604bb":"# Distribution of the Sentiment Column\ntrain_df['sentiment'].value_counts()","8586933b":"sns.countplot(x=train_df['sentiment'],data=train_df)\nplt.show()","f2ac1f1e":"train_df['sentiment'].value_counts(normalize=True)","5e539cae":"train_df['sentiment'].value_counts(normalize=True).plot(kind='bar')\nplt.xlabel('Sentiments')\nplt.ylabel('Percentage')\nplt.show()","d94efe97":"# text preprocessing helper functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n","87142e83":"# Applying the cleaning function to both test and training datasets\ntrain_df['text_clean'] = train_df['text'].apply(str).apply(lambda x: text_preprocessing(x))\ntest_df['text_clean'] = test_df['text'].apply(str).apply(lambda x: text_preprocessing(x))","becf96d5":"train_df.head()","c5b20cf8":"test_df.head()","9de36ca8":"# Analyzing Text statistics\n\ntrain_df['text_len'] = train_df['text_clean'].astype(str).apply(len)\ntrain_df['text_word_count'] = train_df['text_clean'].apply(lambda x: len(str(x).split()))","a5443706":"train_df.head()","6bd640e3":"# Let's create three separate dataframes for positive, neutral and negative sentiments. \n#This will help in analyzing the text statistics separately for separate polarities.\n\npos = train_df[train_df['sentiment']=='positive']\nneg = train_df[train_df['sentiment']=='negative']\nneutral = train_df[train_df['sentiment']=='neutral']","2b71ea7a":"pos.head()","65a77b5b":"# Sentence length analysis\n\nfig, ax = plt.subplots(1, 3, figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.hist(pos['text_len'],bins=50,color='g')\nplt.title('Positive Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 2)\nplt.hist(neg['text_len'],bins=50,color='r')\nplt.title('Negative Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\n\n\nplt.subplot(1, 3, 3)\nplt.hist(neutral['text_len'],bins=50,color='y')\nplt.title('Neutral Text Length Distribution')\nplt.xlabel('text_len')\nplt.ylabel('count')\nplt.show()","88300649":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","78b34597":"#Distribution of top unigrams\npos_unigrams = get_top_n_words(pos['text_clean'],20)\nneg_unigrams = get_top_n_words(neg['text_clean'],20)\nneutral_unigrams = get_top_n_words(neutral['text_clean'],20)\n\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 unigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 unigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 unigram in Neutral text')\nplt.show()","d6f1234e":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","6652094f":"#Distribution of top Bigrams\npos_bigrams = get_top_n_gram(pos['text_clean'],(2,2),20)\nneg_bigrams = get_top_n_gram(neg['text_clean'],(2,2),20)\nneutral_bigrams = get_top_n_gram(neutral['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(pos_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 Bigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_unigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 Bigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_unigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 Bigram in Neutral text')\nplt.show()","fd71cb40":"# Finding top trigram\npos_trigrams = get_top_n_gram(pos['text_clean'],(3,3),20)\nneg_trigrams = get_top_n_gram(neg['text_clean'],(3,3),20)\nneutral_trigrams = get_top_n_gram(neutral['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(pos_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('Count')\nplt.title('Top 20 trigrams in positve text')\nplt.show()\n\ndf2 = pd.DataFrame(neg_trigrams, columns = ['Text' , 'count'])\ndf2.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='red')\nplt.title('Top 20 trigram in Negative text')\nplt.show()\n\ndf3 = pd.DataFrame(neutral_trigrams, columns = ['Text' , 'count'])\ndf3.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='yellow')\nplt.title('Top 20 trigram in Neutral text')\nplt.show()","f2ebd47e":"#  Exploring the selected_text column\n\npositive_text = train_df[train_df['sentiment'] == 'positive']['selected_text']\nnegative_text = train_df[train_df['sentiment'] == 'negative']['selected_text']\nneutral_text = train_df[train_df['sentiment'] == 'neutral']['selected_text']","c3506cad":"negative_text.head()","40ae54b1":"\n# Positive text\nprint(\"Positive Text example :\",positive_text.values[0])\n#negative_text\nprint(\"Negative Tweet example :\",negative_text.values[0])\n#neutral_text\nprint(\"Neutral tweet example  :\",neutral_text.values[0])","928b06cf":"# Preprocess Selected_text\n\npositive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\nnegative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\nneutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))","98259ec9":"negative_text_clean.head()","b0243b9e":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n","d314e689":"top_words_in_positive_text = get_top_n_words(positive_text_clean)\ntop_words_in_negative_text = get_top_n_words(negative_text_clean)\ntop_words_in_neutral_text = get_top_n_words(neutral_text_clean)\n\np1 = [x[0] for x in top_words_in_positive_text[:20]]\np2 = [x[1] for x in top_words_in_positive_text[:20]]\n\n\nn1 = [x[0] for x in top_words_in_negative_text[:20]]\nn2 = [x[1] for x in top_words_in_negative_text[:20]]\n\n\nnu1 = [x[0] for x in top_words_in_neutral_text[:20]]\nnu2 = [x[1] for x in top_words_in_neutral_text[:20]]","9b01f753":"# Top positive word\nsns.barplot(x=p1,y=p2,color = 'green')\nplt.xticks(rotation=45)\nplt.title('Top 20 Positive Word')\nplt.show()\n\nsns.barplot(x=n1,y=n2,color='red')\nplt.xticks(rotation=45)\nplt.title('Top 20 Negative Word')\nplt.show()\n\nsns.barplot(x=nu1,y=nu2,color='yellow')\nplt.xticks(rotation=45)\nplt.title('Top 20 Neutral Word')\nplt.show()\n","faebbd18":"train_df.head()","f663479d":"test_df.head()","b918cb71":"#Wordclouds\n# Wordclouds to see which words contribute to which type of polarity.\n\nfrom wordcloud import WordCloud\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Positive text',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative text',fontsize=40);\n\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Neutral text',fontsize=40)","ef6fad48":"# https:\/\/www.kaggle.com\/ekhtiar\/unintended-eda-with-tutorial-notes\ndef generate_word_cloud(df_data, text_col):\n    # convert stop words to sets as required by the wordcloud library\n    stop_words = set(stopwords.words(\"english\"))\n    \n    data_neutral = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"neutral\", text_col].map(lambda x: str(x).lower()))\n    data_positive = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"positive\", text_col].map(lambda x: str(x).lower()))\n    data_negative = \" \".join(df_data.loc[df_data[\"sentiment\"]==\"negative\", text_col].map(lambda x: str(x).lower()))\n\n    wc_neutral = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_neutral)\n    wc_positive = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_positive)\n    wc_negative = WordCloud(max_font_size=100, max_words=100, background_color=\"white\", stopwords=stop_words).generate(data_negative)\n\n    # draw the two wordclouds side by side using subplot\n    fig, ax = plt.subplots(1, 3, figsize=(20, 20))\n    ax[0].set_title(\"Neutral Wordcloud\" , fontsize=10)\n    ax[0].imshow(wc_neutral, interpolation=\"bilinear\")\n    ax[0].axis(\"off\")\n    \n    ax[1].set_title(\"Positive Wordcloud\", fontsize=10)\n    ax[1].imshow(wc_positive, interpolation=\"bilinear\")\n    ax[1].axis(\"off\")\n    \n    ax[2].set_title(\"Negative Wordcloud\", fontsize=10)\n    ax[2].imshow(wc_negative, interpolation=\"bilinear\")\n    ax[2].axis(\"off\")\n    plt.show()\n    \n    return [wc_neutral, wc_positive, wc_negative]","a880a5de":"train_text_wc = generate_word_cloud(train_df, \"text\")","0dd29a25":"train_sel_text_wc = generate_word_cloud(train_df, \"selected_text\")","a6f1180d":"train_df.head()","63c12f69":"train_df['Num_words_text'] = train_df['text'].apply(lambda x: len(str(x).split()))","6299f451":"train_df.head()","b601fb2a":"train_df = train_df.loc[train_df['Num_words_text']>=3]","5a4291ac":"train_df.head()","89a61f26":"train_df.isnull().sum()","4dabb0ec":"def save_model(output_dir, nlp, new_model_name):\n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","347100da":"def train(train_data, output_dir, n_iter=30, model=None):\n    \"\"\"Load the model,set up the pipeline and train the entity recognizer\"\"\"\n    if model is not None:\n        nlp=spacy.load(model) #load existing spaCy model\n        print(\"Loaded model '%s'\" %model)\n    else:\n        nlp = spacy.blank(\"en\") #create blank Language class\n        print(\"Created blank 'en' model \")\n        \n        # The pipeline execution\n        # Create the built-in pipeline components and them to the pipeline\n        # nlp.create_pipe works for built-ins that are registered in the spacy\n        \n        if \"ner\" not in nlp.pipe_names:\n            ner = nlp.create_pipe(\"ner\")\n            nlp.add_pipe(ner,last=True)\n            \n        # otherwise, get it so we can add labels\n        \n        else:\n            ner = nlp.get_pipe(\"ner\")\n            \n        # add labels \n        for _, annotations in train_data:\n                for ent in annotations.get(\"entities\"):\n                    ner.add_label(ent[2])\n        \n        # get names of other pipes to disable them during training\n        \n        pipe_exceptions = [\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        \n        #other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n        \n        with nlp.disable_pipes(*other_pipes): # training of only NER\n            \n            # reset and intialize the weights randoml - but only if we're\n            # training a model\n            \n            if model is None:\n                nlp.begin_training()\n            else:\n                nlp.resume_training()\n            \n            for itn in tqdm(range(n_iter)):\n                random.shuffle(train_data)\n                losses={}\n                \n                # batch up the example using spaCy's mnibatch\n                batches = minibatch(train_data,size=compounding(4.0,1000.0,1.001))\n                \n                for batch in batches:\n                    texts , annotations = zip(*batch)\n                    nlp.update(\n                        texts, #batch of texts\n                        annotations, # batch of annotations\n                        drop = 0.5,  # dropout - make it harder to memorise data\n                        losses = losses,\n                )\n            print(\"Losses\", losses)\n        save_model(output_dir, nlp, 'st_ner')\n                    ","613a986e":"def get_model_out_path(sentiment):\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    else:\n        model_out_path = 'models\/model_neu'\n    return model_out_path","c7dc672f":"\ndef get_training_data(sentiment):\n    train_data=[]\n    '''\n    Returns Training data in the format needed to train spacy NER\n    '''\n    for index,row in train_df.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start,end,'selected_text']]}))\n    return train_data","d35f04f6":"# Training Positive sentiments\nsentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","a6109cfd":"# Training Negative Sentiment\n\nsentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","f3b9fa9d":"# Training Neutral Sentiment\n\nsentiment = 'neutral'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\n# Training model iteration \ntrain(train_data, model_path, n_iter=10, model=None)","e751a8e0":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","293cf409":"TRAINED_MODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'","8bef8808":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nif TRAINED_MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", TRAINED_MODELS_BASE_PATH)\n    model_pos = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(TRAINED_MODELS_BASE_PATH + 'model_neu')\n        \n    jaccard_score = 0\n    for index, row in tqdm(train_df.iterrows(), total=train_df.shape[0]):\n        text = row.text\n        if row.sentiment == 'neutral':\n            jaccard_score += jaccard(predict_entities(text, model_neu), row.selected_text)\n        elif row.sentiment == 'positive':\n            jaccard_score += jaccard(predict_entities(text, model_pos), row.selected_text)\n        else:\n            jaccard_score += jaccard(predict_entities(text, model_neg), row.selected_text) \n        \n    print(f'Average Jaccard Score is {jaccard_score \/ train_df.shape[0]}') \n","41ef670b":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","99f6e473":"MODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'","0fdf8684":"selected_texts = []\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n    model_neu = spacy.load(MODELS_BASE_PATH + 'model_neu')\n        \n    for index, row in test_df.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) < 2:\n#             output_str = text\n#             selected_texts.append(predict_entities(text, model_neu))\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ntest_df['selected_text'] = selected_texts","d86b2cec":"test_df.head(10)","a55d25a4":"submission_df['selected_text'] = test_df['selected_text']\nsubmission_df.to_csv(\"submission.csv\", index=False)\ndisplay(submission_df.head(10))","b2baa423":"##### The histogram shows that the length of the cleaned text ranges from around 2 to 140 characters and generally,it is almost same for all the polarities","cd9ac010":"## Importing Necesary Library"}}