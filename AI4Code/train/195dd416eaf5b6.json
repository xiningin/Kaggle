{"cell_type":{"63d1dd22":"code","28fb95c9":"code","cc26bb84":"code","3b19a78b":"code","b5be7728":"code","8c931660":"code","cf87aaa6":"code","3570f69d":"code","5b35e6cc":"code","e61fc975":"code","e2f2d7cb":"code","fa42fb92":"code","33bf9f9b":"code","f2aa21e2":"code","a6f2bffc":"markdown","9736d450":"markdown","77cc6da5":"markdown","fb603fa0":"markdown","d21da1b4":"markdown","d3a5b521":"markdown","3806f54f":"markdown","4d3378b9":"markdown","3e068b9e":"markdown","dc026116":"markdown"},"source":{"63d1dd22":"import numpy as np # Linear Algebra Library\nimport pandas as pd # Data Processing Library\nimport matplotlib.pyplot as plt # Visualize Library","28fb95c9":"df = pd.read_csv(\"..\/input\/voice.csv\")\ndf.label = [1 if each == \"male\" else 0 for each in df.label]","cc26bb84":"df.head()","3b19a78b":"print(df.info())","b5be7728":"df.describe()","8c931660":"y = df.label.values\nx_data = df.drop(\"label\",axis = 1) ","cf87aaa6":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))","3570f69d":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train : \",x_train)\nprint(\"x_test : \",x_test)\nprint(\"y_train : \",y_train)\nprint(\"y_test : \",x_test)","5b35e6cc":"def initialize_weight_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.00\n    return w,b\ndef sigmoid(z):\n    y_head = 1\/ (1+np.exp(-z))\n    return y_head","e61fc975":"def forward_backward_propagation(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    #for forward\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    #for backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","e2f2d7cb":"def update(w,b,x_train,y_train,learning_rate,number_of_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []    \n    \n    for i in range(number_of_iterations):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train,learning_rate,number_of_iterations)\n        cost_list.append(cost)\n        w =  w - learning_rate*gradients[\"derivative_weight\"]\n        b =  b - learning_rate*gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i : %f\" %(i,cost))\n        \n        \n        \n    parameters = {\"weight\":w,\"bias\":b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation = \"vertical\")\n    plt.xlabel(\"Number of Iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","fa42fb92":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+ b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    return Y_prediction","33bf9f9b":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,number_of_iterations):\n    dimension = x_train.shape[0]\n    w,b = initialize_weight_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w,b,x_train,y_train,learning_rate,number_of_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n    print(\"test accuracy : {} %\".format(100-np.mean(np.abs(y_prediction_test - y_test))*100))\n\n\nlogistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,number_of_iterations = 500) ","f2aa21e2":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","a6f2bffc":"### Initializing weights and bias to our model.","9736d450":"### Normalization","77cc6da5":"<div id=\"2\"\/>\n## Sklearn Logistic Regression\n>","fb603fa0":"<div id=\"1\"\/>\n## EDA (Explotary Data Analysis)","d21da1b4":"## Welcome to my kernel ! \n## <br>What you will find here . \n\n*      [EDA (Explotary Data Analysis)](#1)\n*      [Hand-made Forward-Backward Functions](#2)\n*     [Sklearn-Logistic Regression](#3)","d3a5b521":"### Depend on our function we need to update our data.","3806f54f":"### We need forward & backward propagation to decrease loss function","4d3378b9":"### Train Test Split with Sklearn ","3e068b9e":"<div id=\"2\"\/>\n## Hand-made Forward-Backward Functions\n\n> ","dc026116":"These all functions all we have defined we can find all of it in sklearn library. Why we have done it? Actually we need to learn in fundamentally. Now you get its functionality."}}