{"cell_type":{"64f8c9b6":"code","2d1b561f":"code","bc91f7c1":"code","002aa26e":"code","a4844d48":"code","3cbc968e":"code","23f3d182":"code","aebcec42":"code","210e230c":"code","c1ad622d":"code","03db6495":"code","c41505f3":"code","2e5f0191":"code","a43476a7":"code","7cbd9f87":"code","a924e9cc":"code","c65eb688":"code","4191e3a2":"code","a99b71c6":"code","07e1f2cc":"code","4fe7db50":"code","42c055eb":"code","dc91e781":"code","87681e0f":"code","4dd3b6d7":"code","e7fdc466":"code","b37bc862":"code","c78c2f7f":"code","f57728b0":"code","394a7efc":"code","cd623659":"code","8d81905b":"code","093ad15f":"code","48013f0f":"code","9d420653":"code","0a7a9348":"code","421f2035":"code","f4311348":"code","c921e78e":"markdown","079442de":"markdown","ee81502f":"markdown","101c1ac6":"markdown","615af9ff":"markdown","60802f81":"markdown","5d40543a":"markdown","16e86581":"markdown","b7abc517":"markdown","9a3adde4":"markdown","796256de":"markdown","8acfde8b":"markdown","15c6cd15":"markdown","181a4abb":"markdown","c81ec10d":"markdown","7ee10509":"markdown","ad5237ca":"markdown","b12894b0":"markdown","6fad08df":"markdown","20916f83":"markdown","70a1c246":"markdown","1271cd49":"markdown","e12d6ab1":"markdown","5d9323be":"markdown","a55440ca":"markdown","0aa179dc":"markdown","bad5dbf6":"markdown","e4a0b1fd":"markdown","c7b4d950":"markdown","f552cc18":"markdown","9f8dcdc3":"markdown","61cdb145":"markdown","864243fe":"markdown","5b804683":"markdown","0d63f640":"markdown","0bd16048":"markdown","3d52ad0e":"markdown"},"source":{"64f8c9b6":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n","2d1b561f":"%time\ntrain = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\",nrows=1e5)\ntest = pd.read_csv(\"..\/input\/jane-street-market-prediction\/example_test.csv\")","bc91f7c1":"print(f\"Train data contains {train.shape[0]} rows and {train.shape[1]} features\")\nprint(f\"Example test data contains {test.shape[0]} rows and {test.shape[1]} features\")","002aa26e":"train.head(5)","a4844d48":"temp = pd.DataFrame(train.isna().sum().sort_values(ascending=False)*100\/train.shape[0],columns=['missing %']).head(20)\ntemp.style.background_gradient(cmap='Purples')","3cbc968e":"train=train[train['weight']!=0]\ntrain['action']=(train['resp']>0)*1\ntrain.action.value_counts()","23f3d182":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(train.action.values,ax=ax[0],palette='husl')\nsns.violinplot(x=train.action.values, y=train.index.values, ax=ax[1], palette=\"husl\")\nsns.stripplot(x=train.action.values, y=train.index.values,\n              jitter=True, ax=ax[1], color=\"black\", size=0.5, alpha=0.5)\nax[1].set_xlabel(\"Target\")\nax[1].set_ylabel(\"Index\");\nax[0].set_xlabel(\"Target\")\nax[0].set_ylabel(\"Counts\");","aebcec42":"def plot_features(df1,target='action',features=[]):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,5,figsize=(14,14))\n    \n    \n    for feature in features:\n        i += 1\n        plt.subplot(5,5,i)\n        sns.distplot(df1[df1[target]==1][feature].values,label='1')\n        sns.distplot(df1[df1[target]==0][feature].values,label='0')\n        plt.xlabel(feature, fontsize=9)\n        plt.legend()\n    \n    plt.show();\n    ","210e230c":"plot_features(train,features=[f'feature_{i}' for i in range(25)])","c1ad622d":"plot_features(train,features=[f'feature_{i}' for i in range(25,50)])","03db6495":"plot_features(train,features=[f'feature_{i}' for i in range(50,75)])","c41505f3":"\nfig,ax = plt.subplots(1,2,figsize=(12,6))\nplt.subplot(1,2,1)\nplt.title(\"Distribution of weight\")\nsns.distplot(train['weight'],color='blue',kde=True,bins=100)\n\nt0 = train[train['action']==0]\nt1 =  train[train['action']==1]\nplt.subplot(1,2,2)\nsns.distplot(train['weight'],color='blue',kde=True,bins=100)\nsns.distplot(t0['weight'],color='blue',kde=True,bins=100,label='action = 0')\nsns.distplot(t1['weight'],color='red',kde=True,bins=100,label='action = 1')\nplt.legend()\n\n\n","2e5f0191":"fig,ax = plt.subplots(2,2,figsize=(12,10))\nfor i,col in enumerate([f'resp_{i}' for i in range(1,5)]):\n    plt.subplot(2,2,i+1)\n    plt.scatter(train[train.weight!=0].weight,train[train.weight!=0][col])\n    plt.ylabel(col)\n    plt.xlabel('weight')\nplt.show()","a43476a7":"def plot_resp():\n    fig,ax = plt.subplots(2,2,figsize=(12,10))\n    i=1\n    for col in ([f'resp_{i}' for i in range(1,5)]):\n        \n        plt.subplot(2,2,i)\n        plt.plot(train.ts_id.values,train.resp.values,label='resp',color='blue')\n        plt.plot(train.ts_id.values,train[f'resp_{i}'].values,label=f'resp_{i}',color='red')\n        plt.xlabel('ts_id')\n        plt.legend()\n        \n        i+=1\n    plt.show()\n    \nplot_resp()","7cbd9f87":"plt.figure(figsize=(10,10))\nplt.scatter(train.resp.values,train.resp_1.values,color='red',label='resp_1')\nplt.scatter(train.resp.values,train.resp_2.values,color='blue',label='resp_2')\nplt.scatter(train.resp.values,train.resp_3.values,color='orange',label='resp_3')\nplt.scatter(train.resp.values,train.resp_4.values,color='green',label='resp_4')\nplt.xlabel(\"resp\")\nplt.ylabel('other resp variables')\nplt.legend()\n\n\n","a924e9cc":"plt.figure(figsize=(8,6))\nfor col in [f'resp_{i}' for i in range(1,5)]:\n    plt.plot(train[col].cumsum().values,label=col)   \nplt.legend()\nplt.title(\"resp in different time horizons\")\nplt.show()","c65eb688":"\nsns.countplot(train.feature_0)","4191e3a2":"features = [col for col in train.columns if 'feature' in col]\nt0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","a99b71c6":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","07e1f2cc":"features = [col for col in train.columns if 'feature' in col]\nt0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per row in the train set\")\nsns.distplot(t0[features].std(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","4fe7db50":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of standard deviation values per column in the train set\")\nsns.distplot(t0[features].std(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].std(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","42c055eb":"t0 = train.loc[train['action'] == 0]\nt1 = train.loc[train['action'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","dc91e781":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","87681e0f":"train_corr = train[features].corr().values.flatten()\ntrain_corr = train_corr[train_corr!=1]\ntest_corr = test[features].corr().values.flatten()\ntest_corr = test_corr[test_corr!=1]\n\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_corr, color=\"Red\", label=\"train\")\nsns.distplot(test_corr, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend();","4dd3b6d7":"plt.figure(figsize=(8,5))\npca = PCA().fit(train[features].iloc[:,1:].fillna(train.fillna(train.mean())))\nplt.plot(np.cumsum(pca.explained_variance_ratio_),linewidth=4)\nplt.axhline(y=0.9, color='r', linestyle='-')\nplt.xlabel(\"number of components\")\nplt.ylabel(\"sum of explained variance ratio\")\nplt.show()","e7fdc466":"rb = RobustScaler()\ndata = rb.fit_transform(train[features].iloc[:,1:].fillna(train[features].fillna(train[features].mean())))\ndata = PCA(n_components=2).fit_transform(data)\nplt.figure(figsize=(7,7))\nsns.scatterplot(data[:,0],data[:,1],hue=train['action'])\nplt.xlabel('pca comp 1')\nplt.ylabel('pca comp 2')","b37bc862":"from sklearn.cluster import KMeans\nX_std = train[[f'feature_{i}' for i in range(1,130)]].fillna(train.mean()).values\nsse = []\nlist_k = list(range(1, 10))\n\nfor k in list_k:\n    km = KMeans(n_clusters=k)\n    km.fit(data)\n    sse.append(km.inertia_)\n\n# Plot sse against k\nplt.figure(figsize=(6, 6))\nplt.plot(list_k, sse, '-o')\nplt.xlabel(r'Number of clusters *k*')\nplt.ylabel('Sum of squared distance');","c78c2f7f":"knn = KMeans(n_clusters=2)\nlabels=knn.fit_predict(data)\nsns.scatterplot(data[:,0],data[:,1],hue=labels)","f57728b0":"target='action'\ncols_drop = list(np.setdiff1d(train.columns,test.columns))+['ts_id','date']\n\nclf = RandomForestClassifier()\nclf.fit(train.drop(cols_drop,axis=1).fillna(-999),train['action'])","394a7efc":"top=20\ntop_features = np.argsort(clf.feature_importances_)[::-1][:top]\nfeature_names = train.drop(cols_drop,axis=1).iloc[:,top_features].columns","cd623659":"plt.figure(figsize=(8,7))\nsns.barplot(clf.feature_importances_[top_features],feature_names,color='blue')","8d81905b":"top=8\ntop_features = np.argsort(clf.feature_importances_)[::-1][:top]\ntop_features = train.drop(cols_drop,axis=1).iloc[:,top_features].columns\n","093ad15f":"def plot_features(df1,target='action',features=[]):\n    \n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,2,figsize=(14,14))\n    \n    \n    for feature in features:\n        i += 1\n        plt.subplot(4,2,i)\n        sns.distplot(df1[df1[target]==1][feature].values,label='1')\n        sns.distplot(df1[df1[target]==0][feature].values,label='0')\n        plt.xlabel(feature, fontsize=9)\n        plt.legend()\n    \n    plt.show();\n    \nplot_features(train,features=top_features)","48013f0f":"sns.pairplot(train[list(feature_names[:10])+['action']],hue='action')","9d420653":"import shap\n","0a7a9348":"explainer = shap.TreeExplainer(clf)\nX = train.drop(cols_drop,axis=1).fillna(-999).sample(1000)\nshap_values = explainer.shap_values(X)","421f2035":"shap.summary_plot(shap_values, X, plot_type=\"bar\")\n","f4311348":"shap.dependence_plot('feature_35', shap_values[1], X, display_features=X.sample(1000))\n","c921e78e":"- Most of weights are in range of 0 to 20 and resp variables are in range of -0.05 to 0.05\n- It seems that all of the resp variables follows almost same pattern.\n- The outlier remains almost the same in all cases.","079442de":"### <font size ='4' color='blue'><a> Distribution of min values per row in the train set<\/a><\/font>","ee81502f":"### <font size ='4' color='blue'><a> Distribution of top features <\/a><\/font>\nLet's check the feature value distribution for each target for the topn 8 features.","101c1ac6":"## <font size ='5' color='blue'><a>Are there correlations between features?<\/a><\/font>\nLet's check the of there are highly correlated features in our data.","615af9ff":"- We only need less than 20 PCA components to explain 90% of varience of features.\n\nNow,let's check if there exists any clusters","60802f81":"### <font size ='4' color='blue'><a> Distribution of min values per column in the train set<\/a><\/font>","5d40543a":"The distribution of weight is highly left skewed,which indicates that there are many samples with 0 weight which we will have to remove.","16e86581":"- The class distribution seems to be almost the same.\n- There is no relation between the target and index value.","b7abc517":"## <font size ='5' color='blue'><a>KMeans clustering <\/a><\/font><a id=\"5\" ><\/a>\nLet's first chose number of clusters K by using elbow method","9a3adde4":"Now,let's cluster and see..","796256de":"#### <font size ='4' color='blue'><a> feature_0<\/a><\/font>\nfeature_0 is a catergorical variable,let's check it's distribution.\n","8acfde8b":"#### <font size ='4' color='blue'><a> Load data files <\/a><\/font>","15c6cd15":"###  <p><font size=\"5\" color=\"blue\">Overview<\/font><\/p><a id=\"1\" ><\/a>\n![](https:\/\/media.giphy.com\/media\/S4178TW2Rm1LW\/giphy.gif)\n\n##### <p><font size='3' color='blue'>In this challenge, our task is to build a quantitative trading model to maximize returns using market data from a major global stock exchange. Each row in the dataset represents a trading opportunity, for which you will be predicting an action value: 1 to make the trade and 0 to pass on it.<\/font><\/p>","181a4abb":"#### <font size ='4' color='blue'><a> Weight <\/a><\/font>\nLet's check the distribution of values of weight feature","c81ec10d":"#### <font size ='4' color='blue'><a> Target distribution <\/a><\/font><a id=\"3\" ><\/a>","7ee10509":"#### <font size ='4' color='blue'><a> Resp <\/a><\/font>\nLet's compare values of resp_1,resp_2,resp_3,resp_4 with resp. There are changes in different time zones.","ad5237ca":"## <font size ='5' color='blue'><a> Exploratory Data analysis <\/a><\/font><a id=\"4\" ><\/a>\n#### <font size ='4' color='blue'><a> Some Feature distribution <\/a><\/font><a id=\"4\" ><\/a>\nLet's check the distribution of some features for each target","b12894b0":"### <font size ='4' color='blue'><a>Distribution of standard deviation values per row in the train set<\/a><\/font>","6fad08df":"### <p><font size ='4' color='blue'> Import libraries<\/font><\/p>","20916f83":"- Most of the values have linear relationship with resp variable.\n","70a1c246":"- <p><font size='3' color='red'> Unhide output to see feature distribution<\/font><\/p>","1271cd49":"## <font size ='5' color='blue'><a> Basic Idea <\/a><\/font><a id=\"2\" ><\/a>","e12d6ab1":"<html>\n    <body>\n        <p><font size=\"6\" color=\"blue\">Contents<\/font><\/p>\n    <\/body>\n    \n- [Overview](#1)\n- [Basic Idea](#2)\n- [Target distribution](#3)\n- [Exploratory data analysis](#4)\n- [Random Forest Feature Selection](#5)\n- [Explainability](#6)    ","5d9323be":"## <font size ='4' color='blue'><a>Top feature interactions <\/a><\/font><a id=\"6\" ><\/a>\n","a55440ca":"#### <font size ='4' color='blue'><a> Missing Values <\/a><\/font>","0aa179dc":"- <p><font size='3' color='red'> Unhide output to see feature distribution<\/font><\/p>","bad5dbf6":"### <font size ='5' color='blue'><a>Feature Stats<\/a><\/font>\n#### <font size ='4' color='blue'><a> Distribution of mean values per row in the train set <\/a><\/font>\n","e4a0b1fd":"There are some highly correlated features in our data,we should probably remove them in the future.","c7b4d950":"#### <font size ='4' color='blue'><a>How response variables changes with weight factor? <\/a><\/font>\n","f552cc18":"## <font size ='5' color='blue'><a> Random Forest feature importances <\/a><\/font><a id=\"5\" ><\/a>\nLet's build a quick tree based model and see which all are the most important features.","9f8dcdc3":"### <font size ='4' color='blue'><a>SHAP Dependence Plots<\/a><\/font>\nSHAP dependence plots show the effect of a single feature across the whole dataset. They plot a feature's value vs. the SHAP value of that feature across many samples. Let's take a look at `feature_35`","61cdb145":"### <font size ='4' color='blue'><a> Distribution of mean values per column in the train set <\/a><\/font>","864243fe":"#### <font size ='4' color='blue'><a> Cumilative sum of response variable in diff time horizons<\/a><\/font>\n","5b804683":"### <p><font size ='4' color=\"red\">Please do an upvote if you liked it :)<\/font><\/p>\n### <font size ='3' color='blue'><a> References. <\/a><\/font>\n- https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n","0d63f640":"## <font size ='4' color='blue'><a>SHAP Exaplainability <\/a><\/font><a id=\"6\" ><\/a>\nLet's take a look at SHAP feature importance values\n","0bd16048":"### <font size ='5' color='blue'><a>PCA components of feature varibles<\/a><\/font>\n","3d52ad0e":"### <font size ='4' color='blue'><a> Distribution of std values per column in the train set <\/a><\/font>"}}