{"cell_type":{"cb85319d":"code","b8b0a71d":"code","de69d438":"code","eebfaee2":"code","ca8773a1":"code","0459dffc":"code","b5d770d2":"code","b7209ffe":"code","e7ee92f6":"code","39cd871d":"code","4314bfa8":"code","8a5c8f64":"code","d4256852":"code","e2e93be8":"code","921cf6a6":"code","e6d65391":"code","85999316":"code","5bafbb34":"code","6f003a18":"code","21c3bbfc":"markdown","3acf5bbc":"markdown","3e430d3a":"markdown","a089b7c5":"markdown","c464931f":"markdown"},"source":{"cb85319d":"import keras\nimport tensorflow as tf\nimport keras.layers as layers\nfrom keras.preprocessing.image import load_img, img_to_array, array_to_img\nimport tensorflow_probability as tfp\nimport numpy as np\nfrom math import floor, ceil\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow import math, random, shape\nimport os\nfrom keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom keras.optimizers import Nadam, SGD, Adam, Adamax\nfrom keras.activations import sigmoid\nfrom tensorflow import convert_to_tensor as tens\nfrom keras import backend as K\nfrom cv2 import getGaborKernel as Gabor\nfrom functools import reduce\nfrom matplotlib import pyplot as plt\nfrom math import sqrt\nimport itertools\nimport re\nfrom random import shuffle, seed\nfrom tensorflow.keras.utils import Sequence\nfrom keras.constraints import NonNeg\nfrom keras.regularizers import l1,l2,l1_l2\nfrom keras.initializers import RandomNormal\nimport pickle\nimport time","b8b0a71d":"BATCH_SIZE = 32\nEXCITATORY_SYNAPSES_WANTED = 40\nINHIBITORY_SYNAPSES_WANTED = 15\nPRESYNAPTIC_THRESHOLD = .001\nN_EXC = 639\nSHD_NEURONS = 700\nSHD_MAT_TIME = 1400\n\n\nLABELS = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\nLABELS += [\"null\", \"eins\", \"zwei\", \"drei\", \"vier\", \"f\u00fcnf\", \"sechs\", \"sieben\", \"acht\", \"neun\"]\nNLABELS = 10","de69d438":"class SHDDirectoryGeneratorSequence(Sequence):\n    def __init__(self, dct_label, dtype='.jpeg', balance=True, randomize=True, noise=0, random_seed=1331, validation_split=None, is_validation=False, batch_size=32):\n        self.directories = dct_label\n        self.dtype = dtype\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self.batch_size = batch_size\n        self.x = self.y = None\n        self.balance = True\n        self.noise = None if not noise else NoiseLayer(noise)\n        if set(dct_label.values()) != {0,1}: raise Exception(\"Labels should be 0 and 1.\")\n            \n        self._create_files()\n        print(self)\n        \n    def _create_files(self):\n        lbls_dct = {0:[], 1:[]}\n        files = []\n        for directory,label in self.directories.items():\n            lbls_dct[label] += [(directory+ ('' if directory[-1] == '\/' else '\/') +i, label) \n                                for i in os.listdir(directory) if re.findall(self.dtype, i)]\n        if self.balance:\n\n            seed(self.seed)\n            l0 = len(lbls_dct[0])\n            l1 = len(lbls_dct[1])\n            if l0 > l1:\n                arr = np.array([1]*l1+[0]*(l0-l1))\n                shuffle(arr)\n                new = []\n                for i, j in zip(lbls_dct[0], list(arr)):\n                    if j: new.append(i)\n                lbls_dct[0] = new\n            elif l0<l1:\n                arr = np.array([1]*l0+[0]*(l1-l0))\n                shuffle(arr)\n                new = []\n                for i, j in zip(lbls_dct[1], list(arr)):\n                    if j: new.append(i)\n                lbls_dct[1] = new\n        files = lbls_dct[0] + lbls_dct[1]\n                \n        if self.randomize:\n            seed(self.seed)\n            shuffle(files)\n        if self.validation_split:\n            if self.is_validation:\n                files = files[floor(len(files) - len(files)*self.validation_split):]\n            else:\n                files = files[:floor(len(files) - len(files)*self.validation_split)]\n                \n        self.x, self.y = zip(*files)\n        self.y = np.array(self.y)\n    \n    def __len__(self):\n        return ceil(self.y.shape[0] \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_x_pre, batch_y = self.x[idx * self.batch_size:(idx + 1) * self.batch_size], self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = [self.getSpikeTrain(file) for file in batch_x_pre]\n        return np.array(batch_x), batch_y\n    \n    def getSpikeTrain(self, file):\n        image = load_img(file)\n        image = img_to_array(image)[:,:,0] \/ 255\n        if self.noise is not None:\n            image = self.noise(image)\n        return image\n    \n    def __str__(self):\n        return (\"Training\" if not self.is_validation else \"Validation\") + f\" generator: Got {len(self.x)} files and 2 categories\"","eebfaee2":"class SHDDirectoryGeneratorSequenceCategories(Sequence):\n    def __init__(self, dct_label, dtype='.jpeg', randomize=True, random_seed=1331, validation_split=None, is_validation=False, batch_size=32):\n        self.directories = dct_label\n        self.categories = sorted(list({cat for cat in dct_label.values()}))\n        self.dtype = dtype\n        self.randomize = randomize\n        self.seed = random_seed\n        self.validation_split = validation_split\n        self.is_validation = is_validation\n        self.batch_size = batch_size\n        self.x = self.y = None\n        self._create_files()\n        \n    def _create_files(self):\n        files = []\n        for directory,label in self.directories.items():\n            files += [(directory+ ('' if directory[-1] == '\/' else '\/') +i, \n                       [1. if cat==label else 0. for cat in self.categories]) \n                           for i in os.listdir(directory) \n                           if re.findall(self.dtype, i)]\n        if self.randomize:\n            seed(self.seed)\n            shuffle(files)\n        if self.validation_split:\n            if self.is_validation:\n                files = files[floor(len(files) - len(files)*self.validation_split):]\n            else:\n                files = files[:floor(len(files) - len(files)*self.validation_split)]\n                \n        self.x, self.y = zip(*files)\n        self.y = np.array(self.y)\n        print(self)\n    \n    def __len__(self):\n        return ceil(self.y.shape[0] \/ self.batch_size)\n    \n    def __getitem__(self, idx):\n        batch_x_pre, batch_y = self.x[idx * self.batch_size:(idx + 1) * self.batch_size], self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_x = [self.getSpikeTrain(file) for file in batch_x_pre]\n        return np.array(batch_x), batch_y\n    \n    def getSpikeTrain(self, file):\n        image = load_img(file)\n        image = img_to_array(image)[:,:,0] \/ 255\n        return image\n    def __str__(self):\n        return (\"Training\" if not self.is_validation else \"Validation\") + f\" generator: Got {len(self.x)} files and {len(self.categories)} categories\"","ca8773a1":"class NoiseLayer(keras.layers.Layer):\n    def __init__(self, p=.003):\n        super().__init__(name=\"NoiseLayer\")\n        self.p = p\n        \n    def call(self, inputs, training=None):\n        if training is not True:\n            noise = layers.Lambda(lambda x: self.get_rand(x))(tf.shape(inputs))\n            together = K.stack([inputs, noise], axis=-1)\n            return together.max(axis=-1)\n        else: return inputs\n        \n    def get_rand(self, shape):\n        return K.random_bernoulli(shape, self.p, dtype=tf.int32)[0]","0459dffc":"# LABELS = [\"NoI\", \"I\"]\n# INums = {3, 5, 6, 8, 9}\n# train_ds = SHDDirectoryGeneratorSequence({f\"..\/input\/matrixshd\/train\/{i}\": int(i in INums) for i in range(10)}, validation_split=0.2, batch_size=BATCH_SIZE)\n# valid_ds = SHDDirectoryGeneratorSequence({f\"..\/input\/matrixshd\/train\/{i}\": int(i in INums) for i in range(10)}, validation_split=0.2, is_validation=True, batch_size=BATCH_SIZE)\n# test_ds = SHDDirectoryGeneratorSequence({f\"..\/input\/matrixshd\/test\/{i}\": int(i in INums) for i in range(10)}, validation_split=0, is_validation=False, batch_size=BATCH_SIZE)","b5d770d2":"train_ds = SHDDirectoryGeneratorSequenceCategories({f\"..\/input\/matrixshd\/train\/{i}\": i for i in range(10)}, validation_split=0.2, batch_size=BATCH_SIZE)\nvalid_ds = SHDDirectoryGeneratorSequenceCategories({f\"..\/input\/matrixshd\/train\/{i}\": i for i in range(10)}, validation_split=0.2, is_validation=True, batch_size=BATCH_SIZE)\ntest_ds = SHDDirectoryGeneratorSequenceCategories({f\"..\/input\/matrixshd\/test\/{i}\": i for i in range(10)}, validation_split=0, is_validation=False, batch_size=BATCH_SIZE)","b7209ffe":"plt.figure(figsize=(40,40))\nfor image, label in train_ds:\n    labels = np.where(label==1)[1]\n    for i in range(BATCH_SIZE):\n        plt.subplot(BATCH_SIZE\/\/4, 4, i+1)\n        plt.imshow(image[i], aspect='auto', origin='lower', cmap=\"binary\")\n        plt.title(LABELS[labels[i]])\n    break","e7ee92f6":"dataset_folder = '\/kaggle\/input\/single-neurons-as-deep-nets-nmda-test-data'\n\nmodels_folder  = os.path.join(dataset_folder, 'Models')\n\nmodel_name = \"NMDA_TCN__DWT_8_224_217__model.h5\"\n# model_name = 'NMDA_TCN__DWT_7_128_153__model.h5'\n\nmodel_filename  = os.path.join(models_folder, model_name)\n\n\n\nold_model = keras.models.load_model(model_filename)\n# old_model.summary()\n\ninp = keras.Input(shape=old_model.layers[0].input.shape[1:])\nx = old_model.layers[1](inp)\nfor layer in old_model.layers[2:-3]:\n    x = layer(x)\noutput = old_model.layers[-3](x),old_model.layers[-2](x) \nL5PC_model = keras.Model(inp, output, name=\"L5PC\")\n\nfor layer in L5PC_model.layers:\n    layer.trainable = False\n\nL5PC_model.summary()","39cd871d":"class WiringLayer(keras.layers.Layer):\n    def __init__(self, filters=1278):\n        super().__init__(name=\"WiringLayer\")\n        self.filters = filters\n        \n    def build(self, shape):\n        self.conv = keras.layers.Conv1D(self.filters, 1, use_bias=False)\n        self.conv.build(shape)\n        weights = np.zeros((1, shape[-1], self.filters))\n        weights[:, 0] = 1\n        np.apply_along_axis(np.random.shuffle,-2,weights) \n        self.conv.set_weights([weights])\n        self.conv.trainable = False\n\n    def call(self, x):\n        return self.conv(x)","4314bfa8":"synapses = 30\npresynaptic = 20\ntime = 50\n\nwiring_layer = WiringLayer(synapses)\nwiring_layer.build((time, presynaptic))\n\n# example input\ninp = np.zeros((1, time, presynaptic))\ninp[:, 5:10, 0] = 1\ninp[:,20:25, 0] = 1\ninp[:, 12:18, 1] = 1\ninp[:, 20:time, presynaptic\/\/2] = 1\n\nresult = wiring_layer(inp)\nweights = wiring_layer.get_weights()[0]\n\nplt.figure(figsize=((10, 10)))\nplt.suptitle(\"Example of Direct Random Wiring (One Presynaptic Neuron per Synapse)\")\n\nplt.subplot(2,2,1)\nplt.imshow(weights[0], cmap=\"binary\", aspect=\"auto\")\nplt.title(\"Wiring Layer Weights (Convolution Weights)\")\nplt.ylabel(\"preSynaptic Neuron\")\nplt.xlabel(\"Synapse (kernel)\")\n\nplt.subplot(2,2,2)\nplt.title(\"Spike Train (Example Input)\")\nplt.imshow(inp[0,:,:].T, aspect='auto', cmap=\"binary\")\nplt.ylabel(\"preSynaptic Neuron\")\nplt.xlabel(\"Time (ms)\")\nplt.xticks([])\nplt.yticks([])\n\nplt.subplot(2,2,4)\nplt.title(\"Synapse Train (Neuron input)\")\nplt.ylabel(\"Synapse\")\nplt.xlabel(\"Time (ms)\")\nplt.imshow(result[0,:,:].numpy().T, aspect='auto', cmap=\"binary\")\n\nplt.show()","8a5c8f64":"class CallNeuron(keras.layers.Layer):\n    def __init__(self, neuronModule, spare=150, name=\"CallNeuronLayer\"):\n        super().__init__(name=f\"CallNeuron-{neuronModule.name}\")\n        self.neuron = neuronModule\n        self.spare = spare\n        self.neuronTime = neuronModule.input.shape[-2]\n        self.synapses = neuronModule.input.shape[-1]\n        self.fullTime = None\n        self.timePerRun = self.neuronTime-self.spare\n        self.times = None\n        self.paddingSize = 0\n        \n    def build(self, shape):\n        print(\"The neuron-mimicing module is:\", self.neuron.name)\n        if shape[-1] != self.synapses:\n            raise Exception(\"Wrong number of synapses!\")\n        self.fullTime = shape[-2]\n        self.paddingSize = (self.spare + self.fullTime)%self.timePerRun\n        self.times = [((self.timePerRun)*i, (self.timePerRun)*i+self.neuronTime) for i in range(self.fullTime\/\/self.timePerRun)]\n        print(\"times:\", self.times)\n        print(\"paddingSize:\", self.paddingSize)\n        print(\"timePerRun:\", self.timePerRun)\n        print(\"fullTime:\", self.fullTime)\n        \n    @tf.autograph.experimental.do_not_convert\n    def call(self, inputs, training=None):\n        inputs = layers.ZeroPadding1D(padding=((0,self.paddingSize)))(inputs)\n        first = self.neuron(inputs[:,self.times[0][0]:self.times[0][1]])\n        after = [self.neuron(inputs[:,i:j]) for i,j in self.times[1:]]\n        aps = [first[0]]+[ap[:,self.spare:] for ap, _ in after]\n        vs = [first[-1]]+[v[:,self.spare:] for _, v in after]\n        concatenated_aps = layers.Concatenate(axis=-2)(aps)\n        concatenated_vs = layers.Concatenate(axis=-2)(vs)\n        outputs = layers.Flatten()(concatenated_aps), layers.Flatten()(concatenated_vs)\n        return outputs","d4256852":"def create_model(drop_rate=.8):\n    inp = keras.Input(shape=(SHD_NEURONS, SHD_MAT_TIME))\n    converted = WiringLayer(2*N_EXC)(tf.transpose(inp, perm=[0,2,1]))\n    if drop_rate: dropped = keras.layers.Lambda(lambda x, training: layers.Dropout(drop_rate)(x, True))(converted)\n    ap, v = CallNeuron(L5PC_model)(dropped if drop_rate else converted)\n    output = (dropped if drop_rate else converted, ap, v)\n    model = keras.Model(inp, output)\n    return model","e2e93be8":"model = create_model()","921cf6a6":"model.summary()","e6d65391":"plt.figure(figsize=(30,10))\nmodel = create_model(0)\nfor image, label in train_ds:\n    converted, ap, v = model(image)\n    converted = np.transpose(converted.numpy(), (0,2,1))\n    labels = np.where(label==1)[1]\n    for i in range(4):\n        \n        start = None\n        ending = None\n        for ms in range(0, image.shape[-1]): \n            if start is None and image[i, :, ms:ms+10].sum(axis=0).mean() > 5:\n                start = ms\n            elif start is not None and image[i, :, ms:ms+10].sum(axis=0).mean()<2:\n                ending = ms\n                break\n        \n        plt.subplot(3, 4, i+1)\n        plt.imshow(image[i],aspect='auto',origin='lower', cmap=\"binary\")\n        plt.title(\"Input - \" + LABELS[labels[i]])\n        if not i: plt.ylabel(\"Cochlear Neurons\")\n        else: plt.yticks([])\n        plt.xticks([])\n        \n        plt.subplot(3,4,4+i+1)\n        plt.imshow(converted[i], cmap=\"binary\", aspect=\"auto\")\n        \n        plt.title(f\"Synapse Train [{start}, {ending}] (Exc.{str(round(converted[i, :N_EXC, start:ending].sum(axis=1).mean(),2))}, Inh.{str(round(converted[i, N_EXC:, start:ending].sum(axis=1).mean(),2))})\")\n        if not i: plt.ylabel(\"L5PC Synapse\")\n        else: plt.yticks([])\n        plt.xticks([])\n        \n        \n        plt.subplot(3,4, 8+i+1)\n        plt.title(\"NN Prediction\")\n        plt.plot(ap[i]*120-80, label=f\"AP {str(round(ap[i].numpy().max()*100))}%\")\n        plt.plot(v[i]-67.7, label=\"v\")\n        if not i: plt.ylabel(\"Soma Voltage (normalised AP %s)\")\n        else: plt.yticks([])\n        plt.xlabel(\"Time (ms)\")\n        plt.ylim((-90, 70))\n        plt.legend()\n    break\n    ","85999316":"main_dir = '..\/working\/shd'\nos.makedirs(main_dir)\nhow_many_to_transform_per_label = None  # set to zero or None to convert all the dataset\ndatasets_to_convert = [(train_ds, \"training\"), (valid_ds, \"validation\"), (test_ds, \"test\")]\ndatasets_to_convert = datasets_to_convert[1:]  # change at your will","5bafbb34":"import shutil\ndef zip_and_delete(directory, zip_name, to_zip=True):\n    if not to_zip: return\n    shutil.make_archive(zip_name, 'zip', directory)\n    print(f'Done Zipping! Check for {zip_name}.')\n    shutil.rmtree(directory)\n    print('Done erasing photos!')","6f003a18":"for ds, dir_name in datasets_to_convert:\n    dct = {lbl: 0 for lbl in range(20)}\n    os.makedirs(main_dir+\"\/\"+dir_name)\n    for lbl in range(10):\n        os.makedirs(f'{main_dir}\/{dir_name}\/{lbl}')\n    \n    for img, label in ds:\n        converted, aps, vs = model(img)\n        labels = np.where(label==1)[1]\n\n        for i in range(converted.shape[0]):\n            im = img[i]\n            lbl = labels[i]\n            inp = converted[i]\n            ap = aps[i]\n            v = vs[i]\n            curDir = f'{main_dir}\/{dir_name}\/{lbl}\/{dct[lbl]}'\n            os.makedirs(curDir)\n            with open(curDir+\"\/image.npy\", 'wb') as f: np.save(f, im)\n            with open(curDir+\"\/matrix.npy\", 'wb') as f: np.save(f, inp)\n            with open(curDir+\"\/spikePrediction.npy\", 'wb') as f: np.save(f, ap)\n            with open(curDir+\"\/voltagePrediction.npy\", 'wb') as f: np.save(f, v)\n            dct[lbl] += 1\n            print(curDir)\n    zip_and_delete(main_dir+'\/'+dir_name, main_dir+'\/'+dir_name, to_zip=True)","21c3bbfc":"# Use David Beniaguev's (selfishgene) trained L5PC model","3acf5bbc":"# Transform To Dataset","3e430d3a":"# Neuron Calling Layer","a089b7c5":"# Module","c464931f":"# Transform data to dataset"}}