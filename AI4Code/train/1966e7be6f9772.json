{"cell_type":{"377daf2f":"code","d03cf54b":"code","36abda51":"code","f4976421":"code","d40f1c1c":"code","93d1f0ae":"code","8d943653":"code","076a1d7a":"code","d4e2b0dd":"code","7b7f61ad":"code","24b73931":"code","252d4aa8":"code","5bc75574":"code","439c8277":"code","71520059":"code","4c0b6ec0":"code","556f6529":"code","59cb7816":"code","026deb4c":"code","37a32264":"markdown","ff116ad5":"markdown","a960c380":"markdown","f62f5377":"markdown","8b5b70be":"markdown","cd6fb503":"markdown","e197e181":"markdown","46a2ba3f":"markdown","8e707f0a":"markdown","5eb4847f":"markdown","c36dba58":"markdown","1c1ce941":"markdown"},"source":{"377daf2f":"from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')","d03cf54b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', strategy.num_replicas_in_sync)","36abda51":"train_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntrain_df.head()","f4976421":"labels, frequencies = np.unique(train_df.language.values, return_counts = True)\nplt.figure(figsize = (8,8))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","d40f1c1c":"model_name = 'bert-base-multilingual-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)","93d1f0ae":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","8d943653":"premise = train_df['premise'].values\nhypothesis = train_df['hypothesis'].values","076a1d7a":"premise = [re.sub('\\d+', '0', s) for s in premise]       #Set all numbers to 0\npremise = [s.lower() for s in premise]                   #English should be all lowercase\nhypothesis = [re.sub('\\d+', '0', s) for s in hypothesis] #Set all numbers to 0\nhypothesis = [s.lower() for s in hypothesis]             #English should be all lowercase","d4e2b0dd":"def bert_encode(premise, hypothesis, tokenizer):\n    num_examples = len(premise)\n    sentence1 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(premise)])\n    sentence2 = tf.ragged.constant([\n        encode_sentence(s)\n        for s in np.array(hypothesis)])\n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis =- 1)\n    input_mask = tf.ones_like(input_word_ids).to_tensor()\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis =- 1).to_tensor()\n    inputs = {\n            'input_word_ids': input_word_ids.to_tensor(),\n            'input_mask': input_mask,\n            'input_type_ids': input_type_ids}\n    return inputs","7b7f61ad":"x_train = bert_encode(premise, hypothesis, tokenizer)","24b73931":"max_len = 20\ndef build_model():\n    bert_encoder = TFBertModel.from_pretrained(model_name)\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_type_ids\")\n    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(embedding[:,0,:])\n    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","252d4aa8":"with strategy.scope():\n    model = build_model()\n    model.summary()","5bc75574":"model.fit(x_train, train_df.label.values, epochs = 8, batch_size = 64)","439c8277":"test_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest_df.head()","71520059":"labels, frequencies = np.unique(test_df.language.values, return_counts = True)\nplt.figure(figsize = (8,8))\nplt.pie(frequencies,labels = labels, autopct = '%1.1f%%')\nplt.show()","4c0b6ec0":"x_test = bert_encode(test_df.premise.values, test_df.hypothesis.values, tokenizer)","556f6529":"y_test = [np.argmax(i) for i in model.predict(x_test)]","59cb7816":"sub = pd.DataFrame({'id': test_df['id'].values, 'prediction': y_test})\nsub.head()","026deb4c":"sub.to_csv('submission.csv', index = False)","37a32264":"**Show the percentage of languages used**","ff116ad5":"**Preprocess data**","a960c380":"**Train using the TF Bert Model**","f62f5377":"**Get ready for tokenizer**","8b5b70be":"**Prepare TPU**","cd6fb503":"# **Watson using Bert**","e197e181":"**Show the percentage of languages used**","46a2ba3f":"**Read dataset**","8e707f0a":"**Preprocess data**","5eb4847f":"**Read dataset**","c36dba58":"**Build Model**","1c1ce941":"**Predict the answer**"}}