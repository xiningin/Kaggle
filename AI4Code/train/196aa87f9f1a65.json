{"cell_type":{"18c97cca":"code","b74f7f58":"code","55e68e19":"code","c8689b62":"code","bc2d8b55":"code","1a938e8d":"code","8b67cfd8":"code","ddf258a6":"code","7c246531":"code","c9850f48":"code","0207c098":"code","9fc59829":"code","6f4a44d5":"code","bc7f12a0":"code","759b12ed":"code","cdd78ba0":"code","d5010124":"code","b162bd74":"code","e8a8deae":"code","76e6aca6":"code","d3774353":"code","ec2e2736":"code","912ad882":"code","8ffd0a91":"code","05b11d2f":"code","d3e24a71":"code","74ad9beb":"code","61ce7e70":"code","05dc2054":"code","24293bc9":"code","b4b0514d":"code","da9b1ec1":"code","6446bae8":"code","460d9880":"code","57545b3b":"code","47429a67":"code","d8ec70b8":"code","70d1b32f":"code","ae8e6b0d":"markdown","d6400e87":"markdown","71da0b9b":"markdown","2f47e775":"markdown","792d72c8":"markdown","16f1a2fc":"markdown","f3bb3982":"markdown","f63a8959":"markdown","3e925f2b":"markdown"},"source":{"18c97cca":"# Importing some important librarys\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","b74f7f58":"df = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')","55e68e19":"df.isnull().sum().sort_values(ascending=False)","c8689b62":"# We will separate the numerical columns from the categorical\n\ncat_data = []\nnum_data = []\n\nfor i,c in enumerate(df.dtypes):\n    if c == object:\n        cat_data.append(df.iloc[:, i])\n    else :\n        num_data.append(df.iloc[:, i])","bc2d8b55":"cat_data = pd.DataFrame(cat_data).transpose()\nnum_data = pd.DataFrame(num_data).transpose()","1a938e8d":"# cat_data\n# If you want to fill every column with its own most frequent value you can use\n\ncat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0]))\ncat_data.isnull().sum().any() # no more missing data ","8b67cfd8":"# num_data\n# fill every missing value with their previous value in the same column\n\nnum_data.fillna(method='bfill', inplace=True)\nnum_data.isnull().sum().any() # no more missing data ","ddf258a6":"from sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\ncat_data.head()","7c246531":"# transform the target column\n\ntarget_values = {'Y': 0 , 'N' : 1}\n\ntarget = cat_data['Loan_Status']\ncat_data.drop('Loan_Status', axis=1, inplace=True)\n\ntarget = target.map(target_values)","c9850f48":"# transform other columns\n\nfor i in cat_data:\n    cat_data[i] = le.fit_transform(cat_data[i])","0207c098":"df = pd.concat([cat_data, num_data, target], axis=1)","9fc59829":"X = pd.concat([cat_data, num_data], axis=1)\ny = target ","6f4a44d5":"# we will use StratifiedShuffleSplit to split the data Taking into consideration that we will get the same ratio on the target column\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train, test in sss.split(X, y):\n    X_train, X_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = y.iloc[train], y.iloc[test]\n    \nprint('X_train shape', X_train.shape)\nprint('y_train shape', y_train.shape)\nprint('X_test shape', X_test.shape)\nprint('y_test shape', y_test.shape)\n\n# almost same ratio\nprint('\\nratio of target in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('ratio of target in y_test :',y_test.value_counts().values\/ len(y_test))\nprint('ratio of target in original_data :',df['Loan_Status'].value_counts().values\/ len(df))","bc7f12a0":"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# 'LogisticRegression': LogisticRegression(random_state=42),\n#     'KNeighborsClassifier': KNeighborsClassifier(),\n#     'SVC': SVC(random_state=42),\n\nmodels = {\n    \n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}","759b12ed":"# loss\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","cdd78ba0":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*30)\n        \ntrain_eval_train(models, X_train, y_train)\n\n","d5010124":"\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n\ndef train_eval_cross(models, X, y, folds):\n   \n    y = pd.DataFrame(y)\n    idx = [' pre', ' rec', ' f1', ' loss', ' acc']\n    for name, model in models.items():\n        ls = []\n        print(name,':')\n\n        for train, test in folds.split(X, y):\n            model.fit(X.iloc[train], y.iloc[train]) \n            y_pred = model.predict(X.iloc[test]) \n            ls.append(loss(y.iloc[test], y_pred, retu=True))\n        print(pd.DataFrame(np.array(ls).mean(axis=0), index=idx)[0])  \n        print('-'*30)\n        \ntrain_eval_cross(models, X_train, y_train, skf)\n\n","b162bd74":"# some explanation of the above function\n\nx = []\nidx = [' pre', ' rec', ' f1', ' loss', ' acc']\n\n# we will use one model\nlog = LogisticRegression()\n\nfor train, test in skf.split(X_train, y_train):\n    log.fit(X_train.iloc[train], y_train.iloc[train])\n    ls = loss(y_train.iloc[test], log.predict(X_train.iloc[test]), retu=True)\n    x.append(ls)\n    \n# thats what we get\npd.DataFrame(x, columns=idx)\n","e8a8deae":"\n\ndata_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n","76e6aca6":"X_train.head()","d3774353":"\n\nX_train['new_col'] = X_train['CoapplicantIncome'] \/ X_train['ApplicantIncome']  \nX_train['new_col_2'] = X_train['LoanAmount'] * X_train['Loan_Amount_Term'] ","ec2e2736":"data_corr = pd.concat([X_train, y_train], axis=1)\ncorr = data_corr.corr()\nplt.figure(figsize=(10,7))\nsns.heatmap(corr, annot=True);\n\n","912ad882":"X_train.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)","8ffd0a91":"train_eval_cross(models, X_train, y_train, skf)\n","05b11d2f":"# first lets take a look at the value counts of every label\n\nfor i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')","d3e24a71":"# new_col_2\n\n\nfrom scipy.stats import norm\n\nfig, ax = plt.subplots(1,2,figsize=(20,5))\n\nsns.distplot(X_train['new_col_2'], ax=ax[0], fit=norm)\nax[0].set_title('new_col_2 before log')\n\nX_train['new_col_2'] = np.log(X_train['new_col_2'])  # logarithm of all the values\n\nsns.distplot(X_train['new_col_2'], ax=ax[1], fit=norm)\nax[1].set_title('new_col_2 after log');","74ad9beb":"# now we will evaluate our models, and i will do that continuously ,so i don't need to mention that every time\n\ntrain_eval_cross(models, X_train, y_train, skf)\n\n# wooow our models improved really good by just doing the previous step .","61ce7e70":"# new_col\n\n# most of our data is 0 , so we will try to change other values to 1\n\nprint('before:')\nprint(X_train['new_col'].value_counts())\n\nX_train['new_col'] = [x if x==0 else 1 for x in X_train['new_col']]\nprint('-'*50)\nprint('\\nafter:')\nprint(X_train['new_col'].value_counts())","05dc2054":"train_eval_cross(models, X_train, y_train, skf)\n\n# ok we are improving our models as we go ","24293bc9":"for i in range(X_train.shape[1]):\n    print(X_train.iloc[:,i].value_counts(), end='\\n------------------------------------------------\\n')\n    \n# looks better","b4b0514d":"\n\nsns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 outliers', fontsize=15);\nplt.xlabel('');","da9b1ec1":"threshold = 0.1  \n            \nnew_col_2_out = X_train['new_col_2']\nq25, q75 = np.percentile(new_col_2_out, 25), np.percentile(new_col_2_out, 75) # Q25, Q75\nprint('Quartile 25: {} , Quartile 75: {}'.format(q25, q75))\n\niqr = q75 - q25\nprint('iqr: {}'.format(iqr))\n\ncut = iqr * threshold\nlower, upper = q25 - cut, q75 + cut\nprint('Cut Off: {}'.format(cut))\nprint('Lower: {}'.format(lower))\nprint('Upper: {}'.format(upper))\n\noutliers = [x for x in new_col_2_out if x < lower or x > upper]\nprint('Nubers of Outliers: {}'.format(len(outliers)))\nprint('outliers:{}'.format(outliers))\n\ndata_outliers = pd.concat([X_train, y_train], axis=1)\nprint('\\nlen X_train before dropping the outliers', len(data_outliers))\ndata_outliers = data_outliers.drop(data_outliers[(data_outliers['new_col_2'] > upper) | (data_outliers['new_col_2'] < lower)].index)\n\nprint('len X_train before dropping the outliers', len(data_outliers))","6446bae8":"X_train = data_outliers.drop('Loan_Status', axis=1)\ny_train = data_outliers['Loan_Status']","460d9880":"sns.boxplot(X_train['new_col_2']);\nplt.title('new_col_2 without outliers', fontsize=15);\nplt.xlabel('');\n\n# good :)","57545b3b":"train_eval_cross(models, X_train, y_train, skf)\n","47429a67":"X_test_new = X_test.copy()","d8ec70b8":"x = []\n\nX_test_new['new_col'] = X_test_new['CoapplicantIncome'] \/ X_test_new['ApplicantIncome']  \nX_test_new['new_col_2'] = X_test_new['LoanAmount'] * X_test_new['Loan_Amount_Term']\nX_test_new.drop(['CoapplicantIncome', 'ApplicantIncome', 'Loan_Amount_Term', 'LoanAmount'], axis=1, inplace=True)\n\nX_test_new['new_col_2'] = np.log(X_test_new['new_col_2'])\n\nX_test_new['new_col'] = [x if x==0 else 1 for x in X_test_new['new_col']]\n\n#X_test_new.drop(['Self_Employed'], axis=1, inplace=True)\n\n# drop all the features Except for Credit_History\n#X_test_new.drop(['Self_Employed','Dependents', 'new_col_2', 'Education', 'Gender', 'Property_Area','Married', 'new_col'], axis=1, inplace=True)","70d1b32f":"for name,model in models.items():\n    print(name, end=':\\n')\n    loss(y_test, model.predict(X_test_new))\n    print('-'*40)","ae8e6b0d":"# Simple process for the data","d6400e87":"### Missing values\n\nhere i am just going to use a simple techniques to handle the missing data","71da0b9b":"# Train the data\n\n* we will stop here for know and train the data.\n\n    we are going to use **StratifiedShuffleSplit**, for more [information](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedShuffleSplit.html) .","2f47e775":"# Outliers\n\n#### there is different techniques to handle outliers, here we are going to use [**IQR**](https:\/\/www.youtube.com\/watch?v=qLYYHWYr8xI)","792d72c8":"# features engineer","16f1a2fc":"DecisionTreeClassifier:\n  pre: 0.850\n  rec: 0.447\n  f1: 0.586\n  loss: 6.739\n  acc: 0.805","f3bb3982":"### we will work on the features that have varied values","f63a8959":"# evaluate the models on Test_data\n\nhere we will just repeat what we did in training data","3e925f2b":"### categorical columns\n\n* we are going to use **LabelEncoder** :\n\n    what it is actually do it encode labels with value between 0 and n_classes-1 , [for more examples](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html) ."}}