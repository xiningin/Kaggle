{"cell_type":{"297334fe":"code","2294fcec":"code","b6f207a6":"code","66da7359":"code","0f78855e":"code","1b252652":"code","b28a7339":"code","0315a241":"code","343f6600":"code","097a35c7":"code","f70d1bfa":"code","eceef290":"code","82afb4ad":"code","23a5f0ff":"code","4373c49b":"code","3b9da8b0":"code","702d86e8":"code","3e1429c6":"code","963832d2":"code","3be88570":"code","c040ef95":"code","807dda04":"code","e7c5bf25":"code","38579fc8":"markdown","6f2516bd":"markdown","be259c8c":"markdown","367ad5fe":"markdown","c9dd2eb3":"markdown","03b66a1e":"markdown","1ae93e61":"markdown","b75ce539":"markdown","a921c502":"markdown","2b48c212":"markdown","0cf3b55e":"markdown","cf9929b5":"markdown","ec4864ca":"markdown","ebe20977":"markdown","5cc4a8fa":"markdown"},"source":{"297334fe":"import numpy as np\nimport pandas as pd\nimport random\n\nimport datetime\nimport os\n\nimport bokeh\n\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import ColumnDataSource, FactorRange, HoverTool, BasicTicker, ColorBar, LinearColorMapper\nfrom bokeh.palettes import Spectral11, colorblind, Inferno, BuGn, brewer, Category20, Viridis256\nfrom bokeh.layouts import row, column, grid\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\noutput_notebook()","2294fcec":"files = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\ntrain_df = pd.read_csv(files[1], index_col = 'id')\n\ntest_df = pd.read_csv(files[2], index_col = 'id')\n\n\ntrain_target = train_df.pop('target')","b6f207a6":"train_df.head(5)","66da7359":"print(train_df.isnull().sum())\n\n# we don't need to fill empty values","0f78855e":"def plot_extremum(dataframe):\n\n    list_x = dataframe.max().to_list()\n    list_y = dataframe.min().to_list()\n\n    desc = dataframe.columns\n\n    source  = ColumnDataSource(data = dict( x= list_x, y= list_y, desc = desc))\n\n    hover = HoverTool(tooltips = [\n        (\"(x,y)\" , \"(@x, @y)\"),\n        ('desc', '@desc')\n    ])\n\n    p = figure(width = 800, height = 800, tools = [hover], title = 'Exploring possible outilers for each features', toolbar_location = 'right')\n    p.circle( 'x', 'y', size = 15, alpha = 0.4 ,source = source, fill_color = 'navy')\n    \n    return p\n\np = plot_extremum(train_df)\n\nshow(p)","1b252652":"f2_val = train_df['f2'].sample(10000).to_list()\nf35_val = train_df['f35'].sample(10000).to_list()\nf44_val = train_df['f44'].sample(10000).to_list()\n\nzeros = list(np.zeros(len(f2_val)))\n\nsource_1 = ColumnDataSource(data =dict(x = f2_val, y = zeros))    \n\n\nsource_2 = ColumnDataSource(data =dict(x = f35_val, y = zeros))  \n\nsource_3 = ColumnDataSource(data =dict(x = f44_val, y = zeros))  \n\n\n\n\np= figure(width = 500, height = 400, title = 'Distribution of f2 values', toolbar_location = 'right')\np1= figure(width = 500, height = 400, title = 'Distribution of f35 values', toolbar_location = 'right')\np2= figure(width = 500, height = 400, title = 'Distribution of f44 values', toolbar_location = 'right')\n\n\np.circle('x', 'y' , source = source_1, size =6,  fill_color = 'black', line_color = 'grey')\np1.circle('x', 'y' , source = source_2, size =6,  fill_color = 'black', line_color = 'grey')\np2.circle('x', 'y' , source = source_3, size =6,  fill_color = 'black', line_color = 'grey')\n\n\nshow(row([p,p1,p2]))","b28a7339":"list_x = train_df.drop(columns = ['f2','f35', 'f44']).max().to_list()\nlist_y = train_df.drop(columns = ['f2','f35', 'f44']).min().to_list()\n\ndesc = train_df.drop(columns = ['f2','f35', 'f44']).columns\n\nsource  = ColumnDataSource(data = dict( x= list_x, y= list_y, desc = desc))\n\n\n\nhover = HoverTool(tooltips = [\n    (\"(x,y)\" , \"(@x, @y)\"),\n    ('desc', '@desc')\n])\n\n\np = figure(width = 600, height = 600, tools = [hover], title = 'Exploring possible outilers for each features', toolbar_location = 'right')\np.circle( 'x', 'y', size = 15, alpha = 0.4 ,source = source, fill_color = 'navy', line_color = 'white')\n\nshow(p)","0315a241":"def plot_violin(dataframe,n = 10):\n\n    select_feat = dataframe.max().loc[lambda val : val <= 10]\n\n    feat_index = select_feat.index.to_list()\n\n    rand_feat = random.sample(feat_index,n)\n\n    distrib_df = dataframe[rand_feat].sample(10000).melt(var_name ='Column', value_name = 'Raw')\n\n\n    plt.figure(figsize=(16,8))\n    sns.violinplot(x = 'Column', y = 'Raw', data = distrib_df)\n    \n    plt.show()\n    \nplot_violin(train_df)","343f6600":"means_df = train_df.mean()\n\nstd_df = train_df.std()\n\n\nnormalised_df = (train_df - means_df) \/std_df","097a35c7":"p5 = plot_extremum(normalised_df)\n\n\nshow(p5)","f70d1bfa":"plot_violin(normalised_df,10)","eceef290":"from sklearn.model_selection import train_test_split","82afb4ad":"BATCH_SIZE = 256\n\nEPOCHS = 30\n\n\nNUMBER_FEATURE = 100","23a5f0ff":"x_train, x_val, y_train, y_val = train_test_split(np.asarray(normalised_df),\n                                                  np.asarray(train_target),\n                                                  test_size = 0.1)\n\n\ndef build_dataset(features_matrix, target):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((features_matrix, target))\n    \n    dataset = dataset.shuffle(buffer_size = 1000).batch(BATCH_SIZE, drop_remainder = True).prefetch(1)\n    \n    return dataset\n\n\n\n\ntrain_dataset = build_dataset(x_train, y_train)\n\nval_dataset = build_dataset(x_val,y_val)","4373c49b":"def build_model(train_dataset, val_dataset, input_size):\n    \n    model = tf.keras.Sequential([\n        keras.layers.InputLayer(input_shape = (input_size,)),\n        keras.layers.Dense(units = 128, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 64, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 32, activation = 'relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(units = 16, activation = 'relu'),\n        keras.layers.Dense(units = 1, activation = 'sigmoid')\n\n    ])\n    \n    \n    \n    model.compile(optimizer = keras.optimizers.SGD(learning_rate = 4e-3,momentum = 0.9, nesterov = True),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])\n\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor ='val_loss', patience = 3, factor = 0.1, verbose = 1)\n\n    history = model.fit(train_dataset,\n                        batch_size = 256,\n                        epochs = EPOCHS,\n                        validation_data = val_dataset,\n                        callbacks = [reduce_lr])\n    \n    return history, model\n\n\n\nhistory, mymodel = build_model(train_dataset = train_dataset,\n                              val_dataset = val_dataset,\n                              input_size = NUMBER_FEATURE)","3b9da8b0":"def plot_metrics(history,epochs):\n    \n    titles = ['Training loss', 'Validation loss', 'Training Accuracy','Validation Accuracy']\n    metrics = ['loss', 'val_loss', 'accuracy','val_accuracy']\n    palette = Inferno[4]\n    figures = []\n    \n    for k in range(4):\n        \n        p = figure( width = 600, height = 400, title = titles[k])\n        \n        p.line(np.arange(epochs), history.history[metrics[k]], line_width = 4, color = palette[k%2+1])   \n        \n        figures.append(p)\n    \n    show(grid([figures[:2], figures[2:]]))\n\nplot_metrics(history,EPOCHS)\n","702d86e8":"p = figure( width = 600, height = 400, title = 'Learning Rate Evolution when using ReduceLROnPlateau')\n        \np.line(np.arange(EPOCHS), history.history['lr'], line_width = 4, color = Inferno[4][2])   \n\nshow(p)","3e1429c6":"normalised_df['target'] = train_target\n\n\ncorr_df = normalised_df.corr()\n","963832d2":"\ncorr_df.index.name = 'Features1'\ncorr_df.columns.name = 'Features2'\n\n\ncorr_matrix = pd.DataFrame(corr_df.stack(), columns=['correlation']).reset_index()\n\n\nmapper = LinearColorMapper(palette=Viridis256,\n                           low=corr_matrix['correlation'].min(),\n                           high=corr_matrix['correlation'].max())\n\n\n\nTOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\"\n\np = figure(title=\"Correlation Matrix\",\n           x_range=corr_matrix['Features1'].drop_duplicates().to_list(),\n           y_range=corr_matrix['Features1'].drop_duplicates().to_list(),\n           x_axis_location=\"below\",\n           width=1200,\n           height=1200,\n           tools=TOOLS,\n           toolbar_location='left')\n\np.grid.grid_line_color = None\np.axis.axis_line_color = None\np.axis.major_tick_line_color = None\np.axis.major_label_text_font_size = \"7px\"\np.axis.major_label_standoff = 0\np.xaxis.major_label_orientation = np.pi \/ 3\n\np.rect(x='Features1', y=\"Features2\", width=1, height=1,\n       source=corr_matrix,\n       fill_color={'field': 'correlation', 'transform': mapper},\n       line_color=None)\n\ncolor_bar = ColorBar(color_mapper=mapper,\n                     major_label_text_font_size=\"7px\",\n                     ticker=BasicTicker(desired_num_ticks=256),\n                     border_line_color=None)\np.add_layout(color_bar, 'right')\n\nshow(p)\n","3be88570":"#removing target\nnormalised_df.drop(columns = ['target'],inplace = True)","c040ef95":"normalised_test_df = (test_df - means_df) \/std_df\n\n#we use means and standard deviation of training set, so the distributions are matching with the ones used during training\n\n\nids = normalised_test_df.index","807dda04":"predictions = mymodel.predict(np.asarray(normalised_test_df), \n               batch_size = 128,\n               verbose = 1)\n\n","e7c5bf25":"submission_df = pd.DataFrame(data = {'id' : ids, 'target' : predictions.round().reshape(-1,)}).set_index('id')\n\nsubmission_df.to_csv('submission.csv')","38579fc8":"Even if several features have some huge outliers, let's keep it like this for the moment\n\n___\n\n## Building a simple Neural Net","6f2516bd":"1. We use `tf.data.dataset` to build our datasets, thus we can apply functions on them much faster\n2. the **`batch_size`** and **`buffer_size`**  are chosen arbitrarily since we are working on kaggle's notebook\n\n\n\n___\n\n\n- Regarding the model, *Dropout layers* are used to simply the model and thus reduce overfitting, \n- 4 hidden layers were used, be you can obtain more or less the same accuracy with only 3\n- The Dropout rate wasn't really tweaked here.\n- I didn't tried adam optimizer, you maybe can achieve better performances with\n- LROnPlateau is usually usefull the gain some performances, but to obtain even better result creating your own learning_rate schedule is the best solution","be259c8c":"We cannot learn much with this graph, values are too spread, mostly because we have a lot of values, <br>\nSo having outliers is most likely to occur <br>\n\n\n\nAs expected in these kind of problems, we are going to normalise\n\n- *We could also standardize our data* ","367ad5fe":"It clearly appears that features : `f35`, `f2` and `f44` might have some outilers, let's expect them closely.\n\n\nFor more information, we built a scatter plot mapping (min,max) for each features, <br>\nEven though, we can't conclude anything with **ONLY** this plot <br>\nIt might help when handling a large number of features, to rapidly gather some information","c9dd2eb3":"## A close look at distributions\n\n\nLet's analyse the group of features between `[-5,10]` using violin plot! <br>\n- For visiblity we are going to plot only 10 features randomly choosen","03b66a1e":"Let's quickly re-run our previous visualisations ","1ae93e61":"Finally,  these three features are just not on the same scale as others, so we must scale them down. <br>\n\nHowever before doing so, we will analyse the other features","b75ce539":"Apparently features are not correlated. We just need to find the most useful features, to simplify our model (not done yet)","a921c502":"## Correlation and feature importance\n\n---\n\n\nChecking features impact is always good, and might help to remove some useless features","2b48c212":"## Librairies","0cf3b55e":"For this month, let's explore the data and create a basic classifier using Tensorflow.\n\n- ***First Question : Among these 100 features, are they all useful? How can we use them?***\n- ***Second Question : How to create simple data pipeline and model using Tensorflow?***\n\n\nLet's check this","cf9929b5":"Most of our features contains values between `[-5,10]` <br>\nTo go further we could analyse each feature not in this range, to make sure they are not outliers <br>\n\nWe will skip this part here","ec4864ca":"Let's explore the data distribution for each features","ebe20977":"- By increasing the batch_size, validation loss is much less volatile","5cc4a8fa":"## Load data"}}