{"cell_type":{"4a1bcff9":"code","e0e95829":"code","d88c188d":"code","77e597c1":"code","6a565a50":"code","bf35cb68":"code","7bd416dc":"code","61469a1e":"code","a5ebdc92":"code","f6b85ca5":"code","73dbaf78":"code","8675d65d":"code","444499d7":"code","75b6c5a1":"code","a1556915":"code","7410e66c":"code","5aef429e":"code","2ffa4830":"code","64cda205":"code","f8c41120":"code","8822f244":"code","8b8290ab":"code","d095a1d3":"markdown","ec4adefb":"markdown","c40e5fd5":"markdown","f1a52716":"markdown","7d4c8d08":"markdown","e6a7a2c5":"markdown","9e2021cc":"markdown","de2a3d4d":"markdown","c37488b0":"markdown","c47805f5":"markdown","4c573e5b":"markdown","09a8682b":"markdown","3cb30c22":"markdown","cdec21f9":"markdown","b1a543e1":"markdown","c1d7d6e2":"markdown"},"source":{"4a1bcff9":"cd '\/kaggle\/working'","e0e95829":"!pwd","d88c188d":"import requests\nfrom bs4 import BeautifulSoup\n\nheaders = {\n    'Access-Control-Allow-Origin': '*',\n    'Access-Control-Allow-Methods': 'GET',\n    'Access-Control-Allow-Headers': 'Content-Type',\n    'Access-Control-Max-Age': '3600',\n    'User-Agent': 'Mozilla\/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko\/20100101 Firefox\/52.0'\n    }\nMediaUrl = 'https:\/\/www.mediafire.com\/file\/kwwoqmau4i1jg7r\/20210714_164812_1.mp4\/file'\n\nurl = MediaUrl\nreq = requests.get(url, headers)\nsoup = BeautifulSoup(req.content, 'html.parser')\n\nurl = soup.find(\"a\", class_=\"popsok\").get('href')\nr = requests.get(url)\n\nprint (\"File Name : \" + soup.find(\"div\", class_=\"filename\").get_text())\nprint (soup.find(\"ul\", class_=\"details\").get_text())\n\nwith open(soup.find(\"div\", class_=\"filename\").get_text(), 'wb') as f:\n    f.write(r.content)\n    print('Done ...')","77e597c1":"!ls","6a565a50":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport time\nfrom timeit import default_timer as timer\nimport matplotlib.pyplot as plt\nimport pickle\n\nfrom keras.models import load_model\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('..\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nprint(os.listdir('..\/input'))\n\n# Any results we write to the current directory are saved as output\n","bf35cb68":"# Reading csv file with labels' names\n# Loading two columns [0, 1] into Pandas dataFrame\nlabels = pd.read_csv('..\/input\/traffic-signs-preprocessed\/label_names.csv')\n\n# Check point\n# Showing first 5 rows from the dataFrame\nprint(labels.head())\nprint()\n\n# To locate by class number use one of the following\n# ***.iloc[0][1] - returns element on the 0 column and 1 row\nprint(labels.iloc[0][1])  # Speed limit (20km\/h)\n# ***['SignName'][1] - returns element on the column with name 'SignName' and 1 row\nprint(labels['SignName'][1]) # Speed limit (30km\/h)\n","7bd416dc":"# Loading trained CNN model to use it later when classifying from 4 groups into one of 43 classes\n#model = load_model('..\/input\/model3x3\/model-3x3.h5')\n# Better CNN model (larger resolution of filter)\n#model = load_model('..\/input\/model5x5\/model-5x5.h5')\n# Najbetter (uczony na 10k)\nmodel = load_model('..\/input\/modele\/model-3x3 (1).h5')\n\n# Loading mean image to use for preprocessing further\n# Opening file for reading in binary mode\nwith open('..\/input\/traffic-signs-preprocessed\/mean_image_rgb.pickle', 'rb') as f:\n    mean = pickle.load(f, encoding='latin1')  # dictionary type\n    \nprint(mean['mean_image_rgb'].shape)  # (3, 32, 32)\n","61469a1e":"model.summary()","a5ebdc92":"import math\nimport random\nfrom PIL import Image\n\npath_prefix = \"\/root\/darknet\/rmarkings\"\nvalidate_percentage = 0.2\n\ndef RepresentsInt(s):\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\ndef parse():\n    train = {}\n    validate = {}\n    cls_labels = {}\n    \n    # Read data\n    with open('dataset_annotations.txt') as file:\n        classes = {}\n        for line in file.readlines():\n            lines = line.strip().split(\",\")\n            file_name, clas_spec = lines[-1], lines[-2]\n            file_name = file_name.replace(\".png\", \".jpg\")\n\n            xs = [int(float(lines[0])), int(float(lines[2])), int(float(lines[4])), int(float(lines[6]))]\n            ys = [int(float(lines[1])), int(float(lines[3])), int(float(lines[5])), int(float(lines[7]))]\n\n            x_min, x_max = min(xs), max(xs)\n            y_min, y_max = min(ys), max(ys)\n\n            width = x_max - x_min\n            height = y_max - y_min\n\n            im = Image.open(file_name)\n            im_width, im_height = im.size\n\n            center_x, center_y = (width \/ 2) + x_min, (height \/2) + y_min\n\n            if classes.get(clas_spec) is None:\n                classes[clas_spec] = []\n            data = {\"name\": file_name, \"x\": center_x \/ im_width, \"y\": center_y \/ im_height, \"width\": width \/ im_width, \"height\": height \/ im_height}\n            classes[clas_spec].append(data)\n            print(data)\n        \n        it = 0\n        for key, values in classes.items():\n            if len(values) > 20 and not RepresentsInt(key):\n                cls_labels[it] = key\n                random.shuffle(values)\n                test_len = math.floor(len(values) * validate_percentage)\n                train[key] = values[-(len(values) - test_len):]\n                validate[key] = values[:test_len]\n\n\n                print(f\"validate: {key}: {len(validate[key])}\")\n                print(f\"train: {key}: {len(train[key])}\")\n\n                for value in values:\n                    f_name = value[\"name\"].replace(\".jpg\", \".txt\")\n                    with open(f_name, \"w+\") as w_file:\n                        w_file.write(f\"{it} {value['x']} {value['y']} {value['width']} {value['height']}\\n\")\n                it += 1\n\n    with open(\"classes.names\", \"w+\") as cls_file:\n        for cls_name in cls_labels.values():\n            cls_file.write(f\"{cls_name}\\n\")\n\n    with open(\"train.txt\", \"w+\") as data_file:\n        for vls in train.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}\/{val['name']}\\n\")\n\n    with open(\"test.txt\", \"w+\") as data_file:\n        for vls in validate.values():\n            for val in vls:\n                data_file.write(f\"{path_prefix}\/{val['name']}\\n\")\n    \n    with open(\"data.data\", \"w+\") as data_file:\n        data_file.write(f\"\"\"classes = {len(cls_labels.keys())}\ntrain = {path_prefix}\/train.txt\nvalid = {path_prefix}\/test.txt\nnames = {path_prefix}\/classes.names\nbackup = backup1\"\"\")","f6b85ca5":"# Trained weights can be found in the course mentioned above\n# Tutaj po dodaniu do Data zmie\u0144 drog\u0119 do pliku kt\u00f3ry wrzucisz\npath_to_weights = '..\/input\/car-data\/znaki_rtx_final.weights'\npath_to_weights_markings = '..\/input\/car-data\/poziome_rtx_final.weights'\npath_to_cfg = '..\/input\/traffic-signs-dataset-in-yolo-format\/yolov3_ts_test.cfg'\npath_to_cfg_markings = '..\/input\/car-data\/markings_test.cfg'\n\n# Loading trained YOLO v3 weights and cfg configuration file by 'dnn' library from OpenCV\nnetwork = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)\nnetwork_markings = cv2.dnn.readNetFromDarknet(path_to_cfg_markings, path_to_weights_markings)\n\n# To use with GPU\nnetwork.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n\nnetwork_markings.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\nnetwork_markings.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","73dbaf78":"# Getting names of all YOLO v3 layers\nlayers_all = network.getLayerNames()\nlayers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\nprint(layers_names_output)\n\nprint(\"<===========>\")\n\n# Getting names of all YOLO v4 layers\nlayers_all_markings = network_markings.getLayerNames()\nlayers_names_output_markings = [layers_all_markings[i[0] - 1] for i in network_markings.getUnconnectedOutLayers()]\nprint(layers_names_output_markings)\n","8675d65d":"# Minimum probability to eliminate weak detections\nprobability_minimum = 0.1\n\n# Setting threshold to filtering weak bounding boxes by non-maximum suppression\nthreshold = 0.1\n\n# Generating colours for bounding boxes\n# randint(low, high=None, size=None, dtype='l')\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\ncolours_markings = np.random.randint(0, 255, size=(1, 3), dtype='uint8')\n\n# Check point\nprint(type(colours))  # <class 'numpy.ndarray'>\nprint(colours.shape)  # (43, 3)\nprint(colours[0])  # [25  65 200]\n","444499d7":"! pwd\n\n","75b6c5a1":"# Reading video from a file by VideoCapture object\n#video = cv2.VideoCapture('..\/input\/car-data\/70maiMiniDashCam-Dzien.mp4')\n#video = cv2.VideoCapture('..\/input\/car-data\/DODRX8W(lusterko)-roadtestwsonecznydzien_podsonce1080p30.mp4')\nvideo = cv2.VideoCapture('.\/20210714_164812_1.mp4')\n\n# Writer that will be used to write processed frames\nwriter = None\n\n# Variables for spatial dimensions of the frames\nh, w = None, None\n","a1556915":"%matplotlib inline\n\n# Setting default size of plots\nplt.rcParams['figure.figsize'] = (3, 3)\n\n# Variable for counting total amount of frames\nf = 0\n\n# Variable for counting total processing time\nt = 0\n\n# Catching frames in the loop\nwhile True:\n    # Capturing frames one-by-one\n    ret, frame = video.read()\n\n    # If the frame was not retrieved\n    if not ret:\n        break\n       \n    # Getting spatial dimensions of the frame for the first time\n    if w is None or h is None:\n        # Slicing two elements from tuple\n        h, w = frame.shape[:2]\n\n    # Blob from current frame\n    blob = cv2.dnn.blobFromImage(frame, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n\n    # Forward pass with blob through output layers\n    network.setInput(blob)\n    network_markings.setInput(blob)\n    start = time.time()\n    output_from_network = network.forward(layers_names_output)\n    output_from_network_markings = network_markings.forward(layers_names_output_markings)\n    end = time.time()\n\n    # Increasing counters\n    f += 1\n    t += end - start\n\n    # Spent time for current frame\n    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n\n    # Lists for detected bounding boxes, confidences and class's number\n    bounding_boxes = []\n    bounding_boxes_markings = []\n    confidences = []\n    confidences_markings = []\n    class_numbers = []\n    class_numbers_markings = []\n\n    # Going through all output layers after feed forward pass\n    for result in output_from_network:\n        # Going through all detections from current output layer\n        for detected_objects in result:\n            # Getting 80 classes' probabilities for current detected object\n            scores = detected_objects[5:]\n            # Getting index of the class with the maximum value of probability\n            class_current = np.argmax(scores)\n            # Getting value of probability for defined class\n            confidence_current = scores[class_current]\n            # Eliminating weak predictions by minimum probability\n            if confidence_current > probability_minimum:\n                try:\n                    # Scaling bounding box coordinates to the initial frame size\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    # Getting top left corner coordinates\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width \/ 2))\n                    y_min = int(y_center - (box_height \/ 2))\n\n                    # Adding results into prepared lists\n                    bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences.append(float(confidence_current))\n                    class_numbers.append(class_current)\n                except Exception as e:\n                    print(e)\n                \n\n    # Implementing non-maximum suppression of given bounding boxes\n    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n    results_markings = cv2.dnn.NMSBoxes(bounding_boxes_markings, bounding_boxes_markings, probability_minimum, threshold)\n\n    # Checking if there is any detected object been left\n    if len(results) > 0:\n        # Going through indexes of results\n        for i in results.flatten():\n            # Bounding box coordinates, its width and height\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n            \n            \n            # Cut fragment with Traffic Sign\n            c_ts = frame[y_min:y_min+int(box_height), x_min:x_min+int(box_width), :]\n            \n            if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n                pass\n            else:\n                # Getting preprocessed blob with Traffic Sign of needed shape\n                blob_ts = cv2.dnn.blobFromImage(c_ts, 1 \/ 255.0, size=(32, 32), swapRB=True, crop=False)\n                blob_ts[0] = blob_ts[0, :, :, :] - mean['mean_image_rgb']\n                blob_ts = blob_ts.transpose(0, 2, 3, 1)\n\n                # Feeding to the Keras CNN model to get predicted label among 43 classes\n                scores = model.predict(blob_ts)\n\n                # Scores is given for image with 43 numbers of predictions for each class\n                # Getting only one class with maximum value\n                prediction = np.argmax(scores)\n\n\n                # Colour for current bounding box\n                colour_box_current = colours[class_numbers[i]].tolist()\n\n                # Drawing bounding box on the original current frame\n                cv2.rectangle(frame, (x_min, y_min),\n                              (x_min + box_width, y_min + box_height),\n                              colour_box_current, 2)\n\n                # Preparing text with label and confidence for current bounding box\n                text_box_current = '{}: {:.4f}'.format(labels['SignName'][prediction],\n                                                       confidences[i])\n\n                # Putting text with label and confidence on the original image\n                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n\n    # For markings\n    for result in output_from_network_markings:\n        for detected_objects in result:\n            scores = detected_objects[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                try:\n                    box_current = detected_objects[0:4] * np.array([w, h, w, h])\n\n                    x_center, y_center, box_width, box_height = box_current\n                    x_min = int(x_center - (box_width \/ 2))\n                    y_min = int(y_center - (box_height \/ 2))\n\n                    bounding_boxes_markings.append([x_min, y_min, int(box_width), int(box_height)])\n                    confidences_markings.append(float(confidence_current))\n                    class_numbers_markings.append(class_current)\n                except Exception as e:\n                    print(e)\n\n    if len(results_markings) > 0:\n        for i in results_markings.flatten():\n            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n\n            cv2.rectangle(frame, (x_min, y_min),\n                          (x_min + box_width, y_min + box_height),\n                            colours[0].toList(), 2)\n\n\n    # Initializing writer only once\n    if writer is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n\n        # Writing current processed frame into the video file\n        writer = cv2.VideoWriter('result.mp4', fourcc, 25,\n                                 (frame.shape[1], frame.shape[0]), True)\n\n    # Write processed current frame to the file\n    writer.write(frame)\n\n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n","7410e66c":"######### test \n\n# Releasing video reader and writer\nvideo.release()\nwriter.release()\n\n","5aef429e":"print('Total number of frames', f)\nprint('Total amount of time {:.5f} seconds'.format(t))\nprint('FPS:', round((f \/ t), 1))\n","2ffa4830":"# Saving locally without committing\nfrom IPython.display import FileLink\nimport os\n\n#os.chdir(r'kaggle\/working')\nFileLink('result.mp4')\n","64cda205":"from IPython.display import FileLink, FileLinks\nFileLinks('.') #lists all downloadable files on server","f8c41120":"import os \nfrom os import startfile\nstartfile('.\/result.mp4')","8822f244":"!cp  '.\/result.mp4' '..\/input\/test-video'","8b8290ab":"# importing libraries\nimport cv2\nimport numpy as np\n\n# Create a VideoCapture object and read from input file\ncap = cv2.VideoCapture('result.mp4')\n\n# Check if camera opened successfully\nif (cap.isOpened()== False):\n    print(\"Error opening video file\")\n\n# Read until video is completed\nwhile(cap.isOpened()):\n\t\n# Capture frame-by-frame\n    ret, frame = cap.read()\n    if ret == True:\n\n\t# Display the resulting frame\n        cv2.imshow('Frame', frame)\n\n\t# Press Q on keyboard to exit\n        if cv2.waitKey(25) & 0xFF == ord('q'):\n        break\n\n# Break the loop\n    else:\n        break\n\n# When everything done, release\n# the video capture object\ncap.release()\n\n# Closes all the frames\ncv2.destroyAllWindows()\n","d095a1d3":"## Setting *probability*, *threshold* and *colour* for bounding boxes","ec4adefb":"## \ud83c\udfc1 FPS results","c40e5fd5":"### \ud83c\udf93 Related Course for Detection Tasks\n**Training YOLO v3 for Objects Detection with Custom Data.** *Build your own detector by labelling, training and testing on image, video and in real time with camera.* **Join here:** [https:\/\/www.udemy.com\/course\/training-yolo-v3-for-objects-detection-with-custom-data\/](https:\/\/www.udemy.com\/course\/training-yolo-v3-for-objects-detection-with-custom-data\/?referralCode=A283956A57327E37DDAD)\n\nExample of detections on video are shown below. **Trained weights** can be found in the course mentioned above.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3400968%2Fbcdae0b57021d6ac3e86a9aa2e8c4b08%2Fts_detections.gif?generation=1581700736851192&alt=media)","f1a52716":"## Loading *trained weights* and *cfg file* into the Network","7d4c8d08":"## Parsing markings dataset","e6a7a2c5":"# \ud83d\udd0e Example of the result","9e2021cc":"# \ud83c\udfac Reading input video","de2a3d4d":"# \ud83d\udcc2 Loading *labels*","c37488b0":"# \u27bf Processing frames in the loop","c47805f5":"## Getting *output layers* where detections are made","4c573e5b":"# \ud83d\udce5 Importing needed libraries","09a8682b":"![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3400968%2Fa57f58b38e3caab6fbf72169895f5074%2Fresult.gif?generation=1585955236302060&alt=media)","3cb30c22":"### \ud83d\udea9 Related Papers\n\n1. Sichkar V. N. **Real time detection and classification of traffic signs based on YOLO version 3 algorithm.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2020, vol. 20, no. 3, pp. 418\u2013424. DOI: 10.17586\/2226-1494-2020-20-3-418-424 (Full-text available on ResearchGate here: [Real time detection and classification of traffic signs based on YOLO version 3 algorithm](https:\/\/www.researchgate.net\/publication\/342638954_Real_time_detection_and_classification_of_traffic_signs_based_on_YOLO_version_3_algorithm)\n\n1. Sichkar V. N. **Effect of various dimension convolutional layer filters on traffic sign classification accuracy.** *Scientific and Technical Journal of Information Technologies, Mechanics and Optics*, 2019, vol. 19, no. 3, pp. 546\u2013552. DOI: 10.17586\/2226-1494-2019-19-3-546-552 (Full-text available on ResearchGate here: [Effect of various dimension convolutional layer filters on traffic sign classification accuracy](https:\/\/www.researchgate.net\/publication\/334074308_Effect_of_various_dimension_convolutional_layer_filters_on_traffic_sign_classification_accuracy)\n\n1. Tao Wu and Ananth Ranganathan **A Practical System for Road Marking Detection and Recognition\u201d, IEEE Intelligent Vehicles Symposium, 2012.** [More about dataset](http:\/\/www.ananth.in\/RoadMarkingDetection.html)","cdec21f9":"# \u26d4\ufe0f Traffic Signs Detection with YOLO v3, OpenCV and Keras","b1a543e1":"# \ud83d\udccd Loading trained Keras CNN model for Classification","c1d7d6e2":"* Firstly, trained model in Darknet framework **detects Traffic Signs among 4 categories** by OpenCV dnn library.\n* Then, trained model in Keras **classifies** cut fragmets of Traffic Signs into one of **43 classes**.\n* Results are experimental, but can be used for further improvements."}}