{"cell_type":{"6bb48921":"code","7eebc506":"code","1f52a13c":"code","d4ac1c7e":"code","28868847":"code","96b5460c":"code","5facd682":"code","32db6c93":"code","bfd82743":"code","868f2251":"code","3fda0599":"code","6810004a":"code","60285035":"code","de986c09":"code","9037bcd8":"code","c1798161":"code","25b0c2ea":"code","274ef8b3":"code","227a35fb":"code","2154f5e3":"code","d05e2a16":"code","0795eb5c":"code","4e9e0155":"code","65b114f3":"code","eca57e45":"code","f23efcdd":"code","8d4566f5":"code","d1706d87":"code","de48ef2c":"markdown","d7cd45ff":"markdown","e0323ef8":"markdown","92747064":"markdown","4be52d53":"markdown","92109343":"markdown","88ca6980":"markdown","8b33f02c":"markdown","01bec8a8":"markdown","6bc2fcd3":"markdown","f8da60ea":"markdown","dcc37789":"markdown","b20a6189":"markdown","4e8fdb58":"markdown","daff1e45":"markdown","49611266":"markdown","43ba6569":"markdown","40895992":"markdown","7f09d1c0":"markdown","0379b99c":"markdown","d07539f0":"markdown"},"source":{"6bb48921":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nsns.set()\n","7eebc506":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","1f52a13c":"X_train = train.iloc[:, 1:-1]\r\ny_train = train.iloc[:, -1]\r\nX_test = test.iloc[:, 1:]\r\nprint(X_train.shape, y_train.shape, X_test.shape)\r\n","d4ac1c7e":"X_train.head()\r\n","28868847":"X_test.head()\r\n","96b5460c":"sns.displot(data=train, x=\"SalePrice\", kde=True, height=10)\r\n","5facd682":"y_train.describe()\r\n","32db6c93":"train.isnull().sum()\r\n","bfd82743":"fig, ax = plt.subplots(figsize=(25, 10))\r\nsns.heatmap(train.isnull(), cbar=False, ax=ax)\r\n","868f2251":"fig, ax = plt.subplots(figsize=(25, 10))\r\nsns.heatmap(test.isnull(), cbar=False, ax=ax)\r\n","3fda0599":"corr = train.corr()\r\nfig, ax = plt.subplots(figsize=(15, 15))\r\nmask = np.triu(train.corr())\r\nsns.heatmap(corr, ax=ax, cmap=sns.diverging_palette(230, 20, as_cmap=True), square=True, mask=mask, linewidths=.5)\r\nplt.show()\r\n","6810004a":"X_train_null_col = [i for i in X_train.columns if X_train[i].isnull().any()]\r\nX_train_null_df = pd.DataFrame(X_train[i] for i in X_train_null_col).T\r\nX_train_null_df\r\n","60285035":"test_null_col = [i for i in X_test.columns if X_test[i].isnull().any()]\r\ntest_null_df = pd.DataFrame(X_test[i] for i in test_null_col).T\r\ntest_null_df\r\n","de986c09":"X_train = X_train.drop([\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\"], axis=1)\r\nX_test = X_test.drop([\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\"], axis=1)\r\n","9037bcd8":"train_list_null_cols = X_train.columns[X_train.isna().any()].tolist()\r\ntest_list_null_cols = X_test.columns[X_test.isna().any()].tolist()\r\nprint(train_list_null_cols)\r\nprint(test_list_null_cols)\r\n","c1798161":"train_nullcol_float = []\r\ntrain_nullcol_object = []\r\ntest_nullcol_float = []\r\ntest_nullcol_object = []\r\nfor i in train_list_null_cols:\r\n    if X_train[i].dtype == \"float64\":\r\n        train_nullcol_float.append(i)\r\n    else:\r\n        train_nullcol_object.append(i)\r\nfor i in test_list_null_cols:\r\n    if X_test[i].dtype == \"float64\":\r\n        test_nullcol_float.append(i)\r\n    else:\r\n        test_nullcol_object.append(i)\r\ntrain_nullcol_object.extend([\"MSSubClass\", \"OverallQual\", \"OverallCond\"])\r\ntest_nullcol_object.extend([\"MSSubClass\", \"OverallQual\", \"OverallCond\"])\r\n","25b0c2ea":"for i in train_nullcol_float:\r\n    if i == \"GarageYrBlt\":\r\n        X_train[i] = X_train[i].fillna(X_train[i].mode().iloc[0])\r\n    else:\r\n        X_train[i] = X_train[i].fillna(X_train[i].mean())\r\nfor i in train_nullcol_object:\r\n    X_train[i] = X_train[i].fillna(X_train[i].mode().iloc[0])\r\nfor i in test_nullcol_float:\r\n    if i == \"GarageYrBlt\":\r\n        X_test[i] = X_test[i].fillna(X_test[i].mode().iloc[0])\r\n    else:\r\n        X_test[i] = X_test[i].fillna(X_test[i].mean())\r\nfor i in test_nullcol_object:\r\n    X_test[i] = X_test[i].fillna(X_test[i].mode().iloc[0])\r\n\r\n\r\nprint(X_test.columns[X_train.isna().any()].tolist())\r\nprint(X_test.columns[X_test.isna().any()].tolist())\r\n","274ef8b3":"pd.options.display.max_rows = 100\r\npd.set_option(\"display.max_rows\", 101)\r\nX_train\r\n","227a35fb":"y_train = y_train.values.reshape(-1, 1)\r\ny_train_cp = y_train.copy()\r\ntarget_scaler = MinMaxScaler()\r\ny_train = target_scaler.fit_transform(y_train)\r\n","2154f5e3":"scaling_list = list(X_train.select_dtypes([\"int64\"]).columns) + list(X_train.select_dtypes([\"float64\"]).columns)\r\n","d05e2a16":"X_train = X_train.copy()\r\nscaling_list = [e for e in scaling_list if e not in (\"MSSubClass\", \"OverallQual\", \"OverallCond\")]\r\nstd_scaler = StandardScaler().fit(X_train[scaling_list].values)\r\nX_train[scaling_list] = std_scaler.transform(X_train[scaling_list].values)\r\nX_test[scaling_list] = std_scaler.transform(X_test[scaling_list])\r\n","0795eb5c":"encoding_list = list(X_train.select_dtypes(['object']).columns)\r\nencoding_list.extend([\"MSSubClass\", \"OverallQual\", \"OverallCond\"])\r\n","4e9e0155":"X_train = pd.get_dummies(X_train, columns=encoding_list)\r\nX_test = pd.get_dummies(X_test, columns=encoding_list)\r\n","65b114f3":"common_col = np.intersect1d(X_train.columns, X_test.columns)\nX_train = X_train[common_col]\nX_test = X_test[common_col]\nt_1 = list(X_train.columns)\nt_2 = list(X_test.columns)\ndifference_list = np.setdiff1d(t_1, t_2)\nprint(difference_list)\n","eca57e45":"GBoost = GradientBoostingRegressor(n_estimators=5000, learning_rate=0.05, max_depth=4, max_features='sqrt',\r\n                                   min_samples_leaf=15, min_samples_split=10, loss='huber', random_state=0)\r\nGBoost.fit(X_train, y_train)\r\n","f23efcdd":"y_predictGB = GBoost.predict(X_test)\r\nar = np.reshape(y_predictGB, (-1, 1))\r\nprint(ar)\r\nyhat = target_scaler.inverse_transform(ar)\r\nyhat = np.ndarray.flatten(yhat)\r\n","8d4566f5":"print(yhat)\r\n","d1706d87":"x_test_id = test[\"Id\"]\r\nfinal_submission = pd.DataFrame({\"Id\": x_test_id, \"SalePrice\": yhat})\r\nfinal_submission.to_csv(\"version_2_submission_.csv\", index=False)\r\n","de48ef2c":"## Final Submission","d7cd45ff":"## Predictions","e0323ef8":"Removing the the following columns which contains very high number of missing values(more than 50%)\r\nAlley, PoolQC, Fence, MiscFeature, FireplaceQu","92747064":"## Housing Price Preciction","4be52d53":"## EDA","92109343":"In the below code block of the notebook we are using the most frequent value for the garage year built.","88ca6980":"## Data preprocessing","8b33f02c":"Using Gradient Boosting Regressor model","01bec8a8":"Here we are scaling all the features with the int64 data-type, but with the exeptions of MSSubClass, OverallQual and OverallCond because these are the features with labels as integers as you can see in the data description.","6bc2fcd3":"## Import the dataset","f8da60ea":"Dealing with missing values\n","dcc37789":"Encoding the categorical features with get_dummies and appending the MSSubClass, OverallQual and OverallCond features.","b20a6189":"## Model Training","4e8fdb58":"## Import the libraries","daff1e45":"Checking that there is not any mismatch in the number of columns in the test and train set<br>Mismatch happens due to encoding of test and train set because they have different number of attributes in the dataframe. ","49611266":"Finding columns with the missing values in the training and test dataset","43ba6569":"## Train Test Split","40895992":"Above we can see that there are 5 columns(common in both train and test set) with atleast 50% missing values, and intotal 19 columns in training set and 33 columns in test set with missing values ","7f09d1c0":"Distribution of the Sale Price(density estimation) - according to the below curve it's a bell curve with positive skewness","0379b99c":"Here's a outline for the work shown in the notebook\n1. Importing the libraries\n2. Importing the datas\n3. Exploratory Data Analysis\n   1. Sale Price distribution\n   2. Finding columns with missing values\n   3. Finding correlation between the different features\n4. Data Preprocessing\n    1. Scalingthe features\n    2. Categorical Encoding using dummies\n5. Model training\n6. Final Submission","d07539f0":"Scaling the target feature - SalePrice<br>Here I am using normalization technique to scale the target feature i have used standard scaler also but it doesnot improve the accuracy of the model it the same."}}