{"cell_type":{"90fd9764":"code","993c8d66":"code","274924dd":"code","c7800f49":"code","b97eaadf":"code","e34e9153":"code","eec353eb":"code","437511ee":"code","ddb70b0c":"code","60b28cdd":"code","79950e8b":"code","9fd596b8":"code","2ab8fc68":"code","d85d827f":"code","deff5303":"code","58ca8cf7":"markdown","c90cbdf9":"markdown","18f85cf7":"markdown","f400449d":"markdown","a2e5015e":"markdown","c07f1ed9":"markdown"},"source":{"90fd9764":"import os\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","993c8d66":"!ls ..\/input\/\n\nprint(\"\\nEmbeddings:\")\n!ls ..\/input\/embeddings\/","274924dd":"print('File sizes')\nfor f in os.listdir('..\/input'):\n    if 'zip' not in f:\n        print(f.ljust(30) + str(round(os.path.getsize('..\/input\/' + f) \/ 1000000, 2)) + 'MB')","c7800f49":"train = pd.read_csv('..\/input\/train.csv').fillna(' ')\ntest = pd.read_csv('..\/input\/test.csv').fillna(' ')","b97eaadf":"print(\"Shape of training set: \", train.shape)\nprint(\"Shape of test set: \", test.shape)\n\ntrain_target = train['target'].values\nnp.unique(train_target)\nprint(\"\\nPercentage of insincere questions irt sincere questions: \", train_target.mean(), \"%\")","e34e9153":"train.sample(10)","eec353eb":"test.sample(10)","437511ee":"insincere_q = train[train[\"target\"] == 1][\"question_text\"].tolist()\n\nwith open('insinceres.txt', 'w') as f:\n    for item in insincere_q:\n        f.write(\"%s\\n\" % item)","ddb70b0c":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nstop_words = set(stopwords.words('english')) \ninsinc_df = train[train.target==1]\nsinc_df = train[train.target==0]\n\ndef plot_ngrams(n_grams):\n\n    ## custom function for ngram generation ##\n    def generate_ngrams(text, n_gram=1):\n        token = [token for token in text.lower().split(\" \") if token != \"\" if token not in stop_words]\n        ngrams = zip(*[token[i:] for i in range(n_gram)])\n        return [\" \".join(ngram) for ngram in ngrams]\n\n    ## custom function for horizontal bar chart ##\n    def horizontal_bar_chart(df, color):\n        trace = go.Bar(\n            y=df[\"word\"].values[::-1],\n            x=df[\"wordcount\"].values[::-1],\n            showlegend=False,\n            orientation = 'h',\n            marker=dict(\n                color=color,\n            ),\n        )\n        return trace\n\n    def get_bar(df, bar_color):\n        freq_dict = defaultdict(int)\n        for sent in df[\"question_text\"]:\n            for word in generate_ngrams(sent, n_grams):\n                freq_dict[word] += 1\n        fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n        fd_sorted.columns = [\"word\", \"wordcount\"]\n        trace = horizontal_bar_chart(fd_sorted.head(10), bar_color)\n        return trace    \n\n    trace0 = get_bar(sinc_df, 'blue')\n    trace1 = get_bar(insinc_df, 'blue')\n\n    # Creating two subplots\n    if n_grams == 1:\n        wrd = \"words\"\n    elif n_grams == 2:\n        wrd = \"bigrams\"\n    elif n_grams == 3:\n        wrd = \"trigrams\"\n    \n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                              subplot_titles=[\"Frequent \" + wrd + \" of sincere questions\", \n                                              \"Frequent \" + wrd + \" of insincere questions\"])\n    fig.append_trace(trace0, 1, 1)\n    fig.append_trace(trace1, 1, 2)\n    fig['layout'].update(height=500, width=1150, paper_bgcolor='rgb(233,233,233)', title=wrd + \" Count Plots\")\n    py.iplot(fig, filename='word-plots')\n","60b28cdd":"plot_ngrams(1)","79950e8b":"plot_ngrams(2)","9fd596b8":"plot_ngrams(3)","2ab8fc68":"## Number of words in the text\ntrain[\"num_words\"] = train[\"question_text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"question_text\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text\ntrain[\"num_unique_words\"] = train[\"question_text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"question_text\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text\ntrain[\"num_chars\"] = train[\"question_text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"question_text\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text\ntrain[\"num_stopwords\"] = train[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\ntest[\"num_stopwords\"] = test[\"question_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n\n## Average length of the words in the text\ntrain[\"mean_word_len\"] = train[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest[\"mean_word_len\"] = test[\"question_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","d85d827f":"## Truncate some extreme values for better visuals ##\ntrain['num_words'].loc[train['num_words']>50] = 50\ntrain['num_unique_words'].loc[train['num_unique_words']>50] = 50\ntrain['num_chars'].loc[train['num_chars']>300] = 300\ntrain['mean_word_len'].loc[train['mean_word_len']>10] = 10\n\nf, axes = plt.subplots(5, 1, figsize=(15,40))\n\nsns.boxplot(x='target', y='num_words', data=train, ax=axes[0])\naxes[0].set_xlabel('Target', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_unique_words', data=train, ax=axes[1])\naxes[1].set_xlabel('Target', fontsize=12)\naxes[1].set_title(\"Number of unique words in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_chars', data=train, ax=axes[2])\naxes[2].set_xlabel('Target', fontsize=12)\naxes[2].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='num_stopwords', data=train, ax=axes[3])\naxes[3].set_xlabel('Target', fontsize=12)\naxes[3].set_title(\"Number of stopwords in each class\", fontsize=15)\n\nsns.boxplot(x='target', y='mean_word_len', data=train, ax=axes[4])\naxes[4].set_xlabel('Target', fontsize=12)\naxes[4].set_title(\"Mean word length in each class\", fontsize=15)\n\nplt.show()","deff5303":"print(train.columns)\ntrain.head()","58ca8cf7":"## 1. Embeddings\n\n* GoogleNews-vectors-negative300 - https:\/\/code.google.com\/archive\/p\/word2vec\/\n* glove.840B.300d - https:\/\/nlp.stanford.edu\/projects\/glove\/\n* paragram_300_sl999 - https:\/\/cogcomp.org\/page\/resource_view\/106\n* wiki-news-300d-1M - https:\/\/fasttext.cc\/docs\/en\/english-vectors.html","c90cbdf9":"## 3. N-gram analysis","18f85cf7":"## 2. Open trainset and testset","f400449d":"## 5. References\n\n* [General EDA](https:\/\/www.kaggle.com\/tunguz\/just-some-simple-eda)\n* [Exploration notebook](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc)","a2e5015e":"## 4. Hyperparameters","c07f1ed9":"# Quora EDA\n\n1. Embeddings\n2. Open trainset and testset\n3. N-gram analysis\n4. Hyperparameters\n5. References"}}