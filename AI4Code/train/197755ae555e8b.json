{"cell_type":{"9c968871":"code","c84d8a9c":"code","6e2394e4":"code","ba5855e9":"code","a1e7da4c":"code","065e6562":"code","34ba3681":"code","66de8932":"code","ceefa8ef":"code","b8e561e9":"code","d66013c8":"code","6e957da1":"code","e3f9f6ac":"code","4ccb21bb":"code","3d210d27":"code","d1df4107":"code","ed4eb043":"code","d72100cf":"code","ea4e4cd7":"code","ac07518e":"markdown","1d0ed91e":"markdown","b9ecd6d3":"markdown","bd9c8a7f":"markdown","6cf57726":"markdown","a4adc555":"markdown","ee6d666b":"markdown","3905ee14":"markdown","007006fa":"markdown","ea9916bb":"markdown","15720f59":"markdown"},"source":{"9c968871":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom xgboost import XGBRegressor\npd.set_option('display.max_columns', None)","c84d8a9c":"def save_file (predictions):\n    \"\"\"Save submission file.\"\"\"\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': sample_submission_file.Id,\n                       'SalePrice': predictions})\n    output.to_csv('submission.csv', index=False)\n    print (\"Submission file is saved\")","6e2394e4":"# Read the data\ntrain_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\nX = train_data.copy()\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice', 'Utilities'], axis=1, inplace=True)\nX_test.drop(['Utilities'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\nsample_submission_file = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv\")\n\nprint('Data is OK')","ba5855e9":"# Select object columns\ncategorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column_train = (X_train.isnull().sum())\nprint(\"Number of missing values in each column:\")\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])","a1e7da4c":"# Number of missing values in numerical columns\nmissing_val_count_by_column_numeric = (X_train[numerical_cols].isnull().sum())\nprint(\"Number of missing values in numerical columns:\")\nprint(missing_val_count_by_column_numeric[missing_val_count_by_column_numeric > 0])","065e6562":"# Imputation lists\n\n# imputation to null values of these numerical columns need to be 'constant'\nconstant_num_cols = ['GarageYrBlt', 'MasVnrArea']\n\n# imputation to null values of these numerical columns need to be 'mean'\nmean_num_cols = list(set(numerical_cols).difference(set(constant_num_cols)))\n\n# imputation to null values of these categorical columns need to be 'constant'\nconstant_categorical_cols = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n                             'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n\n# imputation to null values of these categorical columns need to be 'most_frequent'\nmf_categorical_cols = list(set(categorical_cols).difference(set(constant_categorical_cols)))\n\nmy_cols = constant_num_cols + mean_num_cols + constant_categorical_cols + mf_categorical_cols","34ba3681":"# Define transformers\n# Preprocessing for numerical data\n\nnumerical_transformer_m = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\nnumerical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scaler', StandardScaler())])\n\n\n# Preprocessing for categorical data for most frequent\ncategorical_transformer_mf = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n# Preprocessing for categorical data for constant\ncategorical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_mean', numerical_transformer_m, mean_num_cols),\n        ('num_constant', numerical_transformer_c, constant_num_cols),\n        ('cat_mf', categorical_transformer_mf, mf_categorical_cols),\n        ('cat_c', categorical_transformer_c, constant_categorical_cols)\n    ])","66de8932":"# Define Model\nmodel = XGBRegressor(learning_rate = 0.1,\n                            n_estimators=500,\n                            max_depth=5,\n                            min_child_weight=1,\n                            gamma=0,\n                            subsample=0.8,\n                            colsample_bytree=0.8,\n                            reg_alpha = 0,\n                            reg_lambda = 1,\n                            random_state=0)","ceefa8ef":"# Define XGBRegressor fitting parameters for the pipeline\nfit_params = {\"model__eval_metric\" : \"mae\"}","b8e561e9":"# Create the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","d66013c8":"# Preprocessing of training data\nX_cv = X[my_cols].copy()\nX_sub = X_test[my_cols].copy()","6e957da1":"# Define model parameters for grid search\nparam_grid = {'model__learning_rate': [0.05],\n              'model__n_estimators': [500],\n              'model__max_depth': [5, 6, 7],\n              'model__min_child_weight': [1, 2],\n              'model__gamma': [0],\n              'model__subsample': [0.70, 0.80],\n              'model__colsample_bytree': [0.70, 0.80]}","e3f9f6ac":"# Perform grid search\n# Use model parameters defined above.\nsearch = GridSearchCV(my_pipeline, param_grid, cv=5, n_jobs=-1,scoring='neg_mean_absolute_error')\nsearch.fit(X_cv, y)","4ccb21bb":"# Best estimator\nsearch.best_estimator_","3d210d27":"# Best parameters\nsearch.best_params_","d1df4107":"# Get the best parameters from the grid search\n\n# https:\/\/stackoverflow.com\/questions\/41475539\/using-best-params-from-gridsearchcv\n# @Cybercop, @Oliver Dain, @T. Shiftlet \nparameters={x.replace(\"model__\", \"\"): v for x, v in search.best_params_.items()}\n\n# Update the model and the pipeline with the new set of parameters.\nmodel = XGBRegressor(**parameters)\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])","ed4eb043":"# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X_cv, y,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\n\nprint(\"MAE score:\\n\", scores)\nprint(\"MAE mean: {}\".format(scores.mean()))\nprint(\"MAE std: {}\".format(scores.std()))","d72100cf":"# Fit model\nmy_pipeline.fit(X_cv, y)\n\n# Get predictions\npreds = my_pipeline.predict(X_sub)","ea4e4cd7":"# Use predefined utility function\nsave_file(preds)","ac07518e":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> Cross-validation is performed by using the newly obtained model from grid search and full training set.\n<\/div>","1d0ed91e":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> First we need to define parameters for our model as a dictionary in order to make grid search work.\n<\/div>","b9ecd6d3":"# Preprocessing  <a id='preprocessing'><\/a>","bd9c8a7f":"# Using GridSearchCV <a id='gridsearch'><\/a>","6cf57726":"# Saving and submission   <a id='saving'><\/a>","a4adc555":"# Cross-validation <a id='cross-validation'><\/a>","ee6d666b":"# Helper functions   <a id='functions'><\/a>   ","3905ee14":"# References   <a id='references'><\/a>\n* [10-simple-hacks-to-speed-up-your-data-analysis - Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis)\n* [Tuning the Hyper-parameters of an Estimator](https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html)\n* [Intermediate Machine Learning Course - Pipelines](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)\n* [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)\n* [Using the Best Parameters](https:\/\/stackoverflow.com\/questions\/41475539\/using-best-params-from-gridsearchcv)","007006fa":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> We will use some helper functions throughout the notebook. Collecting them in one place is a good idea. It makes the code more organized.\n<\/div>","ea9916bb":"# Prediction   <a id='prediction'><\/a>","15720f59":"# Introduction  <a id='introduction'><\/a>\n\nThis is a fast and simple starter code for those who want to work with sklearn grid search and pipelines. The score for this notebook is **13677.99** which is within the top **2.5%** for this competition. However, the score may change due to the indeterministic nature of the model. The reader may easily get a better score by performing EDA and feature engineering.   \n\nKagglers who are interested with using early_stopping_rounds in pipelines may refer to a worsen version of this notebook [Housing Prices: Pipeline Starter Code](https:\/\/www.kaggle.com\/erkanhatipoglu\/housing-prices-pipeline-starter-code).\n\nKagglers who are interested in more advanced subjects of sklearn pipelines may refer to my notebook [Introduction to Sklearn Pipelines with Titanic](https:\/\/www.kaggle.com\/erkanhatipoglu\/introduction-to-sklearn-pipelines-with-titanic). \n\nThank you for reading.\n\n# Table of Contents\n* [Introduction](#introduction)\n* [Helper Functions](#functions)\n* [Preprocessing](#preprocessing) \n* [Using GridSearchCV](#gridsearch)\n* [Cross-validation](#cross-validation)    \n* [Prediction](#prediction) \n* [Saving and submission](#saving)  \n* [References](#references)"}}