{"cell_type":{"176f2e73":"code","0d59cbda":"code","14ba0dc3":"code","58ef3b18":"code","3e237797":"code","600bdd2a":"code","24205ba4":"code","1d1d734c":"code","dffb0f66":"code","b7999d20":"code","08fdb3c0":"code","02c54775":"code","db5e2d73":"code","8bcf5a53":"code","abd225f6":"code","7f788ef8":"code","7faebc31":"code","6850bea0":"code","31d76184":"code","57c6dc3d":"code","a1ead211":"code","6ab9bf83":"code","23e3e578":"code","9e2d5165":"code","508d8b33":"code","7ca4b049":"code","02e4fb17":"markdown","14d60dbd":"markdown","4a1f2e43":"markdown","8f0f0f31":"markdown","5cf24a28":"markdown","7f38b095":"markdown","74ab990f":"markdown","b7e21891":"markdown","8e33dcbb":"markdown","fbe47f3e":"markdown","db8f8054":"markdown","0e542a53":"markdown","7f191d39":"markdown"},"source":{"176f2e73":"import tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\n\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n    # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n    # Memory growth must be set before GPUs have been initialized\n        print(e)\n\n# tf.config.run_functions_eagerly(True)\n# tf.data.experimental.enable_debug_mode()        \n        \ntf.__version__","0d59cbda":"def data_size(ds):\n    return tf.data.experimental.cardinality(ds).numpy()\n\n# list_files is a tf.data.Dataset of all files matching one or more glob patterns.\nall_files = tf.data.Dataset.list_files(\"..\/input\/fake-video-images-dataset\/images_from_video_big\/*\",\n                                      shuffle = False)\n# We set shuffle = False, because we will split them into train, val, and test sets.\n# If we set shuffle = True, the dataset will be shuffled in every iteration. That may cause data leak.\n\nall_files = all_files.shuffle(data_size(all_files), seed=12345, \n                              reshuffle_each_iteration = False)","14ba0dc3":"classes = ['0', '1'] # 0 real, 1: fake\n\ntest_size = int(len(all_files) * 0.1)\n\nlist_ds_test = all_files.take(test_size)\nlist_ds_train = all_files.skip(test_size)\n\nlist_ds_val = list_ds_train.take(test_size)\nlist_ds_train = list_ds_train.skip(test_size)\n\n\nprint('Train:', data_size(list_ds_train)) # 66097\nprint('Val:', data_size(list_ds_val)) # 8262\nprint('Test:', data_size(list_ds_test)) # 8262\n\n# Checking for overlapping\ntrain = []\nfor x in list_ds_train:\n    train.append(x.numpy().decode('utf-8'))\n\nval = []\nfor x in list_ds_val:\n    val.append(x.numpy().decode('utf-8'))\n    \nprint('Intersection of two sets: ', len(set(train).intersection(set(val)))) # 0","58ef3b18":"def get_label(files_path):\n    # contents tensor contains:\n#    tf.Tensor(\n#    [b'..' b'input' b'fake-video-images-dataset' b'images_from_video_big'\n#    b'aaaaqqicldbtmpgcdsuljwmsuznhfwyp_17_0.jpg'], shape=(5,), dtype=string\n\n   \n    contents = tf.strings.split(files_path, '\/')[-1]\n    # With index -1, we only get b'aaaaqqicldbtmpgcdsuljwmsuznhfwyp_17_0.jpg'\n\n    # class_idx tensor contains:\n    # tf.strings.substr(contents, -5, 1), we will get 0 or 1, aka class label.\n    # tf.Tensor([ True False], shape=(2,), dtype=bool)>\n    class_idx = tf.strings.substr(contents, -5, 1) == classes\n    class_idx = tf.cast(class_idx, tf.int32)\n\n    # Return: tf.where(class_idx)\n    # tf.Tensor(0, shape=(), dtype=int64)\n    # Alternatively we could have used tf.argmax(class_idx), no indexing would be required.\n    label = tf.where(class_idx)[0][0]\n    return label\n\ndef decode_img(image):\n\n    # Image files read with tf.io.read_file must be decoded\n    # channels = 3, since we want them in RGB format.\n    image = tf.image.decode_jpeg(image, channels=3)\n    return image\n\ndef process_path(file_path):\n    label = get_label(file_path)\n\n    # img :<tf.Tensor: shape=(), dtype=string, numpy=b'\\xff\\xd8\\xff\\x0\\ ...\n    # tf.io.read_file reads the content as it is, images need to be decoded.\n    image = tf.io.read_file(file_path)\n\n    # Now we have the images as float tensors.\n    image = decode_img(image)\n    return image, label","3e237797":"train_ds = list_ds_train.map(process_path, \n                             num_parallel_calls=tf.data.AUTOTUNE)\n\nval_ds = list_ds_val.map(process_path, \n                         num_parallel_calls=tf.data.AUTOTUNE)\n\ntest_ds = list_ds_test.map(process_path, \n                           num_parallel_calls=tf.data.AUTOTUNE)","600bdd2a":"def count_classes(ds): \n    labels, counts = np.unique(np.fromiter(ds.map(lambda x, y: y),np.int32), \n                           return_counts=True)\n    plt.figure(figsize = (10,6))\n    ax = sns.barplot(labels, counts)\n    plt.xticks(size=12)\n    plt.xlabel('Target', size = 14)\n    plt.yticks(size=12)\n    plt.ylabel('Counts', size=14)\n    ax.yaxis.set_major_formatter(ticker.EngFormatter())\n\n    ax.set_xticklabels(ax.get_xticklabels(), ha=\"center\")\n    total = len(ds)\n    for p in ax.patches:\n        percentage = f'{100 * p.get_height() \/ total:.1f}%\\n'\n        x = p.get_x() + p.get_width() \/ 2\n        y = p.get_height()\n        ax.annotate(percentage, (x, y), ha='center', va='center')\n    plt.tight_layout()\n    plt.title('Distribution of the labels')\n    plt.show()\n    plt.show()","24205ba4":"count_classes(train_ds)","1d1d734c":"count_classes(val_ds)","dffb0f66":"count_classes(test_ds)","b7999d20":"# With the help of tf.image and tfa.image we can perform image\n# augmentations on the tf.data easily.\n\nfor x, y in train_ds.take(1): # Take 1 image from dataset. (tf.data is not batched yet)\n    seed = (52, 2)\n    \n    # Here I demonstrate variety of functions that are available under tfa.image\n    # and tf.image. In tf.image we use stateless transformations which I explained below.\n    \n    random_bright = tf.image.stateless_random_brightness(x, max_delta=1.0, \n                                                       seed = seed)\n\n    random_contrast = tf.image.stateless_random_contrast(x, 0.2, 2.0,\n                                                       seed = seed)\n\n    random_saturation = tf.image.stateless_random_saturation(x, 0.2, 1.0,\n                                                           seed = seed)\n\n    random_crop_or_pad = tf.image.resize_with_crop_or_pad(x, \n                               tf.shape(x).numpy()[0] + 5, \n                               tf.shape(x).numpy()[1] + 5)\n\n    random_rotate = tfa.image.rotate(x, tf.constant(tf.random.uniform((1,), \n                                                        minval = 0.01,\n                                                        maxval = 0.4)))\n\n    sharpness = tfa.image.sharpness(x, 5.1)\n\n    plt.figure(figsize = (20, 12))\n\n    plt.subplot(1, 7, 1)\n    plt.imshow(x.numpy() \/ 255.0)\n    plt.title('Original')\n\n    plt.subplot(1, 7, 2)\n    plt.imshow(random_bright.numpy() \/ 255.0)\n    plt.title('Random Brightness')\n\n    plt.subplot(1, 7, 3)\n    plt.imshow(random_contrast.numpy() \/ 255.0)\n    plt.title('Random Contrast')\n\n    plt.subplot(1, 7, 4)\n    plt.imshow(random_saturation.numpy() \/ 255.0)\n    plt.title('Random Saturation')\n\n    plt.subplot(1, 7, 5)\n    plt.imshow(random_crop_or_pad.numpy() \/ 255.0)\n    plt.title('Random Crop or Pad')\n\n    plt.subplot(1, 7, 6)\n    plt.imshow(random_rotate.numpy() \/ 255.0)\n    plt.title('Random Rotate')\n\n    plt.subplot(1, 7, 7)\n    plt.imshow(sharpness.numpy() \/ 255.0)\n    plt.title('Sharpness')","08fdb3c0":"def resize_and_cast(image, label):\n    \n    # We cast images to float32.\n    image = tf.cast(image, tf.float32)\n    \n    # Resizing every image to 128 x 128 pixels\n    image = tf.image.resize(image, [128, 128])\n    \n    # Finally we return label and normalized image.\n    return image \/ 255.0, label","02c54775":"def augment(image_label, seed):\n    image, label = image_label\n    image, label = resize_and_cast(image,label)\n\n    # Transformations are divided into two groups\n    transformation_selection = tf.random.uniform([], minval = 0, \n                                                 maxval = 1, \n                                                 dtype = tf.float32)\n    \n    # And we will divide two groups into two sub-groups\n    \n    # Prob. of first group's first function        \n    prob_1 = tf.random.uniform([], minval = 0, maxval = 1, dtype = tf.float32)\n    \n    # Prob. of second group's first function        \n    prob_2 = tf.random.uniform([], minval = 0, maxval = 1, dtype = tf.float32)\n    \n    # Every transformation has a 25% chance of being applied.\n    # So we apply one transformation per image\n    image = tf.cond(tf.greater(transformation_selection, 0.5),\n\n                    # We write lambda because tf.cond expects conditions\n                    # to be callables.\n                    \n                    # If transformation_selection > 0.5\n                    # If prob_1 > 0.5 alter contrast otherwise change brightness\n                    lambda: tf.cond(tf.greater(prob_1, 0.5),\n                            lambda: tf.image.stateless_random_contrast(image, 0.1, 0.5,\n                                                     seed = seed), \n                            lambda: tf.image.stateless_random_brightness(image, max_delta=0.3, \n                                                     seed = seed),\n                           ),\n                    \n                    # If transformation_selection < 0.5\n                    # If prob_2 > 0.5 apply saturation otherwise rotate\n                    lambda: tf.cond(tf.greater(prob_2, 0.5),\n                            lambda: tf.image.stateless_random_saturation(image, 0.01, 0.1,\n                                                           seed = seed),\n                            lambda: tfa.image.rotate(image, tf.random.uniform((1,), \n                                                        minval = 0.01,\n                                                        maxval = 0.2))                          \n                          )     \n                   )\n\n    #   new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n    #   If you want to split transformations you can create a new seed and pass it to the function.\n    \"\"\"\n    Example: tf.image.stateless_random_saturation(image, 0.01, 0.1,\n                                              seed = new_seed)\n    \"\"\"\n    return image, label","db5e2d73":"random_number = tf.random.Generator.from_seed(874222, alg='philox')\n\n# For seeds we can wrap augmentation function into another function.\n# In this function we generate seeds and send to exact function that \n# will do transformations.\ndef augment_wrapper(x, y):\n    seed = random_number.make_seeds(2)[0]\n    image, label = augment((x, y), seed)\n    return image, label","8bcf5a53":"# Let's explain shuffle() and prefetch()\n\ntrain_ds = (\n    train_ds\n    .map(augment_wrapper, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(128)\n    .shuffle(32)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\n# We don't augment validation and test data.\nval_ds = (\n    val_ds\n    .map(resize_and_cast, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(128)\n    .prefetch(tf.data.AUTOTUNE)\n)\n\ntest_ds = (\n    test_ds\n    .map(resize_and_cast, num_parallel_calls=tf.data.AUTOTUNE)\n    .batch(128)\n    .prefetch(tf.data.AUTOTUNE)\n)","abd225f6":"plt.figure(figsize=(20, 20))\nplt.tight_layout()\nmapped = ['real', 'fake']\n# Since tf.data is batched, take(1) returns a whole batch\n# rather than a single image. So we loop first 16 images.\nfor images, labels in train_ds.take(1):\n    for i in range(16):\n        ax = plt.subplot(4, 4, i + 1)\n        plt.imshow(tf.clip_by_value(images[i], 0, 1).numpy())\n        plt.title(mapped[labels[i]])\n        plt.axis(\"off\")","7f788ef8":"densenet121 = tf.keras.applications.DenseNet121(input_shape = (128, 128, 3),\n                                            include_top = False,\n                                            weights = 'imagenet')\n\nxception = tf.keras.applications.Xception(input_shape = (128, 128, 3),\n                                            include_top = False,\n                                            weights = 'imagenet')","7faebc31":"densenet_out = densenet121.layers[-1].output # last layer's output.\n\ny = tf.keras.layers.GlobalAveragePooling2D()(densenet_out)\ny = tf.keras.layers.BatchNormalization()(y)\ny = tf.keras.layers.Dense(512, activation=tf.nn.silu, # use swish activation\n                         kernel_regularizer = tf.keras.regularizers.l2(1e-5))(y)\ny = tf.keras.layers.Dropout(0.2)(y)\ny = tf.keras.layers.Dense(1, activation = 'sigmoid', name = \"densenet_out\")(y)\ndensenet = tf.keras.Model(densenet121.input, y)\n\n\nxception_out = xception.layers[-1].output\n\nz = tf.keras.layers.GlobalMaxPooling2D()(xception_out)\nz = tf.keras.layers.BatchNormalization()(z)\nz = tf.keras.layers.Dense(512, activation=tf.nn.silu)(z)\nz = tf.keras.layers.Dropout(0.2)(z)\nz = tf.keras.layers.Dense(1, activation = 'sigmoid', name=\"xception_out\")(z)\nxception_model = tf.keras.Model(xception.input, z)","6850bea0":"models = [densenet, xception_model]\nmodel_input = tf.keras.Input(shape=(128, 128, 3))\nmodel_outputs = [model(model_input) for model in models]\n\n\"\"\"\nmodel_outputs:\n[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model')>,\n <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'model_1')>]\n\"\"\"\n\nensemble_output = tf.keras.layers.Average()(model_outputs)\n\nensemble_model = tf.keras.Model(inputs = model_input, \n                                outputs = ensemble_output, \n                                name='ensemble')\nensemble_model.summary()","31d76184":"# Pretrained models act as a layer.\ntf.keras.utils.plot_model(ensemble_model, 'ensemble_model.png', show_shapes = True)","57c6dc3d":"opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n\nensemble_model.compile(optimizer=opt, \n                       loss=tf.keras.losses.BinaryCrossentropy(from_logits = False), \n                       metrics=[tf.keras.metrics.BinaryAccuracy(),\n                               tf.keras.metrics.Precision(),\n                               tf.keras.metrics.Recall()])\n\nes = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                                      patience = 5, verbose = 1)\n\nrd_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                             factor = np.sqrt(0.14), \n                                             patience= 2, verbose = 1, \n                                             min_lr = 5e-8)\n\nhistory = ensemble_model.fit(train_ds, epochs = 16, \n                             validation_data= val_ds,\n                             verbose = 1, \n                             callbacks =[rd_lr, es])","a1ead211":"predictions = []\nlabels = []\n\n# We loop through the dataset to collect predictions and labels at once.\n\nfor image, label in test_ds:\n    labels.append(label)\n\n    # Since tf.data is batched, it generates batches.\n    # So \"image\" has a shape of (128, 128, 128, 3)\n    # 128 is the batch size that we have specified earlier.\n    # For small amount of input data we can directly use __call__\n    # method for the model. --> model(x) instead of model.predict(x)\n    \n    predictions.append(ensemble_model(image))\n\n# We concat. them at axis = 0, because we collected labels and predictions for EACH BATCH.\npredictions = np.concatenate(predictions, axis = 0)\nlabels = np.concatenate(labels, axis = 0)","6ab9bf83":"# These are sigmoid outputs\npredictions","23e3e578":"# We need to convert them to 1s or 0s.\nthreshold = 0.49\npredicted_classes = np.where(predictions > threshold, 1, 0)\npredicted_classes","9e2d5165":"# Generate confusion matrix\ncm = confusion_matrix(labels, predicted_classes)  \n\nindex = ['real', 'fake']  \ncolumns = ['real', 'fake']  \n\n# Build a pandas df\ncm_df = pd.DataFrame(cm,columns,index)                      \n\n# Show in a heatmap with classes as columns and indices.\nplt.figure(figsize=(10,6))  \nsns.heatmap(cm_df, annot=True, fmt='d')","508d8b33":"print(classification_report(labels, predicted_classes))","7ca4b049":"roc_auc_score(labels, predicted_classes)","02e4fb17":"# Setting up tf.data Pipeline","14d60dbd":"### How shuffle() works?\n* You call shuffle and you pass it a shuffle buffer. So for example, if you have 100,000 items in your dataset, \nand you set the buffer to 1000. With that way it will just fill the buffer with the first thousand elements, \npick one of them at random. And after that it will replace that with thousand and first element before randomly \npicking again, and so on.\n\n### How prefetch() works?\n* Prefetching solves the inefficiencies from naive approach as it aims to overlap the preprocessing and model \nexecution of the training step. In other words, when the model is executing training step n, the input pipeline \nwill be reading the data for step n+1.\n\n\n* With prefetch, CPU won't stay idle as GPU trains the model, it will prepare the next batch.\n\n\n![img](https:\/\/miro.medium.com\/max\/1838\/1*t2qd3IDo87K5iFGN_g__Ow.png)","4a1f2e43":"# Shuffle and Prefetch","8f0f0f31":"# Creating Seperate Models using Functional API","5cf24a28":"# Ensembling Models\n* We will not freeze any of the layers. Thus, they will be starting a point for this task.\n* We could have written a training loop which will be faster than `model.fit()`, but then we would lose\n  the high level callbacks. So let's stick to `model.fit()` this time.","7f38b095":"# tf.image & tfa.image","74ab990f":"# Distributions of the Classes","b7e21891":"# Training\n* Metrics `Recall()` and `Precision()` expect outputs in a range (0, 1). This is not a problem since\n  we used `sigmoid` in the last layers of the pretrained models. \n  \n* We can remove `sigmoid` activations and pass `from_logits = True`, but this time we have to remove `Recall() & Precision()`.","8e33dcbb":"# Fake-Video-Images-Dataset\n\n### Sections:\n* Importing required libraries\n* Setting up tf.data Pipeline\n* Showing some tf.image & tfa.image functions\n* Using Pretrained Models for Transfer Learning\n* Average Ensembling Two Pretrained Models\n* Evaluating the Model Performance","fbe47f3e":"#### Difference between normal tf.image functions and stateless ones:\n* There are two sets of random image operations: `tf.image.random*` and `tf.image.stateless_random*`. \n\n* Using `tf.image.random*` operations is strongly discouraged as they use the old RNGs from TF 1.x. (TensorFlow Website)","db8f8054":"# Pretrained Models on ImageNet","0e542a53":"# Evaluating Performance on Test Set","7f191d39":"# Libraries\n\n* All **tf.data** mapping operations are run in graph mode. If you want to debug these functions\n  you need to uncomment `tf.data.experimental.enable_debug_mode()`"}}