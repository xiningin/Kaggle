{"cell_type":{"ace640f6":"code","07ce6259":"code","80e27df6":"code","24afda3b":"code","49059de7":"code","9468dfd3":"code","a71e5bab":"code","a5551274":"code","fbacc144":"code","50d2f18b":"code","8c9abea1":"code","8da16398":"code","388e39fe":"code","0ddb6a74":"code","db0c9c43":"code","a9b6b91c":"code","e2749566":"code","f353d48c":"code","1b5d55e7":"code","43a74a1a":"code","a4ee6120":"code","8778bbef":"code","998291df":"code","f07aac95":"code","d6af1524":"code","b170ff2c":"code","44a58ab8":"code","8eb874b7":"code","9b9b442f":"code","2c170acf":"code","1944deff":"code","bea8cfee":"code","a51b8cb7":"code","3134cf8d":"markdown","6cdc2ee5":"markdown","4c4bc676":"markdown","e60b9e56":"markdown","75069832":"markdown","45ee2693":"markdown","634e7d99":"markdown"},"source":{"ace640f6":"import pandas as pd\nimport numpy as np\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#train=pd.read_csv(\"C:\/\/Users\/\/ULTRON\/\/Videos\/\/ML competetions\/\/mobile classification\/\/train.csv\")\ntrain=pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","07ce6259":"train.head()","80e27df6":"train.isna().sum()","24afda3b":"train.dtypes","49059de7":"from sklearn.ensemble import RandomForestClassifier","9468dfd3":"train.columns","a71e5bab":"##Simplest Random forest classifier \nX=train[['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n       'touch_screen', 'wifi']]\nY=train[\"price_range\"]\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.25)","a5551274":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier() #We will not tune anythinga nd will use the default values for this\nclf.fit(X_train,Y_train)","fbacc144":"y_pred=clf.predict(X_test)","50d2f18b":"y_pred","8c9abea1":"train[\"price_range\"].value_counts","8da16398":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(Y_test, y_pred)","388e39fe":"from sklearn import metrics\nmetrics.accuracy_score(Y_test, y_pred)","0ddb6a74":"##Setting up First feature that is n estimator for random forest \nprint(train.shape)\n##Setting the n-estimator as sqrt(2000)\nimport math\nprint(math.sqrt(2000))","db0c9c43":"from sklearn.ensemble import RandomForestClassifier\nclf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=45,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False) #We will not tune anythinga nd will use the default values for this\nclf.fit(X_train,Y_train)","a9b6b91c":"y_pred=clf.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nprint(confusion_matrix(Y_test, y_pred))\nfrom sklearn import metrics\nprint(metrics.accuracy_score(Y_test, y_pred))","e2749566":"##Improve the accuracy by 1%\n##No pass different estimator and calulate accuracy \nn_est=[30,45,90,100,150,200,250,300,350,400]\naccuracy=[]\nfor i in n_est:\n    clf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=i,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False) #We will not tune anythinga nd will use the default values for this\n    clf.fit(X_train,Y_train)\n    y_pred=clf.predict(X_test)\n#from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(Y_test, y_pred))\n#from sklearn import metrics\n    print(\"Accuracy for \",i,\"estimator \",metrics.accuracy_score(Y_test, y_pred))\n    accuracy.append(metrics.accuracy_score(Y_test, y_pred))","f353d48c":"##To check the variabliey of accuracy as depend on the number of estimators\nimport matplotlib.pyplot as plt\nplt.plot(n_est,accuracy)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"No of estimator\")","1b5d55e7":"crit=[\"gini\",\"entropy\"]\naccuracy=[]\nfor i in crit:\n    clf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion=i, max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=150,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False) #We will not tune anythinga nd will use the default values for this\n    clf.fit(X_train,Y_train)\n    y_pred=clf.predict(X_test)\n#from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(Y_test, y_pred))\n#from sklearn import metrics\n    print(\"Accuracy for \",i,\"criterion \",metrics.accuracy_score(Y_test, y_pred))\n    accuracy.append(metrics.accuracy_score(Y_test, y_pred))","43a74a1a":"max_depths = np.linspace(1, 32, 32, endpoint=True)\naccuracy=[]\nfor i in max_depths:\n    clf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion=\"gini\", max_depth=i, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=150,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False) #We will not tune anythinga nd will use the default values for this\n    clf.fit(X_train,Y_train)\n    y_pred=clf.predict(X_test)\n#from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(Y_test, y_pred))\n#from sklearn import metrics\n    print(\"Accuracy for \",i,\"depth \",metrics.accuracy_score(Y_test, y_pred))\n    accuracy.append(metrics.accuracy_score(Y_test, y_pred))","a4ee6120":"## To plot accuracy vs depth of tree plot\nimport matplotlib.pyplot as plt\nplt.plot(max_depths,accuracy)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"max_depths\")","8778bbef":"##To calucate the accuracy based on max feature \nfea=[\"auto\",\"sqrt\",\"log2\"]\naccuracy=[]\nfor i in fea:\n    clf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion=\"gini\", max_depth=12, max_features=i,\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=150,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False) #We will not tune anythinga nd will use the default values for this\n    clf.fit(X_train,Y_train)\n    y_pred=clf.predict(X_test)\n#from sklearn.metrics import confusion_matrix\n    print(confusion_matrix(Y_test, y_pred))\n#from sklearn import metrics\n    print(\"Accuracy for \",i,\"featuere\",metrics.accuracy_score(Y_test, y_pred))\n    accuracy.append(metrics.accuracy_score(Y_test, y_pred))","998291df":"## To plot accuracy vs feature of tree plot\nimport matplotlib.pyplot as plt\nplt.bar(fea,accuracy)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"feature\")","f07aac95":"from sklearn.model_selection import GridSearchCV\nparam_grid = { \n    'n_estimators': [ 100, 150],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [10,12,13],\n    'criterion' :crit\n}\nrfc=RandomForestClassifier(random_state=42,oob_score=True)\nC_rfc=GridSearchCV(estimator=rfc,param_grid=param_grid,cv=5)#Cross validation with score 5 ","d6af1524":"C_rfc.fit(X_train, Y_train)","b170ff2c":"C_rfc.best_params_","44a58ab8":"test=pd.read_csv('\/kaggle\/input\/mobile-price-classification\/test.csv')\ni_d=test[\"id\"]\ntest=test.drop([\"id\"],axis=1)","8eb874b7":"X_train","9b9b442f":"clf=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion=\"gini\", max_depth=12, max_features=\"auto\",\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=150,\n                       n_jobs=None, oob_score=True, random_state=None,\n                       verbose=1, warm_start=False) #We will not tune anythinga nd will use the default values for this\nclf.fit(X_train,Y_train)\ny_pred=clf.predict(test)","2c170acf":"len(y_pred)","1944deff":"final={\"id\":i_d,\n      \"Prediction\":y_pred}","bea8cfee":"final=pd.DataFrame(final)","a51b8cb7":"final","3134cf8d":"## Random Forest is on of the Bagging Technique to enhance the performance of decision tree ","6cdc2ee5":"## Its important to know each parameter for the random forest classifiation \n# 1. n_estimator = This shows the number of trees in the Decision Tree .One way to evaluate this is to use $\\sqrt{P}$ for classification and p\/3 for regression but this not general .OBB error is also used sometime for determine the No of tree.\n# 2 . Criterion == This is way to split the tree .Gini or Etropy can be used to determine the splitting of a cell\n# Gini = $\\sum_{n=1}^{C} p(i)*(1-p(i))$\n# Entropy =$\\sum_{n=1}^{C} -p(i)*(logp(i))$\n# 3. Max Depth = The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n# 4. MIN-SAMPLE-SPLIT= The minimum number of samples required to split an internal node:\n # 5. Max Feature = To detrmine the number of features you want to build up tree .Auto\/None : This will simply take all the features which make sense in every tree.Here we simply do not put any restrictions on the individual tree.\n# sqrt : This option will take square root of the total number of features in individual run. For instance, if the total number of variables are 100, we can only take 10 of them in individual tree.\u201dlog2\u2033 is another similar type of option for max_features.0.2 : This option allows the random forest to take 20% of variables in individual run. We can assign and value in a format \u201c0.x\u201d where we want x% of features to be considered.\n\n# 6. N-jobs - to determine the number of processors for the tree . -1 for all processors and 1 for no \n# 7. oob-score ==Out of Bag score are the values or rows that are being ignored by multiple trees in random forest and can be used as a cross validation state .","4c4bc676":"## Applying grid search for the tuning \n","e60b9e56":"import pandas as pd\nimport numpy as np","75069832":"## Best depth for this tree is around 10 to 15 so we will choose 12 as optimal depth ","45ee2693":"## Its a simple dataset to elabrate the Random forest .This notebook focuses on paramters of the Random Forest","634e7d99":"## The best numner of estimator is about at 150\n## Now we will try to check the accuracy for spliiting method of tree"}}