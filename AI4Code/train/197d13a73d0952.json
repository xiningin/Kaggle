{"cell_type":{"bd4e93dd":"code","67ba2143":"code","b3e0fd40":"code","f0f9b097":"code","c3509f47":"code","8f000147":"code","c914e10b":"code","9d40f660":"code","31c2787a":"markdown"},"source":{"bd4e93dd":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, Laplace\nfrom torchvision import datasets, transforms\nimport PIL\n\nimport numpy as np\n\n# -----------------------------------------------------------------------------\n\n#Using Google's LeNet, suggested in\n#Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \"Gradient-based learning applied to document recognition,\"\n# in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, Nov. 1998, doi: 10.1109\/5.726791.\nclass LeNet(nn.Module):\n    def __init__(self, in_chan, out_chan, imsize, kernel_size=5):\n        super(LeNet, self).__init__()\n        \"\"\"\n        Implements the LeNet architecture\n        :param in_chan: number of input channels (for example an RGB input will have 3 channels)\n        :param out_chan: number of output classes\n        :param imsize: size of the input image in pixels\n        :param kernel_size: size of the kernel to be passed against the image, by default set to 5\n        \"\"\"\n        \n        #Hard-coded a padding = 1 so after 2 convolutions and max pooling\n        #we get an image with 1\/4 of the size, but need to take away the padding \n        #from the image size to get the following z\n        z = 0.5*(imsize - 2)\n        z = int(0.5*(z - 2))\n        \n        #convolutional layer No.1 takes the in-channels and produces 6 images by going through these with 6 kernels\n        self.conv1 = nn.Conv2d(in_chan, 6, kernel_size, padding=1)\n        #conv layer No.2 produces 16 out of the said 6\n        self.conv2 = nn.Conv2d(6, 16, kernel_size, padding=1)\n        #thus we have 16 images of size z, we can line them up in a vector and feed to the following linear layer\n        self.fc1   = nn.Linear(16*z*z, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, out_chan)\n        #a drop out layer so that the weights are uniformly invested in\n        self.drop  = nn.Dropout(p=0.5)\n        \n        \n\n    def forward(self, x):\n        \"\"\"\n        Implements the LeNet architecture\n        :param x: ndarray, data input of shape (batch_size,in_chan,imsize) \n        :return: ndarray, a vector output of the logit layer.  The softmax of this layer gives the probabilities\n        \"\"\"\n        #Applying conv layer No.1 and rectified linear activation function\n        x = F.relu(self.conv1(x))\n        #Max Pooling to shrink the images to 1\/2 their initial size\n        x = F.max_pool2d(x, 2)\n        #conv layer No.2\n        x = F.relu(self.conv2(x))\n        #Pooling again\n        x = F.max_pool2d(x, 2)\n        #turning the 16 images to one long vector\n        x = x.view(x.size()[0], -1)\n        #passing through the dense neural network\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        #adding drop out between the 2nd and 3rd hidden layers\n        x = self.drop(x)\n        #fully connecting to the outside\n        x = self.fc3(x)\n        \n\n        return x\n","67ba2143":"#Setting image parameters for the surface crack database\nimsize=227 # 227x227 images\nin_chan = 3 # RGB\nout_chan = 2 # 2 output classes: positive and negative","b3e0fd40":"#Setting Hyperparameters\nepochs = 100\nlearning_rate = 1e-3\ndecay = 1e-8\nbatch_size = 64 \nkernel_size = 5\n#adding transforms to the image \ntransform = transforms.Compose([\n    #cropping the centre\n    transforms.CenterCrop(imsize),\n    #adding random rotations\n    transforms.RandomRotation([0,360],resample=PIL.Image.BILINEAR),\n    #transforming the dataset to Torch tensors\n    transforms.ToTensor(),\n    #normalising the image\n    transforms.Normalize((0.0229,), (0.0957,))])\n#loading the dataset and applying the transforms\ndataset = datasets.ImageFolder('..\/input\/surface-crack-detection',transform=transform)","f0f9b097":"#Loading the data into a data loader, chop them into batches and shuffle the batches everytime this object is called\ntrain_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n#Initialise a model \nmodel = LeNet(in_chan=in_chan, out_chan=out_chan, imsize=imsize, kernel_size=kernel_size)\n#Add Adam optimiser with a weight decay parameter to penalise big weights\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)\n\nepoch_trainaccs, epoch_valaccs = [], []\nepoch_trainloss, epoch_valloss = [], []\nfor epoch in range(epochs):  # loop over the dataset multiple times\n        #set the model to train mode (gradient acquiring)\n        model.train()\n        train_losses,  train_accs = [], []; acc = 0\n        #iterate through the train_loader\n        for batch, (x_train, y_train) in enumerate(train_loader):\n            #clear previous gradients\n            model.zero_grad()\n            #forward propagate the model\n            pred = model(x_train)\n            #Use the cross entropy loss because it's logistical regression\n            loss = F.cross_entropy(pred,y_train)\n            #backpropagate the loss\n            loss.backward()\n            #optimise the weights accordingly\n            optimizer.step()\n            #calculate the training accuracy\n            acc = (pred.argmax(dim=-1) == y_train).to(torch.float32).mean()\n            train_accs.append(acc.mean().item())\n            train_losses.append(loss.item())   \n            print(\"Batch=\",batch,\" loss = \",loss.item(), \" accuracy = \",acc.item())\n            \n        epoch_trainloss.append(np.mean(train_losses))\n        epoch_trainaccs.append(np.mean(train_accs))\n        print(\"Epoch = \",epoch,\" Mean loss = \",np.mean(train_losses))","c3509f47":"!pip install pennylane","8f000147":"import pennylane as qml\nfrom pennylane.templates import AmplitudeEmbedding\nfrom pennylane import numpy as np\nfrom pennylane.optimize import AdamOptimizer\n\ndev = qml.device('default.qubit', wires=19)\n\n@qml.qnode(dev,diff_method='backprop')\ndef circuit(f,w):\n    AmplitudeEmbedding(features=f, wires=range(18),normalize=True, pad_with=0.)\n    for i in range(18):\n        qml.RY(w[i],wires=i)\n    qml.broadcast(qml.CNOT, wires=range(18), pattern=\"all_to_all\", parameters=None, kwargs=None)\n    for i in range(18):\n        qml.RY(w[i+18],wires=i)\n    for i in range(18):\n      qml.CNOT(wires=[i,18])\n    return qml.expval(qml.PauliZ(18))","c914e10b":"for batch, (x_train, y_train) in enumerate(train_loader):\n    print(y_train)","9d40f660":"y = np.array([])\nfor batch, (x_train, y_train) in enumerate(train_loader):\n\n        x = x_train[0];\n        x = x.view(x.size()[0], -1).detach().numpy()\n        x = x.flatten()\n        out = circuit(x,w)\n        (out+1)\/2\n        \ndef get_avg_loss(train_loader,w):\n\ndef get_loss(out,y):\n    return -np.log(out)[y]\n     \n    \nw = np.array(np.random.uniform(size=(36,),low=-1,high=1),requires_grad=True)\nlearning_rate = 0.1\noptimiser = AdamOptimizer(learning_rate)\nfor i in range(100):\n    w, train_loss_value = optimiser.step_and_cost(lambda v: get_loss(x,v), w)\n\n    print(train_loss_value)","31c2787a":"# Quantum Methods"}}