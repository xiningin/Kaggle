{"cell_type":{"3c784643":"code","d132a134":"code","b3967543":"code","423310d8":"code","9186db0a":"code","9d16c6a0":"code","ddeb5452":"code","9065f395":"code","8388d935":"code","787f622a":"code","80b61dfd":"code","468b993f":"code","3614353d":"code","5e3db69f":"code","8670bcb3":"code","d7d45020":"code","07082de5":"code","b6e48ffc":"code","249c9e30":"code","156614d1":"code","442787cf":"code","3e9007bc":"code","b46b60dd":"code","dd7a9bd6":"code","923294f5":"code","ce81dc6e":"code","8dee2148":"code","25166094":"code","47f1b9e9":"code","3285ad99":"code","6aca8fe6":"code","dc3eccd1":"code","75635f66":"code","8b8fa02d":"code","faf23c9b":"code","a3217dbc":"code","10cd77aa":"code","c0e4f3f7":"code","45750257":"markdown","6cd77236":"markdown","8680f9c3":"markdown","2415b4ce":"markdown","e2afa46a":"markdown","f050e313":"markdown","453f5d38":"markdown","d73df0b4":"markdown"},"source":{"3c784643":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport random as rn","d132a134":"#Reproducible Results\nimport os\nos.environ['PYTHONHASHSEED'] = str(1337)","b3967543":"#Reproducible Results\nnp.random.seed(1337)","423310d8":"#Reproducible Results\nrn.seed(1337)","9186db0a":"#Reproducible Results\ntf.compat.v1.set_random_seed(1337)","9d16c6a0":"#Reproducible Results\nfrom keras import backend as K\n\nsession_conf = tf.compat.v1.ConfigProto(\n    intra_op_parallelism_threads=1, \n    inter_op_parallelism_threads=1\n)\n\nsess = tf.compat.v1.Session(\n    graph=tf.compat.v1.get_default_graph(), \n    config=session_conf\n)\n\ntf.compat.v1.keras.backend.set_session(sess)","ddeb5452":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n# To display the whole columns\npd.set_option('display.max_columns', 999)\n\nlow_memory=False","9065f395":"# Plotting\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8388d935":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","787f622a":"# Dataset is now stored in a Pandas dataframe\n\n#Fahad,I will add the data to the dataframe\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets =  pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","80b61dfd":"# To use them later in sumbission dataframe\ncolumns_names = list(train_targets.columns)\nsig_id = test_features[['sig_id']]","468b993f":"# Most of observations have only one label or no label, \n# and the others have more than 1 label  \n\nsns.set(style=\"whitegrid\")\nsns.countplot(x=train_targets[train_targets == 1].sum(axis=1), color =\"b\")\nplt.title(\"The Number of Lables Compared to Observations\")\nplt.xlabel(\"Number of Labels\")\nplt.ylabel(\"Number of Observations\")","3614353d":"# Most of the labels are triggered by only one observation, \n# some of them have more than 4 observations\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(30,8))\nsns.countplot(x=train_targets[train_targets == 1].sum(axis=0), color ='r')\nplt.title(\"The Distribution of Observations Among Lables\")\nplt.xlabel(\"The Label\")\nplt.ylabel(\"Number of Observations\")\nplt.xticks(rotation=90)","5e3db69f":"from sklearn.preprocessing import LabelEncoder, LabelBinarizer","8670bcb3":"# Remove sig_id\ntrain_features = train_features.drop(['sig_id'], axis=1)\ntrain_targets = train_targets.drop(['sig_id'], axis=1)\n\ntest_features = test_features.drop(['sig_id'], axis=1)","d7d45020":"# Label encoder \nlb_enc = LabelEncoder()\n\ntrain_features['cp_type'] = lb_enc.fit_transform(train_features['cp_type'])\ntrain_features['cp_dose'] = lb_enc.fit_transform(train_features['cp_dose'])\n\ntest_features['cp_type'] = lb_enc.fit_transform(test_features['cp_type'])\ntest_features['cp_dose'] = lb_enc.fit_transform(test_features['cp_dose'])","07082de5":"# Convert into arrays\ntrain_features = train_features.values\ntrain_targets = train_targets.values\n\ntest_features = test_features.values","b6e48ffc":"# Reshape the features \nx_train_flat = train_features.reshape(-1,875)\nx_test_flat = test_features.reshape(-1,875)","249c9e30":"train_features.shape","156614d1":"# Using PCA to keep most 90% important features\nfrom sklearn.decomposition import PCA\npca = PCA(0.9)\n\npca.fit(x_train_flat)\n\nPCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n  svd_solver='auto', tol=0.0, whiten=False)\n\npca.n_components_\n\ntrain_pca = pca.transform(x_train_flat)\ntest_pca = pca.transform(x_test_flat)","442787cf":"test_pca.shape","3e9007bc":"# Loading required libraries for DL\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout","b46b60dd":"# Hyper-parameters\nbatch_size = 100\nnum_classes = 206\nepochs = 200","dd7a9bd6":"'''\nfrom sklearn.model_selection import KFold\n\n# define 10-fold cross validation test harness\nkfold = KFold(n_splits=10, shuffle=True, random_state=1337)\n\ncvscores = []\n\nfor train, test in kfold.split(train_features, train_targets):\n    \n    x_train, x_test = train_features.iloc[list(train)], train_features.iloc[list(test)]\n    y_train, y_test = train_targets.iloc[list(train)], train_targets.iloc[list(test)]\n\n    # The model architecture\n    model = Sequential()\n    model.add(Dense(1024, activation='tanh', input_shape=(875,)))\n    model.add(Dense(512, activation='tanh'))\n    model.add(Dense(512, activation='tanh'))\n    model.add(Dense(256, activation='tanh'))\n    model.add(Dense(num_classes, activation='sigmoid'))\n    \n    # Compile model\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy', f1_m])\n    \n    # Train the model\n    history = model.fit(x_train, \n                        y_train,\n                        batch_size=batch_size,\n                        epochs=10,\n                        callbacks=callbacks,\n                        verbose=1)                         \n    \n    # Evaluate the model\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\nprint(\"%.2f%% (+\/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n'''","923294f5":"# The network\nmodel = Sequential()\nmodel.add(Dense(1024, activation='tanh', input_shape=(280,)))\nmodel.add(Dense(512, activation='tanh'))\nmodel.add(Dense(512, activation='tanh'))\nmodel.add(Dense(256, activation='tanh'))\nmodel.add(Dense(num_classes, activation='sigmoid'))","ce81dc6e":"# To monitor validation set results , we will use F1 score scince we have multi-label classification\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","8dee2148":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy', f1_m])","25166094":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\ncallbacks = [EarlyStopping(monitor='loss', mode='min', patience=5, verbose=1),\n             ModelCheckpoint('MoA.hdf5', monitor='loss', mode='min', \n                             verbose=1, save_best_only=True, save_weights_only=False, period=1)]","47f1b9e9":"# Training\nhistory = model.fit(train_pca, \n                    train_targets,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    callbacks=callbacks,\n                    verbose=1)","3285ad99":"# Obtain the predictions for test set\npredictions = model.predict(test_pca)","6aca8fe6":"predictions","dc3eccd1":"# Convert the predictions into dataframe\nsubmission = pd.DataFrame(data=predictions)","75635f66":"# Add sig_id to the results\nsubmission.insert(loc=0, column='1', value=sig_id)","8b8fa02d":"# Getting column names for the submission dataframe\nsubmission.columns = columns_names","faf23c9b":"submission.head()","a3217dbc":"#submission.iloc[:,1:] = submission.iloc[:,1:].round(2)","10cd77aa":"#submission.head()","c0e4f3f7":"# Save submission file into a csv\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index = False)","45750257":"## Reading Data\n### Fahad","6cd77236":"## Loading libraries","8680f9c3":"## EDA \n### Haifaa","2415b4ce":"## Pre-processing\n### Haifaa","e2afa46a":"## PCA\n### Haifaa","f050e313":"## Prediction\n### Haifaa","453f5d38":"## Network Design (DNN)\n### Haifaa","d73df0b4":"  ## Team Members:\n  \n  1- EDA (Fahad)  >> Not implemented yet\n  \n  2- Statistical Summary + Data Visualization (Bashayer) >> Not implemented yet\n  \n  3- Feature Engineering (Faris) >> Withdrown \n  \n  4- Pre-processing (Kholoud) >> Withdrown \n\n  5- Pre-processing + Building the model (Haifaa) >> Done\n  \n  6- Deploying the model = Sample Submission"}}