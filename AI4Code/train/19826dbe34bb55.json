{"cell_type":{"ba712677":"code","d1973398":"code","1145b012":"code","2011c0f8":"code","d3177575":"code","7554dab7":"code","b61f665b":"code","772e4143":"code","35b3ff8c":"code","12c96381":"code","1539ee15":"code","02a86653":"code","1e404b30":"code","64de4e1d":"code","493eb3f0":"code","30fea0be":"code","4f56a951":"code","0728f0d0":"code","78af94db":"code","d3f30bc9":"code","830384e3":"code","06b5af6b":"code","db5bdf45":"code","132a82d3":"code","4c91aace":"code","882098a5":"code","13aec829":"code","01af3248":"code","87d4b5c7":"code","18623969":"code","1e24108f":"code","86ba4974":"code","20872885":"code","9297c404":"code","bfcf4a10":"code","0be535da":"code","d86eacdd":"code","039a00bc":"code","ae098756":"code","353adcdb":"code","aa2adeea":"code","920dc408":"code","950b5049":"code","d91379c4":"code","d1b984c2":"code","694cbffc":"code","284bbff5":"code","f74865ce":"markdown","9dabce81":"markdown","e3d2d350":"markdown","13482b3c":"markdown","2b8bb385":"markdown","7daf5241":"markdown","0af1cb25":"markdown","145c34ea":"markdown"},"source":{"ba712677":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n# Import necessary packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport pydicom as dicom\nimport seaborn as sns\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.densenet import DenseNet121\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.models import Model\nfrom keras import backend as K\n\nfrom keras.models import load_model\nsns.set()","d1973398":"# l=[]\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     if dirname=='\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/chest_xray\/train\/PNEUMONIA':\n#         for filename in filenames:\n#             if 'chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/'+filename not in l:\n#                 l.append('chest-xray-pneumonia\/chest_xray\/train\/PNEUMONIA\/'+filename)\n\n\n# dict1={'image_id':l,'Atelectasis':[0]*len(l),'Cardiomegaly':[0]*len(l),'Consolidation':[0]*len(l),'Infiltration':[0]*len(l),'Nodule\/Mass':[0]*len(l),'Pleural effusion':[0]*len(l),'Pleural thickening':[0]*len(l),'Pneumothorax':[0]*len(l),'Pulmonary fibrosis':[0]*len(l),'Pneumonia':[1]*len(l)}\n# train_df1=pd.DataFrame(dict1)\n# train_df1.head()\n# # Extract numpy values from Image column in data frame\n# images = train_df1['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n# train_df2=pd.read_csv(\"..\/input\/nih-df-changed\/mycsvfile (3).csv\")\n# train_df2.head()\n# for i in range(len(train_df2['Image Index'])):\n#     train_df2['Image Index'][i]='data\/'+train_df2['Image Index'][i]\n# train_df2 = train_df2.rename(columns={'Image Index': 'image_id'})\n# # Extract numpy values from Image column in data frame\n# images = train_df2['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n\n# train_df3 = pd.read_csv(\"..\/input\/vinbigdata-original-image-dataset\/vinbigdata\/train.csv\")\n# # Print first 5 rows\n# print(f'There are {train_df3.shape[0]} rows and {train_df3.shape[1]} columns in this data frame')\n# train_df3.head()\n# c='vinbigdata-original-image-dataset\/vinbigdata\/train\/'\n# for i in range(len(train_df3['image_id'])):\n#     train_df3['image_id'][i]=c+train_df3['image_id'][i]+'.jpg'\n# ata1=[];cardi3=[];conso4=[];infi6=[];nod8=[];peff10=[];pth11=[];pntho12=[];pfib13=[];pne14=[]\n\n# for i in train_df3['class_id']:\n#     if i==1:\n#         ata1.append(1)\n#     else:\n#         ata1.append(0)\n        \n#     if i==3:\n#         cardi3.append(1)\n#     else:\n#         cardi3.append(0)\n        \n#     if i==4:\n#         conso4.append(1)\n#     else:\n#         conso4.append(0)\n    \n#     if i==6:\n#         infi6.append(1)\n#     else:\n#         infi6.append(0)\n        \n#     if i==8:\n#         nod8.append(1)\n#     else:\n#         nod8.append(0)\n        \n#     if i==10:\n#         peff10.append(1)\n#     else:\n#         peff10.append(0)\n        \n#     if i==11:\n#         pth11.append(1)\n#     else:\n#         pth11.append(0)\n        \n#     if i==12:\n#         pntho12.append(1)\n#     else:\n#         pntho12.append(0)\n        \n#     if i==13:\n#         pfib13.append(1)\n#     else:\n#         pfib13.append(0)\n        \n#     if i==15:\n#         pne14.append(1)\n#     else:\n#         pne14.append(0)\n# train_df3['Atelectasis']=ata1\n# train_df3['Cardiomegaly']=cardi3\n# train_df3['Consolidation']=conso4\n# train_df3['Infiltration']=infi6\n# train_df3['Nodule\/Mass']=nod8\n# train_df3['Pleural effusion']=peff10\n# train_df3['Pleural thickening']=pth11\n# train_df3['Pneumothorax']=pntho12\n# train_df3['Pulmonary fibrosis']=pfib13\n# train_df3['Pneumonia']=pne14\n# # train_df['PatientId']=[i for i in range(len(train_df['class_id']))]\n# train_df3=train_df3.drop(['class_name','class_id','rad_id','x_min','y_min','x_max','y_max','width', 'height'], axis = 1)\n# train_df3.head()\n# columns = train_df3.keys()\n# columns = list(columns)\n# # print(columns)\n# columns.remove('image_id')\n# # Print out the number of positive labels for each class\n# for column in columns:\n#     print(f\"The class {column} has {train_df3[column].sum()} samples\")\n# # Extract numpy values from Image column in data frame\n# images = train_df3['image_id'].values\n\n# # Extract 9 random images from it\n# random_images = [np.random.choice(images) for i in range(9)]\n\n# # Location of the image dir\n# img_dir = '\/kaggle\/input\/'\n\n# print('Display Random Images')\n\n# # Adjust the size of your images\n# plt.figure(figsize=(20,10))\n\n# # Iterate and plot random images\n# for i in range(9):\n#     plt.subplot(3, 3, i + 1)\n#     img = plt.imread(img_dir+random_images[i])\n#     plt.imshow(img, cmap='gray')\n#     plt.axis('off')\n    \n# # Adjust subplot parameters to give specified padding\n# plt.tight_layout()\n# base=max(train_df2['Patient ID'])\n# f_base=base+len(train_df1['image_id'])+1\n# df1pid=[i for i in range(base+1,f_base)]\n# df3pid=[i for i in range(f_base,f_base+len(train_df3['image_id']))]\n# print(len(df1pid)==len(train_df1['image_id']),len(df3pid)==len(train_df3['image_id']))\n\n# train_df1['Patient ID']=df1pid\n# train_df3['Patient ID']=df3pid\n# train_df=train_df2.append(train_df1,ignore_index=True)\n# train_df=train_df.append(train_df3,ignore_index=True)\n# train_df = train_df.sample(frac = 1)\n# train_df.info()\n# train_df.to_csv('final_merged_df.csv',index=False)","1145b012":"train_df=pd.read_csv('..\/input\/final-merged-df\/final_merged_df.csv')","2011c0f8":"train_df = train_df.sample(frac = 1)","d3177575":"train_df.head()","7554dab7":"columns = train_df.keys()\ncolumns = list(columns)\n# print(columns)\ncolumns.remove('image_id')\ncolumns.remove('Patient ID')\n# Print out the number of positive labels for each class\nfor column in columns:\n#     print(f\"The class {column} has {train_df1[column].sum()} samples\")\n#     print(f\"The class {column} has {train_df2[column].sum()} samples\")\n#     print(f\"The class {column} has {train_df3[column].sum()} samples\")\n    print(f\"The class {column} has {train_df[column].sum()} samples\")","b61f665b":"labels = ['Atelectasis','Cardiomegaly','Consolidation','Infiltration','Nodule\/Mass','Pleural effusion','Pleural thickening','Pneumothorax','Pulmonary fibrosis','Pneumonia']","772e4143":"def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=16, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for training set, normalizing using batch\n    statistics.\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size\n    # and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator","35b3ff8c":"def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size=100, batch_size=16, seed=1, target_w = 320, target_h = 320):\n    \"\"\"\n    Return generator for validation set and test test set using \n    normalization statistics from training set.\n\n    Args:\n      valid_df (dataframe): dataframe specifying validation data.\n      test_df (dataframe): dataframe specifying test data.\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively\n    \"\"\"\n    print(\"getting train and valid generators...\")\n    # get generator to sample dataset\n    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n        dataframe=train_df, \n        directory=IMAGE_DIR, \n        x_col=x_col, \n        y_col=labels, \n        class_mode=\"raw\", \n        batch_size=sample_size, \n        shuffle=True, \n        target_size=(target_w, target_h))\n    # get data sample\n    batch = raw_train_generator.next()\n    data_sample = batch[0]\n\n    # use sample to fit mean and std for test set generator\n    image_generator = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization= True)\n    \n    # fit generator to sample from training data\n    image_generator.fit(data_sample)\n\n    # get test generator\n    valid_generator = image_generator.flow_from_dataframe(\n            dataframe=valid_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n\n    test_generator = image_generator.flow_from_dataframe(\n            dataframe=test_df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=False,\n            seed=seed,\n            target_size=(target_w,target_h))\n    return valid_generator, test_generator","12c96381":"# ########################################## 1st set\n\n# train_df1 = train_df1.sample(frac = 1)\n# # print(train_df1.iloc[0])\n# valid_df1=pd.DataFrame()\n# # valid_df.append(train_df.iloc[0],ignore_index = True)\n# # valid_df.head()\n# # print(type(train_df1['Patient ID'][2]))\n# # # valid_df1=valid_df1.append(train_df1.iloc[0],ignore_index = True)\n# add_pd=[]\n# dropidx=[]\n# i=0\n# for j in range(len(train_df1['Patient ID'])):\n#     if i>1000:\n#         break\n#     if train_df1['Patient ID'].iloc[j] not in add_pd:\n#         valid_df1=valid_df1.append(train_df1.iloc[j],ignore_index = True)\n#         add_pd.append(train_df1['Patient ID'].iloc[j])\n#         dropidx.append(j)\n#         i+=1\n# print(dropidx)\n# # train_df1.drop(dropidx,inplace=True)\n# # train_df1 = train_df1.sample(frac = 1)\n\n\n# # from sklearn.model_selection import train_test_split\n# # train_df1, test_df1 = train_test_split(train_df1, test_size=0.005)\n# # # valid_df1","1539ee15":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(train_df, test_size=0.005)\ntrain_df, valid_df = train_test_split(train_df, test_size=0.01)\n# valid_df1","02a86653":"print(len(train_df),len(test_df),len(valid_df))","1e404b30":"IMAGE_DIR = '\/kaggle\/input\/'\n# X_train,y_train=data_to_array(train_df1,IMAGE_DIR)\n# X_val,y_val=data_to_array(valid_df1,IMAGE_DIR)\n# X_test,y_test=data_to_array(test_df1,IMAGE_DIR)\ntrain_generator = get_train_generator(train_df, IMAGE_DIR, \"image_id\", labels)\nvalid_generator, test_generator= get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"image_id\", labels)","64de4e1d":"x, y = train_generator.__getitem__(0)\nplt.imshow(x[0]);","493eb3f0":"plt.xticks(rotation=90)\nplt.bar(x=labels, height=np.mean(train_generator.labels, axis=0))\nplt.title(\"Frequency of Each Class\")\nplt.show()","30fea0be":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # total number of patients (rows)\n    N = labels.shape[0]\n    \n    positive_frequencies = np.sum(labels, axis = 0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n\n    ### END CODE HERE ###\n    return positive_frequencies, negative_frequencies","4f56a951":"# Test\nlabels_matrix = np.array(\n    [[1, 0, 0],\n     [0, 1, 1],\n     [1, 0, 1],\n     [1, 1, 1],\n     [1, 0, 1]]\n)\nprint(\"labels:\")\nprint(labels_matrix)\n\ntest_pos_freqs, test_neg_freqs = compute_class_freqs(labels_matrix)\n\nprint(f\"pos freqs: {test_pos_freqs}\")\n\nprint(f\"neg freqs: {test_neg_freqs}\")","0728f0d0":"freq_pos, freq_neg = compute_class_freqs(train_generator.labels)\nfreq_pos","78af94db":"data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)","d3f30bc9":"pos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights","830384e3":"data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n                        for l,v in enumerate(neg_contribution)], ignore_index=True)\nplt.xticks(rotation=90)\nsns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);","06b5af6b":"# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n    \"\"\"\n    Return weighted loss function given negative weights and positive weights.\n\n    Args:\n      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n    \n    Returns:\n      weighted_loss (function): weighted loss function\n    \"\"\"\n    def weighted_loss(y_true, y_pred):\n        \"\"\"\n        Return weighted loss value. \n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n        Returns:\n            loss (Float): overall scalar loss summed across all classes\n        \"\"\"\n        # initialize loss to zero\n        loss = 0.0\n        \n        ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n        for i in range(len(pos_weights)):\n            # for each class, add average weighted loss for that class \n            loss += K.mean(-(pos_weights[i] *y_true[:,i] * K.log(y_pred[:,i] + epsilon) \n                             + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon))) #complete this line\n        return loss\n    \n        ### END CODE HERE ###\n    return weighted_loss","db5bdf45":"def weighted_loss(y_true, y_pred):\n    \"\"\"\n    Return weighted loss value. \n\n    Args:\n        y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n        y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n    Returns:\n        loss (Float): overall scalar loss summed across all classes\n    \"\"\"\n    # initialize loss to zero\n    loss = 0.0\n\n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n\n    for i in range(len(pos_weights)):\n        # for each class, add average weighted loss for that class \n        loss += K.mean(-(pos_weights[i] *y_true[:,i] * K.log(y_pred[:,i] + epsilon) \n                         + neg_weights[i]* (1 - y_true[:,i]) * K.log( 1 - y_pred[:,i] + epsilon))) #complete this line\n    return loss","132a82d3":"# Test\nimport tensorflow as tf\ntf.compat.v1.disable_eager_execution()\nsess = tf.compat.v1.keras.backend.get_session()\n# sess = K.get_session()\nwith sess.as_default() as sess:\n    print(\"Test example:\\n\")\n    y_true = K.constant(np.array(\n        [[1, 1, 1],\n         [1, 1, 0],\n         [0, 1, 0],\n         [1, 0, 1]]\n    ))\n    print(\"y_true:\\n\")\n    print(y_true.eval())\n\n    w_p = np.array([0.25, 0.25, 0.5])\n    w_n = np.array([0.75, 0.75, 0.5])\n    print(\"\\nw_p:\\n\")\n    print(w_p)\n\n    print(\"\\nw_n:\\n\")\n    print(w_n)\n\n    y_pred_1 = K.constant(0.7*np.ones(y_true.shape))\n    print(\"\\ny_pred_1:\\n\")\n    print(y_pred_1.eval())\n\n    y_pred_2 = K.constant(0.3*np.ones(y_true.shape))\n    print(\"\\ny_pred_2:\\n\")\n    print(y_pred_2.eval())\n\n    # test with a large epsilon in order to catch errors\n    L = get_weighted_loss(w_p, w_n, epsilon=1)\n\n    print(\"\\nIf we weighted them correctly, we expect the two losses to be the same.\")\n    L1 = L(y_true, y_pred_1).eval()\n    L2 = L(y_true, y_pred_2).eval()\n    print(f\"\\nL(y_pred_1)= {L1:.4f}, L(y_pred_2)= {L2:.4f}\")\n    print(f\"Difference is L1 - L2 = {L1 - L2:.4f}\")","4c91aace":"from keras.utils.generic_utils import get_custom_objects\n# import SSD_Loss\n\nloss = get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7)\nget_custom_objects().update({\"weighted_loss\": loss})","882098a5":"from keras import callbacks\nearlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", verbose=2, patience = 5, \n                                        restore_best_weights = True)","13aec829":"from itertools import chain","01af3248":"from sklearn.datasets import make_blobs\nfrom sklearn.metrics import accuracy_score\nfrom keras.models import load_model\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers.merge import concatenate\nfrom numpy import argmax\n","87d4b5c7":"# # stacked generalization with neural net meta model on blobs dataset\n# from sklearn.datasets import make_blobs\n# from sklearn.metrics import accuracy_score\n# from keras.models import load_model\n# from keras.utils import to_categorical\n# from keras.utils import plot_model\n# from keras.models import Model\n# from keras.layers import Input\n# from keras.layers import Dense\n# from keras.layers.merge import concatenate\n# from numpy import argmax\n\n# # load models from file\n# def load_all_models(n_models):\n#     all_models = list()\n#     for i in range(n_models):\n#         # define filename for this ensemble\n#         if i==0:\n#             filename = '..\/input\/final-resnet152-ep3\/final_resnet152_ep3.h5'\n#         else:\n#             filename = '..\/input\/final-weights-densenet-epoch-4\/final_1_ep4.h5'\n#         # load model from file\n#         model = load_model(filename)\n#         # add to list of members\n#         all_models.append(model)\n#         print('>loaded %s' % filename)\n#     return all_models\n\n# # define stacked model from multiple member input models\n# def define_stacked_model(members):\n#     # update all layers in all models to not be trainable\n#     for i in range(len(members)):\n#         model = members[i]\n#         for layer in model.layers:\n#             # make not trainable\n#             layer.trainable = False\n#             # rename to avoid 'unique layer name' issue\n#             layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n#     # define multi-headed input\n#     ensemble_visible = [model.input for model in members]\n#     # concatenate merge output from each model\n#     ensemble_outputs = [model.output for model in members]\n#     merge = concatenate(ensemble_outputs)\n#     hidden1 = Dense(16, activation='relu')(merge)\n# #     hidden2=Dense(16, activation='relu')(hidden1)\n#     output = Dense(10, activation='sigmoid')(hidden1)\n#     model = Model(inputs=ensemble_visible, outputs=output)\n#     # plot graph of ensemble\n#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n#     # compile\n#     model.compile(optimizer='adam', loss=get_weighted_loss(pos_weights, neg_weights), metrics=['accuracy'])\n#     return model\n\n# # fit a stacked model\n# def fit_stacked_model(model, train_generator,valid_generator):\n#     # prepare input data\n# #     X = [inputX for _ in range(len(model.input))]\n# #     # encode output data\n# #     inputy_enc = to_categorical(inputy)\n# #     # fit model\n# #     model.fit_(X, inputy_enc, epochs=300, verbose=0)\n# #     tgr=chain(train_generator,train_generator)\n# #     vgr=chain(valid_generator,valid_generator)\n#     def combine_generator(gen1, gen2):\n#         while True:\n#             a1=next(gen1)\n            \n#             yield [a1[0],a1[0]],a1[1] \n#     tgr=combine_generator(train_generator,train_generator)\n#     vgr=combine_generator(valid_generator,valid_generator)\n#     model.fit_generator(tgr,validation_data=vgr,callbacks =[earlystopping],steps_per_epoch=11250,validation_steps=125,epochs=300, verbose=0)\n#     model.save(\"stacked_model.h5\")\n# # make a prediction with a stacked model\n# def predict_stacked_model(model, inputX):\n#     # prepare input data\n#     X = [inputX for _ in range(len(model.input))]\n#     # make prediction\n#     return model.predict(X, verbose=0)\n\n# # generate 2d classification dataset\n# # X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2)\n# # # split into train and test\n# # n_train = 100\n# # trainX, testX = X[:n_train, :], X[n_train:, :]\n# # trainy, testy = y[:n_train], y[n_train:]\n# # print(trainX.shape, testX.shape)\n# # train\n\n\n","18623969":"# # load all models\n# n_members = 2\n# members = load_all_models(n_members)\n# print('Loaded %d models' % len(members))\n# # define ensemble model\n# stacked_model = define_stacked_model(members)\n","1e24108f":"# def combine_generator(gen1, gen2):\n#     while True:\n#         a1=next(gen1)\n\n#         yield [a1[0],a1[0]],a1[1] \n# tgr=combine_generator(train_generator,train_generator)\n# vgr=combine_generator(valid_generator,valid_generator)\n# stacked_model.fit_generator(tgr,validation_data=vgr,callbacks =[earlystopping],steps_per_epoch=1125,validation_steps=125,epochs=1, verbose=1)\n","86ba4974":"# stacked_model.save(\"stacked_model.h5\")","20872885":"stacked_model=load_model('..\/input\/final-stacked-model\/stacked_model.h5')","9297c404":"# ################################################\n# # import tensorflow as tf\n# # # from keras import backend as K\n# # import keras\n# # import numpy as np\n\n# model = tf.keras.models.load_model(\"..\/input\/final-weights-densenet-epoch-4\/final_1_ep4.h5\")\n","bfcf4a10":"# sess = tf.compat.v1.keras.backend.get_session()\n# # sess = K.get_session()\n# with sess.as_default() as sess:\n#     print(\"Learning rate before first fit:\", model.optimizer.learning_rate.eval())\n# K.set_value(model.optimizer.learning_rate, 0.00001)","0be535da":"def combine_generator(gen1, gen2):\n    while True:\n        a1=next(gen1)\n\n        yield [a1[0],a1[0]],a1[1] \ntgr=combine_generator(test_generator,test_generator)\npredicted_vals = stacked_model.predict_generator(tgr, steps = len(test_generator))","d86eacdd":"predicted_vals.shape","039a00bc":"true_val=test_generator.labels\ntrue_val.shape","ae098756":"y=true_val\npred=predicted_vals","353adcdb":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import (\n    average_precision_score,\n    precision_recall_curve,\n    roc_auc_score,\n    roc_curve,\n)\n\n\ndef get_true_pos(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == True) & (y == 1))\n\n\ndef get_true_neg(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == False) & (y == 0))\n\n\ndef get_false_neg(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == False) & (y == 1))\n\n\ndef get_false_pos(y, pred, th=0.5):\n    pred_t = (pred > th)\n    return np.sum((pred_t == True) & (y == 0))\n\n\ndef get_performance_metrics(y, pred, class_labels, tp=get_true_pos,\n                            tn=get_true_neg, fp=get_false_pos,\n                            fn=get_false_neg,\n                            acc=None, prevalence=None, spec=None,\n                            sens=None, ppv=None, npv=None, auc=None, f1=None,\n                            thresholds=[]):\n    if len(thresholds) != len(class_labels):\n        thresholds = [.5] * len(class_labels)\n\n    columns = [\"\", \"TP\", \"TN\", \"FP\", \"FN\", \"Accuracy\", \"Prevalence\",\n               \"Sensitivity\",\n               \"Specificity\", \"PPV\", \"NPV\", \"AUC\", \"F1\", \"Threshold\"]\n    df = pd.DataFrame(columns=columns)\n    for i in range(len(class_labels)):\n        df.loc[i] = [\"\"] + [0] * (len(columns) - 1)\n        df.loc[i][0] = class_labels[i]\n        df.loc[i][1] = round(tp(y[:, i], pred[:, i]),\n                             3) if tp != None else \"Not Defined\"\n        df.loc[i][2] = round(tn(y[:, i], pred[:, i]),\n                             3) if tn != None else \"Not Defined\"\n        df.loc[i][3] = round(fp(y[:, i], pred[:, i]),\n                             3) if fp != None else \"Not Defined\"\n        df.loc[i][4] = round(fn(y[:, i], pred[:, i]),\n                             3) if fn != None else \"Not Defined\"\n        df.loc[i][5] = round(acc(y[:, i], pred[:, i], thresholds[i]),\n                             3) if acc != None else \"Not Defined\"\n        df.loc[i][6] = round(prevalence(y[:, i]),\n                             3) if prevalence != None else \"Not Defined\"\n        df.loc[i][7] = round(sens(y[:, i], pred[:, i], thresholds[i]),\n                             3) if sens != None else \"Not Defined\"\n        df.loc[i][8] = round(spec(y[:, i], pred[:, i], thresholds[i]),\n                             3) if spec != None else \"Not Defined\"\n        df.loc[i][9] = round(ppv(y[:, i], pred[:, i], thresholds[i]),\n                             3) if ppv != None else \"Not Defined\"\n        df.loc[i][10] = round(npv(y[:, i], pred[:, i], thresholds[i]),\n                              3) if npv != None else \"Not Defined\"\n        df.loc[i][11] = round(auc(y[:, i], pred[:, i]),\n                              3) if auc != None else \"Not Defined\"\n        df.loc[i][12] = round(f1(y[:, i], pred[:, i] > thresholds[i]),\n                              3) if f1 != None else \"Not Defined\"\n        df.loc[i][13] = round(thresholds[i], 3)\n\n    df = df.set_index(\"\")\n    return df\n\n\ndef print_confidence_intervals(class_labels, statistics):\n    df = pd.DataFrame(columns=[\"Mean AUC (CI 5%-95%)\"])\n    for i in range(len(class_labels)):\n        mean = statistics.mean(axis=1)[i]\n        max_ = np.quantile(statistics, .95, axis=1)[i]\n        min_ = np.quantile(statistics, .05, axis=1)[i]\n        df.loc[class_labels[i]] = [\"%.2f (%.2f-%.2f)\" % (mean, min_, max_)]\n    return df\n\n\ndef get_curve(gt, pred, target_names, curve='roc'):\n    for i in range(len(target_names)):\n        if curve == 'roc':\n            curve_function = roc_curve\n            auc_roc = roc_auc_score(gt[:, i], pred[:, i])\n            label = target_names[i] + \" AUC: %.3f \" % auc_roc\n            xlabel = \"False positive rate\"\n            ylabel = \"True positive rate\"\n            a, b, _ = curve_function(gt[:, i], pred[:, i])\n            plt.figure(1, figsize=(7, 7))\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.plot(a, b, label=label)\n            plt.xlabel(xlabel)\n            plt.ylabel(ylabel)\n\n            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n                       fancybox=True, ncol=1)\n        elif curve == 'prc':\n            precision, recall, _ = precision_recall_curve(gt[:, i], pred[:, i])\n            average_precision = average_precision_score(gt[:, i], pred[:, i])\n            label = target_names[i] + \" Avg.: %.3f \" % average_precision\n            plt.figure(1, figsize=(7, 7))\n            plt.step(recall, precision, where='post', label=label)\n            plt.xlabel('Recall')\n            plt.ylabel('Precision')\n            plt.ylim([0.0, 1.05])\n            plt.xlim([0.0, 1.0])\n            plt.legend(loc='upper center', bbox_to_anchor=(1.3, 1),\n                       fancybox=True, ncol=1)","aa2adeea":"def get_accuracy(y, pred, th=0.5):\n    \"\"\"\n    Compute accuracy of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        accuracy (float): accuracy of predictions at threshold\n    \"\"\"\n    accuracy = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP, FP, TN, FN using our previously defined functions\n    TP = get_true_pos(y, pred, th)   \n    FP = get_false_pos(y, pred, th)\n    TN = get_true_neg(y, pred, th)\n    FN = get_false_neg(y,pred, th)\n\n    # Compute accuracy using TP, FP, TN, FN\n    accuracy = (TP + TN) \/ ( TP + FP + TN + FN)\n    \n    ### END CODE HERE ###\n    \n    return accuracy\n\n\n\ndef get_prevalence(y):\n    \"\"\"\n    Compute prevalence.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n    Returns:\n        prevalence (float): prevalence of positive cases\n    \"\"\"\n    prevalence = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    prevalence = np.mean(y)\n    \n    ### END CODE HERE ###\n    \n    return prevalence\n\n\n\n\n\n# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_sensitivity(y, pred, th=0.5):\n    \"\"\"\n    Compute sensitivity of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        sensitivity (float): probability that our test outputs positive given that the case is actually positive\n    \"\"\"\n    sensitivity = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP and FN using our previously defined functions\n    TP = get_true_pos(y,pred, th)\n    FN = get_false_neg(y, pred, th)\n\n    # use TP and FN to compute sensitivity\n    sensitivity = TP \/ (TP + FN)\n    \n    ### END CODE HERE ###\n    \n    return sensitivity\n\ndef get_specificity(y, pred, th=0.5):\n    \"\"\"\n    Compute specificity of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        specificity (float): probability that the test outputs negative given that the case is actually negative\n    \"\"\"\n    specificity = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TN and FP using our previously defined functions\n    TN = get_true_neg(y,pred, th)\n    FP = get_false_pos(y, pred, th)\n    \n    # use TN and FP to compute specificity \n    specificity = TN \/ (TN + FP)\n    \n    ### END CODE HERE ###\n    \n    return specificity\n\n\n\n# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef get_ppv(y, pred, th=0.5):\n    \"\"\"\n    Compute PPV of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        PPV (float): positive predictive value of predictions at threshold\n    \"\"\"\n    PPV = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TP and FP using our previously defined functions\n    TP = get_true_pos(y,pred,th)\n    FP = get_false_pos(y,pred,th)\n\n    # use TP and FP to compute PPV\n    PPV = TP \/ (TP + FP)\n    \n    ### END CODE HERE ###\n    \n    return PPV\n\ndef get_npv(y, pred, th=0.5):\n    \"\"\"\n    Compute NPV of predictions at threshold.\n\n    Args:\n        y (np.array): ground truth, size (n_examples)\n        pred (np.array): model output, size (n_examples)\n        th (float): cutoff value for positive prediction from model\n    Returns:\n        NPV (float): negative predictive value of predictions at threshold\n    \"\"\"\n    NPV = 0.0\n    \n    ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n    \n    # get TN and FN using our previously defined functions\n    TN = get_true_neg(y,pred,th)\n    FN = get_false_neg(y,pred,th)\n\n    # use TN and FN to compute NPV\n    NPV = TN \/ (TN + FN)\n    \n    ### END CODE HERE ###\n    \n    return NPV","920dc408":"class_labels=labels","950b5049":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\ndef bootstrap_auc(y, pred, classes, bootstraps = 100, fold_size = 1000):\n    statistics = np.zeros((len(classes), bootstraps))\n\n    for c in range(len(classes)):\n        df = pd.DataFrame(columns=['y', 'pred'])\n        df.loc[:, 'y'] = y[:, c]\n        df.loc[:, 'pred'] = pred[:, c]\n        # get positive examples for stratified sampling\n        df_pos = df[df.y == 1]\n        df_neg = df[df.y == 0]\n        prevalence = len(df_pos) \/ len(df)\n        for i in range(bootstraps):\n            # stratified sampling of positive and negative examples\n            pos_sample = df_pos.sample(n = int(fold_size * prevalence), replace=True)\n            neg_sample = df_neg.sample(n = int(fold_size * (1-prevalence)), replace=True)\n\n            y_sample = np.concatenate([pos_sample.y.values, neg_sample.y.values])\n            pred_sample = np.concatenate([pos_sample.pred.values, neg_sample.pred.values])\n            score = roc_auc_score(y_sample, pred_sample)\n            statistics[c][i] = score\n    return statistics\n\nstatistics = bootstrap_auc(y, pred, class_labels)","d91379c4":"get_curve(y, pred, class_labels)","d1b984c2":"get_performance_metrics(y, pred, class_labels, acc=get_accuracy, prevalence=get_prevalence, \n                        sens=get_sensitivity, spec=get_specificity, ppv=get_ppv, npv=get_npv, auc=roc_auc_score,f1=f1_score)","694cbffc":"print_confidence_intervals(class_labels, statistics)","284bbff5":"get_curve(y, pred, class_labels, curve='prc')","f74865ce":"# LOADING THE CSV FILE WHICH CONTAIN ALL THE INFORMATION ABOUT OUR ALL THREE DATASETS IN MERGED FORM","9dabce81":"# we have loaded the finally trained model weight and commented out the training process.\n# weights are also made public","e3d2d350":"# GETTING TRAINING\/VALID\/TEST GENERATOR","13482b3c":"# FUNCTION FOR VALID AND TEST GENRATOR","2b8bb385":"# FUNCTION FOR TRAIN GENRATOR","7daf5241":"# *THE BELOW KERNEL IS USED FOR DATA PREPROCESSING WHICH INCLUDES MERGING IMAGES FROM DIFFRENT DATASETS TO A SINGLE CSV FILE AND WE ALSO NEED TO CHANGE THE DIRECTORY OF SOME IMAGES TO GERNALIZE OUR CODE.HENCE ALL THE INFORMATION ABOUT RESULTS OF THIS KERNEL IS IN FILE  **final_merged_df.csv** .WHICH IS TAKEN AS INPUT AND MAKE PUBLIC.*\n\n# SO ALL THE CODE OF BELOW KERNEL IS COMMENTED OUT FOR SAVING TIME.","0af1cb25":"# SPLITTING DATASET INTO TRAIN TEST VALID BY TAKING CARE OF DATA LEAKING","145c34ea":"# diffrent diseases classes****"}}