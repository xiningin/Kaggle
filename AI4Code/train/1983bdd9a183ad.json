{"cell_type":{"61632e3c":"code","2171ac08":"code","05a504b7":"code","ebfa3bb5":"code","4b1cf846":"code","ba87be06":"code","caa63247":"code","a5c88522":"code","dbe37327":"code","b861cfcf":"code","c42edab5":"code","677c491c":"code","3606199d":"code","e3497b0b":"code","6d5d74ad":"code","56b28eab":"code","6fde2c50":"code","c92adfea":"code","d7fbd2cd":"code","3248d9c2":"code","aac7acdf":"code","93b8c354":"code","f15b4d90":"code","e67652a3":"code","59435f2b":"code","c38858b8":"code","37b11c62":"code","bec73b83":"code","5e3bc3d4":"code","be24fe97":"code","b468ba69":"code","2cd348fe":"code","c70793f9":"code","c237e2cd":"code","82216122":"code","a85f3f4d":"code","3eb5da9e":"code","096a2234":"markdown","2c87aee2":"markdown","5e818bbf":"markdown","1f8525fa":"markdown","5682c52f":"markdown","b1475d9c":"markdown","b6c575e6":"markdown","84fcd17e":"markdown","33fb87a7":"markdown","be89c943":"markdown","233a8609":"markdown","22e38fc5":"markdown","852dccaf":"markdown","9f3089a6":"markdown","e3a38838":"markdown","18c521a3":"markdown","58b95e06":"markdown","69c78c90":"markdown","1e32b432":"markdown","92af2151":"markdown","66e47896":"markdown","e871a669":"markdown","eb8fcad6":"markdown","dc6b1290":"markdown","acae016b":"markdown","3bc98650":"markdown","216bd4d6":"markdown","5eda4dfc":"markdown","8b0db798":"markdown","2585b2bd":"markdown"},"source":{"61632e3c":"# Load packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport itertools\nimport warnings\n\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom statsmodels.tsa import stattools\nfrom statsmodels.graphics import tsaplots\nfrom statsmodels.graphics.api import qqplot\nfrom statsmodels.tsa.arima.model import ARIMA\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Bidirectional\n\nfrom timeit import default_timer as timer    \n\n\nwarnings.filterwarnings('ignore')","2171ac08":"# Load and clean the data\ncolspecs = [(3, 7), (9,11),(14,18),(22,26),(32,34),(37,42),(45,50)]\ndata = pd.read_fwf('..\/input\/heathrow-weather-data\/heathrowdata.txt',colspecs=colspecs)\ndata = data[3:].reset_index(drop=True)\ndata.columns = data.iloc[1]\ndata = data[3:].reset_index(drop=True)","05a504b7":"# Further Cleanup\ndata['Month'] = data['yyyy'] + '\/' + data['mm']\ndata['Month'] = pd.to_datetime(data['Month'])\ndata = data.set_index('Month')\ndata = data.replace('-',0)\ndata = data.replace('---',0)\ndata = data.drop(['yyyy','mm'],axis=1)\ndata[['tmax','tmin','af','rain','sun']] = data[['tmax','tmin','af','rain','sun']].astype('float')","ebfa3bb5":"data.head()","4b1cf846":"plt.figure(figsize=(32,8))\nplt.title('Temperature')\nsns.lineplot(data=data['tmax'],color='r',label='Max')\nsns.lineplot(data=data['tmin'],color='b',label='Min')","ba87be06":"pvalue = stattools.adfuller(data.tmax)[1]\nprint('ADF P-Value = {:.2f}%'.format(pvalue*100))","caa63247":"fig = plt.figure(figsize=(16,8))\nax1 = fig.add_subplot(211)\nfig = tsaplots.plot_acf(data.tmax, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = tsaplots.plot_pacf(data.tmax, ax=ax2)","a5c88522":"#Split data up until last 3 years\nts_train = data['tmax'].iloc[0:-36]\nts_test = data['tmax'].iloc[-36:]\n\nmodel = ARIMA(ts_train,order=(1,0,3)).fit()\nprint(model.summary())","dbe37327":"%%time\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\npq_AIC_output = pd.DataFrame(columns=['p','q','AIC'])\npq_maxp_output = pd.DataFrame(columns=['p','q','Max P-Value'])\n\nfor combo in itertools.product(np.arange(1,9,1),np.arange(1,9,1)):\n       \n    pq_Model = ARIMA(ts_train,order=(combo[0],0,combo[1]),enforce_stationarity=False).fit()\n    pq_AIC_output = pq_AIC_output.append({'p':combo[0],'q':combo[1],'AIC':pq_Model.aic},ignore_index=True)\n    pq_maxp_output = pq_maxp_output.append({'p':combo[0],'q':combo[1],'Max P-Value':pq_Model.pvalues.max()},ignore_index=True)","b861cfcf":"plt.figure(figsize=(10,8))\nplt.title('AIC')\nsns.heatmap(pq_AIC_output.pivot('p','q','AIC'),cmap='Blues_r',annot=True,fmt=',.0f')","c42edab5":"plt.figure(figsize=(10,8))\nplt.title('Maximum p-value for coefficients (over 5% significance results are excluded)')\nheatmap_data = pq_maxp_output.pivot('p','q','Max P-Value')\nsns.heatmap(heatmap_data,cmap='Blues_r',annot=True,fmt='.4f',mask=heatmap_data>0.05)","677c491c":"model = ARIMA(ts_train,order=(4,0,3)).fit()\nprint(model.summary())","3606199d":"resid = model.resid\nfig = plt.figure(figsize=(18,10))\nfig = qqplot(resid, line='q',fit=True)\nplt.show()","e3497b0b":"ARIMA_pred = model.predict(start=ts_test.index[0],end = pd.to_datetime('2020-10-01'))\nARIMA_pred = pd.DataFrame(ARIMA_pred,columns = ['tmax'])\nARIMA_pred.index.name = 'Month'\nARIMA_pred_data = pd.merge(ts_test,ARIMA_pred,on=['Month'])\nARIMA_pred_data.columns = ['Actual','Predicted by ARIMA']\nplt.figure(figsize=(18,6))\nsns.lineplot(data=ARIMA_pred_data,palette=\"Reds\")","6d5d74ad":"errors = {'MAE':metrics.mean_absolute_error(ts_test,ARIMA_pred),\n          'RMSE':metrics.mean_squared_error(ts_test,ARIMA_pred,squared=False)}\n\nerrors = pd.DataFrame(errors,index=['ARIMA'])\nerrors = np.round(errors,3)\nerrors","56b28eab":"print(ts_train.shape)\nprint(ts_test.shape)","6fde2c50":"rnn_ts_train = ts_train.reset_index(drop=True) # Do I need to remove the dates?\nrnn_ts_train = np.array(rnn_ts_train)\nrnn_ts_train = rnn_ts_train.reshape(ts_train.shape[0],1)","c92adfea":"Scaler = MinMaxScaler()\nrnn_ts_train_scaled = Scaler.fit_transform(rnn_ts_train)","d7fbd2cd":"# Create our data RNN np arrays\n\nRNN_X_train = []\nRNN_y_train = []\n\nn_past = 60\nn_future = 1\n\n\nfor i in range(0,len(rnn_ts_train_scaled)-n_past-n_future+1):\n    RNN_X_train.append(rnn_ts_train_scaled[i : i + n_past , 0])\n    RNN_y_train.append(rnn_ts_train_scaled[i + n_past : i + n_past + n_future , 0 ])\n\nRNN_X_train, RNN_y_train = np.array(RNN_X_train), np.array(RNN_y_train)","3248d9c2":"#R eshape to the Keras 3D tensor input shape\nRNN_X_train = np.reshape(RNN_X_train,(RNN_X_train.shape[0],RNN_X_train.shape[1],1))","aac7acdf":"RNN_X_train.shape","93b8c354":"# Initialise the RNN\nregressor = Sequential()\n\n# Add the first LSTM layer\nregressor.add(Bidirectional(LSTM(units=50,return_sequences=True,input_shape=(RNN_X_train.shape[1],1))))\nregressor.add(Dropout(0.2)) # 20% of the neurons will be ignored, for regularisation\n\n# Add more LSTM layers\nregressor.add(LSTM(units=50,return_sequences=True))\nregressor.add(Dropout(0.2))\n\nregressor.add(LSTM(units=50,return_sequences=True))\nregressor.add(Dropout(0.2)) \n\n# Add the last LSTM layer\nregressor.add(LSTM(units=50,return_sequences=False))\nregressor.add(Dropout(0.2)) \n\n#Add the output layer\nregressor.add(Dense(units=n_future,activation='linear'))","f15b4d90":"# Compile & Train\nstart = timer()\n\nregressor.compile(optimizer = 'adam', loss='mean_squared_error')\nRNN_hist = regressor.fit(RNN_X_train, RNN_y_train, epochs = 200, batch_size = 32)\n\nruntime = timer()-start\n ","e67652a3":"print(\"Model Training took {0:0.0f} seconds, with GPU\".format(runtime))","59435f2b":"sns.lineplot(data=np.array(RNN_hist.history['loss']))","c38858b8":"\"\"\"\nHere we create our custom time series for making predictions\nIt needs to be in the correct input format (3D array) and we need to mix train and test datasets\nas the first point of our test data will need the previous 60 values to make a prediction. \nBy doing it this way we can then scale each input before using the model as well.\n\"\"\"\n\ndataset_total = pd.concat((ts_train,ts_test),axis=0)\ninputs = dataset_total[len(dataset_total) - len(ts_test) - n_past - n_future +1:].values\n","37b11c62":"inputs.shape","bec73b83":"inputs = inputs.reshape(-1,1)\ninputs = Scaler.transform(inputs) # only transfrom to prevent data leakage","5e3bc3d4":"RNN_X_test = []\n\nfor i in range(0,len(inputs)-n_past-n_future+1): # we have to start our test data at 60\n    RNN_X_test.append(inputs[i : i + n_past , 0])\n\nRNN_X_test = np.array(RNN_X_test)","be24fe97":"RNN_X_test.shape","b468ba69":"# Move test set to 3D structure, as expected by the RNN\nRNN_X_test = np.reshape(RNN_X_test,(RNN_X_test.shape[0],RNN_X_test.shape[1],1))","2cd348fe":"# Predict\nRNN_pred = regressor.predict(RNN_X_test)","c70793f9":"RNN_pred = Scaler.inverse_transform(RNN_pred)","c237e2cd":"RNN_pred = pd.DataFrame(RNN_pred)[0]\nRNN_pred.index = ts_test.index","82216122":"RNN_pred_data = pd.merge(pd.DataFrame(ts_test),RNN_pred,on=['Month'])\nRNN_pred_data.columns = ['Actual','Predicted by RNN']\nplt.figure(figsize=(18,6))\nsns.lineplot(data=RNN_pred_data,palette=\"Blues\")","a85f3f4d":"plt.figure(figsize=(18,6))\nplt.suptitle(\"Modelling Heathrow Max Temperature, ARIMA vs RNN\",size=16)\nplt.subplot(211)\nsns.lineplot(data=ARIMA_pred_data,palette=\"Reds\")\nplt.subplot(212)\nsns.lineplot(data=RNN_pred_data,palette=\"Blues\")","3eb5da9e":"errors = {'MAE':[metrics.mean_absolute_error(ts_test,ARIMA_pred),\n                metrics.mean_absolute_error(ts_test,RNN_pred)],\n          'RMSE':[metrics.mean_squared_error(ts_test,ARIMA_pred,squared=False),\n                 metrics.mean_squared_error(ts_test,RNN_pred,squared=False)]}\n\nerrors = pd.DataFrame(errors,index=['ARIMA','RNN'])\nerrors = np.round(errors,3)\nerrors","096a2234":"Conclusion: it's under 1% so I'm happy.\n\nNext we look at the Autocorrelation and Partial Autocorrelation functions, to look for seasonality.\n\nSeasonality.....season..ality... this is temparature data.... hmmm, what do we think folks? ","2c87aee2":"### Contents\n\nIn this notebook I explore the methodology for fitting time series using pandas, statsmodel and tensorflow.\n\n1. ARIMA fitting (using statsmodel)\n2. Recurrent Neural Networks (RNNs)\n","5e818bbf":"![image.png](attachment:image.png)","1f8525fa":"Bit of a spoiler but here's the high level results, to save you scrolling\n\n![image.png](attachment:image.png)","5682c52f":"I'm concerned by the significance of the error in the MA(1) term above. This tells me we don't have a good fit. \n\nThere are many ways to assess the quality of a model fit. The most commonly used catch-all metric for traditional time-series modelling is the Akaike Information Criteria (AIC), where lower is better.\n\nIndsutry standard though is to ensure the coefficients p-values are all less than 1% tolerance, or better.\n\nSo a combination of the two is best. Let's cycle over a range of differnet p-q combinations.","b1475d9c":"Please leave an upvote if you enjoyed.","b6c575e6":"What does this mean for our ARIMA(p,d,q) model. Well, we can use the above to estimate p and q, by following these **general** rules:\n\n* The partial autocorrelation of an AR(p) process becomes zero at lag p + 1 and greater\n* The autocorrelation function of a MA(q) process becomes zero at lag q + 1 and greater\n\nSo our PACF flips around zero for at least 15 or so terms, but for simplicity (and hence prevent overfitting) we can choose a value where out PACF changes sign. In our case this is at t=2, so p should take the value of 1.\n\nOur ACF never really becomes zero, but again we can take t=4 as our first occassion of changing sign and so q should take the value of 3.\n\nNote, I say general rules. It pays to try different ranges of p and q in the hunt for the best AIC.\n\nThe only other parameter to worry about is d, which is our differencing term, we would need this if our data wasn't stationary. Not a problem here.\n\nLet's train-test split at 80% of the time since first observation passed and fit our initial ARIMA(1,0,3) model.","84fcd17e":"Seems like a good fit. Let's check the QQ plot just in case.","33fb87a7":"Some variance at the extremes, hard to catch those with ARIMA.\n\nLet's now make our predicitions and plot against our test set.","be89c943":"I think it's clear from our graphical forecast as well as the error metrics that **the RNN is able to outperform traditional ARIMA modelling**.\n\nIt should be noted that the RNN worked particularly well here because the underlying data is particularly predictable. I think what's most interesting is that the ARIMA model struggled to capture the extremes in its forecast, but the RNN handled this challenge with a bit more finesse, the RNN wasn't bound to a regular periodicity, it just used it's memory and made subtle adjustments to improve the forecast.\n\nThanks for reading.\n\n**Jon**","233a8609":"I will be using the weather data for Heathrow Airport. There are a few predicatble time series in there but for now I will be using maximum temperature, as it has the least complexity.","22e38fc5":"It should be noted in the above that each block (x1,m1,o1 etc) is actually an entire neural network. So the diagram above is as if we're looking from above, when compared to the standard nerual network diagram.","852dccaf":"## 3. Fit a Recurrent Neural Network (RNN)\n\nNeural Networks are very tricky things. Here we are fitting a recurrent neural network. The main difference between an RNN and a regular nerual network is the flow of the inputs to outpus. The key difference being the output is fed into the next point in the time series. So $x_n$ is a result of not only the layers $h_n$ but also the output $o_{n-1}$ of the previous time step.\n\nA diagram may help:","9f3089a6":"Seems OK, though we're missing those extreme values as we know from the QQ plot.","e3a38838":"The first challenge for modelling with neural networks is preparing the data in a specific format so that Tensorflow\/Keras can understand it. Another diagram will help here.\n\n![](http:\/\/)![image.png](https:\/\/humboldt-wi.github.io\/blog\/img\/seminar\/lstm_gru_1819\/Format1.png)\n\n","18c521a3":"# Primer for Time Series Modelling in Python","58b95e06":"OK, now we understand and appreciate the complexity of the libraries we are using, lets use them.","69c78c90":"Can we tolerate a root mean squared error of 2.3 degC, I can't. Let's see if we can do better with RNNs","1e32b432":"The model we are choosing is an Long Short-Term Memory (LSTM) network, which is a type of RNN and provides a solution to a problem in RNNs known as the **Vanishing Gradient Problem**. Here is a diagram of the sort of data flow that occurs in the LSTM model, I recommend further reading of this amaziong blog.\n\nsource: [Shi Yan on medium.com](https:\/\/medium.com\/mlreview\/understanding-lstm-and-its-diagrams-37e2f46f1714)\n\n![image.png](attachment:image.png)","92af2151":"The presense of the cycle in both plot tells me that we have seasonilty going on. No surprises there. The values don't decay for the AC plot so we would normally say the series is non-stationary (ie there is a trend). Sure there's the climate change point, but the effect with our data here is extremely minimal, as evidenced by the <1% for our ADF test. So I would say the model is stationary, but there is some seasonality.","66e47896":"Let's look at our temperatures:","e871a669":"The hardest thing to get your head around is that each training sequence (of length 4 in the above diagram), uses the next y train value. So the second row contains the previous row's y train value. The Keras RNNs expect the data to be in this format and the algorithms know how to use it appropriately, data leakage is on my mind, but I trust Keras to handle these matters appropriately.","eb8fcad6":"## 2. Fit ARIMA model with Statsmodels","dc6b1290":"image source: www.towardsdatascience.com","acae016b":"Peek at the data:","3bc98650":"Fitting time series models following the Box-Jenkins methodology.\n\nFirst we look to see if our time series (tmax) is stationary using the Augmented Dickey-Fuller test","216bd4d6":"And comparing side by side:","5eda4dfc":"Hilarious, our rule of thumb couldn't have picked a worse initial model. Evidently, optimising hyperparemeters is not an optional pursuit. From the above, a model of order ARIMA(4,0,3) seems best. It's tempting to add more terms and go even higher with p and q, espeically given the information we have from the ACF and PACF plots, but overcomplicating and overfitting our models leads to madness, plus what we will find is that our covariates will not have significance, i.e. our P>|z| values will be over 5%. This is the main reason we do this modelling with statsmodels rather than the sklearn packages. Sklearn doesn't really offer covariate significance as standard. ","8b0db798":"## 1. Data Preparation & Exporation","2585b2bd":"![image.png](attachment:image.png)"}}