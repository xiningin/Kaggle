{"cell_type":{"90367803":"code","7317ea3c":"code","d3c9437b":"code","e044dbfa":"code","3e83a13a":"code","c3b41a0e":"code","487bfdbe":"code","dbc9bc6e":"code","aee7b5f8":"code","bbebbc99":"code","2bdbc227":"code","dccc90a8":"code","fa5fd162":"markdown","fc439283":"markdown"},"source":{"90367803":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, Dropout, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras import callbacks\nfrom keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import train_test_split\n\n\n#crawl-300d-2M.vec--> https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n#When pre-train embedding is helpful? https:\/\/www.aclweb.org\/anthology\/N18-2084\n#There are many pretrained word embedding models: \n#fasttext, GloVe, Word2Vec, etc\n#crawl-300d-2M.vec is trained from Common Crawl (a website that collects almost everything)\n#it has 2 million words. Each word is represent by a vector of 300 dimensions.\n\n#https:\/\/nlp.stanford.edu\/projects\/glove\/\n#GloVe is similar to crawl-300d-2M.vec. Probably, they use different algorithms.\n#glove.840B.300d.zip: Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n#tokens mean words. It has 2.2M different words and 840B (likely duplicated) words in total\n\n#note that these two pre-trained models give 300d vectors.\nEMBEDDING_FILES = [\n    '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec',\n    '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n]\n\nNUM_MODELS = 2\nBATCH_SIZE = 512\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nEPOCHS = 4 \nMAX_LEN = 200\nMAX_FEATURES = 120000\nIDENTITY_COLUMNS = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n]\nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\nTEXT_COLUMN = 'comment_text'\nTARGET_COLUMN = 'target'\nCHARS_TO_REMOVE = '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014'","7317ea3c":"#functions to build our embedding matrix\ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    with open(path) as f:\n        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            pass\n    return embedding_matrix","d3c9437b":"#importing the data\ntrain_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest_df = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_train = train_df[TEXT_COLUMN].astype(str)\ny_aux_train = train_df[AUX_COLUMNS].values\ny_train = train_df[TARGET_COLUMN].values\nx_test = test_df[TEXT_COLUMN].astype(str)","e044dbfa":"#tokenizing the corpus, limiting the tokenizer to 120000 words\nfor column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n\ntokenizer = text.Tokenizer(num_words=MAX_FEATURES, filters=CHARS_TO_REMOVE)\ntokenizer.fit_on_texts(list(x_train) + list(x_test))\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)","3e83a13a":"#deciding the value MAX_LEN\ntotalNumWords = [len(one_comment) for one_comment in x_train]\nplt.hist(totalNumWords)\nplt.show()","c3b41a0e":"#making sure that every sentence is of equal length by adding padding\nx_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\nx_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","487bfdbe":"#building the embedding matrix\nembedding_matrix = np.concatenate(\n    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","dbc9bc6e":"#function to build the model\ndef build_model(embedding_matrix, num_aux_targets):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n\n    hidden = concatenate([\n        GlobalMaxPooling1D()(x),\n        GlobalAveragePooling1D()(x),\n    ])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=words, outputs=[result, aux_result])\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    return model","aee7b5f8":"checkpoint_predictions = []","bbebbc99":"#fitting model on whole training data\nfor model_idx in range(NUM_MODELS):\n    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            x_train,\n            [y_train, y_aux_train],\n            batch_size=BATCH_SIZE,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())","2bdbc227":"predictions = np.average(checkpoint_predictions, axis=0)","dccc90a8":"submission = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\nsubmission['prediction'] = predictions\nsubmission.to_csv('submission.csv', index=False)","fa5fd162":"**Since the length of sentences gradually goes down and attains a max length at around 220-230 we will use a MAX_LEN of 200**","fc439283":"Shoutout to thousandvoices for the kernel. You can check it out over here -> https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm\/code"}}