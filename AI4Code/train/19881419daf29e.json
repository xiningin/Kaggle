{"cell_type":{"223fb967":"code","dc621a59":"code","9a1a2382":"code","03103c3c":"code","2c6a2348":"code","4f1ad373":"code","03b06dac":"code","f9a5373f":"code","7b6820a6":"code","4b1b4dfc":"code","c4aa814c":"code","c02e8151":"code","afc4168b":"code","093b9e8d":"code","cda70d17":"code","93fe3c12":"code","a3fe54c9":"code","78fbd561":"code","9d193e01":"code","701cc214":"code","f4afe902":"code","c1965aa7":"code","88578e89":"code","30ee3979":"code","e3b19300":"code","19510abe":"code","e9ac283e":"code","37a342ca":"code","c4360d59":"code","ffe6bd18":"code","97913768":"code","e60e3cba":"code","b44d9644":"code","494d997c":"code","cfc6f1b6":"code","dd703717":"code","39d14857":"code","cf397305":"code","c3382b3c":"code","b5680e35":"code","92f1c6ae":"code","850a2ab1":"code","cee2ea53":"code","6f91358a":"code","165e9263":"code","45fa18c3":"code","fa5e5247":"code","40122786":"code","c1391706":"code","7ac09654":"code","d9510c66":"code","ffd8a34f":"code","12ac2d69":"code","2b0a1123":"code","6d425c9d":"code","c96d4b59":"code","bab2d8e1":"code","0d7806fc":"code","d3f9db6a":"code","bb0d66bc":"code","e73dbd32":"code","3f32461c":"code","f3f11194":"code","c03d6334":"code","b1307a33":"code","efd998e7":"code","afeb68f0":"code","9e461e64":"code","52523320":"code","6724b84c":"code","b6843931":"markdown","3a45b765":"markdown","03d6ffae":"markdown","312bc977":"markdown","9bb21acd":"markdown","4498cfaf":"markdown","1312e23c":"markdown","91e6cdc5":"markdown","a5ebc790":"markdown","c3557412":"markdown","9f4ed406":"markdown","a1b6486b":"markdown","9879308b":"markdown","710662e5":"markdown","83c9901f":"markdown","1cb74c4d":"markdown","ac66b924":"markdown","74213d1e":"markdown","3c88e8fd":"markdown","a37aaf8f":"markdown","29c9acfe":"markdown","346793bf":"markdown","110dbf59":"markdown","02e9e016":"markdown","ef31011e":"markdown","11d6d5f6":"markdown","243d5844":"markdown","d5105935":"markdown","7739d20e":"markdown","ad5956fb":"markdown","793f749a":"markdown","df11cb12":"markdown","afaa49ce":"markdown","46d91485":"markdown","5ee3d0fc":"markdown","02c46300":"markdown","67a92c44":"markdown","2d17b7ad":"markdown","0b210d6e":"markdown","a50810da":"markdown","16289a62":"markdown","7493669f":"markdown","454a4035":"markdown","18aa66f2":"markdown","1445c79f":"markdown","5e0bc1f1":"markdown","f15b70ce":"markdown","abbbc8c5":"markdown","3589d22f":"markdown","144a360a":"markdown","97e29e57":"markdown","401deacf":"markdown","706bc3ab":"markdown","603fd385":"markdown","7d1567a6":"markdown"},"source":{"223fb967":"import pandas as pd\n# read data\napplication = pd.read_csv(\"..\/input\/credit-card-approval-prediction\/application_record.csv\")","dc621a59":"application.info()","9a1a2382":"application.head()","03103c3c":"application.describe()","2c6a2348":"credit_status=pd.read_csv(\"..\/input\/credit-card-approval-prediction\/credit_record.csv\")","4f1ad373":"credit_status.columns","03b06dac":"out_df =credit_status.groupby(['MONTHS_BALANCE']).agg(\n    # Counts\n    counts=('MONTHS_BALANCE', lambda x: len(x) ),\n    # Percent\n    percent = ('MONTHS_BALANCE', lambda x: (len(x)*100\/ len(credit_status['MONTHS_BALANCE'])))\n    ).reset_index()","f9a5373f":"pd.pivot_table(out_df,index=['MONTHS_BALANCE'])","7b6820a6":"# rows in Application Data\nlen(application)","4b1b4dfc":"# rows in credit status\nlen(credit_status)","c4aa814c":"app_credit_status= pd.merge(application,credit_status, how='inner', on='ID')\nlen(app_credit_status)","c02e8151":"# Earliest Month\ncredit_card_first_month =credit_status.groupby(['ID']).agg(\n   start_month=  ('MONTHS_BALANCE', min)\n    ).reset_index()","afc4168b":"credit_card_first_month.head()","093b9e8d":"import datetime\ncredit_card_first_month['account_open_month']= datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\")\ncredit_card_first_month['account_open_month']= credit_card_first_month['account_open_month'] + credit_card_first_month['start_month'].values.astype(\"timedelta64[M]\")\ncredit_card_first_month['account_open_month']=credit_card_first_month['account_open_month'].dt.strftime('%b-%Y')","cda70d17":"credit_card_first_month.head()","93fe3c12":"# join the table\ncredit_start_status = pd.merge(credit_card_first_month, credit_status, how='left', on=['ID'])\n\ncredit_start_status['start_month']=abs(credit_start_status['start_month'])+credit_start_status['MONTHS_BALANCE']","a3fe54c9":"credit_start_status.head()","78fbd561":"accounts_counts =pd.DataFrame({'start_month':credit_start_status.groupby('start_month')['start_month'].count()})\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(accounts_counts.index,accounts_counts['start_month'])\nplt.show()","9d193e01":"month_status_counts = credit_start_status.groupby(['start_month', 'STATUS']).size().reset_index(name='counts')\nmonth_counts = credit_start_status.groupby(['start_month']).size().reset_index(name='month_counts')\n# join the table\nmonth_status_pct = pd.merge(month_status_counts, month_counts, how='left', on=['start_month'])\nmonth_status_pct['status_pct']=month_status_pct['counts']\/month_status_pct['month_counts']*100\nmonth_status_pct= month_status_pct.loc[:,['start_month','STATUS','status_pct']]\n","701cc214":"# Restucture\nmonth_status_pct1 = month_status_pct.pivot(index='start_month', columns='STATUS', values='status_pct')\n# Fill with 0\nmonth_status_pct1=month_status_pct1.fillna(0).reset_index()","f4afe902":"import matplotlib.pyplot as pt\npt.plot(month_status_pct1.index, month_status_pct1['4']+month_status_pct1['5'],\n     color='green', \n     linestyle='solid',\n     linewidth=2, \n     markersize=12)\npt.xlabel('Months Since Opened')\npt.ylabel('% Bad Rate')","c1965aa7":"month_status_pct2 = month_status_pct1.loc[month_status_pct1.index<=50]\n# drop column start_month\nmonth_status_pct2=month_status_pct2.drop('start_month', axis=1)","88578e89":"import matplotlib.pyplot as plot\nmonth_status_pct2.plot.area(stacked=True);\nplot.show(block=True);","30ee3979":"import matplotlib.pyplot as pt\npt.plot(month_status_pct2.index, month_status_pct2['4']+month_status_pct2['5'],\n     color='green', \n     linestyle='solid',\n     linewidth=2, \n     markersize=12)","e3b19300":"import warnings\nwarnings.filterwarnings(\"ignore\")\ncredit_start_status.groupby('STATUS')['STATUS'].count()\n# We will remove rows with status as C and X\ncredit_start_status1 = credit_start_status.loc[ (credit_status['STATUS'] !='X') & (credit_status['STATUS'] !='C') ,:]\n# Change status as numeric\ncredit_start_status1['status'] = credit_start_status1['STATUS'].astype('int64', copy=False)\ncredit_start_status1 = credit_start_status1.loc[credit_start_status1['start_month']<=18,['ID','start_month','status']]","19510abe":"credit_start_status1","e9ac283e":"# Find Max Status Values\nstatus = credit_start_status1.groupby(['ID']).agg(\n    # Max Status\n    max_status=('status','max')\n\n    ).reset_index()\n# Validate\nstatus.groupby('max_status')['max_status'].count()","37a342ca":"import numpy as np\n# Define \nstatus['label']=np.where(status['max_status']>=4, 1,0)\n# Validate\nstatus.groupby('label')['label'].count()","c4360d59":"# Bad Rate \nstatus.groupby('label')['label'].count()*100\/len(status['label'])\n","ffe6bd18":"# All with label 1\nlabel_1 = status.loc[status['label']==1,:]\n# All with label 0\nlabel_0 = status.loc[status['label']==0,:]\n# Select randomly few rows\nlabel_0_biased=label_0.sample(n = 1701) \n# Combined Sample IDs with Biased Sampling\n\nframes = [label_1,label_0_biased]\nimport pandas as pd\nlabels_biased = pd.concat(frames)\n\n# Keep only ID and Label Columns\n\nlabels_biased=labels_biased.loc[:,['ID','label']]","97913768":"labels_biased","e60e3cba":"# Combine Labels and Application Data\nmodel_df = pd.merge (labels_biased, application,how='inner',on=['ID'] )\nlen(model_df)","b44d9644":"model_df.tail()","494d997c":"model_df.groupby('label')['label'].count()*100\/len(model_df['label'])","cfc6f1b6":"model_df.info()","dd703717":"model_df.describe()","39d14857":"# Check if missing values\ndef missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns\n# source: https:\/\/stackoverflow.com\/questions\/26266362\/how-to-count-the-nan-values-in-a-column-in-pandas-dataframe\n\nmissing_values_table(model_df)","cf397305":"# Find Continuous and Categorical Features\ndef featureType(df):\n    import numpy as np \n    from pandas.api.types import is_numeric_dtype\n\n    columns = df.columns\n    rows= len(df)\n    colTypeBase=[]\n    colType=[]\n    for col in columns:\n        try:\n            try:\n                uniq=len(np.unique(df[col]))\n            except:\n                 uniq=len(df.groupby(col)[col].count())\n            if rows>10:\n                if is_numeric_dtype(df[col]):\n                    \n                    if uniq==1:\n                        colType.append('Unary')\n                        colTypeBase.append('Unary')\n                    elif uniq==2:\n                        colType.append('Binary')\n                        colTypeBase.append('Binary')\n                    elif rows\/uniq>3 and uniq>5:\n                        colType.append('Continuous')\n                        colTypeBase.append('Continuous')\n                    else:\n                        colType.append('Continuous-Ordinal')\n                        colTypeBase.append('Ordinal')\n                else:\n                    if uniq==1:\n                        colType.append('Unary')\n                        colTypeBase.append('Category-Unary')\n                    elif uniq==2:\n                        colType.append('Binary')\n                        colTypeBase.append('Category-Binary')\n                    else:\n                        colType.append('Categorical-Nominal')\n                        colTypeBase.append('Nominal')\n            else:\n                if is_numeric_dtype(df[col]):\n                    colType.append('Numeric')\n                    colTypeBase.append('Numeric')\n                else:\n                    colType.append('Non-numeric')\n                    colTypeBase.append('Non-numeric')\n        except:\n            colType.append('Issue')\n                \n    # Create dataframe    \n    df_out =pd.DataFrame({'Feature':columns,\n                          'BaseFeatureType':colTypeBase,\n                        'AnalysisFeatureType':colType})\n    return df_out\n\nfeatureType(model_df)  ","c3382b3c":"from datetime import timedelta\n\nmodel_df['BIRTH_DATE'] = datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\") + model_df['DAYS_BIRTH'].apply(pd.offsets.Day)\n","b5680e35":"# DAYS_EMPLOYED: Count backwards from current day(0). If positive, it means the person currently unemployed.\n# Update DAYS_EMPLOYED greater than 0 to 31\nmodel_df.loc[model_df.DAYS_EMPLOYED >0, \"DAYS_EMPLOYED\"] = 31\nmodel_df['EMPLOYMENT_START_DATE'] = datetime.datetime.strptime(\"2020-01-01\", \"%Y-%m-%d\") + model_df['DAYS_EMPLOYED'].apply(pd.offsets.Day)","92f1c6ae":"model_df.head()","850a2ab1":"model_df = pd.merge (model_df, credit_card_first_month.loc[:,['ID','account_open_month']],how='inner',on=['ID'] )\nlen(model_df)","cee2ea53":"# Age in months\n\nmodel_df['age_months'] = ((pd.to_datetime(model_df['account_open_month'],format='%b-%Y')  - model_df.BIRTH_DATE)\/np.timedelta64(1, 'M'))\nmodel_df['age_months'] = model_df['age_months'].astype(int)\n# Experience\/Employment in Months\nmodel_df['employment_months'] = ((pd.to_datetime(model_df['account_open_month'],format='%b-%Y')  - model_df.EMPLOYMENT_START_DATE)\/np.timedelta64(1, 'M'))\nmodel_df['employment_months'] = model_df['employment_months'].astype(int)","6f91358a":"model_df.head()","165e9263":"model_df.loc[model_df.employment_months <0, \"employment_months\"] = -1","45fa18c3":"model_df.head()","fa5e5247":"model_df=model_df.drop(['BIRTH_DATE','EMPLOYMENT_START_DATE','account_open_month','DAYS_BIRTH','DAYS_EMPLOYED','FLAG_MOBIL'], axis=1)","40122786":"featureType(model_df)","c1391706":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nincome_type = model_df.groupby(['NAME_INCOME_TYPE','label'])['NAME_INCOME_TYPE','label'].size().reset_index(name='counts')\n\n# Restucture\nincome_type = income_type.pivot(index='NAME_INCOME_TYPE', columns='label', values='counts')\n# Fill with 0\nincome_type=income_type.fillna(0).reset_index()\n# Rename the columns\nincome_type.columns=['Income_Type','Label_0','Label_1']\n\n# Calculate Bad Rate for each of the income type\nincome_type['pct_obs'] = (income_type['Label_0']+income_type['Label_1'])\/(sum(income_type['Label_0'])+sum(income_type['Label_1']))\nincome_type['pct_label_0']= income_type['Label_0']\/(income_type['Label_0']+income_type['Label_1'])\nincome_type['pct_label_1']= income_type['Label_1']\/(income_type['Label_0']+income_type['Label_1'])\nprint(income_type)","7ac09654":"# change missing value for OCCUPATION_TYPE\nmodel_df.loc[model_df.OCCUPATION_TYPE=='', \"OCCUPATION_TYPE\"] = \"NA\"","d9510c66":"# One hot Encoding using get_dummies function\nmodel_df2=pd.get_dummies(model_df, columns=['CODE_GENDER','FLAG_OWN_CAR','FLAG_OWN_REALTY',\"NAME_INCOME_TYPE\",'NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE','OCCUPATION_TYPE'])","ffd8a34f":"len(model_df2)","12ac2d69":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np","2b0a1123":"model_df2.columns","6d425c9d":"# Features - exclude ID and Label columns\nfeatures = model_df2.iloc[:,2:]\n# Label - select only label column\nlabel = model_df2.iloc[:,1]\n","c96d4b59":"model_df2","bab2d8e1":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, label_train, label_test = train_test_split(features, label, test_size=0.2, random_state=557)","0d7806fc":"creditxgb = xgb.XGBClassifier(\n learning_rate =0.1,\n n_estimators=100,\n max_depth=10,\n min_child_weight=50,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic'\n)\n","d3f9db6a":"creditxgb.fit(features_train,label_train)","bb0d66bc":"pred_prob = creditxgb.predict_proba(features_train)\n\n# \npred_prob_1 = pred_prob[:,1]\n# find cut off Prod to define class label 1-0.11 (11% Bad)\nnp.quantile(pred_prob_1,0.88)\npred_class = np.where(pred_prob_1>=0.18,1,0)\n\n","e73dbd32":"xgb.plot_importance(creditxgb)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","3f32461c":"accuracy = accuracy_score(label_train, pred_class)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nfrom sklearn import metrics\ncnf_matrix_dev = metrics.confusion_matrix(label_train, pred_class)\nprint(cnf_matrix_dev)\n","f3f11194":"# ROC Curve \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n# https:\/\/stackabuse.com\/understanding-roc-curves-with-python\/\nlabel_train\nfpr_dev, tpr_dev, thresholds_dev =roc_curve(label_train,pred_prob_1)\nplot_roc_curve(fpr_dev, tpr_dev)\n","c03d6334":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nparams = {\n    'learning_rate': [0.05,0.1,0.15,0.2,0.25,0.3],\n    'max_depth':[5,10,15],\n    'min_child_weight':[6,8,10,12],\n    'subsample': [0.6,0.7,0.8,0.9], \n    'colsample_bytree':[0.6,0.7,0.8],\n    'gamma':[i\/10.0 for i in range(0,5)]\n \n}\n# Grid Search\ngsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=354), \n param_grid = params, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n#Fit\ngsearch.fit(features_train,label_train)\n\n","b1307a33":" gsearch.best_params_, gsearch.best_score_","efd998e7":"Opticreditxgb = xgb.XGBClassifier(\n learning_rate =0.3,\n n_estimators=100,\n max_depth=15,\n min_child_weight=6,\n gamma=0.0,\n subsample=0.9,\n colsample_bytree=0.6,\n objective= 'binary:logistic'\n)\n\nOpticreditxgb.fit(features_train,label_train)\n","afeb68f0":"pred_prob = Opticreditxgb.predict_proba(features_train)\n\n# \npred_prob_1 = pred_prob[:,1]\n# find cut off Prod to define class label 1-0.11 (11% Bad)\nnp.quantile(pred_prob_1,0.88)\n\npred_class = np.where(pred_prob_1>=0.33,1,0)","9e461e64":"accuracy = accuracy_score(label_train, pred_class)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\nfrom sklearn import metrics\ncnf_matrix_dev = metrics.confusion_matrix(label_train, pred_class)\nprint(cnf_matrix_dev)\n","52523320":"# Score the testing sample\npred_prob_testing = Opticreditxgb.predict_proba(features_test)\n\npred_prob_test_1 = pred_prob_testing[:,1]\n\nnp.quantile(pred_prob_test_1,0.88)\n\npred_class_testing = np.where(pred_prob_test_1>=0.33,1,0)","6724b84c":"accuracy_test = accuracy_score(label_test, pred_class_testing)\nprint(\"Accuracy: %.2f%%\" % (accuracy_test * 100.0))\nfrom sklearn import metrics\ncnf_matrix_test = metrics.confusion_matrix(label_test, pred_class_testing)\nprint(cnf_matrix_test)","b6843931":"To calculate the year of employment and age as of application date, we need the account opening month. We had already created a column in another data frame. We can join that table.","3a45b765":"### Overiew\n\n","03d6ffae":"## Credit Card Approval Model using XGBoost\n\nWe need to install required packages and also make data ready to train the model using XGBoost function. \n\n### Install Packages","312bc977":"### 1. Application\n\nFor a credit card, the customers fill-up the form - online or a physical. The application information is used for assessing the creditworthiness of the customer. In addition to the application information, the Credit Bureau Score e.g. FICO Score in the US, CIBIL Score in India, and other internal information about the applicants are used for the decision.\n\nAlso, gradually the banks are considering a lot of external data to improve the quality of credit decisions.\n\nNow, we will read and explore the application sample data file provided.","9bb21acd":"Account 5001711 has been opened in Oct-2019 and the account 5001715 was opened in Jan-2015. We need to add the start month column to the credit status (credit_status) table. ","4498cfaf":"Let's check if any of the variables have missing values present.","1312e23c":"Now, we are all set to build out Credit Scoring Model. We will use XGBoost method.","91e6cdc5":"In a bank, once a credit card is approved, the account is tracked over a period to monitor the performance of an account and also the overall portfolio health.\n\nFor building an application scorecard, we need a label variable that takes values - Good or Bad- for each of the accounts based on the credit status of the accounts at the end of a period. Then, using available features at the time of application, the modeling technique helps us differentiate between the profiles of the Good and Bad segments.\n\nFor this analysis, the accounts which are overdue for a period of over 120 days (status 4 and 5) are considered as **Bad** and others as **Good**.\n\nThe bad rate for a period is calculated as % of overall accounts which are Bad Accounts.\n\nCohort or vintage analysis helps in tracking Bad Rates (% of accounts bad for that month) across months from the acquired month. This helps us in finding out the periods (# of months between account open month and status month) to be considered for defining the target variable. This period or window is called the **performance window**. For example, a scenario is depicted in the below diagram. The status is checked at the end of 6 months to define the target variable.\n\n![image.png](attachment:image.png)\n\nThe current data file does not have credit card open date. Based on MONTHS_BALANCE, we will find the start month for each of the applicant. Then rearrange the data so that the status is available for month 0 (start month), month 1 (month 1 from the start) and so on.\n\nWe assume for the data the earliest **MONTHS_BALANCE** is the start month for an account. So, we aim to change the as per the below diagram\n ","a5ebc790":"Data Dictionary: Meaning of variables and their values. \n\n![image.png](attachment:image.png)","c3557412":"#### Fitting XGBoost Model","9f4ed406":"Now, with the account open month and date of birth, we can find age as of application date. Similarly, months of experience as of application date.","a1b6486b":"#### Model Evaluation","9879308b":"We may want to see the accounts by the **MONTHS_BALANCE**. Ideally, it would have been useful to get the application date or month. And the status value for each month post credit card open month. So, the credit behavior of the applicants across the application months can be compared.\n\n\n","710662e5":"Credit Card department in a bank is a leading data science adopter. Acquiring new credit card users is always a key priority for the bank. Giving credit cards without due diligence or assessment for creditworthiness is a huge risk.\n\nFrom the last many decades, the credit card department is using a data-driven credit assessment methodology called Credit Scoring, and the model is called application scorecard. The application scorecard helps in the calculating level of risk associated with an applicant and based on strategic priority at a time, they decide the cut off value of the score for approving or rejecting a credit card application.\n\nMostly, Logistic Regression is used as modeling techniques in credit scorecard development as the banks are required to provide justification if a customer demands justification around the decision.\n\nThough, in this blog, we will use the XGBoost method for predicting credit card approval decisions. The defining target variable, data preparation, exploratory data analysis (EDA), Model Building using XGBoost, Fine Tuning Parameters, and Validating Model are some of the key steps followed for developing Credit Card Approval Model.\n","83c9901f":"Now, we have to combine application-level features to prepare model development data sample.","1cb74c4d":"### Variable Importance\n\nWe can view the set of variables coming important in the classifier.","ac66b924":"There is a significant % drop in the accuracy level. We may require large set of data points.","74213d1e":"We assume that the data were extracted as of 1-Jan-2020 and we are working on finding the calendar start month for each of the accounts. It may be useful to have the calendar account open date for a few analyses.\n","3c88e8fd":"**Observation**: There is a drop in the number of observations. We expected 1890 observations. This indicates that is a data issue between Application and Credit Status files shared.","a37aaf8f":"Only Occupation Type is missing for 32% of the applicants. We can treat this as a separate class. Now we would want to see bivariate analysis - the analysis between Label variable and each of the feature variables. Based on the analytical type of the feature variables, the analysis may be different. So, we can find an analytical type of feature first. We have written a function to find the analytical type of variables.","29c9acfe":"#### Predict using Optimized model","346793bf":"The data is highly unbalanced - with a bad rate of 0.47%. We can create a biased sample. Taking all observations of Label 1 but small % of observations from label 0. We may want to improve the bad rate to say 10%. So, in the final sample, we will 189 for label 1 and 1701 for label 0.\n\nNow we want to select randomly 1701 observations from 39562.","110dbf59":"We may want to validate the data read. Row count and feature\/variable type are important for validating the data. Also, it is typically useful to view the first\/last few rows. We can use info() and head()\/tail() methods.","02e9e016":"We need to improve performance of the model using tuning some of the parameters.","ef31011e":"Great improvement in the accuracy of the model using optimized parameters. Now, we need to evaluate the model on the test sample and in real life, the out of time sample to be confident on the model performance and generalization.","11d6d5f6":"#### XGBoost Model Definition","243d5844":"### Model Evaluation on Testing Sample","d5105935":"### 2. Credit Status\n\nOnce a credit card is issued, the customer uses it for shopping items of its use, a statement is generated to make a payment toward the dues by due date and the customer makes payment. This is a typical credit card cycle.\n\nIf a customer is not able to make a payment for the minimum due amount, the customer is considered past due for that month.\nIf the non-payment is continued for a period, the customer is considered as a defaulter and the due amount is written off & becomes bad debt. Of course, there is a lot of effort and steps the bank does to recover the due amount and this falls under the collection process.\n\nWith the modeling process, the aim to learn about the customers who were not able to pay back the dues and not to approve applications of the customers who look similar to these.\nOf course, we do not know the applications that were rejected and how many of those were actually good customers. This is not in the scope of this blog.\n\nFor this exercise, the credit status file is given. In this file, the status value is given for each of the applications post approved.","7739d20e":"Bad Rate jump significantly for the accounts which are opened for over 50 months. There are the accounts that were opened for the initial days of the operations. It may not be a bad idea to exclude these accounts.","ad5956fb":"The bad rate is almost settled after 18 months from the start, we may decide to consider it as a performance window. Any of the accounts which become bad in the first 18 months, will be termed as **Bad** and rest as **Good**.\n\nThere may be a difference in performance - Bad rate % by acquisition month. But we are not it exploring that further.\nBased on status 4 and 5 in the first 18 months, we will term as **Bad** and otherwise **Good**.\n\nWe will select start months less than 18 (so only the first 18 months are considered) and find max status for each of the credit card account. If the status is 4 or 5, we can call **Bad** otherwise **Good**.","793f749a":"### Prepare Data\n","df11cb12":"#### Predict using Fitted Model\n\n**predict** function gives predicted class. Since this sample is not a balanced sample, predict function is not useful. **predict_proba** can give probabilities for each class of label variables and observation.\n\nNow, we need to use a probability of class=1 and then change the probability values to predict the class.","afaa49ce":"**Analysis observations**\n- State servent segment constitutes 8% of the sample and has a lower bad rate - 8% compared to 11% at an overall level. \n- Commercial associate and Pensioner income type segments have similar a bad rate and can be combined into one segment.\n\nA similar analysis can be done for each of the other nominal variables. For simplicity, We can create encoding for each of the nominal variable values. We will create one hot encoding. ","46d91485":"For each of the applicants - a row is available in the **Application** table. The credit status information for each of these applicants is available in the **credit_status** table. ID variable is the joining key between these two tables.\n\nWe joined these two tables using ID Variable, there is a drop in the number of rows from 1,048,575 to 777,715. This can only happen if ID variable values are not available in the Application table.\n\nSo, there is some gap in the data or our understanding of the data.","5ee3d0fc":"**ID**: The joining key between application data and credit status data\n\n**MONTHS_BALANCE**:  The month of the extracted data is the starting point with 0 is the current month, -1 is the previous month, and so on\n\n**STATUS**: Status of the credit card account. \n- 0: 1-29 days past due \n- 1: 30-59 days past due \n- 2: 60-89 days overdue \n- 3: 90-119 days overdue \n- 4: 120-149 days overdue \n- 5: Overdue or bad debts, write-offs for more than 150 days \n- C: paid off that month \n- X: No loan for the month\n","02c46300":"### Model Parameter Tuning\n\nConsidering, we have a relatively small size of the data and features, we are setting a high number of parameters for tuning. If data is large, it may take a bit of time to get the results.","67a92c44":"All the date variables are not useful for analysis, better to be removed from this table. Also, FLAG_MOBIL as it takes only one value.","2d17b7ad":"#### Summary Statistics\n\nSummary statistics for each of the numeric features gives insights around the feature value distribution. ","0b210d6e":"We want to calculate % Bad Rate for the overall portfolio - across all the account open months. This will help us find the period by which the overall bad rate is stable.\n\nAnother important observation is that only a small volume of credit card accounts was opened in the early months. These accounts may not be relevant to consider for the modeling. We can check the Bad Rate distribution for these accounts.","a50810da":"# Credit Card Approval Model using XGBoost","16289a62":"### XGBoost's hyperparameters\n\nFor a detailed overview on the XGBoost technique, you can read a lot of interesting blogs online.  An introduction to hyperparameters is given here. Decision Tree is used as a weak learner. List of Decision Tree algorithm parameters that can be considered for fine-tuning is explained in the earlier blog. You read the earlier blog on Decision Tree classifier for the details on <a href=\"http:\/\/ramgopalprajapat.com\/posts\/tree-based-classifier-using-sklearn-and-python\/\"> hyperparameters for a Decision Tree.<\/a>\n\n**XGBoost Parameters**\n\n- eta : Learning Rate or Shrinkage in XGBoost.  The lower value of etc leads to longer computation time and takes more steps to reach the optimum.\n- max_depth : Depth of Decision Tree (when Decision Tree based classifier is used)\n- min_child_weight : Minimum weight of a leaf node to be considered for a further split. If each of the rows has the same weight, it is equivalent to the minimum number of observations for Decision Tree to split a node. Lower value leads to more leaf nodes and variance is low.\n- subsample : Ratio of observations considered for training sample for each of the subtree. 0.7 meaning 70% of the rows are selected for training a subtree. \n- colsample_bytree : % of features or columns considered for training for each of the subtree. \n- n_estimators: number of trees to be built.\n- gamma : It is a regularisation parameter and is the minimum loss change required to split a leaf node. Great discussion on  <a href=\"https:\/\/stats.stackexchange.com\/questions\/418687\/gamma-parameter-in-xgboost\"> gamma hyperparameter <\/a>\n- alpha : L1 regularization term on weight and can be useful for a very high dimensionality scenarios\n- objective: Loss function.reg: linear for regression problems and binary: logistic for classification problems and returns predicted probability (not the class)\n- eval_metric : Evaluation Criteria and Typical values are:rmse \u2013 root mean square error, error \u2013 Binary classification error or auc: Area under the curve, etc\n","7493669f":"## Target Variable Creation","454a4035":"Source: https:\/\/www.kaggle.com\/rikdifos\/credit-card-approval-prediction\/tasks?taskId=1416\n\nTwo files are available. One, the application data, and the second one monthly credit card account status information.\n\nThe application data will be used for feature creation. And the status (credit payment status) will be required for defining the labels - which of the applications have paid back dues and which of these turn out to bad accounts.","18aa66f2":"**Key observations** from the summary statistics are:\n\n- **FLAG_MOBIL** : Min and Max values are same, so may not be of any use.\n- **CNT_FAM_MEMBERS**: Max value is 20 and 75th percentile is 3, so we may want to check for outliers.\n\nSome of the flag variables have 0 value for 75th percentile, so we may check the count\/percentage for 1 & 0 values for these.\n","1445c79f":"All the applicants that were NOT employed at the time of credit card approval will have negative employment month's values. We can change all of them to -1.","5e0bc1f1":"### Bivariate - Nominal Variables\n\nFor nominal variables, we can check assocation between label and a nominal variable. ","f15b70ce":"## Concluding Thoughts\n\nWe have learned many steps around preparing the data for model development.  There is definitely a more is typically done around feature engineering in a real-world problem.\n\nWe have used the XGBoost algorithm and it gave us a good accuracy level. In the next blog, we will use Logistic Regression with Weight of Evidence-based feature engineering.\n\nPlease share your thoughts and views.","abbbc8c5":"#### Optimized XGBoost Classifier\n\nParameter tuning has helped us get the best combination of the parameters. Now, we will fit the model with these sets of parameters and see the improvement in the accuracy of the model.","3589d22f":"#### Observations\n\nFLAG_MOBIL takes only a single value. So, not useful for the analysis\n\n**DAYS_BIRTH** and **DAYS_EMPLOYED**  are ordinal variables. These variables are days from the day data is extracted. Ideally, we would want the values as of application day.\n\nWe had assumed the date of data extraction as '01-01-2020'. So, we can get the date of birth and employment start date created.","144a360a":"### Model Evaluation\n\nNow, we may want to compare the predicted and observed label classes to see the actual accuracy. Confusion Matrix can be useful.","97e29e57":"## Data Exploration \n\nWe have a list of Credit Card Applications features and Label variables. We want to understand the distribution of these variables, whether some of these variables have missing values.\n\nBefore proceeding further it may be a good starting point to see the type of variables.","401deacf":"You can see the status by month since the start (start_month) and the relevance of what we have done.\n\nAcross all the acquisition months, now we can find portfolio performance across month 1, 2,5, 15, 20 months from their respective account open month. Distribution of account by the status across each of the months is calculated. First, we need to find the accounts by month and status code.","706bc3ab":"## Data","603fd385":"We are all set to proceed with data analysis and model development. Let's start with data exploration now.","7d1567a6":"### Split Sample to Train and Test Samples\n"}}