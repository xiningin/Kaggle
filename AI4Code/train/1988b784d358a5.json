{"cell_type":{"a4455e81":"code","3ccf8190":"code","7ba281af":"code","97c015fd":"code","c4cf8d3d":"code","82023f0b":"code","1ebae0e2":"code","454c2084":"code","acfd0dce":"code","a0a48fe6":"code","efd14205":"code","9d6142dd":"code","e3b2a92b":"code","e2a2de6e":"code","25db799a":"code","17c482fa":"code","6cda0f32":"code","c9e68c06":"code","b4b83fda":"code","1a7ece14":"code","a503c687":"code","ad424193":"code","c0d72538":"code","3df538b3":"code","13d7d83b":"code","48b19971":"code","e808b920":"code","2de5ca44":"code","aca8cd6b":"code","39833045":"markdown","cb406731":"markdown","fc7fc253":"markdown","81b0fa0d":"markdown","37c18cf6":"markdown","508bafa1":"markdown","4e21d39a":"markdown","784cb803":"markdown","8d28c50c":"markdown","e3fab3c0":"markdown","31f49f8d":"markdown","ac19e4b0":"markdown","994df844":"markdown","eecab9d0":"markdown","f84bc177":"markdown","92856ebb":"markdown","e15859ef":"markdown","6250e959":"markdown","61f32360":"markdown","e8cf84ab":"markdown","9d107421":"markdown","e6c1296d":"markdown","44d66f4a":"markdown","37c87ee6":"markdown","74a2aa93":"markdown","db5554c4":"markdown","81f7c48a":"markdown","42246d23":"markdown","9670b91c":"markdown","9489cd21":"markdown","35dcdd29":"markdown","2c7f782a":"markdown","1775b9c0":"markdown","07bd1dce":"markdown","0e459f74":"markdown","c1afcd19":"markdown","15390f0e":"markdown","de09688c":"markdown","5b3a8414":"markdown","29db39fb":"markdown","67697475":"markdown","3b0e1062":"markdown","3ceeab5b":"markdown","9cbf0554":"markdown","8776fec3":"markdown","4b5caf68":"markdown","b6cc51a8":"markdown","59d63303":"markdown","ec0035fe":"markdown","228d153c":"markdown","4b3c108b":"markdown","1f94880c":"markdown","bccdb74b":"markdown","152a1f09":"markdown","7581d9bd":"markdown"},"source":{"a4455e81":"!pip install mglearn","3ccf8190":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport mglearn\nimport warnings\nwarnings.simplefilter(action='ignore')\n\ndf = pd.read_csv(\"..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","7ba281af":"df.head()","97c015fd":"df.columns = ['customer_ID','gender','age','annual_income','spending_score']\ndf.head()","c4cf8d3d":"df.shape","82023f0b":"df.duplicated().any()","1ebae0e2":"df.isnull().any()","454c2084":"df = df.set_index(['customer_ID'])\ndf.head()","acfd0dce":"sns.pairplot(df, hue='gender',aspect = 1.5)\nplt.show()","a0a48fe6":"X = df.drop(['gender'], axis=1)\nX.head()","efd14205":"mglearn.plots.plot_kmeans_algorithm()","9d6142dd":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nclusters = []\nss = []\n\n#Calculate all the sum of within-cluster variance for n_clusters from 2 to 14\n\nfor i in range(2,15):\n    km = KMeans(n_clusters = i)\n    km.fit(X)\n    clusters.append(km.inertia_)\n    ss.append(silhouette_score(X, km.labels_, metric='euclidean'))\n","e3b2a92b":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(2, 15)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(6, 57000), xytext=(3, 50000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(5, 80000), xytext=(5, 150000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","e2a2de6e":"fig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(2, 15)), y=ss, ax=ax)\nax.set_title('Silhouette Score for each n_clusters')\nax.set_xlabel('Clusters')\nax.set_ylabel('Silhouette Score')","25db799a":"km6 = KMeans(n_clusters=6).fit(X)\n\nX['Labels'] = km6.labels_\n\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(121)\nsns.scatterplot(X['annual_income'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 6))\nax.set_title('KMeans with 6 Clusters')\nax.legend(loc='center right')\n\n\nax = fig.add_subplot(122)\nsns.scatterplot(X['age'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 6))\nax.set_title('KMeans with 6 Clusters')\nax.legend(loc='upper right')\n\nplt.show()","17c482fa":"km5 = KMeans(n_clusters=5).fit(X)\n\nX['Labels'] = km5.labels_\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(121)\nsns.scatterplot(X['annual_income'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('KMeans with 5 Clusters')\nax.legend(loc='center right')\n\n\nax = fig.add_subplot(122)\nsns.scatterplot(X['age'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('KMeans with 5 Clusters')\nax.legend(loc='upper right')\nplt.show()","6cda0f32":"from sklearn.datasets import make_moons\nx, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(x)\ny_pred = kmeans.predict(x)\n  \n\nplt.scatter(x[:, 0], x[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n                marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","c9e68c06":"mglearn.plots.plot_agglomerative_algorithm()","b4b83fda":"from sklearn.cluster import AgglomerativeClustering \n\naggloclus = AgglomerativeClustering(n_clusters=5, linkage='complete').fit(X)\n\nlabels = aggloclus.labels_\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(121)\nsns.scatterplot(X['annual_income'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('Agglomerative 5 clusters with complete linkage')\nax.legend(loc='center right')\n\n\nax = fig.add_subplot(122)\nsns.scatterplot(X['age'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('Agglomerative 5 clusters with complete linkage')\nax.legend(loc='upper right')\nplt.show()","1a7ece14":"from sklearn.cluster import AgglomerativeClustering \n\naggloclus = AgglomerativeClustering(n_clusters=5, linkage='ward').fit(X)\n\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(121)\nsns.scatterplot(X['annual_income'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('Agglomerative 5 clusters with ward linkage')\nax.legend(loc='center right')\n\n\nax = fig.add_subplot(122)\nsns.scatterplot(X['age'], X['spending_score'], hue=X['Labels'], \n                palette=sns.color_palette('hls', 5))\nax.set_title('Agglomerative 5 clusters with ward linkage')\nax.legend(loc='upper right')\nplt.show()","a503c687":"import plotly.figure_factory as ff\nimport plotly.graph_objects as go\nfrom scipy.cluster.hierarchy import ward\nlinkage = ward(X)\nfig = ff.create_dendrogram(linkage, color_threshold = 260)\nfig.update_layout(width = 1000, height = 600, yaxis_title = 'Cluster distance', xaxis_title = 'Sample index')\nfig.update_xaxes(showticklabels=False)\nfig.add_shape(\n        type='line',\n        x0=0,\n        y0=260,\n        x1=1985,\n        y1=260,\n        line=dict(\n            color='Black',\n            dash='dash'\n        )\n)\nfig.show()","ad424193":"from sklearn.datasets import make_moons\nx, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\n# cluster the data into two clusters\nagglo =  AgglomerativeClustering(n_clusters=2, linkage='average')\ny_pred = agglo.fit_predict(x)\n\n  \n\nplt.scatter(x[:, 0], x[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","c0d72538":"agglo =  AgglomerativeClustering(n_clusters=2, linkage='single')\ny_pred = agglo.fit_predict(x)\n\n  \n\nplt.scatter(x[:, 0], x[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","3df538b3":"#image taken from 'introduction to machine learning with Python' book\nmglearn.plots.plot_dbscan()","13d7d83b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndbscan = DBSCAN(eps = 0.7)\nclusters = dbscan.fit_predict(X_scaled)\nlength = len(np.unique(clusters))\nfig = plt.figure(figsize=(10,6))\nax = fig.add_subplot(121)\nsns.scatterplot(X['annual_income'], X['spending_score'], hue=clusters, \n                palette=sns.color_palette('hls', length))\nax.set_title('DBSCAN with 6 Clusters')\nax.legend(loc='center right')\n\n\nax = fig.add_subplot(122)\nsns.scatterplot(X['age'], X['spending_score'], hue=clusters, \n                palette=sns.color_palette('hls', length))\nax.set_title('DBSCAN with 6 Clusters')\nax.legend(loc='upper right')\nplt.show()","48b19971":"from itertools import product\n\neps_values = np.arange(0.3,1.6,0.1) # eps values to be investigated\nmin_samples = np.arange(3,8) # min_samples values to be investigated\nDBSCAN_params = list(product(eps_values, min_samples))\n\nno_of_clusters = []\nsil_score = []\n\nfor p in DBSCAN_params:\n    DBS_clustering = DBSCAN(eps=p[0], min_samples=p[1]).fit(X_scaled)\n    no_of_clusters.append(len(np.unique(DBS_clustering.labels_)))\n    sil_score.append(silhouette_score(X_scaled, DBS_clustering.labels_))","e808b920":"from matplotlib.ticker import FormatStrFormatter\n\ntmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \ntmp['No_of_clusters'] = no_of_clusters\n\npivot_1 = pd.pivot_table(tmp, values='No_of_clusters', index='Min_samples', columns='Eps')\n\nfig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(pivot_1, annot=True,annot_kws={\"size\": 16}, cmap=\"YlGnBu\", ax=ax)\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nax.set_xticklabels([str(round(float(label), 2)) for label in labels])\nax.set_title('Number of clusters')\nplt.show()","2de5ca44":"tmp = pd.DataFrame.from_records(DBSCAN_params, columns =['Eps', 'Min_samples'])   \ntmp['Sil_score'] = sil_score\n\npivot_1 = pd.pivot_table(tmp, values='Sil_score', index='Min_samples', columns='Eps')\n\nfig, ax = plt.subplots(figsize=(12,6))\nsns.heatmap(pivot_1, annot=True, annot_kws={\"size\": 10}, cmap=\"YlGnBu\", ax=ax)\nlabels = [item.get_text() for item in ax.get_xticklabels()]\nax.set_xticklabels([str(round(float(label), 2)) for label in labels])\nplt.show()\n","aca8cd6b":"from sklearn.datasets import make_moons\nx, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n\nx_scaled = scaler.fit_transform(x)\n# cluster the data into two clusters\ndbscan = DBSCAN(eps=0.3, min_samples=3) \ny_pred = dbscan.fit_predict(x_scaled)\n  \nplt.scatter(x[:, 0], x[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","39833045":"- __Using DBSCAN, we can clearly identify five of the customer clusters as well. It also has an additional advantage that we can detect outliers. The -1 red cluster is all outliers detected by DBSCAN. Hence, this give us extra information because sometimes outliers have great and insightful information.__\n- __Since we do not specify the number of clusters in DBSCAN, we can manually adjust eps and n_samples to get a desired\/sensible result.__\n- __There is no distinct groups in 'Age' vs 'Spending score'.__","cb406731":"- __The heatmap of Silhouette score can indicate the area where the best clusters are formed according to the Silhouette score. We should always try the dark blue region first since the global maximum of Silhouette score lies in that region.__","fc7fc253":"#### Dendrogram","81b0fa0d":" - __However, we can correctly clusters the half-moon-shaped data when we use 'single' linkage. This is because 'single' use the minimum distance to identify similar clusters and the gap between the two real clusters are big.__","37c18cf6":"- __As we can see from the plot above, K=6 is has a higher Silhouette score than K=5 but both are very close, hence, we can inspect both possibilities and finally choose the one which is more sensible gauged by our domain knowledge.__\n- __The Elbow method is more like a decision rule rather than a metric while the Silhouette score is a metric for clustering validation. Hence, they are not alternative to each other to find optimal K but rather, they can be used together to find a value of K with more confident.__","508bafa1":"What is clustering? Clustering is an unsupervised machine learning algorithm that only takes input, X, but not labelled output, y. Clustering algorithms partition datas into distinct subgroups\/clusters. For example, given a dataset of mall customers with features like 'Age', 'Income', 'Spending Score', we feed the clustering algorithm with the dataset and it will create a number of clusters based on the characteristics of the data and the specific algorithm with its parameters we use for clustering. In clustering, our main goal is to break down a big dataset into a few distinct subgroups so that we can use the subgroups to understand our data more easily or in a technical term, exploratory data analysis. However, since we do not have any labelled data for clustering algorithm, it is often diffucult to measure the performance of our model since we cannot compare the clusters with some ground truth answer to measure the performance. So, domain knowledge of the dataset is very important to interpret if we have reasonable and useful clusters.\n\nThere are many clustering algorithms and we will discuss three of the most popular ones which are:\n - K-Means clustering\n - Hierachical clustering(Agglomerative approach)\n - DBSCAN(Density Based Spatial Clustering of Application with Noise) \n\nGoals of this notebook:\n - Provide intuitive tutorial of clustering technique\n - Applying clustering techniques and evaluate their performance on certain dataset\n - Discuss the pros and cons of each of the three techniques","4e21d39a":"### 1. K-Means Clustering","784cb803":"#### An illustration showing the steps of K-Means clustering, taken from 'Introduction to Machine Learning with Python' book.","8d28c50c":"## Comparison of Clustering algorithms","e3fab3c0":"- __As compare to the 6 clusters plot, 5 clusters can better differentiate each of the subgroups. The five groups are:__\n 1. Low income with low spending score(Cluster 1)\n 2. Low income with high spending score(Cluster 2)\n 3. Medium income with medium spending score(Cluster 0)\n 4. High income with low spending score(Cluster 4)\n 5. High income with high spending score(Cluster 3)\n \n\n- __For the 6 clusters plot, there are two clusters in the middle income with medium spending score which we cannot tell the differences between the two clusters in the middle. Hence, with our domain knowledge, we choose K = 5 which is better representing our data.__\n\n- __We can also conclude that K-Means is very good at customer segmentation, more specifically on data clusters that have a spherical distribution. This is because K-Means utilize a centroid and the centroid measure each data point's distance from the centroid to the point using Euclidean distance in all direction from the centroid.__\n\n- __As a result, K-Means tend to perform very badly when the data clusters do not have spherical shape.__\n- __There is no distinct groups in 'Age' vs 'Spending score'.__","31f49f8d":"- __We can see that the clusters are identical to the one using 'complete' linkage. This might indicate that the clusters are well-defined since changing the linkage does not affect the clusters. This plot uses 'ward' linkage and 5 clusters.__\n- __There is no distinct groups in 'Age' vs 'Spending score'.__","ac19e4b0":"### 2. Hierachical clustering(Agglomerative approach)","994df844":"#### An example of the agglomerative clustering operations with 3 clusters (plot taken from 'Introduction to Machine Learning in Python' book)","eecab9d0":"## Clustering","f84bc177":"#### Implementation for Customer Data","92856ebb":"#### 4. There is no duplicated rows.","e15859ef":"- __We can clearly see KMeans fail to cluster correctly the above two half-moon shaped datas because their distribution of data is not spherical. We can see that the model cluster all the datapoint that is within a certain distance from the centroid(blue and red triangle) as the same cluster. So, we should inspect our data and consider properly when we choose our clustering algorithm.__","6250e959":"- In this plot, each colour is a different cluster label. The bigger coloured points are core points while the smaller coloured points are boundary points. White points are noise.\n- Increasing the eps distance(going left to right in a row) means that more points will be included in the cluster. As a result, the cluster will grows bigger but it might lead to multiple distinct clusters merging into one big cluster.\n- Increasing the min_samples required(going top to down in a column) means that fewer points will be core points because the requirement to become a core point is higher. Also, more points will be labelled as noise.\n- The parameter eps is somewhat more important, as it determines what it means for points to be 'close'.\n- The min_samples setting mostly determines whether points in less dense regions will be labelled as noise or as their own clusters.","61f32360":"#### 2. Renaming the columns so that it is easier to use later.","e8cf84ab":"#### 1. Looking at the first 5 rows of the data","9d107421":"To find a good setting for eps, scaling the data is very important as it ensures all the features have similar ranges.","e6c1296d":"#### Clustering for customer data","44d66f4a":"#### 5. There is no missing values in the dataset. ","37c87ee6":"DBSCAN works by identifying points that are in \"crowded\" regions of the feature space, where many data points are close together(a.k.a dense regions). The core idea of DSBCAN is that clusters form dense regions of data and they are seperated by regions that are relatively empty.\n\nDSBCAN does not require the user to set the number of clusters, it can capture clusters of complex shapes with two input parameter, eps(Euclidean distance) and min_samples(minimum samples within the eps distance). While DBSCAN doesn't require setting the number of clusters explicitly, setting eps and n_samples implicitly controls how many clusters will be found.\n\nThere are three types of point defined by DSBCAN.\n1. Core points: If there are at least min_samples number of samples within the eps distance, it is a core point.\n2. Boundary points: If a point is within the eps distance of a core point, but it have less than min_samples within its eps distance, it is classified as a boundary point.\n3. Noise: If there are less than n_samples of points within the eps distance of a point and there are no core point within the eps distance, the point is a noise.\n\nDSBCAN algorithm works as follows:\n1. A random point is picked to start with. If it there is no n_samples of points within its eps distance, this point is labelled as a noise. If there is n_samples or more points within its eps distance, this point is labelled as core point and assigned a new cluster label. \n2. Then, all the neighbours of the core point are visited, if they are not assigned any labels, they will be assigned to the new cluster label just created.\n3. If the neighbours are core samples, their neighbours are visited in turn, and so on. \n4. Core samples that are closer to each other than the eps distance are put into the same cluster by DBSCAN.\n5. The cluster grows until there are no more core samples within the eps distance fo the cluster.\n6. Finally, another point that hasn't been visited is picked, and the same procedure is repeated.\n7. The core points and noise will always be the same point in the same dataset given the same min_samples and eps.","74a2aa93":"Agglomerative hierachical clustering is a suite of clustering algorithms which build upon the same principles(bottom-up): the algorithms first declaring all the data points as clusters of its own and then the two most similar clusters will merge together and so on until a stopping criterion is achieved. For scikit-learn, the stopping criterion is the number of clusters(n_clusters). The algorithm also takes a 'linkage' parameter which tells us 'how to identify two most similar clusters'. There are four types of linkage implemented by scikit-learn:\n 1. ward: minimize the variance of the clusters being merged.\n 2. average: uses the average distance of each observation of the two sets\n 3. complete\/maximum: uses the maximum distance between all the observations of the two sets\n 4. single: uses the minimum distance between all the observations of the two sets\nward works on most datasets and it is the default option for scikit-learn, if the clusters have very dissimilar number of members(if one is much bigger than all the others) , average or complete might work better.\n\nThe metric(a.k.a dissimilarity measure) to compute the linkage can be chosen as well, the most common ones are:\n1. Euclidean distance(default)\n2. Manhattan distance\n3. Correlation-based distance(Focus on shape of observations rather than magnitudes)\nChoices of linkage and its metric affect the result produce by the algorithm, so we should consider the type of data being examined and the scientific question at hand before picking a certain combination of linkage and metric.\nIn practice, we should try all possibly insightful and useful combinations and then determine which one matches our desired outcome\/ provide us the most insight from the data.\n\nThe algorithm work as follows:\n1. Assign each point as a cluster of their own\n2. Two clusters that are closest to each other, if we uses Euclidean distance as dissimilarity metric and average as linkage.For example, a cluster will compare itself to all the other clusters by finding the Euclidean distance between the cluster centroid(average of a cluster) and all other clusters centroids. \n3. After that, the cluster will merge with the other cluster that have the shortest(minimal) Euclidean distance from its centroid. This is how the algorithm recognize 'how similar two clusters are'(i.e. shortest Euclidean distance between cluster centroids for 'average' linkage case).\n4. Step 2-4 are repeated until the stopping criterion(i.e. the number of clusters determined) is satisfied.\n\nAfter the agglomerative hierachical clustering, we will always use a dendrogram to visually inspect our result. We will look into the details dendrogram later.","db5554c4":"Although we can manually configure the eps and n_samples and compare the result, we can use a heatmap to look at the configuration of eps and n_samples to produce our desired n_clusters and Silhouette score quicker.","81f7c48a":"- __From the scatterplot above, we can see that agglomerative clustering work as well as KMeans clustering in customer segmentation, the five distinct clusters are similar to the five clusters produced by KMeans clustering. In this plot, we used 'complete' linkage and 5 clusters.__\n- __There is no distinct groups in 'Age' vs 'Spending score'.__\n","42246d23":"#### Choosing eps and n_samples using heatmap","9670b91c":"#### Clustering of half-moon-shaped dataset with KMeans","9489cd21":"#### Clustering of half-moon-shaped dataset with Agglomerative Clustering","35dcdd29":"![clusters.png](attachment:clusters.png)","2c7f782a":"- __The dendrogram is a tree-like diagram where the top is the root and the bottom part is the leaves. The leaves are made up of individual data points and a new node parent is added for each two clusters that are merged.__\n\n- __The vertical axis(cluster distance) depicts the Euclidean distance(in this case). As we move up from the bottom, the leaves merge into branches and this corresponds to the samples\/clusters that are similar to each other. The vertical distance represents how similar the clusters are, so for example, the greater the vertical distances before the merge happen, the more dissimilar the clusters are. This can be seen at the merge of the root(the top part), where the left main branch and the right main branch merge together, the vertical distance is as much as 300, which means they are very dissimilar compare to a vertical distance, say 5, which represents very similar clusters.__\n\n- __The dashed line show where we choose to cut the dendrogram to get a desired number of clusters. The number of vertical lines before branching under the dashed lines tells us the number of clusters we will have if we cut at the vertical distance(cluster distance). In this dendrogram, we can see there are five clusters when we cut at cluster distance = 260, and there are 5 vertical lines under the dashed lines before branching.__ \n\n- __So, using this dendrogram, we can look at each individual clusters and how they merge together to form into a larger cluster from bottom up. This provides us a lot of information since we can inspect each individual samples and examine how similar(or dissimilar) are they with the samples they merge with. Also, we can use this to choose how many clusters we would like to have by cutting at different cluster distances.__\n- __In practice, people often look at the dendrogram and select by eye a sensible number of clusters, based on the heights of the merge and the numbers of clusters desired.__","1775b9c0":"- __As we can see from the plot above, agglomerative clustering does not deal well with the half-moon-shaped data when we use 'average' linkage.__\n","07bd1dce":"#### Inspect the relationship between each feature using a pairplot.","0e459f74":"- __First, all the points are initialized as individual clusters. Then, in each step, the two closest clusters are merged together. In the first four steps, 4 set of two points clusters are formed. In step 5 - 7, three 3-point clusters are formed and then finally in step 9, 3 main clusters are formed.__","c1afcd19":"#### 6. Changing the index of the dataset to 'customer_ID' since 'customer_ID' will not provide us any insightful information for that customer except for indexing the specific customer.","15390f0e":"#### Clustering of half-moon-shaped data with DBSCAN","de09688c":"__Thank you for reading this notebook, please upvote if you like it and share it to your friends!__","5b3a8414":"#### Heatmap for Silhouette scores","29db39fb":"#### Clustering for customer data","67697475":"- __Another way is to use another metric, Silhouette score, to validate our choice of n_clusters.The Silhouette score measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation). The range of Silhouette score is between -1 and +1. The score closer to +1 indicates that the data points are in the right cluster. If many points have negative Silhouette score, it may indicate that we either have created too many clusters or too less clusters. Silhouette score also utilize Euclidean distance to compute its score. In contrast to inertia, Silhouette score reaches its global maximum at the optimal K value.__ ","3b0e1062":"#### Heatmap of number of clusters","3ceeab5b":"### 3. DBSCAN(Density Based Spatial Clustering of Application with Noise)","9cbf0554":"- __From the heatmap above, we can choose our desired number of clusters by setting the eps and n_samples accordingly. For our case, we chose 0.7 eps and n_samples = 5 which produce 6 clusters(5 main clusters + 1 outlier cluster).__","8776fec3":"## Understanding the data","4b5caf68":"- From the plot above, we can see that there is no obvious difference between Male and Female in all plots.\n- As a result, we will drop the 'gender' column since most of the distance-based algorithm cannot process categorical\/binary data properly to provide meaningful results. To cluster a dataset with both numerical and categorical data, you can consider and look for K-Modes clustering which is out-of-scope for this discussion.","b6cc51a8":"- __DBSCAN can perfectly classify this complex shaped data because it is a density-based algorithm and the two distinct clusters can be seen to have higher density data region of their own cluster seperated by gap without any datas(low density). It is also very flexible since we can always change eps and min_samples.__","59d63303":"- __The plot above is called an elbow method\/plot. It illustrates the inertia(sum of squared distances of each data point to their closest centroid \/ sum of within-cluster variance) against n_clusters(number of clusters). Practically, we will not choose the n_clusters for the minimum inertia and instead, we will choose the so-called elbow point, as illustrated above, since it explains the majority of the inertia(variance). However, sometimes by just visually inspecting the elbow points are not enough to tell which n_clusters is the optimal choice like the elbow plot illustrated above. One way to tackle this problem is to pick both elbow points, use them for clustering, evaluate the results based on our domain knowledge and finally choose the best n_clusters. Increasing the number of clusters from that elbow point does not provide extra much information and it might produce an undesired results. As a rule of thumb, we choose the n_clusters based on the elbow point.__","ec0035fe":"# Introduction to Clustering with Customer Segmentation","228d153c":"#### Dropping the 'gender' column for our three distance-based clustering techniques.","4b3c108b":"- __As we can see from the scatterplot above, there are 4 obvious clusters and two mixed clusters in the middle(green+red) for spending score vs annual_income.__\n- __There is no distinct groups in 'Age' vs 'Spending score'.__","1f94880c":"Table adapted from: https:\/\/towardsdatascience.com\/k-means-dbscan-gmm-agglomerative-clustering-mastering-the-popular-models-in-a-segmentation-c891a3818e29\n\nReference of notebook:\n1) https:\/\/www.kaggle.com\/fazilbtopal\/popular-unsupervised-clustering-algorithms\n\n2) https:\/\/www.kaggle.com\/datark1\/customers-clustering-k-means-dbscan-and-ap","bccdb74b":"#### 3. The data contains 200 rows and 5 columns.","152a1f09":"## Exploratory Data Analysis and Data Visualization","7581d9bd":"K-Means clustering is one of the most simple and commonly used clustering algorithm. It works as follows:\n 1. Randomly assign k = n_clusters of cluster centre(centroids).\n 2. Assign each datapoint to the closest centroids.\n 3. Calculate the mean of all the points in each cluster and reassign the centroid of each cluster as the mean.\n 4. Reassign each datapoint to the closest centroid based on Euclidean distance.\n 5. Repeat Step 3 and Step 4 until the total sum of the within-cluster variation over the three clusters is the minimum."}}