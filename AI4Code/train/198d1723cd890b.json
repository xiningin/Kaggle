{"cell_type":{"47ad9873":"code","1098b194":"code","fd0501d5":"code","8294cb6d":"code","3445d4f8":"code","7dfe6ff5":"code","b03d410d":"code","99bd17a2":"code","ed3340f6":"code","a8744e2a":"code","838d3d2b":"code","263ce416":"code","09294c4c":"code","3ff3f336":"markdown","3a1732ca":"markdown","485ddc62":"markdown","8a357c82":"markdown","40fac942":"markdown","3029ba66":"markdown","d4505b66":"markdown","a71a8c44":"markdown","a090e961":"markdown","3dda827b":"markdown"},"source":{"47ad9873":"import numpy as np\nimport pandas as pd\nimport os\n\nPATH = \"\/kaggle\/input\/applications-of-deep-learning-wustlfall-2021\/city\/\"\nPATH_TRAIN = os.path.join(PATH, \"train.csv\")\nPATH_TEST = os.path.join(PATH, \"test.csv\")","1098b194":"# What version of Python do you have?\nimport sys\n\nimport tensorflow.keras\nimport pandas as pd\nimport sklearn as sk\nimport tensorflow as tf\n\nprint(f\"Tensor Flow Version: {tf.__version__}\")\nprint(f\"Keras Version: {tensorflow.keras.__version__}\")\nprint()\nprint(f\"Python {sys.version}\")\nprint(f\"Pandas {pd.__version__}\")\nprint(f\"Scikit-Learn {sk.__version__}\")\nprint(\"GPU is\", \"available\" if tf.test.is_gpu_available() \\\n      else \"NOT AVAILABLE\")","fd0501d5":"df_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\n\ndf_train['filename'] = df_train.id.astype(str) + \".jpg\"\ndf_test['filename'] = df_test.id.astype(str) + \".jpg\"","8294cb6d":"TRAIN_PCT = 0.9\nTRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n\ndf_train_cut = df_train[0:TRAIN_CUT]\ndf_validate_cut = df_train[TRAIN_CUT:]\n\nprint(f\"Training size: {len(df_train_cut)}\")\nprint(f\"Validate size: {len(df_validate_cut)}\")","3445d4f8":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\n\nWIDTH = 256\nHEIGHT = 256\n\ntraining_datagen = ImageDataGenerator(\n  rescale = 1.\/255,\n  horizontal_flip=True,\n  #vertical_flip=True,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"sqft\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=32, # Keeping the training batch size small USUALLY increases performance\n        class_mode='raw')\n\nvalidation_datagen = ImageDataGenerator(rescale = 1.\/255)\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"sqft\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=256, # Make the validation batch size as large as you have memory for\n        class_mode='raw')","7dfe6ff5":"from tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import Input\n\ninput_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n\nbase_model = ResNet50(\n    include_top=False, weights=None, input_tensor=input_tensor,\n    input_shape=None)\n\n#base_model.summary()","b03d410d":"from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\n\nx=base_model.output\nx=GlobalAveragePooling2D()(x)\nx=Dense(1024,activation='relu')(x) \nx=Dense(1024,activation='relu')(x) \nmodel=Model(inputs=base_model.input,outputs=Dense(1)(x))","99bd17a2":"from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n\n# Important, calculate a valid step size for the validation dataset\nSTEP_SIZE_VALID=val_generator.n\/\/val_generator.batch_size\n\nmodel.compile(loss = 'mean_squared_error', optimizer='adam', metrics=[RootMeanSquaredError(name=\"rmse\")])\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=1, mode='auto',\n        restore_best_weights=True)\n\nhistory = model.fit(train_generator, epochs=100, steps_per_epoch=250, \n                    validation_data = val_generator, callbacks=[monitor],\n                    verbose = 1, validation_steps=STEP_SIZE_VALID)","ed3340f6":"submit_datagen = ImageDataGenerator(rescale = 1.\/255)\n\nsubmit_generator = submit_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=PATH,\n        x_col=\"filename\",\n        batch_size = 1,\n        shuffle = False,\n        target_size=(HEIGHT, WIDTH),\n        class_mode=None)\n\nsubmit_generator.reset()\npred = model.predict(submit_generator,steps=len(df_test))","a8744e2a":"df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':pred[:,0].flatten()})\ndf_submit.to_csv(\"\/kaggle\/working\/submit.csv\",index = False)","838d3d2b":"p2 = pred[:,0].flatten()","263ce416":"df_submit[df_submit.id==24163]","09294c4c":"submit_generator.filenames","3ff3f336":"Now we train just like before, the only difference is that we do not define the entire neural network here.","3a1732ca":"Now we must add a few layers to the end of the neural network so that it becomes a regression model.","485ddc62":"# Kaggle Transfer Learning Code for the City Square Feed Kaggle In-Class Competition\n\nThis workbook is a starter code for the [City Square Feed Kaggle In-Class Competition](https:\/\/www.kaggle.com\/c\/applications-of-deep-learning-wustlfall-2021)  This competition is one of the assignments for [T81-558: Applications of Deep Neural Netw1orks](https:\/\/sites.wustl.edu\/jeffheaton\/t81-558\/) at [Washington University in St. Louis](https:\/\/www.wustl.edu).\n\nThis is a more advanced notebook that uses transfer learning; however, it is not particularly optimized.  It is also possable to run this project from Google CoLab.  ","8a357c82":"Next we check versions and if the GPU is available. A GPU or TPU will be very helpful.","40fac942":"## Transfer Learning\n\nWe will now use a ResNet neural network as a basis for our neural network.  We will redefine both the input shape and output of the ResNet model, so we will not transfer the weights.  Since we redefine the input; the weights are of minimal value.  We begin by loading, from Keras, the ResNet50 network.","3029ba66":"We want to use early stopping.  To do this, we need a validation set.  We will break the data into 80 percent test data and 20 validation.  Do not confuse this validation data with the test set provided by Kaggle.  This validation set is unique to your program and is just used for early stopping.","d4505b66":"# Build Submission\n\nNow that the neural network is trained; we need to generate a submit CSV file to send to Kaggle.  We will use nearly the same technique to build the submit file.  However, these essential points that we must address:\n\n* We do not want the data generator to create an infinite date like we did when training.  We have a fixed number of cases to score for the Kaggle submit; we only want to process them.\n* We do not want the data generator to randomize the samples' order like it did when training. Therefore we set shuffle to false.\n* We want to always start at the beginning of the data, so we reset the generator.\n\nThese ensure that the predictions align with the id's.","a71a8c44":"We now create the neural network and fit it.  Some essential concepts are going on here.\n\n* **Batch Size** - The number of training samples that should be evaluated per training step.  Smaller batch sizes, or mini-batches, are generally preferred.\n* **Step** - A training step is one complete run over the batch.  At the end of a step, the weights are updated, and the neural network learns.\n* **Epoch** - An arbitrary point at which to measure results or checkpoint the model.  Generally, an epoch is one complete pass over the training set.  However, when generators are used, the training set size is theoretically infinite. Because of this, we set a **steps_per_epoch** parameter.\n* **validation steps** - The validation set may also be infinite; because of this, we must specify how many steps we wish to validate at the end of each Epoch.","a090e961":"Next, we create the generators that will provide the images to the neural network as it is trained.  We normalize the images so that the RGB colors between 0-255 become ratios between 0 and 1.  We also use the **flow_from_dataframe** generator to connect the Pandas dataframe to the actual image files. We see here a straightforward implementation; you might also wish to use some of the image transformations provided by the data generator.\n\nThe **HEIGHT** and **WIDTH** constants specify the dimensions that the image will be scaled (or expanded) to. It is probably not a good idea to expand the images.","3dda827b":"Next, we prepare to read the training data (that we have labels for) and the test data that we must predict and send to Kaggle."}}