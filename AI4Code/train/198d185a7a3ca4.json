{"cell_type":{"76b8cb32":"code","fa1971ab":"code","72032674":"code","d7bde1c7":"code","3b0c9d40":"code","4a07d5f5":"code","cce603fc":"code","cc3eb5a6":"code","df98224e":"code","61d07694":"code","0bed602c":"code","77614943":"code","d56f2ada":"code","762efafc":"code","c430d861":"code","b7deb40a":"code","64e0e9d2":"code","5e90440d":"code","f382b5d7":"code","529b4094":"code","318e4d1c":"markdown","5983b06f":"markdown","a38c440c":"markdown","fcdee4a5":"markdown","955e5a3d":"markdown","6d183a04":"markdown","8d38ccd5":"markdown","10cec621":"markdown","a20a70d7":"markdown","a6abfe6c":"markdown","9be883a4":"markdown","625c2dd0":"markdown","acd6775e":"markdown","06aec0e6":"markdown","db3182bd":"markdown"},"source":{"76b8cb32":"import matplotlib.pyplot as plt\nplt.figure(figsize = (16,16))\nimg = plt.imread('\/kaggle\/input\/sign-language-mnist\/amer_sign2.png')\nplt.imshow(img)\nplt.show()","fa1971ab":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport random\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout\nimport warnings\n\nsns.set()\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","72032674":"df_train = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_train\/sign_mnist_train.csv')\ndf_train.head(-5)","d7bde1c7":"df_test = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\ndf_test.head(-5)","3b0c9d40":"plt.figure(figsize = (15,10))\nsns.set_style(\"darkgrid\")\nsns.countplot(df_train['label'])\nplt.show()","4a07d5f5":"y_train = df_train['label']\ndf_train.drop(['label'], axis=1, inplace=True)\ndf_train.head(-5)","cce603fc":"y_test = df_test['label']\ndf_test.drop(['label'], axis=1, inplace=True)\ndf_test.head(-5)","cc3eb5a6":"size  = 28\nchannels = 1\nbatch = 128\nepochs = 100","df98224e":"X_train = df_train.values.reshape(df_train.shape[0], size, size, channels)\nX_test = df_test.values.reshape(df_test.shape[0], size, size, channels)\n\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test shape:\", X_test.shape)","61d07694":"datagen = ImageDataGenerator(rescale=1.\/255,\n                             zoom_range=0.2,\n                             width_shift_range=.2, height_shift_range=.2,\n                             rotation_range=30,\n                             brightness_range=[0.8, 1.2],\n                             horizontal_flip=True)\n\ndatagenRescale = ImageDataGenerator(rescale=1.\/255)\n\nX_train = datagen.flow(X_train, y_train, batch_size=batch)\n\nX_test = datagenRescale.flow(X_test, y_test)","0bed602c":"alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\nplt.figure(figsize=(15, 15))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    for X_batch, Y_batch in X_train:\n        image = X_batch[i]\n        plt.imshow(image, cmap='gray')\n        plt.xlabel(alphabet[Y_batch[i]])\n        break\nplt.show()","77614943":"checkpoint_filepath = 'best_model.hdf5'\n\ncallback_checkpoint = ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=False, monitor='val_accuracy', mode='max', save_best_only=True)\ncallback_learningrate = ReduceLROnPlateau(monitor='loss', mode='min', min_delta=0.01, patience=3, factor=.75, min_lr=0.00001, verbose=1)\n\ncallbacks = [callback_checkpoint, callback_learningrate]","d56f2ada":"Model = Sequential([Conv2D(filters=32,  kernel_size=(3,3), activation=\"relu\", input_shape=(size,size,channels)),\n                    MaxPool2D(2,2, padding='same'),\n                    Dropout(0.2),\n                 \n                    Conv2D(filters=128,  kernel_size=(3,3), activation=\"relu\"),\n                    MaxPool2D(2,2, padding='same'),\n                    Dropout(0.2),\n                \n                    Conv2D(filters=512, kernel_size=(3,3), activation=\"relu\"),\n                    MaxPool2D(2,2, padding='same'),\n                    Dropout(0.2),\n                    \n                    \n                    Flatten(),\n                    \n                    Dense(units=4096, activation=\"relu\"),                 \n                    Dropout(0.2),\n                    \n                    Dense(units=1024, activation=\"relu\"),\n                    Dropout(0.2),\n                                  \n                    Dense(units=256, activation=\"relu\"),\n                    Dropout(0.2),\n                    \n                    Dense(units=25, activation=\"softmax\"),\n])\n\n\nModel.compile(optimizer='adam', loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n\nModel.summary()","762efafc":"history = Model.fit(X_train, validation_data=X_test, epochs=epochs, callbacks=callbacks)","c430d861":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs_range = range(len(acc))\n\nplt.figure(figsize=(15, 15))\nplt.subplot(2, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","b7deb40a":"score = Model.evaluate(X_test) \n\nprint('Test loss:', score[0]) \nprint('Test accuracy:', score[1])","64e0e9d2":"df_test = pd.read_csv('..\/input\/sign-language-mnist\/sign_mnist_test\/sign_mnist_test.csv')\ny_test = df_test['label']\ndf_test.drop(['label'], axis=1, inplace=True)\nX_test = df_test.values.reshape(df_test.shape[0], size, size, channels)","5e90440d":"y_pred = np.argmax(Model.predict(X_test),axis = 1) ","f382b5d7":"from sklearn.metrics import confusion_matrix\nCM = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (15,15))\nsns.heatmap(CM, annot=True, cmap=\"Blues\", fmt = 'g')\nplt.xlabel(\"Predicted Classes\")\nplt.ylabel(\"True Classes\")\nplt.title(\"Confusion Matrix\")\nplt.show()","529b4094":"plt.figure(figsize=(15,15))\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.imshow(X_test[i],cmap='gray')\n    plt.ylabel(f\"True: {alphabet[y_test[i]]}\")\n    plt.xlabel(f\"Predicted: {alphabet[y_pred[i]]}\")\n    \nplt.show()","318e4d1c":"# 10- Let's Check","5983b06f":"# 8- Let's evaluate","a38c440c":"The classes of data and their number of images.","fcdee4a5":"# Reference\nhttps:\/\/www.kaggle.com\/abdelrahmanzied\/sign-language-classification-cnn-99-accuracy\n\nI will try to make some modifications and practices on it.","955e5a3d":"# 4- Data Augmentation\nLet's make augmentation on training data and only scaling validation and test data.","6d183a04":"Reshape the data to make images ready for **ImageDataGenerator**:\n> reshape(number of rows, img width, img height, channels)","8d38ccd5":"The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2\u2026.pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255.","10cec621":"# 9- Confusion Matrix","a20a70d7":"# 3- Prepare Data\nImport **train** and **test** data as dataframes.","a6abfe6c":"# 7- Let's train","9be883a4":"Let's split the **label** column from train and test data frames.","625c2dd0":"# 5- Setup Callbacks","acd6775e":"# 6- Desighn the Neural Network","06aec0e6":"# 2- Import\nFirst we import our libraries that will we need.","db3182bd":"Define the size of image and its channel, batch size  and the number of epochs."}}