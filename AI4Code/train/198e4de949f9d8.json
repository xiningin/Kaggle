{"cell_type":{"0e9167c6":"code","dceb0795":"markdown"},"source":{"0e9167c6":"# basic imports\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\ndef scrape_tds():\n    # create some basic list in which we can store our items\n    rows = []\n\n    # you can also start with earlier years; for me, 2018 was sufficient\n    for year in [\n        \"2018\", \n        \"2019\", \n        \"2020\", \n        \"2021\"\n    ]:\n        # you can also use zfill to do this programmatically\n        for month in [\n            \"01\", \n            \"02\", \n            \"03\", \n            \"04\", \n            \"05\", \n            \"06\", \n            \"07\", \n            \"08\", \n            \"09\", \n            \"10\", \n            \"11\", \n            \"12\"\n        ]:\n            for day in [\n                \"01\", \n                \"02\", \n                \"03\", \n                \"04\", \n                \"05\", \n                \"06\", \n                \"07\", \n                \"08\", \n                \"09\", \n                \"10\",\n                \"11\", \n                \"12\", \n                \"13\", \n                \"14\", \n                \"15\", \n                \"16\", \n                \"17\", \n                \"18\", \n                \"19\", \n                \"20\",\n                \"21\", \n                \"22\", \n                \"23\", \n                \"24\", \n                \"25\", \n                \"26\", \n                \"27\", \n                \"28\",\n                # you might also want to continue with 29, 30 and 31\n                # but this takes some extra error handling - which I didn't do :-)\n            ]:\n                # BeautifulSoup is an incredibly powerful tool. You might want to check it out for any webscraping-related stuff!\n                soup = BeautifulSoup(requests.get(\n                    f\"https:\/\/towardsdatascience.com\/archive\/{year}\/{month}\/{day}\"\n                ).text, \"html.parser\")\n                # just some logging\n                print(f\"{year}\/{month}\/{day}\")\n                # you need to understand the HTML file for TDS, which is easy with a little bit of experience\n                # the `Inspect` tool of Chrome helps a ton here!\n                for div, h3 in zip(soup.find_all(\"div\", {\"class\": \"postArticle-content js-postField\"}), soup.find_all(\"h3\")):\n                    try:\n                        # just append the scraped article to our rows\n                        row = {\n                            \"title\": h3.text,\n                            # do not worry about the tagline scraping; it looks a bit difficult, but is quite easy\n                            # when you take a look at the respective HTML file\n                            \"tagline\": BeautifulSoup(requests.get(div.parent[\"href\"]).text)\\\n                                       .find(\"h1\").find_parent().find_parent().find_all(\"div\")[1].find(\"h2\").text,\n                            \"url\": div.parent[\"href\"],\n                            \"date\": f\"{year}\/{month}\/{day}\"\n                        }\n                    except:\n                        # this could also be improved, as I didn't do proper error handling!\n                        continue\n                    rows.append(row)\n\n    # store the scraped data using Pandas\n    df = pd.DataFrame(rows)\n    df.to_csv(\"towards_data_science.csv\", index=False)\n    \n# scrape_tds()","dceb0795":"# Scraping the Towards Data Science Archive\n\nIn this notebook, you can take a look at how the data has been collected. I first tried out some selenium webscraper, but then found out that there is a TDS archive you can access to have an overview for all the articles that are available for a given timestamp: `https:\/\/towardsdatascience.com\/archive\/{year}\/{month}\/{day}`.\n\nThis way, it is quite easy to collect the data. However, it took me longer than two days to run the full script, so you might want to run this on a small server or workstation."}}