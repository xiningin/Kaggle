{"cell_type":{"2afc892d":"code","bfae2063":"code","ea0a5bf0":"code","ab25ea5b":"code","27aa3291":"code","06b1284f":"code","7d32b501":"code","e3b6b0f1":"code","890d60b5":"code","897b8387":"code","950fdf49":"code","7aaead88":"code","ac7bf738":"code","124fa637":"code","5d123183":"code","5272409d":"code","d3597611":"code","6b925d5e":"code","d6369a20":"code","bcbe7c6d":"code","1771e9ce":"code","7a0d9176":"code","8d6b499b":"code","db2eb666":"code","affba160":"code","e77d6781":"code","8467853c":"code","8098861b":"markdown","8421a258":"markdown","05efea0f":"markdown","b74bf798":"markdown","2b451046":"markdown","a3a6c0d6":"markdown","515393c3":"markdown","0f6e922a":"markdown","e53ae4a2":"markdown","62b2d220":"markdown","1d7d6006":"markdown","52abbbe6":"markdown","a1d80809":"markdown","86f6c79a":"markdown","1d754f40":"markdown","7aec0d63":"markdown","7d6bf687":"markdown","3c0dc76b":"markdown","af13a5a8":"markdown","62276330":"markdown"},"source":{"2afc892d":"import os\nimport sys\nimport random\nimport math\nimport subprocess\nfrom glob import glob\nfrom collections import OrderedDict\nimport numpy as np\nimport nltk\nfrom skimage import measure\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom matplotlib import pyplot as plt\nimport scipy.ndimage as ndi\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, Conv2D, MaxPooling2D, Lambda, MaxPool2D, BatchNormalization, Input, concatenate, Reshape, LSTM, CuDNNLSTM\nfrom keras.utils import np_utils\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom collections import OrderedDict\nimport math\nimport tensorflow as tf\nfrom tensorflow.contrib.layers import xavier_initializer_conv2d, variance_scaling_initializer, xavier_initializer\n","bfae2063":"import pandas as pd\ntestdata_manual = pd.read_csv(\"..\/input\/testdata.manual.2009.06.14.csv\")","ea0a5bf0":"def convolution(x, weight_shape, stride, padding=\"SAME\", bias=True, initializer=xavier_initializer_conv2d(seed=0)):\n    \"\"\" 2d convolution layer\n    - weight_shape: width, height, input channel, output channel\n    - stride: batch, w, h, c\n    \"\"\"\n    weight = tf.Variable(initializer(shape=weight_shape))\n    x = tf.nn.conv2d(x, weight, strides=stride, padding=padding)\n    if bias:\n        return tf.add(x, tf.Variable(tf.zeros([weight_shape[-1]]), dtype=tf.float32))\n    else:\n        return x","ab25ea5b":"def full_connected(x, weight_shape, bias=True, initializer=xavier_initializer(seed=0)):\n    \"\"\" fully connected layer\n    - weight_shape: input size, output size\n    - priority: batch norm (remove bias) > dropout and bias term\n    \"\"\"\n    weight = tf.Variable(initializer(shape=weight_shape))\n    x = tf.matmul(x, weight)\n    if bias:\n        return tf.add(x, tf.Variable(tf.zeros([weight_shape[-1]]), dtype=tf.float32))\n    else:\n        return x","27aa3291":"\n\nclass CharCNN(object):\n    \"\"\" character-level CNN with BN and scheduled learning rate\n    `Dos Santos, C\u00edcero Nogueira, and Maira Gatti. \"Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.\" COLING. 2014.`\n        - inputs:\n            - onehot char vector (max word num in sentence, max char num in word, character size)\n            - embedded vector of word (max word num in sentence, embedded dimension)\n        - network\n            - word -(CNN)-> feature by word (w)\n            - char -(CNN)-> char embedded vector for each word -(CNN)-> feature by char (c)\n            [c, w] -> CNN + max pool over feature -> FC -> output\n    \"\"\"\n\n    def __init__(self, network_architecture, learning_rate=0.0001, load_model=None, gradient_clip=None,\n                 batch_norm=None, keep_prob=1.0):\n        \"\"\"\n        :param dict network_architecture: dictionary with following elements\n            input_char: shape of char input (list: max word num in sentence, max char num in word, vocabulary size)\n            input_word: shape of word input (list: max word num in sentence, vocabulary size)\n            label_size: unique number of label\n            char_embed_dim: character-level embedding dimension\n            char_cnn_unit: channel size of cnn for character to word representation\n            char_cnn_kernel: kernel size of cnn for character to word representation\n            word_embed_dim: word embedding dimension\n            cnn_unit: channel size of cnn for word and character feature\n            cnn_kernel: kernel size of cnn for word and character feature\n            hidden_unit: hidden unit number for\n        :param float learning_rate: default 0.001\n        :param float gradient_clip: (option) clipping gradient value\n        :param float keep_prob: (option) keep probability of dropout\n        :param str load_model: (option) load saved model\n        :param float batch_norm: (option) decay for batch norm. ex) default for 0.999\n                                shadow_variable = decay * shadow_variable + (1 - decay) * variable\n                                https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/contrib\/layers\/batch_norm\n        \"\"\"\n        self.network_architecture = network_architecture\n        self.binary_class = True if self.network_architecture[\"label_size\"] == 2 else False\n        self.learning_rate = learning_rate\n        self.gradient_clip = gradient_clip\n        self.batch_norm = batch_norm\n        self.keep_prob = keep_prob\n\n        # Create network\n        self.create_network()\n        # Summary\n        tf.summary.scalar(\"loss\", self.loss)\n        tf.summary.scalar(\"accuracy\", self.accuracy)\n        # Launch the session\n        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n        # Summary writer for tensor board\n        self.summary = tf.summary.merge_all()\n        # Load model\n        if load_model:\n            tf.reset_default_graph()\n            self.saver.restore(self.sess, load_model)\n\n    def create_network(self):\n        \"\"\" Create Network, Define Loss Function and Optimizer\n        input: x_char, x_word, batch_norm, keep_prob, y (training)\n        \"\"\"\n\n    def embedding_char(self, x, embed_dim_c=5, embed_dim_w=10, wk=5):\n        \"\"\" Word representation by character level embedding.\n        :param x: input (batch, max word num in sentence, max char num in word, vocabulary size)\n        :param int embed_dim_c: embedding dimension of word\n        :param int embed_dim_w: embedding dimension of character\n        :param int wk: word context window\n        :return embedding: embedded vector (batch, max word num, embed_dim_w)\n        \"\"\"\n\n    def embedding_word(self, x, embed_dim=30):\n        \"\"\" Word representation by character level embedding.\n        :param x: input (batch, max word num in sentence, embedded dim)\n        :param int embed_dim: embedding dimension of word\n        :param bool batch_norm: if False, no batch normalization\n        :return embedding: embedded vector (batch, max word num, embed_dim)\n        \"\"\"\n        # (batch, max word num in sentence, vocabulary size) -> (batch, max word num in sentence, embedded dim, 1)\n","06b1284f":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nnet = {\"label_size\": 2, \"input_char\": [40, 33, 26], \"input_word\": [40, 300],\n        \"char_embed_dim\": 5, \"char_cnn_unit\": 10, \"char_cnn_kernel\": 3, \"word_embed_dim\": 30,\n        \"cnn_unit\": 300, \"cnn_kernel\": 5,\n        \"hidden_unit\": 300}\nmodel = CharCNN(net)","7d32b501":"def quantize_label(score):\n    \"\"\"\n    [0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n    for very negative, negative, neutral, positive, very positive, respectively.\n    :parameter score:\n    :return value: very negative: 1, negative: 2, neutral: 3, positive: 4, very positive: 5\n    \"\"\"\n    label = np.zeros(len(score))\n    label[score <= 0.2] += 1\n    label[score <= 0.4] += 1\n    label[score <= 0.6] += 1\n    label[score <= 0.8] += 1\n    label[score <= 1] += 1\n    return label.astype(int)","e3b6b0f1":"def sst_formatting(path):\n    with open(\"%s\/sentiment_labels.txt\" % path) as f:\n        _tmp = [i.split('|') for i in f.read().split('\\n')]\n        _tmp.pop(-1)\n        _tmp.pop(0)\n        _tmp = np.array(_tmp)\n        _df1 = pd.DataFrame(_tmp[:, 1].astype(float), columns=[\"label\"], index=_tmp[:, 0].astype(int))\n\n    with open(\"%s\/dictionary.txt\" % path) as f:\n        _tmp = [i.split('|') for i in f.read().split('\\n')]\n        _tmp.pop(-1)\n        _tmp = np.array(_tmp)\n        _df2 = pd.DataFrame(_tmp[:, 0], columns=[\"data\"], index=_tmp[:, 1].astype(int))\n\n    return _df1.join(_df2, how=\"inner\")","890d60b5":"#Reset Default path\ndef data_set(path=\".\/data\/stanfordSentimentTreebank\", drop_neutral=True, cut_off=3, binary=True):\n    \"\"\"\n    :param str path: path to the `stanfordSentimentTreebank`\n    :param bool drop_neutral: if ignore neutral label or not\n    :param cut_off: cut off the word based on frequency. (remove word, which frequency < cut_off) If None, no cut off\n    :param binary: binarize label or not\n    :return dict: Stanford Sentiment Treebank data\n    \"\"\"\n    df = sst_formatting(path)\n    label = quantize_label(df[\"label\"].values)\n    df[\"label\"] = label\n    original_size = len(df)\n    if cut_off is not None:\n        df[\"cnt\"] = [len(i.split(' ')) for i in df[\"data\"].values]\n        df = df[df.cnt >= cut_off]\n        label = df[\"label\"].values\n    if drop_neutral:\n        df = df[df.label != 3]\n        if binary:\n            label = df[\"label\"].values\n            label[label > 3] = 1\n            label[label != 1] = 0\n            df[\"label\"] = label\n            bal = [np.sum(label == 1), np.sum(label == 0)]\n        else:\n            bal = [np.sum(label == i) for i in [1, 2, 4, 5]]\n    else:\n        bal = [np.sum(label == i) for i in [1, 2, 3, 4, 5]]\n    return {\"label\": df.label.values, \"sentence\": df.data.values, \"original_size\": original_size, \"balance\": bal}\n","897b8387":"import os\nimport logging\nimport argparse\nimport tensorflow as tf\nimport numpy as np\nimport sequence_modeling\nimport gensim\n\ndef train(epoch, model, feeder, input_format, save_path=\".\/\", lr_decay=1.0, test=False):\n    \"\"\" Train model based on mini-batch of input data.\n    :param model: model instance\n    :param str save_path: Path to save\n    :param int epoch:\n    :param feeder: Feeding data.\n    :param input_format: (optional) Input data format for `model`. For instance\n            def input_format(model, x):\n                return {model.x_char: x[0], model.x_word: x[1]}\n        This example is used when char and word vector is fed through the `feeder`. This function has to return dict.\n        By default, def input_format(model, x): return {model.x: x}\n    :param float lr_decay: learning rate will be divided by lr_decay each epoch\n    \"\"\"\n\n    # logger\n\n    # Initializing the tensor flow variables\n\n","950fdf49":"def create_log(name):\n    \"\"\"Logging.\"\"\"\n    if os.path.exists(name):\n        os.remove(name)\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    # handler for logger file\n    handler1 = logging.FileHandler(name)\n    handler1.setFormatter(logging.Formatter(\"H1, %(asctime)s %(levelname)8s %(message)s\"))\n    # handler for standard output\n    handler2 = logging.StreamHandler()\n    handler2.setFormatter(logging.Formatter(\"H1, %(asctime)s %(levelname)8s %(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef get_options(parser):\n    share_param = {'nargs': '?', 'action': 'store', 'const': None, 'choices': None, 'metavar': None}\n    parser.add_argument('model', help='Name of model to use. (default: cnn_char)',\n                        default='cnn_char', type=str, **share_param)\n    parser.add_argument('-e', '--epoch', help='Epoch number. (default: 500)',\n                        default=500, type=int, **share_param)\n    parser.add_argument('-b', '--batch', help='Batch size. (default: 100)',\n                        default=100, type=int, **share_param)\n    parser.add_argument('-l', '--lr', help='Learning rate. (default: 0.0001)',\n                        default=0.0001, type=float, **share_param)\n    parser.add_argument('-c', '--clip', help='Gradient clipping. (default: None)',\n                        default=None, type=float, **share_param)\n    parser.add_argument('-k', '--keep', help='Keep rate for Dropout. (default: 1.0)',\n                        default=1.0, type=float, **share_param)\n    parser.add_argument('-n', '--norm', help='Decay for batch normalization. if batch is 100, 0.95 (default: None)',\n                        default=None, type=float, **share_param)\n    parser.add_argument('-d', '--decay_lr', help='Decay index for learning rate (default: 1.0)',\n                        default=1.0, type=float, **share_param)\n    return parser.parse_args()\n\n","7aaead88":"\nif __name__ == '__main__':\n    # Ignore warning message by tensor flow\n    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n    _parser = argparse.ArgumentParser(description='This script is ...', formatter_class=argparse.RawTextHelpFormatter)\n    args = get_options(_parser)\n\n    # save path\n    path = \".\/log\/%s\/l%0.6f_e%i_b%i\" % (args.model, args.lr, args.epoch, args.batch)\n    if args.decay_lr != 1.0:  # learning rate decaying\n        path += \"_d%0.2f\" % args.decay_lr\n    if args.clip is not None:  # gradient clipping\n        path += \"_c%0.2f\" % args.clip\n    if args.norm is not None:  # batch normalization\n        path += \"_n%0.3f\" % args.norm\n    if args.keep != 1.0:  # dropout\n        path += \"_k%0.2f\" % args.keep\n\n    # Import word2vec(To Be Done for Kaggle)\n    embedding_model = gensim.models.KeyedVectors.load_word2vec_format(\".\/data\/GoogleNews-vectors-negative300.bin\", binary=True)\n\n    # load data\n    data = data_set()\n\n    # load model\n    model_instance = sequence_modeling.get_model_instance(model_name=args.model,\n                                                          embedding_model=embedding_model,\n                                                          learning_rate=args.lr,\n                                                          gradient_clip=args.clip,\n                                                          batch_norm=args.norm,\n                                                          keep_prob=args.keep)\n\n    # train\n    data_feeder = sequence_modeling.BatchFeeder(data[\"sentence\"], data[\"label\"], batch_size=args.batch,\n                                                validation=0.05, process=model_instance[\"processing\"])\n    train(model=model_instance[\"model\"], input_format=model_instance[\"input_format\"],\n          epoch=args.epoch, feeder=data_feeder, save_path=path, lr_decay=args.decay_lr,\n          test=False)","ac7bf738":"def read(filename):\n    dat = pd.read_csv(filename,header=None)\n   # print(dat)\n    lines = (str(dat[0])+dat[5]).tolist()\n    words_map = {} #\u30de\u30c3\u30d4\u30f3\u30b0\u7528\n    char_map = {} #\u30de\u30c3\u30d4\u30f3\u30b0\u7528\n    word_cnt = 0 #\u5358\u8a9e\u306e\u7a2e\u985e\u6570\n    char_cnt = 0 #\u6587\u5b57\u306e\u7a2e\u985e\u6570\n    \n    k_chr = 3 #\u6587\u5b57\u30b3\u30f3\u30c6\u30af\u30b9\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n    k_wrd = 5 #\u5358\u8a9e\u30b3\u30f3\u30c6\u30af\u30b9\u30c8\u30a6\u30a3\u30f3\u30c9\u30a6\n\n    y = [] \n    x_chr = []\n    x_wrd = []\n\n    max_word_len, max_sen_len, num_sent = 0, 0, 20000\n\n    for line in lines[:num_sent]:\n        words = line[:-1].split()\n        tokens = words[1:]\n        y.append(int(float(words[0])))\n        max_sen_len = max(max_sen_len,len(tokens))\n        for token in tokens:\n            if token not in words_map:\n                words_map[token] = word_cnt\n                word_cnt += 1\n                max_word_len = max(max_word_len,len(token))\n            for i in xrange(len(token)):\n                if token[i] not in char_map:\n                    char_map[token[i]] = char_cnt\n                    char_cnt += 1\n    \n    for line in lines[:num_sent]:\n        words = line[:-1].split()\n        tokens = words[1:]\n        word_mat = [0] * (max_sen_len+k_wrd-1)\n        char_mat = numpy.zeros((max_sen_len+k_wrd-1, max_word_len+k_chr-1))\n\n        for i in xrange(len(tokens)):\n            word_mat[(k_wrd\/2)+i] = words_map[tokens[i]]\n            for j in xrange(len(tokens[i])):\n                char_mat[(k_wrd\/2)+i][(k_chr\/2)+j] = char_map[tokens[i][j]]\n        x_chr.append(char_mat)\n        x_wrd.append(word_mat)\n    max_word_len += k_chr-1\n    max_sen_len += k_wrd-1\n    data = (num_sent, char_cnt, word_cnt, max_word_len, max_sen_len,\\\n            k_chr, k_wrd, x_chr, x_wrd, y)\n    return data\n# num_sent:Number of documents\n    # word_cnt: Number of word types\n    # max_sen_len: Maximum document length\n    # x_wrd: Id string of input word\n    # y: 1 or 0 (i.e., positive or negative)","124fa637":"    (num_sent, char_cnt, word_cnt, max_word_len, max_sen_len,k_chr, k_wrd, x_chr, x_wrd, y)   = read(\"..\/input\/testdata.manual.2009.06.14.csv\")","5d123183":"class EmbedIDLayer(object):\n    \"\"\"\n    Efficient linear function for one-hot input.\n    \"\"\"\n    #Kriplani\n    def __init__(\n        self,\n        rng,\n        input=None,\n        n_input=None,\n        n_output=None,\n        W=None,\n    ):\n        if input is None:\n            input = T.imatrix('x')\n\n        if W is None:\n            W_values = np.asarray(\n                rng.uniform(low=-np.sqrt(6.0\/(n_input+n_output)),\n                            high=np.sqrt(6.0\/(n_input+n_output)),\n                            size=(n_input, n_output)),\n                dtype=theano.config.floatX)\n\n            # W_values[0,0] = 0\n            W_tmp = theano.shared(value=W_values, name='W', borrow=True)\n        else:\n            W_values = W.astype(theano.config.floatX)\n            W_tmp = theano.shared(value=W_values, name='W', borrow=True)\n\n        self.W = W_tmp\n        self.output = self.W[input]\n        self.params = [self.W]\n","5272409d":"class FullyConnectedLayer(object):\n    def __init__(\n        self,\n        rng,\n        input=None, \n        n_input=784,\n        n_output=10,\n        activation=None,\n        W=None,\n        b=None\n    ):\n\n        self.input = input\n\n        if W is None:\n            W_values = np.asarray(\n                rng.uniform(low=-np.sqrt(6.0\/(n_input+n_output)),\n                            high=np.sqrt(6.0\/(n_input+n_output)),\n                            size=(n_input, n_output)),\n                dtype=theano.config.floatX)\n            if activation == sigmoid:\n                W_values *= 4.0\n            W = theano.shared(value=W_values, name='W', borrow=True)\n\n        if b is None:\n            b_values = np.zeros((n_output,), dtype=theano.config.floatX)\n            b = theano.shared(value=b_values, name='b', borrow=True)\n\n        self.W = W\n        self.b = b\n\n        linear_output = T.dot(input, self.W) + self.b\n\n        if activation is None:\n            self.output = linear_output\n        else:\n            self.output = activation(linear_output)\n\n        self.params = [self.W, self.b]","d3597611":"class ConvolutionalLayer(object):\n  #Indraneel\n    def __init__(\n        self,\n        rng,\n        input,\n        filter_shape=None,\n        image_shape=None,\n        activation=relu\n    ):\n        self.input = input\n        self.rng = rng\n\n        \"\"\"filter shape = n_feature_map, in channel, width, height\"\"\"\n        \"\"\"channel * width * height\"\"\"\n        fan_in = np.prod(filter_shape[1:]) # 1*2*3\n        \"\"\"feature map \"\"\"\n        fan_out = filter_shape[0] * np.prod(filter_shape[2:]) # 0*2*3\n        \n        W_bound = np.sqrt(6.0 \/ (fan_in + fan_out))\n        self.W = theano.shared(\n            np.asarray(\n                self.rng.uniform(\n                    low = -W_bound,\n                    high = W_bound,\n                    size = filter_shape\n                ),\n                dtype = theano.config.floatX\n            ),\n            borrow = True\n        )\n\n        b_values = np.zeros((filter_shape[0],),\n                            dtype=theano.config.floatX)\n        self.b = theano.shared(b_values, borrow=True)\n\n        conv_out = conv.conv2d(\n            input=self.input,\n            filters=self.W,\n            filter_shape=filter_shape,\n            image_shape=image_shape\n        )\n\n        self.output = activation(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n\n        self.params = [self.W, self.b]","6b925d5e":"class MaxPoolingLayer(object):\n  #Indraneel\n    def __init__(self, input, poolsize=(2,2)):\n        pooled_out = downsample.pool_2d(\n            input=input,\n            ws=poolsize,\n            ignore_border=True\n        )\n        self.output = pooled_out","d6369a20":"class Optimizer(object):\n    def __init__(self, params=None):\n        if params is None:\n            return NotImplementedError()\n        self.params = params\n\n    def updates(self, loss=None):\n        if loss is None:\n            return NotImplementedError()\n\n        self.updates = OrderedDict()\n        self.gparams = [T.grad(loss, param) for param in self.params]\n\n# SGD exists\nclass SGD(Optimizer):\n    def __init__(self, learning_rate=0.01, params=None):\n        super(SGD, self).__init__(params=params)\n        self.learning_rate = 0.01\n\n    def updates(self, loss=None):\n        super(SGD, self).updates(loss=loss)\n\n        for param, gparam in zip(self.params, self.gparams):\n            self.updates[param] = param - self.learning_rate * gparam\n\n        return self.updates\n#SGD Momentum exists\nclass MomentumSGD(Optimizer):\n    def __init__(self, learning_rate=0.01, momentum=0.9, params=None):\n        super(MomentumSGD, self).__init__(params=params)\n        self.learning_rate = learning_rate\n        self.momentum = momentum\n        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n    def updates(self, loss=None):\n        super(MomentumSGD, self).updates(loss=loss)\n\n        for v, param, gparam in zip(self.vs, self.params, self.gparams):\n            _v = v * self.momentum\n            _v = _v - self.learning_rate * gparam\n            self.updates[param] = param + _v\n            self.updates[v] = _v\n\n        return self.updates\n#Adagrad exists\nclass AdaGrad(Optimizer):\n    def __init__(self, learning_rate=0.01, eps=1e-6, params=None):\n        super(AdaGrad, self).__init__(params=params)\n\n        self.learning_rate = learning_rate\n        self.eps = eps\n        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n\n    def updates(self, loss=None):\n        super(AdaGrad, self).updates(loss=loss)\n\n        for accugrad, param, gparam\\\n        in zip(self.accugrads, self.params, self.gparams):\n            agrad = accugrad + gparam * gparam\n            dx = - (self.learning_rate \/ T.sqrt(agrad + self.eps)) * gparam\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n#RMSprop exists\nclass RMSprop(Optimizer):\n    def __init__(self, learning_rate=0.001, alpha=0.99, eps=1e-8, params=None):\n        super(RMSprop, self).__init__(params=params)\n\n        self.learning_rate = learning_rate\n        self.alpha = alpha\n        self.eps = eps\n\n        self.mss = [build_shared_zeros(t.shape.eval(),'ms') for t in self.params]\n\n    def updates(self, loss=None):\n        super(RMSprop, self).updates(loss=loss)\n\n        for ms, param, gparam in zip(self.mss, self.params, self.gparams):\n            _ms = ms*self.alpha\n            _ms += (1 - self.alpha) * gparam * gparam\n            self.updates[ms] = _ms\n            self.updates[param] = param - self.learning_rate * gparam \/ T.sqrt(_ms + self.eps)\n\n        return self.updates\n\n# adadelta exists\nclass AdaDelta(Optimizer):\n    def __init__(self, rho=0.95, eps=1e-6, params=None):\n        super(AdaDelta, self).__init__(params=params)\n\n        self.rho = rho\n        self.eps = eps\n        self.accugrads = [build_shared_zeros(t.shape.eval(),'accugrad') for t in self.params]\n        self.accudeltas = [build_shared_zeros(t.shape.eval(),'accudelta') for t in self.params]\n\n    def updates(self, loss=None):\n        super(AdaDelta, self).updates(loss=loss)\n\n        for accugrad, accudelta, param, gparam\\\n        in zip(self.accugrads, self.accudeltas, self.params, self.gparams):\n            agrad = self.rho * accugrad + (1 - self.rho) * gparam * gparam\n            dx = - T.sqrt((accudelta + self.eps)\/(agrad + self.eps)) * gparam\n            self.updates[accudelta] = (self.rho*accudelta + (1 - self.rho) * dx * dx)\n            self.updates[param] = param + dx\n            self.updates[accugrad] = agrad\n\n        return self.updates\n\n#Adam exists\nclass Adam(Optimizer):\n    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, gamma=1-1e-8, params=None):\n        super(Adam, self).__init__(params=params)\n\n        self.alpha = alpha\n        self.b1 = beta1\n        self.b2 = beta2\n        self.gamma = gamma\n        self.t = theano.shared(np.float32(1))\n        self.eps = eps\n\n        self.ms = [build_shared_zeros(t.shape.eval(), 'm') for t in self.params]\n        self.vs = [build_shared_zeros(t.shape.eval(), 'v') for t in self.params]\n\n    def updates(self, loss=None):\n        super(Adam, self).updates(loss=loss)\n        self.b1_t = self.b1 * self.gamma ** (self.t - 1)\n\n        for m, v, param, gparam \\\n        in zip(self.ms, self.vs, self.params, self.gparams):\n            _m = self.b1_t * m + (1 - self.b1_t) * gparam\n            _v = self.b2 * v + (1 - self.b2) * gparam ** 2\n\n            m_hat = _m \/ (1 - self.b1 ** self.t)\n            v_hat = _v \/ (1 - self.b2 ** self.t)\n\n            self.updates[param] = param - self.alpha*m_hat \/ (T.sqrt(v_hat) + self.eps)\n            self.updates[m] = _m\n            self.updates[v] = _v\n        self.updates[self.t] = self.t + 1.0\n\n        return self.updates","bcbe7c6d":"#Himanshu\nclass Result(object):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    def negative_log_likelihood(self):\n        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n        return -T.mean(T.log(self.prob_of_y_given_x)[T.arange(self.y.shape[0]), self.y])\n\n    def cross_entropy(self):\n        self.prob_of_y_given_x = T.nnet.softmax(self.x)\n        return T.mean(T.nnet.categorical_crossentropy(self.prob_of_y_given_x, self.y))\n\n    def mean_squared_error(self):\n        return T.mean((self.x - self.y) ** 2)\n\n    def errors(self):\n        if self.y.ndim != self.y_pred.ndim:\n            raise TypeError('y should have the same shape as self.y_pred',\n                            ('y', self.y.type, 'y_pred', self.y_pred.type))\n\n        if self.y.dtype.startswith('int'):\n            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n            return T.mean(T.neq(self.y_pred, self.y))\n        else:\n            return NotImplementedError()\n\n    def accuracy(self):\n        if self.y.dtype.startswith('int'):\n            self.prob_of_y_given_x = T.nnet.softmax(self.x)\n            self.y_pred = T.argmax(self.prob_of_y_given_x, axis=1)\n            return T.mean(T.eq(self.y_pred, self.y))\n        else:\n            return NotImplementedError()","1771e9ce":"class CharSCNN(object):\n    def __init__(self,rng,batchsize=100,activation=relu):\n        \n        import char_load\n        (num_sent, char_cnt, word_cnt, max_word_len, max_sen_len,k_chr, k_wrd, x_chr, x_wrd, y) = char_load.read(\"tweets_clean.txt\")\n\n        dim_word = 30\n        dim_char = 5\n        cl_word = 300\n        cl_char = 50\n        k_word = k_wrd\n        k_char = k_chr\n\n        data_train_word,data_test_word,data_train_char,data_test_char,target_train,target_test = train_test_split(x_wrd, x_chr, y, random_state=1234, test_size=0.1)\n\n        x_train_word = theano.shared(np.asarray(data_train_word, dtype='int16'), borrow=True)\n        x_train_char = theano.shared(np.asarray(data_train_char, dtype='int16'), borrow=True)\n        y_train = theano.shared(np.asarray(target_train, dtype='int8'), borrow=True)\n        x_test_word = theano.shared(np.asarray(data_test_word, dtype='int16'), borrow=True)\n        x_test_char = theano.shared(np.asarray(data_test_char, dtype='int16'), borrow=True)\n        y_test = theano.shared(np.asarray(target_test, dtype='int8'), borrow=True)\n\n        self.n_train_batches = x_train_word.get_value(borrow=True).shape[0] \/ batchsize\n        self.n_test_batches = x_test_word.get_value(borrow=True).shape[0] \/ batchsize        \n        \n        \"\"\"symbol definition\"\"\"\n        index = T.iscalar()\n        x_wrd = T.wmatrix('x_wrd')\n        x_chr = T.wtensor3('x_chr')\n        y = T.bvector('y')\n        train = T.iscalar('train')\n\n        \"\"\"network definition\"\"\"\n        layer_char_embed_input = x_chr#.reshape((batchsize, max_sen_len, max_word_len))\n\n        layer_char_embed = EmbedIDLayer(rng,layer_char_embed_input,n_input=char_cnt,n_output=dim_char)\n\n        layer1_input = layer_char_embed.output.reshape((batchsize*max_sen_len, 1, max_word_len, dim_char))\n\n        layer1 = ConvolutionalLayer(rng,layer1_input,filter_shape=(cl_char, 1, k_char, dim_char),image_shape=(batchsize*max_sen_len, 1, max_word_len, dim_char))\n\n        layer2 = MaxPoolingLayer(layer1.output,poolsize=(max_word_len-k_char+1, 1))\n\n        layer_word_embed_input = x_wrd #.reshape((batchsize, max_sen_len))\n\n        layer_word_embed = EmbedIDLayer(rng,layer_word_embed_input,n_input=word_cnt,n_output=dim_word)\n\n        layer3_word_input = layer_word_embed.output.reshape((batchsize, 1, max_sen_len, dim_word))\n        layer3_char_input = layer2.output.reshape((batchsize, 1, max_sen_len, cl_char))\n\n        layer3_input = T.concatenate([layer3_word_input,layer3_char_input],axis=3)#.reshape((batchsize, 1, max_sen_len, dim_word+cl_char))\n\n        layer3 = ConvolutionalLayer(rng,layer3_input,filter_shape=(cl_word, 1, k_word, dim_word + cl_char),image_shape=(batchsize, 1, max_sen_len, dim_word + cl_char),activation=activation)\n\n        layer4 = MaxPoolingLayer(layer3.output,poolsize=(max_sen_len-k_word+1, 1))\n\n        layer5_input = layer4.output.reshape((batchsize, cl_word))\n\n        layer5 = FullyConnectedLayer(rng,dropout(rng, layer5_input, train),n_input=cl_word,n_output=50,activation=activation)\n\n        layer6_input = layer5.output\n\n        layer6 = FullyConnectedLayer(rng,dropout(rng, layer6_input, train, p=0.1),n_input=50,n_output=2,activation=None)\n\n        result = Result(layer6.output, y)\n        loss = result.negative_log_likelihood()\n        accuracy = result.accuracy()\n        params = layer6.params+layer5.params+layer3.params+layer_word_embed.params+layer1.params+layer_char_embed.params\n        updates = RMSprop(learning_rate=0.001, params=params).updates(loss)\n\n        self.train_model = theano.function(inputs=[index],outputs=[loss, accuracy],updates=updates,\n            givens={\n                x_wrd: x_train_word[index*batchsize: (index+1)*batchsize],\n                x_chr: x_train_char[index*batchsize: (index+1)*batchsize],\n                y: y_train[index*batchsize: (index+1)*batchsize],\n                train: np.cast['int32'](1)\n            }\n        )\n\n        self.test_model = theano.function(inputs=[index],outputs=[loss, accuracy],\n            givens={\n                x_wrd: x_test_word[index*batchsize: (index+1)*batchsize],\n                x_chr: x_test_char[index*batchsize: (index+1)*batchsize],\n                y: y_test[index*batchsize: (index+1)*batchsize],\n                train: np.cast['int32'](0)\n            }\n        )\n\n#Indraneel\n    def train_and_test(self, n_epoch=100):\n        epoch = 0\n        accuracies = []\n        while epoch < n_epoch:\n            epoch += 1\n            sum_loss = 0\n            sum_accuracy = 0\n            for batch_index in xrange(self.n_train_batches):\n                batch_loss, batch_accuracy = self.train_model(batch_index)\n                sum_loss = 0\n                sum_accuracy = 0\n                for batch_index in xrange(self.n_test_batches):\n                    batch_loss, batch_accuracy = self.test_model(batch_index)\n                    sum_loss += batch_loss\n                    sum_accuracy += batch_accuracy\n                loss = sum_loss \/ self.n_test_batches\n                accuracy = sum_accuracy \/ self.n_test_batches\n                accuracies.append(accuracy)\n\n                print('epoch: {}, test mean loss={}, test accuracy={}'.format(epoch, loss, accuracy))\n                print('')\n        return accuracies\n\n","7a0d9176":"#Rough work for Keras model\nmodelchar = Sequential()\nmodelchar.add(Embedding(char_cnt,dim_char,input_length=char_cnt))#Input embedding char embedding\nmodelchar.add(Conv2D(filters=c1_char, kernel_size=(k_char,dim_char),input_shape=(batchsize*max_sen_len, max_word_len, dim_char))) \n#https:\/\/stackoverflow.com\/questions\/30633181\/the-output-size-of-theano-tensor-nnet-conv-conv2d\nmodelchar.add(MaxPooling2D(pool_size=(max_word-k_char+1, 2)))\nmodelword.add(Embedding(char_cnt,dim_char,input_length=char_cnt))#Input embedding char embedding\nword_output = modelword()\nchar_output = modelchar()\nmerge_layer = concatenate([word_out, char_output],axis=3)\nlayer3 = Conv2D(filters = filter_shape,kernel_size=(k_word,dim_word+cl_char),input_shape=(batchsize, max_sen_len, dim_word + cl_char),activation=relu)(merge_layer)\nlayer4 = MaxPooling2D(pool_size=(max_sen_len-k_word+1, 1))(layer3)\ntheta = Dropout(0.5)(layer4)\nlayer5 = Dense(50,activation='relu')(theta)\n","8d6b499b":"    def __init__(self, input, poolsize=(2,2)):\n        pooled_out = downsample.pool_2d(\n            input=input,\n            ws=poolsize,\n            ignore_border=True\n        )\n        self.output = pooled_out","db2eb666":"ConvolutionalLayer(rng,layer1_input,filter_shape=(cl_char, 1, k_char, dim_char),image_shape=(batchsize*max_sen_len, 1, max_word_len, dim_char))\n layer5 = FullyConnectedLayer(rng,dropout(rng, layer5_input, train),n_input=cl_word,n_output=50,activation=activation)\n","affba160":"if __name__ == '__main__':\n    random_state = 1234\n    rng = np.random.RandomState(random_state)\n    charscnn = CharSCNN(rng, batchsize=10, activation=relu)\n    charscnn.train_and_test(n_epoch=3)\n\n\n","e77d6781":"import pandas as pd\ntestdata_manual = pd.read_csv(\"..\/input\/testdata.manual.2009.06.14.csv\")\ntraining_1600000.processed.noemoticon = pd.read_csv(\"..\/input\/training.1600000.processed.noemoticon.csv\")","8467853c":"def plotHistogram(a):\n    #Plot histogram of RGB Pixel Intensities\n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\nplotHistogram(X_train[1])","8098861b":"**References:** <br\/>\n**Easier Explanation** : https:\/\/github.com\/satwantrana\/CharSCNN <br\/>\n**Ideas for a Tensorflow Implementation**: https:\/\/github.com\/asahi417\/DocumentClassification","8421a258":"**Data Visualzation Code**","05efea0f":"Note: Modify all functions using Keras as a framework","b74bf798":"**Work Left : Main function and Visualization Functions(Using Matplotlib)**","2b451046":"**Guidelines:**\n1.   Use Tensorflow, PyTorch or Keras to implement the given CharSCNN architecture given in the paper.\n\n2.  For training: either use STS or SSTb. No need to compare with other baseline models, as specified in the paper.\n3.  Report accuracy of your model using pre-trained word embeddings.\n\n**Dataset Link**: http:\/\/cs.stanford.edu\/people\/alecmgo\/trainingandtestdata.zip","a3a6c0d6":"File upload Testing","515393c3":"Keras should have an in-built optimizer function. Please verify this.(Indraneel)[All optimizzers exist]","0f6e922a":"**To be done once everything above this text is finished**","e53ae4a2":"**NNFL Course Project** <br>\n**Paper** :  Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts (Paper ID: 211)<br>\n**Members**<br>\nIndraneel Ghosh 2016B1A70938P<br>\nMohit Kriplani 2016B1A70870P<br>","62b2d220":"**Search for Equivalent library functions before you start writing code**\n","1d7d6006":"Fully connected layer","52abbbe6":"Maxpooling exists","a1d80809":"References for Keras:\nhttps:\/\/towardsdatascience.com\/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n\nhttps:\/\/www.kaggle.com\/coder98\/blood-cell-subtype-identification-cudnn-cnn-rnn","86f6c79a":"Loading Files to Cloud(Do not run again)","1d754f40":"**CharCNN Architecture Contents**","7aec0d63":"Code starts","7d6bf687":"Theano","3c0dc76b":"**Understand what sst_formatting is doing here exactly adn how the function executes**","af13a5a8":"Tensorflow","62276330":"https:\/\/towardsdatascience.com\/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92"}}