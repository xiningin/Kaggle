{"cell_type":{"e2cb84ac":"code","7dae507d":"code","27754b7b":"code","3cd2253c":"code","5e251844":"code","62e95678":"code","21423deb":"code","06168964":"code","5aa13980":"code","0a3a5f30":"code","01ecd185":"code","ef8dbcbc":"code","57bacaf7":"code","baead9a1":"code","33d86e20":"code","95bb9d8f":"code","9c10c682":"code","ad374241":"code","1285797f":"code","08c55578":"code","ef419113":"code","8610f8f1":"code","d68d4c3b":"code","c3d7e100":"code","93aba97b":"code","b8527327":"code","1f691860":"code","9e38fd8d":"code","8435b382":"code","2f767c14":"code","dd153c06":"markdown","88248762":"markdown","77958533":"markdown","48896769":"markdown","f1154a01":"markdown"},"source":{"e2cb84ac":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing\nimport warnings\nimport time\nimport sys\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.set_option('display.max_columns', 500)","7dae507d":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","27754b7b":"new_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv',\n                               parse_dates=['purchase_date'])\n\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv',\n                                      parse_dates=['purchase_date'])\n\ndef binarize(df):\n    for col in ['authorized_flag', 'category_1']:\n        df[col] = df[col].map({'Y':1, 'N':0})\n    return df\n\nhistorical_transactions = binarize(historical_transactions)\nnew_transactions = binarize(new_transactions)","3cd2253c":"\nfrom datetime import date\n\ntoday = date.today()\n\ndef read_data(input_file):\n    df = pd.read_csv(input_file)\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['elapsed_time'] = (today - df['first_active_month'].dt.date).dt.days\n    return df\n#_________________________________________\ntrain = read_data('..\/input\/train.csv')\ntest = read_data('..\/input\/test.csv')\n\ntarget = train['target']\n","5e251844":"historical_transactions['month_diff'] = ((datetime.datetime.today() - historical_transactions['purchase_date']).dt.days)\/\/30\nhistorical_transactions['month_diff'] += historical_transactions['month_lag']\n\nnew_transactions['month_diff'] = ((datetime.datetime.today() - new_transactions['purchase_date']).dt.days)\/\/30\nnew_transactions['month_diff'] += new_transactions['month_lag']","62e95678":"historical_transactions[:5]","21423deb":"\nhistorical_transactions = pd.get_dummies(historical_transactions, columns=['category_2', 'category_3'])\nnew_transactions = pd.get_dummies(new_transactions, columns=['category_2', 'category_3'])\n\nhistorical_transactions = reduce_mem_usage(historical_transactions)\nnew_transactions = reduce_mem_usage(new_transactions)\n\nagg_fun = {'authorized_flag': ['mean']}\nauth_mean = historical_transactions.groupby(['card_id']).agg(agg_fun)\nauth_mean.columns = ['_'.join(col).strip() for col in auth_mean.columns.values]\nauth_mean.reset_index(inplace=True)\n\nauthorized_transactions = historical_transactions[historical_transactions['authorized_flag'] == 1]\nhistorical_transactions = historical_transactions[historical_transactions['authorized_flag'] == 0]","06168964":"historical_transactions['purchase_month'] = historical_transactions['purchase_date'].dt.month\nauthorized_transactions['purchase_month'] = authorized_transactions['purchase_date'].dt.month\nnew_transactions['purchase_month'] = new_transactions['purchase_date'].dt.month","5aa13980":"def aggregate_transactions(history):\n    \n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).\\\n                                      astype(np.int64) * 1e-9\n    \n    agg_func = {\n    'category_1': ['sum', 'mean'],\n    'category_2_1.0': ['mean'],\n    'category_2_2.0': ['mean'],\n    'category_2_3.0': ['mean'],\n    'category_2_4.0': ['mean'],\n    'category_2_5.0': ['mean'],\n    'category_3_A': ['mean'],\n    'category_3_B': ['mean'],\n    'category_3_C': ['mean'],\n    'merchant_id': ['nunique'],\n    'merchant_category_id': ['nunique'],\n    'state_id': ['nunique'],\n    'city_id': ['nunique'],\n    'subsector_id': ['nunique'],\n    'purchase_amount': ['sum', 'mean', 'max', 'min', 'std'],\n    'installments': ['sum', 'mean', 'max', 'min', 'std'],\n    'purchase_month': ['mean', 'max', 'min', 'std'],\n    'purchase_date': [np.ptp, 'min', 'max'],\n    'month_lag': ['mean', 'max', 'min', 'std'],\n    'month_diff': ['mean']\n    }\n    \n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['_'.join(col).strip() for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history","0a3a5f30":"history = aggregate_transactions(historical_transactions)\nhistory.columns = ['hist_' + c if c != 'card_id' else c for c in history.columns]\nhistory[:5]","01ecd185":"authorized = aggregate_transactions(authorized_transactions)\nauthorized.columns = ['auth_' + c if c != 'card_id' else c for c in authorized.columns]\nauthorized[:5]","ef8dbcbc":"new = aggregate_transactions(new_transactions)\nnew.columns = ['new_' + c if c != 'card_id' else c for c in new.columns]\nnew[:5]","57bacaf7":"train = pd.merge(train, history, on='card_id', how='left')\ntest = pd.merge(test, history, on='card_id', how='left')\n\ntrain = pd.merge(train, authorized, on='card_id', how='left')\ntest = pd.merge(test, authorized, on='card_id', how='left')\n\ntrain = pd.merge(train, new, on='card_id', how='left')\ntest = pd.merge(test, new, on='card_id', how='left')\n\n","baead9a1":"#card_id is encoded from string to numeric value\nfrom sklearn.preprocessing import LabelEncoder \nlbe = LabelEncoder()\nlbe = lbe.fit(train['card_id'])\ncard_id = lbe.transform(train['card_id']) \ntrain['card_id'] = card_id \ntrain['card_id'].head()\n\n","33d86e20":"lbe_activation = LabelEncoder()\nlbe = lbe_activation.fit(train['first_active_month'])\nfirst_active_month = lbe.transform(train['first_active_month']) \ntrain['first_active_month'] = first_active_month \ntrain['first_active_month'].head()","95bb9d8f":"from sklearn.model_selection import train_test_split\ny = train['target']\ndel train['target']\ndel train ['first_active_month']\n\nX_train, X_test, y_train, y_test = train_test_split(train, y, \n                                                    test_size=0.3, \n                                                    random_state=1234)\n","9c10c682":"from sklearn.model_selection import GridSearchCV\nmodel = xgb.XGBRegressor()\nparameters = {'nthread':[5], \n              'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07],\n              'max_depth': [5, 6, 7, 8],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [100]}\n\nxgb_grid = GridSearchCV(model,\n                        parameters,\n                        cv = 5,\n                        n_jobs = 5,\n                        verbose=True) \n\n\n","ad374241":"#this is used to get the best model and parameters\n#xgb_grid.fit(X_train, y_train)\n\n","1285797f":"#print(xgb_grid.best_score_)\n","08c55578":"#print(xgb_grid.best_params_) ","ef419113":"#train the model using the best score parameters \n'''\ncolsample_bytree: Subsample ratio of columns when constructing each tree\nlearning_rate: Boosting learning rate\nmax_depth : Maximum tree depth for base learners\nmin_child_weight: Minimum sum of instance weight(hessian) needed in a child.  \nn_estimators: Number of boosted trees to fit\nnthread: Number of parallel threads used to run xgboost\nobjective: Specify the learning task and the corresponding learning objective\nsilent: Whether to print messages while running boosting \nsubsample: Subsample ratio of the training instance\n'''\n\n'''model_final = xgb.XGBRegressor(colsample_bytree= 0.7, \n                               learning_rate= 0.03,\n                               max_depth= 7, \n                               min_child_weight= 4, \n                               n_estimators= 120,\n                               nthread= 4, \n                               objective= 'reg:linear',\n                               silent= 1, \n                               subsample= 0.7)'''","8610f8f1":"model_final = xgb.XGBRegressor(colsample_bytree= 0.7, \n                               learning_rate= 0.05,\n                               max_depth= 6, \n                               min_child_weight= 4, \n                               n_estimators= 100,\n                               nthread= 5, \n                               objective= 'reg:linear',\n                               silent= 1, \n                               subsample= 0.7)","d68d4c3b":"model_final.fit(X_train, y_train)","c3d7e100":"# make predictions for test data\ny_pred = model_final.predict(X_test)\n","93aba97b":"from math import sqrt\n\nrms = sqrt(mean_squared_error(y_test, y_pred))  \nprint(rms ) ","b8527327":"import matplotlib.pyplot as plt\n\nxgb.plot_tree(model_final,num_trees=0)\nplt.rcParams['figure.figsize'] = [100, 50]\nplt.show()\n","1f691860":"xgb.plot_tree(model_final, num_trees=0, rankdir='LR')","9e38fd8d":"#feature importance \nxgb.plot_importance(model_final)\nplt.show()","8435b382":"#predict  score for test data\ndel test['first_active_month'] \n\nlbe_test = LabelEncoder()\nlbe_test = lbe_test.fit(test['card_id'])\ncard_id = lbe_test.transform(test['card_id']) \ntest['card_id'] = card_id \ntest['card_id'].head() \n\n\nfinal_pred = model_final.predict(test)\n\n","2f767c14":"test2 = read_data('..\/input\/test.csv')\n\nsub_df = pd.DataFrame({\"card_id\":test2[\"card_id\"].values})\nsub_df[\"target\"] = final_pred\nsub_df.to_csv(\"submit.csv\", index=False)","dd153c06":"<a id=\"5\"><\/a> <br>\n## 5. Submission\n","88248762":"We then load the main files, formatting the dates and extracting the target:","77958533":"<a id=\"2\"><\/a> <br>\n## Feature engineering\n","48896769":"<a id=\"3\"><\/a> <br>\n## 3. Training the model\nWe now train the model with the features we previously defined. A first step consists in merging all the dataframes:","f1154a01":"Then I define two functions that aggregate the info contained in these two tables. The first function aggregates the function by grouping on `card_id`:"}}