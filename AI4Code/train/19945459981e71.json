{"cell_type":{"6c6d87e1":"code","d6eaab4e":"code","e4ed17f4":"code","0d3485ac":"code","bfe64869":"code","750a329e":"code","f8facf2f":"code","a5a20650":"code","118c25d6":"code","44e9e5df":"code","452b94a1":"code","45125d87":"code","dadac8d1":"code","61d2b3f9":"code","0fdc4e9e":"code","6c06ea33":"code","b5ce7e54":"code","82c6b31d":"code","1b907f8d":"code","92d9bc08":"code","90d1e951":"code","55c68459":"code","2fefcd51":"code","6a154729":"code","c6e79b54":"code","9427af8b":"code","b6081d69":"code","6bede13c":"code","ed74c133":"code","0d3c85d5":"code","36e036db":"code","393ab314":"code","00c38a80":"code","a46be456":"code","acbe7632":"code","81c6c067":"code","b6d060bd":"markdown","3f340efe":"markdown","3c573889":"markdown","cb2fdf6a":"markdown","ca4bf6c4":"markdown","fa9acd8a":"markdown","67b845aa":"markdown","99cf369c":"markdown","d084cf76":"markdown","7dad8115":"markdown","ac6d15be":"markdown","f71c6025":"markdown","e3ccef6e":"markdown","7ebabbd4":"markdown","5ce82f6e":"markdown","bcfbd894":"markdown","c23ae471":"markdown","2b24c039":"markdown"},"source":{"6c6d87e1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\nfrom transformers import AutoTokenizer, BertTokenizer, TFAutoModel, TFBertModel\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nimport os\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"","d6eaab4e":"df_train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ndf_test = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')","e4ed17f4":"df_train.head()","0d3485ac":"df_train.isnull().sum()","bfe64869":"df_test.head()","750a329e":"df_test.isnull().sum()","f8facf2f":"train_language_cnt = df_train['language'].value_counts(sort=False)\ntest_language_cnt = df_test['language'].value_counts(sort=False)[train_language_cnt.index]\nfig,axes = plt.subplots(nrows=1,ncols=2,figsize=(15,8),sharey=True)\nfor i,cnt in enumerate([train_language_cnt,test_language_cnt]):\n    axes[i].barh(cnt.index,cnt.values,\n            height=0.5, edgecolor='darkgrey',\n            color=sns.color_palette()[i],alpha=0.7)\n    axes[i].set_yticklabels(cnt.index,fontsize=15)\n    axes[i].set_xticks([])\n\n    for j in axes[i].patches:\n        axes[i].text(j.get_width()+.5, j.get_y()+.2,\n                str(j.get_width()), \n                fontsize=13, color='black')\n\n    for pos in ['left','right','bottom','top']:\n        axes[i].spines[pos].set_color(None)\n\n    #axes[i].grid(axis='x',linestyle='-',alpha=0.4)\n    #axes[i].invert_yaxis()\nfig.text(0.13, 0.95, 'Train vs Test: Language Count', fontsize=15, fontweight='bold') ","a5a20650":"label_cnt = df_train['label'].value_counts()\ntrace = go.Pie(labels = label_cnt.index, \n               values = label_cnt.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Label',\n               titlefont = dict(size=15),\n               hole = 0.3,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","118c25d6":"fig = make_subplots(rows=5, cols=3, specs=[[{\"type\": \"domain\"}]*3]*5,\n                    horizontal_spacing=0.005,vertical_spacing=0.01)\nfor i,lang in enumerate(list(train_language_cnt.index)): \n    label_cnt = df_train[df_train.language == lang]['label'].value_counts()\n    fig.add_trace(go.Pie(labels = label_cnt.index, \n                   values = label_cnt.values,\n                   hoverinfo = 'percent+value+label',\n                   textinfo = 'percent',\n                   textposition = 'inside',\n                   textfont = dict(size=14),\n                   title = lang,\n                   titlefont = dict(size=15),\n                   hole = 0.5,\n                   showlegend = True,\n                   marker = dict(line=dict(color='black',width=2))),\n                   row=divmod(i,3)[0]+1,col=divmod(i,3)[1]+1)\nfig.update_layout(height=750, width=750)\nfig.show()","44e9e5df":"df_train['premise_word_cnt'] = df_train['premise'].apply(lambda x: len(x.split()))\ndf_train['hypothesis_word_cnt'] = df_train['hypothesis'].apply(lambda x: len(x.split()))\n\nfig,ax = plt.subplots(nrows=5,ncols=3,figsize=(15,20))\nfor i,lang in enumerate(list(train_language_cnt.index)):\n    sns.distplot(df_train[df_train.language == lang]['premise_word_cnt'],bins=20,\n                 color='red',label='Premise',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    sns.distplot(df_train[df_train.language == lang]['hypothesis_word_cnt'],bins=20,\n                 color='blue',label='Hypothesis',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_title(lang,fontsize=13)\n    ax[divmod(i,3)[0],divmod(i,3)[1]].legend()\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_xlabel('')\nfig.text(0.13, 0.95, 'Train: Premise & Hypothesis Word Count', fontsize=15, fontweight='bold') ","452b94a1":"df_test['premise_word_cnt'] = df_test['premise'].apply(lambda x: len(x.split()))\ndf_test['hypothesis_word_cnt'] = df_test['hypothesis'].apply(lambda x: len(x.split()))\n\nfig,ax = plt.subplots(nrows=5,ncols=3,figsize=(15,20))\nfor i,lang in enumerate(list(train_language_cnt.index)):\n    sns.distplot(df_test[df_test.language == lang]['premise_word_cnt'],bins=20,\n                 color='yellow',label='Premise',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    sns.distplot(df_test[df_test.language == lang]['hypothesis_word_cnt'],bins=20,\n                 color='green',label='Hypothesis',kde=False, ax=ax[divmod(i,3)[0],divmod(i,3)[1]])\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_title(lang,fontsize=13)\n    ax[divmod(i,3)[0],divmod(i,3)[1]].legend()\n    ax[divmod(i,3)[0],divmod(i,3)[1]].set_xlabel('')\nfig.text(0.13, 0.95, 'Test: Premise & Hypothesis Word Count', fontsize=15, fontweight='bold')","45125d87":"def encode_sentence(s):\n    tokens = list(tokenizer.tokenize(s))\n    tokens.append('[SEP]')\n    return tokenizer.convert_tokens_to_ids(tokens)","dadac8d1":"def bert_encode(premise, hypothesis, tokenizer):\n    \n    sentence1 = tf.ragged.constant([encode_sentence(s) for s in np.array(premise)])\n    sentence2 = tf.ragged.constant([encode_sentence(s) for s in np.array(hypothesis)])\n    \n    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])]*(sentence1.shape[0])\n    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n\n    input_masks = tf.ones_like(input_word_ids).to_tensor()\n\n    type_cls = tf.zeros_like(cls)\n    type_s1 = tf.zeros_like(sentence1)\n    type_s2 = tf.ones_like(sentence2)\n    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n\n    inputs = {\n      'input_word_ids': input_word_ids.to_tensor(),\n      'input_masks': input_masks,\n      'input_type_ids': input_type_ids}\n    \n    return inputs","61d2b3f9":"tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\ntrain_input = bert_encode(df_train.premise.values, df_train.hypothesis.values, tokenizer)\n#train_label = tf.keras.utils.to_categorical(df_train.label.values)","0fdc4e9e":"K.clear_session()","6c06ea33":"def build_bert_baseline(max_len=80):\n    bert_layer = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n    input_word_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_word_ids')\n    input_masks = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_masks')\n    input_type_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_type_ids')\n    \n    sequence_output = bert_layer([input_word_ids, input_masks, input_type_ids])[0]\n    #sequence_output = clf_output[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(sequence_output[:,0,:])\n    \n    model = tf.keras.Model(inputs=[input_word_ids,input_masks,input_type_ids],outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","b5ce7e54":"model = build_bert_baseline(max_len=80)\n#for layer in model.layers:\n    #print(layer.output_shape)\n    \nfilepath='best_weight.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                save_best_only=True, save_weights_only=True, \n                                                mode='min',save_freq = 'epoch')\n#model.fit(train_input, df_train.label.values, epochs =3, \n#          batch_size = 16, callbacks=[checkpoint], validation_split=0.2)","82c6b31d":"K.clear_session()","1b907f8d":"def build_bert_LSTM_head(max_len=80):\n    bert_layer = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n    input_word_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_word_ids')\n    input_masks = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_masks')\n    input_type_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_type_ids')\n    \n    sequence_output = bert_layer([input_word_ids, input_masks, input_type_ids])[0]\n    bi_lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(dropout)\n     \n    model = tf.keras.Model(inputs=[input_word_ids,input_masks,input_type_ids],outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=3e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","92d9bc08":"bert_lstm_model = build_bert_LSTM_head(max_len=80)\n#for layer in model.layers:\n    #print(layer.output_shape)\n    \nfilepath='LSTM_best_weight.hdf5'\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                save_best_only=True, save_weights_only=True, \n                                                mode='min',save_freq = 'epoch')\n#bert_lstm_model.fit(train_input, df_train.label.values, epochs =2, \n#          batch_size = 16, callbacks=[checkpoint], validation_split=0.2)","90d1e951":"K.clear_session()","55c68459":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","2fefcd51":"def roberta_encode(text,tokenizer,max_len=80):\n    encoded_text = tokenizer.batch_encode_plus(text,\n                                               pad_to_max_length=True,\n                                               max_length=max_len)\n    return encoded_text","6a154729":"tokenizer = AutoTokenizer.from_pretrained('jplu\/tf-xlm-roberta-large')\n\ntrain_text = df_train[['premise', 'hypothesis']].values.tolist()\nencoded_train_text = roberta_encode(train_text,tokenizer,max_len=80)","c6e79b54":"x_train, x_valid, y_train, y_valid = train_test_split(\n    encoded_train_text['input_ids'], \n    df_train.label.values, \n    test_size=0.2, random_state=98)","9427af8b":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync\nAUTO = tf.data.experimental.AUTOTUNE","b6081d69":"def create_tf_dataset(X, y,val,batch_size= BATCH_SIZE,auto=AUTO):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((X,y)).shuffle(98)\n          \n    if not val:\n        dataset = dataset.repeat().batch(batch_size).prefetch(auto)\n    else:\n        dataset = dataset.batch(batch_size).prefetch(auto)\n\n    return dataset","6bede13c":"tf_train_dataset = create_tf_dataset(x_train,y_train,val=False)\ntf_val_dataset = create_tf_dataset(x_valid,y_valid,val=True)","ed74c133":"tf_train_dataset,tf_val_dataset","0d3c85d5":"def build_roberta(max_len=80):\n    roberta_layer = TFAutoModel.from_pretrained('jplu\/tf-xlm-roberta-large')\n    input_ids = tf.keras.Input(shape = (max_len,),dtype =tf.int32, name='input_ids')\n    \n    sequence_output = roberta_layer(input_ids)[0]\n    #sequence_output = clf_output[0]\n    output = tf.keras.layers.Dense(3, activation='softmax')(sequence_output[:,0,:])\n    \n    model = tf.keras.Model(inputs=input_ids, outputs=output)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5),\n                  loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n    print(model.summary())\n    return model","36e036db":"with strategy.scope():    \n    roberta_model = build_roberta(max_len=80)\n    #for layer in model.layers:\n        #print(layer.output_shape)\n\n    filepath='roberta_best_weight.hdf5'\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                    save_best_only=True, save_weights_only=True, \n                                                    mode='min',save_freq = 'epoch')\n    #roberta_model.fit(tf_train_dataset, epochs =10, \n              #batch_size = 16, callbacks=[checkpoint], validation_data=tf_val_dataset)\n    n_steps = len(x_train)\/\/BATCH_SIZE\n    roberta_model.fit(tf_train_dataset,steps_per_epoch=n_steps,\n                      validation_data=tf_val_dataset,callbacks=[checkpoint],epochs=10)","393ab314":"def model_cv_score(model):    \n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=98)\n    E_cv = 0\n    for fold,(train_idx,val_idx) in enumerate(skf.split(df_train['premise'],df_train['label'])):\n        print('\\nFold %s:'% (fold))\n        K.clear_session()\n\n        train_premise = df_train['premise'].iloc[train_idx]\n        train_hypothesis = df_train['hypothesis'].iloc[train_idx]\n        train_label = df_train['label'].iloc[train_idx]\n\n        val_premise = df_train['premise'].iloc[val_idx]\n        val_hypothesis = df_train['hypothesis'].iloc[val_idx]\n        val_label = df_train['label'].iloc[val_idx]\n        \n        ## use encode method you define, this method is for bert base and \n        ## subject to change when switched to roBerta\n        cv_train_input = bert_encode(train_premise.values, train_hypothesis.values, tokenizer)\n        cv_val_input = bert_encode(val_premise.values, val_hypothesis.values, tokenizer)\n\n        #model = build_bert(max_len=55)\n        filepath='best_weight_fold%s.h5'%(fold)\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, \n                                                    save_best_only=True, save_weights_only=True, \n                                                    mode='min',save_freq = 'epoch')\n        model.fit(cv_train_input, train_label.values, epochs =2, \n              batch_size = 16, callbacks=[checkpoint], \n              validation_data=(cv_val_input,val_label.values))\n\n        print('\\nPredicting OOF Error......')\n        model.load_weights('best_weight_fold%s.h5'%(fold))\n        val_pred = [np.argmax(p) for p in model.predict(cv_val_input)]\n        oof_error = np.sum([pred!=true for pred,true in zip(val_pred,val_label)])\/len(val_label)\n        print('Fold_%s Error: %s' % (fold,oof_error))\n        E_cv += oof_error\n\n    print('\\nE_cv = %s'%(E_cv\/5))","00c38a80":"#test_input = bert_encode(df_test.premise.values, df_test.hypothesis.values, tokenizer)\n#model.load_weights('.\/best_weight_fold0.h5')\n#pred = [np.argmax(res) for res in model.predict(test_input)]","a46be456":"test_text = df_test[['premise', 'hypothesis']].values.tolist()\nencoded_test_text = roberta_encode(test_text,tokenizer,max_len=80)\ntf_test_dataset = (tf.data.Dataset\n                .from_tensor_slices(encoded_test_text['input_ids'])\n                .batch(BATCH_SIZE))\nroberta_model.load_weights('.\/roberta_best_weight.hdf5')\npred = [np.argmax(res) for res in roberta_model.predict(tf_test_dataset)]","acbe7632":"submission = pd.DataFrame({'id':df_test.id,'prediction':pred})\nsubmission.to_csv('submission.csv',index=False)","81c6c067":"submission.head(10)","b6d060bd":"The distribution of train labels is quite balanced on the whole and in differnt languages.","3f340efe":"# Build Bert with LSTM Head and Concat Pooling Result","3c573889":"# Submission","cb2fdf6a":"Unbiased prediction of out of sample error for bert baseline is 0.35825. The best model performance is fold 0.","ca4bf6c4":"# XLM-roBerta-Large","fa9acd8a":"English take the dominant place in both train and test data.","67b845aa":"# Encode Input For Bert Base","99cf369c":"Thanks to the encoding method for Bert base in toturial [notebook](https:\/\/www.kaggle.com\/anasofiauzsoy\/tutorial-notebook)","d084cf76":"# Build Bert Baseline","7dad8115":"The max_len for sentences before encoding is around 60 so the in encoding phase, sequence length should be more than this value.","ac6d15be":"Initialize TPU","f71c6025":"Model idea from [here](https:\/\/keras.io\/examples\/nlp\/semantic_similarity_with_bert\/)","e3ccef6e":"Thanks to the [notebook](https:\/\/www.kaggle.com\/xhlulu\/contradictory-watson-concise-keras-xlm-r-on-tpu) as a complete guidance on TPU configuration and XLM-roBerta-Large fine tuning","7ebabbd4":"No null values in both train and test data","5ce82f6e":"# Cross Validation Score","bcfbd894":"In both training and test data, word count of hypothesis is less than that of premise for all the languages.","c23ae471":"The model performance did not improve after increasing complexity compared with baseline Bert model. Hence, There are two choices for me to boosting my results. The first is to do some translation augmentation and train the model again. The second is to switch to roBerta model.","2b24c039":"# Data Overview and EDA"}}