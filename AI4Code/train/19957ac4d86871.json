{"cell_type":{"5b2e8a37":"code","49f6604b":"code","ef2140e6":"code","dc10fd5f":"code","77b3c240":"code","3a67ef81":"code","90dd07bb":"code","3a20324f":"code","57a7eba7":"code","4a9d9028":"code","87cde556":"code","37b3d642":"code","bc18dce5":"code","2790d791":"code","dce0ebcf":"code","4953bb1e":"code","b8acb148":"code","9882b6f7":"code","409e630e":"code","a3062145":"code","1eb9ba0b":"code","e3bab6fc":"code","e80cb7ae":"code","9024c466":"code","6c4bfac7":"code","51da5dd1":"code","bcb7fd56":"code","521bcdf1":"code","b33b053f":"code","c0485dc7":"code","4ca38582":"code","3c023dd1":"code","ccdd2b17":"code","29f1db65":"code","21270f96":"code","8816e135":"code","3bce955a":"code","002cccf8":"code","062ce0ab":"code","b541c1b6":"code","ad14cc4d":"code","1b212acf":"code","04e20820":"code","5ffae39c":"code","1aa9c112":"code","ff1b6c56":"code","2d58d17c":"code","caa6acad":"code","39bcce63":"code","2471d5ef":"code","5cf6b987":"code","7544ac21":"code","67f34ec9":"code","fe47b102":"code","1d0143be":"code","8b4e7680":"code","085d55fb":"code","c71a4c4d":"code","8fa377e6":"code","09792bd5":"code","b0ab4d6f":"code","d74d7110":"code","03fb41da":"code","7c9f0dc4":"code","c65aad10":"code","31210ef4":"code","279eb48a":"code","768e63bf":"code","a937a96f":"code","4e6711a6":"code","98950106":"code","8ca67e02":"code","745fcdc7":"code","8044763c":"code","30a07b93":"code","6b1aa81e":"code","fc78d1f2":"code","65c587f8":"code","fcea7321":"code","56b64a56":"code","2d89efa8":"code","6f4485b8":"code","af379adf":"code","6a733d09":"code","99626022":"code","87273d42":"code","e440d4f7":"code","665f1fcf":"code","18066eea":"code","89b9a628":"code","c88d19aa":"code","8b890c96":"code","e5a51430":"code","5570b7bf":"code","2c7795a5":"code","599e498d":"code","1ade137a":"code","867ead16":"code","34059f47":"code","98d85cc1":"code","6fb2c2e8":"code","e011077c":"code","903f0d15":"code","c84ff694":"code","8068bbbd":"code","d0bc189c":"markdown","ea80012f":"markdown","1d5f6e1d":"markdown","9f2fb8cc":"markdown","2e8bbabb":"markdown","f9a06122":"markdown","e63642fa":"markdown","db25712c":"markdown","57d7bca5":"markdown","f0cf9693":"markdown","b181a7e5":"markdown","2ecd8c0a":"markdown","f8b2a611":"markdown","57bb4186":"markdown","47477ada":"markdown","ad89f562":"markdown","ef800cc3":"markdown","a3ccf2c6":"markdown","f2f30137":"markdown","6e218c89":"markdown","d1e9ee7e":"markdown","2d71a838":"markdown","0c912cca":"markdown","f9bc00a9":"markdown","ca11529b":"markdown","08be4270":"markdown","fce78205":"markdown","e0491512":"markdown","fa8ccb2e":"markdown","ffb7113f":"markdown","72e7c03c":"markdown","aaa14a56":"markdown","38a518de":"markdown","95fd9dee":"markdown","ca722bc8":"markdown","92209071":"markdown","47e4910f":"markdown","aaa2451f":"markdown","8457a658":"markdown","9c337fff":"markdown","d26aff08":"markdown"},"source":{"5b2e8a37":"# Importa\u00e7\u00e3o das bibliotecas necess\u00e1rias e leitura da base\nimport numpy as np # realiza\u00e7\u00e3o de calculos computacionais\nimport pandas as pd # manipula\u00e7\u00e3o dos dados\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv('\/kaggle\/input\/hmeq-data\/hmeq.csv')\ndf2 = df.copy()","49f6604b":"# Seleciona uma amostra com 10 observa\u00e7\u00f5es da base e as mostra na tela\ndisplay(df.sample(10).T)\n\n# Informa a quantidade de linhas e colunas respectivamente\ndisplay(df.shape)","ef2140e6":"# Informa\u00e7\u00f5es b\u00e1sicas da base como quantidade de colunas e seus respectivos nomes, quantidade des observa\u00e7\u00f5es n\u00e3o nulas e seus respectivos tipos\ndf.info()","dc10fd5f":"def missing_values(data_frame):\n    \"\"\"\n    Fun\u00e7\u00e3o respons\u00e1vel por mostrar quantos \n    valores missing h\u00e1 na base em cada \n    coluna\n    \"\"\"\n    display(data_frame.isnull().sum().rename_axis('Colunas').reset_index(name='Missing Values'))\n\n\n#chamada da fun\u00e7\u00e3o\nmissing_values(df)","77b3c240":"'''\nseleciona as vari\u00e1veis n\u00famericas da base para uma primeira an\u00e1lise, \ndeixando de fora apenas a var\u00edavel target \"BAD\", pois pelo fato da mesma \nser bin\u00e1ria, vizulizaremos-a melhor posteriormente em outro gr\u00e1fico\n'''\nnumeric_feats = [c for c in df.columns if df[c].dtype != 'object' and c not in ['BAD']]\ndf_numeric_feats = df[numeric_feats]\n","3a67ef81":"# Cria um gr\u00e1fico de paridade relacionado cada uma das vari\u00e1veis entre si\nsns.pairplot(df_numeric_feats)","90dd07bb":"# Cria histogramas das vari\u00e1veis selecionadas anteriormente\ndf_numeric_feats.hist(figsize=(20,8), bins=30)\nplt.tight_layout() ","3a20324f":"'''\nCria uma sequ\u00eancia de gr\u00e1ficos relacionando \nas vari\u00e1veis n\u00famericas anteriormente \nselecionadas com a vari\u00e1vel target \"BAD\"\n'''\nplt.figure(figsize=(18,18))\nc = 1\nfor i in df_numeric_feats.columns:\n    if c < len(df_numeric_feats.columns):\n        plt.subplot(3,3,c)\n        sns.boxplot(x='BAD' , y= i, data=df)\n        c+=1\n    else:\n        sns.boxplot(x='BAD' , y= i, data=df)\nplt.tight_layout() ","57a7eba7":"# Cria um sequ\u00eancia de gr\u00e1ficos relacionando as var\u00e1veis n\u00famericas com a vari\u00e1vel \"JOB\".\nplt.figure(figsize=(18,18))\nc = 1\nfor i in df_numeric_feats.columns:\n    if c < len(df_numeric_feats.columns):\n        plt.subplot(3,3,c)\n        sns.boxplot(x='JOB' , y= i, data=df)\n        c+=1\n    else:\n        sns.boxplot(x='JOB' , y= i, data=df)\nplt.tight_layout() ","4a9d9028":"#Dropa os campos com valores faltantes na coluna \"JOB\", sem alterar o datafram original, e pega os poss\u00edveis valores nesta coluna.\njobs = df['JOB'].dropna().unique()\n\n#Cria uma s\u00e9rie de histogramas da v\u00e1riavel \"VALUE\" segmentando pela vari\u00e1vel \"JOB\"\nplt.figure(figsize=(14,15))\nc=1\nfor i in jobs:\n    plt.subplot(7,1,c)\n    plt.title(i)\n    df[df['JOB'] == i]['VALUE'].hist(bins=20)\n    c+=1\nplt.tight_layout() \n","87cde556":"# Motra um histograma dos valores presentes na vari\u00e1vel \"VALUE\" onde na coluna JOB est\u00e1 faltando dado\nprint(df[df['JOB'].isnull()]['VALUE'].hist(bins=20))","37b3d642":"#Cria uma s\u00e9rie de boxplot relacionando as vari\u00e1veis n\u00famericas com a vari\u00e1vel categ\u00f3rica \"REASON\".\nplt.figure(figsize=(18,18))\nc = 1\nfor i in df_numeric_feats.columns:\n    if c < len(df_numeric_feats.columns):\n        plt.subplot(3,3,c)\n        sns.boxplot(x='REASON' , y= i, data=df)\n        c+=1\n    else:\n        sns.boxplot(x='REASON' , y= i, data=df)\n","bc18dce5":"# Importando as bibliotecas necess\u00e1rias\nimport scipy.stats as stats\nfrom scipy.stats import ttest_ind","2790d791":"\n# Seleciona os valores da vari\u00e1vel \"VALUE\" onde o valor da varo\u00e1vel \"REASON\" e igual a \"HomeImp\"\ndf_reason_homeimp = df[df['REASON']=='HomeImp']['VALUE']\n# Seleciona os valores da vari\u00e1vel \"VALUE\" onde o valor da varo\u00e1vel \"REASON\" e igual a \"DebtCon\"\ndf_reason_debtcon = df[df['REASON']=='DebtCon']['VALUE']\n\n# teste Shapiro-Wilk (Normalidade) para o subconjunto da vari\u00e1vel \"VALUE\" onde o valor da vari\u00e1vel \"REASON\" e igual a \"HomeImp\"\nshapiro_stat_reason_homeimp, shapiro_p_valor_reason_homeimp = stats.shapiro(df_reason_homeimp)\n# teste Shapiro-Wilk (Normalidade) para o subconjunto da vari\u00e1vel \"VALUE\" onde o valor da vari\u00e1vel \"REASON\" e igual a \"DebtCon\"\nshapiro_stat_reason_debtcon, shapiro_p_valor_reason_debtcon = stats.shapiro(df_reason_debtcon)\n\n# Mostra o P valor do teste de normalidade\nprint('teste de normalidade')\nprint('reason homeimp: {}'.format(shapiro_p_valor_reason_homeimp))\nprint('reason_debtcon: {}'.format(shapiro_p_valor_reason_debtcon))","dce0ebcf":"# Seleciona os valores da vari\u00e1vel \"VALUE\" onde o valor da varo\u00e1vel \"BAD\" e igual a \"1\"\ndf_bad1 = df[df['BAD']== 1]['VALUE']\n# Seleciona os valores da vari\u00e1vel \"VALUE\" onde o valor da varo\u00e1vel \"BAD\" e igual a \"0\"\ndf_bad0 = df[df['BAD']== 0]['VALUE']\n\n# teste Shapiro-Wilk (Normalidade) da biblioteca scipy para o subconjunto da vari\u00e1vel \"VALUE\" onde o valor da vari\u00e1vel \"bAD\" e igual a \"1\"\nshapiro_stat_bad1,  shapiro_p_valor_bad1 = stats.shapiro(df_bad1)\n# teste Shapiro-Wilk (Normalidade) da biblioteca scipy para o subconjunto da vari\u00e1vel \"VALUE\" onde o valor da vari\u00e1vel \"bAD\" e igual a \"0\"\nshapiro_stat_bad0, shapiro_p_valor_bad0 = stats.shapiro(df_bad0)\n\n# Mostra o P valor do teste de normalidade\nprint('teste de normalidade')\nprint('Value com Bad igual a 1: {}'.format(shapiro_p_valor_bad1))\nprint('Value com Bad igual a 0: {}'.format(shapiro_p_valor_bad0))","4953bb1e":"# Seleciona os valores da vari\u00e1vel \"DEBTINC\" onde o valor da varo\u00e1vel \"BAD\" e igual a \"1\"\ndf_debtinc_bad1 = df[df['BAD']== 1]['DEBTINC']\n# Seleciona os valores da vari\u00e1vel \"DEBTINC\" onde o valor da varo\u00e1vel \"BAD\" e igual a \"0\"\ndf_debtinc_bad0 = df[df['BAD']== 0]['DEBTINC']\nshapiro_stat_bad1,  shapiro_p_valor_debtinc_bad1 = stats.shapiro(df_debtinc_bad1)\nshapiro_stat_bad0, shapiro_p_valor_debtinc_bad0 = stats.shapiro(df_debtinc_bad0)\n\n# Mostra o P valor do teste de normalidade\nprint('teste de normalidade')\nprint('Debtinc com Bad igual a 1: {}'.format(shapiro_p_valor_debtinc_bad1))\nprint('Debtinc com Bad igual a 0: {}'.format(shapiro_p_valor_debtinc_bad0))","b8acb148":"# remove os valores campos com valores faltantes, sem alterar o dataframe original e realiza o t-test atraves da fun\u00e7\u00e3o ttest_ind da biblioteca scipy\n_, ttest_p_value = ttest_ind(df_reason_homeimp.dropna(), df_reason_debtcon.dropna())\n\n# Mostra o P valor  do t-teste\nprint('T-teste: {:.4f}'.format(ttest_p_value))","9882b6f7":"# remove os valores campos com valores faltantes, sem alterar o dataframe original e realiza o t-test atraves da fun\u00e7\u00e3o ttest_ind da biblioteca scipy\n_, ttest_p_value_bad = ttest_ind(df_bad1.dropna(), df_bad0.dropna())\n\n# Mostr o P valor do teste\nprint('T-teste: {:.4f}'.format(ttest_p_value_bad))","409e630e":"# remove os valores campos com valores faltantes, sem alterar o dataframe original e realiza o t-test atraves da fun\u00e7\u00e3o ttest_ind da biblioteca scipy\n_, ttest_p_value_debtinc_bad = ttest_ind(df_debtinc_bad1.dropna(), df_debtinc_bad0.dropna())\n\n# Mostr o P valor do teste\nprint('T-teste: {:.4f}'.format(ttest_p_value_debtinc_bad))","a3062145":"# pegando os valores da vari\u00e1vel \"VALUE\" e agrupando por ocupa\u00e7\u00e3o \"JOB\"\nanova_value_by_job = {job:df['VALUE'][df['JOB'] == job] for job in jobs}\n\n# realizando o teste de an\u00e1lise de vari\u00e1cia\n_, anova_value_job_p = stats.f_oneway(anova_value_by_job['Other'].dropna(),\n                                          anova_value_by_job['Office'].dropna(),\n                                          anova_value_by_job['Sales'].dropna(),\n                                          anova_value_by_job['Mgr'].dropna(),\n                                          anova_value_by_job['ProfExe'].dropna(), \n                                          anova_value_by_job['Self'].dropna())\n# Mostra o P value do teste\nprint('One Way Anova: {:.4f}'.format(anova_value_job_p))","1eb9ba0b":"# Calculando o primeiro quartil da vari\u00e1vel \"VALUE\"\nq1 = df['VALUE'].quantile(0.25)\n# Calculando o terceiro quartil da vari\u00e1vel \"VALUE\"\nq3 = df['VALUE'].quantile(0.75)\n\n# Calculando o IQR\niqr = q3 - q1 \n\n# Guardando domente os valores que n\u00e3o s\u00e3o considerados outliers \ndf_value_and_job_no_outlier = df[~((df['VALUE'] < (q1 - 1.5  * iqr)) | (df['VALUE']  > (q3 + 1.5 * iqr)))][['VALUE', 'JOB', 'BAD']]","e3bab6fc":"# Verificando se as medias continuao diferentes\nanova_value_by_job = {job:df_value_and_job_no_outlier['VALUE'][df_value_and_job_no_outlier['JOB'] == job] for job in jobs}\nanova_job_f, anova_job_p = stats.f_oneway(anova_value_by_job['Other'].dropna(),\n                                          anova_value_by_job['Office'].dropna(),\n                                          anova_value_by_job['Sales'].dropna(),\n                                          anova_value_by_job['Mgr'].dropna(),\n                                          anova_value_by_job['ProfExe'].dropna(), \n                                          anova_value_by_job['Self'].dropna())\n#Mostra o P valor do teste\nprint('One Way Anova: {:.4f}'.format(anova_job_p))","e80cb7ae":"sns.boxplot(x='JOB', y='VALUE', data=df_value_and_job_no_outlier)","9024c466":"\nanova_debtinc_by_job = {job:df['DEBTINC'][df['JOB'] == job] for job in jobs}\nanova_debtinc_f, anova_debtinc_p = stats.f_oneway(anova_debtinc_by_job['Other'].dropna(),\n                                          anova_debtinc_by_job['Office'].dropna(),\n                                          anova_debtinc_by_job['Sales'].dropna(),\n                                          anova_debtinc_by_job['Mgr'].dropna(),\n                                          anova_debtinc_by_job['ProfExe'].dropna(), \n                                          anova_debtinc_by_job['Self'].dropna())\n#Ao menos um dos Jobs tem valores diferentes entre si, estatisticamente falando\nprint('One Way Anova: {:.4f}'.format(anova_debtinc_p))","6c4bfac7":"# Selecionando o primeiro Quartil da vari\u00e1vel \"YOJ\"\nq1 = df['YOJ'].quantile(0.25)\n# Selecionando o segundo Quartil da vari\u00e1vel \"YOJ\"\nq3 = df['YOJ'].quantile(0.75)\n\n#Ralizando o calculo do iqr\niqr = q3 - q1\n\n#descartando os outliers e \ndf_yoj_and_job_no_outlier = df[~((df['YOJ'] < (q1 - 1.5  * iqr)) | (df['YOJ']  > (q3 + 1.5 * iqr)))][['YOJ', 'JOB']]","51da5dd1":"\nanova_yoj_by_job = {job:df_yoj_and_job_no_outlier['YOJ'][df_yoj_and_job_no_outlier['JOB'] == job] for job in jobs}\nanova_yoj_f, anova_yoj_p = stats.f_oneway(anova_yoj_by_job['Other'].dropna(),\n                                          anova_yoj_by_job['Office'].dropna(),\n                                          anova_yoj_by_job['Sales'].dropna(),\n                                          anova_yoj_by_job['Mgr'].dropna(),\n                                          anova_yoj_by_job['ProfExe'].dropna(), \n                                          anova_yoj_by_job['Self'].dropna())\n#Ao menos um dos Jobs tem valores diferentes entre si, estatisticamente falando\nprint('One Way Anova: {:.4f}'.format(anova_yoj_p))","bcb7fd56":"sns.boxplot(x='JOB', y= 'YOJ', data=df_yoj_and_job_no_outlier)","521bcdf1":"# Salvando as m\u00e9dias da vari\u00e1vel VALUE por ocupa\u00e7\u00e3o\nvalue_mean_by_job = df_value_and_job_no_outlier.groupby(['JOB', 'BAD'])['VALUE'].mean()\n\n# instancia um objeto pandas series sem conte\u00fado.\nimp_value = pd.Series([]) \n\n'''\nreseta o idice do data frame para garantir \nque cada itera\u00e7\u00e3o verifique um \u00eddice do \ndataframe evitando com que observa\u00e7\u00f5es \nfiquem sem ser verificadas.\n'''\ndf.reset_index()\n'''\nitera sobre o dataframe e, caso valor do \ncampo \"VALUE\" esteja nulo, verifica a \nocupa\u00e7\u00e3o do indiv\u00edduo e coloca a m\u00e9dia de \n\"VALUE\" para aquele \"JOB\" naquela posi\u00e7\u00e3o \ndentro de um objeto Series, caso \"VALUE\"\nn\u00e3o seja nulo, atribui ao objeto o pr\u00f3prio\nvalor de \"VALUE\"\n'''\nfor i in range(len(df)):\n    if df['VALUE'][i] != df['VALUE'][i]:\n        if df['JOB'][i] == 'Mgr':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] = value_mean_by_job['Mgr'][0]\n            else:\n                imp_value[i] = value_mean_by_job['Mgr'][1]\n        if df['JOB'][i] == 'Office':\n            if df['BAD'][i] == 0:\n                imp_value[i] = value_mean_by_job['Office'][0]\n            else:\n                imp_value[i] = value_mean_by_job['Office'][1]\n        if df['JOB'][i] == 'Other':\n            if df['BAD'][i] == 0:\n                imp_value[i] = value_mean_by_job['Other'][0]\n            else:\n                imp_value[i] = value_mean_by_job['Other'][1]\n        if df['JOB'][i] == 'ProfExe':\n            if df['BAD'][i] == 0:\n                imp_value[i] = value_mean_by_job['ProfExe'][0]\n            else:\n                imp_value[i] = value_mean_by_job['ProfExe'][1]\n        if df['JOB'][i] == 'Sales':\n            if df['BAD'][i] == 0:\n                imp_value[i] = value_mean_by_job['Sales'][0]\n            else:\n                imp_value[i] = value_mean_by_job['Sales'][1]\n        if df['JOB'][i] == 'Self':\n            if df['BAD'][i] == 0:\n                imp_value[i] = value_mean_by_job['Self'][0]\n            else:\n                imp_value[i] = value_mean_by_job['Self'][1]\n            \n    else: \n        imp_value[i] = df['VALUE'][i]\n'''\ncasi j\u00e1 exista alguma coluna com o nome IMP_VALUE\nrealiza a exclus\u00e3o o mesmo\n'''\nif \"IMP_VALUE\" in np.array(df.columns):\n    df.drop(\"IMP_VALUE\", axis=1, inplace=True)\n    \n# Inserie o objeto no dataframe como uma coluna\ndf.insert(13, \"IMP_VALUE\", imp_value) \ndf.head().T","b33b053f":"# Seleciona as observa\u00e7\u00f5es do dataframe onde a coluna \"IMP_VALUE\" apresenta valores faltantes\ndf[df['IMP_VALUE'].isnull()]","c0485dc7":"# Descarta todas as observa\u00e7\u00f5es que possuam mais que 10 campos com valores faltantes.\ndf.dropna(thresh=10, inplace=True)\n# Mostra a estrutura do dataframe\ndf.shape","4ca38582":"missing_values(df)","3c023dd1":"# Seleciona as observa\u00e7\u00f5es do dataframe onde a coluna \"IMP_VALUE\" apresenta valores faltantes\ndf[df['IMP_VALUE'].isnull()]","ccdd2b17":"df.dropna(axis=0,subset=['IMP_VALUE'], inplace=True)\ndf.shape","29f1db65":"df.drop('VALUE', axis=1, inplace=True)","21270f96":"df.head()","8816e135":"missing_values(df)","3bce955a":"df.dropna(axis=0, subset=['JOB'], inplace=True)\ndf.dropna(axis=0, subset=['REASON'], inplace=True)","002cccf8":"df.shape","062ce0ab":"missing_values(df)","b541c1b6":"df[['IMP_VALUE', 'MORTDUE']].corr()","ad14cc4d":"plt.scatter(df['IMP_VALUE'], df['MORTDUE'])\nplt.ylabel('IMP_VALUE')\nplt.xlabel('MORTDUE')\nplt.show()","1b212acf":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics","04e20820":"missing_mortdue = df[df['MORTDUE'].isnull()][['IMP_VALUE', 'MORTDUE']]\nnot_missing_mortdue = df[df['MORTDUE'].notnull()][['IMP_VALUE', 'MORTDUE']]","5ffae39c":"X = not_missing_mortdue['IMP_VALUE'].values.reshape(-1, 1)\ny = not_missing_mortdue['MORTDUE'].values.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","1aa9c112":"lr = LinearRegression()\nlr.fit(X_train, y_train)","ff1b6c56":"mortdue_pred = lr.predict(X_test)","2d58d17c":"real_vs_pred = pd.DataFrame({'Real': y_test.flatten(), 'Predito': mortdue_pred.flatten()})\nreal_vs_pred.sample(20)","caa6acad":"plt.figure(figsize=(10,10))\nplt.scatter(X_test, y_test, color='gray')\nplt.plot(X_test, mortdue_pred, color='red', linewidth=2)\nplt.show()","39bcce63":"print('Raiz quadrada do Erro medio ao quadrado: {}'.format(np.sqrt(metrics.mean_squared_error(y_test, mortdue_pred))))\nprint('R quadrado: {}'.format(metrics.r2_score(y_test, mortdue_pred)))","2471d5ef":"# trantando outliers para tentar diminuir o erro.\n# calculando o iqr\nq1 = not_missing_mortdue.quantile(0.25)\nq3 = not_missing_mortdue.quantile(0.75)\n\niqr = q3-q1\n\nprint(iqr)\nnot_missing_and_outliers_mortdue = not_missing_mortdue[~((not_missing_mortdue < (q1 - 1.5  * iqr)) | (not_missing_mortdue > (q3 + 1.5 * iqr))).any(axis=1)]","5cf6b987":"not_missing_mortdue.shape","7544ac21":"not_missing_and_outliers_mortdue.shape","67f34ec9":"X_no_outliers = not_missing_and_outliers_mortdue['IMP_VALUE'].values.reshape(-1, 1)\ny_no_outliers = not_missing_and_outliers_mortdue['MORTDUE'].values.reshape(-1, 1)\nX_no_outliers_train, X_no_outliers_test, y_no_outliers_train, y_no_outliers_test = train_test_split(X_no_outliers, y_no_outliers, test_size=0.20, random_state=42)","fe47b102":"lr_no_outliers = LinearRegression()\nlr_no_outliers.fit(X_no_outliers_train, y_no_outliers_train)","1d0143be":"mortdue_pred = lr.predict(X_test)\nreal_vs_pred = pd.DataFrame({'Real': y_test.flatten(), 'Predito': mortdue_pred.flatten()})\nreal_vs_pred.sample(20)","8b4e7680":"print('Raiz quadrada do Erro medio ao quadrado: {:.2f}'.format(np.sqrt(metrics.mean_squared_error(y_test, mortdue_pred))))\nprint('R quadrado: {}'.format(metrics.r2_score(y_test, mortdue_pred)))","085d55fb":"plt.figure(figsize=(10,10))\nplt.scatter(X_test, y_test, color='gray')\nplt.plot(X_test, mortdue_pred, color='red', linewidth=2)\nplt.show()","c71a4c4d":"imp_mortdue = pd.Series([])\nimp_mortdue = lr.predict(df['IMP_VALUE'].values.reshape(-1,1))\nimp_mortdue","8fa377e6":"df.insert(13, 'IMP_MORTDUE', np.round(imp_mortdue, 2))\ndf.drop('MORTDUE', axis=1, inplace=True)\n","09792bd5":"df.head()","b0ab4d6f":"missing_values(df)","d74d7110":"sns.boxplot(x='JOB', y='DEBTINC', data=df)","03fb41da":"sns.boxplot(x='BAD', y='DEBTINC', data=df)","7c9f0dc4":"q1 = df['DEBTINC'].quantile(0.25)\nq3 = df['DEBTINC'].quantile(0.75)\n\niqr = q3 - q1\n\ndf_debtinc_and_job_no_outlier = df[~((df['DEBTINC'] < (q1 - 1.5  * iqr)) | (df['DEBTINC']  > (q3 + 1.5 * iqr)))][['DEBTINC', 'JOB', 'BAD']]\ndebtinc_mean_by_job = df_debtinc_and_job_no_outlier.groupby(['JOB', 'BAD'])['DEBTINC'].mean()\ndebtinc_mean_by_job\n","c65aad10":"debtinc_mean_by_job['Mgr'][1]","31210ef4":"imp_debtinc = pd.Series([]) \n\ndf.reset_index(inplace=True)\nfor i in range(len(df)):\n    if df['DEBTINC'][i] != df['DEBTINC'][i]:\n        if df['JOB'][i] == 'Mgr':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['Mgr'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['Mgr'][1]\n        if df['JOB'][i] == 'Office':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['Office'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['Office'][1]\n        if df['JOB'][i] == 'Other':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['Other'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['Other'][1]\n        if df['JOB'][i] == 'ProfExe':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['ProfExe'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['ProfExe'][1]\n        if df['JOB'][i] == 'Sales':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['Sales'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['Sales'][1]\n        if df['JOB'][i] == 'Self':\n            if df['BAD'][i] == 0:\n                imp_debtinc[i] =  debtinc_mean_by_job['Self'][0]\n            else:\n                imp_debtinc[i] =  debtinc_mean_by_job['Self'][1]\n    else: \n        imp_debtinc[i] = df['DEBTINC'][i]\n\nif \"IMP_DEBTINC\" in np.array(df.columns):\n    df.drop(\"IMP_DEBTINC\", axis=1, inplace=True)\n    \ndf.insert(13, \"IMP_DEBTINC\", imp_debtinc) \ndf.head().T","279eb48a":"df.shape","768e63bf":"missing_values(df)","a937a96f":"df.drop('DEBTINC', axis=1, inplace=True)","4e6711a6":"sns.boxplot(x='JOB', y='YOJ', data=df)","98950106":"yoj_mean_by_job = df_yoj_and_job_no_outlier.groupby(['JOB'])['YOJ'].mean()\nimp_yoj = pd.Series([]) \n\ndf.reset_index(inplace=True)\nfor i in range(len(df)):\n    if df['YOJ'][i] != df['YOJ'][i]:\n        if df['JOB'][i] == 'Mgr':\n            imp_yoj[i] =  yoj_mean_by_job['Mgr']\n        if df['JOB'][i] == 'Office':\n            imp_yoj[i] = yoj_mean_by_job['Office']\n        if df['JOB'][i] == 'Other':\n            imp_yoj[i] = yoj_mean_by_job['Other']\n        if df['JOB'][i] == 'ProfExe':\n            imp_yoj[i] = yoj_mean_by_job['ProfExe']\n        if df['JOB'][i] == 'Sales':\n            imp_yoj[i] = yoj_mean_by_job['Sales']\n        if df['JOB'][i] == 'Self':\n            imp_yoj[i] = yoj_mean_by_job['Self']\n    else: \n        imp_yoj[i] = df['YOJ'][i]\n        \nif \"IMP_YOJ\" in np.array(df.columns):\n    df.drop(\"IMP_YOJ\", axis=1, inplace=True)\n    \ndf.insert(13, \"IMP_YOJ\", imp_yoj) \ndf.head().T","8ca67e02":"df.drop('YOJ', axis=1, inplace=True)","745fcdc7":"missing_values(df)","8044763c":"# Em virtude a falta de tempo para as outra variaveis (DEROG,DELINQ, CLAGE e NINQ)  fora imputadas pela media \ndf.fillna(df.mean(), inplace=True)","30a07b93":"missing_values(df)","6b1aa81e":"# antes de ir para o modelo propriamente dito vou realizar um one hot encoding nas variaveis categoricas.\ndf = pd.get_dummies(data=df, columns=['JOB', 'REASON'])","fc78d1f2":"df.head()","65c587f8":"# Iniciando o modelo de predicao","fcea7321":"#pip install -U scikit-learn == 0.22.1\n","56b64a56":"#pip install -U imbalanced-learn","2d89efa8":"# Classes desbalanceadas, vamos rodar um modelo com as classes do jeito que estao.\ndf['BAD'].value_counts().plot(kind='bar')","6f4485b8":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.metrics import classification_report\n","af379adf":"feats = [c for c in df.columns if c not in ['BAD']]","6a733d09":"X = df[feats]\ny = df['BAD']","99626022":"param_grid_decision_tree = {\n    'criterion': ('gini', 'entropy'),\n    'splitter': ('best', 'random'),\n    'max_features': ('auto', 'sqrt', 'log2')\n}","87273d42":"grid_decision_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_decision_tree)","e440d4f7":"grid_decision_tree.fit(X,y)","665f1fcf":"print('\\nOs melhores parametros foram: \\n' + str(grid_decision_tree.best_params_))","18066eea":"cv_decision_tree = cross_val_score(grid_decision_tree, X, y, cv=10)\nprint(cv_decision_tree)\nprint(cv_decision_tree.mean())","89b9a628":"param_grid_random_forest = {\n    'criterion': ('gini', 'entropy'),\n    'max_features': ('log2', 'sqrt')\n}\n","c88d19aa":"grid_random_forest_classifier = GridSearchCV(RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, bootstrap = True, oob_score = True), param_grid_random_forest)\ngrid_random_forest_classifier.fit(X,y)","8b890c96":"print('\\nOs melhores parametros foram: \\n' + str(grid_random_forest_classifier.best_params_))","e5a51430":"cv_random_forest = cross_val_score(grid_random_forest_classifier, X, y, cv=10, n_jobs=-1)\nprint(cv_random_forest)\nprint(cv_random_forest.mean())","5570b7bf":"param_grid_gbost_classifier = {\n    'criterion': ['friedman_mse', 'mse', 'mae'],\n    'max_features': ('log2', 'sqrt')\n} ","2c7795a5":"grid_gradiente_boost_machine = GridSearchCV(GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42, n_jobs=-1), param_grid_gbost_classifier)\ngrid_gradiente_boost_machine.fit(X, y)","599e498d":"print('\\nOs melhores parametros foram: \\n' + str(grid_gradiente_boost_machine.best_params_))","1ade137a":"cv_gradiente_boost_machine = cross_val_score(grid_gradiente_boost_machine, X, y, cv=10)\nprint(cv_gradiente_boost_machine)\nprint(cv_gradiente_boost_machine.mean())","867ead16":"# realizando um over sapling da classe minoritaria\nsmt = SMOTE(sampling_strategy=0.80)\nX, y = smt.fit_sample(X,y)","34059f47":"y.value_counts().plot(kind='bar')","98d85cc1":"grid_decision_tree.fit(X, y)","6fb2c2e8":"cv_decision_tree = cross_val_score(grid_decision_tree, X, y, cv=10, n_jobs=-1)\nprint(cv_decision_tree)\nprint(cv_decision_tree.mean())","e011077c":"grid_random_forest_classifier.fit(X, y)","903f0d15":"cv_random_forest = cross_val_score(grid_random_forest_classifier, X, y, cv=10, n_jobs=-1)\nprint(cv_random_forest)\nprint(cv_random_forest.mean())","c84ff694":"grid_gradiente_boost_machine.fit(X, y)","8068bbbd":"cv_gradiente_boost_machine = cross_val_score(grid_gradiente_boost_machine, X, y, cv=10, n_jobs=-1)\nprint(cv_gradiente_boost_machine)\nprint(cv_gradiente_boost_machine.mean())","d0bc189c":"### <a id=\"2.3.2\"> 2.3.2. Teste de hip\u00f3tese <\/a>","ea80012f":"Nestes gr\u00e1ficos percebemos que enquanto algumas vari\u00e1veis est\u00e3o mais pr\u00f3ximos de uma distribui\u00e7\u00e3o normal outras se aproximam de uma distribui\u00e7\u00e3o poisson.","1d5f6e1d":"Assim como as anteriores e conforme mostrado nos g\u00e1ficos, estatisticamente h\u00e1 pelo menos uma das categorias que possui o valor m\u00e9dio na vari\u00e1vel \"YOJ\" diferente das demais.","9f2fb8cc":"# <a id='2'> 2. Explora\u00e7\u00e3o dos dados <\/a>","2e8bbabb":"## <a id='3.1'> 3.1. Imputa\u00e7\u00e3o dos dados <\/a>","f9a06122":"Considerando um $\\alpha $ = 0.05 n\u00e3o se discarta a hipotese nula, tendo em vista que o P Value resultado destes testes. Ou seja, estas distribui\u00e7\u00f5es se aproximam de uma distribui\u00e7\u00e3o normal.","e63642fa":"Agora que j\u00e1 foram atestadas as informa\u00e7\u00f5es que fora percebido na analise dos gr\u00e1ficos, estas ser\u00e3o utilizadas para o tratamento da base.\nO valor m\u00e9dio da vari\u00e1vel \"VALUE\" foi constatado que difera entre os diferentes tipos de ocupa\u00e7\u00e3o do indiv\u00edduo, vari\u00e1vel \"JOB\", e difere tamb\u00e9m entre os indiv\u00edduos adimplentes e inadimlentes, sendo assim um metodo conciso para para a imputa\u00e7\u00e3o dos dados seria pela m\u00e9dia de \"VALUE\" por \"JOB\" e BAD. ","db25712c":"## <a id='2.1'> 2.1. Vari\u00e1veis <\/a>","57d7bca5":"# <a id='1'> 1. Dados<\/a>\n","f0cf9693":"Houve mundan\u00e7a no resultado, contudo, ainda assim, considerando o alpha j\u00e1 estabelecido de 5%, a m\u00e9dia do valor da vari\u00e1vel \"VALUE\" em pelo menos uma das ocupa\u00e7\u00f5es e diferente das demais.","b181a7e5":"Considerando um $\\alpha $ = 0.05, n\u00e3o se regeita a hip\u00f3tese nula, sendo assim, as media dos valores da propriedades dos indiv\u00edduos que pegaram emprestimos para melhorias da propriedade (HomeImp) e a m\u00e9dia do valor da propriedade dos indiv\u00edduos que pegaram emprestimos para a consolida\u00e7\u00e3o de cr\u00e9ditos (DebtCon) n\u00e3o se diferem.","2ecd8c0a":"Considerando que a distribui\u00e7\u00e3\u00e7o destes dubconjuntos se aproxima de uma distribui\u00e7\u00e3o normal, a vari\u00e1ncia dos mesmos s\u00e3o parecidas e o tamanha das amostras s\u00e3o diferentes, ser\u00e1 realizado o T-Test para testar a diferen\u00e7a entre as m\u00e9dias.\n\n$${{t} = {\\frac{\\tilde{x}_{1} - \\tilde{x}_{2}}{{S}_{x_1 x_2} .\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}}}$$","f8b2a611":"Sendo assim, uma forma interessante de se imputar os dados neste caso seria atrav\u00e9s de uma regress\u00e3o linear.","57bb4186":"# <a id='3'> 3. Manipula\u00e7\u00e3o da base <\/a>","47477ada":"Para atestar estatisticamente algumas das informa\u00e7\u00f5es obtidas com a visualiza\u00e7\u00e3o dos dados anteriormente ser\u00e1 realizado alguns testes estat\u00edsticos.\n","ad89f562":"Considerando um $\\alpha $ = 0.05, podemos dizer que a m\u00e9dia de \"VALUE\" em pelo menos uma das ocupa\u00e7\u00f5es e diferente das demais.","ef800cc3":"Para a vari\u00e1vel \"YOJ\" ser\u00e1 realizado a mesma verifica\u00e7\u00e3o retirando os outliers.","a3ccf2c6":"Verificando agora a vari\u00e1vel \"DEBTINC\" pela ocupa\u00e7\u00e3o dos indiv\u00edduos, considerando o P valor ao menos um das ocupa\u00e7\u00f5es apresenta um valor m\u00e9dio diferente das demais para a vari\u00e1vel \"DEBTINC\".","f2f30137":"Vemos que alguns campos ainda ficaram com dados faltantes, pois o campo \"JOB\" tamb\u00e9m possui dados faltantes. Ao mesmo tempo \u00e9 poss\u00edve ver que existem algumas obsesrva\u00e7\u00f5es que preticamente s\u00f3 possuem o valor da vari\u00e1vel \"BAD\" e \"LOAN\", neste caso a melhor op\u00e7\u00e3o no momento foi dropar esta observa\u00e7\u00f5es que possuem mais que 10 campos missings.","6e218c89":"## <a id= '1.1'> 1.1. Vis\u00e3o geral da base<\/a>\nO dataset Home Equality (HMEQ), termo que se refere \u00e0 diferen\u00e7a entre o pre\u00e7o de mercado da propriedade e o saldo pendente de todos os \u00f4nus atrelados a ela, contem uma base de informa\u00e7\u00f5es sobre a performanse de 5960 emprestimos recentes voltados para a casa pr\u00f3pria. A vari\u00e1vel target (BAD) \u00e9 do tipo bin\u00e1ria e informa quando houve neglig\u00eancia no pagamento deste emprestimo. Esta realidade ocorreu em 1.890 dos casos (20%). Para cada indiv\u00edduo foram colhidas 12 vari\u00e1veis.","d1e9ee7e":"Analisando estes gr\u00e1ficos gerados pode-se a uma primeira vista ver que h\u00e1 id\u00edcios de correla\u00e7\u00e3o entre a vari\u00e1vel \"VALUE\" e \"MORTDUE\".","2d71a838":"Para este teste ser\u00e1 utilizado o teste de Shapiro\u2013Wilk\n\n$${{W} = {\\frac{(\\sum_{i=1}^{n} {a}_{i}{x}_{(i)})^{2}}{\\sum_{i=1}^{n}({x}^{i} - \\tilde{x})^{2}}}}$$","0c912cca":"### <a id=\"2.3.1\"> 2.3.1. Teste de Normalidade <\/a>","f9bc00a9":"Dentro das vari\u00e1veis com valores faltantes, temos as duas vari\u00e1veis qualititavas da base \"JOB\" e \"REASON\".\nTendo em vista as diferentes m\u00e9dias que as outras vari\u00e1veis tem quando agrupadas por estas vari\u00e1veis, seria interessante tentar imputar esta coluna utilizando um ensemble metodo como GBM ou Random Forest, contudo, por hora, foi decidido apenas dropar estas observa\u00e7\u00f5es.","ca11529b":"No int\u00faito de verificar se removendo os outliers estas m\u00e9dias continuariam resultando em um P valor baixo para a ANOVA, fora realizada a remo\u00e7\u00e3o dos outliers utilizando a regra do IQR (Inter Quartile Range) que diz que valores menores que 1.5 * Quartil 1 ou maiores que 1.5 * o Quartil 3 s\u00e3o considerados outliers.","08be4270":"N\u00e3o \u00e9 interessante utilizar os m\u00e9todos para o teste de m\u00e9dia entre duas vari\u00e1veis como o T test para testar  diferen\u00e7a entre muitas muitas vari\u00e1veis, pois isto se tornaria um processo muito oneroso. Para isso existe a analise de vari\u00e2cia (Analise Of Variance - ANOVA)\n","fce78205":"Considerando um $\\alpha $ = 0.05, regeita-se a hip\u00f3tese nula, sendo assim, as media dos valores da propriedades dos indiv\u00edduos que n\u00e3o foram inadimplentes e os que foram, se diferem.","e0491512":"## <a id=\"2.3\"> 2.3. Atestando algumas informa\u00e7\u00f5es <\/a>","fa8ccb2e":"* BAD (Realizou o pagamento do emprestimo \"0\", n\u00e3o realizou \"1\")\n* LOAN (Quantidade de emprestimo solicitado)\n* MORTDUE (Divida referente a uma hipot\u00e9ca j\u00e1 existente)\n* VALUE (Valor da propriedade atual)\n* REASON (Motivo do emprestimo, sendo DebtCon = Consolida\u00e7\u00e3o de d\u00e9bitos e HomeImp = Melhorias na propriedade)\n* JOB (Informa em qual das seis categorias ocupacionais est\u00e1 o indiv\u00edduo)\n* YOJ (Quantidade de anos no atual emprego)\n* DEROG (Quantidade de relat\u00f3rios depreciativos, os principais)\n* DELINQ (Quantidade de inadimpl\u00eancias em linhas de cr\u00e9dito) \n* CLAGE (Quantidade de meses desde a Trade Line mais antiga, trade line e respons\u00e1vel por gravar o comportamento de cr\u00e9ditos ao consumidor)\n* NINQ (Quantidade de linhas de cr\u00e9dito recentes)\n* CLNO (N\u00famero total de linhas de Cr\u00e9dito)\n* DEBTINC (Taxa de d\u00e9bitos que ainda est\u00e3o por vir)","ffb7113f":"Anteriormente durante a an\u00e1lise gr\u00e1fico percebeu-se uma certa correla\u00e7\u00e3o entre a vari\u00e1vel \"VALUE\" agora \"IMP_VALUE\" e a vari\u00e1vel \"MORTDUE\".","72e7c03c":"Agora com a vari\u00e1vel \"IMP_VALUE\", a vari\u00e1vel \"VALUE\" n\u00e3o ser\u00e1 mais necess\u00e1ria, tendo em vista que transmite a mesma informa\u00e7\u00e3o. Sendo assim, a mesma ser\u00e1 descartada.","aaa14a56":"Analisando estes gr\u00e1ficos, percebe-se que algumas das vari\u00e1veis como \"LOAN\", \"YOJ\" e \"CLAGE\" apresentam uma m\u00e9dia ligeiramente diferente entre os que pagaram e os que n\u00e3o pagaram os emprestimos tomados. E \u00e9 poss\u00edvel notar tamb\u00e9m a quantidade de outliers presentes em cada uma dessas v\u00e1ri\u00e1veis, os quais podem estar consideravelmente influ\u00eanciando o valor da m\u00e9dia.","38a518de":"# <center>Tratamento e modelagem da base <a href='https:\/\/www.kaggle.com\/ajay1735\/hmeq-data'>hmeq_data<\/a><\/center>","95fd9dee":"Primeiro, ser\u00e1 realizado um teste para verificar a normalidade da distribui\u00e7\u00e3o dos valores da vari\u00e1vel \"VALUE\" segmentada pela vari\u00e1vel \"REASON\" para seguir ent\u00e3o para o teste de diferen\u00e7a das m\u00e9dias. ","ca722bc8":"### <a id='2.3.3'> 2.3.3. An\u00e1lise de Variancia One Way <\/a>","92209071":"___","47e4910f":"Assim como na plotagem anterior, nesta \u00e9 poss\u00edve identificar as diferentes m\u00e9dias das v\u00e1riaveis entre as diferentes ocupa\u00e7\u00f5es do indiv\u00edduo que tomou o emprestimo. E tamb\u00e9m os outliers que podem estar consideravelmente influenciando no valor da m\u00e9dia.","aaa2451f":"<img style=\"float: left;\" src=\"http:\/\/sindser.org.br\/s\/wp-content\/uploads\/2013\/09\/iesb1.jpg\"  width=\"400\" height=\"400\">\n\n## Instituto de Educa\u00e7\u00e3o Superior de Bras\u00edlia\n## P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados\n## Data Mining e Machine Learning II\n## Victor Hugo - 1931133079\n","8457a658":"Considerando um $\\alpha $ = 0.05, regeita-se a hip\u00f3tese nula, sendo assim, as media dos valores das taxas que ainda est\u00e3o por vir dos indiv\u00edduos que est\u00e3o em dia com seus emprestimos e a m\u00e9dia do valores das taxas que ainda est\u00e3o por vir dos indiv\u00edduos que fora inadimplentos em seus emprestimso se diferem.","9c337fff":"## <a id=\"2.2\"> 2.2. Visualiza\u00e7\u00e3o da base <\/a>","d26aff08":"Como j\u00e1 demonstrado nos gr\u00e1ficos anteriores aqui consguimos perceber tamb\u00e9m a ligeira diferen\u00e7a entre as m\u00e9dias para esta vari\u00e1vel para cada ocupa\u00e7\u00e3 do indiv\u00edduo."}}