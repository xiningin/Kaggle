{"cell_type":{"c122b478":"code","abf218f7":"code","1066bd15":"code","fdeaed51":"code","f8258994":"code","d104de78":"code","584d6190":"code","2c72e4d1":"code","0720581e":"code","6e0bfc82":"code","3b911554":"code","7194f322":"code","7ccea579":"code","8ebaa6e2":"code","728c27eb":"code","3260ecdd":"code","c4771894":"code","214b424f":"code","93db531a":"code","7d435e16":"code","7c6f0184":"code","b9f4d529":"code","dd4b47d9":"code","5a222ef3":"code","cdfd87c2":"code","8dc6c4fc":"code","4c43d339":"code","a27cba17":"code","5febe763":"code","aca03cef":"code","79ee5138":"code","caf7a7a4":"code","e64d2b9c":"code","6a269f54":"code","71f6673f":"code","b6aeb74a":"code","cee2e918":"code","2dac82e0":"code","910f33d4":"code","451b4ac0":"code","0dffa010":"code","657e8246":"code","860a89b0":"code","e8bed690":"code","55a840dd":"code","244c39e6":"code","72cb698c":"code","b7a7ad96":"code","4c59c8a4":"code","30035835":"code","55f872d2":"code","7e1af011":"code","91938679":"code","b2ea4379":"code","36d1d490":"code","462a6efa":"code","45bf461a":"code","60238c7f":"code","5ab80440":"code","7840248f":"code","6248e525":"code","207efb79":"code","3c4b043a":"code","d3748a23":"code","22e75424":"code","24a4c39d":"code","203c2d2b":"code","63e80f9d":"code","e2149995":"code","b923d60f":"code","7d74174b":"code","a6492332":"code","14d5deeb":"code","137521c6":"code","3662462e":"code","4768fcd0":"code","c158ccad":"code","cef4cac1":"code","ceaf7f96":"markdown","3552633c":"markdown","b8554e70":"markdown","60ce693d":"markdown","d64e5a02":"markdown","ee61170a":"markdown","33e76cd6":"markdown","671c12e4":"markdown","e6c8aed5":"markdown","8d0ad69b":"markdown","93a9ae32":"markdown","cfd41591":"markdown","33206694":"markdown","2e9d0b5b":"markdown","c122bd78":"markdown","ddb00d90":"markdown","bea05b7c":"markdown","d35e5223":"markdown","8124b46f":"markdown","f1652641":"markdown","8f235a8a":"markdown","f1aec9bb":"markdown","415fcc32":"markdown","39fc496c":"markdown","d329bbc0":"markdown","b28a134b":"markdown","fefe250b":"markdown","32b79cd0":"markdown","59d1e262":"markdown","34ca6b0f":"markdown","3a4cc81c":"markdown","30bea500":"markdown","dcf4b64e":"markdown","0b7313ca":"markdown","358cae1a":"markdown","cf36ef88":"markdown","e4547c01":"markdown","7f1ff2a8":"markdown","960a4dd2":"markdown","a5d7a896":"markdown","b0314772":"markdown","51e9fa87":"markdown","ebfdaca5":"markdown","50f5dfdc":"markdown","d19b6c27":"markdown","aa937c4c":"markdown","f05f5b4b":"markdown","b964b00d":"markdown","0861c57a":"markdown","b1ded5e5":"markdown","782dbcc7":"markdown","00e46f14":"markdown","3b212428":"markdown","289b28c6":"markdown","62a6cf38":"markdown","d12d98b1":"markdown","ab355161":"markdown","02dcbe13":"markdown","99514c05":"markdown","246ff69d":"markdown","ea1b7b51":"markdown","aa9337f7":"markdown","720956c2":"markdown","474f7682":"markdown","2b4b57eb":"markdown","9f3b9d48":"markdown","fb580b27":"markdown","622be6a3":"markdown","d9d349ee":"markdown","b2b1a7a0":"markdown","f4a3e4c6":"markdown","6b7a4af7":"markdown","0e15e6be":"markdown","c0220885":"markdown","c1cd4947":"markdown","19e26c90":"markdown","164f04d1":"markdown","fabb0708":"markdown","9e8f8c33":"markdown","97a2df82":"markdown","c22ebda0":"markdown","7dc01088":"markdown","f4e413a3":"markdown"},"source":{"c122b478":"import math\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport warnings \nimport seaborn as sns\nimport sklearn\nfrom datetime import datetime\nimport calendar\n%matplotlib inline\nimport shelve\nimport warnings\nwarnings.simplefilter('ignore')","abf218f7":"games = pd.read_csv('..\/input\/epl-stats-20192020\/epl2020.csv')\n","1066bd15":"print(games.columns)\nprint(games.isna().sum()) #no missing data","fdeaed51":"games = games.drop('Unnamed: 0', axis=1) # drop useless column\ngames = games.drop(['scored', 'missed', 'wins', 'draws', 'loses', 'pts',\n                    'tot_points', 'tot_goal', 'tot_con'], axis=1) # drop columns which give away result","f8258994":"games['target']=np.where(games['result']=='l', 0,\n                         np.where(games['result']=='d', 1,2))","d104de78":"games = games.drop('result', axis=1) # drop useless column","584d6190":"games[['target', 'h_a', 'teamId', 'Referee.x', 'matchDay']] = games[['target', 'h_a', 'teamId', 'Referee.x', 'matchDay']].astype('category')","2c72e4d1":"EDA_pairplot1=games.filter(items=['xG', 'xGA', 'npxG', 'npxGA', 'npxGD', 'h_a', 'target'])\nsns.pairplot(EDA_pairplot1, hue='target')","0720581e":"EDA_pairplot2=games.filter(items=['ppda_cal', 'allowed_ppda', 'HS.x', 'HST.x', 'HF.x', 'HC.x', 'target'])\nsns.pairplot(EDA_pairplot2, hue='target')","6e0bfc82":"EDA_pairplot3=games.filter(items=['ppda_cal', 'allowed_ppda', 'AS.x', 'AST.x', 'AF.x', 'AC.x', 'target'])\nsns.pairplot(EDA_pairplot3, hue='target')","3b911554":"EDA_plot4=games.groupby('teamId')['target'].agg(counts='value_counts').reset_index()\nEDA_plot4['target']=np.where(EDA_plot4['target']==0, 'loss',\n                         np.where(EDA_plot4['target']==1, 'draw','win'))\nEDA_plot4=EDA_plot4.reset_index() \nsns.catplot(y=\"teamId\",x='counts', hue='target',data=EDA_plot4)","7194f322":"from sklearn.preprocessing import StandardScaler","7ccea579":"nums= games.select_dtypes(include=['float', 'int64'])\nother= games.select_dtypes(exclude=['float', 'int64']).drop(['date', 'target'], axis=1)","8ebaa6e2":"scaler = StandardScaler()\nnums_scaled = scaler.fit_transform(nums)\ngames_scaled=pd.DataFrame(nums_scaled, columns=nums.columns)","728c27eb":"dummies=pd.get_dummies(other)","3260ecdd":"gameday_pred = pd.concat([games_scaled,dummies, games['target']], axis=1)  ","c4771894":"X=gameday_pred.drop('target', axis=1)\ny=gameday_pred['target']","214b424f":"print(y.dtypes)\nprint(X.dtypes)","93db531a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","7d435e16":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,  QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss, precision_score, recall_score, f1_score","7c6f0184":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]","b9f4d529":"log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)","dd4b47d9":"for clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    \n    # calculate score\n    precision = precision_score(y_test, train_predictions, average = 'macro') \n    recall = recall_score(y_test, train_predictions, average = 'macro') \n    f_score = f1_score(y_test, train_predictions, average = 'macro')\n    \n    \n    print(\"Precision: {:.4%}\".format(precision))\n    print(\"Recall: {:.4%}\".format(recall))\n    print(\"F-score: {:.4%}\".format(recall))\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","5a222ef3":"sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","cdfd87c2":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")","8dc6c4fc":"rf = RandomForestClassifier(n_estimators=500)\nrf.fit(X_train, y_train);\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(10).plot(kind='barh')","4c43d339":"from sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","a27cba17":"xgb = XGBClassifier(objective='multi:softprob',silent=False)","5febe763":"xgb.fit(X_train,y_train)\ny_pred_xgb_basic=xgb.predict(X_test)","aca03cef":"precision = precision_score(y_test, y_pred_xgb_basic, average = 'macro') * 100\nrecall = recall_score(y_test, y_pred_xgb_basic, average = 'macro') * 100\nf_score = f1_score(y_test, y_pred_xgb_basic, average = 'macro') * 100\na_score = accuracy_score(y_test, y_pred_xgb_basic) * 100","79ee5138":"print('Precision: %.3f' % precision)\nprint('Recall: %.3f' % recall)\nprint('F-Measure: %.3f' % f_score)\nprint('Accuracy: %.3f' % a_score)","caf7a7a4":"parameters_xgb = {\n        'learning_rate': [0.05, 0.1, 0.2, 0.3, 0.5], \n        'n_estimators': [200, 300, 400, 500, 600], \n        'max_depth': [1, 5, 10, 15, 20], \n        'gamma' :[0.1, 0.5, 1, 2, 5], \n        'subsample': [0.5, 0.75, 1], \n        'colsample_bytree': [0.01, 0.1, 0.5, 1], \n        }","e64d2b9c":"grid_search_xgb = GridSearchCV(estimator = xgb, param_grid = parameters_xgb, \n                          cv = 3,n_jobs=-1, verbose = 2)","6a269f54":"grid_search_xgb.fit(X_train,y_train)\nprint(grid_search_xgb.best_params_)\n    # {'colsample_bytree': 0.1, 'gamma': 0.5, 'learning_rate': 0.3, 'max_depth': 15, 'n_estimators': 300, 'subsample': 1}","71f6673f":"best_grid_xgb = grid_search_xgb.best_estimator_\nbest_grid_xgb.fit(X_train,y_train)","b6aeb74a":"y_pred_xgb = best_grid_xgb.predict(X_test)","cee2e918":"precision = precision_score(y_test, y_pred_xgb, average = 'macro') * 100\nrecall = recall_score(y_test, y_pred_xgb, average = 'macro') * 100\nf_score = f1_score(y_test, y_pred_xgb, average = 'macro') * 100\na_score = accuracy_score(y_test, y_pred_xgb) * 100","2dac82e0":"print('Precision: %.3f' % precision)\nprint('Recall: %.3f' % recall)\nprint('F-Measure: %.3f' % f_score)\nprint('Accuracy: %.3f' % a_score)","910f33d4":"rf = RandomForestClassifier(random_state = 1)\nrf_model_basic = rf.fit(X_train, y_train)\ny_pred_rf_basic = rf_model_basic.predict(X_test)","451b4ac0":"precision = precision_score(y_test, y_pred_rf_basic, average = 'macro') * 100\nrecall = recall_score(y_test, y_pred_rf_basic, average = 'macro') * 100\nf_score = f1_score(y_test, y_pred_rf_basic, average = 'macro') * 100\na_score = accuracy_score(y_test, y_pred_rf_basic) * 100","0dffa010":"print('Precision: %.3f' % precision)\nprint('Recall: %.3f' % recall)\nprint('F-Measure: %.3f' % f_score)\nprint('Accuracy: %.3f' % a_score)","657e8246":"parameters_rf = {\n    'bootstrap': [True],\n    'n_estimators' : [100, 300, 500, 800, 1200],\n    'max_depth' : [5, 8, 15, 25, 30],\n    'min_samples_split' : [2, 5, 10, 15, 100],\n    'min_samples_leaf' : [1, 2, 5, 10],\n    'max_features': [2, 4]\n}","860a89b0":"grid_search_rf = GridSearchCV(estimator = rf, param_grid = parameters_rf, \n                          cv = 3,n_jobs=-1, verbose = 2)","e8bed690":"grid_search_rf.fit(X_train,y_train)\nprint(grid_search_rf.best_params_)\n    # {'bootstrap': True, 'max_depth': 15, 'max_features': 4, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 800}","55a840dd":"best_grid_rf = grid_search_rf.best_estimator_\nbest_grid_rf.fit(X_train,y_train)","244c39e6":"y_pred_rf = best_grid_rf.predict(X_test)","72cb698c":"precision = precision_score(y_test, y_pred_rf, average = 'macro') * 100\nrecall = recall_score(y_test, y_pred_rf, average = 'macro') * 100\nf_score = f1_score(y_test, y_pred_rf, average = 'macro') * 100\na_score = accuracy_score(y_test, y_pred_rf) * 100","b7a7ad96":"print('Precision: %.3f' % precision)\nprint('Recall: %.3f' % recall)\nprint('F-Measure: %.3f' % f_score)\nprint('Accuracy: %.3f' % a_score)","4c59c8a4":"games2 = pd.read_csv('..\/input\/epl-stats-20192020\/epl2020.csv')\ntable=games2.groupby('teamId')['pts'].agg(table_points='sum').reset_index().sort_values('table_points', ascending=False)\ntable['position']=range(0,len(table), 1)\ntable['position']=table['position']+1\nprint(table)","30035835":"fc_games=games\nfc_games['date'] = pd.to_datetime(fc_games.date , format = '%Y-%m-%d %H:%M:%S')\nfc_games.index = fc_games['date']\nfc_games = fc_games.drop(['date'], axis=1)","55f872d2":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()","7e1af011":"categories= fc_games.select_dtypes(include=['category']).drop(['teamId','target'], axis=1).apply(LabelEncoder().fit_transform)\nfc_games=fc_games.drop(categories.columns.values, axis=1)\nfc_games=pd.concat([fc_games, categories], axis=1)","91938679":"from statsmodels.tsa.stattools import adfuller","b2ea4379":"def adfuller_test(series, signif=0.05, name='', verbose=False):\n    r = adfuller(series, autolag='AIC')\n    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}\n    p_value = output['pvalue'] \n    def adjust(val, length= 6): return str(val).ljust(length)\n\n    # Print Summary\n    print(f'    Augmented Dickey-Fuller Test on \"{name}\"', \"\\n   \", '-'*47)\n    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')\n    print(f' Significance Level    = {signif}')\n    print(f' Test Statistic        = {output[\"test_statistic\"]}')\n    print(f' No. Lags Chosen       = {output[\"n_lags\"]}')\n    for key,val in r[4].items():\n        print(f' Critical value {adjust(key)} = {round(val, 3)}')\n    if p_value <= signif:\n        print(f\" => P-Value = {p_value}. Rejecting Null Hypothesis.\")\n        print(f\" => Series is Stationary.\")\n    else:\n        print(f\" => P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.\")\n        print(f\" => Series is Non-Stationary.\")   \n    ","36d1d490":"x=fc_games[fc_games['teamId']=='Liverpool'] #lets take an example and see if we can test\/produce stationarity\nx=x.drop(['teamId','target'], axis=1) #dont want to forecast either of these.\nfc_train = x[:int(0.8*(len(x)))] #split data\nfc_valid = x[int(0.8*(len(x))):]","462a6efa":"for name, column in fc_train.iteritems(): #run the ADF test of stationarity\n    adfuller_test(column, name=column.name)\n    print('\\n')","45bf461a":"fc_train_diff = fc_train.diff().dropna()","60238c7f":"from statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.tsa.api import VAR\n \nfeat_importances_1=feat_importances.nlargest(5).index.values\nfeat_importances_2=feat_importances.nlargest(10).index.values[5:]\nfeat_importances_3=feat_importances.nlargest(15).index.values[10:]","5ab80440":"def validation_by_team(x):\n    x1=x.filter(items=feat_importances_1)\n    x2=x.filter(items=feat_importances_2)\n    \n    fc_train1 = x1[:int(0.8*(len(x1)))] #split data\n    fc_valid1 = x1[int(0.8*(len(x1))):]\n    \n    fc_train2 = x2[:int(0.8*(len(x2)))] #split data\n    fc_valid2 = x2[int(0.8*(len(x2))):]\n    \n    model1 = VAR(endog=fc_train1) #fit VAR model\n    model_fit1 = model1.fit()\n    \n    model2 = VAR(endog=fc_train2) #fit VAR model\n    model_fit2 = model2.fit()\n    \n    prediction1 = model_fit1.forecast(model_fit1.y, steps=len(fc_valid1)) #predict\n    prediction1 = pd.DataFrame(data=prediction1, columns=x1.columns)\n    \n    prediction2 = model_fit2.forecast(model_fit2.y, steps=len(fc_valid2)) #predict\n    prediction2 = pd.DataFrame(data=prediction2, columns=x2.columns)\n       \n    # Check the performance of the by serial correlation of errors using the Durbin Watson statistic.\n    \n    # The value of this statistic can vary between 0 and 4. The closer it is to the value 2, \n    # then there is no significant serial correlation. The closer to 0, there is a positive \n    # serial correlation, and the closer it is to 4 implies negative serial correlation.\n    \n    out1 = durbin_watson(model_fit1.resid)\n    print(out1)\n    out2 = durbin_watson(model_fit2.resid)\n    print(out2)\n    \n    prediction_performance = pd.DataFrame([out1, out2]).T\n    return(prediction_performance)","7840248f":"prediction_validations=fc_games.groupby('teamId').apply(validation_by_team)","6248e525":"import random","207efb79":"def forecast_by_team(x):\n    target=x['target'].reset_index(drop=True)\n    x=x.drop(['teamId','target'], axis=1) #dont want to forecast either of these.\n    \n    x1=x.filter(items=feat_importances_1)\n    x2=x.filter(items=feat_importances_2)\n    x3=x.filter(items=feat_importances_3)\n    \n    model1 = VAR(endog=x1) #fit VAR model1\n    model_fit1 = model1.fit()\n    \n    model2 = VAR(endog=x2) #fit VAR model2\n    model_fit2 = model2.fit()\n    \n    model3 = VAR(endog=x3) #fit VAR model3\n    model_fit3 = model3.fit()\n    \n    prediction1 = model_fit1.forecast(model_fit1.y, steps=6) #predict\n    prediction1 = pd.DataFrame(data=prediction1, columns=x1.columns)\n    \n    prediction2 = model_fit2.forecast(model_fit2.y, steps=6) #predict\n    prediction2 = pd.DataFrame(data=prediction2, columns=x2.columns)\n    \n    prediction3 = model_fit3.forecast(model_fit3.y, steps=6) #predict\n    prediction3 = pd.DataFrame(data=prediction3, columns=x3.columns)\n    \n    predictions=pd.concat([prediction1, prediction2, prediction3], axis=1)\n    \n    x_forecasted=pd.concat([x, predictions], axis=0).reset_index()\n    \n    # Lets randomly impute home\/away games as 0 or 1\n    # I start by generating random 0s and 1s and selecting the index where data is missing\n    # Then we fill.\n    na_loc =x_forecasted.index[x_forecasted['h_a'].isnull()]\n    num_nas = len(na_loc)\n    fill_values = pd.DataFrame({'h_a': [random.randint(0,1) for i in range(num_nas)]}, index = na_loc)\n    \n    x=pd.concat([x, fill_values], axis=0).reset_index(drop=True)\n    predictions.index=fill_values.index\n    x_forecasted2=x.combine_first(predictions)\n    \n    # Now lets mean (numeric) or mode (categorical) impute the other missing variables\n    # Since these are of less modelling importance, a mean impute shouldn't make much difference\/\n    # Also, it is logical to assume consistency in Season performance, without any new data\n    # to think otherwise.\n    \n    nums= x_forecasted2.select_dtypes(include=['float', 'int64']).apply(lambda x: x.fillna(x.mean()),axis=0).round()\n    x_forecasted3=x_forecasted2.combine_first(nums) #join this imputation back in to fill missingness\n    x_forecasted4=pd.concat([x_forecasted3, target], axis=1)\n    \n    return(x_forecasted4)\n    \nforecasted_data=fc_games.groupby('teamId').apply(forecast_by_team).reset_index().drop('level_1', axis=1) # run seperately for all teams\nprint(forecasted_data.isna().sum()) #no missing data but the target","3c4b043a":"forecasted_data[[\n    'target', 'h_a', 'teamId', 'Referee.x', 'matchDay']] = forecasted_data[[\n        'target', 'h_a', 'teamId', 'Referee.x', 'matchDay']].astype('category')\n        \n# Scale numeric data\nfrom sklearn.preprocessing import StandardScaler","d3748a23":"nums= forecasted_data.select_dtypes(include=['float', 'int64'])\nother= forecasted_data.select_dtypes(exclude=['float', 'int64']).drop('target', axis=1)","22e75424":"scaler = StandardScaler()\nnums_scaled = scaler.fit_transform(nums)\nfc_scaled=pd.DataFrame(nums_scaled, columns=nums.columns)","24a4c39d":"dummies=pd.get_dummies(other)","203c2d2b":"fc_pred = pd.concat([fc_scaled,dummies, forecasted_data['target']], axis=1)  ","63e80f9d":"train_data = fc_pred[fc_pred['target'].notnull()]\nX_train=train_data.drop('target', axis=1)\ny_train=train_data['target']","e2149995":"X_test=fc_pred[fc_pred['target'].isnull()].drop('target', axis=1).reset_index(drop=True)","b923d60f":"xgb = XGBClassifier(objective='multi:softprob',\n                    colsample_bytree = 0.5, gamma = 2, learning_rate= 0.2, max_depth= 10, \n                    n_estimators= 500, subsample = 1)\nxgb.fit(X_train,y_train)\nfinal_predictions=pd.DataFrame({'target':xgb.predict(X_test)})","7d74174b":"final_test=pd.concat([X_test, final_predictions], axis=1)\ncomplete_season=pd.concat([train_data, final_test], axis=0).reset_index(drop=True)","a6492332":"team_cols=complete_season[complete_season.columns[complete_season.columns.to_series().str.contains('teamId')]]#[col==1].stack().reset_index().drop(0,1)\nteam_cols_long=team_cols[team_cols==1].stack().reset_index().drop(0,1).drop('level_0', axis=1)\nteam_cols_long['team']=team_cols_long['level_1'].str.rpartition('_')[2]\nteam_cols_long=team_cols_long.drop('level_1', axis=1)","14d5deeb":"complete_season=pd.concat([complete_season, team_cols_long], axis=1)","137521c6":"new_points=complete_season.filter(items=['team', 'target'])\nnew_points['pts_new']=np.where(new_points['target']==0, 0,\n                           np.where(new_points['target']==1,1, 3))\ntable_final=new_points.groupby('team')['pts_new'].agg(table_points_new='sum').reset_index().sort_values('table_points_new', ascending=False)\ntable_final['position']=range(0,len(table_final), 1)\ntable_final['position']=table_final['position']+1","3662462e":"print(table_final)","4768fcd0":"table_final.sort_values('team', inplace=True)\ntable.sort_values('teamId', inplace=True)\ntable_final['positition_change'] = table['position'] -  table_final['position']\ntable_final.sort_values('position', inplace=True, ascending=True)","c158ccad":"print(table_final)","cef4cac1":"os.chdir(r\"C:\/Users\/jaket\")\n!jupyter nbconvert --to html EPL_2020_predictions.ipynb","ceaf7f96":"Instantiate the grid search model","3552633c":"Scale numeric data","b8554e70":"Tuning the XGb using Grid search","60ce693d":"# Prediction of future games<br>\nFirst we must ensure that the new forecasted df is in the same format as the previous matchday prediction df<br>\nThat includes changing categories back to categories and one-hot encoding as well as scaling.","d64e5a02":"We'll need to check stationarity for each participant. Perform ADFuller to test for Stationarity of given series and print report","ee61170a":"# Predicting the final EPL table: <br>\nUsing a combination of machine learning model generated on game-level data and VAR forecasting to forecast features based on prior features to predict the final league table.\n","33e76cd6":"It is clear that forecasting all columns is given the number of observations we have is not possible. As such I decided to select the 1-5 and 6-10 most important variables<br>\nand forecast these.   ","671c12e4":"# PART 2: how will the league finish?","e6c8aed5":"One hot encode categorical data","8d0ad69b":"Function adapted from: https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.adfuller.html","93a9ae32":"# Model selection and tuning<br>\nThe initial results suggest that XGb and RF are the two best performing classifiers for this problem. Lets begin by tuning the best algorithm (XGb)","cfd41591":"Aim 4: Predict final league table (ML\/forecasting?)","33206694":"Fit to training data and predict. For the XGb parameters we'll insert the tuned hyperparameters","2e9d0b5b":"Note: we could have developed individual team-level predictive models (or combined both) - but did not have time currently.","c122bd78":"Select classification algorithms we'll test","ddb00d90":"# Sort games df ","bea05b7c":"After some preprocessing and EDA, I used a range of classification algorithms to test which would best<br>\npredict match result. I found that XGb and RF performed best. I tuned these both and found that<br>\n RF improved in performance slightly, as did XGB though increased were small, this is probably due to the fact that<br>\nlimited computation power did not allow for a full grid search so I limited the ability to tune<br>\nparameters to save time.","d35e5223":"Generate an analysis dataset","8124b46f":"Initial stage: calculate the current table. We need to reload the table to get points back.","f1652641":"# Algorithm testing and initial selection","8f235a8a":"To forecast using VAR, categorical variables must be label encoded","f1aec9bb":"Calculate score","415fcc32":"# Modelling - Preprocessing","39fc496c":"Since we cannot forecast every one of the features due to limited obs, we'll forecast the top 10, and use mean imputation for the rest.","d329bbc0":"Created on Tue Jun  2 11:06:39 2020<br>\nauthor: jaketuricchi\nGenerated in Spyder 4.0 using J2P for conversion from .py to .ipynb","b28a134b":"Predict test data","fefe250b":"Here we explore HOME team match stats, in relation to the target","32b79cd0":"Load packages","59d1e262":"# Final table","34ca6b0f":"These results suggest that the 'expected' (i.e. betting agent prediction) variables provide more predictive value than the actual game statistics which is surprising.","3a4cc81c":"These values seem suitable. They are generally around 2. The distance between days doesnt matter so we can ignore the errors here.","30bea500":"# Tune random forest","dcf4b64e":"Table changes - did anyone overtake anyone in the last 6 games?","0b7313ca":"Most columns are non-stationary, lets difference and retry","358cae1a":"Here we explore 'expected' variables (goals, concedes) given by the bookmakers, in relation to the target. We could do a more extensive visualisation with more time.","cf36ef88":"Aim 3: Predict the last 6 results for each team using the previously generated model.","e4547c01":"Hyperparameter tuning gives us a very small additional boost in XGB performance. We could improve the grid search but are limited by time and computation power currently.","7f1ff2a8":"Code target","960a4dd2":"What are the most important features?","a5d7a896":"I then forecasting the final 6 games to the end of the season using VAR. I had planned to forecast all<br>\npredictive variables but soon realised we did not have anywhere near enough observations for this.<br>\nI chose to forecast the 15 most importance variables (according to RF importance) in smaller batches of 5.<br>\nThis is not ideal, and for this reason the forecasts are not too informative (though with more data may will<br>\nbe).","b0314772":"Split the data into features and labels","51e9fa87":"Game outcomes","ebfdaca5":"Calculate score","50f5dfdc":"Calculate score","d19b6c27":"Prediction data - since there is slight differences in the feature names, we'll retrain the best (tuned XGB) model on the new data<br>\nThis shouldn't make much difference to results but just improve compatability.","aa937c4c":"The biggest climbers in the final 6 games will be Burnley!","f05f5b4b":"Aim 2: Forecast match features for the final 6 games of the season using VAR","b964b00d":"# Validation of forecasting","0861c57a":"Logging for Visual Comparison","b1ded5e5":"Aim 1: We want to predict match reuslt given match statistics (removing obvious stats such as goals); Supervised classification","782dbcc7":"Fit the grid search to the data","00e46f14":"Put the final df together","3b212428":"Tuning the RF using Grid search","289b28c6":"Next, we'll use multivariate time series forecasting to forecast the match statistics used as features. Then, we'll use the XGb model we generated to predict the results of these matches<br>\nLast, we'll add the results to generate a final league table","62a6cf38":"Predict test data","d12d98b1":"Generate a basic Xgb model and predict the test data","ab355161":"Here we explore AWAY team match stats, in relation to the target","02dcbe13":"Run best params","99514c05":" Set wd and read in data","246ff69d":"Lastly, I applied the XGb model generated earlier to predict the target (match result) in the final 6 games. <br>\nI then put the league table back together based on the results of the newly predicted games and calculated<br>\na change in the table.","ea1b7b51":"All meanings of variables can be found at: https:\/\/www.kaggle.com\/idoyo92\/epl-stats-20192020 along with the complete data set.","aa9337f7":"Set categorical variables","720956c2":"Run best params","474f7682":"Calculate score","2b4b57eb":"# Complete","9f3b9d48":"Seperate categories and numerics for preprocessing, and then rejoin.","fb580b27":"# Rebuilding the final table","622be6a3":"Small increases in performance observed from tuning the RF, but a tuned XGb is still the best performing algorithm","d9d349ee":"# Forecasting of match statistics","b2b1a7a0":"collinearity should be expected given the type of data\n\n# Plot results of algo testing","f4a3e4c6":"# EDA","6b7a4af7":"Generate an analysis dataset","0e15e6be":"\nImport packages","c0220885":"Generate a basic model","c1cd4947":"Drop result string variable","19e26c90":"Backwards transform OneHotEncoded (dummy) variables (specifically, we need team)","164f04d1":"NOTE: having tried differencing multiple times - this will not significantly change the stationarity of the series - this gives us less forecasting power but also impossible to avoid, probably due to<br>\nthe small number of obs.","fabb0708":"Run algo loop","9e8f8c33":"One hot encode categorical data","97a2df82":"Fit the grid search to the data","c22ebda0":"Split the data intro train and test sets for model training","7dc01088":"There are 6 more games in the EPL season, so we'll predict 6 lags ahead.  It is important to note that with the amount of variables and previous observations we have, the predictions will be similar to an approximate mean imputation","f4e413a3":"Instantiate the grid search model"}}