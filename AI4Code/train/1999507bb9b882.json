{"cell_type":{"89310f03":"code","d6d10eff":"code","5b142b7f":"code","c0170c3a":"code","1e272255":"code","b1b7283a":"code","c49e608f":"code","c8586c7f":"code","3a7cc67c":"code","68c2fd69":"code","4287f90c":"code","ed1f7cf0":"code","f0fcb54d":"code","a5cb8ad7":"code","80d1c72f":"code","53f50396":"markdown","2720f513":"markdown","b3567264":"markdown","59f6e926":"markdown","a664b27d":"markdown","b4cffb63":"markdown","f701b563":"markdown","94ac745c":"markdown","e385054e":"markdown","7cdf0290":"markdown","4779912a":"markdown","f4e068d7":"markdown","35a66f24":"markdown","80547c7c":"markdown","0513b765":"markdown"},"source":{"89310f03":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder,OrdinalEncoder\nfrom sklearn.linear_model import LinearRegression\n\n#import libaries to transform our features \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\ndf = pd.DataFrame({\n    'Dog Breed':[\"Beagle\", \"Bulldog\", \"Poodle\", \"German Shepard\",\"Beagle\", \"Bulldog\", \"Poodle\", \"German Shepard\"], \n    'Age':[1, 4, 7, 9,12,22,10,3],\n    'Weight':[60,32,70,100,66,44,24,66],\n    \"Sex\":[\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Female\",\"Male\",\"Female\"],\n    \"Height\":[15,50,77,110,55,66,77,88],\n    \"Intelligence\":[\"Average\",\"Below Average\",\"Above Average\",\"Average\",\"Average\",\"Below Average\",\"Above Average\",\"Average\"],\n    \n    \"Life Expentancy\":[10,20,7,15,10,20,7,15]\n\n})\ndf.head(10)\n","d6d10eff":"try:\n    y=df[\"Life Expentancy\"]\n    X=df.drop(columns=[\"Life Expentancy\"])\n    model=LinearRegression()\n    model.fit(X,y)\nexcept Exception as e: \n    print(e)","5b142b7f":"ordinalEncodingScheme = ['Below Average','Average','Above Average']\nordinalEncoder = OrdinalEncoder(categories=[ordinalEncodingScheme])\n\n\nresults=ordinalEncoder.fit_transform(X[[\"Intelligence\"]])\nprint(f\"Here are my categories:\\n{ordinalEncoder.categories_}\")\nresults=np.reshape(results, (-1,1)).flatten()\n\nprint(results)\ndataframe=pd.DataFrame({\n    \"Intelligence\":df[\"Intelligence\"].array,\n    \"Transformed Intelligence\":results\n\n})\nprint(dataframe.head())\n","c0170c3a":"BinaryEncodingScheme = ['Male','Female']\nordinalEncoder = OrdinalEncoder(categories=[BinaryEncodingScheme])\n\n\nresults=ordinalEncoder.fit_transform(X[[\"Sex\"]])\nprint(f\"Here are my categories:\\n{ordinalEncoder.categories_}\")\nresults=np.reshape(results, (-1,1)).flatten()\n\nprint(results)\ndataframe=pd.DataFrame({\n    \"Sex\":X[\"Sex\"].array,\n    \"Binary Encoded results\":results\n\n})\nprint(dataframe.head())\n","1e272255":"encoder = OneHotEncoder()\nresults=encoder.fit_transform(X[[\"Dog Breed\"]]).toarray()\nprint(f\"Here are my categories:\\n{encoder.categories_}\\n\")\nprint(results)\nprint(pd.get_dummies(X[[\"Dog Breed\"]]))\n","b1b7283a":"scaler = StandardScaler()\nprint(scaler.fit_transform(X[[\"Age\",\"Weight\"]]))","c49e608f":"X[\"Height ^2\"]=np.power(X[[\"Height\"]],2)\nX[\"BMI\"]=X[\"Weight\"]\/X[\"Height ^2\"]\nX[\"Difference between Height and Weight\"]=X[\"Height\"]- X[\"Weight\"]\nX[\"Sum between Height and Weight\"]=X[\"Height\"]+ X[\"Weight\"]\nX.head()\n","c8586c7f":"BinaryEncodingScheme = ['Male','Female']\nbinaryEncoder = OrdinalEncoder(categories=[BinaryEncodingScheme])\n\nordinalEncodingScheme = ['Below Average','Average','Above Average']\nordinalEncoder = OrdinalEncoder(categories=[ordinalEncodingScheme])\n\nord_attribs=[\"Intelligence\"]\nbin_attribs=[\"Sex\"]\none_h_attribs=[\"Dog Breed\"]\nnum_attribs=[\"Age\",\"Weight\"]\nnum_pipline=make_pipeline(StandardScaler())\nfull_pipeline=make_column_transformer(\n    (num_pipline,num_attribs),\n    (OneHotEncoder(),one_h_attribs),\n    (binaryEncoder,bin_attribs),\n    (ordinalEncoder,ord_attribs)\n\n)\n\n\nfull_pipeline=full_pipeline.fit(df)\nX=full_pipeline.transform(df)\n","3a7cc67c":"model.fit(X,y)\nmodel.predict(X)","68c2fd69":"df = pd.DataFrame(\n        {\n            'Date': pd.date_range(start='2015-01-01', end='2020-12-31', freq='D')\n        }\n    )\n\nsize=(df.count())\nnp.random.seed(0)\ndata = np.random.randint(0,1000,size=size)\ndf[\"Data\"]=data\nprint(df.info())\nprint(df.head())","4287f90c":"df['year'] = df[\"Date\"].dt.year\ndf['month'] = df[\"Date\"].dt.month\ndf['day'] = df[\"Date\"].dt.day\ndf['week'] = df[\"Date\"].dt.week\ndf['dayofweek_name']=df['Date'].dt.day_name()\n\ndf['isWeekend'] = (df[\"Date\"].dt.dayofweek >=5).astype(int)\ndf['isMonthStart'] = df[\"Date\"].dt.is_month_start.astype(int)\ndf.head(10)\n","ed1f7cf0":"df[\"Days since since today\"]=(datetime.datetime.today() - df[\"Date\"]).dt.days\ndf.head()","f0fcb54d":"df[\"Lag_1\"]=df[\"Data\"].shift(1)\ndf.head()","a5cb8ad7":"df['rolling_mean'] = df['Data'].rolling(window=7).mean()\ndf.tail(10)","80d1c72f":"df['Expanding_Mean'] = df['Data'].expanding(2).mean()\ndf.tail(10)","53f50396":"# Feature Interaction","2720f513":"# Feature engineering with Dates","b3567264":"# One Hot encoding ","59f6e926":"# Putting everything together \n","a664b27d":"# Expanding Mean","b4cffb63":"# Lag features","f701b563":"# Lets try fiting our model with our raw dataset\n","94ac745c":"# Feature Scaling","e385054e":"# Open source libaries\n\n* [tsfresh](https:\/\/tsfresh.readthedocs.io\/en\/latest\/index.html) - It automatically calculates a large number of time series characteristics, the so called features. Further the package contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks.\n\n* [Featuretools](https:\/\/www.featuretools.com\/)","7cdf0290":"# What is Feature Engineering ? \n* the practice of constructing suitable features from given features that lead to improved predictive performance. \n* an encompassing term that includes feature generation (producing features from raw data), feature transformation (evolving existing features), feature selection (picking most important features), feature analysis (understanding feature behaviour), feature evaluation (determining feature importance) and automatic feature engineering methods (performing FE without human intervention). \n\n# Why should you do Feature Engineering ?\n* Improving your model performance\n* Computational expensive to store and run all features in your dataset\n* Models can\u2019t process raw features such as text , dates  and categorical data \n\n\n\n","4779912a":"# Rolling Mean","f4e068d7":"# Ordinal Encoding\n","35a66f24":"# Standard Scaler\n![image.png](attachment:image.png)","80547c7c":"![image.png](attachment:image.png)","0513b765":"# Binary Encoder"}}