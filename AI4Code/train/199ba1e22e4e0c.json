{"cell_type":{"1f77d7d7":"code","3a72eb64":"code","741cabad":"code","bb60983e":"code","2e6388bb":"code","ec1db4f5":"code","a92fe422":"code","40378d4f":"code","dd4cba5f":"code","2a7c82cd":"code","7af18c43":"code","c9dc3519":"code","73244b8a":"code","3f69fc04":"code","e85597cb":"code","21600efa":"code","2d02708f":"code","c70dedca":"code","02cc01d2":"code","7824373a":"code","9d51d5b5":"code","89bf13c9":"code","606e427d":"code","c26bf76b":"markdown","edeefd60":"markdown","4ca762da":"markdown","d6a05427":"markdown","15177311":"markdown","5df14cf3":"markdown"},"source":{"1f77d7d7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","3a72eb64":"input_path = \"\/kaggle\/input\/commonlitreadabilityprize\/\"\ntrain = pd.read_csv(input_path+\"train.csv\", usecols = [\"excerpt\",\"target\"])\ntest = pd.read_csv(input_path+\"test.csv\", usecols=[\"excerpt\"])\nsub = pd.read_csv(input_path+\"sample_submission.csv\")","741cabad":"import spacy\nimport re\nfrom collections import Counter\nimport string","bb60983e":"tok = spacy.load('en_core_web_sm')\ndef tokenize (text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(nopunct)]","2e6388bb":"# Count number of occurences of each word\ncounts = Counter()\nfor text in list(train['excerpt']):\n    counts.update(tokenize(text))","ec1db4f5":"# Deleting infrequent words\nprint(\"num_words before:\",len(counts.keys()))\nfor word in list(counts):\n    if counts[word] < 2:\n        del counts[word]\nprint(\"num_words after:\",len(counts.keys()))","a92fe422":"# Creating vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)","40378d4f":"def encode_sentence(text, vocab2index, N=200):\n    tokenized = tokenize(text)\n    encoded = np.zeros(N, dtype=int)\n    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n    length = min(N, len(enc1))\n    encoded[:length] = enc1[:length]\n    return encoded, length","dd4cba5f":"train['encoded'] = train['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\ntrain.head()","2a7c82cd":"#from sklearn.model_selection import train_test_split\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","7af18c43":"class CommonLitReadabiltyDataset(Dataset):\n    def __init__(self, X, Y):\n        self.X = X\n        self.y = Y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx] #, self.X[idx][1]","c9dc3519":"from sklearn.model_selection import train_test_split","73244b8a":"X = list(train['encoded'])\ny = list(train['target'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)\n\ntrain_dataset = CommonLitReadabiltyDataset(X_train, y_train)\nval_dataset = CommonLitReadabiltyDataset(X_valid, y_valid)","3f69fc04":"class LSTM_model(nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, linear1) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear1 = nn.Linear(hidden_dim, linear1)\n        self.act1 = nn.Tanh()\n        self.linear2 = nn.Linear(linear1, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        x = self.act1( self.linear1(ht[-1]) )\n        return self.linear2(x)","e85597cb":"vocab_size = len(words)\nembedding_dim = 500\nhidden_dim = 80\nlinear1 = 50\n\nmodel_ft =  LSTM_model(vocab_size, embedding_dim, hidden_dim, linear1).to('cuda')\nmodel_ft","21600efa":"def train_epoch(model,criterion,optimizer,dataset,epoch):\n    \n    train_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=4,shuffle=True,num_workers=4)\n    dataset_size=len(dataset)\n    \n    print(f\"Epoch#{epoch}. Train\")\n    model.train()\n    \n    running_loss=0.0   #\u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043b\u043e\u0441\u0441\u0430    \n    epoch_loss=0.0\n    \n    for inputs,labels in tqdm( data_loader):\n        inputs=inputs.to('cuda').type(torch.long)\n        labels=labels.to('cuda').type(torch.float) #\u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0431\u0430\u0442\u0447 \u043d\u0430 GPU(cuda)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss=criterion(outputs,labels)\n        loss.backward() # \u043e\u0431\u0440\u0430\u0442\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u043e\u0441\u0442\u0440\u0430\u043d\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430\n        optimizer.step() # \u0448\u0430\u0433 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440\u0430\n        running_loss+=loss.item()*inputs.size(0)\n    epoch_loss = running_loss \/ dataset_size\n    print(f'Loss RMSE: { np.sqrt(epoch_loss) }')\n    print(f\"Epoch#{epoch} (Train) completed.\")\n    return model, epoch_loss","2d02708f":"def valid_epoch(model,criterion,optimizer,dataset,epoch):\n    val_dataset=dataset\n    data_loader=DataLoader(dataset,batch_size=4,shuffle=True,num_workers=4)\n    dataset_size=len(val_dataset)\n    print(f\"Epoch#{epoch}. Validation\")\n    model.eval()\n    running_loss=0.0 # \u043d\u0430\u043a\u043e\u043f\u043b\u0435\u043d\u0438\u0435 \u043b\u043e\u0441c\n    epoch_loss=0.0\n    with torch.no_grad():\n        for inputs,labels in tqdm( data_loader):\n            inputs=inputs.to('cuda').type(torch.long)\n            labels=labels.to('cuda').type(torch.float) #\u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0431\u0430\u0442\u0447 \u043d\u0430 GPU(cuda)\n            outputs = model(inputs)\n            loss=criterion(outputs,labels)\n            running_loss+=loss.item()*inputs.size(0)\n    epoch_loss = running_loss \/ dataset_size\n    print(f'Loss RMSE: { np.sqrt(epoch_loss) } ')\n    print(f\"Epoch#{epoch} (Validation) completed. \")\n    return model, epoch_loss","c70dedca":"from tqdm import tqdm","02cc01d2":"criterion = nn.MSELoss()\noptimizer= optim.Adam(params=model_ft.parameters(),lr=3e-5)","7824373a":"best_model = model_ft\nbest_epoch = 1\nbest_loss = 1000000\nnum_epochs = 10\n\ntrain_loss_history = []\nval_loss_history = []\n\nfor epoch in range(1,num_epochs+1):\n    #\u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430\n    model_ft, train_loss = train_epoch(model_ft,criterion,optimizer,train_dataset,epoch)\n    train_loss_history.append(train_loss)\n    \n    #\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n    model_ft, val_loss = valid_epoch(model_ft,criterion,optimizer,val_dataset,epoch)\n    val_loss_history.append(val_loss)\n    \n    if(val_loss<best_loss):\n        best_model = model_ft\n        best_epoch = epoch","9d51d5b5":"model = best_model\nmodel.eval()\n\ntest['encoded'] = test['excerpt'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\nexcerpts_test = test['encoded']\n\nX_test = [excerpts_test[i][0] for i in range(len(test))]\nX_test = torch.LongTensor(X_test).to('cuda')\n\ny_hat = model(X_test)\ny_hat","89bf13c9":"sub[\"target\"] = y_hat.cpu().detach().numpy()\nsub","606e427d":"sub.to_csv(\"submission_lstm.csv\", index=False)","c26bf76b":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","edeefd60":"# \u041a\u043b\u0430\u0441\u0441 Dataset (\u043e\u0441\u043e\u0431\u0435\u043d\u043d\u043e\u0441\u0442\u044c pytorch - \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0442\u044c dataset \u043f\u043e\u0434 \u0441\u0432\u043e\u0438 \u043d\u0443\u0436\u0434\u044b)","4ca762da":"# \u041a\u043b\u0430\u0441\u0441 \u043c\u043e\u0434\u0435\u043b\u0438","d6a05427":"# \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438","15177311":"# Inference - \u0434\u043b\u044f \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0430 Kaggle (\u0432\u044b\u0432\u043e\u0434 \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435)","5df14cf3":"# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 (\u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u044f, \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u044f)"}}