{"cell_type":{"432ef933":"code","4588e6fb":"code","d3aa61cd":"code","d961cd7e":"code","81df905c":"code","e8fa6a80":"code","56dec805":"code","58372e30":"code","af2204c0":"code","7f996869":"code","22f61022":"code","e9131f8f":"markdown","295e042b":"markdown","623065ad":"markdown","fa353819":"markdown","29f2eba0":"markdown","c9a87615":"markdown","402b07d6":"markdown","17623551":"markdown","505c98c9":"markdown"},"source":{"432ef933":"\n# Import data and modules\nimport pandas as pd\nimport numpy as np\nfrom sklearn import datasets\n%pylab inline\npylab.rcParams['figure.figsize'] = (10, 6)\n\niris = datasets.load_iris()\n\n# We'll use the petal length and width only for this analysis\nX = iris.data[:, [2, 3]]\ny = iris.target\n\n# Place the iris data into a pandas dataframe\niris_df = pd.DataFrame(iris.data[:, [2, 3]], columns=iris.feature_names[2:])\n\n# View the first 5 rows of the data\nprint(iris_df.head())\n\n# Print the unique labels of the dataset\nprint('\\n' + 'The unique labels in this data are ' + str(np.unique(y)))","4588e6fb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n\nprint('There are {} samples in the training set and {} samples in the test set'.format(\nX_train.shape[0], X_test.shape[0]))\nprint()","d3aa61cd":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\n\nsc.fit(X_train)\n\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\nprint('After standardizing our features, the first 5 rows of our data now look like this:\\n')\nprint(pd.DataFrame(X_train_std, columns=iris_df.columns).head())","d961cd7e":"from matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\nmarkers = ('s', 'x', 'o')\ncolors = ('red', 'blue', 'lightgreen')\ncmap = ListedColormap(colors[:len(np.unique(y_test))])\nfor idx, cl in enumerate(np.unique(y)):\n    plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n               c=cmap(idx), marker=markers[idx], label=cl)","81df905c":"from sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0)\nsvm.fit(X_train_std, y_train)\n\nprint('The accuracy of the svm classifier on training data is {:.2f} out of 1'.format(svm.score(X_train_std, y_train)))\n\nprint('The accuracy of the svm classifier on test data is {:.2f} out of 1'.format(svm.score(X_test_std, y_test)))","e8fa6a80":"import warnings\n\n\ndef versiontuple(v):\n    return tuple(map(int, (v.split(\".\"))))\n\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=cmap(idx),\n                    marker=markers[idx], label=cl)","56dec805":"plot_decision_regions(X_test_std, y_test, svm)","58372e30":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nknn.fit(X_train_std, y_train)\n\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on training data'.format(knn.score(X_train_std, y_train)))\nprint('The accuracy of the knn classifier is {:.2f} out of 1 on test data'.format(knn.score(X_test_std, y_test)))","af2204c0":"plot_decision_regions(X_test_std, y_test, knn)","7f996869":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf = xgb_clf.fit(X_train_std, y_train)\n\nprint('The accuracy of the xgb classifier is {:.2f} out of 1 on training data'.format(xgb_clf.score(X_train_std, y_train)))\nprint('The accuracy of the xgb classifier is {:.2f} out of 1 on test data'.format(xgb_clf.score(X_test_std, y_test)))","22f61022":"plot_decision_regions(X_test_std, y_test, xgb_clf)","e9131f8f":"Next, we'll split the data into training and test datasets.\n-----------------------------------------------------------","295e042b":"Here we use Python to visualize how certain machine learning algorithms classify certain data points in the Iris dataset. Let's begin by importing the Iris dataset and splitting it into features and labels. We will use only the petal length and width for this analysis.\n\nThese visualizations and their code can be found in Sebastian Raschka's book, Python Machine Learning.","623065ad":"Let's try to use a Linear SVC to predict the the labels of our test data.","fa353819":"For many machine learning algorithms, it is important to scale the data. Let's do that now using sklearn.","29f2eba0":"Now, let's test out a KNN classifier.","c9a87615":"If we plot the original data, we can see that one of the classes is linearly separable, but the other two are not.","402b07d6":"It looks like our classifier performs pretty well. Let's visualize how the model classified the samples in our test data. \n","17623551":"And just for fun, we'll plot an XGBoost classifier.","505c98c9":"In all classifiers, the performance on the test data was better than the training data. At least with the parameters specified in this very simple approach, the KNN algorithm seems to have performed the best. However, this may not be the case depending on the dataset and more careful parameter tuning."}}