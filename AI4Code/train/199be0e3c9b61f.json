{"cell_type":{"880b1ad5":"code","c996bbb1":"code","0684995d":"code","f97138c2":"code","34ded879":"code","84edd668":"code","c6dcde02":"code","e5bc9e7f":"code","c99f5df2":"code","bfc4bd7a":"code","f61e55e3":"code","85ae262e":"code","7cb13a40":"code","00f58535":"code","72d67248":"code","1e44dc6c":"code","58961972":"code","2f748d9e":"code","6b6bc778":"code","00b39343":"code","63b11bb4":"code","a4328a80":"markdown","294b0142":"markdown","b57453eb":"markdown","191a99ad":"markdown","85249ee5":"markdown","e15beb6c":"markdown","e0130ba7":"markdown","f0cd4934":"markdown","1f530a3c":"markdown"},"source":{"880b1ad5":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom itertools import combinations\n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","c996bbb1":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n# Preview the data\ntrain.head()","0684995d":"cat_columns = train.drop(['id','target'], axis=1).select_dtypes(exclude=['int64','float64']).columns\nnum_columns = train.drop(['id','target'], axis=1).select_dtypes(include=['int64','float64']).columns\ntrain[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object']).columns] = train.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n","f97138c2":"num_columns=['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_columns=['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']\nall_columns = (num_columns+cat_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","34ded879":"###################\nRobustscaler  = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n                      \n                        RobustScaler()\n)\nOneHotencoder = make_pipeline(\n            SimpleImputer(strategy='most_frequent',add_indicator=True),\n            OneHotEncoder() )\n# Preprocess Pipe : \n##################\nOneHot_RobustScaler = make_column_transformer(\n    ( OneHotencoder , cat_columns),\n    ( Robustscaler, num_columns))","84edd668":"# Create arrays for the features and the response variable\ny = train['target']\nX = train.drop(['id','target'], axis=1)\nOneHot_RobustScaler.fit(X)\nXpre= OneHot_RobustScaler.transform(X)\ntest_final= test.drop(['id'], axis=1)\ntest_finalpre=OneHot_RobustScaler.transform(test_final)\n","c6dcde02":"# split into train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(Xpre, y, test_size=0.1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","e5bc9e7f":"# set learning rate scheduler\n# we can chage learning rate during learning\n\ndef lr_schedul(epoch):\n    x = 0.01\n    if epoch >= 5:\n        x = 0.005\n    if epoch >= 10:\n        x = 0.001\n    if epoch >= 15:\n        x = 0.0008\n    if epoch >= 20:\n        x = 0.0005\n    if epoch >= 30:\n        x = 0.0001\n    if epoch >= 60:\n        x = 0.00001        \n    return x\n\nlr_decay = LearningRateScheduler(\n    lr_schedul,\n    verbose=1,\n)","c99f5df2":"# mlp for binary classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# determine the number of input features\nn_features = X_train.shape[1]\n\n# define model\n# Create model here\nmodel =tf.keras.Sequential()\nmodel.add(layers.Dense(20,  kernel_initializer='he_normal', input_shape=(n_features,), activation = 'relu')) # Rectified Linear Unit Activation Function\nmodel.add(layers.Dense(10, activation = 'relu'))\nmodel.add(layers.Dense(1, activation = 'linear')) # linear for regression \n# Compile model here\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel.compile(loss = 'mean_squared_error', optimizer = optimizer,\n              metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')])\nmodel.summary()","bfc4bd7a":"EPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_loss',min_delta=0.0000000000001,\n                   restore_best_weights=True,patience=10)\n#batch_size=1000\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n     history = model.fit(Xpre,y,batch_size=256,epochs=EPOCHS, \n                         validation_split = 0.1,\n                         verbose=0 ,callbacks=[lr_decay ,es],shuffle=True)","f61e55e3":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['rmse']\n    val_acc = history.history['val_rmse']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training rmse')\n    plt.plot(x, val_acc, 'r', label='Validation rmse')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    \nplot_history(history)","85ae262e":"# evaluate the keras model\n\nloss, rmse = model.evaluate( X_test, y_test, verbose=2)\nprint(\" rmse\".format(rmse))","7cb13a40":"# mlp for binary classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# determine the number of input features\nn_features = X_train.shape[1]\n\n# define model\n# Create model here\nmodel_reg =tf.keras.Sequential()\nmodel_reg.add(layers.Dense(50,  input_shape=(n_features,), activation = 'relu')) # Rectified Linear Unit Activation Function\nmodel_reg.add(layers.BatchNormalization())\nmodel_reg.add(layers.Dropout(0.4))\nmodel_reg.add(layers.Dense(30, activation = 'relu'))\nmodel_reg.add(layers.Dropout(0.2))\nmodel_reg.add(layers.Dense(5, activation = 'relu'))\nmodel_reg.add(layers.Dense(1, activation = 'linear')) # linear for regression \n# Compile model here\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel_reg.compile(loss = 'mean_squared_error', optimizer = optimizer,\n              metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')])\nmodel_reg.summary()","00f58535":"tf.keras.utils.plot_model(model=model_reg, show_shapes=True, dpi=76, )","72d67248":"EPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_loss',min_delta=0.0000000000001,\n                   restore_best_weights=True,patience=10)\n#batch_size=1000\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n     history2 = model_reg.fit(Xpre,y,batch_size=256,epochs=EPOCHS, \n                         validation_split = 0.1,\n                         verbose=0 ,callbacks=[lr_decay ,es],shuffle=True)","1e44dc6c":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['rmse']\n    val_acc = history.history['val_rmse']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training rmse')\n    plt.plot(x, val_acc, 'r', label='Validation rmse')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    \nplot_history(history2)","58961972":"# evaluate the keras model\n\nloss2, rmse2 = model_reg.evaluate( X_test, y_test, verbose=2)\nprint(\" rmse\".format(rmse2))","2f748d9e":"# mlp for binary classification\nfrom pandas import read_csv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\n# determine the number of input features\nn_features = X_train.shape[1]\n# define model\n# Create model here\nmodel_reg2 =tf.keras.Sequential()\nmodel_reg2.add(layers.Dense(50,  input_shape=(n_features,),\n                            kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                            activation = 'relu')) # Rectified Linear Unit Activation Function\nmodel_reg2.add(layers.BatchNormalization())\nmodel_reg2.add(layers.Dropout(0.45))\nmodel_reg2.add(layers.Dense(40, activation = 'relu'))\nmodel_reg2.add(layers.Dropout(0.3))\nmodel_reg2.add(layers.Dense(30, activation = 'relu'))\nmodel_reg2.add(layers.Dropout(0.2))\nmodel_reg2.add(layers.Dense(5, activation = 'relu'))\nmodel_reg2.add(layers.Dense(1, activation = 'linear')) # linear for regression \n# Compile model here\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nmodel_reg2.compile(loss = 'mean_squared_error', optimizer = optimizer,\n              metrics = [tf.keras.metrics.RootMeanSquaredError(name='rmse')])\nEPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_loss',min_delta=0.0000000000001,\n                   restore_best_weights=True,patience=10)\n#batch_size=1000\n# fit model using our gpu\nwith tf.device('\/gpu:0'):\n     history2 = model_reg2.fit(Xpre,y,batch_size=256,epochs=EPOCHS, \n                         validation_split = 0.1,\n                         verbose=0 ,callbacks=[lr_decay ,es],shuffle=True)","6b6bc778":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['rmse']\n    val_acc = history.history['val_rmse']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training rmse')\n    plt.plot(x, val_acc, 'r', label='Validation rmse')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend() \n    \nplot_history(history2)","00b39343":"# evaluate the keras model\n\nloss3, rmse3 = model_reg2.evaluate( X_test, y_test, verbose=2)\nprint(\" rmse\".format(rmse3))","63b11bb4":"predictions = model_reg2.predict(test_finalpre)\npredictions = predictions.flatten()\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,\n                       'target': predictions})\noutput.to_csv('simpledeep.csv', index=False)\noutput","a4328a80":"# Sequential Model API (Simple)","294b0142":"938\/938 - 1s - loss: 0.5386 - rmse: 0.7339\n rmse\n","b57453eb":"# Summuray \n\nwith more deep mlp  we enhance the results \nlet's try other encoding tecthniques in order to have better results \n\nreference : \nto try : \n\nhttps:\/\/www.kaggle.com\/mtinti\/keras-starter-with-bagging-1111-84364\n\nhttps:\/\/www.kaggle.com\/faressayah\/tensorflow-2-tutorial-get-started-in-deep-learning","191a99ad":"# Load the training data","85249ee5":"## Regularization : ","e15beb6c":"# More deep :Regularization in Deep Learning \u2014 L1, L2,BatchNormalization, and Dropout","e0130ba7":"# Get Started in Deep Learning\nPredictive modeling with deep learning is a skill that modern developers need to know.\n\nTensorFlow is the premier open-source deep learning framework developed and maintained by Google. Although using TensorFlow directly can be challenging, the modern tf.keras API beings the simplicity and ease of use of Keras to the TensorFlow project.\n\nUsing tf.keras allows you to design, fit, evaluate, and use deep learning models to make predictions in just a few lines of code. It makes common deep learning tasks, such as classification and regression predictive modeling, accessible to average developers looking to get things done.\n\nIn this tutorial, you will discover a step-by-step guide to developing deep learning models in TensorFlow using the tf.keras API.\n Deep Learning Model Life-Cycle\n\nIn this section, you will discover the life-cycle for a deep learning model and the two tf.keras APIs that you can use to define models.\n##  The 5-Step Model Life-Cycle\n\nA model has a life-cycle, and this very simple knowledge provides the backbone for both modeling a dataset and understanding the tf.keras API.\n\nThe five steps in the life-cycle are as follows:\n\n    Define the model.\n    Compile the model.\n    Fit the model.\n    Evaluate the model.\n    Make predictions.","f0cd4934":"938\/938 - 1s - loss: 0.5353 - rmse: 0.7317\n rmse\n small enhancement ","1f530a3c":"# Prepare Data :"}}