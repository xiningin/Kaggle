{"cell_type":{"2604322f":"code","caa9fe90":"code","2f596043":"code","e8f7a7d1":"code","90a17633":"code","1b34fcab":"code","ad8a4a78":"code","9bf4a0bd":"code","de558345":"code","5b5cfa88":"code","581d35ff":"code","b3ff8e7a":"code","51cbc482":"code","03191e02":"code","904191a4":"code","e5066cdf":"code","b7a4d9ca":"code","2293693d":"code","7627b91c":"code","3004ca1c":"code","a3d75cbd":"code","40b4ee34":"code","39048355":"code","96cef01e":"code","d9873a0c":"code","6f9cee4a":"code","34c53339":"code","5aabe821":"code","b01ea6e7":"code","1b41e7e7":"code","f7509de2":"code","3db691e9":"code","ece3bd3d":"code","f978e174":"code","49e6d5cd":"code","7b938c40":"code","6ddd2536":"markdown","fcbf88d8":"markdown","07d77a1a":"markdown","04f007fa":"markdown","57b805a1":"markdown","89c43b41":"markdown","6e8bd552":"markdown","29b508d4":"markdown","e3220997":"markdown","8d3829b5":"markdown","c3d14193":"markdown","cb732642":"markdown","70e4658e":"markdown","f5590703":"markdown","31645b0e":"markdown","2d42ec55":"markdown","9f462481":"markdown","7290d065":"markdown","ed101383":"markdown","3851e59d":"markdown","91f7174c":"markdown","c3f69455":"markdown","3bdcf7f0":"markdown","44d87e13":"markdown","6e2249cb":"markdown","44d9fab0":"markdown","e6abe076":"markdown","7e8ec801":"markdown","016dd5bc":"markdown","98e9f004":"markdown","ce253b27":"markdown","38aff69d":"markdown","d8ac067a":"markdown","b6e230a0":"markdown","4f9fd9bf":"markdown","2a95e688":"markdown","169cdb57":"markdown","b9df80b2":"markdown"},"source":{"2604322f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# data prep\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# under\/over sampling\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n# modelling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# classification metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_recall_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","caa9fe90":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.head()","2f596043":"df.nunique()","e8f7a7d1":"df.drop(['id'], inplace=True, axis=1)","90a17633":"df[df['smoking_status'] == 'Unknown'].smoking_status.count()","1b34fcab":"df.describe()","ad8a4a78":"df[df['age'] < 1].head(5)","9bf4a0bd":"fig, ax = plt.subplots(1, 2, figsize=(12,8))\n\ndf['stroke'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%', ax=ax[0])\nsns.countplot(x='stroke', data=df, ax=ax[1])\n\nax[0].set_ylabel('')\nax[0].set_title('Stroke')\nax[1].set_title('Stroke')\n\nplt.show()","de558345":"# convert discrete variables with string values to numeric as label encoding\ndfcorr = df.copy()\ndfcorr[['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']] = dfcorr[['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']].astype('category')\ndfcorr['gender'] = dfcorr['gender'].cat.codes\ndfcorr['ever_married'] = dfcorr['ever_married'].cat.codes\ndfcorr['work_type'] = dfcorr['work_type'].cat.codes\ndfcorr['Residence_type'] = dfcorr['Residence_type'].cat.codes\ndfcorr['smoking_status'] = dfcorr['smoking_status'].cat.codes\n\ncorr = dfcorr.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(10,8))\n\ncmap= sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\":.5})\n\nplt.title('Heatmap of all variables')\n\nplt.show()","5b5cfa88":"corr2 = df.corr()\nmask = np.triu(np.ones_like(corr2, dtype=bool))\n\nfig, ax = plt.subplots(figsize=(10,8))\n\ncmap= sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(corr2, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\":.5})\n\nplt.title('Heatmap with only health-related variables')\n\nplt.show()","581d35ff":"df[['gender','stroke']].groupby(['gender']).count()","b3ff8e7a":"df = df[df['gender'] != 'Other']","51cbc482":"# male\ndfmale = df[df['gender'] == 'Male']\ndfmale = dfmale['stroke'].value_counts(normalize=True)\ndfmale = dfmale.mul(100)\ndfmale = dfmale.rename('percent').reset_index()\n\n# female\ndffemale = df[df['gender'] == 'Female']\ndffemale = dffemale['stroke'].value_counts(normalize=True)\ndffemale = dffemale.mul(100)\ndffemale = dffemale.rename('percent').reset_index()\n\n\n\nfig, ax = plt.subplots(2, 2, figsize=(14,10))\n\ndf[['gender','stroke']].groupby(['gender']).count().plot.bar(ax=ax[0,0])\nsns.countplot(x='gender', hue='stroke', data=df, ax=ax[0,1])\nsns.barplot(x='index', y='percent', data=dfmale, ax=ax[1,0])\nsns.barplot(x='index', y='percent', data=dffemale, ax=ax[1,1])\n\nax[0,0].set_title('Gender Count')\nax[0,1].set_title('Gender and Stroke')\nax[1,0].set_title('Male + Stroke')\nax[1,1].set_title('Female + Stroke')\n\nax[1,0].set_xlabel('Stroke')\nax[1,1].set_xlabel('Stroke')\n\nax[0,0].set_xticklabels(ax[0,0].get_xticklabels(), rotation=0) # labels were vertical\nax[0,0].invert_xaxis() # male and female labels in wrong order\nax[0,0].get_legend().remove()\n\nax[1,0].text(0, 50, \"95.58%\", va='center', ha='center', fontsize=20)\nax[1,0].text(1, 4.7, \"4.43%\", ha='center', fontsize=20)\n\nax[1,1].text(0, 50, \"95.86%\", va='center', ha='center', fontsize=20)\nax[1,1].text(1, 4.3, \"4.15%\", ha='center', fontsize=20)\n\nfig.tight_layout()\nplt.show()","03191e02":"fig = plt.figure(figsize=(20,10))\n\nax = [None for _ in range(5)] # List to save ax for setting parameter\n\nax[0] = plt.subplot2grid((3,4), (0,0), colspan = 2)\nax[1] = plt.subplot2grid((3,4), (1,0), colspan = 1)\nax[2] = plt.subplot2grid((3,4), (1,1), colspan = 1)\nax[3] = plt.subplot2grid((3,4), (2,0), colspan = 1)\nax[4] = plt.subplot2grid((3,4), (2,1), colspan = 1)\n\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df, ax=ax[0])\nsns.histplot(x='age', binwidth=5, data=df[df['stroke'] == 0], ax=ax[1])\nsns.histplot(x='age', color= '#FF8C00', binwidth=5, data=df[df['stroke'] == 1], ax=ax[2])\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df[df['gender'] == 'Male'], ax=ax[3])\nsns.histplot(x='age', hue='stroke', multiple='stack', binwidth=5, data=df[df['gender'] == 'Female'], ax=ax[4])\n\nax[0].set_title('Age and Stroke\/No Stroke')\nax[1].set_title('Age and No Stroke')\nax[2].set_title('Age and Stroke')\nax[3].set_title('Age, Male, Stroke')\nax[4].set_title('Age, Female, Stroke')\n\nfig.tight_layout()\nplt.show()","904191a4":"df[(df['age'] < 20) & (df['stroke'] == 1)]","e5066cdf":"# no hypertension\ndfnohyper = df[df['hypertension'] !=1]\ndfnohyper = dfnohyper['stroke'].value_counts(normalize=True)\ndfnohyper = dfnohyper.mul(100)\ndfnohyper = dfnohyper.rename('percent').reset_index()\n\n# with hypertension\ndfhyper = df[df['hypertension'] !=0]\ndfhyper = dfhyper['stroke'].value_counts(normalize=True)\ndfhyper = dfhyper.mul(100)\ndfhyper = dfhyper.rename('percent').reset_index()\n\nfig, ax = plt.subplots(1,3, figsize=(16,6))\n\nsns.countplot(x='hypertension', hue='stroke', data=df, ax=ax[0])\nsns.barplot(x='index', y='percent', data=dfnohyper, ax=ax[1])\nsns.barplot(x='index', y='percent', data=dfhyper, ax=ax[2])\n\nax[2].set(ylim=(0, 100))\n\nax[0].set_title('Count of Stroke and Hypertension')\nax[1].set_title('No Hypertension with Stroke')\nax[2].set_title('Hypertension with Stroke')\n\nax[1].set_xlabel('Stroke')\nax[2].set_xlabel('Stroke')\n\nax[1].text(0, 48, \"96.66%\", va='center', ha='center', fontsize=20)\nax[1].text(1, 3.5, \"3.34%\", ha='center', fontsize=20)\nax[2].text(0, 43, \"86.70%\", ha='center', fontsize=20)\nax[2].text(1, 4.7, \"13.30%\", ha='center', fontsize=20)\n\nplt.show()","b7a4d9ca":"fig, ax= plt.subplots(1, 2, figsize=(16,8))\n\nsns.violinplot(x='hypertension', y='age', hue='stroke', data=df, split=True, ax=ax[0])\nsns.boxplot(x='hypertension', y='age', hue='stroke', data=df, ax=ax[1])\n\nax[0].set_title('Hypertension, age, stroke')\nax[1].set_title('Hypertension, age, stroke')\n\nplt.show()","2293693d":"# no heart disease\ndfnohd = df[df['heart_disease'] !=1]\ndfnohd = dfnohd['stroke'].value_counts(normalize=True)\ndfnohd = dfnohd.mul(100)\ndfnohd = dfnohd.rename('percent').reset_index()\n\n# with heart disease\ndfhd = df[df['heart_disease'] !=0]\ndfhd = dfhd['stroke'].value_counts(normalize=True)\ndfhd = dfhd.mul(100)\ndfhd = dfhd.rename('percent').reset_index()\n\nfig, ax = plt.subplots(1,3, figsize=(16,6))\n\nsns.countplot(x='heart_disease', hue='stroke', data=df, ax=ax[0])\nsns.barplot(x='index', y='percent', data=dfnohd, ax=ax[1])\nsns.barplot(x='index', y='percent', data=dfhd, ax=ax[2])\n\nax[2].set(ylim=(0, 100))\n\nax[0].set_title('Count of Stroke and Heart Disease')\nax[1].set_title('No Heart Disease with Stroke')\nax[2].set_title('Heart Disease with Stroke')\n\nax[1].set_xlabel('Stroke')\nax[2].set_xlabel('Stroke')\n\nax[1].text(0, 48, \"96.38%\", va='center', ha='center', fontsize=20)\nax[1].text(1, 3.9, \"3.62%\", ha='center', fontsize=20)\nax[2].text(0, 43, \"83.54%\", ha='center', fontsize=20)\nax[2].text(1, 8, \"16.46%\", ha='center', fontsize=20)\n\nplt.show()","7627b91c":"fig, ax = plt.subplots(1,2, figsize=(16, 6))\n\nsns.histplot(x='avg_glucose_level', hue='stroke', multiple='stack', data=df, binwidth=10, ax=ax[0])\nsns.histplot(x='avg_glucose_level', color= '#FF8C00', binwidth=10, data=df[df['stroke'] == 1], ax=ax[1])\n\nax[0].set_title('Average Glucose Level')\nax[1].set_title('Average Glucose Level with Stroke')\nplt.show()","3004ca1c":"fig, ax = plt.subplots(1,2, figsize=(16,6))\n\nsns.violinplot(x='stroke', y='avg_glucose_level', data=df, ax=ax[0])\nsns.boxplot(x='stroke', y='avg_glucose_level', data=df, ax=ax[1])\n\nax[0].set_title('stroke and avg glucose level')\nax[1].set_title('stroke and avg glucose level')\n\n\nplt.show()","a3d75cbd":"fig, ax = plt.subplots(1,2, figsize=(16, 6))\n\nsns.histplot(x='bmi', hue='stroke', multiple='stack', data=df, binwidth=5, ax=ax[0])\nsns.histplot(x='bmi', color= '#FF8C00', binwidth=5, data=df[df['stroke'] == 1], ax=ax[1])\n\nax[0].set_title('bmi')\nax[1].set_title('bmi with stroke')\nplt.show()","40b4ee34":"fig, ax = plt.subplots(figsize=(20,12))\n\nax = [None for _ in range(3)] # create # of axes\n\nax[0] = plt.subplot2grid((4,4), (0,0), colspan=2)\nax[1] = plt.subplot2grid((4,4), (1,0), colspan=1)\nax[2] = plt.subplot2grid((4,4), (1,1), colspan=1)\n    \n    \nsns.regplot(x='avg_glucose_level', y='bmi', data=df, ax=ax[0], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\nsns.regplot(x='avg_glucose_level', y='bmi', data=df[df['stroke'] == 0], ax=ax[1], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\nsns.regplot(x='avg_glucose_level', y='bmi', color= '#FF8C00', data=df[df['stroke'] == 1], ax=ax[2], scatter_kws={'s':2}, line_kws={\"color\": \"black\", 'linewidth':1})\n\nax[0].set_title('avg glucose level and BMI')\nax[1].set_title('No Stroke')\nax[2].set_title('Stroke')\n\nax[2].set_ylim(0, 100)\n\nplt.tight_layout()\nplt.show()","39048355":"df.groupby(['smoking_status'])['smoking_status'].describe()","96cef01e":"fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nsns.violinplot(x='smoking_status', y='age', data=df, ax=ax[0])\nsns.boxplot(x='smoking_status', y='age', data=df, ax=ax[1])\n\nax[0].set_title('Age and smoking status')\nax[1].set_title('Age and smoking status')\n\nplt.show()","d9873a0c":"# split the unknown smoking status into two groups: Unknown >20 age, and Unknown <20 age\ndf[(df['smoking_status'] == 'Unknown') & (df['age'] > 20)] = df[(df['smoking_status'] == 'Unknown') & (df['age'] > 20)].replace('Unknown','Unknown over 20')\ndf['smoking_status'] = df['smoking_status'].replace('Unknown', 'Unknown under 20')","6f9cee4a":"pd.crosstab(df.smoking_status, df.stroke,\n           rownames=['smoking_status'], colnames=['stroke'])","34c53339":"fig, ax = plt.subplots(figsize=(16,6))\n\nsns.countplot(x='smoking_status', hue='stroke', data=df)\n\nax.text(-0.2, 400, \"92.21%\", va='center', ha='center', fontsize=15)\nax.text(0.2, 100, \"7.79%\", va='center', ha='center', fontsize=15)\nax.text(0.8, 400, \"95.24%\", va='center', ha='center', fontsize=15)\nax.text(1.2, 120, \"4.76%\", va='center', ha='center', fontsize=15)\nax.text(1.8, 400, \"94.68%\", va='center', ha='center', fontsize=15)\nax.text(2.2, 80, \"5.32%\", va='center', ha='center', fontsize=15)\nax.text(2.8, 400, \"94.37%\", va='center', ha='center', fontsize=15)\nax.text(3.2, 80, \"5.63%\", va='center', ha='center', fontsize=15)\nax.text(3.8, 400, \"99.97%\", va='center', ha='center', fontsize=15)\nax.text(4.2, 40, \"0.30%\", va='center', ha='center', fontsize=15)\n\nax.text(3.7, 1800, \"Note: local % for each group\", va='center', ha='center', fontsize=15)\n\nplt.title('Smoking Status and Stroke')\n\nplt.tight_layout()\nplt.show()","5aabe821":"df.isnull().sum()\n# will simple impute the bmi null values later on","b01ea6e7":"# separate features and target\nX = df.drop(['stroke'], axis=1)\ny = df['stroke']\n\n# split the data first before doing any\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n\n# Validation set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n\n# separate numerical and categorical columns\nX_num = X[['age', 'avg_glucose_level', 'bmi']]\nX_cat = X[['gender', 'hypertension', 'heart_disease', 'ever_married', \n           'work_type', 'smoking_status', 'Residence_type']]","1b41e7e7":"# pipeline\n\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n     ('scaler', StandardScaler())\n])\n    \ncat_pipeline = Pipeline([\n    ('onehot', OneHotEncoder(sparse=False, drop='first'))\n])\n\nnum_colnames = list(X_num)\ncat_colnames = list(X_cat)\n\n\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_colnames),\n    ('cat', cat_pipeline, cat_colnames)\n])\n\n# SMOTE (resampling)\npipelinebsmote = Pipeline([\n    ('smote', SMOTE(random_state=1))\n])\n\n# training set\nX_train = full_pipeline.fit_transform(X_train)\nX_train, y_train = pipelinebsmote.fit_resample(X_train, y_train)\n\n# validation set\nX_valid = full_pipeline.transform(X_valid)\nX_valid, y_valid = pipelinebsmote.fit_resample(X_valid, y_valid)\n\n# test set \nX_test = full_pipeline.transform(X_test)\n\n# Make sure test set is not resampled ","f7509de2":"# create an empty df with formatted index to be filled later on\niterables = [['LOG', 'KNN', 'SVC', 'RF', 'DT', 'GB'], ['ROC AUC', 'Precision', 'Recall', 'F1']]\niterabletuples = pd.MultiIndex.from_product(iterables, names=[\"Model\", \"Metric\"])\nresults = pd.DataFrame(index = iterabletuples) \n\n\ndef evalmodel(model, X, y):\n    modelpred = model.predict(X)\n    modelroc = roc_auc_score(y, modelpred)\n    modelprec = precision_score(y, modelpred)\n    modelrecall = recall_score(y, modelpred)\n    modelf1 = f1_score(y, modelpred)\n    ROC = round(modelroc, 3)\n    prec = round(modelprec, 3)\n    recall = round(modelrecall, 3)\n    F1 = round(modelf1, 3)\n    return ROC, prec, recall, F1\n\n# models\n\nlogclf = LogisticRegression(random_state=1)\nknnclf = KNeighborsClassifier()\nsvcclf = SVC(random_state=1)\nrfclf = RandomForestClassifier(random_state=1)\ndtclf = DecisionTreeClassifier(random_state=1)\ngbclf = GradientBoostingClassifier(random_state=1)\n\nlistofmodels = [logclf, knnclf, svcclf, rfclf, dtclf, gbclf]\ntrainresults= [] # empty list to be converted to a column for results df\n\nfor i in listofmodels:\n    i.fit(X_train, y_train)\n\nfor a in listofmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_valid, y_valid)\n    trainresults.extend([ROC, prec, recall, F1])\n\nresults['Training Set'] = trainresults\nresults","3db691e9":"# KNN\nKNNparam = {\n    'n_neighbors': range(3,15),\n    'weights': ['uniform', 'distance'],\n    'leaf_size': np.linspace(10,50,4)\n    \n}\n\nKNNgrid = RandomizedSearchCV(KNeighborsClassifier(), KNNparam, cv=3, scoring='roc_auc')\nKNNgrid.fit(X_train, y_train)\n\n\n# SVC\nSVCparam = {\n    'C':[0.01, 0.1, 0.9, 1, 1.1, 10, 100],\n    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n    'degree': range(1,10)\n    \n}\n\nSVCgrid = RandomizedSearchCV(SVC(random_state=1), SVCparam, cv=3, scoring='roc_auc')\nSVCgrid.fit(X_train, y_train)\n\n\n# LOG\nLOGparam = {\n    'C':[0.01, 0.1, 0.9, 1, 1.1, 10, 100],\n    'class_weight':['dict', 'balanced']\n}\n\nLOGgrid = RandomizedSearchCV(LogisticRegression(random_state=1), LOGparam, cv=3, scoring='roc_auc')\nLOGgrid.fit(X_train, y_train)\n\n\n\n# RF\nRFparam = {\n    'max_depth': np.linspace(2,32,2),\n    'n_estimators': range(100,1100,100),\n    'min_samples_split': range(2,22,2)\n}\n\nRFgrid = RandomizedSearchCV(RandomForestClassifier(random_state=1), RFparam, cv=3, scoring='roc_auc')\nRFgrid.fit(X_train, y_train)\n\n# DT\nDTparam = {\n    'splitter': ['best','random'],\n    'max_depth': np.linspace(2,32,2),\n    'min_samples_split': range(2,22,2)\n}\n\nDTgrid = RandomizedSearchCV(DecisionTreeClassifier(random_state=1), DTparam, cv=3, scoring='roc_auc')\nDTgrid.fit(X_train, y_train)\n\n# GB\nGBparam = {\n    'max_depth': np.linspace(2,32,2),\n    'n_estimators': range(100,1100,100),\n    'learning_rate': [0.01,0.05,0.1,0.5, 1]\n}\n\nGBgrid = RandomizedSearchCV(GradientBoostingClassifier(random_state=1), GBparam, cv=3, scoring='roc_auc')\nGBgrid.fit(X_train, y_train)","ece3bd3d":"listofgridmodels = [LOGgrid.best_estimator_, KNNgrid.best_estimator_, SVCgrid.best_estimator_, \n                RFgrid.best_estimator_, DTgrid.best_estimator_, GBgrid.best_estimator_]\n\ngridresults = [] # empty list to be converted to a column for results df\n\nfor a in listofgridmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_valid, y_valid)\n    gridresults.extend([ROC, prec, recall, F1])\n\nresults['Grid Set'] = gridresults\nresults","f978e174":"listofgridmodels = [LOGgrid.best_estimator_, KNNgrid.best_estimator_, SVCgrid.best_estimator_, \n                RFgrid.best_estimator_, DTgrid.best_estimator_, GBgrid.best_estimator_]\n\ntestresults = [] # empty list to be converted to a column for results df\n\nfor a in listofgridmodels:\n    ROC, prec, recall, F1 = evalmodel(a, X_test, y_test)\n    testresults.extend([ROC, prec, recall, F1])\n\nresults['Test Set'] = testresults\nresults","49e6d5cd":"y_predict_proba = LOGgrid.best_estimator_.predict_proba(X_test)[:,1]\nprecision, recall, thresholds = precision_recall_curve(y_test, y_predict_proba)\nrecall = recall[:-1]\nprecision = precision[:-1]\n# :-1 slice because threshold does not exist for last value in array for precision and recall","7b938c40":"# Plot precision\/recall curve with and w\/o threshold values\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,6))\n\nsns.lineplot(recall, precision, ax=ax1) # w\/o threshold values\n\nsns.lineplot(x=thresholds, y=recall, color=\"g\", ax=ax2) # with threshold values\n\nax12 = plt.twinx()\nsns.lineplot(x=thresholds, y=precision, color=\"b\", ax=ax12) \n# axis labels\nax1.set_xlabel('Recall')\nax1.set_ylabel('Precision')\nax12.set_ylabel('Precision')\nax2.set_ylabel('Recall')\nax2.set_xlabel('Thresholds')\n\nax2.yaxis.label.set_color('g')\nax2.tick_params(axis='y', colors='g')\n\nax12.yaxis.label.set_color('b')\nax12.tick_params(axis='y', colors='b')\n\nax1.set_title('Precision vs Recall Curve for LogisticRegression')\nax2.set_title('Precision\/Recall curves with Threshold values for LogisticRegression')\n# show the plot\nplt.show()","6ddd2536":"Patients who formerly smoked or smokes had a slightly higher occurance of stroke than patients who never smoked.","fcbf88d8":"This data suggests that the older the patient is, the more likely that they will have a stroke. There are a few cases where a patient had a stroke before the age of 20.","07d77a1a":"### Precision\/Recall Plots for Logistic Regression","04f007fa":"seems like age is correlated with everything","57b805a1":"Now a child at the age of about 1 and 14 having a stroke is surprising. Not sure whether this is a misinput or not. Maybe the stroke was due to a rare disease\/condition? This makes me question what was considered a stroke when this data was collected, but this is out of our scope for now.","89c43b41":"Even though there are more females than males recorded in the data, the relative % of males and females having experienced a stroke is roughly equal.","6e8bd552":"Although the count of patients with heart disease is very low, there is relatively a 12.84% increased occurance of strokes with patients that have a heart disease","29b508d4":"# Randomized Search","e3220997":"This plot better describes that with higher avg_glucose level, the higher risk of having a stroke. We can see more clearly than the histogram that there is a portionally larger distribution of patients with a higher average glucose level having experienced a stroke.","8d3829b5":"# Evaluate model on Test Set","c3d14193":"Within this notebook, there is a basic exploratory data analysis and a comparison of classification models in predicting stroke in patients.\nFeedback, criticism and comments are much appreciated.","cb732642":"# Modelling","70e4658e":"Again with the imbalanced class issue, we see irregular shaped curves from both plots and so it's difficult to determine the best threshold value depending on if you want higher precision or recall score while optimally balancing between the two.","f5590703":"Seems that there is a lot of unknown values for ages below 20. Let's trying dividing up this group","31645b0e":"The shape of the second peak (avg_glucose_level above 150) for patients with stroke looks relatively higher than the second peak of the left plot, suggesting that there is a higher occurance of stroke in patients with a higher average glucose level ","2d42ec55":"# Gender","9f462481":"# Age","7290d065":"the larger distribution of having hypertension is located at older ages","ed101383":"Attribute Info:\n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","3851e59d":"The GradientBoostingClassifier model seems to be doing the best with the validation data","91f7174c":"Since 'Unknown' smoking status is a large portion of the data, we will not drop it.","c3f69455":"The lifestyle variables, residence_type, work_type and smoking_status don't correlate well with stroke. However, smoking_status has a lot of unknown values. Smoking_status will be investigated further in the notebook.","3bdcf7f0":"The minimum age is 0.08. Lets investigate this","44d87e13":"# Preparing the Data","6e2249cb":"# Average Glucose Level","44d9fab0":"# Target Variable","e6abe076":"# Heart Disease","7e8ec801":"Note that we are dealing with an imbalanced data set. About 5:95 or 1:19 ratio","016dd5bc":"# Correlation","98e9f004":"Are these patients truly younger than 1 year when this data was recorded? I don't know too much about how the data was collected and if these are misinputs, but these values could be true because their work type is 'children'","ce253b27":"patients with hypertension have a higher occurrance of stroke","38aff69d":"lets remove 'Other' since there is only 1 entry","d8ac067a":"# Hypertension","b6e230a0":"Slight positive correlation from all graphs","4f9fd9bf":"# BMI","2a95e688":"looks like LogisticRegression generalized the best since it has the best ROC AUC score.\n\n- Note all models seem to overfit the training data and not generalize well with the test data. However, the training set has been resampled with SMOTE while the test data has not. As a result, the models are predicting on data that is severely imbalanced (few cases of patients with stroke)\n- Interestingly enough for the LogisticRegression and the KNN model, the precision has definitely decreased with the test set, but the recall scores stayed consistent.","169cdb57":"# Exploratory Data Analysis","b9df80b2":"# Smoking Status"}}