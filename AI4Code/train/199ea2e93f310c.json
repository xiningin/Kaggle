{"cell_type":{"f8cba8db":"code","42fcc953":"code","56615333":"code","aaf3a32f":"code","60fa5cfb":"code","7d2ad111":"code","85af7c64":"code","df80093b":"code","d2145ac0":"markdown","ce977562":"markdown","61a8e58a":"markdown","15b46df3":"markdown","893f26f5":"markdown","cb9a5132":"markdown","da605bfe":"markdown","499628a7":"markdown"},"source":{"f8cba8db":"import os\nimport gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython import display\nimport time\nimport cv2\nimport tensorflow as tf\nimport random as rand\nfrom collections import deque","42fcc953":"!pip install gym[atari] ","56615333":"GAME = \"CartPole-v0\"\nenv = gym.envs.make(GAME)\nprint(\"Action space: {}\".format(env.action_space))\nprint(\"Action space size: {}\".format(env.action_space.n))\nobservation = env.reset()\nprint(\"Observation space shape: {}\".format(observation.shape))\n\nprint(\"-\"*10)\naction = env.action_space.sample()\nprint('Take action {}'.format(action))\nobservation, reward, game_over, info = env.step(action)\nprint(\"observation: {}, reward: {}, game_over: {}, info: {} \".format(observation.shape, reward, game_over, info))\n\nenv.close()","aaf3a32f":"class ReplayBuffer:\n    def __init__(self, buffer_size):\n        self.buffer_size = buffer_size\n        # deque is actually implemented as linked list, so this is a suboptimal solution for random sampling. A custom ring buffer would be better.\n        # However, for education purposes this will suffice.\n        self.replay_memory = deque(maxlen=buffer_size)    \n\n    def add(self, state, action, reward, next_state, done):\n        self.replay_memory.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        if batch_size <= len(self.replay_memory):\n            return rand.sample(self.replay_memory, batch_size)\n        else:\n            assert False\n\n    def __len__(self):\n        return len(self.replay_memory)","60fa5cfb":"class LinearSchedule():\n    def __init__(self, start_epsilon=1, final_epsilon=0.1, pre_train_steps=10, final_exploration_step=100):\n        self.pre_train_steps = pre_train_steps\n        self.final_exploration_step = final_exploration_step\n        self.final_epsilon = final_epsilon\n        self.decay_factor = self.pre_train_steps\/self.final_exploration_step\n        self.epsilon = self.pre_train_steps * (1-self.decay_factor) + self.final_exploration_step * self.decay_factor\n    \n    def value(self, t):\n        if t > self.pre_train_steps:\n            self.decay_factor = (t - self.pre_train_steps)\/self.final_exploration_step\n            self.epsilon = 1-self.decay_factor\n            self.epsilon = max(self.final_epsilon, self.epsilon)\n            return self.epsilon\n        else:\n            return 1","7d2ad111":"class DQN(tf.keras.Model):\n    def __init__(self, input_shape, num_actions):\n        super(DQN, self).__init__()\n        self.input_layer = tf.keras.layers.InputLayer(input_shape=input_shape)\n        self.hidden_layers = []\n        self.hidden_layers.append(tf.keras.layers.Dense(64, activation='relu'))\n        self.hidden_layers.append(tf.keras.layers.Dense(32, activation='relu'))\n        self.output_layer = tf.keras.layers.Dense(units=num_actions, activation='linear')\n\n    @tf.function\n    def call(self, inputs):\n        z = self.input_layer(inputs)\n        for l in self.hidden_layers:\n            z = l(z)\n        q_vals = self.output_layer(z)\n        return q_vals","85af7c64":"class Agent:\n    def __init__(self, epsilon_schedule, replay_buffer, num_actions=2, gamma=0.9, batch_size=64, lr=0.001,\n                 max_episodes = 500, max_steps_per_episode=2000, steps_until_sync=20, choose_action_frequency=1,\n                 pre_train_steps = 1, train_frequency=1):\n        \n        # dqn is used to predict Q-values to decide which action to take\n        self.dqn = DQN([4], num_actions)\n        self.dqn.build(tf.TensorShape([None, 4]))\n        \n        # dqn_target is used to predict the future reward\n        self.dqn_target = DQN([4], num_actions)\n        self.dqn_target.build(tf.TensorShape([None, 4]))\n\n        self.num_actions = num_actions\n        self.batch_size = batch_size\n        self.optimizer = tf.optimizers.Adam(lr)\n        self.gamma = gamma\n        self.replay_buffer = replay_buffer\n        self.epsilon_schedule = epsilon_schedule\n        self.steps_until_sync = steps_until_sync\n        self.choose_action_frequency = choose_action_frequency\n        self.max_episodes = max_episodes\n        self.max_steps_per_episode = max_steps_per_episode\n        self.train_frequency = train_frequency\n        self.loss_function = tf.keras.losses.MSE\n        self.pre_train_steps = pre_train_steps\n        \n        self.episode_reward_history = []\n\n    def predict_q(self, inputs):\n        return self.dqn(inputs)\n\n    def get_action(self, states, epsilon):\n        if np.random.random() < epsilon:\n            # explore\n            return np.random.choice(self.num_actions)\n        else:\n            # exploit\n            return np.argmax(self.predict_q(np.expand_dims(states, axis=0))[0])\n\n    def update_target_network(self):\n        self.dqn_target.set_weights(self.dqn.get_weights())\n\n    def train_step(self):\n        mini_batch = self.replay_buffer.sample(self.batch_size)\n\n        observations_batch, action_batch, reward_batch, next_observations_batch, done_batch = map(np.array,\n                                                                                                  zip(*mini_batch))\n\n        with tf.GradientTape() as tape:\n            dqn_variables = self.dqn.trainable_variables\n            tape.watch(dqn_variables)\n\n            future_rewards = self.dqn_target(tf.convert_to_tensor(next_observations_batch, dtype=tf.float32))\n            next_action = tf.argmax(future_rewards, axis=1)\n            target_q = tf.reduce_sum(tf.one_hot(next_action, self.num_actions) * future_rewards, axis=1)\n            target_q = (1 - done_batch) * self.gamma * target_q + reward_batch\n\n            predicted_q = self.dqn(tf.convert_to_tensor(observations_batch, dtype=tf.float32))\n            predicted_q = tf.reduce_sum(tf.one_hot(action_batch, self.num_actions) * predicted_q, axis=1)\n            loss = self.loss_function(target_q, predicted_q)\n            \n        # Backprop\n        gradients = tape.gradient(loss, dqn_variables)\n        self.optimizer.apply_gradients(zip(gradients, dqn_variables))\n        \n        return loss\n\n    def train(self, env):\n        episode = 0\n        total_step = 0\n        episode_step = 0\n        state = env.reset()\n        loss = 0\n        last_hundred_rewards = deque(maxlen=100)\n\n        while episode < self.max_episodes:\n            current_state = env.reset()\n            done = False\n            action = 0\n            episode_reward = 0\n            episode_step = 0\n            epsilon = epsilon_schedule.value(total_step)\n\n            while not done:\n                if total_step % self.choose_action_frequency == 0:\n                    if len(replay_buffer) > self.batch_size:\n                        action = self.get_action(current_state, epsilon)\n                    else:\n                        action = self.get_action(current_state, 1.0)\n\n                next_state, reward, done, info = env.step(action)\n                \n                self.replay_buffer.add(current_state, action, reward, next_state, done)\n                episode_reward += reward\n\n                if total_step > self.pre_train_steps and len(replay_buffer) > self.batch_size:\n                    loss = self.train_step()\n\n                if total_step % self.steps_until_sync == 0:\n                    self.update_target_network()\n                                    \n                #end of step\n                total_step += 1\n                episode_step += 1\n                current_state = next_state\n                \n            # end of episode\n            self.episode_reward_history.append(episode_reward)\n            last_hundred_rewards.append(episode_reward)\n            mean_episode_reward = np.mean(last_hundred_rewards)\n            \n            if episode % 50 == 0:\n                print(f'Episode {episode} (Step {total_step}) - Moving Avg Reward: {mean_episode_reward:.3f} Loss: {loss:.5f} Epsilon: {epsilon:.3f}')\n\n            if mean_episode_reward >= 195:\n                print(f'Task solved after {episode} episodes! (Moving Avg Reward: {mean_episode_reward:.3f})')\n                return\n                \n            episode += 1\n            \n","df80093b":"env = gym.envs.make(GAME)\n\nepsilon_schedule = LinearSchedule(start_epsilon=1, final_epsilon=0.1, pre_train_steps=100, final_exploration_step=10_000)\n\nreplay_buffer = ReplayBuffer(32_000)\n\nagent = Agent(epsilon_schedule, replay_buffer, num_actions=2, gamma=0.99, batch_size=64, lr=0.0007,\n                 max_episodes=3000, steps_until_sync=200, choose_action_frequency=1)\nagent.train(env)\n\nenv.close()","d2145ac0":"## Experience Replay\n\nThe **replay buffer** stores the last $N$ state transitions as tuples $(S, A, R, S')$ in a ring buffer.\nThis way collected experiences can be reused and the correlation between samples is broken by sampling random minibatches of the buffer.","ce977562":"# Training Loop\nThe **Agent** class is wrapping all the necessary building blocks and ties their functionality together to train the model.\n","61a8e58a":"## Check out the game","15b46df3":"## Training","893f26f5":"## Model Implementation\n\nSince states of the CartPole-v0 environment are rather simple (i.e. consisting of 4 float values each), a simple fully-connected neural network is also sufficient.","cb9a5132":"# Dependencies","da605bfe":"## Epsilon Schedule\n\nAn epsilon schedule gives more fine grained control over the way the e-greedy policy unfolds.\nIn this case, a schedule is implemented that linearly interpolates between a start and end epsilon value.\nAdditionally, the schedule can be delayed for a given amount of steps to emphasize exploration very early during training and fill the replay buffer with samples to start the training.","499628a7":"# Reinforcement Learning - Q-Network\n\nIn this notebook a vanilla Q-Network is implemented with Tensorflow 2.3 and trained to complete the CartPole-v0 environment of the OpenAI gym.\nThe **focus** of this implementation is to get the algorithm to work and provide a starting point for further tweaks and experiments.\nTherefore: Simple architecture, simple task, no fancy extras.\n\n## This includes:\n- Experience Replay \/ Replay Buffer\n- An epsilon schedule\n- The E-Greedy Policy\n- The compute graph of the Deep Q-Network\n- Weight synchronization between Q-Network and Target-Network\n- Implementation of the training loop\n\n## References\n- Mnih et al. 2013, [\"Playing Atari with Deep Reinforcement Learning\"](https:\/\/arxiv.org\/abs\/1312.5602)\n- Mnih et al. 2015, [\"Human Level Control Through Deep Reinforcement Learning\"](https:\/\/deepmind.com\/research\/publications\/human-level-control-through-deep-reinforcement-learning)"}}