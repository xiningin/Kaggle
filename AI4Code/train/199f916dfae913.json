{"cell_type":{"04d0b0be":"code","e6177301":"code","a092d2cd":"code","396b36a5":"code","64bc73ed":"code","91fae5e7":"code","7ad1a03d":"code","e46905b0":"code","5cd7da72":"code","7a672c44":"code","eb0f904c":"code","3188f555":"code","97f178b2":"code","95347141":"code","d59a5309":"code","fd5aa8b4":"code","df6fbf67":"code","50a3e64f":"code","13cb81f7":"code","4c67a078":"code","bcd76f29":"code","3bfe691e":"code","7e9b4dca":"code","ddb771b3":"code","bb692bfd":"code","29febdc5":"code","18afd3e0":"code","c960a814":"code","28fd6578":"code","bc186696":"code","d3c1bdcc":"code","e4ceb103":"code","e1815752":"code","39619898":"code","01e14935":"code","9775d255":"code","be34bfc2":"code","024692d6":"code","9feb0ba9":"code","1f1ba4ad":"code","18d2596b":"code","ebdb889b":"code","6797d9be":"code","028f60d6":"code","b0480526":"code","b0c2d959":"code","b20858af":"code","f812758c":"code","07f9399b":"code","0016b4e9":"code","2099e40d":"code","88ee5029":"code","a402c825":"code","2d561e76":"code","ec0aab36":"code","a3543c6a":"code","f90c3ab9":"code","20d7f533":"code","82d6a386":"code","9089fdfc":"code","7415d72c":"code","4c152979":"code","0cd81fb7":"code","321208a3":"code","c5e2cae3":"code","b655949d":"code","4e918862":"code","44601b9d":"code","e614b33a":"code","880e054e":"code","3015bff0":"code","6a23372e":"code","37d3375e":"code","e6e2326c":"code","15b928c8":"code","547491f8":"code","ef1ed144":"code","7c768649":"code","ca71c48f":"code","64885527":"code","f41feb2f":"code","4579ca6e":"code","ea62c602":"code","d16a16a3":"code","8208cddf":"code","8d73a96c":"code","023bb2ed":"code","0741711c":"code","30c30a54":"code","920854dd":"code","42820583":"code","12b58dc1":"code","b9c04b73":"code","58c9e92d":"code","ac324343":"code","61dc936b":"code","79a79de5":"code","7e31e899":"code","8b2ad972":"code","e32f2eaf":"code","996222f5":"code","60002046":"code","ff5564c5":"code","63a80124":"code","9f5c54b5":"code","1920d386":"code","c49541ec":"code","eb9799b4":"code","2e53f5de":"code","3aca49d2":"code","6d93f3f6":"code","8bbf429d":"code","28904b0e":"code","433fa6ad":"code","ed901d90":"code","9d058532":"code","79b5182f":"code","f42593f2":"code","aa5d853c":"code","0c94e1fc":"code","01de95c6":"code","e49d23f6":"code","8cfc2ff8":"code","0c257861":"code","71a7391f":"code","6560355e":"code","b668888a":"code","6d5453cf":"code","1d01827b":"code","91db7aac":"code","46453385":"code","04958017":"code","519732ea":"code","cd711ae8":"code","72f0d105":"code","5f6ce570":"code","baa865d7":"code","ad09a1cf":"code","959b0396":"code","76ff0048":"code","91bf5a4c":"code","dc69ae73":"code","b820edf0":"code","fbdfb102":"code","e8a9e7cb":"code","a8fb67ae":"code","b81e93ca":"code","c9c66ea1":"code","eabb231c":"code","1ce423f6":"code","a7e5b214":"code","5a2b345d":"code","59b90c30":"code","63850ec7":"code","d9064d22":"code","ab8998e8":"code","3a39ee13":"code","d05114be":"code","75272a44":"code","5921613a":"code","4d343050":"code","efa2b57b":"code","d0c00f94":"code","cc3add93":"code","eadbd483":"code","65d35bea":"code","de5046df":"code","471c92b1":"code","4ad883d7":"code","e4d2680f":"code","d1b0939d":"code","54ba783b":"code","4ebad323":"code","593a5dfa":"code","ec2efa6c":"code","fa6f3fab":"code","1b96e2ab":"code","1d29d973":"code","ea25d8b1":"code","d77ff35d":"code","0ee01b15":"code","bc65d8d8":"code","b3593869":"code","f6fa39b1":"code","e3fcface":"code","a416a775":"code","f9431135":"code","d47b8977":"code","87b154f8":"code","72a2bbca":"code","9137fff1":"code","4f8be74b":"code","12ba42f9":"code","03d7361e":"code","741734bd":"code","45df6de3":"code","c44e5796":"code","fe282d4d":"code","840249dd":"code","6a8fa13a":"code","637fc250":"code","a61774d6":"code","823c6000":"code","925b6221":"code","2b3b4aea":"code","f5d1e944":"code","0de3b157":"code","4e14292c":"code","51ae617f":"code","d020115f":"code","70456948":"code","4f5a1de7":"code","724b8499":"markdown","73d7dc16":"markdown","07635494":"markdown","78840c84":"markdown","3cc199da":"markdown","5b37127d":"markdown","5b0ecf87":"markdown","75fdbfcc":"markdown","06174796":"markdown","1e236579":"markdown","1ddcca90":"markdown","866054ec":"markdown","56a872fb":"markdown","304d0d92":"markdown","3b09b2a4":"markdown","1b357239":"markdown","f3aa57e5":"markdown","de69a9b2":"markdown","f0611981":"markdown","8f98798f":"markdown","d5443673":"markdown","0c51442d":"markdown"},"source":{"04d0b0be":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import xticks\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\n\nimport statsmodels.api as sm\n","e6177301":"lead = pd.read_csv('..\/input\/lead-scoring\/Leads.csv')\nlead.head() ","a092d2cd":"lead.shape","396b36a5":"lead.columns\n","64bc73ed":"lead.info()","91fae5e7":"lead.describe()","7ad1a03d":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","e46905b0":"#plotting the null value percentage\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(12,5))\nnull_lead = pd.DataFrame((lead.isnull().sum())*100\/lead.shape[0]).reset_index()\nax = sns.pointplot(\"index\",0,data=null_lead)\nplt.xticks(rotation =90,fontsize =9)\nax.axhline(45, ls='--',color='red')\nplt.title(\"Percentage of Missing values\")\nplt.ylabel(\"PERCENTAGE\")\nplt.xlabel(\"COLUMNS\")\nplt.show()\n","5cd7da72":"##Missing Value treatment\nlead.describe(include='all')","7a672c44":"print(lead.Country.unique())","eb0f904c":"print(lead['City'].unique())\n","3188f555":"sns.countplot(lead.Country)\nxticks(rotation = 90)\n","97f178b2":"## More than 90% of the value is India so we can safely remove this column\n\nlead = lead.drop('Country',axis=1)\n","95347141":"sns.countplot(lead.City)\nxticks(rotation = 90)\n","d59a5309":"## Select values and null values for City can be imputed as Mumbai\n\nlead['City'] = lead['City'].fillna(lead['City'].mode()[0])\nlead['City'] = lead['City'].replace(\"Select\", \"Mumbai\")","fd5aa8b4":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","df6fbf67":"sns.countplot(x = \"City\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Majority Conversion is also from Mumbai\n","50a3e64f":"sns.countplot(lead['Specialization'])\nxticks(rotation = 90)","13cb81f7":"## Specialization has almost 1750 select values and apart from this 15% null values\n## What we can do about this create a category of others for Students or misc.\n\nlead['Specialization'] = lead['Specialization'].replace(np.nan, 'Others')\nlead['Specialization'] = lead['Specialization'].replace(\"Select\", \"Others\")","4c67a078":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Majority Conversion Data is from Others\n","bcd76f29":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","3bfe691e":"sns.countplot(lead['How did you hear about X Education'])\nxticks(rotation = 90)\n","7e9b4dca":"## Column \"How did you hear about X Education\" column can be dropped as most of the values are select and 23% are null values apart from select\n\nlead = lead.drop('How did you hear about X Education',axis=1)","ddb771b3":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","bb692bfd":"sns.countplot(lead['What is your current occupation'])\nxticks(rotation = 90)\n","29febdc5":"## Most of the values are unemployed so null values can be imputed as unemployed\n\nlead['What is your current occupation'] = lead['What is your current occupation'].replace(np.nan, 'Unemployed')","18afd3e0":"sns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Max Conversion is from Unemployed, but the ratio of being converted is better in Working Professionals\n","c960a814":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","28fd6578":"sns.countplot(lead['What matters most to you in choosing a course'])\nxticks(rotation = 90)\n","bc186696":"# Since majority data (more than 80% is for Better Career Prospect, if we impute this then it will be more than 90%) so it is safe to drop this column\n\nlead = lead.drop('What matters most to you in choosing a course', axis=1)\n","d3c1bdcc":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","e4ceb103":"sns.countplot(lead['Tags'])\nxticks(rotation = 90)","e1815752":"# Since most frequent option is Will Revert after reading the email, we can impute the mode in this case:\n\nlead['Tags'] = lead['Tags'].fillna(lead['Tags'].mode()[0])\n","39619898":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = lead\n             )\nxticks(rotation = 90)\n# We can see that conversion rate of the above imputed option is highest\n","01e14935":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","9775d255":"sns.countplot(lead['Lead Quality'])\nxticks(rotation = 90)\n","be34bfc2":"# Lead Quality seems an important parameter as per the business so instead of dropping this we can impute the values to  not sure since whoever was filling the form did not mention explicitly\n\nlead['Lead Quality'] = lead['Lead Quality'].replace(np.nan, 'Not Sure')","024692d6":"sns.countplot(x = \"Lead Quality\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Low, High in relevance have a better converion ratio but Might be have the most conversions","9feb0ba9":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","1f1ba4ad":"sns.countplot(lead['Lead Profile'])\nxticks(rotation = 90)\n","18d2596b":"# Lead Profile already has 4000 select values and then 29% null values which makes this column useless, so its safe to drop it\n\nlead = lead.drop('Lead Profile',axis=1)","ebdb889b":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","6797d9be":" sns.countplot(lead['Asymmetrique Activity Index'])\nxticks(rotation = 90)\n","028f60d6":"# For this column we can impute the null values to 02.Medium\n\nlead['Asymmetrique Activity Index'] = lead['Asymmetrique Activity Index'].fillna(lead['Asymmetrique Activity Index'].mode()[0])\n","b0480526":"sns.countplot(x = \"Asymmetrique Activity Index\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","b0c2d959":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","b20858af":"sns.countplot(lead['Asymmetrique Profile Index'])\nxticks(rotation = 90)\n","f812758c":"sns.countplot(x = \"Asymmetrique Profile Index\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","07f9399b":"# We can not deduce any analysis from this column and since the null values are high we can safely drop this column\n\nlead = lead.drop('Asymmetrique Profile Index',axis=1)","0016b4e9":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead\n                                   .index)), 2)\n","2099e40d":"sns.countplot(lead['Asymmetrique Activity Score'])\nxticks(rotation = 90)\n","88ee5029":"# Values are too close to be imputed in this case and the number of null values is not small, we can drop this column\n\nlead = lead.drop('Asymmetrique Activity Score',axis=1)","a402c825":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","2d561e76":"sns.countplot(lead['Asymmetrique Profile Score'])\nxticks(rotation = 90)\n","ec0aab36":"# Values are too close to be imputed in this case and the number of null values is not small, we can drop this column\n\nlead = lead.drop('Asymmetrique Profile Score',axis=1)","a3543c6a":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)","f90c3ab9":"## For Lead Source, Last Activity, Page Views per Visit, Total visits we can drop \n##those rows which contain null values since the number of missing values is less\n\nlead = lead.dropna()","20d7f533":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","82d6a386":"Converted = (sum(lead['Converted'])\/len(lead['Converted'].index))*100\nConverted\n\n","9089fdfc":"lead.shape\n","7415d72c":"sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","4c152979":"## We can club the values which do not have a considerable impact on Lead Source\n\nlead['Lead Source'] = lead['Lead Source'].replace(['Pay per Click Ads','bing','blog','Social Media','WeLearn','Click2call','Live Chat','welearnblog_Home',\n                                               'youtubechannel','testone','Press_Release','NC_EDM'], 'Others')\n\nlead.loc[(lead['Lead Source'] == 'google'),'Lead Source'] = 'Google'","0cd81fb7":"sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","321208a3":"sns.countplot(x = \"Do Not Email\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# We can see that There is a very small amount of conversion that has happened when this value is Yes","c5e2cae3":"sns.countplot(x = \"Do Not Call\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# We can see that There is no value of conversion for Yes value of Do not call","b655949d":"sns.countplot(x = \"Do Not Call\", data = lead)\nxticks(rotation = 90)","4e918862":"# Also we can see that there is no value of Yes in this case so we can drop this column since it does not add any value to the data.\n\nlead = lead.drop('Do Not Call',axis=1)","44601b9d":"# Checking the percentage of missing values\nround(100*(lead.isnull().sum()\/len(lead.index)), 2)\n","e614b33a":"sns.boxplot(x = \"TotalVisits\", hue = \"Converted\", data = lead\n           \n           )\nxticks(rotation = 90)","880e054e":"lead['TotalVisits'].describe(percentiles=[0.05,.25, .5, .75, .90, .95, .99])","3015bff0":"# We can see that there are some outliers present so we can treat these outliers before proceeding further\n\npercentiles = lead['TotalVisits'].quantile([0.05,0.95]).values\nlead['TotalVisits'][lead['TotalVisits'] <= percentiles[0]] = percentiles[0]\nlead['TotalVisits'][lead['TotalVisits'] >= percentiles[1]] = percentiles[1]","6a23372e":"sns.countplot(x = \"TotalVisits\", hue = \"Converted\", data = lead\n             \n             )\nxticks(rotation = 90)\n\n# This data could mean that when a user visits the website often the ratio of conversion gets better","37d3375e":"sns.boxplot(x = \"Total Time Spent on Website\", data = lead\n           \n           \n           \n           \n           )\nxticks(rotation = 90)","e6e2326c":"sns.boxplot(x = \"Page Views Per Visit\",data = lead\n           )\nxticks(rotation = 90)","15b928c8":"## A number of outliers are also present in this case so we can remove these\n\npercentiles = lead['Page Views Per Visit'].quantile([0.05,0.95]).values\nlead['Page Views Per Visit'][lead['Page Views Per Visit'] <= percentiles[0]] = percentiles[0]\nlead['Page Views Per Visit'][lead['Page Views Per Visit'] >= percentiles[1]] = percentiles[1]\n","547491f8":"sns.boxplot(x = \"Page Views Per Visit\",data = lead)\nxticks(rotation = 90)","ef1ed144":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","7c768649":"# We can club calues which have no or very less data\n\nlead['Last Activity'] = lead['Last Activity'].replace(['View in browser link Clicked','Visited Booth in Tradeshow',\n                                                   'Approached upfront','Resubscribed to emails','Email Received','Email Marked Spam'], 'Others')","ca71c48f":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Last Activity\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","64885527":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","f41feb2f":"# We can club calues which have no or very less data\n\nlead['Specialization'] = lead['Specialization'].replace(['Services Excellence','Retail Management',\n                                                   'Hospitality Management','Rural and Agrbusiness','E-Business'], 'Others')","4579ca6e":"sns.countplot(x = \"Specialization\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","ea62c602":"sns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# From here we can see that most of the people are unemployed and have a good conversion rate, but in case of \n## working professionsals, we can see that the number of conversions is more than not converted","d16a16a3":"sns.countplot(x = \"Search\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","8208cddf":"# Since there are no values for Yes we can delete this column as well. As it will not add up to the model\n\nlead = lead.drop('Search',axis=1)","8d73a96c":"sns.countplot(x = \"Magazine\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","023bb2ed":"sns.countplot(x = \"Magazine\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","0741711c":"sns.countplot(x = \"Newspaper Article\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","30c30a54":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('Newspaper Article',axis=1)","920854dd":"sns.countplot(x = \"X Education Forums\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","42820583":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('X Education Forums',axis=1)","12b58dc1":"sns.countplot(x = \"Newspaper\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","b9c04b73":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('Newspaper',axis=1)","58c9e92d":"sns.countplot(x = \"Digital Advertisement\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","ac324343":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('Digital Advertisement',axis=1)","61dc936b":"sns.countplot(x = \"Through Recommendations\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","79a79de5":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('Through Recommendations',axis=1)\n","7e31e899":"sns.countplot(x = \"Receive More Updates About Our Courses\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","8b2ad972":"# We can drop this column as well as there are no values in yes counterpart so it wont add value to the data\n\nlead = lead.drop('Receive More Updates About Our Courses',axis=1)","e32f2eaf":"plt.figure(figsize=(10,7))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","996222f5":"# We can club calues which have no or very less data\n\nlead['Tags'] = lead['Tags'].replace(['Still Thinking','Lost to Others',\n                                                   'Shall take in the next coming month','Lateral student','Interested in Next batch','in touch with EINS','In confusion whether part time or DLP', 'Recognition issue (DEC approval)','Want to take admission but has financial problems','University not recognized','opp hangup','number not provided'], 'Others')","60002046":"plt.figure(figsize=(10,5))\nsns.countplot(x = \"Tags\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","ff5564c5":"sns.countplot(x = \"Lead Quality\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","63a80124":"sns.countplot(x = \"Update me on Supply Chain Content\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","9f5c54b5":"# We can drop this variable since there is no data for no\n\nlead = lead.drop('Update me on Supply Chain Content',axis=1)","1920d386":"sns.countplot(x = \"Get updates on DM Content\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n","c49541ec":"# We can drop this variable since there is no data for Get updates on DM Content\n\nlead = lead.drop('Get updates on DM Content',axis=1)","eb9799b4":"sns.countplot(x = \"City\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Most of the conversions happened from Mumbai","2e53f5de":"sns.countplot(x = \"Asymmetrique Activity Index\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Medium activity has most conversions","3aca49d2":"sns.countplot(x = \"I agree to pay the amount through cheque\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)","6d93f3f6":"# We can drop this variable since there is no data for I agree to pay through cheque\n\nlead = lead.drop('I agree to pay the amount through cheque',axis=1)","8bbf429d":"sns.countplot(x = \"A free copy of Mastering The Interview\", hue = \"Converted\", data = lead)\nxticks(rotation = 90)\n\n# Highest conversion with value no\n","28904b0e":"lead.shape","433fa6ad":"lead.columns","ed901d90":"lead.head()","9d058532":"# List of variables to map which are in the form of Yes\/No\n\nvarlist =  ['Do Not Email', 'A free copy of Mastering The Interview','Magazine']\n\n# Defining the map function\ndef mapping(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\n\nlead[varlist] = lead[varlist].apply(mapping)\n","79b5182f":"#For categorical variables with multiple levels, create dummy features (one-hot encoded)\u00b6\n","f42593f2":"# Creating a dummy variable for some of the categorical\n#variables and dropping the first one\ndum1 = pd.get_dummies(lead[['Lead Origin', 'Lead Source',\n        'Last Activity', 'Specialization', 'What is your current occupation', 'Tags',\n       'Lead Quality', 'City', 'Asymmetrique Activity Index',\n       'A free copy of Mastering The Interview', 'Last Notable Activity']], drop_first=True)\n\ndum1.head()","aa5d853c":"# Adding the results to the original dataset\nlead = pd.concat([lead, dum1], axis=1)\nlead.head()","0c94e1fc":"lead = lead.drop(['Lead Origin', 'Lead Source',\n        'Last Activity', 'Specialization', 'What is your current occupation', 'Tags',\n       'Lead Quality', 'City', 'Asymmetrique Activity Index',\n       'A free copy of Mastering The Interview', 'Last Notable Activity'],axis=1)","01de95c6":"lead.head()","e49d23f6":"lead = lead.drop('Lead Number',axis=1)\n# Dropping Lead number since both prospect id and Lead Number are unique we can keep just one column\n","8cfc2ff8":"lead.head()\n\n","0c257861":"X = lead.drop(['Prospect ID','Converted'], axis=1)\n","71a7391f":"X.head()","6560355e":"y = lead['Converted']\ny.head()\n","b668888a":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","6d5453cf":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","1d01827b":"X_train.head()","91db7aac":"# Checking the Churn Rate\nConverted = (sum(lead['Converted'])\/len(lead['Converted'].index))*100\nConverted","46453385":"# Let's see the correlation matrix \nplt.figure(figsize = (20,15))        # Size of the figure\nsns.heatmap(X_train.corr(),annot = True)\nplt.show()","04958017":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","519732ea":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 15 variables as output\nrfe = rfe.fit(X_train, y_train)\n","cd711ae8":"rfe.support_","72f0d105":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","5f6ce570":"col = X_train.columns[rfe.support_]\ncol\n","baa865d7":"X_train.columns[~rfe.support_]","ad09a1cf":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()\n","959b0396":"col1 = col.drop('Tags_invalid number',1)","76ff0048":"col1","91bf5a4c":"X_train_sm = sm.add_constant(X_train[col1])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","dc69ae73":"col2 = col1.drop('Tags_wrong number given',1)","b820edf0":"col2","fbdfb102":"X_train_sm = sm.add_constant(X_train[col2])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e8a9e7cb":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]\n","a8fb67ae":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]\n","b81e93ca":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\ny_train_pred_final['Prospect ID'] = y_train.index\ny_train_pred_final.head()","c9c66ea1":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","eabb231c":"\n# Predicted     not_churn    churn\n# Actual\n# not_churn        3270      365\n# churn            579       708","1ce423f6":"from sklearn import metrics\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","a7e5b214":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","5a2b345d":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","59b90c30":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col2].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col2].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","63850ec7":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","d9064d22":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","ab8998e8":"# Let us calculate specificity\nTN \/ float(TN+FP)","3a39ee13":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","d05114be":"# positive predictive value \nprint (TP \/ float(TP+FP))\n","75272a44":"# Negative predictive value\nprint (TN \/ float(TN+ FN))\n","5921613a":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n","4d343050":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, y_train_pred_final.Converted_prob, drop_intermediate = False )\n","efa2b57b":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)\n","d0c00f94":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","cc3add93":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","eadbd483":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","65d35bea":"#### From the curve above, 0.2 is the optimum point to take it as a cutoff probability.\n\ny_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.2 else 0)\n\ny_train_pred_final.head()","de5046df":"\n#Assigning Lead Score","471c92b1":"y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_train_pred_final.head()","4ad883d7":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\n\nconfusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2\n\nTP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","e4d2680f":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","d1b0939d":"# Let us calculate specificity\nTN \/ float(TN+FP)\n\n\n","54ba783b":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","4ebad323":"# Positive predictive value \nprint (TP \/ float(TP+FP))\n","593a5dfa":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","ec2efa6c":"#Precision and Recall","fa6f3fab":"#Looking at the confusion matrix again\n\nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nconfusion","1b96e2ab":"##### Precision\nTP \/ TP + FP\n\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","1d29d973":"##### Recall\nTP \/ TP + FN\n\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","ea25d8b1":"#Using sklearn utilities for the same","d77ff35d":"from sklearn.metrics import precision_score, recall_score","0ee01b15":"precision_score(y_train_pred_final.Converted , y_train_pred_final.predicted)","bc65d8d8":"recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)","b3593869":"#Precision and recall tradeoff","f6fa39b1":"from sklearn.metrics import precision_recall_curve","e3fcface":"y_train_pred_final.Converted, y_train_pred_final.predicted","a416a775":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)\n","f9431135":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()\n","d47b8977":"#Making predictions on the test set","87b154f8":"X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_test[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","72a2bbca":"X_test = X_test[col2]\nX_test.head()","9137fff1":"X_test_sm = sm.add_constant(X_test)","4f8be74b":"y_test_pred = res.predict(X_test_sm)","12ba42f9":"y_test_pred[:10]\n","03d7361e":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","741734bd":"# Let's see the head\ny_pred_1.head()","45df6de3":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","c44e5796":"# Putting CustID to index\ny_test_df['Prospect ID'] = y_test_df.index\n","fe282d4d":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","840249dd":"\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","6a8fa13a":"\n\ny_pred_final.head()","637fc250":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})","a61774d6":"# Let's see the head of y_pred_final\ny_pred_final.head()","823c6000":"y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.2 else 0)\n","925b6221":"y_pred_final.head()\n","2b3b4aea":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)","f5d1e944":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","0de3b157":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","4e14292c":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","51ae617f":"# Let us calculate specificity\nTN \/ float(TN+FP)","d020115f":"precision_score(y_pred_final.Converted , y_pred_final.final_predicted)","70456948":"recall_score(y_pred_final.Converted , y_pred_final.final_predicted)","4f5a1de7":"#8. Final Observations and Recommendations\n\n#The Final Evaluation Metrics for the train Dataset:\n# - The Accuracy is : 0.80\n# - The Sensitivity is : 0.80\n# -The Specificity is : 0.81\n# - The Precision is : 0.93\n# - The Recall is : 0.80\n \n    \n#The Final Evaluation Metrics for the test Dataset:The Accuracy is : 0.81\n# - The Sensitivity is : 0.84\n# - The Specificity is : 0.95\n# - The Precision is : 0.90\n# - The Recall is : 0.84\n\n\n\n#X-Education has a better chance of converting a potential lead when:\n# - The total time spent on the Website is high: Leads who have spent more time on the website have converted\n# - Current Occupation is specified: Leads who are working professionals have high chances of getting converted. People who were looking for better prospects like Unemployed, students, Housewives and Business professionals were also good prospects to focus on.\n# - When the Lead origin was Lead Add form Leads who have responded\/ or engaged through Lead Add Forms have had a higher chances of getting converted\n# - Number of Total Visits were high Leads who have made a greater number of visits have higher chances of getting converted.\n# - When the last activity was SMS sent or Email opened Members who have sent an SMS for enquiry or who have opened the email have a higher chance of getting converted.\n","724b8499":"##Scaling of the features","73d7dc16":"Importing all the relavemt libraries","07635494":"##model 2\n","78840c84":"\n#Assigning Lead Score","3cc199da":"##Performing Test-Train Split","5b37127d":"###Dummy Variable Creation\n","5b0ecf87":"****Univariate Analysis\n","75fdbfcc":"#An ROC curve demonstrates several things:\n\n# - It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n# - The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n# - The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n","06174796":"**** Inferences:\nWe can see that there are some outliers present in a few columns like: Total Visits, Total time spent on website\n","1e236579":"##Inference\n#  Most of the lead have their Email opened as their last activity.\n# - Conversion rate for leads with last activity as SMS Sent is almost 60%.b\n","1ddcca90":"#Checking VIFs\n\n","866054ec":"#Finding Optimal Cutoff-point","56a872fb":"#Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\u00b6\n","304d0d92":"##Model 1: Using all columns\n\n","3b09b2a4":"## Now that all of the data cleaning is done we can go ahead and proceed with dummy variable creation","1b357239":"#**Checking for null values","f3aa57e5":"#Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","de69a9b2":"##Feature selection using RFE","f0611981":"#Metrics beyond simply accuracy\n","8f98798f":"#Creating a dataframe with the actual churn flag and the predicted probabilities\n","d5443673":"#Plotting ROC curve","0c51442d":"#We have almost 38% conversion\n\n"}}