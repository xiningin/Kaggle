{"cell_type":{"29b02006":"code","a798126e":"code","a0dc882a":"code","aa750b83":"code","d36a3cff":"code","fb48e7c0":"code","d54c2391":"code","98268c0e":"code","ff3b1769":"code","ed09606e":"code","b97f288a":"code","bf4e4b68":"code","6c1c1830":"markdown","def1df1d":"markdown","ec337601":"markdown","d24fc22a":"markdown","b441962e":"markdown","6df2987b":"markdown","2eae72b6":"markdown","6f50d153":"markdown","83c7534b":"markdown","4bb1827c":"markdown"},"source":{"29b02006":"!pip install --upgrade tensorflow\n!pip install bert-tensorflow\n!pip install tf-hub-nightly","a798126e":"import tensorflow as tf\nimport tensorflow_hub as hub\nprint(\"TF version: \", tf.__version__)\nprint(\"Hub version: \", hub.__version__)","a0dc882a":"import tensorflow_hub as hub\nimport tensorflow as tf\nfrom bert.tokenization import FullTokenizer     # Still from bert module\nfrom tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\nimport math","aa750b83":"max_seq_length = 128  # Your choice here.\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                       name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                   name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                    name=\"segment_ids\")\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=True)\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])","d36a3cff":"model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])","fb48e7c0":"# See BERT paper: https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n# And BERT implementation convert_single_example() at https:\/\/github.com\/google-research\/bert\/blob\/master\/run_classifier.py\n\ndef get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n\ndef get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\n\ndef get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids","d54c2391":"# Google Colab don't need this. FullTokenizer is not updated to tf2.0 yet\ntf.gfile = tf.io.gfile","98268c0e":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = FullTokenizer(vocab_file, do_lower_case)","ff3b1769":"s = \"This is a nice sentence.\"\nstokens = tokenizer.tokenize(s)\n\ninput_ids = get_ids(stokens, tokenizer, max_seq_length)\ninput_masks = get_masks(stokens, max_seq_length)\ninput_segments = get_segments(stokens, max_seq_length)\n\nprint(stokens)\nprint(input_ids)\nprint(input_masks)\nprint(input_segments)","ed09606e":"pool_embs, all_embs = model.predict([[input_ids],[input_masks],[input_segments]])","b97f288a":"def square_rooted(x):\n    return math.sqrt(sum([a*a for a in x]))\n\n\ndef cosine_similarity(x,y):\n    numerator = sum(a*b for a,b in zip(x,y))\n    denominator = square_rooted(x)*square_rooted(y)\n    return numerator\/float(denominator)","bf4e4b68":"cosine_similarity(pool_embs[0], all_embs[0][0])","6c1c1830":"## Update TF\nWe need Tensorflow 2.0 and TensorHub 0.7 for this Notebook","def1df1d":"## Import modules","ec337601":"## Test BERT embedding generator model","d24fc22a":"If TensorFlow Hub is not 0.7 yet on release, use dev.\n\n**Disclaimer**: bert-tensorflow is not the latest version of BERT. To use bert with TF 2.0, you should use tensorflow\/models where the model is updated to tf2.0. To resolve this issue, I'll use `tf.gfile = tf.io.gfile` at one point of the code. If you use Google Colab, it uses the latest bert version (no need for `pip install bert-tensorflow`), but I couldn't reproduce the same in Kaggle\n\nIf you know, how to install module from here tf\/models, please share in comment!\nThe latest BERT at tf\/models: https:\/\/github.com\/tensorflow\/models\/tree\/master\/official\/nlp\/bert","b441962e":"Import tokenizer using the original vocab file","6df2987b":"Generating segments and masks based on the original BERT","2eae72b6":"Generate Embeddings using the pretrained model","6f50d153":"Building model using tf.keras and hub. from sentences to embeddings.\n\nInputs:\n - input token ids (tokenizer converts tokens using vocab file)\n - input masks (1 for useful tokens, 0 for padding)\n - segment ids (for 2 text training: 0 for the first one, 1 for the second one)\n\nOutputs:\n - pooled_output of shape `[batch_size, 768]` with representations for the entire input sequences\n - sequence_output of shape `[batch_size, max_seq_length, 768]` with representations for each input token (in context)","83c7534b":"## Pooled embedding vs [CLS] as sentence-level representation\n\nPreviously, the [CLS] token's embedding were used as sentence-level representation (see the original paper). However, here a pooled embedding were introduced. This part is a short comparison of the two embedding using cosine similarity","4bb1827c":"# BERT Embeddings with TensorFlow 2.0\nWith the new release of TensorFlow, this Notebook aims to show a simple use of the BERT model.\n- See BERT on paper: https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n- See BERT on GitHub: https:\/\/github.com\/google-research\/bert\n- See BERT on TensorHub: https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\n- See 'old' use of BERT for comparison: https:\/\/colab.research.google.com\/github\/google-research\/bert\/blob\/master\/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"}}