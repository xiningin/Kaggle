{"cell_type":{"186a8423":"code","1ab884f9":"code","489e7ef2":"code","b9c83941":"code","2669f2fb":"code","5c184cd5":"code","45d26213":"code","0ff45b57":"code","5e25caad":"code","7b9e40c9":"code","00f662fa":"code","fce10ecd":"code","e69e4387":"code","f8dc08be":"code","9a1fb19e":"code","ef7516fb":"code","c8325740":"code","7f0a2e16":"code","63f1e8a9":"code","f4fb6e0e":"code","aa7be4b0":"code","8dfc41a4":"code","967be23c":"code","00893419":"code","4e571f56":"code","f9d36335":"code","8a338b24":"code","636a1596":"code","f837eba3":"code","e34762ba":"code","854f00d6":"code","82038674":"code","2ef51dd2":"code","77e0f52b":"code","f9d8ca3c":"code","c06e58dc":"code","02a96b6c":"code","eccee9da":"code","dfd3e050":"markdown","363b235e":"markdown","a7cb5e19":"markdown","47f9b75d":"markdown","dc0427c2":"markdown","3bc3a0d4":"markdown","dec7cbc9":"markdown","8dce18e3":"markdown","d6411c4c":"markdown","80858ec0":"markdown","4de5cb2f":"markdown","75ce23cf":"markdown","6444826e":"markdown","ebdd4762":"markdown","89dec76c":"markdown","ea6d7ee5":"markdown","4fef1e46":"markdown","8d95b939":"markdown","44e45d98":"markdown","7962d27d":"markdown","aad97e72":"markdown","9de49e75":"markdown","aec734a2":"markdown","e8be878c":"markdown","6b6e7df7":"markdown","fe6314b3":"markdown","2d41e6aa":"markdown","75d7bb62":"markdown","baa8c9ba":"markdown","15fc87e3":"markdown","5648fe0c":"markdown","0488d93a":"markdown","06288145":"markdown","a2f372a5":"markdown"},"source":{"186a8423":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Charts\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scikitplot.estimators import plot_learning_curve\nfrom sklearn.metrics import plot_confusion_matrix\nfrom keras.utils.vis_utils import plot_model\n\n#  Models\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as ctb\n\n\n# Preprocessing\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek \nfrom sklearn.preprocessing import LabelEncoder\n\n# Scoring\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_validate, StratifiedKFold\nfrom sklearn.metrics import make_scorer, f1_score, precision_score, recall_score, accuracy_score\nfrom sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix\n\n# Hyperparameters and features importance\nfrom sklearn.model_selection import GridSearchCV\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n# remove verison errors\nimport warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)","1ab884f9":"path = '\/kaggle\/input\/stroke-prediction-dataset\/'\n\ndf_stroke = pd.read_csv(path + 'healthcare-dataset-stroke-data.csv')","489e7ef2":"df_stroke.shape","b9c83941":"df_stroke.info()","2669f2fb":"df_stroke.sample(15)","5c184cd5":"df_stroke.isnull().sum().sum()","45d26213":"df_stroke.dropna(inplace=True)","0ff45b57":"df_stroke.duplicated().sum()","5e25caad":"mycolors = ['red', 'blue', 'brown', 'orange']","7b9e40c9":"categorical_cols = [ 'gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'] # 8\ncontinous_cols = ['age', 'avg_glucose_level', 'bmi'] # 3\nlabel_col = ['stroke'] #1\n\nto_numeric = {\n    'gender':{'Male': 0, 'Female': 1, 'Other': 2},\n    'ever_married':{'No':0, 'Yes':1},\n    'work_type': {'children': 0, 'Govt_job': 1, 'Never_worked': 2, 'Private': 3, 'Self-employed': 4},\n    'Residence_type': {'Rural': 0, 'Urban': 1},\n    'smoking_status': {'formerly smoked': 0, 'never smoked': 1, 'smokes': 2, 'Unknown': 3}\n}\n\nname_change = {\n    'hypertension': {'0': \"patient doesn't have hypertension\", '1': 'patient has hypertension'}, \n    'heart_disease': {'0': \"patient doesn't have any heart diseases\", '1': \"patient has a heart disease\"},\n}","00f662fa":"df_stroke[continous_cols].describe().T","fce10ecd":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    data = df_stroke[x]\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(f'Distribution of {x} variable', fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.ylabel('Count', fontsize=16)\n    sns.histplot(data, bins = 50, kde=50);\n    cnt += 1","e69e4387":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.ylabel('Density', fontsize=16)\n    sns.kdeplot(data=df_stroke, x=x, hue=\"stroke\", fill=True, common_norm=False, alpha=.5, linewidth=0);\n    cnt += 1","f8dc08be":"cnt = 0\nmax_in_row = 1\nfor x in continous_cols:\n    data = df_stroke[x]\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    sns.boxplot(data = data);\n    sns.despine(offset=10, trim=True);\n    cnt += 1","9a1fb19e":"cnt = 0\nmax_in_row = 1\nfor x in categorical_cols:\n    val1 = df_stroke[x].value_counts().index\n    if x in name_change:\n        val1 = [name_change[x][str(val)] for val in val1]\n    cnt1 = df_stroke[x].value_counts().values\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    plt.bar(val1, cnt1, color=mycolors);\n    cnt += 1","ef7516fb":"temp_df = df_stroke.copy()\ntemp_df = temp_df.drop(columns=['id'])\nfor x in categorical_cols:\n    if x in to_numeric:\n        temp_df[x] = temp_df[x].map(lambda a: to_numeric[x][a])","c8325740":"cnt = 0\nmax_in_row = 1\nfor x in categorical_cols:\n    plt.figure(cnt\/\/max_in_row, figsize=(25,8))\n    plt.subplot(1, max_in_row, (cnt)%max_in_row + 1)\n    plt.title(x, fontsize=20)\n    plt.xticks(fontsize=16)\n    plt.yticks(fontsize=16)\n    plt.xlabel(x, fontsize=16)\n    sns.kdeplot(data=temp_df, x=x, hue=\"stroke\", fill=True, common_norm=False, alpha=.5, linewidth=0);\n    cnt += 1","7f0a2e16":"f1 = df_stroke['stroke'].map(lambda x:  '1 = patient had a stroke' if x == 1 else \"0 = patient hadn't a stroke\")\n\nplt.figure(figsize=(18,10))\nval = f1.value_counts().index\ncnt = f1.value_counts().values\n\nplt.title('Count of the target', size=20)\nplt.tick_params(labelsize=16)\nplt.ylabel('Count', size=16)\nplt.xlabel('output', size=16)\nplt.bar(val, cnt, color = mycolors);\nplt.show()","63f1e8a9":"plt.figure(figsize = (24, 24))\nsns.heatmap(temp_df.corr(), cmap = \"coolwarm\", annot=True, fmt='.1f', linewidths=0.1);\nplt.yticks(rotation=0, size=16)\nplt.xticks(rotation=90,size=16)\nplt.title('Correlation Matrix', size=26)\nplt.show()","f4fb6e0e":"plt.figure(figsize = (24, 24))\nsns.heatmap(temp_df.corr()>=0.5, cmap = \"coolwarm\", annot=True, fmt='.1f', linewidths=0.1);\nplt.yticks(rotation=0, size=16)\nplt.xticks(rotation=90, size=16)\nplt.title('Correlation Matrix', size=26)\nplt.show()","aa7be4b0":"sns.pairplot(temp_df, hue='stroke');","8dfc41a4":"df_stroke_tr = df_stroke.copy()\ndf_stroke_tr = df_stroke_tr.drop(columns=['id'])\n\nfor x in categorical_cols:\n    if x in to_numeric:\n        df_stroke_tr[x] = df_stroke_tr[x].map(lambda a: to_numeric[x][a])\n\n\nX = df_stroke_tr.drop(['stroke'],axis=1)\ny = df_stroke_tr['stroke']\n\nsm = SMOTETomek(random_state=42)\nX,y = sm.fit_resample(X, y.ravel())\n\n\n# split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2021, stratify=y)\n\n\nX_train_raw = X_train.copy()\nX_test_raw = X_test.copy()\ny_train_raw = y_train.copy()\ny_test_raw = y_test.copy()\n\nX_train_norm = X_train.copy()\nX_test_norm = X_test.copy()\ny_train_norm = y_train.copy()\ny_test_norm = y_test.copy()\nnorm = MinMaxScaler()\nX_train_norm[continous_cols] = norm.fit_transform(X_train_norm[continous_cols])\nX_test_norm[continous_cols] = norm.transform(X_test_norm[continous_cols])\n\nX_train_stand = X_train.copy()\nX_test_stand = X_test.copy()\ny_train_stand = y_train.copy()\ny_test_stand = y_test.copy()\nscaler = StandardScaler()\nX_train_stand[continous_cols] = scaler.fit_transform(X_train_stand[continous_cols])\nX_test_stand[continous_cols] = scaler.transform(X_test_stand[continous_cols])","967be23c":"def train_model(model, X, y):\n    model.fit(X, y)\n    return model\n\n\ndef predict_model(model, X, proba=False):\n    if ~proba:\n        y_pred = model.predict(X)\n    else:\n        y_pred_proba = model.predict_proba(X)\n        y_pred = np.argmax(y_pred_proba, axis=1)\n\n    return y_pred\n\n\nlist_scores = []\n\ndef run_model(name, model, X_train, X_test, y_train, y_test, fc, proba=False):\n    print(name)\n    print(fc)\n    \n    model = train_model(model, X_train, y_train)\n    y_pred = predict_model(model, X_test, proba)\n    \n    accuracy = accuracy_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print('accuracy: ', accuracy)\n    print('recall: ',recall)\n    print('precision: ', precision)\n    print('f1: ', f1)\n    print(classification_report(y_test, y_pred))\n    \n    \n    plot_confusion_matrix(model, X_test, y_test, cmap='Blues');    \n    plt.show()\n    plot_learning_curve(model, X_train, y_train, cv=3, scoring='f1');    \n    plt.show()\n    \n    list_scores.append({'Model Name': name, 'Feature Scaling':fc, 'Accuracy': accuracy, 'Recall': recall, 'Precision': precision, 'F1':f1})","00893419":"feature_scaling = {\n    'Raw':(X_train_raw, X_test_raw, y_train_raw, y_test_raw),\n    'Normalization':(X_train_norm, X_test_norm, y_train_norm, y_test_norm),\n    'Standardization':(X_train_stand, X_test_stand, y_train_stand, y_test_stand),\n}","4e571f56":"model_svc = SVC(random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('SVC', model_svc, X_train, X_test, y_train, y_test, fc_name)","f9d36335":"logreg = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('Logistic Regression', logreg, X_train, X_test, y_train, y_test, fc_name, proba=True)","8a338b24":"for fc_name, value in feature_scaling.items():\n    scores_1 = []\n    X_train, X_test, y_train, y_test = value\n    \n    for i in range(2,50):\n        knn = KNeighborsClassifier(n_neighbors = i)\n        knn.fit(X_train, y_train)\n        \n        scores_1.append(accuracy_score(y_test, knn.predict(X_test)))\n    \n    max_val = max(scores_1)\n    max_index = np.argmax(scores_1) + 2\n    \n    knn = KNeighborsClassifier(n_neighbors = max_index)\n    knn.fit(X_train, y_train)\n\n    run_model(f'KNeighbors Classifier n_neighbors = {max_index}', knn, X_train, X_test, y_train, y_test, fc_name)","636a1596":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    \n    dt = DecisionTreeClassifier()\n    \n    parameters = { 'max_depth':np.arange(1,5,1),'random_state':[2021]}\n    searcher = GridSearchCV(dt, parameters)\n    \n    run_model('DecisionTree Classifier', searcher, X_train, X_test, y_train, y_test, fc_name )","f837eba3":"rf = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('RandomForest Classifier', rf, X_train, X_test, y_train, y_test, fc_name)","e34762ba":"rf = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('RandomForest Classifier', rf, X_train, X_test, y_train, y_test, fc_name)","854f00d6":"rf = RandomForestClassifier(n_estimators=200, max_depth=2, random_state=2021)\n\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('RandomForest Classifier', rf, X_train, X_test, y_train, y_test, fc_name)","82038674":"gbt = GradientBoostingClassifier(n_estimators = 200, max_depth=3, subsample=0.8, max_features=0.2, random_state=2021)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('GradientBoosting Classifier', gbt, X_train, X_test, y_train, y_test, fc_name)","2ef51dd2":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    xgb_model = xgb.XGBClassifier(n_estimators = 200, max_depth=2, random_state=2021, use_label_encoder=False, eval_metric='mlogloss')\n        \n    run_model('XGBoost Classifier', xgb_model, X_train, X_test, y_train, y_test, fc_name)","77e0f52b":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    xgb_model = xgb.XGBClassifier(n_estimators = 100, max_depth=3, random_state=2021, use_label_encoder=False, eval_metric='mlogloss')\n        \n    run_model('XGBoost Classifier', xgb_model, X_train, X_test, y_train, y_test, fc_name)","f9d8ca3c":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    xgb_model = xgb.XGBClassifier(n_estimators = 200, max_depth=2, random_state=2021, use_label_encoder=False, eval_metric='mlogloss')\n        \n    run_model('XGBoost Classifier', xgb_model, X_train, X_test, y_train, y_test, fc_name)","c06e58dc":"for fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    xgb_model = xgb.XGBClassifier(n_estimators = 500, max_depth=2, random_state=2021, use_label_encoder=False, eval_metric='mlogloss')\n        \n    run_model('XGBoost Classifier', xgb_model, X_train, X_test, y_train, y_test, fc_name)","02a96b6c":"lgbm_model = lgbm.LGBMClassifier(max_depth = 2, n_estimators=500, subsample=0.8, random_state=2021)\nfor fc_name, value in feature_scaling.items():\n    X_train, X_test, y_train, y_test = value\n    run_model('Lightgbm Classifier', lgbm_model, X_train, X_test, y_train, y_test, fc_name)","eccee9da":"df_scores = pd.DataFrame(list_scores)\ndf_scores.style.highlight_max(color = 'lightgreen', axis = 0)","dfd3e050":"## Summary scores","363b235e":"#### I divide features to categorical, continous and label columns","a7cb5e19":"#### In dataset we have more cases with option 0\n","47f9b75d":"# Summary","dc0427c2":"## The size of the dataset","3bc3a0d4":"## Boxplot of continuous features","dec7cbc9":"## Barplot of the categorical features\n","8dce18e3":"#### As we can see, the variables weekly correlate with each other","d6411c4c":"# Exploratory data analysis","80858ec0":"#### Colors to charts","4de5cb2f":"#### We learned a lot of interesting knowledge about stroke.","75ce23cf":"#### Make one-hot encoding for caterical columns and simple scaler train data","6444826e":"![](https:\/\/images.medicinenet.com\/images\/article\/main_image\/stroke-symptoms-and-treatment.jpg)","ebdd4762":"## Checking missing values","89dec76c":"## Running some models on this data","ea6d7ee5":"- Someone in the United States has a stroke every 40 seconds.\n- Every year, more than 795,000 people in the United States have a stroke.","4fef1e46":"## Imports libs ","8d95b939":"### Distribution of continuous features","44e45d98":"## Sample data","7962d27d":"# Stroke Prediction and Analysis","aad97e72":"## Statistics continous columns","9de49e75":"# About this dataset\n\n- `id`: unique identifier\n- `gende`: \"Male\", \"Female\" or \"Other\"\n- `age`: age of the patient\n- `hypertension`: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n- `heart_disease`: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n- `ever_married`: \"No\" or \"Yes\"\n- `work_type`: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n- `Residence_type`: \"Rural\" or \"Urban\"\n- `avg_glucose_level`: average glucose level in blood\n- `bmi`: body mass index\n- `smoking_status`: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n- `stroke`: 1 if the patient had a stroke or 0 if not","aec734a2":"# Training model","e8be878c":"## Basic info about data","6b6e7df7":"### A functions that makes life easier","fe6314b3":"## Removing missing values","2d41e6aa":"#### I would love to know your comments and note about this.\n\n#### If you liked it, make sure to vote :)\n\n#### I'm going to make the next notebook soon.","75d7bb62":"<font size=\"6\">\n    <div style=\"text-align: center\"> <b> Author <\/b> <\/div>\n<\/font>\n\n<font size=\"5\">\n    <div style=\"text-align: center\"> J\u0119drzej <\/div>\n    <div style=\"text-align: center\"> Dudzicz <\/div>\n<\/font>","baa8c9ba":"## Count of the target","15fc87e3":"## Correlation Matrix","5648fe0c":"#### There are more women than men in the data set","0488d93a":"# The Purpose of notebook\n\nIn this notebook, I will analyze a dataset of people who have been tested for stroke.\n","06288145":"## Loading the dataset","a2f372a5":"## Checking duplicates"}}