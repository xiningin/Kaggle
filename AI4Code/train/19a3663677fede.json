{"cell_type":{"83d2ed2b":"code","e21eb232":"code","14679497":"code","007896b3":"code","737344ec":"code","b37ff495":"code","7e3b7711":"code","6c9cba7a":"code","ae5b327d":"code","19b42c0d":"code","c9d831ba":"code","0fd8a86e":"code","77aeeb5f":"markdown","680e5ed2":"markdown","7833fc9a":"markdown","f757422d":"markdown","c148e95f":"markdown","58ca610b":"markdown","82da7f2c":"markdown","680fd37c":"markdown"},"source":{"83d2ed2b":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e21eb232":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata = data.set_index(\"PassengerId\")","14679497":"Y = data.Survived\ndata = data.drop(columns= ['Survived','Name', 'Ticket', 'Cabin'])\n\n","007896b3":"data.head()","737344ec":"data.Pclass = data.Pclass.astype(str)\nfeatures = [ 'Pclass','Sex', 'Embarked']\n\ndums = pd.get_dummies(data[features])\n\ndata = data.drop(columns=features)","b37ff495":"data = pd.concat([data, dums], axis = 1)\ndata.head()","7e3b7711":"data.fillna(data.mean(), inplace=True)\ndata.isnull().values.any() #verify there's no null values","6c9cba7a":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nmodel_features=['Age','SibSp','Parch','Fare','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male','Embarked_C','Embarked_Q','Embarked_S']\n\nX = data[model_features]\n\nX_train, X_dev, y_train, y_dev = train_test_split(X,Y, test_size = 0.15)\n \ntrain_accs=[]\ndev_accs = []\nfor nb_est in range(1,300):\n    model = RandomForestClassifier(n_estimators=nb_est, max_depth=5, random_state=1)\n    model.fit(X_train,y_train)\n    \n    pred_train = model.predict(X_train)\n    acc_train = accuracy_score(y_train, pred_train)\n    \n    pred_dev = model.predict(X_dev)\n    acc_dev = accuracy_score(y_dev, pred_dev)\n    \n    train_accs.append(acc_train)\n    dev_accs.append(acc_dev)","ae5b327d":"\nplt.plot(train_accs)\nplt.plot(dev_accs)\n\nplt.legend(['Training accuracy', 'Dev-set accuracy'], loc='upper left')\nplt.show()","19b42c0d":"model_final = RandomForestClassifier(n_estimators=145, max_depth=5, random_state=1)\nmodel_final.fit(X,Y)","c9d831ba":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest = test.set_index(\"PassengerId\")\ntest = test.drop(columns= ['Name', 'Ticket', 'Cabin'])\n\ntest.Pclass = test.Pclass.astype(str)\ndums2 = pd.get_dummies(test[features])\ntest = test.drop(columns=features)\n\ntest = pd.concat([test, dums2], axis = 1)\n\ntest.fillna(test.mean(), inplace=True)\n\ntest.head()","0fd8a86e":"test = test[model_features]\npred = model_final.predict(test)\n\noutput = pd.DataFrame({'PassengerId':test.index, 'Survived': pred})\noutput.to_csv('my_submission.csv', index=False)","77aeeb5f":"## Manipulate the data to get the correct format\n\nFirst, let's extract Y, the labels (survived or not). Then, we will reformat the data frame to only keep relevant features.","680e5ed2":"# Titanic survival prediction V1\n\nFor my first version of this classification task, I will an ensemble method, the Random Forest classifier. ","7833fc9a":"## Address the NaN values \n\nWe need to address the missing values of our dataset. For this part here, as any passenger should have a value in any of the categories, I decided to replace the Nan by the mean.","f757422d":"## Create a model\n\nNow that our data is preprocessed, we are ready to build our first model.\n","c148e95f":"## Importing the Data\n\nFirst let's import the data.","58ca610b":"## Merge the dummies to the dataframe \n\nNow that we have created the categorical dummies for thos necessary, we can now merge back the two dataframe.","82da7f2c":"## Analyse the accuracy of the model\n\nWe can see that the model accuracy stops to increase at around 35 trees. Thus we will set our model to that value for the final predictions.","680fd37c":"## Final model and predictions generation\n\nFirst, we need to preprocess the test data the same way we preprocessed the training data. Thus, we will reuse the code we previouly wrote."}}