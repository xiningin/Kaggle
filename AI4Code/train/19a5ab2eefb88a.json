{"cell_type":{"d021cdf1":"code","a0f28b39":"code","1fb2e736":"code","e6d3676e":"code","83dce0e2":"code","af3469af":"code","86391a6b":"code","3a5a1531":"code","09359ec4":"code","9b62224c":"code","90a38697":"code","c795f905":"code","6862dc0d":"code","343215d7":"code","d4931056":"code","f6dc2548":"code","ebc3cc66":"code","344f98c2":"code","a5481ef4":"code","7625a9ff":"code","8939742a":"code","182901ae":"code","67e7c9e9":"code","fae94d9d":"code","59e4b127":"code","82d86d1c":"code","f4de26e3":"code","573aea5f":"code","391609af":"code","6c4847d7":"code","162beaaa":"code","63996f17":"code","65f40b7f":"code","0ff7b4b9":"code","10221a28":"code","ba132f33":"code","2c650e5b":"code","ea7d2cff":"code","f187a585":"markdown","ebe944c2":"markdown","f6fbb70b":"markdown","a3050269":"markdown","8471b66b":"markdown","853964ca":"markdown","a88d0c53":"markdown","e5bb0123":"markdown","88b20ee8":"markdown","85517c87":"markdown","ff247d9a":"markdown","739ebfca":"markdown","0e3b4846":"markdown","c263ce61":"markdown","eb62ab8f":"markdown"},"source":{"d021cdf1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport keras\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"Added shopping_cart.png for wordcloud\")","a0f28b39":"data = pd.read_csv('\/kaggle\/input\/multiclass-classification-data-for-turkish-tc32\/ticaret-yorum.csv')\npd.set_option('max_colwidth', 500)\ndata.head(5)","1fb2e736":"data.describe()","e6d3676e":"data.info()","83dce0e2":"data.category.value_counts()","af3469af":"count = 0\ncinemaximum4aydir = \"Cinemaximum 4 Ayd\u0131r Bilet Paralar\u0131n\u0131 Bir T\u00fcrl\u00fc \u0130ade Etmiyor,\"\nfor text in data.text:\n    if cinemaximum4aydir in text[:len(cinemaximum4aydir)]:\n        count += 1\nprint(count)","86391a6b":"exampleArray = np.array([[1,1],[1,2],[4,5]])\nexampleFrame = pd.DataFrame(exampleArray,columns=[\"ex1\",\"ex2\"])\nexampleFrame","3a5a1531":"ex1 = exampleFrame.drop_duplicates(subset=\"ex1\",keep=\"first\")\nprint(\"Without ignore_index\")\nprint(ex1)\nex2 = exampleFrame.drop_duplicates(subset=\"ex1\",keep=\"first\",ignore_index=True)\nprint(\"With ignore_index\")\nprint(ex2)","09359ec4":"data.text.duplicated(keep=\"first\").value_counts()","9b62224c":"data.drop_duplicates(subset=\"text\",keep=\"first\",inplace=True,ignore_index=True)\ndata.describe()","90a38697":"import plotly.graph_objects as go\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.express as px\nimport plotly.io as pio\n\ninit_notebook_mode(True)\n\nfig = px.bar(x=data.category.value_counts().index,y=data.category.value_counts(),color=data.category.value_counts().index,text=data.category.value_counts())\nfig.update_traces(hovertemplate=\"Category:'%{x}' Counted: %{y}\")\nfig.update_layout(title={\"text\":\"Category Counts\",\"x\":0.5,\"font\":{\"size\":35}},xaxis={\"title\":\"Category\",\"showgrid\":False},yaxis={\"title\":\"Value\",\"showgrid\":False},plot_bgcolor=\"white\",width=800,height=500,showlegend=False)\niplot(fig)","c795f905":"fig1 = px.pie(data,values=data.category.value_counts(),names=data.category.value_counts().index)\nfig1.update_traces(textposition='auto', textinfo='percent+label',marker={\"line\":{\"width\":1}},hoverinfo='label+percent',hole=0.4)\nfig1.update_layout(annotations=[{\"text\":\"Percentages\",\"showarrow\":False,\"font_size\":17}])\niplot(fig1)","6862dc0d":"import re\n\nwordList = list()\nfor i in range(len(data)):\n    temp = data.text[i].split()\n    for k in temp:\n        k = re.sub(\"[^a-zA-Z\u011f\u011e\u00fc\u00dc\u015f\u015e\u0131\u0130\u00f6\u00d6\u00e7\u00c7]\",\"\",k)\n        if k != \"\":\n            wordList.append(k)","343215d7":"from collections import Counter\n\nwordCount = Counter(wordList)\ncountedWordDict = dict(wordCount)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\n\nprint(\"Most Used 20 Words\")\nfor word,counted in sortedWordDict[0:20]:\n    print(\"{} : {}\".format(word,counted))","d4931056":"for i in data[\"text\"][7:10]:\n    if \"oku\" in i:\n        print(i)\n        print(\"*\"*20)","f6dc2548":"def dontReadMore(text):\n    temptext = text.split(\".\")\n    if \"Devam\u0131n\u0131\" in temptext[-1]:\n        text = temptext[:-1]\n    return \"\".join(text)\n\ndata[\"text\"] = data[\"text\"].apply(dontReadMore)","ebc3cc66":"for i in data[\"text\"][200:500]:\n    if \"oku\" in i:\n        print(i)\n        print(\"*\"*20)","344f98c2":"wordList = list()\nfor i in range(len(data)):\n    temp = data.text[i].split()\n    for k in temp:\n        k = re.sub(\"[^a-zA-Z\u011f\u011e\u00fc\u00dc\u015f\u015e\u0131\u0130\u00f6\u00d6\u00e7\u00c7]\",\"\",k)\n        if k != \"\":\n            wordList.append(k)\nwordCount = Counter(wordList)\ncountedWordDict = dict(wordCount)\nsortedWordDict = sorted(countedWordDict.items(),key = lambda x : x[1],reverse=True)\nprint(\"REAL Most Used 20 Words\")\nfor word,counted in sortedWordDict[0:20]:\n    print(\"{} : {}\".format(word,counted))","a5481ef4":"num = 75 # For using most used 75 words\nlist1 = list()\nlist2 = list()\nfor i in range(num):\n    list1.append(wordCount.most_common(num)[i][0])\n    list2.append(wordCount.most_common(num)[i][1])","7625a9ff":"fig2 = px.bar(x=list1,y=list2,color=list2,hover_name=list1,hover_data={'Word':list1,\"Count\":list2})\nfig2.update_traces(hovertemplate=\"Word:'%{x}' Value: %{y}\")\nfig2.update_layout(title={\"text\":\"Word Values\",\"x\":0.5,\"font\":{\"size\":30}},xaxis={\"title\":\"Words\",\"showgrid\":False},yaxis={\"title\":\"Value\",\"showgrid\":False},plot_bgcolor=\"white\")\nfig2.show()","8939742a":"from PIL import Image\n\nshopping_cart = np.array(Image.open(\"\/kaggle\/input\/shopping-cart\/shopping_cart.png\"))\nplt.imshow(shopping_cart)","182901ae":"from wordcloud import WordCloud\nfrom nltk.corpus import stopwords\n\ndef grey_color_func(word, font_size, position,orientation,random_state=None, **kwargs):\n    return(\"hsl(0,0%%, %d%%)\" % np.random.randint(50,55))\n\nstopwordCloud = set(stopwords.words(\"turkish\"))\n\nwordcloud = WordCloud(stopwords=stopwordCloud,max_words=1000,background_color=\"white\",min_font_size=3,mask=shopping_cart).generate_from_frequencies(countedWordDict)\nwordcloud.recolor(color_func = grey_color_func)\nplt.figure(figsize=[13,10])\nplt.axis(\"off\")\nplt.title(\"Word Cloud\",fontsize=20)\nplt.imshow(wordcloud)\nplt.show()","67e7c9e9":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk import word_tokenize\nimport time\n\nps = PorterStemmer()\nstopwordSet = set(stopwords.words('turkish'))\n\nt = time.time()\n\ndef leadMyWord(text):\n    text = re.sub('[^a-zA-Z\u011f\u011e\u00fc\u00dc\u015f\u015e\u0131\u0130\u00f6\u00d6\u00e7\u00c7]',\" \",text)\n    text = text.lower()\n    text = word_tokenize(text,language='turkish')\n    text = [word for word in text if not word in stopwordSet]\n    text = \" \".join(text)\n    return text   \n\ntextList = data.text.apply(leadMyWord)\ntextList = list(textList)\n\nprint(\"Before\")\nprint(data[\"text\"][2])\nprint(\"After\")\nprint(textList[2])\nprint(\"Time Passed\")\nprint(time.time()-t)","fae94d9d":"#preparing y\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nle = LabelEncoder()\nlabelEncode = le.fit_transform(data[\"category\"])\nprint(\"LabelEncode\")\nprint(labelEncode)\ncategorical_y = to_categorical(labelEncode)\nprint(\"To_Categorical\")\nprint(categorical_y)","59e4b127":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n#preparing x for ANN\ntfidv = TfidfVectorizer(max_features=20001)\nx = tfidv.fit_transform(textList)\nx.sort_indices()\n\nx_train,x_test,y_train,y_test = train_test_split(x,categorical_y,test_size=0.33,random_state=42)","82d86d1c":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.utils import plot_model\nfrom keras.losses import categorical_crossentropy\n\ndef build_ann_model():\n    model = Sequential()\n    \n    model.add(Dense(units=1024,activation=\"relu\",input_dim=x_train.shape[1]))\n    model.add(Dense(units=512,activation=\"relu\"))\n    model.add(Dense(units=256,activation=\"relu\"))\n    model.add(Dense(units=y_train.shape[1],activation=\"softmax\"))\n    \n    optimizer = Adam(lr=0.000015,beta_1=0.9,beta_2=0.999)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=categorical_crossentropy)\n    return model","f4de26e3":"ann_model = build_ann_model()\nplot_model(ann_model,show_shapes=True)","573aea5f":"ann_history = ann_model.fit(x_train,y_train,epochs=10,batch_size=256,shuffle=True)\nypred = ann_model.predict(x_test)","391609af":"from sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\nann_accuracy = accuracy_score(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\n#print(\"ANN Accuracy:\",ann_accuracy)\nann_cn = confusion_matrix(y_test.argmax(axis=-1),ypred.argmax(axis=-1))\nplt.subplots(figsize=(18,14))\nsns.heatmap(ann_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\nplt.title(\"ANN Accuracy: {}\".format(ann_accuracy),fontsize=50)\nplt.xlabel(\"Predicted\",fontsize=15)\nplt.ylabel(\"Actual\",fontsize=15)\nplt.show()","6c4847d7":"fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxe1[0].plot(ann_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\naxe1[1].plot(ann_history.history[\"loss\"],label=\"loss\",color=\"red\")\naxe1[0].title.set_text(\"ANN Accuracy\")\naxe1[1].title.set_text(\"ANN Loss\")\naxe1[0].set_xlabel(\"Epoch\")\naxe1[1].set_xlabel(\"Epoch\")\naxe1[0].set_ylabel(\"Rate\")\nplt.show()","162beaaa":"from keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\n\n#preparing x for CNN\nMAX_FEATURES = 20001\n\nonehot_corpus = []\nfor text in textList:\n    onehot_corpus.append(one_hot(text,MAX_FEATURES))\n    \nmaxTextLen = 0\nfor text in textList:\n    word_token=word_tokenize(text)\n    if(maxTextLen < len(word_token)):\n        maxTextLen = len(word_token)\n        \nprint(\"Max number of words : \",maxTextLen)\n\npadded_corpus=pad_sequences(onehot_corpus,maxlen=maxTextLen,padding='post')\nx_train2,x_test2,y_train2,y_test2 = train_test_split(padded_corpus,categorical_y,test_size=0.33,random_state=42)","63996f17":"from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten\ndef build_cnn_model():\n    model = Sequential()\n    \n    model.add(Embedding(MAX_FEATURES, 100, input_length=maxTextLen))\n\n\n    model.add(Conv1D(64, 2, padding='same', activation='relu'))\n    model.add(MaxPooling1D(2))\n    #model.add(MaxPooling1D(2))\n    \n    model.add(Flatten())\n    \n    model.add(Dense(units=1024,activation=\"relu\"))\n    model.add(Dense(units=512,activation=\"relu\"))\n    \n    model.add(Dense(units=y_train2.shape[1],activation=\"softmax\"))\n    \n    optimizer = Adam(lr=0.000055,beta_1=0.9,beta_2=0.999)\n    \n    model.compile(optimizer=optimizer,metrics=[\"accuracy\"],loss=categorical_crossentropy)\n    return model","65f40b7f":"cnn_model = build_cnn_model()\nplot_model(cnn_model,show_shapes=True)","0ff7b4b9":"cnn_history = cnn_model.fit(x_train2,y_train2,epochs=10,batch_size=1280,shuffle=True)\nypred2 = cnn_model.predict(x_test2)","10221a28":"cnn_accuracy = accuracy_score(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\n#print(\"CNN Accuracy:\",cnn_accuracy)\ncnn_cn = confusion_matrix(y_test2.argmax(axis=-1),ypred2.argmax(axis=-1))\nplt.subplots(figsize=(18,14))\nsns.heatmap(cnn_cn,annot=True,fmt=\"1d\",cbar=False,xticklabels=le.classes_,yticklabels=le.classes_)\nplt.title(\"CNN Accuracy: {}\".format(cnn_accuracy),fontsize=50)\nplt.xlabel(\"Predicted\",fontsize=15)\nplt.ylabel(\"Actual\",fontsize=15)\nplt.show()","ba132f33":"fig3, axe1 = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxe1[0].plot(cnn_history.history[\"accuracy\"],label=\"accuracy\",color=\"blue\")\naxe1[1].plot(cnn_history.history[\"loss\"],label=\"loss\",color=\"red\")\naxe1[0].title.set_text(\"CNN Accuracy\")\naxe1[1].title.set_text(\"CNN Loss\")\naxe1[0].set_xlabel(\"Epoch\")\naxe1[1].set_xlabel(\"Epoch\")\naxe1[0].set_ylabel(\"Rate\")\nplt.show()","2c650e5b":"def ann_predict(text):\n    puretext = leadMyWord(text)\n    vector = tfidv.transform([puretext])\n    vector.sort_indices()\n    predicted = ann_model.predict(vector)\n    predicted_category = predicted.argmax(axis=1)\n    return le.classes_[predicted_category]\ndef cnn_predict(text):\n    puretext = leadMyWord(text)\n    onehottext = one_hot(puretext,MAX_FEATURES)\n    text_pad = pad_sequences([onehottext],maxlen=maxTextLen,padding='post')\n    predicted = cnn_model.predict(text_pad)\n    predicted_category = predicted.argmax(axis=1)\n    return le.classes_[predicted_category]\n    \nfor _ in range(10):\n    randint = np.random.randint(len(data))\n    text = data.text[randint]  \n    print(\"  Text\")\n    print(\"-\"*8)\n    print(text)\n    print(\"-\"*20)\n    print(\"Actual Category: {}\".format(data.category[randint]))\n    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n    print(\"*\"*50)\n    ","ea7d2cff":"#Let me try it too\ndef predict_print(text):\n    print(\"  Text\")\n    print(\"-\"*8)\n    print(text)\n    print(\"-\"*20)\n    print(\"ANN Predicted Category: {}\".format(ann_predict(text)[0]))\n    print(\"CNN Predicted Category: {}\".format(cnn_predict(text)[0]))\n    print(\"*\"*50)\nmyText = \"Yeme\u011fin i\u00e7inden k\u0131l \u00e7\u0131kt\u0131, gitmenizi \u00f6nermiyorum.\" # hair came out of the dish, I don't suggest you go\npredict_print(myText)\nmyText = \"Tu\u015f bozuk.\" # Key Broken\npredict_print(myText)","f187a585":"# **CNN Building and Fitting** <a id=\"5\"><\/a>","ebe944c2":"# Multi-Class Text Classification with Turkish Dataset\n\n<h3>In this kernel, I will try to classify \"comments\"(text) with \"categories\"(text) using CNN<\/h3>\n<p style=\"font-size:20px\">Table of Content<\/p>\n\n* [Data Overview](#1)\n* [Word Overview](#2)\n* [NLP Processing](#3)\n* [ANN Building and Fitting](#4)\n* [CNN Building and Fitting](#5)","f6fbb70b":"<h2>As we can see 'oku' and 'oku\"' used with \"Devam\u0131n\u0131\" -\"more\" in english-<\/h2>\n<h3>We know this dataset contains \"comments\" so \"... Devam\u0131n\u0131 oku\" is used to see rest of the comment. It means we can clear it too for better visualization!<\/h3>","a3050269":"<h2>So data contains 431306 lines and 32 unique categories and it seems there is no NaN, right?<\/h2>\n<h3>But why text label has 4075 (431306-427231) non-unique comments? And we have a \"top\" comment starts with \"Cinemaximum 4 Ayd\u0131r\" so let's check what it is  <\/h3>","8471b66b":"Before starting, I'm very grateful to Raj Mehrotra for sharing [\"A Detailed Explanation of Keras Embedding Layer\"](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer)","853964ca":"<h3>Now we can visualize with REAL most used words <\/h3>","a88d0c53":"So they really are duplicated...\n\nWe don't need to but I want to get rid of every non-unique comment so I will use pandas' 2 functions named \"duplicated\" and \"drop_duplicates\"\n\n\"duplicated\" gets keep={\u2018first\u2019, \u2018last\u2019, False} parameter and can be used both \"pandas.DataFrame\" and \"pandas.Series\" objects and returns \"pandas.Series\" object with True-False statements \n\n\"drop_duplicates\" works like \"dropna\" so it gets inplace={True,False} and subset={column label} with keep={\u2018first\u2019,\u2018last\u2019 False}\nThere is also another parameter named ignore_index={True,False} let me explain this\nIn default ignore_index=False it means if it remove a duplicated, index will not change so 1-a,2-a,3-b will be 1-a,3-b and it breaks for loop with len(data) because of that we need to change ignore_index\n\n\nIn default keep =\"first\"\n","e5bb0123":"# **ANN Building and Fitting** <a id=\"4\"><\/a>","88b20ee8":"<h2> Now we can visualize Category<\/h2>","85517c87":"<h1 >Thanks for reading, I'm open to your advices.<\/h1>","ff247d9a":"# **Word Overview** <a id=\"2\"><\/a>\n<h3> Let's start with word list<\/h3>","739ebfca":"<h2>So they are most used 20 words... <\/h2>\n<h3>But why is \"oku\" -\"read\" in english- used 386776 times even if there are 427231\ttexts? Let's see...<\/h3>","0e3b4846":"# **NLP Processing** <a id=\"3\"><\/a>","c263ce61":"# **Data Overview** <a id=\"1\"><\/a>","eb62ab8f":"Let me explain categories:\n* Kamu Hizmetleri: Public Service\n* Finans: Finance\n* Cep Telefon Kategori: Mobile Phone Category\n* Enerji: Energy\n* Ula\u015f\u0131m: Transportation\n* Kargo-Nakliyat: Cargo-Shipping\n* Medya: Media\n* Mutfak Ara\u00e7-Gere\u00e7: Kitchen Tools\n* Al\u0131\u015fveri\u015f: Shopping\n* Mekan ve E\u011flence: Venue and Entertainment\n* Elektronik: Electronic\n* Beyaz E\u015fya: Home Appliance\n* K\u00fc\u00e7\u00fck Ev Aletleri: Small Appliances\n* \u0130nternet: Internet\n* Giyim: Clothing\n* Etkinlik ve Organizasyon: Event and Organization\n* \u0130\u00e7ecek: Beverage\n* Sa\u011fl\u0131k: Medical\n* Sigortac\u0131l\u0131k: Insurance\n* Spor: Sport\n* Mobilya-Ev Tekstili: Furniture-Home Textile\n* Otomotiv: Automotive\n* Turizm: Tourism\n* E\u011fitim: Education\n* G\u0131da: Food\n* Temizlik: Cleaning\n* Hizmet Sekt\u00f6r\u00fc: Service Industry\n* M\u00fccevher-Saat-G\u00f6zl\u00fck: Jewel-Watch-Glasses\n* Bilgisayar: Computer\n* Ki\u015fisel Bak\u0131m ve Kozmetik: Personal Care and Cosmetics\n* Anne-Bebek: Mother-Baby\n* Emlak ve \u0130n\u015faat: Real Estate and Construction"}}