{"cell_type":{"4b5d5852":"code","e96dee5b":"code","f56d577b":"code","d823aa1e":"code","5244c060":"code","eae259ea":"code","902f7427":"code","fcc9b877":"code","86410af4":"code","5351ce4f":"code","07dcb8c7":"code","f4b36240":"code","61da0178":"code","5dece37b":"code","0f584a2c":"code","f8983f9e":"code","2e8d1def":"code","b80a133a":"code","60d77ad5":"code","80fcc956":"code","ba4ddca1":"code","8fd7ea01":"code","1fda3fbb":"code","0d421f6d":"code","bfe9f85e":"code","c0ba2a09":"code","d54a1047":"code","99505bbb":"code","fa8063ca":"code","1c229b4d":"code","2589757c":"code","35b175ec":"code","a26993a2":"code","fab72281":"code","f204e162":"code","9e42fab4":"code","288b1648":"code","70e13459":"code","145a5dca":"code","bb1fbca2":"code","f049666a":"code","72faed12":"code","fc07c09d":"markdown","278eb9cf":"markdown","5edb09d4":"markdown","5c5ade0b":"markdown","9b584246":"markdown","3dc8696d":"markdown","db6378e9":"markdown","a78424b7":"markdown","70450b1f":"markdown","e5bdf87e":"markdown","92c9555a":"markdown","16842527":"markdown","0c5c8056":"markdown","ae82cd49":"markdown"},"source":{"4b5d5852":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pydicom\nimport cv2\nfrom tqdm import tqdm\nimport json\nimport os","e96dee5b":"directory = '..\/input\/rsna-intracranial-hemorrhage-detection\/'\ndir_train = '..\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_train_images\/'\ndir_test = '..\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_test_images\/'","f56d577b":"train_df =  pd.read_csv(directory+'stage_1_train.csv')\ntest_df = pd.read_csv(directory+'stage_1_sample_submission.csv')\ntrain_images = os.listdir(dir_train)\ntest_images = os.listdir(dir_test)","d823aa1e":"print(\"Train CSV :\",train_df.shape)\nprint(\"Test CSV :\",test_df.shape)\nprint(\"Train Images:\",len(train_images))\nprint(\"Test Images:\",len(test_images))","5244c060":"display(train_df.head())","eae259ea":"display(train_df.tail())","902f7427":"display(train_df.head())","fcc9b877":"print(\"Train: \\n\",train_df.count())\nprint(\"Test: \\n\",test_df.count())","86410af4":"train_df['Image_ID'] = train_df['ID'].str.rsplit(pat='_',n=1,expand=True)[0]\ntrain_df['Hemorrhage'] = train_df['ID'].str.rsplit(pat='_',n=1,expand=True)[1]\ntrain_df = train_df[['Image_ID','Hemorrhage','Label']]","5351ce4f":"#Nombre d'images uniques\nprint(\"Number of images :\",train_df['Image_ID'].nunique())\nprint(\"Number of Hemorraghes :\",train_df['Hemorrhage'].nunique())","07dcb8c7":"pd.DataFrame(train_df['Image_ID'].value_counts()).reset_index().head(10)","f4b36240":"display(test_df.head())","61da0178":"test_df['Image_ID'] = test_df['ID'].str.rsplit(pat='_',n=1,expand=True)[0]\ntest_df['Image_ID'] = test_df['Image_ID']+\".png\"\ntest_df = test_df['Image_ID'].drop_duplicates().reset_index()[['Image_ID']]","5dece37b":"display(test_df.head())","0f584a2c":"def graph_hemorrhage():\n    percentage = train_df[(train_df['Hemorrhage']=='any')&(train_df['Label']==1)]['Image_ID'].count()\/train_df['Image_ID'].nunique()*100\n    print(\"percentage of images with hemorrhage : \",round(percentage,2),'%')\n\n    pd.DataFrame([percentage,100-percentage],columns=['percentage']).plot(kind='pie',\n                                                                          y='percentage',\n                                                                          labels=['Hemorrhage','Non_Hemorrhage'],\n                                                                          title='Hemorraghe distribution',\n                                                                          autopct='%.1f%%',\n                                                                          legend=False,\n                                                                          figsize=(6,6),\n                                                                          shadow=True,\n                                                                          startangle=90)\n    plt.show()","f8983f9e":"Hemorrhage = pd.DataFrame(train_df[(train_df['Label']==1)&(train_df['Hemorrhage']!='any')]['Hemorrhage'].value_counts()).reset_index()\nHemorrhage.columns = ['Hemorrhage','Number_Pictures']\n\ndisplay(Hemorrhage)\n\nHemorrhage.plot(kind='pie',y='Number_Pictures',labels=Hemorrhage['Hemorrhage'].unique(),title='Hemorrhage distribution',\\\n               legend=False,autopct='%.1f%%',figsize=(6,6),shadow=True, startangle=90,fontsize=11)\n\nplt.show()","2e8d1def":"graph_hemorrhage()","b80a133a":"display(train_df.head())","60d77ad5":"# Remove image with damaged pixels\ntrain_df = train_df[train_df['Image_ID']!='ID_6431af929']\ntrain_images.remove('ID_6431af929.dcm')","80fcc956":"def undersample(dataset): \n    ID_hemorrhage = train_df[(train_df['Hemorrhage']=='any')&(train_df['Label']==1)][['Image_ID']].reset_index(drop=True)\n    n_samples = len(ID_hemorrhage)\n    ID_Healthy = train_df[(train_df['Hemorrhage']=='any')&(train_df['Label']==0)][['Image_ID']].reset_index(drop=True).sample(n=n_samples,random_state=1)\n    \n    if dataset =='train_df':\n        result = pd.concat([train_df.merge(ID_hemorrhage,how='inner',on='Image_ID'),\\\n                            train_df.merge(ID_Healthy,how='inner',on='Image_ID')]).sample(frac=1).\\\n                            sort_values('Image_ID').reset_index(drop=True)\n    \n    elif dataset == 'train_images':\n        data_frame = pd.DataFrame(train_images,columns=['Image_ID'])\n        result = pd.concat([data_frame.merge(ID_hemorrhage+'.dcm',how='inner',on='Image_ID'),\\\n                            data_frame.merge(ID_Healthy+'.dcm',how='inner',on='Image_ID')])['Image_ID'].tolist()\n    return(result)\n    ","ba4ddca1":"train_df = undersample('train_df')\ntrain_images = undersample('train_images')","8fd7ea01":"graph_hemorrhage()","1fda3fbb":"pivot_df = train_df.drop_duplicates().pivot(index='Image_ID', columns='Hemorrhage', values='Label').reset_index()\npivot_df['Image_ID'] = pivot_df['Image_ID']+'.png'\ndisplay(pivot_df.head())","0d421f6d":"def get_first_of_dicom_field_as_int(x):\n    if type(x) == pydicom.multival.MultiValue:\n        return int(x[0])\n    else:\n        return int(x)\n\ndef get_metadata(image):\n    metadata = {\n        \"window_center\": image.WindowCenter,\n        \"window_width\": image.WindowWidth,\n        \"intercept\": image.RescaleIntercept,\n        \"slope\": image.RescaleSlope\n    }\n    return {k: get_first_of_dicom_field_as_int(v) for k, v in metadata.items()}\n\ndef window_image(img, window_center, window_width, intercept, slope):\n    img = img * slope + intercept\n    img_min = window_center - window_width \/\/ 2\n    img_max = window_center + window_width \/\/ 2\n    img[img < img_min] = img_min\n    img[img > img_max] = img_max\n    return img\n\ndef normalize(image):\n    min_image = image.min()\n    max_image = image.max()\n    return (image - min_image) \/ (max_image - min_image)\n\ndef resize(image,width,weight):\n    resized = cv2.resize(image, (width, weight))\n    return resized\n\ndef save(directory,image,image_normalized_resized):\n    save_dir = '\/kaggle\/tmp\/'\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    path = directory+image\n    new_path = save_dir + image.replace('.dcm', '.png')        \n    res = cv2.imwrite(new_path, image_normalized_resized)\n    \ndef normalize_resize_save(dataset,width,weight,directory):\n    for i in tqdm(dataset):\n        image=pydicom.read_file(directory+i)\n        image_windowed = window_image(image.pixel_array, ** get_metadata(image))\n        image_normalized_resized = resize(normalize(image_windowed),width,weight)\n        save(directory,i,image_normalized_resized)","bfe9f85e":"image=pydicom.read_file(dir_train+train_df['Image_ID'][0]+\".dcm\")\nimage_windowed = window_image(image.pixel_array, ** get_metadata(image))\n\ndisplay(image)\nplt.imshow(image_windowed, cmap=plt.cm.bone)","c0ba2a09":"def view_images(data_frame,hemorraghe):\n    width = 5\n    height = 1\n    fig, axs = plt.subplots(height, width, figsize=(20,5))\n\n    list_hem = pd.DataFrame(train_df[(train_df['Label']==1)&(train_df['Hemorrhage']==hemorraghe)][['Image_ID']].\\\n                            head(width*height)+\".dcm\").reset_index()\n    \n    for i in range(0,width*height):\n        image=pydicom.read_file(dir_train+list_hem['Image_ID'][i])\n        image_windowed = window_image(image.pixel_array, ** get_metadata(image))\n        fig.add_subplot(height,width, i+1)\n        axs[i].set_title(list_hem['Image_ID'][i])\n        plt.imshow(image_windowed, cmap=plt.cm.bone)\n        \n    plt.suptitle(\"Images with \"+hemorraghe,fontsize = 20)\n    plt.show()","d54a1047":"for i in train_df['Hemorrhage'].unique():\n    view_images(train_df,i)","99505bbb":"normalize_resize_save(train_images,224,224,dir_train)\nnormalize_resize_save(test_images,224,224,dir_test)","fa8063ca":"print(\"Number of files resized and saved: {}\".format(len(os.listdir(\"\/kaggle\/tmp\/\"))))","1c229b4d":"from keras import layers\nimport tensorflow as tf\nfrom keras.applications import DenseNet121\n# from keras.preprocessing.image import ImageDataGenerator\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras_preprocessing.image import ImageDataGenerator\nimport torch\nimport keras\n\nEPOCHS = 11\nBATCH_SIZE = 32\nmodel_nn='densenet'","2589757c":"def pick_nn(neu_net):\n    if neu_net == 'densenet':\n        neural_network = DenseNet121(\n            weights='..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5',\n            include_top=False,\n            input_shape=(224,224,3)\n        )\n        \n        \n    elif neu_net=='resnet':\n        neural_network = ResNet50 (\n            weights = 'imagenet',\n            include_top=False,\n            input_shape =(224,224,3)\n        )\n        \n    return(neural_network)","35b175ec":"def DataGenerator():\n    return(ImageDataGenerator(zoom_range=0.1,  # set range for random zoom\n        # set mode for filling points outside the input boundaries\n        fill_mode='constant',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=True,  # randomly flip images,\n        validation_split=0.1))\n\ndef create_generator(dataset,batch_size):\n    if dataset == 'test':\n        generator = DataGenerator().flow_from_dataframe(\n                        test_df,\n                        directory='\/kaggle\/tmp\/',\n                        x_col='Image_ID',\n                        class_mode=None,\n                        target_size=(224, 224),\n                        batch_size=batch_size,\n                        shuffle=False\n                    )\n    else:\n        generator = DataGenerator().flow_from_dataframe(dataframe=pivot_df, \n                                            directory=\"\/kaggle\/tmp\/\",\n                                            x_col=\"Image_ID\",\n                                            y_col=['any', 'epidural', 'intraparenchymal', \n                                           'intraventricular', 'subarachnoid', 'subdural'],\n                                            class_mode='other',\n                                            target_size=(224,224),\n                                            batch_size=batch_size,\n                                            subset = dataset)\n    \n    return(generator)","a26993a2":"def build_model(nn):\n    model = Sequential()\n    \n    model.add(pick_nn(neu_net=nn))\n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(6, activation='sigmoid'))\n    \n    model.compile(\n        loss='binary_crossentropy',\n        optimizer=Adam(lr=0.001),\n        metrics=['accuracy']\n    )\n    \n    return model","fab72281":"model = build_model(model_nn)\nmodel.summary()","f204e162":"checkpoint = ModelCheckpoint(\n    'model.h5', \n    monitor='val_loss', \n    verbose=0, \n    save_best_only=True, \n    save_weights_only=False,\n    mode='auto'\n)\n\n\nhistory = model.fit_generator(\n    create_generator(dataset = 'training', batch_size=BATCH_SIZE),\n    steps_per_epoch=3500,\n    validation_data=create_generator(dataset = 'validation', batch_size=BATCH_SIZE),\n    validation_steps=3000,\n    callbacks=[checkpoint],\n    epochs=EPOCHS\n)","9e42fab4":"history_df = pd.DataFrame(history.history)\n\nplt.plot(history_df[['loss', 'val_loss']] )\nplt.title('Value of loss')\nplt.xlabel('Number of epochs')\nplt.ylabel('Loss')\nplt.legend(('Loss_train', 'Loss_Val'))\nplt.grid()\nplt.show()\n\nplt.plot(history_df[['accuracy', 'val_accuracy']])\nplt.title('Value of accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of epochs')\nplt.legend(('Acc', 'Acc_Val'))\nplt.grid()\nplt.show()","288b1648":"model.load_weights('model.h5')\ncreate_data_test = create_generator(dataset = 'test',batch_size=BATCH_SIZE)\ny_test = model.predict_generator(create_data_test,\n    steps=len(create_data_test),\n    verbose=1\n)","70e13459":"test_df = test_df.join(pd.DataFrame(y_test, columns = ['any', 'epidural', 'intraparenchymal', \n         'intraventricular', 'subarachnoid', 'subdural']))","145a5dca":"test_df = test_df.melt(id_vars=['Image_ID'])","bb1fbca2":"test_df['ID'] = test_df.Image_ID.apply(lambda x: x.replace('.png', '')) + '_' + test_df.variable\ntest_df['Label'] = test_df['value']\n\ntest_df[['ID', 'Label']].to_csv('submission.csv', index=False)","f049666a":"test_df[['ID', 'Label']].to_csv('submission.csv', index=False)","72faed12":"from IPython.display import HTML\nimport pandas as pd\nimport numpy as np\n\ndef create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\n# create a link to download the dataframe which was saved with .to_csv method\ncreate_download_link(filename='submission.csv')","fc07c09d":"#### Normalize, resize and save new images in png format[](http:\/\/)","278eb9cf":"## Import","5edb09d4":"## Exploratory analysis","5c5ade0b":"## Submission","9b584246":"##### An additional label for any, which should always be true if any of the sub-type labels is true. We could know the number of images that have any kind of hemorrhage with this variable","3dc8696d":"#### Visualize first image in the Data Set","db6378e9":"## Table of contents\n1. [Import libraries and datasets](#Import)\n2. [Exploratory analysis](#Exploratory-analysis)\n\n    * [Number of samples](#Number-of-samples)\n    * [Data Visualization](#Data-visualization) \n    * [Preprocess images](#Prepocess-Images)\n\n\n3. [Model](#Model) \n4. [Submission](#Submission)\n","a78424b7":"#### References\n\n[Data_Visualisation + Model ](https:\/\/www.kaggle.com\/jesucristo\/rsna-introduction-eda-models) <br>\n[Data_Visualisation ](https:\/\/www.kaggle.com\/marcovasquez\/basic-eda-data-visualization)\n","70450b1f":"#### Visualize images with hemorraghes","e5bdf87e":"### Number of samples","92c9555a":"### Data visualization","16842527":"###  Transform dataset","0c5c8056":"### Prepocess Images","ae82cd49":"## Model"}}