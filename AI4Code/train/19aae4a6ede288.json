{"cell_type":{"0adf8680":"code","000ca772":"code","8d9ce84c":"code","fa9e59bf":"code","5a8a4f43":"code","ad1796b6":"code","bf1e5496":"code","1b323470":"code","63089815":"code","7d88da5e":"code","89e9ee72":"code","6016ea8c":"code","acc4f2c0":"code","1c7ac09d":"code","88e1f20d":"code","93bd500d":"code","abe03dba":"code","b3aa61df":"code","a2d655c7":"code","f8f5ee68":"code","dbb7bf44":"markdown","b29ecf41":"markdown","260b7be0":"markdown","06da49a3":"markdown","d5dbdf3e":"markdown"},"source":{"0adf8680":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nimport random\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import callbacks","000ca772":"#Trying to get reproducible results\nfrom numpy.random import seed\nseed(42)\nfrom tensorflow.random import set_seed\nset_seed(42)\n\nrandom.seed(42)\nos.environ['PYTHONHASHSEED'] = str(42)","8d9ce84c":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\nY_train = df_train['target'].copy()\nX_train = df_train.copy().drop('target', axis = 1)\n\nX_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', index_col = 'id')","fa9e59bf":"class_map = {'Class_1': 0,\n            'Class_2': 1,\n            'Class_3': 2,\n            'Class_4': 3,\n            'Class_5': 4,\n            'Class_6': 5,\n            'Class_7': 6,\n            'Class_8': 7,\n            'Class_9': 8}\nY_train = Y_train.map(class_map).astype('int')\nY_train","5a8a4f43":"#Converting target series to matrix for multiclass classification on Keras\n\nY_train = to_categorical(Y_train)\nY_train","ad1796b6":"def get_wideanddeep():\n    # Wide Network\n    wide = keras.experimental.LinearModel()\n\n    # Deep Network\n    inputs = keras.Input(shape=[75])\n    x = layers.Embedding(360, 8, input_length = 75)(inputs)\n    x = layers.Conv1D(16, kernel_size=1, activation='relu')(x) #added 21\/06\n    x = layers.Flatten()(x)\n    x = layers.Dropout(0.3)(x) #added 21\/06\n    x = layers.Dense(units = 128, activation = 'relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(units = 64, activation = 'relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    x = layers.Dense(units = 32, activation = 'relu')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    #outputs = layers.Dense(9, activation = 'softmax')(x)\n    outputs = layers.Dense(9)(x)\n    deep = keras.Model(inputs=inputs, outputs=outputs)\n    \n    # Wide and Deep Network\n    wide_and_deep = keras.experimental.WideDeepModel(\n        linear_model=wide,\n        dnn_model=deep,\n        activation='softmax',\n    )\n    \n    return wide_and_deep","bf1e5496":"keras.backend.clear_session()\n\nwide_and_deep_model = get_wideanddeep()\nwide_and_deep_model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.0002), metrics='accuracy')","1b323470":"X_train_split, X_val_split, Y_train_split, Y_val_split = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 42\n                                                    , stratify = Y_train)","63089815":"early_stopping = callbacks.EarlyStopping(\n    patience=15,\n    min_delta=0.0000001,\n    restore_best_weights=True,\n)\n\n#New callback\nplateau = callbacks.ReduceLROnPlateau(\n    factor = 0.5,                                     \n    patience = 2,                                   \n    min_delt = 0.0000001,                                \n    cooldown = 0,                               \n    verbose = 1\n) ","7d88da5e":"history = wide_and_deep_model.fit(X_train_split, Y_train_split,                   \n          batch_size = 128, epochs = 100,\n          validation_data=(X_val_split, Y_val_split),\n          callbacks=[early_stopping, plateau]);","89e9ee72":"score = wide_and_deep_model.evaluate(X_val_split, Y_val_split, verbose = 0)\nprint('Test loss: {}'.format(score[0]))\nprint('Test accuracy: {}%'.format(score[1] * 100))\n\n#Test loss: 1.7481427192687988 w\/o dropout after embedding\n#Test loss: 1.7472147941589355 with dropout after embedding\n#Test loss: 1.7464418411254883 with Conv1D (16,1) and dropout after embedding","6016ea8c":"fig, ax = plt.subplots(figsize=(20,8))\nsns.lineplot(x = history.epoch, y = history.history['loss'])\nsns.lineplot(x = history.epoch, y = history.history['val_loss'])\nax.set_title('Learning Curve (Loss)')\nax.set_ylabel('Loss')\nax.set_xlabel('Epoch')\nax.legend(['train', 'test'], loc='best')\nplt.show()","acc4f2c0":"Y_train = df_train['target'].copy()\nY_train = Y_train.map(class_map).astype('int')\nY_train","1c7ac09d":"def prediction (X_train, Y_train, X_test):\n    \n    keras.backend.clear_session()\n\n    kfold = StratifiedKFold(n_splits = 25)\n\n    y_pred = np.zeros((100000,9))\n    train_oof = np.zeros((200000,9))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        ytrain = to_categorical(ytrain)\n        yval = to_categorical(yval)\n        \n        # fit model for current fold\n        wide_and_deep_model = get_wideanddeep()\n        wide_and_deep_model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.0002), metrics='accuracy')\n        \n        wide_and_deep_model.fit(xtrain, ytrain,\n        batch_size = 128, epochs = 100,\n        validation_data=(xval, yval),\n        callbacks=[early_stopping, plateau]);\n\n        #create predictions\n        y_pred += wide_and_deep_model.predict(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = wide_and_deep_model.predict(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        # calculate and append logloss\n        fold_logloss = metrics.log_loss(yval,val_pred)\n        print(\"Logloss: {0:0.5f}\". format(fold_logloss))\n  \n    return y_pred, train_oof","88e1f20d":"nn_pred, train_oof = prediction (X_train, Y_train, X_test)","93bd500d":"print(\"Logloss: {0:0.6f}\".format(metrics.log_loss(Y_train,train_oof)))\n#10 folds - Logloss: 1.744198","abe03dba":"train_oof = pd.DataFrame(train_oof, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\ntrain_oof","b3aa61df":"pred_test = pd.DataFrame(nn_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\npred_test","a2d655c7":"train_oof.to_csv('nn_train_oof.csv', index=False)\ntrain_oof","f8f5ee68":"output = pred_test\noutput['id'] = X_test.index\noutput.to_csv('submission.csv', index=False)\n\noutput","dbb7bf44":"## Creating and Evaluating the Wide and Deep NN","b29ecf41":"My other notebooks in this competition:\n- [Tabular Playground Series - June\/2021: Starter - EDA + Base LightGBM](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-starter-eda-base-lgbm)\n- [Tabular Playground Series - June\/2021: Simple Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-simple-nn-with-keras)\n- [Tabular Playground Series - June\/2021: Keras Neural Network with Embedding Layer](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-keras-nn-with-embedding)\n- [Tabular Playground Series - June\/2021: LightAutoML with KNN Features](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-lightautoml-w-knn-feats)\n- [Tabular Playground Series - June\/2021: Keras Neural Network with Skip Connections](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-keras-nn-with-skip-connections)","260b7be0":"## Making Predictions","06da49a3":"## Importing Libraries and Datasets","d5dbdf3e":"# <center>Tabular Playground Series - June\/2021<center>\n## <center>Wide and Deep Neural Network with Keras<center>\n---\n\nIn the spirit of trying different approaches for Neural Networks in this month\u2019s competition, this notebook presents an attempt with a Wide and Deep Network, easily created using [tf.keras.experimental](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/experimental) module. This same approach can be found in the [bonus lesson from the Kaggle\u2019s course \u2018Intro to Deep Learning\u2019](https:\/\/www.kaggle.com\/ryanholbrook\/detecting-the-higgs-boson-with-tpus).\n<br>\n<br>\n<br>    \n    \n![Wide and Deep NN](https:\/\/i.imgur.com\/Jzj75Nv.png)\n <center>Wide and Deep NN. [Cheng, H. et al. Wide and Deep Learning for Recommender Systems. (2016)](https:\/\/arxiv.org\/pdf\/1606.07792.pdf)<center>\n<br>"}}