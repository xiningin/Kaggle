{"cell_type":{"fafa8950":"code","4912d056":"code","7a76cefe":"code","65d82b5b":"code","f97405aa":"code","7fd5b555":"code","3344dd64":"code","aeafe7d8":"code","75cfa436":"code","fc241932":"code","f3c71921":"code","a3e2c3f5":"code","c029009a":"code","127807e4":"code","09b72c91":"code","340eaea4":"code","570fdd87":"code","c2bf6d6e":"code","136d5013":"code","7543648f":"code","47742b67":"code","def56b84":"code","db4d38e3":"code","3a4d6bed":"code","e294b3a1":"code","abd54892":"code","9d7cffe8":"code","fbdd4101":"code","c53827c3":"code","1e320803":"code","8a495c51":"code","c174fde4":"code","117b5020":"code","df94c6a6":"code","4eb6a5c9":"code","6229694a":"code","02a3a10b":"code","8db57f75":"code","91cdc49c":"code","23316d67":"code","b3f24298":"code","7e576329":"code","b6935678":"code","cfcccbcd":"code","a2135780":"code","627166d0":"code","435d37b0":"code","c6c4e402":"code","9d3ade8f":"code","8184807c":"code","c070da64":"code","5ffdfaca":"code","b1a85373":"code","fb55a369":"code","642a4099":"code","830d1aff":"code","7e35928e":"code","33ee5ece":"code","5857ba36":"code","b1b42978":"code","a75bc01a":"code","9d62615b":"code","081d236e":"code","90115285":"code","e44251a4":"code","26d7cb05":"code","8d9378c2":"code","8924fc01":"code","c1326d7e":"code","7b90d279":"code","9aa1d8ae":"code","cfab5f7a":"markdown","726112af":"markdown","0b26ddb1":"markdown","07ba50d1":"markdown","67932ed0":"markdown","3a1fd0f9":"markdown","9bd9ffdc":"markdown","74f7268c":"markdown","7d2fb8fd":"markdown","cac676fb":"markdown","6469b6e8":"markdown","44db5610":"markdown","b0f00e59":"markdown","5bcf96ce":"markdown","fc4f965a":"markdown","da47d772":"markdown","f24f1bf1":"markdown","af09a302":"markdown","8bead7aa":"markdown","2bea4c9b":"markdown","06f779f3":"markdown","3bbb4283":"markdown","596c194d":"markdown","9b4ba6f9":"markdown","9e3a34e5":"markdown","d038672c":"markdown","8ed0b57a":"markdown","482f3a73":"markdown","6ec8c443":"markdown","b4e258e3":"markdown","b6ce8a09":"markdown","32100f08":"markdown","6cd3bfdb":"markdown","a3a3827e":"markdown","4eacf216":"markdown","f50ddb52":"markdown","fe7412f2":"markdown","ccbc8cdb":"markdown","02648239":"markdown","aa88923b":"markdown","879e0f82":"markdown","9f41fbc2":"markdown","5225f835":"markdown","ac448561":"markdown","f2045eeb":"markdown","440d3882":"markdown","3ecd395e":"markdown","ca129ab3":"markdown","b5121b04":"markdown","c6668b64":"markdown"},"source":{"fafa8950":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport datetime\nfrom datetime import date\nimport sklearn\nfrom sklearn.preprocessing import Imputer\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\npd.set_option(\"display.max_columns\", 100)\npd.set_option('max_colwidth',200)","4912d056":"calendar = pd.read_csv(\"..\/input\/boston\/calendar.csv\")\nlistings = pd.read_csv(\"..\/input\/boston\/listings.csv\")","7a76cefe":"# How much data is missing in each row of the dataset?\nmissing_values_in_rows = listings.isna().mean(axis=1).values\nplt.rcParams['figure.figsize'] = (5, 3)\nax = plt.hist(missing_values_in_rows)\nplt.title('Missing rates (rows)')","65d82b5b":"def lower(s):\n    try:\n        return s.lower()\n    except:\n        return s\n\ndef perc_to_numbers(p):\n    '''\n    \"30%\" --> 0.3\n    '''\n    try:\n        return float(p.split(\"%\")[0])\/100\n    except:\n        return float(\"nan\")","f97405aa":"summary = listings['summary'].apply(lower)\ndowntown_bos = summary.str.contains(\"downtown boston\").astype(\"float32\")\neast_bos = summary.str.contains(\"east boston\").astype(\"float32\")\nhost_age = listings['host_since'].apply(lambda x: (datetime.datetime.today() - pd.to_datetime(x)).days\/30)\nreponse_rate = listings['host_response_rate'].apply(perc_to_numbers)\nacceptance_rate = listings['host_acceptance_rate'].apply(perc_to_numbers)","7fd5b555":"listings[['host_total_listings_count']].describe()","3344dd64":"print(\"{:.2%} of the hosts have no more than 2 listings, {:.2%} of the hosts have more than 10 listings\"\\\n.format(sum(listings['host_total_listings_count']<=2)\/len(listings), sum(listings['host_total_listings_count']>10)\/len(listings)))","aeafe7d8":"one_or_two_listings = listings['host_total_listings_count'] <= 2 \nthree_to_ten_listings = (listings['host_total_listings_count'] > 2) & (listings['host_total_listings_count'] < 10)\nmore_than_ten_listings = listings['host_total_listings_count'] > 10","75cfa436":"response_time_dict = {\n    'a few days or more':1,\n    'within a day': 2,\n    'within a few hours': 3,\n    'within an hour': 4\n}","fc241932":"plt.hist(host_age[listings['host_response_time'].isna()])","f3c71921":"response_time_ordinal = listings['host_response_time']\\\n                        .fillna(\"within a few hours\").apply(lambda x: response_time_dict[x])","a3e2c3f5":"real_bed = listings['bed_type'] == \"Real Bed\"\npopular_neighborhoods = list(listings\\\n                        .groupby('neighbourhood_cleansed')\\\n                        .count()['id'].sort_values(ascending=False)[:10].index)\nneighborhoods_one_hot = pd.get_dummies(listings[['neighbourhood_cleansed']])\namenities_len = listings['amenities'].apply(len)\ncancellation_policy_dict = {\n    'flexible': 1,\n    'moderate': 2,\n    'strict': 3,\n    'super_strict_30': 4\n}\ncancellation_policy_ordinal = listings['cancellation_policy']\\\n                              .fillna(\"super_strict_30\").apply(lambda x: cancellation_policy_dict[x])","c029009a":"final_df = pd.DataFrame({\n    'id': listings['id'],\n    'downtown_bos': downtown_bos,\n    'east_bos': east_bos,\n    'host_age': host_age,\n    'reponse_rate': reponse_rate,\n    'acceptance_rate': acceptance_rate,\n    'one_or_two_listings': one_or_two_listings,\n    'three_to_ten_listings': three_to_ten_listings,\n    'more_than_ten_listings': more_than_ten_listings,\n    'response_time_ordinal': response_time_ordinal,\n    'real_bed': real_bed,\n    'amenities_len': amenities_len,\n    'cancellation_policy_ordinal': cancellation_policy_ordinal\n}).join(neighborhoods_one_hot)","127807e4":"def convert_binary(x):\n    try:\n        return 1 if x == \"t\" else 0\n    except:\n        return 0","09b72c91":"binary_pd = listings[['host_is_superhost',\n              'host_has_profile_pic',\n              'host_identity_verified',\n              'is_location_exact',\n              'requires_license',\n              'instant_bookable',\n              'require_guest_profile_picture',\n              'require_guest_phone_verification']].applymap(convert_binary)","340eaea4":"final_df = final_df.join(binary_pd)","570fdd87":"def convert_money(s):\n    '''\n    \"$250.00\" --> 250\n    '''\n    try:\n        return float(eval(s.split(\"$\")[1]))\n    except:\n        return float(\"nan\")","c2bf6d6e":"money_pd = listings[['price','extra_people']].applymap(convert_money)","136d5013":"final_df = final_df.join(money_pd)","7543648f":"listings['price_num'] = final_df['price']\nlistings['price_per_bed'] = listings['price_num'] \/ listings['beds'].replace(0,1)\navg_price_per_bed = listings.groupby('host_neighbourhood')['price_per_bed'].mean().reset_index()\\\n                              .rename({'price_per_bed':'price_per_bed_nbh'}, axis=1)\nlistings = listings.merge(avg_price_per_bed, on=\"host_neighbourhood\")\nlistings['price_per_bed_compared_to_nbh'] = listings['price_per_bed'] - listings['price_per_bed_nbh']","47742b67":"x = plt.hist(listings['price_per_bed_compared_to_nbh'])\nplt.title('Difference between price per bed v.s. neighborhood average')","def56b84":"final_df = final_df.join(listings[[\n                                   'accommodates',\n                                   'bathrooms',\n                                   'bedrooms',\n                                   'beds',\n                                   'number_of_reviews',\n                                   'review_scores_rating',\n                                   'review_scores_accuracy',\n                                   'review_scores_cleanliness',\n                                   'review_scores_checkin',\n                                   'review_scores_communication',\n                                   'review_scores_location',\n                                   'review_scores_value',\n                                   'price_per_bed_compared_to_nbh'\n                                   ]])","db4d38e3":"m = final_df.isna().sum()\nm[m>0]","3a4d6bed":"final_df[['downtown_bos',\n            'east_bos',\n            'price_per_bed_compared_to_nbh',\n            'number_of_reviews']] = final_df[['downtown_bos',\n                                                            'east_bos',\n                                                            'price_per_bed_compared_to_nbh',\n                                                            'number_of_reviews']].fillna(0)","e294b3a1":"final_df_sub1 = final_df[['reponse_rate',\n                  'review_scores_rating',\n                  'review_scores_accuracy',\n                  'review_scores_cleanliness',\n                  'review_scores_checkin',\n                  'review_scores_communication',\n                  'review_scores_location',\n                  'review_scores_value']].copy()\n\nfinal_df[['reponse_rate',\n          'review_scores_rating',\n          'review_scores_accuracy',\n          'review_scores_cleanliness',\n          'review_scores_checkin',\n          'review_scores_communication',\n          'review_scores_location',\n          'review_scores_value']] = pd.DataFrame(Imputer(missing_values=float('nan'), \n                                                         strategy=\"mean\", \n                                                         axis=0)\\\n                                                         .fit_transform(final_df_sub1),\n                                                 columns = final_df_sub1.columns)","abd54892":"final_df_sub2 = final_df[['bathrooms','bedrooms','beds','acceptance_rate']].copy()\n\nfinal_df[['bathrooms','bedrooms','beds','acceptance_rate']] = pd.DataFrame(Imputer(missing_values=float('nan'), \n                                                         strategy=\"most_frequent\", \n                                                         axis=0)\\\n                                                         .fit_transform(final_df_sub2),\n                                                         columns = final_df_sub2.columns)","9d7cffe8":"final_df['accommodates'] = final_df['accommodates'].fillna(1)","fbdd4101":"m = final_df.isna().sum()\nm[m>0]","c53827c3":"final_df = final_df[final_df['price'] > 0]","1e320803":"final_df.iloc[:,1:] = final_df.iloc[:,1:].astype(\"float32\")","8a495c51":"final_df.describe()","c174fde4":"print(listings['calendar_last_scraped'].min())\nprint(listings['calendar_last_scraped'].max())","117b5020":"calendar['available'] = calendar['available'].apply(convert_binary)\ncalendar['year_month'] = calendar['date'].apply(lambda x: x[:7])\ncalendar.groupby('year_month')['available'].mean().reset_index().plot()\nplt.title('Vacancy rates with months')","df94c6a6":"calendar = calendar.merge(listings[['id','price']].rename({'id':'listing_id'}, axis=1), on='listing_id')\ncalendar['price_x'] = calendar['price_x'].apply(convert_money)\ncalendar['price_y'] = calendar['price_y'].apply(convert_money)\ncalendar_available = calendar[calendar['available']==1]\ncalendar_booked = calendar[calendar['available']==0]","4eb6a5c9":"a = plt.hist(calendar_available['price_x'] - calendar_available['price_y'], bins=100)\nplt.title(\"DIfference between prices in listings and actual prices\")","6229694a":"earnings = calendar_booked.groupby('listing_id')['price_y'].sum().reset_index().rename({'price_y':'future_earnings'}, axis=1)\na = plt.hist(earnings['future_earnings'])\nplt.title(\"Future Earnings\")","02a3a10b":"sum(earnings['future_earnings'] == 0)","8db57f75":"earnings['log_future_earnings'] = np.log(earnings['future_earnings']+1)","91cdc49c":"def y_to_earnings(y):\n    '''\n    The function to convert the log earnings\n    '''\n    return np.exp(y)-1","23316d67":"a = plt.hist(earnings['log_future_earnings'])\nplt.title(\"Log Future Earnings\")","b3f24298":"features = final_df.copy()\nid_earnings = features[['id']].merge(earnings.rename({'listing_id':'id'}, axis=1).drop('future_earnings', axis=1), on=\"id\", how=\"left\").fillna(0)\nprint(\"{:.2%} of the listings don't have future earnings\".format(sum(id_earnings['log_future_earnings']==0)\/len(id_earnings)))","7e576329":"X_all = features.drop('id', axis=1)\ny_all = id_earnings['log_future_earnings']\nX_train, X_test, y_train, y_test = train_test_split(X_all,\n                                                                  y_all,\n                                                                  test_size = 0.2,\n                                                                  random_state = 0)\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","b6935678":"def regressor(X, y, params, random_state=1):\n    '''\n    This function was borrowed from. https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py\n    It takes the X, y and parameters of the model and plots how deviance change as we have more iterations and the important variables\n    '''\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n    print(\"Training set has {} samples.\".format(X_train.shape[0]))\n    print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n    \n    # Fit regression model\n    clf = GradientBoostingRegressor(**params)\n    clf.fit(X_train, y_train)\n    mse = mean_squared_error(y_test, clf.predict(X_test))\n    print(\"MSE: %.4f\" % mse)\n    \n    # Plot training deviance\n    test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n    for i, y_pred in enumerate(clf.staged_predict(X_test)):\n        test_score[i] = clf.loss_(y_test, y_pred)\n\n    plt.figure(figsize=(20, 10))\n    plt.subplot(1, 2, 1)\n    plt.title('Deviance')\n    plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n             label='Training Set Deviance')\n    plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n             label='Test Set Deviance')\n    plt.legend(loc='upper right')\n    plt.xlabel('Boosting Iterations')\n    plt.ylabel('Deviance')\n    \n    # Plot feature importance\n    feature_importance = clf.feature_importances_\n    \n    # make importances relative to max importance\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max())\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    im = pd.DataFrame({'feature': features.columns[1:], 'relative importance': feature_importance}).sort_values('relative importance')[-20:]\n    plt.subplot(1, 2, 2)\n    plt.barh(im['feature'], im['relative importance'])\n    plt.xlabel('Relative Importance')\n    plt.title('Variable Importance')\n    plt.show()\n    \n    plt.subplots_adjust(wspace=30)\n    return clf, im","cfcccbcd":"#the first trial\nparams = {\n    'n_estimators': 200,\n    'max_depth': 6,\n    'min_samples_split': 2,\n    'learning_rate': 0.01,\n    'loss': 'ls',\n}","a2135780":"model, im = regressor(X_train, y_train, params, random_state=1)","627166d0":"np.exp(5**0.5)-1","435d37b0":"im.sort_values('relative importance', ascending=False)","c6c4e402":"plt.rcParams['figure.figsize'] = (20,20)\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    sns.regplot(x=X_train[im.sort_values('relative importance', ascending=False)['feature'].values[i]], y=y_train)","9d3ade8f":"no_earnings = id_earnings[id_earnings['log_future_earnings']==0].merge(features, on='id')\nhave_earnings = id_earnings[id_earnings['log_future_earnings']>0].merge(features, on='id')","8184807c":"plt.rcParams['figure.figsize'] = (20,10)\n\nplt.subplot(2,4,1)\na = plt.hist(no_earnings['host_age'])\nplt.title(\"host_age_no_earnings\")\nplt.subplot(2,4,2)\na = plt.hist(no_earnings['is_location_exact'])\nplt.title(\"is_location_exact_no_earnings\")\nplt.subplot(2,4,3)\na = plt.hist(no_earnings['one_or_two_listings'])\nplt.title(\"one_or_two_listings_no_earnings\")\nplt.subplot(2,4,4)\na = plt.hist(no_earnings['price'])\nplt.title(\"price_no_earnings\")\n\nplt.subplot(2,4,5)\na = plt.hist(have_earnings['host_age'])\nplt.title(\"host_age_have_earnings\")\nplt.subplot(2,4,6)\na = plt.hist(have_earnings['is_location_exact'])\nplt.title(\"is_location_exact_have_earnings\")\nplt.subplot(2,4,7)\na = plt.hist(have_earnings['one_or_two_listings'])\nplt.title(\"one_or_two_listings_have_earnings\")\nplt.subplot(2,4,8)\na = plt.hist(have_earnings['price'])\nplt.title(\"price_have_earnings\")","c070da64":"plt.rcParams['figure.figsize'] = (6,4)\n\na = plt.hist(no_earnings[no_earnings['is_location_exact']==1]['host_age'])\nplt.title('host_age of no earning listings with True is_location_exact')","5ffdfaca":"rg = GradientBoostingRegressor()\nparameters = {'learning_rate': [0.01, 0.05, 0.08, 0.1],\n                    'max_depth': [4, 6, 8, 10],\n                    'max_features': [None, 50, 40, 30, 20, 15, 10]}\nscorer = \"neg_mean_squared_error\"","b1a85373":"grid_obj = GridSearchCV(rg, parameters, scoring=scorer)","fb55a369":"grid_fit = grid_obj.fit(X_train, y_train)","642a4099":"best_model = grid_fit.best_estimator_\nbest_model","830d1aff":"predicted_log_earnings = best_model.predict(X_test)\nprint(\"MSE of the tuned model: {:.3}\".format(np.mean(np.square(predicted_log_earnings - y_test))))\ndifference = abs(y_to_earnings(predicted_log_earnings) - y_to_earnings(y_test))\nprint(\"{:.2%} have less than $1000 absolute error\".format(sum(difference<1000)\/len(y_test)))\nprint(\"{:.2%} have less than $5000 absolute error\".format(sum(difference<5000)\/len(y_test)))\nprint(\"{:.2%} have less than $10000 absolute error\".format(sum(difference<10000)\/len(y_test)))\nprint(\"{:.2%} have less than $20000 absolute error\".format(sum(difference<20000)\/len(y_test)))","7e35928e":"plt.rcParams['figure.figsize'] = (20,5)\na = plt.hist(difference, bins=1000)\nplt.title('Absolute difference')","33ee5ece":"plt.rcParams['figure.figsize'] = (20,5)\n\nplt.subplot(1,2,1)\nplt.hist(y_to_earnings(y_test)[difference<1000], bins=100)\nplt.title('Earnings of listings with less than $1000 predicting error')","5857ba36":"plt.rcParams['figure.figsize'] = (15,3)\n\nplt.subplot(1,2,1)\na = plt.hist(y_to_earnings(predicted_log_earnings))\nplt.title(\"predicted earnings\")\n\nplt.subplot(1,2,2)\na = plt.hist(y_to_earnings(y_test))\nplt.title(\"actual earnings\")","b1b42978":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,300000)\nsns.regplot(x=y_to_earnings(y_test), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"difference\")","a75bc01a":"def prepare_train_test_data(X_all, y_all, test_size=0.2):\n    '''\n    Split the data into traning set and testing set\n    '''\n    X_train, X_test, y_train, y_test = train_test_split(X_all,\n                                                                      y_all,\n                                                                      test_size = 0.2,\n                                                                      random_state = 0)\n    print(\"Training set has {} samples.\".format(X_train.shape[0]))\n    print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n    return X_train, X_test, y_train, y_test\n\ndef tune_train_model(rg, parameters, scorer, X_train, y_train):\n    '''\n    Grid search on parameters to find the best model\n    '''\n    grid_obj = GridSearchCV(rg, parameters, scoring=scorer)\n    grid_fit = grid_obj.fit(X_train, y_train)\n    best_model = grid_fit.best_estimator_\n    return best_model\n\ndef evaluate_model(best_model, X_test, y_test):\n    '''\n    Fit the model on testing set; calculate the MSE; calculate percentages of the records with less than X amount of absolute error\n    '''\n    predicted_log_earnings = best_model.predict(X_test)\n    print(\"MSE of the tuned model: {:.3}\".format(np.mean(np.square(predicted_log_earnings - y_test))))\n    difference = abs(y_to_earnings(predicted_log_earnings) - y_to_earnings(y_test))\n    print(\"{:.2%} have less than $1000 absolute error\".format(sum(difference<1000)\/len(y_test)))\n    print(\"{:.2%} have less than $5000 absolute error\".format(sum(difference<5000)\/len(y_test)))\n    print(\"{:.2%} have less than $10000 absolute error\".format(sum(difference<10000)\/len(y_test)))\n    print(\"{:.2%} have less than $20000 absolute error\".format(sum(difference<20000)\/len(y_test)))\n    return predicted_log_earnings, difference","9d62615b":"parameters1 = {'learning_rate': [0.01, 0.05, 0.08, 0.1],\n                    'max_depth': [4, 6, 8, 10],\n                    'max_features': [None, 50]}","081d236e":"have_future_earnings_index = id_earnings['log_future_earnings'] > 0\nX_train_new, X_test_new, y_train_new, y_test_new = prepare_train_test_data(X_all[have_future_earnings_index.values], y_all[have_future_earnings_index.values])\nbest_model = tune_train_model(rg, parameters, scorer, X_train_new, y_train_new)","90115285":"best_model","e44251a4":"predicted_log_earnings, difference = evaluate_model(best_model, X_test_new, y_test_new)","26d7cb05":"plt.rcParams['figure.figsize'] = (20,5)\na = plt.hist(difference, bins=1000)\nplt.title('Absolute difference')","8d9378c2":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,y_to_earnings(max(y_test_new)))\nsns.regplot(x=y_to_earnings(y_test_new), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"predicted earnings\")","8924fc01":"new_index = (id_earnings['log_future_earnings'] > 0) & (X_all.reset_index().drop(\"index\", axis=1)['is_location_exact'] == 1)\nX_train_new, X_test_new, y_train_new, y_test_new = prepare_train_test_data(X_all.reset_index().drop(\"index\", axis=1)[new_index.values], y_all[new_index.values])\nbest_model = tune_train_model(rg, parameters, scorer, X_train_new, y_train_new)","c1326d7e":"best_model","7b90d279":"predicted_log_earnings, difference = evaluate_model(best_model, X_test_new, y_test_new)","9aa1d8ae":"plt.rcParams['figure.figsize'] = (8, 8)\nx = np.linspace(0,y_to_earnings(max(y_test_new)))\nsns.regplot(x=y_to_earnings(y_test_new), y=y_to_earnings(predicted_log_earnings))\nplt.plot(x, x, linewidth=2)\nplt.xlabel(\"actual earnings\")\nplt.ylabel(\"predicted earnings\")","cfab5f7a":"### 3.6.3 Columns that need be filled with the most common values","726112af":"## 3.1 Generated features","0b26ddb1":"### 3.6.1 Columns that we want to set the NaN values as 0s","07ba50d1":"# Become a data driven Airbnb host 2\n\nThis is a blog continued from the first one: https:\/\/www.kaggle.com\/tianyiwang\/become-a-data-driven-airbnb-host-part-1. In this blog, we try to solve a business problem through machine learning --- predicting future earnings based on the current listing information. Imagine that you are an Airbnb host and you would like to know the expected earning for one of your listings next year. Once you have the model, you can play with the information of your listing to see how you might change your future earning if there's any variations of your current listing. ","67932ed0":"The distribution of host_age for these listings is consistent with the distribution for all the listings. So we fill the nan values with `\"within a few hours\"` (the 2nd popular value for this column; We didn't choose to fill with `within an hour` to avoid overestimating the field. We believe that most hosts can reply within a few hours).","3a1fd0f9":"The model performs slightly better.","9bd9ffdc":"## 3.5 Add other numeric columns and id column (`id`)","74f7268c":"A small investigation on the numbers of listings:","7d2fb8fd":"We can see that there are quite a few listings that actually don't have any future earnings but were predicted to have earnings by the model. We are then curious --- what if we only train and test the model on the listings where `Is_location_exact` is 1 and have future earnings.","cac676fb":"Of course the future earnings are not determined by only one feature. From the scatter plot we can have an idea of whether a variable will positively or negatively affect the future earning.\n\nWe also see that 11% of the listings don't have future earnings. Are they very new listings? Are they of a certain kind?","6469b6e8":"The data was scraped on September 2016, and on that month about 27% of the listings are available, while in the next month 41% of rooms are available. However, for the next year until 2017 December, each month about half of the rooms are booked already. We believe that the future earnings we calculate out of the data are meaningful. Or if we have more historical data, we might find out that usually when the month actually comes, the booking rate will rise by 20% (an example). In that way we can adjust our predicted results to approximate the real values.","44db5610":"This is the best model!","b0f00e59":"Check the table again: ","5bcf96ce":"## 3.3 Convert money value strings to numbers","fc4f965a":"Columns with missing values:","da47d772":"How many records have future earnings as 0?","f24f1bf1":"Hosts with one or two listings might just have two rooms for rent in one apartment\/house. We will group them together. 40% of the hosts have multiple properties and some of them even have hundred of listings. We thus create the groups:\n- hosts with 1~2 listings\n- hosts with 3~10 listings\n- hosts with more than 10 listings","af09a302":"To improve the model, I think it will help to get more enrichment on the address data. For example, we can get the house price data using the addresses. We can also have a look how many hotels there are in the neighborhood of the listing. ","8bead7aa":"We first check that all listings were scraped on the same day:","2bea4c9b":"## 3.6 Deal with missing values","06f779f3":"# 9. What if we leave out listings with no future earnings when training the model? \n\n11% of the listings don't have future earnings --- they are not booked at all for the next year at the time when the data was scraped. Most of them have `Is_location_exact` as 0. They might be outliers. Let's train the model again only with the listings that have future earnings.","3bbb4283":"# 4. Calculate future earnings","596c194d":"Some records have `N\/A` response time. Are they very new listings?","9b4ba6f9":"`price` is a very importance piece of information and we don't want to impute this value here. We will just drop the listings with `NaN` price.","9e3a34e5":"Our first model has a MSE of 5 --- which is about 8 dollars. The most important variable is `is_location_exact` --- we don't quite know what it means. The most important 9 features:","d038672c":"Another thing that we are concerning is that the availability data are probably much lower than what they actually will be, especially for the days that are in far future.","8ed0b57a":"### 3.6.4 Other missing values filling","482f3a73":"# 7. Build the gradient boosting regressor ","6ec8c443":"### 3.6.2 Columns that need be filled with the means","b4e258e3":"## 3.2. Binary columns","b6ce8a09":"# 8. Tune the model","32100f08":"When tested on the test set, our model predicted half of the listings with less than $5000 absolute difference. From the histograms of the predicted earnings and actual earnings, we can see that our model is more conservative and produce smaller numbers for the listings that might have very high earnings.","6cd3bfdb":"# 10. Interesting take-aways ","a3a3827e":"Didn't see that pattern here.","4eacf216":"# 1. Load data and packages","f50ddb52":"Remember that in `calendar` data, if a room is available on a certain date, we can see its price on that day. We also have the price information in `listing` data. Thus we would like to compare the available prices in `calendar` data and those in `listing` data to see if the theoratical prices are very different from the prices in reality.","fe7412f2":"The differences are mostly very small.","ccbc8cdb":"The MSE is much smaller and apparently, this new model performs much better.","02648239":"A bad senario will be that there's a group of rows with significantly higher missing rates. In our case, it's good to see that most of the listings have at least 70% of the fields flled, and the shape of the distribution is kind of ideal.","aa88923b":"From the model we can see that it's important to have `Is_location_exact` as 1 (although we are not quite sure what it means). Price and `Price_per_bed_compared_to_neighborhood` have positive effect on the future earnings but I think it's mostly because listings with higher price have better amenities, locations, etc. So don't worry if your listing is more expensive than most of the other listings in the neighbordhood. If your room is truly in better condition and provides more exciting stuffs for the guests, you will have better future earnings. The length of the amenity field also has a positive effect. Other important factors are acceptance rate and response rate.","879e0f82":"## 3.7 Make sure all the columns except the id column are numeric","9f41fbc2":"# 5. Align the features table   ","5225f835":"# 6. Split the dataset to testing and training sets","ac448561":"# 2. Investigate rows with high missing rates\n\nFirst, let's check if there are special groups of listings with a lot of N\/A columns ---- maybe they are very new or bad listings which we want to leave out when building our model.","f2045eeb":"# 3. Feature engineering\n\nWe then create features from the existing columns. Once this is done, we will add some columns in the raw data that we can directly use without much engineering. The features we generated:\n1. downtown_bos, east_bos: whether the listing is in one of the two neighborhoods. From the exploration, many of the listings are from these areas, maybe they are convenient\/go-to areas in Boston.\n2. host_age: generated from host_since (months)\n3. reponse_rate, acceptance_rate: convert to numbers\n4. one_or_two_listings, three_to_ten_listings, more_than_ten_listings: how many listings the host has\n5. response_time_ordinal\uff1a generated from response_time\n6. real_bed: if the bed is a real bed\n7. neighborhoods_one_hot: one-hot coding to indicate if the listing is in one of 10 most popular neighborhoods\n8. amenities_len: length of the amenities field to roughly indicate how many amenities the listing offers\n9. cancellation_policy_ordinal: the larger the number the more flexible the cancellation policy is\n10. price_per_bed_compared_to_nbh: the difference between the price per bed for that listing and the average price per bed for that neighborhood (**this is a slightly complicated feature to build! We will do it in the end**)","440d3882":"![](https:\/\/kylekleinphotography.com\/wp-content\/uploads\/KKP12002-1.jpg)","3ecd395e":"We can see that listings with no earnings are mostly the listings with `is_location_exact` of 0. Are the no earning listings with `True` is_location_exact very new listings?","ca129ab3":"According to my understanding, the `calendar` data shows the availablity of the listings for the following one year. When the room is not available, we won't have the price information. We know that the price for a listing is very seasonal and usually the price will increase a lot during busy seasons. Since we don't know the prices for the listings that were already booked, we will just use the price in the `listing` data to approximate that.","b5121b04":"## 3.4 Add `price_per_bed_compared_to_nbh`\n-- The difference between the price per bed for that listing and the average price per bed for that neighborhood","c6668b64":"That's not a lot. The earnings data are very right skewed. We will do a log transformation on it:"}}