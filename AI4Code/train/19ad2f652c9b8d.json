{"cell_type":{"e3a9e68c":"code","3e5e6179":"code","258e56f4":"code","3a238ae4":"code","04ab30b4":"code","396a547c":"code","2d1d8375":"code","14636a02":"code","5d8c57b0":"code","6f212111":"code","29ef3bec":"code","13e847c5":"code","b08d218c":"code","d383fc08":"code","b750711f":"code","accd9880":"code","6f7c0e7e":"code","ce4d9e93":"code","caa6c8f1":"code","1f07a62a":"code","d7fdb230":"code","021bfe86":"code","3f113c97":"code","7e24f0c5":"code","19ec4cc7":"code","76250c24":"code","18cb1187":"code","7ea72c93":"code","189b3999":"code","c0a15233":"code","cd01f387":"code","833163dc":"code","2f0bff1a":"code","4b9d15e9":"code","c1d051b3":"code","73077966":"code","1ed3bbcd":"code","1be0a55b":"code","544dc035":"code","71eda963":"code","b67d238a":"code","5e908251":"code","b4e8960b":"code","4f602f90":"code","ad32df5c":"code","e099b019":"code","38aba360":"code","87c4585a":"code","65bb79d4":"code","d232f310":"code","3b026faa":"code","8ea464a5":"code","8dd84d13":"code","e3e4e447":"code","08dd0f92":"code","7f0c67b0":"code","edb3099f":"markdown","33fdab16":"markdown","7f384392":"markdown","2272bd67":"markdown","e037f68d":"markdown","ebc36acc":"markdown","6ebcd1a7":"markdown","3c95f352":"markdown","0dce434c":"markdown","a047fb85":"markdown","29c6af90":"markdown","126565f3":"markdown","e24c62b8":"markdown","a88f2359":"markdown","20e0b47d":"markdown","5ee76c41":"markdown","b843e691":"markdown","ddd05af4":"markdown","669317de":"markdown","8467d2f1":"markdown","5a0ea887":"markdown","584e2a84":"markdown"},"source":{"e3a9e68c":"import os\nimport re\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Audio\n# from entropy import spectral_entropy\nfrom keras import layers\nfrom keras import models\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nimport itertools","3e5e6179":"# Paths to\nRavdess = \"..\/input\/speech-emotion-recognition-en\/Ravdess\/audio_speech_actors_01-24\"\nCrema = \"..\/input\/speech-emotion-recognition-en\/Crema\"\nSavee = \"..\/input\/speech-emotion-recognition-en\/Savee\"\nTess = \"..\/input\/speech-emotion-recognition-en\/Tess\"","258e56f4":"ravdess_directory_list = os.listdir(Ravdess)\n\nemotion_df = []\n\nfor dir in ravdess_directory_list:\n    actor = os.listdir(os.path.join(Ravdess, dir))\n    for wav in actor:\n        info = wav.partition(\".wav\")[0].split(\"-\")\n        emotion = int(info[2])\n        emotion_df.append((emotion, os.path.join(Ravdess, dir, wav)))","3a238ae4":"Ravdess_df = pd.DataFrame.from_dict(emotion_df)\nRavdess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)","04ab30b4":"Ravdess_df.Emotion.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}, inplace=True)\nRavdess_df.head()","396a547c":"emotion_df = []\n\nfor wav in os.listdir(Crema):\n    info = wav.partition(\".wav\")[0].split(\"_\")\n    if info[2] == 'SAD':\n        emotion_df.append((\"sad\", Crema + \"\/\" + wav))\n    elif info[2] == 'ANG':\n        emotion_df.append((\"angry\", Crema + \"\/\" + wav))\n    elif info[2] == 'DIS':\n        emotion_df.append((\"disgust\", Crema + \"\/\" + wav))\n    elif info[2] == 'FEA':\n        emotion_df.append((\"fear\", Crema + \"\/\" + wav))\n    elif info[2] == 'HAP':\n        emotion_df.append((\"happy\", Crema + \"\/\" + wav))\n    elif info[2] == 'NEU':\n        emotion_df.append((\"neutral\", Crema + \"\/\" + wav))\n    else:\n        emotion_df.append((\"unknown\", Crema + \"\/\" + wav))\n\n\nCrema_df = pd.DataFrame.from_dict(emotion_df)\nCrema_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nCrema_df.head()","2d1d8375":"tess_directory_list = os.listdir(Tess)\n\nemotion_df = []\n\nfor dir in tess_directory_list:\n    for wav in os.listdir(os.path.join(Tess, dir)):\n        info = wav.partition(\".wav\")[0].split(\"_\")\n        emo = info[2]\n        if emo == \"ps\":\n            emotion_df.append((\"surprise\", os.path.join(Tess, dir, wav)))\n        else:\n            emotion_df.append((emo, os.path.join(Tess, dir, wav)))\n\n\nTess_df = pd.DataFrame.from_dict(emotion_df)\nTess_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nTess_df.head()","14636a02":"savee_directiory_list = os.listdir(Savee)\n\nemotion_df = []\n\nfor wav in savee_directiory_list:\n    info = wav.partition(\".wav\")[0].split(\"_\")[1].replace(r\"[0-9]\", \"\")\n    emotion = re.split(r\"[0-9]\", info)[0]\n    if emotion=='a':\n        emotion_df.append((\"angry\", Savee + \"\/\" + wav))\n    elif emotion=='d':\n        emotion_df.append((\"disgust\", Savee + \"\/\" + wav))\n    elif emotion=='f':\n        emotion_df.append((\"fear\", Savee + \"\/\" + wav))\n    elif emotion=='h':\n        emotion_df.append((\"happy\", Savee + \"\/\" + wav))\n    elif emotion=='n':\n        emotion_df.append((\"neutral\", Savee + \"\/\" + wav))\n    elif emotion=='sa':\n        emotion_df.append((\"sad\", Savee + \"\/\" + wav))\n    else:\n        emotion_df.append((\"surprise\", Savee + \"\/\" + wav))\n\n\nSavee_df = pd.DataFrame.from_dict(emotion_df)\nSavee_df.rename(columns={1 : \"Path\", 0 : \"Emotion\"}, inplace=True)\n\nSavee_df.head()","5d8c57b0":"# Let's concat all datasets together for doing some analysis\ndf = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\ndf.shape","6f212111":"df.head(10)","29ef3bec":"%matplotlib inline\n\nplt.style.use(\"ggplot\")","13e847c5":"plt.title(\"Count of emotions:\")\nsns.countplot(x=df[\"Emotion\"])\nsns.despine(top=True, right=True, left=False, bottom=False)","b08d218c":"def create_waveplot(data, sr, e):\n    plt.figure(figsize=(10, 3))\n    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n    librosa.display.waveplot(data, sr=sr)\n    plt.show()\n\ndef create_spectrogram(data, sr, e):\n    # stft function converts the data into short term fourier transform\n    X = librosa.stft(data)\n    Xdb = librosa.amplitude_to_db(abs(X))\n    plt.figure(figsize=(12, 3))\n    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar()","d383fc08":"emotion='fear'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)\n","b750711f":"emotion='angry'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","accd9880":"emotion='sad'\npath = np.array(df.Path[df.Emotion==emotion])[1]\ndata, sampling_rate = librosa.load(path)\ncreate_waveplot(data, sampling_rate, emotion)\ncreate_spectrogram(data, sampling_rate, emotion)\nAudio(path)","6f7c0e7e":"def noise(data, random=False, rate=0.035, threshold=0.075):\n    \"\"\"Add some noise to sound sample. Use random if you want to add random noise with some threshold.\n    Or use rate Random=False and rate for always adding fixed noise.\"\"\"\n    if random:\n        rate = np.random.random() * threshold\n    noise_amp = rate*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n\ndef stretch(data, rate=0.8):\n    \"\"\"Stretching data with some rate.\"\"\"\n    return librosa.effects.time_stretch(data, rate)\n\ndef shift(data, rate=1000):\n    \"\"\"Shifting data with some rate\"\"\"\n    shift_range = int(np.random.uniform(low=-5, high = 5)*rate)\n    return np.roll(data, shift_range)\n\ndef pitch(data, sampling_rate, pitch_factor=0.7, random=False):\n    \"\"\"\"Add some pitch to sound sample. Use random if you want to add random pitch with some threshold.\n    Or use pitch_factor Random=False and rate for always adding fixed pitch.\"\"\"\n    if random:\n        pitch_factor=np.random.random() * pitch_factor\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","ce4d9e93":"df.head()","caa6c8f1":"path = df[df[\"Emotion\"] == \"happy\"][\"Path\"].iloc[0]\ndata, sampling_rate = librosa.load(path)","1f07a62a":"plt.figure(figsize=(14,4))\nlibrosa.display.waveplot(data, sampling_rate)\nAudio(path)","d7fdb230":"noised_data = noise(data, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=noised_data, sr=sampling_rate)\nAudio(noised_data, rate=sampling_rate)\n","021bfe86":"stretched_data = stretch(data, rate=0.5)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=stretched_data, sr=sampling_rate)\nAudio(stretched_data, rate=sampling_rate)\n","3f113c97":"shifted_data = shift(data)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=shifted_data, sr=sampling_rate)\nAudio(shifted_data, rate=sampling_rate)\n","7e24f0c5":"pitched_data = pitch(data, sampling_rate, pitch_factor=0.5, random=True)\nplt.figure(figsize=(14,4))\nlibrosa.display.waveplot(y=pitched_data, sr=sampling_rate)\nAudio(pitched_data, rate=sampling_rate)","19ec4cc7":"n_fft = 2048\nhop_length = 512","76250c24":"def chunks(data, frame_length, hop_length):\n    for i in range(0, len(data), hop_length):\n        yield data[i:i+frame_length]\n\n# Zero Crossing Rate\ndef zcr(data, frame_length=2048, hop_length=512):\n    zcr = librosa.feature.zero_crossing_rate(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(zcr)\n\n\ndef energy(data, frame_length=2048, hop_length=512):\n    en = np.array([np.sum(np.power(np.abs(data[hop:hop+frame_length]), 2)) for hop in range(0, data.shape[0], hop_length)])\n    return en \/ frame_length\n\n\ndef rmse(data, frame_length=2048, hop_length=512):\n    rmse = librosa.feature.rms(y=data, frame_length=frame_length, hop_length=hop_length)\n    return np.squeeze(rmse)\n\n\ndef entropy_of_energy(data, frame_length=2048, hop_length=512):\n    energies = energy(data, frame_length, hop_length)\n    energies \/= np.sum(energies)\n\n    entropy = 0.0\n    entropy -= energies * np.log2(energies)\n    return entropy\n\n\ndef spc(data, sr, frame_length=2048, hop_length=512):\n    spectral_centroid = librosa.feature.spectral_centroid(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spectral_centroid)\n\n\n# def spc_entropy(data, sr):\n#     spc_en = spectral_entropy(data, sf=sr, method=\"fft\")\n#     return spc_en\n\ndef spc_flux(data):\n    isSpectrum = data.ndim == 1\n    if isSpectrum:\n        data = np.expand_dims(data, axis=1)\n\n    X = np.c_[data[:, 0], data]\n    af_Delta_X = np.diff(X, 1, axis=1)\n    vsf = np.sqrt((np.power(af_Delta_X, 2).sum(axis=0))) \/ X.shape[0]\n\n    return np.squeeze(vsf) if isSpectrum else vsf\n\n\ndef spc_rollof(data, sr, frame_length=2048, hop_length=512):\n    spcrollof = librosa.feature.spectral_rolloff(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n    return np.squeeze(spcrollof)\n\n\ndef chroma_stft(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    stft = np.abs(librosa.stft(data))\n    chroma_stft = librosa.feature.chroma_stft(S=stft, sr=sr)\n    return np.squeeze(chroma_stft.T) if not flatten else np.ravel(chroma_stft.T)\n\n\ndef mel_spc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mel = librosa.feature.melspectrogram(y=data, sr=sr)\n    return np.squeeze(mel.T) if not flatten else np.ravel(mel.T)\n\ndef mfcc(data, sr, frame_length=2048, hop_length=512, flatten: bool = True):\n    mfcc_feature = librosa.feature.mfcc(y=data, sr=sr)\n    return np.squeeze(mfcc_feature.T) if not flatten else np.ravel(mfcc_feature.T)","18cb1187":"path = np.array(df[\"Path\"])[658]\ndata, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\nlen(data)","7ea72c93":"print(\"ZCR: \", zcr(data).shape)\nprint(\"Energy: \", energy(data).shape)\nprint(\"Entropy of Energy :\", entropy_of_energy(data).shape)\nprint(\"RMS :\", rmse(data).shape)\nprint(\"Spectral Centroid :\", spc(data, sampling_rate).shape)\n# print(\"Spectral Entropy: \", spc_entropy(data, sampling_rate).shape)\nprint(\"Spectral Flux: \", spc_flux(data).shape)\nprint(\"Spectral Rollof: \", spc_rollof(data, sampling_rate).shape)\nprint(\"Chroma STFT: \", chroma_stft(data, sampling_rate).shape)\nprint(\"MelSpectrogram: \", mel_spc(data, sampling_rate).shape)\nprint(\"MFCC: \", mfcc(data, sampling_rate).shape)\n","189b3999":"def extract_features(data, sr, frame_length=2048, hop_length=512):\n    result = np.array([])\n    result = np.hstack((result,\n                        zcr(data, frame_length, hop_length),\n                        # np.mean(energy(data, frame_length, hop_length),axis=0),\n                        # np.mean(entropy_of_energy(data, frame_length, hop_length), axis=0),\n                        rmse(data, frame_length, hop_length),\n                        # spc(data, sr, frame_length, hop_length),\n                        # spc_entropy(data, sr),\n                        # spc_flux(data),\n                        # spc_rollof(data, sr, frame_length, hop_length),\n                        # chroma_stft(data, sr, frame_length, hop_length),\n                        # mel_spc(data, sr, frame_length, hop_length, flatten=True)\n                        mfcc(data, sr, frame_length, hop_length)\n                                    ))\n    return result","c0a15233":"def get_features(path, duration=2.5, offset=0.6):\n    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n    data, sample_rate = librosa.load(path, duration=duration, offset=offset)\n\n     # without augmentation\n    res1 = extract_features(data, sample_rate)\n    result = np.array(res1)\n\n    # data with noise\n    noise_data = noise(data, random=True)\n    res2 = extract_features(noise_data, sample_rate)\n    result = np.vstack((result, res2)) # stacking vertically\n\n    # data with pitching\n    pitched_data = pitch(data, sample_rate, random=True)\n    res3 = extract_features(pitched_data, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    # data with pitching and white_noise\n    new_data = pitch(data, sample_rate, random=True)\n    data_noise_pitch = noise(new_data, random=True)\n    res3 = extract_features(data_noise_pitch, sample_rate)\n    result = np.vstack((result, res3)) # stacking vertically\n\n    return result\n","cd01f387":"X, Y = [], []\nprint(\"Feature processing...\")\nfor path, emotion, ind in zip(df.Path, df.Emotion, range(df.Path.shape[0])):\n    features = get_features(path)\n    if ind % 100 == 0:\n        print(f\"{ind} samples has been processed...\")\n    for ele in features:\n        X.append(ele)\n        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n        Y.append(emotion)\nprint(\"Done.\")","833163dc":"features_path = \".\/features.csv\"","2f0bff1a":"extracted_df = pd.DataFrame(X)\nextracted_df[\"labels\"] = Y\nextracted_df.to_csv(features_path, index=False)\nextracted_df.head()","4b9d15e9":"extracted_df = pd.read_csv(features_path)\nprint(extracted_df.shape)","c1d051b3":"# Fill NaN with 0\nextracted_df = extracted_df.fillna(0)\nprint(extracted_df.isna().any())\nextracted_df.shape\n","73077966":"extracted_df.head()","1ed3bbcd":"X = extracted_df.drop(labels=\"labels\", axis=1)\nY = extracted_df[\"labels\"]","1be0a55b":"lb = LabelEncoder()\nY = np_utils.to_categorical(lb.fit_transform(Y))\nprint(lb.classes_)\nY\n","544dc035":"X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42, test_size=0.2, shuffle=True)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","71eda963":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1, shuffle=True)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","b67d238a":"# Standardize data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_val = scaler.transform(X_val)\nX_train.shape, X_test.shape, X_val.shape, y_train.shape, y_test.shape, y_val.shape","5e908251":"# We have to use 1-dimensional CNN which need specifical shape:\nX_train = np.expand_dims(X_train, axis=2)\nX_val = np.expand_dims(X_val, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","b4e8960b":"earlystopping = EarlyStopping(monitor =\"val_acc\",\n                              mode = 'auto', patience = 5,\n                              restore_best_weights = True)\n","4f602f90":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                            patience=3,\n                                            verbose=1,\n                                            factor=0.5,\n                                            min_lr=0.00001)","ad32df5c":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","e099b019":"model = models.Sequential()\nmodel.add(layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\",\n                        input_shape=(X_train.shape[1], 1)))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(512, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(256, kernel_size=5, strides=1,\n                        padding=\"same\", activation=\"relu\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPool1D(pool_size=5, strides=2, padding=\"same\"))\n\nmodel.add(layers.Conv1D(256, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nmodel.add(layers.Conv1D(128, kernel_size=3, strides=1, padding='same', activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.MaxPooling1D(pool_size=3, strides = 2, padding = 'same'))\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.Dense(7, activation=\"softmax\"))\n\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"acc\", f1_m])\n\n","38aba360":"model.summary()","87c4585a":"EPOCHS = 50\nbatch_size = 64","65bb79d4":"history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n                    epochs=EPOCHS, batch_size=batch_size,\n                    callbacks=[earlystopping, learning_rate_reduction])","d232f310":"print(\"Accuracy of our model on test data : \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")\n\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['acc']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_acc']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(train_loss, label = 'Training Loss')\nax[0].plot(test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(train_acc, label = 'Training Accuracy')\nax[1].plot(test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()\n","3b026faa":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\ny_pred","8ea464a5":"y_check = np.argmax(y_test, axis=1)\ny_check","8dd84d13":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=y_check, y_pred=y_pred)","e3e4e447":"def plot_confusion_matrix(cm, classes,\n                        normalize=False,\n                        title='Confusion matrix',\n                        cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","08dd0f92":"cm_plot_labels = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\nplot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","7f0c67b0":"path_to_model = \".\/res_model.h5\"\n\nmodel.save(path_to_model)","edb3099f":"### TESS dataset","33fdab16":"### Ravdess dataset\n\nHere is the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. 02-01-06-01-02-01-12.mp4 This means the meta data for the audio file is:\n\n* Video-only (02)\n* Speech (01)\n* Fearful (06)\n* Normal intensity (01)\n* Statement \"dogs\" (02)\n* 1st Repetition (01)\n* 12th Actor (12) - Female (as the actor ID number is even)","7f384392":"3. Stretching\n","2272bd67":"## Feature extraction\n\n#### There are some features may be useful:","e037f68d":"### Savee dataset\n\nThe audio files in this dataset are named in such a way that the prefix letters describes the emotion classes as follows:\n\n* 'a' = 'anger'\n* 'd' = 'disgust'\n* 'f' = 'fear'\n* 'h' = 'happiness'\n* 'n' = 'neutral'\n* 'sa' = 'sadness'\n* 'su' = 'surprise'","ebc36acc":"# Speech emotion recognition using 1-dimensional ConvNN\n\nIn this experiment i tried to recognize emotion in short voice message (< 3s). I will use 4 datasets with some english phrases,\nwhich were voiced by professional actors: Ravee, Crema, Savee and Tess.\n\n#### First, lets define SER i.e. Speech Emotion Recognition.\n\n*Speech Emotion Recognition*, abbreviated as *SER*, is the act of attempting to recognize human emotion and affective states from speech.\nThis is capitalizing on the fact that voice often reflects underlying emotion through tone and pitch. This is also the phenomenon\nthat animals like dogs and horses employ to be able to understand human emotion\n\n**Datasets used in this project** contains ~7 types of main emotions: *Happy, Fear, Angry, Disgust, Surprised, Sad or Neutral.*\n\nAlso, my thanks to [this notebook](https:\/\/www.kaggle.com\/shivamburnwal\/speech-emotion-recognition) which helps me to start very much. \n\nSo, let's start!","6ebcd1a7":"For our data augmentation we will use noise and pitch and combination with both of it.\n","3c95f352":"### Let's define our model:","0dce434c":"5. Pitching","a047fb85":"### Crema dataset","29c6af90":"Let's save our features as DataFrame for further processing:","126565f3":"### Due to i don't split the dataset by gender, let's look at distribution by\n","e24c62b8":"2. Noised audio","a88f2359":"## Data augmentation\n\nWe have some ways for data augmentation in sound data:\n\n1. Noise injection\n2. Stretching\n3. Shifting\n4. Pitching","20e0b47d":"#### Let's check data formats:","5ee76c41":"4. Shifting\n","b843e691":"1. Zero Crossing Rate : The rate of sign-changes of the signal during the duration of a particular frame.\n2. Energy : The sum of squares of the signal values, normalized by the respective frame length.\n3. Entropy of Energy :The entropy of sub-frames\u2019 normalized energies. It can be interpreted as a measure of abrupt changes.\n3. Spectral Centroid : The center of gravity of the spectrum.\n4. Spectral Spread : The second central moment of the spectrum.\n5. Spectral Entropy : Entropy of the normalized spectral energies for a set of sub-frames.\n6. Spectral Flux : The squared difference between the normalized magnitudes of the spectra of the two successive frames.\n7. Spectral Rolloff : The frequency below which 90% of the magnitude distribution of the spectrum is concentrated.\n8. MFCCs Mel Frequency Cepstral Coefficients form a cepstral representation where the frequency bands are not linear but distributed according to the mel-scale.","ddd05af4":"1. Simple audio","669317de":"In experimental way was decided to use just 3 main features for this task: *ZCR*, *RMS* and *MFCC*.\n\nAlso in experimental way  was decided to use just 2.5s duration with 0.6 offset - in the dataset first 0.6s contains\nno information about emotion, and most of them are less then 3s.","8467d2f1":"## Data preparation\n\nAs of now we have extracted the data, now we need to normalize and split our data for training and testing.\n","5a0ea887":"## Importing libraries","584e2a84":"## Data preparation"}}