{"cell_type":{"a08f668f":"code","2c5148e4":"code","874e4ba1":"code","e28cd74b":"code","2180d91b":"code","a9c2ef37":"code","a28c7400":"code","9eaf419c":"code","8ac3acd4":"code","614854ec":"code","a9aac370":"code","e3c1088b":"code","d5596a58":"code","0763a1ce":"code","d14647e5":"code","2da4fa30":"code","fcf347e3":"code","36d5360a":"code","7cbdbb48":"code","af7f43c4":"code","c32da91c":"code","6243b372":"code","b6fd1dab":"code","77e401e4":"code","70df437b":"code","1c1ad547":"code","b3235cbb":"code","b3162b95":"code","d32ac40c":"code","aba1b089":"code","710f8ef2":"code","9f0218e9":"code","226504ba":"code","28148a1c":"code","b747e1c4":"code","10f81efc":"code","852cec29":"code","583c600c":"code","f6f4b325":"code","6d05e1a9":"code","401bfc49":"code","043637cf":"code","c3feb106":"code","976defc0":"code","ef4eb826":"code","81b29377":"code","96295499":"code","e796d88a":"markdown","172d6b1f":"markdown","a7cfe317":"markdown","52740028":"markdown","3c9dbd0c":"markdown","6585c53c":"markdown","c348ed80":"markdown","cacbdb12":"markdown","2096b054":"markdown","6b58383c":"markdown","c3289f7f":"markdown","19e75fcd":"markdown","e0158733":"markdown","c07894de":"markdown","eceeda32":"markdown","aa152cfc":"markdown","5620a566":"markdown","2c99abe0":"markdown","593f580f":"markdown","d406b288":"markdown","4831b3b2":"markdown","753a8f38":"markdown","ffb06363":"markdown","9236dd10":"markdown","823ce3d2":"markdown","e3249357":"markdown","a5cafbb4":"markdown","f75f6881":"markdown","30ce3a7a":"markdown","2266502f":"markdown","75581267":"markdown","dd098374":"markdown","f41f674b":"markdown","bea413bb":"markdown","b78a39bb":"markdown","c7756ab6":"markdown","4cd36cb5":"markdown"},"source":{"a08f668f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\n\ndef adj_r2_score(r2, n, k):\n    return 1-((1-r2)*((n-1)\/(n-k-1)))\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport keras.backend as K\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras.models import load_model","2c5148e4":"uber_raw_apr14 = pd.read_csv(\"..\/input\/uber-raw-data-apr14.csv\")\nuber_raw_may14 = pd.read_csv(\"..\/input\/uber-raw-data-may14.csv\")\nuber_raw_jun14 = pd.read_csv(\"..\/input\/uber-raw-data-jun14.csv\")\nuber_raw_jul14 = pd.read_csv(\"..\/input\/uber-raw-data-jul14.csv\")\nuber_raw_aug14 = pd.read_csv(\"..\/input\/uber-raw-data-aug14.csv\")\nuber_raw_sep14 = pd.read_csv(\"..\/input\/uber-raw-data-sep14.csv\")\n\n#Combining dataset of 6 months into 1 dataset\nuber_2014 = [uber_raw_apr14, uber_raw_may14, uber_raw_jun14, uber_raw_jul14,uber_raw_aug14, uber_raw_sep14]\nuber_data_2014 = pd.concat(uber_2014,axis=0,ignore_index=True)\nuber_data_2014.head()","874e4ba1":"uber_data_2014.info()","e28cd74b":"uber_data_2014.Timestamp = pd.to_datetime(uber_data_2014['Date\/Time'],format='%m\/%d\/%Y %H:%M:%S') \nuber_data_2014['Date_only'] = uber_data_2014.Timestamp.dt.date\nuber_data_2014['Date'] = uber_data_2014.Timestamp\nuber_data_2014['Month'] = uber_data_2014.Timestamp.dt.month\nuber_data_2014['DayOfWeekNum'] = uber_data_2014.Timestamp.dt.dayofweek\nuber_data_2014['DayOfWeek'] = uber_data_2014.Timestamp.dt.weekday_name\nuber_data_2014['MonthDayNum'] = uber_data_2014.Timestamp.dt.day\nuber_data_2014['HourOfDay'] = uber_data_2014.Timestamp.dt.hour\n\nuber_data_2014= uber_data_2014.drop(columns = ['Lat','Lon'])\nuber_data_2014.tail()","2180d91b":"uber_data_2014.groupby(pd.Grouper(key='DayOfWeek')).count()\n\nuber_weekdays = uber_data_2014.pivot_table(index=['DayOfWeekNum','DayOfWeek'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_weekdays.plot(kind='bar', figsize=(15,8))\nplt.ylabel('Total Journeys')\nplt.xlabel('Day')\nplt.title('Journeys by Week Day');","a9c2ef37":"uber_hour = uber_data_2014.pivot_table(index=['HourOfDay'],\n                                  values='Base',\n                                  aggfunc='count')\nuber_hour.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Hour');","a28c7400":"uber_data_2014.groupby(pd.Grouper(key='Base')).count()\n\nuber_monthdays = uber_data_2014.pivot_table(index=['Base'], values='Date' ,\n                                  aggfunc='count')\nuber_monthdays.plot(kind='bar', figsize=(8,6))\nplt.ylabel('Total Journeys')\nplt.title('Journeys by Month Day');","9eaf419c":"uber_data_2014= uber_data_2014.drop(columns = ['Month','DayOfWeekNum','Base', 'DayOfWeek', 'MonthDayNum', 'HourOfDay'])\n#uber_data_2014.tail()","8ac3acd4":"'''\nThe df uber_count is the grouping of the above dataset on hourly basis with time stamp of both date and time.\nThis df is used mostly for ANN analysis.\n'''\nuber_count=uber_data_2014.groupby(pd.Grouper(key='Date')).count()\nuber_count= uber_count.drop(columns = ['Date_only'])\nprint(uber_count.info())\n\ntrain = uber_count[:][:234083]             #90% of 260093\ntest = uber_count[:][234084:]\ndisplay(train.tail())\ntest.head()\n","614854ec":"train['Date\/Time'].plot(kind='line',figsize=(15,8), title= 'Hourly Ridership', fontsize=14)\ntest['Date\/Time'].plot(figsize=(15,5), title= 'Hourly Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","a9aac370":"'''\nThe df uber_dates is the grouping of the above dataset on daily basis with time stamp of onlu date.\nThis df is used to for univariate Time Series Forecasting.\n'''\nuber_dates=uber_data_2014.groupby(pd.Grouper(key='Date_only')).count()\nuber_dates= uber_dates.drop(columns = ['Date'])\nprint(uber_dates.info())\nuber_dates_d= uber_dates.drop(columns = ['Date\/Time'])\n\ntrain_ts = uber_dates[:][:163]                     #split is 90-10\ntest_ts = uber_dates[:][164:]\ntest_ts_d = uber_dates_d[:][164:]\ntest_ts.head()","e3c1088b":"train_ts['Date\/Time'].plot(kind='line',figsize=(15,8), title= 'Daily Ridership', fontsize=14)\ntest_ts['Date\/Time'].plot(figsize=(15,5), title= 'Daily Ridership', fontsize=14)\nplt.ylabel('Total Journeys')\nplt.xlabel('Month')\nplt.show()","d5596a58":"y_hat_avg = test_ts.copy()\nfit1 = ExponentialSmoothing(np.asarray(train_ts['Date\/Time']) ,seasonal_periods=7 ,trend='add', seasonal='add',).fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(test_ts))\nplt.figure(figsize=(15,5))\nplt.plot( train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_Winter'], label='Holt_Winter')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()","0763a1ce":"y_hat_avg = test_ts.copy()\nfit1 = sm.tsa.statespace.SARIMAX(train_ts['Date\/Time'], order=(2, 1, 4),seasonal_order=(1,1,1,7)).fit()\ny_hat_avg['SARIMA'] = fit1.predict(start=\"2014-09-11\", end=\"2014-09-30\", dynamic=True)\nplt.figure(figsize=(15,6))\nplt.plot( train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')\nplt.ylabel('Total Journeys')\nplt.xlabel('Months')\nplt.show()\n","d14647e5":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nimport statsmodels.api as sm\nsm.tsa.seasonal_decompose(train_ts['Date\/Time'].values,freq=30).plot()\nresult = sm.tsa.stattools.adfuller(uber_dates['Date\/Time'])\nplt.show()","2da4fa30":"y_hat_avg = test_ts.copy()\n\nfit1 = Holt(np.asarray(train_ts['Date\/Time'])).fit(smoothing_level = 0.3,smoothing_slope = 0.1)\ny_hat_avg['Holt_linear'] = fit1.forecast(len(test_ts))\n\nplt.figure(figsize=(16,5))\nplt.plot(train_ts['Date\/Time'], label='Train')\nplt.plot(test_ts['Date\/Time'], label='Test')\nplt.plot(y_hat_avg['Holt_linear'], label='Holt_linear')\nplt.legend(loc='best')\nplt.show()","fcf347e3":"from statsmodels.tsa.stattools import adfuller\ndef test_stationary(timeseries):\n    #Determine rolling statistics\n    #rolmean = pd.rolling_mean(timeseries,window = 24)\n    #rolstd = pd.rolling_std(timeseries, window = 24)\n    \n    rolmean = timeseries.rolling(24).mean()\n    rolstd = timeseries.rolling(24).std()\n    \n    \n    #Plot rolling Statistics\n    orig = plt.plot(timeseries, color = \"blue\", label = \"Original\")\n    mean = plt.plot(rolmean, color = \"red\", label = \"Rolling Mean\")\n    std = plt.plot(rolstd, color = \"black\", label = \"Rolling Std\")\n    plt.legend(loc = \"best\")\n    plt.title(\"Rolling Mean and Standard Deviation\")\n    plt.show(block = False)\n    \n    #Perform Dickey Fuller test\n    print(\"Results of Dickey Fuller test: \")\n    dftest = adfuller(timeseries, autolag = 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index = ['Test Statistics', 'p-value', '# Lag Used', 'Number of Observations Used'])\n    \n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","36d5360a":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize']=(20,10)\ntest_stationary(uber_count['Date\/Time'])","7cbdbb48":"Train_log = np.log(train_ts['Date\/Time'])\nvalid_log = np.log(test_ts['Date\/Time'])","af7f43c4":"moving_avg = Train_log.rolling(24).mean()\nplt.plot(Train_log)\nplt.plot(moving_avg, color = 'red')","c32da91c":"train_log_moving_diff = Train_log - moving_avg\ntrain_log_moving_diff.dropna(inplace = True)\ntest_stationary(train_log_moving_diff)","6243b372":"train_log_diff = Train_log - Train_log.shift(1)\ntest_stationary(train_log_diff.dropna())","b6fd1dab":"from statsmodels.tsa.seasonal import seasonal_decompose\nplt.figure(figsize = (16,10))\ndecomposition = seasonal_decompose(pd.DataFrame(Train_log)['Date\/Time'].values, freq = 24)\nplt.style.use('default')\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(Train_log, label = 'Original')\nplt.legend(loc = 'best')\nplt.subplot(412)\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\nplt.subplot(413)\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\nplt.subplot(414)\nplt.plot(residual, label = 'Residuals')\nplt.legend(loc = 'best')\nplt.tight_layout()","77e401e4":"plt.figure(figsize = (16,8))\ntrain_log_decompose = pd.DataFrame(residual)\ntrain_log_decompose['date'] = Train_log.index\ntrain_log_decompose.set_index('date', inplace = True)\ntrain_log_decompose.dropna(inplace = True)\ntest_stationary(train_log_decompose[0])","70df437b":"from statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(train_log_diff.dropna(), nlags = 25)\nlag_pacf = pacf(train_log_diff.dropna(), nlags = 25, method= \"ols\")","1c1ad547":"plt.figure(figsize = (15,8))\nplt.style.use(\"fivethirtyeight\")\nplt.plot(lag_acf)\nplt.axhline( y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline( y= -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline(y = 1.96 \/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Autocorrelation Function\")\nplt.show()\n# PACF\nplt.figure(figsize = (15,8))\nplt.plot(lag_pacf)\nplt.axhline(y = 0, linestyle = \"--\", color = \"gray\")\nplt.axhline(y = -1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.axhline( y = 1.96\/np.sqrt(len(train_log_diff.dropna())), linestyle = \"--\", color = \"gray\")\nplt.title(\"Partial Autocorrelation Function\")\nplt.show()","b3235cbb":"from statsmodels.tsa.arima_model import ARIMA\nplt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (2,1,0))  #here q value is zero since it is just AR Model\nresults_AR = model.fit(disp=-1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_AR.fittedvalues, color = 'red', label = 'Predictions')\nplt.legend(loc = 'best')","b3162b95":"AR_predict = results_AR.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nAR_predict = AR_predict.cumsum().shift().fillna(0)\nAR_predict1 = pd.Series(np.ones(test_ts.shape[0])* np.log(test_ts['Date\/Time'])[0], index = test_ts_d)\nAR_predict = np.exp(AR_predict1)","d32ac40c":"# Moving Average Model","aba1b089":"plt.figure(figsize = (15,8))\nmodel = ARIMA(Train_log, order = (0,1,2)) # here the p value is 0 since it is moving average model\nresults_MA = model.fit(disp = -1)\nplt.plot(train_log_diff.dropna(), label = \"Original\")\nplt.plot(results_MA.fittedvalues, color = \"red\", label = \"Prediction\")\nplt.legend(loc = \"best\")","710f8ef2":"MA_predict = results_MA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\nMA_predict=MA_predict.cumsum().shift().fillna(0)\nMA_predict1=pd.Series(np.ones(test_ts.shape[0]) * np.log(test_ts['Date\/Time'])[0], index = test_ts_d)\n#MA_predict1=MA_predict1.add(MA_predict,fill_value=0)\nMA_predict = np.exp(MA_predict1)","9f0218e9":"# Combined Model","226504ba":"plt.figure(figsize = (16,8))\nmodel = ARIMA(Train_log, order=(2, 1, 2))  \nresults_ARIMA = model.fit(disp=-1)  \nplt.plot(train_log_diff.dropna(),  label='Original')\nplt.plot(results_ARIMA.fittedvalues, color='red', label='Predicted')\nplt.legend(loc='best')\nplt.show()","28148a1c":"# Function to scale model to original scale","b747e1c4":"def check_prediction_diff(predict_diff, given_set):\n    predict_diff= predict_diff.cumsum().shift().fillna(0)\n    predict_base = pd.Series(np.ones(given_set.shape[0]) * np.log(given_set['Date\/Time'])[0], index = given_set.index)\n    #predict_log = predict_base.add(predict_diff,fill_value=0)\n    predict = np.exp(predict_base)\n    \n    plt.plot(given_set['Date\/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date\/Time']))\/given_set.shape[0]))\n    plt.show()\n\ndef check_prediction_log(predict_log, given_set):\n    predict = np.exp(predict_log)\n    \n    plt.plot(given_set['Date\/Time'], label = \"Given set\")\n    plt.plot(predict, color = 'red', label = \"Predict\")\n    plt.legend(loc= 'best')\n    plt.title('RMSE: %.4f'% (np.sqrt(np.dot(predict, given_set['Date\/Time']))\/given_set.shape[0]))\n    plt.show()\n\nARIMA_predict_diff=results_ARIMA.predict(start=\"2014-09-11\", end=\"2014-09-30\")\n\nplt.figure(figsize = (16,8))\ncheck_prediction_diff(ARIMA_predict_diff, test_ts)","10f81efc":"ARIMA_predict_diff.shape \n\n\ntest_ts.shape","852cec29":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date\/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')","583c600c":"abc=y_hat['SES'].values.tolist()\nrmse = sqrt(mean_squared_error(test_ts['Date\/Time'],abc))\nrmse","f6f4b325":"y_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date\/Time'].rolling(10).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = 'Moving Average Forecast with 10 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast'] = train_ts['Date\/Time'].rolling(20).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'],label = 'Moving Average Forecast with 20 Observations')\nplt.legend(loc = 'best')\nplt.show()\ny_hat_avg = test_ts.copy()\ny_hat_avg['moving_average_forecast']= train_ts['Date\/Time'].rolling(50).mean().iloc[-1]\nplt.figure(figsize = (15,5))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat_avg['moving_average_forecast'], label = \"Moving Average Forecast with 50 Observations\")\nplt.legend(loc = 'best')\nplt.show()","6d05e1a9":"rmse = sqrt(mean_squared_error(test_ts['Date\/Time'], y_hat_avg['moving_average_forecast']))\nrmse","401bfc49":"y_hat = test_ts.copy()\nfit2 = SimpleExpSmoothing(np.asarray(train_ts['Date\/Time'])).fit(smoothing_level = 0.6,optimized = False)\ny_hat['SES'] = fit2.forecast(len(test_ts))\nplt.figure(figsize =(15,8))\nplt.plot(train_ts['Date\/Time'], label = 'Train')\nplt.plot(test_ts['Date\/Time'], label = 'Validation')\nplt.plot(y_hat['SES'], label = 'Simple Exponential Smoothing')\nplt.legend(loc = 'best')\n","043637cf":"sc = MinMaxScaler()\ntrain_sc = sc.fit_transform(train)\ntest_sc = sc.transform(test)\n\nX_train = train_sc[:-1]\ny_train = train_sc[1:]\n\nX_test = test_sc[:-1]\ny_test = test_sc[1:]","c3feb106":"K.clear_session()\n\nmodel = Sequential()\nmodel.add(Dense(9, input_dim=1, activation='relu'))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nearly_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\nhistory = model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], shuffle=False)","976defc0":"y_pred_test_ann = model.predict(X_test)\ny_train_pred_ann = model.predict(X_train)\nrmse = sqrt(mean_squared_error(y_train,y_train_pred_ann))\nprint(\"Train : {:0.3f}\".format(rmse))\n\nrmse = sqrt(mean_squared_error(y_test,y_pred_test_ann))\nprint(\"Test : {:0.3f}\".format(rmse))\n\nmodel.save('Uber_ANN')","ef4eb826":"model_ann = load_model('Uber_ANN')","81b29377":"y_pred_test_ANN = model_ann.predict(X_test)\nplt.plot(y_test, label='True')\nplt.plot(y_pred_test_ANN, label='ANN')\nplt.title(\"ANN's_Prediction\")\nplt.xlabel('Observation')\nplt.ylabel('INR_Scaled')\nplt.legend()\nplt.show()","96295499":"score_ann= model_ann.evaluate(X_test, y_test, batch_size=1)\nprint('ANN: %f'%score_ann)","e796d88a":"This model also gives best accuracy so far and we can see from the above plot that the predicted Holt winter graph is almost overlapping with the actual test dataset available to us","172d6b1f":"As we have a very large dataset of 4.5 million+ values. I have used 90-10 split","a7cfe317":"#### Dividing the above Date\/Time columns into several columns for visualising and analysing the dataset","52740028":"#### Peak Days","3c9dbd0c":"#### Removing Seasonailty","6585c53c":"#### Base","c348ed80":"### Simple Exponential Smoothing","cacbdb12":"#### Simple Exponential Smoothing is not the right model for our dataset","2096b054":"# Splitting the dataset","6b58383c":"### AR model","c3289f7f":"#### Error Score for this model is very low, hence ANN model gives good results","19e75fcd":"# Import and transform Dataset","e0158733":"Here we can see that the larest number of uber pickups were done on Thurdays and Fridays","c07894de":"#### Differncing can help to make series stable and eliminate trend","eceeda32":"#### The huge RMSE value in AR model shows that this model is not suitable for our dataset","aa152cfc":"rmse = sqrt(mean_squared_error(train_ts['Date\/Time'], y_hat['SES']))\nrmse","5620a566":"This model also predicts with comparable accuracy as the above holt winter season method as we can see here as well that the predicted sarima graph is almost overlapping with the actual test dataset available to us","2c99abe0":"Here we can see that Holt Linear Trend is hardly a good model for our dataset.","593f580f":"# Time Series Forcasting","d406b288":"#### This model gives huge RMSE value too showing this model isn't good for our dataset","4831b3b2":"### Holt\u2019s Winter seasonal method","753a8f38":"#### Analysing peek hours","ffb06363":"## Result\n\n#### From the abve analysis we can see that Holt's Winter Season and SABRIMA and ANN gave the best reults with small RMSE values compared to other models used.","9236dd10":"# ANN","823ce3d2":"From here we can see that peak hours of booking a cab are in evening from 4pm to 6pm.\nWe can also see that more cabs are booked in evenings compared to mornings","e3249357":"# Visualization","a5cafbb4":"Here we can see that Base B02617 provided most cabs. Closely followed by B02598","f75f6881":"#### Here we can see the Root Mean Square Error values are less. \nThis is the least when compared with other standard splits of 70-30, 80-20 and 95- 05","30ce3a7a":"plt.figure(figsize = (15,8))\nplt.plot(test_ts['Date\/Time'], label = \"Valid\")\nplt.plot(MA_predict, color = 'red', label = \"Predict\")\nplt.legend(loc= 'best')\nplt.title('RMSE: %.4f'% (np.sqrt(np.dot(MA_predict, test_ts['Date\/Time']))\/test_ts.shape[0]))\nplt.show()","2266502f":"### Holt\u2019s Linear Trend method","75581267":"# Import Libraries","dd098374":"#### Remove Trend","f41f674b":"### SARIMA Model","bea413bb":"### RMSE Error for Simple Exponential Smoothing","b78a39bb":"#### Checking stationarity of residuals","c7756ab6":"Here we can see the dataset has 4 columns:\n\n    Date\/Time : The date and time of the Uber pickup.\n    Lat : The latitude of the Uber pickup\n    Lon : The longitude of the Uber pickup\n    Base : The TLC base company code affiliated with the Uber pickup","4cd36cb5":"### ARIMA"}}