{"cell_type":{"7379de96":"code","66bc329b":"code","ed00aca1":"code","316637be":"code","f1e4456b":"code","f67ef985":"code","7c877726":"code","5b3604d6":"code","f8aa8385":"code","f22e4e98":"markdown","f442e424":"markdown","c77ae9a3":"markdown","d56d6a11":"markdown","18e648d3":"markdown","610e3a18":"markdown","cbb9b8d7":"markdown","bccf4fb3":"markdown"},"source":{"7379de96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint('directory contents:', ', '.join(os.listdir('\/kaggle\/input\/deepfake-detection-challenge')))\n\nprint(\n    'num train videos:', len(os.listdir('\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/')) - 1,\n    '\\nnum test videos: ',  len(os.listdir('\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/'))\n)","66bc329b":"import cv2 as cv\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm","ed00aca1":"train_dir = '\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/'\ntrain_video_files = [train_dir + x for x in os.listdir(train_dir) if x.endswith('.mp4')]\ntest_dir = '\/kaggle\/input\/deepfake-detection-challenge\/test_videos\/'\ntest_video_files = [test_dir + x for x in os.listdir(test_dir)]","316637be":"train_metadata = pd.read_json('\/kaggle\/input\/deepfake-detection-challenge\/train_sample_videos\/metadata.json')\ntrain_metadata = train_metadata.T\ntrain_metadata.head()","f1e4456b":"train_metadata['label'].value_counts(normalize=True)","f67ef985":"def show_first_frame(video_files, num_to_show=25):\n    root = int(num_to_show**.5)\n    fig, axes = plt.subplots(root,root, figsize=(root*5,root*5))\n    for i, video_file in tqdm(enumerate(video_files[:num_to_show]), total=num_to_show):\n        cap = cv.VideoCapture(video_file)\n        success, image = cap.read()\n        image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n        cap.release()   \n        \n        axes[i\/\/root, i%root].imshow(image)\n        fname = video_file.split('\/')[-1]        \n        try:\n            label = train_metadata.loc[fname, 'label']\n            axes[i\/\/root, i%root].title.set_text(f\"{fname}: {label}\")\n        except:\n            axes[i\/\/root, i%root].title.set_text(f\"{fname}\")","7c877726":"show_first_frame(train_video_files, num_to_show=25)","5b3604d6":"show_first_frame(test_video_files, num_to_show=25)","f8aa8385":"fig, ax = plt.subplots(1,1, figsize=(12,12))\ncap = cv.VideoCapture(test_dir + 'ahjnxtiamx.mp4')\ncap.set(1,2)\nsuccess, image = cap.read()\nimage = cv.cvtColor(image, cv.COLOR_BGR2RGB)\ncap.release()   \n\nax.imshow(image)\nfname = 'ahjnxtiamx.mp4'\nax.title.set_text(f\"{fname}\")","f22e4e98":"This is just one example, but a few questions it makes me think of are: <br><br>\n1) Were many different GANs architectures used to generate the dataset? (probably yes since many different teams collaberated to create the full dataset).   <br><br>\n2) Does it change our modelling approach if there's some very obvious fakes (created from older methods) in the dataset, mixed in with the very realistic fakes (created from newer methods)? ","f442e424":"## This test video frame looks very fake","c77ae9a3":"## Wheres the full training data?\n\n*Copied from the getting-started page . https:\/\/www.kaggle.com\/c\/deepfake-detection-challenge\/overview\/getting-started*\n\n<div><div class=\"markdown-converter__text--rendered\"><h3><strong>Datasets<\/strong>:<\/h3>\n<p>There are 4 groups of datasets associated with this competition.<\/p>\n<ol>\n<li><strong>Training Set: This dataset, containing labels for the target, is available for download outside of Kaggle for competitors to build their models.<\/strong> It is broken up into 50 files, for ease of access and download. Due to its large size, it must be accessed through a GCS bucket which is only made available to participants after accepting the competition\u2019s rules. Please read the rules fully before accessing the dataset, as they contain important details about the dataset\u2019s permitted use. It is expected and encouraged that you train your models outside of Kaggle\u2019s notebooks environment and submit to Kaggle by uploading the trained model as an external data source.<\/li>\n<li><strong>Public Validation Set<\/strong>: When you commit your Kaggle notebook, the submission file output that is generated will be based on the small set of 400 videos\/ids contained within this Public Validation Set. This is available on the Kaggle Data page as <code>test_videos.zip<\/code><\/li>\n<li><strong>Public Test Set: This dataset is completely withheld and is what Kaggle\u2019s platform computes the public leaderboard against.<\/strong> When you \u201cSubmit to Competition\u201d from the \u201cOutput\u201d file of a committed notebook that contains the competition\u2019s dataset, your code will be re-run in the background against this Public Test Set. When the re-run is complete, the score will be posted to the public leaderboard. If the re-run fails, you will see an error reflected in your \u201cMy Submissions\u201d page. Unfortunately, we are unable to surface any details about your error, so as to prevent error-probing. You are limited to 2 submissions per day, including submissions which error.<\/li>\n<li><strong>Private Test Set: This dataset is privately held outside of Kaggle\u2019s platform, and is used to compute the private leaderboard.<\/strong> It contains videos with a similar format and nature as the Training and Public Validation\/Test Sets, but are real, organic videos with and without deepfakes. After the competition deadline, Kaggle transfers your 2 final selected submissions\u2019 code to the host. They will re-run your code against this private dataset and return prediction submissions back to Kaggle for computing your final private leaderboard scores.<\/li>\n<\/ol>","d56d6a11":"**Update:**<br>\nMy mistake, I thought we didn't have access to the train_sample labels, but we do and they are there hiding as a json file in the train_sample videos folder.  ","18e648d3":"Are about 80% of the labels FAKE for train and test?  Would be interested to know for the full training dataset.","610e3a18":"There appears to be multiple videos per person in the train_sample videos and test videos.","cbb9b8d7":"## test videos","bccf4fb3":"## train videos"}}