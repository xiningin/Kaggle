{"cell_type":{"ab879b83":"code","00ee2977":"code","c535e18e":"code","9f9f8f82":"code","6515e08e":"code","d8dad835":"code","7b56be3b":"code","3c337124":"code","de3c6e44":"code","30d63d5d":"code","2e537533":"code","91048baa":"code","e9b94643":"code","4ee94d09":"code","edc05c87":"code","fdd33269":"code","c16ff612":"code","e4db097b":"code","b7da3cb2":"code","f4d9cdd3":"code","92ea5afd":"code","a14f8d54":"code","310f2e47":"code","9c36139c":"code","873c4890":"code","176ccdf8":"code","ec6a3a29":"code","00cc0ad3":"code","211bc016":"code","237fe7c5":"code","98d548a0":"code","97be8e2b":"code","cff90fab":"code","2d9b4feb":"code","fece6904":"code","bc9c1d61":"code","40bbe068":"code","21cf0ed6":"code","a1ed5038":"code","ece83968":"code","2f7b6b39":"code","3478138a":"code","674b391b":"code","a3317fc5":"code","40cf1297":"code","504466d6":"code","edeeab89":"code","e71e7027":"code","b4925e0d":"code","f6c197a5":"code","b0be872e":"code","6e9f6aaa":"code","47b472af":"code","b5acfd24":"code","7e8397cb":"code","68a197df":"code","6491756c":"code","e3191f5a":"code","c139fd92":"code","aebc4cc1":"code","2db53659":"code","9c7b6b98":"code","c27f689e":"code","712b8284":"code","2fe8532b":"code","dd3a37c7":"code","a892a850":"code","8a9a67cc":"code","9a310307":"code","97c294f5":"code","f9597df2":"code","786379bd":"markdown","a84dadeb":"markdown","aacbb6e2":"markdown","040066f0":"markdown","3f67dc96":"markdown","d41424b8":"markdown","d5c304c9":"markdown","6927c8b4":"markdown","9d0a57b4":"markdown","d213365a":"markdown","13bfbf2d":"markdown","ad8b1f44":"markdown","60078cd2":"markdown","2be14769":"markdown","e3060610":"markdown","ecc0b664":"markdown","4a7baf20":"markdown","e2423978":"markdown","e37045fc":"markdown","ed4901c3":"markdown","30b1533c":"markdown","604c08b1":"markdown","09d002b2":"markdown","cda412d2":"markdown"},"source":{"ab879b83":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")\nfrom sklearn.preprocessing import  StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV,train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error","00ee2977":"trainD=pd.read_csv('..\/input\/house-price-prediction-challenge\/train.csv')\ntestD =pd.read_csv('..\/input\/house-price-prediction-challenge\/test.csv')","c535e18e":"#getting to know the kind of data we have\ntrainD.shape,testD.shape","9f9f8f82":"trainD.head()","6515e08e":"testD.head()","d8dad835":"#displaying some more detials about the data\ntrainD.info()","7b56be3b":"#brief statistics  about numerical variables\ntrainD.describe()","3c337124":"#brief statistics  about categorical variables\ntrainD.describe(include=['O'])","de3c6e44":"#lets examine the count of  values in the posted_by categorical variable.\nposted_counts= pd.value_counts(trainD.POSTED_BY)\nposted_counts","30d63d5d":"# A categorical plot of POSTED_BY variable\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='POSTED_BY',kind='count',data=trainD)\nplt.xlabel('POSTED_BY')\nplt.ylabel('Count of Posted_By Records')\nplt.title('A count distribution of Posted_by category ')\nplt.show()","2e537533":"UNDER_CONSTRUCTION_counts= pd.value_counts(trainD.UNDER_CONSTRUCTION)\nUNDER_CONSTRUCTION_counts","91048baa":"# A plot of UNDER_CONSTRUCTION categories\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='UNDER_CONSTRUCTION',kind='count',data=trainD)\nplt.xlabel('UNDER_CONSTRUCTION')\nplt.ylabel('Count of UNDER_CONSTRUCTION ')\nplt.title('A count distribution of UNDER_CONSTRUCTION categories')\nplt.show()","e9b94643":"RERA_counts= pd.value_counts(trainD.RERA)\nRERA_counts","4ee94d09":"# A plot of RERA categories\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='RERA',kind='count',data=trainD)\nplt.xlabel('RERA')\nplt.ylabel('Count of RERA records')\nplt.title('A count distribution of UNDER_CONSTRUCTION categories')\nplt.show()","edc05c87":"BHK_NO_counts= pd.value_counts(trainD['BHK_NO.'])\nBHK_NO_counts","fdd33269":"# A plot of BHK_NO categories\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='BHK_NO.',kind='count',data=trainD)\nplt.xlabel('number of Rooms.')\nplt.ylabel('Count of houses')\nplt.title('A count distribution of number of Room categories')\nplt.show()","c16ff612":"BHK_OR_RKcounts= pd.value_counts(trainD['BHK_OR_RK'])\nBHK_OR_RKcounts","e4db097b":"# A plot of property type\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='BHK_OR_RK',kind='count',data=trainD)\nplt.xlabel('Type of Property.')\nplt.ylabel('Count of Property')\nplt.title('A count distribution of Type of Property')\nplt.show()","b7da3cb2":"#visualise the square feet of the data using seaborn distribution plot.\nax=sns.distplot(trainD.SQUARE_FT,kde=True)\nplt.title('A Square feet distribution ')\nplt.show()","f4d9cdd3":"# decreasing the skewness in square feet feature using log transformation in both train and test data\ntrainD['SQUARE_FT'] = np.log(trainD['SQUARE_FT'])\ntestD['SQUARE_FT'] = np.log(testD['SQUARE_FT'])","92ea5afd":"#Aplot close to normal distribution as a result of decrease in skewness\nax=sns.distplot(trainD.SQUARE_FT,kde=True)\nplt.title('A Square feet distribution of Total area of the house ')\nplt.show()","a14f8d54":"READY_TO_MOVE_counts= pd.value_counts(trainD['READY_TO_MOVE'])\nREADY_TO_MOVE_counts","310f2e47":"# A plot of Category marking Ready to move or Not\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='READY_TO_MOVE',kind='count',data=trainD)\nplt.xlabel('Category marking Ready to move or Not.')\nplt.ylabel('Count of Category marking')\nplt.title('A count distribution of Category marking Ready to move or Not')\nplt.show()","9c36139c":"Resale_counts= pd.value_counts(trainD['RESALE'])\nResale_counts","873c4890":"# A plot of Resale records or not\nplt.figure(figsize=(10,8))\nax=sns.catplot(x='RESALE',kind='count',data=trainD)\nplt.xlabel('Category marking Resale or not')\nplt.ylabel('Count of marking Resale or not ')\nplt.title('A distribution of Resale or not ')\nplt.show()","176ccdf8":"#visualise the square feet of the data using seaborn distribution plot.\nax=sns.distplot(trainD.LONGITUDE,kde=True)\nplt.title('A Longitude distribution ')\nplt.show()","ec6a3a29":"#visualise the square feet of the data using seaborn distribution plot.\nax=sns.distplot(trainD.LATITUDE,kde=True)\nplt.title('A Latitude distribution ')\nplt.show()","00cc0ad3":"#visualise the square feet of the data using seaborn distribution plot.\nax=sns.distplot(trainD['TARGET(PRICE_IN_LACS)'],kde=True)\nplt.title('A Price distribution')\nplt.show()","211bc016":"trainD['TARGET(PRICE_IN_LACS)'].describe()","237fe7c5":"#Lets examine the house with the highest price\ntrainD[trainD['TARGET(PRICE_IN_LACS)']==30000]","98d548a0":"#Lets examine the house with the lowest price\ntrainD[trainD['TARGET(PRICE_IN_LACS)']==0.25]","97be8e2b":"plt.figure(figsize=(10,6))\nsns.heatmap(trainD.corr(),vmax=0.8, annot=True)","cff90fab":"BHK_NO_counts= pd.value_counts(trainD['BHK_NO.'])\nBHK_NO_counts = list(BHK_NO_counts[BHK_NO_counts.values > 1000].index)","2d9b4feb":"BHK_NO_counts","fece6904":"# Plot of distribution of sprices for rooms\nplt.figure(figsize=(8,6))\n\n# Plot each room distribution of prices\nfor BHK in BHK_NO_counts:\n    # Select the room category\n    subset = trainD[trainD['BHK_NO.'] == BHK]\n    \n    # Density plot of prices\n    sns.kdeplot(subset['TARGET(PRICE_IN_LACS)'],label = BHK,shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('House prices', size = 10); plt.ylabel('Density', size = 10); \nplt.title('Density Plot of house prices by rooms', size = 8);","bc9c1d61":"#lets investigate the relationship between posted_by and prices.\nposted_counts= pd.value_counts(trainD.POSTED_BY)\nposts=posted_counts.index","40bbe068":"# Plot of distribution of Prices for Posted_By\nplt.figure(figsize=(8,6))\n\n# Plot each Posted_by distribution of prices\nfor post in posts:\n    # Select the posted_by type\n    subset = trainD[trainD['POSTED_BY'] == post]\n    \n    # Density plot of prices\n    sns.kdeplot(subset['TARGET(PRICE_IN_LACS)'],label = post,shade = False, alpha = 0.8);\n    \n# label the plot\nplt.xlabel('House prices', size = 10); plt.ylabel('Density', size = 10); \nplt.title('Density Plot of house prices by Posted ', size = 10);","21cf0ed6":"base_features = ['POSTED_BY','UNDER_CONSTRUCTION','RERA','BHK_NO.','BHK_OR_RK','SQUARE_FT','LONGITUDE','LATITUDE']","a1ed5038":"train_data = trainD[base_features]\ntest_data=   testD[base_features]","ece83968":"train_data.shape,test_data.shape","2f7b6b39":"y = trainD['TARGET(PRICE_IN_LACS)']","3478138a":"cat_cols = [cname for cname in train_data.columns \n                    if  train_data[cname].dtype == \"object\"]","674b391b":"Train_cat_colsOH= pd.get_dummies(train_data[cat_cols])\nTest_cat_colsOH= pd.get_dummies(test_data[cat_cols])","a3317fc5":"#Select numerical columns\nnum_cols = [cname for cname in train_data.columns \n            if train_data[cname].dtype in ['int64', 'float64']]\n\nscaler = StandardScaler()\ntrain_data[num_cols] = scaler.fit_transform(train_data[num_cols] )","40cf1297":"test_data[num_cols] = scaler.transform(test_data[num_cols] )","504466d6":"train_num_data = pd.DataFrame(train_data[num_cols])\ntest_num_data = pd.DataFrame(test_data[num_cols])","edeeab89":"train_data =pd.concat([Train_cat_colsOH, train_num_data],axis=1) \ntest_data =pd.concat([Test_cat_colsOH, test_num_data],axis=1) ","e71e7027":"train_data.head()","b4925e0d":"test_data.head()","f6c197a5":"# split validation set from training data\nX_train, X_val, y_train, y_val =train_test_split(train_data,y,test_size=0.2,random_state=0)","b0be872e":"# function to  train a given  model and evaluate it on the validation set\ndef fit_and_evaluate(model):\n    \n    # Train the model\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_val)\n    mea = mean_absolute_error(y_val,y_pred)\n    R2_score =r2_score(y_val,y_pred)\n    rmse = np.sqrt((mean_squared_error(y_val, y_pred)))\n    print(\"The root mean squared error generated...is {:.2f}\".format(rmse))\n    print(\"The R2_score value .....................is {:.4f}\".format(R2_score))\n    print(\"The mean absolute  error generated is .......is {:.2f}\".format(mea))","6e9f6aaa":"Ridge_model = Ridge()\n\nfit_and_evaluate(Ridge_model) ","47b472af":"linear_model = LinearRegression()\n\nfit_and_evaluate(linear_model)","b5acfd24":"random_forest = RandomForestRegressor(random_state=0)\n\nfit_and_evaluate(random_forest)","7e8397cb":"gradient_boosted = GradientBoostingRegressor(random_state=4)\n\nfit_and_evaluate(gradient_boosted) ","68a197df":"# Number of trees used in the boosting process\nn_estimators = [100, 500, 900, 1100, 1500]\n\n#loss function to be minimized\nloss = ['ls', 'lad', 'huber']\n\n# Maximum depth of each tree\nmax_depth = [2, 3, 5, 10, 15]\n#how much the contribution of each tree will shrink.\n\nlearning_rate = [0.005,0.01,0.05,0.1,0.5]\n\n# Minimum number of samples to split a node\nmin_samples_split = [2, 4, 6, 10]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]","6491756c":"# Define the grid of hyperparameters to search\nhyperparameter_grid = {'loss': loss,\n                       'learning_rate':learning_rate,\n                       'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}","e3191f5a":"model = GradientBoostingRegressor(random_state=4)","c139fd92":"random_cv = RandomizedSearchCV(estimator=model,\n                               param_distributions=hyperparameter_grid,\n                               scoring='neg_mean_absolute_error',\n                               cv=5, n_iter=30, \n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=42)","aebc4cc1":"random_cv.fit(X_train,y_train)","2db53659":"# Get all of the cv results and sort by the test performance\nrandom_results = pd.DataFrame(random_cv.cv_results_).sort_values('mean_test_score', ascending = False)\nrandom_results.head(3)","9c7b6b98":"random_cv.best_estimator_","c27f689e":"# Create a range of trees to evaluate\nparam_grid = {'n_estimators': [200,300,400,500, 800,1000 ]}\nmodel =  GradientBoostingRegressor( max_depth =15,\n                                   loss = 'ls',\n                                   alpha=0.9,\n                                   learning_rate=0.1,\n                                  min_samples_leaf = 1,\n                                  min_samples_split = 6,\n                                  max_features ='auto',\n                                  max_leaf_nodes=None,\n                                  random_state = 4)\n","712b8284":"# Grid Search Object using the trees range and the random forest model\ngrid_search = GridSearchCV(estimator = model, param_grid=param_grid, cv = 5, \n                           scoring = 'neg_mean_absolute_error', verbose = 1,\n                           n_jobs = -1, return_train_score = True)","2fe8532b":"grid_search.fit(X_train,y_train)","dd3a37c7":"# Get the results into a dataframe\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Plot the training and testing error vs number of trees\nplt.figure(figsize=(8, 8))\nplt.style.use('fivethirtyeight')\nplt.plot(results['param_n_estimators'], -1 * results['mean_test_score'], label = 'Test_Err')\nplt.plot(results['param_n_estimators'], -1 * results['mean_train_score'], label = 'Train_Err')\nplt.xlabel('Number of Trees'); plt.ylabel('Mean Abosolute Error'); plt.legend(\"best\");\nplt.title('Performance vs Number of Trees');","a892a850":"results.sort_values('mean_test_score', ascending = False).head(3)","8a9a67cc":"# Select the best model\nfinal_modelGBR = grid_search.best_estimator_\nfinal_modelGBR","9a310307":"#final model performance\n\nfit_and_evaluate(final_modelGBR)","97c294f5":"SalesPrediction = final_modelGBR.predict(test_data)","f9597df2":"submission = pd.DataFrame({'Id': test_data.index, 'SalePrice': SalesPrediction})\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10).set_index('Id')","786379bd":"The room price distribution is largely skewed to the right with room category 4 having a high density distribution","a84dadeb":"All comments are welcome! there is always room for improvement.","aacbb6e2":"The above code shows us that most  houses are majorly built with 2 rooms or 3 then followed by 1,4,5. the other number of rooms are highly skewed with few records","040066f0":"The above code shows that we have  houses with very high prices of about 30000 lacs. These could be probably fancy ones. The median price is about 62 implying that most houses are relatively cheaper.","3f67dc96":"****Lets do an exploratory data analysis on the data to train our models.****","d41424b8":"**After fitting my final model, my model performance price prediction greatly improved to approximately within 30 points from the  true market price with a root mean squared error of 129 and an R2_score accuracy value of 96%**","d5c304c9":"The distribution of posted_by variable is largely skewed to the right with dealers having generaly high prices, with the builder category having more prices centered around median thus having a high density distribution","6927c8b4":"We realise that as we increase the number of trees, the model starts overfitting \nhowever, we shall consider the optimum value of the model before it starts to overfit with 200 estimators","9d0a57b4":"The code above shows that the highest house price was posted by category dealer, it's available for resale,it has got 3 rooms and  it goes for upto 30000 lacs .","d213365a":"The above code shows us that most house records have been posted by dealers, followed by owners and then builders post the least.","13bfbf2d":"We can see from the above heat map that the target is more correlated to the square feet feature than any other numerical columns.\nwe will probably use all these features in our modeling process","ad8b1f44":"* This Dataset has the following variables:\n\nColumn and it's                                       Description\n* POSTED_BY  -                                 Category marking who has listed the property\n* UNDER_CONSTRUCTION -                        Under Construction or Not\n* RERA -                                       Rera approved or Not\n* BHK_NO -                                     Number of Rooms\n* BHKORRK -                                    Type of property\n* SQUARE_FT -                                  Total area of the house in square feet\n* READYTOMOVE -                                Category marking Ready to move or Not\n* RESALE -                                     Category marking Resale or not\n* ADDRESS -                                    Address of the property\n* LONGITUDE -                                  Longitude of the property\n* LATITUDE -                                   Latitude of the property\n* TARGET -                                     Price in lacs","60078cd2":"The above code shows that the major property type is BHK with close to 30k records constituting almost all the records posted","2be14769":"From the above display, we can note that there are no rows with missing values among other information\nthat we can extract such as the various data types of the columns","e3060610":"The code above and visualisation shows that we have about 20k recordes that are not approved by RERA and about 10k records  approved by RERA","ecc0b664":"The code above shows us that we have 3 categorical variables, very many records with unique addresses\namong other information that is extractable","4a7baf20":"**Lets investigate if there is a relationship between number of rooms  and the prices**","e2423978":"The above code and visualisation shows that we have about 24k records of houses not under construction  and about 5k records of houses under construction.","e37045fc":"**The metric am caring more about is the model with the a least considerable root mean sqaured value,therefore am going to take the gradient boosting model for this prediction since it has more paramters to tune than the random forest. However, for me to furture improve on my accuracy, am going to user another metric of mean absolute error generated using the random search as the baseline then fine tune with gridsearch basing on the variation in the number of estimators.**","ed4901c3":"The above code and visual shows that over 90% of houses are available for resale  and just a few not available for  resale.","30b1533c":"The code above shows that the lowest house price was posted by category owner,it's not under construction, it has got 3 rooms , it's available for resale and it's ready to be occupied","604c08b1":"**For my analysis here, i won't include the address variable.It needs further manipulation before it can become more informative  and be used in our modelling since it's a text variable yet our models work best with numbers.**","09d002b2":"This is a regression problem because we have a continuous target variable and the feautures with which we shall use to train our model.\nBelow are the 12 variables which are contained in the datasets.\n","cda412d2":"# India House Price Prediction Challenge"}}