{"cell_type":{"48470ae4":"code","50940376":"code","7e3e107a":"code","0cff74c1":"code","c01deca1":"code","93afe0d8":"code","173ae7ca":"code","eef804f4":"code","afbe400f":"code","c76348c7":"code","682ea6ea":"code","12a75980":"code","0ef120e5":"code","82bfbb9c":"code","04860def":"markdown","cc8e0dc6":"markdown","cc54c1fe":"markdown","69887f73":"markdown","39fb680b":"markdown","27c3d31e":"markdown"},"source":{"48470ae4":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc, random, time\n# import cudf\nimport pandas as pd\nimport numpy as np\n# import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F","50940376":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything(seed=42)","7e3e107a":"print('Loading...')\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nfeatures = [c for c in train.columns if 'feature' in c]\n\nprint('Filling...')\nf_mean = train[features[1:]].mean()\ntrain = train.loc[train.weight > 0].reset_index(drop = True)\ntrain[features[1:]] = train[features[1:]].fillna(f_mean)\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nprint('Converting...')\n# train = train.to_pandas()\nf_mean = f_mean.values#.get()\nnp.save('f_mean.npy', f_mean)\n\nprint('Finish.')","0cff74c1":"class Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.batch_norm0 = nn.BatchNorm1d(len(features))\n        self.dropout0 = nn.Dropout(0.10143786981358652)\n\n        hidden_size = 256\n        self.dense1 = nn.Linear(len(features), 384)\n        self.batch_norm1 = nn.BatchNorm1d(384)\n        self.dropout1 = nn.Dropout(0.19720339053599725)\n\n        self.dense2 = nn.Linear(384, 896)\n        self.batch_norm2 = nn.BatchNorm1d(896)\n        self.dropout2 = nn.Dropout(0.2703017847244654)\n\n        self.dense3 = nn.Linear(896, 896)\n        self.batch_norm3 = nn.BatchNorm1d(896)\n        self.dropout3 = nn.Dropout(0.23148340929571917)\n\n        self.dense4 = nn.Linear(896, 394)\n        self.batch_norm4 = nn.BatchNorm1d(394)\n        self.dropout4 = nn.Dropout(0.2357768967777311)\n\n        self.dense5 = nn.Linear(394, 1)\n\n        self.Relu = nn.ReLU(inplace=True)\n        self.PReLU = nn.PReLU()\n        self.LeakyReLU = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        # self.GeLU = nn.GELU()\n        self.RReLU = nn.RReLU()\n\n    def forward(self, x):\n        x = self.batch_norm0(x)\n        x = self.dropout0(x)\n\n        x = self.dense1(x)\n        x = self.batch_norm1(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout1(x)\n\n        x = self.dense2(x)\n        x = self.batch_norm2(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout2(x)\n        \n        x = self.dense3(x)\n        x = self.batch_norm3(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout3(x)\n        \n        x = self.dense4(x)\n        x = self.batch_norm4(x)\n        x = x * F.sigmoid(x)\n        x = self.dropout4(x)\n\n        x = self.dense5(x)\n\n        return x","c01deca1":"class MarketDataset:\n    def __init__(self, df):\n        self.features = df[features].values\n\n        self.label = (df['resp'] > 0).astype('int').values.reshape(-1, 1)\n\n    def __len__(self):\n        return len(self.label)\n\n    def __getitem__(self, idx):\n        return {\n            'features': torch.tensor(self.features[idx], dtype=torch.float),\n            'label': torch.tensor(self.label[idx], dtype=torch.float)\n        }","93afe0d8":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n\n    for data in dataloader:\n        optimizer.zero_grad()\n        features = data['features'].to(device)\n        label = data['label'].to(device)\n        outputs = model(features)\n        loss = loss_fn(outputs, label)\n        loss.backward()\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n\n        final_loss += loss.item()\n\n    final_loss \/= len(dataloader)\n\n    return final_loss\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n\n    for data in dataloader:\n        features = data['features'].to(device)\n\n        with torch.no_grad():\n            outputs = model(features)\n\n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n\n    preds = np.concatenate(preds).reshape(-1)\n\n    return preds","173ae7ca":"class SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","eef804f4":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            # ema.apply_shadow()\n            self.save_checkpoint(epoch_score, model, model_path)\n            # ema.restore()\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","afbe400f":"def utility_score_bincount(date, weight, resp, action):\n    count_i = len(np.unique(date))\n    # print('weight: ', weight)\n    # print('resp: ', resp)\n    # print('action: ', action)\n    # print('weight * resp * action: ', weight * resp * action)\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u","c76348c7":"batch_size = 4096\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\nstart_time = time.time()\noof = np.zeros(len(train['action']))\ngkf = GroupKFold(n_splits = 5)\nfor fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n    train_set = MarketDataset(train.loc[tr])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n    valid_set = MarketDataset(train.loc[te])\n    valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=4)\n    \n    torch.cuda.empty_cache()\n    device = torch.device(\"cuda:0\")\n    model = Model()\n    model.to(device)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    loss_fn = SmoothBCEwLogits(smoothing=label_smoothing)\n    \n    ckp_path = f'JSModel_{fold}.pth'\n    \n    es = EarlyStopping(patience=3, mode=\"max\")\n    for epoch in range(10):\n        train_loss = train_fn(model, optimizer, None, loss_fn, train_loader, device)\n        valid_pred = inference_fn(model, valid_loader, device)\n        auc_score = roc_auc_score((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n        logloss_score = log_loss((train.loc[te]['resp'] > 0).astype('int').values.reshape(-1, 1), valid_pred)\n        valid_pred = np.where(valid_pred >= 0.5, 1, 0).astype(int)\n        u_score = utility_score_bincount(date=train.loc[te].date.values, weight=train.loc[te].weight.values, resp=train.loc[te].resp.values, action=valid_pred)\n\n        print(f\"FOLD{fold} EPOCH:{epoch:3}, train_loss:{train_loss:.5f}, u_score:{u_score:.5f}, auc:{auc_score:.5f}, logloss:{logloss_score:.5f}, \"\n              f\"time: {(time.time() - start_time) \/ 60:.2f}min\")\n        \n        es(auc_score, model, model_path=ckp_path)\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n#     break # only train 1 model for fast, you can remove it to train 5 folds","682ea6ea":"models = []\nfor i in range(5): # for fast inference, you can change 1-->5 to get higher score\n    torch.cuda.empty_cache()\n    device = torch.device(\"cuda:0\")\n    model = Model()\n    model.to(device)\n    model.eval()\n    \n    ckp_path = f'JSModel_{i}.pth'\n    model.load_state_dict(torch.load(ckp_path))\n    models.append(model)","12a75980":"f_mean = np.load('.\/f_mean.npy')","0ef120e5":"env = janestreet.make_env()\nenv_iter = env.iter_test()","82bfbb9c":"opt_th = 0.5\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = 0.\n        \n        for i, clf in enumerate(models):\n            if i == 0:\n                pred = model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ len(models)\n            else:\n                pred += model(torch.tensor(x_tt, dtype=torch.float).to(device)).sigmoid().detach().cpu().numpy() \/ len(models)\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","04860def":"# Load Models\n\nJust use three models to reduce running time.","cc8e0dc6":"# Preprocessing","cc54c1fe":"# Training","69887f73":"# Submitting","39fb680b":"# Neural Network Starter Pytorch Version\n\nThis kernel is a pytorch version of yirun's https:\/\/www.kaggle.com\/gogo827jz\/jane-street-neural-network-starter kernel.\nIf you think this kernel is helpful, please upvote!!","27c3d31e":"# Just upvoting if it helps!!!"}}