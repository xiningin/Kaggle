{"cell_type":{"1b2a7069":"code","f0bb61ce":"code","796187e3":"code","e6ccff3b":"code","a5118b3f":"code","70b0341e":"code","18f4b337":"code","b1796969":"code","1f1ac4d0":"code","8f597e16":"code","b58e4cfd":"code","afb79c2d":"code","a769569c":"code","ad7a19c3":"code","deb26e92":"code","98190980":"code","16358247":"code","650cc8a7":"code","4c1355bc":"code","2993584c":"code","8146a8ba":"code","b2b8adc1":"code","9c41a394":"code","d53579c0":"code","d304e52b":"code","dcbf335a":"code","e68b7190":"code","a74317e7":"code","a22e1a46":"code","1aceed27":"code","5025c28d":"code","171d4608":"code","a2f27b84":"code","6788495d":"code","ec801c24":"code","c3216eb5":"code","7a62b3f0":"code","cdd53a84":"code","84991ac4":"code","daaeeb51":"code","e4b5615d":"code","8b4fe6e3":"code","f115995f":"code","9353deb0":"code","37913b9e":"code","ad4cbc60":"code","3733059c":"code","463dc618":"code","45ae5612":"code","584574f4":"code","6c0d8ba6":"code","9be3dc6a":"code","46d984cf":"code","5cacc918":"code","9d068d53":"code","13ffca8d":"code","65847377":"code","cd3462f6":"markdown","7fb2505d":"markdown","06cbc90d":"markdown","dbeabcbe":"markdown","0ebf7e53":"markdown","6da822c5":"markdown","991de763":"markdown","8dec3587":"markdown","a82e6653":"markdown","8c3d224f":"markdown","658e2efd":"markdown","76f62b9c":"markdown","909be686":"markdown","f101fcf5":"markdown","c6579848":"markdown","308b2b7b":"markdown","831b75ff":"markdown"},"source":{"1b2a7069":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0bb61ce":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","796187e3":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\ndf.shape","e6ccff3b":"# 1 = yes, 0 = no\ndf.target.value_counts()","a5118b3f":"df.target.value_counts().plot(kind='bar', color=['salmon', 'lightgreen']);","70b0341e":"df.info()","18f4b337":"# Check for missing values\ndf.isna().sum()","b1796969":"# Summary statistics\ndf.describe()","1f1ac4d0":"# 1 = male, 0 = female\ndf.sex.value_counts()","8f597e16":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","b58e4cfd":"# Create a crosstab plot\npd.crosstab(df.target, df.sex).plot(kind='bar',\n                                    figsize=(10, 6),\n                                    color=['salmon', 'lightblue'])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('0 = No Disease, 1 = Disease')\nplt.ylabel('Amount')\nplt.legend(['Female', 'Male'])\nplt.xticks(rotation=0);","afb79c2d":"plt.figure(figsize=(10, 6))\n# Scatter with positive examples\nplt.scatter(df.age[df.target==1],\n            df.thalach[df.target==1],\n            c='salmon');\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],\n            df.thalach[df.target==0],\n            c='lightblue');\n\nplt.title('Heart Disease in function of Age and Max Heart Rate')\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend(['Disease', 'No Disease']);","a769569c":"# Check the distribution of the age column with a histogram\ndf.age.plot.hist();","ad7a19c3":"pd.crosstab(df.cp, df.target)","deb26e92":"pd.crosstab(df.cp, df.target).plot(kind='bar', figsize=(10, 6), color=['salmon', 'lightblue']);\nplt.title('Heart Disease Frequncy per Chest Pain Type')\nplt.xlabel('Chest pain type')\nplt.ylabel('Amount')\nplt.legend(['No Disease', 'Disease'])\nplt.xticks(rotation=0);","98190980":"# Make a correlation matrix\ncorr_matrix = df.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt='.2f', cmap='jet');","16358247":"# Split data into X and y\nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]","650cc8a7":"# Split data into train and test sets\nnp.random.seed(42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","4c1355bc":"# Put models into a dictionary\nmodels = {'Logistic Regression': LogisticRegression(),\n          'KNN': KNeighborsClassifier(),\n          'Random Forest': RandomForestClassifier()}\n\n# Create a function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given ML models. \n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    # Keep track of model scores \n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","2993584c":"# Call function\nmodel_scores = fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\nmodel_scores","8146a8ba":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","b2b8adc1":"# Tune KNN\ntrain_scores = []\ntest_scores = []\n\n# Create list of different values for n_neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instance\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    # Update the test scores list\n    test_scores.append(knn.score(X_test, y_test))    ","9c41a394":"train_scores","d53579c0":"test_scores","d304e52b":"plt.plot(neighbors, train_scores, label='Train score')\nplt.plot(neighbors, test_scores, label='Test score')\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel('Number of neighbors')\nplt.ylabel('Model score')\nplt.legend()\n\nprint(f'Maximum KNN score on the test data: {max(test_scores)*100:.2f}%')","dcbf335a":"# Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","e68b7190":"# Grids are setup for each model, now they are being tuned\n\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","a74317e7":"rs_log_reg.best_params_","a22e1a46":"rs_log_reg.score(X_test, y_test)","1aceed27":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(X_train, y_train)","5025c28d":"rs_rf.best_params_","171d4608":"# Evaluation\nrs_rf.score(X_test, y_test)","a2f27b84":"# Different hyperparameters for the model\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 30),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparamter search for LR\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train)","6788495d":"gs_log_reg.best_params_","ec801c24":"# Evaluation\ngs_log_reg.score(X_test, y_test)","c3216eb5":"model_scores","7a62b3f0":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","cdd53a84":"y_preds ","84991ac4":"y_test ","daaeeb51":"# Plot ROC curve and caluculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","e4b5615d":"# Confusion matrix\nprint(confusion_matrix(y_test, y_preds))","8b4fe6e3":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plot nice confustion matrix using seaborn\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True,\n                     cbar=False)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n\nplot_conf_mat(y_test, y_preds)","f115995f":"# Classification report, simple, not cross\/validated!\nprint(classification_report(y_test, y_preds))","9353deb0":"# Classification report with cross-validation. Function: cross_val_score()\n# Check best hyperparameters\ngs_log_reg.best_params_","37913b9e":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear')","ad4cbc60":"# Cross-validated ACCURACY\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring='accuracy')\ncv_acc = np.mean(cv_acc)\ncv_acc","3733059c":"# Cross-validated PRECISION\ncv_precision = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring='precision')\ncv_precision = np.mean(cv_precision)\ncv_precision","463dc618":"# Cross-validated RECALL\ncv_recall = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring='recall')\ncv_recall = np.mean(cv_recall)\ncv_recall","45ae5612":"# Cross-validated F1\ncv_f1 = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5,\n                         scoring='f1')\ncv_f1 = np.mean(cv_f1)\ncv_f1","584574f4":"# Visualise cross-validated metrics\ncv_metrics = pd.DataFrame({'Accuracy': cv_acc,\n                           'Precision': cv_precision,\n                           'Recall': cv_recall,\n                           'F1': cv_f1},\n                          index=[0])\n\ncv_metrics.T.plot.bar(title='Cross-validated classification metrics', legend=False);\n#sns.barplot(data=cv_metrics);","6c0d8ba6":"df.head()","9be3dc6a":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                                     solver='liblinear')\n\nclf.fit(X_train, y_train);","46d984cf":"# Check coef_\nclf.coef_","5cacc918":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","9d068d53":"# Visualise feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title='Feature Importance', legend=False);","13ffca8d":"pd.crosstab(df['sex'], df['target'])","65847377":"pd.crosstab(df['slope'], df['target'])","cd3462f6":"### Heart frequency according to sex","7fb2505d":"# Load data","06cbc90d":"### Age vs. maximum heart rate for heart disease","dbeabcbe":"# Predicting Heart Disease Using Machine Learning\n\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation","0ebf7e53":"# Preparing the tools","6da822c5":"## Hyperparameter tuning with RandomizedSearchCV\n\nTuning:\n- LogisticRegression()\n- RandomForestClassifier()","991de763":"## Hyperparameter tuning with GridSearchCV\n\nLogisticRegression provides the best score so far, let's try to improve it further!","8dec3587":"### Feature Importance\n\n\"Which features contributed most to the outcome of the model?\" (Different for each ML model)","a82e6653":"# Applying 3 different models:\n\n1. Logistic Regression\n2. KNN Classifier\n3. Random Forest Classifier","8c3d224f":"### Tuning RandomForestClassifier","658e2efd":"## Evaluating the tuned model beyond accuracy\n\n- ROC curve and AUC score\n- Confusion matrix\n- Classification report\n- Precision \n- Recall\n- F1 Score\n- Cross-validation where possible","76f62b9c":"# Modelling","909be686":"# EDA","f101fcf5":"### Model comparison","c6579848":"### Heart disease frequency per chest pain type","308b2b7b":"# Next steps...\n\n- Hyperparameter tuning\n- Feature importance\n- Confusion matrix\n- Cross-validation\n- Precision\n- Recall\n- F1 Score\n- Classification report\n- ROC curve\n- AUC\n\n### Hyperparameter tuning","831b75ff":"##### No improvement for the LogisticRegression!"}}