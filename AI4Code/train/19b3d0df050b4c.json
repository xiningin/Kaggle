{"cell_type":{"60f98f21":"code","b85ff7ca":"code","781f9292":"code","a9fa9662":"code","92e27002":"code","3be22c59":"code","7625588e":"code","56f2f114":"code","718ef0fd":"code","9637b0d8":"code","e7576186":"code","eeb5d642":"code","0b8dae09":"code","6136902e":"code","c9bdb00e":"code","7c16ddbb":"code","e1bf8655":"code","06dca5bd":"code","50c9b94f":"code","ec220335":"code","a60dea6d":"code","eee5db6c":"code","f34cc8b8":"code","0b17944d":"code","d98188aa":"code","a8851f19":"code","2f438f8f":"code","e2abdd78":"code","5ec1e6b0":"code","ce5aa06c":"code","ffdd57ec":"code","24b20d33":"code","e77fe273":"code","293eb7ce":"code","724802fa":"code","8bffad41":"code","d4a42d8a":"code","38d4906c":"code","1f5c9009":"markdown","76073803":"markdown","9fd01075":"markdown","814003db":"markdown","9c9da6d8":"markdown","def7c590":"markdown","8bb03a66":"markdown","04044a82":"markdown","de657993":"markdown","a978434d":"markdown"},"source":{"60f98f21":"import re\nimport string\nimport numpy as np\nimport time\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader","b85ff7ca":"with open('..\/input\/mahabharat-english\/Mahabharat.txt', 'r') as file:\n    mahabharat_text_raw = file.read()","781f9292":"type(mahabharat_text_raw)","a9fa9662":"#mahabharat_text_raw","92e27002":"def clean_data(data):    \n    # removing \u00ef\u00bb\u00bfSECTION\n    data = re.sub('\u00ef\u00bb\u00bfSECTION [I-X]*', '', data)\n    \n    # Removing \\n\n    data = re.sub(r'\\n', ' ', data)\n    \n    # removing extra symbols\n    data = re.sub(\"[^\\w]\", ' ', data)\n    data = data.replace(r'[^0-9a-zA-Z:,]+','  ')\n    tokens = data.split()\n    table = str.maketrans('', '', string.punctuation)\n    tokens = [w.translate(table) for w in tokens]\n    tokens = [w for w in tokens if w.isalpha()]\n    tokens = [w.lower() for w in tokens]\n \n    \n    return data","3be22c59":"mahabharat_text_cleaned = clean_data(mahabharat_text_raw)","7625588e":"mahabharat_text_cleaned","56f2f114":"mahabharat_text_cleaned_split = mahabharat_text_cleaned.split()","718ef0fd":"print('Total word in Mahabharat = ',len(mahabharat_text_cleaned_split))","9637b0d8":"def create_seq(text):\n    length = 50\n    sequences = list()\n    for i in range(length, len(text)):\n        # select sequence of tokens\n        seq = text[i-length:i+1]\n        # store\n        line = ' '.join(seq)\n        sequences.append(line)\n    print('Total Sequences: %d' % len(sequences))\n    return sequences","e7576186":"mahabharat_seq = create_seq(mahabharat_text_cleaned_split)","eeb5d642":"n_vocab = len(set(mahabharat_text_cleaned_split))\nn_vocab","0b8dae09":"len(mahabharat_seq[0])","6136902e":"# Tokenizing based on vocab and dict\nword_counts = Counter(mahabharat_text_cleaned_split)\nsorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\nint_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\nvocab_to_int = {w: k for k, w in int_to_vocab.items()}\nn_vocab = len(int_to_vocab)\nn_vocab","c9bdb00e":"# Tokenizing sequenced text\nt0 = time.time()\nmahabharat_int_text = []\nfor seq in mahabharat_seq:\n    int_text = [vocab_to_int[w] for w in seq.split()]\n    mahabharat_int_text.append(int_text)\nprint('{} seconds'.format(time.time() - t0))","7c16ddbb":"sequences_array = np.array(mahabharat_int_text)","e1bf8655":"sequences_array","06dca5bd":"\"\"\"\n# Tokenization\ntokenizer = Tokenizer(num_words=n_vocab, filters='@')\ntokenizer.fit_on_texts(mahabharat_seq)\nsequences = tokenizer.texts_to_sequences(mahabharat_seq)\nsequences_array = np.array(sequences)\n\"\"\"","50c9b94f":"X, y = sequences_array[:, :-1], sequences_array[:,-1]","ec220335":"print('x :', X[0])\nprint('y :', y[0])\nprint('y :', y.shape)","a60dea6d":"# one-hot encoding y\ndef one_hot_encode(arr, n_labels):\n    \n    # Initialize the the encoded array\n    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n    \n    # Fill the appropriate elements with ones\n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    \n    # Finally reshape it to get back to the original array\n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot\ny_encoded = one_hot_encode(y, n_vocab)","eee5db6c":"seq_size = X.shape[0]\/\/32","f34cc8b8":"#Limiting data size for 32 batches\nX_new = X[:seq_size]\ny_new = y_encoded[:seq_size]","0b17944d":"# List of x and y\nX_list = X_new\ny_list = y_new","d98188aa":"X_tensor = torch.Tensor(X_list)\ny_tensor = torch.Tensor(y_list)","a8851f19":"X_tensor.shape","2f438f8f":"seq_size = 50\nembedding_size = 64\nlstm_size = 64\nbatch_size = 32\nlr = 0.003","e2abdd78":"my_dataset = TensorDataset(X_tensor, y_tensor)\ntrain_set = DataLoader(my_dataset, batch_size=batch_size, drop_last=True)","5ec1e6b0":"# [batch, seq]\nprint('Shape of Input:', next(iter(train_set))[0].shape)","ce5aa06c":"train_check = torch.cuda.is_available()\nif train_check:\n    print('Training on GPU')\nelse:\n    print('Training on CPU')","ffdd57ec":"class Mahabharat_Model(nn.Module):\n    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n        super(Mahabharat_Model, self).__init__()\n        self.seq_size = seq_size\n        self.lstm_size = lstm_size\n        self.embedding = nn.Embedding(num_embeddings=n_vocab, embedding_dim=embedding_size)\n        self.lstm = nn.LSTM(embedding_size, lstm_size, batch_first=True)\n        self.dense0 = nn.Linear(lstm_size, 512)\n        self.dense2 = nn.Linear(512, n_vocab)\n        \n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        dense_0_out = F.relu(self.dense0(output))\n        logits = F.softmax(self.dense2(dense_0_out), dim=1)\n        \n        return logits, state\n    \n    def zero_state(self, batch_size):\n        return(torch.zeros(1, batch_size, self.lstm_size), torch.zeros(1, batch_size, self.lstm_size))","24b20d33":"model = Mahabharat_Model(n_vocab, seq_size, embedding_size, lstm_size)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","e77fe273":"# Loss and optimizers\ncritertion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)","293eb7ce":"\nepochs = 50\nfor e in range(epochs):\n    print('Epoch running:', e)\n    state_h, state_c = model.zero_state(batch_size)\n    \n    # transfer data to gpu if available\n    state_h = state_h.to(device)\n    state_c = state_c.to(device)\n    \n    # Claculating time of each epoch\n    t0 = time.time()\n    \n    for x, y in train_set:\n        \n        # Training mode\n        model.train()\n        \n        # Reset all gradients\n        model.zero_grad()\n        \n        # Transfer data to gpu\n        x = x.to(device)\n        y = y.to(device)\n        logits, (state_h, state_c) = model(x.long(), (state_h, state_c))\n        \n        # loss calculation\n        loss = critertion(logits, y.long())\n        \n        state_h = state_h.detach()\n        state_c = state_c.detach()\n        \n        loss_value = loss.item()\n        \n        # Backprop\n        loss.backward(create_graph=True)\n        \n        # Gradient clipping\n        _ = torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n        \n        # optimizer step\n        optimizer.step()\n    \n    print('Time For 1 epoch :', time.time() - t0)    \n    print('Epoch: {}\/{}'.format(e, 200),'Loss: {}'.format(loss_value))","724802fa":"torch.save(model, 'model')","8bffad41":"PATH = '.\/model'\nnet = torch.load(PATH, map_location=device)\nnet.eval()","d4a42d8a":"def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n    model.eval()\n\n    state_h, state_c = net.zero_state(1)\n    state_h = state_h.to(device)\n    state_c = state_c.to(device)\n    for w in words:\n        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n        output, (state_h, state_c) = net(ix.long(), (state_h, state_c))\n    \n    _, top_ix = torch.topk(output[0], k=top_k)\n    choices = top_ix.tolist()\n    choice = np.random.choice(choices[0])\n\n    words.append(int_to_vocab[choice])\n    \n    for _ in range(100):\n        ix = torch.tensor([[choice]]).to(device)\n        output, (state_h, state_c) = net(ix.long(), (state_h, state_c))\n\n        _, top_ix = torch.topk(output[0], k=top_k)\n        choices = top_ix.tolist()\n        choice = np.random.choice(choices[0])\n        words.append(int_to_vocab[choice])\n\n    print(' '.join(words))","38d4906c":"predict(device=device, net=model, words=[\"Narayana\" ,\"said\"], n_vocab=n_vocab, vocab_to_int=vocab_to_int, int_to_vocab=int_to_vocab)","1f5c9009":"# Importing File","76073803":"# Cleaning data\nData in its current form is not acceptable it conatins some weird symbols and '\\n' (endline) and some '\\', We need to fix it before we can train model to get better generated Text.","9fd01075":"# Generating Text","814003db":"# Pytorch LSTM model","9c9da6d8":"# Data Pre-Processing\nGetting vocabulary of data and converting it in one hot encoding","def7c590":"# MahaBharat Language Model\nThis is a simple language model build on english translated mahabharat. This project to understand language models using RNN and pytorch.\nThe final application of this particular language model is just to generate some text based on the epic of Mahabharat. It can be further expand to make auto-complete system based on mahabharat.","8bb03a66":"# preparing X and y","04044a82":"# Loading Model","de657993":"Now we we convert this sequence to one hot encoded vector","a978434d":"\n# Batching Data"}}