{"cell_type":{"f5ca9294":"code","1470f69f":"code","bfa88f3e":"code","cacf36a5":"code","8fac5d1e":"code","0fec225a":"code","61a4a6e4":"code","de2aff23":"code","1bb62690":"code","c8884ed4":"code","1e4e40f2":"code","11cc7c3f":"code","94be4d43":"code","dc24d229":"code","240f750a":"code","03abccc9":"code","251f6b4d":"code","e132f8fc":"code","4925fd36":"code","96e1a39b":"code","058b8a1b":"code","e7b781c9":"code","14bf6795":"markdown","42d778fa":"markdown","803ac5e5":"markdown","e7d6bf07":"markdown","db7b83b3":"markdown","f6d80cda":"markdown","30cfc036":"markdown","281aea49":"markdown","76af74b3":"markdown","babe249d":"markdown","2409bfba":"markdown","fc30240c":"markdown","9ac62a69":"markdown","3a380440":"markdown"},"source":{"f5ca9294":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1470f69f":"df = pd.read_csv('\/kaggle\/input\/social-network-ads\/Social_Network_Ads.csv')\ndf=df.iloc[:,2:]\ndf.head()","bfa88f3e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('Purchased', axis=1),\n                                                    df['Purchased'],\n                                                    test_size=0.3,\n                                                    random_state=0)\n\nX_train.shape, X_test.shape","cacf36a5":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# fit the scaler to the train set, it will learn the parameters\nscaler.fit(X_train)\n\n# transform train and test sets\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","8fac5d1e":"scaler.mean_","0fec225a":"X_train_scaled ","61a4a6e4":"X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)","de2aff23":"X_train_scaled.head()","1bb62690":"X_train_scaled.describe()","c8884ed4":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\nax1.scatter(X_train['Age'], X_train['EstimatedSalary'])\nax1.set_title(\"Before Scaling\")\nax2.scatter(X_train_scaled['Age'], X_train_scaled['EstimatedSalary'],color='red')\nax2.set_title(\"After Scaling\")\nplt.show()","1e4e40f2":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# before scaling\nax1.set_title('Before Scaling')\nsns.kdeplot(X_train['Age'], ax=ax1)\nsns.kdeplot(X_train['EstimatedSalary'], ax=ax1)\n\n# after scaling\nax2.set_title('After Standard Scaling')\nsns.kdeplot(X_train_scaled['Age'], ax=ax2)\nsns.kdeplot(X_train_scaled['EstimatedSalary'], ax=ax2)\nplt.show()","11cc7c3f":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# before scaling\nax1.set_title('Age Distribution Before Scaling')\nsns.kdeplot(X_train['Age'], ax=ax1)\n\n# after scaling\nax2.set_title('Age Distribution After Standard Scaling')\nsns.kdeplot(X_train_scaled['Age'], ax=ax2)\nplt.show()","94be4d43":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\n# before scaling\nax1.set_title('Salary Distribution Before Scaling')\nsns.kdeplot(X_train['EstimatedSalary'], ax=ax1)\n\n# after scaling\nax2.set_title('Salary Distribution Standard Scaling')\nsns.kdeplot(X_train_scaled['EstimatedSalary'], ax=ax2)\nplt.show()","dc24d229":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr_scaled = LogisticRegression()\nlr.fit(X_train,y_train)\nlr_scaled.fit(X_train_scaled,y_train)\ny_pred = lr.predict(X_test)\ny_pred_scaled = lr_scaled.predict(X_test_scaled)\n\nfrom sklearn.metrics import accuracy_score\nprint(f\"Actual:- {accuracy_score(y_test,y_pred)}\")\nprint(f\"Scaled:- {accuracy_score(y_test,y_pred_scaled)}\")","240f750a":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt_scaled = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ndt_scaled.fit(X_train_scaled,y_train)\ny_pred = dt.predict(X_test)\ny_pred_scaled = dt_scaled.predict(X_test_scaled)\n\nprint(f\"Actual:- {accuracy_score(y_test,y_pred)}\")\nprint(f\"Scaled:- {accuracy_score(y_test,y_pred_scaled)}\")","03abccc9":"df = df.append(pd.DataFrame({'Age':[5,90,95],'EstimatedSalary':[1000,250000,350000],'Purchased':[0,1,1]}),ignore_index=True)\ndf.shape","251f6b4d":"plt.scatter(df['Age'], df['EstimatedSalary'])","e132f8fc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('Purchased', axis=1),\n                                                    df['Purchased'],\n                                                    test_size=0.3,\n                                                    random_state=0)\n\nX_train.shape, X_test.shape","4925fd36":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# fit the scaler to the train set, it will learn the parameters\nscaler.fit(X_train)\n\n# transform train and test sets\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","96e1a39b":"X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)","058b8a1b":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\nax1.scatter(X_train['Age'], X_train['EstimatedSalary'])\nax1.set_title(\"Before Scaling\")\nax2.scatter(X_train_scaled['Age'], X_train_scaled['EstimatedSalary'],color='red')\nax2.set_title(\"After Scaling\")\nplt.show()","e7b781c9":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n\nax1.scatter(X_test['Age'], X_test['EstimatedSalary'])\nax1.set_title(\"Before Scaling\")\nax2.scatter(X_test_scaled['Age'], X_test_scaled['EstimatedSalary'],color='red')\nax2.set_title(\"After Scaling\")\nplt.show()","14bf6795":"# WHERE TO USE STANDARDIZATION?","42d778fa":"# Effect of Scaling\n","803ac5e5":"### **NO CHANGE IN SHAPE OF DISTRIBUTION, JUST SCALE CHANGES**\n\n","e7d6bf07":"CAN AVOID BUT WON'T HURT - Decision Tree,  XG Boost, Random Forest, Gradient Boost ","db7b83b3":"## Comparison of Distributions\n","f6d80cda":"# FEATURE SCALING IS IMPORTANT FOR LINE BASED ALGORITHMS, NOT TREE BASED","30cfc036":"**NOTE** - There are total three outlier, 2 in went in Training set and 1 went in Testing set.","281aea49":"![image.png](attachment:65cbb594-5a8e-4b4b-b36c-c7b8f413c7de.png)","76af74b3":"### **THE IMPACT OF OUTLIER HAS NO REDUCED AFTER FEATURE SCALING**","babe249d":"# Standard Scaler","2409bfba":"# Effect of Outlier","fc30240c":"# Train test split","9ac62a69":"**ADDED 3 DATA POINTS**","3a380440":"### **AS THERE ARE NO OUTLIER IN THE DATASET, I WILL INJECT FEW OUTLIERS IN THE DATASET TO SHOW THE EFFECT**"}}