{"cell_type":{"11ba6f6a":"code","b4d6481f":"code","33a86f79":"code","e3dc03bb":"code","2e73df6b":"code","220c84d4":"code","042370d5":"code","b0378f5f":"code","498b8221":"code","9f78ae74":"code","8fba4ca3":"code","1e4940a9":"code","77387d20":"code","9248768f":"code","e6b32edb":"code","fd135b2b":"code","13668e5c":"code","cb7cf928":"code","98309291":"code","accda96a":"code","4ae8c367":"code","6200a572":"code","7e3f290f":"code","0dc56639":"code","365f2e4a":"code","594dd4b2":"code","5089294f":"code","f28556d5":"code","48201099":"code","9a4b04eb":"code","5d7bbdbe":"code","bdd3a564":"code","013a4e92":"code","91f9c989":"code","0651a3cc":"code","47861f3b":"code","6aac42f0":"code","c588dca2":"code","0cda9f88":"code","568d99d5":"code","fc6b9765":"code","7d899763":"code","212e1b80":"code","6c45d3fd":"code","e69c054b":"code","6e3478b5":"code","267cd5b8":"code","134ddc0c":"code","6e9334f6":"code","8b3b5067":"code","ad646246":"code","94214aee":"code","45890a5a":"markdown","e04e7732":"markdown","0b25ea09":"markdown","41018c8c":"markdown","13ffbd8d":"markdown","bbd3e37b":"markdown","dd4e87cc":"markdown","fbd95e88":"markdown","a5a9b801":"markdown"},"source":{"11ba6f6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import KFold\nfrom sklearn import linear_model\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn import svm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn import neighbors\nfrom math import sqrt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","b4d6481f":"# read data\ndf = pd.read_csv('..\/input\/pmsm_temperature_data.csv', \n                 usecols=[0,1,2,3,4,5,6,7,8,9,10,11])\ndf.head(10)","33a86f79":"df.info()","e3dc03bb":"df.describe().T","2e73df6b":"df.isnull().values.any()","220c84d4":"# Count the number of NaNs each column has.\nnans=pd.isnull(df).sum()\nnans[nans>0]","042370d5":"# Count the column types\ndf.dtypes.value_counts()","b0378f5f":"df.corr()","498b8221":"import seaborn as sns\nsns.jointplot(x='i_d', y='motor_speed', data=df, kind='reg')","9f78ae74":"#correlation map\nf,ax=plt.subplots(figsize=(12,12))\ncorr=df.corr()\n\nsns.heatmap(corr, annot=True, linewidths=.5, fmt='.2f', \n            mask= np.zeros_like(corr,dtype=np.bool), \n            cmap=sns.diverging_palette(100,200,as_cmap=True), \n            square=True, ax=ax)\n\nplt.show()","8fba4ca3":"import statsmodels.api as sm\n#Defining dependet and independent variable\nX = df['i_d']\nX=sm.add_constant(X)\n\ny = df['motor_speed']\n\nlm=sm.OLS(y,X)\nmodel=lm.fit()\n\nmodel.summary()","1e4940a9":"model.params","77387d20":"print(\"f_pvalue:\", \"%.4f\" % model.f_pvalue)","9248768f":"model.mse_model #mean squared error is too much. It is not good.","e6b32edb":"model.rsquared #Not bad","fd135b2b":"model.rsquared_adj #Not bad","13668e5c":"model.fittedvalues[0:5] #Predicted values","cb7cf928":"y[0:5] #Real values","98309291":"#Model equation\nprint(\"Motor speed = \" + \n      str(\"%.3f\" % model.params[0]) + ' + i_d' + \"*\" + \n      str(\"%.3f\" % model.params[1]))","accda96a":"#Model Visualization \ng=sns.regplot(df['i_d'] , df['motor_speed'], \n              ci=None, scatter_kws={'color': 'r', 's':9})\ng.set_title('Model equation: motor_speed = -0.002 + i_d * -0.725')\ng.set_ylabel('Motor_speed')\ng.set_xlabel('i_d');","4ae8c367":"from sklearn.metrics import r2_score,mean_squared_error\n\nmse=mean_squared_error(y, model.fittedvalues)\nrmse=np.sqrt(mse)\nrmse","6200a572":"k_t=pd.DataFrame({'Real_values':y[0:50], \n                  'Predicted_values' :model.fittedvalues[0:50]})\nk_t['error']=k_t['Real_values']-k_t['Predicted_values']\nk_t.head()","7e3f290f":"model.resid[0:10] #It is easy way to learn residuals.","0dc56639":"plt.plot(model.resid);","365f2e4a":"X=df.drop(\"motor_speed\", axis=1)\ny=df[\"motor_speed\"]","594dd4b2":"from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)\n\ntraining=df.copy()","5089294f":"lm=sm.OLS(y_train, X_train)\n\nmodel=lm.fit()\nmodel.summary() #All coefficients are significant for the model by looking at the p-value. ( P>|t| )","f28556d5":"#Root Mean Squared Error for Train\nrmse1=np.sqrt(mean_squared_error(y_train,model.predict(X_train)))\nrmse1","48201099":"#Root Mean Squared Error for Test\nrmse2=np.sqrt(mean_squared_error(y_test,model.predict(X_test)))\nrmse2","9a4b04eb":"#Model Tuning for Multiple Linear Regression\nmodel = LinearRegression().fit(X_train,y_train)\ncross_val_score1=cross_val_score(model, X_train, y_train, cv=10, scoring='r2').mean() #verified score value for train model\nprint('Verified R2 value for Training model: ' + str(cross_val_score1))\n\ncross_val_score2=cross_val_score(model, X_test, y_test, cv=10, scoring='r2').mean() #verified score value for test model\nprint('Verified R2 value for Testing Model: ' + str(cross_val_score2))","5d7bbdbe":"RMSE1=np.sqrt(-cross_val_score(model, X_train, y_train, cv=10, \n                               scoring='neg_mean_squared_error')).mean() #verified RMSE score value for train model\nprint('Verified RMSE value for Training model: ' + str(RMSE1))\n\nRMSE2=np.sqrt(-cross_val_score(model, X_test, y_test, cv=10, \n                               scoring='neg_mean_squared_error')).mean() #verified RMSE score value for test model\nprint('Verified RMSE value for Testing Model: ' + str(RMSE2))","bdd3a564":"#Visualizing for Multiple Linear Regression y values\n\nimport seaborn as sns\nax1 = sns.distplot(y_train, hist=False, color=\"r\", label=\"Actual Value\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Fitted Values\" , ax=ax1);","013a4e92":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\n\npca=PCA()\nX_reduced_train=pca.fit_transform(scale(X_train))","91f9c989":"explained_variance_ratio=np.cumsum(np.round(pca.explained_variance_ratio_ , decimals=4)* 100)[0:20]","0651a3cc":"plt.bar(x=range(1, len(explained_variance_ratio)+1), height=explained_variance_ratio)\nplt.ylabel('percentange of explained variance')\nplt.xlabel('principal component')\nplt.title('bar plot')\nplt.show()\n# 7 component is enough for model.","47861f3b":"lm=LinearRegression()\npcr_model=lm.fit(X_reduced_train,y_train)\nprint('Intercept: ' + str(pcr_model.intercept_))\nprint('Coefficients: ' + str(pcr_model.coef_))","6aac42f0":"#Prediction\ny_pred=pcr_model.predict(X_reduced_train)\nnp.sqrt(mean_squared_error(y_train,y_pred))","c588dca2":"df['motor_speed'].mean()","0cda9f88":"#R squared\nr2_score(y_train,y_pred)","568d99d5":"# Prediction For testing error \npca2=PCA()\n\nX_reduced_test=pca2.fit_transform(scale(X_test))\npcr_model2=lm.fit(X_test,y_test)\n\ny_pred=pcr_model2.predict(X_reduced_test)\n\nprint('RMSE for test model : ' +str(np.sqrt(mean_squared_error(y_test,y_pred))))","fc6b9765":"#Model Tuning for PCR\n\nlm=LinearRegression()\npcr_model=lm.fit(X_reduced_train[:,0:10],y_train)\ny_pred=pcr_model.predict(X_reduced_test[:,0:10])\n\nfrom sklearn import model_selection\n\ncv_10=model_selection.KFold(n_splits=10,\n                           shuffle=True,\n                           random_state=1)","7d899763":"lm=LinearRegression()\nRMSE=[]\n\nfor i in np.arange(1,X_reduced_train.shape[1] + 1):\n    score=np.sqrt(-1*model_selection.cross_val_score(lm,\n                                                    X_reduced_train[:,:i],\n                                                    y_train.ravel(),\n                                                    cv=cv_10,\n                                                    scoring='neg_mean_squared_error').mean())\n    RMSE.append(score)","212e1b80":"plt.plot(RMSE)\nplt.xlabel('# of Components')\nplt.ylabel('RMSE')\nplt.title('PCR Model Tuning for Motor_Speed Prediction'); ","6c45d3fd":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","e69c054b":"knn_model=KNeighborsRegressor().fit(X_train, y_train)\ny_pred=knn_model.predict(X_test)","6e3478b5":"y_pred.shape","267cd5b8":"#Model Tuning (learning best n_neighbors hyperparameter)\nknn_params={'n_neighbors' : np.arange(1,5,1)}\n\nknn=KNeighborsRegressor()\nknn_cv_model=GridSearchCV(knn, knn_params, cv=5)\n\nknn_cv_model.fit(X_train,y_train)\nknn_cv_model.best_params_[\"n_neighbors\"]","134ddc0c":"# Train error values from n=1 up n=2\nRMSE=[]\nRMSE_CV=[]\nfor k in range(2):\n    k=k+1\n    knn_model=KNeighborsRegressor(n_neighbors=k).fit(X_train, y_train)\n    y_pred=knn_model.predict(X_train)\n    rmse=np.sqrt(mean_squared_error(y_train,y_pred))\n    rmse_cv=np.sqrt(-1*cross_val_score(knn_model,X_train,y_train,cv=2,\n                                       scoring='neg_mean_squared_error').mean())\n\n    RMSE.append(rmse)\n    RMSE_CV.append(rmse_cv)\n\n    print(\"RMSE value: \", rmse, 'for k= ',k,\n          \"RMSE values with applying Cross Validation: \", rmse_cv)","6e9334f6":"#Model Tuning according to best parametre for KNN Regression\nknn_tuned=KNeighborsRegressor(n_neighbors=knn_cv_model.best_params_[\"n_neighbors\"])\nknn_tuned.fit(X_train,y_train)\nnp.sqrt(mean_squared_error(y_test,knn_tuned.predict(X_test)))","8b3b5067":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score,mean_squared_error","ad646246":"quad = PolynomialFeatures (degree = 2)\nx_quad = quad.fit_transform(X_train)\n\nX_train,X_test,y_train,y_test = train_test_split(x_quad,y_train, random_state = 0)\n\nplr = LinearRegression().fit(X_train,y_train)\n\nY_train_pred = plr.predict(X_train)\nY_test_pred = plr.predict(X_test)\n\nprint('Polynomial Linear Regression:' ,plr.score(X_test,y_test))","94214aee":"#Plotting Residual in Linear Regression \n\nfrom sklearn import linear_model,metrics\n#Create linear regression object\nreg=linear_model.LinearRegression()\n\n#train the model using the train data sets\nreg.fit(X_train,y_train)\n\n#regression coefficients\nprint(\"Coefficients: \\n\", reg.coef_)\n\n#Variance score\nprint(\"Variance score: {}\".format(reg.score(X_test,y_test)))\n\nplt.style.use('fivethirtyeight')\n\n#plotting residual errors in training data\nplt.scatter(reg.predict(X_train),reg.predict(X_train)-y_train, \n            color=\"green\", s=10, label=\"train data\")\n\n#plotting residual errors in test data\nplt.scatter(reg.predict(X_test),reg.predict(X_test)-y_test, \n            color=\"blue\", s=10, label=\"test data\")\n\n#plot line for zero residual error\nplt.hlines(y=0,xmin=-2, xmax=2, linewidth=2)\n\n#plot legend\nplt.legend(loc='upper right')\n\n#plot title\nplt.title(\"residual error\")\n\nplt.show()","45890a5a":"<a id= \"4\" ><\/a><br>**KNN REGRESSION**","e04e7732":"[**BASIC LINEAR REGRESSION**](#1)\n\n[**MULTIPLE LINEAR REGRESSION**](#2)\n\n[**PRINCIPAL COMPONENT REGRESSION**](#3)\n\n[**K-NEAREST NEIGHBORHOOD REGRESSION**](#4)\n\n[**POLYNOMIAL REGRESSION**](#5)","0b25ea09":"y=-0.002-0.7245*x this is formula of basic regression model. p-value is less than 0.05 so it is meaningful model.","41018c8c":"<a id= \"5\" ><\/a><br> POLYNOMIAL REGRESSION","13ffbd8d":"<a id= \"3\" ><\/a><br>**PRINCIPAL COMPONENT REGRESSION**","bbd3e37b":"<a id= \"2\" ><\/a><br>**MULTIPLE LINEAR REGRESSION**","dd4e87cc":"There are high correlation between values.","fbd95e88":"10 component is good for the model because RMSE value is the smallest for this component number. That's why there is no need to tune the model.","a5a9b801":"<a id= \"1\" ><\/a><br>**BASIC LINEAR REGRESSION**"}}