{"cell_type":{"305f3ff8":"code","507654ee":"code","09531f4f":"code","209df4de":"code","27614f3f":"code","fb4bf122":"code","797b4eb7":"code","e6d5831e":"code","3d664cd7":"code","10a0effb":"code","078150a6":"code","940ca443":"code","6a86c6b1":"code","e11deff4":"code","98fd9b32":"code","8ce30d46":"markdown","74832720":"markdown","a2c7ea85":"markdown","347fb64d":"markdown","36297e8c":"markdown","3d6dda05":"markdown","c0c0ea13":"markdown","3841a492":"markdown","0ac4f59b":"markdown","3c8694bd":"markdown","8ad9149c":"markdown"},"source":{"305f3ff8":"import pandas as pd\n\nreuters = pd.read_csv('..\/input\/fake-news-data\/reuters-newswire-2017.v5.csv')\ndata = reuters.drop(['publish_time'], axis=1).rename(columns={'headline_text': 'headline'})\n\ncount = data['headline'].str.split().str.len()\ndata = data[count<=13]\n\ndata = data.sample(frac=1)\ndata = data[:200000]\n\ndata = data.reset_index(drop=True)","507654ee":"START, END = '\u00f7', '\u25a0'","09531f4f":"import numpy as np\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef format_data(data, max_features, maxlen, shuffle=False):\n    if shuffle:\n        data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Add start and end tokens\n    data['headline'] = START + ' ' + data['headline'].str.lower() + ' ' + END\n\n    text = data['headline']\n    \n    # Tokenize text\n    filters = \"\\\"#$%&()*+.\/<=>@[\\\\]^_`{|}~\\t\\n\"\n    tokenizer = Tokenizer(num_words=max_features, filters=filters)\n    tokenizer.fit_on_texts(list(text))\n    corpus = tokenizer.texts_to_sequences(text)\n    \n    # Build training sequences of (context, next_word) pairs.\n    # Note that context sequences have variable length. An alternative\n    # to this approach is to parse the training data into n-grams.\n    X, Y = [], []\n    for line in corpus:\n        for i in range(1, len(line)-1):\n            X.append(line[:i+1])\n            Y.append(line[i+1])\n    \n    # Pad X and convert Y to categorical (Y consisted of integers)\n    X = pad_sequences(X, maxlen=maxlen)\n    Y = to_categorical(Y, num_classes=max_features)\n\n    return X, Y, tokenizer","209df4de":"max_features, max_len = 2500, 15\nX, Y, tokenizer = format_data(data, max_features, max_len)","27614f3f":"import pickle\npickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))","fb4bf122":"tokenizer.word_index['trump'], tokenizer.word_index[START], tokenizer.word_index[END], tokenizer.word_index[';']","797b4eb7":"from keras.layers import Input, Dense, Bidirectional, Embedding, LSTM, SpatialDropout1D\nfrom keras.models import Sequential\n\nepochs = 35\n\nmodel = Sequential()\n\n# Embedding and RNN\nmodel.add(Embedding(max_features, 250, trainable=True))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(Bidirectional(LSTM(100)))\n\n# Output layer\nmodel.add(Dense(max_features, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, Y, epochs=epochs, batch_size=128, verbose=1)\n\nmodel.save_weights('model{}.h5'.format(epochs))","e6d5831e":"model.evaluate(X, Y)","3d664cd7":"def sample(preds, temp=1.0):\n    \"\"\"\n    Sample next word given softmax probabilities, using temperature.\n    \n    Taken and modified from:\n    https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py\n    \"\"\"\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temp\n    preds = np.exp(preds) \/ np.sum(np.exp(preds))\n    preds = np.random.multinomial(1, preds, 1)\n    return np.argmax(preds)","10a0effb":"\"\"\"When sampling from the distribution, we do not know which word is being\nsampled, only its index. We need a way to go from index to word. Unfortunately,\nthe tokenizer class only contains a dictionary of {word: index} items. We will\nreverse that dictionary to get {index: word} items. That way, going from\nindices to words is much faster.\"\"\"\nidx_to_words = {value: key for key, value in tokenizer.word_index.items()}\n\n\ndef process_input(text):\n    \"\"\"Tokenize and pad input text\"\"\"\n    tokenized_input = tokenizer.texts_to_sequences([text])[0]\n    return pad_sequences([tokenized_input], maxlen=max_len-1)\n\n\ndef generate_text(input_text, model, temp=1.0):\n    \"\"\"Takes some input text and feeds it to the model (after processing it).\n    Then, samples a next word and feeds it back into the model until the end\n    token is produced.\n    \n    :input_text: A string or list of strings to be used as a generation seed.\n    :model:      The model to be used during generation.\n    :temp:       A float that adjusts how 'volatile' predictions are. A higher\n                 value increases the chance of low-probability predictions to\n                 be picked.\"\"\"\n    if type(input_text) is str:\n        sent = input_text\n    else:\n        sent = ' '.join(input_text)\n    \n    tokenized_input = process_input(input_text)\n    \n    while True:\n        preds = model.predict(tokenized_input, verbose=0)[0]\n        pred_idx = sample(preds, temp=temp)\n        pred_word = idx_to_words[pred_idx]\n        \n        if pred_word == END:\n            return sent\n        \n        sent += ' ' + pred_word\n        tokenized_input = process_input(sent)\n    \n        if len(sent.split()) > 25:\n            return sent","078150a6":"text = generate_text(START, model, temp=0.01)\ntext[2:] # the first two elements are '\u00f7 '","940ca443":"text = generate_text(START, model, temp=0.25)\ntext[2:] # the first two elements are '\u00f7 '","6a86c6b1":"text = generate_text(START, model, temp=0.5)\ntext[2:] # the first two elements are '\u00f7 '","e11deff4":"text = generate_text(START, model, temp=0.75)\ntext[2:] # the first two elements are '\u00f7 '","98fd9b32":"text = generate_text(START, model, temp=1.0)\ntext[2:] # the first two elements are '\u00f7 '","8ce30d46":"## Headline Generation\n\nIn this kernel, we will train a model to model language from a corpus of headlines and then use it to generate text.\n\n* Language Model: Learn to predict the next word given some context.\n* Text Generation: Continuously sample from the model until the end of headline character is reached.","74832720":"After appending the start and end tokens, we are going to tokenize our text using the `keras` built-in libraries. Next, after the tokenization, we need to format our data for training. When generating text, we are given a sequence of words and want to predict the next one in line. This is what we are going to do here: we will create `(context, next_word)` pairs. In our case `context` is a variable-length sequence of words, but it can be n-grams. Given a list of words, we want to predict `next_word`.","a2c7ea85":"## Data Processing\n\nFirst we will read data from a [Reuters dataset](https:\/\/dataverse.harvard.edu\/dataset.xhtml?persistentId=doi:10.7910\/DVN\/XDB74W&version=2.0). We will only keep samples that are shorter than 14 words and to pick samples we are going to shuffle the data and retain the first 200k items. We do this to avoid any bias in our dataset (for example, there may be duplicate values and this technique reduces the chance of picking them).","347fb64d":"The tokenizer is basically a large dictionary of `{word: index}` items. We are going to print indices for some words and tokens (to make sure they are included in the tokenizer).","36297e8c":"After building our tokenizer, we will store it in case we need it in other kernels.","3d6dda05":"We now need to convert the textual data into a format appropriate for training (ie. a vector).\n\nFirst though, we need to take a small step back and think about the generation process as a whole. We want to be able to generate text over and over again until... when? Maybe when we generate 10 words, we call it quits. Would that be desirable though? No, since most sentences\/titles are not just ten words; some have more words, some fewer. Also, stopping generation after a fixed number of words may result in text that is abruptly cut. To avoid these problems, we are going to ask the model when to stop generation. How? We will add a stop token and the model will learn to predict it like any other word. So, we are going to add a special end character at the end of each training item. We will also add another special character at the start of each item, to denote the beginning of each sentence.\n\nThese special characters are simply '\u00f7' and '\u25a0' (start\/end respectively).","c0c0ea13":"We can now start generating text, using the starting symbol as input.","3841a492":"## Model Building\n\nNow we are ready to build our model. As usual, we will first pass the input through an embedding layer. On these embeddings, a bidirectional LSTM layer will operate and will produce an output for the Dense classifier. The output of this final Dense layer is the probability distribution of all words.","0ac4f59b":"## Text Generation\n\nWith the model at hand, we are ready to start generating text. We are going to feed the starting character, '\u00f7', to the model and then continuously sample and input generated text until we reach the end character, '\u25a0'.\n\nIn our sampling function, we are going to adjust the softmax probabilities by temperature.","3c8694bd":"In the generation function, we are given an input text. We will tokenize this input and pad it so that it is in the correct form. Then, we will feed the tokenized input into the model to compute the probability distribution of next words. We will sample from this distribution and add the selected word to the generated sentence. Then, we will feed the generated sentence in its whole to the network and generate the next word (alternatively, we can only feed part of the generated sentence). We repeat this proess until we generate the end token ('\u25a0').","8ad9149c":"Let's tokenize our data!"}}