{"cell_type":{"33643568":"code","1d3d71e4":"code","62b827e4":"code","0179812c":"code","cc6ac9c1":"code","7e644101":"code","5d7c134a":"code","dcac5356":"code","e3d7619a":"code","005432d7":"code","b8f4fe93":"code","9aa8f45a":"code","5cdf32d5":"code","ed47fab5":"code","9eb6b136":"code","4fc63b80":"code","89eade07":"code","a8e6c4de":"code","170bfc51":"code","2756c4b8":"code","a4b49b54":"code","e9744a66":"code","b74bbdc1":"code","36e902b4":"code","2cf69e0b":"code","c41f12c4":"code","fff8515b":"code","e321e365":"code","c9c5e42a":"code","922563e6":"code","97ad85ea":"code","bfd9e0e5":"markdown"},"source":{"33643568":"import os\nfrom tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\npd.set_option('display.float_format', '{:.3f}'.format)\nx_min, x_max = -1., 1.\ny_min, y_max = -.3, .3","1d3d71e4":"trn_index_list = np.load(\"..\/input\/eyn-folds\/trn_index_list_10f.npy\", allow_pickle=True)\nval_index_list = np.load(\"..\/input\/eyn-folds\/val_index_list_10f.npy\", allow_pickle=True)\nprint([np.shape(trn_index_list[i]) for i in range(np.shape(trn_index_list)[0])])\nprint([np.shape(val_index_list[i]) for i in range(np.shape(val_index_list)[0])])\ntrain_targets_inside = np.load(\"..\/input\/eyn-original\/train_targets_inside.npy\")","62b827e4":"df_train = pd.read_pickle(\"..\/input\/eyn-pre-unravel-df\/df_train.pickle\")\ndf_test = pd.read_pickle(\"..\/input\/eyn-pre-unravel-df\/df_test.pickle\")\nprint(df_train.shape, df_test.shape)\ndf_train.head(7)","0179812c":"df_train_cluster = pd.read_pickle(\"..\/input\/eyn-pre-cluster-calc\/df_train_cluster_s.pickle\")\ndf_test_cluster = pd.read_pickle(\"..\/input\/eyn-pre-cluster-calc\/df_test_cluster_s.pickle\")\nprint(df_train_cluster.shape, df_test_cluster.shape)\ndf_train_cluster = df_train_cluster.astype(float)\ndf_test_cluster = df_test_cluster.astype(float)\ndf_train_cluster.head(7)","cc6ac9c1":"df_train = pd.concat([df_train, df_train_cluster], ignore_index=False, axis=1, sort=True)\ndf_test = pd.concat([df_test, df_test_cluster], ignore_index=False, axis=1, sort=True)\nprint(df_train.shape)\ndf_train.head(7)","7e644101":"# not using the individual data points\n# df_train_cluster = pd.read_pickle(\"..\/input\/eyn-pre-cluster-calc\/df_train_cluster_w.pickle\")\n# df_test_cluster = pd.read_pickle(\"..\/input\/eyn-pre-cluster-calc\/df_test_cluster_w.pickle\")\n# print(df_train_cluster.shape, df_test_cluster.shape)\n# df_train_cluster = df_train_cluster.astype(float)\n# df_test_cluster = df_test_cluster.astype(float)\n# df_train_cluster.head(7)","5d7c134a":"# df_train = pd.concat([df_train, df_train_cluster], ignore_index=False, axis=1, sort=True)\n# df_test = pd.concat([df_test, df_test_cluster], ignore_index=False, axis=1, sort=True)\n# print(df_train.shape)\n# df_train.head(7)","dcac5356":"import math\ntime_dummy_train = []\nfor i in df_train[\"t_entry\"]:\n    entry = np.repeat(0, 16)\n    if math.isnan(i):\n        time_dummy_train.append([np.nan]*16)\n    else:\n        num = int((i + 1.5) \/\/ 0.1)\n        if num != 0:\n            entry[num-1] = 1\n        time_dummy_train.append(entry)\n        \nimport math\ntime_dummy_test = []\nfor i in df_test[\"t_entry\"]:\n    entry = np.repeat(0, 16)\n    if math.isnan(i):\n        time_dummy_test.append([np.nan]*16)\n    else:\n        num = int((i + 1.5) \/\/ 0.1)\n        if num != 0:\n            entry[num-1] = 1\n        time_dummy_test.append(entry)","e3d7619a":"time_dummy_train = np.array(time_dummy_train)\ntime_dummy_test = np.array(time_dummy_test)\n\ntime_dummy_col_name = [\"td{}\".format(i) for i in range(16)]\n\nfor i,col_name in enumerate(time_dummy_col_name):\n    df_train[col_name] = time_dummy_train[:,i]\n    df_test[col_name] = time_dummy_test[:,i]\n    \ndf_train[time_dummy_col_name] = df_train[time_dummy_col_name].astype('category')\ndf_test[time_dummy_col_name] = df_test[time_dummy_col_name].astype('category')\ndf_train.head(7)","005432d7":"cat_col = ['entry_in', 'exit_in', 'tid_0', 'tid_1']\ndf_train[cat_col] = df_train[cat_col].astype('category')\ndf_test[cat_col] = df_test[cat_col].astype('category')\n# cluster targets has been made categorical in eyn-pre-cluster-unif, but can be ensured categorical again here","b8f4fe93":"drop_col = ['x_exit_0', 'y_exit_0', 'vmean_0', 'vmax_0', 'vmin_0', \n             'exit_in_0', 'dist_0', 'speed_0', 'dir_x_0', 'dir_y_0']\n\ndf_columns = df_train.columns\nflatten = lambda l: [item for sublist in l for item in sublist]\ndrop_col += flatten([[name + \"_\" + str(i) for name in df_columns] for i in range(15,21)])\n# drop_col = []  # not dropping any columns, if you choose","9aa8f45a":"df_train_unstack = df_train.unstack()\nunstack_col_names = [\"_\".join([tup[0],str(tup[1])]) for tup in df_train_unstack.columns.values]\ndf_train_unstack.columns = unstack_col_names\ndf_train_unstack = df_train_unstack.drop(drop_col, axis=1)\ndf_train_unstack.head()","5cdf32d5":"df_test_unstack = df_test.unstack()\nunstack_col_names = [\"_\".join([tup[0],str(tup[1])]) for tup in df_test_unstack.columns.values]\ndf_test_unstack.columns = unstack_col_names\ndf_test_unstack = df_test_unstack.drop(drop_col, axis=1)\ndf_test_unstack.head()","ed47fab5":"trn_embedding = np.load(\"..\/input\/eynembedding\/train_lstm.npy\")\ntest_embedding = np.load(\"..\/input\/eynembedding\/test_lstm.npy\")\nfor e in range(len(trn_embedding[0])):\n    df_train_unstack[\"e{}\".format(e)] = trn_embedding[:,e]\n    df_test_unstack[\"e{}\".format(e)] = test_embedding[:,e]\ndf_train_unstack.head()","9eb6b136":"# coding: utf-8\n# pylint: disable = invalid-name, C0111\nimport lightgbm as lgb\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, log_loss\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n\ndef scoring_package(y_true, y_pred, plotting=False):\n    threshold_search = np.arange(0, 1., 0.01)\n    f1_arr = [f1_score(y_true, [k > threshold for k in y_pred]) for threshold in threshold_search]\n    precision_arr = [precision_score(y_true, [k > threshold for k in y_pred]) for threshold in threshold_search]\n    recall_arr = [recall_score(y_true, [k > threshold for k in y_pred]) for threshold in threshold_search]\n    \n    if plotting:\n        plt.figure(figsize=(24,3))\n        plt.plot(threshold_search, f1_arr)\n        plt.plot(threshold_search, precision_arr)\n        plt.plot(threshold_search, recall_arr)\n        plt.show()\n    \n    threshold = threshold_search[np.argmax(f1_arr)]\n    y_pred_class = np.array([k > threshold for k in y_pred])\n    f1 = f1_score(y_true, y_pred_class)\n    precision = precision_score(y_true, y_pred_class)\n    recall = recall_score(y_true, y_pred_class)\n    roc = roc_auc_score(y_true, y_pred)\n    return f1, precision, recall, roc, threshold","4fc63b80":"df_train_values = df_train.values\ndf_train_columns = list(df_train.columns)\ndf_test_values = df_test.values\ndf_test_columns = list(df_test.columns)\ntrain_last_is_stationary = np.argwhere(df_train_values[::21,df_train_columns.index(\"dur\")] == 0)[:,0]\ntrain_last_not_stationary = np.argwhere(df_train_values[::21,df_train_columns.index(\"dur\")] != 0)[:,0]\ntest_last_is_stationary = np.argwhere(df_test_values[::21,df_test_columns.index(\"dur\")] == 0)[:,0]\ntest_last_not_stationary = np.argwhere(df_test_values[::21,df_test_columns.index(\"dur\")] != 0)[:,0]\nprint(train_last_is_stationary.shape, train_last_not_stationary.shape)\nprint(test_last_is_stationary.shape, test_last_not_stationary.shape)\n\ntrain_last_seen_is_inside = np.argwhere(df_train_values[::21,df_train_columns.index(\"entry_in\")] == 1)[:,0]\ntrain_last_seen_not_inside = np.argwhere(df_train_values[::21,df_train_columns.index(\"entry_in\")] == 0)[:,0]\ntest_last_seen_is_inside = np.argwhere(df_test_values[::21,df_test_columns.index(\"entry_in\")] == 1)[:,0]\ntest_last_seen_not_inside = np.argwhere(df_test_values[::21,df_test_columns.index(\"entry_in\")] == 0)[:,0]\nprint(train_last_seen_is_inside.shape, train_last_seen_not_inside.shape)\nprint(test_last_seen_is_inside.shape, test_last_seen_not_inside.shape)\n\ny_pred_full = np.zeros(np.shape(train_targets_inside))\ny_pred_full[np.intersect1d(train_last_is_stationary, train_last_seen_is_inside)] = 1\ny_pred_full[np.intersect1d(train_last_is_stationary, train_last_seen_not_inside)] = 0\n# print('The F1-PC-RC-ROC of full prediction is: {:.5f}-{:.5f}-{:.5f}-{:.5f} at threshold {:.3f}'\n#       .format(*scoring_package(train_targets_inside, y_pred_full)))\ntest_preds = []","89eade07":"# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'num_leaves': 63,\n    'learning_rate': 0.05,  # dynamic one below\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.9,\n    'bagging_freq': 5,\n    'verbose': 0,\n    'lambda': 0.1,\n    'num_threads': 4,\n    'seed' : 42,\n#     'histogram_pool_size' : 2048  # to restrict memory usage\n#     'boost_from_average' : False  # as per warning message\n}\nnum_boost_round = 3000","a8e6c4de":"# inputs: train_pd, train_targets_inside, and test_pd\n# probably try https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html#lightgbm.cv, but cluster complications\nfor fold_num, (trn_index, val_index) in enumerate(zip(trn_index_list, val_index_list)):\n    print(\"Training fold {}, hash: {}\".format(fold_num, np.sum(val_index)%999))\n    \n    x_trn = df_train_unstack.iloc[trn_index]\n    x_val = df_train_unstack.iloc[val_index]\n    y_trn = pd.DataFrame(train_targets_inside).astype('bool').iloc[trn_index]\n    y_val = pd.DataFrame(train_targets_inside).astype('bool').iloc[val_index]\n    \n    # create dataset for lightgbm\n    lgb_train = lgb.Dataset(x_trn, y_trn)\n    lgb_eval = lgb.Dataset(x_val, y_val, reference=lgb_train)\n    \n    gbm = lgb.train(params,\n                    lgb_train,\n                    valid_sets=[lgb_train, lgb_eval],\n                    num_boost_round=num_boost_round,\n                    early_stopping_rounds=25,\n                    learning_rates=lambda iter: 0.1 * (0.995 ** iter),\n                    verbose_eval=50)\n\n    # eval\n    y_pred = gbm.predict(x_val, num_iteration=gbm.best_iteration)\n    print('The F1-PC-RC-ROC score of fold {} is: {:.5f}-{:.5f}-{:.5f}-{:.5f} at threshold {:.3f}'\n          .format(fold_num, *scoring_package(y_val, y_pred)))\n    y_pred_full[val_index] = y_pred\n    \n    # testing\n    test_pred = gbm.predict(df_test_unstack, num_iteration=gbm.best_iteration)\n    test_preds.append(test_pred)","170bfc51":"scoring_results = scoring_package(train_targets_inside, y_pred_full, plotting=True)\nprint('The F1-PC-RC-ROC of full prediction is: {:.5f}-{:.5f}-{:.5f}-{:.5f} at threshold {:.3f}'\n      .format(*scoring_results))","2756c4b8":"plt.figure(figsize = (30,5))\nfor i in range(gbm.num_trees())[:2]:\n    plt.plot([gbm.get_leaf_output(i,j) for j in range(gbm.params['num_leaves'])])","a4b49b54":"# very blur cannot see, don't know how to plot just a small part of it\n# lags when asked to plot everything\n# ax = lgb.plot_tree(gbm, tree_index=0, figsize=(30, 10), \n#               show_info=['split_gain', 'internal_value', 'internal_count', 'leaf_count'])\n\n# doesn't work on Kaggle\n# graph = lgb.create_tree_digraph(gbm, tree_index=0, name='Tree54')\n# graph.render(view=True)","e9744a66":"num_features = len(gbm.feature_importance())\nplt.figure(figsize=(100,3))\nplt.bar(np.arange(num_features), gbm.feature_importance(importance_type='split'), align='center', width=0.4)\nplt.xticks(np.arange(0,num_features,21), gbm.feature_name()[::21], rotation='vertical', fontsize=20)\nplt.title('Feature Importance - Split')\nplt.yscale('log') \nplt.show()","b74bbdc1":"plt.figure(figsize=(100,3))\nplt.bar(np.arange(num_features), gbm.feature_importance(importance_type='gain'), align='center', width=0.4)\nplt.xticks(np.arange(0,num_features,21), gbm.feature_name()[::21], rotation='vertical', fontsize=20)\nplt.title('Feature Importance - Gain')\nplt.yscale('log') \nplt.show()","36e902b4":"print([i[2] for i in test_preds])\nprint([i[-4] for i in test_preds])\ntest_preds","2cf69e0b":"np.save(\"train_preds\", y_pred_full)\nnp.save(\"test_preds\", test_preds)","c41f12c4":"test_preds_mean = np.mean(test_preds, axis=0)\ntest_preds_mean = np.array([1 if pred>scoring_results[-1] else 0 for pred in test_preds_mean])\nprint(np.sum(test_preds_mean))","fff8515b":"test_preds_mean[np.intersect1d(test_last_is_stationary, test_last_seen_is_inside)] = 1.\ntest_preds_mean[np.intersect1d(test_last_is_stationary, test_last_seen_not_inside)] = 0.\nprint(np.sum(test_preds_mean))","e321e365":"df_submit = pd.read_csv(\"..\/input\/ey-nextwave\/data_test\/data_test.csv\")\ndf_submit = df_submit[df_submit['x_exit'].isnull()]\ndf_submit = df_submit[['trajectory_id']].copy()\ndf_submit = df_submit.rename(columns = {'trajectory_id':'id'})\ndf_submit['target'] = test_preds_mean\ndf_submit.to_csv('submission.csv', index=False)","c9c5e42a":"df_submit = pd.read_csv(\"submission.csv\")\ndf_submit.head()","922563e6":"df_submit.tail()","97ad85ea":"print(scoring_results)\nprint(params)","bfd9e0e5":"# Submission"}}