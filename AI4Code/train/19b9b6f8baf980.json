{"cell_type":{"0e843bda":"code","14444745":"code","6cfa483c":"code","92f2da81":"code","713df0e8":"code","05eea634":"code","28de682e":"code","cda08a55":"code","3640c740":"code","8dd62e5b":"code","6f2aacd1":"code","c142393e":"code","14841055":"code","c6d8a11f":"code","ce86a58c":"code","a562a87c":"code","42ca77ee":"code","b423d919":"code","4c792457":"code","8e2f8034":"code","ec747d7e":"code","702a6c7f":"code","8a9a86fa":"code","eaff7841":"code","39001ac0":"code","c5fded09":"code","b07df517":"code","d861a547":"code","50bf714d":"code","9db20541":"code","ee182a22":"code","516a5e03":"code","a08df0a7":"code","29c31676":"code","8294ccfa":"code","5983b45d":"code","153dfedc":"code","af36ff22":"code","cdf0f46c":"code","172a31f1":"code","c9000874":"code","40755524":"code","eb7aaf2a":"code","5df2fc28":"markdown","42d816ac":"markdown","c05c48d7":"markdown","37831539":"markdown","7bf387b4":"markdown","2432a548":"markdown","5e3572d6":"markdown","7fbd8881":"markdown","da13d79a":"markdown","dd3002ea":"markdown","7126a47c":"markdown","31f7a487":"markdown","a387a601":"markdown","efc0b3d2":"markdown","0a85f6b3":"markdown","35bb71e8":"markdown","b9d4d2af":"markdown","f3a4e596":"markdown","91bbda5c":"markdown"},"source":{"0e843bda":"from datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","14444745":"# We're trying to predict the last column target based on the attributes of \n# the patient. 0 = No heart disease, 1 = Heart Disease present\n#df = pd.read_csv('datasets\/heart.csv')\ndf = pd.read_csv('..\/input\/exercise-datasets\/heart.csv')\n\ndf.sample(5)","6cfa483c":"df.shape","92f2da81":"df.isna().sum()","713df0e8":"df.describe().T","05eea634":"# 1 = male, 0 = female\ndf['sex'].value_counts()","28de682e":"# cp = numeric categories 0, 1, 2, and 3 represent some kinds of anginal chest\n# pain and asymptomatic chest pain\n\ndf['cp'].value_counts()","cda08a55":"# Which gender do you think suffers from high incidence of heart disease?\nsns.countplot('sex', hue = 'target', data = df)\n\nplt.title('Heart Disease Frequency for Gender')\nplt.legend([\"No Disease\", \"Yes Disease\"])\n\nplt.xlabel('Gender (0 = Female, 1 = Male)')\nplt.ylabel('Frequency')\n\nplt.show()","3640c740":"# Presence or absence of heart disease corresponding to age chart using seaborn\n# Which age group do we observe evidence of heart disease?\n\nplt.figure(figsize = (20, 8))\nsns.countplot('age', hue = 'target', data = df)\n\nplt.title('Heart Disease Frequency for Age')\nplt.legend([\"No Disease\", \"Yes Disease\"])\n\nplt.xlabel('Age')\nplt.ylabel('Frequency')\n\nplt.show()","8dd62e5b":"# Do younger patients have lower or higher cholestrol levels?\n\nplt.figure(figsize = (10, 8))\n\nplt.scatter(df['age'], df['chol'], s = 200)\n\nplt.xlabel('Age', fontsize = 20)\nplt.ylabel('Cholestrol', fontsize = 20)\nplt.show()","6f2aacd1":"# We drop the target column\nfeatures = df.drop('target', axis=1)\n\ntarget = df[['target']]","c142393e":"features.sample(5)","14841055":"target.sample(10)","c6d8a11f":"categorical_features = features[['sex', 'fbs', 'exang', 'cp', 'ca', 'slope', 'thal', 'restecg']].copy()\n\ncategorical_features.head()","ce86a58c":"# Looking at the numeric features. We'll drop all of the categorical features and you're left with numeric features. \n\nnumeric_features = features[['age', 'trestbps', 'chol', 'thalach', 'oldpeak']].copy()\n\nnumeric_features.head()","a562a87c":"standardScaler = StandardScaler()\n\nnumeric_features = pd.DataFrame(standardScaler.fit_transform(numeric_features), \n                                columns=numeric_features.columns,\n                                index=numeric_features.index)\n\nnumeric_features.describe()","42ca77ee":"# We place all of our features together into a single data frame called \n# processed_features. This contains the process numeric features and our categorical features. \n\nprocessed_features = pd.concat([numeric_features, categorical_features], axis=1,\n                               sort=False)\n\nprocessed_features.head()","b423d919":"# split our dataset into training data and test data using train_test_split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(processed_features, \n                                                    target, \n                                                    test_size = 0.2, \n                                                    random_state=1)","4c792457":"x_train.shape, y_train.shape","8e2f8034":"x_test.shape, y_test.shape","ec747d7e":"# further split the training data into training data and validation data. Once this is done, we have our datasets set up. \n# We'll use 205 records to train our model\n# 37 records to validate our model\n# 61 records to test our model\n\nx_train, x_val, y_train, y_val = train_test_split(x_train, \n                                                  y_train, \n                                                  test_size=0.15,\n                                                  random_state=10)","702a6c7f":"x_train.shape, x_val.shape, x_test.shape","8a9a86fa":"y_train.shape, y_val.shape, y_test.shape","eaff7841":"def build_model():\n    \n    # shape of the input is equal to the number of features in our training data, \n    # we get using x_train.shape of 1\n    inputs = tf.keras.Input(shape=(x_train.shape[1],))\n    \n    # first layer is a dense layer with 12 neurons and relu activation\n    dense_layer1 = layers.Dense(12, activation='relu')\n    \n    # pass the input layer into the dense layer by invoking dense_layer1, x = dense_layer1(inputs)\n    x = dense_layer1(inputs)\n    \n    # Using dropout of 30% Dropout is used with our neural network models to \n    # mitigate the effects of overfitting on the training data\n    dropout_layer = layers.Dropout(0.3)\n    \n    # Which means in a epoch of training, this dropout_layer will turn out or off 30% of the neurons in our \n    # first dense_layer, forcing the other neurons to learn more from the underlying data\n    x = dropout_layer(x)\n    \n    #  instantiate our second dense_layer, dense_layer2, with 8 neurons and relu activation. \n    # We then invoke this dense_layer2 and pass in x, the outputs from the previous layer.\n    dense_layer2 = layers.Dense(8, activation='relu')\n    x = dense_layer2(x)\n\n    # Last prediction layer is sigmoid activation layer with just one neuron. \n    # The output of our classification model is a probability score\n    predictions_layer = layers.Dense(1, activation='sigmoid')\n    predictions = predictions_layer(x)\n    \n    # instantiate tf.keras.Model, specify the input layer and the output predictions layer. \n    # All of the remaining layers have been invoked on the inputs passed in \n    # We will receive the output at the predictions layer\n    model = tf.keras.Model(inputs=inputs, outputs=predictions)\n    \n    # Print out model summary\n    model.summary()\n    \n    # compile this model using the Adam optimizer and a learning rate of 0.001. \n    # The loss function that we'll use here is the BinaryCrossentropy loss. \n    # Our classification model is a binary classifier. We classify into one of two categories, \n    # whether the person has heart disease or not. \n    # Using the BinaryCrossentropy as the loss function\n    model.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=['accuracy', \n                           tf.keras.metrics.Precision(0.5),\n                           tf.keras.metrics.Recall(0.5),])\n    \n    # metrics that we'll track will be the accuracy, precision, and recall of our model\n    return model","39001ac0":"model = build_model()","c5fded09":"# If you run this on your local env, you need pydot (graphviz) to show this\nkeras.utils.plot_model(model, show_shapes=True)","b07df517":"dataset_train = tf.data.Dataset.from_tensor_slices((x_train.values, y_train.values))\n\n# You can also use the dataset API to specify a batch size for the training process\ndataset_train = dataset_train.batch(16)\n\ndataset_train.shuffle(128)","d861a547":"num_epochs = 100","50bf714d":"# Setup dataset for valuation data\n\ndataset_val = tf.data.Dataset.from_tensor_slices((x_val.values, y_val.values))\ndataset_val = dataset_val.batch(16)","9db20541":"model = build_model()\n\ntraining_history = model.fit(dataset_train, epochs=num_epochs, validation_data=dataset_val)","ee182a22":"training_history.history.keys()","516a5e03":"train_acc = training_history.history['accuracy']\ntrain_loss = training_history.history['loss']\n\nprecision = training_history.history['precision_1']\nrecall = training_history.history['recall_1']\n\nepochs_range = range(num_epochs)\n\nplt.figure(figsize=(14, 8))\n\nplt.subplot(1, 2, 1)\n\nplt.plot(epochs_range, train_acc, label='Training Accuracy')\nplt.plot(epochs_range, train_loss, label='Training Loss')\n\nplt.title('Accuracy and Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\n\nplt.plot(epochs_range, precision, label='Precision')\nplt.plot(epochs_range, recall, label='Recall')\n\nplt.title('Precision and Recall')\nplt.legend()","a08df0a7":"score = model.evaluate(x_test, y_test)\n\nscore_df = pd.Series(score, index = model.metrics_names)\n\nscore_df","29c31676":"y_pred = model.predict(x_test)\n\ny_pred[:10]","8294ccfa":"y_pred = np.where(y_pred>=0.5, 1, y_pred)\n\ny_pred = np.where(y_pred<0.5, 0, y_pred)","5983b45d":"y_pred[:10]","153dfedc":"pred_results = pd.DataFrame({'y_test': y_test.values.flatten(),\n                             'y_pred': y_pred.flatten().astype('int32') }, index = range(len(y_pred)))","af36ff22":"pred_results.sample(10)","cdf0f46c":"pd.crosstab(pred_results.y_pred, pred_results.y_test)","172a31f1":"accuracy_score(y_test, y_pred)","c9000874":"precision_score(y_test, y_pred)","40755524":"recall_score(y_test, y_pred)","eb7aaf2a":"#End of Code","5df2fc28":"### Splitting dataset into training and testing data","42d816ac":"### Splitting the data","c05c48d7":"### Heart Disease classification\n\nPurpose: Build models with more complex topologies, using the Keras functional API.\n\nThe original source of this heart disease dataset is the University of California Irvine's machine learning repository - https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+Disease\n14 columns\n303 records\n","37831539":"Dataset : https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci","7bf387b4":"#### 1) Sex","2432a548":"### Plotting accuracy","5e3572d6":"#### 2) Age","7fbd8881":"### Understanding the dataset","da13d79a":"### Building the model","dd3002ea":"### Standardization of our values\n\nWe preprocess our features by standardizing their values. \n\nWe'll instantiate the standardScalar and call fit_transform on the numeric features. \n\nStandardization for each feature subtracts the mean from every value and divides by the standard deviation for that feature, expressing the data in terms of z\u2011scores or number of standard deviations away from the mean","7126a47c":"### Functional API\n\nWe're now ready to build our classification model using the functional API, using the build_model helper function. \n\nFirst define inputs to our model using the tf.keras.Input layer","31f7a487":"### Model evaluation","a387a601":"### Plot Model To See Individual Layers\n\nview the layers in this model using the plot_model utility that Keras offers. \n\nFor every layer, you can view the shape of the input and the shape of the output. \n\nThe final layer, which has an output with exactly one value, will output a probability score, prediction of whether an individual has heart disease or not","efc0b3d2":"### Data Pipelines in TF2.0\n\nIn TF2, we use tf.data.Dataset API to build pipelines to transform your data and to feed your data into your ML model for training, evaluation, or prediction","0a85f6b3":"### Data VIsualization","35bb71e8":"The statistical summary of our numeric features above shows that all features now have a mean very close to 0, and a standard deviation very close to 1","b9d4d2af":"### Importing Libraries","f3a4e596":"Columns:\n    - age: age in years\n    - sex: (1 = male; 0 = female)\n    - cp: chest pain type\n    - trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n    - chol: serum cholestoral in mg\/dl\n    - fbs: (fasting blood sugar &gt; 120 mg\/dl) (1 = true; 0 = false)\n    - restecg: resting electrocardiographic results\n    - thalach: maximum heart rate achieved\n    - exang: exercise induced angina (1 = yes; 0 = no)\n    - oldpeak: ST depression induced by exercise relative to rest\n    - slope: the slope of the peak exercise ST segment\n    - ca: number of major vessels (0-3) colored by flourosopy\n    - thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","91bbda5c":"### Prediction"}}