{"cell_type":{"18427692":"code","8b5559e8":"code","17a609b2":"code","81ffbb7c":"code","a58fa36e":"code","31a36d1d":"code","d323c8e3":"code","af044409":"code","9d149f02":"code","c42c7274":"code","fa84aed7":"code","09053273":"code","881923cf":"code","aec13943":"code","296135d5":"code","f18e14ab":"code","5b5bed75":"code","7ab57ea6":"code","e784bb43":"code","b3b13109":"code","894187d9":"code","3b66f28e":"code","51d2df92":"code","7589e7eb":"code","be037ec2":"code","ebb2086b":"code","9a66f8c1":"code","60517877":"markdown","7f6cd89d":"markdown","c5717d81":"markdown","8286d6dc":"markdown","a091cae4":"markdown","1590fb78":"markdown","0ecd77c4":"markdown","52d8f850":"markdown","c5bbe836":"markdown"},"source":{"18427692":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b5559e8":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport glob\nimport os\nimport re\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer  ","17a609b2":"\ndf_train = pd.read_csv('\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train.csv')\ndf_train.shape","81ffbb7c":"df_train.groupby(by='label').count()","a58fa36e":"df_train.head()","31a36d1d":"#read images as grayscale as colors aren't actually effect in our example\nimages_train = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/train\/*.png\"))]\nimages_train = np.array(images_train)\nprint(\"train images shape: {}\".format(images_train.shape))","d323c8e3":"# add 1 more dimenision for images train to prepare for CNN\nimages_train = images_train.reshape((-1, 32, 32, 1))\nprint(\"images shape: {}\".format(images_train.shape))\n\n#preprocessing images train\nimages_train = images_train\/255","af044409":"\n# show first 50 images\nplt.figure(figsize=(20,10))\nfor i in range(50):\n    plt.subplot(5,10,i+1)\n    plt.imshow(images_train[i], cmap='gray')\n    plt.axis('off')\n    plt.title('label : {}'.format(df_train.label.iloc[i]))","9d149f02":"\n#prepare y to CNN we use labelBinarizer\nbinencoder = LabelBinarizer()\ny = binencoder.fit_transform(df_train.label.to_numpy())\nprint(\"y shape: {}\".format(y.shape))\nprint(y[0:5])","c42c7274":"\nX_train, X_val, y_train, y_val = train_test_split(images_train, y, test_size = 0.2, random_state=42, stratify= y)\nprint(\"X_train shape: {}\\nX_val shape: {}\".format(X_train.shape, X_val.shape))","fa84aed7":"#checking if strartify is working right\nprint(y_train.sum(axis=0))\nprint(y_val.sum(axis=0))","09053273":"# Train with ALL DATA to increase accuracy after it has been validated.\n\n# X_train = images_train\n# y_train = y","881923cf":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","aec13943":"import tensorflow as tf\nfrom tensorflow.python.keras import backend as K\n\n# adjust values to your needs\nconfig = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 8} )\nsess = tf.compat.v1.Session(config=config) \nK.set_session(sess)","296135d5":"# model = tf.keras.models.Sequential([\n#     tf.keras.layers.Conv2D(filters=16, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same', input_shape=(32, 32, 3)),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n\n#     tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same'),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same'),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu', kernel_initializer='he_normal', padding='same'),\n#     tf.keras.layers.MaxPooling2D(pool_size=2),\n    \n#     tf.keras.layers.GlobalAveragePooling2D(),\n#     tf.keras.layers.Dense(29, activation='softmax')\n \n# ])\n\n# Total params: 101,181\n# Trainable params: 101,181\n# Non-trainable params: 0","f18e14ab":"# we can change initializer weights types and see about kernal size\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=16,kernel_size=3,input_shape=(32,32,1),padding ='same', activation='relu',kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(Conv2D(filters=32,kernel_size=3,padding ='same',activation='relu',kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=64,kernel_size=3,padding ='same',activation='relu',kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters=128,kernel_size=3,padding ='same',activation='relu',kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Flatten())\nmodel.add(Dense(128,activation='tanh', kernel_initializer='he_uniform',kernel_regularizer='l2'))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Dense(28, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n# __________________________\n#(16->128)(5,5)(tanh)\n# Total params: 168,156\n# Trainable params: 167,420\n# Non-trainable params: 736","5b5bed75":"model.summary()","7ab57ea6":"# We will import a call back to save the best epoch's weights\nfrom tensorflow.keras.callbacks import ModelCheckpoint                                     \n\ncheckpointer = ModelCheckpoint(filepath='weights.hdf5', verbose=1, monitor='val_accuracy', mode='max', save_best_only=True)\n\nearltstopping = EarlyStopping(monitor='val_accurracy',patience=7, min_delta=0.001)\nhist = model.fit(X_train, y_train, validation_split=0.2, epochs=500, batch_size=20, callbacks=[checkpointer,earltstopping])","e784bb43":"# Loading the best weights\nmodel.load_weights('weights.hdf5')   \n# Evaluating our model\nmodel.evaluate(X_val, y_val)","b3b13109":"y_pred = model.predict(X_val)\n\npred = np.argmax(y_pred, axis=1) + 1 \nground = np.argmax(y_val, axis=1) + 1\n\nprint(classification_report(ground,pred))","894187d9":"get_acc = hist.history['accuracy']\nvalue_acc = hist.history['val_accuracy']\nget_loss = hist.history['loss']\nvalidation_loss = hist.history['val_loss']\n\n\nepochs = range(len(get_acc))\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation Loss')\nplt.legend()\n\nplt.show()","3b66f28e":"\n#read images as grayscale as colors aren't actually effect in our example\nimages_test = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob(\"\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/test\/*.png\"))]\nimages_test = np.array(images_test)\n\nprint(\"train images shape: {}\".format(images_test.shape))","51d2df92":"# add 1 more dimenision for images train to prepare for CNN\nimages_test = images_test.reshape((-1, 32, 32, 1))\nprint(\"images shape: {}\".format(images_test.shape))\n\n#preprocessing images train\nimages_test = images_test\/255","7589e7eb":"imagesName_test = [re.sub(r'\\D', \"\",os.path.basename(file)) for file in sorted(glob.glob(\"\/kaggle\/input\/arabic-hwr-ai-pro-intake1\/test\/*.png\"))]\ndf_test = pd.DataFrame(imagesName_test,columns=[\"id\"])\n","be037ec2":"\ny_pred_test = model.predict(images_test)\ndf_test[\"label\"] = np.argmax(y_pred_test, axis=1) + 1\n\ndf_test.head()","ebb2086b":"# show first 50 images\nplt.figure(figsize=(20,10))\nfor i in range(50):\n    plt.subplot(5,10,i+1)\n    plt.imshow(images_test[i], cmap='gray')\n    plt.axis('off')\n    plt.title('label : {}'.format(df_test.label.iloc[i]))","9a66f8c1":"df_test[['id', 'label']].to_csv('submission.csv', index=False)","60517877":"## \u26a1 Data Splitting:","7f6cd89d":"## \u26a1 Model evaluation:","c5717d81":"## \u26a1 Read the test data:","8286d6dc":"## \u26a1 Submitting csv file:","a091cae4":"## \u26a1 Import main libraries necessary for this project:","1590fb78":"## \u26a1 CNN Model Training:","0ecd77c4":"## \u26a1 Data Preprocessing:","52d8f850":"## \u26a1 Exploratory Data Analysis:","c5bbe836":"## \u26a1 Read the train data:"}}