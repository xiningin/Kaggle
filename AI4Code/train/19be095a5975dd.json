{"cell_type":{"30223da3":"code","b3f91439":"code","a3582fbb":"code","8e81d31a":"code","755d2548":"code","e4ab1024":"code","93dd6a5b":"code","87d14cf9":"code","13e0014d":"code","81878a5a":"code","0696f35f":"code","42f94d32":"code","93d949c7":"code","74faa2ed":"code","fca11392":"code","6b2c6471":"code","8d94e414":"code","9d921554":"code","24b46fae":"code","85c2bd40":"code","600fbe1b":"code","a63f5b1f":"code","110cc4b0":"code","73d4a932":"markdown","be44fea2":"markdown","9f31f038":"markdown","bb08aafe":"markdown","36a78b51":"markdown","1a1fa732":"markdown"},"source":{"30223da3":"# load some default Python modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline\nplt.style.use('seaborn-whitegrid')","b3f91439":"# read data in pandas dataframe\ndf_train =  pd.read_csv('..\/input\/train.csv', nrows = 50_000, parse_dates=[\"pickup_datetime\"])","a3582fbb":"# define bounding box\nBB = (-75, -72.9, 40, 41.8)\n\n# this function will be used with the test set below\ndef select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\n            \n# This function is based on https:\/\/stackoverflow.com\/questions\/27928\/\n# calculate-distance-between-two-latitude-longitude-points-haversine-formula \n# Returns distance in miles\ndef distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295 # Pi\/180\n    a = 0.5 - np.cos((lat2 - lat1) * p)\/2 + np.cos(lat1 * p) * np.cos(lat2 * p) * (1 - np.cos((lon2 - lon1) * p)) \/ 2\n    return 0.6213712 * 12742 * np.arcsin(np.sqrt(a)) # 2*R*asin...","8e81d31a":"# add distance in miles\ndf_train['distance_miles'] = distance(df_train.pickup_latitude, df_train.pickup_longitude, \\\n                                      df_train.dropoff_latitude, df_train.dropoff_longitude)\n# add distance to NYC center\nnyc = (-74.0063889, 40.7141667)\ndf_train['distance_to_center'] = distance(nyc[1], nyc[0], df_train.pickup_latitude, df_train.pickup_longitude)\n# add year\ndf_train['year'] = df_train.pickup_datetime.apply(lambda t: t.year)\n# add hour\ndf_train['hour'] = df_train.pickup_datetime.apply(lambda t: t.hour)\n# add weekday 0:monday, 6:sunday\ndf_train['weekday'] = df_train.pickup_datetime.apply(lambda t: t.weekday())","755d2548":"print('Old size: %d' % len(df_train))\n# remove non-zero fare\ndf_train = df_train[df_train.fare_amount>=0]\n# remove missing data\ndf_train = df_train.dropna(how = 'any', axis = 'rows')\n# remove datapoints outside boundingbox near NYC\ndf_train = df_train[select_within_boundingbox(df_train, BB)]\n# remove datapoints with zero distance traveled\ndf_train = df_train[df_train.distance_miles >= 0]\n# remove datapoints with zero passengers\ndf_train = df_train[df_train.passenger_count > 0]\nprint('New size: %d' % len(df_train))","e4ab1024":"features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n            'passenger_count', 'distance_miles', 'distance_to_center', 'year', 'weekday', 'hour']\nX = df_train[features].values\ny = df_train['fare_amount'].values","93dd6a5b":"# create training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","87d14cf9":"# define some handy analysis support function\nfrom sklearn.metrics import mean_squared_error, explained_variance_score\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, train_test_split\n\ndef calculate_kfold_rmse(model, X, y, nfolds):\n    kf = KFold(n_splits=nfolds, shuffle=False, random_state=None)\n    return np.sqrt(-cross_val_score(model, X, y, cv=kf, scoring=\"neg_mean_squared_error\")).mean()\n\n\ndef plot_prediction_analysis(y, y_pred, figsize=(10,4), title=''):\n    fig, axs = plt.subplots(1, 2, figsize=figsize)\n    axs[0].scatter(y, y_pred)\n    mn = min(np.min(y), np.min(y_pred))\n    mx = max(np.max(y), np.max(y_pred))\n    axs[0].plot([mn, mx], [mn, mx], c='red')\n    axs[0].set_xlabel('$y$')\n    axs[0].set_ylabel('$\\hat{y}$')\n    rmse = np.sqrt(mean_squared_error(y, y_pred))\n    evs = explained_variance_score(y, y_pred)\n    axs[0].set_title('rmse = {:.2f}, evs = {:.2f}'.format(rmse, evs))\n    \n    axs[1].hist(y-y_pred, bins=50)\n    avg = np.mean(y-y_pred)\n    std = np.std(y-y_pred)\n    axs[1].set_xlabel('$y - \\hat{y}$')\n    axs[1].set_title('Histrogram prediction error, $\\mu$ = {:.2f}, $\\sigma$ = {:.2f}'.format(avg, std))\n    \n    if title!='':\n        fig.suptitle(title)\n        \n        \n# some handy function to see how sensitive the model is to the selection\n# of the training and test set\ndef plot_rmse_analysis(model, X, y, N=400, test_size=0.25, figsize=(10,4), title=''):\n    rmse_train, rmse_test = [], []\n    for i in range(N):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n\n        model.fit(X_train, y_train)\n        y_train_pred = model.predict(X_train)\n        y_test_pred = model.predict(X_test)\n\n        rmse_train.append(np.sqrt(mean_squared_error(y_train, y_train_pred)))\n        rmse_test.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n    g = sns.jointplot(np.array(rmse_train), np.array(rmse_test), kind='scatter', stat_func=None, size=5)\n    g.set_axis_labels(\"RMSE training ($\\mu$={:.2f})\".format(np.mean(rmse_train)), \n                      \"RMSE test ($\\mu$={:.2f})\".format(np.mean(rmse_test)))\n    plt.subplots_adjust(top=0.9)\n    g.fig.suptitle('{} (N={}, test_size={:0.2f})'.format(title, N, test_size))\n    \ndef plot_learning_curve(model, X_train, X_test, y_train, y_test, nsteps=1, figsize=(6, 5), title=''):\n    train_error, test_error = [], []\n    number_of_samples = []\n    m_samples = X_train.shape[0]\n    for m in range(int(m_samples\/nsteps), m_samples+1, int(m_samples\/nsteps)):\n        number_of_samples.append(m)\n        model.fit(X_train[:m,:], y_train[:m])\n        y_train_pred = model.predict(X_train[:m,:])\n        train_error.append(np.sqrt(mean_squared_error(y_train[:m], y_train_pred)))\n        y_test_pred = model.predict(X_test)\n        test_error.append(np.sqrt(mean_squared_error(y_test, y_test_pred)))\n    plt.figure(figsize=figsize)\n    plt.plot(number_of_samples, train_error, label='Training data')\n    plt.plot(number_of_samples, test_error, label='Test data')\n    plt.xlabel('Training set size')\n    plt.ylabel('RMSE')\n    plt.legend()\n    if title!='':\n        plt.title(title)","13e0014d":"# prepare python dictionary with models to test\nmodels = {}","81878a5a":"# Add linear regression model\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nmodels['linear_model'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"lin_reg\", LinearRegression()),\n    ))","0696f35f":"# Add linear model with polynomial features. Use Ridge for L2 regularization\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\n\nmodels['polynomial'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),    \n        (\"poly_features\", PolynomialFeatures(degree=2)),\n        (\"ridge\", Ridge()),\n    ))","42f94d32":"# Add KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nmodels['kneighbors'] = Pipeline((\n        (\"standard_scaler\", StandardScaler()),\n        (\"kneighborsregressor\", KNeighborsRegressor()),\n    ))","93d949c7":"# Add RandomForestRegressor with several different parameters\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodels['random_forest_regressor_n10'] = RandomForestRegressor(n_estimators=10, max_depth=10, min_samples_leaf=10)\nmodels['random_forest_regressor_n100'] = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_leaf=10)","74faa2ed":"# Add RandomForestRegressor with several different parameters\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nmodels['gradient_boosting_n10'] = GradientBoostingRegressor(max_depth=2, n_estimators=10, learning_rate=1.0)\nmodels['gradient_boosting_n100'] = GradientBoostingRegressor(max_depth=2, n_estimators=100, learning_rate=0.2)","fca11392":"from xgboost import XGBRegressor\n\nmodels['xgboost10'] = XGBRegressor(n_estimators=10, max_depth=3)\nmodels['xgboost100'] = XGBRegressor(n_estimators=100, max_depth=3)","6b2c6471":"nfolds = 10\nscores = []\nprint(\"Starting evaluating all models: datapoints = {}, nfolds = {}\".format(X.shape[0], nfolds))\nfor name, model in models.items():\n    print('\\n... calculating {} ...'.format(name))\n    %time score = calculate_kfold_rmse(model, X, y, nfolds)\n    scores.append((name, score))\n    \nprint(\"\\n\")\nsorted_scores = sorted(scores, key=lambda x: x[1], reverse=False)\nprint(\"rmsr - model (nfolds={})\".format(nfolds))\nprint(\"============================================\")\nfor r in sorted_scores:\n    print(\"{:0.4f} - {}\".format(r[1], r[0]))","8d94e414":"name_best_model, best_model = sorted_scores[0][0], models[sorted_scores[0][0]]\n\nbest_model.fit(X_train, y_train)\n\ny_train_pred = best_model.predict(X_train)\nplot_prediction_analysis(y_train, y_train_pred, title='{} - Trainingset'.format(name_best_model))\n\ny_test_pred = best_model.predict(X_test)\nplot_prediction_analysis(y_test, y_test_pred, title='{} - Testset'.format(name_best_model))","9d921554":"plot_rmse_analysis(best_model, X, y, N=100)","24b46fae":"plot_learning_curve(best_model, X_train, X_test, y_train, y_test, nsteps=20, title=name_best_model)","85c2bd40":"# read test data\ndf_test =  pd.read_csv('..\/input\/test.csv', parse_dates=[\"pickup_datetime\"])","600fbe1b":"# add distance in km\ndf_test['distance_miles'] = distance(df_test.pickup_latitude, df_test.pickup_longitude, \\\n                                     df_test.dropoff_latitude, df_test.dropoff_longitude)\n# add distance to NYC center\ndf_test['distance_to_center'] = distance(nyc[1], nyc[0], df_test.pickup_latitude, df_test.pickup_longitude)\n# add year\ndf_test['year'] = df_test.pickup_datetime.apply(lambda t: t.year)\n# add hour\ndf_test['hour'] = df_test.pickup_datetime.apply(lambda t: t.hour)\n# add weekday 0:monday, 6:sunday\ndf_test['weekday'] = df_test.pickup_datetime.apply(lambda t: t.weekday())","a63f5b1f":"# define dataset\nXTEST = df_test[features].values\n\nfilename = 'submission_best_model_{}.csv'.format(name_best_model)\n\ny_pred_final = best_model.predict(XTEST)\n\nsubmission = pd.DataFrame(\n    {'key': df_test.key, 'fare_amount': y_pred_final},\n    columns = ['key', 'fare_amount'])\nsubmission.to_csv(filename, index = False)","110cc4b0":"submission","73d4a932":"## Preparing dataset for model training","be44fea2":"## Generate Kaggle submission","9f31f038":"## Train models\n\nFirst some functions are defined for analysing the models. Next, a python dictionary is created with models. Each model will be evaluated. The best model will be analysed further.","bb08aafe":"## Import and preprocess data\n\nSee my previous notebook \"NYC Taxi Fare Data Exploration\" (  https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration) for an indepth analysis of the data and reasoning for selecting & preprocessing the data.","36a78b51":"## Start evaluating all models and selecting the best one","1a1fa732":"# New York City Taxi Fare Prediction Playground Competition\n\n## Comparing models\n\nThis notebook is a framework for testing multiple models, selecting the best one and analysing the best model. You can add\/remove your own models.\n\nIt will generate a Kaggle submission file for the best model. \n\nThis kernel can take hours to compute all models. By default I use 50k datapoints. Select your own models and number of datapoints for your research purpose."}}