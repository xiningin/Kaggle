{"cell_type":{"ba3073f1":"code","c48ac041":"code","133f9b27":"code","2dad589e":"code","dbfb3475":"code","44c986e9":"code","d4b8e241":"code","2e236b6d":"code","8e0f91e7":"code","f89ca9b2":"code","33f24e03":"code","87bcce24":"code","cd752093":"code","5fa1d8d4":"code","0718ba07":"code","582e5974":"code","90d72d15":"code","cdd69b9f":"code","6356a600":"code","ebb910e3":"markdown","92d3a8d1":"markdown","3dd8b3a5":"markdown"},"source":{"ba3073f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport time\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c48ac041":"data = pd.read_csv(\"..\/input\/massive-stock-news-analysis-db-for-nlpbacktests\/analyst_ratings_processed.csv\",error_bad_lines=False)\ndata.sample(5).head(5)\n\n#data = data.sample(1000)\nhistdata = pd.read_csv(\"..\/input\/10y-historical-stock-data\/stockhistory.csv\")\n\ndata = data.dropna()\ndata.isna().sum()","133f9b27":"def getUniqueness(dataset):\n    for (columnName, columnData) in dataset.iteritems():\n        print(f\"unique values in [{columnName}]: {columnData.nunique()}\")\n\n    print(f\"total rows: {len(dataset.index)}\")\ngetUniqueness(data)","2dad589e":"data.rename(columns={'date':'datetime'},inplace = True)\ndata[['date','time']] = data.datetime.str.split(expand=True)\ndata[['year','month','day']]= data.date.str.split('-',expand =True)\ndata['time'] = data.time.str[:-6]\ndata.sample(10).head(10)\n","dbfb3475":"def getUnixTime(row):\n    dt = datetime.datetime(int(row[\"year\"]), int(row[\"month\"]),int(row[\"day\"]))\n    return time.mktime(dt.timetuple())\n   \ndata[\"unix\"] = data.apply(lambda row: getUnixTime(row),axis=1 )\ndata.sample(10).head()","44c986e9":"getUniqueness(data)","d4b8e241":"data.title = data.title.replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)","2e236b6d":"import re\nimport nltk\nimport ssl\n \n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer","8e0f91e7":"freq = pd.Series(' '.join(data['title']).lower().split()).value_counts()[:20]\nfreq\nstop_words = set(stopwords.words(\"english\"))\nstop_words = stop_words.union(freq.index.tolist())\nextra_words = ['amp']\nstop_words = stop_words.union(extra_words)","f89ca9b2":"corpus = []\n\ndef editText(textColumn):\n    #Remove punctuations\n   \n    text = re.sub('[^a-zA-Z]', ' ', textColumn)\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    text = text.replace(\"\\n\",\"\")\n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)\n    string = \"\".join(word for word in text)\n    return string\n\ndata[\"keywords\"] = data.apply(lambda row: editText(row[\"title\"]),axis=1 )\n","33f24e03":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\ncv=CountVectorizer(max_df=0.8,stop_words=stop_words, max_features=100000, ngram_range=(1,3))\nX=cv.fit_transform(corpus)\n\nwith open(\"countVector.pkl\", 'wb') as fout:\n    pickle.dump(X, fout)\n#list(cv.vocabulary_.keys())[:20]","87bcce24":"keywordData = data\nstockseries =histdata[\"stock\"].value_counts().index.tolist()\n#print(stockseries)\ndata = data[data[\"stock\"].isin(stockseries)]\ngetUniqueness(data)","cd752093":"\ndef getDataForDay(stock,timestamp,days):\n    #t = time.process_time_ns() \n    timestamp = timestamp + (days* 86400)\n    lower = timestamp - 43200\n    higher = timestamp + 43200\n    match = histdata.loc[(histdata['stock']==stock) & (histdata['timestamps']>lower)&(histdata['timestamps']<higher) ]\n    #d = time.process_time_ns() - t\n    #print(f'getDataforday time [{stock}]: {d}')\n    return match\n\ndef getDelta(row):\n    global matches\n    \n    current = getDataForDay(row[\"stock\"],row[\"unix\"],0)\n    \n    \n    if not current.empty:\n        \n        future = getDataForDay(row[\"stock\"],row[\"unix\"],2)\n        if not future.empty:\n           \n            diff =  future[\"close\"].iloc[0]-current[\"close\"].iloc[0]\n            if diff >=0:\n                return 1\n            else:\n                return 0\n    return None\n\ndata = data.sample(120000)\ndata.set_index(\"unix\")\ndata.dropna()\nhistdata.set_index(\"timestamps\")\nt = time.process_time() \ndata[\"increased\"]=data.apply(lambda row: getDelta(row),axis=1 )\nd = time.process_time() - t\nprint(f'lambda time : {(d)\/60\/60}')\ndata.sample(5).head(5)\ndata['increased'].value_counts()","5fa1d8d4":"def get_top_n_words(corpus, n=None):\n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = []\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==1:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","0718ba07":"def get_top_n2_words(corpus, n=None):\n    \n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq =[]\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==2:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","582e5974":"def get_top_n3_words(corpus, n=None):\n    \n    bag_of_words = cv.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = []\n    for word, idx in cv.vocabulary_.items():\n        if  len(word.split()) ==3:\n            words_freq.append((word, sum_words[0, idx]))\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\n\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","90d72d15":"#data[\"delta1day\"] = np.random.randint(0,2, size=len(data))\n#data[\"delta7day\"] = np.random.randint(0,2, size=len(data))\n#data[\"delta30day\"] = np.random.randint(0,2, size=len(data))\nprint(len(np.where(data.applymap(lambda x: x == ''))))\n#data['delta1day'].replace('', np.nan, inplace=True)\n#data.dropna(subset=['delta1day'], inplace=True)\ndata.head(10)","cdd69b9f":"df1 = data[data[\"time\"].isna()]\ndf1.head()\nlen(df1)","6356a600":"data.to_csv(\"processed_news.csv\",index = False)\nkeywordData.to_csv(\"keyword_data.csv\",index = False)\n","ebb910e3":"# Adding historical stock data","92d3a8d1":"# Date\nLet's split the date again. We can see that this dataset covers a much larger timeframe than the financial tweets dataset.","3dd8b3a5":"# Introduction\nIn this notebook i will be analysing a news article dataset containing 4 million articles covering 6000 stocks. First, lets see the data we have in this dataset. I will only go in depth on data that has not been covered in the other analysis.\n\nIn this notebook, i will also add the data for the labels. I have made a dataset with the 10 year data of the most occuring stocks."}}