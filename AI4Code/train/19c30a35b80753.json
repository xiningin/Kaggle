{"cell_type":{"ce7d6895":"code","a4974444":"code","e01d6ae4":"code","6777a944":"code","7d2ec01a":"code","3e421f98":"code","91759bf5":"code","c362f827":"code","5c0dd30e":"code","87dd7fdb":"code","38fd29cb":"code","cc155107":"code","37f0732a":"code","4d53f9c3":"code","288d8683":"code","92254f38":"code","e8d30df1":"code","4603437b":"code","1ce54985":"code","2aaf973c":"code","e5a21932":"code","a3a1b2a5":"code","8088a7d4":"code","39143b1a":"code","f560a297":"code","bc3b6393":"code","163202ac":"code","51b7f1e7":"code","66530263":"code","4f9120ed":"code","9a767d38":"code","ed06e6c2":"code","f0eb6edd":"code","9332d3aa":"code","afe42ff0":"code","47a63f2d":"code","da3b5bb6":"code","eb7235c2":"code","ead7ec9f":"code","50c7cc32":"code","4fe02fd0":"code","54293621":"code","6ebeafc3":"code","5398ddde":"code","0e982e53":"markdown","a86a5930":"markdown","a7af801e":"markdown","a82f9168":"markdown","701368d2":"markdown","d63e553a":"markdown","979f3c3a":"markdown","b977ee91":"markdown","665c0897":"markdown","8268b1ff":"markdown","72b847d2":"markdown","b6e6c32f":"markdown","ebbde8b0":"markdown","647ea464":"markdown","2973a3ee":"markdown","346228d1":"markdown","42af06e7":"markdown","4da4e1f9":"markdown","a717f017":"markdown","6fe7b9c5":"markdown","62ba0683":"markdown","187fb04e":"markdown","4795372d":"markdown","b182ce0f":"markdown","ebc76b19":"markdown","e8634b7b":"markdown","5c7c987c":"markdown","2318e34e":"markdown","678fb7de":"markdown","16e1a2db":"markdown","ffbe00e4":"markdown","326692d1":"markdown","976d6e78":"markdown","822869bf":"markdown"},"source":{"ce7d6895":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # visualization\nimport matplotlib.gridspec as gridspec\n\nimport seaborn as sns # visualization\n\nimport warnings # Supress warnings \nwarnings.filterwarnings('ignore')\n\nimport category_encoders as ce\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom tqdm import tqdm\n\n# Classifiers\nimport lightgbm as lgb\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support, f1_score, confusion_matrix\n\nimport time\n\nSEED = 2021\n\n# Load data\ntrain_df = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\", index_col=[0])\ntest_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\", index_col=[0])\ndata_dict_df = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\n# Rename similar columns\n# Copied from https:\/\/www.kaggle.com\/siavrez\/2020fatures\ntrain_df = train_df.rename(columns={'ph_apache':'arterial_ph_apache'})\ntest_df = test_df.rename(columns={'ph_apache':'arterial_ph_apache'})\n\n# Shuffle training data\ntrain_df = train_df.sample(frac=1, random_state = SEED).reset_index(drop=True)\n\ntrain_df.head().style.set_caption('Sample of shuffled training data')","a4974444":"fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n(train_df.diabetes_mellitus.value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'])\ndisplay(train_df.diabetes_mellitus.value_counts(normalize=True)*100)\nax.set_ylim([0, 100])\nax.set_ylabel('Percentage of Patients[%]', fontsize=14)\nax.set_xticklabels(['No Diabetes', 'Diabetes'], fontsize=14, rotation=0)\nplt.show()","e01d6ae4":"print(train_df['readmission_status'].unique())\nprint(test_df['readmission_status'].unique())\n\n# Drop readmission_status column\ntrain_df.drop(\"readmission_status\", axis=1, inplace=True)\ntest_df.drop(\"readmission_status\", axis=1, inplace=True)","6777a944":"for i, col_1 in enumerate(train_df.columns):\n    for col_2 in train_df.columns[(i+1):]:\n        if train_df[col_1].equals(train_df[col_2]):\n            print(f\"{col_1} and {col_2} are identical.\")\n            \nidentical_cols = [\"paco2_for_ph_apache\"]            \ntrain_df.drop(identical_cols, axis=1, inplace=True)\ntest_df.drop(identical_cols, axis=1, inplace=True)","7d2ec01a":"drop_columns = train_df.columns[train_df.columns.str.contains('invasive')]\n\ntrain_df.drop(drop_columns, axis=1, inplace=True)\ntest_df.drop(drop_columns, axis=1, inplace=True)","3e421f98":"print(f\"There are {len(train_df[train_df.age<16])} data points in the training data set with age = 0.\")\nprint(f\"There are {len(test_df[test_df.age<16])} data points in the training data set with age = 0.\")\n\ndisplay(train_df[train_df.age<16])\n\n# Delete implausible age (test data has no implausible age)\ntrain_df.loc[train_df.age == 0, 'age'] = np.nan\ntest_df.loc[test_df.age == 0, 'age'] = np.nan","91759bf5":"display(train_df[train_df.gender.isna() & train_df.height.isna() & train_df.weight.isna()])\n\ndisplay(test_df[test_df.gender.isna() & test_df.height.isna() & test_df.weight.isna()])\nprint(len(train_df))\ntrain_df = train_df[~(train_df.gender.isna() & train_df.height.isna() & train_df.weight.isna())].reset_index(drop=True)\nprint(len(train_df))","c362f827":"train_df[\"gender\"] = train_df[\"gender\"].fillna(\"Unknown\").replace({'M':0, 'F':1, 'Unknown':-1})\ntrain_df[\"ethnicity\"] = train_df[\"ethnicity\"].fillna(\"Other\/Unknown\")\n\ntest_df[\"gender\"] = test_df[\"gender\"].fillna(\"Unknown\").replace({'M':0, 'F':1, 'Unknown':-1})\ntest_df[\"ethnicity\"] = test_df[\"ethnicity\"].fillna(\"Other\/Unknown\")\n\n# Fill Unknown gender by prediction\ntrain_df_mod = train_df[['age', 'ethnicity', 'height', 'weight', 'gender']].copy()\ntest_df_mod = test_df[['age', 'ethnicity', 'height', 'weight', 'gender']].copy()\n\nmean_age = train_df_mod.age.mean()\nmean_height = train_df_mod.height.mean()\nmean_weight = train_df_mod.weight.mean()\n\ntrain_df_mod['age'] = train_df_mod['age'].fillna(mean_age)\ntrain_df_mod['height'] = train_df_mod['height'].fillna(mean_height)\ntrain_df_mod['weight'] = train_df_mod['weight'].fillna(mean_weight)\n\ntest_df_mod['age'] = test_df_mod['age'].fillna(mean_age)\ntest_df_mod['height'] = test_df_mod['height'].fillna(mean_height)\ntest_df_mod['weight'] = test_df_mod['weight'].fillna(mean_weight)\n\n\nfeatures = ['age', 'height', 'weight']\ntarget = ['gender']\n\nfor ethn in train_df.ethnicity.unique():\n    if ethn != 'Other\/Unknown':\n        X = train_df_mod[(train_df_mod.ethnicity == ethn) & (train_df_mod.gender != -1)][features]\n        y = train_df_mod[(train_df_mod.ethnicity == ethn)& (train_df_mod.gender != -1)][target]\n        X_test_train = train_df_mod[(train_df_mod.ethnicity == ethn) & (train_df_mod.gender == -1)][features]\n        X_test_test = test_df_mod[(test_df_mod.ethnicity == ethn) & (test_df_mod.gender == -1)][features]\n\n    else:\n        X = train_df_mod[(train_df_mod.gender != -1)][features]\n        y = train_df_mod[(train_df_mod.gender != -1)][target]\n        X_test_train = train_df_mod[(train_df_mod.gender == -1)][features]\n        X_test_test = test_df_mod[(test_df_mod.gender == -1)][features]\n    \n\n    model = LogisticRegression(random_state=SEED).fit(X, y)\n    if len(X_test_train) > 0:\n        pred = model.predict(X_test_train)\n        for i, idx in enumerate(X_test_train.index):\n            train_df.at[idx, 'gender'] = pred[i]\n        \n    if len(X_test_test) > 0:\n        pred = model.predict(X_test_test)\n        for i, idx in enumerate(X_test_test.index):\n            test_df.at[idx, 'gender'] = pred[i]\n    ","5c0dd30e":"# Fill missing height and weight from lookup table\ntrain_df['age_bins'] = pd.cut(train_df.age, [0, 20, 40, 60, 80, 120], labels=False)\ntest_df['age_bins'] = pd.cut(test_df.age, [0, 20, 40, 60, 80, 120], labels=False)\nlookup_df = train_df[(train_df.ethnicity != 'Other\/Unknown')].groupby(['gender', 'ethnicity', 'age_bins'])[[\"height\", \"weight\"]].mean()\nlookup_df.columns = ['height_lookup', 'weight_lookup']\ndisplay(lookup_df.style.set_caption(\"Lookup Table for Weight and Height\"))\nlookup_df = lookup_df.reset_index(drop=False)\n\ntrain_df = train_df.merge(lookup_df, on=['gender', 'ethnicity', 'age_bins'], how='left')\ntest_df = test_df.merge(lookup_df, on=['gender', 'ethnicity', 'age_bins'], how='left')\n\ntrain_df[\"height\"] = np.where(train_df[\"height\"].isna(), train_df[\"height_lookup\"], train_df[\"height\"])\ntrain_df[\"weight\"] = np.where(train_df[\"weight\"].isna(), train_df[\"weight_lookup\"], train_df[\"weight\"])\n\ntest_df[\"height\"] = np.where(test_df[\"height\"].isna(), test_df[\"height_lookup\"], test_df[\"height\"])\ntest_df[\"weight\"] = np.where(test_df[\"weight\"].isna(), test_df[\"weight_lookup\"], test_df[\"weight\"])\n\ntrain_df.drop(['height_lookup', 'weight_lookup', 'age_bins'], axis=1, inplace=True)\ntest_df.drop(['height_lookup', 'weight_lookup', 'age_bins'], axis=1, inplace=True)\n\n# Fill on gender and ethnicity only for unknown age\nlookup_df = train_df[(train_df.ethnicity != 'Other\/Unknown')].groupby(['gender', 'ethnicity'])[[\"height\", \"weight\"]].mean()\nlookup_df.columns = ['height_lookup', 'weight_lookup']\n#display(lookup_df)\nlookup_df = lookup_df.reset_index(drop=False)\n\ntrain_df = train_df.merge(lookup_df, on=['gender', 'ethnicity'], how='left')\ntest_df = test_df.merge(lookup_df, on=['gender', 'ethnicity'], how='left')\n\ntrain_df[\"height\"] = np.where(train_df[\"height\"].isna(), train_df[\"height_lookup\"], train_df[\"height\"])\ntrain_df[\"weight\"] = np.where(train_df[\"weight\"].isna(), train_df[\"weight_lookup\"], train_df[\"weight\"])\n\ntest_df[\"height\"] = np.where(test_df[\"height\"].isna(), test_df[\"height_lookup\"], test_df[\"height\"])\ntest_df[\"weight\"] = np.where(test_df[\"weight\"].isna(), test_df[\"weight_lookup\"], test_df[\"weight\"])\n\ntrain_df.drop(['height_lookup', 'weight_lookup'], axis=1, inplace=True)\ntest_df.drop(['height_lookup', 'weight_lookup'], axis=1, inplace=True)\n\n\n# Fill on gender only for unknown ethnicity\nlookup_df = train_df.groupby(['gender'])[[\"height\", \"weight\"]].mean()\nlookup_df.columns = ['height_lookup', 'weight_lookup']\n#display(lookup_df)\nlookup_df = lookup_df.reset_index(drop=False)\n\ntrain_df = train_df.merge(lookup_df, on=['gender'], how='left')\ntest_df = test_df.merge(lookup_df, on=['gender'], how='left')\n\ntrain_df[\"height\"] = np.where(train_df[\"height\"].isna(), train_df[\"height_lookup\"], train_df[\"height\"])\ntrain_df[\"weight\"] = np.where(train_df[\"weight\"].isna(), train_df[\"weight_lookup\"], train_df[\"weight\"])\n\ntest_df[\"height\"] = np.where(test_df[\"height\"].isna(), test_df[\"height_lookup\"], test_df[\"height\"])\ntest_df[\"weight\"] = np.where(test_df[\"weight\"].isna(), test_df[\"weight_lookup\"], test_df[\"weight\"])\n\ntrain_df.drop(['height_lookup', 'weight_lookup'], axis=1, inplace=True)\ntest_df.drop(['height_lookup', 'weight_lookup'], axis=1, inplace=True)","87dd7fdb":"def get_bmi_category(bmi):\n    \n    if bmi != bmi: # NaN\n        return np.nan\n    elif bmi < 18.5: # Underweight\n        return -1\n    elif bmi < 25: # Healthy weight\n        return 0\n    elif bmi < 30: # Overweight\n        return 1\n    else: # Obese\n        return 2\n\ntrain_df[\"bmi_cat\"] = train_df[\"bmi\"].apply(get_bmi_category)\ntest_df[\"bmi_cat\"] = test_df[\"bmi\"].apply(get_bmi_category)\n\ntrain_df[\"bmi_calc\"] = np.round(train_df.weight \/ ((train_df.height\/100)**2), 8)\ntest_df[\"bmi_calc\"] = np.round(test_df.weight \/ ((test_df.height\/100)**2), 8)\n\ntrain_df[\"bmi\"] = np.where(train_df[\"bmi\"].isna(), train_df[\"bmi_calc\"], train_df[\"bmi\"])\ntest_df[\"bmi\"] = np.where(test_df[\"bmi\"].isna(), test_df[\"bmi_calc\"], test_df[\"bmi\"])\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\nsns.scatterplot(x=train_df.weight, y=train_df.height, hue=train_df.bmi_cat, palette=['red', 'lightgreen', 'darkorange', 'darkred'], linewidth=0, marker='.', ax=ax)\nax.set_title('Original Weight and Height', fontsize=16)\nax.set_xlabel('Weight [kg]', fontsize = 14)\nax.set_ylabel('Height [cm]', fontsize = 14)\n\nax.set_xlim([0, 220])\nax.set_ylim([0, 220])\nax.legend(['', '', 'Underweight', 'Healthy weight', 'Overweight', 'Obese'])\n\nplt.show()\ntrain_df.drop(['bmi_calc', 'bmi_cat'], axis=1, inplace=True)\ntest_df.drop(['bmi_calc', 'bmi_cat'], axis=1, inplace=True)\n","38fd29cb":"train_df[\"hospital_admit_source\"] = train_df[\"hospital_admit_source\"].fillna(\"Other\")\ntrain_df[\"icu_admit_source\"] = train_df[\"icu_admit_source\"].fillna(\"Other ICU\")\ntrain_df[\"icu_stay_type\"] = train_df[\"icu_stay_type\"].fillna(\"admit\")\ntrain_df[\"icu_type\"] = train_df[\"icu_type\"].fillna(\"Med-Surg ICU\")\n\ntest_df[\"hospital_admit_source\"] = test_df[\"hospital_admit_source\"].fillna(\"Other\")\ntest_df[\"icu_admit_source\"] = test_df[\"icu_admit_source\"].fillna(\"Other ICU\")\ntest_df[\"icu_stay_type\"] = test_df[\"icu_stay_type\"].fillna(\"admit\")\ntest_df[\"icu_type\"] = test_df[\"icu_type\"].fillna(\"Med-Surg ICU\")\n\nicu_dict = {'Other ICU' : 'ICU',\n            'ICU to SDU' : 'SDU', \n            'Step-Down Unit (SDU)' : 'SDU',\n            'Other Hospital':'Other',\n            'Observation': 'Recovery Room',\n            'Acute Care\/Floor': 'Floor'}\n\ntrain_df['hospital_admit_source'] = train_df['hospital_admit_source'].replace(icu_dict)\ntest_df['hospital_admit_source'] = test_df['hospital_admit_source'].replace(icu_dict)","cc155107":"admit_source_df = (train_df.groupby('hospital_admit_source').icu_admit_source.value_counts(normalize=True)).to_frame()\nadmit_source_df.columns = ['percentage']\nadmit_source_df.reset_index(drop=False, inplace=True)\nadmit_source_df = admit_source_df.pivot(index='hospital_admit_source', columns='icu_admit_source')['percentage']\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nsns.heatmap(admit_source_df, cmap='Blues', vmin=0, vmax=1, annot=True)\nplt.tight_layout()\nplt.show()\n\nicu_hospital_admit_source_dict = {\n    'Floor' : 'Floor', \n    'Emergency Department' : 'Accident & Emergency', \n    'Operating Room' : 'Operating Room \/ Recovery', \n    'Direct Admit' : 'Accident & Emergency' , \n    'ICU' :  'Other ICU', \n    'SDU' : 'Floor', \n    'Recovery Room' : 'Operating Room \/ Recovery', \n    'Chest Pain Center' : 'Floor', \n    'PACU' : 'Operating Room \/ Recovery'\n}\n\ndef get_icu_admit_source_from_hospital(df):\n    for key in icu_hospital_admit_source_dict:\n        if df.hospital_admit_source == key:\n            df.icu_admit_source = icu_hospital_admit_source_dict[key]\n    return df.icu_admit_source\n\ntrain_df['icu_admit_source'] = train_df.apply(lambda x: get_icu_admit_source_from_hospital(x) if x.icu_admit_source == 'Other Hospital' else x.icu_admit_source, axis=1)\ntrain_df['icu_admit_source'] = train_df.apply(lambda x: get_icu_admit_source_from_hospital(x) if x.icu_admit_source == 'Other ICU' else x.icu_admit_source, axis=1)\n\ntest_df['icu_admit_source'] = test_df.apply(lambda x: get_icu_admit_source_from_hospital(x) if x.icu_admit_source == 'Other Hospital' else x.icu_admit_source, axis=1)\ntest_df['icu_admit_source'] = test_df.apply(lambda x: get_icu_admit_source_from_hospital(x) if x.icu_admit_source == 'Other ICU' else x.icu_admit_source, axis=1)\n\n# Drop first column because it is identical to index\ntrain_df.drop([\"hospital_admit_source\", \"icu_stay_type\",],  axis = 1, inplace = True)\ntest_df.drop([\"hospital_admit_source\", \"icu_stay_type\", ], axis = 1, inplace = True)\n\n\n# Electrive Surgery\ntrain_df[\"icu_admit_source\"] = np.where(((train_df[\"elective_surgery\"] == 1) & ((train_df[\"icu_admit_source\"] == 'Other ICU') | (train_df[\"icu_admit_source\"] == 'Other Hospital'))), \n                                   \"Operating Room \/ Recovery\", \n                                   train_df[\"icu_admit_source\"])\n                                        \ntest_df[\"icu_admit_source\"] = np.where(((test_df[\"elective_surgery\"] == 1) & ((test_df[\"icu_admit_source\"] == 'Other ICU') | (test_df[\"icu_admit_source\"] == 'Other Hospital'))), \n                                   \"Operating Room \/ Recovery\", \n                                   test_df[\"icu_admit_source\"])\n\ntrain_df.drop(\"elective_surgery\", axis = 1, inplace = True)\ntest_df.drop(\"elective_surgery\", axis = 1, inplace = True)","37f0732a":"# Edited from https:\/\/www.kaggle.com\/jayjay75\/3rd-place-nn-wids2020?scriptVersionId=29209297\n\n# comorbidity_score\ntrain_df['comorbidity_score'] = train_df['aids'] * 23 + train_df['cirrhosis'] * 4 + train_df['hepatic_failure'] * 16 + train_df['immunosuppression'] * 10 + train_df['leukemia'] * 10 + train_df['lymphoma'] * 13 + train_df['solid_tumor_with_metastasis'] * 11\ntrain_df['comorbidity_score'] = train_df['comorbidity_score'].fillna(0)\n\ntest_df['comorbidity_score'] = test_df['aids'] * 23 + test_df['cirrhosis'] * 4 + test_df['hepatic_failure'] * 16 + test_df['immunosuppression'] * 10 + test_df['leukemia'] * 10 + test_df['lymphoma'] * 13 + test_df['solid_tumor_with_metastasis'] * 11\ntest_df['comorbidity_score'] = test_df['comorbidity_score'].fillna(0)\n\ntrain_df[\"total_chronic\"] = train_df[[\"aids\", \"cirrhosis\", 'hepatic_failure']].sum(axis=1)\ntest_df[\"total_chronic\"] = test_df[[\"aids\",\"cirrhosis\", 'hepatic_failure']].sum(axis=1)\n\n# GCS\ntrain_df['gcs_eyes_apache'] = train_df['gcs_eyes_apache'].fillna(4)\ntrain_df['gcs_verbal_apache'] = train_df['gcs_verbal_apache'].fillna(5)\ntrain_df['gcs_motor_apache'] = train_df['gcs_motor_apache'].fillna(6)\n\ntrain_df['gcs_score'] = train_df[['gcs_eyes_apache', 'gcs_motor_apache','gcs_verbal_apache']].sum(axis=1)\ntrain_df['gcs_score_type'] = train_df.gcs_score.apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\n\ntest_df['gcs_eyes_apache'] = test_df['gcs_eyes_apache'].fillna(4)\ntest_df['gcs_verbal_apache'] = test_df['gcs_verbal_apache'].fillna(5)\ntest_df['gcs_motor_apache'] = test_df['gcs_motor_apache'].fillna(6)\n\ntest_df['gcs_score'] = test_df[['gcs_eyes_apache', 'gcs_motor_apache','gcs_verbal_apache']].sum(axis=1)\ntest_df['gcs_score_type'] = test_df.gcs_score.apply(lambda x: 2.5 * (round(int(x)\/2.5))).divide(2.5)\n\ngcs_cols = ['gcs_eyes_apache', 'gcs_motor_apache','gcs_unable_apache','gcs_verbal_apache']\n\ntrain_df.drop(gcs_cols, axis=1, inplace=True)\ntest_df.drop(gcs_cols, axis=1, inplace=True)\n","4d53f9c3":"train_df['apache_2_diagnosis'] = (train_df.apache_2_diagnosis).fillna(0).astype(int)\ntrain_df['apache_2_diagnosis_type'] = train_df.apache_2_diagnosis.round(-1).fillna(0).astype(int)\ntrain_df['apache_3j_diagnosis'] = (train_df.apache_3j_diagnosis).fillna(0).astype(int)\ntrain_df['apache_3j_diagnosis_type'] = train_df.apache_3j_diagnosis.round(-2).fillna(0).astype(int)\n\ntest_df['apache_2_diagnosis'] = (test_df.apache_2_diagnosis).fillna(0).astype(int)\ntest_df['apache_2_diagnosis_type'] = test_df.apache_2_diagnosis.round(-1).fillna(0).astype(int)\ntest_df['apache_3j_diagnosis'] = (test_df.apache_3j_diagnosis).fillna(0).astype(int)\ntest_df['apache_3j_diagnosis_type'] = test_df.apache_3j_diagnosis.round(-2).fillna(0).astype(int)\n","288d8683":"train_df['APACHE_diabetic_ketoacidosis'] = train_df.apache_3j_diagnosis.apply(lambda x: 1 if x == 702 else 0)\ntest_df['APACHE_diabetic_ketoacidosis'] = test_df.apache_3j_diagnosis.apply(lambda x: 1 if x == 702 else 0)\n\ndiabetic_ketoacidosis_df = train_df[train_df.APACHE_diabetic_ketoacidosis == 1]\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\nsns.countplot(diabetic_ketoacidosis_df.diabetes_mellitus, ax=ax)\nax.set_title('Diabetes Counts for Patients with Diabetic Ketoacidosis')\nplt.show()","92254f38":"def get_pregnancy_probability(df):\n    if df.gender == 0: # Male\n        return 0\n    else: # Female\n        if df.apache_3j_diagnosis == 1802: # Pregnancy-related disorder \n            return 1\n        else:\n            if df.age < 20:\n                return 0.07\n            elif df.age < 25:\n                return 0.15\n            elif df.age < 30:\n                return 0.16\n            elif df.age < 35:\n                return 0.14\n            elif df.age < 40:\n                return 0.08\n            elif df.age < 45:\n                return 0.02\n            else:\n                return 0\n            \ntrain_df['pregnancy_probability'] = train_df.apply(lambda x: get_pregnancy_probability(x), axis=1)\ntest_df['pregnancy_probability'] = test_df.apply(lambda x: get_pregnancy_probability(x), axis=1)\n\n# Visualization\ntrain_df['age_bins'] = pd.cut(train_df.age, [0, 20, 30, 35, 40, 45, 120], labels=False)\ntemp = train_df.groupby(['gender', 'age_bins']).pregnancy_probability.mean().to_frame().reset_index(drop=False)\ntemp = temp.pivot(index='gender', columns='age_bins').pregnancy_probability\ntemp.columns = ['<20', '20-30', '30-35', '35-40', '40-45', '>45']\ntemp = temp.T\ntemp.columns = ['Male', 'Female']\ndisplay(temp.style.set_caption('Mean Pregnancy Probability According to Age and Gender').background_gradient(cmap='Reds'))\ntrain_df.drop('age_bins', axis=1, inplace=True)","e8d30df1":"# Categorical Feature Encoding\nobject_cols = ['ethnicity', 'icu_admit_source', 'icu_type' ]\ndisplay(train_df[object_cols].head().style.set_caption(\"Object columns before One Hot Encoding\"))\n\n# Apply OH Encoding\nohe = ce.OneHotEncoder(handle_unknown='value', use_cat_names=True)\nOH_cols_train = pd.DataFrame(ohe.fit_transform(train_df[object_cols]))\nOH_cols_train.index = train_df.index\n\nOH_cols_test = pd.DataFrame(ohe.transform(test_df[object_cols]))\nOH_cols_test.index = test_df.index\n\ndisplay(OH_cols_train.head().style.set_caption(\"Object columns after One Hot Encoding\"))\n\n# Drop original categorical columns and replace with OH encoded columns\n#train_df.drop(object_cols, axis=1, inplace=True)\ntrain_df = pd.concat([train_df, OH_cols_train], axis=1)\n\n#test_df.drop(object_cols, axis=1, inplace=True)\ntest_df = pd.concat([test_df, OH_cols_test], axis=1)","4603437b":"target = ['diabetes_mellitus']\ndemo_cols = [c for c in data_dict_df[data_dict_df.Category.str.contains('demographic')]['Variable Name'].unique() if c in train_df.columns]\napache_comorbidity_cols = [c for c in data_dict_df[data_dict_df.Category.str.contains('APACHE comorbidity')]['Variable Name'].unique() if c in train_df.columns]\napache_covariate_cols = [c for c in data_dict_df[data_dict_df.Category.str.contains('APACHE covariate')]['Variable Name'].unique() if c in train_df.columns]\n\nbinary_cols = [ c for c in train_df.columns if (train_df[c].nunique() < 3) & (c not in target)]\nobject_cols = [ c for c in train_df.columns if ((train_df[c].dtype == object) | (train_df[c].nunique() < 10))  & (c not in binary_cols) & (c not in target)]\ncat_cols = [c for c in train_df.columns if ((c in binary_cols) | (c in object_cols) | (c in ['gcs_score', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_3j_diagnosis_class'])) & (c not in target)]\nnumeric_cols = [ c for c in train_df.columns if (c not in cat_cols) & (c not in target) & (c not in ['encounter_id'])]\n\n# Reduced demographic columns\ncols = numeric_cols\ndisplay(train_df[cols].isna().sum(axis=0))\ndisplay(test_df[cols].isna().sum(axis=0))\n\napache_vitals = [(c.split(\"_\")[0]) for c in apache_covariate_cols if (train_df[c].nunique() > 10) & (c not in ['apache_2_diagnosis', 'apache_3j_diagnosis'])]\napache_vital_cols = [c for c in apache_covariate_cols if (train_df[c].nunique() > 10) & (c not in ['apache_2_diagnosis', 'apache_3j_diagnosis'])]\n\nlab_vital_cols = [v for v in train_df.columns[train_df.columns.str.startswith('d1') | train_df.columns.str.startswith('h1')]]\n\nlab_vitals = [((v.split(\"d1_\")[1]).split(\"_min\")[0]) for v in train_df.columns[train_df.columns.str.startswith('d1') & train_df.columns.str.endswith('min')]]\n\nvital_cols = list(apache_vital_cols) + list(lab_vital_cols)\ncommon_vitals = [c for c in apache_vitals if c in lab_vitals]","1ce54985":"min_max_features = [c.split('_min')[0] for c in train_df.columns[train_df.columns.str.endswith('_min')]]\nfor v in tqdm(min_max_features):\n    # Swap columns where d1_min > d1_max and h1_min > h1_max\n    train_df[f\"{v}_min\"], train_df[f\"{v}_max\"] = np.where((train_df[f\"{v}_min\"] > train_df[f\"{v}_max\"]), \n                                                            [train_df[f\"{v}_max\"], train_df[f\"{v}_min\"]], \n                                                            [train_df[f\"{v}_min\"], train_df[f\"{v}_max\"]])\n    test_df[f\"{v}_min\"], test_df[f\"{v}_max\"] = np.where((test_df[f\"{v}_min\"] > test_df[f\"{v}_max\"]), \n                                                        [test_df[f\"{v}_max\"], test_df[f\"{v}_min\"]], \n                                                        [test_df[f\"{v}_min\"], test_df[f\"{v}_max\"]])\n\nfor v in tqdm(lab_vitals):\n    # Swap values if h1_min < d1_min or h1_max > d1_max\n    train_df[f\"d1_{v}_min\"] = np.where((train_df[f\"d1_{v}_min\"] > train_df[f\"h1_{v}_min\"]), train_df[f\"h1_{v}_min\"], train_df[f\"d1_{v}_min\"])\n    train_df[f\"d1_{v}_max\"] = np.where((train_df[f\"d1_{v}_max\"] < train_df[f\"h1_{v}_max\"]), train_df[f\"h1_{v}_max\"], train_df[f\"d1_{v}_max\"])","2aaf973c":"# Copied from https:\/\/www.kaggle.com\/danofer\/wids-2020-competitive-1st-place-component\n\nd_cols = [c for c in train_df.columns if(c.startswith(\"d1\"))]\nh_cols = [c for c in train_df.columns if(c.startswith(\"h1\"))]\n\ntrain_df[\"dailyLabs_row_nan_count\"] = train_df[d_cols].isna().sum(axis=1)\ntrain_df[\"hourlyLabs_row_nan_count\"] = train_df[h_cols].isna().sum(axis=1)\ntrain_df[\"diff_labTestsRun_daily_hourly\"] = train_df[\"dailyLabs_row_nan_count\"] - train_df[\"hourlyLabs_row_nan_count\"]\n\ntest_df[\"dailyLabs_row_nan_count\"] = test_df[d_cols].isna().sum(axis=1)\ntest_df[\"hourlyLabs_row_nan_count\"] = test_df[h_cols].isna().sum(axis=1)\ntest_df[\"diff_labTestsRun_daily_hourly\"] = test_df[\"dailyLabs_row_nan_count\"] - test_df[\"hourlyLabs_row_nan_count\"]","e5a21932":"apache_cols = train_df.columns[train_df.columns.str.contains('apache')]\napache_cols = [c.split('_apache')[0] for c in apache_cols] \n\nvital_cols = train_df.columns[train_df.columns.str.startswith('d1') & train_df.columns.str.contains('_max')]\nvital_cols = [(c.split('d1_')[1]).split('_max')[0] for c in vital_cols]\n\ncommon_cols = [c for c in apache_cols if c in vital_cols]\n\nfig, ax = plt.subplots(nrows=4, ncols=3, figsize=(24, 16))\nfig.suptitle('Comparison of Apache Values with D1 Min & Max Values', fontsize=16)\n\nk = 0\nj = 0\nfor i, c in enumerate(common_cols):\n    k = (k+1) if j == 2 else k\n    j = i% 3\n    \n    sns.distplot(train_df[f\"d1_{c}_max\"], ax=ax[k, j], label = 'd1_max', color='green')\n    sns.distplot(train_df[f\"d1_{c}_min\"], ax=ax[k, j], label = 'd1_min', color='blue')\n    sns.distplot(train_df[f\"h1_{c}_max\"], ax=ax[k, j], label = 'h1_max', color='yellow')\n    sns.distplot(train_df[f\"h1_{c}_min\"], ax=ax[k, j], label = 'h1_min', color='purple')\n    sns.distplot(train_df[f\"{c}_apache\"], ax=ax[k, j], label = 'apache', color='red')\n    ax[k, j].legend()\n\nfig.delaxes(ax[3,1])\nfig.delaxes(ax[3,2])\nplt.show()","a3a1b2a5":"for c in ['albumin', 'hematocrit', 'temp', 'sodium']:\n    # Fill empty d1_..._max column from available ..._apache column\n    train_df[f\"d1_{c}_min\"] = np.where((train_df[f\"d1_{c}_min\"].isna() \n                                        & train_df[f\"{c}_apache\"].notna()), \n                                       train_df[f\"{c}_apache\"], \n                                       train_df[f\"d1_{c}_min\"])\n\n    test_df[f\"d1_{c}_min\"] = np.where((test_df[f\"d1_{c}_min\"].isna() \n                                       & test_df[f\"{c}_apache\"].notna()), \n                                       test_df[f\"{c}_apache\"], \n                                       test_df[f\"d1_{c}_min\"])\n    # Drop ..._apache column\n    train_df.drop(f\"{c}_apache\", axis=1, inplace=True)\n    test_df.drop(f\"{c}_apache\", axis=1, inplace=True)\n\nfor c in ['bilirubin', 'bun']:\n    # Fill empty d1_..._max column from available ..._apache column\n    train_df[f\"d1_{c}_max\"] = np.where((train_df[f\"d1_{c}_max\"].isna() \n                                        & train_df[f\"{c}_apache\"].notna()), \n                                       train_df[f\"{c}_apache\"], \n                                       train_df[f\"d1_{c}_max\"])\n\n    test_df[f\"d1_{c}_max\"] = np.where((test_df[f\"d1_{c}_max\"].isna() \n                                       & test_df[f\"{c}_apache\"].notna()), \n                                       test_df[f\"{c}_apache\"], \n                                       test_df[f\"d1_{c}_max\"])\n    # Drop ..._apache column\n    train_df.drop(f\"{c}_apache\", axis=1, inplace=True)\n    test_df.drop(f\"{c}_apache\", axis=1, inplace=True)","8088a7d4":"bimodal_apache = ['glucose_apache', 'heart_rate_apache', 'map_apache', 'resprate_apache', 'wbc_apache', 'creatinine_apache']\n\n# Drop ..._apache column\ntrain_df.drop(bimodal_apache, axis=1, inplace=True)\ntest_df.drop(bimodal_apache, axis=1, inplace=True)","39143b1a":"train_df[\"d1_pao2fio2ratio_max\"] = np.where((train_df[\"pao2_apache\"].notna() \n                                             & train_df[\"fio2_apache\"].notna()\n                                             & train_df[\"d1_pao2fio2ratio_max\"].isna() ), \n                                            train_df[\"pao2_apache\"] \/ train_df[\"fio2_apache\"], \n                                            train_df[\"d1_pao2fio2ratio_max\"])\n\ntest_df[\"d1_pao2fio2ratio_max\"] = np.where((test_df[\"pao2_apache\"].notna() \n                                             & test_df[\"fio2_apache\"].notna()\n                                             & test_df[\"d1_pao2fio2ratio_max\"].isna() ), \n                                            test_df[\"pao2_apache\"] \/ test_df[\"fio2_apache\"], \n                                            test_df[\"d1_pao2fio2ratio_max\"])","f560a297":"# Edited from https:\/\/www.kaggle.com\/siavrez\/2020fatures\/data\n\n# Patient profile\ntrain_df['age_bins'] = pd.cut(train_df.age, [0, 20, 30, 40, 50, 60, 70, 80, 120], labels=False)\ntest_df['age_bins'] = pd.cut(test_df.age, [0, 20, 30, 40, 50, 60, 70, 80, 120], labels=False)\n\ntrain_df['bmi_bins'] = pd.cut(train_df.bmi, [0, 18.5, 25, 30, 75 ], labels=False)\ntest_df['bmi_bins'] = pd.cut(test_df.bmi, [0, 18.5, 25, 30, 75  ], labels=False)\n\nprofile_features = ['age_bins', 'ethnicity', 'gender', 'bmi_bins'] \ntrain_df['profile'] = train_df[profile_features].apply(lambda x: hash(tuple(x)), axis = 1)\ntest_df['profile'] = test_df[profile_features].apply(lambda x: hash(tuple(x)), axis = 1)\n\nbin_cols = ['age_bins', 'bmi_bins']\ntrain_df.drop(bin_cols, axis=1, inplace=True)\ntest_df.drop(bin_cols, axis=1, inplace=True)\n\ncat_cols = ['ethnicity', 'icu_admit_source', 'icu_type']\ntrain_df.drop(cat_cols, axis=1, inplace=True)\ntest_df.drop(cat_cols, axis=1, inplace=True)\n\nprint(f'Number of unique Profiles : {train_df[\"profile\"].nunique()}')\nprint(f'Number of unique Profiles : {test_df[\"profile\"].nunique()}')\nprint(f'Number of profiles only in test set : {len([c for c in test_df[\"profile\"].unique() if c not in train_df[\"profile\"].unique()])}')","bc3b6393":"vital_cols = list(apache_vital_cols) + list(lab_vital_cols)\nvital_cols = [c for c in vital_cols if c in train_df.columns]\n\n# Edited from https:\/\/www.kaggle.com\/kainsama\/lgbm-wids2021-v0-1-1\n# Edited from https:\/\/www.kaggle.com\/siavrez\/2020fatures\ngroupers = ['apache_3j_diagnosis', 'profile']\n\nfor g in groupers:\n    for v in tqdm(vital_cols):\n        agg = train_df.groupby(g)[v].agg(['mean'])\n        agg.columns = [f\"{v}_{g}_mean\"] \n\n        # Merge\n        train_df = train_df.merge(agg, on=[g], how='left')\n        test_df = test_df.merge(agg, on=[g], how='left')\n\n        # Calculate difference\n        train_df[f\"{v}_{g}_diff\"]  =  train_df[f\"{v}_{g}_mean\"] - train_df[v]\n        test_df[f\"{v}_{g}_diff\"]  =  test_df[f\"{v}_{g}_mean\"] - test_df[v]\n\n        train_df.drop([f\"{v}_{g}_mean\"], axis=1, inplace=True)\n        test_df.drop([f\"{v}_{g}_mean\"], axis=1, inplace=True)","163202ac":"for v in tqdm(lab_vitals):\n    v_cols = [x for x in test_df.columns if v in x]\n    train_df[f\"{v}_nans\"] = train_df.loc[:, v_cols].isna().sum(axis=1)\n    train_df[f\"{v}_d1_h1_max_eq\"] = (train_df[f\"d1_{v}_max\"]== train_df[f\"h1_{v}_max\"]).astype(np.int8)\n    train_df[f\"{v}_d1_h1_min_eq\"] = (train_df[f\"d1_{v}_min\"]== train_df[f\"h1_{v}_min\"]).astype(np.int8)\n\n    # Edited from https:\/\/www.kaggle.com\/siavrez\/2020fatures\/data\n    train_df[f'd1_{v}_range'] = train_df[f'd1_{v}_max'] - train_df[f'd1_{v}_min']\n    train_df[f'd1_{v}_mean'] = (train_df[f'd1_{v}_max'] + train_df[f'd1_{v}_min']) \/ 2\n    train_df[f'd1_{v}_s2n'] = (train_df[f'd1_{v}_mean'] \/ (train_df[f'd1_{v}_range']))\n   \n    train_df[f'h1_{v}_range'] = train_df[f'h1_{v}_max'] - train_df[f'h1_{v}_min']\n    train_df[f'h1_{v}_mean'] = (train_df[f'h1_{v}_max'] + train_df[f'h1_{v}_min']) \/ 2\n    train_df[f'h1_{v}_s2n'] = (train_df[f'h1_{v}_mean'] \/ (train_df[f'h1_{v}_range']))   \n    \n    # Edited from https:\/\/www.kaggle.com\/siavrez\/2020fatures\/data\n    test_df[f\"{v}_nans\"] = test_df.loc[:, v_cols].isna().sum(axis=1)\n    test_df[f\"{v}_d1_h1_max_eq\"] = (test_df[f\"d1_{v}_max\"]== test_df[f\"h1_{v}_max\"]).astype(np.int8)\n    test_df[f\"{v}_d1_h1_min_eq\"] = (test_df[f\"d1_{v}_min\"]== test_df[f\"h1_{v}_min\"]).astype(np.int8)\n\n    test_df[f'd1_{v}_range'] = test_df[f'd1_{v}_max'] - test_df[f'd1_{v}_min']\n    test_df[f'd1_{v}_mean'] = (test_df[f'd1_{v}_max'] + test_df[f'd1_{v}_min']) \/ 2\n    test_df[f'd1_{v}_s2n'] = (test_df[f'd1_{v}_mean'] \/ test_df[f'd1_{v}_range'])\n    \n    test_df[f'h1_{v}_range'] = test_df[f'h1_{v}_max'] - test_df[f'h1_{v}_min']\n    test_df[f'h1_{v}_mean'] = (test_df[f'h1_{v}_max'] + test_df[f'h1_{v}_min']) \/ 2\n    test_df[f'h1_{v}_s2n'] = (test_df[f'h1_{v}_mean'] \/ (test_df[f'h1_{v}_range']))  ","51b7f1e7":"def get_blood_pressure_category(sysbp, diasbp):\n    if ((sysbp < 90) & (diasbp < 60)):\n        return -1 # Low blood pressure\n    elif ((sysbp < 120) & (diasbp < 80)):\n        return 0 # Normal\n    elif ((sysbp < 140) & (diasbp < 90)):\n        return 1 # Pre-Hypertension\n    elif ((sysbp < 160) & (diasbp < 100)):\n        return 2 # Stage 1 Hypertension\n    else:\n        return 3 # Stage 2 Hypertension\n    \ntrain_df['bp_cat'] = train_df[['d1_sysbp_mean', 'd1_diasbp_mean']].apply(lambda x: get_blood_pressure_category(x.d1_sysbp_mean, x.d1_diasbp_mean), axis=1)\ntest_df['bp_cat'] = test_df[['d1_sysbp_mean', 'd1_diasbp_mean']].apply(lambda x: get_blood_pressure_category(x.d1_sysbp_mean, x.d1_diasbp_mean), axis=1)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 7))\n\nsns.scatterplot(y=train_df.d1_sysbp_mean, \n                x=train_df.d1_diasbp_mean, \n                hue=train_df.bp_cat, \n                palette=['steelblue', 'mediumseagreen', 'peachpuff', 'lightcoral', 'indianred'], \n                linewidth=0, \n                marker='.', \n                color='grey',\n               ax=ax[0])\nax[0].set_xlim([20,130])\nax[0].set_ylim([60,200])\nax[0].set_xlabel('Diastolic Blood Pressure', fontsize=14)\nax[0].set_ylabel('Systolic Blood Pressure', fontsize=14)\nax[0].legend(['', 'Blood Pressure Category', 'low', 'normal', 'pre hypertension', 'stage 1 hypertension', 'stage 2 hypertension'])\n\n\n# Prepare temporary dataframe for visualization\ntemp = (train_df.groupby(['bp_cat']).diabetes_mellitus.value_counts(normalize=True)*100).to_frame()\ntemp.columns = ['percentage']\ntemp.reset_index(inplace=True)\ntemp = temp.pivot(index='bp_cat', columns = 'diabetes_mellitus')['percentage']\ntemp.insert(0, '', 100 - temp[0])\n\n# Plot\ntemp.plot.barh(stacked=True, color=['white', 'mediumseagreen', 'lightcoral'], edgecolor='none', legend=True, ax=ax[1])\nax[1].legend(['Diabetes Mellitus', 'No', 'Yes'], loc='upper right')\nax[1].axvline(100, linestyle='--', color='black', alpha=.5)\nax[1].set_zorder(-100)\n\nax[1].set_xlim(0, 200)\nax[1].set_xticklabels(['100', '75', '50','25', '0','25', '50', '75','100'])\nax[1].set_yticklabels(['Low', 'Normal', 'Pre Hypertension', 'Stage 1 Hypertension', 'Stage 2 Hypertension'])\n\n\nax[1].set_xlabel('Percentage [%]', fontsize=14)\nax[1].set_ylabel('', fontsize=14)\n\nfor tick in ax[1].yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n\nfor tick in ax[1].xaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n\nax[1].set_title(\"Percentage of Patients with Diabetes Mellitus\\n According to Blood Pressure Levels\", fontsize=16)\nplt.tight_layout()\nplt.show()\n","66530263":"features = [c for c in train_df.columns if (c not in  ['diabetes_mellitus', 'encounter_id', 'hospital_id'])]\ntarget = ['diabetes_mellitus']\n\n# Prepare training and test data\nX, y = train_df[features], train_df[target]\n    \ndisplay(X.head().style.set_caption(\"X\"))\ndisplay(y.head().style.set_caption(\"y\"))\n\nX_test = test_df[features]","4f9120ed":"model_params = {\n    \"objective\": \"binary\",\n    \"metric\":\"auc\",\n    \"seed\": SEED,\n    'num_iterations': 500, \n    \"learning_rate\": 0.05,\n    \"max_depth\": 8,\n    \"num_leaves\" : 32, \n    \"is_unbalace\" : True, \n    \"min_data_in_leaf\": 200,\n    \"lambda_l1\" : 1,\n    \"lambda_l2\": 0.1,\n    \"bagging_fraction\" : 0.7,\n    \"feature_fraction\" : 0.8, \n    \"min_split_gain\" : 0.5, \n    \"subsample_for_bin\" : 200,\n    \"n_jobs\" : -1,\n}\n\n#Copied from https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances\n\ndef get_feature_importances(X_train, y_train, params, shuffle, seed=None):\n\n    # Shuffle target if required\n    if shuffle:\n        # Here you could as well use a binomial distribution\n        y_train = y_train.sample(frac=1.0)\n    \n    # Fit LightGBM\n    train_data = lgb.Dataset(X_train, y_train, free_raw_data=False, silent=True)\n    \n    # Fit the model\n    clf = lgb.train(params=params, train_set=train_data)\n\n    # Get feature importances\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = list(X_train.columns)\n    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n    imp_df['trn_score'] = roc_auc_score(y_train, clf.predict(X_train))\n    \n    return imp_df\n\nactual_imp_df = get_feature_importances(X, y, model_params, shuffle=False)\n\ndisplay(actual_imp_df.head())\n\nnull_imp_df = pd.DataFrame()\nnb_runs = 32\nstart = time.time()\ndsp = ''\nfor i in range(nb_runs):\n    # Get current run importances\n    imp_df = get_feature_importances(X, y, model_params, shuffle=True)\n    imp_df['run'] = i + 1 \n    # Concat the latest importances with the old ones\n    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n\ndisplay(null_imp_df.head())\n\ndef display_distributions(actual_imp_df_, null_imp_df_, feature_):\n    plt.figure(figsize=(13, 6))\n    gs = gridspec.GridSpec(1, 2)\n    # Plot Split importances\n    ax = plt.subplot(gs[0, 0])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n    # Plot Gain importances\n    ax = plt.subplot(gs[0, 1])\n    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n    ax.legend()\n    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())\n\n    \nfeature_scores = []\nfor _f in actual_imp_df['feature'].unique():\n    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n    gain_score = np.log(1e-10 + f_act_imps_gain \/ (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid didvide by zero\n    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n    split_score = np.log(1e-10 + f_act_imps_split \/ (1 + np.percentile(f_null_imps_split, 75)))  # Avoid didvide by zero\n    feature_scores.append((_f, split_score, gain_score))\n\nscores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n\nplt.figure(figsize=(16, 16))\ngs = gridspec.GridSpec(1, 2)\n# Plot Split importances\nax = plt.subplot(gs[0, 0])\nsns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).head(50), ax=ax)\nax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n# Plot Gain importances\nax = plt.subplot(gs[0, 1])\nsns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).head(50), ax=ax)\nax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nscores_df['sum_score'] = scores_df.split_score+ scores_df.gain_score\ndrop_cols = scores_df[(scores_df.split_score<0) & (scores_df.gain_score <0) & (scores_df.sum_score < -5)].feature.values\nprint(drop_cols)\n\n\"\"\"\ndrop_cols = ['apache_post_operative', 'intubated_apache', 'h1_bilirubin_min',\n 'h1_hco3_min', 'h1_hematocrit_max', 'h1_hematocrit_min', 'h1_inr_max',\n 'h1_lactate_min' ,'h1_pao2fio2ratio_max' ,'aids', 'hepatic_failure',\n 'leukemia', 'lymphoma', 'ethnicity_Native American',\n 'ethnicity_Other\/Unknown', 'icu_admit_source_Accident & Emergency',\n 'icu_admit_source_Other Hospital' ,'icu_admit_source_Other ICU',\n 'icu_type_CTICU', 'icu_type_Cardiac ICU',\n 'h1_albumin_max_apache_3j_diagnosis_diff',\n 'h1_albumin_min_apache_3j_diagnosis_diff',\n 'h1_bilirubin_min_apache_3j_diagnosis_diff',\n 'h1_lactate_max_apache_3j_diagnosis_diff',\n 'h1_lactate_min_apache_3j_diagnosis_diff',\n 'h1_arterial_pco2_max_apache_3j_diagnosis_diff',\n 'h1_albumin_max_profile_diff', 'diasbp_nans', 'diasbp_d1_h1_max_eq',\n 'diasbp_d1_h1_min_eq', 'heartrate_nans', 'mbp_nans' ,'mbp_d1_h1_min_eq',\n 'resprate_nans', 'resprate_d1_h1_max_eq', 'resprate_d1_h1_min_eq',\n 'spo2_nans', 'spo2_d1_h1_max_eq', 'sysbp_nans', 'temp_nans',\n 'temp_d1_h1_max_eq', 'albumin_nans' ,'albumin_d1_h1_max_eq',\n 'albumin_d1_h1_min_eq' ,'h1_albumin_range' ,'h1_albumin_mean',\n 'h1_albumin_s2n', 'bilirubin_nans', 'bilirubin_d1_h1_max_eq',\n 'bilirubin_d1_h1_min_eq', 'h1_bilirubin_range', 'h1_bilirubin_mean',\n 'h1_bilirubin_s2n' ,'bun_nans', 'bun_d1_h1_max_eq', 'h1_bun_range',\n 'h1_bun_s2n', 'calcium_nans' ,'h1_calcium_s2n', 'creatinine_nans',\n 'creatinine_d1_h1_max_eq', 'creatinine_d1_h1_min_eq' ,'h1_creatinine_range',\n 'h1_creatinine_s2n', 'glucose_nans', 'hco3_nans', 'hco3_d1_h1_max_eq',\n 'h1_hco3_range' ,'h1_hco3_s2n', 'hemaglobin_nans', 'hemaglobin_d1_h1_max_eq',\n 'hemaglobin_d1_h1_min_eq', 'h1_hemaglobin_range' ,'h1_hemaglobin_s2n',\n 'hematocrit_nans', 'hematocrit_d1_h1_max_eq', 'hematocrit_d1_h1_min_eq',\n 'h1_hematocrit_range', 'h1_hematocrit_s2n' ,'inr_nans' ,'inr_d1_h1_max_eq',\n 'inr_d1_h1_min_eq' ,'h1_inr_range', 'h1_inr_mean', 'lactate_nans',\n 'lactate_d1_h1_max_eq', 'lactate_d1_h1_min_eq', 'h1_lactate_range',\n 'h1_lactate_mean' ,'h1_lactate_s2n', 'platelets_nans',\n 'platelets_d1_h1_min_eq', 'h1_platelets_range', 'h1_platelets_s2n',\n 'potassium_nans' ,'h1_potassium_s2n', 'sodium_nans', 'sodium_d1_h1_max_eq',\n 'sodium_d1_h1_min_eq', 'wbc_nans' ,'wbc_d1_h1_min_eq' ,'h1_wbc_range',\n 'h1_wbc_s2n', 'arterial_pco2_d1_h1_max_eq', 'arterial_pco2_d1_h1_min_eq',\n 'h1_arterial_pco2_range' ,'h1_arterial_pco2_mean' ,'h1_arterial_pco2_s2n',\n 'arterial_ph_nans' ,'arterial_ph_d1_h1_max_eq', 'arterial_ph_d1_h1_min_eq',\n 'h1_arterial_ph_range' ,'h1_arterial_ph_s2n', 'arterial_po2_nans',\n 'arterial_po2_d1_h1_max_eq' ,'h1_arterial_po2_range' ,'h1_arterial_po2_s2n',\n 'pao2fio2ratio_nans' ,'pao2fio2ratio_d1_h1_max_eq',\n 'h1_pao2fio2ratio_range', 'h1_pao2fio2ratio_mean' ,'h1_pao2fio2ratio_s2n']\n\"\"\"\n#","9a767d38":"features = [c for c in train_df.columns if (c not in  ['diabetes_mellitus', 'encounter_id', 'hospital_id']) & (c not in  drop_cols)]\ntarget = ['diabetes_mellitus']\n\n# Prepare training and test data\nX, y = train_df[features], train_df[target]\n    \ndisplay(X.head().style.set_caption(\"X\"))\ndisplay(y.head().style.set_caption(\"y\"))\n\nX_test = test_df[features]","ed06e6c2":"\"\"\"\n# Hyper Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Create parameters to search\ngridParams = {\"num_leaves\" : [16, 32, 64, 128],\n             }\n\nmdl = lgb.LGBMClassifier(objective = 'binary',\n                         metric = \"auc\",\n                         seed = SEED,\n                         num_iterations = 300,\n                         #min_data_in_leaf=50,\n                         learning_rate =  0.1,\n                         lambda_l1 = 0,\n                         lambda_l2 = 0.1,\n                         bagging_fraction = 0.7,\n                         feature_fraction = 0.9, \n                        min_data_in_leaf = 200,)\n\n# Create the grid\ngrid = GridSearchCV(mdl, gridParams,\n                    verbose=4,\n                    cv=5,\n                    n_jobs=-1)\n# Run the grid\ngrid.fit(X, y)\n\n# Print the best parameters found\nprint(grid.best_params_)\nprint(grid.best_score_)\n\"\"\"","f0eb6edd":"N_SPLITS = 5\n\nmodel_params = {\n    \"objective\": \"binary\",\n    \"metric\":\"auc\",\n    \"seed\": SEED,\n    'num_iterations': 500,\n    \"learning_rate\": 0.05,\n    \"max_depth\": 8,\n    \"num_leaves\" : 32, \n    \"is_unbalace\" : True, \n    \"min_data_in_leaf\": 200,\n    \"lambda_l1\" : 1,\n    \"lambda_l2\": 0.1,\n    \"bagging_fraction\" : 0.7,\n    \"feature_fraction\" : 0.8, \n    \"min_split_gain\" : 0.5, \n    \"subsample_for_bin\" : 200,\n    \"n_jobs\" : -1,\n}\n\n# Initialize variables\ny_oof_pred = np.zeros(len(X))\ny_test_pred = np.zeros(len(X_test))\n\nkf = GroupKFold(n_splits=5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y, train_df.hospital_id)):\n    print(f\"Fold {fold + 1}:\")\n\n    # Prepare training and validation data\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)  \n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    # Define model\n    model = lgb.train(params=model_params,\n                      train_set=train_data,\n                      valid_sets=[train_data, val_data],\n                      verbose_eval=50,\n                     )\n\n    # Calculate evaluation metric\n    y_val_pred = model.predict(X_val)\n    print(f\"Original ROC AUC: {roc_auc_score(y_val, y_val_pred)}\")\n\n    # Make predictions\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred += model.predict(X_test)\n\n# Calculate evaluation metric for out of fold validation set\ny_test_pred = y_test_pred \/ N_SPLITS","9332d3aa":"def get_predictions(y_pred):\n    return (y_pred > 0.5).astype(int)\n\nprint(f\"Mean ROC AUC: {roc_auc_score(y, y_oof_pred)} \\n\")\n\ny_oof_pred = get_predictions(y_oof_pred) \n\nprint(f\"F1 Macro Score: {f1_score(y, y_oof_pred, average='macro')}\\n\")\nprecision, recall, _, _ = precision_recall_fscore_support(y, y_oof_pred, average=None)\nprint(f\"Precision: {precision} \\nRecall: {recall}\\n\")\n\nsns.heatmap(confusion_matrix(y, y_oof_pred), cmap='Blues', annot=True, fmt=\".5g\")\nplt.show()","afe42ff0":"train_df_pseudo = test_df[(pd.Series(y_test_pred) > 0.8) | (test_df.APACHE_diabetic_ketoacidosis == 1)].copy()\ntrain_df_pseudo['diabetes_mellitus'] = 1\nprint(f\"Additional {len(train_df_pseudo)} data points for pseudo labelling.\")\ntrain_df_mod = train_df.append(train_df_pseudo)\n\n# Shuffle dataframe\ntrain_df_mod = train_df_mod.sample(frac=1).reset_index(drop=True)","47a63f2d":"# Prepare training and test data\nX, y = train_df_mod[features], train_df_mod[target]\n\ndisplay(X.head().style.set_caption(\"X\"))\ndisplay(y.head().style.set_caption(\"y\"))\n\nX_test = test_df[features]","da3b5bb6":"N_SPLITS = 5\n\nmodel_params = {\n    \"objective\": \"binary\",\n    \"metric\":\"auc\",\n    \"seed\": SEED,\n    'num_iterations': 5000,\n    \"early_stopping_rounds\": 250,\n    \"learning_rate\": 0.01,\n    \"max_depth\": 8,\n    \"num_leaves\" : 32, \n    \"is_unbalace\" : True, \n    \"min_data_in_leaf\": 200,\n    \"lambda_l1\" : 1,\n    \"lambda_l2\": 0.1,\n    \"bagging_fraction\" : 0.7,\n    \"feature_fraction\" : 0.8, \n    \"min_split_gain\" : 0.5, \n    \"subsample_for_bin\" : 200,\n    \"n_jobs\" : -1,\n}","eb7235c2":"# Initialize variables\ny_oof_pred = np.zeros(len(X))\ny_test_pred = np.zeros(len(X_test))\n\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n    print(f\"Fold {fold + 1}:\")\n\n    # Prepare training and validation data\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)  \n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    # Define model\n    model = lgb.train(params=model_params,\n                      train_set=train_data,\n                      valid_sets=[train_data, val_data],\n                      verbose_eval=50,\n                     )\n\n    # Calculate evaluation metric\n    y_val_pred = model.predict(X_val)\n    print(f\"Original ROC AUC: {roc_auc_score(y_val, y_val_pred)}\")\n\n    # Make predictions\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred += model.predict(X_test)\n\n# Calculate evaluation metric for out of fold validation set\ny_test_pred_stratified = y_test_pred \/ N_SPLITS","ead7ec9f":"print(f\"Mean ROC AUC: {roc_auc_score(y, y_oof_pred)} \\n\")\n\ny_oof_pred = get_predictions(y_oof_pred) \n\nprint(f\"F1 Macro Score: {f1_score(y, y_oof_pred, average='macro')}\\n\")\nprecision, recall, _, _ = precision_recall_fscore_support(y, y_oof_pred, average=None)\nprint(f\"Precision: {precision} \\nRecall: {recall}\\n\")\n\nsns.heatmap(confusion_matrix(y, y_oof_pred), cmap='Blues', annot=True, fmt=\".5g\")\nplt.show()\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importance(importance_type=\"gain\"),X.columns)), columns=['Value','Feature'])\n\nfig, ax = plt.subplots(1,2, figsize=(16, 12))\nfeature_imp.sort_values('Value', ascending=True).tail(30).set_index('Feature').plot.barh(ax=ax[0])\nax[0].set_title('Top 20 Most Important Features')\nfeature_imp.sort_values('Value', ascending=True).head(30).set_index('Feature').plot.barh(ax=ax[1])\nax[1].set_title('Bottom 20 Least Important Features')\nplt.tight_layout()\nplt.show()\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n(train_df.diabetes_mellitus.value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[0])\nax[0].set_title('Ground Truth')\n\ndisplay((pd.Series(get_predictions(y_test_pred_stratified)).value_counts(normalize=True)*100))\n(pd.Series(get_predictions(y_test_pred_stratified)).value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[1])\nax[1].set_title('Predicted')\nfor i in range(2):\n    ax[i].set_ylim([0, 100])\n    ax[i].set_ylabel('Percentage of Patients[%]', fontsize=14)\n    ax[i].set_xticklabels(['No Diabetes', 'Diabetes'], fontsize=14, rotation=0)\nplt.show()","50c7cc32":"# Initialize variables\ny_oof_pred = np.zeros(len(X))\ny_test_pred = np.zeros(len(X_test))\n\nkf = GroupKFold(n_splits=5)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X, y, train_df_mod.hospital_id)):\n    print(f\"Fold {fold + 1}:\")\n\n    # Prepare training and validation data\n    X_train = X.iloc[train_idx].reset_index(drop=True)\n    X_val = X.iloc[val_idx].reset_index(drop=True)\n\n    y_train = y.iloc[train_idx].reset_index(drop=True)\n    y_val = y.iloc[val_idx].reset_index(drop=True)  \n\n    train_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_val, label=y_val)\n\n    # Define model\n    model = lgb.train(params=model_params,\n                      train_set=train_data,\n                      valid_sets=[train_data, val_data],\n                      verbose_eval=50,\n                     )\n\n    # Calculate evaluation metric\n    y_val_pred = model.predict(X_val)\n    print(f\"Original ROC AUC: {roc_auc_score(y_val, y_val_pred)}\")\n\n    # Make predictions\n    y_oof_pred[val_idx] = y_val_pred\n    y_test_pred += model.predict(X_test)\n\n# Calculate evaluation metric for out of fold validation set\ny_test_pred_grouped = y_test_pred \/ N_SPLITS","4fe02fd0":"print(f\"Mean ROC AUC: {roc_auc_score(y, y_oof_pred)} \\n\")\n\ny_oof_pred = get_predictions(y_oof_pred) \n\nprint(f\"F1 Macro Score: {f1_score(y, y_oof_pred, average='macro')}\\n\")\nprecision, recall, _, _ = precision_recall_fscore_support(y, y_oof_pred, average=None)\nprint(f\"Precision: {precision} \\nRecall: {recall}\\n\")\n\nsns.heatmap(confusion_matrix(y, y_oof_pred), cmap='Blues', annot=True, fmt=\".5g\")\nplt.show()\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importance(importance_type=\"gain\"),X.columns)), columns=['Value','Feature'])\n\nfig, ax = plt.subplots(1,2, figsize=(16, 12))\nfeature_imp.sort_values('Value', ascending=True).tail(30).set_index('Feature').plot.barh(ax=ax[0])\nax[0].set_title('Top 20 Most Important Features')\nfeature_imp.sort_values('Value', ascending=True).head(30).set_index('Feature').plot.barh(ax=ax[1])\nax[1].set_title('Bottom 20 Least Important Features')\nplt.tight_layout()\nplt.show()\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n(train_df.diabetes_mellitus.value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[0])\nax[0].set_title('Ground Truth')\n\ndisplay((pd.Series(get_predictions(y_test_pred_grouped)).value_counts(normalize=True)*100))\n(pd.Series(get_predictions(y_test_pred_grouped)).value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[1])\nax[1].set_title('Predicted')\nfor i in range(2):\n    ax[i].set_ylim([0, 100])\n    ax[i].set_ylabel('Percentage of Patients[%]', fontsize=14)\n    ax[i].set_xticklabels(['No Diabetes', 'Diabetes'], fontsize=14, rotation=0)\nplt.show()","54293621":"y_test_pred = 0.6 * y_test_pred_stratified + 0.4 * y_test_pred_grouped","6ebeafc3":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n(train_df.diabetes_mellitus.value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[0])\nax[0].set_title('Ground Truth')\n\ndisplay((pd.Series(get_predictions(y_test_pred)).value_counts(normalize=True)*100))\n(pd.Series(get_predictions(y_test_pred)).value_counts(normalize=True)*100).plot(kind='bar', color=['mediumseagreen', 'lightcoral'], ax=ax[1])\nax[1].set_title('Predicted')\nfor i in range(2):\n    ax[i].set_ylim([0, 100])\n    ax[i].set_ylabel('Percentage of Patients[%]', fontsize=14)\n    ax[i].set_xticklabels(['No Diabetes', 'Diabetes'], fontsize=14, rotation=0)\nplt.show()","5398ddde":"submission = test_df[[\"encounter_id\"]].copy()\nsubmission['diabetes_mellitus'] = y_test_pred\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head()","0e982e53":"## Body Mass Index (BMI)\nThe Body Mass Index (BMI) is a metric to indicate how a persons weight is in relation to their height and is calculated as follows: $\\text{BMI} = \\dfrac{\\text{weight}}{\\text{height}^2}$\n\nSince we have the patients' height and weight, we can do a quick sanity check of the BMI data. The sanity check showed that there are deviations between the calculated BMI and the given BMI values due to the limited values of weight and height. The weight and height can be reconstructed, however, we will skip this step here.","a86a5930":"## Pseudo Labelling \nWe will run the model once to use predictions with a high confidence from the testing set as additional training data.","a7af801e":"## Vitals\n\nThere are 32 vital sign and blood work related variables that are measured. Each variable has a minimum and a maxmimum value over the timespan of the first hour and the first 24 hours after being admitted to the ICU. Therefore, there are four measures to each variable (`h1_[...]_min`, `h1_[...]_max`,`d1_[...]_min`,`d1_[...]_max`). This results in a total of 128 (32x4) variables related to vital signs and blood work.\n\nFrom below plot we can see that the **measurements from the first hour have a lot more missing values** than the measurements taken in the first 24 hours.\n\nFurthermore, there are three blood pressure related variables (`diasbp`, `mbp`, `sysbp`) which each have two additional columns (`[...]_invasive` and `[...]_noninvasive`). For these variables **all invasively measured columns have a high rate of missing values**.","a82f9168":"As suggested by https:\/\/www.kaggle.com\/siavrez\/2020fatures\/ and https:\/\/www.kaggle.com\/kainsama\/lgbm-wids2021-v0-1-1, we can group the vital signs by the APACHE diagnosis and patient profile.","701368d2":"## Categorical Columns\nWe will fill the missing values for the `hospital_admit_source` and `icu_admit_source` already have columns indicating missing values, such as 'Other' or 'Unknown. We will be using these.\n\nFor `icu_stay_type` and `icu_type`, we will be filling the missing values with the column most common value.","d63e553a":"# References\n\n## Links and Papers\n[1] American Diabetes Association. (2019). 2. Classification and diagnosis of diabetes: standards of medical care in diabetes\u20142019. Diabetes care, 42(Supplement 1), S13-S28.\n\n[2] World Health Organization (2020): Diabetes. https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/diabetes \n\n[3] Nnamoko, N., & Korkontzelos, I. (2020). Efficient treatment of outliers and class imbalance for diabetes prediction. Artificial Intelligence in Medicine, 104, 101815.\n\n[4] Sirivole, M. R., & Eturi, S. A. (2017). A study on blood urea and serum creatinine in diabetes mellitus from Sangareddy District, Telangana, India. Intern J Med Health Res, 3, 132-6.\n\n[5] https:\/\/pubmed.ncbi.nlm.nih.gov\/26972930\/\n\n\n## Most Valuable Kernels\nhttps:\/\/www.kaggle.com\/siavrez\/2020fatures \n\nhttps:\/\/www.kaggle.com\/kainsama\/single-lgbm-v0-1-0\n\nhttps:\/\/www.kaggle.com\/kainsama\/lgbm-wids2021-v0-1-1\n\nhttps:\/\/www.kaggle.com\/danofer\/wids-2020-competitive-1st-place-component\n\nhttps:\/\/www.kaggle.com\/jayjay75\/3rd-place-nn-wids2020?scriptVersionId=29209297\n\nhttps:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances\n\n## Most Valuable Dataset\nhttps:\/\/www.kaggle.com\/danofer\/apache-iiij-icu-diagnosis-codes?select=icu-apache-codes-ANZICS.csv\n","979f3c3a":"## Categorical Features: One Hot Encoding\nOne Hot Encode categorical columns.","b977ee91":"## APACHE\n\nhttps:\/\/www.kaggle.com\/danofer\/apache-iiij-icu-diagnosis-codes?select=icu-apache-codes-ANZICS.csv\n\nAPACHE II diagnosis and APACHE IIIj diagnosis contain redundant information. APACHE IIIj diagnosis contains more fine-grained information than APACHE II diagnosis. APACHE IIIj diagnosis will be dropped later.\n","665c0897":"## Drop Duplicate Columns\n`paco2_apache` and `paco2_for_ph_apache` are identical and one of them will be dropped. \n`d1_inr_min\/max` and `h1_inr_min\/max` are also identical. However, they will not be dropped at this stage to use them for further feature engineering in the followng steps.","8268b1ff":"## Admit Source\n`hospital_admit_source` and `icu_admit_source` will be condensed to one feature. The following heatmap shows how both are related to each other. We will fill missing values for `icu_admit_source` based on the information of `hospital_admit_source` if available.","72b847d2":"## Unknown Gender, Weight & Height\nWe will drop 18 rows of training data where gender, weight and height of the patient are unknown.","b6e6c32f":"## Weight and Height\nFor the weight and height, we will create lookup tables with the mean values grouped by gender, ethnicity, and age. The missing values for weight and height will be filled from the lookup tables.","ebbde8b0":"# Handling Missing Values\n\n## Gender\nBased on the age, height, and weight of a patient, we will fill the missing values for gender by prediction with logistic regression.","647ea464":"## Blood Pressure\n\nA patient's blood pressure level can be categorized according to the systolic and diastolic blood pressure.","2973a3ee":"The vitals have a minimum and a maximum values from the first 24 hours. From the values we can create some additional features: **range, mean and signal to noise**. Additionally, we can counts the number of nans and whether d1 and h1 values are equals as suggested by https:\/\/www.kaggle.com\/siavrez\/2020fatures\/.","346228d1":"We will drop the `invasive`\/`noninvasive` columns due to their high rate of missing values and their redundancy with the  `diasbp`, `sysbp`, and `mpb` columns. ","42af06e7":"Next, we will create some features based on the NaN counts. This idea is originally from https:\/\/www.kaggle.com\/danofer\/wids-2020-competitive-1st-place-component.","4da4e1f9":"### Model 2: CV2 with StratifiedKFold","a717f017":"# Data Cleaning\n\n## Identifiers\nThere are 204 unique `hospital_id` in the training data and 190 in the test data. The `hospital_id` in the training and the testing data are disjunct - meaning that none of the `hospital_id`  in the training data are in the testing data and vice versa. The `hospital_id` in the training data range from 1 to 204 while they range from 10001 to 10199 in the testing data. We could check whether `hospital_id` 1 would correspond to `hospital_id` 10001 in testing data. However, since intuitively the `hospital_id` is not a relevant feature whether a patient has been diagnosed with diabetes before, we will not analyse this further. Therefore, **`hospital_id` should probably be not included in the final features.**\nHowever, we will **use `hospital_id` as a group for GroupKFold validation in the training step.**\n\nFurthermore, `encounter_id` is used to identify a patient. This identifier will also not be included as a feature but is relevant for further steps.\n\nFinally, `icu_id` has shared values across the raining and testing datasets and therefore, it will not be dropped.\n\n## Readmission Status\n\nFurthermore, we will drop the column `readmission_status` because it only has one unique value of 0.","6fe7b9c5":"### Model 2: CV2 with GroupKFold","62ba0683":"## Pregnancy Probability\nSince diabetes can occur during pregnancy, we will create a feature that can help the model evaluate whether a patient might be pregnant of not\nhttps:\/\/www.cdc.gov\/nchs\/products\/databriefs\/db136.htm","187fb04e":"## Training and Validation","4795372d":"We can also see that there are columns with similar names, such as e.g. `glucose_apache` and `d1_glucose_min`\/`d1_glucose_max`.\nAccording to the data dictionary, the `_apache` variables are:\n> [...] measured during the first 24 hours which results in the highest APACHE III score\n\nand the `d1_..._max` and `d1_..._min` variable are:\n> The [lowested\/highest ...] concentration of the patient [...] during the first 24 hours of their unit stay\n\nThe variables for which we seem to have multiple columns are: \n* `albumin`:\n* `bilirubin`:\n* `bun`: blood urea nitrogen\n* `creatinine`:\n* `glucose`: \n* `hematocrit`: volume proportion of red blood cells in blood\n* `resprate`: respiratory rate\n* `sodium`:\n* `temp`: patient's core temperature\n* `wbc`: white blood cell count\n* `heart_rate`\n\nFrom below plots, we can see that:\n* `glucose_apache`, `resprate_apache`, `creatinine_apache` and `wbc_apache` have a bimodal distribution (two peaks). This could be caused by different measurement techniques depending on the facility. These columns are not feasible to fill the missing values in the `d1_...` columns.\n* `_apache` and `d1_..._min` have a similar distribution for albumin, hematocrit, temp, sodium. These can be used to fill missing values.\n* `_apache` and `d1_..._max` have a similar distribution for bilirubin and bun. These can be used to fill missing values.\n\nThe `d1...min\/max` values are similar to the `...apache` values. Therefore, we can **reconstruct the missing values in `d1_...max` from the `..._apache` columns**. ","b182ce0f":"The **oxigenation index or Horowitz index** is the ratio of partial pressure of oxygen in blood (PaO2) and the fraction of oxygen in the inhaled air (FiO2). Therefore, we can fill the missing values of `d1_pao2fio2ratio_max` with `pao2_apache`\/`fio2_apache`.","ebc76b19":"## Diabetic Ketoacidosis\nSince APACHE 3J Diagnosis 702 corresponds to diabetic ketoacidosis, we will create this feature. However, not all patients with diabetic ketoacidosis are marked with diabetes mellitus as discussed [in this discussion](https:\/\/www.kaggle.com\/c\/widsdatathon2021\/discussion\/220093).","e8634b7b":"# Model\n## Feature Selection\n\nAs suggested in https:\/\/www.kaggle.com\/siavrez\/2020fatures, we will select features based on null importances as described in https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances.","5c7c987c":"# Feature Engineering","2318e34e":"# Women in Data Science Datathon 2021\n\nThis is my [WiDS Datathon 2021](https:\/\/www.kaggle.com\/c\/widsdatathon2021\/overview\/description) solution write up.\n\nA lot of feature engineering and feature selection ideas are based on the following two notebooks. \n* https:\/\/www.kaggle.com\/siavrez\/2020fatures \n* https:\/\/www.kaggle.com\/kainsama\/lgbm-wids2021-v0-1-1\n\n# Problem Definition\n\nIn the Women in Data Science (WiDS) Datathon 2021, we will build a model to **determine whether a patient has been diagnosed with Diabetes Mellitus before** within the first 24 hours of being admitted to an Intensive Care Unit (ICU).\nTo improve a patient's outcome in an ICU knowledge about their medical conditions can improve clinical decisions.\nHowever, often a patient's medical records are not immediately available due to transfer times. \nAn additional challenge is when a patient is not able to provide such information due to their health condition, e.g. shock or unconsiousness.\nTherefore, it is important to be able to diagnose whether a patient has chronical diseases based on data that can be gathered within the first 24 hours of being admitted to an ICU.\n![20944859.jpg](attachment:20944859.jpg)\n<a href=\"http:\/\/www.freepik.com\">Designed by vectorjuice \/ Freepik<\/a>\n\n## Diabetes Mellitus\n\nAccording to the World Health Organization (WHO) Diabetes Mellitus, or commonly know as diabetes, is defined as follows:\n> Diabetes is a chronic disease that occurs either when the pancreas **does not produce enough insulin or when the body cannot effectively use the insulin it produces**. Insulin is a hormone that regulates blood sugar. Hyperglycaemia, or **raised blood sugar**, is a common effect of uncontrolled diabetes and over time leads to serious damage to many of the body's systems, especially the nerves and blood vessels. - [WHO Diabetes Fact Sheet](https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/diabetes)\n\nThere are two types of diabetes - **Type 1 diabetes and Type 2 diabetes**. Type 2 diabetes is more common than Type 1 diabetes and often results from excess body weight and physical inactivity while Type 1 diabetes is independent on body size. Additionally, there is **Gestational diabetes** in which a woman without diabetes develops high blood sugar levels during pregnancy. The latter usually resolves after birth while the other two types of diabetes have to be treated in the longterm.\n\nAround **8.5 % of the adult population** is diagnosed with Diabetes [2] independent of the gender. \n\n**Indicators for Diabetes Mellitus**\nDiabetes mellitus is characterized by high blood sugar levels over a prolonged period of time and is diagnosed by demonstrating any one of the following:\n* Fasting plasma glucose level \u2265 7.0 mmol\/L (126 mg\/dL)\n* Plasma glucose \u2265 11.1 mmol\/L (200 mg\/dL) two hours after a 75 gram oral glucose load as in a glucose tolerance test\n* Symptoms of high blood sugar and casual plasma glucose \u2265 11.1 mmol\/L (200 mg\/dL)\n* Glycated hemoglobin (HbA1C) \u2265 48 mmol\/mol (\u2265 6.5 DCCT %)\n\nSo, here are some ideas for the following analysis:\n* Look at blood sugar levels but keep in mind that patients without diabetes can have high blood sugar levels as well\n* Look at effects of diabetes on other organs, such as the kidney but keep in mind that patients without diabetes can have kidney disfunctions as well\n* Race could be included since some races apparently tend to get diabetes at lower BMIs\n* Most interesting categories of features are: demographic, vitals, labs, labs blood gas\n* APACHE is an illness severity score. Also the APACHE score has suboptimal calibration which makes it hard to compare across different hospitals. Therefore, the APACHE covariates features are probably not as interesting for detecting diabetes. We will focus on them if we have some time left at the end.\n* We need to consider whether we want to use less frequently measured variables\n\n\n# Data Overview\nThe data is provided by MIT\u2019s GOSSIS (Global Open Source Severity of Illness Score) initiative. It contains:\n* 179 features from 6 feature categories: identifier, demographic, APACHE covariate, vitals, labs, labs blood gas, APACHE comorbidity\n* 1 target `diabetes_mellitus`","678fb7de":"Let's do some data cleaning, by swapping min and max if the min value is bigger than the max value. Also, if the value in the first hour is more extreme than the value of the first day it will be replaced.","16e1a2db":"## Ensemble","ffbe00e4":"# Submission","326692d1":"The class imbalance of the target variable is a challenge in any machine learning problem. Therefore, it is also a challenge in the application for diabetes diagnosis [3]. \nAccording to WHO in 2014 8.5 % of adults were diagnoses with diabetes [2]. **In the provided training data 22% of patients are diagnosed with diabetes**.","976d6e78":"## Implausible Age\nWe are working with data from **young adults and adults aging 16 and older**. However, in the training data, there are 30 data points with `age = 0`. For the initial analysis purposes, we will drop these data points because they have on average have twice as many NaN values then the average data point. Usually, dropping data is not recommended. However, these data points only account for 0.02% of data loss in addition to the earlier argument.","822869bf":"## Hyper Parameter Tuning\n\n* `objective`: `binary` because we have binary classification problem\n* `metric`: `auc` as the competition metric\n* `seed`: can be set to anything as long as it is set for reproducible results\n* It is recommended to use smaller `learning_rate` with larger `num_iterations`. Also, you should use `early_stopping_rounds` if you go for higher `num_iterations` to stop your training when it is not learning anything useful. `learning_rate` is set to 0.1 during hyper parameter tuning and decreased afterwards\n* `num_leaves` and `max_depth`: Control the complexity of the tree. `num_leaves` < 2^`max_depth` to avoid overfitting. Default 31,-1\n* `sample_pos_weight` = number of negative samples \/ number of positive samples or `is_unbalace`"}}