{"cell_type":{"eee5e48f":"code","44f545af":"code","6f0b4b1a":"code","01b8c557":"code","239a4f0a":"code","6ca295db":"code","6cdb8925":"code","e022efbf":"code","0d69ca98":"code","9f38db4f":"code","8fade042":"code","9cde843f":"code","78bec359":"code","367d2d7c":"code","7d215a5f":"code","caf57154":"code","b19b0f33":"code","2d153f39":"code","e31e8752":"code","056e412e":"code","975f0421":"code","f8d2fda7":"code","710f71e1":"code","f440221a":"code","44399ad7":"code","7463b295":"code","6286668e":"code","385fb8f3":"code","46ba97e5":"code","b9df5798":"code","6121fad1":"code","618032dc":"code","712bedf7":"code","47592064":"code","d1a58e2d":"code","f022fd76":"code","e3239517":"code","2ec13cbc":"code","d7916ccc":"code","be3ec73b":"code","58f66113":"code","99cccf3e":"code","d8460412":"code","374ac3d2":"code","19d4c9eb":"code","88b0b769":"code","b4fdb4c8":"code","f2f7e267":"code","a5ab6f2c":"code","ac180256":"code","4eb14024":"code","7f39c88d":"code","b92dc1db":"code","b867b99f":"code","aa11bbd2":"code","be178aef":"code","61af69c0":"code","b2e34094":"code","f4953df3":"code","9560035c":"code","a8472e3f":"code","2e69feb9":"code","8690fdbc":"code","36cc7087":"code","9a229ad8":"code","3d2a74e0":"code","36cb92a9":"markdown","1d3e8577":"markdown","a224364d":"markdown","69065d3f":"markdown","f3ed9071":"markdown","def4cb41":"markdown","3296dd8a":"markdown","1a51e775":"markdown","3f8d9882":"markdown","66bfdc04":"markdown","7d42b59d":"markdown","b0ad7997":"markdown","b5dc925f":"markdown","85a59a88":"markdown","e0e33bb7":"markdown","82fc4e56":"markdown","3fea63f3":"markdown","cec2f581":"markdown","7d930d84":"markdown","4bce6662":"markdown","27371498":"markdown","b39f6577":"markdown","201c2228":"markdown","ca079c9a":"markdown","c0c9c1f7":"markdown","fdb9e9d6":"markdown","b88be045":"markdown","bb01cf1c":"markdown"},"source":{"eee5e48f":"%env WANDB_DISABLED=True\nimport collections\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoModelForQuestionAnswering, AutoTokenizer\n\nMODEL_NAME = '..\/input\/xlm-roberta-squad2\/deepset\/xlm-roberta-large-squad2'\n\n\nmax_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","44f545af":"def prepare_train_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start\/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","6f0b4b1a":"def prepare_train_features(examples):\n    # Tokenize\n    tokenized_examples = tokenize(examples) \n    \n    # Set the start and end position tokens\n    tokenized_examples = set_start_and_end_positions(tokenized_examples, examples['answers']) \n    return tokenized_examples","01b8c557":"def tokenize(examples):\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\"\n    )\n    return tokenized_examples","239a4f0a":"question = \"What was the World War II?\"\nctx = \"\"\"World War II, often abbreviated as WW2, was a global war that lasted from 1939 to 1945. It involved the vast majority of the world's countries forming two opposing military alliances: the Allies and the Axis powers.\"\"\"","6ca295db":"tokenized_example = tokenizer(question, ctx)\nprint(tokenized_example)","6cdb8925":"tokenized_example.keys()","e022efbf":"# The attention mask for now is full of ones.\n# It will come into play with padding and max_length in a moment.\nprint(tokenized_example['attention_mask'])","0d69ca98":"print(tokenized_example['input_ids'])","9f38db4f":"# With decode we can go back from the token ids to the tokens\ntokenizer.decode(tokenized_example['input_ids'])","8fade042":"tokenized = tokenizer(question, ctx, padding=\"max_length\", max_length=100)\ntokenized","9cde843f":"# The token with id 1 is the special token <pad>\ntokenizer.decode(tokenized['input_ids'])","78bec359":"# Call with max_length=40 and stride=5\ntokenized = tokenizer(question, ctx, truncation=\"only_second\", stride=5, max_length=40, return_overflowing_tokens=True)\ntokenized","367d2d7c":"# The previous call created 3 tokenized samples for our unique (question, context)\nlen(tokenized['input_ids'])","7d215a5f":"# See the length of each sample (40, 40, 21)\n# Also note that the question is always present (we only want to split the context, that's why truncate is set to \"only_second\")\n# In this case, we allowed an overlap of 5 tokens\nfor tokens in tokenized['input_ids']:\n    print(len(tokens), tokenizer.decode(tokens))","caf57154":"tokenized['overflow_to_sample_mapping']","b19b0f33":"tokenized = tokenizer(question, ctx, \n                      truncation=\"only_second\", \n                      stride=10, \n                      max_length=80, \n                      return_overflowing_tokens=True, \n                      return_offsets_mapping=True, \n                      padding=\"max_length\")\ntokenized","2d153f39":"tokens = tokenized['input_ids'][0]\noffsets = tokenized['offset_mapping'][0]","e31e8752":"tokens[:10]","056e412e":"offsets[:10]","975f0421":"# The id of token at index 4\ntokens[4]","f8d2fda7":"# Which is this token\ntokenizer.decode(tokens[4])","710f71e1":"# Here, we get the same word using the offset mapping\nstart, end = offsets[4]\nquestion[start:end]","f440221a":"def set_start_and_end_positions(tokenized_examples, example_answers):\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    \n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        sample_index = sample_mapping[i]\n        answers = example_answers[sample_index]\n        \n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            \n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n    return tokenized_examples\n","44399ad7":"# We need the answer start and the answer text for this second part:\nquestion, ctx[44:56]","7463b295":"answers = [{'answer_start': [44], 'text': ['a global war']}]","6286668e":"# This is one of the examples we went over previously, it's the one that generates 3 splits\ntokenized_example = tokenizer(question, ctx, truncation=\"only_second\", stride=10, max_length=40, return_overflowing_tokens=True, return_offsets_mapping=True, padding=\"max_length\")\ntokenized_example","385fb8f3":"final_result = set_start_and_end_positions(tokenized_example, answers)\nfinal_result","46ba97e5":"# This means: splits 2 and 3 don't have the answer, split 1 has the answer and it starts in token 25\nfinal_result['start_positions']","b9df5798":"# This means: splits 2 and 3 don't have the answer, split 1 has the answer and it ends in token 27\nfinal_result['end_positions']","6121fad1":"# We can verify it easily:\ntokenizer.decode(final_result['input_ids'][0][25:27+1])","618032dc":"def set_start_and_end_positions(tokenized_examples, example_answers):\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n    \n    # The goal of this function is to populate these two lists\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    # Go over all the final training samples (already split)\n    for i, offsets in enumerate(offset_mapping):\n        \n        input_ids = tokenized_examples[\"input_ids\"][i]\n        \n        # Get the class token, will be used to fill samples with no answer\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        \n        # Sequece ids is a mask of 0s for the question and 1s for the context\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        \n        # Go back to the original set of samples (before the split)\n        sample_index = sample_mapping[i]\n        # And get the answer for it\n        answers = example_answers[sample_index] \n        \n        \n        # If there is not answer set, return CLS special token \n        # (I don't think this case applies in ChaII)\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start and end of the answer in the original full context \n            # (char position, 44 and 56 in the previous example)\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n            \n            # The two whiles below are a weird way to initialize two pointers \n            # to the context start and end\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            # If the answer is not in this split of the context, return CLS special token\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Advance with the start token pointer till the start_char is reached\n                while (token_start_index < len(offsets) \n                       and offsets[token_start_index][0] <= start_char):\n                    token_start_index += 1\n                # That is our start position\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                \n                # Go back with the end token pointer till the end_char is reached\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                # That is our end position\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n    return tokenized_examples","712bedf7":"# I named it plural to signal that it's a batch (although it's a batch of length 1)\nexamples = {'question': [question], 'context': [ctx], 'answers': [{'answer_start': [44], 'text': ['a global war']}], 'id': ['ww2-q1']}\ntokenized_examples = prepare_train_features(examples)\ntokenized_examples","47592064":"import torch\nfrom transformers import AutoModelForQuestionAnswering\n# In order to pass the example to a model we need to cast it to torch tensors\n# This is typically handled automatically by the \ud83e\udd17 Datasets, but here we are\n# outside of it\ntokenized_examples = {k: torch.tensor(v) for k, v in tokenized_examples.items()}\n\n# Load the model\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)","d1a58e2d":"# Since the model is already for English QA, we can just use it out of the box\npredictions = model(**tokenized_examples)\npredictions.keys()","f022fd76":"predictions.loss","e3239517":"# The 1 is the batch size\n# The 384 is the total tokens (we have padded to the max length of the model, which is 384)\n\npredictions.start_logits.size()","2ec13cbc":"# These are the logits for the first 5 tokens. \n# A higher logit means the model considers \n# it is more probable it is the starting token of the answer\npredictions.start_logits[0][:5]","d7916ccc":"# The token 25 is the one with the highest probability\ntorch.argmax(predictions.start_logits[0])","be3ec73b":"# Which is correct!\ntokenized_examples['start_positions']","58f66113":"# All the same can be done for end_logits\ntorch.argmax(predictions.end_logits[0])","99cccf3e":"# The most probable end position is not the actual one (27) but 34.\ntokenized_examples['end_positions']","d8460412":"# Turn logits into probabilities\nstart_probas = torch.nn.functional.softmax(predictions.start_logits[0], dim=0)\n# Get the token ids\ntoken_ids =tokenized_examples['input_ids'][0]\n\n# Iterate through the tokens and their probabilities, in descending order of probability\nfor i, (proba, token) in enumerate(sorted(zip(start_probas, token_ids), key=lambda x: x[0], reverse=True), 1):\n    # Print the first 10\n    if i <= 10:  \n        print(f\"{i:2d}) {proba*100:5.2f}%  - {tokenizer.decode(token)}\")","374ac3d2":"# Same for the end logits\nend_probas = torch.nn.functional.softmax(predictions.end_logits[0], dim=0)\nfor i, (proba, token) in enumerate(sorted(zip(end_probas, token_ids), key=lambda x: x[0], reverse=True), 1):\n    if i <= 10:  \n        print(f\"{i:2d}) {proba*100:5.2f}%  - {tokenizer.decode(token)}\")","19d4c9eb":"def prepare_validation_features(examples):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n\n    # We keep the example_id that gave us this feature and we will store the offset mappings.\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        context_index = 1 if pad_on_right else 0\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n\n        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n        # position is part of the context or not.\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n\n    return tokenized_examples","88b0b769":"def prepare_validation_features(examples):\n    tokenized_examples = tokenize(examples)\n    tokenized_examples = add_example_id_and_modify_offset_mapping(tokenized_examples, examples[\"id\"])\n    return tokenized_examples","b4fdb4c8":"def add_example_id_and_modify_offset_mapping(tokenized_examples, example_ids):\n    context_index = 1 \n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    tokenized_examples[\"example_id\"] = []\n\n    for i in range(len(tokenized_examples[\"input_ids\"])):\n        \n        sample_index = sample_mapping[i]\n        # Populate the \"example_id\" key with a reference\n        # to the original sample (before the context split phase)\n        # This is going to be used downstream\n        tokenized_examples[\"example_id\"].append(example_ids[sample_index])\n        \n        # The offset_mapping maps tokens to chars in the original piece of text\n        # Here we are setting to None all the tokens that don't belong to the context\n        # This is going to be used downstream\n        sequence_ids = tokenized_examples.sequence_ids(i)\n        tokenized_examples[\"offset_mapping\"][i] = [\n            (o if sequence_ids[k] == context_index else None)\n            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n        ]\n    return tokenized_examples","f2f7e267":"examples","a5ab6f2c":"# It's very similar to prepare_training_features\n# with example_id (which would have been useful had the input been large enough)\n# and a lot of None in the offset_mapping\ntokenized_validation_examples = prepare_validation_features(examples)\ntokenized_validation_examples","ac180256":"large_ctx = \"\"\"World War II or the Second World War, often abbreviated as WWII or WW2, was a global war that lasted from 1939 to 1945. It involved the vast majority of the world's countries\u2014including all of the great powers\u2014forming two opposing military alliances: the Allies and the Axis powers. In a total war directly involving more than 100 million personnel from more than 30 countries, the major participants threw their entire economic, industrial, and scientific capabilities behind the war effort, blurring the distinction between civilian and military resources. Aircraft played a major role in the conflict, enabling the strategic bombing of population centres and the only two uses of nuclear weapons in war to this day. World War II was by far the deadliest conflict in human history; it resulted in 70 to 85 million fatalities, a majority being civilians. Tens of millions of people died due to genocides (including the Holocaust), starvation, massacres, and disease. In the wake of the Axis defeat, Germany and Japan were occupied, and war crimes tribunals were conducted against German and Japanese leaders.\nWorld War II is generally considered to have begun on 1 September 1939, when Nazi Germany, under Adolf Hitler, invaded Poland. The United Kingdom and France subsequently declared war on Germany on the 3rd of September. Under the Molotov\u2013Ribbentrop Pact of August 1939, Germany and the Soviet Union had partitioned Poland and marked out their \"spheres of influence\" across Finland, Romania and the Baltic states. From late 1939 to early 1941, in a series of campaigns and treaties, Germany conquered or controlled much of continental Europe, and formed the Axis alliance with Italy and Japan (along with other countries later on). Following the onset of campaigns in North Africa and East Africa, and the fall of France in mid-1940, the war continued primarily between the European Axis powers and the British Empire, with war in the Balkans, the aerial Battle of Britain, the Blitz of the UK, and the Battle of the Atlantic. On 22 June 1941, Germany led the European Axis powers in an invasion of the Soviet Union, opening the Eastern Front, the largest land theatre of war in history and trapping the Axis powers, crucially the German Wehrmacht, in a war of attrition.\"\"\"","4eb14024":"# https:\/\/stackoverflow.com\/questions\/2465921\/how-to-copy-a-dictionary-and-only-edit-the-copy\nlarge_examples = dict(examples) # Copy losing the reference to the original\nlarge_examples","7f39c88d":"large_examples['question'].append(\"When did the WW2 begin?\")\nlarge_examples['context'].append(large_ctx)\nlarge_examples['answers'].append({'answer_start': [large_ctx.find(\"1 September\")], 'text': ['1 September 1939']})\nlarge_examples['id'].append('ww2-q2')\nlarge_examples","b92dc1db":"tokenized_validation_large_examples = prepare_validation_features(large_examples)\n# The second example generated 2 features\nlen(tokenized_validation_large_examples['input_ids'])","b867b99f":"# The second and third features reference ww2-q2 as their originating example\ntokenized_validation_large_examples['example_id']","aa11bbd2":"def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n    all_start_logits, all_end_logits = raw_predictions\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    predictions = collections.OrderedDict()\n\n    # Logging.\n    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_score = None # Only used if squad_v2 is True.\n        valid_answers = []\n        \n        context = example[\"context\"]\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n\n            # Update minimum null prediction.\n            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n            if min_null_score is None or min_null_score < feature_null_score:\n                min_null_score = feature_null_score\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index >= len(offset_mapping)\n                        or end_index >= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or offset_mapping[end_index] is None\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n                    valid_answers.append(\n                        {\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"text\": context[start_char: end_char]\n                        }\n                    )\n        \n        if len(valid_answers) > 0:\n            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n            # failure.\n            best_answer = {\"text\": \"\", \"score\": 0.0}\n        \n        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n        if not squad_v2:\n            predictions[example[\"id\"]] = best_answer[\"text\"]\n        else:\n            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n            predictions[example[\"id\"]] = answer\n\n    return predictions\n\n\nN_BEST = 20\nMAX_ANSWER_LENGTH = 30\n\ndef build_feature_per_example_map(examples, features):\n     # Build a map example to its corresponding features.\n    example_id_to_index = {k[\"id\"]: i for i, k in enumerate(examples)}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n    \n    return features_per_example\n\n\ndef is_valid_answer(start_index, end_index, offset_mapping):\n\n    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n    # to part of the input_ids that are not in the context.\n    if (\n        start_index >= len(offset_mapping)\n        or end_index >= len(offset_mapping)\n        or offset_mapping[start_index] is None\n        or offset_mapping[end_index] is None\n    ):\n        return False\n    # Don't consider answers with a length that is either < 0 or > MAX_ANSWER_LENGTH.\n    if end_index < start_index or end_index - start_index + 1 > MAX_ANSWER_LENGTH:\n        return False\n\n    return True\n\ndef get_valid_answer(start_index, end_index, offset_mapping, start_logits, end_logits, context):\n    start_char = offset_mapping[start_index][0]\n    end_char = offset_mapping[end_index][1]\n    answer = {\n            \"score\": start_logits[start_index] + end_logits[end_index],\n            \"text\": context[start_char: end_char]\n        }\n    return answer\n\ndef get_n_best_token_ids(logits):\n    return np.argsort(logits)[-1 : -N_BEST - 1 : -1].tolist()\n\n\ndef get_best_answer(valid_answers):\n    if len(valid_answers) > 0:\n        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        best_answer = best_answer[\"text\"]\n    else:\n        best_answer = \"\"\n    return best_answer\n\n\ndef get_valid_answers(example, features, logits, feature_indices):\n    # Logits for all the features, not only these ones\n    all_start_logits, all_end_logits = logits\n    \n    valid_answers = []\n    for feature_index in feature_indices:\n        # We grab the predictions of the model for this feature.\n        start_logits = all_start_logits[feature_index]\n        end_logits = all_end_logits[feature_index]\n        start_indexes = get_n_best_token_ids(start_logits)\n        end_indexes = get_n_best_token_ids(end_logits)\n    \n        offset_mapping = features[feature_index][\"offset_mapping\"]\n\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                if is_valid_answer(start_index, end_index, offset_mapping):\n                    answer = get_valid_answer(start_index, end_index, offset_mapping, start_logits, end_logits, example[\"context\"])\n                    valid_answers.append(answer)\n    return valid_answers\n\ndef process_one_example(example, features, logits, feature_indices):\n    valid_answers = get_valid_answers(example, features, logits, feature_indices)\n    return get_best_answer(valid_answers)","be178aef":"def postprocess_qa_predictions(examples, features, logits):\n    features_per_example = build_feature_per_example_map(examples, features)\n    predictions = {}\n    for example_index, example in enumerate(tqdm(examples)):\n        # The features (final validation examples) that were originated by this example\n        feature_indices = features_per_example[example_index]\n        # Get the best answer for this example with process_one_example\n        predictions[example[\"id\"]] = best_answer = process_one_example(example, features, logits, feature_indices)\n    return predictions","61af69c0":"# https:\/\/stackoverflow.com\/questions\/5558418\/list-of-dicts-to-from-dict-of-lists\nexamples_as_list = [dict(zip(examples,t)) for t in zip(*examples.values())]\ntokenized_validation_examples_as_list = [dict(zip(tokenized_validation_examples,t)) for t in zip(*tokenized_validation_examples.values())]","b2e34094":"# Get the start and end logits together in a tuple\nstart_logits = predictions.start_logits.detach().numpy()\nend_logits = predictions.end_logits.detach().numpy()\nlogits = (start_logits, end_logits)","f4953df3":"result = postprocess_qa_predictions(examples_as_list, tokenized_validation_examples_as_list, logits)\n\n# The best result, the one we have previously checked\nresult","9560035c":"# Here, all the same but for large_examples, with the 2nd example generating 2 features\ntokenized_large_train_examples = prepare_train_features(large_examples)\ntokenized_large_train_examples = {k: torch.tensor(v) for k, v in tokenized_large_train_examples.items()}\nlarge_preds = model(**tokenized_large_train_examples)\n\nlarge_examples_as_list = [dict(zip(examples,t)) for t in zip(*large_examples.values())]\ntokenized_validation_large_examples_as_list = [dict(zip(tokenized_validation_large_examples,t)) for t in zip(*tokenized_validation_large_examples.values())]\n\nlarge_logits = (large_preds.start_logits.detach().numpy(), large_preds.end_logits.detach().numpy())\npostprocess_qa_predictions(large_examples_as_list, tokenized_validation_large_examples_as_list, large_logits)","a8472e3f":"def build_feature_per_example_map(examples, features):\n    example_id_to_index = {k[\"id\"]: i for i, k in enumerate(examples)}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)    \n    return features_per_example","2e69feb9":"# The example with index 0 has only one feature, the one in position 0\nbuild_feature_per_example_map(examples_as_list, tokenized_validation_examples_as_list)","8690fdbc":"# Here, for large_examples, the example with index 1 (the second one) has 2 features associated with it, the 2nd and 3rd (with indexes 1 and 2)\nbuild_feature_per_example_map(large_examples_as_list, tokenized_validation_large_examples_as_list)","36cc7087":"def process_one_example(example, features, logits, feature_indices):\n    # Get all the valid answers from the features\n    valid_answers = get_valid_answers(example, features, logits, feature_indices)\n    # Return the best one\n    return get_best_answer(valid_answers)","9a229ad8":"def get_valid_answers(example, features, logits, feature_indices):\n    # Logits for all the features, not only these ones\n    all_start_logits, all_end_logits = logits\n    \n    # We will populate this list going through all the features for this example\n    valid_answers = []\n    \n    for feature_index in feature_indices:\n        # For each feature, get all the start and end logits\n        start_logits = all_start_logits[feature_index]\n        end_logits = all_end_logits[feature_index]\n        \n        # Get the indexes of the tokens with the highest ranked logits\n        # We are getting the best ranked 20 starts and 20 ends\n        start_indexes = get_n_best_token_ids(start_logits)\n        end_indexes = get_n_best_token_ids(end_logits)\n    \n        offset_mapping = features[feature_index][\"offset_mapping\"]\n\n        # Go through all the combinations of (start, end) looking for valid answers\n        # These are 20x20 (400) with the current configuration\n        for start_index in start_indexes:\n            for end_index in end_indexes:\n                # Check if (start_index, end_index) define a valid answer\n                if is_valid_answer(start_index, end_index, offset_mapping):\n                    # Extract the string defined by (start_index and end_index) if it is a valid one\n                    answer = get_valid_answer(start_index, end_index, offset_mapping, start_logits, end_logits, example[\"context\"])\n                    valid_answers.append(answer)\n    return valid_answers","3d2a74e0":"MAX_ANSWER_LENGTH = 30\n\ndef get_n_best_token_ids(logits, n_best=20):\n    # Sort a set of logits and get the indexes of the `n_best` largest ones\n    return np.argsort(logits)[-1 : -n_besto - 1 : -1].tolist()\n\ndef is_valid_answer(start_index, end_index, offset_mapping):\n    # The end token is before the start token, doesn't make sense. Invalid.\n    if end_index < start_index:\n        return False\n    \n    # Length exceeds a certain predefined amount of tokens. Invalid.\n    length = end_index - start_index + 1\n    if length > MAX_ANSWER_LENGTH:\n        return False\n    \n    # Indexes point to somewhere outside of the current tokens. Invalid.\n    # I am not sure if this case is possible sincerely.\n    if start_index >= len(offset_mapping) or end_index >= len(offset_mapping):\n        return False\n    \n    # The start or end indexes point outside of the CONTEXT. Invalid!\n    # The offset_mapping was prepared for this in prepare_validation_features()\n    # Setting to None all the tokens that are part of the question or special tokens\n    if offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n         return False\n\n    # In any other case, the start and end indexes are pointing to a valid answer\n    return True\n\n\ndef get_valid_answer(start_index, end_index, offset_mapping, start_logits, end_logits, context):\n    # Map from token indexes to char indexes of the context\n    start_char = offset_mapping[start_index][0]\n    end_char = offset_mapping[end_index][1]\n    \n    answer = {\n            # Extract the text\n            \"text\": context[start_char: end_char],\n            # Get the sum of the logits as a simple-enough way of scoring (and ranking)\n            # different valid answers\n            \"score\": start_logits[start_index] + end_logits[end_index],\n            \n        }\n    return answer\n\ndef get_best_answer(valid_answers):\n    if len(valid_answers) > 0:\n        # Get the answer with the best score\n        # The score was defined as the sum of the logits of the start and end indexes\n        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        best_answer = best_answer[\"text\"]\n    else:\n        # If no valid answers were found, return the empty string\n        best_answer = \"\"\n    return best_answer","36cb92a9":"It is not uncommon in NLP tasks to receive two pieces of text as the input. This is why the tokenizer is prepared to accept two string parameters as its first two arguments.\n\nIt can also receive a batch (a list) of elements as each of those parameters, but let's start with a simple call with plain strings and build on top of that.","1d3e8577":"Below we define the last four auxiliary functions with abundant comments:\n\n\n```python\ndef get_n_best_token_ids()\ndef is_valid_answer()\ndef get_valid_answer()\ndef get_best_answer()\n```","a224364d":"The original code receives a Hugging Face Dataset as the first and the second input, and relies on that for some simple tasks.\nIn particular, it uses the \"dual nature\" of the Dataset (the fact that it offers both the interfaces of a list and of a dictionary).\nSince our toy data is just a dictionary, I had to make a smallish change to a line of code and turn the dictionaries of lists into lists of dictionaries:\n","69065d3f":"That's it, that's the interface. Let's get inside the code now, we have to cover two functions and we will have finished: `build_feature_per_example_map` and `process_one_example`.\n\nLet's start with the easy one, `build_feature_per_example_map()`, by checking its output for our known examples:","f3ed9071":"Let's see 10 most probable tokens for start and for end positions:","def4cb41":"The offset mapping maps the tokens to their character start and end positions in the string they come from:","3296dd8a":"With this already on the table, we can discuss what the next two functions do.\nThey handle three different problems, each of them very simple in its nature:\n\n\n**1. Map from two logits to a string**\n\nFirst of all, the answers are built with a `start_position` and an `end_position`. The answer with the highest probability, and therefore the one the model would return with a simple unique solution, starts with \nthe token `a` and ends with the token `1945`: `a global war that lasted from 1939 to 1945`.\n\nWe need to implement this, tough \u2014all the way from the 2 logits to the string.\n\n**2. Get a _valid_ answer**\n\nOn the other hand, the start and end logits returned as the first answer might be invalid. For example, the end position might be pointing to a token that is before the one pointed by the start position. Or they might be pointing to the question section of the input and not to the context.\n\nThese are two consistency checks:\n1. Start position is before end position\n2. Both positions point to the context and not somewhere else\n\n\nSo a more thoughtful approach to obtaining the answer would be to get a set of the most probable ones and keep the best scored valid one.\n\n\n**3. Rollback context splitting**\n\nFinally, we have to rollback the splitting done to the context that we extensively covered in the previous section.\n\n\nWith these three tasks on the top of our head, we can jump into the two remaining functions:\n\n\n\n# prepare_validation_features()\n\n`prepare_validation_features` does three things:\n1. It tokenizes, exactly as in the train case\n2. It adds an \"example_id\" key to each tokenized example, with a reference to the original sentence the sample belonged to, before the context splitting.\n3. It modifies the key \"offset_mapping\" of each tokenized example, to make it easy to map from the logits to the string later on (in `postprocess_qa_predictions`)\n\nThe actual code is hidden in the cell below, and it has 45 lines of code.\nBelow, visible, you will find a modified version, with the same exact functionality, using two auxiliary functions. \n\nThe `tokenize` function, which is exactly the same as the one used for the `prepare_training_features` and a new one, defined below, `add_example_id_and_modify_offset_mapping`. This function performs the steps 2 and 3 in the enumeration presented above. I added my own comments to it.","1a51e775":"# Introduction\n\nIf you have been checking the public work, there are 3 functions that look a little bit intimidating: `prepare_train_features`, `prepare_validation_features`, and `postprocess_qa_predictions`.\n\nThe idea of this notebook is to approach them slowly, in order to lose our fears and gain confidence with them. Let's go!","3f8d9882":"### padding=\"max_length\"\n`padding=\"max_length\"` will put dummy tokens (`1`) at the end of the input sequence till it reaches the max_length.\n\nThis will affect the `attention_mask` output as well, setting to `0` all the positions associated with the dummy tokens, signaling to the model that they are not relevant pieces of information.\n","66bfdc04":"# Evaluation-related pre and post-processing\n\n\nWhat comes next is, as well, technically complex but theoretically simple enough.\n\nLet's start by understanding what the Question Answering model returns, which will make the understanding of everything else much easier.\n\n\n## The predictions of the QA head\nFor a given sample, the model should find a `start_position` and an `end_position` for the answer.\nThese positions are tokens indices. In concrete, the model works as a kind of double classifier.\n\nFor the `start_position`, it will output a probability distribution over all the possible tokens.\nThe ground truth, from this perspective, is a one-hot encoded vector with zeros in all positions and a one in the position of the token starting the answer.\nThis can be evaluated with Cross entropy, and, in fact, it is. The code below is the actual code of the roberta model in huggingface\n(you can check it by yourself in github [here](https:\/\/github.com\/huggingface\/transformers\/blob\/master\/src\/transformers\/models\/roberta\/modeling_roberta.py#L1542)):\n\n```python\nloss_fct = CrossEntropyLoss(ignore_index=ignored_index)\nstart_loss = loss_fct(start_logits, start_positions)\nend_loss = loss_fct(end_logits, end_positions)\ntotal_loss = (start_loss + end_loss) \/ 2\n```\n\nFor the `end_position`, it will generate a probability distribution as well. And this probability distribution can be evaluated against the one-hot-encoded vector of the actual end position.\n\nEach one of these problems is a common classification problem. Consider, for example, MNIST and digit recognition. For a sample showing a `3`, the actual answer would be encoded as `y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` (where there is a 1 in the forth position, corresponding to 3 starting from zero), while a prediction would be probability distribution over the 10 possible values: `y_proba = [0.01, 0.0, 0.05, 0.44, 0.15, 0.0, 0.05, 0.02, 0.25, 0.03]`\n\nIn the case of the `start_position`, instead of 10 possible digits, when have `n` possible tokens, and only one of them is the actual starting-position token (the one which will have the `1` in the `y_true`). \nThe model then generates a distribution over all the tokens, and with cross entropy we get a loss, that will help the model to learn in the subsequent iterations.\n\nThe same happens for the `end_position` and in the last line, the final reported loss is just the average of both losses. \n\nA little caveat: the softmax layer is not applied, so there is no probability distribution but logits, which can be translated to probabilities easily and provide a ranking or a score for the possible answers (the highest logit will map to the highest probability and so on).\n\nAll in all, the conclusion of this section is the following:\n\n<h2 style=\"text-align: center; background-color:#C8FF33;padding:40px;border-radius: 30px;\">\n    Question Answering is modeled as a double classification problem.\n<\/h2>\n\n\nNow let's see this in action:","7d42b59d":"# Wrapping up\n\nTo sum up, let's go through a high-level description of the functionality each of the three functions implements:\n\n### prepare_train_features()\nThis function turns the original train examples into train features, tokenizing them and taking care of large contexts when they appear, generating more than one \"train feature\" (tokenized train example).\nIt also takes care of massaging the squad-like target (`answer_start` pointing to a char and `answer_text`) into the model-friendly ones `start_position` and `end_position`, pointing to tokens in the feature.\n\n### prepare_validation_features()\nThis function tokenizes the input and takes care of large contexts mimiking the previous one. It adds a back-reference from features to their originating examples (`example_id`) and massages the `offset_mapping` to help `postprocess_qa_predictions()` down the path.\n\n### postprocess_qa_predictions()\nThis function gets one unique answer for an original example, starting with the model predictions for all the generated features for that example. It performs various steps for this:\n1. It gets a list of ranked valid answers from the predicted logits for each given feature performing various consistency checks of the pair (start, end). \n2. It translates from tokens to chars, extracting the final string answer.\n3. It takes care of going back from the features to the original example.\n\n\n&nbsp;\n&nbsp;\n\n# References\n* [\ud83e\udd17 QA fine-tuning example reference](https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb)\n* [Question Answering Tutorial](https:\/\/www.kaggle.com\/thedrcat\/question-answering-tutorial) by thedrcat.\n* [\ud83e\udd17 Preprocessing tutorial](https:\/\/huggingface.co\/transformers\/preprocessing.html)\n* [The tokenization pipeline](https:\/\/huggingface.co\/docs\/tokenizers\/python\/latest\/pipeline.html#the-tokenization-pipeline)\n* [HuggingFace course](https:\/\/huggingface.co\/course\/chapter1)\n\n\n\n## What's next?\n\nStay tuned, I'm working on the next 2 notebooks of the series: `Exploring Public Models Revisited` and `Reviewing squad2, mlqa and others`\n\n\n\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n&nbsp;\n\n## Remember to upvote the notebook if you found it useful! \ud83e\udd17\n\n","b0ad7997":"The simplest returned value from a `tokenizer` call has the keys `input_ids` and `attention_mask`.","b5dc925f":"### return_offsets_mapping\n\nThe last argument: `return_offsets_mapping=True`. This argument will add the key `offset_mapping` to the output, which will be used, along with `overflow_to_sample_mapping` in the next step.\n\nThis is a full call with all the arguments for the tokenizer by the way:","85a59a88":"### truncation, stride, return_overflowing_tokens\n\nThis set of arguments require a more extensive explanation. Stick with me till the end, please:\n\n* A model can only process sequences up to a certain amount of tokens, the maximum sequence length (`max_length`, which in this case is 384)\n* A common practice is to truncate the sequences to that length, using `truncation=True`\n* But for QA, since truncation can drop the answer, we cannot do just that. We need to do something smarter.\n* If the concatenation of the question and the context is larger than the max_length, we will split the context into various pieces, creating multiple train samples from a unique question-context pair. These  samples are referred to as \"features\" in all the code.\n* These train samples \/ \"features\" have the following form: (question, part 1 of the context), (question, part 2 of the context),... (question, part n of the context)\n* Some of these train samples will have an answer; some won't.\n* This is controlled by the parameters `truncation=\"only_second\"` and `return_overflowing_tokens=True`\n* An overlap between context splits is allowed to avoid the edge case when an answer is cut by the split. This is controlled by the `stride` parameter, which provides the size of this overlap.\n\n\nThe problem is explained in much more detail in the notebook [Question Answering Tutorial](https:\/\/www.kaggle.com\/thedrcat\/question-answering-tutorial) by thedrcat.\n","e0e33bb7":"\nThe function receives 3 main arguments:\n1. the original examples (what we are calling `examples`), \n2. the tokenized, split examples that `prepare_validation_features()` outputs (`tokenized_validation_examples`), as the argument `features` and \n3. the start and end logits returned by the model as predictions.\n\nIt first creates a map from the features to the original examples with the function `build_feature_per_example_map()`.\n\nAfter that, it processes each original example, extracting all the answers from the features, keeping the valid ones, ranking them, and returning the best one.\nI have wrapped all that code in `process_one_example()`, which we will cover in detail shortly.\n\nWe will unpack these two auxiliary functions below, but I want the high-level function to be functional, so I have put the auxiliary functions hidden above.\n\nLet's see the high-level view of the function for now, and let's use it with our toy data:","82fc4e56":"This map will be required in the following part. \nFor now, we have covered all the Tokenizer part of `prepare_train_features()`,... that was a lot!\n\nBy the way, this step is the same in `prepare_validation_features()`, so... that counts twice.\n\nNice work!\n\nGrab a coffee, and let's jump into the second part:\n\n\n## prepare_train_features() part 2: Adding start and end positions keys to the tokenized data\n\nFor this part, we will go over a high-level description first, and dig into the commented code after.\n\nFor each training sample, the model requires the keys `start_position` and `end_position`. These keys signal the start and the end token of the answer in the full token input (this is, the question and the context concatenated). This is the case when the answer is present in that piece of the context. If there is no answer in the piece of the context, these values are set to the \"class token\" (a special token).\n\nOriginally, the SQuAD dataset has the keys `answer_start` and `text`, where `answer_start` signals the _character_ (and not the token) in which the answer starts in the context. \nThis function maps from `answer_start` and `text` to `start_position` and `end_position`, taking care of the fact that more than one features (tokenized split samples) might exist for each original sample (due to the split potentially performed in the previous tokenization step).\n\nFirst, we will verify this behaviour running with only one example. After that, a commented version of the function will be presented in order to understand the underlying code.","3fea63f3":"FYI, I have changed slightly one line of the original code: `example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}` to `example_id_to_index = {k[\"id\"]: i for i, k in enumerate(examples)}` because it was assuming the dual nature of the Dataset which my list doesn't have.\n\n`process_one_example()` is as follows:","cec2f581":"### inputs_ids\ninput_ids has a tokenized version of the question and the context concatenated with the strings `<s>` and `<\/s>` (that are special tokens signaling \"Sentence start\" and \"sentence end\"), already codified as integers using the vocabulary indexes.\n\nNote that this is the common way to pass a pair of sentences to a transformer model: the transformer always receives one sentence.  The fact that they are two is encoded _within_ the sentence using the special separator tokens.","7d930d84":"Finally, note that the result has a new key, `overflow_to_sample_mapping`. This is just the mapping of the final samples to the original inputs. Since here there was only one question, all the samples were generated from it and we have `[0, 0, 0]` (3 samples generated from the first input (zero-indexed).\n\nThis value will be used in the following step.","4bce6662":"Ok, let's check it with our simple WW2 example:","27371498":"# postprocess_qa_predictions()\n\nOk, now that we have `prepare_validation_features()`, we can dig into the last function: `postprocess_qa_predictions()`\n\nThe actual code is hidden in the next code cell. It has ~84 lines.\n\nA functionaly-equivalent modularized version, with auxiliary functions, is visible below. Hopefully, it will help in the learning process.","b39f6577":"And that was it; we have gone through all the pre and post-processing code relevant for the current competitions.\n\nCongratulations! ","201c2228":"## prepare_train_features() part 1: Tokenization\n\nThe tokenization part has some particularities, but it's still a basic usage of huggingface's tokenizer. You can have a good overview of all the relevant arguments used by following [this tutorial](https:\/\/huggingface.co\/transformers\/preprocessing.html).\n\nAlso, the [The tokenization pipeline](https:\/\/huggingface.co\/docs\/tokenizers\/python\/latest\/pipeline.html#the-tokenization-pipeline) is an excellent reference for the Tokenizer.\n\nTo make it more simple, we will remove the `pad_on_right` clause for now. We will assume the model pads on its right, which means that the question goes first and the context goes second.\n\nLet's see the code and explain it right below:","ca079c9a":"# prepare_train_features()\n\nThis function has a lot of code which makes it look overwhelming, but conceptually it is not. I have rewritten it with two auxiliary functions to make it more accessible.\n\nIt is doing two operations:\n1. Tokenizing the input\n2. Adding the start and end position of the answer to the tokenized data\n\n\nThe original code is hidden below. It has 77 lines of code. The version that is used in all the training notebooks is the same as in the tutorial notebook.\n\nFrom those 77 lines, 22 are for the part 1 (tokenizing) and 55 for the part 2 (finding the start and end positions of the answer).","c0c9c1f7":"Let's get into the code now. A caveat: the function is long, but what it does is not that complex to grasp conceptually, so understanding _the details_ is not very important, as long as you understand its general behaviour. \n\nIt has 4 pointers in 2 different worlds, making it obscure. Don't get me wrong: I am not saying it is _bad code_ at all. I'm just saying it's a simple yet annoying task. \n\nSaid this, let's dig into it:","fdb9e9d6":"Let's use a larger example, where the `example_id` key will be actually useful:","b88be045":"<img src=\"https:\/\/i.imgur.com\/RFR6UZX.jpg\" width=\"100%\"\/>\n\n\n# 6- \ud83e\udd17 Pre & post-processing\n### [chaii - Hindi and Tamil Question Answering](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering) - A quick overview for QA noobs\n\nHi and welcome! This is the sixth kernel of the series `chaii - Hindi and Tamil Question Answering - A quick overview for QA noobs`.\n\n---\n\n\n**In this kernel, we will go over the 3 main functions of huggingface's [QA example notebook](https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/examples\/question_answering.ipynb), these are:**\n1. **`prepare_train_features`,**\n2. **`prepare_validation_features`, and**\n3. **`postprocess_qa_predictions`**\n\n**These functions are used in all of the [top models](https:\/\/www.kaggle.com\/julian3833\/4-exploring-public-models-qa-for-qa-noobs\/), so gaining an understanding of them is crucial.**\n\n\n---\n\nThe full series consist of the following notebooks:\n1. [The competition](https:\/\/www.kaggle.com\/julian3833\/1-the-competition-qa-for-qa-noobs)\n2. [The dataset](https:\/\/www.kaggle.com\/julian3833\/2-the-dataset-qa-for-qa-noobs)\n3. [The metric (Jaccard)](https:\/\/www.kaggle.com\/julian3833\/3-the-metric-jaccard-qa-for-qa-noobs)\n4. [Exploring Public Models](https:\/\/www.kaggle.com\/julian3833\/4-exploring-public-models-qa-for-qa-noobs\/)\n5. [\ud83e\udd47 XLM-Roberta + Torch's extra data [LB: 0.749]](https:\/\/www.kaggle.com\/julian3833\/5-xlm-roberta-torch-s-extra-data-lb-0-749)\n6. _[\ud83e\udd17 Pre & post processing](https:\/\/www.kaggle.com\/julian3833\/6-pre-post-processing-qa-for-qa-noobs\/) (This notebook)_\n7. [Public Models Revisited](https:\/\/www.kaggle.com\/julian3833\/7-public-models-revisited-qa-for-qa-noobs\/)\n\nThis is an ongoing project, so expect more notebooks to be added to the series soon. Actually, we are currently working on the following ones:\n* Reviewing `squad2`, `mlqa` and others\n* About `xlm-roberta-large-squad2`\n* Own improvements\n\n---\n","bb01cf1c":"That was it for `preparare_train_features()`. To recap: it does the 1) tokenization part and after 2) it adds the start and end positions:\n\n```python\ndef prepare_train_features(examples):\n    # Tokenize\n    tokenized_examples = tokenize(examples)\n    # Set the start and end position tokens\n    tokenized_examples = set_start_and_end_positions(tokenized_examples, examples['answers']) \n    return tokenized_examples\n```\n\nFinally, below we will execute a full call to `prepare_train_features`. Note that the `max_length` is set to `384`, so the _padding_ takes the lead over the the _truncation_. You should be able to identify all the components if you look with patience:"}}