{"cell_type":{"46f14595":"code","6cd1a55d":"code","a4244022":"code","f5ceebaf":"code","08eb057c":"code","b058b49b":"code","d257dc62":"code","797b0e95":"code","8930140e":"code","3924e96d":"code","fb209ff5":"code","15b038ef":"code","b05ec64e":"code","d89530ce":"code","027397cb":"code","3719e0f4":"code","f59085c1":"code","39568e7d":"code","179ac16b":"code","895e9343":"code","1b2e6c0f":"code","2c019ff9":"code","c31799be":"code","26c95cbb":"code","32ac2907":"code","06e3d12d":"code","e49c70f2":"code","fa05e8ba":"code","5d4358e3":"markdown","219c1b40":"markdown","e6190a82":"markdown","6c563514":"markdown","a1583385":"markdown","ac2dff93":"markdown","1eb527f6":"markdown","f937b8e3":"markdown","33d727e6":"markdown","3442ca79":"markdown","f76d15c8":"markdown","a003d1be":"markdown"},"source":{"46f14595":"! pip install -q efficientnet\n! pip install -q tensorflow-addons==0.10.0","6cd1a55d":"import re\nimport os\nimport gc\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom time import time\nfrom tqdm.auto import tqdm \n\nfrom PIL import Image\nimport Levenshtein\n\nfrom sklearn.utils.class_weight import compute_class_weight\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\n\nimport efficientnet.tfkeras as efn\nfrom kaggle_datasets import KaggleDatasets\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(tf.__version__)","a4244022":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment \n    # variable is set. On Kaggle this is always the case.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","f5ceebaf":"ROOT_DIR         = \"..\/input\/mlub-mongolian-car-plate-prediction-v1\"\nPLATE_LEN        = 7\nNFOLDS           = 4\nNBEST            = 1\nEPOCHS           = 100\nBATCH_SIZE       = 16 * strategy.num_replicas_in_sync\n\nEffNetSize       = 1\nIMG_SIZE         = [110, 470]\n\nIS_UPSAMPLE      = False\n\nMODELS           = [\n        {\n            \"EffSize\": 1,\n            \"model_dir\": \"..\/input\/mlub-no-16-weights\",\n            \"weight\": 0.85,\n        },\n#         {\n#             \"EffSize\": 2,\n#             \"model_dir\": \"..\/input\/mlub-no-17-weights\",\n#             \"weight\": 0.05,\n#         },\n        {\n            \"EffSize\": 0,\n            \"model_dir\": \"..\/input\/mlub-no-18-weights\",\n            \"weight\": 0.15,\n        },\n    ]","08eb057c":"for model_dict in MODELS:\n    # print(model_dict[\"model_dir\"])\n    ! du -sh {model_dict[\"model_dir\"]}","b058b49b":"df_train = pd.read_csv(os.path.join(ROOT_DIR, \"training.csv\"))\ndf_subm  = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\ndf_test  = df_subm.copy()\n\nprint(df_train.shape, df_test.shape, df_subm.shape)","d257dc62":"df_train_correct = pd.read_csv(f\"{ROOT_DIR}\/mlub_train_correction.csv\")\ndf_train_correct[\"plate_number\"] = df_train_correct[\"correct_plate_number\"]\ndel df_train_correct[\"correct_plate_number\"]\n\ndf_train_1 = df_train[~df_train[\"file_name\"].isin(df_train_correct[\"file_name\"])]\ndf_train = pd.concat([df_train_1, df_train_correct]).sort_values(\"file_name\")\ndf_train.head()","797b0e95":"# exclude 6digit plates\ndf_train = df_train[df_train.plate_number.apply(lambda x: len(x) == PLATE_LEN)]\nfor i in range(PLATE_LEN):\n    df_train[f\"plate_number_{i}\"] = df_train.plate_number.apply(\n        lambda x: \n        \"nan\" if len(x) != PLATE_LEN and i == PLATE_LEN - 1 else x[i]\n    )\n    \ndf_train.shape","8930140e":"dict_labels = {\n    0: list(range(10)),\n    1: list(range(10)),\n    2: list(range(10)),\n    3: list(range(10)),\n    4: sorted(df_train[\"plate_number_4\"].unique().tolist()),\n    5: sorted(df_train[\"plate_number_5\"].unique().tolist()),\n    6: sorted(df_train[\"plate_number_6\"].unique().tolist()),\n}\n# checking\ndict_labels[4]","3924e96d":"TF_RECORD_PATH = KaggleDatasets().get_gcs_path('mlub-tf-records-4-fold')\nprint(\"GCS PATH:\", TF_RECORD_PATH)","fb209ff5":"ALL_FNAMES  = tf.io.gfile.glob(TF_RECORD_PATH + \"\/*train*.tfrec\")\nTEST_FNAMES = tf.io.gfile.glob(TF_RECORD_PATH + \"\/*test-*.tfrec\")\nprint(\"Number of TF record files:\", len(ALL_FNAMES))\n\nVAL_FNAMES = {f\"fold_{fold_num}\": tf.io.gfile.glob(f\"{TF_RECORD_PATH}\/*train_fold-{fold_num}*.tfrec\")\n                          for fold_num in range(NFOLDS)}\nTRAIN_FNAMES = {f\"fold_{fold_num}\": list(set(ALL_FNAMES) - set(VAL_FNAMES[f\"fold_{fold_num}\"])) for fold_num in range(NFOLDS)}","15b038ef":"def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, \n    # i.e. test10-687.tfrec = 687 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    \n    return np.sum(n)","b05ec64e":"%%time\nN_TRAIN_IMGS = {f'fold_{i}': count_data_items(TRAIN_FNAMES[f'fold_{i}'])\n                for i in range(NFOLDS)}\n\nN_VAL_IMGS = {f'fold_{i}': count_data_items(VAL_FNAMES[f'fold_{i}'])\n              for i in range(NFOLDS)}\n\nN_TEST_IMGS = count_data_items(TEST_FNAMES)\n\nSTEPS_PER_EPOCH = {f'fold_{i}': N_TRAIN_IMGS[f'fold_{i}'] \/\/ BATCH_SIZE\n                   for i in range(NFOLDS)}\n\nprint(\"=\"*75)\n\nprint(f\"The number of unlabeled test image is {N_TEST_IMGS}. It is common for all folds.\")\n\nfor i in range(NFOLDS):\n    print(\"=\"*75)\n    print(f\"Fold {i}: {N_TRAIN_IMGS[f'fold_{i}']} training and {N_VAL_IMGS[f'fold_{i}']} validation images.\")\nprint(\"=\"*75)","d89530ce":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32)# \/ 255.0 \n    # explicit size needed for TPU\n    \n    image = tf.reshape(image, [*IMG_SIZE, 3])\n\n    return image\n\ndef read_unlabeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        # shape [] means single element\n        ################################\n        # bytestring features\n        \"image\":         tf.io.FixedLenFeature([], tf.string), \n        \"file_name\":     tf.io.FixedLenFeature([], tf.string), \n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    \n    # image data\n    image        = decode_image(example['image']) \n    # bytestring features\n    file_name    = tf.cast(example['file_name'],    tf.string)\n\n    return image, file_name\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        # tf.string means bytestring\n        # shape [] means single element\n        ################################\n        # bytestring features\n        \"image\":         tf.io.FixedLenFeature([], tf.string), \n        \"file_name\":     tf.io.FixedLenFeature([], tf.string),\n        \"plate_number\":  tf.io.FixedLenFeature([], tf.string),\n        \n        # integer features\n        \"out0\": tf.io.FixedLenFeature([], tf.int64), \n        \"out1\": tf.io.FixedLenFeature([], tf.int64), \n        \"out2\": tf.io.FixedLenFeature([], tf.int64), \n        \"out3\": tf.io.FixedLenFeature([], tf.int64), \n        \"out4\": tf.io.FixedLenFeature([], tf.int64), \n        \"out5\": tf.io.FixedLenFeature([], tf.int64), \n        \"out6\": tf.io.FixedLenFeature([], tf.int64), \n        \n        # float features\n        # \"age_scaled\": tf.io.FixedLenFeature([], tf.float32),\n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    \n    # image data\n    image        = decode_image(example['image']) \n    # bytestring features\n    file_name    = tf.cast(example['file_name'],    tf.string)\n    plate_number = tf.cast(example['plate_number'], tf.string)\n    \n    label = {}\n    for i in range(PLATE_LEN):\n        label[f\"out_{i}\"] = [tf.cast(example[f'out{i}'], tf.int32)]\n        \n#     label = []\n#     for i in range(PLATE_LEN):\n#         label.append(tf.cast(example[f'out{i}'], tf.int32))\n\n    return image, file_name, plate_number, label","027397cb":"def load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files \n    # at once and disregarding data order. Order does not matter since we will \n    # be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed\n        ignore_order.experimental_deterministic = False\n\n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord if labeled \n                          else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    \n    return dataset","3719e0f4":"LR_START = 0.00005\nLR_MAX = 0.0000125 * BATCH_SIZE\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 10\nLR_SUSTAIN_EPOCHS = 20\nLR_EXP_DECAY = 0.992\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\n\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","f59085c1":"def build_model(EffNetSize):\n    with strategy.scope():\n        \n        # img = Input((*IMG_SIZE, 3), dtype=tf.int32, name='input')\n        img = Input((*IMG_SIZE, 3), dtype=tf.float32, name='input')\n\n        base_model = getattr(efn, f\"EfficientNetB{EffNetSize}\")(\n            input_shape=(*IMG_SIZE, 3),\n            weights='imagenet',\n            include_top=False\n        )\n        base_model.trainable = True\n\n        x = base_model(img)\n        x = GlobalAveragePooling2D()(x)\n        x = Dropout(0.25)(x)\n\n        outputs = []\n\n        for key in dict_labels:\n\n            out = Dense(\n                len(dict_labels[key]), \n                kernel_regularizer=tf.keras.regularizers.l2(l=0.01),\n                activation='softmax', \n                name=f\"out_{key}\"\n            )(x)\n\n            outputs.append(out)\n\n        model = tf.keras.models.Model(inputs=[img], outputs=outputs)\n\n        optimizer = tf.keras.optimizers.Adam(learning_rate=(1e-4))\n\n        model.compile(\n            loss=\"sparse_categorical_crossentropy\", \n            optimizer=optimizer, \n            metrics=[\"accuracy\"],\n        )\n\n        return model","39568e7d":"%%time\ntf.keras.backend.clear_session()\n# tf.tpu.experimental.initialize_tpu_system(tpu)\nmodel = build_model(0)\nmodel.summary()","179ac16b":"def decode_y_preds_to_plate(y_preds, df_valid_):\n    for key, y_i in enumerate(y_preds):\n        if key < 4:\n            df_valid_[f\"pred_{key}\"] = np.argmax(y_i, axis=-1).tolist()\n        else:\n            df_valid_[f\"pred_{key}\"] = [dict_labels[key][x] for x in np.argmax(y_i, axis=-1)]\n\n        df_valid_[f\"prob_{key}\"] = np.max(y_i, axis=-1).tolist()\n\n    df_valid_['pred_plate_number'] = df_valid_[[f\"pred_{key}\" for key in dict_labels]].agg(\n        lambda x: ''.join([str(l) for l in x]), axis=1\n    )\n\n    df_valid_ = df_valid_.drop(columns=[f\"pred_{key}\" for key in range(len(y_preds))])\n    \n    return df_valid_\n\n        \ndef prepare_valid_df(valid_dataset_):\n    df_valid_ = pd.DataFrame()\n\n    valid_ids = []\n    valid_plate_number = []\n    for x in valid_dataset_.map(\n        lambda image, file_name, plate_number, label: (file_name, plate_number)\n    ).take(N_VAL_IMGS[f'fold_{fold_num}']):\n        valid_ids         .append(x[0].numpy().decode())\n        valid_plate_number.append(x[1].numpy().decode())\n\n    df_valid_[\"file_name\"]    = valid_ids\n    df_valid_[\"plate_number\"] = valid_plate_number\n\n    return df_valid_\n\ndef prepare_test_df(dataset):\n    df_test = pd.DataFrame()\n\n    ids = []\n    \n    for x in dataset.take(N_TEST_IMGS):\n        ids.append(x[1].numpy().decode())\n\n    df_test[\"file_name\"] = ids\n\n    return df_test","895e9343":"def setup_input(image, file_name, plate_number, label):\n    return (image, label)\n\ndef setup_test_input(image, file_name):\n    return image\n\ndef setup_valid_image_input(image, file_name, plate_number, label):\n    return image\n\ndef data_augment_cutout(image):\n\n    # image = tfa.image.random_cutout(image, (35,35), constant_values=0)\n    # image = tfa.image.random_cutout(image, (35,35), constant_values=0)\n    image = tfa.image.random_cutout(image, (35,35), constant_values=0)\n    image = tfa.image.random_cutout(image, (45,45), constant_values=0)\n    image = tfa.image.random_cutout(image, (50,50), constant_values=0)\n\n    return image\n\ndef data_augment(image):\n    \n    # image = tf.image.random_saturation(image, 0, 50)\n    image = tf.image.random_contrast(image, 0.5, 2.5)\n    image = tf.image.random_brightness(image, 100)\n    image = tf.image.random_hue(image, 0.1)\n    \n    return image\n\ndef get_training_dataset(dataset):\n    \n    # the training dataset must repeat for several epochs\n    dataset = dataset.map(lambda image, label: (data_augment(image), label), num_parallel_calls=AUTO)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n\n    dataset = dataset.batch(BATCH_SIZE)\n\n    # augmentation\n    dataset = dataset.map(lambda image, label: (data_augment_cutout(image), label), num_parallel_calls=AUTO)\n\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO)\n\n    return dataset\n\ndef get_validation_dataset(dataset):\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.cache()\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset\n\ndef get_test_dataset(dataset):\n    \n    # the training dataset must repeat for several epochs\n    if TTA:\n        dataset = dataset.repeat()\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n\n    dataset = dataset.batch(BATCH_SIZE)\n    \n    if TTA:\n        # augmentation\n        dataset = dataset.map(data_augment_cutout, num_parallel_calls=AUTO)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset","1b2e6c0f":"## Checking the data pipeline!\n\ntrain_dataset  = load_dataset(TRAIN_FNAMES[\"fold_1\"])\ntrain_dataset  = train_dataset.map(setup_input, num_parallel_calls=AUTO,)\ntrain_dataset  = get_training_dataset(train_dataset)\n\nTTA = None\n\nimage_batch = next(iter(train_dataset))\n\nfig, axs = plt.subplots(1,5, figsize=(21, 7))\nprint(\"Example of the training data:\")\nfor i, im in enumerate(image_batch[0][:5]):\n    axs[i].imshow(im\/255.)","2c019ff9":"%%time\n\nVERBOSE = 0\nTTA = None\n\nscore_list={fn: [] for fn in range(NFOLDS)}\n\nlevenstein_list = []\ndf_valid_list   = []\n\n# TEST\ntest_dataset_ = load_dataset(TEST_FNAMES, labeled=False, ordered=True)\ntest_dataset  = test_dataset_.map(setup_test_input, num_parallel_calls=AUTO,)\ntest_dataset  = get_test_dataset(test_dataset)\n\ndf_test = prepare_test_df(test_dataset_)\ny_tests = [0] * PLATE_LEN\n\nfor fold_num in range(NFOLDS):\n\n    st = time()\n    print(\"#\"*25)\n    print(\"# FOLD %d\" % fold_num)\n    print(\"#\"*25)\n    \n    ####################################\n\n    # VALID\n    valid_dataset_ = load_dataset(VAL_FNAMES[f\"fold_{fold_num}\"], ordered=True)\n    valid_dataset  = valid_dataset_.map(setup_input, num_parallel_calls=AUTO,)\n    valid_dataset  = get_validation_dataset(valid_dataset)\n\n    df_valid_ = prepare_valid_df(valid_dataset_)\n    \n    ###################################\n\n    print(\"TESTING...\")\n    \n    y_valids = [0] * PLATE_LEN\n    \n    for c_model_dict in MODELS:\n        \n        print(\"DICT USED...\\n\", c_model_dict)\n        \n        with strategy.scope():\n            model = build_model(c_model_dict[\"EffSize\"])\n        \n        model.load_weights(f\"{c_model_dict['model_dir']}\/weights_fold_{fold_num}_0.hdf5\")\n\n        # inference_valid_dataset = valid_dataset.map(setup_test_input, num_parallel_calls=AUTO)\n        inference_valid_dataset = valid_dataset_.map(setup_valid_image_input, num_parallel_calls=AUTO)\n        inference_valid_dataset = get_test_dataset(inference_valid_dataset)\n        \n        if TTA:\n            VALID_STEPS = TTA * math.ceil(N_VAL_IMGS[f'fold_{fold_num}'] \/ BATCH_SIZE)\n            TEST_STEPS = TTA * math.ceil(N_TEST_IMGS \/ BATCH_SIZE)\n\n            y_valid_fold = model.predict(inference_valid_dataset, verbose=1, steps=VALID_STEPS)\n            y_test_fold  = model.predict(test_dataset, verbose=1, steps=TEST_STEPS)\n\n        else:\n            y_valid_fold = model.predict(inference_valid_dataset, verbose=1)\n            y_test_fold  = model.predict(test_dataset, verbose=1)\n\n        for i in range(PLATE_LEN):\n            if TTA:\n                ## TTA\n                y_valid_fold[i] = y_valid_fold[i][:N_VAL_IMGS[f'fold_{fold_num}'] * TTA, ]\n                y_valid_fold[i] = y_valid_fold[i].reshape((N_VAL_IMGS[f'fold_{fold_num}'], TTA, -1), order=\"F\")\n                y_valid_fold[i] = np.mean(y_valid_fold[i], axis=1)\n\n                y_test_fold[i] = y_test_fold[i][:N_TEST_IMGS * TTA, ]\n                y_test_fold[i] = y_test_fold[i].reshape((N_TEST_IMGS, TTA, -1), order=\"F\")\n                y_test_fold[i] = np.mean(y_test_fold[i], axis=1)\n            \n#             y_valids[i] += y_valid_fold[i] \/ len(MODELS)\n#             y_tests [i] += y_test_fold[i] \/ (len(MODELS) * NFOLDS)\n            \n            y_valids[i] += y_valid_fold[i] * c_model_dict[\"weight\"]\n            y_tests [i] += y_test_fold[i] * c_model_dict[\"weight\"] \/ NFOLDS\n\n        ################################\n        # break\n        ################################\n\n    \n    df_valid_ = decode_y_preds_to_plate(y_valids, df_valid_)\n    df_valid_[\"levenshtein\"] = df_valid_[[\"plate_number\", \"pred_plate_number\"]].apply(\n        lambda x: Levenshtein.distance(x[0], x[1]), axis=1\n    )\n\n    if IS_UPSAMPLE:\n        df_valid_ = df_valid_.drop_duplicates(\"file_name\")\n\n    score = df_valid_[\"levenshtein\"].mean()\n    print(\"Levenstein: %.5f  (%.1f min)\" % (score, (time()-st)\/60))\n\n    levenstein_list.append(score)\n\n    df_valid_[\"fold\"] = fold_num\n    df_valid_list.append(df_valid_)\n\n    ###################################\n    # tf.tpu.experimental.initialize_tpu_system(tpu)\n    tf.keras.backend.clear_session()\n    gc.collect()","c31799be":"df_valid = pd.concat(df_valid_list, axis=0)\nprint(df_valid.shape)\ndf_valid.head(2)","26c95cbb":"print(\"CV : %.4f\"%(np.mean(levenstein_list)))\nprint(\"OOF: %.4f\"%(df_valid[\"levenshtein\"].mean()))\nprint(\"STD: %.4f\"%(np.std(levenstein_list)))","32ac2907":"df_test = decode_y_preds_to_plate(y_tests, df_test) \ndf_test.head()","06e3d12d":"df_test = df_test.rename(columns={\"pred_plate_number\":\"plate_number\"})","e49c70f2":"df_test.head()","fa05e8ba":"df_test[[\"file_name\", \"plate_number\"]].to_csv(\"submission.csv\", index=False)","5d4358e3":"## Modeling","219c1b40":"## Learning Rate Scheduler Setting","e6190a82":"## OOF score:","6c563514":"## MLUB Submissions\n\n### Introduction\n\u042d\u043d\u044d\u0445\u04af\u04af \u0442\u044d\u043c\u0446\u044d\u044d\u043d\u0434 \u043e\u0440\u043e\u043b\u0446\u043e\u0436 \u0431\u043e\u0434\u0438\u0442 \u0430\u043c\u044c\u0434\u0440\u0430\u043b \u0431\u0430\u0439\u0433\u0430\u0430 \u0434\u0430\u0442\u0430\u043d\u0443\u0443\u0434\u044b\u0433 \u0442\u0430\u043d\u0438\u0445 \u0442\u0443\u043d \u0441\u043e\u043d\u0438\u0440\u0445\u043e\u043b\u0442\u043e\u0439 \u0431\u0430\u0439\u043b\u0430\u0430!\n\n\u042e\u0443\u043d\u044b \u04e9\u043c\u043d\u04e9 \u0437\u043e\u0440\u0438\u0433\u0442\u043e\u0439\u0433\u043e\u043e\u0440 \u0434\u0430\u0442\u0430 open source \u0445\u0438\u0439\u0436, \u044d\u043d\u044d\u0445\u04af\u04af \u0442\u044d\u043c\u0446\u044d\u044d\u043d\u0438\u0439\u0433 \u0437\u043e\u0445\u0438\u043e\u043d \u0431\u0430\u0439\u0433\u0443\u0443\u043b\u0441\u0430\u043d **Machine Learning UB**-community-\u0434\u044d\u044d \u0431\u0430\u044f\u0440\u043b\u0430\u043b\u0430\u0430.\n\n\u0422\u044d\u043c\u0446\u044d\u044d\u043d\u0438\u0439 \u044d\u0445\u043d\u044d\u044d\u0441 \u0431\u0438 \u0434\u0430\u0440\u0430\u0430\u0445 \u0434\u0430\u0440\u0430\u0430\u043b\u043b\u044b\u043d \u0431\u0430\u0440\u044c\u0436 \u043a\u043e\u0434\u043e\u043e \u044d\u0445\u043b\u04af\u04af\u043b\u0441\u044d\u043d \u0431\u043e\u043b\u043d\u043e.\n1. \u042f\u043c\u0430\u0440 \u043d\u044d\u0433 \u043c\u043e\u0434\u0435\u043b\u0438\u0439\u0433 \u044d\u0445\u043d\u044d\u044d\u0441 \u043d\u044c train \u0445\u0438\u0439\u0445\u0433\u04af\u0439, \u0437\u04e9\u0432\u0445\u04e9\u043d fine-tuning \u0445\u0438\u0439\u043d\u044d.\n2. \u041d\u0438\u0439\u0442 \u043c\u043e\u0434\u0435\u043b\u0438\u0439\u043d \u0445\u044d\u043c\u0436\u044d\u044d 200MB \u0433\u044d\u0441\u044d\u043d \u0445\u044f\u0437\u0433\u0430\u0430\u0440 \u0431\u0430\u0439\u0433\u0430\u0430 \u0442\u0443\u043b [TensorFlow](https:\/\/www.tensorflow.org\/) \u0430\u0448\u0438\u0433\u043b\u0430\u0445\u044b\u0433 \u0438\u043b\u04af\u04af\u0434 \u04af\u0437\u0441\u044d\u043d \u0431\u0430\u0439\u0433\u0430\u0430.\n3. \u0414\u0430\u0442\u0430 \u0431\u043e\u043b\u043e\u043d \u043c\u043e\u0434\u0435\u043b\u0438\u043e tf serialization (tf record) \u0445\u0438\u0439\u0436 \u0434\u0430\u043c\u0436\u0443\u0443\u043b\u0430\u043d \u0430\u0448\u0438\u0433\u043b\u0430\u0436 \u0431\u0430\u0439\u0441\u0430\u043d. [link to get TF Records Notebook](https:\/\/www.kaggle.com\/bayartsogtya\/mlub-tf-record-4-folds-correction)\n\n### Results\n\u042d\u043d\u044d\u0445\u04af\u04af notebook-\u0442 \u043e\u0440\u043e\u043b\u0446\u0441\u043e\u043d submission-\u04af\u04af\u0434\u0438\u0439\u043d \u0436\u0430\u0433\u0441\u0430\u0430\u043b\u0442\u044b\u0433 \u0434\u043e\u043e\u0440 \u0445\u0430\u0440\u0443\u0443\u043b\u0436 \u0431\u0430\u0439\u043d\u0430.\n\n| No | Model | CrossValidation | LeaderBoard | PrivateBoard | isUsed |\n| --- | --- | --- | --- | --- | --- |\n| 16 | (best) EfficientNetB1 | 0.0130 | 0.00865 | 0.02163 | 1 |\n| 17 | EfficientNetB2 | 0.0152 | -       | -       | 0 |\n| 18 | EfficientNetB0 | 0.0133 | 0.00919 | 0.02325 | 1 |","a1583385":"## Total MODEL SIZE Calculation:","ac2dff93":"## Keep Kaggling!","1eb527f6":"## Data from MLUB V2 (105 examples changed)","f937b8e3":"## Parameters","33d727e6":"## Submission","3442ca79":"## Train + Inference","f76d15c8":"## Data from TF records","a003d1be":"checking if pipeline working correct!"}}