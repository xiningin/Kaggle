{"cell_type":{"946410cf":"code","7b560fe1":"code","a85c086e":"code","0341382c":"code","0a745eda":"code","6e4c9fde":"code","d25dd1cf":"code","ffdf7ee4":"code","5941e1c6":"code","24b220df":"code","5bef689f":"code","8f9c0879":"code","2dc1f033":"code","7e00d24c":"code","3207db9b":"code","ab74a89f":"code","2a93d1a8":"code","4fb3f3a4":"code","8f539161":"code","4996cd4e":"code","06d28932":"code","374fe385":"code","0e0004a0":"code","7c0681af":"code","b00efe6a":"code","02ffee08":"code","0b81c015":"code","36bd5c93":"code","8bc8a641":"code","8c6980e5":"code","363509c1":"code","86f11f74":"code","cdd70106":"code","75477467":"code","7f5450e6":"code","d7cc9873":"code","2c4c57c6":"code","05eb920a":"code","0f45040f":"code","e65bc01a":"code","bc1104f5":"code","62e0d2f3":"code","6f513a15":"code","5c7c4ec2":"code","3a7c7e6d":"code","1610e433":"code","4f488d6b":"code","f99dc027":"code","f0254842":"code","b21bd3e6":"code","609dbbf9":"code","8967cb01":"code","2f27a970":"code","52f95e58":"code","4841d795":"code","81ea9436":"code","7544e10c":"code","95649606":"code","40411e6b":"code","fb1e0341":"code","8b91a92e":"code","440a7339":"code","77b95d86":"code","3597c43a":"code","20d9e3be":"code","37b57f0c":"code","64e5e6f3":"code","1df89e8c":"code","7a76399e":"code","62dce3e0":"code","b0b6d8df":"code","0c5cd20e":"code","a9996e58":"code","9b713cf8":"code","06e7ad4d":"code","683f0752":"code","de73a2b4":"code","057b5ad2":"code","4a9d43d2":"code","1291e454":"markdown","b22df8c2":"markdown","7e13f67a":"markdown","b277fa3a":"markdown","69cc4d6e":"markdown","1c02ad14":"markdown","dc04d1c7":"markdown","196ac79b":"markdown","18663d68":"markdown","7c374f3d":"markdown","a74539e3":"markdown","8733d024":"markdown","284af5b5":"markdown","d3c0e838":"markdown","325b434f":"markdown","75389b7d":"markdown","626803f5":"markdown","dbb3812b":"markdown","f32c95d8":"markdown","9a99bb25":"markdown","839aa566":"markdown","0c16f9ce":"markdown","61c105f0":"markdown","d0cf441d":"markdown","75e1d230":"markdown","52d2d14e":"markdown","cbd263b8":"markdown","4a221d35":"markdown","2bf29a05":"markdown","1bf78fc1":"markdown","1cbb4152":"markdown","1e0ef341":"markdown","0c304e18":"markdown","618ec218":"markdown","3015612c":"markdown","62a6dcff":"markdown","5225db81":"markdown","a256fbc3":"markdown","f989f8e2":"markdown","fa10b608":"markdown","1c2e9608":"markdown","701b6639":"markdown","7bc8db59":"markdown","f049d349":"markdown","6bc23ed5":"markdown","ab83a122":"markdown","16b5e19e":"markdown","b44e0d2d":"markdown","a65b7d1f":"markdown","0883ba7c":"markdown","ae5c6c08":"markdown"},"source":{"946410cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7b560fe1":"ls","a85c086e":"cd data","0341382c":"cd ..","0a745eda":"ls","6e4c9fde":"cd working\/rking","d25dd1cf":"cd working\/","ffdf7ee4":"ls","5941e1c6":"pip install tensorflow==2.1.0","24b220df":"pip install tf-nightly","5bef689f":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport pathlib","8f9c0879":"data_dir = '..\/input\/african-wildlife'","2dc1f033":"data_dir = pathlib.Path(data_dir)","7e00d24c":"image_count = len(list(data_dir.glob('*\/*.jpg')))\nprint(image_count)","3207db9b":"buffalo = list(data_dir.glob('buffalo\/*'))\nPIL.Image.open(str(buffalo[0]))","ab74a89f":"PIL.Image.open(str(buffalo[1]))","2a93d1a8":"elephant = list(data_dir.glob('elephant\/*.jpg'))\nPIL.Image.open(str(elephant[0]))","4fb3f3a4":"PIL.Image.open(str(elephant[1]))","8f539161":"batch_size =  32\nimg_height = 160\nimg_width = 160","4996cd4e":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split = 0.2,\n    subset=\"training\",\n    seed = 123,\n    image_size=(img_height, img_width),\n    batch_size=batch_size\n)","06d28932":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split = 0.2,\n    subset=\"validation\",\n    seed = 123,\n    image_size = (img_height, img_width),\n    batch_size = batch_size\n)","374fe385":"class_names = train_ds.class_names\nprint(class_names)","0e0004a0":"plt.figure(figsize = (10,10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3,3,i+1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","7c0681af":"for image_batch, labels_batch in train_ds:\n    print(image_batch.shape)\n    print(labels_batch.shape)\n    break","b00efe6a":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","02ffee08":"normalization_layer = layers.experimental.preprocessing.Rescaling(1.\/255)","0b81c015":"normalized_ds = train_ds.map(lambda x,y: (normalization_layer(x), y))\nimage_batch, labels_batch = next(iter(normalized_ds))\nfirst_image = image_batch[0]\n#Notice the pixels values are now in `[0,1]`.\nprint(np.min(first_image), np.max(first_image))","36bd5c93":"num_classes = 4\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","8bc8a641":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","8c6980e5":"model.summary()","363509c1":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","86f11f74":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","cdd70106":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","75477467":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","7f5450e6":"model = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","d7cc9873":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","2c4c57c6":"model.summary()","05eb920a":"epochs = 15\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","0f45040f":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()\n","e65bc01a":"from skimage import io\nimport cv2","bc1104f5":"cd ..","62e0d2f3":"cd input\/","6f513a15":"cd ..","5c7c4ec2":"cd ..","3a7c7e6d":"cd working\/","1610e433":"img = keras.preprocessing.image.load_img(\n    \"..\/input\/animaldetectiontest\/download.jpg\", target_size=(img_height, img_width)\n)","4f488d6b":"img_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nscore = tf.nn.softmax(predictions[0])\n\nprint(\n    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n    .format(class_names[np.argmax(score)], 100 * np.max(score))\n)","f99dc027":"val_batches = tf.data.experimental.cardinality(val_ds)\ntest_dataset = val_ds.take(val_batches \/\/ 5)\nval_ds = val_ds.skip(val_batches \/\/ 5)","f0254842":"print('Number of validation batches: %d' % tf.data.experimental.cardinality(val_ds))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))","b21bd3e6":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","609dbbf9":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","8967cb01":"for image, _ in train_ds.take(1):\n    plt.figure(figsize=(10, 10))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] \/ 255)\n        plt.axis('off')","2f27a970":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","52f95e58":"rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5, offset= -1)","4841d795":"# Create the base model from the pre-trained model MobileNet V2\nIMG_SHAPE = (img_height,img_width) + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","81ea9436":"image_batch, label_batch = next(iter(train_ds))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","7544e10c":"base_model.trainable = False","95649606":"base_model.summary()","40411e6b":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","fb1e0341":"prediction_layer = layers.Dense(4)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","8b91a92e":"inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\ntr_model = tf.keras.Model(inputs, outputs)","440a7339":"base_learning_rate = 0.0001\ntr_model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","77b95d86":"tr_model.summary()","3597c43a":"len(tr_model.trainable_variables)","20d9e3be":"initial_epochs = 10\nloss0, accuracy0 = tr_model.evaluate(val_ds)","37b57f0c":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","64e5e6f3":"history = tr_model.fit(train_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_ds)","1df89e8c":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","7a76399e":"base_model.trainable = True","62dce3e0":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 100\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","b0b6d8df":"tr_model.compile( optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","0c5cd20e":"tr_model.summary()","a9996e58":"len(tr_model.trainable_variables)","9b713cf8":"fine_tune_epochs = 10\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = tr_model.fit(train_ds,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=val_ds)","06e7ad4d":"print(history_fine.history['loss'])","683f0752":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","de73a2b4":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","057b5ad2":"loss, accuracy = tr_model.evaluate(test_dataset)\nprint('Test accuracy :', accuracy)","4a9d43d2":"#Retrieve a batch of images from the test set\nimage_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = tr_model.predict_on_batch(image_batch)\npredictions = tf.nn.softmax(predictions)\npred_label =[]\nfor predict in predictions:\n    pred_label.append(class_names[np.argmax(predict)])\n\nprint(pred_label)\n# Apply a sigmoid since our model returns logits\n#\n# print(predictions)\n# print(predictions[1])\n# print(tf.argmax(predictions))\n# print(len(image_batch))\n# predictions = np.argmax(predictions)\n\n# print('Predictions:\\n', predictions)\n# print('Labels:\\n', label_batch)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].astype(\"uint8\"))\n    plt.title(pred_label[i])\n    plt.axis(\"off\")","1291e454":"Let us visualize what a few augmented examples look like applying data augmentation to the same image several times:","b22df8c2":"**Standarize the data**\n\nThe RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general we should seek to make our input values small. Now we will standarize the values to be in the [0,1] by using Rescaling layer.\n\nNote: The below code only works in below Tensorflow version 2.1.0. I recommend installing this version.","7e13f67a":"**Model Summary**\n\nThis views all the layers of the network using the model's summary method","b277fa3a":"**Data augmentation**\n\nOverfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting then using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n\nWe will implement data augmentation using experimental Keras Preprocessing Layers. These can be included inside your model like other layers and run on the GPU","69cc4d6e":"**Use data augmentation**\n\nWhen you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting. You can learn more about data augmentation in this [tutorial](https:\/\/www.tensorflow.org\/tutorials\/images\/data_augmentation).","1c02ad14":"**Feature extraction**\n\nIn this step, we will freeze the convolutional base created from the previous step and to use as a feature extractor. Additionally, we will add a classifier on top of it and train the top-level classifier.","dc04d1c7":"**Explore the dataset**\n\nFirst import the data and explore the datasets","196ac79b":"**Configure the data for performance**\n\nUse buffered prefetching to load images from disk without having I\/O become blocking. To learn mode about see the [data performance](https:\/\/www.tensorflow.org\/guide\/data_performance) guide.","18663d68":"It is always a good practice to use a validation split when developing your model. We will use 80% of the images for training, and 20% for validation.","7c374f3d":"***Evaluation and prediction***\n\nFinaly we can verify the performance of the model on new data using test set.","a74539e3":"**Data preprocessing**\n\nAs the dataset doesnot contains a test set, we will create one. To do so, determine how many batches of data are available in the validation set using tf.data.experimental.cardinality, then move 20% of them to a test set.","8733d024":"**Visualize training results**\n\nCreate plots of loss and accuracy on the training and validation sets.","284af5b5":"**Dropout**\n\nAnother technique to reduce overfitting is to introduce Dropout to the network, a form of regularization.\n\nWhen we apply Dropout to a layer it randomly drops out (by setting the activation to zero) a number of output units from the layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1,0.2,0.4 etc. This means dropping out 10% 20% or 40% of the output units randomly from the applied layer.","d3c0e838":"**Rescale pixel values**\n\nIn a moment, we will download tf.keras.applications.MobileNetV2 for use as your base model. This model expects pixel values in [-1,1], but at this point, the pixel values in our images are in [0-255]. To rescale them, use the preprocessing method included with the model.","325b434f":"We can see from the plots, training accuracy and validation accuracy are off by large margin.","75389b7d":"Visualize training result\n\nAfter applying data augmentation and dropout, there is less overfitting than before and training and validation accuracy are closer aligned.","626803f5":"**SecondPart: Transfer Learning and fine-tuning**\n\nIn this part we will be doing transfer learning from a pre-trained network to classify the animals.\n\nA pre-trained model is a saved network that was previousl trained on a large dataset, typically on a large-scale image-classification task. You either us the pretrained model as is or use transfer learning to customize this model to a given task.\n\nThe intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on large dataset.\n\nHere, we will try two ways to customize a pretrained model:\n\n\n1.   Feature Extraction: Use the representation learned by previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n\n    You do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.\n\n2.   Fine-Tuning: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them mode relevant for the specific task.\n\nWe will add following workflow from first part before training and evaluating model:\n1.   Compose model\n    *   Load in the pretrained base model (and pretrained weights)\n    *   Stack the classification layers on top\n\n\n","dbb3812b":"This feature extractor converts eachh 160x160x3 images into a 5x5x1280 block of features.","f32c95d8":"**Visualize the data**\n\nWe load 9 images from the training dataset.","9a99bb25":"**Create the model**\n\nThe model consists of three convolutional blocks with a max pool layer in each of them. There is a fully connected layers with 128 units on top of it that is activated by relu activation function. This model has not been tuned for high accuracy, the goal is to show a standard approach.","839aa566":"**Compile the model**\n\nWe chose the optimaer.Adam optimiazer and losses.SparseCategoricalCrossentropy loss function. To view training and validation accuracy for each training epoch, pass the metrics arguement.","0c16f9ce":"**Dataset.cache()** keeps the images in memory after they are loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your datset is too large to fit into memory, you can also use this method to create a performant on-disk cache.","61c105f0":"**Configure the dataset for performance**\n\nNow, to use buffered prefetching so we can yield data from disk without having I\/O become blocking. These two important methods should be used while loading data\n\n","d0cf441d":"Or, we can include the layer inside our model definition, whhich can simplify deployment. We will use second approach below.","75e1d230":"**Create the base model from the pre-trained convnets**\n\nWe will create the base model from the MobileNetV2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. \n\nFirst, we will pick which layer of MobileNet V2 to use for feature extraction. The very last classification layer (on \"top\", as most diagram of machine learning models go from bottom to top) is not very useful. Instead, we will follow common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final\/top layer.\n\nFirst, instanitiate MobileNetV2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False arguement, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.","52d2d14e":"**Dataset.prefetch()** overlaps data preprocessing and model execution while training.\n\nYou can learn more about both methods, as well as how to cache data to disk in the data [performance guide.](https:\/\/colab.research.google.com\/drive\/1txs8-2Swlq3XuWCTPEcgDWAK3uVF5-nL#scrollTo=JD7K_urHDOFH&line=2&uniqifier=1)","cbd263b8":"**Load using keras.preprocessing**\n\nLet us load this images off disk using the image_dataset_from_directory utility. This will take us from a directory of images on disk to a tf.data.Dataset in just a couple lines of code.","4a221d35":"\nWe can manually iterate over the dataset and retrieve batches of images:","2bf29a05":"**Train the model**","1bf78fc1":"**Fine Tuning**\n\nIn the feature extraction experiment, we were only training a few layers on top of an MobileNetV2 base model. The weights of the pre-trained network were not updated during training.\n\nOne way to increase performance eventfurther is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic features maps to features associated specifically with the dataset.\n\nNote: This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned.\n\nAlso we should try to fine-tune a small number of top layers rather than the whole MobileNet model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n","1cbb4152":"Note: If you are wondering why the validation metrics are clearly better than the training metrics, the main factor is because layers like tf.keras.layers.BatchNormalization and tf.keras.layers.Dropout affect accuracy during training. They are turned off when calculating validation loss.\n\nTo a lesser extent, it is also because training metrics report the average for an epoch, while validation metrics are evaluated after the epoch, so validation metrics see a model that has trained slightly longer.","1e0ef341":"**Overfitting**\n\nIn the plot above, the training accuracy is increasing lineraly over time, whereas validation accuracy stalls around 70% in the training process. Also, the difference in accuracy between training and validation accuracy is noticeable which is a sign of overfitting.\n\nWhen there are a small number of training exapmles, the model sometimes learns from noises or unwanted details from training examples - to an extent that it negatively impacts the performance of the model on new examples. This phenomenon is known as overfitting. It means that the model will have a difficult time generalizing on a new datset.\n\nThere are multiple ways to fight overfitting in the training process. Here, we will be using data augmentation and Dropout concept in our model.","0c304e18":"**Compile the model**\n\nAs you are training a much larger model and want to readapt the pretrained weights, it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly.","618ec218":"**Create a dataset**\n\nAt first define some parameters for the loader:","3015612c":"**Important note about BatchNormalization layers**\n\nMany models contain tf.keras.layers.BatchNormalization layers. This layer is a special case and precautions should be taken in the context of fine-tuning.\n\nWhen we set layer.trainable = False, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics.\n\nWhen we unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, we should keep the BatchNormalization layers in inference mode by passing training = False when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model the model has learned.\n\n[Transfer learning guide.](https:\/\/www.tensorflow.org\/guide\/keras\/transfer_learning)","62a6dcff":"**Freeze the convolutional base**\n\nIt is important to freeze the convolutional base before we compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. MobileNetV2 has many layers, so setting the entire model's trainable falg to False will freeze all of them.","5225db81":"Let us take a look at the learning curves of the training and validation accuracy\/loss when fine-tuning the last few layers of the MobileNet V2 base model and training the calssifier on top of it. The validation loss is much higher than the training loss, so you may get some overfitting.\n\nWe may also get some overfitting as the new training set is relatively small and similar to the original MobileNet V2 datasets.","a256fbc3":"**Learning curves**\n\nLet us take a look at the learning curves of the training and validation accuracy\/loss when using the MobileNetV2 base model as a fixed feature extractor.","f989f8e2":"Apply a tf.keras.layers.Dense layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value.","fa10b608":"You can find the class names in the class_names attribute on these datasets. These correspond to the directory names in alphabetical order.","1c2e9608":"**Add a calssification head**\n\nTo generate predictions from the block of features, average over the spatial 5x5 spatial locations, using a tf.keras.layers.GlobalAveragePooling2D layer to convert the feature to a single 1280-element vector per image.","701b6639":"The image_batch is a tensor of the shape (32,160,160,3). This is a batch of 32 images of shape 160 x 150 x 3 ( the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these corresponding to the 32 images. Note: You can call .numpy() on the image_batch and labels_batch tensor to convert them to a numpy.ndarray.","7bc8db59":"**Continue training the model**\n\nIf we trained to convergence earlier, this step will improve your accuracy by a few percentage points.","f049d349":"There are two ways to use this layer. We can apply it to the dataset by calling map:","6bc23ed5":"**Un-freeze the top layers of the model**\n\nAll we need to do is unfreeze the base_model and set the bottom layers to be un-trainable. Then, we should recompile the model(ncessary for these changes to take effect), and resume training.","ab83a122":"**Train the Model**","16b5e19e":"**Predict on new data**","b44e0d2d":"The 2.5M parameters in MobileNet are frozen, but there are 5.1K trainable parameters in the Dense layer. These are divided between two tf.Variable objects, the weights and biases.","a65b7d1f":"\nBuild a model by changing together the data augmentation, rescaling, base_model and feature extractor layers using Keras Functional API. As previously mentioned, use trainable=False as our model contains BatchNormolization layer.","0883ba7c":"**Animal Classification**\n\nThis contains Two parts:\n\nAnimal classification using Keras.Sequential model.\nAnimal classification by using transfer learning from a pretrained network.\n\n**First Part**\n\nThis part is basically the basic structure for someone to gain practical experience with following concepts\":\n\nEfficient loading a dataset off disk\nIdentifying overfitting and applying techniques to mitigate it, including data augmentation and Dropout.\nIt also covers a basic machine learining workflow:\n1. Examine and understand data\n2. Build an input pipeline\n3. Build the model\n4. Train the model\n5. Test the model\n6. Improve the model and repeat the process\n\nImport all the libraries required","ae5c6c08":"**Compile the model**\n\nCompile the model before training it. Since there are 4 classes, we will use SparseCatgoricalCrossentropy loss with form_logits =True."}}