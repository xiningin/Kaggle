{"cell_type":{"820e7aa6":"code","bf6bff9e":"code","49a7b1c3":"code","1b85d1e0":"code","dbd3100f":"code","4a88cbba":"code","187b9b7b":"code","065f3e27":"code","8d9bfd0e":"code","9a281932":"code","01c8b9f4":"code","2f19f0b6":"code","ef9fa6f3":"code","deae5cfd":"code","4f49f47a":"code","c751f6d3":"code","b55114ad":"code","cb6f35a7":"code","aa0b575b":"code","9c078049":"code","a669998c":"code","77626de2":"markdown","e34b485c":"markdown","ad684764":"markdown","ea284b4a":"markdown","21adf10c":"markdown","860b27d4":"markdown","b38e62ac":"markdown","ba1916d9":"markdown","67d593ae":"markdown","6d907045":"markdown","4ec10e9c":"markdown","a385a26e":"markdown","d60b9c18":"markdown","b68e7c49":"markdown","5404250b":"markdown","046c5e28":"markdown","0db28642":"markdown","90e7746d":"markdown","36ec7b58":"markdown"},"source":{"820e7aa6":"!pip install --upgrade wandb\n!pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https:\/\/download.pytorch.org\/whl\/cu113\/torch_stable.html\n!pip install git+https:\/\/github.com\/rwightman\/pytorch-image-models","bf6bff9e":"import os\nfrom kaggle_secrets import UserSecretsClient\nimport copy \nimport time\n\nimport wandb\n\nimport numpy as np\nfrom collections import defaultdict\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error as mse\n\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\nimport matplotlib.pyplot as plt\nimport cv2\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda import amp\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler as LR\nimport timm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom colorama import Fore, Back, Style\n\nfrom IPython.display import IFrame\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')","49a7b1c3":"TRAIN_CONFIG = dict(\n    height = 128,\n    width = 128,\n    channels = 3,\n    num_classes = 1,\n    n_splits = 5,\n    batch_size = 32,\n    model = \"tf_efficientnet_b4_ns\",\n    pretrained = True,\n    epochs = 10,\n    seed = 2021,\n    learning_rate = 0.0001,\n    min_lr = 1e-6,\n    T_max = 100,\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n)","1b85d1e0":"b_crma = Fore.BLUE\nr_crma = Fore.RED\ny_crma = Fore.YELLOW\nsr_crma = Style.RESET_ALL\n\nTRAIN_PATH = \"..\/input\/petfinder-pawpularity-score\/train\"\nCSV_PATH = \"..\/input\/petfinder-pawpularity-score\/train.csv\"\n\ntrain_df = pd.read_csv(CSV_PATH)\nfeature_cols = list(train_df.columns)[1:-1]\ntrain_df[\"Filepath\"] = train_df[\"Id\"].apply(lambda x : os.path.join(TRAIN_PATH,x+\".jpg\"))\n\nbin_ranges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nbin_names = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\npseudo_labels = np.array(pd.cut(np.array(train_df[\"Pawpularity\"]),bins=bin_ranges,labels=bin_names))\nfiles_ls = list(train_df[\"Filepath\"])\n\nrand_img = torch.randn(1,3,TRAIN_CONFIG[\"height\"],TRAIN_CONFIG[\"width\"]).to(TRAIN_CONFIG[\"device\"])\nrand_meta = torch.randn(1,len(feature_cols)).to(TRAIN_CONFIG[\"device\"])\n    \nprint(f\"Number of Training Images : {len(train_df)} \\n\\n\")\ntrain_df.head()","dbd3100f":"# Check https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-pawpularity-training for more details\ntry:\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('''If you want to use your W&B account, Follow these steps :\n            -> go to Add-ons {Below name of notebook} -> Secrets -> Add a new Secret\n            -> Label = wandb_api\n            -> Value = W&B access token from https:\/\/wandb.ai\/authorize \n         ''')","4a88cbba":"WANDB_CONFIG = {\n   \"project_name\" : \"PetFinder.my - Pawpularity Contest\",\n   \"group_name\" : \"PyTorch Training\",\n   \"job_type_data\" : \"Data Visualization\",\n    \"job_type_train\" : \"Training\",\n   \"anonymity\" : \"must\",\n    \"artifact\" : \"training_data\"\n}\n\nrun = wandb.init(\n    project = WANDB_CONFIG[\"project_name\"],\n    group = WANDB_CONFIG[\"group_name\"],\n    job_type = WANDB_CONFIG[\"job_type_data\"],\n    anonymous= WANDB_CONFIG[\"anonymity\"]\n)\n\nwb_table = wandb.Table(columns = [\n    'Id', 'Image', 'Pawpularity', 'Subject Focus', 'Eyes', 'Face',\n    'Near', 'Action', 'Accessory', 'Group', 'Collage', \n     'Human', 'Occlusion', 'Info', 'Blur'\n])","187b9b7b":"for i in tqdm(range(len(train_df))):\n    row = train_df.loc[i]\n    impath = os.path.join(TRAIN_PATH,row[\"Id\"]+\".jpg\")\n    wb_table.add_data(row['Id'],\n                      wandb.Image(impath),\n                      row['Pawpularity'],\n                      row['Subject Focus'],\n                      row['Eyes'],\n                      row['Face'],\n                      row['Near'],\n                      row['Action'],\n                      row['Accessory'],\n                      row['Group'],\n                      row['Collage'],\n                      row['Human'],\n                      row['Occlusion'],\n                      row['Info'],\n                      row['Blur'])\n    \nwandb.log({'Data Visualization': wb_table})\nrun.finish()","065f3e27":"frame = IFrame(run.url, width=1280, height=720)\nframe","8d9bfd0e":"train_report = ProfileReport(train_df,title=\"Metadata of Training images\")\ntrain_report.to_file(\".\/train_metadata.html\")\ntrain_report","9a281932":"class PetfinderDataset(Dataset):\n    def __init__(self, image_dir, df, data_type):\n        self.image_dir = image_dir\n        self.df = df\n        self.data_type = data_type\n        self.filepaths = df[\"Filepath\"].values\n        self.labels = df[\"Pawpularity\"].values\n        self.meta_features = df[feature_cols].values\n        self.data_type = data_type\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        image = cv2.imread(self.filepaths[idx])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.data_type == \"train\":\n            transforms = A.Compose([\n                A.Resize(TRAIN_CONFIG[\"width\"], TRAIN_CONFIG[\"height\"]),\n                A.HorizontalFlip(p=0.5),\n                A.RandomBrightnessContrast(p=0.2),\n                A.Normalize(\n                        mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225], \n                        max_pixel_value=255.0, \n                        p=1.0 \n                    ),\n                ToTensorV2()], p=1.0)\n        else:\n            transforms = A.Compose([\n                A.Resize(TRAIN_CONFIG[\"width\"], TRAIN_CONFIG[\"height\"]),\n                A.Normalize(\n                        mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225], \n                        max_pixel_value=255.0, \n                        p=1.0 \n                    ),\n                ToTensorV2()], p=1.0)\n            \n        image = transforms(image=image)[\"image\"]\n        meta_feature = self.meta_features[idx, :]\n        label = self.labels[idx]\n        \n        return image, meta_feature, label","01c8b9f4":"def create_dataloader(train_idx, val_idx):\n    train_ds = PetfinderDataset(TRAIN_PATH, train_df.iloc[train_idx], \"train\")\n    val_ds = PetfinderDataset(TRAIN_PATH, train_df.iloc[val_idx], \"valid\")\n    \n    train_loader = DataLoader(train_ds, batch_size=TRAIN_CONFIG[\"batch_size\"],shuffle=True, drop_last=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_ds, batch_size=TRAIN_CONFIG[\"batch_size\"], shuffle=False, drop_last=True, num_workers=4, pin_memory=True)\n    \n    return train_loader, val_loader","2f19f0b6":"class PetfinderModel(nn.Module):\n    def __init__(self, model, pretrained):\n        super(PetfinderModel, self).__init__()\n        self.base_model = timm.create_model(model, pretrained=pretrained, num_classes=0)\n        self.dropout = nn.Dropout(p=0.3)\n        self.fc = nn.LazyLinear(TRAIN_CONFIG[\"num_classes\"])\n        \n    def forward(self, images, meta_features):\n        features = self.base_model(images)\n        features = self.dropout(features)\n        features = torch.cat([meta_features, features], dim=1)\n        output = self.fc(features)\n        \n        return output","ef9fa6f3":"def loss_criterion(y_pred, y_true):\n    mse_loss = nn.MSELoss()(y_pred.view(-1), y_true.view(-1))\n    return torch.sqrt(mse_loss)","deae5cfd":"def model_optimizer():\n    return optim.Adam(model.parameters(), lr = TRAIN_CONFIG[\"learning_rate\"])","4f49f47a":"def lr_scheduler(optimizer):\n    scheduler = LR.CosineAnnealingLR(optimizer, T_max=TRAIN_CONFIG['T_max'], \n                                                   eta_min=TRAIN_CONFIG['min_lr'])\n    return scheduler","c751f6d3":"def train_one_epoch(model, optimizer, scheduler, data_loader, epoch, device):\n    model.train()\n    total_loss, total_instances = 0, 0\n    y_pred, y_true = [], []\n    \n    grad_scaler = amp.GradScaler()\n    \n    print(f\"----------Epoch {epoch}\/{TRAIN_CONFIG['epochs']}--------------\")\n    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Training\")\n    \n    for batch_id, (images, meta_features, targets) in pbar:\n        images = images.to(device, dtype=torch.float)\n        meta_features = meta_features.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        num_instances = targets.size(0)\n        \n        with amp.autocast():\n            preds = model(images, meta_features)\n            loss = loss_criterion(preds, targets)\n            \n        grad_scaler.scale(loss).backward()\n        grad_scaler.step(optimizer)\n        grad_scaler.update()\n        optimizer.zero_grad()\n        \n        total_loss += (loss.item()*num_instances)\n        total_instances += num_instances\n        \n        y_true.append(targets.view(-1).cpu().detach().numpy())\n        y_pred.append(preds.view(-1).cpu().detach().numpy())\n    \n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    train_loss = total_loss\/total_instances\n    train_rmse = mse(y_true, y_pred, squared=False)\n    \n    gc.collect() \n    \n    return train_loss, train_rmse","b55114ad":"@torch.no_grad()\ndef val_one_epoch(model, data_loader, epoch, device):\n    model.eval()\n    total_loss, total_instances = 0, 0\n    y_pred, y_true = [], []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=\"Validation\")\n    \n    for batch_id, (images, meta_features, targets) in pbar:\n        images = images.to(device, dtype=torch.float)\n        meta_features = meta_features.to(device, dtype=torch.float)\n        targets = targets.to(device, dtype=torch.float)\n        \n        num_instances = targets.size(0)\n        \n        preds = model(images, meta_features)\n        loss = loss_criterion(preds, targets)\n        \n        total_loss += (loss.item()*num_instances)\n        total_instances += num_instances\n        \n        y_true.append(targets.view(-1).cpu().detach().numpy())\n        y_pred.append(preds.view(-1).cpu().detach().numpy())\n    \n    y_true = np.concatenate(y_true)\n    y_pred = np.concatenate(y_pred)\n    \n    val_loss = total_loss\/total_instances\n    val_rmse = mse(y_true, y_pred, squared=False)\n    \n    gc.collect()\n    \n    return val_loss, val_rmse","cb6f35a7":"def fit_model(model, optimizer, scheduler,train_loader, val_loader, device, epochs, fold):\n    if fold == 0:\n        wandb.watch(model, log_freq=100)\n    \n    if TRAIN_CONFIG['device'] == 'cpu':\n        print(\"Using CPU \\n\")\n    else:\n        print(\"Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n        \n    #best_weights = copy.deepcopy(model.state_dict())\n    best_val_rmse = np.inf\n    history = defaultdict(list)\n    model_path = \"bestmodel_fold_{:.0f}.bin\".format(fold)\n    \n    start = time.time()\n    for epoch in range(1, epochs+1):\n        gc.collect()\n        \n        train_loss, train_rmse = train_one_epoch(model, optimizer, scheduler, train_loader, epoch, device)\n        val_loss, val_rmse = val_one_epoch(model, val_loader, epoch, device)\n        \n        print(f\"Train Loss : {train_loss} - Train RMSE : {train_rmse} - Val Loss : {val_loss} - Val RMSE : {val_rmse}\")\n        \n        history[\"train_loss\"].append(train_loss)\n        history[\"train_rmse\"].append(train_rmse)\n        history[\"val_loss\"].append(val_loss)\n        history[\"val_rmse\"].append(val_rmse)\n        \n        wandb.log({\"Train Loss Fold {}\".format(fold) : train_loss})\n        wandb.log({\"Val RMSE Fold {}\".format(fold) : val_rmse})\n        wandb.log({\"Valid Loss Fold {}\".format(fold) : val_loss})\n        wandb.log({\"Valid RMSE Fold {}\".format(fold) : val_rmse})\n        \n        if val_rmse <= best_val_rmse:\n            print(f\"{b_crma}Validation RMSE Improved from {best_val_rmse} to {val_rmse}, saving model to {model_path} {sr_crma} \\n\")\n            best_val_rmse = val_rmse\n            #best_weights = copy.deepcopy(model.state_dict())\n            torch.save(model.state_dict(), model_path)\n    \n        else:\n            print(f\"{r_crma}Validation RMSE didn't improve from {best_val_rmse} {sr_crma} \\n\")\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training completed in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(f\"{y_crma}Best Val RMSE: {best_val_rmse} {sr_crma}\")\n    \n    #model.load_state_dict(best_weights)\n    gc.collect()\n    return history","aa0b575b":"run = wandb.init(\n    project = WANDB_CONFIG[\"project_name\"],\n    group = WANDB_CONFIG[\"group_name\"],\n    job_type = WANDB_CONFIG[\"job_type_train\"],\n    anonymous= WANDB_CONFIG[\"anonymity\"]\n)","9c078049":"kfold = StratifiedKFold(n_splits=TRAIN_CONFIG[\"n_splits\"], shuffle=True, random_state=TRAIN_CONFIG[\"seed\"])\nhistory_dict = {}\n\nfor fold, (tID, vID) in enumerate(kfold.split(files_ls, pseudo_labels)):\n    print(f\"Working in fold {fold+1}\")\n    train_loader, val_loader = create_dataloader(tID, vID)\n    \n    model = PetfinderModel(TRAIN_CONFIG['model'], pretrained=TRAIN_CONFIG[\"pretrained\"])\n    model.to(TRAIN_CONFIG['device'])\n    model(rand_img, rand_meta)\n    \n    optimizer = model_optimizer()\n    scheduler = lr_scheduler(optimizer)\n    \n    history_dict[fold] = fit_model(model, optimizer, scheduler, \n                                   train_loader=train_loader,\n                                   val_loader=val_loader,\n                                   device=TRAIN_CONFIG[\"device\"],\n                                   epochs=TRAIN_CONFIG[\"epochs\"],\n                                   fold = fold)\n    print(\"\\n\\n\")","a669998c":"frame = IFrame(run.url, width=1280, height=720)\nframe","77626de2":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px;  text-align:center; ; color:#FF69B4\">Callbacks Function<\/h2>","e34b485c":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Necessary Functions<\/h1>","ad684764":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Initializing Constants<\/h1>","ea284b4a":"<div style = \"font-family : Lucida Sans Typewriter;background: rgb(224,224,224);border-radius: 25px;\">\n    <h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Weights and Biases<\/h2>\n    <center><img src = \"https:\/\/i.imgur.com\/KISYcqD.png\" width=200 height = 200><\/center>\n    <a href = \"\"; style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; font-size:25px\">Checkout Dashboard created for this notebook<\/a>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    Weights and Biases is a set of Machine Learning tools used for experiment tracking, dataset versioning, and collaborating on ML projects. Weights and Biases is useful in many applications such as\n    <\/p>  \n    <ul>\n          <li>Experiment Tracking<\/li>\n          <li>Hyperparameter Tuning<\/li>\n          <li>Data Visualization<\/li>\n          <li>Data and model Versioning<\/li>\n          <li>Collaborative Reports<\/li>\n    <\/ul>\n    <a href = \"https:\/\/wandb.ai\/site\">Go to offocial website for more tutorials and Documentation<\/a>\n<\/div>","21adf10c":"<center style = \"font-family: 'Lucida Console', 'Courier New', monospace;\">\n    <img src = \"https:\/\/www.bargainmarket.com.au\/images\/banner-pets-dog-cat-boarding.png\" width=600 height = 400>\n    <h1 style = \"background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%);border-radius: 20px; font-size:30px\">PetFinder.my - Pawpularity Contest \ud83d\udc36\ud83d\udc08<\/h1>\n<\/center>\n\n<div style = \"background: rgb(224,224,224);border-radius: 42px;\">\n    <h1 style = \"font-family: Consolas; text-align:center; color:#FF69B4\">Sponsors \ud83d\udcb0<\/h1>\n    <h2 style = \"font-family: Consolas; text-align:center\">PetFinder.my<\/h2>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    PetFinder.my is Malaysia\u2019s leading animal welfare platform, featuring over 180,000 animals with 54,000 happily adopted. PetFinder collaborates closely with animal lovers, media, corporations, and global organizations to improve animal welfare.\n        <a href = \"https:\/\/petfinder.my\/\">Check their official website<\/a>\n<\/div>\n\n<div style = \"background: rgb(224,224,224);border-radius: 42px;\">\n    <h1 style = \"font-family: Consolas; text-align:center; color:#FF69B4\">Introduction<\/h1>\n    <h2 style = \"font-family: Consolas; text-align:center\">Why this Competition \u2753<\/h2>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n        Currently, PetFinder.my uses a basic <a href = \"https:\/\/petfinder.my\/cutenessmeter\">Cuteness Meter<\/a> to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. While this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved.\n    <\/p>\n    <h2 style = \"font-family: Consolas; text-align:center\">Goal of Competition \ud83e\udd45<\/h2>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    The goal of this competition is to predict \"PawPularity\" of pet using raw images and metadata which will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements. As a result, stray dogs and cats can find their \"furever\" homes much faster.\n    <\/p>\n<\/div>\n\n<h2 style = \"font-family: Consolas\">More Details<\/h2>\n<p style = \"font-family : Lucida Sans Typewriter\">Check <a href = \"https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/data\">competition page<\/a> for details<\/p>\n<h2 style = \"font-family : Comic Sans MS\">Let's dive in \u2b07\ufe0f<\/h2>\n\n<center><img src = \"https:\/\/img.shields.io\/badge\/Upvote-If%20you%20found%20this%20notebook%20useful-blue\" width=400 height = 400><\/center>\n\n<p style = \"font-family : Lucida Sans Typewriter\">Thanks @debarshichanda for wonderful <a href = \"https:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-pawpularity-training\">notebook<\/a>. It helped me to get started with PyTorch<\/p>","860b27d4":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; ; color:#FF69B4\">Model Function<\/h2>","b38e62ac":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; ; color:#FF69B4\">All at One Place<\/h2>","ba1916d9":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px;  text-align:center; ; color:#FF69B4\">Validation Function<\/h2>","67d593ae":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Import Libraries<\/h1>","6d907045":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Install Dependencies<\/h1>","4ec10e9c":"<div style = \"font-family : Lucida Sans Typewriter;background: rgb(224,224,224);border-radius: 25px;\">\n    <h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; font-size:30px; text-align:center; ; color:#FF69B4\">Pandas Profiling<\/h2>\n    <center><img src = \"https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/assets\/logo_header.png\" width=200 height = 200><\/center>\n    <p style = \"font-family : Lucida Sans Typewriter\">\n    Pandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Besides, if this is not enough to convince us to use this tool, it also generates interactive reports in web format that can be presented to any person, even if they don\u2019t know programming.\n    <\/p>  \n    <a href = \"https:\/\/pandas-profiling.github.io\/pandas-profiling\/\">Go to offocial website for documentation<\/a>\n<\/div>","a385a26e":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px;  text-align:center; ; color:#FF69B4\">Training Function<\/h2>","d60b9c18":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; ; color:#FF69B4\">Optimizer Function<\/h2>","b68e7c49":"<center><img src = \"https:\/\/img.shields.io\/badge\/Completed-The%20End-brightgreen\" width=400 height = 400><\/center>","5404250b":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">History of Training<\/h1>","046c5e28":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Data Visualization \ud83d\udcca\ud83d\udcb9<\/h1>","0db28642":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px;  text-align:center; ; color:#FF69B4\">Loss Function<\/h2>","90e7746d":"<h1 style = \"font-family: 'Lucida Console', 'Courier New', monospace; background: rgb(44,169,201);\nbackground: linear-gradient(180deg, rgba(44,169,201,1) 0%, rgba(1,94,125,1) 100%); border-radius: 20px; font-size:30px; text-align:center; \">Training<\/h1>","36ec7b58":"<h2 style = \"font-family: 'Lucida Console', 'Courier New', monospace; border-radius: 20px; text-align:center; ; color:#FF69B4\">DataLoader<\/h2>"}}