{"cell_type":{"fe75ce8d":"code","708b21a3":"code","cd02c0bd":"code","02d30105":"code","129966b4":"code","d2910ad1":"code","acd1cbac":"code","664ae733":"code","128bc6da":"code","ccd0b57d":"code","89b76334":"code","ce81d313":"code","a4d6457c":"code","e72b9081":"code","ccf5d787":"code","6df0fba7":"code","77880531":"code","ed3e3994":"code","5e31f902":"code","41428de8":"code","126ca22c":"code","478e23b5":"code","8c1a3a6f":"code","0c114364":"code","19a7fbfc":"code","4b93d32d":"code","968d657d":"code","64a2740a":"code","8dd2a269":"code","9826f665":"code","7a4aff31":"code","a9286f88":"code","745ab134":"code","4f35aba5":"code","df25bf66":"markdown","7c4ffb0e":"markdown","dcab08c7":"markdown","8ed23216":"markdown","2664d8b0":"markdown","4ef22fb3":"markdown","06a54927":"markdown","2d1551bc":"markdown","ba59b671":"markdown","f0af4e7a":"markdown","e128a010":"markdown","300936a0":"markdown","7861fdd2":"markdown","93744821":"markdown","ec2afc52":"markdown","cb814c59":"markdown","e590919d":"markdown","b3120a3f":"markdown","16f5f51c":"markdown","56fbf429":"markdown","ffbf13aa":"markdown","64d4a627":"markdown"},"source":{"fe75ce8d":"#for data processing\nimport numpy as np \nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom math import isnan\n#pd.set_option('display.max_rows', 100) # for displaying full dataset\n\n#for feature engineering\nfrom sklearn.feature_selection import mutual_info_regression\ndef make_mi_scores(X, y, discrete_features): # from feature engeneering course\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\n#for model creation and evaluation\nfrom sklearn.ensemble import GradientBoostingClassifier # for XGboost model\nfrom sklearn.linear_model import LogisticRegression # for logistic regression model\nfrom sklearn.svm import SVC # for SVM model\nfrom sklearn.neighbors import KNeighborsClassifier # for KNN\nfrom sklearn.naive_bayes import GaussianNB # for NB\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score,make_scorer\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\n\n# for plotting the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n%matplotlib inline","708b21a3":"raw_data_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nraw_data_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nfulldata = pd.concat([raw_data_train,raw_data_test])\ntrain_index = fulldata[\"Survived\"].isna() == False\ntest_index = fulldata[\"Survived\"].isna()\ntrain_data.head()","cd02c0bd":"train_data.info()","02d30105":"test_data.info()","129966b4":"train_data.isna().sum()","d2910ad1":"test_data.isna().sum()","acd1cbac":"train_data.describe()","664ae733":"test_data.describe()","128bc6da":"cat_cols = fulldata.select_dtypes(include = \"object\").columns\nnum_cols = fulldata.select_dtypes(exclude = \"object\").columns\nprint(f\"categorial columns are: {cat_cols}\")\nprint(f\"numerical columns are: {num_cols}\")","ccd0b57d":"n_uniqe = {col:fulldata[col].nunique() for col in cat_cols}\nfor key in n_uniqe:\n    print(f\"{key} has {n_uniqe[key]} unique values\")","89b76334":"#for later use\nfulldata[\"Last_Name\"] = [name.split(',')[0].strip() for name in fulldata[\"Name\"]]","ce81d313":"#getting Cabin prefix when it exists\ndef cabin_transform(df):\n    cdf = df.copy()\n    cabin = []\n    for x in cdf[\"Cabin\"]:\n        if type(x) != str:\n            cabin.append(np.nan)\n        else:\n            cabin.append(x[0])\n    return(cabin)\nfulldata[\"Cabin\"] = cabin_transform(fulldata)","a4d6457c":"last_name_cabin = pd.crosstab(fulldata[\"Last_Name\"],fulldata[\"Cabin\"])\n(last_name_cabin != 0).sum(axis = 1).sum() - len(last_name_cabin)  #number of same last name and staying in diffrent cabin = 8\nticket_cabin = pd.crosstab(fulldata[\"Ticket\"],fulldata[\"Cabin\"])\n(ticket_cabin != 0).sum(axis = 1).sum() - len(ticket_cabin)  #number of same last name and staying in diffrent cabin = 2\ncabin_survival = pd.crosstab(fulldata[\"Cabin\"],fulldata[\"Survived\"],normalize=\"index\")\ncabin_survival","e72b9081":"#fill na using last name\ngrp_data_lname = fulldata[fulldata[\"Cabin\"].isna() == False].groupby([\"Last_Name\"])[\"Cabin\"].sum()\nlast_name_cabin_map = {grp_data_lname.index[i]:x[0] for i,x in enumerate (grp_data_lname)}\nfulldata[\"Cabin\"] = fulldata[\"Last_Name\"].map(last_name_cabin_map)\n\n#fill na using last ticket\ngrp_data_ticket = fulldata[fulldata[\"Cabin\"].isna() == False].groupby([\"Ticket\"])[\"Cabin\"].sum()\nticket_cabin_map = {grp_data_ticket.index[i]:x[0] for i,x in enumerate (grp_data_ticket)}\nfulldata[\"Cabin\"] = fulldata[\"Ticket\"].map(ticket_cabin_map)\n\nfulldata[\"Cabin\"].fillna(\"Z\",inplace =True) # for separating na","ccf5d787":"#drop unused columns\nfulldata.drop([\"Name\",\"Ticket\",\"PassengerId\"],axis = 1,inplace = True) \n#filling na's\nembk_mode = fulldata[\"Embarked\"].mode()[0]\nfulldata[\"Embarked\"].fillna(embk_mode,inplace = True)\nfulldata[\"Fare\"].fillna(fulldata[\"Fare\"].median(),inplace = True) #Fare is skewed so we will use the median","6df0fba7":"plt.hist(fulldata[\"Age\"])\nplt.vlines(fulldata[\"Age\"].mean(),colors = \"red\",ymin = 0,ymax =275)\nplt.vlines(fulldata[\"Age\"].median(),colors = \"green\",ymin = 0,ymax =275)\nplt.vlines(fulldata[\"Age\"].mode(),colors = \"black\",ymin = 0,ymax =275)\nplt.legend([\"mean\",\"median\",\"mode\"])\nprint(fulldata.corr()[\"Age\"])\nprint(fulldata.groupby([\"Pclass\",\"Sex\"])[\"Age\"].mean()) # all other subgroups has low counts therefore the mean is irrelevant\nfulldata[\"Age\"].fillna(fulldata.groupby([\"Pclass\",\"Sex\"])[\"Age\"].transform(\"mean\"),inplace = True)","77880531":"def label_encode_for_df(train_data,col):\n    le = LabelEncoder()\n    train_data[col] = le.fit_transform(train_data[col])\n    print(f\"classes encoded for {col} are {le.classes_}\")\n    return(train_data)\n\nfulldata_for_plots = fulldata.copy()\nfor col in [\"Sex\",\"Embarked\",\"Cabin\"]:\n    fulldata_for_plots = label_encode_for_df(fulldata_for_plots,col)\n\nfulldata[\"Pclass\"] = fulldata[\"Pclass\"].apply(str)\nfulldata = pd.concat([fulldata.drop([\"Sex\",\"Cabin\",\"Embarked\",\"Pclass\"],axis = 1),pd.get_dummies(fulldata[[\"Sex\",\"Cabin\",\"Embarked\",\"Pclass\"]],drop_first= True)],axis = 1)","ed3e3994":"# gets model and data, returns model acc\ndef model_score(model,X_train,y_train,X_test,y_test):\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    return(accuracy_score(y_test,y_pred))\n\ndef cv_eval_model(X,y):\n    xgb_model = GradientBoostingClassifier()\n    lr_model = LogisticRegression(solver='liblinear')\n    svm_model = SVC(kernel=\"linear\")\n    KNN_model = KNeighborsClassifier()\n    NB_model = GaussianNB()\n    for model in [xgb_model,lr_model,svm_model,KNN_model,NB_model]:\n        print(f\"{model} cv acc is : {cross_val_score(model,X,y,cv=10,scoring=make_scorer(accuracy_score)).mean()}\")\n\nX = fulldata[train_index].drop([\"Survived\",\"Last_Name\"],axis = 1)\ny = fulldata[train_index].Survived\ntest_data = fulldata[test_index].drop([\"Survived\",\"Last_Name\"],axis = 1)\ncv_eval_model(X,y)","5e31f902":"fig, axes = plt.subplots(3,3, figsize=(18, 10))\nfor i in range(len(fulldata_for_plots.columns.drop([\"Survived\",\"Last_Name\"]))):\n    col = fulldata_for_plots.columns.drop([\"Survived\",\"Last_Name\"])[i]\n    sns.histplot(fulldata_for_plots[col],ax =axes[int(np.floor(i\/3)),i%3])","41428de8":"fulldata[\"Fare_transformed\"] = np.log(fulldata[\"Fare\"]+1)\nfulldata.drop(\"Fare\",axis = 1,inplace = True)\n\nfulldata_for_plots[\"Fare_transformed\"] = np.log(fulldata_for_plots[\"Fare\"]+1)\nfulldata_for_plots.drop(\"Fare\",axis = 1,inplace = True)\n\nsns.displot(fulldata_for_plots[\"Fare_transformed\"])","126ca22c":"age_groups = np.array([0,5,15,25,30,40,50,60,70,81])\nfulldata[\"age_groups\"] = np.digitize(fulldata[\"Age\"],age_groups)\nfulldata.drop(\"Age\",axis = 1,inplace = True)\n\nfulldata_for_plots[\"age_groups\"] = np.digitize(fulldata_for_plots[\"Age\"],age_groups)\nfulldata_for_plots.drop(\"Age\",axis = 1,inplace = True)","478e23b5":"nominal = fulldata_for_plots.columns.drop([\"Survived\",\"Last_Name\",\"Fare_transformed\"])\n\nfig, axes = plt.subplots(3,3, figsize=(18, 10))\nfor i in range(len(nominal)):\n    col = nominal[i]\n    sns.countplot(x=col, hue=\"Survived\", data=fulldata_for_plots,ax = axes[int(np.floor(i\/3)),i%3])","8c1a3a6f":"X = fulldata[train_index].drop([\"Survived\",\"Last_Name\"],axis = 1)\ny = fulldata[train_index].Survived\ntest_data = fulldata[test_index].drop([\"Survived\",\"Last_Name\"],axis = 1)\nsns.heatmap(fulldata_for_plots.corr(),annot = True)","0c114364":"#shap values is used for a better understanding of what the model is doing\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,random_state = 100)\nxgb_model = GradientBoostingClassifier()\nxgb_model.fit(X_train,y_train)\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)\ncv_eval_model(X,y)","19a7fbfc":"fulldata[\"Family_members\"] = fulldata[\"Parch\"] + fulldata[\"SibSp\"]\nsns.countplot(x=\"Family_members\", hue=\"Survived\", data=fulldata)","4b93d32d":"Family_members_groups = np.array([0,1,4,7,11])\nfulldata[\"Family_members_groups\"] = np.digitize(fulldata[\"Family_members\"],Family_members_groups)\nfulldata.drop([\"Family_members\"],axis = 1,inplace = True)\n\nsns.countplot(x=\"Family_members_groups\", hue=\"Survived\", data=fulldata)","968d657d":"X = fulldata[train_index].drop([\"Survived\",\"Last_Name\"],axis = 1)\ncv_eval_model(X,y)\nprint(make_mi_scores(X, y, X.dtypes == \"int64\"))\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,random_state = 100)\nxgb_model = GradientBoostingClassifier()\nxgb_model.fit(X_train,y_train)\nperm = PermutationImportance(xgb_model, random_state=1).fit(X_test, y_test,n_iter=20)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(),top  = 20)","64a2740a":"explainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","8dd2a269":"X = fulldata[train_index].drop([\"SibSp\",\"Parch\",\"Survived\",\"Last_Name\"],axis = 1)\ncv_eval_model(X,y)","9826f665":"xgb_model = GradientBoostingClassifier()\nX_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.8,random_state = 100)\nxgb_model.fit(X_train,y_train)\nperm = PermutationImportance(xgb_model, random_state=1).fit(X_test, y_test,n_iter=20)\neli5.show_weights(perm, feature_names = X_test.columns.tolist(),top  = 20)","7a4aff31":"explainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_test)\nshap.summary_plot(shap_values, X_test)","a9286f88":"test_data = fulldata[test_index].drop([\"SibSp\",\"Parch\",\"Survived\",\"Last_Name\"],axis = 1)\ncv_eval_model(X,y)","745ab134":"# this is the code for the GridSearchCV I used\ndef find_best_params_gs(clf,param_grid,X, y):\n    gcv = GridSearchCV(clf,param_grid = param_grid,scoring =make_scorer(accuracy_score),cv = 10,verbose = 1,n_jobs = -1)\n    gcv.fit(X_train,y_train)\n    print(f\"{clf} model best params are: {gcv.best_params_}, and score is {gcv.best_score_}\")\n    return(gcv.best_params_,gcv.best_score_)\n#KNN_param_grid = {'n_neighbors': [1,3,5,7,9,11,13,15,17,19],'weights':['uniform', 'distance'],'algorithm':['ball_tree', 'kd_tree','brute']}\n#svm_param_grid = {'kernel':['linear','poly','sigmoid'],'degree':[2,3],'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1]}#\n\n#lr_param_grid = {'penalty':[\"l1\",\"l2\", \"none\"],'C': [0.001,0.01,0.1,1,10,100],\"fit_intercept\":[True,False],\"solver\":['newton-cg', 'lbfgs', 'liblinear']}\n\n#xgb_param_grid = {\"learning_rate\":[0.1,0.5,1],\"n_estimators\":[50,100,250,500,1000,2500],\"subsample\":[0.6,0.8,1],'max_depth': [3,4,5,6,7,8,9,10]}\n#knn_parm,knn_score = find_best_params_gs(KNeighborsClassifier(),KNN_param_grid,X, y)\n#svm_parm,svm_score = find_best_params_gs(SVC(),svm_param_grid,X, y)\n#lr_parm,lr_score = find_best_params_gs(LogisticRegression(),lr_param_grid,X, y)\n#xgb_parm,xgb_score = find_best_params_gs(GradientBoostingClassifier(),xgb_param_grid,X, y)","4f35aba5":"knn_parm = {'algorithm': 'ball_tree', 'n_neighbors': 13} # from gridsearchCV\nlr_parm = {'C': 1, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'newton-cg'}# from gridsearchCV\nsvm_parm = {'C': 1, 'degree': 2, 'gamma': 0.1, 'kernel': 'poly'}# from gridsearchCV\nxgb_parm = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50, 'subsample': 1}# from gridsearchCV\n\nxgb_model = GradientBoostingClassifier(**xgb_parm)\nlr_model = LogisticRegression(**lr_parm)\nsvm_model = SVC(**svm_parm)\nKNN_model = KNeighborsClassifier(**knn_parm)\nestimator = [('xgb_model',xgb_model),('lr_model',lr_model),('svm_model',svm_model),('KNN_model',KNN_model)]\nvot = VotingClassifier(estimators = estimator, voting ='hard')\nvot.fit(X,y)\n\nfor model in [xgb_model,lr_model,svm_model,KNN_model]:\n    print(f\"{model} cv acc after tune is : {cross_val_score(model,X,y,cv=10,scoring=make_scorer(accuracy_score)).mean()}\")\n\ny_pred = vot.predict(X_test)\nprint(f\"voting acc after tune is:{accuracy_score(y_test, y_pred)}\")\n\ny_pred_sbu = vot.predict(test_data)\nsub = pd.DataFrame({\"Survived\":y_pred_sbu.astype(int),\"PassengerId\" : raw_data_test[\"PassengerId\"]})\nsub.to_csv('submission.csv', index=False)\n","df25bf66":"from the summary we can see that the test and train data features has similar statistics, we will work with the full data to fill our na values","7c4ffb0e":"# 3. *now that we have cleaned the data let's make baseline models before we move to some data visualization and feature engineering*","dcab08c7":"it looks like we can group the number of family memebers that have similar survival ratio","8ed23216":"Let's group age values to bins","2664d8b0":"Looks like Parch and SibSp have no importance to our model, let's keep the more important features","4ef22fb3":"It looks like all the data is relevant, for each feature we have at least 1 class where the odss of surviving are < 50%","06a54927":"Let's see survival rate for each of the features values","2d1551bc":"# 1. *Let's get familiar with the data*","ba59b671":"we have missing values in both the data_sets, and the test set size is about 50% of the train set size","f0af4e7a":"let's see age distribution before making decisions for what to do with the missing data","e128a010":"**now that we have cleaned and visualzed the data let's start engineering some features**\n\nfirst re evaluate our model","300936a0":"first let's see how are data is distributed and transform some features if necessary","7861fdd2":"we have 7 numeric columns and 5 categorial ones.\ncabin has 687 na values and age has 177, embarked has only 2","93744821":"encodeding for categorial data:","ec2afc52":"most of our data is nominal so there is no transformation needed for most features, Fare price is skewd so will use a transformation","cb814c59":"# 4. **Data visualization**","e590919d":"looks like Pclass,Fare,Sex and Cabin have the best correlation although its rather low","b3120a3f":"# **Thanks for reading! fell free to leave a comment \/ question.**","16f5f51c":"# 5. **Feature Engineering**","56fbf429":"# 2. **Data Cleaning**","ffbf13aa":"Now that we have engineered some features let's see which ones are more important, we will investigate for the XGB model","64d4a627":"# **6. Lets tune our paramaters!**"}}