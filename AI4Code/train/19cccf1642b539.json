{"cell_type":{"02b5c9ea":"code","316b4e83":"code","75109ca4":"code","14aa2086":"code","25d07950":"code","5310f55f":"code","80f6f926":"code","106b69a3":"code","fc24c861":"code","a73fc188":"code","3f4bab64":"code","cfdd6bd4":"code","69720001":"code","3427f534":"code","d92fcfcd":"code","30bf3e4b":"code","8e2bde57":"code","fadc4fa3":"code","068c02a2":"code","3d529fd8":"code","5255e479":"code","6346db89":"code","d66c6361":"code","61ec6f4b":"code","1c277db3":"code","9cb8bcd8":"code","284b9f98":"code","fcb2b615":"code","bdf67595":"code","68d017c1":"code","40bef4c4":"code","d38c488b":"code","c21cbdc2":"code","7a965663":"code","9a0d0372":"code","ea10bd5c":"code","b4bf9879":"code","6ec6d5d1":"code","664a64e5":"code","8cf8223f":"code","595c05a6":"code","4c4cc68a":"code","32ee8d7c":"code","7377225d":"code","74c1d2e0":"code","a1ee8196":"code","589fe4a9":"code","471bb2a8":"code","b13874dc":"code","86cf42fd":"code","28ce82e9":"code","4c7bcb98":"code","204af29a":"code","28a4f8b1":"code","11747302":"code","df19c416":"code","55bcdf98":"code","a3274dae":"code","cbf67a39":"code","2e950f9d":"code","f19bd1df":"code","f5e7ef66":"code","0d0434d9":"code","5d083532":"code","6259c4ba":"code","348bb008":"code","2bbeade8":"code","bbbf8fae":"code","71577842":"code","0e8ed3bc":"code","050a58b9":"code","bfe697cf":"code","b710e1f0":"code","f3e758d9":"code","b9058215":"code","5bc4090c":"code","d008e594":"code","86913f51":"code","0027ebb3":"code","54696c5d":"code","79b7a24f":"code","da3b33e5":"code","970edfe0":"code","0f715130":"code","1af301de":"code","ee726af8":"code","27d6147e":"code","8bd59f7b":"code","3140932c":"code","1d2d1ffc":"code","6da62f5a":"code","f3e39d86":"code","33b86d64":"code","e08f470d":"code","8cd1df89":"code","1985a4cd":"code","5e1a5ef7":"code","1a4f990e":"code","19e0e31a":"code","58d691de":"code","140f529f":"code","b20f731d":"code","2ff9240c":"code","f14eaa8a":"code","2fa57713":"code","8d0ea429":"code","c5acb416":"code","78e3e836":"code","c6fdb9fc":"code","a07a934a":"code","90563413":"code","38d63a07":"code","f0f1a355":"code","2f71ff09":"code","ab255324":"code","1a4b3998":"code","19e10e6d":"code","a8a25cc1":"code","d98b9cea":"code","eafc59d4":"code","bca96ae0":"code","b0584f09":"code","440def6f":"code","6e4c29b4":"code","35210961":"code","75bc8f5f":"code","be114907":"code","e087b62d":"code","9ba6b9a6":"code","e7ca6f83":"code","58d799b8":"code","c231add9":"code","e742dc40":"code","7dc81565":"code","30bfbcbd":"code","8969ea93":"code","75e7974d":"code","7ad6d5aa":"code","10343c9c":"code","e272e622":"code","a92b1874":"code","ad0559fc":"code","32982dc9":"code","acdfdf1d":"markdown","541a70c5":"markdown","349e7ee0":"markdown","94b20e6e":"markdown","c3d15205":"markdown","17e1f026":"markdown","bce89851":"markdown","78911d30":"markdown","7b3e9ec6":"markdown","3fd6f7dc":"markdown","bdec37b4":"markdown","788c113e":"markdown","61f9ff50":"markdown","3300b0bd":"markdown","4a526a53":"markdown","05aa1de4":"markdown","f09985e7":"markdown","58541c62":"markdown","6859283a":"markdown","a33ba36d":"markdown","3cd5589e":"markdown","cd7ba73f":"markdown","79398a64":"markdown","a4f8a0d1":"markdown","98c381a5":"markdown","bf80e770":"markdown","32b9bad2":"markdown","2ebcb24a":"markdown","8fc089b9":"markdown","2886222a":"markdown","2739eac8":"markdown","31d96be9":"markdown","57f23cdb":"markdown","bf9e78e5":"markdown","c256b53f":"markdown","5e0b6aac":"markdown","d75f13d4":"markdown","d1d16808":"markdown","7e83ac02":"markdown","b2c9e3f1":"markdown","8721b79d":"markdown","8de4c01c":"markdown","283ec8f4":"markdown","73c74740":"markdown","1ac9dcfb":"markdown","d041b8d2":"markdown","205685cf":"markdown","4d8086b9":"markdown","727f7903":"markdown","fb4e17f1":"markdown","045bc712":"markdown","f76eb947":"markdown","b9b39575":"markdown","607b98b9":"markdown","c0564609":"markdown","697820dc":"markdown","ef5e8fa9":"markdown","411001ed":"markdown","d6302fe5":"markdown","9130a676":"markdown","7056ebf9":"markdown","62a20849":"markdown","bf39ad4b":"markdown","fcb64b95":"markdown","d7130043":"markdown","d67d0f9b":"markdown","38451495":"markdown"},"source":{"02b5c9ea":"from IPython.display import Image\n","316b4e83":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Cover2.PNG\")","75109ca4":"\n!pip install bert-extractive-summarizer\n#!pip install nxviz","14aa2086":"!pip install -U sentence-transformers","25d07950":"import pandas as pd\nimport os\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport scipy\n\n\nimport textwrap\nimport json\nimport logging\nimport pickle\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk import tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer, word_tokenize\nfrom nltk.translate.bleu_score import sentence_bleu\n\nimport json\nimport glob\nimport string\n\n\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom IPython.core.display import display, HTML\nimport torch\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer, AutoModel\n#from sentence_transformers import models, SentenceTransformer\nimport shutil\n\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\nimport pandas as pd\n\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\n\n# Load the library with the CountVectorizer method\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom summarizer import Summarizer\nimport math\n\nimport os, stat\nimport ntpath\nfrom string import ascii_uppercase\n#import nxviz\n","5310f55f":"%matplotlib inline\nimport numpy as np\nimport math\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os, stat\nimport ntpath\nfrom string import ascii_uppercase","80f6f926":"import spacy\nfrom spacy.matcher import PhraseMatcher #import PhraseMatcher class\n","106b69a3":"import matplotlib\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","fc24c861":"# Specify the Kaggle Username and Key to use the Kaggle Api\n\n# os.environ['KAGGLE_USERNAME'] = '*************'\n# os.environ['KAGGLE_KEY'] = '****************'","a73fc188":"# from kaggle.api.kaggle_api_extended import KaggleApi\n\n# api = KaggleApi()\n# api.authenticate()\n\n# api.dataset_download_files(dataset=\"allen-institute-for-ai\/CORD-19-research-challenge\", path=DATA_PATH, unzip=True)\n\n# HTML('''<script>\n# code_show=true; \n# function code_toggle() {\n#  if (code_show){\n#  $('div.input').hide();\n#  } else {\n#  $('div.input').show();\n#  }\n#  code_show = !code_show\n# } \n# $( document ).ready(code_toggle);\n# <\/script>\n# The raw code for this IPython notebook is by default hidden for easier reading.\n# To toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","3f4bab64":"DATA_PATH = os.getcwd()+'\/kaggle\/input\/CORD-19-research-challenge\/'","cfdd6bd4":"#Get data from path '\/data1\/cov19\/kaggle_data_0331\/'\n'''\nbio_path = '\/kaggle\/input\/CORD-19-research-challenge\/\/biorxiv_medrxiv\/'\ncomm_path = '\/kaggle\/input\/CORD-19-research-challenge\/\/comm_use_subset\/'\nnon_comm_path = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/'\ncustom_path = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/'\njournals = {\"BIORXIV_MEDRXIV\": bio_path,\n             \"COMMON_USE_SUB\" : comm_path,\n             \"NON_COMMON_USE_SUB\" : non_comm_path,\n             \"CUSTOM_LICENSE\" : custom_path}\n'''","69720001":"'''\ndef parse_each_json_file(file_path,journal):\n    inp = None\n    with open(file_path) as f:\n        inp = json.load(f)\n    rec = {}\n    rec['sha'] = inp['paper_id'] or None\n    rec['title'] = inp['metadata']['title'] or None\n    abstract = \"\\n \".join([inp['abstract'][_]['text'] for _ in range(len(inp['abstract']) - 1)]) \n    rec['abstract'] = abstract or None\n    full_text = []\n    for _ in range(len(inp['body_text'])):\n        try:\n            full_text.append(inp['body_text'][_]['text'])\n        except:\n            pass\n\n    rec['full_text'] = \"\\n \".join(full_text) or None\n    rec['source'] =  journal     or None    \n    return rec\n\ndef parse_json_and_create_csv(journals):\n    journal_dfs = []\n    for journal, path in journals.items():\n        parsed_rcds = []  \n        json_files = glob.glob('{}\/**\/*.json'.format(path), recursive=True)\n        for file_name in json_files:\n            rec = parse_each_json_file(file_name,journal)\n            parsed_rcds.append(rec)\n        df = pd.DataFrame(parsed_rcds)\n        journal_dfs.append(df)\n    return pd.concat(journal_dfs)\n\n\nfin_df = parse_json_and_create_csv(journals=journals)\nfin_df.head()\n\n'''","3427f534":"fin_df = pd.read_csv('\/kaggle\/input\/task3-data\/fin_df.csv')","d92fcfcd":"fin_df.head()","30bf3e4b":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/SequenceConversion.PNG\")","8e2bde57":"###-------------------------------------------------------------\n### READING AND ASSEMBLING GENOME SEQUENCE FILE\n###-------------------------------------------------------------\n#File 1 to compare\nfname = open(\"\/kaggle\/input\/task3-data\/gnome_data_countries\/gnome_data_countries\/CHINA_SHENZHEN_MN938384.1.fasta\",\"r\")\nA = fname.read().replace('\\n','')\nfname.close()\nsequence1 = (list(A))\n\n#File2 to compare\nfname2 = open(\"\/kaggle\/input\/task3-data\/gnome_data_countries\/gnome_data_countries\/italy_MT066156.1.fasta\",\"r\")\nB = fname2.read().replace('\\n','')\nfname.close()\nsequence2 = (list(B))","fadc4fa3":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/SignalSegmentation.PNG\")","068c02a2":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/SC1.PNG\")","3d529fd8":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/SC2.PNG\")","5255e479":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/SC3.PNG\")","6346db89":"###-------------------------------------------------------------\n### corrERSION OF SEQUENCE INTO A TIME DOMAIN RANDOM COMPLEX SIGNAL\n###-------------------------------------------------------------\nfrom string import ascii_uppercase\n\nseq_len0 = len(sequence1)\nseq_len1 = len(sequence2)\n    \nsignal0 = np.zeros(seq_len0, dtype=np.complex_)\nsignal1 = np.zeros(seq_len1, dtype=np.complex_)\n    \n    \nmax_seq_len=max(seq_len0,seq_len1);\n### 26 is total number of alphabets\nL = 26\n\n\nRANDOM_VEC = (1\/np.sqrt(2))*((np.random.randn(1,L))+1j*(np.random.randn(1,L)))\n\nprob0 = np.zeros(L)\nprob1 = np.zeros(L)\n\nfor i in ascii_uppercase:\n    prob0[ord(i)-65] =  sequence1.count(i)\n    prob1[ord(i)-65] =  sequence2.count(i)\n  \n       \nfor i in range (seq_len0):\n    signal0[i] = RANDOM_VEC[0][ord(sequence1[i])-65]\n    #print('signal0[', i, ']=', signal0[i], '\\n')\n        \nfor i in range (seq_len1):\n    signal1[i] = RANDOM_VEC[0][ord(sequence2[i])-65]","d66c6361":"###------------------------------------------------\n###TIME DOMAIN SIGNAL SEGMENTATION - FFT SIZE\n###------------------------------------------------\nfft_size = 1024;\nimport math\nrows = math.ceil(max_seq_len\/fft_size)\ncols = fft_size\n\n###print(\"seq_len=\", seq_len, \" rows=\", rows, \" columns=\", cols)\n\nseq_arr0=np.zeros((rows,cols), dtype=np.complex_)\nseq_arr1=np.zeros((rows,cols), dtype=np.complex_)\n\n\nfor row_pointer in range (rows):\n    start_index = fft_size*(row_pointer)\n    end_index = fft_size*(row_pointer+1)\n    if(end_index <= len(sequence1)):\n        seq_arr0[row_pointer,:] = signal0[start_index:end_index]\n    else:\n        if (start_index >= len(sequence1)):\n            ###fill all 0\n            seq_arr0[row_pointer,:] = np.zeros(1,fft_size);\n                \n        else:\n            ###copy till seq_len and then all 0s\n            ###print('row pointer:', row_pointer, ' start:',start_index, ' end:',end_index, ' seq_len:', seq_len0)\n            seq_arr0[row_pointer,0:(seq_len0-start_index)] = signal0[start_index:seq_len0]\n            seq_arr0[row_pointer,(seq_len0-start_index):] = np.zeros((1,end_index-seq_len0), dtype=np.complex_)\n                \nfor row_pointer in range (rows):\n    start_index = fft_size*(row_pointer)\n    end_index = fft_size*(row_pointer+1)\n    if(end_index <= len(sequence2)):\n        seq_arr1[row_pointer,:] = signal1[start_index:end_index]\n    else:\n        if (start_index >= len(sequence2)):\n            ###fill all 0\n            seq_arr1[row_pointer,:] = np.zeros(1,fft_size);\n                \n        else:\n            ###copy till seq_len and then all 0s\n            seq_arr1[row_pointer,0:(seq_len1-start_index)] = signal1[start_index:seq_len1]\n            seq_arr1[row_pointer,(seq_len1-start_index):] = np.zeros((1,end_index-seq_len1), dtype=np.complex_)\n","61ec6f4b":"###------------------------------------------------\n### SIGNAL IN FREQ DOMAIN  -- CHECK\n###------------------------------------------------\n\nSEQ_ARR0=np.zeros((rows,cols), dtype=np.complex_)\nSEQ_ARR1=np.zeros((rows,cols), dtype=np.complex_)\nfor j in range (rows):\n    SEQ_ARR0[j,:]=np.fft.fft(seq_arr0[j,:])\n    SEQ_ARR1[j,:]=np.fft.fft(seq_arr1[j,:])\n","1c277db3":"###------------------------------------------------\n###SIGNAL CORRELATION IN FREQ DOMAIN\n###------------------------------------------------\nnrof_segments=rows\n    \nCORR_CELL=np.zeros((nrof_segments,2,2,cols), dtype=np.complex_)\n    \nfor m in range (nrof_segments):\n    CORR_CELL[m, 0, 0, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR0[m,:])) )\n    CORR_CELL[m, 0, 1, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR1[m,:])) )\n    CORR_CELL[m, 1, 0, :]= np.multiply( (SEQ_ARR1[m,:]),(np.conj(SEQ_ARR0[m,:])) )\n    CORR_CELL[m, 1, 1, :]= np.multiply( (SEQ_ARR1[m,:]),(np.conj(SEQ_ARR1[m,:])) )","9cb8bcd8":"###------------------------------------------------\n### BACK TO TIME DOMAIN - correlation of piece wise time domain signal\n###------------------------------------------------\n\ncorr_cell=np.zeros((nrof_segments,2,2,cols), dtype=np.complex_)\ncoef0 = np.zeros(nrof_segments);\ncoef1 = np.zeros(nrof_segments);\nfor m in range (nrof_segments):\n    for i in range (2):\n        for j in range(2):\n            corr_cell[m, i, j, :]= np.fft.ifft(CORR_CELL[m,i,j])\n    coef0[m]=np.absolute(corr_cell[m, 0, 0, 0])\n    coef1[m]=np.absolute(corr_cell[m, 1, 0, 0])","284b9f98":"###------------------------------------------------\n###PLOT DATA\n###------------------------------------------------\nimport matplotlib.pyplot as plt\nmetric1_arr = np.divide(coef0,coef1)\nmetric1 = np.sum(np.absolute(metric1_arr) - 1)\nplt.rcParams['figure.figsize'] = [15,10]\nfig, axs = plt.subplots(2,1, constrained_layout=True)\n\nfig = plt.figure()\naxs[0].plot(range(nrof_segments), metric1_arr, 'cs')\naxs[0].set_title('metric1 : coef1\/coef2 - 1 : '+str(metric1));\naxs[0].set_xlabel('number of segments');\naxs[0].set_ylabel('C1\/C2-1');\naxs[0].grid()\nfig.suptitle('COVID19 GENOME Variation Plot', fontsize=16)\n    \nmetric2_arr = np.subtract(coef0,coef1)\nmetric2 = np.sum(np.absolute(metric2_arr))\naxs[1].plot(range(nrof_segments), metric2_arr, 'gs')\naxs[1].set_title('metric2 : coef1-coef2 : '+str(metric2));\naxs[1].set_xlabel('number of segments');\naxs[1].set_ylabel('C1-C2');\naxs[1].grid()\n","fcb2b615":"files = []\n\n#currdir = os.getcwd()\n#path = currdir + '\/' + 'gnome_data'\n\npath = '\/kaggle\/input\/task3-data\/gnome_data_countries\/gnome_data_countries'\n# r=root, d=directories, f = files\nfor r, d, f in os.walk(path):\n    for file in f:\n        if '.txt' in file or '.fasta' in file:\n            files.append(os.path.join(r, file))\n\ng_fname = files    ","bdf67595":"for f in g_fname:\n    print(f)","68d017c1":"out_file_path = os.getcwd()","40bef4c4":"def run_corr_2files(num1, num2):\n    \n    \n    ###-------------------------------------------------------------\n    ### corrERSION OF SEQUENCE INTO A TIME DOMAIN RANDOM COMPLEX SIGNAL\n    ###-------------------------------------------------------------\n    \n    seq_len0 = len(sequence[num1])\n    seq_len1 = len(sequence[num2])\n    \n    \n    ###max_seq_len=max(seq_len0,seq_len1);\n    min_seq_len = min(seq_len0,seq_len1)\n    ### 26 is total number of alphabets\n    L = 26\n    \n    B=100\n    metric_samples=np.zeros(B)\n    for b in range(B):\n        \n        signal0 = np.zeros(seq_len0, dtype=np.complex_)\n        signal1 = np.zeros(seq_len1, dtype=np.complex_)\n    \n        RANDOM_VEC = (1\/np.sqrt(2))*((np.random.randn(1,L))+1j*(np.random.randn(1,L)))\n        \n        prob0 = np.zeros(L)\n        prob1 = np.zeros(L)\n        \n        for i in ascii_uppercase:\n            prob0[ord(i)-65] =  sequence[num1].count(i)\n            prob1[ord(i)-65] =  sequence[num2].count(i)\n         \n          \n        for i in range (seq_len0):\n            signal0[i] = RANDOM_VEC[0][ord(sequence[num1][i])-65]\n            #print('signal0[', i, ']=', signal0[i], '\\n')\n            \n            \n        for i in range (seq_len1):\n            signal1[i] = RANDOM_VEC[0][ord(sequence[num2][i])-65]\n        \n        ###------------------------------------------------\n        ###TIME DOMAIN SIGNAL SEGMENTATION - FFT SIZE\n        ###------------------------------------------------\n        fft_size = 1024\n    \n        rows = math.ceil(min_seq_len\/fft_size)\n        cols = fft_size\n        ###print(\"seq_len=\", seq_len, \" rows=\", rows, \" columns=\", cols)\n        \n        seq_arr0=np.zeros((rows,cols), dtype=np.complex_)\n        seq_arr1=np.zeros((rows,cols), dtype=np.complex_)\n        \n        for row_pointer in range (rows):\n            start_index = fft_size*(row_pointer)\n            end_index = fft_size*(row_pointer+1)\n            if(end_index <= len(sequence[num1])):\n                seq_arr0[row_pointer,:] = signal0[start_index:end_index]\n            else:\n                if (start_index >= len(sequence[num1])):\n                    ###fill all 0\n                    seq_arr0[row_pointer,:] = np.zeros((1,fft_size), dtype=np.complex_);\n                    \n                else:\n                    ###copy till seq_len and then all 0s\n                    ###print('row pointer:', row_pointer, ' start:',start_index, ' end:',end_index, ' seq_len:', seq_len0)\n                    seq_arr0[row_pointer,0:(seq_len0-start_index)] = signal0[start_index:seq_len0]\n                    seq_arr0[row_pointer,(seq_len0-start_index):] = np.zeros((1,end_index-seq_len0), dtype=np.complex_)\n                    \n        for row_pointer in range (rows):\n            start_index = fft_size*(row_pointer)\n            end_index = fft_size*(row_pointer+1)\n            if(end_index <= len(sequence[num2])):\n                seq_arr1[row_pointer,:] = signal1[start_index:end_index]\n            else:\n                if (start_index >= len(sequence[num2])):\n                    ###fill all 0\n                    seq_arr1[row_pointer,:] = np.zeros((1,fft_size), dtype=np.complex_);\n                    \n                else:\n                    ###copy till seq_len and then all 0s\n                    seq_arr1[row_pointer,0:(seq_len1-start_index)] = signal1[start_index:seq_len1]\n                    seq_arr1[row_pointer,(seq_len1-start_index):] = np.zeros((1,end_index-seq_len1), dtype=np.complex_)\n                              \n    \n        ###------------------------------------------------\n        ###SIGNAL IN FREQ DOMAIN  -- CHECK\n        ###------------------------------------------------\n        \n        SEQ_ARR0=np.zeros((rows,cols), dtype=np.complex_)\n        SEQ_ARR1=np.zeros((rows,cols), dtype=np.complex_)\n        for j in range (rows):\n            SEQ_ARR0[j,:]=np.fft.fft(seq_arr0[j,:])\n            SEQ_ARR1[j,:]=np.fft.fft(seq_arr1[j,:])\n        \n        ###------------------------------------------------\n        ###SIGNAL CORRELATION IN FREQ DOMAIN\n        ###------------------------------------------------\n        nrof_segments=rows\n        \n        CORR_CELL=np.zeros((nrof_segments,1,2,cols), dtype=np.complex_)\n        \n        for m in range (nrof_segments):\n            CORR_CELL[m, 0, 0, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR0[m,:])) )\n            CORR_CELL[m, 0, 1, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR1[m,:])) )\n                   \n        \n        ###------------------------------------------------\n        ###BACK TO TIME DOMAIN - correlation of piece wise time domain signal\n        ###------------------------------------------------\n        \n        corr_cell=np.zeros((nrof_segments,1,2,cols), dtype=np.complex_)\n        coef0 = np.zeros(nrof_segments);\n        coef1 = np.zeros(nrof_segments);\n        for m in range (nrof_segments):\n            corr_cell[m, 0, 0, :]= np.fft.ifft(CORR_CELL[m,0,0])\n            corr_cell[m, 0, 1, :]= np.fft.ifft(CORR_CELL[m,0,1])\n            coef0[m]=max(np.absolute(corr_cell[m, 0, 0]))\n            coef1[m]=max(np.absolute(corr_cell[m, 0, 1]))\n            \n        coef = np.divide(coef0,coef1) - 1\n        \n        metric_samples[b] = np.sum(np.absolute(coef))\n        \n              \n    np.sort(metric_samples)\n    #print('sorted: ', metric_samples,'\\n')\n    return(metric_samples[89])","d38c488b":"#print(g_fname)\n    \nsequence = [] \ncheck_var1 = 0\n\n\ni = 0\n\nfor j in range (len(g_fname)):\n    A=''\n    with open(g_fname[j]) as fhandler:\n        first_line = fhandler.readline()\n        if (first_line.find('genome') != -1):\n            ###print(first_line)\n            print(\"Discarding first line of:\", g_fname[j],'\\n')\n            ##messagebox.showwarning('Warning', 'Discarding first line: '+first_line)\n        else :\n            A = first_line\n\n        for line in fhandler:\n            A += line   \n\n        A = A.replace('\\n','') \n\n        sequence.append([])\n        #####-----------------------------\n        #####DNA to Amino\n        #####-----------------------------          \n        if(check_var1 == 1):\n            #####-----------------------------\n            #####DNA to RNA\n            #####-----------------------------\n            A = A.replace('T','U') \n\n            ####------------------------------\n            #### RNA to amino Acid\n            ####------------------------------                \n            key = ''\n            for k in range(0,3*math.floor(len(A)\/3),3):\n                key = A[k]+A[k+1]+A[k+2]\n                ##print(\"here:\",k, A[k],A[k+1],A[k+2],'\\n')\n                if(rna_to_amino.get(key) == None):\n                    print('Data Corrupted for file...abanding:\\n', g_fname[j])\n                    messagebox.showerror('Error', 'Data corrupted for file: '+g_fname[j]+' => expected only A,C,G and T in the DNA sequence')\n\n\n                if(rna_to_amino[key] != '*'):\n                    sequence[i].append(rna_to_amino[key])\n        else :\n            sequence[i] = list(A)\n\n    i = i+1","c21cbdc2":"global out_file_path\nnrof_samples = len(g_fname)\nnrof_calc_metrics = int((nrof_samples*(nrof_samples-1))\/2)\n\nmetric_arr = np.zeros(nrof_calc_metrics)\nsample_loc_arr = np.chararray((nrof_calc_metrics,1,2), itemsize=100, unicode=True)\nsample_number_arr = np.zeros((nrof_calc_metrics,3))\nrecord_arr = np.chararray((nrof_calc_metrics), itemsize=100, unicode=True)\nloc_arr = np.chararray((nrof_calc_metrics), itemsize=100, unicode=True)\n\nk=0\nfor i in range(nrof_samples):\n    for j in range((i+1),nrof_samples):\n        if(len(sequence[i]) > len(sequence[j])):\n            metric_arr[k] = run_corr_2files(i, j)\n        else :\n            metric_arr[k] = run_corr_2files(j, i)\n\n        sample_loc_arr[k,0,0] = ntpath.basename(g_fname[i]).replace('.fasta','')\n        sample_loc_arr[k,0,1] = ntpath.basename(g_fname[j]).replace('.fasta','')\n        record_arr[k] = sample_loc_arr[k,0,0]+', '+sample_loc_arr[k,0,1]+' = '+str(metric_arr[k]) \n        loc_arr[k] = sample_loc_arr[k,0,0]+', '+sample_loc_arr[k,0,1]\n        print(record_arr[k]+'\\n')\n        sample_number_arr[k,0] = i;\n        sample_number_arr[k,1] = j;\n        sample_number_arr[k,2] = metric_arr[k];\n        k = k+1\n\n#-----------\n#OUtput file\n#-----------\nsort_metric_arr = np.sort(metric_arr, axis=0)\nindex = np.argsort(metric_arr, axis=0)\nsort_record_arr = []\nsort_loc_arr = []\nfor i in index:\n    sort_record_arr.append(record_arr[i])\n    sort_loc_arr.append(loc_arr[i])\n\n","7a965663":"#remove previous created output folder\n#!rmdir 'Outpur'\n!mkdir 'Output'","9a0d0372":"filename_output= 'Output\/output_metrics1.csv'\nfilename_output_raw= 'Output\/output_raw_data1.csv'\nfilename_output_loc= 'Output\/output_loc1.txt'\nfilename_output_sample_number= 'Output\/\/sample_number1.txt'\n\nfileID = open(filename_output,'w');\nfileID2 = open(filename_output_raw,'w');\nfileID3 = open(filename_output_loc,'w');\nfileID4 = open(filename_output_sample_number,'w');\nfor k in range(nrof_calc_metrics):    \n    fileID.write(str(sort_metric_arr[k])+'\\n')  \n    fileID2.write(sort_record_arr[k]+'\\n')   \n    fileID3.write(sort_loc_arr[k]+'\\n')\n    fileID4.write(str(sample_number_arr[k,:])+'\\n')\n\nfileID.close()\nfileID2.close()\nfileID3.close()\nfileID4.close()","ea10bd5c":"filename_output","b4bf9879":"filename_output_raw","6ec6d5d1":"filename_output_loc","664a64e5":"filename_output_sample_number","8cf8223f":"files = []\n\npath = 'Output\/'\n\nfor r, d, f in os.walk(out_file_path):\n    for file in f:\n        print(file)","595c05a6":"files = []\n\n#currdir = os.getcwd()\n#path = currdir + '\/' + 'gnome_data'\n\npath = '.\/' + 'gnome_data_time'\n# r=root, d=directories, f = files\nfor r, d, f in os.walk(path):\n    for file in f:\n        if '.txt' in file or '.fasta' in file:\n            files.append(os.path.join(r, file))\n\ng_fname = files    ","4c4cc68a":"for f in g_fname:\n    print(f)","32ee8d7c":"# Output File Location\nout_file_path = 'Output\/'","7377225d":"def run_corr_2files(num1, num2):\n    \n    \n    ###-------------------------------------------------------------\n    ### corrERSION OF SEQUENCE INTO A TIME DOMAIN RANDOM COMPLEX SIGNAL\n    ###-------------------------------------------------------------\n    \n    seq_len0 = len(sequence[num1])\n    seq_len1 = len(sequence[num2])\n    \n    \n    ###max_seq_len=max(seq_len0,seq_len1);\n    min_seq_len = min(seq_len0,seq_len1)\n    ### 26 is total number of alphabets\n    L = 26\n    \n    B=100\n    metric_samples=np.zeros(B)\n    for b in range(B):\n        \n        signal0 = np.zeros(seq_len0, dtype=np.complex_)\n        signal1 = np.zeros(seq_len1, dtype=np.complex_)\n    \n        RANDOM_VEC = (1\/np.sqrt(2))*((np.random.randn(1,L))+1j*(np.random.randn(1,L)))\n        \n        prob0 = np.zeros(L)\n        prob1 = np.zeros(L)\n        \n        for i in ascii_uppercase:\n            prob0[ord(i)-65] =  sequence[num1].count(i)\n            prob1[ord(i)-65] =  sequence[num2].count(i)\n         \n          \n        for i in range (seq_len0):\n            signal0[i] = RANDOM_VEC[0][ord(sequence[num1][i])-65]\n            #print('signal0[', i, ']=', signal0[i], '\\n')\n            \n            \n        for i in range (seq_len1):\n            signal1[i] = RANDOM_VEC[0][ord(sequence[num2][i])-65]\n        \n        ###------------------------------------------------\n        ###TIME DOMAIN SIGNAL SEGMENTATION - FFT SIZE\n        ###------------------------------------------------\n        fft_size = 1024\n    \n        rows = math.ceil(min_seq_len\/fft_size)\n        cols = fft_size\n        ###print(\"seq_len=\", seq_len, \" rows=\", rows, \" columns=\", cols)\n        \n        seq_arr0=np.zeros((rows,cols), dtype=np.complex_)\n        seq_arr1=np.zeros((rows,cols), dtype=np.complex_)\n        \n        for row_pointer in range (rows):\n            start_index = fft_size*(row_pointer)\n            end_index = fft_size*(row_pointer+1)\n            if(end_index <= len(sequence[num1])):\n                seq_arr0[row_pointer,:] = signal0[start_index:end_index]\n            else:\n                if (start_index >= len(sequence[num1])):\n                    ###fill all 0\n                    seq_arr0[row_pointer,:] = np.zeros((1,fft_size), dtype=np.complex_);\n                    \n                else:\n                    ###copy till seq_len and then all 0s\n                    ###print('row pointer:', row_pointer, ' start:',start_index, ' end:',end_index, ' seq_len:', seq_len0)\n                    seq_arr0[row_pointer,0:(seq_len0-start_index)] = signal0[start_index:seq_len0]\n                    seq_arr0[row_pointer,(seq_len0-start_index):] = np.zeros((1,end_index-seq_len0), dtype=np.complex_)\n                    \n        for row_pointer in range (rows):\n            start_index = fft_size*(row_pointer)\n            end_index = fft_size*(row_pointer+1)\n            if(end_index <= len(sequence[num2])):\n                seq_arr1[row_pointer,:] = signal1[start_index:end_index]\n            else:\n                if (start_index >= len(sequence[num2])):\n                    ###fill all 0\n                    seq_arr1[row_pointer,:] = np.zeros((1,fft_size), dtype=np.complex_);\n                    \n                else:\n                    ###copy till seq_len and then all 0s\n                    seq_arr1[row_pointer,0:(seq_len1-start_index)] = signal1[start_index:seq_len1]\n                    seq_arr1[row_pointer,(seq_len1-start_index):] = np.zeros((1,end_index-seq_len1), dtype=np.complex_)\n                              \n    \n        ###------------------------------------------------\n        ###SIGNAL IN FREQ DOMAIN  -- CHECK\n        ###------------------------------------------------\n        \n        SEQ_ARR0=np.zeros((rows,cols), dtype=np.complex_)\n        SEQ_ARR1=np.zeros((rows,cols), dtype=np.complex_)\n        for j in range (rows):\n            SEQ_ARR0[j,:]=np.fft.fft(seq_arr0[j,:])\n            SEQ_ARR1[j,:]=np.fft.fft(seq_arr1[j,:])\n        \n        ###------------------------------------------------\n        ###SIGNAL CORRELATION IN FREQ DOMAIN\n        ###------------------------------------------------\n        nrof_segments=rows\n        \n        CORR_CELL=np.zeros((nrof_segments,1,2,cols), dtype=np.complex_)\n        \n        for m in range (nrof_segments):\n            CORR_CELL[m, 0, 0, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR0[m,:])) )\n            CORR_CELL[m, 0, 1, :]= np.multiply( (SEQ_ARR0[m,:]),(np.conj(SEQ_ARR1[m,:])) )\n                   \n        \n        ###------------------------------------------------\n        ###BACK TO TIME DOMAIN - correlation of piece wise time domain signal\n        ###------------------------------------------------\n        \n        corr_cell=np.zeros((nrof_segments,1,2,cols), dtype=np.complex_)\n        coef0 = np.zeros(nrof_segments);\n        coef1 = np.zeros(nrof_segments);\n        for m in range (nrof_segments):\n            corr_cell[m, 0, 0, :]= np.fft.ifft(CORR_CELL[m,0,0])\n            corr_cell[m, 0, 1, :]= np.fft.ifft(CORR_CELL[m,0,1])\n            coef0[m]=max(np.absolute(corr_cell[m, 0, 0]))\n            coef1[m]=max(np.absolute(corr_cell[m, 0, 1]))\n            \n        coef = np.divide(coef0,coef1) - 1\n        \n        metric_samples[b] = np.sum(np.absolute(coef))\n        \n              \n    np.sort(metric_samples)\n    #print('sorted: ', metric_samples,'\\n')\n    return(metric_samples[89])","74c1d2e0":"sequence = [] \ncheck_var1 = 0\ni = 0\n\nfor j in range (len(g_fname)):\n    A=''\n    with open(g_fname[j]) as fhandler:\n        first_line = fhandler.readline()\n        if (first_line.find('genome') != -1):\n            ###print(first_line)\n            print(\"Discarding first line of:\", g_fname[j],'\\n')\n            messagebox.showwarning('Warning', 'Discarding first line: '+first_line)\n        else :\n            A = first_line\n\n        for line in fhandler:\n            A += line   \n\n        A = A.replace('\\n','') \n\n        if ((A.count('A')+A.count('T')+A.count('C')+A.count('G')) != len(A)):\n            print ('Error :Data corrupted for file: '+g_fname[j]+' => expected only A,C,G and T in the DNA sequence')\n            ##root.destroy()\n            ##return()\n\n        sequence.append([])\n        #####-----------------------------\n        #####DNA to Amino\n        #####-----------------------------          \n        if(check_var1 == 1):\n            #####-----------------------------\n            #####DNA to RNA\n            #####-----------------------------\n            A = A.replace('T','U') \n\n            ####------------------------------\n            #### RNA to amino Acid\n            ####------------------------------                \n            key = ''\n            for k in range(0,3*math.floor(len(A)\/3),3):\n                key = A[k]+A[k+1]+A[k+2]\n                ##print(\"here:\",k, A[k],A[k+1],A[k+2],'\\n')\n                if(rna_to_amino.get(key) == None):\n                    print('Data Corrupted for file...abanding:\\n', g_fname[j])\n                    #messagebox.showerror('Error', 'Data corrupted for file: '+g_fname[j]+' => expected only A,C,G and T in the DNA sequence')\n                    ##root.destroy()\n                    ##return()\n\n                if(rna_to_amino[key] != '*'):\n                    sequence[i].append(rna_to_amino[key])\n        else :\n            sequence[i] = list(A)\n\n    i = i+1","a1ee8196":"global out_file_path\nnrof_samples = len(g_fname)\nnrof_calc_metrics = int((nrof_samples*(nrof_samples-1))\/2)\n\nmetric_arr = np.zeros(nrof_calc_metrics)\nsample_loc_arr = np.chararray((nrof_calc_metrics,1,2), itemsize=100, unicode=True)\nsample_number_arr = np.zeros((nrof_calc_metrics,3))\nrecord_arr = np.chararray((nrof_calc_metrics), itemsize=100, unicode=True)\nloc_arr = np.chararray((nrof_calc_metrics), itemsize=100, unicode=True)\n\nk=0\nfor i in range(nrof_samples):\n    for j in range((i+1),nrof_samples):\n        if(len(sequence[i]) > len(sequence[j])):\n            metric_arr[k] = run_corr_2files(i, j)\n        else :\n            metric_arr[k] = run_corr_2files(j, i)\n\n        sample_loc_arr[k,0,0] = ntpath.basename(g_fname[i]).replace('.fasta','')\n        sample_loc_arr[k,0,1] = ntpath.basename(g_fname[j]).replace('.fasta','')\n        record_arr[k] = sample_loc_arr[k,0,0]+', '+sample_loc_arr[k,0,1]+' = '+str(metric_arr[k]) \n        loc_arr[k] = sample_loc_arr[k,0,0]+', '+sample_loc_arr[k,0,1]\n        print(record_arr[k]+'\\n')\n        sample_number_arr[k,0] = i;\n        sample_number_arr[k,1] = j;\n        sample_number_arr[k,2] = metric_arr[k];\n        k = k+1\n\n#-----------\n#OUtput file\n#-----------\nsort_metric_arr = np.sort(metric_arr, axis=0)\nindex = np.argsort(metric_arr, axis=0)\nsort_record_arr = []\nsort_loc_arr = []\nfor i in index:\n    sort_record_arr.append(record_arr[i])\n    sort_loc_arr.append(loc_arr[i])\n\nfilename_output= out_file_path+'\/output_metrics1.csv'\nfilename_output_raw= out_file_path+'\/output_raw_data1.csv'\nfilename_output_loc= out_file_path+'\/output_loc1.txt'\nfilename_output_sample_number= out_file_path+'\/sample_number1.txt'\n","589fe4a9":"filename_output","471bb2a8":"fileID = open(filename_output,'w');\nfileID2 = open(filename_output_raw,'w');\nfileID3 = open(filename_output_loc,'w');\nfileID4 = open(filename_output_sample_number,'w');\nfor k in range(nrof_calc_metrics):    \n    fileID.write(str(sort_metric_arr[k])+'\\n')  \n    fileID2.write(sort_record_arr[k]+'\\n')   \n    fileID3.write(sort_loc_arr[k]+'\\n')\n    fileID4.write(str(sample_number_arr[k,:])+'\\n')\n\nfileID.close()\nfileID2.close()\nfileID3.close()\nfileID4.close()","b13874dc":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Res1.PNG\")","86cf42fd":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Res2.PNG\")","28ce82e9":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Res3.PNG\")","4c7bcb98":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Res4.PNG\")","204af29a":"Image(\"\/kaggle\/input\/task3-data\/Images\/Images\/Res5.PNG\")","28a4f8b1":"def wordcloud_draw(text, color = 'white'):\n    \"\"\"\n    Plots wordcloud of string text after removing stopwords\n    \"\"\"\n    cleaned_word = \" \".join([word for word in text.split()])\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color=color,\n                      width=1000,\n                      height=1000\n                     ).generate(cleaned_word)\n    plt.figure(1,figsize=(15, 15))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    display(plt.show())","11747302":"animals = []\nanimalList = []\nwith open('\/kaggle\/input\/task3-data\/animals.txt', \"r\") as f:\n    animals = f.readlines()\nanimalList = [s.replace('\\n', '') for s in animals]\nanimalList.append('pangolin')\nanimalList.append('mice')\nanimalList.append('animal')\nanimalList = [string for string in animalList if string != \"\"]\nanimalList = list(map(lambda x:x.lower(), animalList))\nanimalList.remove('human')\nanimalList.remove('discus')\npluralList = ['{0}s'.format(elem) for elem in animalList]\nanimalList = animalList + pluralList\n#animalList = [' {0} '.format(elem) for elem in animalList]\nanimalList[0:5]","df19c416":"covid19_list = []\nwith open('\/kaggle\/input\/task3-data\/covid19.txt', \"r\") as f:\n    words = f.readlines()\ncovid19_list = [s.replace('\\n', '') for s in words]","55bcdf98":"covid19_list[0:5]","a3274dae":"#Load Meta data\nmetadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nmetadata.head()","cbf67a39":"#### Merging Articles with Meta data. Loading data from already merged csv\n\nLeft_join = pd.merge(fin_df,  \n                     metadata,  \n                     on ='sha',  \n                     how ='left') \nLeft_join = Left_join.drop(columns=['cord_uid', 'doi', 'title_y', 'pmcid', 'abstract_y', 'Microsoft Academic Paper ID', 'WHO #Covidence',\n                       'full_text_file', 'url', 'pubmed_id'])\nLeft_join.head()","2e950f9d":"Left_join['title_x'].fillna(\"NoTitle\", inplace = True)\nLeft_join['abstract_x'].fillna(\"NoAbstract\", inplace = True)\nLeft_join['full_text'].fillna(\"NoText\", inplace = True)\nLeft_join['combined'] = Left_join['title_x'] + ' ' + Left_join['abstract_x'] + ' ' + Left_join['full_text']\n","f19bd1df":"Left_join.head()","f5e7ef66":"cond2 = Left_join['abstract_x'].str.contains('livestock')\nprint(sum(cond2))","0d0434d9":"abstract_livestock = Left_join[cond2]\nabstract_livestock.shape\nabstract_livestock.head(5)","5d083532":"cond3 = abstract_livestock['abstract_x'].str.contains('farmer')\nabstract_livestock_farmer = abstract_livestock[cond3]\nabstract_livestock_farmer.head(5)","6259c4ba":"def clean_text(article):\n    clean1 = re.sub(r'['+string.punctuation + '\u2019\u2014\u201d'+']', \"\", article.lower())\n    return re.sub(r'\\W+', ' ', clean1)","348bb008":"import re\n\n#Left_join['tokenized'] = Left_join['combined'].map(lambda x: clean_text(x))","2bbeade8":"\nimport nltk\n'''\nw_tokenizer = nltk.tokenize.WhitespaceTokenizer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\ndef lemmatize_text(text):\n    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\nLeft_join['tokenized'] = Left_join['tokenized'].apply(lemmatize_text)\n'''","bbbf8fae":"## If we need space\n#del Left_join\n","71577842":"def find_spillover_wds(content, wds = ['transfer','spillover','pass on','transmit','contract',\n                                       'distribute','progress','incubate','spread','disseminate','zoonosis','zoonotic']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","0e8ed3bc":"def find_human_wds(content, wds = ['human','people','man','child','kid']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","050a58b9":"def find_covid_wds(content, wds = ['covid-19','covid','cov','coronavirus','corona']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","bfe697cf":"def find_evidence_wds(content, wds = ['domestic animal', 'backyard*livestock', 'wet markets', 'meat markets','seafood markets', 'bites', 'bitten', \n                                      'laboratory*accident', 'fairs', 'petting*zoo',  'trading*wild animal', \n                                      'destruction*habitat', 'wild animal*food', 'lifestock pathogens','genetic mutations',\n                                      'animal testing', ' hunting ', 'industrial*farming', ' pet ', 'butcher', ' eat ', ' meat ']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ','\n    return found","b710e1f0":"'''\nLeft_join['evidence_wds'] = Left_join['combined'].apply(find_evidence_wds)\nLeft_join.evidence_wds.unique()\n'''","f3e758d9":"'''\nLeft_join['evidence_wds'].replace('', np.nan, inplace=True)\n\n\nLeft_join['animal_wds'] = Left_join['tokenized'].apply(find_animal_wds)\nLeft_join['spillover_wds'] = Left_join['tokenized'].apply(find_spillover_wds)\nLeft_join['human_wds'] = Left_join['tokenized'].apply(find_human_wds)\nLeft_join['virus_wds'] = Left_join['tokenized'].apply(find_covid_wds)\nLeft_join['evidence_wds'] = Left_join['combined'].apply(find_evidence_wds)\n\nLeft_join['animal_wds'] = Left_join['animal_wds'].str.replace('discus', '')\nLeft_join['animal_wds'].replace('', np.nan, inplace=True)\nLeft_join['spillover_wds'].replace('', np.nan, inplace=True)\nLeft_join['human_wds'].replace('', np.nan, inplace=True)\nLeft_join['virus_wds'].replace('', np.nan, inplace=True)\nLeft_join['evidence_wds'].replace('', np.nan, inplace=True)\n\narticlesAnimal = Left_join[Left_join[['animal_wds', 'spillover_wds', 'human_wds', 'virus_wds','evidence_wds']].notnull().all(1)]\narticlesAnimal['animal_wds'] = articlesAnimal['animal_wds'].map(lambda x: clean_text(x))\narticlesAnimal['animal_wds'].replace(' ', np.nan, inplace=True)\narticlesAnimal = articlesAnimal.dropna(subset=['animal_wds'])\narticlesAnimal\n'''","b9058215":"'''\nprint('Total articles containing animal keywords: ' + str(len(articlesAnimal)))\nprint('Total articles available: ' + str(len(fin_df)))\nprint(str(float(len(articlesAnimal)\/len(fin_df)*100)) + '% of the articles available contains animal keywords')\n'''","5bc4090c":"'''\ndef find_country_wds(content, wds = ['transfer','spillover','pass on','transmit','contract',\n                                       'distribute','progress','incubate','spread','disseminate','zoonosis']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found\n'''","d008e594":"#articlesAnimal.to_csv('animal_articles.csv', sep=',', encoding='utf-8')","86913f51":"del Left_join","0027ebb3":"articlesAnalysis = pd.read_csv(\"\/kaggle\/input\/task3-data\/animal_articles.csv\")\n#articlesAnalysis['combined'] = articlesAnalysis['combined'].str.replace(r'\\b(\\w{1,2})\\b', '')\narticlesAnalysis.head()","54696c5d":"# Join the different processed titles together.\nanimal_string = ''.join(list(articlesAnalysis['animal_wds'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(width = 600, height = 400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', collocations=False)\n# Generate a word cloud\nwordcloud.generate(animal_string)\n# Visualize the word cloud\nwordcloud.to_image()","79b7a24f":"def find_animal_wds(content, wds = [\n ' dog ',\n ' cat ',\n 'pig',\n 'mouse',\n 'bird',\n ' bat ',\n 'horse',\n ' rat ',\n 'sheep',\n 'chicken',\n 'rabbit',\n 'insect',\n 'goat',\n 'monkey',\n 'fox',\n 'fish',\n 'cow',\n 'ferret',\n 'deer',\n 'fly',\n 'raccoon',\n 'camel',\n 'hamster',\n 'bear',\n'pangolin',\n'tiger']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ','\n            if found.count(',') == 5:\n                break\n    return found","da3b33e5":"articlesAnalysis['animal_wds'] = articlesAnalysis['combined'].apply(find_animal_wds)\narticlesAnalysis['animal_wds'] = articlesAnalysis['animal_wds'].str.replace(',', ' ')\narticlesAnalysis['animal_wds'].replace('', np.nan, inplace=True)\narticlesAnalysis = articlesAnalysis.dropna(subset=['animal_wds'])\narticlesAnalysis","970edfe0":"# Join the different processed titles together.\nanimal_string = ''.join(list(articlesAnalysis['animal_wds'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(width = 600, height = 400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', collocations=False)\n# Generate a word cloud\nwordcloud.generate(animal_string)\n# Visualize the word cloud\nwordcloud.to_image()","0f715130":"def plot_50_most_common_words(count_data, count_vectorizer):\n    import matplotlib.pyplot as plt\n    words = count_vectorizer.get_feature_names()\n    total_counts = np.zeros(len(words))\n    for t in count_data:\n        total_counts+=t.toarray()[0]\n    \n    count_dict = (zip(words, total_counts))\n    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:20]\n    words = [w[0] for w in count_dict]\n    counts = [w[1] for w in count_dict]\n    x_pos = np.arange(len(words)) \n    \n    plt.figure(2, figsize=(10, 8\/1.6180))\n    plt.subplot(title='20 most common words')\n    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n    sns.barplot(x_pos, counts, palette='husl')\n    plt.xticks(x_pos, words, rotation=90) \n    plt.xlabel('words')\n    plt.ylabel('counts')\n    plt.show()","1af301de":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data2 = count_vectorizer.fit_transform(articlesAnalysis['animal_wds'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data2, count_vectorizer)","ee726af8":"import re\ndef listToString(s):  \n    # initialize an empty string \n    str1 = \" \" \n    # return string   \n    return (str1.join(s)) \n\narticlesAnalysisTiger= articlesAnalysis[articlesAnalysis['animal_wds'].str.contains(\"tiger\")]\n\narticlesAnalysisTiger['animal_sentence'] = ''\nfor i in range(0,len(articlesAnalysisTiger)):\n    articlesAnalysisTiger['animal_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*? tiger[^.]*\\.)\",'.' + articlesAnalysisTiger['combined'].iloc[i])).lower()\n\narticlesAnalysisTiger","27d6147e":"import re\ndef listToString(s):  \n    # initialize an empty string \n    str1 = \" \" \n    # return string   \n    return (str1.join(s)) \n\narticlesAnalysisBat= articlesAnalysis[articlesAnalysis['animal_wds'].str.contains(\"bat\")]\n\narticlesAnalysisBat['animal_sentence'] = ''\nfor i in range(0,len(articlesAnalysisBat)):\n    articlesAnalysisBat['animal_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*? bats [^.]*\\.)\",'.' + articlesAnalysisBat['combined'].iloc[i])).lower()\n\narticlesAnalysisBat","8bd59f7b":"import re\ndef listToString(s):  \n    # initialize an empty string \n    str1 = \" \" \n    # return string   \n    return (str1.join(s)) \n\narticlesAnalysisPangolin= articlesAnalysis[articlesAnalysis['combined'].str.contains(\"pangolin\")]\n\narticlesAnalysisPangolin['animal_sentence'] = ''\nfor i in range(0,len(articlesAnalysisPangolin)):\n    articlesAnalysisPangolin['animal_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*? pangolin[^.]*\\.)\",'.' + articlesAnalysisPangolin['combined'].iloc[i])).lower()\n\narticlesAnalysisPangolin","3140932c":"import re\ndef listToString(s):  \n    # initialize an empty string \n    str1 = \" \" \n    # return string   \n    return (str1.join(s)) \n\narticlesAnalysisCat= articlesAnalysis[articlesAnalysis['animal_wds'].str.contains(\"cat\")]\n\narticlesAnalysisCat['animal_sentence'] = ''\nfor i in range(0,len(articlesAnalysisCat)):\n    articlesAnalysisCat['animal_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*? cat [^.]*\\.)\",'.' + articlesAnalysisCat['combined'].iloc[i])).lower()\n\narticlesAnalysisCat","1d2d1ffc":"articlesAnalysisTigerCov = articlesAnalysisTiger[articlesAnalysisTiger['animal_sentence'].str.contains(\"cov\" or \"corona\")]\narticlesAnalysisTigerCov","6da62f5a":"# Join the different processed titles together.\nanimal_string = ''.join(list(articlesAnalysisBat['animal_sentence'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(width = 600, height = 400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', collocations=False)\n# Generate a word cloud\nwordcloud.generate(animal_string)\n# Visualize the word cloud\nwordcloud.to_image()","f3e39d86":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(articlesAnalysisBat['animal_sentence'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data, count_vectorizer)","33b86d64":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(count_data)\n\ndocs_test=articlesAnalysisBat['animal_sentence'].tolist()\n\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results\nfeature_names=count_vectorizer.get_feature_names()\n \n# get the document that we want to extract keywords from\ndoc=listToString(docs_test)\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(count_vectorizer.transform([doc]))\n \n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n \n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\n=====Doc=====\")\nprint(doc)\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n","e08f470d":"from collections import Counter\n\nevidence = ['backyard*livestock', 'wet markets', 'meat markets','seafood markets', 'bit*', \n                                      'laboratory*accident', 'fairs', 'petting*zoo',  'trading*wild animal', \n                                      'destruction*habitat', 'wild animal*food', 'lifestock pathogens','genetic mutations',\n                                      'animal testing', ' hunting ', 'industrial*farming', ' pet ', 'butcher', ' eat ', ' meat ']\nfrom operator import itemgetter\narticlesAnalysis['evidence_wds'] = articlesAnalysis['evidence_wds'].str.replace('domestic animal','')\nflat_list = [item for sublist in articlesAnalysis['evidence_wds'].str.split(',') for item in sublist]\nflat_list.remove('')\ntest_dict = Counter(flat_list)\ndel test_dict['']","8cd1df89":"test_dict","1985a4cd":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n\n\n#explsion\nexplode = (0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05)\n \nplt.pie(test_dict.values(), labels=test_dict, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Spillover Source')\nplt.tight_layout()\nplt.show()\n","5e1a5ef7":"articlesAnalysis['animal_wds'] = articlesAnalysis['animal_wds'].str.replace('animal','')\n#articlesAnalysisAnimal = articlesAnalysis[articlesAnalysis['animal_wds'] == \"bat \"]\n#articlesAnalysisAnimal = articlesAnalysis[articlesAnalysis['animal_wds'].str.contains(\"dog\")]\narticlesAnalysisAnimal = articlesAnalysis[articlesAnalysis['evidence_wds'].str.contains(\"meat\")]\narticlesAnalysisAnimal = articlesAnalysisAnimal[articlesAnalysisAnimal['evidence_wds'].str.contains(\" eat \")]\narticlesAnalysisAnimal = articlesAnalysisAnimal[articlesAnalysisAnimal['spillover_wds'].str.contains(\"zoono\")]\narticlesAnalysisAnimal","1a4f990e":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data2 = count_vectorizer.fit_transform(articlesAnalysisAnimal['animal_wds'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data2, count_vectorizer)","19e0e31a":"import matplotlib as mpl\nfrom matplotlib.pyplot import figure\nmpl.rcParams['font.size'] = 9.0\nfigure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n\nflat_list = [item for sublist in articlesAnalysisAnimal['evidence_wds'].str.split(',') for item in sublist]\nflat_list.remove('')\ntest_dict = Counter(flat_list)\ndel test_dict['']\n\n#explsion\nexplode = (0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05)\n \nplt.pie(test_dict.values(), labels=test_dict, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = None, textprops={'fontsize': 8})\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Spillover Source')\nplt.tight_layout()\nplt.show()","58d691de":"import datetime as dt\narticlesAnalysis['publish_time'] = pd.to_datetime(articlesAnalysis['publish_time'])\narticlesAnalysis.head()","140f529f":"import datetime as dt\n\narticlesAnalysis3 = articlesAnalysis[articlesAnalysis['evidence_wds'].str.contains(\" eat \")]\narticlesAnalysis3 = articlesAnalysis3[articlesAnalysis3['publish_time'].dt.year > 2018]\narticlesAnalysis3 = articlesAnalysis3[~articlesAnalysis3['evidence_wds'].str.contains(\"bit\")]\narticlesAnalysis3 = articlesAnalysis3[articlesAnalysis3['spillover_wds'].str.contains(\"zoono\")]\n\narticlesAnalysis3\n","b20f731d":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data4 = count_vectorizer.fit_transform(articlesAnalysis3['animal_wds'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data4, count_vectorizer)","2ff9240c":"articlesAnalysis2 = articlesAnalysis[articlesAnalysis['combined'].str.contains(\"zoono\")]\narticlesAnalysis2","f14eaa8a":"\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(articlesAnalysis2['combined']) \n# Helper function\ndef print_topics(model, count_vectorizer, n_top_words):\n    words = count_vectorizer.get_feature_names()\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"\\nTopic #%d:\" % topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n        \n# Tweak the two parameters below\nnumber_topics = 10\nnumber_words = 10\n# Create and fit the LDA model\nlda = LDA(n_components=number_topics, n_jobs=-1)\nlda.fit(count_data)\n# Print the topics found by the LDA model\nprint(\"Topics found via LDA:\")\nprint_topics(lda, count_vectorizer, number_words)\n","2fa57713":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(articlesAnalysis2['combined'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data, count_vectorizer)","8d0ea429":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(count_data)","c5acb416":"docs_test=articlesAnalysis2['combined'].tolist()","78e3e836":"def sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n \ndef extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n \n    score_vals = []\n    feature_vals = []\n    \n    # word index and corresponding tf-idf score\n    for idx, score in sorted_items:\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n \n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","c6fdb9fc":"feature_names=count_vectorizer.get_feature_names()\n \n# get the document that we want to extract keywords from\ndoc=docs_test[0]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(count_vectorizer.transform([doc]))\n \n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n \n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\n=====Doc=====\")\nprint(doc)\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n ","a07a934a":"def find_africa_wds(content, wds = ['Africa','Burundi', 'Comoros', 'Djibouti', 'Eritrea', 'Ethiopia', 'Kenya', 'Madagascar', 'Malawi', 'Mauritius', 'Mayotte\u2019,\u00a0\u00a0\u2018Mozambique', 'Reunion', 'Rwanda', 'Seychelles', 'Somalia', 'Tanzania', 'United\u00a0Republic of\u00a0Uganda', 'Zambia', 'Zimbabwe', 'Angola', 'Cameroon', 'Chad', 'Congo', 'Algeria', 'Egypt', 'Libyan Arab\u00a0Jamahiriya', 'Morroco', 'South Sudan', 'Sudan', 'Tunisia', 'Western Sahara', 'Botswana', 'Eswatini\u2019, \u2019Swaziland', 'Lesotho', 'Namibia', 'South Africa', 'Benin', 'Burkina Faso', 'Cape Verde', 'Ivory Coast', 'Gambia', 'Ghana', 'Guinea', 'Guinea-Bissau', 'Liberia', 'Mali', 'Mauritania', 'Niger', 'Nigeria', 'Saint Helena', 'Senegal', 'Sierra Leone', 'Togo']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","90563413":"def find_asia_wds(content, wds = ['Asia','Afganistan', 'Armenia', 'Azerbaijan', 'Bangladesh', 'Bhutan', 'Brunei\u00a0Darussalam', 'Cambodia', 'China', 'Georgia', 'Hong\u00a0Kong', 'India', 'Indonesia', 'Japan', 'Kazakhstan', 'North\u00a0Korea\u2019, \u201cSouth\u00a0Korea', 'Kyrgyzstan', 'Laos', 'Macao', 'Malaysia', 'Maldives', 'Mongolia', 'Myanmar', 'Nepal', 'Pakistan', 'Phillipines', 'Singapore', 'Sri\u00a0Lanka', 'Taiwan', 'Tajikistan', 'Thailand', 'Turkmenistan', 'Uzbekistan', 'Vietnam']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","38d63a07":"def find_america_wds(content, wds = ['Bermuda', 'Canada', 'Greenland', 'United\u00a0States', 'U.S.A.', 'USA', 'US', 'Argentina', 'Bolivia', 'Brazil', 'Chile', 'Colombia', 'Ecuador', 'Guyana', 'Paraguay', 'Peru', 'Uruguay', 'Venezuela', 'Anguilla', 'Antigua', 'Barbuda', 'Aruba', 'Bahamas', 'Barbados', 'Bonaire', 'British Virgin\u00a0Islands', 'Cayman Islands', 'Cuba', 'Cura\u00e7ao', 'Dominican Republic', 'Grenada', 'Guadeloupe', 'Haiti', 'Jamaica', 'Martinique', 'Monserrat', 'Puerto Rico', 'Saint Lucia', 'Saint Martin', 'Saint\u00a0Vincent and the Grenadines', 'Sint Maarten', 'Trinidad and Tobago', 'Turks and Caicos\u00a0Islands', 'Virgin Islands (US)', 'Belize', 'Costa\u00a0Rica', 'El Salvador', 'Guatemala', 'Honduras', 'Mexico', 'Nicaragua', 'Panama']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found\n\ndef find_europe_wds(content, wds = ['Albania', 'Andorra', 'Belarus', 'Bosnia', 'Croatia', 'European\u00a0Union', 'Faroe Islands', 'Gibraltar\u2019,\u00a0\u00a0\u2018Iceland', 'Jersey', 'Kosovo', 'Liechtenstein', 'Moldova', 'Monaco', 'Montenegro', 'North\u00a0Macedonia', 'Norway', 'Russia', 'San\u00a0Marino', 'Serbia', 'Switzerland', 'Turkey', 'Ukraine']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found\n\ndef find_middleeast_wds(content, wds = ['Bahrain', 'Iraq', 'Iran', 'Israel', 'Jordan', 'Kuwait', 'Lebanon', 'Oman', 'Palestine', 'Qatar', 'Saudi Arabia', 'Syria', 'United Arab Emirates', 'Yemen']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found\n\ndef find_oceania_wds(content, wds = ['Australia', 'Fiji', 'French Polynesia', 'Guam', 'Kiribati', 'Marshall Islands', 'Micronesia', 'New Caledonia', 'New\u00a0Zealand', 'Papua New Guinea', 'Samoa', 'Samoa, American', 'Solomon, Islands', 'Tonga', 'Vanuatu']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ' '\n    return found","f0f1a355":"articlesAnalysis['africa'] = articlesAnalysis['combined'].apply(find_africa_wds)\narticlesAnalysis['africa'].replace('', np.nan, inplace=True)\narticlesAnalysis['asia'] = articlesAnalysis['combined'].apply(find_asia_wds)\narticlesAnalysis['asia'].replace('', np.nan, inplace=True)\narticlesAnalysis['america'] = articlesAnalysis['combined'].apply(find_america_wds)\narticlesAnalysis['america'].replace('', np.nan, inplace=True)\narticlesAnalysis['europe'] = articlesAnalysis['combined'].apply(find_europe_wds)\narticlesAnalysis['europe'].replace('', np.nan, inplace=True)\narticlesAnalysis['middleeast'] = articlesAnalysis['combined'].apply(find_middleeast_wds)\narticlesAnalysis['middleeast'].replace('', np.nan, inplace=True)\narticlesAnalysis['oceania'] = articlesAnalysis['combined'].apply(find_oceania_wds)\narticlesAnalysis['oceania'].replace('', np.nan, inplace=True)\narticlesAnalysis\n\narticlesAnalysisAmerica = articlesAnalysis.dropna(subset=['america'])\narticlesAnalysisAmerica\n\narticlesAnalysisUSA = articlesAnalysisAmerica[articlesAnalysisAmerica['america'].str.contains('USA' or 'United States')]\narticlesAnalysisUSA = articlesAnalysisUSA[articlesAnalysisUSA[['africa', 'asia', 'europe', 'middleeast','oceania']].isnull().all(1)]\narticlesAnalysisUSA","2f71ff09":"flat_list = [item for sublist in articlesAnalysisUSA['evidence_wds'].str.split(',') for item in sublist]\nflat_list.remove('')\ntest_dict = Counter(flat_list)\ndel test_dict['']\ntest_dict","ab255324":"figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n#explsion\nexplode = (0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05)\n \nplt.pie(test_dict.values(), labels=test_dict, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = None)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Spillover Source')\nplt.tight_layout()\nplt.show()","1a4b3998":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data4 = count_vectorizer.fit_transform(articlesAnalysisUSA['animal_wds'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data4, count_vectorizer)","19e10e6d":"### Filtering Dataset\n\narticlesAnalysisRiskReduction = articlesAnalysis[articlesAnalysis['combined'].str.contains('risk reduction')]\narticlesAnalysisRiskReduction['risk_reduction_sentence'] = ''\nfor i in range(0,len(articlesAnalysisRiskReduction)):\n    articlesAnalysisRiskReduction['risk_reduction_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*?[^.]*?risk reduction[^.]*\\.[^.]*\\.)\",'.' + articlesAnalysisRiskReduction['combined'].iloc[i])).lower()\narticlesAnalysisRiskReduction['risk_reduction_sentence'].replace('', np.nan, inplace=True)\narticlesAnalysisRiskReduction = articlesAnalysisRiskReduction.dropna(subset=['risk_reduction_sentence'])\narticlesAnalysisRiskReduction","a8a25cc1":"### Keywords Visualization\n\n# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n# Fit and transform the processed titles\ncount_data10 = count_vectorizer.fit_transform(articlesAnalysisRiskReduction['risk_reduction_sentence'])\n# Visualise the 50 most common words\nplot_50_most_common_words(count_data10, count_vectorizer)\n\n# Join the different processed titles together.\nanimal_string = ''.join(list(articlesAnalysisRiskReduction['risk_reduction_sentence'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(width = 600, height = 400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', collocations=False)\n# Generate a word cloud\nwordcloud.generate(animal_string)\n# Visualize the word cloud\nwordcloud.to_image()","d98b9cea":"### Filtering Dataset\n\ndef find_socioeconomic_wds(content, wds = ['socioeconomic','economy', 'behavioral risk', 'unemployment', 'population density', 'school closure', 'daily wage', 'discrimination', 'racism', 'religion', 'financial status', 'education']):\n    found = ''\n    for w in wds:\n        if w in content:\n            found += w + ', '\n    return found\n\narticlesAnalysis['social_wds'] = articlesAnalysis['combined'].apply(find_socioeconomic_wds)\narticlesAnalysis['social_wds'].replace('', np.nan, inplace=True)\narticlesAnalysisSocial = articlesAnalysis.dropna(subset=['social_wds'])\narticlesAnalysisSocio = articlesAnalysisSocial[articlesAnalysisSocial['social_wds'].str.contains('socioeconomic')]\narticlesAnalysisSocio","eafc59d4":"### Keywords Visualization\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfigure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n\nfrom operator import itemgetter\nflat_list = [item for sublist in articlesAnalysisSocial['social_wds'].str.split(', ') for item in sublist]\ntest_dict = Counter(flat_list)\ndel test_dict['']\n\n#explsion\nexplode = (0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05,0.05)\n \nplt.pie(test_dict.values(), labels=test_dict, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = None, textprops={'fontsize': 8})\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nplt.title('Socioeconomic Keywords')\nplt.tight_layout()\nplt.show()","bca96ae0":"test_dict","b0584f09":"import re\ndef listToString(s):  \n    # initialize an empty string \n    str1 = \" \" \n    # return string   \n    return (str1.join(s)) \n\narticlesAnalysisSocio['socioeconomic_sentence'] = ''\nfor i in range(0,len(articlesAnalysisSocio)):\n    articlesAnalysisSocio['socioeconomic_sentence'].iloc[i] = listToString(re.findall(r\"([^.]*?socioeconomic[^.]*\\.[^.]*\\.[^.]*\\.[^.]*\\.)\",'.' + articlesAnalysisSocio['combined'].iloc[i])).lower()\narticlesAnalysisSocio['socioeconomic_sentence'].replace('', np.nan, inplace=True)\narticlesAnalysisSocialEco = articlesAnalysisSocio.dropna(subset=['socioeconomic_sentence'])\narticlesAnalysisSocialEco\n\narticlesAnalysisSocio['socioeconomic_sentence'].iloc[0]\n\n# Join the different processed titles together.\nanimal_string = ''.join(list(articlesAnalysisSocio['socioeconomic_sentence'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(width = 600, height = 400, background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue', collocations=False)\n# Generate a word cloud\nwordcloud.generate(animal_string)\n# Visualize the word cloud\nwordcloud.to_image()","440def6f":"\nfin_df = pd.read_csv('\/kaggle\/input\/task3-data\/fin_df.csv')\nfin_df.head()","6e4c29b4":"fin_df['title'].fillna(\"NoTitle\", inplace = True)\nfin_df['abstract'].fillna(\"NoAbstract\", inplace = True)\nfin_df['full_text'].fillna(\"NoText\", inplace = True)\nfin_df['combined'] = 'Title : '+fin_df['title'] + '; Abstract ' + fin_df['abstract'] + '; Full Text ' + fin_df['full_text']","35210961":"\n\n# Load the BERT model. Various models trained on Natural Language Inference (NLI) https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/docs\/pretrained-models\/nli-models.md and \n# Semantic Textual Similarity are available https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/docs\/pretrained-models\/sts-models.md\n\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')","75bc8f5f":"#Uncomment line below if you want to embed\n#content = [re.sub(' \\n\\n\\n ','',x) for x in fin_df['full_text'].to_list()]\n","be114907":"### Embed the article contents\n'''\nembedding = model.encode(content, show_progress_bar=True)\n'''","e087b62d":"#Save embeddings\n'''\n\nwith open('full_text_embeddings.pkl', 'wb') as embed:\n    pickle.dump(embedding, embed)\n'''","9ba6b9a6":"with open('\/kaggle\/input\/task3-data\/full_text_embeddings.pkl','rb') as f:\n    embedding = pickle.load(f)","e7ca6f83":"queries = ['How is livestock affected due to Corona virus?',\n          'How are farmers afftected due to Coronoa vurus?',\n          'How is the spread of corona virus?',\n          'Has corona virus infected animals?',\n          'How does corona virus transfer?']","58d799b8":"query_embeddings = model.encode(queries)","c231add9":"fin_df.head()","e742dc40":"df = pd.DataFrame(columns=['Query','Cosine Similarity','Summary','Article Full Text'])","7dc81565":"\nimport scipy as sc\ntop_n_selects = 1\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = sc.spatial.distance.cdist([query_embedding], embedding, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n    \n    \n    \n    print('Query : ',query)\n    print('###########################################')\n    \n    for idx, distance in results[0:top_n_selects]:\n        print(\"\\nCosine Similarity (Score: %.4f)\" % (1-distance),\"\\n\")\n        \n        body = fin_df['combined'][idx].strip() \n        summary_model = Summarizer()\n        result = summary_model(body, min_length=60)\n        summary = ''.join(result)\n        print('Summary:',summary)\n        print('\\n')\n        print('Article:',body)\n        \n        similarity = (1-distance)\n        to_append = [query, similarity, summary, body]\n        a_series = pd.Series(to_append, index = df.columns)\n        df = df.append(a_series, ignore_index=True)\n\n        \n        print('_________________________________________')","30bfbcbd":"df.head()","8969ea93":"'''\nqueries = []\nquery = input('Enter you query:')\nqueries.append(query)\nqueries\n'''","75e7974d":"# Load the BERT model. Various models trained on Natural Language Inference (NLI) https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/docs\/pretrained-models\/nli-models.md and \n# Semantic Textual Similarity are available https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/docs\/pretrained-models\/sts-models.md\n\n#model = SentenceTransformer('bert-base-nli-mean-tokens')","7ad6d5aa":"#query_embeddings = model.encode(queries)","10343c9c":"'''\nwith open('\/kaggle\/input\/task3-data\/full_text_embeddings.pkl','rb') as f:\n    embedding = pickle.load(f)\n'''","e272e622":"#summary_model = Summarizer()","a92b1874":"#df = pd.DataFrame(columns=['Query','Cosine Similarity','Summary','Article'])","ad0559fc":"'''\ntop_n_selects = 2\nimport scipy as sc\n\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = sc.spatial.distance.cdist([query_embedding], embedding, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print('Query : ',query)\n    print('###########################################')\n    \n    for idx, distance in results[0:top_n_selects]:\n        print(\"\\nCosine Similarity (Score: %.4f)\" % (1-distance),\"\\n\")\n        \n        body = fin_df['combined'][idx].strip() \n        summary_model = Summarizer()\n        result = summary_model(body, min_length=60)\n        summary = ''.join(result)\n        similarity = (1-distance)\n        to_append = [query, similarity, summary, body]\n        a_series = pd.Series(to_append, index = df.columns)\n        df = df.append(a_series, ignore_index=True\n'''","32982dc9":"#df.head()","acdfdf1d":"#### Read files","541a70c5":"### 4.2 Risk Reduction\n\nRisk Reduction Strategies (Task 4.2)\nSustainable risk mitigation strategies are critical to prevent future instances of similar viral outbreaks involving animals. Research and implementation of such strategies would require very specific data mining using keywords related to risk.\n\u2022\tRisk reduction strategy key search terms: education, economy, socioeconomic, unemployment, school closure, population density, religion, discrimination, behavioral risk, racism, financial status\n\nAdditional weight is added to sentences with \u201crisk reduction\u201d phrase in them. \n","349e7ee0":"#### SIGNAL CORRELATION IN FREQ DOMAIN","94b20e6e":"### List Questions","c3d15205":"## Search Based on New Query","17e1f026":"## Reference ","bce89851":"## Summary\n\nThe medical and science community is at the fore-front of the fight against COVID-19, and as technologists the best we can do is to enable and empower them in this critical endeavor. With this motivation, our team has strived to design and deliver a solution that makes existing knowledge more readily accessible through the use of technology. The project required many crucial design and approach decisions and we relied on a few key tenets for guidance - practicality, user-friendliness and scalability. As we deliver this project, we hope it adds value in this battle against Corona virus.\n","78911d30":"fin_df.isnull().sum()","7b3e9ec6":"### Combine articles to a dataframe","3fd6f7dc":"#### Since we have saved 'animal_articles.csv', in the previous step, we can delete 'Left_join' to free up space. We only need 'animal_articles' going forward","bdec37b4":"## Section 1 : Real-time tracking of whole genomes to track variations of the virus over time\n \n\n### Method to measure variations in different Genome sequences","788c113e":"## LDA ","61f9ff50":"### Import Processed Data","3300b0bd":"#### Explore Livestock articles","4a526a53":"## Let's free up some space","05aa1de4":"## Spillover Analysis","f09985e7":"##### Load animal List","58541c62":"### Add here","6859283a":"#### BACK TO TIME DOMAIN - correlation of piece wise time domain signal","a33ba36d":"# Section 1 : Summary\n\n### Section 1: Real-time tracking of whole genomes to track variations of the virus over time\n\n\nSummary:\n\u2022\tTo accomplish this task we follow the steps below: \no\tRetrieve the genome sequences from publicly available realtime databases such as China National GeneBank & GISAID EpiCoV\u2122 Database\no\tUse same algorithm from Task 3.2 to classify whether the new samples or sequences belong to exsiting or new groups \no\tDetermine whether or not if new sequences are changing over time\n\u2022\tThis task tracks temporal variations \n\n\u2022\tBootstrap confidence interval of the measured metric\n\n\u2022\tHierarchical classification of the sample sequence\nThis subtask relates to the evolution of the genome sequence in time.  To study these variations, we use the same approach as described in section 3.2.  The only two differences are as follows.  \n\n1.\tFirst, the input sequences must be specified for an individual geographical location, to remove the spatial dependency.  Also, these sequences must be specified for a given period of time and sorted by date in ascending order.  \n2.\tThe algorithm of section 3.2 is then applied but is modified so that it only analyzes the consecutive pairs of sequences, e.g. 1st & 2nd sequences; 2nd & 3rd; 3rd & 4th, \u2026, (Nth-1) & Nth, where N is the number of input sequences.\n\nThe results below show the genome evolution for China, from end of December 2019 to beginning of February 2020. Figure 1 shows the ordinate the measured metric (that estimates the difference between two consecutive sequences) and in the abscissa the time stamp of the sequence.   \nFigure 1 shows some similar variation for the first three sequences, which were each taken about 10 days apart.  However, there is a larger variation for sequence 4th, with respect to 3rd and 5th, this could indicate that sequence 4th corresponds to a different strain of the virus within the same location of China. Finally, there is almost no variation in the last two sequences.\n\n\n### Section 2: Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences\n\nSummary:\n\u2022\tGenome sequence conversion (obtained from publicly available realtime databases such as China National GeneBank & GISAID EpiCoV\u2122 Database) to a random signal\n\u2022\tSignal segmentation \n\u2022\tSignal correlation \n\u2022\tResults\n\u2022\tThis task tracks geographical variations\n\nTask 3.2: Genome Sequence Conversion\nIn this step, the input genome sequence is converted to a random signal.  This is useful, since the new waveform is much easier to visualize and manipulate than letter patterns.  Furthermore, this allows us to use signal correlation in order to find how different or similar the two signals are.     \n\nThe Figure below shows the signal conversion for the two input sequences that will be analyzed. Each of the characters of the sequence is translated to a complex random number where the same random number is assigned to every character occurrence.\n\nTask 3.2: Signal Segmentation \nSince the input sequences are about 30,000 samples long, the signal is divided in equal length segments.  The length of the segment selected in our study is 1024 but it can be changed to any given value.  Figure below shows how the signal is segmented.  In this example, we get 30 segments of size 1024 for the ~30,000 samples of the input sequence.\n\nTask 3.2: Signal Correlation \u2013 Part 1\n\nIn order to perform this step, we do the following. \nLet s_1,1,s_1,2,s_1,3,\u2026..,s_(1,n) be the n  segments of signal s_1, the signal corresponding to the first sequence and s_2,1,s_2,2,s_2,3,\u2026..,s_(2,n) the n  segments of signal s_2 be the signal corresponding to the second sequence, then for each segment:\n\n\tCalculate the FFT of the signal to obtain the frequency signal (see Figure)\n\tS_1,1,S_1,2,S_1,3,\u2026..,S_(1,n) for signal s_1\n\tS_2,1,S_2,2,S_2,3,\u2026..,S_(2,n) for signal s_2\n\tLet S_1,1,S_1,2,S_1,3,\u2026..,S_(1,n) be the n segments of signal S_1\n\tLet S_2,1,S_2,2,S_2,3,\u2026..,S_(2,n) be the n segments of signal S_2\n\tCalculate the following vectors as shown in Figure 5\n\tR_11,1=S_1,1 \u3016 S\u3017_1,1^*  ,R_11,2=S_1,2 \u3016 S\u3017_1,2^*  ,\u2026..R_(11,n)=S_(1,n) \u3016 S\u3017_(1,n)^*     \n\tR_12,1=S_1,1 \u3016 S\u3017_2,1^*  ,R_12,2=S_1,2 \u3016 S\u3017_2,2^*  ,\u2026..R_(12,n)=S_(1,n) \u3016 S\u3017_(2,n)^*     \n\tPerform IFFT to obtain the autocorrelation and cross correlation vectors rxx and rxy (see Figure) and set rxx,ryy as the ith segment entry of the diagonal of matrix r (below figure)\n\tThen we calculate the correlation vectors as follows (see Figure 6):\n\t r_11,1=\u3016IFFT(R\u3017_11,1),r_11,2=\u3016IFFT(R\u3017_11,2),\u2026,r_(11,n)=\u3016IFFT(R\u3017_(11,n))   \n\tr_21,1=\u3016IFFT(R\u3017_21,1),r_21,2=\u3016IFFT(R\u3017_21,2),\u2026,r_(21,n)=\u3016IFFT(R\u3017_(21,n))   \n\nTask 3.2: Correlation Coefficients and Metrics\n\nWe obtain c_1,\u3016  c\u3017_2, which are given by the sum of the peak values of the correlation vectors over all the segments i=(1,\u2026,n) for each sequence.  This is shown below\nc_1=\u2211_(i=0)^n\u2592\u3016\u3016max\u2061(r\u3017_(11,i))\u3017,    c_2=\u2211_(i=0)^n\u2592\u3016\u3016max\u2061(r\u3017_(12,i))\u3017\nRefer Figure 7.\n\nFinally, we propose the following metric to measure the differences between a pair of sequences\nm_12= (c_1\/c_2 -1)\n\nFurthermore, we calculate the \u03b1=90% confidence interval of  m_12 using bootstrap and if this value is greater than a given threshold, then the sequences have significant variations\n  \u3016(m_12)\u3017_\u03b1>  T\n\nRefer Figure 8.\n\nFigure 9 illustrates the input sequences on the left and the associated sequence pair to perform the comparison.  The number of calculated metrics values is p=q(q-1)\/2   Which corresponds to all the possible combinations of the input sequences.\n\nTask 3.2: Algorithm 2: Genome Strains Classification\nFigure 10 shows different Thresholds values T_1,T_2, T_3.\nWhich can be adjusted to see different degrees of variations.  Also, the number of thresholds needed could be optimized so that only significant variations are detected. In our case, we have selected three levels.  Finally Figure 10, shows the tree structure where each sequence is ordered according to their confidence interval metric estimates and Threshold levels.  The data is ordered in time from oldest to the top of the tree to the most recent to the bottom of the tree.  \nThe largest threshold level captures the most significant differences between a pair of sequences, the medium threshold level captures medium differences and the lower threshold level captures small differences between a pair of sequences.\n\n\n### Pros and cons of the approach \n\nTask 3.1\nThis section describes the pros and cons for Task 3.1.\n\nPros\n\u2022\tThe results are very easy to interpret since it shows the difference between consecutive samples.  The measured data represents the time evolution for the specified geographical location. \n\nCons\n\u2022\tThe input sequences need to be collected manually from the publicly available databases. It would be useful to have a tool that can do this automatically, by specifying the geographical location and dates one is interested to analyze.\n\n4.2\tTask 3.2\nThis section describes the pros and cons for Task 3.2.\n\nPros \n\u2022\tThe method is flexible since it allows one to specify the branch tree depths, it does not need alignment in the input sequences, and it can be easily adapted to measure different features of the sequence.  We have analyzed samples from seven countries, and we have been able to identify different variations and their evolution paths.\n\nCons \n\u2022\tThe variations that this method has identified may not coincide with the definition of a strain but can be adapted to satisfy the strain criteria. \n\n\u2022\tThe current solution does not implement the sorting algorithm in the tree structure of the input data sequences.  This is now done manually.  For large number of input sequences this task is complex.  \n\n\u2022\tA post-processing tool that classifies this data will make visualization and interpretation of results more efficient.\n\n\u2022\tThe results of our study are very limited since the number of samples used was rather small.  A much larger set of samples is needed to get more meaningful results.","3cd5589e":"### 4.3 Socioeconomic Risk\n\n4.3. Socioeconomic and Behavioral Risk Factors (Task 4.3)\nIn order to analyze socioeconomic impacts of the potential future transfer of the virus from animals to humans, further keywords were utilized to further refine search results.\n\u2022\tSocioeconomic terms used:  socioeconomic, behavioral, economy, population density, school closure, daily wage, crowd, wet market, discrimination, racism, religion, financial status, education\n\nAdditional weight is added to sentences with \u201csocioeconomic\u201d and \u201cbehavioral\u201d word in them. \n","cd7ba73f":"#### External References \n1. Center for Disease Prevention (CDC)\n\n2. NextstrainNextstrain is an open-source project to harness the scientific and public health  potential  of  pathogen  genome  data.  We  provide  a continually updatedview of publicly available data alongside powerful analytic and visualization tools for use by the community. \n\n3. China National GeneBankFrom the web page of the China National GeneBank, we can get the full coronavirus genome sequence. They have data from 2020-01-03 to 2020-03-15\n\n4. GISAID EpiCoV\u2122 DatabaseGISAID  EpiCoV\u2122  Database  Until  2020-03-27  01:16:12  CST,  the  total number of viruses is 1836.  China National GeneBank DataBase (CNGBdb) is an official partner of the GISAID Initiative. It provides access to EpiCoV \n2020-04-05PA1UenRev PA1\u00a9Ericsson AB 202025(30)Commercial in Confidenceand features the most complete collection of hCoV-19 genome sequences along with related clinical and epidemiological data\n\n5. China National GeneBankoFrom the web page of the China National GeneBank, we can get the  full  coronavirus  genome  sequence.  They  have  data  from 2020-01-03 to 2020-03-15\n\n6. GISAID EpiCoV\u2122 DatabaseoGISAID  EpiCoV\u2122  Database  Until  2020-03-27  01:16:12  CST, the total number of viruses is 1836.  China National GeneBank DataBase   (CNGBdb)   is   an   official   partner   of   the   GISAID Initiative.  It  provides  access  to  EpiCoV  and  features  the  most complete  collection  of  hCoV-19  genome  sequences  along  with related clinical and epidemiological data.\n\n7.\tBERT: https:\/\/arxiv.org\/abs\/1810.04805\n\n8.\tBERT Code: https:\/\/github.com\/google-research\/bert\n\n9.\tScibert: https:\/\/arxiv.org\/pdf\/1903.10676.pdf\n\n10.\tCosine Similarity: https:\/\/en.wikipedia.org\/wiki\/Cosine_similarity\n\n11.\tBERT Summarizer: https:\/\/pypi.org\/project\/bert-extractive-summarizer\/\n\n12.\tBERT Summarizer : https:\/\/arxiv.org\/abs\/1906.04165\n\n13.\thttps:\/\/github.com\/huggingface\/neuralcoref\n","79398a64":"#### Explore Livestock Articles related to farmer\n","a4f8a0d1":"\n## Introduction\n\nCovid-19 has plunged the world into an unprecedented crisis and the global medical community is working tirelessly to not only contain the pandemic but also prepare to manage the disease effectively in the long term. Key to this endeavor is a robust fundamental understanding of the disease itself \u2013 how it originated, what do the genetics tell us and can we learn and predict its evolution. There is a myriad of existing research that contains crucial knowledge, but a lack of standard nomenclature or a scheme of organization makes it a difficult resource for the scientific community to leverage. \nTo alleviate this constraint, our team has worked towards delivering a scalable solution that makes it easier for the medical researchers and global scientific community globally to find what they need from this ever-expanding trove of disease-related research. \n","98c381a5":"### Load Picked Embedding file","bf80e770":"## Section 5 : Retrieve Relevant articles using BERT\n\nIn the spirit of building a scalable and reusable solution, we utilized BERT that allows a user to repurpose the code to find relevant articles for any phrase. As an output, the code will return the relevant articles along with the summary and a similarity score as a measure of relevance.","32b9bad2":"#### Method executed with two genome sequences\n\nGenome Sequence ConversionIn this step, the input genome sequence is converted to a random signal.  This is  useful,  since  the  new waveform  is  much  easier  to  visualize and  manipulate than  letter  patterns.    Furthermore,  this  allows  us  to  use  signal  correlation  in order to find how different or similar the two signals are.    The Figure below shows the signal conversion for the two input sequences that will  be  analyzed.  Each  of  the  characters  of  the  sequence  is  translated  to  a complex random number where the same random number is assigned to every character occurrence.","2ebcb24a":"To save time, combined data is stored as a csv 'fin_df.csv'. That is loaded and used. Uncooment the below lines to combine articles to a DataFrame","8fc089b9":"### Tokenize combined data","2886222a":"## Retrieve Questions based on new question ","2739eac8":"## Download Data (In case you want a local copy of the data)","31d96be9":"## Data Pre-processing\n\nFor the ease of searching and indexing, all the research articles were converted into flat text files and stored in a table format. In this step, all the articles were parsed and filtered for relevance. As a framework, we used categories of keywords. For example, to address animal source, the keywords were categorized as animal words, spill-over words and virus words. Through this exercise, we were able to narrow down the number of relevant articles to around 8000. This framework was also utilized for the other two sub-tasks. ","57f23cdb":"#### Plot Data","bf9e78e5":"#### Generating Genome Outputs","c256b53f":"### NOTE : THE CELL BELOW WILL TAKE A LONG TIME TO EXECUTE. SKIP AND LOAD SAVED MODEL\n","5e0b6aac":"### Note :  A few cells are commented to save time. Processed files are saved In order to view processing steps, plese un comment the relevant code","d75f13d4":"#### Process files","d1d16808":"## Import Packages\n\nPython was chosen as the platform to build the tool for its versatility and ubiquity. The final deliverable involved complex text processing, array manipulation and search, and creative visualizations. For this purpose, several specialized Python libraries were utilized including pandas, numpy, scipy and sklearn. ","7e83ac02":"*** The Cell below Encodes the articles. It takes quite a while to run. The encoded embedding is then saved as a pickle file. For Testing on New queries, please load the pickle file.\nGo To section 'Test New Queries'\n***","b2c9e3f1":"#### Processing Genome files","8721b79d":"### Specify Data Folders","8de4c01c":"### Embed the query","283ec8f4":"### Ouput files Description\n\n-------------------------------------------------------------------------------------\n\n\n**output_metrics.txt:**\n\n(M,1) vector,   where M = N* (N-1)\/2   # of combinations of the input sequences and N is the number of input sequences N.\n\n[metric ]\n[metric ]\n\n\n\n**output_raw_data.csv:**   \n\n(M,3) vector\n=\n[seq1, seq2,metric]\n[seq1, seq2,metric]\n\u2026\n\n\n**output_loc.txt:**\n\n(M,3) vector \nseq1,seq2]\n[seq1,seq2]\n..\n\n\n\n**sample_number.txt:**\n\n\n (M,3) vector, with elements:  (combination index, ordinal number for seq1, ordinal number for seq2)\n=\n[1 1 2]\n[2 1 2]\n[3 1 3]\n\u2026\n\n[m 2 3]\n[m 2 4]\n\u2026.\n\n\u2026\n\n[n  N-1, N]","73c74740":"#### Reading Sequence Files","1ac9dcfb":"### Visualization Functions","d041b8d2":"#### Generating Output files","205685cf":"#### Reading and Assembling Genome sequence file","4d8086b9":"### Embed Queries","727f7903":"## Task Details\n\nWhat do we know about virus genetics, origin, and evolution? \nWhat do we know about the virus origin and management measures at the human-animal interface?\n\nSpecifically, we want to know what the literature reports about:\n\nReal-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time.\nAccess to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged.\nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.\nEvidence of whether farmers are infected, and whether farmers could have played a role in the origin.\nSurveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\nExperimental infections to test host range for this pathogen.\nAnimal host(s) and any evidence of continued spill-over to humans\nSocioeconomic and behavioral risk factors for this spill-over\nSustainable risk reduction strategies","fb4e17f1":"### 1.1 Approach\n\n\nTo accomplish this task we follow the steps below:oRetreive the genome suquences from publicly available realtime databases   such   as China   National   GeneBank& GISAID EpiCoV\u2122 DatabaseoUse  same  algorithm  from  Task  3.2  to classify  wether  the  new samples or sequences belong to exsiting or new groups oDetermine wether  or  not  if  new  sequences  are  changing  over time\u2022This task tracks temporal variations \u2022Bootstrap confidence interval of the measured metric\u2022Hierarchical classification of the sample sequenceThis subtask relates to the evolution of the genome sequence in time.  To study these  variations,we  use  the  same  approach  as  described  in  section  3.2.    The only two differences are as follows.  1.First,   the   input sequences   must   be   specifiedfor   an   individual geographical  location,  to  remove  the  spatial  dependency.    Also,  these sequences  must  be  specified  for  a  given  period  of  time  and  sorted  by date in ascending order.  2.The algorithm of section 3.2 is then appliedbut is modified so that it only analyzes the consecutive pairs of sequences, e.g. 1st & 2nd sequences; 2nd & 3rd; 3rd & 4th, ..., (Nth-1)& Nth, where N is the number of input sequences.\n\n\nOur approach is to convert the genome sequences to waveforms and determine a metric that measures how much different or similar two  sequences are.    In order to do this, we find the confidence interval of this metric  and compare it to different threshold levels","045bc712":"\n## Section 2: Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences","f76eb947":"## Approach\nThe tool that the Team had to create must be powerful enough to extract data from nearly 60,000 available publications to be searched regarding coronavirus or the more specific COVID-19. General \u201cFind\u201d or \u201cGoogle\u201d searches were not powerful enough to support this type of data-mining platform, so the team had to be creative in order to be successful. \nThe data set used is the COVID-19 Complete Dataset via Kaggle.\n\nBelow is an outline of this team\u2019s effort to provide a viable research tool from scientists to utilize to cross-reference other publications across the world.\n\n#### Section 1 : Real-time tracking of whole genomes to track variations of the virus over time\n\n#### Section 2: Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences\n\n#### Section 3 : Evidence that livestock could be infected\n    3.1 Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over: Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n\n    3.2 Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.: Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n\n    3.3 Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.: Experimental infections to test host range for this pathogen.\n\n#### Section 4:  What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n\n    Task 4.1- Animal host(s) and any evidence of continued spill-over to humans\n    Task 4.2- Sustainable risk reduction strategies\n    Task 4.3- Socioeconomic and behavioral risk factors for this spill-over\n    \n\n#### Section 5:  Retrieve relevant articles using BERT\n\n","b9b39575":"### Word embeddings on full article text","607b98b9":"#### Output Description\n\n-------------------------------------------------------------------------------------\n\n\n**output_metrics.txt:**\n\n(M,1) vector,   where M = N* (N-1)\/2   # of combinations of the input sequences and N is the number of input sequences N.\n\n[metric ]\n[metric ]\n\n\n\n**output_raw_data.csv:**   \n\n(M,3) vector\n=\n[seq1, seq2,metric]\n[seq1, seq2,metric]\n\u2026\n\n\n**output_loc.txt:**\n\n(M,3) vector \nseq1,seq2]\n[seq1,seq2]\n..\n\n\n\n**sample_number.txt:**\n\n\n (M,3) vector, with elements:  (combination index, ordinal number for seq1, ordinal number for seq2)\n=\n[1 1 2]\n[2 1 2]\n[3 1 3]\n\u2026\n\n[m 2 3]\n[m 2 4]\n\u2026.\n\n\u2026\n\n[n  N-1, N]\n\n\n--------------------------------------------------------------------------\n\n","c0564609":"# Task 3 - CORD Hackathon Submission","697820dc":"#### TIME DOMAIN SIGNAL SEGMENTATION - FFT SIZE","ef5e8fa9":"fin_df = pd.read_csv('\/kaggle\/input\/task3-data\/fin_df.csv')\nfin_df.head()","411001ed":"### 4.1 Animal Source (Task 4.1)\nIn this step, we took the narrowed down list of 8000 articles and applied an additional level of precision to the search. To prevent skewing of the analysis due to irrelevant terms, we began with pulling together a manually curated list of 22 animals that were most likely related to the origin and spread of COVID-19.\nThe next logical step was to determine which topics each of the 22 animals were related to. For this purpose, we identified the most frequently used words and phrases for each animal term and demonstrated the relative frequency through wordcloud visualizations. \nWe also observed that many articles often referenced a pair of animals in describing the traits of the disease. Based on this, we built a circos-plot visualization that would represent which animal pairs appeared together most frequently in the articles.\n","d6302fe5":"#### Conversion of Sequence into a time domain Complex signal\n\nSince  the  input  sequences  are  about  30,000  samples  long,  the  signal  is divided in equal length segments.  The length of the segment selected in our study is 1024 but it can be changed to any given value.  Figure below showshow the signal is segmented.  In this example,we get 30 segments of size 1024 for the ~30,000 samples of the input sequence.","9130a676":"## What are we delivering?\nThe following Notebook will provide a data extraction mechanism for researchers to readily find relevant publications regarding the overall Task 3 objective","7056ebf9":"### Explore Articles","62a20849":"#### SIGNAL IN FREQ DOMAIN ","bf39ad4b":"## Section 4:  What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?\n\n###\tTask 4.1- Animal host(s) and any evidence of continued spill-over to humans\n###\tTask 4.2- Sustainable risk reduction strategies\n###\tTask 4.3- Socioeconomic and behavioral risk factors for this spill-over\n\n","fcb64b95":"### Method Applied to samples form same location in diferent times.","d7130043":"## Load the Pickled embedding file","d67d0f9b":"The following cells takes time to execute. After execution, the file is saved as 'animal_articles.csv'. This file is loaded in again. Uncomment the cell if need to process again.\n\n","38451495":"## Section 3 : Evidence that livestock could be infected\n#### 3.1\nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over: Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.\n\n#### 3.2 \nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.: Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.\n\n#### 3.3 \nEvidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over.: Experimental infections to test host range for this pathogen."}}