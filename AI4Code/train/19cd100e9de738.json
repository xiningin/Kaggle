{"cell_type":{"4490e8aa":"code","94aede7e":"code","12566b6b":"code","4f2714db":"code","cea0f18d":"code","bdcad387":"code","103be576":"code","0b9b8dcd":"code","5dcef422":"code","5350d291":"code","f9781de3":"code","8242b8b9":"code","e375a2ac":"markdown","907d1a13":"markdown","286389bc":"markdown","c65f6f83":"markdown","5c9b4e96":"markdown","a5378920":"markdown"},"source":{"4490e8aa":"import janestreet\nimport numpy as np\nimport pandas as pd\n\nimport os, sys\nimport gc\nimport math\nimport random\nimport pathlib\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn import linear_model\nimport operator\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom tqdm import tqdm\n\n# visualize\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nfrom matplotlib import pyplot\nfrom matplotlib.ticker import ScalarFormatter\nsns.set_context(\"talk\")\nstyle.use('fivethirtyeight')\npd.options.display.max_columns = None\n\nimport warnings\nwarnings.filterwarnings('ignore')","94aede7e":"SEED = 2020 # The end of it is coming...\n# INPUT_DIR = '..\/input\/jane-street-market-prediction\/'\nINPUT_DIR = '..\/input\/janestreet-save-as-feather\/'\nTRADING_THRESHOLD = 0.5 # 0 ~ 1: The smaller, the more aggressive\nDATE_BEGIN = 0","12566b6b":"os.listdir(INPUT_DIR)","4f2714db":"%%time\n\ndef load_data(input_dir=INPUT_DIR):\n    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n    features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n    example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n    ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n    return train, features, example_test, ss\n\ntrain, features, example_test, ss = load_data(INPUT_DIR)","cea0f18d":"del features, example_test, ss\ngc.collect()","bdcad387":"# remove weight = 0 for saving memory \noriginal_size = train.shape[0]\ntrain = train.query('weight > 0').reset_index(drop=True)\n\n# use data later than DATE_BEGIN\ntrain = train.query(f'date >= {DATE_BEGIN}')\n\nprint('Train size reduced from {:,} to {:,}.'.format(original_size, train.shape[0]))","103be576":"# target\ntrain['action'] = train['resp'] * train['weight']\ntrain['action'] = 1 * (train['action'] > 0)","0b9b8dcd":"# features to use\ndrops = []\nfeats = [f for f in train.columns.values.tolist() if f.startswith('feature') & (f not in drops)]\nprint('There are {:,} features.'.format(len(feats)))","5dcef422":"%%time\n\n# same hyperparameters from https:\/\/www.kaggle.com\/hamditarek\/market-prediction-xgboost-with-gpu-fit-in-1min?scriptVersionId=48127254\nparams = { \n    'task_type': \"CPU\",\n    'learning_rate': 0.08, \n    'iterations': 600,\n    'colsample_bylevel': 0.2,\n    'random_seed': SEED,\n    'has_time': True\n    }\nparams[\"loss_function\"] = \"Logloss\"\nparams[\"eval_metric\"] = \"Logloss\"\nmodel = CatBoostClassifier(**params)\n    \nmodel.fit(train[feats], train['action'], verbose=100)","5350d291":"pd.DataFrame(model.get_feature_importance(), index=feats, columns=['importance']).sort_values(by='importance', ascending=False).style.background_gradient(cmap='viridis')","f9781de3":"env = janestreet.make_env()\ntest = env.iter_test()\n        \nfor (t, sub) in test:\n    sub.action = (model.predict_proba(t[feats])[:, 1] > TRADING_THRESHOLD).astype('int')\n    env.predict(sub)       ","8242b8b9":"# env = janestreet.make_env()\n# test = env.iter_test()\n\n# for (t, sub) in test:\n#     sub.action = (model.predict(t[feats]) > TRADING_THRESHOLD - 0.5).astype('int')\n#     env.predict(sub)","e375a2ac":"# Config","907d1a13":"# Feature importance\nLet's see feature importance given by the model.","286389bc":"# Load data","c65f6f83":"# Submit","5c9b4e96":"<center><h2>Jane Street Market Prediction | XGB overfit | katsu1110 <\/h2><\/center><hr>\n\n**CatBoost** has the **has_time** feature where the order of objects in the input data is kept (do not perform random permutations during the Transforming categorical features to numerical features and Choosing the tree structure stages). It sounds promising in this time-series forecasting competition.\n\nThis notebook loads feather data from [my another notebook](https:\/\/www.kaggle.com\/code1110\/janestreet-save-as-feather?scriptVersionId=47635784).\n\nThis notebook treats this task as a binary classification.","a5378920":"# Model fitting\nFor now, let's use a simple XGBoost which is also used in the example in the Numerai Tournament."}}