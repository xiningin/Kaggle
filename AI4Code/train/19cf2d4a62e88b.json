{"cell_type":{"7ed815e5":"code","fef52bd4":"code","2b970988":"code","6cb59908":"code","f77a4242":"code","fb41d495":"code","3ee2163b":"code","0a0983b9":"code","7f9635a3":"code","9bcbf41d":"code","bc1b1362":"code","fd66d842":"code","0d7a82b9":"code","97f4b03c":"code","e948a3de":"code","b133e280":"code","b22ce30d":"code","dc8f21ba":"code","8cd01d87":"code","c7384317":"code","68c0588e":"code","7dc34353":"code","10bcb359":"code","78667810":"code","25ce9eeb":"code","72c5ffc4":"code","e6b3f427":"code","d6b82404":"code","edaa2f73":"code","144cd534":"code","e60cdce2":"code","b1eb8a26":"code","1b2b750e":"code","46ae7f3d":"code","4f5d7146":"code","51f44772":"code","7c839db5":"code","94270da9":"code","1fbb02be":"code","57cd2bae":"code","1c9d0afa":"code","80a2c360":"code","afe18889":"code","af430528":"markdown","ed1ea8e6":"markdown","234497ba":"markdown","c5784f74":"markdown","48e7d744":"markdown","ab537495":"markdown","789ef2d0":"markdown","fe399a47":"markdown","f3342e03":"markdown","e49c6f93":"markdown","daa63cbe":"markdown","b7de5f1a":"markdown","1d6f3719":"markdown","598d57c6":"markdown","7c38d5ff":"markdown","a1871b16":"markdown","b60369f8":"markdown","e8af64fa":"markdown","cb853ee0":"markdown","961b8582":"markdown","9a067499":"markdown","448d4a62":"markdown","2a65acca":"markdown","82a33dc1":"markdown","2ebd3b96":"markdown","bfc91b02":"markdown","bb96f408":"markdown","6596a772":"markdown","a537eab9":"markdown","cf289562":"markdown","bed1b50a":"markdown","5b41f13d":"markdown","a44a2133":"markdown"},"source":{"7ed815e5":"#Install\/load required packages\n!pip install dython\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","fef52bd4":"#Upload and record data \ntrain_file_path = '..\/input\/titanic\/train.csv'\ntest_file_path = '..\/input\/titanic\/test.csv'\n\ntrain_data = pd.read_csv(train_file_path, index_col=\"PassengerId\")\ntest_data = pd.read_csv(test_file_path, index_col=\"PassengerId\")","2b970988":"#Visualize the first entries of the dataset\ntrain_data.head()\n#test_data.head()","6cb59908":"#Brief summary\ntrain_data.describe()\n#test_data.describe()","f77a4242":"#Identify missing entries\ntrain_data.isnull().sum()\n#test_data.isnull().sum()","fb41d495":"from dython.nominal import associations\n\ncategorical_cols = ['Survived', 'Pclass', 'Sex', 'SibSp', \n                    'Parch', 'Ticket', 'Cabin', 'Embarked']\n\nfig1 = associations(train_data.drop(['Name'],axis=1), figsize=(14,7),\n                    theil_u=True, nominal_columns=categorical_cols,\n                    title='Fig. 1. Associations between features.',\n                    mark_columns=True);","3ee2163b":"train_data.describe(include=['O'])","0a0983b9":"#Create a copy of the train dataset and create 6 age groups\ntrain_data_age_groups = train_data.copy()\ntrain_data_age_groups['Age'] = train_data_age_groups['Age']\/\/14\ntrain_data_age_groups.fillna('unknown',inplace=True)\n\n#Create suplemental lists for the data visualization purposes\norder_list_fig2 = [0, 1, 2, 3, 4, 5, 'unknown']\nbar_counts_fig2 = [x for x in train_data_age_groups.Age.value_counts()[order_list_fig2]]\n\n#Visualize results\nplt.figure(figsize=(10,6))\nplt.title('Fig. 2. Chances of survival for different age groups.')\nfig2 = sns.barplot(x=\"Age\", y=\"Survived\", data=train_data_age_groups, \n                   order=order_list_fig2)\nplt.xlabel('Age group')\nplt.ylabel('Survival rate');\nfig2.set_xticklabels(['0-14', '14-28', '28-42', '42-56', \n                      '56-70', '70-84', 'unknown']);\n\n#Add value counts at the bar bottom\nfor i, patch in enumerate(fig2.patches):\n    fig2.annotate(f'{patch.get_height()*bar_counts_fig2[i]:.0f}\/{bar_counts_fig2[i]}', \n                  (patch.get_x() + patch.get_width() \/ 2., 0), \n                  ha = 'center', va = 'center', \n                  size=15, xytext = (0, 15), \n                  textcoords = 'offset points')","7f9635a3":"#Drop rows with NaN values in the 'Age' column \ntrain_data_no_nan_age = train_data.dropna(subset=['Age'])\n\n#Produce an updated associations table\ncategorical_cols = ['Survived', 'Pclass', 'Sex', 'SibSp', \n                    'Parch', 'Ticket', 'Cabin', 'Embarked']\nfig3 = associations(train_data_no_nan_age.drop(['Name'],axis=1), figsize=(14,7), \n             theil_u=True, nominal_columns=categorical_cols,\n             title='Fig. 3. Feature associations (excluding \\'NaN\\' age entries).',\n             mark_columns=True);","9bcbf41d":"#Create suplemental lists for data visualization purposes\norder_list_fig4 = [1, 2, 3]\nbar_counts_fig4 = [x for x in train_data.Pclass.value_counts()[order_list_fig4]]\n\n#Visualize results\nplt.figure(figsize=(10,7))\nplt.title('Fig. 4. Chances of survival for different passenger classes.')\nfig4 = sns.barplot(data=train_data, x='Pclass', y='Survived',\n                   order=order_list_fig4)\nplt.ylabel('Survival rate')\nplt.xlabel('Passengers class');\n\nfor i, patch in enumerate(fig4.patches):\n    fig4.annotate(f'{patch.get_height()*bar_counts_fig4[i]:.0f}\/{bar_counts_fig4[i]}', \n                  (patch.get_x() + patch.get_width() \/ 2., 0), \n                  ha = 'center', va = 'center', \n                  size=15, xytext = (0, 15), \n                  textcoords = 'offset points')","bc1b1362":"# Produce an associations table\ncategorical_cols = ['Survived', 'Sex','Ticket', 'Cabin', 'Embarked']\nfig5 = associations(train_data.drop(['Name'],axis=1), figsize=(14,7), \n             theil_u=True, nominal_columns=categorical_cols,\n             title='Fig. 5. Associations between features',\n             mark_columns=True);","fd66d842":"#Create a copy of the train dataset \ntrain_data_emb = train_data.copy()\ntrain_data_emb.fillna('unknown',inplace=True)\n\n#Create suplemental lists for the data visualization purposes\norder_list_fig6 = ['C', 'Q', 'S', 'unknown']\nbar_counts_fig6 = [x for x in train_data_emb.Embarked.value_counts()[order_list_fig6]]\n\n#Visualize data\nplt.figure(figsize=(10,6))\nplt.title('Fig. 6. Chances of survival for different ports of embarkation.')\nfig6 = sns.barplot(data=train_data_emb, x='Embarked', y='Survived', \n                   order=order_list_fig6)\nplt.ylabel('Survival rate')\n\nfor i, patch in enumerate(fig6.patches):\n    fig6.annotate(f'{patch.get_height()*bar_counts_fig6[i]:.0f}\/{bar_counts_fig6[i]}', \n                  (patch.get_x() + patch.get_width()\/2., 0), \n                  ha = 'center', va = 'center', \n                  size=15, xytext = (0, 15), \n                  textcoords = 'offset points')","0d7a82b9":"train_data[train_data.Embarked.isnull()]","97f4b03c":"#Select appropriate rows\nfirst_class_cabB = train_data[(train_data.Pclass == 1) & \n                              (train_data['Cabin'].str.contains('B'))]\n\n#Visualize the distribution of 'Fare' for zone B\nplt.figure(figsize=(10, 6))\nplt.title('Fig. 7. First class passengers with a cabin in zone B.')\nfig7 = sns.histplot(data=first_class_cabB, x='Fare', hue='Embarked', \n                    kde=False, binwidth=5)\nplt.xlim([0,100]);","e948a3de":"#1st class passengers emparked in Southampton \nfirst_class_embS = train_data[(train_data.Pclass == 1) &\n                              (train_data.Embarked.str.contains('S'))]\n\n#1st class passengers emparked in Cherbourg\nfirst_class_embC = train_data[(train_data.Pclass == 1) & \n                              (train_data.Embarked.str.contains('C'))]\n\nfig, axes = plt.subplots(1, 2, figsize=(14,7))\n\nfig8a = sns.histplot(x=first_class_embC.Fare, ax=axes[0], kde=True, binwidth=5)\naxes[0].set_xlim([0,150])\naxes[0].set_title('Fig. 8a. First class passengers embarked in (C)herbourg.')\n\nfig8b = sns.histplot(x=first_class_embS['Fare'], ax=axes[1], kde=True, binwidth=5)\naxes[1].set_xlim([0,150])\naxes[1].set_title('Fig. 8b. First class passengers embarked in (S)outhampton.');","b133e280":"train_data_imputed = train_data.copy() \ntrain_data_imputed.Embarked.fillna('C',inplace=True)\n#train_data_imputed.isnull().sum()","b22ce30d":"num_surv_cab_notnull = len(train_data[(train_data.Cabin.notnull()) & \n                                      (train_data.Survived == 1)])\nnum_died_cab_notnull = len(train_data[(train_data.Cabin.notnull()) & \n                                      (train_data.Survived == 0)])\ntot_cab_notnull = num_surv_cab_notnull + num_died_cab_notnull\nprint('Probability to survive with known cabin is ' \n      f'{num_surv_cab_notnull\/tot_cab_notnull*100:.1f}%!')\n\nnum_surv_cab_null = len(train_data[(train_data.Cabin.isnull()) & \n                                   (train_data.Survived == 1)])\nnum_died_cab_null = len(train_data[(train_data.Cabin.isnull()) & \n                                   (train_data.Survived == 0)])\ntot_cab_null = num_surv_cab_null + num_died_cab_null\nprint('Probability to survive with unknown cabin is only ' \n      f'{num_surv_cab_null\/tot_cab_null*100:.1f}%.')","dc8f21ba":"#Create a copy of the train dataset to rename survival groups for the data visualization purposes\ntrain_data_survival = train_data.copy()\ntrain_data_survival['Survived'].replace(0, 'No',inplace=True) \ntrain_data_survival['Survived'].replace(1, 'Yes',inplace=True)\n\n#Create suplemental lists for the data visualization purposes\ncolors=['C0','C1']\nbar_counts_fig9 = [x for x in train_data_survival.Survived.value_counts()]\n\n#Visualize the survival data \nplt.figure(figsize=(10, 6))\nplt.title('Fig. 9. Survival probability.')\nfig9 = sns.histplot(train_data_survival,  x='Survived', stat='probability')\nfor i, patch in enumerate(fig9.patches):\n    patch.set_facecolor(colors[i])\n    fig9.annotate(f'{bar_counts_fig9[i]:.0f}\/{sum(bar_counts_fig9)}', \n                  (patch.get_x() + patch.get_width() \/ 2., 0), \n                  ha = 'center', va = 'center', \n                  size=15, xytext = (0, 15), \n                  textcoords = 'offset points')","8cd01d87":"test_data_imputed = test_data.copy()\ntest_data_imputed.Cabin.fillna('unknown', inplace=True)\n\ntrain_data_imputed.Cabin.fillna('unknown', inplace=True)\ntrain_data_imputed.isnull().sum()","c7384317":"num_surv_age_notnull = len(train_data[(train_data.Age.notnull()) & \n                                      (train_data.Survived == 1)])\nnum_died_age_notnull = len(train_data[(train_data.Age.notnull()) & \n                                      (train_data.Survived == 0)])\ntot_age_notnull = num_surv_age_notnull + num_died_age_notnull\nprint('Probability to survive with known age is '\n      f'{num_surv_age_notnull\/tot_age_notnull*100:.1f}%')\n\nnum_surv_age_null = len(train_data[(train_data.Age.isnull()) & \n                                   (train_data.Survived == 1)])\nnum_died_age_null = len(train_data[(train_data.Age.isnull()) & \n                                   (train_data.Survived == 0)])\ntot_age_null = num_surv_age_null + num_died_age_null\nprint('Probability to survive with unknown age is '\n      f'{num_surv_age_null\/tot_age_null*100:.1f}%')","68c0588e":"#Separate the 'Survived' column from the training dataset\ny_train = train_data['Survived']\nX_train_imputed = train_data_imputed.drop(['Survived'], axis=1)\n\n#Rename the test dataset correspondingly\nX_test_imputed = test_data_imputed\n\n#Concat training and test data to have more statistics for 'Age' \nX_all = pd.concat([X_train_imputed, X_test_imputed])\n\n#Add a boolean column indicating if the age was missing \nX_all['Age_was_missing'] = X_all['Age'].isnull()\n\n#Check out the tail of the combined dataset\nX_all.tail()","7dc34353":"# Check out the unique titles in the combined dataset\nX_all.Name.str.extract(' ([A-Za-z]+)\\.', expand=False).value_counts()","10bcb359":"# Check out the unique titles for the 'NaN' age entries in the combined dataset \nX_all[X_all['Age_was_missing']==True].Name.str.extract(' ([A-Za-z]+)\\.', \n                                                       expand=False).value_counts()","78667810":"#Generate the 'boys_data' table and record relevant statistical info\nboys_data = X_all[X_all['Name'].str.split(\",\").str.get(1).str.contains('Master.')]\nboys_mean_age = boys_data.Age.mean()\nboys_age_std = boys_data.Age.std()\nboys_nan_count = boys_data.Age.isnull().sum()\n\n#Visualize the distribution of boys age \nplt.figure(figsize=(10,6))\nsns.histplot(boys_data, x='Age', kde=True, binwidth=1)\nplt.title('Fig. 10. Boys age distribution.');","25ce9eeb":"#Get indices of the 'NaN' age values for boys\nboys_age_nan_ind = X_all[(X_all['Name'].str.split(\",\").str.get(1).str.contains('Master.'))\n                         & (X_all['Age'].isnull())].index\n\n#Impute the boys 'NaN' age\nX_all.loc[boys_age_nan_ind,'Age'] = boys_mean_age\nprint('Number of entries that are left with an unknown age is:',\n      len(X_all[X_all['Age'].isnull()]))\nX_all.loc[boys_age_nan_ind]","72c5ffc4":"#Generate the 'married_women_data' table and record the relevant statistical info\nmarried_women_data = X_all[X_all['Name'].str.split(\",\").str.get(1).str.contains('Mrs.')]\nmarried_women_mean_age = married_women_data.Age.mean()\nmarried_women_age_std = married_women_data.Age.std()\nmarried_women_nan_count = married_women_data.Age.isnull().sum()\n\n#Visualize the married women age distribution\nplt.figure(figsize=(10,6))\nsns.histplot(married_women_data, x='Age', kde=True, binwidth=3)\nplt.title('Fig. 11. Married women age distribution.');","e6b3f427":"#Get indices of the 'NaN' age values for married women\nmarried_women_age_nan_ind = X_all[(X_all['Name'].str.split(\",\").str.get(1).str.contains('Mrs.'))\n                         & (X_all['Age'].isnull())].index\n\n#Impute the married women 'NaN' age\nX_all.loc[married_women_age_nan_ind,'Age'] = married_women_mean_age\nprint('Number of entries that are left with unknown age is:',\n      len(X_all[X_all['Age'].isnull()]))\nX_all.loc[married_women_age_nan_ind]","d6b82404":"#Generate the 'single_women_data' table and record the relevant statistical info\nsingle_women_data = X_all[X_all['Name'].str.split(\",\").str.get(1).str.contains('Miss.')]\nsingle_women_mean_age = single_women_data.Age.mean()\nsingle_women_age_std = single_women_data.Age.std()\nsingle_women_nan_count = single_women_data.Age.isnull().sum()\n\n#Visualize the age distribution of single women \nplt.figure(figsize=(10,6))\nsns.histplot(single_women_data, x='Age', kde=True, binwidth=3)\nplt.title('Fig. 12. Single women age distribution.');","edaa2f73":"#Get indices of 'NaN' age values for single women\nsingle_women_age_nan_ind = X_all[(X_all['Name'].str.split(\",\").str.get(1).str.contains('Miss.'))\n                         & (X_all['Age'].isnull())].index\n\n#Impute the single women 'NaN' age\nX_all.loc[single_women_age_nan_ind,'Age'] = single_women_mean_age\nprint('Number of entries that are left with unknown age is:',\n      len(X_all[X_all['Age'].isnull()]))\nX_all.loc[single_women_age_nan_ind]","144cd534":"#Generate the 'men_data' table and record the relevant statistical info\nmen_data = X_all[(~(X_all['Name'].str.split(\",\").str.get(1).str.contains('Master.')))\n                 & (X_all['Sex']=='male')]\nmen_mean_age = men_data.Age.mean()\nmen_age_std = men_data.Age.std()\nmen_nan_count = men_data.Age.isnull().sum()\n\n#Visualize the age distribution of men \nplt.figure(figsize=(10,6))\nsns.histplot(men_data, x='Age', kde=True, binwidth=3)\nplt.title('Fig. 13. Men age distribution.');","e60cdce2":"#Get indices of the 'NaN' age values for men\nmen_age_nan_ind = X_all[(~(X_all['Name'].str.split(\",\").str.get(1).str.contains('Master.')))\n                        & (X_all['Age'].isnull()) & (X_all['Sex']=='male')].index\n\n#Impute the men 'NaN' age\nX_all.loc[men_age_nan_ind,'Age'] = men_mean_age\nprint('Number of entries that are left with unknown age is:',\n      len(X_all[X_all['Age'].isnull()]))\nX_all.loc[men_age_nan_ind]","b1eb8a26":"X_all[X_all.Age.isnull()]","1b2b750e":"#Generate the 'women_data' table and record the relevant statistical info\nwomen_data = X_all[X_all['Sex']=='female']\nwomen_mean_age = women_data.Age.mean()\n\n#Replace the 'Nan' age entry with the mean women's age\nX_all.loc[980, 'Age'] = women_mean_age\nX_all.loc[980, :]","46ae7f3d":"X_all[X_all.Fare.isnull()]","4f5d7146":"#Create a list of 3rd class passengers embarked in 'S'\nfare_imputation_data = X_all[(X_all.Pclass==3)&(X_all.Embarked=='S')]\n\n#Impute the 'NaN' fare with the median fare value\nX_all.loc[1044, 'Fare'] = fare_imputation_data.Fare.median()\nX_all.isnull().sum()","51f44772":"#Fully imputed train data\nX_train_imputed = X_all.iloc[:891].drop(['Name','Ticket'], axis = 1)\nX_train_imputed\n\n#Fully imputed test data\nX_test_imputed = X_all.iloc[891:].drop(['Name', 'Ticket'], axis = 1)\nX_test_imputed","7c839db5":"#Load required packages\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\n\n#Create lists of numerical and categorical columns\nnum_cols = [c for c in X_train_imputed.columns \n            if X_train_imputed[c].dtype in ['int64','float64']]\ncat_cols = [c for c in X_train_imputed.columns if \n            X_train_imputed[c].dtype in ['object','bool']]\n\n#Categorical and numerical columns transformation pipelines\ncat_transformer_onehot = Pipeline(steps=[\n    ('onehot_transf', OneHotEncoder(handle_unknown='ignore'))\n])\nnum_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n\n#Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer([\n    ('categoricals', cat_transformer_onehot, cat_cols),\n    ('numericals', num_transformer, num_cols)],\n    remainder = 'passthrough')","94270da9":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel_RFC = RandomForestClassifier()\n\n#Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_RFC = Pipeline(steps=[('preprocessor', preprocessor), \n                                  ('RFC_model', model_RFC)])\n\n#Hyperparameter tuning implementation\nparam_grid_RFC = {\n    'RFC_model__n_estimators': [100, 235, 300], \n    'RFC_model__max_depth': [10, 30, 100], \n    'RFC_model__min_samples_split': [5, 15, 25],\n    'RFC_model__random_state': [10],\n}\n\nsearchCV_RFC = GridSearchCV(my_pipeline_RFC, \n                            param_grid=param_grid_RFC,\n                            cv=5, scoring='accuracy',n_jobs=-1)\n\nsearchCV_RFC.fit(X_train_imputed, y_train)\nprint('Best parameters for the Random Forest Classifier: \\n',\n      searchCV_RFC.best_params_) \nprint('Best accuracy score for the Random Forest Classifier: ',\n      searchCV_RFC.best_score_)","1fbb02be":"from xgboost import XGBClassifier\n\nmodel_XGBC = XGBClassifier()\n\n#Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_XGBC = Pipeline(steps=[('preprocessor', preprocessor), \n                                   ('XGBC_model', model_XGBC)])\n\n#Hyperparameter tuning implementation \n#To speed up this block, non-optimal parameters are commented out\nparam_grid_XGBC = {\n    'XGBC_model__eval_metric': ['error'],\n    'XGBC_model__objective': ['binary:logistic'],\n    'XGBC_model__tree_method': ['gpu_hist'],\n    'XGBC_model__n_estimators': [100], #[50, 100, 250, 350]\n    'XGBC_model__learning_rate': [0.1], #[0.03, 0.1, 0.3, 0.6] \n    'XGBC_model__max_depth': [6], #[6, 9]\n    'XGBC_model__min_child_weight': [2], #[1, 2, 3] \n    'XGBC_model__reg_alpha': [1], #[0.03, 1, 3]\n    'XGBC_model__reg_lambda': [0.01], #[0.003, 0.01, 0.03, 0.1] \n    'XGBC_model__random_state': [10],\n}\n\nsearchCV_XGBC = GridSearchCV(my_pipeline_XGBC, param_grid=param_grid_XGBC,\n                             cv=5, scoring='accuracy',n_jobs=-1)\n\nsearchCV_XGBC.fit(X_train_imputed, y_train)\nprint('Best parameters for the XGBoost Classifier: \\n',\n      searchCV_XGBC.best_params_) \nprint('Best accuracy score for the XGBoost Classifier: ', \n      searchCV_XGBC.best_score_)","57cd2bae":"from sklearn.linear_model import LogisticRegression\n\nmodel_LRC = LogisticRegression()\n\n#Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_LRC = Pipeline(steps=[('preprocessor', preprocessor), \n                                  ('LRC_model', model_LRC)])\n\n#Hyperparameter tuning implementation\nparam_grid_LRC = {\n    'LRC_model__max_iter': [10, 50, 200, 1000],\n    'LRC_model__penalty': ['l1', 'l2'], \n    'LRC_model__C': [0.03, 0.1, 0.3, 1, 3, 10, 30], \n    'LRC_model__solver': ['liblinear'],\n    'LRC_model__random_state': [10],\n}\n\nsearchCV_LRC = GridSearchCV(my_pipeline_LRC, param_grid=param_grid_LRC,\n                            cv=5, scoring='accuracy',n_jobs=-1)\n\nsearchCV_LRC.fit(X_train_imputed, y_train)\nprint('Best parameters for the LogisticRegression Classifier: \\n',\n      searchCV_LRC.best_params_) \nprint('Best accuracy score for the LogisticRegression Classifier: ', \n      searchCV_LRC.best_score_)","1c9d0afa":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel_KNNC = KNeighborsClassifier()\n\n#Bundle preprocessing and modeling code in a pipeline\nmy_pipeline_KNNC = Pipeline(steps=[('preprocessor', preprocessor), \n                                   ('KNNC_model', model_KNNC)])\n\n#Hyperparameter tuning implementation\nparam_grid_KNNC = {\n    'KNNC_model__n_neighbors': [3, 5, 7, 9, 11], \n    'KNNC_model__weights': ['uniform', 'distance'], \n    'KNNC_model__leaf_size': [1, 3, 10, 30],\n    'KNNC_model__p': [1, 2],\n}\n\nsearchCV_KNNC = GridSearchCV(my_pipeline_KNNC, param_grid=param_grid_KNNC,\n                             cv=5, scoring='accuracy',n_jobs=-1)\n\nsearchCV_KNNC.fit(X_train_imputed, y_train)\nprint('Best parameters for kNN Classifier: \\n',searchCV_KNNC.best_params_) \nprint('Best accuracy score for kNN Classifier: ', searchCV_KNNC.best_score_)","80a2c360":"my_pipeline_final = searchCV_RFC.best_estimator_\npredictions = my_pipeline_final.predict(X_test_imputed)","afe18889":"#Generate output\noutput = pd.DataFrame({'PassengerId': X_test_imputed.index, \n                       'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","af430528":"Even though the XGBoost classifier provides the best accuracy on the cross validation data, the RandomForest classifier performs best on the test data. ","ed1ea8e6":"### <a id='Link-SecModel-Training-XGBC'>Gradient boosting classifier<\/a>","234497ba":"## <a id='Link-SecModel-Training'>Model training<\/a>\n\nIn this subsection, we will fit different machine learning algorithms with our preprocessed data and choose the optimal parameters for each model. We will consider the following models:\n* Random forest classifier\n* Gradient boost classifier\n* Logistic regression classifier\n* K-nearest neighbors classifier\n\n### <a id='Link-SecModel-Training-RFC'>Random forest classifier<\/a>","c5784f74":"This means that there is a slight association between known (unknown) age and survival rate. Therefore, before imputing the unknown ages with numerical values, we will add an extra (boolean) column indicating that the age was missing, as discussed in one of the [Intermediate Machine Learning courses](https:\/\/www.kaggle.com\/alexisbcook\/missing-values). There is a chance that this extra column may improve the accuracy of our model. Below, we will work with training and test data because both contain 'NaN' age values.","48e7d744":"Among the 1st class passengers in the zone B who payed between 60 and 80 for their tickets, 6 embarked in 'C', while only 2 embarked in 'S'. We conclude that most likely the two passengers of our interest belonged to the 'Embarked' == 'C' category. To check this assumption further, we study the ticket price distribution for the 1st class passengers embarked in 'C' and 'S', independently of their lodging zone.","ab537495":"**2. The 'Age'-'Survived' association.**\n\nLet's take a closer look at passengers age data and check if the evaluated 0.01 association (displayed in [Fig. 1](#Fig1Link)) with the survaval rate makes sense. In order to visualize the survival vs age data, we define 6 age groups: 0-14, 14-28, ..., 70-84. (Check out [this turotial](https:\/\/www.kaggle.com\/alexisbcook\/bar-charts-and-heatmaps) to learn more about data visualization with python.) ","789ef2d0":"# <a id='Link-SecSubm'>Submission<\/a> \n\nFinally, we prepare our predictions for a submition to the [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) competition. \ud83e\udd73\ud83c\udf89","fe399a47":"Before imputing the 'NaN' entries in the 'Age' column, we will analyze below the people's titles provided in the 'Name' column. As a result of this, one can clearly see that the passengers with the 'NaN' age belonged to 4 relevant age groups (with only one exception). These age groups are: \n* Boys (title 'Master.')\n* Married Women (title 'Mrs.')\n* Single Women (title 'Miss.')\n* Men (title 'Mr.')\n\nWe will impute the corresponding 'NaN' values depending on the person's age group.","f3342e03":"Given this, we decide to mark the 'NaN' cabin values (in training and testing datasets) as 'unknown' and this marking should help our model to lower the survival chances for entries with the 'NaN' values in the 'Cabin' column.","e49c6f93":"### <a id='Link-SecModel-Training-LRC'> Logistic regression classifier<\/a>","daa63cbe":"## <a id='Link-SecDataExpl-GenAss'>General assesment of feature associations<\/a>\n\nFirst, let's peform a general assesment of assosiation (correlation) between different features.\n\nSince our dataset contains both numerical and categotical types of features, we will need to analyze 3 different types of association \/ correlation: a) categorical-categorical association, b) categorical-numerical association, c) numerical-numerical correlation. To estimate the corresponding associations, we will use the [dython](http:\/\/shakedzy.xyz\/dython\/) library (see [this link](https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9) and [this kaggle notebook](https:\/\/www.kaggle.com\/shakedzy\/alone-in-the-woods-using-theil-s-u-for-survival) for more details about this library), which evaluates the following measures:\n\n* The _categorical-categorical_ feature association (e.g. 'Sex'-'Survived') is computed using the _Theil's U_ measure.\n* The _numerical-categorical_ feature association (e.g. 'Fare'-'Survived') is computed using the _correlation ratio_ measure called Eta. \n* The _numerical-numerical_ correlation (e.g. 'Fare'-'Age') is computed using the _Pearson's R_ measure.\n<a id='Fig1Link'><\/a>","b7de5f1a":"<h2><center><font size='6'>Top 7% Titanic Dataset Notebook<\/font><\/center><\/h2>\n\nTable Of Contents  \n=\n\n1. <a href='#Link-SecIntro'>Introduction<\/a>  \n2. <a href='#Link-SecDataPrep'>Data preparation<\/a>  \n3. <a href='#Link-SecDataExpl'>Data exploration<\/a>   \n - <a href='#Link-SecDataExpl-Summary'>Data summary<\/a>   \n - <a href='#Link-SecDataExpl-GenAss'>General assesment of feature associations<\/a>   \n4. <a href='#Link-SecImp'>Data imputation<\/a>\n - <a href='#Link-SecImp-Emb'>Embarked feature<\/a>\n - <a href='#Link-SecImp-Cabin'>Cabin feature<\/a>\n - <a href='#Link-SecImp-Age'>Age feature<\/a>\n - <a href='#Link-SecImp-Fare'>Fare feature<\/a>\n5. <a href='#Link-SecModel'>Model training, selection, and predictions<\/a>\n - <a href='#Link-SecModel-DataPrepr'>Data preprocessing<\/a>\n - <a href='#Link-SecModel-Training'>Model training<\/a><br>\n  a) <a href='#Link-SecModel-Training-RFC'>Random forest classifier<\/a><br>\n  b) <a href='#Link-SecModel-Training-XGBC'>Gradient boosting classifier<\/a><br>\n  c) <a href='#Link-SecModel-Training-LRC'>Logistic regression classifier<\/a><br>\n  d) <a href='#Link-SecModel-Training-KNNC'>k-nearest neighbors classifier<\/a>\n - <a href='#Link-SecModel-Selection'>Model selection and predictions<\/a>\n6. <a href='#Link-SecSubm'>Submission<\/a>  ","1d6f3719":"These plots don't bring a definitive answer for passengers who payed ~80 for their tickets, so we stick to our previous conclusion and impute the corresponding 'NaN' values with 'C'.","598d57c6":"Below, we take a closer look at the entries with the 'NaN' value in the 'Embarked' column. We notice that both passengers had 1st class tickets, were lodged in zone B, and payed 80 for their tickets.","7c38d5ff":"**3. The 'Pclass'-'Survived' association.**\n\nLet's take a closer look at the 'Pclass' column and check if the corresponding association with the 'Survival' column makes sense. ","a1871b16":"Perfect!!! As seen above, _all the missing values are now imputed and a new feature is generated._ To fully complete the preprocessing stage, we will drop the 'Name' and 'Ticket' features from our imputed dataset and split it back into the training and testing datasets.","b60369f8":"One can see that there seem to be some association between the survival rate and age groups. More specifically: \n* Children (0-14) are more likely to survive than other age groups. \n* Seniors (70-84) seem to have the smallest survival chances. Though, due to limited statistics, elderly's survival rate has large uncertainties. \n* Various adult age groups have very similar chances for survival. \n\nThese results seem to contradict with the association coefficient (0.01) between the 'Age' and 'Survival' columns displayed in [Fig. 1](#Fig1Link). This is because the associations method of the dython.nominal module by default replaces all the 'NaN' values in the 'Age' column with 0. As a result, all the 'NaN' age values were prescribed to the 0-14 age group, thereby lovering the survival chances for children. If we now remove all the 'NaN' values in the 'Age' column, the association coefficient should increase. The visualisation below confirm this assumption.","e8af64fa":"We are interested in the output of the first column that shows assotiations between the survival rate and the rest of the features. There are several interesting observations that can be made:\n1. There is a very strong association between the 'Ticket' and 'Survived' columns.   \n2. The association between the 'Age' and 'Survived' features is surprisingly low (only 0.01). This needs to be checked.  \n3. The association between the 'Pclass' and 'Survived' seem to be quite low, too. On the other hand, the association between the 'Fare' and 'Survived' columns is much stronger. This needs to be understood.\n\n\n**1. The 'Ticket'-'Survived' association.**\n\nAs it is shown below, the reason for such a high association (0.86) is due to 681 unique entries (out of 891) in the 'Ticket' column, resulting in a strong association between these columns. Most passengers had unique ticket numbers, therefore knowledge of their ticket number uniquely determines their survival.  ","cb853ee0":"The association seem to be quite strong. The possible reason why the association coefficient in Fig. 4 is low is because we treat the pessenger class as a nominal categorical column. In fact, 'Pclass' is an ordinal column. It might be better to treat this column as numerical, so that the Eta correlation ratio is evaluated instead of the Thiel's U measure. Below, we can see that, indeed, the corresponding association coefficient increases when we treat the 'Pclass' column as numerical. The same observation can be made for the 'SibSp' and 'Parch' columns.","961b8582":"## <a id='Link-SecModel-Selection'>Model selection and predictions<\/a>\n\nBelow, we provide a table that summarizes the Accuracy and F1 score predictions of 4 different models, after testing these models on cross validation data.","9a067499":"| Model  | Accuracy | F1 score |\n| --- | --- | -- |\n| Random forest classifier | 0.836 | 0.765 |\n| Gradient boosting classifier | 0.844 | 0.783 |\n| Logistic regression classifier | 0.800 | 0.733 |\n| kNN classifier | 0.820 | 0.749 |\n","448d4a62":"### <a id='Link-SecModel-Training-KNNC'> k-nearest neighbors classifier<\/a>","2a65acca":"# <a id='Link-SecImp'>Data imputation<\/a>\n\n## <a id='Link-SecImp-Emb'>Embarked feature<\/a>\n\nWe will start with imputing the 'Embarked' column.\n\nFirst, we visualize the corresponding data and notice that there are only 2 'NaN' values in the 'Embarked' column.","82a33dc1":"# <a id='Link-SecModel'>Model training, selection, and predictions<\/a>\n\n## <a id='Link-SecModel-DataPrepr'>Data preprocessing<\/a>\n\nBefore we train our model and make predictions, we will first preprocess our categorical and numerical columns using the ColumnTransformer class of the `sklearn.compose` module and prepare the data for fitting a machine learning model. (Check out [this turotial](https:\/\/www.kaggle.com\/alexisbcook\/pipelines) to learn more about keeping your data preprocessing and modeling code organized) ","2ebd3b96":"In the same time, the average survival rate in the training dataset, as visualized below, is 38.4%.","bfc91b02":"Let us now investigate the ticket price distribution for all 1st class passengers lodged in zone B.","bb96f408":"# <a id='Link-SecDataExpl'>Data exploration<\/a>  \n\n## <a id='Link-SecDataExpl-Summary'>Data summary<\/a>","6596a772":"## <a id='Link-SecImp-Age'>Age feature<\/a>\n\nNow, we will proceed to impute the missing values in the 'Age' column.\n\nFirst, we check below if passengers with 'NaN' values in the 'Age' column had different survival chances than those with known age.","a537eab9":"Yahoo, we have finally imputed all the 'NaN' entries in the 'Age' column!!! The only entry to be fixed now is a single 'NaN' value in the 'Fare' column, which is coming from the testing dataset.\n\n## <a id='Link-SecImp-Fare'>Fare feature<\/a>\n\nLet's fix the only 'NaN' value in the 'Fare' column right away, taking into account that the corresponding passenger had a 3rd class ticket and embarked in (S)outhampton.","cf289562":"# <a id='Link-SecIntro'>Introduction<\/a>  \n\nThis is <u>my first<\/u> public notebook on Kaggle! \n\nHere we will explore and analyze the famous Titanic Dataset using Python libraries such as `pandas`, `matplotlib`, `seaborn`, and `dython`. After that, we will prepare the dataset for model training, consider four different ML models from the popular `sklearn` and `xgboost` libraries, and then assess the accuracy of each model using the cross validation data. Ultimately, we will choose the best ML model, apply it the test dataset to predict which passengers of Titanic survived, and prepare the predictions for a submission in the [Titanic - Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) competition. This notebook achieves 79.4% accuracy on the test set.\n\n\ud83d\ude00 !!!Any comments or suggestions will be appreciated!!! \ud83d\ude00","bed1b50a":"# <a id='Link-SecDataPrep'>Data preparation<\/a>  ","5b41f13d":"## <a id='Link-SecImp-Cabin'>Cabin feature<\/a>\n\nIn [Section 3.1](#Link-SecDataExpl-Examine), we observed that the number of 'NaN' entries in the 'Cabin' column of the training dataset is huge (~77%). The initial guess would be to drop entirely this column from our analysis. However, the code below suggests that the chances of survival for a person with a 'NaN' value in the 'Cabin' column are considerably lower than those for a person with a known cabin.","a44a2133":"One 'unconventional' missing entry in the 'Age' column still needs to be fixed. As it can be seen from the table below, the corresponding passenger's title is 'Ms.'. Therefore, we will impute this entry with the mean women's age. "}}