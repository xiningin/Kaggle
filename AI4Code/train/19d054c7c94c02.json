{"cell_type":{"ccff24f3":"code","09e588a5":"code","d9d698bd":"code","8e451df3":"code","e46e3e4c":"code","b4c68b80":"code","eb3aa432":"code","e89ee87d":"code","449c78e1":"code","4a59bf34":"code","bcabaa0c":"code","344a2c61":"code","2026b6c5":"code","a4dbfbcc":"code","7a5a76e1":"code","58dcf277":"code","cf8123aa":"code","e9095efa":"code","333c202e":"code","ccdf44fb":"code","f2a1a962":"code","ba98409d":"code","aedb4966":"code","19592554":"code","be0598b3":"code","30c81584":"code","d867a68f":"code","972e36be":"code","883415fa":"code","f457218c":"code","d02bb487":"code","0c59adab":"code","ed7b30df":"code","9873f673":"code","a9e515d0":"code","fcb761f8":"code","26569820":"code","21d12c3d":"code","af93a785":"code","076ddef4":"code","5bb5a618":"code","9cb9157b":"code","25b95fd1":"code","86b48b1d":"code","0eb70a14":"code","a2151fd7":"code","c6d21f59":"code","504f6a4f":"code","21b1b137":"code","48db852d":"code","0ad945be":"code","79e1da77":"code","bb28d3d5":"code","5b54da4b":"code","d94bd581":"code","21d7fd43":"code","deb3f0ec":"code","cff8501d":"code","72261287":"code","bcbb9458":"code","c09ed38e":"code","1f7aefa0":"code","abd74257":"code","92c7a8a2":"code","0b39a510":"code","4119d1d6":"code","5dec63d5":"code","bf22a895":"code","3bc07322":"code","e02e4c05":"code","fe7eab06":"code","ba82186d":"code","227533a9":"code","c111a387":"code","fb38560f":"code","4cdf570c":"code","8c231756":"code","992c0dce":"code","3fbdf845":"code","5d1ca542":"code","6ed3bc78":"code","b182e527":"code","970da05b":"code","fb7a24c5":"code","e1d4581c":"code","c1c49d12":"code","3297073a":"code","e0a9f730":"code","2a14a4f0":"code","fa2e7ef6":"code","c2f745bd":"code","5529086d":"code","274f5336":"markdown","527c2a3b":"markdown","b4454a08":"markdown","ecca4ded":"markdown","806ad9ba":"markdown","5f543429":"markdown","1c822ba3":"markdown","740b4357":"markdown","49ee0074":"markdown","ba621f24":"markdown","79565801":"markdown","f4d16ad3":"markdown","eb446c80":"markdown","75d603f8":"markdown","0c2ab93f":"markdown","2d9a435e":"markdown","c967e4e2":"markdown","2ec20dc6":"markdown","1a32460a":"markdown","1b6ec9b8":"markdown"},"source":{"ccff24f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","09e588a5":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","d9d698bd":"from sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","8e451df3":"df.drop(['order_id' , 'line_item_type_id'], axis = 1, inplace=True)","e46e3e4c":"df.head()","b4c68b80":"corr = df.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True)\nplt.show()","eb3aa432":"df['total_revenue'].sum()","e89ee87d":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\n","449c78e1":"#we can remove integration type as it has only one value and revenue share percent as that we have already used and \n#is only one single value as well\ndf.drop(['integration_type_id' , 'revenue_share_percent'], axis = 1, inplace=True)","4a59bf34":"corr = df.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","bcabaa0c":"# we can remove total impressions as well as that is account the same information as measurable impressions \n# also let us try to see if viewable\/measurable impressions are corellated to revenue or not \n\ndf['View\/measurable'] = df.apply(lambda x: weird_division(x['viewable_impressions'],x['measurable_impressions']) , axis=1)\n","344a2c61":"df.drop([ 'total_impressions'], axis = 1, inplace=True)","2026b6c5":"corr = df.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","a4dbfbcc":"df.nunique()","7a5a76e1":"df.head()","58dcf277":"df.fillna(0,  inplace = True)","cf8123aa":"df.isnull().sum()","e9095efa":"corr = df.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","333c202e":"df_numericals = df[['total_revenue','viewable_impressions','measurable_impressions','CPM','View\/measurable']]","ccdf44fb":"df_numericals.describe()","f2a1a962":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df_numericals)\nscaled_data = scaler.transform(df_numericals)","ba98409d":"corr = df_numericals.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","aedb4966":"sns.boxplot(df['total_revenue'] )","19592554":"sns.boxplot(df['CPM'] )","be0598b3":"df[df['total_revenue']<0].head(10)\n#we have a value which has a negative revenue not sure what it means or if it is even valid ! removing some outliers ","30c81584":"#remove the outliers \n\ndf = df[df['CPM'].between(df['CPM'].quantile(.05), df['CPM'].quantile(.95))]","d867a68f":"df.reset_index(inplace= True)","972e36be":"df[df['total_revenue']<0].head(10)\n#we had a value which has a negative revenue which is removed and extreme values are removed as well ","883415fa":"corr = df.corr()\nplt.figure(figsize=(14,10))\nsns.heatmap(data=corr,vmin=0, vmax=1, cmap=\"YlGnBu\",  square=True, annot= True)\nplt.show()","f457218c":"monetization_channel_list = pd.get_dummies(df.monetization_channel_id, prefix='monetization_channel_', drop_first=True)\n","d02bb487":"from sklearn.decomposition import PCA\npca2 = PCA(n_components=2)\npca2.fit(monetization_channel_list)\nX_transformed_mone_list = pd.DataFrame(pca2.transform(monetization_channel_list))\n\nvariance = pca2.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(pca2.explained_variance_ratio_, decimals=3)*100)\nvar\n#with just 2 PC we are able to get 95% variance explained so let us add the PC and remove the column","0c59adab":"df.shape","ed7b30df":"#with just 2 PC we are able to get 95% variance explained so let us add the PC and remove the column\nX_transformed_mone_list = X_transformed_mone_list.add_prefix('Monetization_')\ndf = pd.concat([df, X_transformed_mone_list], axis=1)","9873f673":"df.columns","a9e515d0":"X_transformed_mone_list.shape","fcb761f8":"#performing one hot encoding \n\ndevice_category_list = pd.get_dummies(df.device_category_id, prefix='Ad_type', drop_first=True)","26569820":"# performing PCA on top of one hot encoding \n\npca2 = PCA(n_components=2)\npca2.fit(device_category_list)\nX_transformed_devise = pd.DataFrame(pca2.transform(device_category_list))\n\nvariance = pca2.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(pca2.explained_variance_ratio_, decimals=3)*100)\nvar\n#with just two PC we are able to get 99% variance explained so let us add the PC and remove the column","21d12c3d":"#with just two PC we are able to get 99% variance explained so let us add the PC and remove the column\n\nX_transformed_devise = X_transformed_devise.add_prefix('Devise_')\ndf = pd.concat([df, X_transformed_devise], axis=1)","af93a785":"# performing one hot encoding\n\nsite_list = pd.get_dummies(df.site_id, prefix='Site', drop_first=True)","076ddef4":"#Checking if PCA helps in reduction of the number of features (If we are able to explain above 95% variance we will go ahead)\n\npca2 = PCA(n_components=7)\npca2.fit(site_list)\nX_transformed_site = pd.DataFrame(pca2.transform(site_list))\n\nvariance = pca2.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(pca2.explained_variance_ratio_, decimals=3)*100)\nvar","5bb5a618":"#only able to reduce 3 features so let us leave all the columns as is as one hot encoded\ndf = pd.concat([df, site_list], axis=1)","9cb9157b":"advertiser_list = pd.get_dummies(df.advertiser_id, prefix='advertiser_', drop_first=True)","25b95fd1":"pca2 = PCA(n_components=10)\npca2.fit(advertiser_list)\nX_transformed_adver = pd.DataFrame(pca2.transform(advertiser_list))\n\nvariance = pca2.explained_variance_ratio_ #calculate variance ratios\n\nvar=np.cumsum(np.round(pca2.explained_variance_ratio_, decimals=3)*100)\nvar\n#with just 10 PC we are able to get 99% variance explained so let us add the PC and remove the column","86b48b1d":"#with just 10 PC we are able to get 99% variance explained so let us add the PC and remove the column\nX_transformed_adver = X_transformed_adver.add_prefix('Advert_')\ndf = pd.concat([df, X_transformed_adver], axis=1)","0eb70a14":"df.describe()","a2151fd7":"df.columns","c6d21f59":"df_ad_unit = df.groupby(['ad_unit_id'])['CPM'].agg(['sum', 'idxmax'])\ndf_ad_unit['sum'] = df_ad_unit['sum'].astype('int64') \ndf_ad_unit.sort_values(by='sum',ascending=False, inplace = True)\n#only certain advertisers give a lot of money ","504f6a4f":"len(df_ad_unit)","21b1b137":"df_ad_unit['ad_unit_id'] = df_ad_unit.index\ndf_ad_unit.index = range(0, 132)\ndf_ad_unit['ad_unit_rank'] =  range(0, 132)\ndf_ad_unit.head()","48db852d":"df = pd.merge(df,\n                 df_ad_unit[['ad_unit_rank', 'ad_unit_id']],\n                 on='ad_unit_id') ","0ad945be":"df.head()","79e1da77":"df_geo = df.groupby(['geo_id'])['CPM'].agg(['sum', 'idxmax'])\ndf_geo['sum'] = df_geo['sum'].astype('int64') \ndf_geo.sort_values(by='sum',ascending=False, inplace = True)\n#only certain advertisers give a lot of money ","bb28d3d5":"len(df_geo)\ndf_geo['geo_id'] = df_geo.index\ndf_geo.index = range(0, 219)\ndf_geo['geo_rank'] =  range(0, 219)\ndf_geo.head()","5b54da4b":"df = pd.merge(df,\n                 df_geo[['geo_rank', 'geo_id']],\n                 on='geo_id') ","d94bd581":"sns.lineplot(data=df_geo['sum'])","21d7fd43":"df_date = df.groupby(['date'])['CPM'].agg(['sum', 'idxmax'])\ndf_date['sum'] = df_date['sum'].astype('int64') \ndf_date.head(30)","deb3f0ec":"plt.figure(figsize=(14,10))\nsns.lineplot(data = df_date, x=df_date.index, y = 'sum' )\n\nplt.xticks(rotation=70)\nplt.show()","cff8501d":"df.columns","72261287":"df2 = df[df['total_revenue'] !=0.0 ]","bcbb9458":"from sklearn.model_selection import train_test_split\nX = df[['View\/measurable','measurable_impressions', 'viewable_impressions','ad_type_id','Monetization_0', 'Monetization_1', 'Advert_0', 'Advert_1', 'Advert_2', 'Advert_3', 'Advert_4', 'Advert_5',\n       'Advert_6', 'Advert_7', 'Advert_8', 'Advert_9', 'Devise_0', 'Devise_1', 'geo_rank', 'Site_343', 'Site_344', 'Site_345', 'Site_346', 'Site_347', 'Site_348',\n       'Site_349', 'Site_350', 'Site_351', 'ad_unit_rank']]\ny = df['CPM']\n#y = df['total_revenue']\n\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns= X.columns)\n\nX.fillna(0, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","c09ed38e":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)","1f7aefa0":"\npredictions = lm.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\n# Perfect predictions\nplt.plot(y_test,y_test,'r')","abd74257":"from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\nnp.sqrt(mean_squared_error(y_test,predictions))","92c7a8a2":"df['CPM'].mean()","0b39a510":"#trying without value where rev is 0\ndf2 = df[df['total_revenue'] !=0.0 ]","4119d1d6":"len(df2)","5dec63d5":"from sklearn.model_selection import train_test_split\nX = df2[['View\/measurable','measurable_impressions', 'viewable_impressions','ad_type_id','Monetization_0', 'Monetization_1', 'Advert_0', 'Advert_1', 'Advert_2', 'Advert_3', 'Advert_4', 'Advert_5',\n       'Advert_6', 'Advert_7', 'Advert_8', 'Advert_9', 'Devise_0', 'Devise_1', 'geo_rank', 'Site_343', 'Site_344', 'Site_345', 'Site_346', 'Site_347', 'Site_348',\n       'Site_349', 'Site_350', 'Site_351', 'ad_unit_rank']]\ny = df2['CPM']\n#y = df['total_revenue']\n\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns= X.columns)\n\nX.fillna(0, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","bf22a895":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train,y_train)","3bc07322":"\npredictions = lm.predict( X_test)\nplt.scatter(y_test,predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\n# Perfect predictions\nplt.plot(y_test,y_test,'r')","e02e4c05":"from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score\nnp.sqrt(mean_squared_error(y_test,predictions))","fe7eab06":"df2['CPM'].mean()","ba82186d":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.optimizers import Adam","227533a9":"X.shape","c111a387":"#tensor flow\nmodel = Sequential()\n\nmodel.add(Dense(29,activation='relu'))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(8,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mse')\n","fb38560f":"model.fit(x=X_train.values,y=y_train.values,\n         validation_data=(X_test,y_test.values),\n         batch_size=128,epochs=400)","4cdf570c":"predictions = model.predict(X_test)\nmean_absolute_error(y_test,predictions)\n","8c231756":"np.sqrt(mean_squared_error(y_test,predictions))","992c0dce":"# Our predictions\nplt.scatter(y_test,predictions)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r')","3fbdf845":"#trying without value where rev is 0\ndf2 = df[df['total_revenue'] !=0.0 ]","5d1ca542":"from sklearn.model_selection import train_test_split\nX = df2[['View\/measurable','measurable_impressions', 'viewable_impressions','ad_type_id','Monetization_0', 'Monetization_1', 'Advert_0', 'Advert_1', 'Advert_2', 'Advert_3', 'Advert_4', 'Advert_5',\n       'Advert_6', 'Advert_7', 'Advert_8', 'Advert_9', 'Devise_0', 'Devise_1', 'geo_rank', 'Site_343', 'Site_344', 'Site_345', 'Site_346', 'Site_347', 'Site_348',\n       'Site_349', 'Site_350', 'Site_351', 'ad_unit_rank']]\ny = df2['CPM']\n#y = df['total_revenue']\n\n\nscaler = StandardScaler()\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X), columns= X.columns)\n\nX.fillna(0, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","6ed3bc78":"X.shape","b182e527":"#tensor flow\nmodel = Sequential()\n\nmodel.add(Dense(29,activation='relu'))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(8,activation='relu'))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam',loss='mse')\n","970da05b":"model.fit(x=X_train.values,y=y_train.values,\n         validation_data=(X_test,y_test.values),\n         batch_size=128,epochs=400)","fb7a24c5":"predictions = model.predict(X_test)\nmean_absolute_error(y_test,predictions)\n","e1d4581c":"np.sqrt(mean_squared_error(y_test,predictions))","c1c49d12":"df2['CPM'].mean()","3297073a":"# Our predictions\nplt.scatter(y_test,predictions)\n\n# Perfect predictions\nplt.plot(y_test,y_test,'r')","e0a9f730":"single_value = df2.drop('CPM',axis=1).iloc[0]","2a14a4f0":"single_value = scaler.transform(single_value[['View\/measurable','measurable_impressions', 'viewable_impressions','ad_type_id','Monetization_0', 'Monetization_1', 'Advert_0', 'Advert_1', 'Advert_2', 'Advert_3', 'Advert_4', 'Advert_5',\n       'Advert_6', 'Advert_7', 'Advert_8', 'Advert_9', 'Devise_0', 'Devise_1', 'geo_rank', 'Site_343', 'Site_344', 'Site_345', 'Site_346', 'Site_347', 'Site_348',\n       'Site_349', 'Site_350', 'Site_351', 'ad_unit_rank']].values.reshape(-1, 29))","fa2e7ef6":"single_value","c2f745bd":"model.predict(single_value)","5529086d":"df2['CPM'].iloc[0]","274f5336":"SITE","527c2a3b":"Predicted value","b4454a08":"Approach 2 : Simple linear regression where the revenue is not 0 Takling this approach because i feel maybe because the ads were visible and still the revenue was 0 maybe because the data was not recorded for the same.","ecca4ded":"# Data descriptive analysis and Preprocessing\u00b6","806ad9ba":"Removing the transaction id related data","5f543429":"# Predicting on a brand new Bid\u00b6","1c822ba3":"Ad units\n\nI am not taking the approach of doing a PCA around Ad units as i tried that already and was not able to reduce it by much and explain the variance till 95% thus what i am doing now is - ranking the units based on the amount of CPM they corrospond to and create an ordinal ranking to make the algorithm's job easier rather than giving more that 90 PCA componenets or more than 100 one hot encoded features which reduced the algorithm's performace which i observed.\n\nSame approach is taken for GEO ID as well","740b4357":"approach 3 : THE MODEL PERFORMS POORLY","49ee0074":"Actual value","ba621f24":"APPROACH 2: AGAIN THE MODEL PERFORMS POORLY","79565801":"# Creating new dummy variables by one hot encoding and then PCA","f4d16ad3":"APPROACH 1 : THE MODEL PERFORMS POORLY","eb446c80":"# Modeling","75d603f8":"#  Objective and Theory\u00b6","0c2ab93f":"We were not able to accurately predict the values probably because the data does not hold a lot of information, Please let me know if i can try something else on the same. I would love to improve my model. For now i was not able to extract a lot of information from the data present. Looking forward for your thoughts on how to improve the same.","2d9a435e":"Approach 4: ANN with revenue not = 0","c967e4e2":"ADVERTISERS","2ec20dc6":"The main idea in this note book will be to first calculate the CPM which is the calculated based on the revenue generaed by the publisher(which is 1% of the actual revenue) thus calculating the CPM.\n\nOnce the CPM is calculated keeping that as a target variable we will try to create a model which will try to predict the reverse price ( We will take CPM roughly as the reverse price because any advertisers will try to reduce the cost and increase the value which they generate from Inventory and without the reverse price it will be most often [Rationally] lower than the actual Intrinsic value to the advertisers.)\n\nNow once we have the reverse price algorithm we know that any advertisers will be willing to go around 20% above the value in order to secure the slot if he sees value in the slot.","1a32460a":"# About the models","1b6ec9b8":"Approach 3 : Neural network approach with a simple dense network"}}