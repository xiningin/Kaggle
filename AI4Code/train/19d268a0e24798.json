{"cell_type":{"7f38c78d":"code","183d71db":"code","5023c1b4":"code","093dcd29":"code","b1350a2c":"code","dcf6a952":"code","10c4f948":"code","461f9224":"code","c17a32be":"code","48904915":"code","0fa4f48e":"code","14c02e52":"code","67fca5dd":"code","a9f836da":"code","b359eab5":"code","0f98b6c9":"code","0f25dc77":"code","51686546":"code","9951a981":"code","34187ce5":"code","583f05a1":"code","ce882dd0":"code","378631b3":"code","21a6ac34":"code","1d71645d":"code","36024397":"code","780f8edf":"code","ece46c26":"code","c3574c46":"code","04ba3061":"code","ffb2acd3":"markdown","837df668":"markdown","7466b072":"markdown","b4ab076a":"markdown","760679e2":"markdown","d81aa2dc":"markdown","1647aa38":"markdown","c848027b":"markdown","e2639293":"markdown","3c2103c8":"markdown","e84892f0":"markdown","24bc0d5e":"markdown","ff1a26c5":"markdown","7a60d9ef":"markdown","d2255d75":"markdown","f224e139":"markdown","a0a50eae":"markdown","9791379e":"markdown","c604d8ac":"markdown","94d8420b":"markdown","ed696457":"markdown","38283bf7":"markdown","099c015f":"markdown","1a0b01c2":"markdown","35dafb94":"markdown","441bf1ec":"markdown","fa697ba8":"markdown","c7a42ab4":"markdown","8403cd1e":"markdown"},"source":{"7f38c78d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","183d71db":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split","5023c1b4":"Test_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nTrain_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nprint(Test_df.shape)\nprint(Train_df.shape)\n","093dcd29":"values = {'Alley': 'None','BsmtQual': 'None','BsmtCond': 'None','BsmtExposure': 'None','BsmtFinType1': 'None','BsmtFinType2': 'None','FireplaceQu': 'None','GarageType': 'None','GarageQual': 'None','GarageCond': 'None','PoolQC': 'None','Fence': 'None', 'MiscFeature': 'None'}\n\nTest_df.fillna(value=values, inplace=True)\n\n#Next we will look at GarageYrBlt and GarageFinish, which have 159 missing variables each. \n#The missing data in these fields is due to the property having no garage. We can replace the GarageYrBlt and GarageFinish with 0. \n#This will ensure the model treats properties with and without garages separately given that we already have a fetaure GarageType \n#that identifies whether or not the property has a garage.\n\nTest_df.replace({'GarageYrBlt':np.nan,'GarageFinish':np.nan},0, inplace=True)\n\n# Fills the blank values in Exterior1st, Exterior2nd, MasVnrType with group modes\nTest_df.replace({'Exterior1st':np.nan},pd.Series.mode(Test_df['Exterior1st'])[0], inplace=True)\nTest_df.replace({'Exterior2nd':np.nan},pd.Series.mode(Test_df['Exterior2nd'])[0], inplace=True)\nTest_df.replace({'MasVnrType':np.nan},pd.Series.mode(Test_df['MasVnrType'])[0], inplace=True)\n\n#Blank entries in MasVnrArea occur where there is no MasVnrType there MasVnrArea should be set to 0\nTest_df.replace({'MasVnrArea':np.nan},0, inplace=True)\n\n#Fill the blank entries in MSZoning with  mode\nTest_df.replace({'MSZoning':np.nan},pd.Series.mode(Test_df['MSZoning'])[0], inplace=True)\n\n#Fill utilities with  modes \nTest_df.replace({'Utilities':np.nan},pd.Series.mode(Test_df['Utilities'])[0], inplace=True)\n\n#Fill blank entry  with 0 corresponding to no basement\nTest_df.replace({'BsmtFinSF1':np.nan},0, inplace=True)\nTest_df.replace({'BsmtFinSF2':np.nan},0, inplace=True)\nTest_df.replace({'BsmtUnfSF':np.nan},0, inplace=True)\nTest_df.replace({'TotalBsmtSF':np.nan},0, inplace=True)\nTest_df.replace({'BsmtFullBath':np.nan},0, inplace=True)\nTest_df.replace({'BsmtHalfBath':np.nan},0, inplace=True)\n\n#Fill blank entry for Electrical with mode\nTest_df.replace({'Electrical':np.nan},pd.Series.mode(Test_df['Electrical'])[0], inplace=True)\n\n#Fill blank entry for KitchenQual with mode\nTest_df.replace({'KitchenQual':np.nan},pd.Series.mode(Test_df['KitchenQual'])[0], inplace=True)\n\n#Fill blank entry for Functional with mode\nTest_df.replace({'Functional':np.nan},pd.Series.mode(Test_df['Functional'])[0], inplace=True)\n\n#Fill blank entry for GarageCars and GarageArea with mode and median respectively\nTest_df.replace({'GarageCars':np.nan},pd.Series.mode(Test_df['GarageCars'])[0], inplace=True)\nTest_df.replace({'GarageArea':np.nan},0, inplace=True)\n\n\n#Fill blank entry for SaleType with mode\nTest_df.replace({'SaleType':np.nan},pd.Series.mode(Test_df['SaleType'])[0], inplace=True)\n\n#Lot Frontage \nTest_df.replace({'LotFrontage':np.nan},Test_df.LotFrontage.median(), inplace = True)\n\n","b1350a2c":"values = {'Alley': 'None','BsmtQual': 'None','BsmtCond': 'None','BsmtExposure': 'None','BsmtFinType1': 'None','BsmtFinType2': 'None','FireplaceQu': 'None','GarageType': 'None','GarageQual': 'None','GarageCond': 'None','PoolQC': 'None','Fence': 'None', 'MiscFeature': 'None'}\n\nTrain_df.fillna(value=values, inplace=True)\n\n\nTrain_df.replace({'GarageYrBlt':np.nan,'GarageFinish':np.nan},0, inplace=True)\n\n# Fills the blank values in Exterior1st, Exterior2nd, MasVnrType with group modes\nTrain_df.replace({'Exterior1st':np.nan},pd.Series.mode(Train_df['Exterior1st'])[0], inplace=True)\nTrain_df.replace({'Exterior2nd':np.nan},pd.Series.mode(Train_df['Exterior2nd'])[0], inplace=True)\nTrain_df.replace({'MasVnrType':np.nan},pd.Series.mode(Train_df['MasVnrType'])[0], inplace=True)\n\n#Blank entries in MasVnrArea occur where there is no MasVnrType there MasVnrArea should be set to 0\nTrain_df.replace({'MasVnrArea':np.nan},0, inplace=True)\n\n#Fill the blank entries in MSZoning with  mode\nTrain_df.replace({'MSZoning':np.nan},pd.Series.mode(Train_df['MSZoning'])[0], inplace=True)\n\n#Fill utilities with  modes \nTrain_df.replace({'Utilities':np.nan},pd.Series.mode(Train_df['Utilities'])[0], inplace=True)\n\n#Fill blank entry  with 0 corresponding to no basement\nTrain_df.replace({'BsmtFinSF1':np.nan},0, inplace=True)\nTrain_df.replace({'BsmtFinSF2':np.nan},0, inplace=True)\nTrain_df.replace({'BsmtUnfSF':np.nan},0, inplace=True)\nTrain_df.replace({'TotalBsmtSF':np.nan},0, inplace=True)\nTrain_df.replace({'BsmtFullBath':np.nan},0, inplace=True)\nTrain_df.replace({'BsmtHalfBath':np.nan},0, inplace=True)\n\n#Fill blank entry for Electrical with mode\nTrain_df.replace({'Electrical':np.nan},pd.Series.mode(Train_df['Electrical'])[0], inplace=True)\n\n#Fill blank entry for KitchenQual with mode\nTrain_df.replace({'KitchenQual':np.nan},pd.Series.mode(Train_df['KitchenQual'])[0], inplace=True)\n\n#Fill blank entry for Functional with mode\nTrain_df.replace({'Functional':np.nan},pd.Series.mode(Train_df['Functional'])[0], inplace=True)\n\n#Fill blank entry for GarageCars and GarageArea with mode and median respectively\nTrain_df.replace({'GarageCars':np.nan},pd.Series.mode(Train_df['GarageCars'])[0], inplace=True)\nTrain_df.replace({'GarageArea':np.nan},0, inplace=True)\n\n\n#Fill blank entry for SaleType with mode\nTrain_df.replace({'SaleType':np.nan},pd.Series.mode(Train_df['SaleType'])[0], inplace=True)\n\n#Lot Frontage \nTrain_df.replace({'LotFrontage':np.nan},Train_df.LotFrontage.median(), inplace = True)\n\n","dcf6a952":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfig_dims = (10, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nd = Train_df['SalePrice']\nsns.set_style('darkgrid')\nsns.distplot(d)","10c4f948":"fig_dims = (10, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\n\n\nsns.set_style('darkgrid')\nsns.distplot(np.log(Train_df['SalePrice']))\n","461f9224":"fig_dims = (10, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nplt.scatter(Train_df['GrLivArea'],Train_df['SalePrice'])\nplt.ylabel('Sale Price')\nplt.xlabel(('Above ground living area'))","c17a32be":"#Strip out anomalous data points\nTrain_df = Train_df.loc[Train_df['GrLivArea']<4000]\n\n#Repeat scatter plot with anamalous results omitted\nfig_dims = (10, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\n\nplt.scatter(Train_df['GrLivArea'],Train_df['SalePrice'])\nplt.ylabel('Sale Price')\nplt.xlabel(('Above ground living area'))","48904915":"Full_df = pd.concat([Train_df,Test_df])\nprint(Full_df.shape)\nprint(Test_df.shape)\nprint(Train_df.shape)","0fa4f48e":"#create a function to generate dummies\ndef dum_df(feature,total_df):\n    dummy_feature = pd.get_dummies(total_df[feature],prefix=feature)\n    return dummy_feature","14c02e52":"#Convert all categorical variables to numeric variables so data set can be analysed\n\nfeatures = ['MSZoning',\n'Street',\n'Alley',\n'LotShape',\n'LandContour',\n'Utilities',\n'LotConfig',\n'LandSlope',\n'Neighborhood',\n'Condition1',\n'Condition2',\n'BldgType',\n'HouseStyle',\n'RoofStyle',\n'RoofMatl',\n'Exterior1st',\n'Exterior2nd',\n'MasVnrType',\n'ExterQual',\n'ExterCond',\n'Foundation',\n'BsmtQual',\n'BsmtCond',\n'BsmtExposure',\n'BsmtFinType1',\n'BsmtFinType2',\n'Heating',\n'HeatingQC',\n'CentralAir',\n'Electrical',\n'KitchenQual',\n'Functional',\n'FireplaceQu',\n'GarageType',\n'GarageFinish',\n'GarageQual',\n'GarageCond',\n'PavedDrive',\n'PoolQC',\n'Fence',\n'MiscFeature',\n'SaleType',\n'SaleCondition',\n'MSSubClass',\n'OverallQual',\n'OverallCond'\n]\n\n\ndf_initial = pd.DataFrame()\n\nfor f in features:\n    df_initial =  pd.concat([df_initial,dum_df(f,Full_df)], axis=1)\n    \nFull_dfDummies = pd.concat([df_initial,Full_df.drop(columns=features)], axis=1)\n\n# don't forget to apply log transformation on sales price \nFull_dfDummies['logSalePrice'] = np.log(Full_dfDummies['SalePrice'])\n","67fca5dd":"#from sklearn import preprocessing\n# Get list of numeric fetaures\n\nnames = ['LotFrontage',\n'LotArea',\n'YearBuilt',\n'YearRemodAdd',\n'MasVnrArea',\n'BsmtFinSF1',\n'BsmtFinSF2',\n'BsmtUnfSF',\n'TotalBsmtSF',\n'1stFlrSF',\n'2ndFlrSF',\n'LowQualFinSF',\n'GrLivArea',\n'BsmtFullBath',\n'BsmtHalfBath',\n'FullBath',\n'HalfBath',\n'BedroomAbvGr',\n'KitchenAbvGr',\n'TotRmsAbvGrd',\n'Fireplaces',\n'GarageYrBlt',\n'GarageCars',\n'GarageArea',\n'WoodDeckSF',\n'OpenPorchSF',\n'EnclosedPorch',\n'3SsnPorch',\n'ScreenPorch',\n'PoolArea',\n'MiscVal',\n'MoSold',\n'YrSold',\n]\n\n\n\n","a9f836da":"Full_dfDummies.shape","b359eab5":"Train_df2 = Full_dfDummies.loc[Full_dfDummies['SalePrice'].notnull()].drop(columns='Id')\nprint(Train_df2.isna().sum())\nTrain_df2.head()\nTrain_df2\nprint(Train_df2.shape)\n","0f98b6c9":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom scipy.stats import pearsonr\n\n\nprint(Train_df2.shape)\n\n#create a function that returns tuple of pearsons correlation, \n\ndef multivariate_pearsonr(X,y):\n    scores,pvalues = [],[]\n    for column in range (X.shape[1]):\n        cur_score,cur_p = pearsonr(X[:,column],y)\n        scores.append(abs(cur_score))\n        pvalues.append(cur_p)\n    return(np.array(scores),np.array(pvalues))\n    \n\n#Run a persons correlation against SalesPrice\nK=len(names)\n\nX = Train_df2[names]\ny = Train_df2['logSalePrice']\n#apply SelectKBest class to extract top k best features\nbestfeatures = SelectKBest(score_func=multivariate_pearsonr, k=K)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\ndfpvals = pd.DataFrame(fit.pvalues_)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores,dfpvals],axis=1)\nfeatureScores.columns = ['Specs','Score','Pvalue']  #naming the dataframe columns\n#print(featureScores.nlargest(K,'Score'))  #print k best features\n\n#Numeric_Feat = featureScores.nlargest(K,'Score')[['Specs','Score']]\n\n#set parameters \ncorrval = 0.3\npvalues = 0.05\n\nNumeric_Feat2 = featureScores.loc[featureScores['Pvalue']<pvalues]\nNumeric_Feat3 = Numeric_Feat2.loc[np.absolute(Numeric_Feat2['Score'])>corrval]\n\n\nFeatures_1 = list(Numeric_Feat3['Specs'])\nFeatures_1","0f25dc77":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom scipy.stats import pearsonr\n\nprint(Train_df2.shape)\n\nX = Train_df2.drop(columns=names).drop(columns=['logSalePrice','SalePrice'])\ny = Train_df2['SalePrice']\n\nK= len(X.columns)\n\n#apply SelectKBest class to extract top k best features\nbestfeatures = SelectKBest(score_func=chi2, k=K)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\ndfpvals = pd.DataFrame(fit.pvalues_)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores,dfpvals],axis=1)\nfeatureScores.columns = ['Specs','Score','Pvalue']  #naming the dataframe columns\n\n# Record top 10 features by predictive power \nchival = 10\npvalues = 0.05\n\n#cat_features = featureScores.nlargest(K,'Score')[['Specs','Score']]\n\n#eliminate insignificant results \ncat_features = featureScores.loc[(featureScores['Pvalue']<pvalues)]\n\n#Keep results over threshold\ncat_features2 = cat_features.loc[(cat_features['Score']>chival)]\n\n\n\nFeatures_2 = list(cat_features2['Specs'])\nFeatures_2\n\n","51686546":"Predictive_Features = Features_1+Features_2\nPredictive_Features","9951a981":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\nX = Train_df2[Predictive_Features]\nVIF_tab = calc_vif(X)\nprint(VIF_tab)\n\n\n","34187ce5":"remove = ['YearRemodAdd','GarageYrBlt','Exterior1st_CemntBd', 'BsmtQual_None',\n'BsmtCond_None',\n'BsmtCond_Po',\n'BsmtFinType1_None',\n'Electrical_Mix',\n'OverallCond_1',\n'1stFlrSF'\n\n]\n\n\nPredictive_Features2= list(set(Predictive_Features).difference(set(remove)))\n\n\n\nprint(len(Predictive_Features2))\nlen(Predictive_Features)\n\n#Look at VIF for shortlist of features\nX = Train_df2[Predictive_Features2]\nVIF_tab = calc_vif(X)\nprint(VIF_tab)\n\n","583f05a1":"from sklearn.preprocessing import StandardScaler\nScaler = StandardScaler()\n\nTrain_normalised_a = Train_df2.drop(columns=['SalePrice','logSalePrice'])\n\nTrain_normalised = Scaler.fit_transform(Train_normalised_a)\n\n\nTrain_normalised = pd.DataFrame(Train_normalised, columns=Train_normalised_a.columns)\n\n\nX = Train_normalised\ny = Train_df2[['logSalePrice','SalePrice']]\n\n\nTrain_df3 = pd.concat([X.reset_index(drop=True),y.reset_index(drop=True)], axis=1)\nTrain_df3","ce882dd0":"from sklearn.model_selection import train_test_split\n\nX = Train_df3[Predictive_Features2]\ny = Train_df3['logSalePrice']\nmeancrossvalscore = []\ntestscore = []\nalphavalues = [1,10,50,100,1000]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nfor a in alphavalues:\n    RidgeModel2 = Ridge(solver='saga', alpha=a)\n    Input=[('scale',StandardScaler()), ('model',RidgeModel2)]\n    Pipe_RR = Pipeline(Input)\n    Pipe_RR.fit(X_train,y_train)\n    crossvalscore = cross_val_score(Pipe_RR,X_train,y_train,scoring ='r2',cv=5).mean()\n    testing_score = Pipe_RR.score(X_test,y_test)\n    meancrossvalscore.append(crossvalscore)\n    testscore.append(testing_score)\n\n\nprint(meancrossvalscore)\nprint(testscore)\nprint(alphavalues)","378631b3":"#plt.plot(alphavalues,meancrossvalscore)\nplt.plot(alphavalues,testscore , label = 'Test Score')\nplt.plot(alphavalues,meancrossvalscore , label= 'Mean Cross Val')\n\nplt.ylabel('R^2')\nplt.xlabel('Alpha')\nplt.legend(loc=\"upper left\")\nplt.show()\n\nprint('Regularisation param that maximises testscore:' ,alphavalues[testscore.index(max(testscore))])\nprint('Regularisation param that maximises Mean cross val score:' ,alphavalues[meancrossvalscore.index(max(meancrossvalscore))])\n","21a6ac34":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\n\nX = (Train_df2.drop(columns=['logSalePrice', 'SalePrice']))\n#X = Train_df2[Predictive_Features]\ny = Train_df2['logSalePrice']\n\n\nMeanR2CrossVal = []\nTestScore = []\nmaxdepth  = [15,20,25,30]\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n\nfor i in maxdepth:\n    rf = RandomForestRegressor(criterion = 'mse', bootstrap=True, n_jobs =-1, max_features=len(Predictive_Features), n_estimators=1000, max_depth =i, random_state=42)\n    rf.fit(X_train,y_train)\n    meanscore = cross_val_score(rf,X_train,y_train,scoring='r2',cv=5).mean()\n    testing_Score  = rf.score(X_test,y_test) \n    MeanR2CrossVal.append(meanscore)\n    TestScore.append(testing_Score)\n    \n\nprint(maxdepth)\nprint(MeanR2CrossVal)\nprint(TestScore)\n\n","1d71645d":"plt.plot(maxdepth,TestScore , label = 'Test Score')\nplt.plot(maxdepth,MeanR2CrossVal, label= 'Mean Cross Val')\n#plt.legend(handles, labels)\n\nplt.ylabel('R^2')\nplt.xlabel('Max Depth')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n","36024397":"#added gradient boosted regressor to improve predictive performance\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n#Parameters for gridsearch\nparam_grid = {'model__learning_rate':[0.1,1,2],'model__loss':['lad'], 'model__n_estimators' :[1000]}\n\n\n#Pipeline will standardise input then fit model. \ngbr = GradientBoostingRegressor()\nInput=[('scale',StandardScaler()), ('model',gbr)]\ngbr2 = Pipeline(Input)\n\n#optimise with gridsearchCV\nmod_out = GridSearchCV(gbr2, param_grid = param_grid, scoring ='r2')\n\n#gbr2.get_params().keys()\n\nmod_out.fit(X_train,y_train)\n\n\n#Train xgboost model with parameters \n\n# cross validation on total train data\nprint('Test score:', mod_out.score(X_test,y_test))\n\n\n#run cross val with best grid search params\nparams = mod_out.best_params_\n\n#rename dictionary keys to correct model inputs\nparams['learning_rate'] = params.pop('model__learning_rate')\nparams['loss'] = params.pop('model__loss')\nparams['n_estimators'] = params.pop('model__n_estimators')\n\n\n#run cross validation on entire training set with paraemeters set to results of best gridsearch \ngbr3 = GradientBoostingRegressor(**params)\ncross_valscores = cross_val_score(gbr3,X,y,scoring='r2')\n                                    \n\n\n\n\n\n ","780f8edf":"plt.plot([1,2,3,4,5],cross_valscores, label= 'Cross Val')\n#plt.legend(handles, labels)\n\nplt.ylabel('R^2')\nplt.xlabel('Sample')\nplt.legend(loc=\"upper left\")\nplt.show()\n\n","ece46c26":"#fit model with optmised params to final train set\ngbr3.fit(X,y)","c3574c46":"\nTest_df2 = Full_dfDummies.loc[Full_dfDummies['SalePrice'].isnull()]\nX = Test_df2.drop(columns=['SalePrice','logSalePrice', 'Id'])\n\nTest_df2['SalePrice'] = np.exp(gbr3.predict(X))\nFinal_Out = Test_df2[['Id','SalePrice']]\n\n\n","04ba3061":"# import the modules we'll need\nfrom IPython.display import HTML\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = Final_Out\n\n# create a link to download the dataframe\ncreate_download_link(df)","ffb2acd3":"Let's look at the distribution for the target variable sales price.","837df668":"Fetaures with a variance inflation factor of greater than 5.5 (as a rule of thumb) have significant colinearity. I'll remove these from any future models. ","7466b072":"Plot the cross vaildation scores ","b4ab076a":"# Modelling","760679e2":"We will apply PCA on the dataset to reduce the number of dimensions. Since PCA works on distances, we will standardise the numeric variables in the dataset since they are all working on different scales. ","d81aa2dc":"There are some anomalous results that stand out in the data. There are some properties with extremley large GrLivArea at a low price that look like outliers. These could distort the patterns our models pick up. We will remove the outlier observations ","1647aa38":"Combine the list of categorical and numerical predictive features. ","c848027b":"Run pearsons correlation for all numeric variables against the SalesPrice. We will include all significant features with an absolute pearsons correlation value of >0.5","e2639293":"Next I'll  test for coliniearity between the predictive features using the variance inflation factor.","3c2103c8":"We will run our final random forest regressor on the testing data set to predict our house prices.","e84892f0":"Run Chi^2 test against sales price on categorical variables. Chi^2 test will give us an idea of the strength of the relationship between each categorical feature and the Sale Price. We will include significant features with a chi^2 value above 900.","24bc0d5e":"Create list of numeric variables, continous, discrete and ordinal","ff1a26c5":"Prior analysis shows that SalePrice is correlated with GrLivArea. I'll examine SalePrice vs GrLivArea in a scatter plot","7a60d9ef":"Some features are not truly missing, they have been marked NA because there is no suitable category for them. For example GarageType, there are many properties without a garage so this is marked NA. We will change the value of NA to remove these observations from our missing obs. To prevent data leakage across train and test will will replace the variables across the sets separately. \n\nFor the rest that are missing, we will impute with either their median or mode value depending on the type of feature (categorical or numeric)","d2255d75":"In this notebook I've built a model to predict house prices in Ames Iowa. The data provided has 79 features describing almost every aspect of a property. ","f224e139":"Sales price is right skewed. Outliers could impact accuracy of regression models. Therefore we will apply a log transformation on sale price to \"normalise\" the distribution","a0a50eae":"GBR performs better than random forest and linear regression. Finalise model by fitting to entire train set with optimised params","9791379e":"# **Advanced Regression Techniques**","c604d8ac":"We can try to fit a random forest regression on all features. We will vary the number of trees in the model to see where the optimum value lies. Again I'll use k-fold cross validation to test model variance and bias.","94d8420b":"I'm using a gradient boosted regressor as a third approach. It should outperform the rnadom forrest and the linear regression. I'll optmise paramters with a gridsearch. ","ed696457":"Read in data into pandas data frames","38283bf7":"The regression model(s) that will be applied later on require the data to be numerical. Therefore we will transform our categorical variables into dummy variables","099c015f":"First we split out the training data set.","1a0b01c2":"We will scale the variables that feed our linear regression model. ","35dafb94":"# Data Wrangling & Feature Engineering","441bf1ec":"# Transforming Data","fa697ba8":"# Feature Selection","c7a42ab4":"I'll build a ridge regression model using the selected features. I'll \"optimise\" by changing the hyper paramter alpha using K fold cross validation to test model variance and bias.","8403cd1e":"Do the same for the training set"}}