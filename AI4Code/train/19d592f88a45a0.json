{"cell_type":{"b1d076ff":"code","dd69383c":"code","ac97ae41":"code","be0c4138":"code","57b2839c":"code","c9db87ca":"code","cc75e15a":"code","bbcb781a":"code","d6b17a69":"code","378727d0":"code","0fa074ee":"code","1c1c2608":"code","df28823f":"code","3c9a5a93":"code","ac230fca":"code","2639079e":"code","2e6cb0ad":"code","e0b9d6c6":"code","35b9718d":"code","474139ca":"code","d8c5e7c2":"code","5aebe83c":"code","2cdec592":"code","689607cc":"markdown","0932dc25":"markdown","859c67a7":"markdown","f1be5183":"markdown","5a707ba5":"markdown","37fc2a55":"markdown","ab340965":"markdown","d760b8fe":"markdown","d4e928eb":"markdown","0fc4dbb5":"markdown"},"source":{"b1d076ff":"import timeit\nimport pickle\n\nimport numpy as np\nimport pandas as pd\n\n# scikit-learn 0.21.3  \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\n\nrandom_state = 77\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, auc, roc_auc_score\n\n# Create Model Pipeline\ndef create_pipeline(model_object):\n    return Pipeline([('MinMaxScaler', MinMaxScaler()), ('Model', model_object)])\n\n\ndef evaluation(model_name, y_test, y_pred, average=None, prefix=''):\n    average = None\n    result_dict = {}\n    \n    result_dict['MODEL'] = model_name\n    result_dict[f'{prefix}ACCURACY'] = accuracy_score(y_test, y_pred)\n    result_dict[f'{prefix}AUC'] = roc_auc_score(y_test, y_pred)\n    \n    no_of_class = len(set(y_test))\n    for i in range(no_of_class):\n        result_dict[f'{prefix}PRECISION_{i}'] = precision_score(y_test, y_pred, average=average)[i]\n        result_dict[f'{prefix}RECALL_{i}'] = recall_score(y_test, y_pred, average=average)[i]\n        result_dict[f'{prefix}F1_{i}'] = f1_score(y_test, y_pred, average=average)[i]\n\n    return result_dict","dd69383c":"train_df = pd.read_csv('..\/input\/nida-competition1\/train.csv')\ntrain_df['y'] = train_df['y'].map({'yes': 1, 'no': 0})\ncol_to_drop = [\n               'marital', 'education',\n               'contact',\n               'default', \n               'housing', \n               'loan',\n               'month', 'day_of_week', \n               'pdays', \n               'poutcome'\n]\n\ntrain_df.drop(col_to_drop, axis = 1, inplace = True)","ac97ae41":"train_df","be0c4138":"def one_hot_feature(df):\n    \n    data = df.copy()\n    \n    avg_duration_greater_than = []\n    for i in data['duration']:\n        if i >= 400:\n            avg_duration_greater_than.append(1)\n        else:\n            avg_duration_greater_than.append(0)\n            \n    avg_duration_less_than = []\n    for i in data['duration']:\n        if i <= 250:\n            avg_duration_less_than.append(1)\n        else:\n            avg_duration_less_than.append(0)\n            \n    med_duration_greater_than = []\n    for i in data['duration']:\n        if i >= 300:\n            med_duration_greater_than.append(1)\n        else:\n            med_duration_greater_than.append(0)\n    \n    med_duration_less_than = []\n    for i in data['duration']:\n        if i <= 200:\n            med_duration_less_than.append(1)\n        else:\n            med_duration_less_than.append(0)\n            \n    avg_euri_less_than =[]\n    for i in data['euribor3m']:\n        if i <= 3:\n            avg_euri_less_than.append(1)\n        else:\n            avg_euri_less_than.append(0)\n            \n    avg_euri_greater_than =[]\n    for i in data['euribor3m']:\n        if i >= 3.5:\n            avg_euri_greater_than.append(1)\n        else:\n            avg_euri_greater_than.append(0)\n            \n    med_euri_less_than =[]\n    for i in data['euribor3m']:\n        if i <= 2:\n            med_euri_less_than.append(1)\n        else:\n            med_euri_less_than.append(0)\n    \n    med_euri_greater_than =[]\n    for i in data['euribor3m']:\n        if i >= 3:\n            med_euri_greater_than.append(1)\n        else:\n            med_euri_greater_than.append(0)\n    \n    med_employed_less_than = []\n    for i in data['euribor3m']:\n        if i <= 5100:\n            med_employed_less_than.append(1)\n        else:\n            med_employed_less_than.append(0)\n    \n    med_employed_greater_than = []\n    for i in data['euribor3m']:\n        if i >= 5200:\n            med_employed_greater_than.append(1)\n        else:\n            med_employed_greater_than.append(0)\n                \n    data['avg_duration_greater_than'] = avg_duration_greater_than\n    data['med_duration_greater_than'] = med_duration_greater_than\n    data['avg_euri_less_than'] = avg_euri_less_than\n    data['med_euri_less_than'] = med_euri_less_than\n    data['med_employed_less_than'] = med_employed_less_than\n    \n    data['avg_duration_less_than'] = avg_duration_less_than\n    data['med_duration_less_than'] = med_duration_less_than\n    data['avg_euri_greater_than'] = avg_euri_greater_than\n    data['med_euri_greater_than'] = med_euri_greater_than\n    data['med_employed_greater_than'] = med_employed_greater_than\n    \n#     data['duration_per_emp.var.rate'] = (data['duration']\/data['emp.var.rate'])\n#     data['duration_per_cons.price.idx'] = (data['duration']\/data['cons.price.idx'])\n#     data['duration_per_cons.conf.idx'] = (data['duration']\/data['cons.conf.idx'])\n#     data['duration_per_euribor3m'] = (data['duration']\/data['euribor3m'])\n    \n#     data['emp.var.rate_mul_cons.price.idx'] = data['emp.var.rate']*data['cons.price.idx']\n#     data['emp.var.rate_mul_cons.conf.idx'] = data['emp.var.rate']*data['cons.conf.idx']\n#     data['emp.var.rate_mul_euribor3m'] = data['emp.var.rate']*data['euribor3m']\n    \n#     data['cons.price.idx_mul_euribor3m'] = data['cons.price.idx']*data['euribor3m']\n#     data['cons.price.idx_mul_cons.conf.idx'] = data['cons.price.idx']*data['cons.conf.idx']\n   \n    \n#     data['dpe_per_dpp'] = data['duration_per_emp.var.rate']*data['duration_per_cons.price.idx']\n#     data['dpe_per_dpc'] = data['duration_per_emp.var.rate']*data['duration_per_cons.conf.idx']\n    \n#     data['du_em_co_1'] = data['duration_per_emp.var.rate']*data['emp.var.rate_mul_cons.price.idx']*data['cons.price.idx_mul_euribor3m']\n#     data['du_em_co_2'] = data['duration_per_cons.price.idx']*data['emp.var.rate_mul_cons.conf.idx']*data['cons.price.idx_mul_cons.conf.idx']\n#     data['du_em_co_3'] = data['duration_per_cons.conf.idx']*data['emp.var.rate_mul_euribor3m']*data['cons.price.idx_mul_euribor3m']\n#     data['du_em_co_4'] = data['duration_per_cons.conf.idx']*data['emp.var.rate_mul_euribor3m']*data['cons.price.idx_mul_cons.conf.idx']\n    \n    \n    return data\n\ntrain_df = one_hot_feature(train_df)","57b2839c":"train_df.columns","c9db87ca":"print(train_df.shape)\nprint('class 0:', str(len(train_df[train_df['y'] == 0])))\nprint('class 1:', str(len(train_df[train_df['y'] == 1])))\nsns.countplot(x='y',data=train_df)","cc75e15a":"test_df = pd.read_csv('..\/input\/nida-competition1\/test.csv')\n\ntest_df.drop(col_to_drop, axis = 1, inplace = True)\ntest_df = one_hot_feature(test_df)","bbcb781a":"def under_sampling(train_set_df, random_state):\n    df_majority = train_set_df[train_set_df['y']==0]\n    df_minority = train_set_df[train_set_df['y']==1]\n\n    df_majority_downsampled = resample( df_majority, \n                                        n_samples = df_minority.shape[0], \n                                        replace=True, \n                                        random_state=random_state)\n\n    train_set_df_ds = pd.concat([df_majority_downsampled, df_minority])\n    return train_set_df_ds\n    \ndef over_sampling(train_set_df, random_state):\n    df_majority = train_set_df[train_set_df['y']==0]\n    df_minority = train_set_df[train_set_df['y']==1]\n\n    df_minority_upsampled = resample( df_minority, \n                                    n_samples=df_majority.shape[0], \n                                    replace=True, \n                                    random_state=random_state)\n\n    train_set_df_us = pd.concat([df_minority_upsampled, df_majority])\n    return train_set_df_us\n\ntrain_df = over_sampling(train_df, random_state)","d6b17a69":"print('class 0:', str(len(train_df[train_df['y'] == 0])))\nprint('class 1:', str(len(train_df[train_df['y'] == 1])))","378727d0":"def change_type(data):\n    data['age'] = data['age'].astype('float32')\n    data['job'] = data['job'].astype('category')\n    #data['marital'] = data['marital'].astype('category')\n    #data['education'] = data['education'].astype('category')\n    #data['default'] = data['default'].astype('category')\n    #data['housing'] = data['housing'].astype('category')\n    #data['loan'] = data['loan'].astype('category')\n    #data['contact'] = data['contact'].astype('category')\n    data['duration'] = data['duration'].astype('float32')\n    data['campaign'] = data['campaign'].astype('float32')\n    #data['pdays'] = data['pdays'].astype('float32')\n    data['previous'] = data['previous'].astype('float32')\n    #data['poutcome'] = data['poutcome'].astype('category')\n    data['emp.var.rate'] = data['emp.var.rate'].astype('float32')\n    data['cons.price.idx'] = data['cons.price.idx'].astype('float32')\n    data['cons.conf.idx'] = data['cons.conf.idx'].astype('float32')\n    data['euribor3m'] = data['euribor3m'].astype('float32')\n    data['nr.employed'] = data['nr.employed'].astype('float32')\n    # data['month'] = data['month'].astype('category')\n    # data['day_of_week'] = data['day_of_week'].astype('category')\n    # data['y'] = data['y'].astype('uint8')\n    \n    return data\n\ndef convert_to_dummy(df):\n    cate_var = []\n    num_var = []\n    for i in df.columns:\n        try:\n            if df.dtypes[i] == \"category\":\n                cate_var.append(i)\n        except:\n            num_var.append(i)\n\n    # get-dummies\n    df_train_dummies = pd.concat([df[num_var], pd.get_dummies(df[cate_var])], axis=1, sort=False)\n    return df_train_dummies\n\nX_train = convert_to_dummy(change_type(train_df)).drop(['y'],axis=1)\ny_train = train_df['y']\n\nX_test = convert_to_dummy(change_type(test_df))","0fa074ee":"X_1 = X_train.columns.to_list()\nX_2 = X_test.columns.to_list()\n\na = set(str(x) for x in X_1)\nb = set(str(x) for x in X_2)\n\njoint_items = a.union(b)\nunmatched_item_a = (a - b)\nunmatched_item_b = (b - a)\n\nfor i in list(unmatched_item_a):\n    X_test[i] = 0\nfor i in list(unmatched_item_b):\n    X_train[i] = 0","1c1c2608":"ss = StandardScaler()\n\n# Rescale Train Set\nX_train = ss.fit_transform(X_train)\n\n# Rescale Test Set\nX_test = ss.transform(X_test)","df28823f":"model_list = {\n    'QuadraticDiscriminantAnalysis':QuadraticDiscriminantAnalysis(),\n    'LinearDiscriminantAnalysis':LinearDiscriminantAnalysis(),\n    'XGBoost':XGBClassifier(random_state=random_state),\n    'CatBoost':CatBoostClassifier(random_state=random_state),\n    'k-nearest neighbors':KNeighborsClassifier(),\n    'Decision Tree':DecisionTreeClassifier(random_state=random_state),\n    'Support Vector Machine':svm.SVC(random_state=random_state),\n    'Logistic Regression':LogisticRegression(random_state=random_state),\n    'Random Forest':RandomForestClassifier(random_state=random_state),\n    'Ada Boost':AdaBoostClassifier(random_state=random_state), \n    'Gradient Boosting':GradientBoostingClassifier(random_state=random_state)\n}\nneed_to_scale = (\n    'k-nearest neighbors',\n    'Support Vector Machine',\n    'Logistic Regression'\n)\n\n# Create Pipelines\npipeline_model_list = {}\nfor model in model_list:\n    if model in need_to_scale:\n        pipeline_model_list[model] =  create_pipeline(model_list[model])\n    else:\n        pipeline_model_list[model] = model_list[model]\n    print(f'Created pipeline for {model}')","3c9a5a93":"summary_result = []\nfor model in pipeline_model_list:\n    \n    y_pred = cross_val_predict(pipeline_model_list[model], X_train, y_train, cv = 5, n_jobs=-1,verbose=10)\n    cv_score = cross_val_score(pipeline_model_list[model], X_train, y_train, cv = 5, scoring = 'roc_auc', n_jobs=-1, verbose=10)\n    result_dict = evaluation(model, y_train, y_pred, prefix='CV_AVG_')\n    result_dict['CV_SD_AUC'] = np.std(cv_score)\n    result_dict['CV_SD_ACC'] = np.std(cv_score)\n    summary_result.append(result_dict)\n    \n    print(f'Validation Result for {model}')\n    print('\\n-- Confusion Martix --')\n    print(confusion_matrix(y_train,y_pred))\n    print('\\n-- Classification Report --')\n    print(classification_report(y_train, y_pred))\n    print('\\n')\n    \nsummary_result_cv_df = pd.DataFrame(summary_result)","ac230fca":"col = ['MODEL','CV_AVG_AUC', 'CV_AVG_ACCURACY','CV_SD_ACC','CV_AVG_PRECISION_0','CV_AVG_PRECISION_1','CV_AVG_RECALL_0',\n      'CV_AVG_RECALL_1','CV_AVG_F1_0','CV_AVG_F1_1']\nsummary_result_cv_df[col].sort_values('CV_AVG_ACCURACY', ascending = False)","2639079e":"y_pred_dict = {}","2e6cb0ad":"model = XGBClassifier(max_depth = 6, \n                      eval_metric ='auc',\n                      subsample = 0.8, #0.8\n                      learning_rate = 0.08, #0.08\n                      min_child_weight = 3, #3\n                      colsample_bytree = 0.8, #0.8\n                      scale_pos_weight = 1, #1\n                      n_jobs=-1,\n                      random_state=random_state)\nmodel = model.fit(X_train, y_train)\ny_pred_score = model.predict_proba(X_test)[:,1]\ny_pred = np.where(y_pred_score>0.5,1,0)\ny_pred_dict['XGBClassifier'] = y_pred_score","e0b9d6c6":"model = CatBoostClassifier(iterations=1000, \n                           learning_rate=0.01,\n                           depth = 10,\n                           l2_leaf_reg = 5,\n                           border_count=64,\n                           boost_from_average = True,\n                           eval_metric='AUC',\n                           random_state=random_state, verbose = 200)\n\nmodel = model.fit(X_train, y_train)\ny_pred_score = model.predict_proba(X_test)[:,1]\ny_pred = np.where(y_pred_score>0.5,1,0)\ny_pred_dict['CatBoostClassifier'] = y_pred_score","35b9718d":"model = GradientBoostingClassifier(learning_rate = 0.05,\n                                   n_estimators = 500,\n                                   subsample = 0.5,\n                                   min_samples_split = 5,#5\n                                   max_depth = 3,#3\n                                   random_state=random_state)\n\nmodel = model.fit(X_train, y_train)\ny_pred_score = model.predict_proba(X_test)[:,1]\ny_pred = np.where(y_pred_score>0.5,1,0)\ny_pred_dict['GradientBoostingClassifier'] = y_pred_score","474139ca":"voting_df = pd.DataFrame(y_pred_dict)\nvoting_df['voting_score'] = pd.DataFrame(y_pred_dict).sum(axis = 1)\/3\nvoting_y_pred_score = np.array(voting_df['voting_score'])","d8c5e7c2":"voting_df","5aebe83c":"print('-- XGBClassifier --')\nprint('Class 1:', np.where(y_pred_dict['XGBClassifier']>=0.5,1,0).sum())\nprint('Class 0:', len(y_pred_dict['XGBClassifier']) - np.where(y_pred_dict['XGBClassifier']>=0.5,1,0).sum())\nprint('\\n')\nprint('-- CatBoostClassifier --')\nprint('Class 1:', np.where(y_pred_dict['CatBoostClassifier']>=0.5,1,0).sum())\nprint('Class 0:', len(y_pred_dict['CatBoostClassifier']) - np.where(y_pred_dict['CatBoostClassifier']>=0.5,1,0).sum())\nprint('\\n')\nprint('-- GradientBoostingClassifier --')\nprint('Class 1:', np.where(y_pred_dict['GradientBoostingClassifier']>=0.5,1,0).sum())\nprint('Class 0:', len(y_pred_dict['GradientBoostingClassifier']) - \n      np.where(y_pred_dict['GradientBoostingClassifier']>=0.5,1,0).sum())\nprint('\\n')\nprint('-- VotingScore --')\nprint('Class 1:', np.where(voting_df['voting_score']>=0.5,1,0).sum())\nprint('Class 0:', len(voting_df['voting_score']) - np.where(voting_df['voting_score']>=0.5,1,0).sum())\nprint('\\n')","2cdec592":"# submit_df = test_df.copy()\n# submit_df['y'] = voting_y_pred_score\n# submit_df['y'].value_counts()\n# submit_df = submit_df.reset_index()\n# submit_df['Id'] = submit_df['index']+1\n# submit_df = submit_df[['Id','y']]\n# submit_df.to_csv('submission_result.csv',index=False)","689607cc":"# Train Target Class","0932dc25":"# Change type and Get Dummy","859c67a7":"# Test DataFrame","f1be5183":"# Impute 0 to unmatched items in train and test","5a707ba5":"# Import Library","37fc2a55":"# Sampling Method","ab340965":"# Feature Engineering","d760b8fe":"# Train DataFrame","d4e928eb":"# Model Selection","0fc4dbb5":"# Standard Scale"}}