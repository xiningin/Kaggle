{"cell_type":{"6f1b24c2":"code","2ecc8525":"code","5b21af3c":"code","1ebd9f35":"code","9c069777":"code","c77757ab":"code","4bbad113":"code","f051ee23":"code","41bc0cf8":"code","600f1cc7":"code","dc7c9b77":"code","277abf11":"code","508a72c9":"code","637f55ef":"code","79f51241":"code","06f16c2d":"code","a3834b9d":"code","66bf95a3":"code","4b157c66":"code","365a5150":"code","026981e2":"code","dcf328d2":"code","ef290170":"code","f3551305":"code","17082ac9":"code","d082be86":"code","f346c37c":"code","c4940bfc":"code","73520ef7":"code","6dbd7f9c":"code","75e71960":"code","4bf47d5a":"code","67c0991f":"code","245f0221":"code","fda9134a":"code","937d7bfa":"code","f895ca3d":"code","4dacd7be":"code","b79f6d4e":"code","c599c101":"code","cd21ae51":"code","422feaed":"code","ad8a5ca0":"markdown","a864b440":"markdown","d9748e45":"markdown","e5d40c50":"markdown","d3bed561":"markdown","d6160ad4":"markdown","42c3227a":"markdown","bebca2aa":"markdown","ae884720":"markdown","4ea11f67":"markdown","2de87e53":"markdown","00fe663b":"markdown","9067baba":"markdown","fa89fe53":"markdown","3ee15348":"markdown"},"source":{"6f1b24c2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import model_selection\n#from sklearn.metrics import accuracy_score\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.linear_model import LinearRegression\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.svm import SVC\nimport seaborn as sns\n#sns.set(style=\"whitegrid\")\n","2ecc8525":"train = pd.read_csv(\"..\/input\/black-friday\/train.csv\")\ntrain = train.drop([\"User_ID\",\"Product_ID\"] , axis = 1)\ntrain.head()","5b21af3c":"train.shape","1ebd9f35":"train.info()","9c069777":"train.isnull().sum()","c77757ab":"train[train.duplicated() ].shape","4bbad113":"train.describe(include=\"object\")","f051ee23":"print(train[\"Product_Category_1\"].unique())\nprint(train[\"Product_Category_2\"].unique())\nprint(train[\"Product_Category_3\"].unique())","41bc0cf8":"train[\"Product_Category_2\"].fillna(0,inplace=True)\ntrain[\"Product_Category_3\"].fillna(0,inplace=True)\n\ntrain[\"Product_Category_2\"] = train[\"Product_Category_2\"].astype('int64')\ntrain[\"Product_Category_3\"] = train[\"Product_Category_3\"].astype('int64')","600f1cc7":"train.isnull().sum()","dc7c9b77":"from sklearn import preprocessing\n\ndef preprocessing_train(df,column):\n\n    le = preprocessing.LabelEncoder()\n\n    df[column] = le.fit_transform(df[column])","277abf11":"preprocced_data = train.copy()\n\npreprocessing_train(preprocced_data,\"Gender\")\npreprocessing_train(preprocced_data,\"Age\")\npreprocessing_train(preprocced_data,\"City_Category\")\npreprocessing_train(preprocced_data,\"Stay_In_Current_City_Years\")","508a72c9":"plt.figure(figsize=(17, 7))\n\nsns.set(style=\"whitegrid\")\n\nsns.boxplot(data=preprocced_data.drop(\"Purchase\",axis=1))\n\nplt.tick_params(axis='x', rotation=70)","637f55ef":"plt.figure(figsize=(17, 7))\n\nsns.set(style=\"whitegrid\")\n\nsns.boxplot(data=preprocced_data)\n\nplt.tick_params(axis='x', rotation=70)","79f51241":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=[17,7])\n\nsns.set(style=\"whitegrid\")\n\nsns.boxplot(x = train[\"Purchase\"],ax=ax[0]).set_title(\"Purchase boxplot\", fontsize=18)\n\nsns.violinplot(train[\"Purchase\"],ax=ax[1]).set_title(\"Purchase violinplot\", fontsize=18)","06f16c2d":"train.describe(include=\"all\")","a3834b9d":"print(train.skew())\nsns.pairplot(train,\n             diag_kind=\"kde\",\n             corner=True,\n             markers=\"+\",\n             plot_kws=dict(s=1, edgecolor=\"b\", linewidth=1),\n             diag_kws=dict(shade=True))\n","66bf95a3":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,9))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),\n            vmin=-1,\n            vmax=1,\n            cmap='RdBu',\n            annot=True)","4b157c66":"prod_cat = train[train[\"Product_Category_1\"] == 5]\nprod_cat = prod_cat[prod_cat[\"Product_Category_2\"] == 8] \nprod_cat[prod_cat[\"Product_Category_3\"] == 14]","365a5150":"prod_cat[\"Purchase\"].value_counts()","026981e2":"def Countplot_with_hue(df , x , y ,hue ,ax ,title):\n    \n    plt.figure(figsize=(15, 7))\n\n    sns.countplot(x=x, y=y , hue=hue, data=df, ax=ax).set_title(title, fontsize=18)\n    \n    \n    \nfrom scipy import stats\n\ndef p_value_and_pearson_coeff(col1 ,col2):\n    \n    pearson_coef , p_value = stats.pearsonr(train[col1],train[col2])\n    \n    print(\"pearson_coef = \",pearson_coef)\n    \n    print(\"p_value = \",p_value)\n    ","dcf328d2":"def product_and_purchase(col1 ,col2 ,color , ax1 , ax2 ):\n    \n    print(col1 + \" and Purchase\")\n    \n    p_value_and_pearson_coeff(col1,col2)\n    \n    print(\"-\"*100)\n\n    train[col1].value_counts().plot(kind='bar',color = color,ax=ax1, title=col1+\" count\")\n    \n    train.plot(kind='scatter',x=col1,y=col2,color = color,marker=\"+\" ,ax=ax2, title=col1)","ef290170":"fig, axes = plt.subplots(nrows=3, ncols=2, figsize=[15, 15])\n\nproduct_and_purchase(\"Product_Category_1\" ,\"Purchase\", \"r\" , axes[0,0] ,axes[0,1])\n\nproduct_and_purchase(\"Product_Category_2\" ,\"Purchase\", \"g\" , axes[1,0] ,axes[1,1])\n\nproduct_and_purchase(\"Product_Category_3\" ,\"Purchase\", \"b\", axes[2,0] ,axes[2,1])","f3551305":"print(\"Occupation and Purchase\")\np_value_and_pearson_coeff(\"Occupation\",\"Purchase\")\n","17082ac9":"#Countplot_with_hue(df , x , y ,hue ,ax ,title)\nCountplot_with_hue(train , \"Occupation\", None ,None ,None ,\"Occupation count\")","d082be86":"sorted_data = train.sort_values(by=['City_Category', 'Age',\"Stay_In_Current_City_Years\"] , ascending=True)","f346c37c":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[17,7])\n\nCountplot_with_hue(sorted_data , \"Age\",None ,None,axes[0] ,\"age\")\nCountplot_with_hue(sorted_data , \"Stay_In_Current_City_Years\",None ,None ,axes[1],\"Stay_In_Current_City_Years\")\n","c4940bfc":"train.columns","73520ef7":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=[17, 17])\n\nCountplot_with_hue(train , \"Age\" , None ,\"Gender\",axes[0,0],\"Gender according to Age\")\nCountplot_with_hue(train , \"City_Category\" , None ,\"Gender\",axes[0,1],\"Gender according to City_Category\")\nCountplot_with_hue(sorted_data , \"Occupation\" , None ,\"Gender\",axes[1,0],\"Gender according to Occupation\")\nCountplot_with_hue(sorted_data , \"Occupation\" , None ,\"City_Category\",axes[1,1],\"City Category according to Occupation\")\n\n","6dbd7f9c":"def pie_chart(column ,explode , labels):\n    plt.figure(figsize=(12, 6))\n\n\n    plt.pie(train[column].value_counts(),\n            explode=explode,    #explode=[0.04,0]\n            startangle=90, \n            autopct='%1.1f%%',\n            labels=labels, #labels=['Males','Females']\n            colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99'],\n            pctdistance=.6,\n            textprops={'fontsize': 20})\n\n\n\n\n    centre_circle = plt.Circle((0,0),0.7,fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n\n    plt.axis('equal')  \n    plt.tight_layout()\n    plt.show()\n\n\n","75e71960":"train.columns","4bf47d5a":"pie_chart(\"Gender\" ,[0.04,0] , ['Males','Females'])","67c0991f":"pie_chart(\"City_Category\" ,[0.05,0.05,0.05] , ['A','B',\"C\"])","245f0221":"pie_chart(\"Marital_Status\" ,[0.05,0.05] , [\"Married\",\"Single\"])","fda9134a":"train.head()","937d7bfa":"le = preprocessing.LabelEncoder()\n\ntrain[\"Age\"] = le.fit_transform(train[\"Age\"])\ntrain[\"Stay_In_Current_City_Years\"] = le.fit_transform(train[\"Stay_In_Current_City_Years\"])\ntrain[\"City_Category\"] = le.fit_transform(train[\"City_Category\"])\n\n\ntrain = pd.get_dummies(train, prefix=[\"Gender\"])\n","f895ca3d":"train.head()","4dacd7be":"colormap = plt.cm.RdBu\nplt.figure(figsize=(20,10))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","b79f6d4e":"# Imports\nimport torch\nimport torchvision\nimport torch.nn as nn # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\nimport torch.optim as optim # For all Optimization algorithms, SGD, Adam, etc.\nimport torch.nn.functional as F # All functions that don't have any para\/'meters\nfrom torch.utils.data import DataLoader # Gives easier dataset managment and creates mini batches\nimport torchvision.datasets as datasets # Has standard datasets we can import in a nice way\nimport torchvision.transforms as transforms # Transformations we can perform on our dataset","c599c101":"# 0) Prepare data\nfeatures = train.drop(\"Purchase\",axis = 1)\ntarget = train[\"Purchase\"]","cd21ae51":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nfeatures_scaled = scaler.fit_transform(features)\ntarget_scaled = scaler.fit_transform(target.values.reshape(-1, 1))\n\n# cast to float Tensor\nX = torch.from_numpy(np.asarray(features_scaled).astype(np.float32))\ny = torch.from_numpy(np.asarray(target_scaled).astype(np.float32))\ny = y.view(y.shape[0], 1)\n\nn_samples, n_features = X.shape","422feaed":"# 1) Model\n# Linear model f = wx + b\ninput_size = n_features\noutput_size = 1\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 1\n    \nclass Regressor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(input_size, 144)\n        self.fc2 = nn.Linear(144, 72)\n        self.fc3 = nn.Linear(72, output_size)\n\n        #self.dropout = nn.Dropout(p=0.1)\n\n    def forward(self, x):\n\n        #x = self.dropout(F.relu(self.fc1(x)))\n        #x = self.dropout(F.relu(self.fc2(x)))\n        #x = self.dropout(F.relu(self.fc3(x)))\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n\n        return x\n    \ninput_size = n_features\noutput_size = 1\n#model = NN(input_size, output_size)\nmodel = Regressor()\n\n# 2) Loss and optimizer\nlearning_rate = 0.01\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n# 3) Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass and loss\n    y_predicted = model(X)\n    loss = criterion(y_predicted, y)\n\n    \n    # Backward pass and update\n    loss.backward()\n    optimizer.step()\n    # zero grad before new step\n    optimizer.zero_grad()\n\n\n    if (epoch+1) % 10 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n","ad8a5ca0":"this is my first time working with pytorch so i searched a lot about it and i found a good simple tutorial to apply a linear regression with Pytorch \n\nvideo link: https:\/\/www.youtube.com\/watch?v=YAJ5XBwlN4o&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=7","a864b440":"## 5- Linear Regression With Pytorch","d9748e45":"### without Purchase","e5d40c50":"**Now, let's see difference between products categories, i have used countplot for each individual category and scatterplot between them and Purchase feature**\n\n**also calculated the correlation between each category and Purchase**\n","d3bed561":"#### The five-number summary is a set of descriptive statistics that provides information about a dataset. It consists of the five most important sample percentiles:\n\nthe sample minimum (smallest observation)\n\nthe lower quartile or first quartile\n\nthe median (the middle value)\n\nthe upper quartile or third quartile\n\nthe sample maximum (largest observation)\n\nWe can see these five numbers with boxplot or violinplot as follows:\n\n![boxplot.jpg](attachment:boxplot.jpg)","d6160ad4":"Welcome to my kernel in black friday dataset.\n\nin this kernel i will try to explain some mysterious features in this dataset, and also i will do some EDA and some visualization.\n\nif you have any suggest,advice or correction please don't hesitate to write it, i think it will be very helpful for me.\n\nwe will go through these topics:\n\n         1- descriptive data analysis\n\n         2- product categories explaination\n\n         3- Occupation explaination\n\n         4- Visualization\n         \n         5- Linear Regression With Pytorch","42c3227a":"## 1- descriptive data analysis","bebca2aa":"### Now let's see the correlation between features","ae884720":"## 2- product categories explaination","4ea11f67":"**we will fill \"Product_Category_2\" and \"Product_Category_3\" with zero, i will explain why in Product categories's section** ","2de87e53":"## 3- Occupation explaination\n\n**after some searching it seems that this featuer is about the classification of job status like Management Occupations, Business and Financial Operations Occupations, Computer and Mathematical Occupations, etc. according to U.S. bureau of labour statistics https:\/\/www.bls.gov\/soc\/2018\/major_groups.htm**\n\n**unfortunaitlly i can't find the equivalnt group name for each number in Occupation feature so i will work with what i have**\n","00fe663b":"**So the only features that have null values are \"Product_Category_2\" and \"Product_Category_3\" ,after filling them we will go through some descriptive statistics**","9067baba":"## 4- Visualization\n\n**now let's go and see some visuals like count of each feature and relation between some of them**","fa89fe53":"**lets see the skewness of data**\n\n**skewness = 0 : normally distributed.**\n\n**skewness > 0 : more weight in the left tail of the distribution.**\n\n**skewness < 0 : more weight in the right tail of the distribution.**","3ee15348":"**So, the product categories features are mysterious and for me, the only explanation is that each product could lay under more than one category and this explain why there are some products have value in each one of the 3 categories and others lay under \"Product_Category_1\" only, and that's why i filled nan values with zeros**\n\n**also exploring products that have the same value in each one of the 3 product categories gives us different purchase value as follows**"}}