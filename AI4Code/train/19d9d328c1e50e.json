{"cell_type":{"61ffb1ec":"code","4a08dc08":"code","0c09b57e":"code","02211cb3":"code","4ec3f9ec":"code","125798b3":"code","d40af4b1":"code","2e746767":"code","49674234":"code","ecd6722f":"code","2c6e2e27":"code","52f5560f":"code","98d47b57":"markdown","83dd8562":"markdown","6738d531":"markdown","3589b7fd":"markdown","b00aa7c4":"markdown","4c7c76ba":"markdown","df764952":"markdown","f0db3a6f":"markdown","af6e66b1":"markdown","ccc8d246":"markdown","77b8e591":"markdown","f7539f69":"markdown","7f4a32a0":"markdown"},"source":{"61ffb1ec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\nfrom keras.regularizers import l1","4a08dc08":"df_train = pd.read_csv('..\/input\/train.csv', dtype = {'acoustic_data': np.int16, 'time_to_failure': np.float32} ) # float32 is enough :)\n\ntrain_X = np.resize(df_train['acoustic_data'].values, (len(df_train['acoustic_data']) \/\/ 150000, 150000)).astype(np.float32) # rearrange into 150k windows\ntrain_X = (train_X - np.mean(train_X)) ** 2 # calculate power density\ntrain_X = np.mean(train_X.reshape(-1,300,500), axis=2) # downsample 500x\n\ntrain_y = np.resize(df_train['time_to_failure'].values, (len(df_train['time_to_failure']) \/\/ 150000, 150000))[:,-1] # train_y is ttf on right window edge\n\ndel df_train # free some memory\n\nprint(train_X.shape, train_y.shape)","0c09b57e":"df_subm = pd.read_csv('..\/input\/sample_submission.csv')\n\ntest_X = []\n\nfor fname in df_subm['seg_id'].values:\n    test_X.append(pd.read_csv('..\/input\/test\/' + fname + '.csv').acoustic_data.values.astype(np.int16))\ntest_X = np.array(test_X).astype(np.float32)\n\ntest_X = (test_X - np.mean(test_X)) ** 2  # calculate power density\ntest_X = np.mean(test_X.reshape(-1,300,500), axis=2) # downsample 500x\n\nprint(test_X.shape)","02211cb3":"fig, axes = plt.subplots(1,2)\nfig.set_size_inches(16,5)\naxes[0].grid(True)\naxes[1].grid(True)\n\naxes[0].plot(train_X[28]);\naxes[0].plot(train_X[38]);\naxes[1].plot(test_X[28]);\naxes[1].plot(test_X[38]);","4ec3f9ec":"ratio_mean_train_test = np.mean(test_X) \/ np.mean(train_X)\nprint('Ratio of mean power in train\/test : ', np.mean(test_X) \/ np.mean(train_X))","125798b3":"def prepare_X_for_cnn(data):\n    data = np.log10(data) # take log10 to handle the huge peaks\n    data -= np.mean(data) # remove mean\n    data \/= np.std(data) # set std to 1.0\n    data = np.expand_dims(data, axis=-1) # reshaping for CNN\/RNN\n    return data\n\ntrain_X = prepare_X_for_cnn(train_X)\ntest_X  = prepare_X_for_cnn(test_X)","d40af4b1":"fig, axes = plt.subplots(1,2)\nfig.set_size_inches(16,5)\naxes[0].grid(True)\naxes[1].grid(True)\n\naxes[0].plot(train_X[28]);\naxes[0].plot(train_X[38]);\naxes[0].plot(train_X[29]);\naxes[1].plot(test_X[28]);\naxes[1].plot(test_X[38]);","2e746767":"def model_cnn():\n    model = Sequential([\n        Conv1D(filters=16, kernel_size=3, activation='relu', input_shape = train_X.shape[1:]),\n        MaxPooling1D(2),\n        Conv1D(filters=128, kernel_size=3, activation='relu'),\n        MaxPooling1D(2),\n        Conv1D(filters=16, kernel_size=3, activation='relu'),\n        MaxPooling1D(2),\n        Flatten(),\n        Dropout(0.1),\n        Dense(16, activation='relu', kernel_regularizer=l1(0.01)),\n        Dense(16, activation='relu', kernel_regularizer=l1(0.01)),\n        Dense(1, activation='linear') # regression\n    ])\n    model.compile(\n        loss='mse',\n        optimizer='adam',\n        metrics=['mae']\n    )\n    return model\n\nmodel = model_cnn()\nmodel.summary()\nmodel.save_weights('\/tmp\/model_weights_init.h5')","49674234":"test_y_pred = []\nnum_iter = 64\n\nfor i in range(num_iter):\n    model.load_weights('\/tmp\/model_weights_init.h5')\n    model.fit(train_X, train_y, epochs=16,  verbose=0)\n    test_y_pred.append(model.predict(test_X))\n    \ntest_y_pred = np.array(test_y_pred).reshape(num_iter,-1)","ecd6722f":"test_y_pred_avg = np.mean(test_y_pred, axis=0)\ntest_y_pred_avg","2c6e2e27":"fig, axes = plt.subplots(1,1)\nfig.set_size_inches(16,5)\naxes.grid(True)\n\naxes.plot(model.predict(train_X));\naxes.plot(train_y);","52f5560f":"df_subm['time_to_failure'] = test_y_pred_avg * ratio_mean_train_test\ndf_subm.to_csv('submission.csv', index=False)","98d47b57":"Another look at the same windows as above after the CNN preparation. I've added another window to train which contains a major quake. The log10 scales everything such that a CNN can handle it:","83dd8562":"Same CV approach here: 16 epochs, average of 64 runs as the net isn't really stable after 16 epochs. batch_size etc. could still be tuned...","6738d531":"Read the train data and prepare train_X and train_y:","3589b7fd":"Now prepare the X data for the CNN:","b00aa7c4":"This are the predicted train ttfs. Note that the predictions are an average over all cycles. Training the CNN for more epochs would lead to overfitting, as it would simply memorize all train data points:","4c7c76ba":"Now something important: as we will see below, the model (like all other models in this competition, due to the underlying physics) is actually unable to truly predict ttf for individual quake cycles. For ~ the first half of the cycle, the acoustic data is always the same in every cycle. There is no information in the data which tells whether there will be a minor quake in 3 sec or a major one in 5 sec. So the model *has* to predict a kind of average ttf over all cycles (see plot of ttf below).\n\nNow the test data has a higher mean power density than the train data. This means that the cycles in test will on average be longer. It is therefore reasonable to multiply the predicted ttfs by this ratio, which is ~1.05:","df764952":"\nThis is a simple kernel which demonstrates that it would have been **possible to get a top 25 result (private LB 2.40) using a simple CNN without any feature engineering and without utilizing the p4677 information about the test data**. The approach is different from all other ideas I've seen here: Everything is based on the *signal power density*, which is simply the acoustic signal amplitude squared (after subtracting the mean). I am making this kernel public for two reasons:\n* I haven't seen any other kernel using the power density\n* This seems to be the only way to use neural nets directly on the data: the power density is always > 0 and can therefore be averaged over many data points (unlike the original signal, where positive and negative amplitudes would cancel, leading to a loss of information). A 150,000 points window can therefore be efficiently downsampled to several 100 data points.\n* I achieved a (unfortunately late :) ) top 25 result without much effort. The classic story: I didn't like my public LB for this method and didn't choose it... Maybe someone else is interested in improving this result? Using the p4677 info and optimizing the CNN via CV might lead to a (late) top result...\n\nAll that was done to achieve this result is the following:\n1. split the train data into non-overlapping 150k data point windows, analogous to the test data\n2. remove the global mean from train\/test, square the result, reshape the squared data into (300,500) windows and take the average over each 300 point window, resulting in downsampled 300 point power density windows.\n3. train a CNN (3 convolution layers, 2 dense layers)\n4. multiply the predicted ttf by a factor which accounts for the fact that the mean power density in test is ~5% higher than in train (more on that below).\n\nI probably didn't even use the best possible CNN topology \/ parameters.\nHope that this solution is of interest! Thanks to the organizers for this great and frustrating :) competition, and congratulation to the winners!\n","f0db3a6f":"Read the submission file and the test data and prepare test_X:","af6e66b1":"Now the CNN: The parameters have been set according to the results of a quick RandomGridCV. I'm sure that there are better topologies:","ccc8d246":"Multiply by the corrective factor (see above) and submit:","77b8e591":"That's all. Hope this was helpful, maybe even for the organizers :). Remember: the CNN didn't get any info on mean, std, etc. directly but learned all that *from the shape of the peaks*.","f7539f69":"Average the ttfs: ","7f4a32a0":"This is what the individual windows look like: some examples from train(left) and test(right):"}}