{"cell_type":{"b9739515":"code","b2b495da":"code","989a648c":"code","569e71de":"code","827663b2":"code","dfaad617":"code","4e188ac9":"code","01638fff":"code","24917627":"code","5c366346":"code","7b7dbf38":"code","84a021b8":"code","87a02fc3":"code","c6097468":"code","3a21cba5":"code","1f54a619":"code","302bd7d4":"code","d3d7e17c":"code","94825d95":"code","e4efe907":"code","d9ed05d6":"code","37e48521":"code","d0718ef9":"code","d23537fa":"code","08f64bfb":"markdown","23bcb486":"markdown","6f3ca333":"markdown","455d935c":"markdown"},"source":{"b9739515":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve, auc\nfrom itertools import cycle\nfrom scipy import interp\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b2b495da":"data_df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndata_df.head()","989a648c":"data_df.head()","569e71de":"data_df.describe()","827663b2":"temp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\n\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig, filename='class')","dfaad617":"data_df.head()","4e188ac9":"data_df[data_df['Class'] == 1].head(1)\n","01638fff":"data_df[data_df['Class'] == 0].head(1)","24917627":"corr = data_df.corr(method='pearson').head()","5c366346":"def print_full(x):\n    pd.set_option('display.max_rows', len(x))\n    print(x)\n    pd.reset_option('display.max_rows')","7b7dbf38":"corr = data_df.corr().unstack().sort_values()\nprint_full(corr)","84a021b8":"sns.heatmap(corr)","87a02fc3":"component_var = {}\nfor i in range(1,28):\n    pca = PCA(n_components=i)\n    res = pca.fit(data_df)\n    component_var[i] = sum(pca.explained_variance_ratio_)\n    \nprint(component_var)","c6097468":"plt.matshow(data_df.corr())\nplt.show()","3a21cba5":"f = plt.figure(figsize=(19,15))\nplt.matshow(data_df.corr(),fignum=f.number)\nplt.xticks(range(data_df.shape[1]),data_df.columns,fontsize=14, rotation=45)\nplt.yticks(range(data_df.shape[1]),data_df.columns,fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\nplt.title('Correlation Matrix', fontsize=16)","1f54a619":"corr = data_df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","302bd7d4":"pd.scatter_matrix(data_df, figsize(19,15))\nplt.show()","d3d7e17c":"plt.hist(data_df['Class'], color='blue', edgecolor='black', bins = int(185\/5))\nsns.distplot(data_df['Class'], hist=True, kde=False, bins=int(180\/5), color='blue', hist_kws={'edgecolor':'black'})\n\nplt.title('histo')\nplt.xlabel('delay ')\nplt.ylabel('flight')","94825d95":"print(\"Normal :\", data_df['Class'][data_df['Class'] == 0].count())\n\nprint(\"Fraud :\", data_df['Class'][data_df['Class'] == 1].count())","e4efe907":"# separate classes into different dataset\nclassNormal = data_df.query('Class == 0')\nclassFraud = data_df.query('Class == 1')\n\n#randomize the dataset\nclassNormal = classNormal.sample(frac=1)\nclassFraud = classFraud.sample(frac=1)","d9ed05d6":"classNormaltrain = classNormal.iloc[0:6000]\nclassFraudtrain = classFraud\n\n#combine become one\ntrain = classNormaltrain.append(classFraudtrain, ignore_index=True).values\n","37e48521":"X = train[:,0:30].astype(float)\nY = train[:,30]","d0718ef9":"model = XGBClassifier()\nkfold = StratifiedKFold(n_splits=10, random_state=7)\n\nscoring = 'roc_auc'\nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","d23537fa":"mean_tpr = 0.0\nmean_fpr = np.linspace(0,1,100)\n\ncolors = cycle(['cyan', 'indigo', 'seagreen', 'yellow', 'blue', 'darkorange'])\nlw = 2\n\ni = 0\nfor(train,test), color in zip(kfold.split(X,Y), colors) :\n    probas_ = model.fit(X[train], Y[train]).predict_proba(X[test])\n    \n    fpr, tpr, thresholds = roc_curve(Y[test], probas_[:,1])\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, lw=lw, color=color, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n    i +=1\n    \nplt.plot([0,1],[0,1], linestyle='--', lw=lw, color='k', label='luck')\n\nmean_tpr \/= kfold.get_n_splits(X,Y)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, color='g', linestyle='--', label ='Mean ROC(area = %0.20f)' % mean_auc, lw=lw)\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05,1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC example')\nplt.legend(loc=\"lower right\")\nplt.show()","08f64bfb":"1. CHECK descriptive analysis \n2. Check null \/ missing value\n3. check the imbalanced dataset\n4. check correlation dataset, pick the potent variable\n5. pake algoritma apa\n6. evaluasi","23bcb486":"Looking to the Time feature, we can confirm that the data contains 284,807 transactions, during 2 consecutive days (or 172792 seconds).","6f3ca333":"**Data Unbalanced**","455d935c":"This dataset is imbalanced dataset. We have to carefull to pick a technique to process and evaluate.\nHere the link https:\/\/www.kdnuggets.com\/2017\/06\/7-techniques-handle-imbalanced-data.html\n\n1. use the evaluation metrics : not use the accuracy but precision, recall, f1 score (harmonic mean of precision and recall), mcc or auc\n2. resampling training set : undersampling or oversampling\n3. cluster the abudant class\n4. the famous **XGBoost** is already a good starting point if the classes are not skewed too much, because it internally takes care that the bags it trains on are not imbalanced.\n"}}