{"cell_type":{"6e6bc651":"code","f12e0b14":"code","23b2ce2a":"code","19288c10":"code","7f2387d0":"code","6b25bb77":"code","cefc603c":"code","c306eb67":"code","d902a3a5":"code","4e07a149":"code","b407025e":"code","326e3697":"code","a62c2c1a":"markdown"},"source":{"6e6bc651":"import os\nimport gc\nimport sys\nimport cv2\nimport glob\nimport math\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n!pip install accelerate\nfrom accelerate import Accelerator\n\nfrom functools import partial\nfrom sklearn.model_selection import StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom transformers import get_cosine_schedule_with_warmup, AdamW\n\nfrom colorama import Fore, Back, Style\nr_ = Fore.RED\nb_ = Fore.BLUE\nc_ = Fore.CYAN\ng_ = Fore.GREEN\ny_ = Fore.YELLOW\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","f12e0b14":"config = {'lr':1e-3,\n          'wd':1e-2,\n          'bs':256,\n          'img_size':128,\n          'epochs':100,\n          'seed':1000}\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONASSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(seed=config['seed'])\n\ntrain_paths = np.random.choice(glob.glob('..\/input\/imagenetmini-1000\/imagenet-mini\/train\/**\/*.JPEG'),10000)\nvalid_paths = np.random.choice(glob.glob('..\/input\/imagenetmini-1000\/imagenet-mini\/val\/**\/*.JPEG'),1000)","23b2ce2a":"def get_train_transforms():\n    return A.Compose(\n        [\n            A.Resize(config['img_size'],config['img_size'],always_apply=True),\n            A.Normalize(),\n            ToTensorV2(p=1.0)\n        ])","19288c10":"class ImageNetDataset(Dataset):\n    def __init__(self,paths,augmentations):\n        self.paths = paths\n        self.augmentations = augmentations\n    \n    def __getitem__(self,idx):\n        path = self.paths[idx]\n        image = cv2.imread(path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']\n        \n        return image\n    \n    def __len__(self):\n        return len(self.paths)","7f2387d0":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=16,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\n\nimg = torchvision.utils.make_grid(sample).permute(1,2,0).numpy()\nplt.figure(figsize=(15,15))\nplt.imshow(img);","6b25bb77":"class Model(nn.Module):\n\n    def __init__(self,latent_dim=512):\n        super().__init__()\n\n        self.latent_dim = latent_dim\n        self.shape = 32\n\n        #encode\n        self.conv1 = nn.Conv2d(3,32,kernel_size=3,stride=2)\n        self.conv2 = nn.Conv2d(32,64,kernel_size=3,stride=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(64*(self.shape-1)**2,2*self.latent_dim)\n        self.relu = nn.ReLU()\n        self.scale = nn.Parameter(torch.tensor([0.0]))\n\n        #decode\n        self.fc2 = nn.Linear(self.latent_dim,(self.shape**2) *32)\n        self.conv3 = nn.ConvTranspose2d(32,64,kernel_size=2,stride=2)\n        self.conv4 = nn.ConvTranspose2d(64,32,kernel_size=2,stride=2)\n        self.conv5 = nn.ConvTranspose2d(32,3,kernel_size=1,stride=1)\n\n\n    def encode(self,x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n#         print(x.shape)\n        x = self.relu(self.flatten(x))\n        x = self.fc1(x)\n        mean,logvar = torch.split(x,self.latent_dim,dim=1)\n        return mean, logvar\n\n    def decode(self,eps):\n        x = self.relu(self.fc2(eps))\n        x = torch.reshape(x,(x.shape[0],32,self.shape,self.shape))\n        x = self.relu(self.conv3(x))\n        x = self.relu(self.conv4(x))\n        x = self.conv5(x)\n        return x\n\n    def reparamatrize(self,mean,std):\n        q = torch.distributions.Normal(mean,std)\n        return q.rsample()\n\n    def kl_loss(self,z,mean,std):\n        p = torch.distributions.Normal(torch.zeros_like(mean),torch.ones_like(std))\n        q = torch.distributions.Normal(mean,torch.exp(std\/2))\n\n        log_pz = p.log_prob(z)\n        log_qzx = q.log_prob(z)\n\n        kl_loss = (log_qzx - log_pz)\n        kl_loss = kl_loss.sum(-1)\n        return kl_loss\n\n    def gaussian_likelihood(self,inputs,outputs,scale):\n        dist = torch.distributions.Normal(outputs,torch.exp(scale))\n        log_pxz = dist.log_prob(inputs)\n        return log_pxz.sum(dim=(1,2,3))\n\n    def loss_fn(self,inputs,outputs,z,mean,std):\n        kl_loss = self.kl_loss(z,mean,std)\n        rec_loss = self.gaussian_likelihood(inputs,outputs,self.scale)\n\n        return torch.mean(kl_loss - rec_loss)\n\n    def forward(self,inputs):\n        mean,logvar = self.encode(inputs)\n        std = torch.exp(logvar\/2)\n        z = self.reparamatrize(mean,std)\n        outputs = self.decode(z)\n\n        loss = self.loss_fn(inputs,outputs,z,mean,std)\n        return loss,(outputs,z,mean,std)","cefc603c":"def run():\n        \n    def evaluate(model,valid_loader):\n        model.eval()\n        valid_loss = 0\n        with torch.no_grad():\n            for i, inputs in enumerate(valid_loader):\n                loss,_ = model(inputs)\n                valid_loss += loss.item()\n\n        valid_loss \/= len(valid_loader)\n        return valid_loss\n        \n    def train_and_evaluate_loop(train_loader,valid_loader,model,optimizer,\n                                epoch,best_loss,lr_scheduler=None):\n        train_loss = 0\n        for i, inputs in enumerate(train_loader):\n            optimizer.zero_grad()\n            model.train()\n            loss,_ = model(inputs)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            \n            if lr_scheduler:\n                lr_scheduler.step()\n        \n        train_loss \/= len(train_loader)\n        valid_loss = evaluate(model,valid_loader) \n\n        if valid_loss <= best_loss:\n            print(f\"Epoch:{epoch} |Train Loss:{train_loss}|Valid Loss:{valid_loss}\")\n            print(f\"{g_}Loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n\n            best_loss = valid_loss\n            torch.save(model.state_dict(),'.\/imagenet_vae_model.bin')\n                    \n        return best_loss\n        \n    accelerator = Accelerator()\n    print(f\"{accelerator.device} is used\")\n\n    model = Model()\n    \n    ## train\n    train_dataset = ImageNetDataset(train_paths,augmentations=get_train_transforms())\n    train_dl = DataLoader(train_dataset,batch_size=config['bs'],shuffle=True,num_workers=4)\n    \n    \n    #valid\n    valid_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\n    valid_dl = DataLoader(valid_dataset,batch_size=config['bs'],shuffle=False,num_workers=4)\n    \n      \n    optimizer = AdamW(model.parameters(),lr=config['lr'],weight_decay=config['wd'])\n    lr_scheduler = get_cosine_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps= config['epochs'] * len(train_dl))\n\n    model,train_dl,valid_dl,optimizer,lr_scheduler = accelerator.prepare(model,train_dl,valid_dl,optimizer,lr_scheduler)\n\n    best_loss = 9999999\n    start_time = time.time()\n    for epoch in range(config[\"epochs\"]):\n        print(f\"Epoch Started:{epoch}\")\n        best_loss = train_and_evaluate_loop(train_dl,valid_dl,model,optimizer,epoch,best_loss,lr_scheduler)\n        \n        end_time = time.time()\n        print(f\"{m_}Time taken by epoch {epoch} is {end_time-start_time:.2f}s{sr_}\")\n        start_time = end_time\n        \n    return best_loss","c306eb67":"run()","d902a3a5":"def generate_and_save_images(model,test_sample,figsize=(20,15)):\n    f, axarr = plt.subplots(1,2,figsize=figsize)\n    img= torchvision.utils.make_grid(test_sample).permute(1,2,0).numpy()\n    axarr[0].imshow(img)\n    \n    mean, logvar = model.encode(test_sample)\n    std = torch.exp(logvar\/2)\n    z = model.reparamatrize(mean, std)\n    predictions = model.decode(z).detach().cpu()\n    fig = plt.figure(figsize=(15, 7))\n    img = torchvision.utils.make_grid(predictions).permute(1,2,0).numpy()\n\n    plt.savefig('image.png')\n    axarr[1].imshow(img)","4e07a149":"model = Model()\nmodel.load_state_dict(torch.load('.\/imagenet_vae_model.bin'))\nmodel.eval()\n\ntest_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=64,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\n\ngenerate_and_save_images(model,sample)","b407025e":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=4,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\n\ngenerate_and_save_images(model,sample)","326e3697":"test_dataset = ImageNetDataset(valid_paths,augmentations=get_train_transforms())\ntest_dl = DataLoader(test_dataset,batch_size=1,shuffle=False,num_workers=4)\n\ndataiter = iter(test_dl)\nsample = dataiter.next()\n\ngenerate_and_save_images(model,sample)","a62c2c1a":"# Training VAE On Pytorch\n\nReferences: 1. [variational-autoencoder-demystified](https:\/\/towardsdatascience.com\/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed) 2. [Variational Autoencoders](https:\/\/www.jeremyjordan.me\/variational-autoencoders\/)"}}