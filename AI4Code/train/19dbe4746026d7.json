{"cell_type":{"302b61e5":"code","afbebc56":"code","e441e539":"code","61d60113":"code","fdf9edda":"code","95c6b43e":"code","a9e14571":"code","acd0be17":"code","58761c1d":"code","ac53a5f2":"code","94e259db":"code","a9b199ed":"code","dbd2f2fb":"code","5a1850b7":"code","0afeaff0":"code","3a9e4e66":"code","55600ca7":"code","5e327f6c":"code","147e4b94":"code","d104418d":"code","acd5677a":"code","addf7efb":"code","e1ed8e95":"code","0ed33391":"code","e7ed17b0":"code","e64788f4":"code","d3671d55":"code","67fb5cfc":"code","932c4ab0":"code","80b105c1":"code","ac23e775":"code","fc40389e":"code","aef348fb":"code","0ffb48a4":"code","8a6c4978":"code","86049606":"code","5eda783b":"code","3a337b9d":"code","609cad1b":"code","e6df327f":"code","65605e57":"code","b4dc8378":"code","39788688":"code","d17fc915":"code","6f47beea":"code","7224c4d9":"code","f8d02310":"code","0e9e4a38":"code","5bbf5879":"code","efd54c76":"code","3b704cb6":"code","ae4aa2b2":"code","203d954b":"code","b02a0b4e":"code","a5777116":"code","9aa79cf0":"markdown","7757a002":"markdown","a77fd290":"markdown","e8961f3e":"markdown","f1ce6e64":"markdown","25987677":"markdown","de5b8773":"markdown","dd1739da":"markdown","5a48a211":"markdown","70906d52":"markdown","43026424":"markdown","ae08da10":"markdown","5fe8b6cf":"markdown"},"source":{"302b61e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afbebc56":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nimport numpy as np","e441e539":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\ndisplay(train.head())\nprint(len(train))\ndisplay(test.head())\nprint(len(test))","61d60113":"x = train[\"target\"].value_counts()\nplt.grid()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel(\"samples\")\nplt.title(\"distribution\")","fdf9edda":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 1][\"text\"].str.len())\nplt.title(\"Disaster tweets length\")","95c6b43e":"plt.grid()\n\nplt.hist(train[train[\"target\"] == 0][\"text\"].str.len(), color= 'r')\nplt.title(\"No disaster tweets length\")","a9e14571":"plt.grid()\n\nword1 = train[train[\"target\"] == 1][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)))\nplt.title(\"Disaster tweets length\")","acd0be17":"plt.grid()\n\nword1 = train[train[\"target\"] == 0][\"text\"].str.split().apply(lambda x:[len(i) for i in x])\nsns.distplot(word1.map(lambda x: np.mean(x)), color = 'r')\nplt.title(\"Disaster tweets length\")","58761c1d":"def create_corpus(target):\n    corpus = []\n    for x in train[train[\"target\"] == target][\"text\"].str.split():\n        print(x)\n        for i in x:\n            corpus.append(i)\n            \n    return corpus","ac53a5f2":"from collections import defaultdict\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams","94e259db":"corpus = create_corpus(0)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","a9b199ed":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"top words 0\")","dbd2f2fb":"corpus = create_corpus(1)\n\nstop = set(stopwords.words(\"english\"))\n\ndictionary = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dictionary[word] +=1\n        \ntop = sorted(dictionary.items(), key = lambda x:x[1], reverse=True)[:10]","5a1850b7":"x, y = zip(*top)\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"top words 1\")","0afeaff0":"corpus = create_corpus(1)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1\n        \n        \n","3a9e4e66":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y)\nplt.title(\"Punctuation disaster 1\")","55600ca7":"corpus = create_corpus(0)\n\ndictionary = defaultdict(int)\n\nimport string\n\nspecial_char = string.punctuation\n\nfor i in corpus:\n    if i in special_char:\n        dictionary[i] +=1","5e327f6c":"x,y = zip(*dictionary.items())\n\nplt.grid()\nplt.bar(x,y, color = 'r')\nplt.title(\"Punctuation disaster 0\")","147e4b94":"from collections import Counter","d104418d":"counter = Counter(corpus)\nmost = counter.most_common()\nx = []\ny = []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)","acd5677a":"plt.title(\"most common words\")\nplt.grid()\nsns.barplot(x = y, y = x)","addf7efb":"df = pd.concat([train, test])\ndf.shape","e1ed8e95":"df","0ed33391":"import re","e7ed17b0":"def remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)","e64788f4":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_url(x))","d3671d55":"df","67fb5cfc":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)","932c4ab0":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_html(x))","80b105c1":"df","ac23e775":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\" #emoticons\n                               u\"\\U0001F300-\\U0001F5FF\" #symbols&pics\n                               u\"\\U0001F680-\\U0001F6FF\" #transportation pic\n                               u\"\\U0001F1E0-\\U0001F1FF\" #flags\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"    \n                               \"]+\", flags = re.UNICODE)\n    return emoji_pattern.sub(r'', text)","fc40389e":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_emoji(x))","aef348fb":"df","0ffb48a4":"def remove_punctuation(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)","8a6c4978":"df[\"text\"] = df[\"text\"].apply(lambda x: remove_punctuation(x))","86049606":"df","5eda783b":"!pip install pyspellchecker","3a337b9d":"from spellchecker import SpellChecker","609cad1b":"spell = SpellChecker()\n\ndef correct_spellings(text):\n    corrected_text = []\n    \n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","e6df327f":"#df['text']=df['text'].apply(lambda x : correct_spellings(x))","65605e57":"from tqdm import tqdm\nfrom nltk.tokenize import word_tokenize","b4dc8378":"def create_corpus(df):\n    corpus = []\n    for tweet in tqdm(df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if \\\n        ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n        \n    return corpus","39788688":"corpus = create_corpus(df)","d17fc915":"embedding_dict = {}\n\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt','r') as glove:\n    for line in glove:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors\n        \nglove.close()","6f47beea":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","7224c4d9":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\n\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences,\n                          maxlen = MAX_LEN, \n                         truncating = 'post', \n                         padding = 'post')","f8d02310":"word_index = tokenizer_obj.word_index\nprint('number of unique words: ', len(word_index))","0e9e4a38":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    embedding_vector = embedding_dict.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","5bbf5879":"from keras import regularizers\n\nmodel = Sequential()\n\nglove_embedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix), \n                     input_length = MAX_LEN, \n                     trainable = False)\n\nmodel.add(glove_embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(128, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(128, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dense(256, activation = 'relu', kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate=1e-5)\n\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = [\"accuracy\"])","efd54c76":"model.summary()","3b704cb6":"train_data = tweet_pad[:train.shape[0]]\ntest_data = tweet_pad[train.shape[0]:]","ae4aa2b2":"X_train, X_test, y_train, y_test = train_test_split(train_data, train[\"target\"].values, test_size = 0.15)\n","203d954b":"hist = model.fit(X_train, y_train, batch_size = 64, epochs = 50, validation_data = (X_test, y_test))","b02a0b4e":"submit = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","a5777116":"y_predict = model.predict(test_data)\ny_predict = np.round(y_predict).astype(int).reshape(3263)\n\nsub=pd.DataFrame({'id':submit['id'].values.tolist(),'target': y_predict})\nsub.to_csv('submission.csv',index=False)\n","9aa79cf0":"## Common words","7757a002":"# 1. Quick look","a77fd290":"## Remove punctuation","e8961f3e":"# Glove vectorization (word2vec)","f1ce6e64":"## remove html tag","25987677":"## Remove emoji","de5b8773":"# 2. Create corpus","dd1739da":"# Data cleaning","5a48a211":"## Spelling checker\n\nAdditional: spelling checker for indonesian dataset","70906d52":"This notebook is forked from \nhttps:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\/notebook","43026424":"## punctuation","ae08da10":"## removing URLs","5fe8b6cf":"## distribution"}}