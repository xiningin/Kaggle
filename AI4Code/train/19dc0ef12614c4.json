{"cell_type":{"9cbf6441":"code","22e5f8ac":"code","73dd0488":"code","304b9038":"code","ed15b30b":"code","04f0fd56":"code","bdde2bb9":"code","cec93aab":"code","e687c18a":"code","4705c365":"code","f9996f7f":"code","9b584fea":"code","47980472":"code","91ed7f0b":"code","8a865d84":"markdown","efb993f5":"markdown","a160b7b1":"markdown","178e9c68":"markdown","961acfab":"markdown","fb2975da":"markdown","47f075c2":"markdown","f495ef96":"markdown","0b2a620a":"markdown","f1c513f8":"markdown","02c7194d":"markdown","d4053581":"markdown","d1e5978a":"markdown","b59f2d83":"markdown","9a73aae2":"markdown"},"source":{"9cbf6441":"import pandas as pd\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.mixture import GaussianMixture #For GMM clustering\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n","22e5f8ac":"review_data = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\nreview_data.dropna(inplace=True)\n#review_data=review_data.drop(['Unnamed: 0', 'Clothing ID'],axis=1)\nprint(review_data.dtypes)\nreview_data.head(5)","73dd0488":"# remove all the columns that are categorical variables\nreview_data_k_means=review_data.drop(['Unnamed: 0', 'Clothing ID','Class Name','Department Name','Title','Division Name','Recommended IND'],axis=1)","304b9038":"## adding the minimum value of sentiment score so as to remove negative sentiment scores\n\n\nimport pickle\n\nwith open(\"..\/input\/sentiment-score-new\/sent.txt\", \"rb\") as fp:   # Unpickling\n    b = pickle.load(fp)\n\nb\n\nreview_data_k_means['sent_score'] = b\n\n\nmin_sent = abs(np.min(review_data_k_means['sent_score']))\nreview_data_k_means['sent_score'] =  review_data_k_means['sent_score'] + abs(np.min(review_data_k_means['sent_score']))\nreview_data_k_means.drop('Review Text',axis=1,inplace=True)\n\n","ed15b30b":"sns.set_style('darkgrid')\nplt.title('Distribution of Each Column in the Data')\n\nfor i,col in enumerate(review_data_k_means.columns):\n    plt.figure(i)\n    sns.distplot(review_data_k_means[col])","04f0fd56":"# box cox transform can help with the skewed transformation\nfrom scipy.stats import boxcox\ntmp = review_data_k_means \n# adding one to each data variable to make it positive\ntmp = tmp+1\nfor i in tmp.columns:\n    tmp[i]=np.log(tmp[i])\n# log modified data    \nreview_data_mod = tmp\n# checking the distributions after transforming\nsns.set_style('darkgrid')\nplt.title('Distribution of Each Column in the Data')\n\nfor i,col in enumerate(review_data_mod.columns):\n    plt.figure(i)\n    sns.distplot(review_data_mod[col])\n\n\n# just take age and sent score - the variables that display a nearly normal distribution\n\n#review_data_mod = review_data_mod[['Age','sent_score']]\n\n","bdde2bb9":"\nreview_data_mod = review_data_mod[['Age','sent_score']]\n\n\nfrom scipy import stats\nreview_data_std = stats.zscore(review_data_mod)\nreview_data_std = np.array(review_data_std)\n\n\n\n\n","cec93aab":"# This snippet is sourced from https:\/\/www.kaggle.com\/mariadobreva\/k-means-clustering-in-python\n# also refer to https:\/\/stackoverflow.com\/questions\/32370543\/understanding-score-returned-by-scikit-learn-kmeans\/32371258\nimport pylab as pl\nnumber_of_clusters = range(1,20)\nkmeans = [KMeans(n_clusters=i,max_iter=1000,random_state=42) for i in number_of_clusters]\nscore = [-1*kmeans[i].fit(review_data_std).score(review_data_std) for i in range(len(kmeans))]\npl.plot((number_of_clusters),score)\npl.xlabel('Number of Clusters')\npl.ylabel('Score')\npl.title('Elbow Curve')\npl.show()\n\n\n    \n\n\n\n\n\n","e687c18a":"k_means_test = KMeans(n_clusters=6,max_iter=1000,random_state=42)","4705c365":"-1*k_means_test.fit(review_data_std).score(review_data_std)","f9996f7f":"review_data_k_means['labels'] = k_means_test.labels_","9b584fea":"size_of_each_cluster = review_data_k_means.groupby('labels').size().reset_index()\nsize_of_each_cluster.columns = ['labels','number_of_points']\nsize_of_each_cluster['percentage'] = size_of_each_cluster['number_of_points']\/np.sum(size_of_each_cluster['number_of_points'])\n\nprint(size_of_each_cluster)","47980472":"# a look at Age and Sentiment Scores\n# we subtract the added absolute value of the minimum sentiment score\nreview_data_k_means['sent_score'] = review_data_k_means['sent_score'] - min_sent\nsns.lmplot('Age','sent_score',data=review_data_k_means,hue='labels',fit_reg=False)\nplt.show()","91ed7f0b":"sent_score_labels = review_data_k_means[['sent_score','labels']]\n\nsent_score_labels.boxplot(by='labels',figsize=(20,10))\nplt.xticks(rotation=90)\nplt.show()\n\n\nage_labels = review_data_k_means[['labels','Age']]\n\nage_labels.boxplot(by='labels',figsize=(20,10))\nplt.xticks(rotation=90)\nplt.show()\n","8a865d84":"# Making sense of each cluster\nTo make sense of each cluster, we plot a grouped scatter chart with Age and sentiment score variables.","efb993f5":"# converting review text to sentiment score\nfrom afinn import Afinn\nafinn= Afinn()\nreview_data_k_means['Review Text'] = review_data_k_means['Review Text'].str.lower()\nreview_data_k_means['sent_score'] = review_data_k_means.apply(lambda row: afinn.score(row['Review Text']), axis=1)\n\nsent_score = review_data_k_means['sent_score'].values\nwith open(\"sent.txt\", \"wb\") as fp:   #Pickling\n    pickle.dump(sent_score, fp)\n\nwith open(\"sent.txt\", \"rb\") as fp:   # Unpickling\n    b = pickle.load(fp)\n\n#print(b)\n\n\n\n\n\n\n\n## adding the minimum value of sentiment score so as to remove negative sentiment scores\n\n\n\n\n\n","a160b7b1":"In this script, we look at one of the most popular  segmentation algorithms; the k-means clustering algorithm.  k-means clustering is an *unsupervised learning* algorithm that groups data into $K$ number of sets by using an iterative process. For each cluster, the centroid is chosen in such a way that the distance between the centroid and the data points in the cluster is minimized. Before we carry on, we drop variables that are nominal in nature. We also try to convert the review text to sentiment scores as K Means only works with numerical data.\n\n# Library Load\n\nHere we will be using the pandas library to do data processing. The sklearn modules also help with necessary scaling of variables.\n\n![](https:\/\/media.giphy.com\/media\/5nueNT9SqFQQX9HQra\/giphy.gif)\n","178e9c68":"Each product has a text review associated with it. Text can be converted to a numerical value using sentiment scores. One way to do this is to use predefined sentiment lexicons and match them accordingly. For this example, we will use the AFINN lexicon.\n\nAs the Afinn library is not available on Kaggle, I have tried to post this portion of the code in raw format. A negative value denotes negative sentiment and a positive score indicates positive sentiments. The scores that are returned are then stored in a `.txt` format and called using the `pickle.load` function as seen below.","961acfab":"Before doing any further processing , we need to check how each of our variables are distributed.","fb2975da":"**After the log  transformation, the sent_score variable and Age variable seem to have a normal like distribution. We will drop the other variables.**\n\n*As each variable scales differently, it is essential to bring them to a common scale.\nWe use z scaling here for this very purpose . The z score tells us how far each data point is from the mean in terms of standard deviations.*","47f075c2":"From the above box plots we get the following observations.\n\n\n**Labels**\n* Label 0 : Middle Age consumers consumer with fairly positive sentiments\n* Label 1 : Middle age consumers with relatively negative reviews\n* Label 2 : Older consumers who has had fairly positive reviews\n* Label 3 : Older  consumers who had positive reviews\n* Label 4 : Younger consumers who had faily positive reviews.\n* Label 5 : Fairly young consumer with positive reviews.","f495ef96":"* A majority of the variables are not normally distributed. K Means algorithm do not handle skewed distributions well. To transform each variable to a normal distrbution, we use the log transformation transformation. To ensure that all values are positive, we add 1 to all values.","0b2a620a":"* In the next portion of the notebook we remove all the categorical variables.","f1c513f8":"![](https:\/\/media.giphy.com\/media\/UPm0PtsTDCCDm\/giphy.gif)","02c7194d":"# Feedback\n\nThanks for reading! \nPlease let me know about how I  could have improved the results.\nYou can comment below\/ email me at padhokshaja@gmail.com\n\n\n","d4053581":"# Elbow Method\nAfter normalization, we need to choose the optimal number of clusters so as to get a good within cluster score.To achieve that we iterate through different K values and plot the total within cluster distances for each K value. We choose the K value that causes a sharp drop in total within cluster distance. This drop often resembles an \"Elbow\".","d1e5978a":"# Consumer Segmentation\nConsumer segmentation is the practice of dividing a customer base into groups. In each of these groups the individuals are similar to each other in some ways. This practice allows a business to create a group specific campaign or digital advertisement. This not only increases the effectiveness of the business's marketing efforts, it helps in reducing the marketing costs by reducing spends on certain audience groups that might react differently to a campaign. Segmentation also enables businesses to focus marketing activities more on certain platforms based on the characteristics of the segments.\nThough segmentation is used to create homogenous groups of individuals, we will use it to understand consumer behaviour in this notebook.","b59f2d83":"* Label 0 has 25% of the the points. The rest of the labels have nearly equal percentage of data points.\n","9a73aae2":"Even though the \"elbow\" is located at $K=3$, we will use  $K=6$ for better explanation of the data."}}