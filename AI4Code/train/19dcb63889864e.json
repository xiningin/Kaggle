{"cell_type":{"073c2764":"code","6daf456f":"code","991255fc":"code","53c0c786":"code","788a2449":"code","bc30557d":"code","4eeb120f":"code","0efb5a6d":"code","aa64c321":"code","e1be8ccb":"code","94892a27":"code","c0803ecc":"code","47b51fee":"code","1d23f781":"code","c6bab504":"code","11539c71":"code","bcf99721":"code","6b9f0041":"code","3dc630a5":"code","7bb6d35b":"code","004da52a":"code","8a73eb58":"code","4d4fec4c":"code","4aafa974":"code","afd1af1c":"code","facb3c51":"code","d7e6bda6":"code","5cf0db7e":"code","48f6e3f4":"code","2592b634":"code","5de37131":"code","59cc1a5f":"code","4356fcac":"code","61f592db":"code","75df7408":"markdown","7ba83fd1":"markdown"},"source":{"073c2764":"#import packages and dataset\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport sklearn.metrics as metrics\nimport os\n\ndf= pd.read_csv('\/kaggle\/input\/brasilian-houses-to-rent\/houses_to_rent_v2.csv')","6daf456f":"#Let's check for missing data\ndf.isnull().sum()","991255fc":"#totalprice correlation matrix\nk = 10 #number of variables for heatmap\nplt.figure(figsize=(16,8))\ncorrmat = df.corr()\n# picking the top 15 correlated features\ncols = corrmat.nlargest(k, 'total (R$)')['total (R$)'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n","53c0c786":"#finding outliers\nfig, ax = plt.subplots()\nax.scatter(x = df['hoa (R$)'], y = df['total (R$)'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('hora', fontsize=13)\nplt.show()","788a2449":"#Deleting outliers\ndf= df.drop(df[(df['hoa (R$)']>400000) & (df['total (R$)']>800000)].index)\n","bc30557d":"#checking for outliers again\nfig, ax = plt.subplots()\nax.scatter(x = df['hoa (R$)'], y = df['total (R$)'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('hora', fontsize=13)\nplt.show()","4eeb120f":"#deleting outliers\ndf= df.drop(df[(df['hoa (R$)']>100000) & (df['total (R$)']>200000)].index)","0efb5a6d":"#finding outliers\nfig, ax = plt.subplots()\nax.scatter(x = df['hoa (R$)'], y = df['total (R$)'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('hora', fontsize=13)\nplt.show()","aa64c321":"#deleting outliers\ndf= df.drop(df[(df['hoa (R$)']>60000) & (df['total (R$)']>90000)].index)","e1be8ccb":"#finding outliers\nfig, ax = plt.subplots()\nax.scatter(x = df['hoa (R$)'], y = df['total (R$)'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('hora', fontsize=13)\nplt.show()","94892a27":"#deleting outliers\ndf= df.drop(df[(df['total (R$)']>300000)].index)\ndf= df.drop(df[(df['hoa (R$)']>30000)].index)","c0803ecc":"#finding outliers\nfig, ax = plt.subplots()\nax.scatter(x = df['hoa (R$)'], y = df['total (R$)'])\nplt.ylabel('price', fontsize=13)\nplt.xlabel('hora', fontsize=13)\nplt.show()","47b51fee":"#target variable- sale price\nsns.distplot((df['total (R$)']), fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit((df['total (R$)']))\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df['total (R$)'], plot=plt)\nplt.show()\n","1d23f781":"sns.pairplot(df)","c6bab504":"#hora x total\nsns.lmplot(x='hoa (R$)',y='total (R$)',data=df) #hour is very correlated to total price.\n","11539c71":"plt.figure(figsize=(13,8))\nsns.boxplot(x= 'bathroom',y='total (R$)',data=df)\nplt.show()","bcf99721":"plt.figure(figsize=(13,8))\nsns.boxplot(x= 'rooms',y='total (R$)',data=df)\nplt.show()","6b9f0041":"#histogram of the number of rooms\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8))\nplt.hist(df['bathroom'])\nplt.title(\"number of rooms\")\nplt.xlabel(\"quantity\")\nplt.ylabel(\"number of rooms\")\nplt.grid()\nplt.show()\n","3dc630a5":"#casas x pre\u00e7o\nplt.scatter(df['area'],df['total (R$)'])\nplt.title(\"area x price\")\nplt.xlabel(\"area\")\nplt.ylabel(\"price\")\nplt.show()","7bb6d35b":"plt.figure(figsize=(10,10))\nsns.boxplot(x=\"city\", y= 'rooms', palette=[\"m\", \"g\"], data=df)\nplt.title('City and number of rooms')","004da52a":"plt.figure(figsize=(13,8))\nsns.boxplot(x= 'city',y='total (R$)',data=df)\nplt.show()","8a73eb58":"sns.countplot(df['animal'],hue = df['city']).set_title('animals allowed per city')","4d4fec4c":"sns.violinplot(x ='furniture', y ='rent amount (R$)', data = df, hue ='city').set_title=(\"furniture per city and total price\")","4aafa974":"#parking spaces\nplt.figure(figsize =(6,6))\nplt.subplot(2,1,1)\nax = sns.regplot(df['parking spaces'],df['rent amount (R$)'])\nplt.subplot(2,1,2)\nsns.distplot(df['parking spaces'],kde =False)","afd1af1c":"#fire insurance x total price per city\nplt.figure(figsize =(12,6))\nsns.violinplot(x ='city', y ='fire insurance (R$)', data = df,hue ='city')\n","facb3c51":"#fire insurance is very related to total price\nax = sns.regplot(df['fire insurance (R$)'],df['rent amount (R$)'])\n","d7e6bda6":"# Categorical boolean mask\ncategorical_feature_mask = df.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = df.columns[categorical_feature_mask].tolist()\n\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: labelencoder.fit_transform(col.astype(str)))\n","5cf0db7e":"#selecting dependent and independent variables\nX= df.drop([\"total (R$)\"], axis=1)\ny= df.loc[:,[\"total (R$)\"]]\n","48f6e3f4":"#split the dataset\nfrom sklearn.model_selection import train_test_split as tts\nX_train,X_test,y_train,y_test = tts(X,y,test_size =0.3)","2592b634":"#building the machine learning models\nacc= []\n\n\n#Decision Tree Regression\nfrom sklearn.tree import DecisionTreeRegressor as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nfrom sklearn.metrics import r2_score\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['DTR',r2_score(y_test,model.predict(X_test))])\n\n\n#Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['RFN',r2_score(y_test,model.predict(X_test))])\n\n\n#Linear regression\nfrom sklearn.linear_model import LinearRegression as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['LIR',r2_score(y_test,model.predict(X_test))])\n\n\n#SVM Regression\nfrom sklearn.svm import SVR as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['SVM',r2_score(y_test,model.predict(X_test))])\n\n\n\n#K Nearest Neighbour Regression\nfrom sklearn.neighbors import KNeighborsRegressor as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['KNNR',r2_score(y_test,model.predict(X_test))])\n\n#Lasso Regression\nfrom sklearn.linear_model import Lasso as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['LaR',r2_score(y_test,model.predict(X_test))])\n\n#Ridge Regression\nfrom sklearn.linear_model import Ridge as regr\nmodel =regr()\nmodel.fit(X_train,y_train)\nprint(r2_score(y_test,model.predict(X_test)))\nacc.append(['RiR',r2_score(y_test,model.predict(X_test))])\n\n\n#Different Algorithms and their performance\nacc.sort(key = lambda y:y[1],reverse =True)\n","5de37131":"#print all the models accurancy score\nprint(acc)","59cc1a5f":"#As the RiR tops the list we will use it as our final model!!!\nfrom sklearn.linear_model import Ridge as regr\nmodel =regr()\nmodel.fit(X_train,y_train)","4356fcac":"#making the predictions\ny_pred = model.predict(X_test)","61f592db":"#ploting the model prediction with the y_test values the check the model prediction power\nax1 = sns.distplot(y_test,hist=False,kde =True,color =\"r\",label =\"Actual Value\")\nsns.distplot(model.predict(X_test),color =\"b\",hist = False,kde =True, label = \"Preicted Value\",ax =ax1)\n","75df7408":"Hello everyone :)\n\nThe first part of this notebook contains data exploration and data engineering. The second and last part contains the machine leaning models, models evaluation and total price predictions.\n\nI hope you enjoy it and feel free leave a comment, any feedback is welcome!!\n","7ba83fd1":"Models used:\n\n* Decision Tree Regression;\n* Random Forest Regression;\n* Linear Regression;\n* SVM Regression;\n* K Nearest Neighbour Regression;\n* Lasso Regression;\n* Ridge Regression;"}}