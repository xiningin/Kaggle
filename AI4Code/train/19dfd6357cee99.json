{"cell_type":{"45b97f58":"code","2c24c560":"code","5e7a6c16":"code","bb84ab24":"code","55277e00":"code","fe48fcc1":"code","d744e738":"code","92a2d46b":"code","2bc75e3e":"code","0c9786f8":"code","b64be405":"code","976b8777":"code","4359de6f":"code","dafc793d":"code","4a039487":"code","78d8010a":"code","d61b8c99":"code","c6b0c780":"code","b44b542e":"code","37b1e190":"code","8cd442dc":"code","d02c0c47":"code","aaecee82":"code","a2571e50":"code","32acb46b":"code","b5238586":"code","4e57f996":"code","dec4745c":"code","525ec6ae":"code","597f732a":"code","a8c1e7e1":"code","5e186d1e":"code","e1c79f98":"code","e49a2f98":"code","33c67657":"code","70f26ca1":"code","d917f9a9":"code","62169fb3":"code","aca4bdb5":"code","cd70b2e8":"code","9a7398e7":"code","a48cf307":"code","314fd0c3":"code","2f4bc427":"code","b0db5571":"code","8e8bbf7c":"code","ef8f41e4":"code","16a1572a":"code","bb80d0a3":"code","aa443bfa":"code","f13f06b9":"code","2d2d4bb8":"code","5c00305e":"code","13e32112":"markdown","327d79ed":"markdown","56baab84":"markdown","864e4c61":"markdown","b1b40869":"markdown","906acd29":"markdown","7217122b":"markdown","2592b502":"markdown","0fc447e4":"markdown","b58f0db2":"markdown","267f53a8":"markdown","cc96d7bd":"markdown","1497d6bc":"markdown","a8db3fad":"markdown","03d7c05d":"markdown"},"source":{"45b97f58":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling","2c24c560":"#data data\ndf =pd.read_csv('..\/input\/bank-churn\/Bank_churn_modelling.csv')","5e7a6c16":"df.shape","bb84ab24":"df.head()","55277e00":"df.columns","fe48fcc1":"df.info()","d744e738":"df.describe()","92a2d46b":"df.Gender.unique()","2bc75e3e":"df.Geography.unique()","0c9786f8":"df.profile_report()","b64be405":"#check for duplicate entries\ndf.duplicated().sum()","976b8777":"# check for missing values\ndf.isnull().sum()","4359de6f":"df.columns","dafc793d":"#df.drop([\"Surname\",\"RowNumber\",\"CustomerId\"],axis=1,inplace=True)\ndf.columns","4a039487":"x=df[['CreditScore', 'Geography', 'Gender', 'Age','Balance','NumOfProducts', 'IsActiveMember']]\ny=df[\"Exited\"]","78d8010a":"x.head()","d61b8c99":"from sklearn.preprocessing import LabelEncoder\nlel = LabelEncoder()\nx[\"Gender\"]= lel.fit_transform(x[\"Gender\"])\nx.head()","c6b0c780":"y.head()","b44b542e":"#here in Gender column female changed to 0 and male changed to 1\nx.head(8)","37b1e190":"#onehotencoding for geography\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nct = ColumnTransformer([(\"encoder\",OneHotEncoder(),[1])],remainder=\"passthrough\")# here 1 represents column geography\n#here we can use remainder=\"drop\" ,here it only represents geography\nx = ct.fit_transform(x)","8cd442dc":"x.shape","d02c0c47":"x=pd.DataFrame(x)","aaecee82":"x.head(10)","a2571e50":"#here 0-france 1-germany 2-spain (in alaphabetical model)","32acb46b":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx=sc.fit_transform(x)","b5238586":"#splitting data info train and test set\nfrom sklearn.model_selection import train_test_split\nxtr,xts,ytr,yts = train_test_split(x,y,test_size=0.2)\nprint(x.shape)\nprint(xtr.shape)\nprint(xts.shape)","4e57f996":"from sklearn.linear_model import LogisticRegression\nmodel =LogisticRegression()","dec4745c":"#train the model -using training data -xtr,ytr\nmodel.fit(xtr,ytr)","525ec6ae":"#france, cs=580, age=58, Male,numofprod=3,isactmember=0,balance=456782\nnew_customer=[[1,0,0,580,1,58,456782,3,0]]\nmodel.predict(new_customer)","597f732a":"#here [1] is exited","a8c1e7e1":"#check perfomance of model on test data\n# getting prediction for test data\nypred = model.predict(xts)\nfrom sklearn import metrics\nmetrics.accuracy_score(yts,ypred)","5e186d1e":"#here the accuracy which we got is 0.79 which is not good \n#after adding standardization it become 80% 0r 0.80","e1c79f98":"#calcuate recall\nmetrics.recall_score(yts,ypred)","e49a2f98":"#here we used feature scaling to get features in same range","33c67657":"from sklearn.neighbors import KNeighborsClassifier\nmodel2 = KNeighborsClassifier(n_neighbors = 3)#no of neighbors is hpyer parameter\nmodel2.fit(xtr,ytr)\n","70f26ca1":"ypred2=model2.predict(xts)\nmetrics.accuracy_score(yts,ypred2)#here it checks for both class 1 and class 0( here 0 and 1  are told as class)","d917f9a9":"metrics.recall_score(yts,ypred2)#here it ckecks only for class 1","62169fb3":"from sklearn.tree import DecisionTreeClassifier\nmodel3= DecisionTreeClassifier(criterion=\"gini\")\n#here we are facing the problem of overfitting\n#train the model\nmodel3.fit(xtr,ytr)","aca4bdb5":"ypred3=model3.predict(xts)\nmetrics.accuracy_score(yts,ypred3)","cd70b2e8":"metrics.recall_score(yts,ypred3)","9a7398e7":"from sklearn.tree import DecisionTreeClassifier\nmodel3= DecisionTreeClassifier(criterion=\"entropy\")\n#train the model\nmodel3.fit(xtr,ytr)","a48cf307":"ypred3=model3.predict(xts)\nmetrics.accuracy_score(yts,ypred3)","314fd0c3":"metrics.recall_score(yts,ypred3)","2f4bc427":"metrics.recall_score(ytr,model3.predict(xtr))","b0db5571":"from sklearn.tree import DecisionTreeClassifier\nmodel3= DecisionTreeClassifier(criterion=\"gini\",max_depth=8,min_samples_leaf=10)\n#here max_depth and min_samples_leaf is used to control overfitting\n#train the model\nmodel3.fit(xtr,ytr)","8e8bbf7c":"ypred3=model3.predict(xts)\nmetrics.accuracy_score(yts,ypred3)","ef8f41e4":"metrics.recall_score(yts,ypred3)","16a1572a":"metrics.recall_score(ytr,model3.predict(xtr))","bb80d0a3":"import graphviz\nfrom sklearn import tree\nfname=['France','Germany','Spain','CreditScore','Gender','Age','Balance','NumofProducts','IsActiveMember']\ncname=['Not Excited','Excited']\ngraph_data = tree.export_graphviz(model3,out_file=None,feature_names=fname,class_names=cname,filled=True,rounded=True)\ngraph=graphviz.Source(graph_data)","aa443bfa":"from sklearn.ensemble import RandomForestClassifier\nmodel4 = RandomForestClassifier(n_estimators=50,criterion='gini',max_depth=10,min_samples_leaf=20)\nmodel4.fit(xtr,ytr)","f13f06b9":"ypred4=model4.predict(xts)\nmetrics.accuracy_score(yts,ypred4)","2d2d4bb8":"metrics.recall_score(yts,ypred4)","5c00305e":"metrics.recall_score(ytr,model4.predict(xtr))","13e32112":"#above part where we did standardization is part of optimisation and tunning","327d79ed":"# Random forest","56baab84":"# 6. Perfomance Analysis","864e4c61":"# 100 customer\n1. actual-\n * 90-0-notleaving\n * 10-1.leaving\n\n2. ML model -1\n  * 95-0-notleaving -90\n  * 05-1-leaving -05\n\n3. Accuracy = 95%\n  * recall=50% (accuracy of class1)\n  * 95% is only solving 50% of the whole scenario\n  * this means 10 are leaving but model says 5 are leaving\n \n4. ML model -2 \n  * 70-0-notleaving -70\n  * 30-1-leaving -10\n  * accuracy = 80%\n  * recall= 100%\n \n ","b1b40869":"# 2. data cleaning","906acd29":"# 1.Data Exploration","7217122b":"# 3. Feature Engineering\n * Feature Extraction\n * Feature Selection","2592b502":"# 7.Optimization and tuning","0fc447e4":"# KNN  ALGO","b58f0db2":"# Data Preprocessing","267f53a8":"#  therefore the model id 0.79% accurate and the perfomance is not so good or not so bad","cc96d7bd":"**Bank churn modeling**\n* Data Exploration\n* Data Cleaning\n* Feature Engineering\n* Data Preprocessing\n* Apply ML Algorithim\n* Perfomance Analysis\n* Optimization and Tunning\n1. 1.KNN\n1. 2.Decision Tree\n1. 3.Random Forest","1497d6bc":"# 5. Apply ML algorithim","a8db3fad":"# accuracy=(no of correct prediction\/total no of prediction)*100","03d7c05d":"# Decision Tree Algorithim"}}