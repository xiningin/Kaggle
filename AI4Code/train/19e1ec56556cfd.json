{"cell_type":{"c412a61d":"code","28f74e91":"code","6538370b":"code","b05e87c9":"code","98b16f55":"code","56d67f6b":"code","8db14ce8":"code","2c049bd5":"code","9151630d":"code","1ca158b3":"code","34922435":"code","675e9e30":"code","c7d65df6":"code","2cf97c94":"code","49f05cef":"code","d649a10c":"code","42e64002":"code","f98bd3f8":"code","321acf19":"code","f79abaf9":"code","f37571fb":"code","9d97a43f":"code","faf25353":"code","d399267b":"code","eda3e148":"code","393ea93d":"code","ec519ec5":"code","9f5bce21":"code","523f64c6":"code","32ffcda0":"code","6b1a743c":"code","e533c561":"code","abe1fdd0":"code","ca60c80d":"markdown","3eb92c86":"markdown","495a1960":"markdown","8f8830ab":"markdown","792bf892":"markdown","4772cbd1":"markdown","2bd77d26":"markdown","3fb51484":"markdown","711fca68":"markdown","eb175e89":"markdown","b917b1ae":"markdown","5e817dfd":"markdown","622dbbb2":"markdown","f39565ae":"markdown","050e4f1d":"markdown","6e5f4465":"markdown","43bf5836":"markdown","1821caf1":"markdown","959df2b6":"markdown"},"source":{"c412a61d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28f74e91":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","6538370b":"np.random.seed(0)","b05e87c9":"full_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","98b16f55":"excerpts = full_data[\"excerpt\"]\ntarget = full_data[\"target\"]","56d67f6b":"excerpts = excerpts.str.lower()","8db14ce8":"from nltk.stem import PorterStemmer\nps = PorterStemmer()\nexcerpts = excerpts.apply(ps.stem)","2c049bd5":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\nexcerpts = excerpts.apply(wnl.lemmatize)","9151630d":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\nexcerpts = excerpts.apply(lambda text: remove_stopwords(text))","1ca158b3":"excerpts_train, excerpts_val, y_train, y_val = train_test_split(excerpts, target, test_size=0.30)","34922435":"vectorizer = CountVectorizer()\nvectorizer.fit(excerpts_train)","675e9e30":"X_train = vectorizer.transform(excerpts_train)\nX_val = vectorizer.transform(excerpts_val)\nprint(X_train[3].size)","c7d65df6":"vec = TfidfVectorizer(max_features = 5000)\nX_train = vec.fit_transform(excerpts_train).toarray()\nX_val = vec.transform(excerpts_val).toarray()","2cf97c94":"from sklearn.decomposition import PCA\nprincipal=PCA(n_components=1000)\nprincipal.fit(X_train)\nX_train = principal.transform(X_train)\nX_val = principal.transform(X_val)","49f05cef":"rf_model = RandomForestRegressor(random_state=42)\nrf_model.fit(X_train,y_train)","d649a10c":"reg = LinearRegression()\nreg.fit(X_train,y_train)","42e64002":"dl_model = keras.Sequential([\n    # the hidden ReLU layers\n    layers.Dense(units=1024, activation='relu', input_shape=[5000]),\n    layers.Dense(units=512, activation='relu'),\n    layers.Dense(units=128, activation='relu'),\n    layers.Dense(units=32, activation='relu'),\n    # the linear output layer \n    layers.Dense(units=1),\n])\n\n# Define an early stopping callback\nearly_stopping = callbacks.EarlyStopping(\n    patience=5,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\ndl_model.compile(\n    optimizer='adam',\n    loss='mae',\n)\n\nhistory = dl_model.fit(\n    X_train, y_train,\n    validation_data=(X_val, y_val),\n    batch_size=512,\n    epochs=100,\n    callbacks=[early_stopping]\n)","f98bd3f8":"import catboost\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool, cv\nfrom sklearn.model_selection import GridSearchCV\n\n## Prepara a pool of train and validation set\npool_train=Pool(X_train,y_train)\npool_val=Pool(X_val,y_val)\n### Define a cv function to fit on data and find the optimal number of iteration keeping other parameters fixed\n### Function takes input = catboost object with default params , train data ,train y data \ndef modelfit(params,poolX,useTrainCV=True,cv_folds=5,early_stopping_rounds=40):\n    if useTrainCV:\n        cvresult = cv(params=params, pool=poolX,nfold=cv_folds,early_stopping_rounds=early_stopping_rounds,plot=True)\n    return cvresult ## return dataframe for the iteration till the optimal iteration is reached\n\n## Prepara a cv class\nparams={\n    'loss_function':'RMSE'\n}\n\n### Object return the optimal number of trees to grow\nn_est=modelfit(params,pool_train)","321acf19":"from sklearn import metrics\n### Fit the model with iteration=664\ncboost1=CatBoostRegressor(iterations=664,loss_function='RMSE',random_seed=123)\ncboost1.fit(X_train,y_train)\n#Predict training set:\ntrain_predictions = cboost1.predict(X_train)\n#Print model report:\nprint(\"\\nModel Report Train\")\nprint(\"Root Mean Square Error : %.4g\" % metrics.mean_squared_error(y_train, train_predictions))\nprint(\"R^2 Score (Train): %f\" % metrics.r2_score(y_train, train_predictions))","f79abaf9":"readable_preds = rf_model.predict(X_val)\nrf_val_mae = mean_absolute_error(y_val,readable_preds)\nprint(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))","f37571fb":"readable_preds = reg.predict(X_val)\nreg_val_mae = mean_absolute_error(y_val,readable_preds)\nprint(\"Validation MAE for Linear regression Model: {}\".format(reg_val_mae))","9d97a43f":"readable_preds = dl_model.predict(X_val)\ndl_val_mae = mean_absolute_error(y_val,readable_preds)\nprint(\"Validation MAE for Deep learning Model: {}\".format(dl_val_mae))","faf25353":"readable_preds = cboost1.predict(X_val)\ncb_val_mae = mean_absolute_error(y_val,readable_preds)\nprint(\"Validation MAE for Catboost Model: {}\".format(cb_val_mae))","d399267b":"test_data = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","eda3e148":"test_excerpts = test_data[\"excerpt\"]","393ea93d":"test_excerpts = test_excerpts.str.lower()\ntest_excerpts = test_excerpts.apply(ps.stem)\ntest_excerpts = test_excerpts.apply(wnl.lemmatize)\ntest_excerpts = test_excerpts.apply(lambda text: remove_stopwords(text))","ec519ec5":"#X_test = vectorizer.transform(test_excerpts)","9f5bce21":"X_test = vec.transform(test_excerpts).toarray()\nX_test = principal.transform(X_test)","523f64c6":"#test_preds = rf_model.predict(X_test)","32ffcda0":"#test_preds = reg.predict(X_test)","6b1a743c":"#test_preds = dl_model.predict(X_test)","e533c561":"test_preds = cboost1.predict(X_test)","abe1fdd0":"x_sub = test_data[[\"id\"]].copy()\nx_sub[\"target\"] = test_preds\nx_sub.to_csv('submission.csv', index = False)\nx_sub","ca60c80d":"## Create a train-test split","3eb92c86":"## Loading required packages","495a1960":"## Extract the text separately","8f8830ab":"## Train a random forest and linear regression models","792bf892":"## Obtain the predictions for the test set","4772cbd1":"## Transform the test data using the count vectorizer","2bd77d26":"## Text processing: Removing stopwords","3fb51484":"## Evaluate performance on validation set","711fca68":"## Text processing 1: Converting to lower case","eb175e89":"## Fit count vectorizer based on the training vocabulary","b917b1ae":"## Performing pre-processing","5e817dfd":"## Read the test dataset","622dbbb2":"## Set seed to get the same results each time","f39565ae":"## Text processing 2: Performing stemming ","050e4f1d":"## Convert to submission format","6e5f4465":"## Text processing 3: Lemmatization","43bf5836":"## Transform training and test sets to count vectors","1821caf1":"## Separate the text from the target- readability score","959df2b6":"## Load the training data"}}