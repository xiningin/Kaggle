{"cell_type":{"888a1923":"code","faf29943":"code","c91dd821":"code","d367c2b0":"code","73adf6dd":"code","ebc189db":"code","81fe253c":"code","3dcd9b13":"code","a198ab07":"code","332d033e":"code","1a26b051":"code","f96bed8c":"code","74f6385e":"code","8ccadbf8":"code","10dea536":"code","ad38e38f":"code","6ed4e6fa":"code","25ea4a3b":"code","e63b1f9b":"code","9a33cba0":"code","95188252":"code","9adc3fcb":"code","74185db6":"code","6bf50049":"code","cbb6a883":"code","6c4e79c3":"code","775ce349":"code","d27ecd32":"code","9c0a51c6":"code","770ed0d0":"code","5135bd4c":"markdown","8b483edc":"markdown","5f77a569":"markdown","0887d9b4":"markdown","c0b24c81":"markdown"},"source":{"888a1923":"###\n# Code adopted from https:\/\/github.com\/WillKoehrsen\/machine-learning-project-walkthrough\/blob\/master\/Machine%20Learning%20Project%20Part%202.ipynb\n# By WillKoehrsen\n###\nimport pandas as pd\nimport numpy as np\nfrom pprint import pprint\npd.set_option('display.max_columns', None)\n\nfrom sklearn.datasets import load_boston\ndata = load_boston()\ndf = pd.concat([pd.DataFrame(data['data'], columns = data['feature_names']), \n                pd.DataFrame(data['target'], columns=['MEDV'])], axis=1)","faf29943":"from sklearn.model_selection import train_test_split\nX = df.drop(['MEDV'], axis=1)\ny = df['MEDV']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","c91dd821":"from sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.pipeline import make_pipeline\n#Algorithms\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import RobustScaler","d367c2b0":"model=make_pipeline(RobustScaler(), RandomForestRegressor(random_state=42))\nmodel_fit=model.fit(X_train, y_train)","73adf6dd":"y_pred=model_fit.predict(X_test)\nprint('Model Score(R2 Score):', round(model_fit.score(X_test, y_test), 3))\nprint('MAE:', round(mean_absolute_error(y_test, y_pred), 3))","ebc189db":"SGDRegressor(random_state=42).get_params()","81fe253c":"RandomForestRegressor(random_state=42).get_params()","3dcd9b13":"KNeighborsRegressor().get_params()","a198ab07":"XGBRegressor(random_state=42).get_params()","332d033e":"grid = {\n    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n    'max_iter': [1000], # number of epochs\n    'loss': ['squared_loss'], # logistic regression,\n    'penalty': ['l2']\n}","1a26b051":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# Create the model to use for hyperparameter tuning\nmodel = SGDRegressor(random_state=42)\n\n# Set up the random search with 4-fold cross validation\nrandom_sgdr = RandomizedSearchCV(estimator=model,\n                                 param_distributions=grid,\n                                 scoring = 'roc_auc',\n                                 n_jobs = -1,\n                                 verbose = 1,\n                                 random_state=42)","f96bed8c":"random_sgdr.fit(X, y)","74f6385e":"sgdrmodel=random_sgdr.best_estimator_\nsgdrmodel=make_pipeline(RobustScaler(), sgdrmodel)\nsgdrmodel_fit=sgdrmodel.fit(X_train, y_train)","8ccadbf8":"y_pred=sgdrmodel_fit.predict(X_test)\nprint('Model Score(R2 Score):', round(sgdrmodel_fit.score(X_test, y_test), 3))\nprint('MAE:', round(mean_absolute_error(y_test, y_pred), 3))","10dea536":"sgdrmodel","ad38e38f":"# Number of trees used in the boosting process\nn_estimators = [500, 900, 1100]\n\n# Maximum depth of each tree\nmax_depth = [3, 5, 10]\n\n# Minimum number of samples per leaf\nmin_samples_leaf = [2, 4, 6]\n\n# Minimum number of samples to split a node\nmin_samples_split = [4, 6]\n\n# Maximum number of features to consider for making splits\nmax_features = ['auto', 'sqrt', 'log2', None]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {'n_estimators': n_estimators,\n                       'max_depth': max_depth,\n                       'min_samples_leaf': min_samples_leaf,\n                       'min_samples_split': min_samples_split,\n                       'max_features': max_features}","6ed4e6fa":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# Create the model to use for hyperparameter tuning\nmodel = RandomForestRegressor(random_state = 42)\n\n# Set up the random search with 4-fold cross validation\nrandom_cv = RandomizedSearchCV(estimator=model,\n                         param_distributions=hyperparameter_grid,\n                         scoring = 'neg_mean_absolute_error',\n                         n_jobs = -1, \n                         verbose = 1,\n                         return_train_score = True)","25ea4a3b":"random_cv.fit(X, y)","e63b1f9b":"hmodel=random_cv.best_estimator_\nhmodel=make_pipeline(RobustScaler(), hmodel)\nhmodel_fit=hmodel.fit(X_train, y_train)","9a33cba0":"y_pred=hmodel_fit.predict(X_test)\nprint('Model Score(R2 Score):', round(hmodel_fit.score(X_test, y_test), 3))\nprint('MAE:', round(mean_absolute_error(y_test, y_pred), 3))","95188252":"# Define our candidate hyperparameters\nhp_candidates = [{'n_neighbors': [2,3,4,5,6], \n                  'weights': ['uniform','distance']}]","9adc3fcb":"# Create the model to use for hyperparameter tuning\nmodel = KNeighborsRegressor()\n\n# Set up the random search with 4-fold cross validation\nrandom_knr = RandomizedSearchCV(estimator=model,\n                             param_distributions=hp_candidates,\n                             scoring = 'neg_mean_absolute_error',\n                             n_jobs = -1, \n                             verbose = 1,\n                             return_train_score = True)","74185db6":"random_knr.fit(X, y)","6bf50049":"knnmodel=random_knr.best_estimator_\nknnmodel=make_pipeline(RobustScaler(), knnmodel)\nknnmodel_fit=knnmodel.fit(X_train, y_train)","cbb6a883":"y_pred=knnmodel_fit.predict(X_test)\nprint('Model Score(R2 Score):', round(knnmodel_fit.score(X_test, y_test), 3))\nprint('MAE:', round(mean_absolute_error(y_test, y_pred), 3))","6c4e79c3":"parameters = {'objective':['reg:linear'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'silent': [1],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}","775ce349":"# Create the model to use for hyperparameter tuning\nmodel = XGBRegressor(random_state=42)\n\n# Set up the random search with 4-fold cross validation\nrandom_xgbr = RandomizedSearchCV(estimator=model,\n                             param_distributions=parameters,\n                             scoring = 'neg_mean_absolute_error',\n                             n_jobs = -1, \n                             verbose = 1,\n                             n_iter=10,\n                             return_train_score = True)","d27ecd32":"random_xgbr.fit(X, y)","9c0a51c6":"xgbrmodel=random_xgbr.best_estimator_\nxgbrmodel=make_pipeline(RobustScaler(), xgbrmodel)\nxgbrmodel_fit=xgbrmodel.fit(X_train, y_train)","770ed0d0":"y_pred=xgbrmodel_fit.predict(X_test)\nprint('Model Score(R2 Score):', round(xgbrmodel_fit.score(X_test, y_test), 3))\nprint('MAE:', round(mean_absolute_error(y_test, y_pred), 3))","5135bd4c":"# Fitting a model","8b483edc":"## XGBRegressor","5f77a569":"## RandomForestRegressor","0887d9b4":"## KNeighborsRegressor","c0b24c81":"## SGDRegressor"}}