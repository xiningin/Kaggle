{"cell_type":{"683239e2":"code","dad0b537":"code","be16c10d":"code","aeda70d1":"code","0ff54b9f":"code","80e5e22e":"code","2dfa8290":"code","4f645762":"code","7ab931f3":"code","2a86cb75":"code","26b0b824":"code","6630842e":"code","3f21b054":"code","04202cae":"code","5b367aef":"code","12704c16":"code","2e08b8a6":"code","c6c99ad8":"code","4419892f":"code","790771d2":"code","a912e2d6":"markdown","2fb1356e":"markdown","23aa2b8e":"markdown","af4fc760":"markdown","7a4cc100":"markdown","bdb1f51a":"markdown"},"source":{"683239e2":"from keras.layers import Bidirectional, Concatenate, Dot, Input, Permute, Reshape\nfrom keras.layers import RepeatVector, Dense, Activation, GRU, Lambda, Add\nfrom keras import optimizers, regularizers, initializers\nfrom keras.engine.topology import Layer\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nfrom keras.utils import Sequence\nfrom keras.callbacks import ModelCheckpoint, Callback\nimport keras.backend as K\nimport keras\nimport numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\nimport random\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n\nrandom.seed(40)\n\ndef preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n    \n    X, Y = zip(*dataset)\n    \n    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n    \n    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n\n    return X, np.array(Y), Xoh, Yoh\n        \n        \ndef string_to_int(string, length, vocab):\n    \"\"\"\n    Converts all strings in the vocabulary into a list of integers representing the positions of the\n    input string's characters in the \"vocab\"\n    \n    Arguments:\n    string -- input string\n    length -- the number of time steps you'd like, determines if the output will be padded or cut\n    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n    \n    Returns:\n    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n    \"\"\"\n\n    u = vocab[\"<unk>\"]   \n    if len(string) > length:\n        string = string[:length]\n        \n    rep = list(map(lambda x: vocab.get(x, u), string))\n    \n    if len(string) < length:\n        rep += [vocab['<pad>']] * (length - len(string))\n    \n    #print (rep)\n    return rep\n\n\ndef int_to_string(ints, inv_vocab):\n    \"\"\"\n    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n    \n    Arguments:\n    ints -- list of integers representing indexes in the machine's vocabulary\n    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n    \n    Returns:\n    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n    \"\"\"\n    \n    l = [inv_vocab[i] for i in ints]\n    return l\n\n\ndef softmax(x, axis=-1):\n    \"\"\"Softmax activation function.\n    # Arguments\n        x : Tensor.\n        axis: Integer, axis along which the softmax normalization is applied.\n    # Returns\n        Tensor, output of softmax transformation.\n    # Raises\n        ValueError: In case `dim(x) == 1`.\n    \"\"\"\n    ndim = K.ndim(x)\n    if ndim == 2:\n        return K.softmax(x)\n    elif ndim > 2:\n        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n        s = K.sum(e, axis=axis, keepdims=True)\n        return e \/ s\n    else:\n        raise ValueError('Cannot apply softmax to a tensor that is 1D')","dad0b537":"K.set_floatx('float32')\n#K.set_epsilon(1e-05)","be16c10d":"n_a = 256\nn_s = 256\nBATCH_SIZE = 512\ndata_path = '..\/input\/ocrtrain.csv'\nlines = open(data_path).read().split('\\n')","aeda70d1":"input_characters = set()\ntarget_characters = set()\ndata_path = '..\/input\/ocrtrain.csv'\nlines = open(data_path).read().split('\\n')\n\nmax_x = 5\nmax_y = 5\nmax_len = 50\nall_idx = []\n#for line in lines[0: len(lines) - 1]:\nfor idx, line in enumerate(lines[0: len(lines) - 1]):\n    input_text, target_text = line.split(',')\n    input_text = input_text[1:-2]\n    input_text = input_text.split(' ')\n    target_text = target_text.split(' ')\n    if len(input_text) <= max_len and len(target_text) <= max_len:\n        all_idx.append(idx)\n        \n        if len(input_text) > max_x:\n            max_x = len(input_text)\n\n        if len(target_text) > max_y:\n            max_y = len(target_text)\n\n        for char in input_text:\n            if char not in input_characters:\n                input_characters.add(char)\n        for char in target_text:\n            if char not in target_characters:\n                target_characters.add(char)\n                \nTx = max_x\nTy = max_y\n#all_idx = list(range(0,len(lines) - 1))\ntrain_idx, valid_idx = train_test_split(all_idx, test_size=0.05, random_state = 43)\n\ninput_characters = sorted(list(input_characters)) + ['<unk>', '<pad>']\ntarget_characters = sorted(list(target_characters)) + ['<unk>', '<pad>']\nreactants_vocab = {v:k for k,v in enumerate(input_characters)}\nproducts_vocab = {v:k for k,v in enumerate(target_characters)}\ninv_products_vocab = {v:k for k,v in products_vocab.items()}","0ff54b9f":"def load_data(idx):\n    dataset = []\n    #for line in lines[idx]:\n    line = lines[idx]\n    input_text, target_text = line.split(',')\n    input_text = input_text[1:-2]\n    input_text = input_text.split(' ')\n    target_text = target_text.split(' ')\n    ds = (input_text,target_text)\n    dataset.append(ds)\n    return dataset    ","80e5e22e":"class SMILESDataGenerator(keras.utils.Sequence):\n            \n    def __init__(self, all_idx, batch_size, shuffle = False):\n        self.all_idx = all_idx\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n    \n    def __len__(self):\n        return int(np.ceil(len(self.all_idx) \/ float(self.batch_size)))\n    \n    def __getitem__(self, idx):\n        #indexes = [idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_indexes = self.all_idx[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        #all_idx = self.all_idx[indexes]\n        X = np.zeros((self.batch_size, Tx, len(reactants_vocab)))\n        y = np.zeros((self.batch_size, Ty, len(products_vocab)))\n        # Generate data\n        for i, idx_ in enumerate(batch_indexes):\n            X[i,:,:], y[i,:,:] = self.__load_data(idx_)\n        s0 = np.zeros((self.batch_size, n_s))\n        y = list(y.swapaxes(0,1))\n        X = [X, s0]\n        return X,y\n        \n    def on_epoch_end(self):\n        \n        # Updates indexes after each epoch\n        #self.indexes = np.arange(len(self.all_idx))\n        if self.shuffle == True:\n            np.random.shuffle(self.all_idx)\n\n    def __iter__(self):\n        \"\"\"Create a generator that iterate over the Sequence.\"\"\"\n        for item in (self[i] for i in range(len(self))):\n            yield item\n            \n    def __load_data(self, idx_):\n        dataset = load_data(idx_)\n        X, Y, Xoh, Yoh = preprocess_data(dataset, reactants_vocab, products_vocab, Tx, Ty)\n        return Xoh, Yoh","2dfa8290":"repeator = RepeatVector(1)\npermutor = Permute((2,1))\ndotor1 = Lambda(lambda x: K.batch_dot(x[0],x[1]))\nactivator = Activation('softmax')\ndotor2 = Lambda(lambda x: K.batch_dot(x[0],x[1]))\n\ndef one_step_attention(a, s_prev):\n    \"\"\"\n    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n    \"alphas\" and the hidden states \"a\" of the Bi-GRU.\n    \n    Arguments:\n    a -- hidden state output of the Bi-GRU, numpy-array of shape (m, Tx, n_a)\n    s_prev -- previous hidden state of the (post-attention) GRU, numpy-array of shape (m, n_s)\n    \n    Returns:\n    context -- context vector, input of the next (post-attetion) GRU cell\n    \"\"\"\n    \n    s_prev = repeator(s_prev)\n    a_trans = permutor(a)\n    alphas = dotor1([s_prev,a_trans])\n    alphas = activator(alphas)\n    c = dotor2([alphas,a])\n    \n    return c","4f645762":"repeator_rev = RepeatVector(1)\npermutor_rev = Permute((2,1))\ndotor1_rev = Lambda(lambda x: K.batch_dot(x[0],x[1]))\nactivator_rev = Activation('softmax')\ndotor2_rev = Lambda(lambda x: K.batch_dot(x[0],x[1]))\n\ndef one_step_attention_rev(a, s_prev):\n    \"\"\"\n    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n    \"alphas\" and the hidden states \"a\" of the Bi-GRU.\n    \n    Arguments:\n    a -- hidden state output of the Bi-GRU, numpy-array of shape (m, Tx, n_a)\n    s_prev -- previous hidden state of the (post-attention) GRU, numpy-array of shape (m, n_a)\n    \n    Returns:\n    context -- context vector, input of the next (post-attetion) GRU cell\n    \"\"\"\n    \n    s_prev = repeator_rev(s_prev)\n    a_trans = permutor_rev(a)\n    alphas = dotor1_rev([s_prev,a_trans])\n    alphas = activator_rev(alphas)\n    c = dotor2_rev([alphas,a])\n    \n    return c","7ab931f3":"post_activation_GRU_cell = GRU(n_s,dropout=0.1,recurrent_dropout=0.1)\npost_activation_rev_GRU_cell = GRU(n_s,dropout=0.1,recurrent_dropout=0.1)\nadd_states1 = Add()\nadd_states2 = Add()\nreshaper1 = Reshape((n_s,))\nreshaper2 = Reshape((n_s,))\nconcatenator1 = Concatenate(axis=-1)\nconcatenator2 = Concatenate(axis=-1)\ndensor1 = Dense(n_s,activation='tanh')\ndensor2 = Dense(n_s,activation='tanh')\noutput_layer = Dense(len(products_vocab), activation='softmax',\n                     bias_initializer=initializers.Constant(value=0.025),\n                     activity_regularizer=regularizers.l2(0.05))\n\n\ndef model(Tx, Ty, n_a,n_s,reactants_vocab_size, products_vocab_size):\n    \"\"\"\n    Arguments:\n    Tx -- length of the input sequence\n    Ty -- length of the output sequence\n    n_a -- hidden state size of the Bi-LSTM\n    n_s -- hidden state size of the post-attention LSTM\n    human_vocab_size -- size of the python dictionary \"human_vocab\"\n    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    X = Input(shape=(Tx, reactants_vocab_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    s = s0\n    context_rev_seq = []\n    av2_seq = []\n    outputs = []\n    a = Bidirectional(GRU(n_a,return_sequences=True,\n                          recurrent_dropout=0.1),merge_mode='sum')(X)\n    a_rev = Lambda(lambda x: K.reverse(x,axes=1),output_shape=(Tx,n_a,))(a)\n    \n    for t in range(Ty):\n        context_rev = one_step_attention_rev(a_rev, s)\n        s = post_activation_rev_GRU_cell(context_rev, initial_state = s)\n        contextr = reshaper2(context_rev)\n        c2 = concatenator2([contextr,s])\n        av2 = densor2(c2)\n        av2_seq.append(av2)\n        context_rev_seq.append(context_rev)\n    \n    for t in range(Ty):\n        context = one_step_attention(a, s)\n        context_add = add_states1([context,context_rev_seq[Ty-1-t]])\n        s = post_activation_GRU_cell(context_add, initial_state = s)\n        context = reshaper1(context)\n        c1 = concatenator1([context,s])\n        av1 = densor1(c1)\n        av = add_states2([av1,av2_seq[Ty-1-t]])\n        out = output_layer(av)\n        outputs.append(out)\n        \n    model = Model(inputs = [X,s0], outputs = outputs)\n    return model\n\n\nmodel = model(Tx, Ty, n_a,n_s, len(reactants_vocab), len(products_vocab))\nmodel.summary()","2a86cb75":"#plot_model(model, to_file='OCR.png', show_shapes=True)","26b0b824":"tg = SMILESDataGenerator(train_idx, BATCH_SIZE, shuffle = True)\nvg = SMILESDataGenerator(valid_idx, BATCH_SIZE, shuffle = True)\ncheckpoint1 = ModelCheckpoint('model-CRP_best.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\ncheckpoint2 = ModelCheckpoint('model-CRP.h5', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=True, mode='min',period=1) \n#model.fit([Xoh,s0], outputs, batch_size = batch_size, epochs=1)\n#model.save_weights('weights256.h5')","6630842e":"class SGDRScheduler(Callback):\n    '''Cosine annealing learning rate scheduler with periodic restarts.\n    # Usage\n        ```python\n            schedule = SGDRScheduler(min_lr=1e-5,\n                                     max_lr=1e-2,\n                                     steps_per_epoch=np.ceil(epoch_size\/batch_size),\n                                     lr_decay=0.9,\n                                     cycle_length=5,\n                                     mult_factor=1.5)\n            model.fit(X_train, Y_train, epochs=100, callbacks=[schedule])\n        ```\n    # Arguments\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset. Calculated as `np.ceil(epoch_size\/batch_size)`. \n        lr_decay: Reduce the max_lr after the completion of each cycle.\n                  Ex. To reduce the max_lr by 20% after each cycle, set this value to 0.8.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    # References\n        Blog post: jeremyjordan.me\/nn-learning-rate\n        Original paper: http:\/\/arxiv.org\/abs\/1608.03983\n    '''\n    def __init__(self,\n                 min_lr,\n                 max_lr,\n                 steps_per_epoch,\n                 lr_decay=1,\n                 cycle_length=10,\n                 mult_factor=2):\n\n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.lr_decay = lr_decay\n\n        self.batch_since_restart = 0\n        self.next_restart = cycle_length\n\n        self.steps_per_epoch = steps_per_epoch\n\n        self.cycle_length = cycle_length\n        self.mult_factor = mult_factor\n\n        self.history = {}\n\n    def clr(self):\n        '''Calculate the learning rate.'''\n        fraction_to_restart = self.batch_since_restart \/ (self.steps_per_epoch * self.cycle_length)\n        lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(fraction_to_restart * np.pi))\n        return lr\n\n    def on_train_begin(self, logs={}):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        K.set_value(self.model.optimizer.lr, self.max_lr)\n\n    def on_batch_end(self, batch, logs={}):\n        '''Record previous batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        K.set_value(self.model.optimizer.lr, self.clr())\n\n    def on_epoch_end(self, epoch, logs={}):\n        '''Check for end of current cycle, apply restarts when necessary.'''\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += self.cycle_length\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs={}):\n        '''Set weights to the values from the end of the most recent cycle for best performance.'''\n        self.model.set_weights(self.best_weights)\n        \nschedule = SGDRScheduler(min_lr=1e-6,\n                         max_lr=1e-3,\n                         steps_per_epoch=len(tg),\n                         lr_decay=1,\n                         cycle_length=10,\n                         mult_factor=1)","3f21b054":"opt = optimizers.Adam(lr = 0.005, beta_1=0.9, beta_2=0.999, decay=0.0005)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","04202cae":"model_name = 'model-CRP-1.h5'\nweigh_path = 'https:\/\/www.kaggleusercontent.com\/kf\/12327992\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..zIlibl9Qa0Pcug8AeifFOA.RXz6fPMGmmw9s6QEsIDDuhYpttFCFmmH-Ac2GYAo_Jasdgl0EOLyfIQLeeVYU-3Ng2BZ7-yUqrpPXHL99uc5AGDYpMKXI5BxCnf5XRSZdLLdFalhWjYSCWhyzLV2u2epUnREdi5ymC316CFZZu8LZxqJEejamoiTmCJTch5crt4.xf9ut4ls0iePtHldkbaDpQ\/model-CRP.h5' \nweights_path = get_file(model_name, weigh_path,cache_subdir='models')\nmodel.load_weights(weights_path)","5b367aef":"hist = model.fit_generator(\n    tg,\n    steps_per_epoch=len(tg),\n    validation_data=vg,\n    #validation_steps=8,\n    epochs=30,\n    use_multiprocessing=False,\n    workers=1,\n    verbose=1,\n    callbacks=[checkpoint1,checkpoint2,schedule])","12704c16":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(hist.epoch, hist.history[\"loss\"], label=\"Train loss\")\nax[0].plot(hist.epoch, hist.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('acc')\nax[1].plot(hist.epoch, hist.history[\"acc\"], label=\"Train acc\")\nax[1].plot(hist.epoch, hist.history[\"val_acc\"], label=\"Validation acc\")\nax[0].legend()\nax[1].legend()","2e08b8a6":"def beam_search_decoder(data, k=3):\n    sequences = [[list(), 0.0]]\n    # walk over each step in sequence\n    for row in data:\n        all_candidates = list()\n        # expand each current candidate\n        for i in range(len(sequences)):\n            seq, score = sequences[i]\n            for j in range(len(row)):\n                candidate = [seq + [j], score-np.log(row[j])]\n                all_candidates.append(candidate)\n        # order all candidates by score\n        ordered = sorted(all_candidates, key=lambda tup:tup[1])\n        # select k best\n        sequences = ordered[:k]\n    return sequences","c6c99ad8":"def predict_result(model,x_test): # predict both orginal and reflect x\n    preds_test = model.predict_generator(x_test)\n    return preds_test","4419892f":"output = []\npad = '<pad>'\nm = len(valid_idx)\nbleu_score = np.zeros((m,1))\n#prediction = model.predict([Xoh, s0])\nprediction = predict_result(model,vg)\ncount = 0\nbw = 5\nfor i in range(m):\n    p0 = np.array(prediction)[:,i,:]\n    p0 = beam_search_decoder(p0,bw)\n    for j in reversed(range(bw)):\n        p = p0[j][0]\n        p = int_to_string(p,inv_products_vocab)\n        o2 = []\n        for x in p:\n            if x != pad:\n                o2.append(x)\n        o1 = o2\n        o2 = ''.join(o2)\n        \n        if o2 == ''.join(dataset[i][1]):\n            count += 1\n            bleu_score[i] = sentence_bleu([dataset[i][1]], o1)\n            output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n            break;\n        elif j==0:\n            bleu_score[i] = sentence_bleu([dataset[i][1]], o1)\n            output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n\nf = open('accuracy_bw.txt','w')\nf.write(str(count\/m))\nf.close()\n\nf = open('bleu_score_bw.txt','w')\nf.write(str(sum(bleu_score)\/m))\nf.close()\n\nprint(count\/m)\nprint(sum(bleu_score)\/m)\n\nwith open('predicted_bw.csv','w') as file:\n    for line in output:\n        file.write(line)\n        file.write('\\n')","790771d2":"output = []\npad = '<pad>'\nbleu_score = np.zeros((m,1))\n#prediction = model.predict([Xoh, s0, c0])\n#prediction = model.predict([Xoh, s0])\ncount = 0\nfor i in range(m):\n    p = np.argmax(np.array(prediction)[:,i,:], axis = 1)\n    p = int_to_string(p,inv_products_vocab)\n    o2 = []\n    for x in p:\n        if x != pad:\n            o2.append(x)\n    bleu_score[i] = sentence_bleu([dataset[i][1]], o2)\n    o2 = ''.join(o2)\n    if o2 == ''.join(dataset[i][1]):\n        count += 1\n    output.append(''.join(dataset[i][0])+','+''.join(dataset[i][1])+','+o2+','+str(o2 == ''.join(dataset[i][1])))\n\nprint(count\/m)\nprint(sum(bleu_score)\/m)\n\nf = open('accuracy.txt','w')\nf.write(str(count\/m))\nf.close()\n\nf = open('bleu_score.txt','w')\nf.write(str(sum(bleu_score)\/m))\nf.close()\n\nwith open('predicted.csv','w') as file:\n    for line in output:\n        file.write(line)\n        file.write('\\n')","a912e2d6":"**Organic chemistry reaction prediction using neural machine translation (NMT).**","2fb1356e":"This seq2seq with attention uses Asynchronous Bidirectional Decoding with beam search for predicting reactions in SMILES format. ","23aa2b8e":"Attention Layer for the forward Decoder:","af4fc760":"Preprocessing the data:","7a4cc100":"Seq2Seq Model:\nSlightly based on the model discussed in \"Asynchronous Bidirectional Decoding for Neural Machine Translation\" (https:\/\/arxiv.org\/abs\/1801.05122)\n","bdb1f51a":"Attention Layer for the backward Decoder:"}}