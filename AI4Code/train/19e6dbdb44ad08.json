{"cell_type":{"57643cfb":"code","2f254557":"code","8f8cee88":"code","415488f4":"code","f750bf44":"code","1bf79991":"code","4436ba06":"code","6d13f412":"code","d3b737ab":"code","49dea7b8":"code","39da37c1":"code","a507bea6":"code","a1ee57ac":"code","edc52107":"code","70885e9d":"code","1f9f4150":"code","4e510563":"code","01473edf":"code","4b606354":"code","9594d5b8":"code","90757b61":"code","b5b91456":"code","a0f3a22b":"code","f5c7ec6e":"code","87c879bb":"code","22273fe1":"code","6d4b7a1c":"markdown","10ade7d1":"markdown","8c20ceb6":"markdown","e1de29bb":"markdown","b814478f":"markdown","30ce1362":"markdown","2277a575":"markdown","63bba8fa":"markdown","50cc572c":"markdown","a55cbf49":"markdown","e91ae6d4":"markdown","84556c14":"markdown","e8baf2c5":"markdown","76ca2342":"markdown"},"source":{"57643cfb":"!pip uninstall keras -y\n!pip install git+https:\/\/github.com\/qubvel\/segmentation_models\n!git clone https:\/\/github.com\/SlinkoIgor\/ImageDataAugmentor.git","2f254557":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f8cee88":"prefix = '\/kaggle\/input\/covid-segmentation\/'\n\nimages_radiopedia = np.load(os.path.join(prefix, 'images_radiopedia.npy')).astype(np.float32)\nmasks_radiopedia = np.load(os.path.join(prefix, 'masks_radiopedia.npy')).astype(np.int8)\nimages_medseg = np.load(os.path.join(prefix, 'images_medseg.npy')).astype(np.float32)\nmasks_medseg = np.load(os.path.join(prefix, 'masks_medseg.npy')).astype(np.int8)\n\ntest_images_medseg = np.load(os.path.join(prefix, 'test_images_medseg.npy')).astype(np.float32)","415488f4":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef visualize(image_batch, mask_batch=None, pred_batch=None, num_samples=8):\n    num_classes = mask_batch.shape[-1] if mask_batch is not None else 0\n    fix, ax = plt.subplots(num_classes + 1, num_samples, figsize=(num_samples * 2, (num_classes + 1) * 2))\n\n    for i in range(num_samples):\n        ax_image = ax[0, i] if num_classes > 0 else ax[i]\n        ax_image.imshow(image_batch[i,:,:,0], cmap='Greys')\n        ax_image.set_xticks([]) \n        ax_image.set_yticks([])\n        \n        if mask_batch is not None:\n            for j in range(num_classes):\n                if pred_batch is None:\n                    mask_to_show = mask_batch[i,:,:,j]\n                else:\n                    mask_to_show = np.zeros(shape=(*mask_batch.shape[1:-1], 3)) \n                    mask_to_show[..., 0] = pred_batch[i,:,:,j] > 0.5\n                    mask_to_show[..., 1] = mask_batch[i,:,:,j]\n                ax[j + 1, i].imshow(mask_to_show, vmin=0, vmax=1)\n                ax[j + 1, i].set_xticks([]) \n                ax[j + 1, i].set_yticks([]) \n\n    plt.tight_layout()\n    plt.show()","f750bf44":"visualize(images_radiopedia[30:], masks_radiopedia[30:])","1bf79991":"visualize(images_medseg, masks_medseg)","4436ba06":"visualize(test_images_medseg)","6d13f412":"def plot_hists(images1, images2=None):\n    plt.hist(images1.ravel(), bins=100, density=True, color='b', alpha=1 if images2 is None else 0.5)\n    if images2 is not None:\n        plt.hist(images2.ravel(), bins=100, density=True, alpha=0.5, color='orange')\n    plt.show();","d3b737ab":"plot_hists(images_radiopedia, images_medseg)","49dea7b8":"plot_hists(test_images_medseg, images_medseg)","39da37c1":"def preprocess_images(images_arr, mean_std=None):\n    images_arr[images_arr > 500] = 500\n    images_arr[images_arr < -1500] = -1500\n    min_perc, max_perc = np.percentile(images_arr, 5), np.percentile(images_arr, 95)\n    images_arr_valid = images_arr[(images_arr > min_perc) & (images_arr < max_perc)]\n    mean, std = (images_arr_valid.mean(), images_arr_valid.std()) if mean_std is None else mean_std\n    images_arr = (images_arr - mean) \/ std\n    print(f'mean {mean}, std {std}')\n    return images_arr, (mean, std)\n\nimages_radiopedia, mean_std = preprocess_images(images_radiopedia)\nimages_medseg, _ = preprocess_images(images_medseg, mean_std)\ntest_images_medseg, _ = preprocess_images(test_images_medseg, mean_std)","a507bea6":"plot_hists(images_radiopedia, images_medseg)","a1ee57ac":"plot_hists(test_images_medseg, images_medseg)","edc52107":"val_indexes, train_indexes = list(range(24)), list(range(24, 100))\n\ntrain_images = np.concatenate((images_medseg[train_indexes], images_radiopedia))\ntrain_masks = np.concatenate((masks_medseg[train_indexes], masks_radiopedia))\nval_images = images_medseg[val_indexes]\nval_masks = masks_medseg[val_indexes]\n\nbatch_size = len(val_masks)\n\ndel images_radiopedia\ndel masks_radiopedia\ndel images_medseg\ndel masks_medseg","70885e9d":"import tensorflow\n\nimport albumentations\nimport cv2\n\nSOURCE_SIZE = 512\nTARGET_SIZE = 256\n\ntrain_augs = albumentations.Compose([\n    albumentations.Rotate(limit=360, p=0.9, border_mode=cv2.BORDER_REPLICATE),\n    albumentations.RandomSizedCrop((int(SOURCE_SIZE * 0.75), SOURCE_SIZE), \n                                   TARGET_SIZE, \n                                   TARGET_SIZE, \n                                   interpolation=cv2.INTER_NEAREST),\n    albumentations.HorizontalFlip(p=0.5),\n\n])\n\nval_augs = albumentations.Compose([\n    albumentations.Resize(TARGET_SIZE, TARGET_SIZE, interpolation=cv2.INTER_NEAREST)\n])","1f9f4150":"class Dataset:   \n    def __init__(\n            self, \n            images, \n            masks,\n            augmentations=None\n    ):\n        self.images = images\n        self.masks = masks\n        self.augmentations = augmentations\n    \n    def __getitem__(self, i):\n        image = self.images[i]\n        mask = self.masks[i]\n        \n        if self.augmentations:\n            sample = self.augmentations(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        return image, mask\n        \n    def __len__(self):\n        return len(self.images)\n    \n    \nclass Dataloder(tensorflow.keras.utils.Sequence):\n    \"\"\"Load data from dataset and form batches\n    \n    Args:\n        dataset: instance of Dataset class for image loading and preprocessing.\n        batch_size: Integet number of images in batch.\n        shuffle: Boolean, if `True` shuffle image indexes each epoch.\n    \"\"\"\n    \n    def __init__(self, dataset, batch_size=1, shuffle=False):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.indexes = np.arange(len(dataset))\n\n        self.on_epoch_end()\n\n    def __getitem__(self, i):\n        \n        # collect batch data\n        start = i * self.batch_size\n        stop = (i + 1) * self.batch_size\n        images = []\n        masks = []\n        for j in range(start, stop):\n            image, mask = self.dataset[self.indexes[j]]\n            images.append(image)\n            masks.append(mask)\n        \n        images = np.stack(images, axis=0)\n        masks = np.stack(masks, axis=0).astype(np.float32)\n        \n        return (images, masks)\n    \n    def __len__(self):\n        \"\"\"Denotes the number of batches per epoch\"\"\"\n        return len(self.indexes) \/\/ self.batch_size\n    \n    def on_epoch_end(self):\n        \"\"\"Callback function to shuffle indexes each epoch\"\"\"\n        if self.shuffle:\n            self.indexes = np.random.permutation(self.indexes)\n            \ntrain_dataset = Dataset(train_images, train_masks, train_augs)\nval_dataset = Dataset(val_images, val_masks, val_augs)\n\ntrain_dataloader = Dataloder(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = Dataloder(val_dataset, batch_size=batch_size, shuffle=False)","4e510563":"assert train_dataloader[0][0].shape == (batch_size, TARGET_SIZE, TARGET_SIZE, 1)\nassert train_dataloader[0][1].shape == (batch_size, TARGET_SIZE, TARGET_SIZE, 4)","01473edf":"visualize(*next(iter(train_dataloader)))\nvisualize(*next(iter(val_dataloader)))","4b606354":"def fscore_glass(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 0:1], \n                               y_pred[..., 0:1])\n    \ndef fscore_consolidation(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 1:2], \n                               y_pred[..., 1:2])\n\ndef fscore_lungs_other(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., 2:3], \n                               y_pred[..., 2:3])\n\ndef fscore_glass_and_consolidation(y_true, y_pred):\n    return sm.metrics.f1_score(y_true[..., :2], \n                               y_pred[..., :2])","9594d5b8":"from segmentation_models import Unet\nimport segmentation_models as sm\n\nfrom tensorflow.keras.layers import Input, Conv2D\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nnp.random.seed(0)\n\nbase_model = Unet(backbone_name='efficientnetb0',\n                  encoder_weights='imagenet',\n                  classes=4, \n                  activation='softmax')\n\n\nmodel = Sequential([Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n                    Conv2D(3, (1, 1)),  # map N channels data to 3 channels\n                    base_model])\n    \nmodel.compile(Adam(learning_rate=0.001, amsgrad=True),\n              loss=sm.losses.categorical_crossentropy,\n              metrics=[fscore_glass, fscore_consolidation, fscore_lungs_other, fscore_glass_and_consolidation])\n\ncheckpoint_callback = ModelCheckpoint('best_model',\n                                      monitor='fscore_glass_and_consolidation',\n                                      mode='max',\n                                      save_best_only=True)\n\nmodel.fit(\n    train_dataloader,\n    steps_per_epoch=len(train_dataloader) * 6,\n    epochs=10,\n    validation_data=val_dataloader,\n    validation_steps=len(val_dataloader),\n    callbacks=[checkpoint_callback],\n    workers=4)","90757b61":"del train_images\ndel train_masks","b5b91456":"model = tensorflow.keras.models.load_model('best_model\/',\n                                           compile=False,\n                                           custom_objects={\n                                                'categorical_crossentropy': sm.losses.categorical_crossentropy,\n                                                'fscore_consolidation': fscore_consolidation, \n                                                'fscore_glass': fscore_glass, \n                                                'fscore_lungs_other': fscore_lungs_other,\n                                                'fscore_glass_and_consolidation': fscore_glass_and_consolidation})\n\nmodel.compile(Adam(learning_rate=0.001, amsgrad=True),\n              loss=sm.losses.jaccard_loss)","a0f3a22b":"input = val_dataloader[0]\nimage_batch, mask_batch = input\n\npreds = model.predict_on_batch(image_batch)\nvisualize(image_batch, mask_batch, pred_batch=preds)\n\n# yellow is TP, red is FP, green is FN.","f5c7ec6e":"image_batch = np.stack([val_augs(image=img)['image'] for img in test_images_medseg], axis=0)\ntest_preds = model.predict_on_batch(image_batch)\ntest_masks_prediction = test_preds > 0.5\nvisualize(image_batch, test_masks_prediction, num_samples=len(test_images_medseg))","87c879bb":"import scipy\ntest_masks_prediction_original_size = scipy.ndimage.zoom(test_masks_prediction[..., :-2], (1, 2, 2, 1), order=0)\ntest_masks_prediction_original_size.shape","22273fe1":"import pandas as pd\n\npd.DataFrame(\n             data=np.stack((np.arange(len(test_masks_prediction_original_size.ravel())), \n                            test_masks_prediction_original_size.ravel().astype(int)),\n                            axis=-1), \n             columns=['Id', 'Predicted'])\\\n.set_index('Id').to_csv('submission.csv')","6d4b7a1c":"## Test preds:","10ade7d1":"## Load best model and visualize predicions on val:","8c20ceb6":"### Plot images hists:\nHU (Hounsfield scale) of radiopedia data (blue) vs medseg data (orange):","e1de29bb":"HU (Hounsfield scale) of test medseg data (blue) vs medseg data (orange):","b814478f":"## Resize prediction to original size:","30ce1362":"### Test images from medseg are individual axial slices:\nYou should make predictions for class 0 \"ground glass\" and class 1 \"consolidation\"","2277a575":"Normalized values of radiopedia data (blue) vs medseg data (orange):","63bba8fa":"Normalized values of test medseg data (blue) vs medseg data (orange):","50cc572c":"### Images from radiopedia are full CT volumes:\nClass 0 is \"ground glass\"<br>\nClass 1 is \"consolidations\"<br>\nClass 2 is \"lungs other\" \u2013 it doesn't mean that it is healthy lungs (you don't need to predict this class)<br>\nClass 3 is \"background\" \u2013 not lungs (you don't need to predict this class)<br>","a55cbf49":"## Split train \/ val:","e91ae6d4":"### Images from medseg are individual axial slices:\nClasses are same as in radiopedia part","84556c14":"## Data generator and augmentations:","e8baf2c5":"## Metrics:","76ca2342":"### Preprocess images:"}}