{"cell_type":{"4c1dcfa9":"code","97c37a52":"code","6f384760":"code","689bd774":"code","1f864fff":"code","60f3c94e":"code","184335d7":"code","4bad3a1d":"code","221517b6":"code","8984d1dd":"code","af337070":"code","e8c0e46c":"code","5a12fefb":"code","2a358074":"code","29bf7096":"code","65196167":"code","385df4bb":"code","73a35446":"code","f6daae49":"code","aec9ae2b":"code","5daed618":"code","23d66afe":"code","1f90846d":"code","429cbcee":"code","a2f311e3":"code","229a67de":"code","5d69ccc6":"code","bec7b08e":"code","7cc637b0":"code","78974c02":"code","47dcc1ec":"code","50644fc6":"code","abd47244":"code","8a29b8c2":"code","0b24f53a":"code","9c031199":"code","73c8cda4":"code","e8adff76":"code","7e0b0b38":"code","c1b26b30":"code","e9859b5f":"code","900232f9":"code","70c3b13d":"code","6fd827e9":"code","f84c9ce2":"code","baa6f8e4":"code","df2f060d":"code","79b1f542":"code","c7219b8b":"code","94224684":"code","4a7abce7":"code","1c18ea92":"code","27c45155":"code","b3f260e7":"code","f63aaa7f":"code","4250adf3":"code","6bd7a490":"code","8f1d1e3e":"code","8ba59aa3":"code","3ee500fa":"code","3b7bf441":"code","b6f752f9":"code","c94ebf69":"code","6774e01a":"markdown","a231b6b2":"markdown","ffc52bd6":"markdown","b0cc0220":"markdown","a1e956e7":"markdown","ac834eb8":"markdown","31dfbe71":"markdown","7c775438":"markdown","5020b76b":"markdown","cfcb058f":"markdown","af8c68c2":"markdown","93bc51f5":"markdown","1da40ec1":"markdown","c1c2fa1d":"markdown","f3c9d0bf":"markdown","45418d54":"markdown","5960a695":"markdown","e369469f":"markdown","5ed2e7fe":"markdown"},"source":{"4c1dcfa9":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup\nimport re\nimport nltk\nnltk.download(\"all\")","97c37a52":"!pip install beautifulsoup4","6f384760":"!unzip \/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip\n!unzip \/kaggle\/input\/word2vec-nlp-tutorial\/unlabeledTrainData.tsv.zip\n!unzip  \/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv.zip","689bd774":"train=pd.read_csv(\".\/labeledTrainData.tsv\",delimiter=\"\\t\",quoting=3)","1f864fff":"train.head()","60f3c94e":"#Sample review\nprint(train['review'][0])","184335d7":"from nltk.corpus import stopwords\nstopwords=stopwords.words(\"english\")\n\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n","4bad3a1d":"def clean_tweets(raw_text,stopwords=stopwords):\n    '''Golden function for cleaning text data'''\n    \n    # Removing HTML Tags\n    html_removed_text=BeautifulSoup(raw_text).get_text()\n    \n    # Remove any non character\n    character_only_text=re.sub(\"[^a-zA-Z]\",\" \",html_removed_text)\n    \n    # Lowercase and split\n    lower_text=character_only_text.lower().split()\n    \n    #Get STOPWORDS and remove\n    stop_remove_text=[i for i in lower_text if not i in stopwords]\n    \n    #Lemmatization\n    lemma_removed_text=[wordnet_lemmatizer.lemmatize(word,'v') for word in stop_remove_text]\n    \n    # Remove one character words\n#     lemma_removed_text=[word for word in stop_remove_text if len(word)>1]\n    \n    return \" \".join(lemma_removed_text)\n    ","221517b6":"# check on sample\ntrain.loc[:1,\"review\"].apply(clean_tweets)[0]","8984d1dd":"# orginal Review\ntrain.loc[0,\"review\"]","af337070":"train['clean_review']=train['review'].apply(clean_tweets)","e8c0e46c":"train.head()","5a12fefb":"from collections import Counter\nword_counter=Counter(\" \".join(train['clean_review'].tolist()).split())","2a358074":"word_counter.most_common(4)","29bf7096":"#Top Words in negative reviews\nnegative_word_counter=Counter(\" \".join(train.loc[train['sentiment']==1,\"clean_review\"].tolist()).split())\n\n#Top words in positive reviews\npositive_word_counter=Counter(\" \".join(train.loc[train['sentiment']==0,\"clean_review\"].tolist()).split())","65196167":"negative_word_counter.most_common(10)","385df4bb":"positive_word_counter.most_common(10)","73a35446":"# Baseline Model\n# If you are seeing a high overlap in unigram between two categories(here its positive or negative)\n# Then the next thing you should try is to look for bigrams or trigrams","f6daae49":"from sklearn.feature_extraction.text import CountVectorizer","aec9ae2b":"X=train['clean_review'] #Predictors\ny=train['sentiment'] #Target","5daed618":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","23d66afe":"def create_vector(vectorizer,data):\n    '''Pass vectorizer and data'''\n    train_vector=vectorizer.transform(data.tolist())\n    return train_vector.toarray()\n    ","1f90846d":"vectorizer = CountVectorizer(max_features=1000)\nvectorizer.fit(X_train.tolist())","429cbcee":"X_train_vector=create_vector(vectorizer,X_train)\nX_test_vector=create_vector(vectorizer,X_test)","a2f311e3":"X_test_vector.shape, X_train_vector.shape","229a67de":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\n# TRY with XGBOOST,SVM","5d69ccc6":"forest=RandomForestClassifier()\nforest.fit(X_train_vector,y_train)\n\n\ny_pred=forest.predict(X_test_vector)\nprint(classification_report(y_test,y_pred))","bec7b08e":"test=pd.read_csv(\".\/testData.tsv\",delimiter=\"\\t\")\ntest['clean_review']=test['review'].apply(clean_tweets)","7cc637b0":"test_feature_vector=create_vector(vectorizer,test['clean_review'])\ntest_predictions=forest.predict(test_feature_vector)\n\ntest['sentiment']=test_predictions\ntest[['id','sentiment']].to_csv(\"submission_file_rf_count.csv\",index=False)","78974c02":"# ----- PLEASE TRY THIS ------","47dcc1ec":"from gensim.models import Word2Vec\n\ntrain_unlabelled=pd.read_csv(\".\/unlabeledTrainData.tsv\",delimiter=\"\\t\",quoting=3)\ntrain_unlabelled['clean_review']=train_unlabelled['review'].apply(clean_tweets)","50644fc6":"sentences=[]\nsentences.extend(train['clean_review'])\nsentences.extend(test['clean_review'])\nsentences.extend(train_unlabelled['clean_review'])","abd47244":"#remove duplicate sentences,if any\nsentences=list(set(sentences))","8a29b8c2":"len(sentences)","0b24f53a":"sentences=[i.split() for i in sentences]","9c031199":"sentences[0]","73c8cda4":"#Beginner\ndel w2v_model\nw2v_model = Word2Vec(sentences=sentences,min_count=20,\n                     window=2,\n                     vector_size=100,\n                     workers=-1)\nw2v_model.wv.most_similar(\"great\")","e8adff76":"# For Advance users , we create in three steps\nimport multiprocessing\ncores = multiprocessing.cpu_count()\n\nw2v_model = Word2Vec(min_count=20,\n                     window=2,\n                     vector_size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,\n                     workers=cores-1)","7e0b0b38":"\n# w2v_model = Word2Vec(min_count=20,\n#                      window=2,\n#                      vector_size=300,\n#                      sample=6e-5, \n#                      alpha=0.03, \n#                      min_alpha=0.0007, \n# #                      negative=20,\n#                      workers=-1)","c1b26b30":"from time import time\nt = time()\n\nw2v_model.build_vocab(sentences, progress_per=10000)\n\nprint('Time to build vocab: {} mins'.format(round((time() - t) \/ 60, 2)))","e9859b5f":"t = time()\n\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=10, report_delay=1)\n\nprint('Time to train the model: {} mins'.format(round((time() - t) \/ 60, 2)))","900232f9":"w2v_model.wv.most_similar(\"leave\")","70c3b13d":"w2v_model.wv.most_similar(positive=[\"home\"])","6fd827e9":"w2v_model.wv.similarity(\"stupid\", 'worse')","f84c9ce2":"w2v_model.wv.doesnt_match(['great', 'stupid', 'good'])","baa6f8e4":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","df2f060d":"def tsnescatterplot(model, word, list_names):\n    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n    its list of most similar words, and a list of words.\n    \"\"\"\n    arrays = np.empty((0, 300), dtype='f')\n    word_labels = [word]\n    color_list  = ['red']\n\n    # adds the vector of the query word\n    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)\n    \n    # gets list of most similar words\n    close_words = model.wv.most_similar([word])\n    \n    # adds the vector for each of the closest words to the array\n    for wrd_score in close_words:\n        wrd_vector = model.wv.__getitem__([wrd_score[0]])\n        word_labels.append(wrd_score[0])\n        color_list.append('blue')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n    \n    # adds the vector for each of the words from list_names to the array\n    for wrd in list_names:\n        wrd_vector = model.wv.__getitem__([wrd])\n        word_labels.append(wrd)\n        color_list.append('green')\n        arrays = np.append(arrays, wrd_vector, axis=0)\n        \n    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n    reduc = PCA(n_components=14,svd_solver='full').fit_transform(arrays)\n    \n    # Finds t-SNE coordinates for 2 dimensions\n    np.set_printoptions(suppress=True)\n    \n    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n    \n    # Sets everything up to plot\n    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n                       'y': [y for y in Y[:, 1]],\n                       'words': word_labels,\n                       'color': color_list})\n    \n    fig, _ = plt.subplots()\n    fig.set_size_inches(9, 9)\n    \n    # Basic plot\n    p1 = sns.regplot(data=df,\n                     x=\"x\",\n                     y=\"y\",\n                     fit_reg=False,\n                     marker=\"o\",\n                     scatter_kws={'s': 40,\n                                  'facecolors': df['color']\n                                 }\n                    )\n    \n    # Adds annotations one by one with a loop\n    for line in range(0, df.shape[0]):\n         p1.text(df[\"x\"][line],\n                 df['y'][line],\n                 '  ' + df[\"words\"][line].title(),\n                 horizontalalignment='left',\n                 verticalalignment='bottom', size='medium',\n                 color=df['color'][line],\n                 weight='normal'\n                ).set_size(15)\n\n    \n    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n            \n    plt.title('t-SNE visualization for {}'.format(word.title()))\n    ","79b1f542":"tsnescatterplot(w2v_model, 'good',  [i[0] for i in w2v_model.wv.most_similar(positive=[\"bad\"])])","c7219b8b":"num_features=300\n\n","94224684":"def get_vectors(model,sentence):\n    \n    '''Get sentence vectors'''\n    \n    vectors=[]\n    for i in sentence.split():\n        try:\n            vectors.append(model.wv[i])\n        except:\n            continue\n    return np.average(vectors,axis=0)\n            \n        ","4a7abce7":"get_vectors(w2v_model,\"this is good today okay thats fine\")","1c18ea92":"w2v_model.wv['aayush']","27c45155":"get_vectors(w2v_model,\"aayush is a good actor and prove his skills\")","b3f260e7":"def get_doc_vectors(model,documents,num_features=300):\n    \n    # Initialize a counter\n    counter = 0\n    \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(documents),num_features),dtype=\"float32\")\n    \n    # Loop through the reviews\n    for sentence in documents:\n        # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\" % (counter, len(documents)))\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = get_vectors(model,sentence)\n        \n        # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs\n    ","f63aaa7f":"documents=[\"hey\",\"hey this\"]","4250adf3":"#sample\nget_doc_vectors(w2v_model,documents,num_features=300).shape","6bd7a490":"X_train_w2v_vectors=get_doc_vectors(w2v_model,X_train,num_features=300)\nX_test_w2v_vectors=get_doc_vectors(w2v_model,X_test,num_features=300)","8f1d1e3e":"X_train_w2v_vectors.shape, X_test_w2v_vectors.shape","8ba59aa3":"forest=RandomForestClassifier()\nforest.fit(X_train_w2v_vectors,y_train)\ny_pred=forest.predict(X_test_w2v_vectors)\nprint(classification_report(y_test,y_pred))","3ee500fa":"import gensim.downloader\nprint(gensim.downloader.info()['models'].keys())","3b7bf441":"glove_vectors = gensim.downloader.load('glove-twitter-25')","b6f752f9":"def get_pretrained_vectors(model,sentence):\n    '''Get sentence vectors'''\n    \n    vectors=[]\n    for i in sentence.split():\n        try:\n            vectors.append(model[i])\n        except:\n            continue\n    return np.average(vectors,axis=0)\n            \n    \ndef get_pretrained_models(model,documents,num_features=25):\n    \n    # Initialize a counter\n    counter = 0\n    \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(documents),num_features),dtype=\"float32\")\n    \n    # Loop through the reviews\n    for sentence in documents:\n        # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\" % (counter, len(documents)))\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = get_pretrained_vectors(model,sentence)\n        \n        # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs\n    ","c94ebf69":"X_train_w2v_vectors=get_pretrained_models(glove_vectors,X_train,num_features=25)\nX_test_w2v_vectors=get_pretrained_models(glove_vectors,X_test,num_features=25)\n\nforest=RandomForestClassifier()\nforest.fit(X_train_w2v_vectors,y_train)\ny_pred=forest.predict(X_test_w2v_vectors)\nprint(classification_report(y_test,y_pred))","6774e01a":"# Reference\nhttps:\/\/www.kaggle.com\/aayushkubba\/twitter-sentiment-analysis-word2vec-doc2vec\nhttps:\/\/www.kaggle.com\/aayushkubba\/nlp-word2vec","a231b6b2":"## Building the Vocabulary Table:\nWord2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):","ffc52bd6":"## Training of the model:\n_Parameters of the training:_\n* `total_examples` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Count of sentences;\n* `epochs` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Number of iterations (epochs) over the corpus - [10, 20, 30]","b0cc0220":"# Read the data","a1e956e7":"Our goal in this section is to plot our 300 dimensions vectors into 2 dimensional graphs, and see if we can spot interesting patterns.<br>\nFor that we are going to use t-SNE implementation from scikit-learn.\n\nTo make the visualizations more relevant, we will look at the relationships between a query word (in <font color='red'>**red**<\/font>), its most similar words in the model (in <font color=\"blue\">**blue**<\/font>), and other words from the vocabulary (in <font color='green'>**green**<\/font>).","ac834eb8":"### t-SNE visualizations:\nt-SNE is a non-linear dimensionality reduction algorithm that attempts to represent high-dimensional data and the underlying relationships between vectors in a lower-dimensional space.<br>\nHere is a good tutorial on it: https:\/\/medium.com\/@luckylwk\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b","31dfbe71":"# Pretrained Models","7c775438":"# Train a custom word2vec model","5020b76b":"## TFIDF Vectorizer","cfcb058f":"## Split the data","af8c68c2":"1. 1. 1. ","93bc51f5":"## The parameters:\n\n* `min_count` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n\n\n* `window` <font color='purple'>=<\/font> <font color='green'>int<\/font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n\n\n* `size` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Dimensionality of the feature vectors. - (50, 300)\n\n\n* `sample` <font color='purple'>=<\/font> <font color='green'>float<\/font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n\n\n* `alpha` <font color='purple'>=<\/font> <font color='green'>float<\/font> - The initial learning rate - (0.01, 0.05)\n\n\n* `min_alpha` <font color='purple'>=<\/font> <font color='green'>float<\/font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n\n\n* `negative` <font color='purple'>=<\/font> <font color='green'>int<\/font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n\n\n* `workers` <font color='purple'>=<\/font> <font color='green'>int<\/font> - Use these many worker threads to train the model (=faster training with multicore machines)","1da40ec1":"# Text Pre processing Pipeline","c1c2fa1d":"# Create ML Model","f3c9d0bf":"# Unzip folders","45418d54":"# Lets make a submission","5960a695":"Here, \"header=0\" indicates that the first line of the file contains column names, \"delimiter=\\t\" indicates that the fields are separated by tabs, and quoting=3 tells Python to ignore doubled quotes, otherwise you may encounter errors trying to read the file.","e369469f":"# Word2Vec Model","5ed2e7fe":"# Bag of Words - Model\n\n## Count Vectorizer"}}