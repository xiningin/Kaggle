{"cell_type":{"e0d3c783":"code","fc13d9dc":"code","2ff28c03":"code","b96fcb19":"code","3482a70e":"code","0078565d":"code","b0e58692":"code","3b7d66d2":"code","b64a7ac9":"code","0496f728":"code","ba6fea5e":"code","fd626da4":"code","e3a0c9ae":"code","87ad0924":"code","33060d75":"code","61bf3e2a":"code","f9d80029":"code","1c1d0c05":"code","87129f98":"code","c4e2b1fa":"code","8c230188":"code","062745ed":"code","1ccd7483":"code","705d5fe2":"code","3bda4bc6":"code","00092f27":"code","b1219464":"code","99cf34db":"code","611256f4":"code","528511b6":"code","a4bd368c":"code","086085c1":"code","187cd303":"code","dd9e41a5":"code","7244a254":"code","a6b49bab":"code","f8812bcc":"code","790c7a1e":"code","727bf044":"code","3e27121e":"code","d9a6df83":"code","b0db4673":"code","0e822488":"code","fea9e301":"code","0d440fc9":"code","77306ead":"markdown","253ab482":"markdown","e3592e53":"markdown","63310838":"markdown","02955fc3":"markdown","730248c0":"markdown"},"source":{"e0d3c783":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc13d9dc":"#importing libraries\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd    \nimport numpy as n  \nimport matplotlib\nimport matplotlib.pyplot as plt    \nimport seaborn as sns  \nimport scipy    \nimport statsmodels.formula.api as smf    \nimport statsmodels.api as sm  \nfrom sklearn.preprocessing import robust_scale\nfrom sklearn import model_selection, preprocessing, feature_selection, ensemble, linear_model, metrics, decomposition\n## for explainer    \nfrom lime import lime_tabular\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing.data import QuantileTransformer\nfrom scipy.stats import skew","2ff28c03":"#import data as df\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test_main = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_train\n","b96fcb19":"df_test_main","3482a70e":"#df = df.drop(df[df['SalePrice'] > 350000].index)","0078565d":"df = pd.concat((df_train.loc[:,'MSSubClass':'SaleCondition'], df_test_main.loc[:,'MSSubClass':'SaleCondition']))","b0e58692":"## checking for normal distribution of saleprice\nmatplotlib.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":df_train[\"SalePrice\"], \"log(price + 1)\":np.log1p(df_train[\"SalePrice\"])})\nprices.hist()","3b7d66d2":"#log transform the target saleprice:\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","b64a7ac9":"##preliminary feature engeneering\n\ndf['BsmtQual'] = df['BsmtQual'].fillna(1) # we have some NaNs there - let's replace them to 1\ndf['GarageType'] = df['GarageType'].fillna(1) # we have some NaNs there - let's replace them to 1\ndf['GarageFinish'] = df['GarageFinish'].fillna(1) # we have some NaNs there - let's replace them to 1\ndf['FireplaceQu'] = df['FireplaceQu'].fillna(1) # we have some NaNs there - let's replace them to 1\ndf = df.replace({\"BsmtQual\" : { \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5}, \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5}, \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}})\ndf = df.replace({'GarageType': {'Detchd' : 1, 'CarPort': 2, 'BuiltIn': 3, 'Basment': 4, 'Attchd': 5, '2Types': 6}})\ndf = df.replace({'GarageFinish': {'Unf' : 1, 'RFn': 2, ' Fin': 3}})\ndf = df.replace({'HeatingQC': {'Po' : 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}})\ndf = df.replace({'Foundation': {'Wood' : 1, 'Stone': 5, 'Slab': 2, 'BrkTil': 3, 'CBlock': 4, 'PConc': 6}})\ndf = df.replace({'PavedDrive': {'N' : 1, 'P': 2, ' Y': 3}})\ndf['ExterQual'] = df['ExterQual'].astype(object)\ndf = df.replace({'ExterQual': {'Po' : 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}})\n#df = df.replace({'ExterCond': {'Po' : 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}}) # not actual feature\n#df['Exter'] = df['ExterQual']*df['ExterCond'] #worse\ndf['YearRemodAdd'] = df['YearRemodAdd']\/2000\n\n\ndf['Bath'] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"]) + df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\n\ndf = df.replace({'FireplaceQu': {'Po' : 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}})\n\n\ndf['YrBltAndRemod']=(df['YearBuilt']+df['YearRemodAdd'])\/2000\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['OverallCond'] = df['OverallCond'].astype(str)\n\n## imputing NaNs for GarageYrBlt \n#start\ndf['YearBuiltCut'] = pd.qcut(df['YearBuilt'], 10)\ndf['GarageYrBlt'] = df.groupby(['YearBuiltCut'])['GarageYrBlt'].transform(lambda x : x.fillna(x.median()))\ndf['GarageYrBlt'] = df['GarageYrBlt'].astype(int)\/2000\ndf.drop('YearBuiltCut', axis=1, inplace=True)\n#end\n\ndf['LotFrontage'].fillna(df['LotFrontage'].median(), inplace = True)\ndf['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace = True)\ndf['Bath'].fillna(df['Bath'].median(), inplace = True)\ndf['TotalBsmtSF'].fillna(df['TotalBsmtSF'].median(), inplace = True)\ndf['GarageArea'].fillna(df['GarageArea'].median(), inplace = True)\ndf['GarageCars'].fillna(df['GarageCars'].median(), inplace = True)\ndf['KitchenQual'].fillna(df['KitchenQual'].median(), inplace = True)\ndf['BsmtFinSF1'].fillna(df['BsmtFinSF1'].median(), inplace = True)\n\ndf['BsmtUnfSF'].fillna(df['BsmtUnfSF'].median(), inplace = True)\ndf['LotFrontage'] = df['LotFrontage'].astype(int)\ndf['BsmtQual'] = df['BsmtQual'].fillna(1) # we have some NaNs (no basement = 1 category)\n\n#df = df.drop(df[df['GrLivArea'] > 3500].index)\n#df = df.drop(df[df['GarageArea'] > 1200].index)\n#df = df.drop(df[df['TotalBsmtSF'] > 2400].index)\n\ndf[\"AllSF\"] = df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]\ndf['Garage'] = df['GarageCars'] * df['GarageArea'] \ndf[\"WoodDeckSF\"] = df[\"WoodDeckSF\"].fillna(0)\ndf[\"OpenPorchSF\"] = df[\"OpenPorchSF\"].fillna(0)\n\n\n\n#df = df.drop(df[df['Garage'] > 3500].index)\n#df['Front*Area'] = df['LotFrontage'] * df['LotArea'] ##worse\n#df = df.drop(df[df['AllSF'] > 4000].index)\n\ndef year_category(yb):\n    if yb <= 1910:\n        return 1\n    elif yb <= 1950 and yb > 1910:\n        return 2\n    elif yb >= 1950 and yb < 1980:\n        return 3\n    elif yb >= 1980 and yb < 2000:\n        return 4\n    return 5\n\n#df['YearBuilt_cat'] = df['YearBuilt'].apply(year_category) # not actual\n\ndf[\"Qual\"] = df[\"ExterQual\"] * df[\"KitchenQual\"] * df[\"BsmtQual\"]\n#df['OverallQual_2'] = df['OverallQual']**2 #worse\n#df['OverallQual_3'] = df['OverallQual']**3 \n#df[\"OverallQual-Sq\"] = np.sqrt(df[\"OverallQual\"]) #worse\n#df['OverallQual_log'] = np.log1p('OverallQual')\ndf['FinalQual'] = df['OverallQual']*df['Qual']\ndf['Fireplaces-Sq'] = np.sqrt(df['Fireplaces'])\ndf['FireRooms'] = df['TotRmsAbvGrd']*df['Fireplaces-Sq']\ndf['Area'] = df['LotArea']*df['LotFrontage']\n\n## final feature engeneering\n\n\n#df[\"Qual\"] = df[\"ExterQual\"] * df[\"KitchenQual\"] * df[\"BsmtQual\"]\n#df['OverallQual_2'] = df['OverallQual']**2\n#df['OverallQual_3'] = df['OverallQual']**3 \n#df[\"OverallQual-Sq\"] = np.sqrt(df[\"OverallQual\"]) #worse\n#df['OverallQual_log'] = np.log1p('OverallQual')\n\n\n#df['AllSF_2'] = df['AllSF']**2 ##worse\n#df['AllSF_3'] = df['AllSF']**3 ## worse\n#df['AllSF-Sq'] = np.sqrt(df['AllSF']) ##worse\n#df['AllSF-log'] = np.log1p(df['AllSF'])\n\n#df['GarageArea_2'] = df['GarageArea']**2 #worse\n#df['GarageArea_3'] = df['GarageArea']**3 #worse\n#df['GarageArea-Sq'] = np.sqrt(df['GarageArea']) #worse\n\n#df['YearBuilt_cat_2'] = df['YearBuilt_cat']**2 #worse\n#df['YearBuilt_cat_3'] = df['YearBuilt_cat']**3 #worse\n#df['YearBuilt_cat-Sq'] = np.sqrt(df['YearBuilt_cat']) #worse\n\n#df['Qual_2'] = df['Qual']**2 \n#df['Qual_3'] = df['Qual']**3\n#df['Qual-Sq'] = np.sqrt(df['Qual']) \n\n#df['Bath_2'] = df['Bath']**2 \n#df['Bath_3'] = df['Bath']**3\n#df['Bath-Sq'] = np.sqrt(df['Bath']) \n\n#df['Garage_2'] = df['Garage']**2 \n#df['Garage_3'] = df['Garage']**3\n#df['Garage-Sq'] = np.sqrt(df['Garage']) \n\n#df['FinalQual'] = df['OverallQual_2']*df['Qual']\n#df['FinalQual1'] = df['OverallQual']*df['Qual-Sq']\n#df['FinalQual2'] = df['OverallQual_2']*df['Qual']\n#df['FinalQual3'] = df['OverallQual']*df['Qual']\n\n\n#df['Fireplaces_2'] = df['Fireplaces']**2 \n#df['Fireplaces_3'] = df['Fireplaces']**3\n#df['Fireplaces-Sq'] = np.sqrt(df['Fireplaces'])\n\n#df['FireRooms'] = df['TotRmsAbvGrd']*df['Fireplaces-Sq']\n\n#df['OpenPorchSF_2'] = df['OpenPorchSF']**2 \n#df['OpenPorchSF_3'] = df['OpenPorchSF']**3\n#df['OpenPorchSF-Sq'] = np.sqrt(df['OpenPorchSF']) \n#df['OpenPorchSF-log'] = np.log1p(df['OpenPorchSF'])\n\n#df['FinalQual-log'] = np.log1p(df['FinalQual']) \n\n#scaler = RobustScaler()\n#df['AllSF'] = scaler.fit_transform(df[['AllSF']])\n\n#scaler = StandardScaler()\n#df[['Garage', 'Y', 'AllSF']] = scaler.fit_transform(df[['Garage', 'Y', 'AllSF']])\n\n#df['Y'] = np.log1p(df['Y'])\n\n\n#df['Foundation_2'] = df['Foundation']**2 \n#df['Foundation_3'] = df['Foundation']**3\n#df['Foundation-Sq'] = np.sqrt(df['Foundation'])\n#df['Foundation-log'] = np.log1p(df['Foundation'])\n\n","0496f728":"# delete features from df with zero-values more than 50%\ndf = df.loc[:, ((df == 0).sum(axis=0) <= len(df.index)*0.6)]\n# removing objects with nan-values more than 50% in frame\ndf = df.loc[:, (df.isnull().sum(axis=0) <= len(df.index)*0.6)]\n\n# checking for Object columns\nobject_columns = df.select_dtypes(include=[np.object])\n\n\n\n# replace NaNs with \"None\"\nfor col in object_columns:\n    object_columns[col].fillna('None', inplace = True)\n    \n# replace 0 to median\n\ndf.replace(0,df.median(axis=0),inplace=True)\n\n# checking for NANs in objects\n\nobject_columns.isnull().sum().sort_values(ascending = False).head()","ba6fea5e":"#log transform skewed numeric features:\nnumerical_features = df.select_dtypes(exclude = [\"object\"]).columns\nnum = df[numerical_features]\n\n# Log transform of the skewed numerical features to lessen impact of outliers\n# Inspired by Alexandru Papiu's script : https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models\n# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nskewness = num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\ndf[skewed_features] = np.log1p(df[skewed_features])\n","fd626da4":"# Apply label encoder for category columns\nfrom sklearn.preprocessing import LabelEncoder\nfrom category_encoders import TargetEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nle = LabelEncoder()\n#le = TargetEncoder(cols=object_cols)\n#le = CatBoostEncoder(cols=object_cols)\nfor col in object_columns:\n    df[col] = le.fit_transform(df[col].astype(str))","e3a0c9ae":"df.shape","87ad0924":"#creating matrices for feature selection:\nX_train = df[:df_train.shape[0]]\nX_test_fin = df[df_train.shape[0]:]\ny = df_train.SalePrice","33060d75":"X_test_fin","61bf3e2a":"X_train['Y'] = y\ndf = X_train\ndf ## DF for Model training","f9d80029":"#df.plot.scatter(x='AllSF', y='Y', figsize=(16, 6), ylim=(0,90000))","1c1d0c05":"#Correlation with output variable\ncor = df.corr()\ncor_target = (cor['Y'])\n#Selecting highly correlated features (50% level)\nrelevant_features = cor_target[(cor_target<=-0.5) | (cor_target>=0.25) ]\nrelevant_features.sort_values(ascending = False).head(50)\n\n## another way \n#dfcorrw=df.loc[:,df.columns[:-1]].corrwith(df.loc[:,'cnt'])\n#dfcorrw.sort_values(ascending = False)","87129f98":"##all highly correlated features arrangement in new dataframe\nfeatures = relevant_features.keys().tolist()\ndf = df[features]\n#df = df.rename(columns={\"SalePrice\":\"Y\"})\n#df = pd.concat([ID, df[features]], axis=1)","c4e2b1fa":" \ndef utils_recognize_type(df, col, max_cat=20):  \n    '''    \nRecognize whether a column is numerical or categorical.    \n''' \n    if (df[col].dtype == \"O\") | (df[col].nunique() < max_cat):    \n        return \"cat\"    \n    else:    \n        return \"num\"","8c230188":"# heatmap of missed values, numerical and categoryal\n\nmax_cat = 20\ndic_cols = {col:utils_recognize_type(df, col, max_cat) for col in df.columns}\nheatmap = df.isnull()    \nfor k,v in dic_cols.items():    \n    if v == \"num\":    \n        heatmap[k] = heatmap[k].apply(lambda x: 0.5 if x is False else 1)    \n    else:    \n        heatmap[k] = heatmap[k].apply(lambda x: 0 if x is False else 1)\nsns.heatmap(heatmap, cbar=False).set_title('Dataset Overview')    \nplt.show()\nprint(\"\\033[1;37;40m Categerocial \", \"\\033[1;30;41m Numeric \", \"\\033[1;30;47m NaN \")","062745ed":"#correlation matrix\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt  # Matlab-style plotting\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(30, 20))\n#sns.heatmap(corrmat, vmax=.8, square=True);\n#saleprice correlation matrix\nk = 35 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'Y')['Y'].index\ncm = np.corrcoef(df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","1ccd7483":"x = \"Y\"\nfig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)    \nfig.suptitle(x, fontsize=20)\n\n### distribution    \nax[0].title.set_text('   distribution')    \nvariable = df[x].fillna(df[x].mean())    \nbreaks = np.quantile(variable, q=np.linspace(0, 1, 11))\n\n#variable = variable[ (variable > breaks[quantile_breaks[0]]) & (variable < breaks[quantile_breaks[1]]) ]     \nsns.distplot(variable, hist=True, kde=True, kde_kws={\"shade\": True}, ax=ax[0])    \ndes = df[x].describe()    \nax[0].axvline(des[\"25%\"], ls='--')    \nax[0].axvline(des[\"mean\"], ls='--')    \nax[0].axvline(des[\"75%\"], ls='--')    \nax[0].grid(True)    \ndes = round(des, 2).apply(lambda x: str(x))    \nbox = '\\n'.join((\"min: \"+des[\"min\"], \"25%: \"+des[\"25%\"], \"mean: \"+des[\"mean\"], \"75%: \"+des[\"75%\"], \"max: \"+des[\"max\"]))    \nax[0].text(0.95, 0.95, box, transform=ax[0].transAxes, fontsize=10, va='top', ha=\"right\", bbox=dict(boxstyle='round', facecolor='white', alpha=1))\n### boxplot     \nax[1].title.set_text('outliers (log scale)')    \ntmp_df = pd.DataFrame(df[x])    \ntmp_df[x] = np.log(tmp_df[x])    \ntmp_df.boxplot(column=x, ax=ax[1])    \nplt.show()","705d5fe2":"y = \"Y\"\nx = \"FullBath\"\ncat, num = \"FullBath\", \"Y\"\nfig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)    \nfig.suptitle(x+\"   vs   \"+y, fontsize=16)\n\n### distribution    \nax[0].title.set_text('      density')    \nfor i in df[cat].unique():    \n    sns.distplot(df[df[cat]==i][num], hist=False, label=i, ax=ax[0])    \nax[0].grid(True)\n\n### stacked    \nax[1].title.set_text('bins')    \nbreaks = np.quantile(df[num], q=np.linspace(0,1,11))    \ntmp = df.groupby([cat, pd.cut(df[num], breaks, duplicates='drop')]).size().unstack().T    \ntmp = tmp[df[cat].unique()]    \ntmp[\"tot\"] = tmp.sum(axis=1)    \nfor col in tmp.drop(\"tot\", axis=1).columns:    \n     tmp[col] = tmp[col] \/ tmp[\"tot\"]    \ntmp.drop(\"tot\", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)\n\n### boxplot       \n#ax[2].title.set_text('outliers')    \nsns.catplot(x=cat, y=num, data=df, kind=\"box\")    \n#ax[2].grid(True)    \n#plt.show()","3bda4bc6":"cat, num = \"FullBath\", \"Y\"\nmodel = smf.ols(num+' ~ '+cat, data=df).fit()    \ntable = sm.stats.anova_lm(model)    \np = table[\"PR(>F)\"][0]    \ncoeff, p = None, round(p, 3)    \nconclusion = \"Correlated\" if p < 0.05 else \"Non-Correlated\"    \nprint(\"Anova F conclusion: FullBath and SalePrice are\", conclusion, \"(p-value: \"+str(p)+\")\")","00092f27":"x, y = \"GrLivArea\", \"Y\"\n### bin plot    \ndf_noNan = df[df[x].notnull()]    \nbreaks = np.quantile(df_noNan[x], q=np.linspace(0, 1, 11))    \ngroups = df_noNan.groupby([pd.cut(df_noNan[x], bins=breaks,     \n           duplicates='drop')])[y].agg(['mean','median','size'])    \nfig, ax = plt.subplots(figsize=(10,3))    \nfig.suptitle(x+\"   vs   \"+y, fontsize=20)    \ngroups[[\"mean\", \"median\"]].plot(kind=\"line\", ax=ax)    \ngroups[\"size\"].plot(kind=\"bar\", ax=ax, rot=45, secondary_y=True,    \n                    color=\"grey\", alpha=0.3, grid=True)    \nax.set(ylabel=y)    \nax.right_ax.set_ylabel(\"Observazions in each bin\")    \nplt.show()\n### scatter plot    \nsns.jointplot(x=x, y=y, data=df, dropna=True, kind='reg')    \nplt.show()","b1219464":"x, y = \"GrLivArea\", \"Y\"\ndf_noNan = df[df[x].notnull()]    \ncoeff, p = scipy.stats.pearsonr(df_noNan[x], df_noNan[y])    \ncoeff, p = round(coeff, 3), round(p, 3)    \nconclusion = \"Significant\" if p < 0.05 else \"Non-Significant\"    \nprint(\"Pearson Correlation:\", coeff, conclusion, \"(p-value: \"+str(p)+\")\")","99cf34db":"## split data    \ndf_train, df_test = model_selection.train_test_split(df, test_size=0.3, random_state=1)\n\n## print info    \nprint(\"X_train shape:\", df_train.drop(\"Y\",axis=1).shape, \"| X_test shape:\", df_test.drop(\"Y\",axis=1).shape)    \nprint(\"y_train mean:\", round(np.mean(df_train[\"Y\"]),2), \"| y_test mean:\", round(np.mean(df_test[\"Y\"]),2))    \nprint(df_train.shape[1], \"features:\", df_train.drop(\"Y\",axis=1).columns.to_list())","611256f4":"X = df_train.drop(\"Y\", axis=1).values    \ny = df_train[\"Y\"].values    \nfeature_names = df_train.drop(\"Y\", axis=1).columns\n\n## p-value    \nselector = feature_selection.SelectKBest(score_func=feature_selection.f_regression, k=10).fit(X,y)    \npvalue_selected_features = feature_names[selector.get_support()]\n\n## regularization    \nselector = feature_selection.SelectFromModel(estimator=     \n              linear_model.Ridge(alpha=1.0, fit_intercept=True),     \n                                 max_features=10).fit(X,y)    \nregularization_selected_features = feature_names[selector.get_support()]\n\n## plot  \n\ndf_features = pd.DataFrame({\"features\":feature_names})    \ndf_features[\"p_value\"] = df_features[\"features\"].apply(lambda x: \"p_value\" if x in pvalue_selected_features else \"\")    \ndf_features[\"num1\"] = df_features[\"features\"].apply(lambda x: 1 if x in pvalue_selected_features else 0)    \ndf_features[\"regularization\"] = df_features[\"features\"].apply(lambda x: \"regularization\" if x in regularization_selected_features else \"\")    \ndf_features[\"num2\"] = df_features[\"features\"].apply(lambda x: 1 if x in regularization_selected_features else 0)    \ndf_features[\"method\"] = df_features[[\"p_value\",\"regularization\"]].apply(lambda x: (x[0]+\" \"+x[1]).strip(), axis=1)    \ndf_features[\"selection\"] = df_features[\"num1\"] + df_features[\"num2\"]    \ndf_features[\"method\"] = df_features[\"method\"].apply(lambda x: \"both\" if len(x.split()) == 2 else x)\n\nsns.barplot(y=\"features\", x=\"selection\", hue=\"method\", data=df_features.sort_values(\"selection\", ascending=False), dodge=False)","528511b6":"X = df_train.drop(\"Y\", axis=1).values    \ny = df_train[\"Y\"].values    \nfeature_names = df_train.drop(\"Y\", axis=1).columns.tolist()\n\n## call model    \nmodel = ensemble.GradientBoostingRegressor()\n## Importance    \nmodel.fit(X,y)    \nimportances = model.feature_importances_\n## Put in a pandas df    \ndf_importances = pd.DataFrame({\"IMPORTANCE\":importances,     \n            \"VARIABLE\":feature_names}).sort_values(\"IMPORTANCE\",     \n            ascending=False)    \ndf_importances['cumsum'] = df_importances['IMPORTANCE'].cumsum(axis=0)    \ndf_importances = df_importances.set_index(\"VARIABLE\")\n## Plot    \nfig, ax = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(20, 7)) \nfig.suptitle(\"Features Importance\", fontsize=15)    \nax[0].title.set_text('variables')    \ndf_importances[[\"IMPORTANCE\"]].sort_values(by=\"IMPORTANCE\").plot(    \n                kind=\"barh\", legend=False, ax=ax[0]).grid(axis=\"x\")    \nax[0].set(ylabel=\"\")    \nax[1].title.set_text('cumulative')    \ndf_importances[[\"cumsum\"]].plot(kind=\"line\", linewidth=4,     \n                                 legend=False, ax=ax[1])    \nax[1].set(xlabel=\"\", xticks=np.arange(len(df_importances)),     \n          xticklabels=df_importances.index)    \nplt.xticks(rotation=70)    \nplt.grid(axis='both') \nplt.show()","a4bd368c":"y = \"Y\"\nx = \"BsmtQual\"\ncat, num = \"BsmtQual\", \"Y\"\nfig, ax = plt.subplots(nrows=1, ncols=2,  sharex=False, sharey=False)    \nfig.suptitle(x+\"   vs   \"+y, fontsize=16)\n\n### distribution    \nax[0].title.set_text('      density')    \nfor i in df[cat].unique():    \n    sns.distplot(df[df[cat]==i][num], hist=False, label=i, ax=ax[0])    \nax[0].grid(True)\n\n### stacked    \nax[1].title.set_text('bins')    \nbreaks = np.quantile(df[num], q=np.linspace(0,1,11))    \ntmp = df.groupby([cat, pd.cut(df[num], breaks, duplicates='drop')]).size().unstack().T    \ntmp = tmp[df[cat].unique()]    \ntmp[\"tot\"] = tmp.sum(axis=1)    \nfor col in tmp.drop(\"tot\", axis=1).columns:    \n     tmp[col] = tmp[col] \/ tmp[\"tot\"]    \ntmp.drop(\"tot\", axis=1).plot(kind='bar', stacked=True, ax=ax[1], legend=False, grid=True)\n\n### boxplot       \n#ax[2].title.set_text('outliers')    \nsns.catplot(x=cat, y=num, data=df, kind=\"box\")    \n#ax[2].grid(True)    \n#plt.show()","086085c1":"sns.distplot(df['FinalQual'])","187cd303":"X_names = ['LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'ExterQual', 'Foundation', 'BsmtQual', 'TotalBsmtSF', 'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', 'GrLivArea', 'FullBath', 'KitchenQual', 'TotRmsAbvGrd', 'FireplaceQu', 'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'Bath', 'YrBltAndRemod', 'AllSF', 'Garage', 'Qual', 'FinalQual', 'FireRooms', 'Area']\n\nX_train = df_train[X_names].values    \ny_train = df_train[\"Y\"].values\nX_test = df_test[X_names].values    \ny_test = df_test[\"Y\"].values","dd9e41a5":"## call model    \nmodel = linear_model.LinearRegression(normalize=True)\n## K fold validation    \nscores = []    \ncv = model_selection.KFold(n_splits=5, shuffle=True)    \nfig = plt.figure()    \ni = 1    \nfor train, test in cv.split(X_train, y_train):    \n    prediction = np.expm1(model.fit(X_train[train], y_train[train]).predict(X_train[test]))  \n    true = np.expm1(y_train[test])   \n    score = metrics.r2_score(true, prediction)    \n    scores.append(score)    \n    plt.scatter(prediction, true, lw=2, alpha=0.3, label='Fold %d (R2 = %0.2f)' % (i,score))    \n    i = i+1    \nplt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], linestyle='--', lw=2, color='black')    \nplt.xlabel('Predicted')    \nplt.ylabel('True')    \nplt.title('K-Fold Validation')    \nplt.legend()    \nplt.show()","7244a254":"## train    \nmodel.fit(X_train, y_train)\n## test    \npredicted = np.expm1(model.predict(X_test))","a6b49bab":"## Kpi    \nprint(\"R2 (explained variance):\", round(metrics.r2_score(np.expm1(y_test), predicted), 2))    \nprint(\"Mean Absolute Perc Error (\u03a3(|y-pred|\/y)\/n):\", round(np.mean(np.abs((np.expm1(y_test)-predicted)\/predicted)), 2))    \nprint(\"Mean Absolute Error (\u03a3|y-pred|\/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(np.expm1(y_test), predicted)))    \nprint(\"Root Mean Squared Error (sqrt(\u03a3(y-pred)^2\/n)):\", \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), predicted))))## residuals    \nresiduals = np.expm1(y_test) - predicted    \nmax_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)    \nmax_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))    \nmax_true, max_pred = y_test[max_idx], predicted[max_idx]    \nprint(\"Max Error:\", \"{:,.0f}\".format(max_error))","f8812bcc":"df_test_main.Id","790c7a1e":"predicted = np.expm1(model.predict(X_test_fin[X_names].values))\nsolution = pd.DataFrame({\"Id\":df_test_main.Id, \"SalePrice\":predicted})\nsolution.to_csv(\"\/kaggle\/working\/solution.csv\", index = False)","727bf044":"## call model    \nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nmodel = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n## K fold validation    \nscores = []    \ncv = model_selection.KFold(n_splits=5, shuffle=True)    \nfig = plt.figure()    \ni = 1    \nfor train, test in cv.split(X_train, y_train):    \n    prediction = np.expm1(model.fit(X_train[train], y_train[train]).predict(X_train[test]))    \n    true = np.expm1(y_train[test])   \n    score = metrics.r2_score(true, prediction)    \n    scores.append(score)    \n    plt.scatter(prediction, true, lw=2, alpha=0.3, label='Fold %d (R2 = %0.2f)' % (i,score))    \n    i = i+1    \nplt.plot([min(y_train),max(y_train)], [min(y_train),max(y_train)], linestyle='--', lw=2, color='black')    \nplt.xlabel('Predicted')    \nplt.ylabel('True')    \nplt.title('K-Fold Validation')    \nplt.legend()    \nplt.show()","3e27121e":"model.fit(X_train, y_train,              early_stopping_rounds=5, \n             eval_set=[(X_test, y_test)],\n             verbose=False)\npredicted = np.expm1(model.predict(X_test))","d9a6df83":"## Kpi    \nprint(\"R2 (explained variance):\", round(metrics.r2_score(np.expm1(y_test), predicted), 2))    \nprint(\"Mean Absolute Perc Error (\u03a3(|y-pred|\/y)\/n):\", round(np.mean(np.abs((np.expm1(y_test)-predicted)\/predicted)), 2))  \nprint(\"Mean Absolute Error (\u03a3|y-pred|\/n):\", \"{:,.0f}\".format(metrics.mean_absolute_error(np.expm1(y_test), predicted)))    \nprint(\"Root Mean Squared Error (sqrt(\u03a3(y-pred)^2\/n)):\", \"{:,.0f}\".format(np.sqrt(metrics.mean_squared_error(np.expm1(y_test), predicted))))## residuals    \nresiduals = np.expm1(y_test) - predicted    \nmax_error = max(residuals) if abs(max(residuals)) > abs(min(residuals)) else min(residuals)    \nmax_idx = list(residuals).index(max(residuals)) if abs(max(residuals)) > abs(min(residuals)) else list(residuals).index(min(residuals))    \nmax_true, max_pred = y_test[max_idx], predicted[max_idx]    \nprint(\"Max Error:\", \"{:,.0f}\".format(max_error))","b0db4673":"## Plot predicted vs true    \n#fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 4))    \n#from statsmodels.graphics.api import abline_plot    \n#ax[0].scatter(predicted, y_test, color=\"black\")    \n#abline_plot(intercept=0, slope=1, color=\"red\", ax=ax[0])    \n#ax[0].vlines(x=max_pred, ymin=max_true, ymax=max_true-max_error, color='red', linestyle='--', alpha=0.7, label=\"max error\")    \n#ax[0].grid(True)    \n#ax[0].set(xlabel=\"Predicted\", ylabel=\"True\", title=\"Predicted vs True\")    \n#ax[0].legend()\n## Plot predicted vs residuals    \n#ax[1].scatter(predicted, residuals, color=\"red\")    \n#ax[1].vlines(x=max_pred, ymin=0, ymax=max_error, color='black', linestyle='--', alpha=0.7, label=\"max error\")    \n#ax[1].grid(True)    \n#ax[1].set(xlabel=\"Predicted\", ylabel=\"Residuals\", title=\"Predicted vs Residuals\")    \n#ax[1].hlines(y=0, xmin=np.min(predicted), xmax=np.max(predicted))    \n#ax[1].legend()    \n#plt.show()","0e822488":"#fig, ax = plt.subplots()    \n#sns.distplot(residuals, color=\"red\", hist=True, kde=True, kde_kws={\"shade\":True}, ax=ax)    \n#ax.grid(True)    \n#ax.set(yticks=[], yticklabels=[], title=\"Residuals distribution\")    \n#plt.show()","fea9e301":"#print(\"True:\", \"{:,.0f}\".format(y_test[1]), \"--> Pred:\", \"{:,.0f}\".format(predicted[1]))","0d440fc9":"#explainer = lime_tabular.LimeTabularExplainer(training_data=X_train, feature_names=X_names, class_names=\"Y\", mode=\"regression\")    \n#explained = explainer.explain_instance(X_test[1], model.predict, num_features=10)    \n#explained.as_pyplot_figure()","77306ead":"# FullBath: Full bathrooms above grade(This will be the case of categorical (FullBath) vs numerical (Y))","253ab482":"EDA","e3592e53":"# SalePrice","63310838":"# GrLivArea: Above grade (ground) living area square feet (numerical  vs numerical )","02955fc3":"# Modelling","730248c0":"# Checking for multicollinearity features"}}