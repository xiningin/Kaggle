{"cell_type":{"e0ac4627":"code","4f45a7a1":"code","fb63059b":"code","009024df":"code","dd30ce2a":"code","d5b437c6":"code","8120ad77":"code","ab8ddb45":"code","f946ea1b":"code","3db79a18":"code","6991a7f8":"code","0e598ef6":"code","c6d144e5":"code","4f0568aa":"code","37a91ca8":"code","3f07022f":"code","05fd9805":"code","703d679c":"code","3faefbeb":"code","e1ccec78":"code","086ac4a7":"code","191b61f6":"code","fc5ab5ce":"code","5a1662c8":"code","b0815d1a":"code","e6a22d3f":"code","433dd9ba":"code","6394d4af":"code","f2da4c90":"code","06d7179f":"code","67c19742":"code","aa453005":"code","c90f3895":"code","42235416":"code","d10f1a66":"code","cdfafb33":"code","fd73135a":"code","78a5bcaf":"code","396c228b":"code","a7985ec3":"code","4a5aba99":"code","e3c9f14b":"code","78ab48d6":"code","093fd093":"code","271cadf5":"code","6937435a":"code","a64bf89a":"code","80bb3523":"code","d92b2ab6":"code","4b9755a8":"code","7d9cea9e":"code","9636af51":"code","2cbed68c":"code","d70f8cc9":"code","89b08eec":"code","a0b9525c":"code","9ce129f4":"code","cc94201c":"code","29a22bfb":"code","9269bc89":"code","2013e0dd":"code","99894e3a":"code","2600525f":"code","f2752a12":"code","f269fe25":"code","04797e8a":"code","7248f449":"code","961fcce4":"code","b5c2fc62":"code","bb862576":"code","dca08022":"code","86854e42":"code","907e9ee9":"code","1b67863b":"code","1f39ac5f":"code","3a363b77":"code","0f962a77":"code","13b20a8f":"code","35997e68":"code","3890904f":"code","440568cf":"code","8a38094f":"code","54b3c343":"code","d191f72e":"code","1d513b27":"code","1e156ed3":"code","2da0c896":"code","4a497009":"code","60f81b37":"code","0fa49f8a":"code","dd8f119a":"code","1beae0f9":"code","4ff8a5cd":"code","6a76109c":"code","10d4694c":"code","43e66fda":"code","ddf5e834":"code","b8742320":"code","7b030307":"code","478ddee4":"code","2ad72c75":"code","e9d9c46a":"code","b693ea56":"code","5196536f":"code","9b178476":"code","18ccf4da":"markdown","9346caaa":"markdown","a62cfa10":"markdown","24650f6b":"markdown","3bd05c4d":"markdown","778e46ed":"markdown","1b9c91b8":"markdown","3c968167":"markdown","56526165":"markdown","05cc1b3a":"markdown","80eeba19":"markdown","18a3c2dc":"markdown","80b7da6a":"markdown","967fbbd6":"markdown","9213f75e":"markdown","e967a2b4":"markdown","e7001715":"markdown","016dad60":"markdown","1f7f4015":"markdown","7d89f0a0":"markdown","a55b634b":"markdown","924d0f84":"markdown","9bba3ee0":"markdown","e864d53d":"markdown","4816844d":"markdown"},"source":{"e0ac4627":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('ggplot')","4f45a7a1":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom scipy.stats import skew,norm\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer","fb63059b":"from sklearn.model_selection import cross_val_score, GridSearchCV, KFold, cross_val_predict\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import scale\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR, LinearSVR\nimport lightgbm as lgb\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor","009024df":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","dd30ce2a":"train","d5b437c6":"train.dtypes","8120ad77":"train['SalePrice'].describe()","ab8ddb45":"sns.distplot(train['SalePrice'])","f946ea1b":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","3db79a18":"train.describe()","6991a7f8":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=0.8, square=True)","0e598ef6":"k = 11\ncols = train.corr().nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale = 1.25)\nplt.figure(figsize = (30,9))\nhm = sns.heatmap(cm, cbar=True, annot=True, \\\n                 square=True, fmt='.2f', annot_kws={'size': 14}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","c6d144e5":"sns.distplot(train['1stFlrSF'])","4f0568aa":"sns.distplot(train['GrLivArea'])","37a91ca8":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","3f07022f":"train.drop(train[(train[\"GrLivArea\"]>4000)&(train[\"SalePrice\"]<300000)].index,inplace=True)","05fd9805":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.OverallQual, y=train.SalePrice)\nplt.xlabel(\"OverallQual\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)","703d679c":"train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)","3faefbeb":"full=pd.concat([train,test], ignore_index=True)\nfull.drop(['Id'],axis=1, inplace=True)\nfull.shape","e1ccec78":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nfull['MSSubClass'] = full['MSSubClass'].apply(str)\nfull['YrSold'] = full['YrSold'].astype(str)\nfull['MoSold'] = full['MoSold'].astype(str)","086ac4a7":"# missing data\ntotal = full.isnull().sum()\ntotal = total[total>0]\npercent = (full.isnull().sum()\/full.isnull().count())\npercent = percent[percent>0]\ndtypes = full.dtypes\nnulls = np.sum(full.isnull())\ndtypes2 = dtypes.loc[(nulls != 0)]\nmissing_data = pd.concat([total,percent,dtypes2], axis = 1).sort_values(by=0,ascending=False)\nmissing_data","191b61f6":"# Before getting hands dirty with missing values, I will creat dummy variables for all non-missing categorical features.\nfull_missing_col = [col for col in full.columns if full[col].isnull().any()]                                  \nfull_predictors = full.drop(full_missing_col, axis=1)","fc5ab5ce":"full_category_cols= [cname for cname in full_predictors.columns if \nfull_predictors[cname].dtype == \"object\"]\nfull_category_cols","5a1662c8":"# create numerical columns list for future use.\nfull_num_cols= [cname for cname in full.columns if full[cname].dtype != \"object\"]\nfull_num_cols","b0815d1a":"full = pd.get_dummies(full,columns=full_category_cols)","e6a22d3f":"for col in (\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n           \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\",\n           \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\n            \"BsmtFinType2\", \"MasVnrType\"):\n    full[col] = full[col].fillna(\"None\")","433dd9ba":"for col in (\"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \n           \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\",\n           \"BsmtFullBath\", \"BsmtHalfBath\"):\n    full[col] = full[col].fillna(0)","6394d4af":"train.groupby(['Neighborhood'])[['LotFrontage']].agg(['mean','median','count'])","f2da4c90":"train[\"LotFrontage\"] = train.groupby([\"Neighborhood\"])[[\"LotFrontage\"]].transform(lambda x: x.fillna(x.median())) \ntest[\"LotFrontage\"] =  test.groupby([\"Neighborhood\"])[['LotFrontage']].transform(lambda x: x.fillna(x.median()))\nfull[\"LotFrontage\"] = pd.concat([train[\"LotFrontage\"],test[\"LotFrontage\"]],axis =0,ignore_index=True)","06d7179f":"test[\"LotFrontage\"].describe()","67c19742":"full[\"LotFrontage\"].describe()","aa453005":"sns.stripplot(x=\"PoolQC\", y=\"SalePrice\", data=full, size = 5, jitter = True)","c90f3895":"full.drop('PoolQC', axis=1, inplace=True)","42235416":"sns.stripplot(x=\"MiscFeature\", y=\"SalePrice\", data=full, size = 5, jitter = True)","d10f1a66":"# There are only a low number of houses in this area with any miscalleanous features.\nfull.drop(['MiscFeature'],axis=1, inplace=True)","cdfafb33":"sns.stripplot(x=\"Alley\", y=\"SalePrice\", data=full, size = 5, jitter = True)","fd73135a":"full['Alley'] = full['Alley'].map({\"None\":0, \"Grvl\":1, \"Pave\":2})\nfull.head(3)","78a5bcaf":"sns.stripplot(x=\"Fence\", y=\"SalePrice\", data=full, size = 5, jitter = True)","396c228b":"full['Fence'] = full['Fence'].map({\"None\":0, \"MnWw\":1, \"MnPrv\":2, \"GdPrv\":3, \"GdWo\":4})\nfull.head(3)","a7985ec3":"sns.stripplot(x=\"FireplaceQu\", y=\"SalePrice\", data=full, size = 5, jitter = True)","4a5aba99":"# this is a categorical feature with order, I will replace the values by hand.\nfull['FireplaceQu'] = full['FireplaceQu'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})\nfull['FireplaceQu'].unique()","e3c9f14b":"sns.stripplot(x=\"GarageCond\", y=\"SalePrice\", data=full, size = 5, jitter = True)","78ab48d6":"full['GarageCond'] = full['GarageCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})","093fd093":"sns.stripplot(x=\"GarageQual\", y=\"SalePrice\", data=full, size = 5, jitter = True)","271cadf5":"full['GarageQual'] = full['GarageQual'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})","6937435a":"sns.stripplot(x=\"GarageFinish\", y=\"SalePrice\", data=full, size = 5, jitter = True)","a64bf89a":"full['GarageFinish'] = full['GarageFinish'].map({\"None\":0, \"Unf\":1, \"RFn\":2, \"Fin\":3})","80bb3523":"sns.stripplot(x=\"GarageType\", y=\"SalePrice\", data=full, size = 5, jitter = True)","d92b2ab6":"full['GarageType'] = full['GarageType'].map({\"None\":0,\"2Types\":1,\"Attchd\":2,\"Basment\":3,\"BuiltIn\":4,\"CarPort\":5,\"Detchd\":6})","4b9755a8":"sns.stripplot(x=\"BsmtExposure\", y=\"SalePrice\", data=full, size = 5, jitter = True)","7d9cea9e":"full['BsmtExposure'] = full['BsmtExposure'].map({\"None\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4})","9636af51":"sns.stripplot(x=\"BsmtCond\", y=\"SalePrice\", data=full, size = 5, jitter = True)","2cbed68c":"full['BsmtCond'] = full['BsmtCond'].map({\"None\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4, \"Ex\":5})","d70f8cc9":"sns.stripplot(x=\"BsmtQual\", y=\"SalePrice\", data=full, size = 5, jitter = True)","89b08eec":"full['BsmtQual'] = full['BsmtQual'].map({\"None\":0, \"Fa\":1, \"TA\":2, \"Gd\":3, \"Ex\":4})","a0b9525c":"sns.stripplot(x=\"BsmtFinType2\", y=\"SalePrice\", data=full, size = 5, jitter = True)","9ce129f4":"full['BsmtFinType2'] = full['BsmtFinType2'].map({\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5,\"GLQ\":6})","cc94201c":"sns.stripplot(x=\"BsmtFinType1\", y=\"SalePrice\", data=full, size = 5, jitter = True)","29a22bfb":"full['BsmtFinType1'] = full['BsmtFinType1'].map({\"None\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5,\"GLQ\":6})","9269bc89":"sns.stripplot(x=\"MasVnrType\", y=\"SalePrice\", data=full, size = 5, jitter = True)","2013e0dd":"full = pd.get_dummies(full, columns = [\"MasVnrType\"], prefix=\"MasVnrType\")","99894e3a":"sns.stripplot(x=\"MSZoning\", y=\"SalePrice\", data=full, size = 5, jitter = True)","2600525f":"full = pd.get_dummies(full, columns = [\"MSZoning\"], prefix=\"MSZoning\")","f2752a12":"plt.subplots(figsize =(15, 5))\nplt.subplot(1, 2, 1)\ng = sns.countplot(x = \"Utilities\", data = train).set_title(\"Utilities - Training\")\nplt.subplot(1, 2, 2)\ng = sns.countplot(x = \"Utilities\", data = test).set_title(\"Utilities - Test\")","f269fe25":"# Since there is only one category for Utilities feature, therefore I will drop this feature.\nfull = full.drop(['Utilities'], axis=1)","04797e8a":"# 'GarageYrBlt' is missing 164 values. I assume 'GarageYrBlt' equal to YearBuilt time.Thus these two columns will be highly correlated. Therefore, I delete the 'GarageYrBlt'.\nfull['GarageYrBlt'] = full['GarageYrBlt'].fillna(full['YearBuilt'])","7248f449":"for col in ( \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \n           \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"MasVnrArea\",\n           \"BsmtFullBath\", \"BsmtHalfBath\"):\n    full[col] = full[col].fillna(0)","961fcce4":"full['Electrical'] = full['Electrical'].fillna(full['Electrical'].mode()[0])\nfull['KitchenQual'] = full['KitchenQual'].fillna(full['KitchenQual'].mode()[0])\nfull['Exterior1st'] = full['Exterior1st'].fillna(full['Exterior1st'].mode()[0])\nfull['Exterior2nd'] = full['Exterior2nd'].fillna(full['Exterior2nd'].mode()[0])\nfull['SaleType'] = full['SaleType'].fillna(full['SaleType'].mode()[0])\nfull[\"Functional\"] = full[\"Functional\"].fillna(full['Functional'].mode()[0])\nfull = pd.get_dummies(full,columns=['Electrical','KitchenQual','Exterior1st','Exterior2nd','SaleType',\"Functional\"])","b5c2fc62":"# check missing data again\ntotal = full.isnull().sum()\ntotal = total[total>0]\npercent = (full.isnull().sum()\/full.isnull().count())\npercent = percent[percent>0]\ndtypes = full.dtypes\nnulls = np.sum(full.isnull())\ndtypes2 = dtypes.loc[(nulls != 0)]\nmissing_data = pd.concat([total,percent,dtypes2], axis = 1).sort_values(by=0,ascending=False)\nmissing_data","bb862576":"# Adding total sqfootage feature \nfull['TotalSF'] = full['TotalBsmtSF'] + full['1stFlrSF'] + full['2ndFlrSF']","dca08022":"# Check skewness for all numerical variables\nskew_features = full[full_num_cols + ['TotalSF']].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})","86854e42":"# Box-Cox Transformation\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nhigh_skew = skew_features[abs(skew_features) > 0.75]\nskew_index = high_skew.index\n\nfor i in skew_index:\n    full[i]= boxcox1p(full[i], boxcox_normmax(full[i]+1))\n\nskew_features2 = full[full_num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews2 = pd.DataFrame({'skew':skew_features2})\nskews2","907e9ee9":"# logistic for SalePrice\nplt.subplot(1, 2, 1)\nsns.distplot(train.SalePrice, kde=False, fit = norm)\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log(train.SalePrice + 1), kde=False, fit = norm)\nplt.xlabel('Log SalePrice')","1b67863b":"train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","1f39ac5f":"full = full.drop(['SalePrice'],axis=1)","3a363b77":"# use robustscaler since maybe there are other outliers.\nscaler = RobustScaler()","0f962a77":"n_train=train.shape[0]\n\nX = full[:n_train]\ntest_X = full[n_train:]\ny= train.SalePrice\n\nX_scaled = scaler.fit(X).transform(X)\ntest_X_scaled = scaler.transform(test_X)","13b20a8f":"print(X.shape)\nprint(test_X.shape)\nprint (y.shape)","35997e68":"#Validation function\nn_folds = 5\ndef rmse_cv(model,X,y):\n    kf = KFold(n_folds, shuffle=True, random_state=40).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return rmse","3890904f":"models = [LinearRegression(),Ridge(),Lasso(alpha=0.01,max_iter=10000),DecisionTreeRegressor(random_state=1),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n          ElasticNet(alpha=0.001,max_iter=10000),SGDRegressor(max_iter=1000,tol=1e-3),BayesianRidge(),KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n          ExtraTreesRegressor(),XGBRegressor()]","440568cf":"names = [\"LR\", \"Ridge\", \"Lasso\", \"DT\",\"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\nfor name, model in zip(names, models):\n    score = rmse_cv(model, X_scaled, y)\n    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))","8a38094f":"class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])\n","54b3c343":"grid(Lasso()).grid_get(X_scaled,y,{'alpha': [.0001, .0003, .0004,.0005, .0006,.0007, .0009, \n          .01, 0.05, 0.1]})","d191f72e":"grid(Ridge()).grid_get(X_scaled,y,{'alpha':[.0001, .0003, .0005, .0007, .0009, \n          .01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50, 60, 70, 80]})","1d513b27":"grid(Ridge()).grid_get(X_scaled,y,{'alpha':[13,13.1,13.2,13.3,13.4,13.5,13.6,13.7]})","1e156ed3":"grid(ElasticNet()).grid_get(X_scaled,y,{'alpha':[0.005,0.0007,0.0008,0.0009],'l1_ratio':[0.5,0.6,0.7,0.8]})","2da0c896":"grid(SVR()).grid_get(X_scaled,y,{'C':[11,13,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})","4a497009":"param_grid={'alpha':[0.1,0.15,0.2,0.25,0.3,0.4], 'kernel':[\"polynomial\"], 'degree':[1,2,3,4],'coef0':[0.8,1]}\ngrid(KernelRidge()).grid_get(X_scaled,y,param_grid)","60f81b37":"import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","0fa49f8a":"score = rmse_cv(model_xgb,X_scaled,y)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","dd8f119a":"import lightgbm as lgb\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","1beae0f9":"score = rmse_cv(model_lgb,X_scaled,y)\nprint(\"Lgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4ff8a5cd":"class AverageWeight(BaseEstimator, RegressorMixin):\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n        \n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.mod]\n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.models_])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w","6a76109c":"lasso = Lasso(alpha=0.0005,max_iter=10000)\nridge = Ridge(alpha=13.1)\nsvr = SVR(gamma= 0.0004,kernel='rbf',C=15,epsilon=0.008)\nker = KernelRidge(alpha=0.1 ,kernel='polynomial',degree=2, coef0=1)\nela = ElasticNet(alpha=0.0007,l1_ratio=0.7,max_iter=10000)\nbay = BayesianRidge()","10d4694c":"# assign weights based on their gridsearch score\nw1 = 0.3\nw2 = 0.05\nw3 = 0.15\nw4 = 0.1\nw5 = 0.3\nw6 = 0.1","43e66fda":"weight_avg = AverageWeight(mod = [lasso,ridge,svr,ker,ela,bay],weight=[w1,w2,w3,w4,w5,w6])","ddf5e834":"score = rmse_cv(weight_avg,X_scaled,y)\nprint(score.mean())","b8742320":"a = Imputer().fit_transform(X_scaled)\nb = Imputer().fit_transform(y.values.reshape(-1,1)).ravel()\nfrom mlxtend.regressor import StackingCVRegressor\nstack_gen = StackingCVRegressor(regressors=(ridge,ela, \n                                            ker, svr,bay), \n                               meta_regressor=lasso,\n                               use_features_in_secondary=False)\nstack_gen_model = stack_gen.fit(a, b)\nscore = rmse_cv(stack_gen_model,a,b)\nprint(score.mean())","7b030307":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","478ddee4":"stack_train_pred = stack_gen_model.predict(X_scaled)\nc = Imputer().fit_transform(test_X_scaled)\nstack_pred = np.expm1(stack_gen_model.predict(c))\nprint(rmsle(stack_train_pred, y))","2ad72c75":"model_xgb.fit(X_scaled, y)\nxgb_train_pred = model_xgb.predict(X_scaled)\nxgb_pred = np.expm1(model_xgb.predict(test_X_scaled))\nprint(rmsle(xgb_train_pred, y))","e9d9c46a":"model_lgb.fit(X_scaled, y)\nlgb_train_pred = model_lgb.predict(X_scaled)\nlgb_pred = np.expm1(model_lgb.predict(test_X_scaled))\nprint(rmsle(lgb_train_pred, y))","b693ea56":"ensemble_train = stack_train_pred*0.7 + xgb_train_pred*0.15 + lgb_train_pred*0.15\nprint(rmsle(ensemble_train, y))","5196536f":"ensemble = stack_pred*0.8 + xgb_pred*0.1 + lgb_pred*0.1","9b178476":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': ensemble})\nmy_submission.to_csv('submission.csv', index=False)","18ccf4da":"4. Data cleaning (outliers,  data errors, missing data)\n  * 4.1 outliers","9346caaa":"2. Data Exploratory Analysis","a62cfa10":"Firstly,  \"LotFrontage\"is missing 14% values. I will fill with the median value group by LotAreaCut. Here I will fill separately in train and test dataset.","24650f6b":"I also tried the PCA to decrease feature number, but the performance does not work well.\n  * pca = PCA(n_components=250)\n     X_scaled=pca.fit_transform(X_scaled)\n     test_X_scaled = pca.transform(test_X_scaled)","3bd05c4d":"Next, I will deal with garage related variables.\n  * GarageCond, GarageQual, GarageFinish, and GarageType","778e46ed":"2.1 Univariate 'SalePrice'","1b9c91b8":"5. Machine Learning Modeling \nFor this analysis I am trying 14 different algorithms:\n * LinearRegression()\n * Ridge()\n * Lasso(alpha=0.01,max_iter=10000)\n * DecisionTreeRegressor(random_state=1)\n * RandomForestRegressor()\n * GradientBoostingRegressor()\n * SVR()\n * LinearSVR()\n * ElasticNet(alpha=0.001,max_iter=10000)\n * SGDRegressor(max_iter=1000,tol=1e-3)\n * BayesianRidge()\n * KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n * ExtraTreesRegressor()\n * XGBRegressor()\nThe method of measuring accuracy was chosen to be Root Mean Squared Error, as described within the competition.","3c968167":"2.2 Multi-Variate Analysis","56526165":"5. Data transformation via pipelines\nFeature Engineering\n  * 5.1 Convert some numerical variables into categorical variables: LabelEncoder and dummy variables. (Already did during missing value filling)\n  * 5.2 Run Box-Cox transformation for skewed numercial variables\n  * 5.3 Perform RobustScaler() to better fit outlier values\n  * 5.4 PCA (doesn't work well)","05cc1b3a":"* Peak Kurtosis  > 3 (normal distribution kurtosis). \nPositive skewness > 1 (normal distribution skewness).","80eeba19":"Hyperparameter Tuning","18a3c2dc":"Great, there is no missing value, except the SalePrice variable in test dataset.","80b7da6a":"4.2 data error","967fbbd6":"It is nice to know in which features miss values in both train and test dataset. I will only fill missing data in the train dataset with \"None\" or \"0\", then using get_dummy, without changing test data value.","9213f75e":"Ensemble Learning","e967a2b4":"Basement related variables: BsmtExposure, BsmtCond, BsmtQual, BsmtFinType2, BsmtFinType1","e7001715":"  * 4.3 Missing Data\n   * Here I will spend lots of time on missing data. I will go over with overall missing variables. Then fill missing variables from the most to least. Some categorical features will be transform to dummy variables and some will be deleted. Some numerical will be fill with median or 0. ","016dad60":"1. Define business object\n  * Aim: to forecast the house price in test.csv\n  * Target variable: sale price\n  * Input variables: 79 variables including sale price in train.csv; 78 variables without sale price in test.csv","1f7f4015":"There is no missing value for Sale Price and data type is numerical.","7d89f0a0":"3. Correlation analysis (pair-wise and attribute combinations)","a55b634b":"Seems like there are some numerical variables also have positive skews. Will do logistic algorithms for those variables later.","924d0f84":"House Price Forecast Project Workflow\n1. Define business object\n2. Data Exploratory Analysis\n   * Univariate\n   * Multi-Variate\n3. Correlation analysis (pair-wise and attribute combinations)\n4. Data cleaning (missing data, outliers, data errors)\n   * missing data: fill train dataset with median, and fill test dataset with 0\n   * delete some outliers\n   * convert some numerical variables to categorical data type (year and month)\n5. Data transformation via pipelines (categorical text to number using one hot encoding, feature scaling via normalization\/standardization, feature combinations)\n  * convert categorical variables using labelencoder( )\n  * log-normality for skewed variables\n  * robustscaler (sensitive to outliers)\n  * feature combinations: combine some variables into one (like garage and basement variables)\n6. Train and cross validate 14 models\n  * calculated rmse score\n  * hyperparameter tuning by GridSerach\n  * ensemble learning first stacking meta model\n  * ensemble learning second tier stacking meta model with lgboost and xgboost (good for outliers)\n7. Evaluate the model with best estimators in the test set","9bba3ee0":"The bottom right two points with very large GrLivArea seem like outliers to me (might be large arigucultural land). Also since they are not in the line regression cluster, I will delete them.","e864d53d":"In order to better analyzing data, I will combine train and test dataset. Since there may miss a lot of data and may miss different variable value between train and test dataset. I will be very careful dealing with test dataset in data cleaning and data transformation parts.","4816844d":"PoolQC is missing 99% data  and it doesn't show any relation with sale price. Therefore, I will delete PoolQC."}}