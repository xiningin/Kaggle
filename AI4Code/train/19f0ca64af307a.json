{"cell_type":{"c2377a51":"code","c83460ed":"code","40e04f56":"code","44b6346d":"code","4b4cb7f8":"code","487ce662":"code","53204f31":"code","adb671c7":"code","b22b906a":"code","1cbfeb83":"code","309c84e9":"code","5ead5bc0":"code","392a77a6":"code","722a3717":"code","2780ca9a":"code","e580e044":"code","01214203":"code","060c4a95":"code","44d36ce4":"code","a7b6d4be":"code","dad832c1":"code","17235036":"code","5bd7dff6":"code","68af27b6":"markdown","488551f3":"markdown","49a4a6ff":"markdown","1f9ffe80":"markdown","df8e6faa":"markdown","1bb1f1dd":"markdown","cfb5f984":"markdown","1c2f44a6":"markdown","f850f31a":"markdown","76ca0d25":"markdown","67ab2a8a":"markdown","0c22c71c":"markdown","06ec347f":"markdown","dfb8ba10":"markdown","11383f19":"markdown","68420a33":"markdown","48627d80":"markdown","e6d67eb4":"markdown"},"source":{"c2377a51":"!pip install skorch -q ","c83460ed":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import EpochScoring\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.manual_seed(0)","40e04f56":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id').values.astype('float32')\n\nX = train.drop('target', axis = 1).values.astype('float32')\n\nlencoder = LabelEncoder()\ny = lencoder.fit_transform(train['target']).astype('int64')","44b6346d":"num_features = 50\n\nblock_config = {\n        'fc_block1':{\n            'values':[num_features, 128]},\n        'fc_block2':{\n            'values':[num_features, 128, 64]},\n        'fc_block3':{\n            'values':[num_features, 256, 128, 64]},\n        'fc_block4':{  \n            'values':[num_features, 512, 256, 128, 64]} \n    }\n\n\ndef linear_block(in_features, out_features, p_drop, nonlinear, batch_norm, *args, **kwargs):\n    \n    layers = []\n    layers.append(nn.Linear(in_features, out_features))\n    if batch_norm:\n        layers.append(nn.BatchNorm1d(out_features))\n    layers.append(nonlinear)\n    layers.append(nn.Dropout(p = p_drop))    \n    \n    return nn.Sequential(*layers)\n\nclass TPS05Classification(nn.Module):\n    def __init__(self, num_class = 4, dropout = 0.3, nonlinear = nn.ReLU(), block = 1, batch_norm = True):\n        super(TPS05Classification, self).__init__()\n        \n        self.non_linear = nonlinear\n        \n        self.lin_sizes = block_config['fc_block'+str(block)]['values']\n        \n        lin_blocks = [linear_block(in_f, out_f, dropout, self.non_linear, batch_norm) \n                      for in_f, out_f in zip(self.lin_sizes, self.lin_sizes[1:])]\n        \n        self.linear = nn.Sequential(*lin_blocks)\n        \n        self.out = nn.Sequential(\n            nn.Linear(block_config['fc_block'+str(block)]['values'][-1], num_class))\n    \n    def forward(self, x):\n        x = self.linear(x)\n        return  F.softmax(self.out(x), dim = -1)","4b4cb7f8":"from skorch.callbacks import LRScheduler, EarlyStopping\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nlr_scheduler = LRScheduler(policy = ReduceLROnPlateau, monitor = 'valid_loss', mode = 'min', patience = 3, factor = 0.1, verbose = True)\nearly_stopping = EarlyStopping(monitor='valid_loss', patience = 10, threshold = 0.0001, threshold_mode='rel', lower_is_better=True)","487ce662":"from skorch.callbacks import Callback\n\n\nclass TPS05CustomCallback(Callback):\n    def __init__(self, ):\n        self.best_epoch_ = 0\n\n    def initialize(self):\n        self.best_log_loss_ = 999\n\n    def on_epoch_end(self, net, **kwargs):\n        if net.history[-1, 'valid_loss'] < self.best_log_loss_:\n            self.best_log_loss_ = net.history[-1, 'valid_loss']\n            self.best_epoch_ = len(net.history)\n\n    def on_train_end(self, net, **kwargs):\n        print(f\">>>> Training end. The best log_loss: {self.best_log_loss_} on epoch: {self.best_epoch_} <<<< \\n\")","53204f31":"# No additional parameters - we will find them using GridSearchCV\n\nnet = NeuralNetClassifier(TPS05Classification, device = device, lr = 0.001, max_epochs = 50, callbacks = [lr_scheduler, early_stopping, TPS05CustomCallback])","adb671c7":"steps = [('scaler', StandardScaler()), ('net', net)]\npipeline = Pipeline(steps)","b22b906a":"grid_params = {\n    # For the first two params we used Callbacks \n    #'net__max_epochs':[20, 40], \n    #'net__lr': [0.001, 0.0001], \n    'net__module__dropout': [0.2, 0.3],\n    'net__optimizer': [optim.Adam, optim.RMSprop], \n    'net__module__block': [2, 3],\n    'net__module__nonlinear': [nn.ReLU(), nn.Softmax(dim = 1)], # we can play with network architecture as well \n    'net__module__batch_norm': [True, False] # BatchNormalization test\n} \n\ngrid_net = GridSearchCV(pipeline, grid_params, refit = True, cv = 3, scoring = 'neg_log_loss', verbose = 1)","1cbfeb83":"result = grid_net.fit(X,y)","309c84e9":"print(grid_net.best_params_)","5ead5bc0":"def report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n\nreport(grid_net.cv_results_,5)","392a77a6":"grid_net.best_estimator_","722a3717":"epochs = [i for i in range(len(grid_net.best_estimator_[1].history))]\ntrain_loss = grid_net.best_estimator_[1].history[:,'train_loss']\nvalid_loss = grid_net.best_estimator_[1].history[:,'valid_loss']","2780ca9a":"plt.plot(epochs,train_loss,'g-');\nplt.plot(epochs,valid_loss,'r-');\nplt.title('Training Loss Curves');\nplt.xlabel('Epochs');\nplt.ylabel('Mean Squared Error');\nplt.legend(['Train','Validation']);","e580e044":"y_pred = grid_net.predict_proba(test)","01214203":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n\npredictions_df = pd.DataFrame(y_pred, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']","060c4a95":"predictions_df.head(5)","44d36ce4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\n\npalette = itertools.cycle(sns.color_palette())\n\nplt.figure(figsize=(16, 8))\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    c = next(palette)\n    sns.histplot(predictions_df, x = f'Class_{i+1}', color=c)\nplt.suptitle(\"Class prediction distribution\")","a7b6d4be":"predictions_df.drop(\"id\", axis=1).describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","dad832c1":"predictions_df.to_csv(\"skorch_nn_tutorial_submission.csv\", index = False)","17235036":"# GREAT kernel: https:\/\/www.kaggle.com\/lazaro97\/tps-may-stacking-blending-pseudolabelling \n\nblend_l1 = pd.read_csv(\"..\/input\/tps-may-stacking-blending-pseudolabelling\/sub_lb_0.09080355083449096_0.7.csv\")\n\n\noutput = predictions_df.copy()\noutput[\"Class_1\"] = (predictions_df.Class_1 * 0.3 + blend_l1.Class_1 * 0.7)\noutput[\"Class_2\"] = (predictions_df.Class_2 * 0.3 + blend_l1.Class_2 * 0.7)\noutput[\"Class_3\"] = (predictions_df.Class_3 * 0.3 + blend_l1.Class_3 * 0.7) \noutput[\"Class_4\"] = (predictions_df.Class_4 * 0.3 + blend_l1.Class_4 * 0.7) ","5bd7dff6":"predictions_df = pd.DataFrame(output, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sub['id']\npredictions_df.to_csv(\"TPS-05-skorch_blended_submission.csv\", index = False)","68af27b6":"#### CUSTOM\n\nThis is very easy sample to show you how to jump into two stages: epoch_end and train_end but you can define more callbacks: https:\/\/skorch.readthedocs.io\/en\/stable\/callbacks.html# ","488551f3":"BONUS (for LB score lovers) - What if we blend NN score with my blender database .... ? :)","49a4a6ff":"### CALLBACKS\n\nInstead of searching LR and max_epochs params using GridSearch I decided to use better strategy - use Callbacks. Skorch supports wide range of callbacks (you can even write custom one).\n\n#### BUILD-IN","1f9ffe80":"## PLOT BEST ESTIMATOR LEARNING CURVES","df8e6faa":"## SKORCH SCIKIT-LEARN WRAPPER ","1bb1f1dd":"## SCIKIT-LEARN PIPELINE","cfb5f984":"## LET'S DEFINE GRIDSEARCH PARAMETERS","1c2f44a6":"## MINIMAL DATASET PREPARATION","f850f31a":"# SKORCH EXAMPLE ON TPS-05 - SIMPLE NN START WITH FEW LINES OF CODE\n\n<div class=\"alert alert-warning\">\nThe goal of skorch is to make it possible to use PyTorch with sklearn. This is achieved by providing a wrapper around PyTorch that has an sklearn interface. In that sense, skorch is the spiritual successor to nolearn, but instead of using Lasagne and Theano, it uses PyTorch. Skorch does not re-invent the wheel, instead getting as much out of your way as possible. If you are familiar with sklearn and PyTorch, you don\u2019t have to learn any new concepts, and the syntax should be well known. (If you\u2019re not familiar with those libraries, it is worth getting familiarized.)\n<\/div>\n<div><br><\/div>\n<div aligh=\"center\"><img src=\"https:\/\/skorch.readthedocs.io\/en\/stable\/_static\/logo.svg\"><\/div>\n\n","76ca0d25":"## LET'S TRAIN THE NETWORK AND FIND THE BEST HYPERPARAMETERS","67ab2a8a":"## SHOW BEST PARAMETERS FOR CURRENT NN ARCHITECTURE","0c22c71c":"## LET'S DEFINE NN MODEL (WE USE PYTORCH WAY OF DEFINING MODEL)","06ec347f":"## PREDICT ","dfb8ba10":"The goal of this notebook is to show how to create very simple NN using Pytorch model with Scikit-Learn Wrapper. Content:\n\n<ul>\n    <li>Install skorch<\/li>\n    <li>Prepare data<\/li>\n    <li>Define Pytorch simple Sequential model<\/li>\n    <li>Define skorch wrapper<\/li>\n    <li>Create simple scikit-learn Pipeline<\/li>\n    <li>Search for NN hyperparameters using GridSearchCV<\/li>\n    <li>Callback implemented<\/li>\n    <ul>\n        <li>EarlyStopping<\/li>\n        <li>Lerning Scheduler<\/li>\n    <\/ul>\n    <li>Gridsearch - searching for best Network Architecture<\/li>\n    <ul>\n        <li>The best NonLinear module search using GridSearch<\/li>\n        <li>Module configuration (eg. BatchNormalization on\/off)<\/li>\n    <\/ul>\n<\/ul>\n\n<div class=\"alert alert-info\">\n    <strong>Important links:<\/strong>\n<ul>\n    <li><a href=\"https:\/\/skorch.readthedocs.io\/en\/stable\/\">Skorch documentation<\/a><\/li>\n    <li><a href=\"https:\/\/github.com\/skorch-dev\/skorch\">Skorch repo<\/a><\/li>\n<\/ul>\n<\/div>","11383f19":"## SHOW MODEL RANKING (TOP 5)","68420a33":"### SKORCH - SCIKIT PYTORCH WRAPPER","48627d80":"## ... AND SUBMIT","e6d67eb4":"## SHOW THE BEST ESTIMATOR CONFIGURATION"}}