{"cell_type":{"8fb68268":"code","5c15356f":"code","200050e1":"code","5570034b":"code","2e695d61":"code","98be510b":"code","2c3574be":"code","4f463b80":"code","f7b403e5":"code","29faa1c4":"code","f26e67d1":"code","13528b1a":"code","37110a2c":"code","6b2a9cfc":"code","646b10c6":"code","b2261e92":"code","c5d4a9ab":"code","0fb56073":"code","6ad9b4ff":"code","a71780bc":"code","fa220eb0":"code","80e3c84b":"code","1ad83201":"code","8ae0e659":"code","f1667c77":"code","5e98c43e":"code","a7b2ad5f":"code","7b03ba24":"code","6871fecb":"code","5be8be3d":"code","56eb6321":"code","03ad1d94":"code","fcdb2157":"code","4b7b9a55":"code","254ea041":"code","1de131cd":"code","781b0944":"code","c2f0ba6f":"code","1da65f5d":"code","fe80efb6":"code","8a217cde":"code","58935702":"code","7e7aedea":"code","4edfe062":"code","327c0621":"code","66e37196":"code","e4538f94":"code","514a85ca":"code","2e8693c3":"code","c3ceb284":"code","271135a5":"code","476f2fa1":"code","1ba20ae8":"code","a269f43e":"code","c8cab82a":"code","2b58a9ce":"code","bd36ab43":"code","53021524":"code","753cb56b":"code","ed96fd8a":"code","a7cc3c51":"markdown","966d6366":"markdown","44a80344":"markdown","a182b4c8":"markdown","9411ec81":"markdown","a4d4f143":"markdown","82cf05da":"markdown","9c833f3a":"markdown","7800226d":"markdown","250e0455":"markdown","69259b6e":"markdown","0ee0c246":"markdown","e7ae4cba":"markdown","5013e089":"markdown","e415a489":"markdown","e3a95e31":"markdown","6a041e85":"markdown","3b0329aa":"markdown","0d33a53b":"markdown","d2ee2ed7":"markdown","57f3bf7f":"markdown","186f6aef":"markdown","4f7014d1":"markdown","66b721a7":"markdown","fea2ba2e":"markdown"},"source":{"8fb68268":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import chi2_contingency\nfrom imblearn.over_sampling import SMOTE\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\ndf = pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')\nprint(df.shape)\ndf.head()","5c15356f":"df1 = df.dropna(how = 'any', axis = 0)\nprint(df1.shape)\ndf1.head()","200050e1":"print('Gender')\nprint(df1['male'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Education')\nprint(df1['education'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('BP Medication')\nprint(df1['BPMeds'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Stroke')\nprint(df1['prevalentStroke'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Hypertension')\nprint(df1['prevalentHyp'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Diabetes')\nprint(df1['diabetes'].value_counts(normalize = True))\nprint('----')\nprint('\\n')","5570034b":"disease = df1.groupby('TenYearCHD')\n\nprint('Gender')\nprint(disease['male'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Education')\nprint(disease['education'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('BP Medication')\nprint(disease['BPMeds'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Stroke')\nprint(disease['prevalentStroke'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Hypertension')\nprint(disease['prevalentHyp'].value_counts(normalize = True))\nprint('----')\nprint('\\n')\n\nprint('Diabetes')\nprint(disease['diabetes'].value_counts(normalize = True))\nprint('----')\nprint('\\n')","2e695d61":"print('Gender')\ncontigency= pd.crosstab(df1['male'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')\n\nprint('Education')\ncontigency= pd.crosstab(df1['education'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')\n\nprint('BP Medication')\ncontigency= pd.crosstab(df1['BPMeds'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')\n\nprint('Stroke')\ncontigency= pd.crosstab(df1['prevalentStroke'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')\n\nprint('Hypertension')\ncontigency= pd.crosstab(df1['prevalentHyp'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')\n\nprint('Diabetes')\ncontigency= pd.crosstab(df1['diabetes'], df['TenYearCHD']) \nc, p, dof, expected = chi2_contingency(contigency) \nprint(p)\nprint('____')","98be510b":"num_var = ['totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose', 'cigsPerDay']\ndf_num = df1[num_var]","2c3574be":"for i in np.arange(0.0, 0.5, 0.01):\n    sel = VarianceThreshold(threshold=i) #Checking different Variance Thresholds\n    sel.fit(df_num)\n    quasi_constant = df_num.columns[~sel.get_support()]\n    print('Variance Threshold = ', i)\n    print(quasi_constant)\n    print('----')","4f463b80":"fig, ax = plt.subplots(figsize=(20,20))\nsns.set(font_scale=1)\nsns.heatmap(df_num.corr(), annot = True, ax = ax)","f7b403e5":"disease['totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose', 'cigsPerDay'].agg(['mean', 'median'])","29faa1c4":"for i in df_num.columns:\n    print(i)\n    no_hd = df1[df1['TenYearCHD']== 0]\n    hd = df1[df1['TenYearCHD']== 1]\n    print(ttest_ind(no_hd[i], hd[i]))\n    print('----')","f26e67d1":"df1.head()","13528b1a":"dfmod = df1[['male', 'age', 'education', 'currentSmoker', 'cigsPerDay', 'BPMeds',\n       'prevalentStroke', 'prevalentHyp', 'diabetes', 'totChol', 'sysBP',\n       'BMI', 'glucose', 'TenYearCHD']]\ndfmod.head()","37110a2c":"df_cat = dfmod[['education', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']]\ndfmod = pd.get_dummies(dfmod, columns = df_cat.columns, drop_first = True)\ndfmod = dfmod[['male', 'age', 'currentSmoker', 'cigsPerDay', 'totChol', 'sysBP', 'BMI',\n       'glucose', 'education_2.0', 'education_3.0',\n       'education_4.0', 'BPMeds_1.0', 'prevalentStroke_1', 'prevalentHyp_1',\n       'diabetes_1', 'TenYearCHD']]\ndfmod.head()","6b2a9cfc":"vif_data = pd.DataFrame() \nvif_data[\"Feature\"] = dfmod.columns\nvif_data[\"VIF Score\"] = [variance_inflation_factor(dfmod.values, i) for i in range(len(dfmod.columns))]\nvif_data","646b10c6":"dfmod = dfmod.drop('sysBP', axis = 1)\nvif_data = pd.DataFrame() \nvif_data[\"Feature\"] = dfmod.columns\nvif_data[\"VIF Score\"] = [variance_inflation_factor(dfmod.values, i) for i in range(len(dfmod.columns))]\nvif_data","b2261e92":"dfmod = dfmod.drop('BMI', axis = 1)\nvif_data = pd.DataFrame() \nvif_data[\"Feature\"] = dfmod.columns\nvif_data[\"VIF Score\"] = [variance_inflation_factor(dfmod.values, i) for i in range(len(dfmod.columns))]\nvif_data","c5d4a9ab":"dfmod = dfmod.drop('totChol', axis = 1)\nvif_data = pd.DataFrame() \nvif_data[\"Feature\"] = dfmod.columns\nvif_data[\"VIF Score\"] = [variance_inflation_factor(dfmod.values, i) for i in range(len(dfmod.columns))]\nvif_data","0fb56073":"dfmod = dfmod.drop('glucose', axis = 1)\nvif_data = pd.DataFrame() \nvif_data[\"Feature\"] = dfmod.columns\nvif_data[\"VIF Score\"] = [variance_inflation_factor(dfmod.values, i) for i in range(len(dfmod.columns))]\nvif_data","6ad9b4ff":"selected_feat = []\nfor i in vif_data['Feature']:\n    selected_feat.append(i)\nselected_feat","a71780bc":"X = dfmod.drop(['TenYearCHD'], axis = 1)\ny = dfmod['TenYearCHD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\nselected_feat.remove('TenYearCHD')\nX_train = X_train[selected_feat]\nX_train.head()","fa220eb0":"std_scaler = StandardScaler()\nX_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns=X_train.columns)\nX_train","80e3c84b":"clf = SVC(random_state=42)\n\nparam_grid = { \n    'kernel': ['poly', 'rbf', 'sigmoid'],\n    'degree': [2, 3, 4, 5, 6],\n    'gamma' : ['scale', 'auto']\n}\n\ngscv_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 10)\ngscv_clf.fit(X_train, y_train)","1ad83201":"gscv_clf.best_params_","8ae0e659":"clf = SVC(degree = 2, gamma = 'scale', kernel= 'poly')\nprint(cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy'))","f1667c77":"clf = LogisticRegression(random_state=42)\n\nparam_grid = { \n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\ngscv_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 10)\ngscv_clf.fit(X_train, y_train)","5e98c43e":"gscv_clf.best_params_","a7b2ad5f":"clf = LogisticRegression(penalty = 'l1', solver = 'liblinear')\nprint(cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy'))","7b03ba24":"X = dfmod.drop(['TenYearCHD'], axis = 1)\ny = dfmod['TenYearCHD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\nX_train = X_train[selected_feat]\nX_train.head()","6871fecb":"clf = RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4, 5, 6, 7, 8, 9, 10],\n    'criterion' :['gini', 'entropy']\n}\n\ngscv_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 10)\ngscv_clf.fit(X_train, y_train)","5be8be3d":"gscv_clf.best_params_","56eb6321":"clf = RandomForestClassifier(criterion = 'entropy', max_depth = 7, max_features = 'auto', n_estimators = 200)\nprint(cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy'))","03ad1d94":"df.head()\ndf = df.dropna(how = 'any', axis = 0)","fcdb2157":"X = df.drop(['TenYearCHD'], axis = 1)\ny = df['TenYearCHD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)","4b7b9a55":"sfs = SFS(RandomForestClassifier(n_estimators=100, n_jobs=4, random_state=101), \n           k_features=10,\n           forward=True, \n           floating=False,\n           verbose=0,\n           scoring='accuracy',\n           cv=5)\nsfs = sfs.fit(np.array(X_train), y_train)","254ea041":"selected_feat = X_train.columns[list(sfs.k_feature_idx_)]\nselected_feat","1de131cd":"X_train = X_train[selected_feat]\nclf = RandomForestClassifier(random_state=42)\n\nparam_grid = { \n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4, 5, 6, 7, 8, 9, 10],\n    'criterion' :['gini', 'entropy']\n}\n\ngscv_clf = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 10)\ngscv_clf.fit(X_train, y_train)","781b0944":"gscv_clf.best_params_","c2f0ba6f":"clf = RandomForestClassifier(criterion = 'entropy', max_depth = 6, max_features = 'auto', n_estimators = 500)\nprint(cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy'))","1da65f5d":"selected_feat = []\nfor i in vif_data['Feature']:\n    selected_feat.append(i)\nselected_feat","fe80efb6":"X = dfmod.drop(['TenYearCHD'], axis = 1)\ny = dfmod['TenYearCHD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\nselected_feat.remove('TenYearCHD')\nX_train = X_train[selected_feat]\nX_train.head()","8a217cde":"std_scaler = StandardScaler()\nX_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns=X_train.columns)\nX_train","58935702":"clf = LogisticRegression(penalty = 'l1', solver = 'liblinear')\nclf.fit(X_train, y_train)\nX_test = std_scaler.transform(X_test)\npredictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))","7e7aedea":"print(classification_report(y_test, predictions))","4edfe062":"print(confusion_matrix(y_test, predictions))","327c0621":"sm = SMOTE(sampling_strategy='auto', k_neighbors=9, random_state=100)\nX, y = sm.fit_resample(X, y) ","66e37196":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\nX_train = X_train[selected_feat]\nX_train.head()","e4538f94":"std_scaler = StandardScaler()\nX_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns=X_train.columns)\nX_train","514a85ca":"clf = LogisticRegression(penalty = 'l1', solver = 'liblinear')\nclf.fit(X_train, y_train)\nX_test = std_scaler.transform(X_test)\npredictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))","2e8693c3":"print(classification_report(y_test, predictions))","c3ceb284":"print(confusion_matrix(y_test, predictions))","271135a5":"clf = SVC(degree = 2, gamma = 'scale', kernel= 'poly')\nclf.fit(X_train, y_train)\nX_test = std_scaler.transform(X_test)\npredictions = clf.predict(X_test)\nprint(accuracy_score(y_test, predictions))","476f2fa1":"print(classification_report(y_test, predictions))","1ba20ae8":"print(confusion_matrix(y_test, predictions))","a269f43e":"X = dfmod.drop(['TenYearCHD'], axis = 1)\ny = dfmod['TenYearCHD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 101)\nX_train = X_train[selected_feat]\nX_train.head()","c8cab82a":"std_scaler = StandardScaler()\nX_train = pd.DataFrame(std_scaler.fit_transform(X_train), columns=X_train.columns)\nX_train","2b58a9ce":"clf = LogisticRegression(penalty = 'l1', solver = 'liblinear')\nclf.fit(X_train, y_train)","bd36ab43":"from sklearn import metrics\nX_test = std_scaler.transform(X_test)\nprobs = clf.predict_proba(X_test)\nfpr, tpr, thresholds = metrics.roc_curve(y_test, probs[:,1])","53021524":"threshold_chosen = thresholds[np.argmax(tpr - fpr)]\nthreshold_chosen","753cb56b":"preds = np.where(clf.predict_proba(X_test)[:,1] > threshold_chosen, 1, 0)","ed96fd8a":"print(accuracy_score(y_test, preds))\nprint(confusion_matrix(y_test, preds))\nprint(classification_report(y_test, preds))","a7cc3c51":"#### Since all combinations that have been tried are giving an accuracy of 83-84% - Let us try SVM and LR to predict. Because there is a pretty good imbalance in class, there is an expectation that 0s will be predicted accurately while 1s may not. For this we will need to check Precision\/Recall\/F1 Score. In any case, in a later stage I will also try oversampling the data to see the results","966d6366":"#### Looks like all the categorical predictor variables have a p-value < 0.05 and every variable is a predictor in a patient getting a heart disease. ","44a80344":"##### We now have all columns with VIF value < 5 - We will use this for Modeling - Why 5? In general as a thumb rule, I have noticed that for columns with VIF < 5 it tends to work better for a model","a182b4c8":"##### Now lets redo the Logistic Regression Model and change our thresholds. ","9411ec81":"##### From the above it is clear that there is no quasi-constant variance in any of the continuous variable","a4d4f143":"### Using an SVM - Why? - This is a 2 class model - So intuitively I would choose a Logistic Regression or a Support Vector Classifier. Also SVM is less susceptible to Irrelevant features + General High accuracy - While speed of learning is slower, the SVM will take slightly longer","82cf05da":"#### All of the above classifiers are showing accuracy scores of aroun 84% - Can we try another approach to this?","9c833f3a":"### SVM seems to be performing worse. Also because SVM needs a much larger amount of Data, we cannot expect accuracy to be great with a small dataset","7800226d":"#### Checking for multicollinearity between continuous variables","250e0455":"## Observations: \n\n1. Assuming that 0 is female and 1 is male - 55% is Female, 45% is Male\n2. Most of the patients in the database (70%+) is below education level 2 and lower \n3. 96% of the patients are not on BP Medication \n4. 99% have not had a Stroke before \n5. About 69% are not Hypertension patients and 31% are\n6. 97% users are not diabetic \n\n## Question - How are the above percentages split  among someone who has Heart Disease and someone who does not have Heart Disease? Can we form Hypothesis that give us insight into this?","69259b6e":"##### Looking at the data above, we can see that there isnt too much of a difference in the Means and Medians for Heart Disease and No Heart Disease\n\n###### But there are certain subtelties \n\n1. There seems to be a difference in the mean and median of sysBP \n2. There also seems to be a difference in the mean and median of Total Cholestrol \n\n###### To check these, we should conduct an independent t-test","0ee0c246":"# Now lets take a look at out continuous variables","e7ae4cba":"##### For Model - Dropping - diaBP (Correlated with sysBP), Heart Rate (Because of p-value >> 0.05). Other columns stay because they all have p-values < 0.05","5013e089":"##### Overall, we would need a lot more data to be able to model the outcome","e415a489":"## Unscaling for Random Forests","e3a95e31":"# Dropping all the NaN values since only a small number of NaN values present.","6a041e85":"# There is a very slight improvement ","3b0329aa":"#### Scaling the features first because with SVM - the algorithm optimizes by minimizing the errors created by the decision boundary. ","0d33a53b":"## Observations (Hypothesis): \n\n1. Males seem to be slightly susceptible to Heart Disease compared to Females. (Basicaly Gender may play a role)\n2. While it looks like lower education level patients are more susceptible, the overall number of lower education level patients are also much higher \n3. BP Medication, Stroke and Diabetes dont seem to have too much of an impact\n4. Hypertension, however, seems like it has an impact. \n\n##### All of the above will need statistical tests to see if Hypothesis is true\n##### Running a Chi-Square Test on the variables wrt to TenYearCHD","d2ee2ed7":"##### As we can see the 1s are being heavily misclassified. Sampling methods need to be tried to decrease the misclassifications of 1s","57f3bf7f":"##### There seems to be a decently strong correlation between sysBP and diaBP","186f6aef":"#### Let's see if Logistic Regression gives us a better score","4f7014d1":"##### Checking for Multicollinearity using VIF - We start by removing each column with the highest VIF Value till all VIF Values < 5","66b721a7":"#### Figuring out Quasi Constant Features \n\n1. Numerical Features that have low variance give very little discriminatory power to the Target Class\n2. Using Sklearn's Variance Threshold Method, we can check if there are numerical columns with very little Variance \n3. We do this for features with pure Int\/Float Values (Not the Categorical Columns) ","fea2ba2e":"## While the overall accuracy of classifying 1s has improved, overall accuracy has decreased quite badly\n\n##### Let's try the same using SVM"}}