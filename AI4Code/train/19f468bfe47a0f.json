{"cell_type":{"10e80f98":"code","e570f354":"code","d6736af9":"code","7d612b64":"code","300843fd":"code","bba9b445":"code","981c93c1":"code","981a7564":"code","9ab55097":"code","dc66a982":"code","fde5ef8f":"code","c8695160":"code","a7e77814":"code","5be54bf8":"code","c7607053":"code","0d00f457":"code","d43b9db0":"code","9e8e7970":"code","8026eaaa":"code","ac3e00de":"code","00ed837a":"code","4784d8b6":"code","ea9f67ea":"code","3db425c8":"code","271ac131":"code","2066c28e":"code","f77707f0":"code","be88166b":"code","a581dca2":"code","709f8120":"code","9c2d82ae":"code","21e1c6bf":"code","6c13b0a9":"code","48006601":"code","a7549de9":"code","e7ad20fc":"code","4606b6db":"code","66149029":"code","0e3efc9a":"code","06ea6b09":"code","a5220b04":"code","77cb0448":"code","5a353571":"code","c42bc7c9":"code","906b9db4":"code","c1f540d1":"code","547d723f":"code","d7442279":"code","fbcaa62d":"code","062ba012":"code","092bda38":"code","51a482a6":"code","336bdc30":"code","5a21219d":"code","8da6e596":"code","6e778d59":"code","6ce40125":"code","0b76d287":"code","eb1100ab":"markdown","04ed5d75":"markdown","0ed48c7a":"markdown","92cb5e1a":"markdown","2f2af22a":"markdown","62ecca5f":"markdown","93746874":"markdown","d0d35f13":"markdown","0e1bb2e3":"markdown","2e881704":"markdown","f4120c5e":"markdown","ca813209":"markdown"},"source":{"10e80f98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nfrom IPython.display import Image\nimport os\n!ls ..\/input\/","e570f354":"#Importing Libraries\n\n#data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport math\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.offline as py","d6736af9":"data_train = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv',nrows = 200000)\ndata_test = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv',nrows = 200000)","7d612b64":"print(len(data_train));","300843fd":"data_train.head()","bba9b445":"data_test.head()","981c93c1":"data_train.info()","981a7564":"data_train.describe()","9ab55097":"sns.set_style('whitegrid')\nsns.countplot(x='isFraud',data=data_train,palette='RdBu_r')","dc66a982":"statistics_of_data = []\nfor col in data_train.columns:\n  statistics_of_data.append((col,\n                             data_train[col].nunique(),\n                             data_train[col].isnull().sum()*100\/data_train.shape[0],\n                             data_train[col].value_counts(normalize=True, dropna=False).values[0] * 100, \n                             data_train[col].dtype\n                             ))\nstats_df = pd.DataFrame(statistics_of_data, columns=['Feature', 'Uniq_val', 'missing_val', 'val_biggest_cat', 'type'])","fde5ef8f":"stats_df.sort_values('missing_val', ascending=False)","c8695160":"stats_df.sort_values('val_biggest_cat', ascending=False)","a7e77814":"#There are some biased records having more than 98% same value. We consider them to be out of this data set\n\nfor col in data_train.columns:\n    rate = data_train[col].value_counts(normalize=True, dropna=False).values[0]\n    if rate > 0.98:\n        #data_train = data_train.drop(col,inplace=True)\n        print(col)","5be54bf8":"#Dropping all biased columns\n\ncombine = [data_train, data_test]\nprint(\"Before\", data_train.shape, data_test.shape,combine[0].shape, combine[1].shape)","c7607053":"data_train = data_train.drop(['C3','V107','V108','V109','V110','V111','V112','V113','V114','V116','V117','V118','V119','V120','V121','V122','V305'],axis =1)\n","0d00f457":"data_test = data_test.drop(['C3','V107','V108','V109','V110','V111','V112','V113','V114','V116','V117','V118','V119','V120','V121','V122','V305'],axis =1)","d43b9db0":"combine = [data_train, data_test]\nprint(\"After\", data_train.shape, data_test.shape,combine[0].shape, combine[1].shape)","9e8e7970":"def exploreFeatures(col):\n  top_n=10\n  top_n = top_n if data_train[col].nunique() > top_n else data_train[col].nunique()\n  print(\"col has {0} unique values and type {1}:\".format(data_train[col].nunique(),data_train[col].dtype))\n  print(data_train[col].value_counts(normalize=True, dropna=False).head(10))\n    ","8026eaaa":"exploreFeatures('ProductCD')","ac3e00de":"exploreFeatures('card4')","00ed837a":"exploreFeatures('card6')","4784d8b6":"exploreFeatures('P_emaildomain')","ea9f67ea":"exploreFeatures('R_emaildomain')","3db425c8":"exploreFeatures('M1')","271ac131":"exploreFeatures('M2')","2066c28e":"exploreFeatures('M3')","f77707f0":"exploreFeatures('M4')","be88166b":"exploreFeatures('M5')","a581dca2":"exploreFeatures('M6')","709f8120":"exploreFeatures('M7')","9c2d82ae":"exploreFeatures('M8')","21e1c6bf":"exploreFeatures('M9')","6c13b0a9":"combine = [data_train,data_test]\nfreq_feature = data_train.M1.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature =1\n    row['M1'] = row['M1'].map(titlemapping)\n    row['M1'] = row['M1'].fillna(freq_feature)","48006601":"combine = [data_train,data_test]\nfreq_feature = data_train.M2.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 1\n    row['M2'] = row['M2'].map(titlemapping)\n    row['M2'] = row['M2'].fillna(freq_feature)","a7549de9":"combine = [data_train,data_test]\nfreq_feature = data_train.M3.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 1\n    row['M3'] = row['M3'].map(titlemapping)\n    row['M3'] = row['M3'].fillna(freq_feature)","e7ad20fc":"\n\ncombine = [data_train,data_test]\nfreq_feature = data_train.M4.dropna().mode()[0]\ntitlemapping = {'M0':3, 'M2':2,'M1':1}\nfor row in combine:\n    freq_feature = 3\n    row['M4'] = row['M4'].map(titlemapping)\n    row['M4'] = row['M4'].fillna(freq_feature)","4606b6db":"combine = [data_train,data_test]\nfreq_feature = data_train.M5.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 0\n    row['M5'] = row['M5'].map(titlemapping)\n    row['M5'] = row['M5'].fillna(freq_feature)","66149029":"combine = [data_train,data_test]\nfreq_feature = data_train.M6.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 0\n    row['M6'] = row['M6'].map(titlemapping)\n    row['M6'] = row['M6'].fillna(freq_feature)","0e3efc9a":"combine = [data_train,data_test]\nfreq_feature = data_train.M7.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature =0\n    row['M7'] = row['M7'].map(titlemapping)\n    row['M7'] = row['M7'].fillna(freq_feature)","06ea6b09":"combine = [data_train,data_test]\nfreq_feature = data_train.M8.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 0\n    row['M8'] = row['M8'].map(titlemapping)\n    row['M8'] = row['M8'].fillna(freq_feature)","a5220b04":"combine = [data_train,data_test]\nfreq_feature = data_train.M9.dropna().mode()[0]\ntitlemapping = {'F':0, 'T':1}\nfor row in combine:\n    freq_feature = 1\n    row['M9'] = row['M9'].map(titlemapping)\n    row['M9'] = row['M9'].fillna(freq_feature)","77cb0448":"data_train['ProductCD'] = data_train['ProductCD'].astype('category')\ndata_test['ProductCD'] = data_test['ProductCD'].astype('category')\n\ndata_train['card4'] = data_train['card4'].astype('category')\ndata_test['card4'] = data_test['card4'].astype('category')\n\ndata_train['card6'] = data_train['card6'].astype('category')\ndata_test['card6'] = data_test['card6'].astype('category')\n\ndata_train['P_emaildomain'] = data_train['P_emaildomain'].astype('category')\ndata_test['P_emaildomain'] = data_test['P_emaildomain'].astype('category')\n\ndata_train['R_emaildomain'] = data_train['R_emaildomain'].astype('category')\ndata_test['R_emaildomain'] = data_test['R_emaildomain'].astype('category')\n","5a353571":"y = data_train['isFraud']\nX = data_train.drop(['isFraud'], axis=1)\n","c42bc7c9":"binary_variables = [c for c in X.columns if X[c].nunique() == 2]\nbinary_variables","906b9db4":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnewdf = X.select_dtypes(include=numerics)","c1f540d1":"trueNumericCol = [c for c in newdf.columns \n                       if (c not in binary_variables)]\ntrueNumericCol","547d723f":"categorical_columns = [c for c in X.columns \n                       if (c not in trueNumericCol) & (c not in binary_variables)]\ncategorical_columns\n","d7442279":"#NOW, We Will Start Frequency encoding:\n\n#Frequency Encoding\n\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook\n#from sklearn.preprocessing import LabelEncoder\ndef frequency_encoding(variable):\n    t = pd.concat([data_train[variable], data_test[variable]]).value_counts().reset_index()\n    t = t.reset_index()\n    t.loc[t[variable] == 1, 'level_0'] = np.nan\n    t.set_index('index', inplace=True)\n    max_label = t['level_0'].max() + 1\n    t.fillna(max_label, inplace=True)\n    return t.to_dict()['level_0']","fbcaa62d":"for variable in tqdm(categorical_columns):\n  freq_enc_dict = frequency_encoding(variable)\n  data_train[variable] = data_train[variable].map(lambda x: freq_enc_dict.get(x, np.nan))\n  data_test[variable] = data_test[variable].map(lambda x: freq_enc_dict.get(x, np.nan))","062ba012":"#Initalizing the parameters first\n\nparam = {'num_leaves': 60,\n         'min_data_in_leaf': 60, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.1,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1}","092bda38":"import gc\nmax_iter = 5\ngc.collect()","51a482a6":"from sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(X))\ncategorical_columns = [c for c in categorical_columns]\nfeatures = [c for c in X.columns]\npredictions = np.zeros(len(data_test))","336bdc30":"import time\nimport datetime\nstart = time.time()\nfeature_importance_df = pd.DataFrame()\nstart_time= time.time()","5a21219d":"score = [0 for _ in range(folds.n_splits)]","8da6e596":"from sklearn import metrics\nimport lightgbm as lgb","6e778d59":"for fold_, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(X.iloc[trn_idx][features],\n                           label=y.iloc[trn_idx],\n                           categorical_feature = categorical_columns\n                          )\n    val_data = lgb.Dataset(X.iloc[val_idx][features],\n                           label=y.iloc[val_idx],\n                           categorical_feature = categorical_columns\n                          )\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(X.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    # we perform predictions by chunks\n    initial_idx = 0\n    chunk_size = 100000\n    current_pred = np.zeros(len(data_test))\n    while initial_idx < data_test.shape[0]:\n        final_idx = min(initial_idx + chunk_size, data_test.shape[0])\n        idx = range(initial_idx, final_idx)\n        current_pred[idx] = clf.predict(data_test.iloc[idx][features], num_iteration=clf.best_iteration)\n        initial_idx = final_idx\n    predictions += current_pred \/ min(folds.n_splits, max_iter)\n    print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n    score[fold_] = metrics.roc_auc_score(y.iloc[val_idx], oof[val_idx])\n    if fold_ == max_iter - 1: break\n\nif (folds.n_splits == max_iter):\n    print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(y, oof)))\nelse:\n     print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))","6ce40125":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,50))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","0b76d287":"sub_df = pd.DataFrame({\"TransactionID\": data_test[\"TransactionID\"].values})\nsub_df['isFraud'] = predictions\n\nsub_df","eb1100ab":"**Submission:**","04ed5d75":"Non fraudlent transaction are high as obvious, These type of datasets is a challenge to cater.","0ed48c7a":"**Some Important Observations are:**\n\nMissing Values are starting with 93 %.\n\nThere are columns which do have above 99% same values, We need to remove them lately otherwise these columns will bias the model. **\n","92cb5e1a":"**Now we need to do Exploratory data analysis to find out which feature is more important for TARGET variable.**","2f2af22a":"**First of all we will explore our categorical Features:**","62ecca5f":"**FEATURE ENGINEERING**","93746874":"**Credit\/Debit card frauds** are now inclining all over the World. Before the creation of this notebook I was curious that how can any credit\/debit card can be hacked? But then on other thought it seems that this competition is to prevent the money of those bank consumers who lost their card.\n\nAfter some research, the things got clearer to me and I have gain a perspective about how these transaction works in a banking industry, and after that I have no doubts about how easily a card can be hacked. \n\n\nData lies in magnetic strips of a card is vulnerable because the data is unchanged. By using fake terminals and fake pin pads, it is very easy to steal data from card and use for fraudulent transactions.\n\n![image.png](attachment:image.png)\n\n\nSo to overcome this problem banking industry uses chip card or EMV authentication with pin which is an expensive option because you need PTI\/PTS approved terminals\/machines for transactions. Important and different feature of EMV cards or EMV transactions is that the data always changes for each transaction. Thus, no one can steal cards data. But what if a bank\u2019s consumer loses his card?\n\nFor this I come up with a new idea of card-less transactions through which there is no fear of card stolen\/misuse. The diagram is as follows:","d0d35f13":"Now, Transforming these categorical features into binary and order format","0e1bb2e3":"**Now, Lets start with Light Gradient Boosting Implementation**","2e881704":"Conclusion:\n\nSo This is the end of EDA, FE and lgb of credit card fraud detection notebook, I have enjoyed and learned a lot while creating this notebook. A note book which helped me alot is the masterpiece by \"FabienDaniel\", His notebook's link is: https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm.\n\nI have worked hard for this notebook, Hope this will be helpful for you. Up vote this notebook if you like it. Thanks.\n\n","f4120c5e":"![image.png](attachment:image.png)\n\n\nIn any case if anyone leads to do a fraudulent transaction, she could have caught through Machine learning algorithms too. So let\u2019s start with this kernel. \n\n","ca813209":"Now Transforming categorical column's datatypes into 'category', intiall they were in '0'"}}