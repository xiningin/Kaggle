{"cell_type":{"b4f9b2b9":"code","8220f8bd":"code","38daf70e":"code","0309a670":"code","769112b8":"code","24c69307":"code","f2ac4d72":"code","e733f305":"code","0514945e":"code","3280a173":"code","8f80316a":"code","8ba56696":"code","f047f4ea":"code","59e96dda":"code","1c5f6ca7":"code","66f9581b":"code","9bb3db88":"code","1dcfe004":"code","68074127":"code","acb9224d":"code","d247121d":"code","48b8ad91":"code","ff020a6d":"code","9cc206e7":"code","1cf58530":"code","f2d31140":"code","005b1ebb":"code","a77358b8":"code","843aad34":"code","c7708364":"code","4c46bcfc":"code","296aec81":"code","aacb6806":"code","5c792a0f":"code","c8d4490d":"code","3ec73e3f":"code","d771c255":"code","f0f614bd":"code","9c2eb6d8":"code","53aa2935":"code","4da18f93":"code","030911a2":"code","fbfcbde8":"code","8e7ce6be":"code","06f4436c":"code","e12bc780":"code","433c0409":"code","578c4c54":"code","0b2c158e":"code","a4d60659":"code","175cb633":"code","84706d78":"code","ea97b8e1":"code","e004c245":"code","f744f8a4":"code","d6bb265e":"code","8b3a8b9f":"code","dd9f1768":"code","d5eba792":"code","35050095":"code","8390a658":"code","499831c4":"code","0d15199a":"code","c0c5f5b7":"code","1de55a28":"code","d1289a03":"code","d78500ba":"code","13dd3a7c":"code","5be0b0f1":"code","de647b00":"code","ab3b2343":"code","599260ac":"code","918aa101":"code","30d446d6":"code","a612b757":"code","52806466":"code","a09c53ae":"code","d5a8db1a":"code","dcf4f384":"code","28b0d1a9":"code","8566d918":"code","0cb216ae":"code","6b286528":"code","6129f61b":"code","a8f7f01b":"code","b9d608a4":"code","36cf4898":"code","8c6a9d22":"code","39a34b32":"code","952a57ac":"code","caba107c":"markdown","4e6a990e":"markdown","33ca1050":"markdown","ac4918b2":"markdown","74ff89de":"markdown","e3b55fc2":"markdown","7e42b9d5":"markdown","8b8b8524":"markdown","71fbfab6":"markdown","25aece13":"markdown","86484eba":"markdown","7796f1de":"markdown","40106958":"markdown","524a3364":"markdown","9f349de8":"markdown","c0872532":"markdown","03846e91":"markdown","6e427f23":"markdown","792679b5":"markdown","ab03aaeb":"markdown","1231c01f":"markdown"},"source":{"b4f9b2b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8220f8bd":"#importing the nessessary libs\n%matplotlib inline\nimport numpy as np \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\n#Displaying all the columns of the data set\npd.pandas.set_option('display.max_columns',None)\npd.pandas.set_option('display.max_rows',100)","38daf70e":"#Reading in the data\ntrain_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n#Printing the shape of the columns \nprint(train_df.shape)","0309a670":"train_df.head()","769112b8":"print(train_df.columns)","24c69307":"#Visual plot of the null features \nplt.figure(figsize=(10,6))\nsns.heatmap(train_df.isnull(),cmap='Greens')","f2ac4d72":"#This will give us a more detailed look on the data columns with the more null values as a percentage value\n# making a list of all the cols which have a null value \nna_features=[x for x in train_df.columns if train_df[x].isnull().sum()>1]\n#printing the percentages\nfor i in na_features:\n    print(i,np.round(train_df[i].isnull().mean(),4),' % missing values')","e733f305":"for feature in na_features:\n    data = train_df.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # let's calculate the mean SalePrice where the information is missing or present\n    plt.style.use('seaborn')\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","0514945e":"#Some of the features that we dont need is Id\nprint(f'id of houses {len(train_df.Id)}')","3280a173":"#list of all the numarical variables\nnum_feature=[x for x in train_df.columns if train_df[x].dtypes!='O']\nprint('Num of Numerical Variables',len(num_feature))\ntrain_df[num_feature].head()","8f80316a":"yr_features=[x for x in num_feature if 'Year' in x or 'Yr' in x]\n","8ba56696":"yr_features","f047f4ea":"for i in yr_features:\n    print(i,train_df[i].unique())","59e96dda":"# plotting the relation between year of selling and selling price\nplt.plot(train_df.groupby('YrSold')['SalePrice'].median())\nplt.xticks(ticks=[2006,2007,2008,2009,2010])\nplt.title('Year Sold VS Selling price')\nplt.show()","1c5f6ca7":"# Finding the Correlationbwtween price of house with the diffence in the years values\nfor i in yr_features:\n    if i != 'YrSold':\n        data=train_df.copy()\n        data[i]=data['YrSold']-data[i]\n        plt.scatter(data[i],data['SalePrice'])\n        plt.xlabel(i)\n        plt.ylabel('Sales Price')\n        plt.show()","66f9581b":"discrete_feature=[x for x in num_feature if len(train_df[x].unique())<25 and x not in yr_features and x != 'Id' ]\nprint('Number of Discrete Fetures',len(discrete_feature))","9bb3db88":"discrete_feature","1dcfe004":"#plotting the relationship between the Discrete Fetures and the Sales Price\nfor x in discrete_feature:\n    data=train_df.copy()\n    data.groupby(x)['SalePrice'].median().plot.bar()\n    plt.xlabel(x)\n    plt.ylabel('Sales Price')\n    plt.show()","68074127":"cont_features=[x for x in num_feature if x not in discrete_feature and x not in yr_features and x not in 'Id']\nlen(cont_features)","acb9224d":"cont_features","d247121d":"# plotting the relationship between sales price and the diff cont features\n\nfor i in cont_features:\n    data=train_df.copy()\n    data[i].hist(bins=25)\n    plt.xlabel(i)\n    plt.ylabel('Sales Price')\n    plt.title(i)\n    plt.show()","48b8ad91":"#log normalization to the data\nfor feature in cont_features:\n    data=train_df.copy()\n    if 0 in data[feature].unique():\n        pass\n    if True:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('Sales Price')\n        plt.title(feature)\n        plt.show()","ff020a6d":"for feature in cont_features:\n    data=train_df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","9cc206e7":"cat_features=[x for x in train_df.columns if train_df[x].dtypes=='O']\nlen(cat_features)","1cf58530":"cat_features","f2d31140":"train_df[cat_features].head()","005b1ebb":"for x in cat_features:\n    print(f'The feature is {x} and the number of categories are {len(train_df[x].unique())}')","a77358b8":"for x in cat_features:\n    data=train_df.copy()\n    data.groupby(x)['SalePrice'].median().plot.bar()\n    plt.xlabel(x)\n    plt.ylabel('Sale Price')\n    plt.title(x)\n    plt.show()","843aad34":"train_df.head()","c7708364":"#This will give us a more detailed look on the data columns with the more null values as a percentage value\n# making a list of all the categorrical features which have a null value \nna_features_cat=[x for x in train_df.columns if train_df[x].isnull().sum()>1 and train_df[x].dtypes=='O']\n#printing the percentages\nfor i in na_features:\n    print(i,'=',np.round(train_df[i].isnull().mean(),4),'% missing values')","4c46bcfc":"#Replace Missing Vales with a New label\ndef replace_na_cat_feature(df,features_nan):\n    data=df.copy()\n    data[features_nan]=data[features_nan].fillna('Missing')\n    return data\ntrain_df=replace_na_cat_feature(train_df,na_features_cat)\ntrain_df[na_features].isnull().sum()","296aec81":"#This will give us a more detailed look on the data columns with the more null values as a percentage value\n# making a list of all the numerical features which have a null value \nna_features_num=[x for x in train_df.columns if train_df[x].isnull().sum()>1 and train_df[x].dtypes!='O']\n#printing the percentages\nfor i in na_features_num:\n    print(i,'=',np.round(train_df[i].isnull().mean(),4),'% missing values')","aacb6806":"#Replaceing the numerical missing values with median of the value set \nfor x in na_features_num:\n    # replace with median\n    median_value=train_df[x].median()\n    #create a new feature to capture nan values\n    train_df[x+'nan']=np.where(train_df[x].isnull(),1,0)\n    train_df[x].fillna(median_value,inplace=True)","5c792a0f":"train_df[na_features_num].isnull().sum()","c8d4490d":"train_df.YearRemodAdd","3ec73e3f":"# Converting the data time variables(Date-Time)\nfor x in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    train_df[x]=train_df['YrSold']-train_df[x]\ntrain_df[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","d771c255":"# converting skewed fatures into a log normal distribution\nimport numpy as np \nnum_features=['LotFrontage','LotArea','1stFlrSF','GrLivArea','SalePrice']\nfor x in num_features:\n    train_df[x]=np.log(train_df[x])","f0f614bd":"train_df.head()","9c2eb6d8":"categorical_features=[x for x in train_df.columns if train_df[x].dtype=='O']\ncategorical_features","53aa2935":"for x in categorical_features:\n    temp=train_df.groupby(x)['SalePrice'].count()\/len(train_df)\n    temp_df=temp[temp>0.01].index\n    train_df[x]=np.where(train_df[x].isin(temp_df),train_df[x],'Rare_var')","4da18f93":"train_df.head()","030911a2":"for feature in categorical_features:\n    labels_ordered=train_df.groupby([feature])['SalePrice'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    train_df[feature]=train_df[feature].map(labels_ordered)","fbfcbde8":"train_df.head(10)","8e7ce6be":"features_scale=[x for x in train_df.columns if x not in ['Id','SalePrice']]\n#converts the numerical data to a 0 to 1 range\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(train_df[features_scale])","06f4436c":"#an array is returned form the scaler which needs to be converted into a data frame\nscaler.transform(train_df[features_scale])","e12bc780":"data=pd.concat([train_df[['Id','SalePrice']].reset_index(drop=True),\n                pd.DataFrame(scaler.transform(train_df[features_scale]),\n                columns=features_scale)],\n                axis=1)","433c0409":"data.head()","578c4c54":"data.to_csv('X_train.csv',index=False)","0b2c158e":"test_df","a4d60659":"na_features_cat_test=[x for x in test_df.columns if test_df[x].isnull().sum()>1 and test_df[x].dtypes=='O']\n#printing the percentages\nfor i in na_features:\n    print(i,'=',np.round(test_df[i].isnull().mean(),4),'% missing values')","175cb633":"#Replace Missing Vales with a New label\ntest_df=replace_na_cat_feature(test_df,na_features_cat_test)\ntest_df[na_features_cat_test].isnull().sum()","84706d78":"na_features_num_test=[x for x in test_df.columns if test_df[x].isnull().sum()>1 and test_df[x].dtypes!='O']\n#printing the percentages\nfor i in na_features_num:\n    print(i,'=',np.round(test_df[i].isnull().mean(),4),'% missing values')","ea97b8e1":"for x in na_features_num_test:\n    # replace with median\n    median_value=test_df[x].median()\n    #create a new feature to capture nan values\n    test_df[x+'nan']=np.where(test_df[x].isnull(),1,0)\n    test_df[x].fillna(median_value,inplace=True)","e004c245":"test_df[na_features_num_test].isnull().sum()","f744f8a4":"for x in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n    test_df[x]=test_df['YrSold']-test_df[x]\ntest_df[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","d6bb265e":"# converting skewed fatures into a log normal distribution\nimport numpy as np \nnum_features=['LotFrontage','LotArea','1stFlrSF','GrLivArea']\nfor x in num_features:\n    test_df[x]=np.log(test_df[x])# converting skewed fatures into a log normal distribution\nimport numpy as np \nnum_features=['LotFrontage','LotArea','1stFlrSF','GrLivArea']\nfor x in num_features:\n    test_df[x]=np.log(test_df[x])","8b3a8b9f":"test_df.head()","dd9f1768":"categorical_features=[x for x in test_df.columns if test_df[x].dtype=='O']\ncategorical_features","d5eba792":"for x in categorical_features:\n    temp=test_df.groupby(x)['Id'].count()\/len(test_df)\n    temp_df=temp[temp>0.01].index\n    test_df[x]=np.where(test_df[x].isin(temp_df),test_df[x],'Rare_var')","35050095":"test_df.head(10)","8390a658":"for feature in categorical_features:\n    labels_ordered=test_df.groupby([feature])['Id'].mean().sort_values().index\n    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}\n    test_df[feature]=test_df[feature].map(labels_ordered)","499831c4":"features_scale=[x for x in test_df.columns if x not in ['Id','SalePrice']]\n#converts the numerical data to a 0 to 1 range\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(test_df[features_scale])","0d15199a":"#an array is returned form the scaler which needs to be converted into a data frame\nscaler.transform(test_df[features_scale])","c0c5f5b7":"data=pd.concat([test_df[['Id']].reset_index(drop=True),\n                pd.DataFrame(scaler.transform(test_df[features_scale]),\n                columns=features_scale)],\n                axis=1)","1de55a28":"data.head()","d1289a03":"data.to_csv('X_test.csv',index=False)","d78500ba":"X_test=pd.read_csv('X_test.csv')\nX_train=pd.read_csv('X_train.csv')","13dd3a7c":"# for feature selection\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import SelectFromModel","5be0b0f1":"y_train=X_train['SalePrice']","de647b00":"X_train=X_train.drop(['Id','SalePrice'],axis=1)","ab3b2343":"### Apply Feature Selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\nfeature_sel_model.fit(X_train, y_train)","599260ac":"# to check which features are being used for training the model\nfeature_sel_model.get_support()","918aa101":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feat = X_train.columns[(feature_sel_model.get_support())]\n\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\n","30d446d6":"selected_feat","a612b757":"X_train=X_train[selected_feat]","52806466":"X_train","a09c53ae":"X_test=X_test[selected_feat]","d5a8db1a":"#Fitting Random Forest Regression to the dataset \n# import the regressor \nfrom sklearn.ensemble import RandomForestRegressor \n  \n # create regressor object \nrf = RandomForestRegressor(random_state = 42) \n  \n\nrf.get_params()","dcf4f384":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","28b0d1a9":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 10 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","8566d918":"rf_random.best_params_","0cb216ae":"rf = RandomForestRegressor(n_estimators=1400,\n min_samples_split=5,\n min_samples_leaf=1,\n max_features= 'sqrt',\n max_depth=80,\n bootstrap= False)","6b286528":"rf.fit(X_train,y_train)","6129f61b":"X_test=X_test.astype('float32')","a8f7f01b":"X_test.GarageCars.fillna(X_test.GarageCars.mean(),inplace=True)","b9d608a4":"X_test.info()","36cf4898":"test_prediction=np.exp(rf.predict(X_test))","8c6a9d22":"prediction_df=pd.concat([test_df.Id,pd.Series(test_prediction)],axis=1)","39a34b32":"prediction_df.rename(columns={0: \"SalePrice\"},inplace=True)","952a57ac":"prediction_df.to_csv('predictions.csv',index=False)","caba107c":"# Feature Selection","4e6a990e":"## Since There are a lot of missing values we have to check if the data cols with a lot of missing values have some correlation to the selling price\n","33ca1050":"## Finding the number of numerical variables","ac4918b2":"## Numerical Values Feature Enginnering ","74ff89de":"### Converting all the categorical variables to numerical ones ","e3b55fc2":"## Handling Rare Categorical Features\n### we will remove the categorical features that are present in less than 1 percent of the observations ","7e42b9d5":"# finding all the datetime variables","8b8b8524":"## Categorical Variables\n","71fbfab6":"#### Here we can see that some of the features have a high sales price when the number of missing data points is high.\n#### So we tend to replace the missing vlaues with something meaningfull","25aece13":"## <u>Data Analysis Phase<\/u>","86484eba":"## Doing the Same Thing for the test data","7796f1de":"## Checking for Outliers in the data(only for contnuous data)","40106958":"### We will be normalizing the data using the logarithmic transformation","524a3364":"### Finding all the Discrete Numerical Variables","9f349de8":"# <u><b>Feature Enginnering<\/b><\/u>","c0872532":"## finding the relationship between the categorical features and the sales price ","03846e91":"In the steps below we will be performing the following steps in feature engineering\n<ol>\n<li> Dealing with the missing values <\/li>\n<li> Dealing with the temporal variables(Date time)<\/li>\n<li>Categorical variables(feature encoding)<\/li>\n<li>Standardization if the variables to the same range<\/li>\n<\/ol>","6e427f23":"# Building a Machine Learning Pipeline","792679b5":"## Feature Scaling","ab03aaeb":"# Finding a relatonship between the Continuous Features and sales price","1231c01f":"### Part of Data Analysis\n<ol>\n    <li>Finding out the Missing values in the data set<\/li>\n    <li>Finding out more about the numerical values<\/li>\n    <li>Distribution of the numerical Variables<\/li>\n    <li>Categorical Variable<\/li>\n    <li>Cardinality of numerical Variables<\/li>\n    <li>Finding out the outliers of the data<\/li>\n    <li>Finding out more about the correlation between the dependent and the independent features<\/li>\n<\/ol>"}}