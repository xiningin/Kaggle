{"cell_type":{"7742b2d2":"code","4b621dcd":"code","95e9e359":"code","646c0d42":"code","0c229a6b":"code","493a6d94":"code","0c8e5634":"code","50108540":"code","0628b110":"code","7d5edd6d":"code","236a9626":"code","cc14d9f6":"code","274ed61b":"code","62688e9e":"code","23beb82f":"code","5a068a00":"code","b882dcc9":"code","62fcc764":"code","e53b3a26":"code","51b38dba":"code","a8438eaa":"code","e200991b":"code","33f2e55d":"code","6576cb2f":"code","133aedbe":"code","930170a3":"code","1b8ed5a2":"code","fda2b492":"code","3c8524f0":"code","17e55dcd":"code","252730f1":"code","f2a77ec8":"code","50fea6ab":"code","e59fd7f0":"code","8ef2c1ed":"code","351c6d4e":"code","ab471bc3":"code","52427fc2":"code","f3232ead":"code","68daa4b0":"code","148c5b3b":"code","d06c01c3":"code","45b88fa5":"code","3892ce15":"code","07a73558":"code","777fb709":"code","44730e2f":"code","61dfe675":"code","13d44c6f":"code","6931fe4b":"code","25906b14":"code","359dd34e":"code","531b9e04":"code","9193ef41":"code","3205da9d":"code","7f63a5ba":"code","b30157bf":"code","2527c975":"code","0a15aedb":"code","0657aea6":"code","41809826":"code","786d7ede":"code","661e9196":"code","4a32b813":"code","a8c0ec65":"code","e3a1b8c5":"code","65f3c938":"code","e50a7926":"code","c1c17639":"code","a52574e6":"code","e9d18d8f":"markdown","28b759d5":"markdown","afe1a9c7":"markdown","113d991c":"markdown","92b0f6eb":"markdown","e19e51af":"markdown","237187ec":"markdown","8c715a53":"markdown","1fefd9a6":"markdown","3b8dd8f9":"markdown","7c02f8bd":"markdown","78490ebe":"markdown","09b3a4eb":"markdown","3e70f897":"markdown","1d4b05ba":"markdown","c8b3f53e":"markdown","5c3d574e":"markdown"},"source":{"7742b2d2":"from distutils.dir_util import copy_tree\nfromDyrectory = '..\/input'\ntoDyrectory = 'temp'\ncopy_tree(fromDyrectory, toDyrectory)","4b621dcd":"!pip install strsimpy","95e9e359":"!pip install --upgrade --force-reinstall --no-deps albumentations","646c0d42":"!pip install imgaug","0c229a6b":"import torch\nimport pandas as pd\n\n\nfrom PIL import Image\nimport cv2\nimport gc\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Adagrad, SGD, AdamW\nimport torch.nn.functional as F\n\nimport random\n\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\nfrom torchvision import transforms\n\n\nimport torchvision\nimport pickle\nimport json\n\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nfrom itertools import chain\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\n\nfrom string import digits, ascii_uppercase\n\nimport math \nimport utils\n\nfrom strsimpy.levenshtein import Levenshtein\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom PIL import Image\n\nimport imageio\nimport imgaug as ia\nfrom imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\nfrom imgaug import augmenters as iaa","493a6d94":"SEED = 1489\n\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)","0c8e5634":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","50108540":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\n\n#\u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c\nTEST_PATH = \".\/temp\/payment-detection\/test\/test\/\" \nTRAIN_PATH = \".\/temp\/payment-detection\/train\/train\/\"\nSUBMISSION_PATH = \".\/temp\/payment-detection\/submission.csv\"\nTRAIN_INFO = \".\/temp\/payment-detection\/train.csv\"\n\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435\nTHRESHOLD = 0.85\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n# \u0412 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 pytorch \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043e, \u0447\u0442\u043e \u043c\u043e\u0434\u0435\u043b\u044c \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u044f \u0431\u0443\u0434\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 \u043d\u0430 \u0432\u0445\u043e\u0434 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \n# \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u043e\u0442 800*800 \u0434\u043e 1333*1333, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432\u044b\u0431\u0435\u0440\u0435\u043c \u0447\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c \u0438\u0437 \u044d\u0442\u043e\u0433\u043e \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u0430\nIMAGE_WIDTH = 1200\nIMAGE_HEIGHT = 1200\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nN_ITER = 15\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nBATCH_SIZE = 10\n\nLR = 2.5e-4","0628b110":"train  = pd.read_csv(TRAIN_INFO)","7d5edd6d":"train_UY = train[train.label == 'UY']\nboxes_UY = []\nfor i in range(len(train_UY)):\n  row = train_UY.iloc[i]\n  x, y, w, h, image_name = row[1], row[2], row[3], row[4], row[7]\n  img = cv2.imread(TRAIN_PATH + image_name)\n  box = img[y:y + h, x:x + w]\n  box_image = [image_name, box]\n  boxes_UY.append(box_image)","236a9626":"%pylab inline\nfor i in range(len(boxes_UY)):\n  print(i)\n  imgplot = plt.imshow(boxes_UY[i][1])\n  plt.show()","cc14d9f6":"image_to_del = boxes_UY[16][0]\ntrain = train[train.image != image_to_del]","274ed61b":"train_EX = train[train.label == 'EX']\nboxes_EX = []\nfor i in range(len(train_EX)):\n  row = train_EX.iloc[i]\n  x, y, w, h, image_name = row[1], row[2], row[3], row[4], row[7]\n  img = cv2.imread(TRAIN_PATH + image_name)\n  box = img[y:y + h, x:x + w]\n  box_image = [image_name, box]\n  boxes_EX.append(box_image)","62688e9e":"%pylab inline\nfor i in range(len(boxes_EX)):\n  print(i)\n  imgplot = plt.imshow(boxes_EX[i][1])\n  plt.show()","23beb82f":"indexes = [8, 44]\nimage_to_del = boxes_EX[8][0]\ntrain = train[train.image != image_to_del]\nimage_to_del = boxes_EX[44][0]\ntrain = train[train.image != image_to_del]","5a068a00":"train_ST = train[train.label == 'ST']\nboxes_ST = []\nfor i in range(len(train_ST)):\n  row = train_ST.iloc[i]\n  x, y, w, h, image_name = row[1], row[2], row[3], row[4], row[7]\n  img = cv2.imread(TRAIN_PATH + image_name)\n  box = img[y:y + h, x:x + w]\n  box_image = [image_name, box]\n  boxes_ST.append(box_image)","b882dcc9":"%pylab inline\nfor i in range(len(boxes_ST)):\n  print(i)\n  imgplot = plt.imshow(boxes_ST[i][1])\n  plt.show()","62fcc764":"indexes = [30]\nimage_to_del = boxes_ST[30][0]\ntrain = train[train.image != image_to_del]","e53b3a26":"train_PC = train[train.label == 'PC']\nboxes_PC = []\nfor i in range(len(train_PC)):\n  row = train_PC.iloc[i]\n  x, y, w, h, image_name = row[1], row[2], row[3], row[4], row[7]\n  img = cv2.imread(TRAIN_PATH + image_name)\n  box = img[y:y + h, x:x + w]\n  box_image = [image_name, box]\n  boxes_PC.append(box_image)","51b38dba":"%pylab inline\nfor i in range(len(boxes_PC)):\n  print(i)\n  imgplot = plt.imshow(boxes_PC[i][1])\n  plt.show()","a8438eaa":"indexes = [79, 80, 128, 139, 140]\nimage_to_del = boxes_PC[79][0]\ntrain = train[train.image != image_to_del]\nimage_to_del = boxes_PC[80][0]\ntrain = train[train.image != image_to_del]\nimage_to_del = boxes_PC[128][0]\ntrain = train[train.image != image_to_del]\nimage_to_del = boxes_PC[139][0]\ntrain = train[train.image != image_to_del]\nimage_to_del = boxes_PC[140][0]\ntrain = train[train.image != image_to_del]","e200991b":"train.shape","33f2e55d":"boxes_EX = []\nboxes_PC = []\nboxes_ST = []\nboxes_UY = []","6576cb2f":"train_UY = train[train.label == 'UY']\nfor i in range(len(train_UY.image.unique())):\n    image = train_UY.image.unique()[i]\n    image_inf = train[train.image == image]\n    if len(image_inf) > 1:\n      continue\n    labels = []\n    boxes = []\n    for k in range(len(image_inf)):\n      row = image_inf.iloc[k]\n      labels.append(row[0])\n      xmin = row[1]\n      ymin = row[2]\n      xmax = row[1] + row[3]\n      ymax = row[2] + row[4]\n      box = [xmin, ymin, xmax, ymax]\n      boxes.append(box)\n    for angle in [10, 20, 30, 40, 50, 60, 70, -10, -20, -30, -40, -50, -60, -70]:\n      bbs =  BoundingBox(box[0], box[1], box[2], box[3])\n      sample_img = mpimg.imread(TRAIN_PATH + image)\n      image_aug, bbs_aug = iaa.Affine(rotate=angle, fit_output=True, mode = 'edge')(image=sample_img, bounding_boxes=bbs)\n      xmin1 = int(float(bbs_aug.x1))\n      xmax1 = int(float(bbs_aug.x2))\n      ymin1 = int(float(bbs_aug.y1))\n      ymax1 = int(float(bbs_aug.y2))\n      if (xmin1>=0 and xmax1>=0 and ymin>=0 and ymax1>=0):\n        #img_to_save = Image.fromarray(image_aug, 'BGR')\n        image_name = image + '_' + str(angle) + '.jpg'\n        image_save_directory = TRAIN_PATH + image_name\n        #img_to_save.save(image_save_directory)\n        cv2.imwrite(image_save_directory, image_aug)\n        width, height = image_aug.shape[1], image_aug.shape[0]\n        x1new = xmin1\n        y1new = ymin1\n        hnew = ymax1 - ymin1\n        wnew = xmax1 - xmin1\n        x1 = x1new\n        y1 = y1new\n        h = hnew\n        w = wnew\n        d = {'label': labels, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'width' : width, 'height' : height, 'image' : image_name}\n        df = pd.DataFrame(data=d)\n        train = train.append(df, ignore_index = True)","133aedbe":"train_UY = train[train.label == 'PC']\nfor i in range(len(train_UY.image.unique())):\n    image = train_UY.image.unique()[i]\n    image_inf = train[train.image == image]\n    if len(image_inf) > 1:\n      continue\n    labels = []\n    boxes = []\n    for k in range(len(image_inf)):\n      row = image_inf.iloc[k]\n      labels.append(row[0])\n      xmin = row[1]\n      ymin = row[2]\n      xmax = row[1] + row[3]\n      ymax = row[2] + row[4]\n      box = [xmin, ymin, xmax, ymax]\n      boxes.append(box)\n    for angle in [10, 20, 30, 40, -20, -30, -40]:\n      bbs =  BoundingBox(box[0], box[1], box[2], box[3])\n      sample_img = mpimg.imread(TRAIN_PATH + image)\n      image_aug, bbs_aug = iaa.Affine(rotate=angle, fit_output=True, mode = 'edge')(image=sample_img, bounding_boxes=bbs)\n      xmin1 = int(float(bbs_aug.x1))\n      xmax1 = int(float(bbs_aug.x2))\n      ymin1 = int(float(bbs_aug.y1))\n      ymax1 = int(float(bbs_aug.y2))\n      if (xmin1>=0 and xmax1>=0 and ymin>=0 and ymax1>=0):\n        #img_to_save = Image.fromarray(image_aug, 'BGR')\n        image_name = image + '_' + str(angle) + '.jpg'\n        image_save_directory = TRAIN_PATH + image_name\n        #img_to_save.save(image_save_directory)\n        cv2.imwrite(image_save_directory, image_aug)\n        width, height = image_aug.shape[1], image_aug.shape[0]\n        x1new = xmin1\n        y1new = ymin1\n        hnew = ymax1 - ymin1\n        wnew = xmax1 - xmin1\n        x1 = x1new\n        y1 = y1new\n        h = hnew\n        w = wnew\n        d = {'label': labels, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'width' : width, 'height' : height, 'image' : image_name}\n        df = pd.DataFrame(data=d)\n        train = train.append(df, ignore_index = True)","930170a3":"train_UY = train[train.label == 'EX']\nfor i in range(len(train_UY.image.unique())):\n    image = train_UY.image.unique()[i]\n    image_inf = train[train.image == image]\n    if len(image_inf) > 1:\n      continue\n    labels = []\n    boxes = []\n    for k in range(len(image_inf)):\n      row = image_inf.iloc[k]\n      labels.append(row[0])\n      xmin = row[1]\n      ymin = row[2]\n      xmax = row[1] + row[3]\n      ymax = row[2] + row[4]\n      box = [xmin, ymin, xmax, ymax]\n      boxes.append(box)\n    for angle in [10, 20, 30, 40, -20, -30, -40]:\n      bbs =  BoundingBox(box[0], box[1], box[2], box[3])\n      sample_img = mpimg.imread(TRAIN_PATH + image)\n      image_aug, bbs_aug = iaa.Affine(rotate=angle, fit_output=True, mode = 'edge')(image=sample_img, bounding_boxes=bbs)\n      xmin1 = int(float(bbs_aug.x1))\n      xmax1 = int(float(bbs_aug.x2))\n      ymin1 = int(float(bbs_aug.y1))\n      ymax1 = int(float(bbs_aug.y2))\n      if (xmin1>=0 and xmax1>=0 and ymin>=0 and ymax1>=0):\n        #img_to_save = Image.fromarray(image_aug, 'BGR')\n        image_name = image + '_' + str(angle) + '.jpg'\n        image_save_directory = TRAIN_PATH + image_name\n        #img_to_save.save(image_save_directory)\n        cv2.imwrite(image_save_directory, image_aug)\n        width, height = image_aug.shape[1], image_aug.shape[0]\n        x1new = xmin1\n        y1new = ymin1\n        hnew = ymax1 - ymin1\n        wnew = xmax1 - xmin1\n        x1 = x1new\n        y1 = y1new\n        h = hnew\n        w = wnew\n        d = {'label': labels, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'width' : width, 'height' : height, 'image' : image_name}\n        df = pd.DataFrame(data=d)\n        train = train.append(df, ignore_index = True)","1b8ed5a2":"train_UY = train[train.label == 'ST']\nfor i in range(len(train_UY.image.unique())):\n    image = train_UY.image.unique()[i]\n    image_inf = train[train.image == image]\n    if len(image_inf) > 1:\n      continue\n    labels = []\n    boxes = []\n    for k in range(len(image_inf)):\n      row = image_inf.iloc[k]\n      labels.append(row[0])\n      xmin = row[1]\n      ymin = row[2]\n      xmax = row[1] + row[3]\n      ymax = row[2] + row[4]\n      box = [xmin, ymin, xmax, ymax]\n      boxes.append(box)\n    for angle in [10, 20, 30, 40, -20, -30, -40]:\n      bbs =  BoundingBox(box[0], box[1], box[2], box[3])\n      sample_img = mpimg.imread(TRAIN_PATH + image)\n      image_aug, bbs_aug = iaa.Affine(rotate=angle, fit_output=True, mode = 'edge')(image=sample_img, bounding_boxes=bbs)\n      xmin1 = int(float(bbs_aug.x1))\n      xmax1 = int(float(bbs_aug.x2))\n      ymin1 = int(float(bbs_aug.y1))\n      ymax1 = int(float(bbs_aug.y2))\n      if (xmin1>=0 and xmax1>=0 and ymin>=0 and ymax1>=0):\n        #img_to_save = Image.fromarray(image_aug, 'BGR')\n        image_name = image + '_' + str(angle) + '.jpg'\n        image_save_directory = TRAIN_PATH + image_name\n        #img_to_save.save(image_save_directory)\n        cv2.imwrite(image_save_directory, image_aug)\n        width, height = image_aug.shape[1], image_aug.shape[0]\n        x1new = xmin1\n        y1new = ymin1\n        hnew = ymax1 - ymin1\n        wnew = xmax1 - xmin1\n        x1 = x1new\n        y1 = y1new\n        h = hnew\n        w = wnew\n        d = {'label': labels, 'x1': x1, 'y1': y1, 'w': w, 'h': h, 'width' : width, 'height' : height, 'image' : image_name}\n        df = pd.DataFrame(data=d)\n        train = train.append(df, ignore_index = True)","fda2b492":"images_array = train.image.unique()\nids = np.arange(start = 1, stop = len(images_array) + 1, step = 1)\nimages_array = images_array.reshape((len(train.image.unique()), 1))\nids = ids.reshape((len(train.image.unique()), 1))\nimage_id_pairs = np.hstack((images_array, ids))","3c8524f0":"id_column = []\nfor i in range(len(train)):\n  row = train.iloc[i]\n  image = row[7]\n  index = np.where(image_id_pairs == image)\n  index = index[0][0]\n  id = image_id_pairs[index, 1]\n  id_column.append(id)\ntrain['id'] = id_column","17e55dcd":"for i in range(len(images_array)):\n  image_name = images_array[i][0]\n  image_df = train[train.image == image_name]\n  num_objects = len(image_df)\n  image_df_array = np.array(image_df)\n  old_width, old_height = image_df_array[0][5], image_df_array[0][6]\n  x_min = []\n  x_max = []\n  y_min = []\n  y_max = []\n  for j in range(num_objects):\n    row = image_df.iloc[j]\n    x1, y1, w, h = row[1], row[2], row[3], row[4]\n    xmin = int(x1 * (IMAGE_WIDTH \/ old_width))\n    xmax = int((x1 + w) * (IMAGE_WIDTH \/ old_width))\n    ymin = int(y1 * (IMAGE_HEIGHT \/ old_height))\n    ymax = int((y1 + h) * (IMAGE_HEIGHT \/ old_height))\n    x_min.append(xmin)\n    x_max.append(xmax)\n    y_min.append(ymin)\n    y_max.append(ymax)\n  \n  image_path = TRAIN_PATH + image_name\n  img = cv2.imread(image_path)\n  img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n  \n  cv2.imwrite(image_path, img) \n  image_df = image_df.assign(width = IMAGE_WIDTH)\n  image_df = image_df.assign(height = IMAGE_HEIGHT)\n  image_df['xmin'] = x_min\n  image_df['xmax'] = x_max\n  image_df['ymin'] = y_min\n  image_df['ymax'] = y_max\n  if (i == 0):\n    new_train = image_df\n  else:\n    new_train = new_train.append(image_df)","252730f1":"new_train = new_train.apply(pd.to_numeric, errors='ignore')","f2a77ec8":"new_train","50fea6ab":"i = 1991\nimage_name = new_train.iloc[i][7]\nimg = cv2.imread(TRAIN_PATH + image_name)\nlabel, xmin, xmax, ymin, ymax = new_train.iloc[i][0], new_train.iloc[i][9], new_train.iloc[i][10], new_train.iloc[i][11], new_train.iloc[i][12]\nprint(label)\nimgplot = plt.imshow(img)\nplt.show()\nimgplot = plt.imshow(img[ymin:ymax, xmin:xmax])\nplt.show()","e59fd7f0":"test = pd.read_csv(SUBMISSION_PATH)","8ef2c1ed":"test.head()","351c6d4e":"train_set = new_train","ab471bc3":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","52427fc2":"A_transform = A.Compose([\n    A.RandomSizedBBoxSafeCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH, erosion_rate=0.0, interpolation=2, p=0.5),                               \n    A.RandomRotate90(p=0.5),\n    A.Blur(p=0.1),\n    A.RandomBrightnessContrast(p=0.2),\n    A.ToGray(p=0.2)    \n], bbox_params=A.BboxParams(format='pascal_voc'))","f3232ead":"class ShapeDataset(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform, totensor = transforms.Compose([ToTensor(), \n                                               transforms.Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n        self.totensor = totensor\n\n\n        self.image_ids = self.data.id.unique()\n\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return self.data.image.unique().shape[0]\n    \n    def __getitem__(self, idx):\n        \n\n        image_id = self.image_ids[idx]\n        bboxes = self.data[self.data['id'] == image_id]\n        bboxes_array = np.array(bboxes)\n        num_objects = len(bboxes)\n        image_name = bboxes_array[0][7]\n\n        img = cv2.imread(TRAIN_PATH + image_name)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        boxes = bboxes[['xmin', 'ymin', 'xmax', 'ymax']].values\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        cards = {\n                'VI':1,\n                'MA':2,\n                'EX':3, \n                'PC':4, \n                'ST':5,\n                'UY':6\n                }\n        labels = bboxes_array[:, 0]\n        #labels = bboxes.label\n        label_list = []\n        boxes_list = boxes.tolist()\n        for i in range(num_objects):\n          label = cards[labels[i]]\n          label_list.append(label)\n          boxes_list[i].append(label)\n\n        if self.transform:\n          augmented = self.transform(image=img, bboxes=boxes_list)\n          image = augmented['image']\n          augmented_bboxes = augmented['bboxes']\n          boxes = np.array(augmented_bboxes).reshape((num_objects, 5))[:, 0:4]\n          label_list = np.array(augmented_bboxes).reshape((num_objects, 5))[:, 4]\n          area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        area = torch.as_tensor(area, dtype=torch.float32)\n        labels = torch.as_tensor(label_list, dtype=torch.int64)\n        iscrowd = torch.zeros((num_objects,), dtype=torch.int64)\n\n\n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.as_tensor(image_id,  dtype=torch.int64)\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        if self.totensor:\n            image = self.totensor(image)\n\n        return image, target\n\n    \nclass ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform = transforms.Compose([ToTensor(), \n                                               transforms.Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n        \n        \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n            \n        return image","68daa4b0":"train_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_set, transform = A_transform)\ntest_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)","148c5b3b":"dataloader_train = DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)","d06c01c3":"it = iter(dataloader_train)","45b88fa5":"first= next(it)","3892ce15":"plt.imshow(first[0][2].permute(1, 2, 0))","07a73558":"box1 = first[1][2]['boxes'][0]\nlabel1 = first[1][2]['labels'][0]\nxmin, ymin, xmax, ymax = int(box1[0]), int(box1[1]), int(box1[2]), int(box1[3])\nplt.imshow(first[0][2].permute(1, 2, 0)[ymin:ymax, xmin:xmax])\nprint(label1)","777fb709":"device = get_device()\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 7\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n\nmodel = model.to(device)","44730e2f":"params = [p for p in model.parameters() if p.requires_grad == True]","61dfe675":"optimizer = optim.Adam(params = params, lr=LR)\nloss_fn = fnn.mse_loss\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=1,\n                                               gamma=0.9)","13d44c6f":"def lev_dist(preds, df):\n    \n    \n    def c_sort(sub_li):\n        \"\"\"\n        \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0431\u043e\u043a\u0441\u043e\u0432 \u043f\u043e \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c. \u0421\u043b\u0435\u0432\u0430 \u043d\u0430\u043f\u0440\u0430\u0432\u043e \u0441\u0432\u0435\u0440\u0445\u0443 \u0432\u043d\u0438\u0437.\n        \"\"\"\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n        \n        \"\"\"\n        \u0414\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430.\n        \"\"\"\n        cards = {\n                1:'VI',\n                2:'MA',\n                3:'EX', \n                4:'PC', \n                5:'ST',\n                6:'UY'\n                    }\n        return cards[number]\n    \n    levenshtein = Levenshtein()     \n            \n    imgs = {}\n    \n    \n    \n    for index, row in df.iterrows():\n        \n        #\u0438\u0434\u0451\u043c \u043f\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        \n        if row['image'] in imgs.keys():    \n            \n            \n            \n            imgs[row['image']].append([row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label'] ])\n            imgs[row['image']] = c_sort(imgs[row['image']])\n\n        else:\n            imgs[row['image']] =  [[row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label']]]\n            \n            \n    labels = {}\n    \n    for i in imgs.keys():\n        \n        #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443\n        \n        labels[i] = \" \".join([j[4] for j in imgs[i]])\n        \n    preds_list = []\n    \n    cnt = 0\n    \n    \n    labels_pred = {}\n    imgs_name = df['image'].tolist()\n\n    for i in preds:\n        for j in i:\n            #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b\n            _temp_boxes = i[j]['boxes'].cpu().detach().numpy().tolist()\n            _temp_label = i[j]['labels'].cpu().detach().numpy().tolist()\n            _temp_confidence = i[j]['scores'].cpu().detach().numpy().tolist()\n               \n            for index, _ in enumerate(_temp_boxes):\n                # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430\n                _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n                _temp_boxes[index].append(_temp_confidence[index])\n                \n                \n            _temp_boxes = c_sort(_temp_boxes)\n\n            # \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u044b\u0448\u0435 THRESHOLD\n            \n            labels_pred[imgs_name[cnt]] = \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD])\n            \n            cnt+=1\n        \n    assert len(imgs) == len(labels_pred)\n    \n    lev_dist = 0\n    \n    for i in imgs.keys():\n        # \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        \n        lev_dist += levenshtein.distance(imgs[i], labels_pred[i])\n    \n    print(\"Levenstein distance {0}\".format(lev_dist\/len(imgs)))\n    \n    return lev_dist\/len(imgs)","6931fe4b":"train_losses = []\ntest_losses = []","25906b14":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            #sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if epoch == 0:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        \n    \n\n    return metric_logger\n\n\ndef _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n   # torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Val:'\n\n    preds = []\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        \n        preds.append(res)\n        \n\n    # accumulate predictions from all images\n    torch.set_num_threads(n_threads)\n    return preds","359dd34e":"for epoch in range(N_ITER):\n    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=10)\n    lr_scheduler.step()","531b9e04":"def get_prediction(dataset,  model):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    \n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = []\n    \n     #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n        \n        labels_pred = []\n        names_pred = []\n        \n    imgs_name = test.image.unique().tolist()\n\n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for ind, i in enumerate(preds):\n\n        _temp_boxes = i[0]['boxes'].cpu().detach().numpy().tolist()\n        _temp_label = i[0]['labels'].cpu().detach().numpy().tolist()\n        _temp_confidence = i[0]['scores'].cpu().detach().numpy().tolist()\n\n        for index, _ in enumerate(_temp_boxes):\n            _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n            _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(imgs_name[ind])\n        labels_pred.append( \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD]))\n             \n    d = {'image': names_pred, 'payment': labels_pred}\n\n    df = pd.DataFrame(data=d)\n        \n        \n    return df","9193ef41":"metric_logger = utils.MetricLogger(delimiter=\"  \")\nheader = \"Test:\"\n\ndataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\npredictions = get_prediction(dataloader_test, model)","3205da9d":"predictions.head()","7f63a5ba":"predictions.to_csv(SUBMISSION_PATH, index=None)","b30157bf":"predictions_empty = predictions[predictions.payment == '']","2527c975":"predictions_empty","0a15aedb":"empty_frac = predictions_empty.iloc[0:0]\nfor i in range(len(predictions_empty)):\n  image_name = predictions_empty.iloc[i][0]\n  img = cv2.imread(TEST_PATH + image_name)\n  img = cv2.resize(img, ((3600, 2400)))\n  img1 = img[0:1200, 0:1200]\n  img2 = img[1200:2400, 0:1200]\n  img3 = img[0:1200, 1200:2400]\n  img4 = img[1200:2400, 1200:2400]\n  img5 = img[0:1200, 2400:3600]\n  img6 = img[1200:2400, 2400:3600]\n  cv2.imwrite(TEST_PATH + '1' + image_name, img1)\n  cv2.imwrite(TEST_PATH + '2' + image_name, img2)\n  cv2.imwrite(TEST_PATH + '3' + image_name, img3)\n  cv2.imwrite(TEST_PATH + '4' + image_name, img4)\n  cv2.imwrite(TEST_PATH + '5' + image_name, img5)\n  cv2.imwrite(TEST_PATH + '6' + image_name, img6)\n\n  img_name_list = []\n  \n  img_name_list.append('1' + image_name)\n  img_name_list.append('2' + image_name)\n  img_name_list.append('3' + image_name)\n  img_name_list.append('4' + image_name)\n  img_name_list.append('5' + image_name)\n  img_name_list.append('6' + image_name)\n\n  d = {'image': img_name_list, 'payment': ''}\n  df = pd.DataFrame(data=d)\n  empty_frac = empty_frac.append(df, ignore_index = True)","0657aea6":"test_data2  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, empty_frac)\ndataloader_test2 = DataLoader(\n    test_data2, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)","41809826":"def get_prediction_empty(dataset,  model):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    \n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = []\n    \n     #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n        \n        labels_pred = []\n        names_pred = []\n        \n    imgs_name = empty_frac.image.unique().tolist()\n\n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for ind, i in enumerate(preds):\n\n        _temp_boxes = i[0]['boxes'].cpu().detach().numpy().tolist()\n        _temp_label = i[0]['labels'].cpu().detach().numpy().tolist()\n        _temp_confidence = i[0]['scores'].cpu().detach().numpy().tolist()\n\n        for index, _ in enumerate(_temp_boxes):\n            _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n            _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(imgs_name[ind])\n        labels_pred.append( \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD]))\n             \n    d = {'image': names_pred, 'payment': labels_pred}\n\n    df = pd.DataFrame(data=d)\n        \n        \n    return df","786d7ede":"THRESHOLD = 0.75  # \u043d\u0435\u043c\u043d\u043e\u0433\u043e \u0443\u043c\u0435\u043d\u044c\u0448\u0438\u043c threshold\npredictions_empty_frac = get_prediction_empty(dataloader_test2, model)","661e9196":"payment_list = []\nfor i in range(len(predictions_empty)):\n  payment = []\n  for j in range(6):\n    if predictions_empty_frac.iloc[6*i+j][1] != '':\n      payment.append(predictions_empty_frac.iloc[6*i+j][1])\n  payment = ' '.join(payment)\n  payment_list.append(payment)\npredictions_empty.payment = payment_list","4a32b813":"predictions_empty","a8c0ec65":"predictions2 = predictions\nfor i in range(len(predictions2)):\n  image = predictions2.iloc[i][0]\n  if predictions2.iloc[i][1] == '':\n    payment = predictions_empty[predictions_empty.image == image].iloc[0][1]\n    predictions2.at[i, 'payment'] = payment","e3a1b8c5":"predictions2","65f3c938":"predictions2.to_csv('submission.csv', index=False)","e50a7926":"predictions2.to_csv('\/kaggle\/working\/predictions.csv',index=False) ","c1c17639":"import os\nimport sys\nimport shutil","a52574e6":"shutil.rmtree('\/kaggle\/working\/temp')","e9d18d8f":"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","28b759d5":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0435\u0434\u043a\u0438\u0435 \u0431\u0430\u043d\u043a\u0438 (\u0432\u0438\u0437\u0443 \u0438 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043e\u0447\u0435\u043d\u044c \u0434\u043e\u043b\u0433\u043e) \u0438 \u0447\u0438\u0441\u0442\u0438\u043c \u0438\u0445 \u043e\u0442 \"\u043c\u0443\u0441\u043e\u0440\u0430\" \u0438 \u043e\u0448\u0438\u0431\u043e\u0447\u043d\u043e \u0440\u0430\u0437\u043c\u0435\u0447\u0435\u043d\u043d\u044b\u0445 \u043a\u0430\u0440\u0442 (\u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443, \u043d\u043e  \u043f\u0440\u0430\u0432\u0438\u043b\u0430\u0445 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043e \u0447\u0442\u043e \u0441\u0430\u043c\u043e\u0441\u0442\u043e\u044f\u0442\u0435\u043b\u044c\u043d\u043e \u0440\u0430\u0437\u043c\u0435\u0447\u0430\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0435\u043b\u044c\u0437\u044f)","afe1a9c7":"\u041f\u0440\u043e\u043a\u0440\u0443\u0447\u0438\u0432\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u0440\u0435\u0434\u043a\u0438\u043c\u0438 \u0431\u0430\u043d\u043a\u0430\u043c\u0438 \u043d\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0443\u0433\u043b\u044b","113d991c":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0447\u0442\u043e-\u043d\u0438\u0431\u0443\u0434\u044c \u043d\u0430\u0439\u0442\u0438 \u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430\u0445, \u0433\u0434\u0435 \u0441\u0435\u0442\u044c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0448\u043b\u0430. \u041e\u0431\u044b\u0447\u043d\u043e \u0442\u0430\u043a\u043e\u0435 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0442\u0430\u043c, \u0433\u0434\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u043e\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u043e \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0432\u0441\u0435\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0442\u044c \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0432 6 \u0440\u0430\u0437, \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u0438\u0445, \u0438 \u043f\u043e\u043f\u044b\u0442\u0430\u0442\u044c\u0441\u044f \u0447\u0442\u043e-\u0442\u043e \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0435\u0449\u0435 \u0440\u0430\u0437.","92b0f6eb":"\u0412\u0438\u0434\u0438\u043c \u043d\u0435\u043a\u0438\u0439 \u0431\u0430\u0433 \u0441 np.sum(generator), \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0432\u044b\u043b\u0435\u0437 \u043f\u0440\u0438 \u0437\u0430\u043b\u0438\u0432\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u043d\u0430 kaggle. \u0418\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c \u0435\u0433\u043e \u044f \u043d\u0435 \u0443\u0441\u043f\u0435\u043b, \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0435 \u0432\u043b\u0438\u044f\u0435\u0442","e19e51af":"\u0421\u043e\u0437\u0434\u0430\u044e \u043d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u0441 \u043d\u043e\u0432\u044b\u043c\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c\u0438 - id \u0438 xmin, xmax, ymin, ymax (\u043f\u043e\u043d\u0430\u0434\u043e\u0431\u044f\u0442\u0441\u044f \u0432 ShapeDataset), \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0440\u0435\u0441\u0430\u0439\u0437\u044f \u0432\u0441\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u0432 1200*1200","237187ec":"\u0421\u043a\u0430\u0447\u0438\u0432\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u0443\u044e \u043d\u0430 imagenet resnet50, \u0443\u0441\u0442\u0430\u043d\u0430\u0432\u043b\u0438\u0432\u0430\u0435\u043c \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 \u0438 lr_scheduler","8c715a53":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u043d\u0430\u0448\u043b\u0430 \u043b\u0438 \u0441\u0435\u0442\u044c \u043b\u0438 \u0447\u0442\u043e-\u0442\u043e","1fefd9a6":"\u041f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0430\u043b\u044c\u0431\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438. ShiftScaleRotate \u044f \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u0438\u0437-\u0437\u0430 \u0442\u043e\u0433\u043e \u0447\u0442\u043e \u043f\u0440\u0438 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u0430\u0445 \u043d\u0430 \u0443\u0433\u043b\u044b \u043d\u0435 \u043a\u0440\u0430\u0442\u043d\u044b\u0435 90 \u0433\u0440\u0430\u0434\u0443\u0441\u0430\u043c, bounding boxes \u0441\u0442\u0430\u043d\u043e\u0432\u044f\u0442\u0441\u044f \u0431\u043e\u043b\u044c\u0448\u0435 \u0432 \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u0445, \u0447\u0442\u043e \u043a\u0430\u043a \u043c\u043d\u0435 \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0445\u043e\u0440\u043e\u0448\u043e \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0441\u0435\u0442\u0438.","3b8dd8f9":"\u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0441\u0435\u0442\u044c, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f warmup - \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0432\u043d\u0430\u0447\u0430\u043b\u0435, \u043a\u043e\u0433\u0434\u0430 \u043c\u043e\u043c\u0435\u043d\u0442\u044b \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043e\u0432 \u0435\u0449\u0435 \u043d\u0435 \"\u0443\u0441\u0442\u0430\u043a\u0430\u043d\u0438\u043b\u0438\u0441\u044c\", \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0441\u0438\u043b\u044c\u043d\u043e \u043d\u0435 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u044c \u0432\u0435\u0441\u0430","7c02f8bd":"\u0412 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u044f \u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u043b\u0441\u044f \u043d\u0430 \u0440\u0435\u0448\u0435\u043d\u0438\u0438 \u0432 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d\u0435, \u043f\u0440\u0438\u0448\u043b\u043e\u0441\u044c \u043c\u043d\u043e\u0433\u043e \u0447\u0442\u043e \u0438\u0441\u043f\u0440\u0430\u0432\u0438\u0442\u044c (\u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0431\u044b\u043b\u0430 \u0432 ShapeDataset) \u0438 \u0434\u043e\u0434\u0435\u043b\u0430\u0442\u044c.\n\n\u041a\u0440\u0430\u0442\u043a\u043e\u0435 \u0440\u0435\u0437\u044e\u043c\u0435:\n\u0422\u043e \u0447\u0442\u043e \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0438 \u0447\u0442\u043e \u0437\u0430\u0448\u043b\u043e:\n    1) \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 (\u0437\u0430 \u0441\u0447\u0435\u0442 \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u043e\u0432) \u043d\u043e\u0432\u044b\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0441 \u0440\u0435\u0434\u043a\u0438\u043c\u0438 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u044b\u043c\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438 (\u0432\u0441\u0435 \u043a\u0440\u043e\u043c\u0435 \u0432\u0438\u0437\u044b \u0438 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434\u0430).\n    2) \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 albumentations \u0434\u043b\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u043e\u0439 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439.\n    3) \u0423\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u0435\u0442\u044c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043d\u0430\u0448\u043b\u0430 \u0438 \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0438\u0445 \u043d\u0430 \u0447\u0430\u0441\u0442\u0438\n    4) \u0427\u0438\u0441\u0442\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043e\u0442 \u043e\u0448\u0438\u0431\u043e\u043a \u0432 \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0435 \u0437\u0430\u043c\u0435\u0442\u043d\u043e\u0433\u043e \u0432\u043b\u0438\u044f\u043d\u0438\u044f \u043d\u0430 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u043d\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u0430\n\u0422\u043e \u0447\u0442\u043e \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0438 \u0447\u0442\u043e \u043d\u0435 \u0437\u0430\u0448\u043b\u043e:\n    1) \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0434\u0435\u043b\u0430\u0442\u044c \u043a\u043e\u043f\u0438\u043f\u0430\u0441\u0442-\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e, \u0442\u043e \u0435\u0441\u0442\u044c \u0431\u0440\u0430\u0442\u044c \u0431\u043e\u043a\u0441\u044b \u0441 \u043f\u043b\u0430\u0442\u0435\u0436\u043d\u044b\u043c\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u0430\u043c\u0438 \u0438\u0437 \u0434\u0440\u0443\u0433\u0438\u0445 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u0438 \u0440\u0430\u043d\u0434\u043e\u043c\u043d\u043e \u0441\u043f\u0430\u043c\u0438\u0442\u044c \u0438\u0445 \u043f\u043e\u0432\u0441\u044e\u0434\u0443. \u042d\u0442\u043e \u043f\u043e\u0432\u044b\u0448\u0430\u043b\u043e \u0441\u043f\u043e\u0441\u043e\u0431\u043d\u043e\u0441\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430\u0445\u043e\u0434\u0438\u0442\u044c \u043e\u0431\u044a\u0435\u043a\u0442, \u043d\u043e \u043e\u043d\u0430 \u043d\u0430\u0447\u0438\u043d\u0430\u043b\u0430 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0442\u0430\u043c, \u0433\u0434\u0435 \u0438\u0445 \u043d\u0435\u0442, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043e\u0442 \u044d\u0442\u043e\u0439 \u0438\u0434\u0435\u0438 \u044f \u0440\u0435\u0448\u0438\u043b \u043e\u0442\u043a\u0430\u0437\u0430\u0442\u044c\u0441\u044f.\n    2) \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0432\u0441\u0435 \u0441\u043b\u043e\u0438 \u043a\u0440\u043e\u043c\u0435 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0442\u043e\u0440\u0430 \u0438 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u0435\u0433\u043e, \u0437\u0430\u0442\u0435\u043c \u0440\u0430\u0437\u0431\u043b\u043e\u043a\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0438\u0435 \u0441\u043b\u043e\u0438 - \u044d\u0444\u0444\u0435\u043a\u0442\u0430 \u043d\u0435 \u0434\u0430\u043b\u043e.\n    3) \u0417\u0430\u043c\u0435\u043d\u0430 \u0430\u0434\u0430\u043c\u0430 \u043d\u0430 \u0434\u0440\u0443\u0433\u043e\u0439 \u043e\u043f\u0442\u0438\u043c\u0438\u0437\u0430\u0442\u043e\u0440 - \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u0439 \u0432 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430\u0445 \u044f \u043d\u0435 \u0443\u0432\u0438\u0434\u0435\u043b.\n    \n    \n\u0412 \u0442\u0430\u043a\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u043a\u043e\u0440 \u0441\u0438\u043b\u044c\u043d\u043e \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0440\u0430\u043d\u0434\u043e\u043c\u0430, \u044d\u0442\u043e, \u043a\u0430\u043a \u044f \u043f\u043e\u043d\u044f\u043b, \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u0438\u0437-\u0437\u0430 albumentations - \u0432\u0441\u0435 \u043e\u043d\u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u0441 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u043d\u043e\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e, \u0438 \u043e\u0442 \u044d\u0442\u043e\u0433\u043e \u043e\u0447\u0435\u043d\u044c \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0432 \u0438\u0442\u043e\u0433\u0435 \u043b\u0435\u0432\u0435\u043d\u0448\u0442\u0435\u0439\u043d \u043d\u0430 \u0442\u0435\u0441\u0442\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e 0.5 +- 0.1.\n\n\u041a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442:\n    1) \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u043e\u043c\u043e\u0449\u043d\u0435\u0435, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 yolov5.\n    2) \u041b\u0443\u0447\u0448\u0435 \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u0442\u044c \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b.\n    3) \u041f\u0440\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u043e\u0442 \u043e\u0448\u0438\u0431\u043e\u0447\u043d\u044b\u0445 \u0432\u0438\u0437 \u0438 \u043c\u0430\u0441\u0442\u0435\u0440\u043a\u0430\u0440\u0434\u043e\u0432 (\u044f \u0447\u0438\u0441\u0442\u0438\u043b \u0442\u043e\u043b\u044c\u043a\u043e \u0432\u0441\u0435 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0435).\n    4) \u0412\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u0435\u0441\u0442\u044c \u043a\u0430\u043a\u0438\u0435-\u0442\u043e \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0430\u044e\u0442 \u044d\u0444\u0444\u0435\u043a\u0442 \u043b\u0443\u0447\u0448\u0435 \u0447\u0435\u043c \u0442\u0435, \u0447\u0442\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u044f.\n    5) \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u0435\u0435 \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u043a \u043d\u0430 \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u0434\u0440\u0443\u0433\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0434\u043b\u044f \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0440\u0430\u0441\u043f\u043e\u0437\u043d\u0430\u0432\u0430\u043d\u0438\u044f \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u043d\u0430 \u0440\u0435\u0437\u043d\u0435\u044250 \u043e\u0442 pytorch \u044d\u0442\u043e\u0433\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u0435\u043b\u044c\u0437\u044f, \u0437\u0434\u0435\u0441\u044c \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0440\u0430\u0437\u043c\u0435\u0440 1333.","78490ebe":"\u0414\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0435 \u0431\u0430\u043d\u043a\u0438 \u043a \u0443\u0436\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u043c\u0441\u044f","09b3a4eb":"\u0421\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0438 \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0435","3e70f897":"\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0438 \u0438\u043c\u043f\u043e\u0440\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","1d4b05ba":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u0430\u043a\u0443\u044e-\u043d\u0438\u0431\u0443\u0434\u044c \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443","c8b3f53e":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c, \u0447\u0442\u043e \u0432\u044b\u0434\u0430\u0435\u0442 DataLoader","5c3d574e":"\u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435"}}