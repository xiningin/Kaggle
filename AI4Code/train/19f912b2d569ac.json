{"cell_type":{"efa32d1f":"code","a0808266":"code","f11be759":"code","bad4c2ac":"code","232a8c9a":"code","109217c5":"code","690bc265":"code","fb269b65":"code","c579ddf7":"code","1f0ebe8f":"code","b3cbda83":"code","a67a170d":"code","19bd929a":"code","8d862f0d":"code","3c2e116b":"code","f62afb6a":"code","de98d2d1":"code","0e76350b":"code","9999c337":"code","bfbb9474":"code","6826b24a":"code","d2a83b4e":"code","1d155813":"code","f21d9238":"code","bca61685":"markdown","66399082":"markdown","896a807c":"markdown","69b02049":"markdown","a96c6d24":"markdown","1b21c0a6":"markdown","769b0cca":"markdown"},"source":{"efa32d1f":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport os \nimport cv2\nimport random \n\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestRegressor # used for prediction \nfrom sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a0808266":"data = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\ntest = pd.read_csv('..\/input\/petfinder-pawpularity-score\/test.csv')\nss = pd.read_csv('..\/input\/petfinder-pawpularity-score\/sample_submission.csv')","f11be759":"data.head()","bad4c2ac":"data.shape","232a8c9a":"X= data[data.columns[1:-1]] # other features \ny= data[\"Pawpularity\"] # Pawpularity","109217c5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)","690bc265":"X_train.shape , X_test.shape","fb269b65":"# create the base model to tune\nrf = RandomForestRegressor()","c579ddf7":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1500, num = 15)]\n\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n \n# Minimum number of samples required to split a node\nmin_samples_split = [5, 10 , 15, 20 , 25]\n\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [5, 10, 15]","1f0ebe8f":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","b3cbda83":"# Random search of parameters, using 5 fold cross validation, \n \nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, # Dictionary with parameters names (str) as keys and distributions or lists of parameters to try\n                               scoring='neg_mean_squared_error', #  to evaluate the performance of the cross-validated model on the test set.\n                               n_iter = 10, \n                               cv = 4, \n                               refit = True, # Refit an estimator using the best found parameters on the whole dataset.\n                               verbose=2, \n                               random_state=42, \n                               n_jobs = -1 # Number of jobs to run in parallel. -1 means using all processors \n                              )\n","a67a170d":"rf_random.fit(X_train,y_train)","19bd929a":"# Best parameters choosen \n\nrf_random.best_params_","8d862f0d":"# Get best score ( neg_mean_squared_error )\n\nrf_random.best_score_ ","3c2e116b":"predictions_X_test = rf_random.predict(X_test)","f62afb6a":"RMSE_model1_RfR = np.sqrt(mean_squared_error(y_test, predictions_X_test))\n\nprint(RMSE_model1_RfR)","de98d2d1":"final_model = RandomForestRegressor(n_estimators = 100,\n                                     min_samples_split = 15,\n                                     min_samples_leaf = 10,\n                                     max_features = 'sqrt',\n                                     max_depth = 5,\n                                     n_jobs = -1 )","0e76350b":"final_model.fit(X,y)","9999c337":"predictions_final = final_model.predict(test[test.columns[1:]])","bfbb9474":"predictions_final","6826b24a":"ss.head()","d2a83b4e":"f_ss = pd.DataFrame()\n\nf_ss[\"Id\"] = test[\"Id\"]\nf_ss[\"Pawpularity\"] = predictions_final","1d155813":"f_ss.head()","f21d9238":"f_ss.to_csv('submission.csv', index=False)","bca61685":"### Problem description: \n\nPetFinder.my uses a basic Cuteness Meter to rank pet photos. It analyzes picture composition and other factors compared to the performance of thousands of pet profiles. \n\nWhile this basic tool is helpful, it's still in an experimental stage and the algorithm could be improved. The participants needs to build an AI model using provided data to help make the tool better.  \n\n**Task** \n\nThe task is to predict engagement with a pet's profile( **Pawpularity** ) based on the photograph for that profile. \n\n**Data** \n\nThe dataset for this competition comprises both images and tabular data(hand-labelled metadata for each photo). \n\nThe train set contains 9912 pet photos \n\nThe test set contains 8 pet photos\n> NOTE: The actual test data comprises about **6800** pet photos similar to the training set photos. \n\n\n####  **Previous Notebook**: [*Understanding the problem & EDA*](!https:\/\/www.kaggle.com\/vivmankar\/understanding-the-problem-eda) ","66399082":"## Imports","896a807c":" \n#### **In this notebook and upcoming notebooks we will analyze four different approaches to the problem.**  \n \n1. With provided tabular data only ( Score : 20.47458 ) \n2. With provided image data only \n3. Image + Tabular data as inputs to an end to end model. \n4. image + Tabular data models ensembled \n","69b02049":"### Retrain the final model on whole data ","a96c6d24":"## RandomForest Regressor","1b21c0a6":"\n### Hyperparameter Tuning\n\nHyperparaymeter tuening using Randomized Search CV\n\nUse the random grid to search for best hyperparameters","769b0cca":"## Data"}}