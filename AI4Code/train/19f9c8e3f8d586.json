{"cell_type":{"27786ec7":"code","b52c0f9b":"code","ec8e82ea":"code","7a887d68":"code","51c65fb5":"code","7e8ba6a1":"code","630120ed":"code","f89272e4":"code","979096e4":"code","973966f7":"code","594b45c0":"code","804adc7b":"code","c2b75031":"code","9d416341":"code","051fb287":"code","5398d48b":"code","4c454f33":"code","7a176340":"code","15a79d0b":"code","e99693c5":"code","5c54bf73":"code","bc66a0f0":"code","098d681a":"code","a4d9954d":"code","59ea16cb":"code","a08ef197":"code","e6698a18":"code","39e96d58":"code","83638b9c":"code","457b6fd5":"code","ce6dfdd1":"code","ecacc57f":"code","1684196f":"code","73f3bcc1":"code","bbb1255d":"code","accf83a0":"code","06cf4f9b":"code","f84b4f66":"code","e24633d4":"code","98e17dcc":"code","8904a134":"code","16f1f556":"code","cb1c7395":"code","1d7aab60":"code","d83056c1":"markdown","2ce7287f":"markdown","e4e5dd54":"markdown"},"source":{"27786ec7":"import pandas as pd","b52c0f9b":"import seaborn as sns\npd.set_option('display.max_rows', 1000)\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\nimport string\n\nseed =2055\n\nplt.style.use('fivethirtyeight')","ec8e82ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a887d68":"#Data Read, Data Visualization,EDA Analysis,Data Pre-Processing,Data Splitting","51c65fb5":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","7e8ba6a1":"def basic_analysis(df1, df2):\n    '''the function compares the average values of  2 dataframes'''\n    b = pd.DataFrame()\n    b['First df_mean'] = round(df1.mean(),2)\n    b['Second df_mean'] = round(df2.mean(),2)\n    c = (b['First df_mean']\/b['Second df_mean'])\n    if [c<=1]:\n        b['Variation, %'] = round((1-((b['First df_mean']\/b['Second df_mean'])))*100)\n    else:\n        b['Variation, %'] = round(((b['First df_mean']\/b['Second df_mean'])-1)*100)\n        \n    b['Influence'] = np.where(abs(b['Variation, %']) <= 9, \"feature's effect on the target is not defined\", \n                              \"feature value affects the target\")\n\n    return b","630120ed":"train.loc[train['PassengerId'] == 631, 'Age'] = 48\n\n# Passengers with wrong number of siblings and parch\ntrain.loc[train['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\ntest.loc[test['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]","f89272e4":"## checking for Survived dependence of Sex feature\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","979096e4":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.7 * IQR                #### increased to 1.7\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows","973966f7":"## distribution and checking for outliers in numeric features\nimport matplotlib.pyplot as plt\nfeatures = train[['Age', 'SibSp', 'Parch', 'Fare']].columns\nfor i in features:\n    sns.boxplot(x=\"Survived\",y=i,data=train)\n    plt.title(i+\" by \"+\"Survived\")\n    plt.show()","594b45c0":"## distribution of cat features\ncat_features = train[['Pclass', 'Sex', 'Embarked']].columns\nfor i in cat_features:\n    sns.barplot(y=\"Survived\",x=i,data=train)\n    plt.title(i+\" by \"+\"Survived\")\n    plt.show()","804adc7b":"## let's concatenate test and train datasets excluding ID and Target features\ndf = pd.concat((train.loc[:,'Pclass':'Embarked'], test.loc[:,'Pclass':'Embarked'])).reset_index(drop=True)","c2b75031":"survived = train.drop(train[train['Survived'] != 1].index)\nnot_survived = train.drop(train[train['Survived'] != 0].index)\nbasic_analysis(survived,not_survived)","9d416341":"#Pclass\ng = sns.kdeplot(train['Pclass'][(train[\"Survived\"] == 0) & (train['Pclass'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Pclass'][(train[\"Survived\"] == 1) & (train['Pclass'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Pclass')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","051fb287":"#Age\ng = sns.kdeplot(train['Age'][(train[\"Survived\"] == 0) & (train['Age'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Age'][(train[\"Survived\"] == 1) & (train['Age'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Age')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","5398d48b":"#Fare\ng = sns.kdeplot(train['Fare'][(train[\"Survived\"] == 0) & (train['Fare'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['Fare'][(train[\"Survived\"] == 1) & (train['Fare'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('Fare')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","4c454f33":"## PPS matrix of correlation of non-linear relations between features\n\nmatrix_pps = pps.matrix(train)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nmatrix_pps = matrix_pps.apply(lambda x: round(x, 2))\n\nsns.heatmap(matrix_pps, vmin=0, vmax=1, cmap=\"icefire\", linewidths=0.75, annot=True)","7a176340":"train.corr()","15a79d0b":"# df feature distribution before features tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()\/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","e99693c5":"## for Age imputation let's check its dependence from Pclass\npd.DataFrame(df.groupby('Pclass')['Age'].describe())","5c54bf73":"# New Title feature\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title'] = df['Title'].map(title_mapping)\ndf['Title'] = df['Title'].fillna(0)\n\n##dropping Name feature\ndf = df.drop(['Name'], axis=1)\n\n# Convert 'Sex' variable to integer form!\ndf[\"Sex\"][df[\"Sex\"] == \"male\"] = 0\ndf[\"Sex\"][df[\"Sex\"] == \"female\"] = 1\ndf[\"Sex\"] = df[\"Sex\"].astype(int)\n\n## Age tuning:\ndf['Age'] = df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\ndf[\"Age\"] = df[\"Age\"].astype(int)\n\ndf['Age_cat'] = pd.qcut(df['Age'],q=[0, .16, .33, .49, .66, .83, 1], labels=False, precision=1)\n\n\n\n# Ticket tuning\ntickets = []\nfor i in list(df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ndf[\"Ticket\"] = tickets\ndf['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\ndf = pd.get_dummies(df, columns= [\"Ticket\"], prefix = \"T\")\n\n## Fare tuning:\ndf['Fare'] = df.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median())) \n\ndf['Zero_Fare'] = df['Fare'].map(lambda x: 1 if x == 0 else (0))\n\n\ndef fare_category(fr): \n    if fr <= 7.91:\n        return 1\n    elif fr <= 14.454 and fr > 7.91:\n        return 2\n    elif fr <= 31 and fr > 14.454:\n        return 3\n    return 4\n\ndf['Fare_cat'] = df['Fare'].apply(fare_category) \n\n\n# Replace missing values with 'U' for Cabin\ndf['Cabin'] = df['Cabin'].fillna('U')\nimport re\n# Extract first letter\ndf['Cabin'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ncabin_category = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\n# Mapping 'Cabin' to group\ndf['Cabin'] = df['Cabin'].map(cabin_category)\n\n\ndf[\"Embarked\"] = df[\"Embarked\"].fillna(\"C\")\ndf[\"Embarked\"][df[\"Embarked\"] == \"S\"] = 1\ndf[\"Embarked\"][df[\"Embarked\"] == \"C\"] = 2\ndf[\"Embarked\"][df[\"Embarked\"] == \"Q\"] = 3\ndf[\"Embarked\"] = df[\"Embarked\"].astype(int)\n\n# New 'familySize' feature & dripping 2 features:\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n\n\ndf['FamilySize_cat'] = df['FamilySize'].map(lambda x: 1 if x == 1 \n                                                            else (2 if 5 > x >= 2 \n                                                                  else (3 if 8 > x >= 5 \n                                                                       else 4 )\n                                                                 ))       \n\ndf['Alone'] = [1 if i == 1 else 0 for i in df['FamilySize']]\n\n\n\n\ndummy_col=['Title', 'Sex',  'Age_cat', 'SibSp', 'Parch', 'Fare_cat', 'Embarked', 'Pclass', 'FamilySize_cat']\ndummy = pd.get_dummies(df[dummy_col], columns=dummy_col, drop_first=False)\ndf = pd.concat([dummy, df], axis = 1)\n\n\ndummy_fare = ['Fare']\ndummy_f = pd.get_dummies(df[dummy_fare], columns=dummy_fare, drop_first=True)\ndf = pd.concat([dummy_f, df], axis = 1)\n\n## some little dance with features\ndf['FareCat_Sex'] = df['Fare_cat']*df['Sex']\ndf['Pcl_Sex'] = df['Pclass']*df['Sex']\ndf['Pcl_Title'] = df['Pclass']*df['Title']\ndf['Age_cat_Sex'] = df['Age_cat']*df['Sex']\ndf['Age_cat_Pclass'] = df['Age_cat']*df['Pclass']\ndf['Title_Sex'] = df['Title']*df['Sex']\n\ndf['SmallF'] = df['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndf['MedF']   = df['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf['LargeF'] = df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\ndf['Senior'] = df['Age'].map(lambda s:1 if s>70 else 0)\n\ndf[\"Fare\"] = df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","bc66a0f0":"df.shape","098d681a":"## adding new featrures\ndef des_stat_feat(df):\n    df = pd.DataFrame(df)\n    dcol= [c for c in df.columns if df[c].nunique()>=10]\n    d_median = df[dcol].median(axis=0)\n    d_mean = df[dcol].mean(axis=0)\n    q1 = df[dcol].apply(np.float32).quantile(0.25)\n    q3 = df[dcol].apply(np.float32).quantile(0.75)\n    \n    #Add mean and median column to data set having more then 3 categories\n    for c in dcol:\n        df[c+str('_median_range')] = (df[c].astype(np.float32).values > d_median[c]).astype(np.int8)\n        df[c+str('_mean_range')] = (df[c].astype(np.float32).values > d_mean[c]).astype(np.int8)\n        df[c+str('_q1')] = (df[c].astype(np.float32).values < q1[c]).astype(np.int8)\n        df[c+str('_q3')] = (df[c].astype(np.float32).values > q3[c]).astype(np.int8)\n    return df\n","a4d9954d":"df.shape","59ea16cb":"# df after tuning\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum()\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(df)","a08ef197":"def reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()\/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()\/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(df)","e6698a18":"df = df.loc[:,~df.columns.duplicated()]","39e96d58":"df.describe()","83638b9c":"df.apply(lambda x: sum(x.isnull()),axis=0)","457b6fd5":"df.corr()","ce6dfdd1":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split","ecacc57f":"le = LabelEncoder()\nfor col in df.select_dtypes('object').columns:\n    df[col] = le.fit_transform(df[col])","1684196f":"#creating matrices for feature selection:\nX_train = df[:train.shape[0]]\nX_test_fin = df[train.shape[0]:]\ny = train.Survived\nX_train['Y'] = y\ndf = X_train\ndf.head(20) ## DF for Model training\n\nX = df.drop('Y', axis=1)\ny = df.Y","73f3bcc1":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=2020)","bbb1255d":"x_train.head()","accf83a0":"y_train","06cf4f9b":"d_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(X_test_fin)\n\nparams = {\n        'objective':'binary:logistic',\n        'max_depth':10,\n        'learning_rate':0.1,\n        'eval_metric':'auc',\n        'min_child_weight':1,\n        'subsample':0.64,\n        'colsample_bytree':0.4,\n        'seed':45,\n        'reg_lambda':2.79,\n        'reg_alpha':0.1,\n        'gamma':0,\n        'scale_pos_weight':1.68,\n#         'n_estimators': 0,\n        'nthread':-1\n}\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nnrounds=12000  \nmodel = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=100, \n                           maximize=True, verbose_eval=10)","f84b4f66":"accuracy = pd.DataFrame()\naccuracy['predict'] = model.predict(d_valid)\naccuracy['predict'] = accuracy['predict'].apply(lambda x: 1 if x>0.6 else 0)\naccuracy_score(y_valid, accuracy['predict'])","e24633d4":"fig,ax = plt.subplots(figsize=(15,20))\nxgb.plot_importance(model,ax=ax,max_num_features=20,height=0.8,color='r')\n# plt.tight_layout()\nplt.show()","98e17dcc":"xgb.to_graphviz(model)","8904a134":"sub = pd.DataFrame()\nsub['PassengerId'] = test['PassengerId']\nsub['Survived'] = model.predict(d_test)\nsub['Survived'] = sub['Survived'].apply(lambda x: 1 if x>0.6 else 0)","16f1f556":" leaks = {\n 897:1,\n 899:1, \n 930:1,\n 932:1,\n 949:1,\n 987:1,\n 995:1,\n 998:1,\n 999:1,\n 1016:1,\n 1047:1,\n 1083:1,\n 1097:1,\n 1099:1,\n 1103:1,\n 1115:1,\n 1118:1,\n 1135:1,\n 1143:1,\n 1152:1, \n 1153:1,\n 1171:1,\n 1182:1,\n 1192:1,\n 1203:1,\n 1233:1,\n 1250:1,\n 1264:1,\n 1286:1,\n 935:0,\n 957:0,\n 972:0,\n 988:0,\n 1004:0,\n 1006:0,\n 1011:0,\n 1105:0,\n 1130:0,\n 1138:0,\n 1173:0,\n 1284:0,\n }","cb1c7395":"\nsub['Survived'] = sub.apply(lambda r: leaks[int(r['PassengerId'])] if int(r['PassengerId']) in leaks else r['Survived'], axis=1)\nsub.to_csv('submission.csv', index=False)\n\nsub.head()","1d7aab60":"#If u like this Notebbok, Upvote this Kernel","d83056c1":"Feature Engineering","2ce7287f":"**Outlier detection**","e4e5dd54":"making feature matrices for modelling"}}