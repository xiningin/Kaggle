{"cell_type":{"a1f6064d":"code","60ebb113":"code","a728e14a":"code","f36be6a8":"code","ccaba667":"code","f0170017":"code","78df33bd":"code","2b583321":"code","c46d303b":"code","3d726387":"code","31515f17":"code","b975a57b":"code","73718fab":"code","fc8f08d0":"code","96037b36":"code","9dad5ec8":"code","79f02dca":"code","2b883731":"code","27410cd1":"code","c4182337":"code","7f883e17":"code","0d158913":"code","d388482e":"code","ae1b63c6":"code","829bf16a":"code","c8456a28":"code","6df54ab1":"code","23e20ab9":"code","5f346b4b":"code","8a15e1c9":"code","0d00f486":"code","c6d983c0":"code","9c0ebb42":"code","f63d53f9":"code","949e5c2a":"code","05470a03":"code","445744c4":"code","7a27299a":"code","a4894b33":"code","90040d8e":"code","588f60f9":"code","80960baa":"code","52a7060e":"code","be529107":"code","8af4f860":"markdown","b7633fd2":"markdown","df843ab0":"markdown","822018d6":"markdown","29dd266f":"markdown","47ea753a":"markdown","32ec6e79":"markdown","61201702":"markdown","1321dabd":"markdown","869eccfd":"markdown","32631e2e":"markdown","598b7e2d":"markdown","7af027a8":"markdown","d5572ee0":"markdown","df2c5762":"markdown","34e7b0ab":"markdown","4c6d8909":"markdown","b9483cf5":"markdown","92177cb4":"markdown","505ec4ff":"markdown","f5ca9afd":"markdown","2d44fea4":"markdown","3136e338":"markdown","bd1eafc8":"markdown","c45f3ccf":"markdown","57a3efae":"markdown","a446875e":"markdown","a6009a4d":"markdown","713ee02a":"markdown","0952e092":"markdown","d6b5f1f0":"markdown","b19e6775":"markdown","b32eb086":"markdown","dcc9590f":"markdown","442c433b":"markdown","f9202136":"markdown","1395798b":"markdown","895b4981":"markdown","5c8eee78":"markdown","442b6cec":"markdown","2d9414b8":"markdown","060068bd":"markdown","346fb9e6":"markdown","58226817":"markdown","e8db33a4":"markdown"},"source":{"a1f6064d":"import os\nimport re\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nimport xgboost as xgb\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nimport statsmodels.api as sm\nimport statsmodels.stats.diagnostic as smd\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom scipy.stats import shapiro, boxcox, kstest, probplot\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials","60ebb113":"random_state=10\n#warnings.filterwarnings(\"error\")\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)","a728e14a":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f36be6a8":"def validate(y_true, y_pred):\n    resid = y_true - y_pred\n    \n    mse = mean_squared_error(y_true, y_pred)\n    r2 = r2_score(y_true, y_pred)\n    print(\"MSE: %s\" % mse)\n    print(\"R^2: %s\" % r2)\n    print(\"Residuals mean: {0}\".format(np.mean(resid)))\n    \n    fig, ax = plt.subplots(figsize=(19,4), ncols=4)\n    ax[0] = sns.scatterplot(x=y_true, y=resid, ax=ax[0])\n    ax[1] = sns.scatterplot(x=y_true, y=y_pred, ax=ax[1])\n    ax[2] = sns.histplot(resid, ax=ax[2])\n    probplot(resid, dist=\"norm\",  plot=ax[3])\n    \n    statistic, p_value = kstest(resid, 'norm')\n    if p_value>0.05:\n        print(\"Distribution is normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))\n    else:\n        print(\"Distribution is not normal. Statistic: {0:.3}, p-value: {1:.4}\".format(statistic, p_value))","ccaba667":"data = pd.read_csv(\"\/kaggle\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv\")\ndisplay(data.head(3))","f0170017":"display(data.info())","78df33bd":"data[\"mileage\"] = data[\"mileage\"].str.replace(\" kmpl\", \"\")\ndata[\"mileage\"] = data[\"mileage\"].str.replace(\" km\/kg\", \"\")\ndata[\"mileage\"] = data[\"mileage\"].astype(float)\n\ndata[\"engine\"] = data[\"engine\"].str.replace(\" CC\", \"\")\ndata[\"engine\"] = data[\"engine\"].astype(float, errors=\"ignore\")\n\ndata[\"max_power\"] = data[\"max_power\"].str.replace(\" bhp\", \"\")\ndata.loc[data[\"max_power\"]=='', \"max_power\"]=np.NaN\ndata[\"max_power\"] = data[\"max_power\"].astype(float, errors=\"ignore\")","2b583321":"remapped = {'First Owner': 1, 'Second Owner': 2, 'Third Owner': 3, 'Fourth & Above Owner': 4, 'Test Drive Car': 0}\ndata = data.replace({\"owner\": remapped})","c46d303b":"max_date = max(data[\"year\"])\ndata[\"year2\"] = data[\"year\"].apply(lambda x: max_date - x)","3d726387":"def torque_parser(x):\n    try:\n        try:\n            parsed = re.findall(r\"([\\d]+).*(nm|kgm)\", x, re.IGNORECASE)[0]\n        except Exception as e:\n            parsed = [re.findall(r\"[\\d]+\", x)[0], \"nm\"]\n        finally:\n            if parsed[1].lower() == \"nm\":\n                torque = float(parsed[0])\n            else:\n                kgm = float(parsed[0])\n                if kgm < 100:\n                    torque = float(parsed[0])\/0.10197\n                else:\n                    torque = float(parsed[0])\n    except Exception as e:\n        torque = np.NaN\n    return torque \n\ndata[\"torque2\"] = data[\"torque\"].apply(torque_parser)","31515f17":"data = data.drop([\"year\", \"torque\"], axis=1)","b975a57b":"display(data.describe())\ndisplay(data.describe(include=object))","73718fab":"ax = sns.pairplot(data)","fc8f08d0":"def brand_parser(x):\n    try:\n        parsed = re.findall(r\"^(\\S*)\\s(\\S*)\", x, re.IGNORECASE)[0]\n    except Exception as e:\n        parsed = [\"unparsed\", \"value\"]\n    finally:\n        return parsed[0] + \" \" + parsed[1]\n    \ndata[\"brand_model\"] = data[\"name\"].apply(brand_parser)","96037b36":"fig, ax = plt.subplots(figsize=(16,3))\n\nvals, cnts = np.unique(data[\"brand_model\"], return_counts=True)\nidxs = np.argsort(-cnts)\n\nmodels = np.random.choice(vals, 40)\ndf = data[data[\"brand_model\"].isin(models)]\nax = sns.boxplot(x=df[\"brand_model\"], y=df[\"selling_price\"], ax=ax)\n\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)","9dad5ec8":"data[\"mileage\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"mileage\"]\ndata[\"engine\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"engine\"]\ndata[\"max_power\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"max_power\"]\ndata[\"seats\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(np.round(x.mean())))[\"seats\"]\ndata[\"torque2\"] = data.groupby(\"brand_model\").transform(lambda x: x.fillna(x.mean()))[\"torque2\"]","79f02dca":"na_count = data.isna().any(axis=1).sum()\nprint(\"Records with NA values: %s\" % na_count)\ndata = data.dropna()","2b883731":"data = data[data[\"mileage\"]>0]\ndata = data[data[\"max_power\"]>0]","27410cd1":"data[\"selling_price2\"] = np.log(data[\"selling_price\"])\ncols = data.columns.tolist()\ncols = cols[-1:] + cols[:-1]\ndata = data[cols]","c4182337":"# km_driver more than 300k km looks like outliars. \u041e\u0442 \u0433\u0440\u0435\u0445\u0430 \u043f\u043e\u0434\u0430\u043b\u044c\u0448\u0435...\ndata = data[data[\"km_driven\"]<300000]","7f883e17":"# mileage more than 35 looks like outliars. \u0422\u0443\u0434\u0430 \u0436\u0435.\ndata = data[data[\"mileage\"]<35]","0d158913":"# \"Maruti Zen D\" torque looks like a mistake. It isn't 789nm, but 78nm. \u0425\u043e\u0442\u0435\u043b \u0431\u044b \u044f 790 \u041d\u043c, \u043d\u043e \u043d\u0435\u0442.\ndata.loc[data[\"name\"]==\"Maruti Zen D\", \"torque2\"] = 78\n# this will make the relationship between torque2 and target variable more linear \ndata[\"torque2\"] = np.log(data[\"torque2\"])","d388482e":"# Fix some mistakes\ndata.loc[data[\"brand_model\"]==\"Honda BRV\", \"brand_model\"] = \"Honda BR-V\"\ndata.loc[data[\"brand_model\"]==\"Ford Ecosport\", \"brand_model\"] = \"Ford EcoSport\"\ndata.loc[data[\"brand_model\"]==\"Ambassador CLASSIC\", \"brand_model\"] = \"Ambassador Classic\"","ae1b63c6":"ax = sns.pairplot(data)","829bf16a":"fig, ax = plt.subplots(figsize=(15,5))\ncorr = data.corr()\nax = sns.heatmap(corr, annot=True, ax=ax, cmap=\"YlGnBu\")","c8456a28":"data = data.drop([\"selling_price\"], axis=1)\ndata_cleared = data.copy()","6df54ab1":"data = data_cleared.copy()","23e20ab9":"data_lr = data.copy()\n\ny = data_lr[\"selling_price2\"]\nX = data_lr.drop([\"name\", \"selling_price2\"], axis=1)\n\nX = pd.get_dummies(X, columns=[\"fuel\", \"seller_type\", \"transmission\", \"owner\", \"seats\", \"brand_model\"])\n\ndisplay(X.shape)\ndisplay(X.head(2))","5f346b4b":"X_ = sm.add_constant(X)\nmodel_ols = sm.OLS(y, X_).fit()\nprint(model_ols.summary())","8a15e1c9":"y_pred = model_ols.predict(X_)\nvalidate(y, y_pred)","0d00f486":"influence = model_ols.get_influence()\n(c, p) = influence.cooks_distance\n    \ndistances = pd.DataFrame(c, index=X.index)\ndistances = distances.fillna(1)","c6d983c0":"n_max = 30\nmax_values = distances.nlargest(n_max, columns=0)[0]\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.set_yscale(\"log\")\nax = sns.barplot(y=max_values, x=np.arange(n_max), ax=ax, palette=\"Blues_r\")","9c0ebb42":"X = X.drop(max_values.index)\ny = y.drop(max_values.index)","f63d53f9":"X_ = sm.add_constant(X)\nmodel_ols2 = sm.OLS(y, X_).fit()\nprint(model_ols2.summary())\n","949e5c2a":"y_pred = model_ols.predict(X_)\nvalidate(y, y_pred)","05470a03":"scaler = StandardScaler()\nX_sc = pd.DataFrame(scaler.fit_transform(X), index=X.index)\n\nX_train, X_test, y_train, y_test = train_test_split(X_sc, y, test_size=0.33, random_state=random_state)","445744c4":"def hyperopt(X, y, params):\n    try:\n        model = Ridge(**params, normalize=False)\n        score = cross_val_score(model, X, y, cv=5, n_jobs=-1, scoring='neg_mean_squared_error')\n        return -score.mean()\n    \n    except Exception as ex :\n        print(ex)\n        return np.inf\n\ndef f_model(params):\n    global best\n    global best_params\n    acc = hyperopt(X_train, y_train, params)\n    if (acc < best):\n        best = acc\n        best_params = params\n        print(\"new best: {0:.7} {1}\".format(best, params))\n    return {'loss': acc, 'status': STATUS_OK}\n\n\ndef model_tune(space, random_state=random_state, iters=10):\n    global best\n    global best_params\n    best, best_params = np.inf, None \n    res = fmin(f_model, space, algo=tpe.suggest, max_evals=iters, rstate=np.random.RandomState(random_state))\n    model = Ridge(random_state=random_state, **best_params, normalize=False)\n    print(\"\\nBest_params: \\n\", best_params)\n    return model\n\n\nspace_l = {\n    'alpha': hp.uniform('alpha', 0.00001, 2),\n    'tol': hp.uniform('tol', 0.000001, 0.5),\n}","7a27299a":"model_reg = model_tune(space_l, iters=30)","a4894b33":"model_reg = model_reg.fit(X_train, y_train)","90040d8e":"y_pred = model_reg.predict(X_test)\nvalidate(y_test, y_pred)","588f60f9":"data_xgb = data.copy()\n\ny = data_xgb[\"selling_price2\"].copy()\nX = data_xgb.drop([\"name\", \"selling_price2\"], axis=1).copy()\n\nX = pd.get_dummies(X, columns=[\"fuel\", \"seller_type\", \"seats\", \"transmission\", \"brand_model\"], drop_first=True)\n\nscaler = MinMaxScaler()\nX = pd.DataFrame(scaler.fit_transform(X), index=X.index)\n\ndisplay(X.head(3))\ndisplay(y.head(3))","80960baa":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","52a7060e":"xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', \n                          colsample_bytree = 0.3, \n                          learning_rate = 0.1, \n                          max_depth = 10, \n                          alpha = 1, \n                          n_estimators = 250)\n\nxg_reg = xg_reg.fit(X_train,y_train)\n","be529107":"y_pred = xg_reg.predict(X_test)\nvalidate(y_test, y_pred)","8af4f860":"#### Other mistakes","b7633fd2":"#### Brand and model","df843ab0":"#### Drop bad values","822018d6":"Let's build a regression model taking into account the identified problems.\n\nI will assume that hetetoscedasticity may be due to the absence of a predictor. For example, the equipment of the car, which affects the cost.","29dd266f":"<a id=\"eda.c\"><\/a>\n## Conclusion \n1. The distribution of the target variable appears to be normal. This does not linear regression assume, but in this case it improves the result.\n2. Removed explicit outliers and corrected data errors\n3. The dependence of predictors with target variable appears to be linear\n4. Correlation matrix does not show strong linear relationship between predictors","47ea753a":"<a id=\"eda.dt2\"><\/a>\n### Data transformation. Stage 2","32ec6e79":"<a id=\"lr.rrc\"><\/a>\n### Conclusion","61201702":"#### Torque  values","1321dabd":"<a id=\"conclusion\"><\/a>\n# Conclusion","869eccfd":"For comparison, let's build a XGBoost model.","32631e2e":"This model explains 94.6% of the variation in the dependent variable, while the MSE was 0.038.\n\nWhen diagnosing the model, 2 problems were identified:\n - Abnormal distribution of residuals\n - Signs of heteroscedasticity\n \nViolating the linear regression assumptions can result in the trained model not being optimal for a given dataset.\nAlso, if the assumption about the random distribution of residuals is violated, we cannot reliably use statistical tests to determine the significance of the predictor.\n\nViolations of linear regression assumptions may be due to outliers, non-linear relationships, or the absence of a predictor.\n\nLooking ahead, I will say that the transformation of predictors did not lead to an increase in the accuracy of the model.\n\nLet's try to identify and remove outliers.","598b7e2d":"<a id=\"xgb\"><\/a>\n# XGBOOST","7af027a8":"- [Imports](#imports)\n- [Read the data](#read)\n- [EDA](#eda)\n  - [Overview](#eda.overview)\n  - [Data transformation. Stage 1](#eda.dt1)\n  - [Let's take a closer look at the data](#eda.closer)\n  - [Data transformation. Stage 2](#eda.dt2)\n  - [Deal with NA](#eda.na)\n  - [Data transformation. Stage 3](#eda.dt3)\n  - [And final pairplot...](#eda.fpp)\n  - [Conclusion](#eda.c)\n- [Linear Regression](#lr)\n  - [Dataset](#lr.ds)\n  - [Regression analysis](#lr.ra)\n  - [Conclusion](#lr.c)\n  - [Ridge regression](#lr.rr)\n  - [Conclusion](#lr.rrc)\n- [XGBoost](#xgb)\n  - [Dataset](#xgb.ds)\n  - [Model](#xgb.m)\n- [Conclusion](#conclusion)","d5572ee0":"The model is built, the previously mentioned problems are observed - the non-normality of the distribution of the residuals and the signs of heteroscedasticity.\n\nThe MSE value is 0.042. But the average of the errors is close to zero.","df2c5762":"We see missing values and some data type mismatches.\n\nThe most important features are filled in completely, so i'll deal with missing data later.","34e7b0ab":"<a id=\"eda.overview\"><\/a>\n### Overview","4c6d8909":"<a id=\"lr.ds\"><\/a>\n## Dataset","b9483cf5":"#### owner","92177cb4":"<a id=\"eda.fpp\"><\/a>\n### And final pairplot...","505ec4ff":"<a id=\"lr.rr\"><\/a>\n## Ridge regression","f5ca9afd":"<a id=\"eda\"><\/a>\n# EDA","2d44fea4":"#### torque","3136e338":"In this solution the following steps were taken\n\n\n1. Data understanding and preparing\n    - Removed outliers and erroneous values\n    - Parsed text values\n    - Filled missing values\n    - Features are transformed\n2. Performing regression analysis\n    - Evaluated the fulfillment of the linear regression assumptions\n    - Removed outliers based on Cook's distance\n3. Fitted a linear regression model. Optimal parameters are configured via HyperOpt.\n4. Fitted a comparative model based on XGBoost.\n\nThe linear regression model showed 2 problems - the residuals are not normally distributed, and heteroscedasticity is also observed.\nI will assume that the reasons lie in the absence of an important predictor. When using such a model, it should be borne in mind that it may not be optimal for the given task\/dataset.\n\nIn addition, I note that the accuracy of linear regression almost coincided with the accuracy of the model based on XGBoost. It seems that it is difficult to achieve a better result on the current data.\n\nThanks for attention!","bd1eafc8":"<a id=\"eda.na\"><\/a>\n### Deal with NA","c45f3ccf":"#### Data types","57a3efae":"Deleting points with great influence allowed to slightly improve performance, but the model has not changed fundamentally.","a446875e":"<a id=\"lr.c\"><\/a>\n### Conclusion","a6009a4d":"# Table of contents","713ee02a":"<a id=\"eda.dt3\"><\/a>\n### Data transformation. Stage 3","0952e092":"#### date","d6b5f1f0":"#### km_driver","b19e6775":"<a id=\"read\"><\/a>\n# Read the data ","b32eb086":"- Not all predictors have a linear relationship with the target variable\n- I assume that the brand and model will affect the value\n- Zero mileage and zero max_power looks bad\n- km_driver more than 300k km looks like outliars.\n- mileage more than 35 looks like outliars.\n- 789nm looks like a bug","dcc9590f":"<a id=\"xgb.m\"><\/a>\n## Model","442c433b":"<img src=\"https:\/\/s1.1zoom.ru\/b5050\/215\/BMW_E46_M3_silver_450821_1366x768.jpg\" alt=\"Drawing\" style=\"width: 900px;\">\n\n","f9202136":"#### Target variable","1395798b":"#### mileage","895b4981":"# Car price prediction","5c8eee78":"<a id=\"lr.ra\"><\/a>\n## Regression analysis","442b6cec":"Fill in the missing values with the average for each brand_model","2d9414b8":"<a id=\"eda.dt1\"><\/a>\n### Data transformation. Stage 1","060068bd":"<a id=\"lr\"><\/a>\n# Linear regression model","346fb9e6":"<a id=\"eda.closer\"><\/a>\n### Let's take a closer look at the data","58226817":"<a id=\"xgb.ds\"><\/a>\n## Dataset","e8db33a4":"<a id=\"imports\"><\/a>\n# Imports"}}