{"cell_type":{"0110ad2d":"code","cc2f1d57":"code","cd9d01fc":"code","b8a9c7ae":"code","48706404":"code","72c058e5":"code","9f9fd5d7":"code","70ad7017":"code","085ca2ec":"code","e60b617b":"code","5dd3344d":"code","88623619":"code","f6e9bdd3":"code","f5000cf8":"code","d608949a":"code","50d9004a":"code","faf48ec2":"code","98569fd1":"markdown","09a32a04":"markdown","df79e864":"markdown","b71dbe76":"markdown","8772428b":"markdown","881a4005":"markdown","29749e4e":"markdown"},"source":{"0110ad2d":"import pandas as pd\nimport tensorflow as tf\nimport numpy as np\nimport pydot\nimport os\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout \nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical\nimport sklearn\nfrom sklearn.metrics import confusion_matrix","cc2f1d57":"train_dir = '..\/input\/tabular-playground-series-may-2021\/train.csv'\ntest_dir = '..\/input\/tabular-playground-series-may-2021\/test.csv'","cd9d01fc":"train_df = pd.read_csv(train_dir)\ntest_df = pd.read_csv(test_dir)\ntest_id = pd.read_csv(test_dir)","b8a9c7ae":"train_df.head()\ntest_df.head()","48706404":"train_df = train_df.drop(['id'], axis=1)","72c058e5":"test_df = test_df.drop(['id'], axis=1)","9f9fd5d7":"train_df_features = train_df.copy()\ntrain_df_labels = train_df_features.pop('target')","70ad7017":"label_dict = {'Class_1': 0,'Class_2': 1, 'Class_3': 2, 'Class_4': 3}\ntrain_df_labels = [label_dict[item] for item in train_df_labels]","085ca2ec":"train_df_labels","e60b617b":"train_df_features = np.array(train_df_features)\ntrain_df_labels = np.array(train_df_labels)","5dd3344d":"train_df_labels = to_categorical(train_df_labels)","88623619":"num_classes = 4\nnum_features = 50","f6e9bdd3":"def dnn_model():\n    input_layer = Input(shape=(num_features,))\n    normalize_layer = tf.keras.layers.experimental.preprocessing.Normalization()(input_layer)\n\n    #Layer 1\n    dense_1 = Dense(1024, activation='relu')(normalize_layer)\n    batch_norm_1 = BatchNormalization()(dense_1)\n\n    dense_2 = Dense(512, activation='relu')(batch_norm_1)\n    batch_norm_2 = BatchNormalization()(dense_2)\n    drop_1 = Dropout(0.1)(batch_norm_2)\n\n    dense_3 = Dense(256, activation='relu')(drop_1)\n    batch_norm_3 = BatchNormalization()(dense_3)\n\n    dense_4 = Dense(128, activation='relu')(batch_norm_3)\n    batch_norm_4 = BatchNormalization()(dense_4)\n    drop_2 = Dropout(0.1)(batch_norm_4)\n\n    #Layer 2\n    dense_1_2 = Dense(1024, activation='relu')(normalize_layer)\n    batch_norm_1_2 = BatchNormalization()(dense_1_2)\n\n    dense_2_2 = Dense(512, activation='relu')(batch_norm_1_2)\n    batch_norm_2_2 = BatchNormalization()(dense_2_2)\n    drop_1_2 = Dropout(0.2)(batch_norm_2_2)\n\n    dense_3_2 = Dense(512, activation='relu')(drop_1_2)\n    batch_norm_3_2 = BatchNormalization()(dense_3_2)\n\n    dense_4_2 = Dense(256, activation='relu')(batch_norm_3_2)\n    batch_norm_4_2 = BatchNormalization()(dense_4_2)\n\n    dense_5_2 = Dense(128, activation='relu')(batch_norm_4_2)\n    batch_norm_5_2 = BatchNormalization()(dense_5_2)\n    drop_2_2 = Dropout(0.2)(batch_norm_5_2)\n\n    #Layer 3\n    dense_1_3 = Dense(1024, activation='relu')(normalize_layer)\n    batch_norm_1_3 = BatchNormalization()(dense_1_3)\n    drop_1_3 = Dropout(0.1)(batch_norm_1_3)\n\n    dense_2_3 = Dense(512, activation='relu')(batch_norm_1_3)\n    batch_norm_2_3 = BatchNormalization()(dense_2_3)\n    drop_2_3 = Dropout(0.1)(batch_norm_2_3)\n\n    dense_3_3 = Dense(256, activation='relu')(drop_1_3)\n    batch_norm_3_3 = BatchNormalization()(dense_3_3)\n    drop_3_3 = Dropout(0.1)(batch_norm_3_3)\n\n    dense_4_3 = Dense(256, activation='relu')(batch_norm_3_3)\n    batch_norm_4_3 = BatchNormalization()(dense_4_3)\n    drop_4_3 = Dropout(0.1)(batch_norm_4_3)\n\n    dense_5_3 = Dense(128, activation='relu')(batch_norm_4_3)\n    batch_norm_5_3 = BatchNormalization()(dense_5_3)\n    drop_5_3 = Dropout(0.1)(batch_norm_5_3)\n\n    #Concat\n    concat = layers.concatenate([drop_2, drop_2_2, drop_5_3, normalize_layer])\n\n    #Final Layer\n    dense_f1 = Dense(512, activation='relu')(concat)\n    batch_norm_f1 = BatchNormalization()(dense_f1)\n    drop_f1 = Dropout(0.1)(batch_norm_f1)\n    \n    dense_f2 = Dense(256, activation='relu')(drop_f1)\n    batch_norm_f2 = BatchNormalization()(dense_f2)\n    \n    dense_f3 = Dense(128, activation='relu')(batch_norm_f2)\n    batch_norm_f3 = BatchNormalization()(dense_f3)\n    drop_f2 = Dropout(0.1)(batch_norm_f3)\n\n    dense_f4 = Dense(64, activation='relu')(drop_f2)\n    batch_norm_f4 = BatchNormalization()(dense_f4)\n\n    final = Dense(num_classes, activation='relu')(batch_norm_f4)\n\n    model = Model(inputs = input_layer, outputs = final)\n    return model\n    ","f5000cf8":"model = dnn_model()\nmodel.summary()","d608949a":"model.compile(loss=['categorical_crossentropy'], optimizer=Adam(lr=0.001), metrics=['accuracy'])","50d9004a":"training = model.fit(train_df_features, train_df_labels, batch_size=200, validation_split=0.2, epochs=10)","faf48ec2":"preds = pd.DataFrame(model.predict(test_df))\npreds.columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4']\npreds['id'] = test_id['id']\npreds = preds[['id', 'Class_1', 'Class_2', 'Class_3', 'Class_4']]\n\npreds.to_csv('tps_may_submission', index=False)\npreds","98569fd1":"**Data Preparation**","09a32a04":"**CSV Reading**","df79e864":"**Training**","b71dbe76":"**Import Libraries**","8772428b":"Go check out https:\/\/www.kaggle.com\/soyabulislam\/tps-may-tf-functional-api-approach as well if you want as this notebook also uses the tensorflow functional approach to solve this problem.","881a4005":"**Predictions**","29749e4e":"**Model**"}}