{"cell_type":{"fe54f0ec":"code","fce3cad1":"code","8cee485b":"code","c18c5d89":"code","76ee3131":"code","0d3cf16d":"code","fbfb189b":"code","22c82bad":"markdown"},"source":{"fe54f0ec":"# Imports\n\n########################################################################\n# Python Standard Libraries\nimport os\n\n########################################################################\n# Numpy Library\nimport numpy as np # linear algebra\n\n########################################################################\n# Pandas Library\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n########################################################################\n# MATPLOT Library\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\n########################################################################\n# SKLearn Library\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, classification_report, confusion_matrix, average_precision_score, roc_curve, auc, multilabel_confusion_matrix\n\n########################################################################\n# PYLAB Library\nfrom pylab import bone, pcolor, colorbar, plot, show, title","fce3cad1":"# Utility functions\n\n########################################################################\n# Walk through input files\ndef print_input_files():\n    # Input data files are available in the \"..\/input\/\" directory.\n    for dirname, _, filenames in os.walk('\/kaggle\/input'):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n########################################################################\n# Dump text files\ndef dump_text_file(fname):\n    with open(fname, 'r') as f:\n        print(f.read())\n\n########################################################################\n# Dump CSV files\ndef dump_csv_file(fname, count=5):\n    # count: 0 - column names only, -1 - all rows, default = 5 rows max\n    df = pd.read_csv(fname)\n    if count < 0:\n        count = df.shape[0]\n    return df.head(count)\n\n########################################################################\n# Dataset related functions\nds_nbaiot = '\/kaggle\/input\/nbaiot-dataset'\ndn_nbaiot = ['Danmini_Doorbell', 'Ecobee_Thermostat', 'Ennio_Doorbell', 'Philips_B120N10_Baby_Monitor', 'Provision_PT_737E_Security_Camera', 'Provision_PT_838_Security_Camera', 'Samsung_SNH_1011_N_Webcam', 'SimpleHome_XCS7_1002_WHT_Security_Camera', 'SimpleHome_XCS7_1003_WHT_Security_Camera']\n\ndef fname(ds, f):\n    if '.csv' not in f:\n        f = f'{f}.csv'\n    return os.path.join(ds, f)\n\ndef fname_nbaiot(f):\n    return fname(ds_nbaiot, f)\n\ndef get_nbaiot_device_files():\n    nbaiot_all_files = dump_csv_file(fname_nbaiot('data_summary'), -1)\n    nbaiot_all_files = nbaiot_all_files.iloc[:,0:1].values\n    device_id = 1\n    indices = []\n    for j in range(len(nbaiot_all_files)):\n        if str(device_id) not in str(nbaiot_all_files[j]):\n            indices.append(j)\n            device_id += 1\n    nbaiot_device_files = np.split(nbaiot_all_files, indices)\n    return nbaiot_device_files\n\ndef get_nbaiot_device_data(device_id, count_norm=-1, count_anom=-1):\n    if device_id < 1 or device_id > 9:\n        assert False, \"Please provide a valid device ID 1-9, both inclusive\"\n    if count_anom == -1:\n        count_anom = count_norm\n    device_index = device_id -1\n    device_files = get_nbaiot_device_files()\n    device_file = device_files[device_index]\n    df = pd.DataFrame()\n    y = []\n    for i in range(len(device_file)):\n        fname = str(device_file[i][0])\n        df_c = pd.read_csv(fname_nbaiot(fname))\n        count = count_anom\n        if 'benign' in fname:\n            count = count_norm\n        rows = count if count >=0 else df_c.shape[0]\n        print(\"processing\", fname, \"rows =\", rows)\n        y_np = np.ones(rows) if 'benign' in fname else np.zeros(rows)\n        y.extend(y_np.tolist())\n        df = pd.concat([df.iloc[:,:].reset_index(drop=True),\n                      df_c.iloc[:rows,:].reset_index(drop=True)], axis=0)\n    X = df.iloc[:,:].values\n    y = np.array(y)\n    return (X, y)\n\ndef get_nbaiot_devices_data():\n    devices_data = []\n    for i in range(9):\n        device_id = i + 1\n        (X, y) = get_nbaiot_device_data(device_id)\n        devices_data.append((X, y))\n    return devices_data\n#print_input_files()","8cee485b":"# ********************************************************************************************************\n# This is a third party implementation\n# https:\/\/github.com\/JustGlowing\/minisom\n# MiniSom by Giuseppe Vettigli is licensed under the Creative Commons Attribution 3.0 Unported License.\n# To view a copy of this license, visit http:\/\/creativecommons.org\/licenses\/by\/3.0\/.\n# ********************************************************************************************************\n\nfrom math import sqrt\n\nfrom numpy import (array, unravel_index, nditer, linalg, random, subtract,\n                   power, exp, pi, zeros, arange, outer, meshgrid, dot)\nfrom collections import defaultdict\nfrom warnings import warn\n\n\n\"\"\"\n    Minimalistic implementation of the Self Organizing Maps (SOM).\n\"\"\"\n\n\ndef fast_norm(x):\n    \"\"\"Returns norm-2 of a 1-D numpy array.\n    * faster than linalg.norm in case of 1-D arrays (numpy 1.9.2rc1).\n    \"\"\"\n    return sqrt(dot(x, x.T))\n\n\nclass MiniSom(object):\n    def __init__(self, x, y, input_len, sigma=1.0, learning_rate=0.5, decay_function=None, random_seed=None):\n        \"\"\"\n            Initializes a Self Organizing Maps.\n            x,y - dimensions of the SOM\n            input_len - number of the elements of the vectors in input\n            sigma - spread of the neighborhood function (Gaussian), needs to be adequate to the dimensions of the map.\n            (at the iteration t we have sigma(t) = sigma \/ (1 + t\/T) where T is #num_iteration\/2)\n            learning_rate - initial learning rate\n            (at the iteration t we have learning_rate(t) = learning_rate \/ (1 + t\/T) where T is #num_iteration\/2)\n            decay_function, function that reduces learning_rate and sigma at each iteration\n                            default function: lambda x,current_iteration,max_iter: x\/(1+current_iteration\/max_iter)\n            random_seed, random seed to use.\n        \"\"\"\n        if sigma >= x\/2.0 or sigma >= y\/2.0:\n            warn('Warning: sigma is too high for the dimension of the map.')\n        if random_seed:\n            self.random_generator = random.RandomState(random_seed)\n        else:\n            self.random_generator = random.RandomState(random_seed)\n        if decay_function:\n            self._decay_function = decay_function\n        else:\n            self._decay_function = lambda x, t, max_iter: x\/(1+t\/max_iter)\n        self.learning_rate = learning_rate\n        self.sigma = sigma\n        self.weights = self.random_generator.rand(x,y,input_len)*2-1 # random initialization\n        for i in range(x):\n            for j in range(y):\n                self.weights[i,j] = self.weights[i,j] \/ fast_norm(self.weights[i,j]) # normalization\n        self.activation_map = zeros((x,y))\n        self.neigx = arange(x)\n        self.neigy = arange(y) # used to evaluate the neighborhood function\n        self.neighborhood = self.gaussian\n\n    def _activate(self, x):\n        \"\"\" Updates matrix activation_map, in this matrix the element i,j is the response of the neuron i,j to x \"\"\"\n        s = subtract(x, self.weights) # x - w\n        it = nditer(self.activation_map, flags=['multi_index'])\n        while not it.finished:\n            self.activation_map[it.multi_index] = fast_norm(s[it.multi_index])  # || x - w ||\n            it.iternext()\n\n    def activate(self, x):\n        \"\"\" Returns the activation map to x \"\"\"\n        self._activate(x)\n        return self.activation_map\n\n    def gaussian(self, c, sigma):\n        \"\"\" Returns a Gaussian centered in c \"\"\"\n        d = 2*pi*sigma*sigma\n        ax = exp(-power(self.neigx-c[0], 2)\/d)\n        ay = exp(-power(self.neigy-c[1], 2)\/d)\n        return outer(ax, ay)  # the external product gives a matrix\n\n    def diff_gaussian(self, c, sigma):\n        \"\"\" Mexican hat centered in c (unused) \"\"\"\n        xx, yy = meshgrid(self.neigx, self.neigy)\n        p = power(xx-c[0], 2) + power(yy-c[1], 2)\n        d = 2*pi*sigma*sigma\n        return exp(-p\/d)*(1-2\/d*p)\n\n    def winner(self, x):\n        \"\"\" Computes the coordinates of the winning neuron for the sample x \"\"\"\n        self._activate(x)\n        return unravel_index(self.activation_map.argmin(), self.activation_map.shape)\n\n    def update(self, x, win, t):\n        \"\"\"\n            Updates the weights of the neurons.\n            x - current pattern to learn\n            win - position of the winning neuron for x (array or tuple).\n            t - iteration index\n        \"\"\"\n        eta = self._decay_function(self.learning_rate, t, self.T)\n        sig = self._decay_function(self.sigma, t, self.T) # sigma and learning rate decrease with the same rule\n        g = self.neighborhood(win, sig)*eta # improves the performances\n        it = nditer(g, flags=['multi_index'])\n        while not it.finished:\n            # eta * neighborhood_function * (x-w)\n            self.weights[it.multi_index] += g[it.multi_index]*(x-self.weights[it.multi_index])\n            # normalization\n            self.weights[it.multi_index] = self.weights[it.multi_index] \/ fast_norm(self.weights[it.multi_index])\n            it.iternext()\n\n    def quantization(self, data):\n        \"\"\" Assigns a code book (weights vector of the winning neuron) to each sample in data. \"\"\"\n        q = zeros(data.shape)\n        for i, x in enumerate(data):\n            q[i] = self.weights[self.winner(x)]\n        return q\n\n    def random_weights_init(self, data):\n        \"\"\" Initializes the weights of the SOM picking random samples from data \"\"\"\n        it = nditer(self.activation_map, flags=['multi_index'])\n        while not it.finished:\n            self.weights[it.multi_index] = data[self.random_generator.randint(len(data))]\n            self.weights[it.multi_index] = self.weights[it.multi_index]\/fast_norm(self.weights[it.multi_index])\n            it.iternext()\n\n    def train_random(self, data, num_iteration):\n        \"\"\" Trains the SOM picking samples at random from data \"\"\"\n        self._init_T(num_iteration)\n        for iteration in range(num_iteration):\n            rand_i = self.random_generator.randint(len(data)) # pick a random sample\n            self.update(data[rand_i], self.winner(data[rand_i]), iteration)\n\n    def train_batch(self, data, num_iteration):\n        \"\"\" Trains using all the vectors in data sequentially \"\"\"\n        self._init_T(len(data)*num_iteration)\n        iteration = 0\n        while iteration < num_iteration:\n            idx = iteration % (len(data)-1)\n            self.update(data[idx], self.winner(data[idx]), iteration)\n            iteration += 1\n\n    def _init_T(self, num_iteration):\n        \"\"\" Initializes the parameter T needed to adjust the learning rate \"\"\"\n        self.T = num_iteration\/2  # keeps the learning rate nearly constant for the last half of the iterations\n\n    def distance_map(self):\n        \"\"\" Returns the distance map of the weights.\n            Each cell is the normalised sum of the distances between a neuron and its neighbours.\n        \"\"\"\n        um = zeros((self.weights.shape[0], self.weights.shape[1]))\n        it = nditer(um, flags=['multi_index'])\n        while not it.finished:\n            for ii in range(it.multi_index[0]-1, it.multi_index[0]+2):\n                for jj in range(it.multi_index[1]-1, it.multi_index[1]+2):\n                    if ii >= 0 and ii < self.weights.shape[0] and jj >= 0 and jj < self.weights.shape[1]:\n                        um[it.multi_index] += fast_norm(self.weights[ii, jj, :]-self.weights[it.multi_index])\n            it.iternext()\n        um = um\/um.max()\n        return um\n\n    def activation_response(self, data):\n        \"\"\"\n            Returns a matrix where the element i,j is the number of times\n            that the neuron i,j have been winner.\n        \"\"\"\n        a = zeros((self.weights.shape[0], self.weights.shape[1]))\n        for x in data:\n            a[self.winner(x)] += 1\n        return a\n\n    def quantization_error(self, data):\n        \"\"\"\n            Returns the quantization error computed as the average distance between\n            each input sample and its best matching unit.\n        \"\"\"\n        error = 0\n        for x in data:\n            error += fast_norm(x-self.weights[self.winner(x)])\n        return error\/len(data)\n\n    def win_map(self, data):\n        \"\"\"\n            Returns a dictionary wm where wm[(i,j)] is a list with all the patterns\n            that have been mapped in the position i,j.\n        \"\"\"\n        winmap = defaultdict(list)\n        for x in data:\n            winmap[self.winner(x)].append(x)\n        return winmap\n\n### unit tests\nfrom numpy.testing import assert_almost_equal, assert_array_almost_equal, assert_array_equal\n\n\nclass TestMinisom:\n    def setup_method(self, method):\n        self.som = MiniSom(5, 5, 1)\n        for i in range(5):\n            for j in range(5):\n                assert_almost_equal(1.0, linalg.norm(self.som.weights[i,j]))  # checking weights normalization\n        self.som.weights = zeros((5, 5))  # fake weights\n        self.som.weights[2, 3] = 5.0\n        self.som.weights[1, 1] = 2.0\n\n    def test_decay_function(self):\n        assert self.som._decay_function(1., 2., 3.) == 1.\/(1.+2.\/3.)\n\n    def test_fast_norm(self):\n        assert fast_norm(array([1, 3])) == sqrt(1+9)\n\n    def test_gaussian(self):\n        bell = self.som.gaussian((2, 2), 1)\n        assert bell.max() == 1.0\n        assert bell.argmax() == 12  # unravel(12) = (2,2)\n\n    def test_win_map(self):\n        winners = self.som.win_map([5.0, 2.0])\n        assert winners[(2, 3)][0] == 5.0\n        assert winners[(1, 1)][0] == 2.0\n\n    def test_activation_reponse(self):\n        response = self.som.activation_response([5.0, 2.0])\n        assert response[2, 3] == 1\n        assert response[1, 1] == 1\n\n    def test_activate(self):\n        assert self.som.activate(5.0).argmin() == 13.0  # unravel(13) = (2,3)\n\n    def test_quantization_error(self):\n        self.som.quantization_error([5, 2]) == 0.0\n        self.som.quantization_error([4, 1]) == 0.5\n\n    def test_quantization(self):\n        q = self.som.quantization(array([4, 2]))\n        assert q[0] == 5.0\n        assert q[1] == 2.0\n\n    def test_random_seed(self):\n        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        assert_array_almost_equal(som1.weights, som2.weights)  # same initialization\n        data = random.rand(100,2)\n        som1 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        som1.train_random(data,10)\n        som2 = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        som2.train_random(data,10)\n        assert_array_almost_equal(som1.weights,som2.weights)  # same state after training\n\n    def test_train_batch(self):\n        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        data = array([[4, 2], [3, 1]])\n        q1 = som.quantization_error(data)\n        som.train_batch(data, 10)\n        assert q1 > som.quantization_error(data)\n\n    def test_train_random(self):\n        som = MiniSom(5, 5, 2, sigma=1.0, learning_rate=0.5, random_seed=1)\n        data = array([[4, 2], [3, 1]])\n        q1 = som.quantization_error(data)\n        som.train_random(data, 10)\n        assert q1 > som.quantization_error(data)\n\n    def test_random_weights_init(self):\n        som = MiniSom(2, 2, 2, random_seed=1)\n        som.random_weights_init(array([[1.0, .0]]))\n        for w in som.weights:\n            assert_array_equal(w[0], array([1.0, .0]))","c18c5d89":"class Som:\n    def __init__(self, x=20, y=None):\n        '''\n            parameter: x, y, size of the grid,\n                     default: 20x20, if y is not supplied, y is the same as x\n        '''\n        self.som = None\n        self.map_x = x\n        if not y:\n            self.map_y = x # square grid\n        else:\n            self.map_y = y\n\n    def fit(self, X):\n        '''\n            Fit SOM to the input data.\n            Parameters: X = a numpy array and it should contain\n                            all columns as features and any manually\n                            labeled columns should be removed before\n                            calling this function.\n        '''\n        nb_features = X.shape[1] # number of features\n        som = MiniSom(x = self.map_x, y = self.map_y, input_len = nb_features, sigma = 1.0, learning_rate = 0.5)\n        som.random_weights_init(X)\n        som.train_random(data = X, num_iteration = 2000)\n        dm = som.distance_map()\n        mid = []\n        for x in X:\n            w = som.winner(x)\n            (x,y) = w\n            mid.append(dm[x][y])\n\n        self.dm = dm\n        self.som = som\n        self.grid = (self.map_x, self.map_y)\n        self.mid = mid\n\n    def predict(self, threshold = 0.02, anomaly_label=0):\n        '''Predict data as normal or anomalous based upon mean inter-neuron distance.\n             Need to call fit() before calling this.\n             Parameters: threshold = the threshold (default = 0.02) that is used to\n                                     determine if normal = 1 (when mid <= threshold),\n                                     or anomalous = 0 otherwise.\n                         anomaly_label = the value to label anomalies, default = 0'''\n        if self.som is None:\n            raise Exception('Call fit() before calling this')\n\n        y_pred = []\n        for m in self.mid:\n            normal = (1 if m <= threshold else anomaly_label)\n            y_pred.append(normal)\n        return y_pred\n\n    def plot_marker(self, xy, m, c):\n        plot(xy[0] + 0.5,\n             xy[1] + 0.5,\n             m,\n             markeredgecolor = c,\n             markerfacecolor = 'None',\n             markersize = 10,\n             markeredgewidth = 2)\n\n    def plot_distance_map_labels(self, t, X, Y):\n        '''Plots distance map with labels. Need to call fit() before calling this.\n             Parameters: X = input features\n                         Y = labels, 1 = normal, 0 = anomalous.'''\n        if self.som is None:\n            raise Exception('Call fit() before calling this')\n\n        red_set = set() # normal instances\n        green_set = set() # anomalous instances\n        for i, x in enumerate(X):\n            w = self.som.winner(x)\n            if int(Y[i]) == 0:\n                red_set.add(w)\n            else:\n                green_set.add(w)\n        bone()\n        pcolor(self.dm.T)\n        colorbar()\n        (map_x, map_y) = self.grid\n        for x in range(map_x):\n            for y in range(map_y):\n                xy = (x,y)\n                if (xy in red_set) and (xy in green_set):\n                    self.plot_marker(xy, 'h', 'y')\n                elif xy in red_set:\n                    self.plot_marker(xy, 'o', 'r')\n                elif xy in green_set:\n                    self.plot_marker(xy, 's', 'g')\n                else:\n                    pass #plot_marker(xy, 'v', 'b')\n        title(t)\n        show()","76ee3131":"# Imports\nimport os\nimport time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.python.client import device_lib\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.pyplot as plt\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nfrom matplotlib.cbook import get_sample_data\n\nfrom keras.layers import Input, Dense\nfrom keras import regularizers, Model\nfrom keras.models import Sequential\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\nfrom sklearn.metrics import accuracy_score, precision_recall_curve, classification_report, confusion_matrix, average_precision_score, roc_curve, auc\nfrom sklearn.datasets import make_blobs\n\nfrom imageio import imwrite\n#from scipy.misc import imsave\n\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom PIL import Image, ImageChops","0d3cf16d":"# Test MiniSom\n'''\noutliers_percentage = 0.35\ninliers = 300\noutliers = int(inliers * outliers_percentage)\ndata = make_blobs(centers=[[2, 2], [-2, -2]], cluster_std=[.3, .3],\n                  n_samples=inliers, random_state=0)[0]\ndata = scale(data)\ndata = np.concatenate([data, \n                       (np.random.rand(outliers, 2)-.5)*4.])\nsom = MiniSom(2, 1, data.shape[1], sigma=1, learning_rate=0.5, random_seed=10)\nsom.train_batch(data, 100)  # random training\n\nquantization_errors = np.linalg.norm(som.quantization(data) - data, axis=1)\nerror_treshold = np.percentile(quantization_errors, \n                               100*(1-outliers_percentage)+5)\nis_outlier = quantization_errors > error_treshold\n\nplt.figure(figsize=(8, 8))\nplt.scatter(data[~is_outlier, 0], data[~is_outlier, 1],\n            label='inlier')\nplt.scatter(data[is_outlier, 0], data[is_outlier, 1],\n            label='outlier')\nplt.legend()\n#plt.savefig('resulting_images\/som_outliers_detection.png')\nplt.show()\n'''","fbfb189b":"for i in range(9):\n    device_index = i\n    device_id = i + 1\n    device_name = dn_nbaiot[device_index]\n    #(X, y) = get_nbaiot_device_data(device_id, 5, 2)\n    (X, y) = get_nbaiot_device_data(device_id)\n    print('X.shape', X.shape, 'y.shape', y.shape)\n    X_std = StandardScaler().fit_transform(X)\n    som = Som()\n    som.fit(X_std)\n    som.plot_distance_map_labels(device_name, X_std, y)\n    step = 0.01\n    threshold = 0.0\n    i = 1\n    print(device_name)\n    print(\"threshold,acc,tn,fp,fn,tp\")\n    while threshold <= 1:\n        y_pred = som.predict(threshold)\n        tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n        acc = accuracy_score(y, y_pred)\n        print(f'{threshold:.2f},{acc:.2f},{tn},{fp},{fn},{tp}')\n        threshold += step\n        i += 1","22c82bad":"# Celosia - Unsupervised labelling demo"}}