{"cell_type":{"7a6ad7e2":"code","1827ab95":"code","8dc17ec2":"code","f859ce12":"code","6fa7a87e":"code","f49ffe2b":"code","b0aded6a":"code","d12b450b":"code","52f3ae19":"code","0239d8a5":"code","368c034f":"code","03ae1912":"code","99f95ce4":"code","cac64b77":"code","e94a002a":"code","ff06fe25":"code","84230e0d":"code","9df3d0bf":"code","bd482a37":"code","90be8c20":"code","7c7fa116":"code","c33f8caa":"code","e5eb63e2":"code","a69b5445":"code","929f531a":"code","8806ae4f":"code","3831c4e9":"code","636624d8":"code","03770e90":"code","23d17366":"code","a4af6106":"code","af36a224":"code","b576936c":"code","18f9495f":"code","491e2a89":"code","18a2501c":"code","dbcc0eb0":"code","4960e271":"code","cb228a88":"code","ae873000":"code","6e5a99ab":"code","07938825":"code","6c693271":"code","eee9bebd":"code","738c5f3e":"code","905e5414":"code","bcbf3043":"markdown","3648d8b3":"markdown","677d53e4":"markdown","cd841aaf":"markdown","c0dc347d":"markdown","0c981452":"markdown","7bd0d8d2":"markdown","d1b79fe8":"markdown","a7568f9b":"markdown","da0efcf1":"markdown","8474a500":"markdown","0135e405":"markdown","6cfc3ab0":"markdown","b8839f9b":"markdown","dc221afb":"markdown","af9a71f6":"markdown","f924c0a6":"markdown","a8aee74b":"markdown","0a6f037c":"markdown","f3e9194f":"markdown","fdf4537b":"markdown","0bf4a162":"markdown","8cc8fa59":"markdown","46878c2c":"markdown","e81c924c":"markdown","c63a8db7":"markdown","c9319624":"markdown","ef99883c":"markdown","5846667f":"markdown","bd67f73f":"markdown","0759602c":"markdown","9f4a5e02":"markdown","ad8119cf":"markdown","58bd21bb":"markdown","9cac0feb":"markdown","0dec6e94":"markdown","1044031c":"markdown","ae7254c5":"markdown","d6714e8a":"markdown","af36f41b":"markdown","4b289552":"markdown","6df4027d":"markdown","5840020e":"markdown","b4bbdec1":"markdown","1ca3b126":"markdown","0f7ed5d4":"markdown"},"source":{"7a6ad7e2":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns","1827ab95":"titanic = pd.read_csv('..\/input\/titanic3.csv')\n\n# how many features and data-points ?\nprint('Number of data points: ', titanic.shape[0])\nprint('Number of data features: ', titanic.shape[1])","8dc17ec2":"# The columns\/features of the dataset\nprint('Columns: ', list(titanic.columns))\ntitanic.head(2) # Print the 2 first rows","f859ce12":"# Number of classes in the survived column\nclasses = titanic.survived.unique()\nprint('Survivors represented by: ', classes[0])\nprint('Usurvivors represented by: ', classes[1])","6fa7a87e":"# Passengers per class\nnumber_surv_unsurv = titanic.survived.value_counts()\nprint('Number of survivors: ', number_surv_unsurv[0])\nprint('Number of unsurvivors: ', number_surv_unsurv[1])\n\n# Code for the graphic\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\n\n# Array for labels\nrecipe = [\"Number of survivors\", \"Number of unsurvivors\"]\n\n# Array for value corresponding to labels\ndata = [number_surv_unsurv[0], number_surv_unsurv[1]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\n\nax.set_title(\"Number of passengers who survived - did not survive\")\n\nplt.show()","f49ffe2b":"# Some advanced statictical information about the dataset  \n# Table-1: Statistical summary\ntitanic.describe()","b0aded6a":"def stack_plot(data, xtick, col2='survived', col3='total'):\n    ind = np.arange(data.shape[0])\n    \n    plt.figure(figsize=(20,5))\n    p1 = plt.bar(ind, data[col3].values)\n    p2 = plt.bar(ind, data[col2].values)\n\n    plt.ylabel('Number of total passengers')\n    plt.title('Number survived passengers')\n    plt.xticks(ind, list(data[xtick].values))\n    plt.legend((p1[0], p2[0]), ('total', 'survived'))\n    plt.show()","d12b450b":"def univariate_barplots(data, col1, col2='survived', top=False):\n    # Count number of zeros in dataframe python: https:\/\/stackoverflow.com\/a\/51540521\/4084039\n    temp = pd.DataFrame(titanic.groupby(col1)[col2].agg(lambda x: x.eq(1).sum())).reset_index()\n\n    # Pandas dataframe grouby count: https:\/\/stackoverflow.com\/a\/19385591\/4084039\n    temp['total'] = pd.DataFrame(titanic.groupby(col1)[col2].agg({'total':'count'})).reset_index()['total']\n    temp['Avg'] = pd.DataFrame(titanic.groupby(col1)[col2].agg({'Avg':'mean'})).reset_index()['Avg']\n    \n    temp.sort_values(by=['total'],inplace=True, ascending=False)\n    \n    if top:\n        temp = temp[0:top]\n    \n    stack_plot(temp, xtick=col1, col2=col2, col3='total')\n    print(temp.head(5))\n    print(\"=\"*50)\n    print(temp.tail(5))","52f3ae19":"# Graphic N\u00b01\nunivariate_barplots(titanic, 'pclass', 'survived', False)","0239d8a5":"# Graphic N\u00b02\nunivariate_barplots(titanic, 'sex', 'survived', False)","368c034f":"# Graphic N\u00b03\nunivariate_barplots(titanic, 'cabin', 'survived', top=15)","03ae1912":"# Graphic N\u00b04\nunivariate_barplots(titanic, 'home.dest', 'survived', top=10)","99f95ce4":"# Graphic N\u00b05\nunivariate_barplots(titanic, 'embarked', 'survived')","cac64b77":"# Graphic N\u00b06\nunivariate_barplots(titanic, 'sibsp', 'survived')","e94a002a":"# Graphic N\u00b06\nunivariate_barplots(titanic, 'parch', 'survived')","ff06fe25":"# PDF of passengers' age  : Graphic N\u00b07\nsns.FacetGrid(titanic, hue='survived', height=5) \\\n   .map(sns.distplot, 'age') \\\n   .add_legend();\nplt.title('Probability Density Function (PDF) of Passengers\\'s age')\nplt.show()","84230e0d":"# CDF of passengers' age  : Graphic N\u00b08\n\n'''\nIn our dataset, we have some NaN values, it means missing values. To do so, we are going to \nreplace the missing value by the median value. \n'''\n\ntitanic_survived = titanic.loc[titanic['survived'] == 1] \ntitanic_unsurvived = titanic.loc[titanic['survived'] == 0] \n\n# Replace missing values by median values.\ntitanic_survived['age'].fillna((titanic_survived['age'].median()), inplace=True)\ntitanic_unsurvived['age'].fillna((titanic_unsurvived['age'].median()), inplace=True)","9df3d0bf":"# Age for survived patients: \n# refers to understand bins counts egdes, refer to : https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.histogram.html \n\ncounts, bin_edges = np.histogram(titanic_survived['age'], bins=10, density = True) \npdf = counts\/(sum(counts)) \n\ncdf = np.cumsum(pdf) \n#plt.plot(bin_edges[1:],pdf) \nplt.plot(bin_edges[1:], cdf)\n \n# Age for unsurvived patients \ncounts, bin_edges = np.histogram(titanic_unsurvived['age'], bins=10, density = True) \npdf = counts\/(sum(counts)) \n\ncdf = np.cumsum(pdf) \n#plt.plot(bin_edges[1:],pdf) \nplt.plot(bin_edges[1:], cdf) \nplt.legend(['cdf of survivors', 'cdf of unsurvivors']) #fig5","bd482a37":"sns.boxplot(x='survived', y='age', data=titanic)","90be8c20":"sns.violinplot(x='survived',y='age', data=titanic, size=8) \nplt.show()\n","7c7fa116":"# Check if any cleaning is needed\nprint(set((titanic.cabin.values)))","c33f8caa":"titanic.cabin=titanic.cabin.replace(' ', '_', regex=True)\n\nprint(set((titanic.cabin.values)))","e5eb63e2":"# Check if any cleaning is needed \n# Here, we are just checking on the 20 first values to avoid printing all the values.\n# print(set((titanic['home.dest'].values))) if you want to see all the values\nprint(set((titanic['home.dest'].values[0:20])))","a69b5445":"# Now apply the function to the 'Home.dest column'\n# Without .astype(str), we are going to have the error below : \n# AttributeError: 'float' object has no attribute 'replace'\n\nhome_destinations = list(titanic['home.dest'].astype(str).values)\n\nhome_list = []\n\nfor dest in home_destinations:\n    dest = dest.replace(',','_')\n    dest = dest.replace('[ ]+','')\n    dest = dest.replace('\/','_')\n    home_list.append(dest)","929f531a":"titanic['home.dest'] = home_list\nprint(set((titanic['home.dest'].values[0:20])))\ntitanic.head(2)","8806ae4f":"# Check if any cleaning is needed \nprint(set((titanic['embarked']))) # Nothing to do. ","3831c4e9":"print(set((titanic['sibsp']))) # Nothing to do. ","636624d8":"print(set((titanic['parch']))) # Nothing to do. ","03770e90":"from sklearn.preprocessing import StandardScaler","23d17366":"age_scalar = StandardScaler()\nage_scalar.fit(titanic['age'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {age_scalar.mean_[0]}, Standard deviation : {np.sqrt(age_scalar.var_[0])}\")\n\n# Now standardize the data with above mean and variance.\nage_standardized = age_scalar.transform(titanic['age'].values.reshape(-1, 1))","a4af6106":"# Look at the standardized age feature.\nage_standardized","af36a224":"fare_scalar = StandardScaler()\nfare_scalar.fit(titanic['fare'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\nprint(f\"Mean : {fare_scalar.mean_[0]}, Standard deviation : {np.sqrt(fare_scalar.var_[0])}\")\n\n# Now standardize the data with above mean and variance.\nfare_standardized = fare_scalar.transform(titanic['age'].values.reshape(-1, 1))","b576936c":"# Look at the standardized fare feature.\nfare_standardized","18f9495f":"from sklearn.feature_extraction.text import CountVectorizer\n\n# As the process will be the same all the time, we will create a function\ndef create_one_hot_vect(data, column):\n    non_string_columns = ['sibsp', 'cabin', 'embarked', 'pclass', 'parch', 'boat']\n    \n    if(column in non_string_columns):\n        data[column] =data[column].apply(lambda x: np.str_(x))\n        \n    col = set(data[column].values)\n\n    vectorizer = CountVectorizer(vocabulary=list(col), lowercase=False, binary=True)\n    vectorizer.fit(data[column].values)\n    print(vectorizer.get_feature_names())\n\n    col_one_hot = vectorizer.transform(data[column].values)\n    #print(\"Shape of matrix after one hot encodig \",col_one_hot.shape)\n    return col_one_hot","491e2a89":"sex_one_hot = create_one_hot_vect(titanic, 'sex')\nprint(\"Shape of matrix after one hot encodig \",sex_one_hot.shape)\n\n# Look at the binarized sex feature.\nsex_one_hot","18a2501c":"cabin_one_hot = create_one_hot_vect(titanic, 'cabin')\nprint(\"Shape of matrix after one hot encodig \",cabin_one_hot.shape)\n\n# Look at the binarized cabin feature.\ncabin_one_hot","dbcc0eb0":"home_dest_one_hot = create_one_hot_vect(titanic, 'home.dest')\nprint(\"Shape of matrix after one hot encodig \",home_dest_one_hot.shape)\n\n# Look at the binarized cabin feature.\n#home_dest_one_hot","4960e271":"embarked_one_hot = create_one_hot_vect(titanic, 'embarked')\nprint(\"Shape of matrix after one hot encodig \",embarked_one_hot.shape)\n\n# Look at the binarized cabin feature.\nembarked_one_hot","cb228a88":"pclass_one_hot = create_one_hot_vect(titanic, 'pclass')\nprint(\"Shape of matrix after one hot encodig \",pclass_one_hot.shape)\n\n# Look at the binarized cabin feature.\npclass_one_hot","ae873000":"sibsp_one_hot = create_one_hot_vect(titanic, 'sibsp')\nprint(\"Shape of matrix after one hot encodig \",sibsp_one_hot.shape)\n\n# Look at the binarized cabin feature.\nsibsp_one_hot","6e5a99ab":"parch_one_hot = create_one_hot_vect(titanic, 'parch')\nprint(\"Shape of matrix after one hot encodig \",parch_one_hot.shape)\n\n# Look at the binarized cabin feature.\nparch_one_hot","07938825":"boat_one_hot = create_one_hot_vect(titanic, 'boat')\nprint(\"Shape of matrix after one hot encodig \",boat_one_hot.shape)\n\n# Look at the binarized cabin feature.\nboat_one_hot","6c693271":"# merge two sparse matrices: https:\/\/stackoverflow.com\/a\/19710648\/4084039\nfrom scipy.sparse import hstack\nfrom sklearn.manifold import TSNE","eee9bebd":"# Create the data matrix\ntitanic_data_matrix = hstack((age_standardized, fare_standardized,\n                             sex_one_hot, cabin_one_hot, home_dest_one_hot,\n                             embarked_one_hot, pclass_one_hot,sibsp_one_hot,\n                             parch_one_hot, boat_one_hot))\n\nprint(titanic_data_matrix.shape)","738c5f3e":"titanic_data_matrix","905e5414":"# Convert the matrix into dense matrix\nX = titanic_data_matrix.toarray()\ny = titanic['survived']\n\ntsne = TSNE(n_components=2, random_state=0, perplexity=50)\n\n# Without the expression \"np.nan_to_num(X)\" we get the below error: \n# ValueError: Input contains NaN, infinity or a value too large for dtype('float32')\n\nX_embedding = tsne.fit_transform(np.nan_to_num(X))\n\nfor_tsne = np.vstack((X_embedding.T, y)).T\n\nfor_tsne_df = pd.DataFrame(data=for_tsne, columns=['Dimension_x','Dimension_y','Score'])\ncolors = {0:'red', 1:'green'}\nplt.scatter(for_tsne_df['Dimension_x'], for_tsne_df['Dimension_y'], c=for_tsne_df['Score'].apply(lambda x: colors[x]))\nplt.xlabel('Dimension_x')\nplt.ylabel('Dimension_y')\nplt.title('TSNE: TFIDF | perplexity = 50, learning_rate=200 (Default value)')\nplt.show()\n","bcbf3043":"## I- PROBLEM UNDERSTANDING\n\n## II- EXPLORATORY DATA ANALYSIS  \n\n        II-1 SOME STATISTICS ON THE DATASET  \n        II-2 UNIVARIATE ANALYSIS  \n\n        II-2-1 Univariate analysis for categorical - non continuous numerical features.  \n\n        II-2-2 Univariate analysis (PDF,CDF, boxplot, violin plot) for numerical-continuous features.  \n\n## III- VECTORIZATION OF OUR FEATURES  \n        III-0 Data cleaning before vectorization  \n\n        III-1 Apply standardization technics to numerical features.  \n\n        III-2 Apply binarization\/one-hot encoding technics to categorical features.  \n\n        III-0 Data cleaning before vectorization  \n            III-0-1 Data cleaning on Cabin feature  \n            III-0-2 Data cleaning on Home.dest feature  \n            III-0-3 Data cleaning on Embarked feature  \n            III-0-4 Data cleaning on Sibs feature  \n            III-0-5 Data cleaning on Parch feature  \n\n        III-1 Apply standardization technics to numerical features  \n            III-1-1 Standardization: Age feature  \n            III-1-2 Standardization: Fare feature  \n\n        III-2 Apply binarization\/one-hot encoding technics to categorical features  \n            III-2-1 Binarization: Sex feature  \n            III-2-2 Binarization: Cabin feature  \n            III-2-3 Binarization: Home dest feature  \n            III-2-4 Binarization: Embarked feature  \n            III-2-5 Binarization: P-class feature  \n            III-2-6 Binarization: sibsp feature  \n            III-2-7 Binarization: parch feature  \n            III-2-8 Binarization: boat feature  \n            \n## IV- VISUALIZATION USING T-SNE  \n        IV-1 T-Distributed Stochastic Neighbourhood Embedding (t-SNE)  ","3648d8b3":"**III-2-5 Binarization:** P-class feature  ","677d53e4":"## III-2 Apply binarization\/one-hot encoding technics to categorical features  \nWe are to use sklearn CountVectorizer module.  ","cd841aaf":"![image.png](attachment:image.png)  \n\n**Analysis:** The age of almost 65% of unsurvivors were less or equal to 30. ","c0dc347d":"**Observation**: We will be doing the same processing like previously. But in this case, we have more special characters like: \n<ul>\n    <li>Backslash: \/<\/li>\n    <li>Coma: ,<\/li>  \n    And we will replace all of them by underscore. \n<\/ul>","0c981452":"**Univariate Analysis: Sibsp**   ","7bd0d8d2":"**III-0-5 Data cleaning on Parch feature**","d1b79fe8":"**observation**: As we can see in the values taken by this feature, some are separated with spaces like: \n<ul>\n    <li>C23 C25 C27<\/li>\n    <li>B52 B54 B56<\/li>\n    <li>F E46<\/li>\n    <li>etcetera, etcetera ...<\/li>\n    So we are going to replace all the blancks by underscore in order to create a \"sigle string\".  \n    For example: \n    <li>C23 C25 C27 ===> C23_C25_C27<\/li> \n    <li>B52 B54 B56 ===> B52_B54_B56<\/li>\n    <li>F E46 ===> F_E46<\/li>\n<\/ul>","a7568f9b":"**IMPORT USEFUL LIBRARIES**","da0efcf1":"Parch","8474a500":"**Observation**: We can notice that the application performed very well. ","0135e405":"**III-2-7 Binarization:** parch feature  ","6cfc3ab0":"**III-0-3 Data cleaning on Embarked feature**","b8839f9b":"**III-1-2 Standardization:** Fare feature","dc221afb":"<ol>**II-2 UNIVARIATE ANALYSIS**  \nWe are going to perform some univariate analysis technics and do an observation for each part. The analyis technics are: \n    <li>Univariate analysis for categorical - non continuous numerical features<\/li>\n    <li>Univariate analysis (PDF,CDF, boxplot, violin plot) for numerical-continuous features<\/li>\n<\/ol>  \n","af9a71f6":"**III-2-2 Binarization:** Cabin feature  ","f924c0a6":"**Observation of barplots:** \nBased on the above visualization, we can notice that : \n<ul>\n    <li>More passengers in the First class survived to the drama (Graphic N\u00b01)<\/li>\n    <li>The majority of survivors where women (Graphic N\u00b02)<\/li>\n    <li>The majority of survivors where women (Graphic N\u00b02)<\/li>\n    <li>The majority of survivors where thos who embarked at Cherbourg (Graphic N\u00b05)<\/li>\n    <li>All the passengers having more than 4 ciblings\/spouses did not survive (Graphic N\u00b06 & 6)<\/li>\n<\/ul>","a8aee74b":"**III-2-3 Binarization:** Home dest feature  ","0a6f037c":"**Univariate Analysis: Cabin**   \nFor better visualization we will be showing the top 15 Cabin that had the most survivors. Beyond this value, it becomes difficult to read. ","f3e9194f":"**Box-plot**  \nThis analysis give more statistical informations about the features we are analysing. It also show the corrupted values (if they exist) related to the feature. Regarding the statistical informations about quantiles (Q1, Q2, Q3), where  \n\nQ1 ==> 25th percentile  \nQ2 ==> median value\/50th percentile  \nQ3 ==> 75th percentile  \n","fdf4537b":"**Violin-plot analysis**  \nThis technic combines PDFs and box-plots\n","0bf4a162":"## I- PROBLEM UNDERSTANDING  \nThe titanic dataset consist of demographic and travelling information for passengers. And the goal of this EDA is to determine which features are much more relevant for further prediction. So in the prediction part that will be done in the machine learning part will be to predict the survival of these passengers. \nBelow are the attributes\/features used in the dataset.  \n<ul>**Dataset attributes**\n    <li>pclass: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)<\/li>\n    <li>survival: A Boolean indicating whether the passenger survived or not (0 = No; 1 = Yes); this is our target<\/li>\n    <li>name: Title and family names of passengers<\/li>\n    <li>age: Age <\/li>\n    <li>sibsp: Number of siblings\/spouses aboard<\/li>\n    <li>parch: Number of parents\/children aboard<\/li>\n    <li>ticket: Ticket number<\/li>\n    <li>fare: Passenger fare (British Pound)<\/li>\n    <li>cabin: Doesthe location of the cabin influence chances of survival?<\/li>\n    <li>embarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)<\/li>\n    <li>boat: Lifeboat, many missing values<\/li>\n    <li>body: Body Identification Number<\/li>\n    <li>home.dest: Home\/destination<\/li>\n<\/ul>","8cc8fa59":"# III- VECTORIZATION OF OUR FEATURES\nFor further machine learning technic application, and good visualization, we are going to transform our features into vectors. To do so will be working following the below process:  \n<ol>**III-0 Data cleaning before vectorization**  \n    Let's have a look at some of our categorical features in order to know if cleaning will be helpfull. \n<\/ol>\n\n<ol>**III-1 Apply standardization technics to numerical features**.  \n    In our case, this technic will be applied to \n    <li>Age: numerical feature<\/li>\n    <li>Fare: numerical feature<\/li>\n<\/ol>\n\n<ol>**III-2 Apply binarization\/one-hot encoding technics to categorical features**.   \n    In our case, this technic will be applied to \n    <li>Sex: categorical feature<\/li>\n    <li>Cabin: categorical feature <\/li>\n    <li>Home.dest: categorical feature<\/li>\n    <li>Embarked: categorical feature<\/li>\n    <li>P-class: categorical feature<\/li>\n    <li>Sibsp: categorical feature<\/li>\n    <li>Parch: categorical feature<\/li>\n    <li>Boat: categorical feature<\/li>\n<\/ol>","46878c2c":"### II-2-1 Univariate analysis for categorical - non continuous numerical features.  \nBelow are the features we are going to analyse are\n<ul>\n    <li>P-class: non-continuous numerical feature<\/li>\n    <li>Sex: categorical feature<\/li>\n    <li>Cabin: categorical feature<\/li>\n    <li>Embarked: categorical feature<\/li>\n    <li>Home.dest: categorical feature<\/li>\n<\/ul>\n\n**Univariate Analysis: P-Class** ","e81c924c":"**Observation:** We can notice that the application performed very well. ","c63a8db7":"**III-0-1 Data cleaning on Cabin feature**","c9319624":"## III-0 Data cleaning before vectorization  \nLet's have a look at some of our categorical features in order to know if cleaning will be helpfull.  ","ef99883c":"**Univariate Analysis: Home dest**   \nFor better visualization we will be showing the top 10 home destination that had the most survivors. Beyond this value, it becomes difficult to read.","5846667f":"**III-2-4 Binarization:** Embarked feature  ","bd67f73f":"# EXPLORATORY DATA ANALYSIS AND T-SNE VISUALIZATION ON TITANIC DATASET  \n\n## INTRODUCTION  \nIn this tutorial, we will be using the Titanic dataset. The main goal of the kernel is the EDA as mentionned previously. This kernel will be followe by another one which will cover machine learning aspect. \nSo, feel free to **upvote** if you like the content or drop a comment for any suggestion.\nThis kernel is going to cover some aspect of exploratory data analysis and visualuzation technic. After completing this kernel, you will be able to understand the following aspect and the interpretation behind all of them: ","0759602c":"**Cumulative Density Function**  \nThis analysis can give us more precise information about the probability for a feature to be less or equal to a specific value taken by that feature (on the x-axis).","9f4a5e02":"**Univariate Analysis: Embarked**   ","ad8119cf":"**III-0-4 Data cleaning on Sibs feature**","58bd21bb":"**Univariate Analysis: Sex** ","9cac0feb":"**III-0-2 Data cleaning on Home.dest feature**","0dec6e94":"**III-2-1 Binarization:** Sex feature  ","1044031c":"**III-2-6 Binarization:** sibsp feature  ","ae7254c5":"**Probability Density Function**  \nThe goal of this analysis technic is to determine the probability to be in a certain range of the considered feature, age in our case.  ","d6714e8a":"**Observation:** With the T-SNE technic, we can see that most survived passengers are grouped to the top-left (green) which contains a few overlap. Then, most of the other survivors can be seen to the right, very well regrouped. So T-SNE is a very good visualization technic which help us having better visualization of our passengers.","af36f41b":"# IV- VISUALIZATION USING T-SNE \nWhen we look at the number of parameters\/features we have, we notice that we have more than 5 features. So to visualize such data (having such number of feature), it is better to use technics of dimentinality reduction. In our case, we are going to use the following one:  \n<ol>**IV-1 T-Distributed Stochastic Neighbourhood Embedding (t-SNE)**<\/ol>   \n\nBefore performin the T-SNE technic we will build the data matrix using the vectors created in the above cells.  ","4b289552":"**III-2-8 Binarization:** boat feature  ","6df4027d":"**III-1-1 Standardization:** Age feature","5840020e":"## III-1 Apply standardization technics to numerical features   \nWe are going to use the **sklearn StandardScaler** module to standardize our numerical features.  ","b4bbdec1":"## II- EXPLORATORY DATA ANALYSIS \n**II-1 SOME STATISTICS ON THE DATASET**  \nWe are going to perform some statistical studies on the dataset in order to understand more about: \n<ol>\n    <li>The number of passengers - columns<\/li>\n    <li>The meaning of the values of the 'Survived column'<\/li>\n    <li>The number of survivors - unsurvivors<\/li>\n    <li>Some advanced statistical analysis<\/li>\n    <li>First observation<\/li>\n<\/ol>  \n","1ca3b126":"**First observation:**  \nAfter this first part, we noticed that our dataset contains information about 1309 passengers, and those information were based on 14 parameters. the survived column is the one telling us if a passenger survived with the value of 1 or did not survive with the value of 2. And 70% of the passengers have no parents-children aboard (Table-1, column **parch** row **70%**).","0f7ed5d4":"### II-2-2 Univariate analysis (PDF,CDF, boxplot, violin plot) for numerical-continuous features.  \nHere are the features we are going to analyse the following features  \n<ul>\n    <li>Age<\/li>     \n<\/ul>  "}}