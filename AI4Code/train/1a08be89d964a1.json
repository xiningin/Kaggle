{"cell_type":{"090afcb8":"code","a9eb6289":"code","a32cb087":"code","257a9abe":"code","855c4500":"code","183d1a30":"code","c85b6d4f":"code","d24309dc":"code","fb808a07":"code","79735952":"code","09b156e3":"code","d5e61f08":"code","fda0949e":"code","f6253b97":"code","1c55b8b7":"code","a83b1734":"code","6041a6a1":"code","788603e0":"code","285fac33":"code","baaf01e9":"code","5a2c44a3":"code","8f210892":"code","56840d29":"code","cc8bf157":"code","ef937aa2":"code","0dc2aec3":"code","a8794b0b":"code","bc0d494c":"code","cfcbe7a7":"code","9d7dc509":"code","108b5c49":"code","0dab8b50":"code","2d61ea1b":"code","0d6f2698":"code","6d3d5fbc":"code","2101fefb":"code","b31bb6f6":"code","fe66ddb3":"code","86df75a1":"code","a1c1ef54":"code","7f899e86":"code","1eaa8e36":"code","b48ed368":"code","6e403fe6":"code","fadd0f29":"code","13a2e067":"code","3d56eb51":"code","50c88a11":"code","ceb72942":"code","8d9d451d":"code","0f30d00f":"code","5c9b3343":"code","2836173b":"code","932ce37f":"code","b55163d9":"markdown","9cb8a486":"markdown","6d2dc7f3":"markdown","9d61303f":"markdown","c31bced7":"markdown","f3efe14c":"markdown","a4c8a9d5":"markdown","4572862c":"markdown","0b06d938":"markdown","ec21e50a":"markdown","737890d9":"markdown","8a260dca":"markdown","70d195fd":"markdown","5c90f06c":"markdown","44d54527":"markdown","bc3641d0":"markdown"},"source":{"090afcb8":"import pandas as pd\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.express as px\nimport re\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import word2vec\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression","a9eb6289":"def load_data():\n    file = open('..\/input\/dataset\/data.txt', 'r', encoding='utf-8') \n    count = 0\n\n    text_list = []\n    sentiment_list = []\n    company_list = []\n    while True: \n        count += 1\n\n        # Get next line from file \n        line = file.readline() \n        if not line :\n            break\n        text = line.split(')')[1]\n        sentiment = line.split(')')[0].split(',')[1]\n        company = line.split(')')[0].split(',')[2]\n        # if line is empty \n        # end of file is reached \n        company_list.append(company)\n        sentiment_list.append(sentiment)\n        text_list.append(text)\n    file.close() \n    \n    df = pd.DataFrame()\n\n    df['company'] = company_list\n    df['sentiment'] = sentiment_list\n    df['text'] = text_list\n    return df","a32cb087":"df = load_data()","257a9abe":"df.sentiment.unique()","855c4500":"import re\nimport string\nfrom langdetect import detect\n\ntagging_regex = re.compile(r\"@\\S*\")\nurl_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nsignature_pattern = re.compile(r\"-\\S*\")\nweird_thing_pattern = re.compile(r\"\\^\\S*\")\nnew_line_pattern = re.compile(r\"\\n+\\S*\")\n\nchat_words = {\n    \"AFAIK\": \"As Far As I Know\",\n    \"AFK\": \"Away From Keyboard\",\n    \"ASAP\": \"As Soon As Possible\",\n    \"ATK\": \"At The Keyboard\",\n    \"ATM\": \"At The Moment\",\n    \"A3\": \"Anytime, Anywhere, Anyplace\",\n    \"BAK\": \"Back At Keyboard\",\n    \"BBL\": \"Be Back Later\",\n    \"BBS\": \"Be Back Soon\",\n    \"BFN\": \"Bye For Now\",\n    \"B4N\": \"Bye For Now\",\n    \"BRB\": \"Be Right Back\",\n    \"BRT\": \"Be Right There\",\n    \"BTW\": \"By The Way\",\n    \"B4\": \"Before\",\n    \"B4N\": \"Bye For Now\",\n    \"CU\": \"See You\",\n    \"CUL8R\": \"See You Later\",\n    \"CYA\": \"See You\",\n    \"FAQ\": \"Frequently Asked Questions\",\n    \"FC\": \"Fingers Crossed\",\n    \"FWIW\": \"For What It's Worth\",\n    \"FYI\": \"For Your Information\",\n    \"GAL\": \"Get A Life\",\n    \"GG\": \"Good Game\",\n    \"GN\": \"Good Night\",\n    \"GMTA\": \"Great Minds Think Alike\",\n    \"GR8\": \"Great!\",\n    \"G9\": \"Genius\",\n    \"IC\": \"I See\",\n    \"ICQ\": \"I Seek you (also a chat program)\",\n    \"ILU\": \"ILU: I Love You\",\n    \"IMHO\": \"In My Honest\/Humble Opinion\",\n    \"IMO\": \"In My Opinion\",\n    \"IOW\": \"In Other Words\",\n    \"IRL\": \"In Real Life\",\n    \"KISS\": \"Keep It Simple, Stupid\",\n    \"LDR\": \"Long Distance Relationship\",\n    \"LMAO\": \"Laugh My A.. Off\",\n    \"LOL\": \"Laughing Out Loud\",\n    \"LTNS\": \"Long Time No See\",\n    \"L8R\": \"Later\",\n    \"MTE\": \"My Thoughts Exactly\",\n    \"M8\": \"Mate\",\n    \"NRN\": \"No Reply Necessary\",\n    \"OIC\": \"Oh I See\",\n    \"PITA\": \"Pain In The A..\",\n    \"PRT\": \"Party\",\n    \"PRW\": \"Parents Are Watching\",\n    \"ROFL\": \"Rolling On The Floor Laughing\",\n    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n    \"ROTFLMAO\": \"Rolling On The Floor Laughing My Ass Off\",\n    \"SK8\": \"Skate\",\n    \"STATS\": \"Your sex and age\",\n    \"ASL\": \"Age, Sex, Location\",\n    \"THX\": \"Thank You\",\n    \"TTFN\": \"Ta-Ta For Now!\",\n    \"TTYL\": \"Talk To You Later\",\n    \"U\": \"You\",\n    \"U2\": \"You Too\",\n    \"U4E\": \"Yours For Ever\",\n    \"WB\": \"Welcome Back\",\n    \"WTF\": \"What The F...\",\n    \"WTG\": \"Way To Go!\",\n    \"WUF\": \"Where Are You From?\",\n    \"W8\": \"Wait\",\n    \"IMMA\": \"I am going to\",\n    \"2NITE\": \"tonight\",\n    \"DMED\": \"mesaged\",\n    'DM': \"message\",\n    \"SMH\": \"I am dissapointed\"\n}\n\n# Thanks to https:\/\/stackoverflow.com\/a\/43023503\/3971619\ncontractions = {\n    \"ain't\": \"are not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he shall have \/ he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"I would\",\n    \"i'd've\": \"I would have\",\n    \"i'll\": \"I will\",\n    \"i'll've\": \"I will have\",\n    \"i'm\": \"I am\",\n    \"i've\": \"I have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so is\",\n    \"that'd\": \"that had\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'll\": \"you will\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\",\n}\n\n# Reference : https:\/\/stackoverflow.com\/a\/49986645\/3971619\ndef remove_emoji(inputString):\n    return inputString.encode('ascii', 'ignore').decode('ascii')\n\n# Thanks to user sudalairajkumar\ndef remove_url(string):\n    return url_pattern.sub(r'', string)\n\ndef remove_chat_words_and_contractions(string):\n    new_text = []\n    for word in string.split(' '):\n        if word.upper() in chat_words.keys():\n            new_text += chat_words[word.upper()].lower().split(' ')\n        if word.lower() in contractions.keys():\n            new_text += contractions[word.lower()].split(' ')\n        else:\n            new_text.append(word)\n            \n    return ' '.join(new_text)\n\ndef remove_signature(text):\n    return signature_pattern.sub(r'', text)\n    \n\n# Thanks to user sudalairajkumar\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\ndef clean_message(message):\n    # Remove user taggings\n    message = re.sub(tagging_regex, '', message) # Replace by you. Good idea?\n    \n    # Remove the emojis\n    message = remove_emoji(message)\n    \n    # Remove urls\n    message = remove_url(message)\n    \n    # Remove signatures\n    message = remove_signature(message)\n    \n    # Remove the chat words and contractions\n    message = remove_chat_words_and_contractions(message)\n    \n    # Remove weird things\n    message = weird_thing_pattern.sub(r'', message)\n\n    # Change new line to dot\n    message = new_line_pattern.sub(r'.', message)\n    \n    # Remove punctuation\n    message = remove_punctuation(message)\n    \n    # Remove start and end whitespace\n    message = message.strip()\n    \n    # Make multiple spaces become a single space\n    message = ' '.join(message.split())\n    \n    # Lower case the message\n    message = message.lower()\n    \n    # If not in english, return empty string\n#     if message and len(message) > 15:\n#         if detect(message) != 'en':\n#             return \"\"\n    \n    return message\n","183d1a30":"f = lambda row: clean_message(row['text'])\ndf['cleaned_text'] = df.apply(f, axis = 1)","c85b6d4f":"# We only keep tweets written in English\nfrom langdetect import detect\n\ndef detectLanguage(text):\n    try:\n        return detect(text) \n    except:\n        return 'None'\n    \nf = lambda row: detectLanguage(str(row['text']))\ndf['language'] = df.apply(f, axis = 1)","d24309dc":"df = df[df['language']=='en']","fb808a07":"# Sentiment classes encoding\ndef sentiment_encode(s):\n    if s == 'neu':\n        return 0\n    if s == 'neg':\n        return 1\n    if s == 'pos':\n        return 2\n    else:\n        return 3\n\nf = lambda row: int(sentiment_encode(row['sentiment']))\ndf['sentiment_encode'] = df.apply(f, axis = 1)","79735952":"from PIL import Image\nimport requests\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nresponse = requests.get('https:\/\/banner2.cleanpng.com\/20180723\/vvy\/kisspng-computer-icons-clip-art-twitter-logo-vector-5b5693f7952128.7797517715324006316109.jpg')\nbird = np.array(Image.open(BytesIO(response.content)))\n\n# d = '..\/input\/twitter\/'\n# bird = np.array(Image.open(d + 'twitter_mask.png'))\nfig, (ax2, ax3) = plt.subplots(1, 2, figsize=[30, 15])\nwordcloud2 = WordCloud( background_color='white',mask=bird,colormap=\"Reds\",\n                        width=600,\n                        height=400).generate(\" \".join(df[df['sentiment']=='pos']['cleaned_text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Negative Sentiment',fontsize=35);\n\nwordcloud3 = WordCloud( background_color='white',mask=bird,colormap=\"Greens\",\n                        width=600,\n                        height=400).generate(\" \".join(df[df['sentiment']=='neg']['cleaned_text']))\nax3.imshow(wordcloud3)\nax3.axis('off')\nax3.set_title('Positive Sentiment',fontsize=35);","09b156e3":"def find_hash(text):\n    line=re.findall(r'(?<=#)\\w+',text)\n    return \" \".join(line)\ndf['hash']=df['text'].apply(lambda x:find_hash(x))\n\nfrom collections import Counter\n\nhastags=list(df[(df['hash'].notnull())&(df['hash']!=\"\")]['hash'])\nhastags = [each_string.lower() for each_string in hastags]\nhash_df=dict(Counter(hastags))\ntop_hash_df=pd.DataFrame(list(hash_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_hash_df.head(4)\n\nfig = go.Figure(go.Bar(\n    x=top_hash_df['word'],y=top_hash_df['count'],\n    marker={'color': top_hash_df['count'], \n    'colorscale': 'blues'},  \n    text=top_hash_df['count'],\n    textposition = \"outside\",\n))\nfig.update_layout(title_text='Top Trended Hastags',xaxis_title=\"Hashtags \",\n                  yaxis_title=\"Number of Tags \",height=700,width=1500,title_x=0.5)\nfig.show()","d5e61f08":"def find_at(text):\n    line=re.findall(r'(?<=@)\\w+',text)\n    return \" \".join(line)\ndf['mention']=df['text'].apply(lambda x:find_at(x))\n\nimport itertools\n\nmentions=list(df[(df['mention'].notnull())&(df['mention']!=\"\")]['mention'])\nmentions = [each_string.lower().split() for each_string in mentions]\nmentions=list(itertools.chain.from_iterable(mentions))\nmention_df=dict(Counter(mentions))\ntop_mention_df=pd.DataFrame(list(mention_df.items()),columns = ['word','count']).sort_values('count',ascending=False)[:20]\ntop_mention_df.head(10)","fda0949e":"fig = go.Figure(go.Bar(\n    x=top_mention_df['word'],y=top_mention_df['count'],\n    marker={'color': top_mention_df['count'], \n    'colorscale': 'blues'},  \n    text=top_mention_df['count'],\n    textposition = \"outside\",\n))\n\nfig.update_layout(title_text='Top Trended Mentions ',xaxis_title=\"Mentions\",\n                  yaxis_title=\"Number of Tags \",height=700,width=1500,title_x=0.5)\nfig.show()","f6253b97":"df['text_length']=df['text'].str.split().map(lambda x: len(x))\nfig = go.Figure(data=go.Violin(y=df['text_length'], box_visible=True, line_color='black',\n                               meanline_visible=True, fillcolor='royalblue ', opacity=0.6,\n                               x0='Tweet Text Length '))\n\nfig.update_layout(yaxis_zeroline=False,title=\"Distribution of Text length \",template='ggplot2')\nfig.show()","1c55b8b7":"def ngram_df(corpus,nrange,n=None):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=nrange).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    total_list=words_freq[:n]\n    df=pd.DataFrame(total_list,columns=['text','count'])\n    return df\n\nunigram_df=ngram_df(df['cleaned_text'],(1,1),20)\nbigram_df=ngram_df(df['cleaned_text'],(2,2),20)\ntrigram_df=ngram_df(df['cleaned_text'],(3,3),20)","a83b1734":"from plotly.subplots import make_subplots\nfig = make_subplots(\n    rows=1, cols=3,subplot_titles=(\"Unigram\",\"Bigram\",'Trigram'),\n)\n\nfig.add_trace(go.Bar(\n    y=unigram_df['text'][::-1],\n    x=unigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=unigram_df['count'],\n    textposition = \"outside\",\n    orientation=\"h\",\n    name=\"Months\",\n),row=1,col=1)\n\nfig.add_trace(go.Bar(\n    y=bigram_df['text'][::-1],\n    x=bigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=bigram_df['count'],\n     name=\"Days\",\n    textposition = \"outside\",\n    orientation=\"h\",\n),row=1,col=2)\n\nfig.add_trace(go.Bar(\n    y=trigram_df['text'][::-1],\n    x=trigram_df['count'][::-1],\n    marker={'color': \"blue\"},  \n    text=trigram_df['count'],\n     name=\"Days\",\n    orientation=\"h\",\n    textposition = \"outside\",\n),row=1,col=3)\n\nfig.update_xaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_yaxes(showline=True, linewidth=2, linecolor='black', mirror=True)\nfig.update_layout(title_text='Top N Grams',xaxis_title=\" \",yaxis_title=\" \", showlegend=False,title_x=0.5,\n                  height=800, width=2000)\nfig.show()","6041a6a1":"df['lemmatized_text'] = df['cleaned_text']","788603e0":"cv = CountVectorizer()\nx = cv.fit_transform(df['lemmatized_text']).toarray()","285fac33":"x_train,x_test,y_train,y_test = train_test_split(x,df['sentiment_encode'],test_size = 0.2,random_state = 42,shuffle=True)","baaf01e9":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(x_train,y_train)","5a2c44a3":"print('Training Data Accuracy:', model.score(x_train, y_train))","8f210892":"print('Testing Data Accuracy:', model.score(x_test, y_test))","56840d29":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, model.predict(x_test)))","cc8bf157":"from sklearn.svm import LinearSVC\n\nsvm = LinearSVC()\nsvm.fit(x_train, y_train)","ef937aa2":"print('Training Data Accuracy:', svm.score(x_train, y_train))","0dc2aec3":"print('Testing Data Accuracy:', svm.score(x_test, y_test))","a8794b0b":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, svm.predict(x_test)))","bc0d494c":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, svm.predict(x_test))","cfcbe7a7":"model = RandomForestClassifier()\nmodel.fit(x_train,y_train)","9d7dc509":"print('Training Data Accuracy:', model.score(x_train, y_train))","108b5c49":"print('Testing Data Accuracy:', model.score(x_test, y_test))","0dab8b50":"confusion_matrix(y_test, model.predict(x_test))","2d61ea1b":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, model.predict(x_test)))","0d6f2698":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, model.predict(x_test))","6d3d5fbc":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nmodel.fit(x_train,y_train)\nprint('Testing Data Accuracy:', model.score(x_test, y_test))\nprint(classification_report(y_test, model.predict(x_test)))","2101fefb":"cv = CountVectorizer(ngram_range=(2,3))\nx = cv.fit_transform(df['lemmatized_text']).toarray()\nx_train,x_test,y_train,y_test = train_test_split(x,df['sentiment_encode'],test_size = 0.2,random_state = 42)","b31bb6f6":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(x_train,y_train)\nprint('Naive Bayes')\nprint('----------')\nprint('Training Data Accuracy:', model.score(x_train, y_train))\nprint('Testing Data Accuracy:', model.score(x_test, y_test))\nprint(confusion_matrix(y_test, model.predict(x_test)))\nprint('----------')","fe66ddb3":"from sklearn.svm import LinearSVC\n\nmodel = LinearSVC()\nmodel.fit(x_train, y_train)\nprint('SVM')\nprint('----------')\nprint('Training Data Accuracy:', model.score(x_train, y_train))\nprint('Testing Data Accuracy:', model.score(x_test, y_test))\nprint(confusion_matrix(y_test, model.predict(x_test)))\nprint('----------')","86df75a1":"model = RandomForestClassifier()\nmodel.fit(x_train,y_train)\nprint('Random Forest')\nprint('----------')\nprint('Training Data Accuracy:', model.score(x_train, y_train))\nprint('Testing Data Accuracy:', model.score(x_test, y_test))\nprint(confusion_matrix(y_test, model.predict(x_test)))\nprint('----------')","a1c1ef54":"#\nfrom simpletransformers.classification import ClassificationModel\n\n\n# Create a TransformerModel\n#model = ClassificationModel('bert', 'bert-base-cased', num_labels=4, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs':12},use_cuda=False)\nmodel = ClassificationModel('roberta', 'roberta-base', num_labels=4, args={'overwrite_output_dir': True, 'num_train_epochs':32},use_cuda=False)","7f899e86":" train,eva = train_test_split(df[['cleaned_text', 'sentiment']], test_size = 0.2, shuffle=True)","1eaa8e36":"def making_label(st):\n    if(st=='pos'):\n        return 0\n    elif(st=='neu'):\n        return 2\n    elif (st=='neg'):\n        return 3\n    else:\n        return 1\n    \ntrain['label'] = train['sentiment'].apply(making_label)\neva['label'] = eva['sentiment'].apply(making_label)","b48ed368":"train_df = pd.DataFrame({\n    'text': train['cleaned_text'],\n    'label': train['label']\n})\n\neval_df = pd.DataFrame({\n    'text': eva['cleaned_text'],\n    'label': eva['label']\n})","6e403fe6":"train_df.label.unique()","fadd0f29":"model.train_model(train_df)","13a2e067":"result, model_outputs, wrong_predictions = model.eval_model(train_df)","3d56eb51":"print('Training accuracy \/ loss', result)","50c88a11":"result, model_outputs, wrong_predictions = model.eval_model(eval_df)","ceb72942":"print('Test accuracy \/ loss')","8d9d451d":"result","0f30d00f":"#model.save_model()\nmodel = ClassificationModel('roberta', r'.\/outputs\\checkpoint-4144-epoch-14', use_cuda=False)","5c9b3343":"predictions, raw_outputs = model.predict(eval_df.text.values)","2836173b":"import plotly.figure_factory as ff\nfrom sklearn.metrics import confusion_matrix\n\nx = ['pos', 'irr', 'neu', 'neg']\ny = ['pos', 'irr', 'neu', 'neg']\nz = confusion_matrix(eval_df.label, predictions)\n\nfig = ff.create_annotated_heatmap(z, x = x, y = y, colorscale='Viridis')\nfig.update_layout(title_text='Confusion matrix on Test Data<\/b><\/i>')\n# add colorbar\nfig['data'][0]['showscale'] = True\nfig.show()","932ce37f":"from sklearn.metrics import classification_report\n\nprint(classification_report(eval_df.label, predictions))","b55163d9":"### Bi-grams ","9cb8a486":"# Data Collection ","6d2dc7f3":"### EDA","9d61303f":"#### Random Forest","c31bced7":"### Bag of Words (BoW) ","f3efe14c":"#### Naive Bayes ","a4c8a9d5":"#### All features ","4572862c":"## BERT Model ","0b06d938":"#### Logistic Regression","ec21e50a":"The example above shows that our model is not able to find the real sentiment due to the unigram method we are using here ! \nWe will then apply bi-grams to avoid this. ","737890d9":"# Results\n","8a260dca":"# Import packages","70d195fd":"# Data Exploratory & Cleaning","5c90f06c":"### Unigram ","44d54527":"#### SVM ","bc3641d0":"# Models training"}}