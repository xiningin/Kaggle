{"cell_type":{"625141a3":"code","1949c5d3":"code","be11ace2":"code","ff84857e":"code","81a96ba1":"code","54876033":"code","727952e0":"code","e36a6022":"code","65351aec":"code","0d8b5568":"code","f70774ec":"code","0b2260a6":"code","81720412":"code","0d30d58e":"code","57c6ff02":"code","f7e0f4f8":"code","d255df71":"markdown","ac70ed88":"markdown","8e1ab2d2":"markdown","911e89a9":"markdown"},"source":{"625141a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1949c5d3":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom datetime import datetime #To Know the Running Time\nfrom sklearn.model_selection import train_test_split # Data Splitting\nfrom imblearn.over_sampling import ADASYN # Oversampling Data with SVMSMOTE\nfrom sklearn.preprocessing import StandardScaler # Data Standadization\nfrom sklearn.preprocessing import MinMaxScaler #Min-Max Data Normalization\nfrom sklearn.linear_model import LogisticRegression # Logistic regression algorithm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nimport itertools # advanced tools\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import VotingClassifier","be11ace2":"#Import Dataset\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\ndf.head()","ff84857e":"#Drop Unnecessary Column\ndf1 = df.drop(['PassengerId','Name', 'Ticket', 'Cabin'], axis=1)\n#Count the Categorical Data\ncat_cols = df1.select_dtypes(include=object).columns.tolist()\n(pd.DataFrame(\n    df[cat_cols]\n    .melt(var_name='column', value_name='value')\n    .value_counts())\n.rename(columns={0: 'counts'})\n.sort_values(by=['column', 'counts']))\n#Convert Categorical Data to Integer\ndf1['Embarked'] = df1['Embarked'].replace(['Q'], 1).replace(['C'], 2).replace(['S'], 3)\ndf1['Sex'] = df1['Sex'].replace(['female'], 1).replace(['male'], 2)\ndf1.head()","81a96ba1":"#Define imputer to replace missing value\nimputer = KNNImputer(n_neighbors=5, weights='uniform', metric='nan_euclidean')\nimputer.fit(df1)\ndftrans = imputer.transform(df1)\ndf1 = pd.DataFrame(dftrans, columns = ['Survived', 'Pclass',\t'Sex',\t'Age',\t'SibSp',\t'Parch',\t'Fare',\t'Embarked'])\ndf1.head()","54876033":"#Pearson Correlation for Feature Selection\ncorr = df1.corr()[\"Survived\"].sort_values(ascending=False)[1:]\nabs_corr = abs(corr)\nrelevant_features = abs_corr[abs_corr>0.3]\nrelevant_features\ndf1 = df1[relevant_features.index]\n#Rebuild the Dataframe\ndf1['Survived'] = df['Survived']\ndf1['Survived'] = df['Survived'].values\ndf1.head()","727952e0":"#Pre-process Data\n#Data Split\nx = df1.drop('Survived', axis = 1).values\ny = df1['Survived'].values\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = None)\n\n#Oversampling Data\nada = ADASYN(sampling_strategy='auto', random_state=None)\nx_train, y_train = ada.fit_resample(x_train, y_train)\n\n#Min-Max Data Scalling\nminmaxscaler = MinMaxScaler()\nx_train = minmaxscaler.fit_transform(x_train)\nx_test = minmaxscaler.transform(x_test)\n\n#Standarization\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","e36a6022":"#Logistic Regression\nlr = LogisticRegression()\nlr_parameter = {\n    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'multi_class': ['auto', 'ovr', 'multinomial'],\n}\nlr_tuned = GridSearchCV(lr, lr_parameter, n_jobs=-1, cv=3)\nlr_tuned.fit(x_train, y_train)\nlr_model = lr_tuned.predict(x_test)\n\n# Best parameter set\nprint('Best parameters found:\\n', lr_tuned.best_params_)","65351aec":"#Decision Tree\ndt = DecisionTreeClassifier()\ndt_parameter = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_depth': [10, 15, 30],\n}\ndt_tuned = GridSearchCV(dt, dt_parameter, n_jobs=-1, cv=3)\ndt_tuned.fit(x_train, y_train)\ndt_model = dt_tuned.predict(x_test)\n\n# Best parameter set\nprint('Best parameters found:\\n', dt_tuned.best_params_)","0d8b5568":"#Multi Layer Perceptron Classifier\nmlp = MLPClassifier(max_iter=100)\nmlp_parameter = {\n    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive'],\n}\nmlp_tuned = GridSearchCV(mlp, mlp_parameter, n_jobs=-1, cv=3)\nmlp_tuned.fit(x_train, y_train)\nmlp_model = mlp_tuned.predict(x_test)\n\n# Best parameter set\nprint('Best parameters found:\\n', mlp_tuned.best_params_)","f70774ec":"#Ensemble Learning Voting\nclassifier1 = LogisticRegression(multi_class=lr_tuned.best_params_['multi_class'], penalty=lr_tuned.best_params_['penalty'], solver=lr_tuned.best_params_['solver'])\nclassifier2 = DecisionTreeClassifier(criterion=dt_tuned.best_params_['criterion'], max_depth=dt_tuned.best_params_['max_depth'], splitter=dt_tuned.best_params_['splitter'])\nclassifier3 = MLPClassifier(max_iter=100, activation=mlp_tuned.best_params_['activation'], alpha=mlp_tuned.best_params_['alpha'], hidden_layer_sizes=mlp_tuned.best_params_['hidden_layer_sizes'], learning_rate=mlp_tuned.best_params_['learning_rate'], solver=mlp_tuned.best_params_['solver'])\nclassifier4 = GaussianNB()\nensemble = VotingClassifier(estimators=[('lr', classifier1), ('dt', classifier2), ('mlp', classifier3), ('gnb', classifier4)], voting='soft', weights=[1, 1, 1, 1])\n\nclassifier1.fit(x_train, y_train)\nclassifier2.fit(x_train, y_train)\nclassifier3.fit(x_train, y_train)\nclassifier4.fit(x_train, y_train)\nensemble.fit(x_train, y_train)\n\nclf1_model = classifier1.predict(x_test)\nclf2_model = classifier2.predict(x_test)\nclf3_model = classifier3.predict(x_test)\nclf4_model = classifier4.predict(x_test)\nensemble_model = ensemble.predict(x_test)","0b2260a6":"#Scoring\nprint('Logistic Regression')\nprint('Precision {}'.format(precision_score(y_test, clf1_model)))\nprint('Recall {}'.format(recall_score(y_test, clf1_model)))\nprint('F1 Score {}'.format(f1_score(y_test, clf1_model)))\nprint('Accuracy {}'.format(accuracy_score(y_test, clf1_model)))\nprint('Decision Tree')\nprint('Precision {}'.format(precision_score(y_test, clf2_model)))\nprint('Recall {}'.format(recall_score(y_test, clf2_model)))\nprint('F1 Score {}'.format(f1_score(y_test, clf2_model)))\nprint('Accuracy {}'.format(accuracy_score(y_test, clf2_model)))\nprint('Multi-Layer Perceptron')\nprint('Precision {}'.format(precision_score(y_test, clf3_model)))\nprint('Recall {}'.format(recall_score(y_test, clf3_model)))\nprint('F1 Score {}'.format(f1_score(y_test, clf3_model)))\nprint('Accuracy {}'.format(accuracy_score(y_test, clf3_model)))\nprint('Gaussian Naive Bayes')\nprint('Precision {}'.format(precision_score(y_test, clf4_model)))\nprint('Recall {}'.format(recall_score(y_test, clf4_model)))\nprint('F1 Score {}'.format(f1_score(y_test, clf4_model)))\nprint('Accuracy {}'.format(accuracy_score(y_test, clf4_model)))\nprint('Ensemble Classifier Soft Voting')\nprint('Precision{}'.format(precision_score(y_test, ensemble_model)))\nprint('Recall {}'.format(recall_score(y_test, ensemble_model)))\nprint('F1 Score {}'.format(f1_score(y_test, ensemble_model)))\nprint('Accuracy {}'.format(accuracy_score(y_test, ensemble_model)))","81720412":"#Read Testing Dataset\nfinal = pd.read_csv('..\/input\/titanic\/test.csv')\nfinal_trans = final.drop(['PassengerId','Name', 'Age', 'SibSp', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Parch'], axis=1)\nfinal_trans['Sex'] = final_trans['Sex'].replace(['female'], 1).replace(['male'], 2)\nfinal_trans.head()","0d30d58e":"#Data Pre-processing\nfinal_trans = minmaxscaler.fit_transform(final_trans)\nfinal_trans = scaler.fit_transform(final_trans)\nfinal_trans = pd.DataFrame(final_trans)\nfinal_trans","57c6ff02":"ensemble_model_final = ensemble.predict(final_trans)\nprediction = pd.DataFrame(ensemble_model_final)\nprediction.columns = ['Survived']\nresult = pd.concat([final, prediction], axis=1)\nresult = result.drop(['Pclass','Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], axis=1)\nresult","f7e0f4f8":"result.to_csv('file_name.csv', index=False)","d255df71":"Feature Selection was conducted to reduce the unnecessary feature","ac70ed88":"Print scoring to compare the performance between classifier and its performance\n*How ever in this case, ensemble-learning classifier does not improve the classifier significantly","8e1ab2d2":"Model building start from here with hyper-parameter tuning using GridSearchCV","911e89a9":"Classifying the Test.csv dataset and collect the output via csv file"}}