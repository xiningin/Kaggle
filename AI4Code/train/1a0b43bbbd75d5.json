{"cell_type":{"8d918630":"code","3b80ad09":"code","aa3f3a46":"code","fffee097":"code","522bc39c":"code","56df597e":"code","1be2169d":"code","4bf55f91":"code","9100080b":"code","e47cd1d6":"code","67ee03a1":"code","d0278765":"code","069e0288":"code","43467b85":"code","e52870f5":"code","23355cb7":"code","c9701117":"code","93058200":"code","3b53d70f":"code","e956a2fc":"code","06aa8242":"code","e7d90c12":"code","f402691b":"code","47610a56":"code","22f4947f":"code","3ea68a29":"code","989c395e":"code","f05fd98a":"code","512e95f3":"code","b9cca51c":"code","ac81a988":"code","705cfa79":"code","463b941d":"code","67e3f844":"code","f4401d28":"code","a62b556b":"code","ccdfdb14":"code","3e4f23c4":"code","826b18b8":"code","59856ca2":"code","db86900e":"code","2c44b539":"code","4ec31156":"code","1761cf04":"code","001af15f":"code","d38a1369":"code","a8b3e1a0":"code","4a27c25d":"code","57fa6314":"code","a84c494c":"code","e0820d3b":"code","c5516687":"code","752e23e7":"code","1b408d12":"code","8034699b":"code","d9cd0128":"code","8379794c":"code","4d3d80c6":"code","62a855d8":"code","8ffb86ce":"code","68adbd51":"code","3c0092af":"code","20fa3d21":"code","1ea72e1f":"code","c8ad4a34":"code","1efa3873":"code","ef5318bc":"code","b3edc6f2":"code","6e80836a":"code","8c3ea4c6":"code","3f73f485":"code","77cd76f0":"code","974d3293":"code","8b9444c2":"code","1c49ea82":"code","76ba5f4d":"code","eaf77ddf":"code","ea9d6971":"code","bcb211db":"code","9058e98d":"code","cab5fcca":"code","8f77e7f5":"code","12defbab":"code","53d7f231":"code","624ad9f9":"code","be3526d4":"code","cf3069d8":"code","f7036d00":"code","c2f2b894":"code","f7841216":"code","612f4b62":"code","b570c29c":"code","80382af5":"code","0c2c9bad":"code","3ee6025c":"code","3f483aa6":"code","637cd712":"code","63541580":"code","5e138ab3":"code","3d1d841a":"code","c0cf1ae5":"code","23a035b6":"code","68d4711f":"code","5f514f3b":"code","11e06398":"code","3a55627f":"code","e4ef068c":"code","887f712a":"code","b09c6b11":"code","40491f67":"code","eb66e23a":"code","b07752fe":"code","2a17bfc2":"code","7c5fc7b1":"code","dd5c3456":"code","2752df47":"code","69548062":"code","1428a623":"code","5b2c7861":"code","0bcf4a94":"code","873dd751":"code","34da08c9":"code","48593907":"code","f4c54734":"code","a349d41b":"code","98310176":"code","2af35195":"code","7fe332e5":"code","b4166cf3":"code","053d7c00":"code","e3bab43d":"code","2e3e7f52":"code","77526354":"code","788c8be5":"code","c40de586":"markdown","7296194b":"markdown","eaef9881":"markdown","6349150f":"markdown","7f0d1c9e":"markdown","70285ce3":"markdown","f679d0ef":"markdown","b8b0085f":"markdown","02c49b20":"markdown","341a3f7d":"markdown","6eeaae85":"markdown","54e3e99e":"markdown","365edebc":"markdown","370cc8f0":"markdown","7e954673":"markdown","fb577587":"markdown","d3b54a55":"markdown","724e2948":"markdown","7f298266":"markdown","b7a718de":"markdown","4e1b1487":"markdown","4807875e":"markdown","3289bcc9":"markdown","01d8b291":"markdown","4d34e54a":"markdown","fa8d6378":"markdown","b03204f1":"markdown","441d7945":"markdown","54cb76c6":"markdown","c19a5e19":"markdown","37a90504":"markdown","9c481926":"markdown","2fc6ad33":"markdown","eb77005f":"markdown","0447742b":"markdown","e0f183b2":"markdown","ef50ef98":"markdown","ab50624a":"markdown","f5ffa551":"markdown","786fd036":"markdown","e01e8f5c":"markdown","8ed78669":"markdown","b798b177":"markdown","ab8dd67d":"markdown","10e7cf62":"markdown","72ee6df1":"markdown","899e6ee1":"markdown","0e1c749e":"markdown","68d1abb7":"markdown","0964666c":"markdown","053041bb":"markdown","0d391fc8":"markdown","14e5c729":"markdown","ff2748fc":"markdown","0f2f95f9":"markdown","c0349a46":"markdown","5ea9b6a7":"markdown","563b5356":"markdown","9ed50763":"markdown","a162c734":"markdown","5afc1e01":"markdown","47c3c277":"markdown","fb90e873":"markdown","25d02ece":"markdown","16fe3d3f":"markdown","eaddb770":"markdown","ddb9d89f":"markdown","63e54699":"markdown","fd4aef3e":"markdown","0954d530":"markdown","b788268c":"markdown","20c87dc7":"markdown","f8cc845c":"markdown","03cc4178":"markdown","77496fe0":"markdown","6beba96c":"markdown","ed499cd7":"markdown","981dc420":"markdown","894f69f5":"markdown","b302a94f":"markdown","9ab6fe7e":"markdown","6ea60dcd":"markdown","6ebfdd5c":"markdown","768cb336":"markdown","d83f4969":"markdown","85f54092":"markdown","11a94bab":"markdown","68fd891c":"markdown","a5144253":"markdown","7616d06c":"markdown","0aafce25":"markdown","d9b2c0bb":"markdown","cf41778f":"markdown","126410b6":"markdown","f1c48919":"markdown","8c04255b":"markdown","c2e98115":"markdown","3c6cfe99":"markdown","9d2bbc9b":"markdown","a10323ef":"markdown","26070130":"markdown","e9021b66":"markdown","29179077":"markdown","b6f7390a":"markdown","627e9e66":"markdown","a0cb71ae":"markdown","5747d4db":"markdown","9d51a2d2":"markdown","3130baae":"markdown","c389e269":"markdown","427bec21":"markdown","7b15c2b0":"markdown","af9b8c1f":"markdown","2135a512":"markdown","c1a6ebc0":"markdown","2360d38d":"markdown","1fcb244f":"markdown","424818b9":"markdown","f7953446":"markdown","823dc3fc":"markdown","0008f7a4":"markdown","86f364a1":"markdown","757e5f1b":"markdown","36329897":"markdown","6c4b8513":"markdown","5c031fe7":"markdown","715d93fe":"markdown","e856231b":"markdown","57008c1f":"markdown","a34960e5":"markdown","a332a3a7":"markdown","378c4835":"markdown","79fcc946":"markdown","2087c081":"markdown","3d6b781f":"markdown","0ad6cef2":"markdown","05c46f52":"markdown","9d34bc71":"markdown","8dca4983":"markdown","36aec641":"markdown","ec9170ab":"markdown","196b7ab0":"markdown","c654d5c2":"markdown","2baa0968":"markdown","bb476687":"markdown","3341c460":"markdown","908f1043":"markdown","13adc894":"markdown","ac4e5a27":"markdown","f985c495":"markdown","575f53a6":"markdown","ff689d4b":"markdown","04a10795":"markdown","78273b31":"markdown","c84a096f":"markdown","2756d4e1":"markdown","89f66a78":"markdown","c44a020f":"markdown","fb78502b":"markdown","da73050a":"markdown","9cf88bf3":"markdown","3bf6d1f7":"markdown","76e1cd02":"markdown","7f5aade3":"markdown","992753bc":"markdown","c1a0881c":"markdown","2e7e19be":"markdown","2629d9ca":"markdown","aa66219e":"markdown","26aefd71":"markdown","0311ae6a":"markdown","01a7378a":"markdown","b8e387a7":"markdown","b1913639":"markdown","12f66120":"markdown","a4fe3166":"markdown","6ea4a58a":"markdown","db1d32a5":"markdown","d5b1a3b4":"markdown","e4c3e686":"markdown","07a8552e":"markdown","086f3925":"markdown","062ac189":"markdown","e51da631":"markdown","2c33d41e":"markdown","a38a805a":"markdown","04d714cf":"markdown","2c8c6430":"markdown","5f2d7062":"markdown","2b62661f":"markdown","bf676938":"markdown","ad25a52e":"markdown","f99a8f19":"markdown"},"source":{"8d918630":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport sklearn as sk\nimport itertools\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom statsmodels.graphics.mosaicplot import mosaic\nsns.set(style=\"white\", context=\"notebook\", palette=\"deep\")","3b80ad09":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.tree import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, VotingClassifier)\nfrom sklearn.svm import SVC\n\n#XGBoost\nimport xgboost as xgb\nimport lightgbm as lgbm\n\n# mlextend\nfrom mlxtend.classifier import StackingClassifier","aa3f3a46":"#\uac80\uc99d, \ud3c9\uac00, \ud559\uc2b5 \ubc0f \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc138\ud2b8 \ubd84\ub9ac\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV","fffee097":"def load_datasets():\n    \"\"\"\n    Load datasets and return train, test and full_data\n    \"\"\"\n    train = pd.read_csv(\"..\/input\/train.csv\")\n    test  = pd.read_csv(\"..\/input\/test.csv\")\n    \n    full_data = pd.concat([train.drop(\"Survived\", axis = 1), test])\n    return train, test, full_data\n\n# load data\ntrain, test, full_data = load_datasets()","522bc39c":"display(train.head(3), full_data.head(3))","56df597e":"display(train.info())","1be2169d":"train.describe()","4bf55f91":"display(train.isnull().sum())  # \ub110\uc774 \uc544\ub2cc\uac83\uc740 0, \ub110\uc778 \uac83\uc740 1\uc744 \ub9ac\ud134\ud568. \uacb0\uad6d, \ub110\uc778 \uac83\uc758 \uac2f\uc218\ub97c \uc758\ubbf8\ud568.\nprint(\"\\n\")\ndisplay(test.info())","9100080b":"survived = train[train[\"Survived\"]==1]\nnonsurvived = train[train[\"Survived\"]==0]\n\nsurv_col, nonsurv_col = \"blue\", \"red\"\nsurvived_ratio = 1 * survived.shape[0] \/ train.shape[0] * 100.0\nnonsurvived_ratio = 1 * nonsurvived.shape[0] \/ train.shape[0] * 100.0\n\ndisplay(f\"\uc0dd\uc874\uc790 : {survived.shape[0]} ( {survived_ratio:.1f}%) , \uc0ac\ub9dd\uc790 : {nonsurvived.shape[0]} ( {nonsurvived_ratio:.1f}%)\")","e47cd1d6":"xCols = [\"Age\", \"Sex\", \"Embarked\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"]\nfor i in zip(xCols, range(1, 8)):\n    print(f\"{i}\")","67ee03a1":"plt.figure(figsize=(12, 10))\nnrows, ncols = 3, 3\nsurv_data_notna = survived[\"Age\"].dropna()\nnonsurv_data_notna = nonsurvived[\"Age\"].dropna()\n\n\nfor i in zip(xCols, range(1, 8)):\n    plt.subplot(nrows, ncols, i[1])\n    if i[1] ==1:\n        sns.distplot(a=surv_data_notna, bins=range(0, 81, 1), kde=False, color=surv_col)\n        sns.distplot(a=nonsurv_data_notna, bins=range(0, 81,1), kde=False, color=nonsurv_col, axlabel=i[0])\n    elif i[1] in [2, 3, 4, 5, 6]:\n        sns.barplot(x=i[0], y=\"Survived\", data = train)\n    else:\n        sns.distplot(a=np.log1p(survived[i[0]]).dropna(), kde=False, color=surv_col)\n        sns.distplot(a=np.log1p(nonsurvived[i[0]]).dropna(), kde=False, color=nonsurv_col)        \n        \nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,wspace=0.35)\nplt.show()     \n\ndisplay(f\"Survivor average age : {np.median(np.median(surv_data_notna)): .1f}   , The decease average age : {np.median(nonsurv_data_notna): .2f}\")","d0278765":"tab = pd.crosstab(index = train[\"SibSp\"], columns=train[\"Survived\"])\ndisplay(tab)\ndisplay(tab.sum(axis= 1), type(tab))  # Total count by SibSp\ndisplay(tab.div(tab.sum(axis=1).astype(float), axis=0))  # Ratio by Survival - Criterion by SibSp\n\nsurvival_ratio = tab.div(tab.sum(axis=1).astype(float), axis=0)\nsurvival_ratio.plot(kind='bar', stacked=True, color=[nonsurv_col, surv_col])\nplt.xlabel(\"Siblings + Spouses\")\nplt.ylabel(\"Survival Ration by SibSp\")\nplt.show()","069e0288":"stats.binom_test(x=5, n=5, p=0.62)","43467b85":"nan_cnt = train[\"Cabin\"].isna().sum()\ntot_cnt = train[\"Cabin\"].shape[0]\nnotnull_cnt = test[\"Cabin\"].dropna().shape[0]","e52870f5":"display(f\"Total count :  {train['Cabin'].shape[0]} , NaN count : {train['Cabin'].isna().sum()} in train datasets\")\ndisplay(f\"Total count :  {test['Cabin'].shape[0]} , Not null count : {test['Cabin'].dropna().shape[0]}\")\n\n# \ub110\uac12\uc744 \uc81c\uc678\ud55c \ud559\uc2b5\ub370\uc774\ud130\ndisplay(train.loc[:, [\"Survived\", \"Cabin\", \"PassengerId\"]].dropna().head(10))\ndisplay(train.loc[train[\"PassengerId\"]==28])","23355cb7":"display(f\"There are {train['Ticket'].nunique()} unqiue ticket numbers among the {train['Ticket'].count()}\")\nprint(\"There are {} unique ticket numbers among the {} tickets\".format(train['Ticket'].nunique(), train['Ticket'].count()))\nprint(\"There are %d unique ticket numbers among the %d tickets.\" %(train['Ticket'].nunique(), train['Ticket'].count()))","c9701117":"grouped = train.groupby('Ticket')\n#display(grouped, dir(grouped), type(grouped)) # pandas.core.groupby.groupby.DataFrameGroupBy object at 0x7f3f1d1c4908\n\nk = 0\n# name - \uc9d1\uacc4\ub2e8\uc704\uac00 \ub418\ub294 \uac1c\ubcc4 Ticket\ubc88\ud638\n# group - Groupby\uc5d0 \ubb36\uc778 \uceec\ub7fc\nfor name, group in grouped:  \n    if (len(grouped.get_group(name)) > 1):\n        print(f\"Name : {name}, group : {group.head(3)}\")\n        print(group.loc[:, [\"Survived\", \"Name\", \"Fare\"]])\n    k += 1\n    if (k > 10):\n        break","93058200":"plt.figure(figsize=(14, 12))\nfoo = sns.heatmap(train.drop(\"PassengerId\", axis=1).corr(), vmax=0.6, square=True, annot=True)","3b53d70f":"col_name = train.dtypes[train.dtypes != object].drop('PassengerId', inplace=False)\ncols = col_name.index.values.tolist()\ng = sns.pairplot(data = train.dropna(), vars = cols, size = 1.5, hue='Survived', palette = [nonsurv_col, surv_col])\ng.set(xticklabels = [])","e956a2fc":"male_surv = train[(train['Survived']==1) & (train['Sex']=='male')]\nfemale_surv = train[(train['Survived']==1) & (train['Sex']=='female')]\n\nmale_nonsurv = train[(train['Survived']==0) & (train['Sex']=='male')]\nfemale_nonsurv = train[(train['Survived']==0) & (train['Sex']=='female')]\n\nplt.figure(figsize=(13, 5))\nplt.subplot(121)\n\nsns.distplot(female_surv['Age'].dropna().values, bins=range(0,  81, 1), kde=False, color=surv_col)\nsns.distplot(female_nonsurv['Age'].dropna().values, bins=range(0, 81,1), kde=False, color=nonsurv_col,axlabel='Female Age')\n\nplt.subplot(122)\nsns.distplot(male_surv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(male_nonsurv['Age'].dropna().values, bins=range(0,81, 1), kde=False, color=nonsurv_col, axlabel='Male Age')\n\nplt.show()","06aa8242":"foo = full_data['Age'].hist(by=full_data[\"Pclass\"], bins=np.arange(0, 81, 1), layout=[3,1], sharex=True,\\\n                           figsize=[8, 12])","e7d90c12":"foo= sns.boxplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train)","f402691b":"sns.violinplot(x=\"Pclass\", y=\"Age\", data=full_data, inner=None)","47610a56":"sns.swarmplot(x=\"Pclass\", y=\"Age\", data=full_data, color='r', alpha=.5)","22f4947f":"sns.violinplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, split=True)\nplt.hlines([0, 10], xmin=-1, xmax=3, linestyle='dotted')","3ea68a29":"dummy = mosaic(train, index = [\"Survived\", \"Sex\", \"Pclass\"], gap=0.02, title=\"Survival rate Sex vs Pclass\")","989c395e":"g = sns.factorplot(x=\"Pclass\", y=\"Survived\", data=train, hue=\"Sex\", col=\"Embarked\", aspect=0.9, size=3.5, ci=95.0)","f05fd98a":"g = sns.FacetGrid(train, col=\"Embarked\", size=2.2, aspect=1.6)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\", ci=95.0, palette=\"deep\")\ng.add_legend()","512e95f3":"tab = pd.crosstab(index = full_data[\"Embarked\"], columns=full_data[\"Pclass\"])\nprint(tab)\n\ndummy = tab.div(tab.sum(axis=1).astype(float), axis = 0)\ndummy.plot(kind='bar', stacked=True)\ndummy = plt.xlabel(\"Port Embarked\")\ndummy = plt.ylabel(\"Percentage\")","b9cca51c":"tab = pd.crosstab(index = full_data[\"Embarked\"], columns=full_data[\"Pclass\"])\ndisplay(tab)\n\ndummy = tab.div(tab.sum(axis = 1).astype(float), axis=0)\ndisplay(dummy)\n\ndummy.plot(kind='bar', stacked=True)\ndummy = plt.xlabel(\"Port Embarked\")\ndummy = plt.ylabel(\"Pclass Occupancy Ratio\")","ac81a988":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Pclass\", data=train)","705cfa79":"tab = pd.crosstab(index=full_data[\"Embarked\"], columns=full_data[\"Sex\"])\n\ndisplay(tab)\n\ndummy = tab.div(tab.sum(axis = 1).astype(float), axis = 0)\ndisplay(dummy)\n\ndummy.plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel(\"Port Embarked\")\ndummy = plt.ylabel(\"Percentage\")","463b941d":"tab = pd.crosstab(index=full_data[\"Pclass\"], columns = full_data[\"Sex\"])\ndisplay(tab)\n\ndummy = tab.div(tab.sum(axis = 1).astype(float), axis=0)\ndisplay(dummy)\n\ndummy.plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel(\"Pclass\")\ndummy = plt.ylabel(\"Percentage\")","67e3f844":"sib = pd.crosstab(index=train[\"SibSp\"], columns=train[\"Sex\"])\ndisplay(sib)\n\ndummy = sib.div(sib.sum(axis=1).astype(float), axis=0)\ndisplay(dummy)\n\ndummy.plot(kind='bar', stacked=True)\nplt.xlabel(\"Siblings + Parents\")\nplt.ylabel(\"Ratio vs Sex\")\nplt.show()","f4401d28":"parch = pd.crosstab(train[\"Parch\"], train[\"Sex\"])\ndisplay(parch)\ndummy = parch.div(parch.sum(axis=1).astype(float), axis=0)\ndisplay(dummy)\n\ndummy.plot(kind='bar', stacked=True)\nplt.xlabel(\"Parent\/Children\")\nplt.ylabel(\"Dominance Ratio\")\nplt.show()","a62b556b":"sns.violinplot(x='Embarked', y=\"Age\", data=train, hue='Survived', split=True)\nplt.hlines([0, 10], xmin=-1, xmax= 3, linestyle=\"dotted\")\nplt.show()","ccdfdb14":"plt.figure(figsize=(12, 10))\nnrows, ncols = 3, 1\nfor i in list(range(ncols, 4)):\n    plt.subplot(nrows, ncols, i)\n    ax= sns.distplot(a=np.log10(survived['Fare'][survived['Pclass']==i].dropna().values+1), kde=False, color=surv_col)\n    ax = sns.distplot(a=np.log10(nonsurvived['Fare'][nonsurvived['Pclass']==i].dropna().values+1), kde=False, color=nonsurv_col, axlabel=\"Fare\")\n    ax.set_xlim(0, np.max(np.log10(train[\"Fare\"].dropna().values)))\n    \nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)\nplt.show()","3e4f23c4":"def display_boxplot(scale):\n    import seaborn as sns\n    from scipy import stats\n    import matplotlib.pyplot as plt\n    sns.set(style=\"white\", context=\"notebook\", palette=\"deep\") \n    \"\"\"\n    \uc2a4\ucf00\uc77c\uc744 \uc785\ub825\ubc1b\uc544\uc11c \ubc15\uc2a4\ud50c\ub86f \ubcf4\uc5ec\uc8fc\uae30 \n    \"\"\"\n    try:\n        plt.figure(figsize=(10, 8))\n        ax = sns.boxplot(x=\"Pclass\", y=\"Fare\", hue=\"Survived\", data=train);\n        ax.set_yscale(scale)\n        plt.show()\n    except RuntimeError as e:\n        print(e.message)","826b18b8":" # Set the y-axis scale , {\"linear\", \"log\", \"symlog\", \"logit\"}\ny_scale = [\"linear\", \"log\", \"symlog\"]\nfor i in y_scale:\n    display_boxplot(i)","59856ca2":"display(train.info())","db86900e":"display(train[train[\"Embarked\"].isnull()])\ndisplay(f\"NaN count for Embarked feature  {train['Embarked'].isnull().sum()}\")","2c44b539":"cond1 = full_data[\"Embarked\"] != 'Q'\ncond2 = full_data[\"Pclass\"] < 1.5\ncond3 = full_data[\"Sex\"] == 'female'\n\ngb_full_data = full_data.where(cond1 & cond2 & cond3).groupby([\"Embarked\", \"Pclass\", \"Sex\", \"Parch\", \"SibSp\"])\n\n# Queenstown\uc5d0 \uc2b9\uc120\ud558\uc9c0 \uc54a\uace0 1\ub4f1\uae09 \uac1d\uc2e4\uc5d0 \ud0c4 \uc5ec\uc790\uc2b9\uac1d\uc758 \ubd80\ubaa8\ub3d9\ubc18 \ud639\uc740 \uc790\uc2dd, \uc190\uc790 , \uc870\ubd80\ubaa8 \ub3d9\ubc18\uc790 \uc870\uc0ac\ndisplay(gb_full_data.size())","4ec31156":"full_data.where(cond1 & cond2 & cond3).groupby([\"Embarked\", \"Pclass\", \"Sex\"]).size()","1761cf04":"train[\"Embarked\"].iloc[61] = \"C\"\ntrain[\"Embarked\"].iloc[289] = \"C\"","001af15f":"display(test[test[\"Fare\"].isnull()])\ndisplay(train[train[\"Fare\"].isnull()])","d38a1369":"fare_median = full_data.loc[full_data[\"Pclass\"]==3, \"Fare\"].dropna().median()","a8b3e1a0":"test[\"Fare\"].iloc[152] = fare_median\ndisplay(pd.DataFrame(test.iloc[152, :]))","4a27c25d":"display(full_data[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0].head(2))\ndisplay(pd.DataFrame(np.floor(np.log10(full_data[\"Fare\"].dropna()+1))).astype(int).head(10))\n\n# \ud2f0\uac9f\ud53c\ucc98\ub85c \uc9d1\uacc4\ud574\uc11c Name\uc744 \uac00\uc838\uc640\uc11c \uadf8\uac83\uc758 \uce74\uc6b4\ub97c \uad6c\ud558\uba74 \n# \uac19\uc740 \ud2f0\uac9f\ubc88\ud638\ub85c \uacf5\uc720\ub41c \uac2f\uc218\ub97c \uc54c\uc218\uc788\ub2e4.\ndisplay(full_data.groupby(\"Ticket\")[\"Name\"].transform('count'))","57fa6314":"full_data = pd.concat([train.drop(labels=[\"Survived\"], axis=1), test])\ndisplay(full_data.head(n=2))\n\nsurvived = train[\"Survived\"]\n\n# Child boolean\nfull_data[\"Child\"] = full_data[\"Age\"].apply(lambda x:1 if x <= 10 else 0)\nfull_data[\"Cabin_known\"] = full_data[\"Cabin\"].isnull() == False\nfull_data[\"Age_known\"] = full_data[\"Age\"].isnull() == False\nfull_data[\"Family\"] = full_data[\"SibSp\"] + full_data[\"Parch\"]\nfull_data[\"Alone\"] = full_data[\"Family\"].apply(lambda  x : 0 if x ==0 else 1)\nfull_data[\"Large_Family\"] = full_data[\"Family\"].apply(lambda x : 1 if x > 2 else 0 )\nfull_data[\"Deck\"] = full_data[\"Cabin\"].str[0]\nfull_data[\"Deck\"] = full_data[\"Deck\"].fillna(value=\"U\")\nfull_data[\"Ttype\"] = full_data[\"Ticket\"].str[0]\nfull_data[\"Title\"] = full_data[\"Name\"].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\nfull_data[\"Fare_cat\"] = pd.DataFrame(np.floor(np.log10(full_data[\"Fare\"]+1))).astype(int)\nfull_data[\"Bad_ticket\"] = full_data[\"Ttype\"].isin(['3', '4', '5', '6', '7', '8', 'A', 'L', 'W'])\nfull_data[\"Young\"] = (full_data[\"Age\"]<=30) | (full_data[\"Title\"].isin(['Master', 'Miss', \"Mlle\"]))\n#\ud2f0\uac9f\uc774 \uacf5\uc720\ub418\uc5c8\uc73c\uba74 1\uc544\ub2c8\uba74 0\nfull_data[\"Shared_ticket\"] = np.where(full_data.groupby(\"Ticket\")[\"Name\"].transform('count') > 1, 1, 0)\nfull_data[\"Ticket_group\"]  = full_data.groupby(\"Ticket\")[\"Name\"].transform('count') \nfull_data[\"Fare_eff\"] = full_data[\"Fare\"] \/ full_data[\"Ticket_group\"]\nfull_data[\"Fare_eff_cat\"] = np.where(full_data[\"Fare_eff\"]>16.0, 2, 1)\nfull_data[\"Fare_eff_cat\"] = np.where(full_data[\"Fare_eff\"] < 8.5, 0, full_data[\"Fare_eff_cat\"])\n\ntest = full_data.iloc[len(train):]\ntrain = full_data.iloc[:len(train)]\ntrain[\"Survived\"] = survived\n\nsurv = train[train[\"Survived\"]==1]\nnonsurv = train[train[\"Survived\"]==0]","a84c494c":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Child\", col=\"Pclass\", data = train, aspect=0.9, size=3.5, ci=95.0)\ntab = pd.crosstab(index=train[\"Child\"], columns=train[\"Pclass\"])\ndisplay(tab)\n\ntab = pd.crosstab(index=train[\"Child\"], columns=train[\"Sex\"])\ndisplay(tab)","e0820d3b":"cab = pd.crosstab(index=train[\"Cabin_known\"], columns = train[\"Survived\"])\ndisplay(cab)\n\ndummy = cab.div(cab.sum(axis = 1).astype(float), axis=0)\ndisplay(dummy)\n\ndummy.plot(kind='bar', stacked=True, alpha=0.95)\nplt.xlabel(\"Cabin Known\")\nplt.ylabel(\"Percentage\")","c5516687":"g = sns.factorplot(x=\"Sex\", y=\"Survived\", hue=\"Cabin_known\", col=\"Pclass\", data=train, aspect=0.9, size=3.5, ci=95.0)","752e23e7":"tab = pd.crosstab(index=train[\"Deck\"], columns=train[\"Survived\"])\ndisplay(tab)\ndummy = tab.div(tab.sum(axis =1).astype(float), axis = 0)\ndisplay(dummy)\ndummy.plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel(\"Deck\")\ndummy = plt.ylabel(\"Survival Ratio\")","1b408d12":"help(stats.binom_test)","8034699b":"stats.binom_test(x=12, n = 12 + 35, p=24\/ (24. + 35.))","d9cd0128":"g = sns.factorplot(x=\"Deck\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\", data=train, aspect=0.98, size=3.5, ci=95.0)","8379794c":"display(train[\"Ttype\"].unique())\ndisplay(test[\"Ttype\"].unique())","4d3d80c6":"tab = pd.crosstab(train[\"Ttype\"], train[\"Survived\"])\ndisplay(tab)\nsns.barplot(x=\"Ttype\", y=\"Survived\", data = train, ci=95.0, color=\"blue\")","62a855d8":"tab = pd.crosstab(train[\"Bad_ticket\"], train[\"Survived\"])\ndisplay(tab)\ng = sns.factorplot(x=\"Bad_ticket\", y=\"Survived\", hue=\"Sex\", col=\"Pclass\", data = train, aspect=0.9, size=3.5, ci=95.0)","8ffb86ce":"tab = pd.crosstab(train[\"Deck\"], train[\"Bad_ticket\"])\ndisplay(tab)\n\n\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked =True)\ndummy = plt.xlabel(\"Deck\")\ndummy = plt.ylabel(\"Percentage\")","68adbd51":"tab = pd.crosstab(train[\"Age_known\"], train[\"Survived\"])\ndisplay(tab)\ndummy = tab.div(tab.sum(axis = 1).astype(float), axis = 0)\ndisplay(dummy)\ndummy.plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel(\"Age known\")\ndummy = plt.ylabel(\"Survival ratio\")","3c0092af":"stats.binom_test(x=424,n=424+290,p=125\/(125.+52.))","20fa3d21":"g = sns.factorplot(x=\"Sex\", y=\"Age_known\", data=train, hue=\"Embarked\", col=\"Pclass\", aspect=0.9, size=3.5, ci=95.0)","1ea72e1f":"tab = pd.crosstab(train[\"Family\"], train[\"Survived\"])\ndisplay(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True)\ndummy = plt.xlabel(\"Family members\")\ndummy = plt.ylabel(\"Survival probability\")","c8ad4a34":"tab = pd.crosstab(index= train[\"Alone\"], columns=train[\"Survived\"])\ndisplay(tab)\nsns.barplot(x=\"Alone\", y=\"Survived\", data=train)","1efa3873":"g = sns.factorplot(x=\"Sex\", y=\"Alone\", hue=\"Embarked\", col=\"Pclass\", data=train, aspect=0.9, size=3.5, ci=95.0)","ef5318bc":"tab = pd.crosstab(index=train[\"Large_Family\"], columns=train[\"Survived\"])\ndisplay(tab)\nsns.barplot(\"Large_Family\", \"Survived\", data=train)","b3edc6f2":"#\uc5b4\ub290 \uac1d\uc2e4\uc5d0 \ub300\uac00\uc871\uc774 \ub9ce\uc774 \uc788\ub294\uac83\uc778\uac00?\ng = sns.factorplot(x=\"Sex\", y=\"Large_Family\", col=\"Pclass\", data=train, aspect=0.9, size=3.5, ci=95.0)","6e80836a":"tab = pd.crosstab(train[\"Shared_ticket\"], train[\"Survived\"])\ndisplay(tab)\nsns.barplot(x=\"Shared_ticket\", y=\"Survived\", data=train)","8c3ea4c6":"tab = pd.crosstab(train[\"Shared_ticket\"], train[\"Sex\"])\ndisplay(tab)\ng = sns.factorplot(x=\"Sex\", y=\"Shared_ticket\", hue=\"Embarked\", col=\"Pclass\", data=train, aspect=0.9, ci=95.0)","3f73f485":"tab= pd.crosstab(index=train[\"Shared_ticket\"], columns=train[\"Sex\"])\ndisplay(tab)\ng =sns.factorplot(x=\"Sex\", y=\"Shared_ticket\", data=train, hue=\"Embarked\")","77cd76f0":"display(full_data[\"Age\"].groupby(full_data[\"Title\"]).count())\ndisplay(fill_data[\"Age\"].groupby(full_data[\"Title\"]).mean())\ndisplay(full_data.groupby(\"Title\")[\"Age\"].count())\ndisplay(full_data.groupby(\"Title\")[\"Age\"].mean())\n\ndisplay(\"There are %d unique titles in total.\" %(len(full_data[\"Title\"].unique())))","974d3293":"cond_1 = full_data[\"Title\"].isin(['Mr', 'Miss', 'Mrs', 'Master'])\ndummy = full_data[cond_1]\ndisplay(dummy.head(2))\n\ndummy[\"Age\"].hist(by=dummy[\"Title\"], bins = np.arange(0, 81, 1))","8b9444c2":"cond_1 = full_data[\"Title\"].isin([\"Mr\", \"Miss\", \"Mrs\",\"Master\"])\ndummy = full_data[cond_1]\ndisplay(dummy.head(3))\n\ndummy[\"Age\"].hist(by=dummy[\"Title\"], bins = np.arange(0, 81, 1))","1c49ea82":"tab = pd.crosstab(index=train[\"Young\"], columns=train[\"Survived\"])\ndisplay(tab)\nsns.barplot(x=\"Young\", y=\"Survived\", data=train)","76ba5f4d":"tab = pd.crosstab(index=train[\"Young\"], columns=train[\"Pclass\"])\ndisplay(tab)\ng= sns.factorplot(x=\"Sex\", y=\"Young\", data=train, aspect=0.9, size=3.5, ci=95.0, col=\"Pclass\")","eaf77ddf":"surv_pclass1 = surv[\"Fare\"][surv[\"Pclass\"]==1].dropna()\nnosurv_pclass1 = nonsurv[\"Fare\"][nonsurv[\"Pclass\"]==1].dropna()\nsurv_pclass2 = surv[\"Fare\"][surv[\"Pclass\"]==2].dropna()\nnosurv_pclass2 = nonsurv[\"Fare\"][nonsurv[\"Pclass\"]==2].dropna()\nsurv_pclass3 = surv[\"Fare\"][surv[\"Pclass\"]==3].dropna()\nnosurv_pclass3 = nonsurv[\"Fare\"][nonsurv[\"Pclass\"]==3].dropna()\n\nplt.figure(figsize=(12, 10))\nplt.subplot(311)\nax1 = sns.distplot(np.log10(surv_pcalss1.values+1), kde=False, color=surv_col)\nax1 = sns.distplot(np.log10(nosurv_pclass1.values+1), kde=False, color=nonsurv_col, axlabel=\"Fare\")\nax1.set_xlim(0, np.max(np.log10(train[\"Fare\"].dropna().values+1)))\nax1.set_title(\"Pclass 1 vs Life or Death\")\n\nplt.subplot(312)\nax2 = sns.distplot(np.log10(surv_pcalss2.values+1), kde=False, color=surv_col)\nax2 = sns.distplot(np.log10(nosurv_pclass2.values+1), kde=False, color=nonsurv_col, axlabel=\"Fare\")\nax2.set_xlim(0, np.max(np.log10(train[\"Fare\"].dropna().values+1)))\nax2.set_title(\"Pclass 2 vs Life or Death\")\n\nplt.subplot(313)\nax3 = sns.distplot(np.log10(surv_pcalss3.values+1), kde=False, color=surv_col)\nax3 = sns.distplot(np.log10(nosurv_pclass3.values+1), kde=False, color=nonsurv_col, axlabel=\"Fare\")\nax3.set_xlim(0, np.max(np.log10(train[\"Fare\"].dropna().values+1)))\nax3.set_title(\"Pclass 3 vs Life or Death\")\n\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.35, wspace=0.35)","ea9d6971":"pd.DataFrame(np.floor(np.log10(train['Fare'] + 1))).astype('int').head(5)","bcb211db":"tab = pd.crosstab(train['Fare_cat'], train['Survived'])\nprint(tab)\nsns.barplot('Fare_cat', 'Survived', data=train)","9058e98d":"g = sns.factorplot(x=\"Sex\", y=\"Fare_cat\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","cab5fcca":"combine.groupby('Ticket')['Fare'].transform('std').hist()\nnp.sum(combine.groupby('Ticket')['Fare'].transform('std') > 0)","8f77e7f5":"combine.iloc[np.where(combine.groupby('Ticket')['Fare'].transform('std') > 0)]","12defbab":"plt.figure(figsize=[12,10])\nplt.subplot(311)\nax1 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==1].dropna().values+1), kde=False, color=surv_col)\nax1 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==1].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax1.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplot(312)\nax2 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==2].dropna().values+1), kde=False, color=surv_col)\nax2 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==2].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax2.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplot(313)\nax3 = sns.distplot(np.log10(surv['Fare_eff'][surv['Pclass']==3].dropna().values+1), kde=False, color=surv_col)\nax3 = sns.distplot(np.log10(nosurv['Fare_eff'][nosurv['Pclass']==3].dropna().values+1), kde=False, color=nosurv_col,axlabel='Fare')\nax3.set_xlim(0,np.max(np.log10(train['Fare_eff'].dropna().values+1)))\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)","53d7f231":"print(combine[combine['Fare']>1].groupby('Pclass')['Fare'].std())\nprint(combine[combine['Fare_eff']>1].groupby('Pclass')['Fare_eff'].std())","624ad9f9":"combine[(combine['Pclass']==1) & (combine['Fare_eff']>0) & (combine['Fare_eff']<10)]","be3526d4":"combine[(combine['Pclass']==3) & (np.log10(combine['Fare_eff'])>1.2)]","cf3069d8":"ax = sns.boxplot(x=\"Pclass\", y=\"Fare_eff\", hue=\"Survived\", data=train)\nax.set_yscale('log')\nax.hlines([8.5,16],-1,4, linestyles='dashed')","f7036d00":"tab = pd.crosstab(train['Fare_eff_cat'], train['Survived'])\nprint(tab)\nsns.barplot('Fare_eff_cat', 'Survived', data=train)","c2f2b894":"g = sns.factorplot(x=\"Sex\", y=\"Fare_eff_cat\", hue=\"Embarked\", col=\"Pclass\",\n                   data=train, aspect=0.9, size=3.5, ci=95.0)","f7841216":"combine = pd.concat([train.drop('Survived',1),test])\nsurvived = train['Survived']\n\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"category\")\ncombine[\"Sex\"].cat.categories = [0,1]\ncombine[\"Sex\"] = combine[\"Sex\"].astype(\"int\")\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"category\")\ncombine[\"Embarked\"].cat.categories = [0,1,2]\ncombine[\"Embarked\"] = combine[\"Embarked\"].astype(\"int\")\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"category\")\ncombine[\"Deck\"].cat.categories = [0,1,2,3,4,5,6,7,8]\ncombine[\"Deck\"] = combine[\"Deck\"].astype(\"int\")\n\ntest = combine.iloc[len(train):]\ntrain = combine.iloc[:len(train)]\ntrain['Survived'] = survived\n\ntrain.loc[:,[\"Sex\",\"Embarked\"]].head()","612f4b62":"ax = plt.subplots( figsize =( 12 , 10 ) )\nfoo = sns.heatmap(train.drop('PassengerId',axis=1).corr(), vmax=1.0, square=True, annot=True)","b570c29c":"training, testing = train_test_split(train, test_size=0.2, random_state=0)\nprint(\"Total sample size = %i; training sample size = %i, testing sample size = %i\"\\\n     %(train.shape[0],training.shape[0],testing.shape[0]))","80382af5":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Parch',\n        'SibSp','Young','Alone','Shared_ticket','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])","0c2c9bad":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = clf_log.score(X,y)\nprint(score_log)","3ee6025c":"pd.DataFrame(list(zip(X.columns, np.transpose(clf_log.coef_))))","3f483aa6":"cols = ['Sex','Pclass','Cabin_known','Large_Family','Shared_ticket','Young','Alone','Child']\ntcols = np.append(['Survived'],cols)\n\ndf = training.loc[:,tcols].dropna()\nX = df.loc[:,cols]\ny = np.ravel(df.loc[:,['Survived']])\n\ndf_test = testing.loc[:,tcols].dropna()\nX_test = df_test.loc[:,cols]\ny_test = np.ravel(df_test.loc[:,['Survived']])","637cd712":"clf_log = LogisticRegression()\nclf_log = clf_log.fit(X,y)\nscore_log = cross_val_score(clf_log, X, y, cv=5).mean()\nprint(score_log)","63541580":"clf_pctr = Perceptron(\n    class_weight='balanced'\n    )\nclf_pctr = clf_pctr.fit(X,y)\nscore_pctr = cross_val_score(clf_pctr, X, y, cv=5).mean()\nprint(score_pctr)","5e138ab3":"clf_knn = KNeighborsClassifier(\n    n_neighbors=10,\n    weights='distance'\n    )\nclf_knn = clf_knn.fit(X,y)\nscore_knn = cross_val_score(clf_knn, X, y, cv=5).mean()\nprint(score_knn)","3d1d841a":"clf_svm = svm.SVC(\n    class_weight='balanced'\n    )\nclf_svm.fit(X, y)\nscore_svm = cross_val_score(clf_svm, X, y, cv=5).mean()\nprint(score_svm)","c0cf1ae5":"clf_bay = GaussianNB()\nclf_bay.fit(X,y)\nscore_bay = cross_val_score(clf_bay, X, y, cv=5).mean()\nprint(score_bay)","23a035b6":"bagging = BaggingClassifier(\n    KNeighborsClassifier(\n        n_neighbors=2,\n        weights='distance'\n        ),\n    oob_score=True,\n    max_samples=0.5,\n    max_features=1.0\n    )\nclf_bag = bagging.fit(X,y)\nscore_bag = clf_bag.oob_score_\nprint(score_bag)","68d4711f":"clf_tree = tree.DecisionTreeClassifier(\n    #max_depth=3,\\\n    class_weight=\"balanced\",\\\n    min_weight_fraction_leaf=0.01\\\n    )\nclf_tree = clf_tree.fit(X,y)\nscore_tree = cross_val_score(clf_tree, X, y, cv=5).mean()\nprint(score_tree)","5f514f3b":"clf_rf = RandomForestClassifier(\n    n_estimators=1000, \\\n    max_depth=None, \\\n    min_samples_split=10 \\\n    #class_weight=\"balanced\", \\\n    #min_weight_fraction_leaf=0.02 \\\n    )\nclf_rf = clf_rf.fit(X,y)\nscore_rf = cross_val_score(clf_rf, X, y, cv=5).mean()\nprint(score_rf)","11e06398":"clf_ext = ExtraTreesClassifier(\n    max_features='auto',\n    bootstrap=True,\n    oob_score=True,\n    n_estimators=1000,\n    max_depth=None,\n    min_samples_split=10\n    #class_weight=\"balanced\",\n    #min_weight_fraction_leaf=0.02\n    )\nclf_ext = clf_ext.fit(X,y)\nscore_ext = cross_val_score(clf_ext, X, y, cv=5).mean()\nprint(score_ext)","3a55627f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nclf_gb = GradientBoostingClassifier(\n            #loss='exponential',\n            n_estimators=1000,\n            learning_rate=0.1,\n            max_depth=3,\n            subsample=0.5,\n            random_state=0).fit(X, y)\nclf_gb.fit(X,y)\nscore_gb = cross_val_score(clf_gb, X, y, cv=5).mean()\nprint(score_gb)","e4ef068c":"clf_ada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nclf_ada.fit(X,y)\nscore_ada = cross_val_score(clf_ada, X, y, cv=5).mean()\nprint(score_ada)","887f712a":"clf_xgb = xgb.XGBClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_xgb.fit(X,y)\nscore_xgb = cross_val_score(clf_xgb, X, y, cv=5).mean()\nprint(score_xgb)","b09c6b11":"clf_lgb = lgb.LGBMClassifier(\n    max_depth=2,\n    n_estimators=500,\n    subsample=0.5,\n    learning_rate=0.1\n    )\nclf_lgb.fit(X,y)\nscore_lgb = cross_val_score(clf_lgb, X, y, cv=5).mean()\nprint(score_lgb)","40491f67":"clf_ext = ExtraTreesClassifier(max_features='auto',bootstrap=True,oob_score=True)\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"],\n              \"min_samples_leaf\" : [1, 5, 10],\n              \"min_samples_split\" : [8, 10, 12],\n              \"n_estimators\": [20, 50, 100]}\ngs = GridSearchCV(estimator=clf_ext, param_grid=param_grid, scoring='accuracy', cv=3)\ngs = gs.fit(X,y)\nprint(gs.best_score_)\nprint(gs.best_params_)","eb66e23a":"clf_ext = ExtraTreesClassifier(\n    max_features='auto',\n    bootstrap=True,\n    oob_score=True,\n    criterion='gini',\n    min_samples_leaf=5,\n    min_samples_split=8,\n    n_estimators=50\n    )\nclf_ext = clf_ext.fit(X,y)\nscore_ext = clf_ext.score(X,y)\nprint(score_ext)\npd.DataFrame(list(zip(X.columns, np.transpose(clf_ext.feature_importances_))) \\\n            ).sort_values(1, ascending=False)","b07752fe":"# Taner's code\ndef show_confusion_matrix(cnf_matrix, class_labels):\n    plt.matshow(cnf_matrix,cmap=plt.cm.YlGn,alpha=0.7)\n    ax = plt.gca()\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks(range(0,len(class_labels)))\n    ax.set_xticklabels(class_labels,rotation=45)\n    ax.set_ylabel('Actual Label', fontsize=16, rotation=90)\n    ax.set_yticks(range(0,len(class_labels)))\n    ax.set_yticklabels(class_labels)\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n\n    for row in range(len(cnf_matrix)):\n        for col in range(len(cnf_matrix[row])):\n            ax.text(col, row, cnf_matrix[row][col], va='center', ha='center', fontsize=16)\n\n# sklearn example code\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = [\"Dead\", \"Alive\"]\ncnf_matrix = confusion_matrix(clf_ext.predict(X_test),y_test)\n\n# from: http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html\ndef show_confusion_matrix2(C,class_labels=['0','1']):\n    \"\"\"\n    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n    class_labels: list of strings, default simply labels 0 and 1.\n\n    Draws confusion matrix with associated metrics.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    \n    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n    \n    # true negative, false positive, etc...\n    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1];\n\n    NP = fn+tp # Num positive examples\n    NN = tn+fp # Num negative examples\n    N  = NP+NN\n\n    fig = plt.figure(figsize=(8,8))\n    ax  = fig.add_subplot(111)\n    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n\n    # Draw the grid boxes\n    ax.set_xlim(-0.5,2.5)\n    ax.set_ylim(2.5,-0.5)\n    ax.plot([-0.5,2.5],[0.5,0.5], '-k', lw=2)\n    ax.plot([-0.5,2.5],[1.5,1.5], '-k', lw=2)\n    ax.plot([0.5,0.5],[-0.5,2.5], '-k', lw=2)\n    ax.plot([1.5,1.5],[-0.5,2.5], '-k', lw=2)\n\n    # Set xlabels\n    ax.set_xlabel('Predicted Label', fontsize=16)\n    ax.set_xticks([0,1,2])\n    ax.set_xticklabels(class_labels + [''])\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.tick_top()\n    # These coordinate might require some tinkering. Ditto for y, below.\n    ax.xaxis.set_label_coords(0.34,1.06)\n\n    # Set ylabels\n    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n    ax.set_yticklabels(class_labels + [''],rotation=90)\n    ax.set_yticks([0,1,2])\n    ax.yaxis.set_label_coords(-0.09,0.65)\n\n\n    # Fill in initial metrics: tp, tn, etc...\n    ax.text(0,0,\n            'True Neg: %d\\n(Num Neg: %d)'%(tn,NN),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,1,\n            'False Neg: %d'%fn,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,0,\n            'False Pos: %d'%fp,\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    ax.text(1,1,\n            'True Pos: %d\\n(Num Pos: %d)'%(tp,NP),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    # Fill in secondary metrics: accuracy, true pos rate, etc...\n    ax.text(2,0,\n            'False Pos Rate: %.2f'%(fp \/ (fp+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,1,\n            'True Pos Rate: %.2f'%(tp \/ (tp+fn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(2,2,\n            'Accuracy: %.2f'%((tp+tn+0.)\/N),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(0,2,\n            'Neg Pre Val: %.2f'%(1-fn\/(fn+tn+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n    ax.text(1,2,\n            'Pos Pred Val: %.2f'%(tp\/(tp+fp+0.)),\n            va='center',\n            ha='center',\n            bbox=dict(fc='w',boxstyle='round,pad=1'))\n\n\n    plt.tight_layout()\n    plt.show()","2a17bfc2":"show_confusion_matrix(cnf_matrix,class_names)\n#show_confusion_matrix2(cnf_matrix,class_names)\n#plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n#                     title='Normalized confusion matrix')\n#sns.heatmap(cnf_matrix, annot=True)","7c5fc7b1":"clf = clf_ext\nscores = cross_val_score(clf, X, y, cv=5)\nprint(scores)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(scores),np.std(scores)))","dd5c3456":"score_ext_test = clf_ext.score(X_test,y_test)\nprint(score_ext_test)","2752df47":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Gradient Boosting', 'Bagging KNN', \n              'Decision Tree','XGBoost','LightGBM','ExtraTree','Perceptron', 'Naive Bayes'],\n    'Score': [score_svm, score_knn, score_log, score_rf, score_gb, score_bag,\n              score_tree,score_xgb,score_lgb,score_ext,score_pctr, score_bay]})\nmodels.sort_values(by='Score', ascending=False)","69548062":"summary = pd.DataFrame(list(zip(X.columns, \\\n    np.transpose(clf_tree.feature_importances_), \\\n    np.transpose(clf_rf.feature_importances_), \\\n    np.transpose(clf_ext.feature_importances_), \\\n    np.transpose(clf_gb.feature_importances_), \\\n    np.transpose(clf_ada.feature_importances_), \\\n    np.transpose(clf_xgb.feature_importances_), \\\n    np.transpose(clf_lgb.feature_importances_), \\\n    )), columns=['Feature','Tree','RF','Extra','GB','Ada','XGBoost','LightGBM'])\n  \nsummary['Median'] = summary.median(1)\nsummary.sort_values('Median', ascending=False)","1428a623":"clf_vote = VotingClassifier(\n    estimators=[\n        #('tree', clf_tree),\n        ('knn', clf_knn),\n        ('svm', clf_svm),\n        ('extra', clf_ext),\n       #('gb', clf_gb),\n        ('xgb', clf_xgb),\n        ('percep', clf_pctr),\n        ('logistic', clf_log),\n        #('RF', clf_rf),\n        ],\n    weights=[2,2,3,3,1,2],\n    voting='hard')\nclf_vote.fit(X,y)\n\nscores = cross_val_score(clf_vote, X, y, cv=5, scoring='accuracy')\nprint(\"Voting: Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n\n#for clf, label in zip(\n#    [clf_tree,clf_knn,clf_svm,clf_ext,clf_gb,clf_xgb,clf_pctr,clf_log,clf_rf,clf_bag,clf_vote],\n#    ['tree','knn','svm','extra','gb','xgb','percep','logistic','RF','Bag','Ensemble']):\n#    scores = cross_val_score(clf, X, y, cv=5, scoring='accuracy')\n#    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","5b2c7861":"# adjust these methods to my notation:\ntrain = X\n\n# training and train\/test split parameters\nntrain = train.shape[0]\nntest = test.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier; this basically unifies the way we call each classifier \nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)","0bcf4a94":"# function for out-of-fold prediction\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    # split data in NFOLDS training vs testing samples\n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        # select train and test sample\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        # train classifier on training sample\n        clf.train(x_tr, y_tr)\n        \n        # predict classifier for testing sample\n        oof_train[test_index] = clf.predict(x_te)\n        # predict classifier for original test sample\n        oof_test_skf[i, :] = clf.predict(x_test)\n    \n    # take the median of all NFOLD test sample predictions\n    # (changed from mean to preserve binary classification)\n    oof_test[:] = np.median(oof_test_skf,axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","873dd751":"# Put in our parameters for selected classifiers\n# Random Forest parameters\nrf_params = {\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","34da08c9":"# Create objects for each classifier\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=svm.SVC, seed=SEED, params=svc_params)","48593907":"# Create Numpy arrays of train, test and target dataframes to feed into our models\ny_train = y\ntrain = X\nfoo = test.loc[:,cols]\nx_train = train.values \nx_test = foo.values","f4c54734":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","a349d41b":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'SVM' : svc_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","98310176":"plt.figure(figsize=(12,10))\nfoo = sns.heatmap(base_predictions_train.corr(), vmax=1.0, square=True, annot=True)","2af35195":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","7fe332e5":"x_train","b4166cf3":"clf_stack = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n scale_pos_weight=1)\nclf_stack = clf_stack.fit(x_train, y_train)\nstack_pred = clf_stack.predict(x_test)","053d7c00":"scores = cross_val_score(clf_stack, x_train, y_train, cv=5)\nprint(scores)\nprint(\"Mean score = %.3f, Std deviation = %.3f\"%(np.mean(scores),np.std(scores)))","e3bab43d":"clf = clf_vote\ndf2 = test.loc[:,cols].fillna(method='pad')\nsurv_pred = clf.predict(df2)","2e3e7f52":"submit = pd.DataFrame({'PassengerId' : test.loc[:,'PassengerId'],\n                       #'Survived': surv_pred.T})\n                       'Survived': stack_pred.T})\nsubmit.to_csv(\"..\/working\/submit.csv\", index=False)\n#submit.to_csv(\"submit.csv\", index=False)","77526354":"submit.head()","788c8be5":"submit.shape","c40de586":"***Age* and *Embarked***","7296194b":"To simplify this broad distribution, we decide to classify the fares into *3 fare categories*: 0-10, 10-100, and above 100. This transformation can be easily achieved using the base 10 logarithm:","eaef9881":"The factorplot suggests that bad tickets are worse for male passengers, and 3rd class passengers. The individual significances are not overwhelming, but the trend itself might be useful.","6349150f":"*Perceptron:* This is a binary classifier that creates a linear decision boundary based on a (hyper-) plane in the parameter space.\n\n[Source](https:\/\/en.wikipedia.org\/wiki\/Perceptron)","7f0d1c9e":"As we would expect intuitively, it appears that we are more likely to know someones' age if the survived the disaster. There's a difference of about 30% vs 40% and it should be significant.","70285ce3":"Finally, we model a fare category, Fare_cat, as an ordinal integer variable based on the logarithm fare values.","f679d0ef":"Because of the larger number of \"Miss\" vs \"Master\" mostly women are classified as \"Young\".\nWe also recover the age difference between the ticker classed that was already obvous in earlier plots.Both factors mean that inpact of Young has to be studied carefully.","b8b0085f":"In addition, there is some variation between the 1st class male passengers, but it doesn't look overly significant.","02c49b20":"We want to make sure that our classifiers are not overfitting random data features. One of the most popular ways to check a model for robustness is called *cross validation*.\n\nIt's an approach similar to bootstrapping, where we use smaller samples from our data set to check whether the classifier gives similar results for each of them.\n\nFirst a simple cross-validation using the helper function *cross\\_val\\_score*. By default, the data is divided up into *k* equally sized sub-samples (or *folds*) and the classifier is trained on *k-1* of them and evaluated on the remaining one (e.g. for k = 4 we use 4 samples, leave each of them out once and train on the other 3, then evaluate on the one we've left out). This process is called *K-fold cross validation*.  The parameter *cv* here defines the*number* of folds (or alternatively something more complex as described in the [docs](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html) ).  The method used for computing the scores is by default the native scoring method of the classifier (but can be changed).\n\nMore background info [here](http:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html).\n\n*We've already used this cross-validation above to compute the scores for the individual classifiers.*","341a3f7d":"**We learn:**\n\n* A high percentage of those embarked at \"C\" were 1st class passengers.\n* Almost everyone who embarked at \"Q\" went to 3rd class (this means that the clear separation in the factorplot for \"Q\" isn't very meaningful, unfortunately).\n\nThe 2nd point is somewhat curious, since we recall from above that the survival chances for \"Q\" were actually slightly better than for \"S\".\n\nNot significantly so, of course, but certainly not worse even though \"S\" had a higher percentage of 1st and 2nd class passengers.\n\nIt seems that embarking at \"Q\" improved your chances for survival if you were a 3rd class passenger. Let's investigate that a bit more:","6eeaae85":"**Gradient Boosting:**","54e3e99e":"[Go to the top of the page](#top)","365edebc":"# 7. Modelling","370cc8f0":"<a id='relations'><\/a>","7e954673":"**Perceptron**","fb577587":"Based on the first look we define the input columns we'll be working with. We also create our training and testing feature sets.","d3b54a55":"#### Enhanced function for displot","724e2948":"# **1. Load Data and Modules**","7f298266":"# 5. Derived (engineered) features","b7a718de":"Together with the PassengerId which is just a running index and the indication whether this passenger survived(1) or not(0). We have the following for each person:\n\n* *Pclass* is the Ticket-class: first(1), second(2), and third(3) class tikcets were used. This is and ordinal integer feature.\n* *Name* is the name of the passenger. The names also contain titles and some persons might share the same surname; indicating family relations. We know that some titles can indicate a certain age group. For instance *Master* is a boy while *Mr* is a man. \nThis feature is a character strong of variable length but similiar format.\n* *Sex* is an indicator whether the passenger was femaleor male. This is a categorical text string feature.\n* *Age* is the integer age of the passenger. There are NaN values in this column.\n* *SibSp* is another ordinal integer feature describing the number of siblings or spouses travelling with each passenger.\n* *Parch* is another ordinal integer feature that gives the number of parents or children travelling with each passenger.\n* *Ticket* is a character string of variable length that gives the ticket number.\n* *Fare*   is a float feature showing how much each passenger paid for their rather memorably journey.\n* *Cabin* gives the cabin number of each passenger. There are NaN in this column. This is another string feature.\n* *Embarked* shows the prot of embarkation as a categorical character value.\n\nIn summary we have 1 floating point feature(Fare), 1 integer variable(Age), 3 Ordinal integer features(Pclass, SibSp, Parch), 2 Categorical text feature(Sex, Embarked), and 3 text string features(Ticket, Cabin, Name)","4e1b1487":"# 8. Preparing our prediction for submission","4807875e":"**Note:** \n\n\ub370\uc774\ud130 \ubd84\uc11d\uc744 \uc704\ud574 [Titanic Tutorials](https:\/\/www.kaggle.com\/c\/titanic#tutorials)\uc744 \ucd94\ucc9c\ud569\ub2c8\ub2e4\n\nR\uc744 \ud1b5\ud55c \ud0c0\uc774\ud0c0\ub2c9 \ub370\uc774\ud130\ubd84\uc11d\ub3c4 \uc2dc\ub3c4\ud574\ubcfc\ub9cc \ud569\ub2c8\ub2e4.\n\n[R kernel for Titanic](https:\/\/www.kaggle.com\/headsortails\/tidy-titarnic\/)\n\n[titanic in wikepedia](https:\/\/en.wikipedia.org\/wiki\/Sinking_of_the_Titanic)","3289bcc9":"<a id='submit'><\/a>","01d8b291":"**Finally**, we pick our favourite classifier and **predict** the expected survival for the passengers in the *test* data set. The result is **written to a submission file** according to the competition rules (418 rows; only include the columns *PassengerId* and *Survived*). ","4d34e54a":"A 60-yr old 3rd class passenger without family on board. We will base our *Fare* prediction on the median of the 3rd-class fares:\n\nA 60-yr old 3rd class passenger without family on baord, We will base our **Fare** prediction on the median of the 3rd-class fares.","fa8d6378":"Let's investigate the *Fare affair* in more detail. First, we make sure that the passengers in each group really had the same *Fare* values:","b03204f1":"*eXtreme Gradient Boosting:* It's not just a good name for a band, but XGBoost was also the flavour of the month tool for kaggle competitions in 2016.","441d7945":"**Ada Boost:**","54cb76c6":"* Library Load for visualization","c19a5e19":"## Ticket numbers","37a90504":"**Logistic Regression** again, this time with only the selected columns","9c481926":"<a id='derived'><\/a>","2fc6ad33":"## *Splitting the train sample into two sub-samples: training and testing*\n\nThis is best practice for evaluating the performance of our models, which should not be tested on the same data they are trained on. This avoids overfitting.","eb77005f":"But more men were travelling alone than women did. Especially among the 3rd class passengers.Alson this feature should be evaluated in our modeling step, to see if it's still significant of the **Sex feature**.","0447742b":"### *Fare\\_eff\\_cat*","e0f183b2":"**Support Vector Machine:**","ef50ef98":"**We learn:**\n\n* There is a broad distribution between the 1st class passenger fares( rich -> super rich)\n* There's an intersting bimodality in the 2nd class cabins and a long tail in the 3rd class ones.(*TODO: check cumulative fare question*)\n* For each class there is strong evidene that the cheaper cabins were worse for survival. A similiar effect can be seen in a *boxplot:*","ab50624a":"For additional insight we compare the *feature\\_importance* output of all the classifiers for which it exists:","f5ffa551":"What can we learn from the title in the passenger names? These could give us a direct, independent way to estimate missing age values, so let's look at all the availabel title, their frequency, and mean age. For this, we look at the combined data to make sure that we don't miss any titles that might be in train or test only.","786fd036":"Sort of, yes. This goes some way to explain features like better survival for SibSp = 1-3.\nBut I think that is doesn't xover all the signal in the Parch feature.\n\n**We learn:**\n\n* Different percentages of passenger classes and sexes have embarked from different ports, whick is reflected in the lower survival rates for \"S\" - Southampton(more mean, fewer 1st class) compared to \"C\" (more women and 1st class).\n* It's hard to say at this stage whether there is any real impace left for the *Embarked* feature once we correct for these connections. We will come back to this in the modelling state when we will study feature importances and significances(soon).\n\nFinally, lets' check what's going on between *Age* and *Embarked*:","e01e8f5c":"We might even be at a stage now where we can investigate the few outliers more in detail:","8ed78669":"[Go to the top of the page](#top)","b798b177":"And that's quite expensive for a 3rd class ticket. Maybe these two actually shared a ticket \/ cabin and we have another transcription \/ data entry error? The ticket numbers are very similar and someone could easily write \"303\" instead of \"304\". Will we ever know? Maybe not. Does it matter much? Probably not.\n\nMore importantly, there is a reasonable argument to be made for this new *Fare_eff* feature to represent the actual fare better than the original feature. For once, it splits much cleaner between the *Pclasses*:","ab8dd67d":"Final validation with the testing data set:","10e7cf62":"But most large families were travelling in 3rd class. The tentative imbalance between male and female 3rd class probably refelct the observation wer mad earlier that men were more likely to travel alone.","72ee6df1":"Now **that** is interesting. We see that the distributions become significantly narrower and that the tails and bimodality become much weaker (after getting rid of the zero-fare values for both groups). The really expensive *Fares* in *Pclass == 1* are pretty much all gone. Here's how the standard deviations compare:","899e6ee1":"That seems to be a hopeless variable at first because it just looks like random strings. But in these days, when you were travelling as a group\/family did everyone really get their own ticket?\n\nLet's find out how many unique ticket numbers there are:","0e1c749e":"[*Naive Bayes*](http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html) is a rapid classification method. It uses the famous [Bayes Theorem](https:\/\/en.wikipedia.org\/wiki\/Bayes%27_theorem) under the 'naive' assumption that all predictor features are independent from each other (and only related to the target variable).\n\nDespite this oversimplification Naive Bayes classifiers are performing well in many cases. In addition, they are fast to compute and only require relatively little data to perform well.","68d1abb7":"### *Missing values* \n* Missing values is matters - \uacb0\uce21\uce58.","0964666c":"Stacking of classifiers that have less correlation gives better results. Intuitively, classifiers that are highly correlated, like *ExtraTrees* and *GradientBoost* above, are already so similar that stacking doesn't change the result in a significant way. This is reflected in the relatively low correlation index of the SVM with everything else.\n\nTherefore, it would be more useful to replace the predominantly tree-based sample of classifiers with a more diverse set. ","053041bb":"*Gradient boosting:* This is what we call the step-by-step improvement of a weak classifier (like a tree with only 1 node) by successively applying this classifier to the residuals of the previous classifier's results. \n\nFor example: we fit a tree, determine its results (prediction: survived vs not survived), compute the residuals of this prediction vs the real survival numbers (all in the training data, of course), and then fit another tree to these residuals. This tree can now consider the full number of training samples for splitting a node at another feature, instead of having to deal with the decreased sample after the first original node (and the resulting impact of random fluctuations). This can be done again and again for n_estimator number of times.\n\nThe weak classifier itself does not necessarily have to be a tree, but a tree seems to be the favourite approach to use here. Another convention is to initialise this sequence of models with a single prediction value (like the mean of the training survival values).\n\nInstead of reducing the residuals (and the corresponding squared errors) Gradient Boosting focusses on minimising the *Loss Function* by training the classifier on the *gradient* of this function. The Loss Function describes how much the prediction is improved when shifting the predicted values by a certain amount. The method of *Gradient Decent* uses this Loss Function to iteratively move into the direction of its greatest decent (i.e. most negative first derivative). The step sizes can vary from iteration to iteration.\n\nAn additional concept is *Shrinkage*. Here, the size of each step multiplied by a factor (0,1]. In the model parameters, this factor is called the *learning_rate*. Lower learning rates make for a slower decent which seems to be empirically more effective. \n\nOne more step is to provide a sampling of rows and features, like in the random forest discussed above, to increase the diversity in tree splits and thereby a larger amount of information for the method to work with.\n\nThe important parameters are:\n\n- n_estimators: number of boosting stages; more is better\n\n- learning_rate: smaller steps need more stages\n\n- max_depth: tune for best performance; depends on interaction of features\n\n- subsample: only train on a sub sample of the data set drawn without replacement. This is called *Stochastic Gradient Decent*\n\n[Source 1](http:\/\/blog.kaggle.com\/2017\/01\/23\/a-kaggle-master-explains-gradient-boosting\/)\n\n[Source 2](<http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#gradient-tree-boosting)\n\n\n\n\nIn addition: This is the only instance where we import a module right when it's needed instead of up top. Normally, I would recommend not to ignore warnings but to fix what's causing them. However, here we get 1 warning per n_estimators from a depreciation warning in the inner workings of the classifier, over which we have no control. Therefore: ignore.","0d391fc8":"*Bagging* is a general ensemble method. This means it's a way to average over a (large) number of individual classifiers to improve their accuracy by reducing the variance (= noise). The estimator (above it's a KNN) is used multiple times on *subsets* of the training sample and then it uses the average vote.\n\nBagging for a decision tree classifier should be the same as using a *Random Forest* (see below).\n\nStrictly speaking, bagging is only the correct term if the sub samples are drawn with *replacement* (i.e. put back into the bag, I suppose). Otherwise it's called *Pasting*.\n\nIf sub-samples are used then the remaining samples (the ones not in the bag we're drawing the data from) can be used in *out-of-bag (oob)* estimates (-> *oob\\_score=True*). This is a kind of inbuilt cross-validation step, since the accuracy (score) of the classifier is estimated on data it wasn't trained on.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#bagging-meta-estimator)","14e5c729":"The min\/max values for Pclass, Age, SibSp and Parch shows us the range for these features. Also we see that there's quite a range in fares.","ff2748fc":"Knowing about missing values is matters cause they indicate how much we don't know about out data.\n\nMaking inference based on just a few causes is often leads to unwise conclusion. In addition to that, many ML modeling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or replaced missing values into some meaningfule value.\n\nWe learn:\n\n* In the **training data** a large majority of *Cabin* numbers are missing, together with 177 *Age* values and 2 *Embarked* values.\n* Also, in the **Test data** there is one *Fare* missing, almost 100 *Age* values are unknown and only 91 *Cabin* numbers were preserved.\n\nBest to keep that in mind.","0f2f95f9":"* **Mosaic Plot**","c0349a46":"Let's have a look at the ticket numbers and see whether we can extract some additional deck information from them. Above, we created a new feature called *Ttype* whick defines the type of a ticket through the first digit of the ticket number.","5ea9b6a7":"## *Stacking \/ Ensemble methods*","563b5356":"*Extremely Randomised Trees* is an ensemble classifier similar to random forests. An additional randomness is introduced by selecting random thresholds for each feature and using the best-performing threshold.\n\nHere we also use an \"Out-of-bag score\" (*oob\\_score = True*). This means that we grow our trees from a sub-sample of the training sample (using bootstrapping: *boostrap = True*) and estimate the accuracy based on those entries that were not picked (i.e. \"left out of the bag\"). This gives us a better impression how robust our results are towards generalisation, i.e. how well the classifier that was trained on a particular sample can be applied to new data. \n\nBecause this is ultimately our goal: to apply the classification method we \"learn\" from the training data to any data (in particular the one that is used to judge this competition). There is little use in having a classifier that replicates perfectly the training data by following every random noise feature in that data (called *overfitting*) but doesn't perform well with new data.\n\nThe principles of *bootstrapping* and the *out-of-bag score* can be applied to most classifiers and we already used them in the *bagging* classifier above. Here we just focus a bit on the underlying idea.","9ed50763":"## *Ranking of models and features*","a162c734":"**Extremely Randomised Trees**","5afc1e01":"**LightGBM:**","47c3c277":"After inspecting the available features individually you might have realized that some of them are likely to be connected. Does the age-dependent survival change with sex?\n\nHow are *Pclass* and *Fare* related? Are they strongly enough connected so that one of them is superfluous? Let's find out.\n\nNow we are connecting individual clues to get a glimpse of the bigger picture.\n\nWe start with an **overview plot of the feature relations:** Here we show a *correlation matrix* for each numerical variable with all the other numerical variables.\n\nWe excluded *PassengerID*, which is mereley a row index. In the plot, stronger correlations have brighter colours in either red(positive correlation) or blue(negative correlation). The closer to white a colour is the weak the correlation.","fb90e873":"**Bagging:**","25d02ece":"But again the sharing of tickets is more frequent with females and 1st class passengers. This is consistent with the other statistics that show that that women were more ikely to travel together families.\n\n**We learn:** Several of these derived parameters are strongly correlated with **Sex and Pclass**. Whether there is actual signal in them that a model can use to imporve the learnng accuracy needs to be investigated.","16fe3d3f":"**Naive Bayes**","eaddb770":"Interesting. Sharing a ticket number is not uncommon. Let's follow that up a bit.","ddb9d89f":"# 6. Preparing for modelling","63e54699":"Admittedly, these are quite a few grouping levels, but 30(\"C\") vs 20(\"S\") are numbers that are still large enough to be useful in this context. \n\nIn addition, already a grouping whithout the *Parch and SibSp* features suggests similiar numbers for women in 1st class embarking from \"C\"(71) vs \"S\"(69) (in contrast to the larger overall number of all 1st class passengers leaving from \"S\").\n\nAnother recent kernel ([definitely worth checking out](https:\/\/www.kaggle.com\/varimp\/a-mostly-tidyverse-tour-of-the-titanic) makes a convincing case for predicting Embarked == \"S\" for these two passsengers (see also the comments). However, in my opinion we have better reasons to impute \"C\" instead. I recommended that you weight the arguments and make your own decision.\n\n(*How much does it actually matter? Well, in the big picture these are only 2 passengers and their impact on our model accuracy won't be large.However, since the main point of this challenge is to practice data analysis it is certainly worth to take your time to examine the question in a bit more detail.*)","fd4aef3e":"Just abount formally significant(i.e. < 5%). It might be worth our while to include this feature in at least the initial stages of modeling to see how it perfomrs.","0954d530":"At face value, some classifiers perform better than others. However, the differences between the methods are relatively small and more likely due to more or less over-fitting than anything else. (Except, possibly, for the Perceptron. There a bit more tuning might be appropriate.)","b788268c":"A little follow up: **SibSp** we wee in the plot that most of the differences are not very significant (overlapping erros bars).\n\nAnother way of checking the actual numbers are through **cross tables.**","20c87dc7":"* Library for Evaluation, train and Fit","f8cc845c":"OK, now from here it looks more like \"S\" is the interesting port since survival chances is less probably for that one if you are a 3rd class passengers.\n\nOtherwise to be some impact here that isn't captured by the passenger class.\nWhat about the other strong feature, Sex?","03cc4178":"*Positive vs negative correlation* needs to be understood in terms of whether an increase in one feature leads to an increase (positive) or decrease(negative) in the correlated feature.\n\nPerfect correlation would have an correlation index 0f 1; perfect anti-correlation(=negative correlation) would have -1(obviousyl each feature is perfectly correlated with itself; leading to the deep red diagonal). The upper right vs lower left triangle that make up this plot contain the same information, since the corresponding cells show the correlation coefficients of the same features.\n\nThe matrix gives us an overview as to which features are particularly interesting for our analysis. Both strongly or negative correlations with the *Survived* feature are valuable.\nStorng correlations between two other features would suggest that only one of them is necessary for out model(and including the other would in fzct induce noise and potentiallt lead to over-fitting).\n\n**We learn:**\n\n* *Pclass* is somewhat correlated with *Fare*(1st class tickets would be more expensive than 3rd class ones)\n* *SibSp* and *Parch* are weakly correlated(large families would have high values for both; solo travellers would have zero for both)\n* *Pclass* already correlates with *Survived* in a noticeable way\n\nIn addition, we plot a **Pairplot** of the numerical features. This kind of plot is a more detailed visualization of relationships between variables.\nIt shows scatter plots for the different feature combinations plus a distribution of each feature on the diagonal.\n\nAgain, the upper right and lower left triangle contain the same information. This kind of plot is vastly more useful for a set of continuous variables, insetead of the categorical or integer values we have here. Nonetheless, it is a valuable exploratory tool that has a place in everyones; toolbox.\n\nThis plot is inspired by, and relaized much more aesthetically in, the [comprehensive Ensemble Stacking Kernel by Anisotropic](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)   ","77496fe0":"### *Large\\_Family*","6beba96c":"**Load input data.** ","ed499cd7":"The \"+1\" means that our boundaries are slightly shifted in terms of the \"real\" *Fare*. However, this shift avoids computing issues for the zero-fare passengers and it makes little difference for our understanding of the fare groups. In fact, in the plot above the offset had already been applied as well.\n\nAt the start of this section we define a new feature, *Fare\\_cat*, as fare categories in the same way. Let's try it out:","981dc420":"<a id='missing'><\/a>","894f69f5":"**We can try out:**\n* I suppose one could take the starting letters, which might indicate specific decks on the ship, and use them as an additional classifier. But for less than 25% of cabins known this might not be very useful.We'll see.\n\n* Also, given that so few cabin numbers are known it doesn't seem that there are good records of them. Therefore, one should assume that it's more likely to know someone's cabin number if they survived. Let's see about that in the derived features.\n\n*TODO: Why do some people have multiple cabin numbers? What does that mean?*","b302a94f":"Based on this plot we define a new feature called **Bad_ticket** under which we collect all the ticket numbers that start with digits which suggest less than 25% survival(e.g. 4, 5 or A). We are aware that some of the survival fractions we see above are based on small numbers statistics (e.g. 2 vs 0 for 8). It is well possible that some of our \"bad tickets\" are merely statistical fluctuations from the base survival rate of 38%.\nThe barplot shows mean survival fractions and the associcated 95% confidence limits , which are large for the sparse samples.\n\nHowever. the significant differences between e.g. **1 and 3**(based on large enough numbers) suggested that this new feature could still contain some useful information.\nI think that without external information, which we are avoiding in this notebook, we can't do much better in trying to tie the ticket number to the survival statistics.\n\nOf course, it's not the tikets themselves thart are \"bad\" for survival, but the possibility that the tikcet numbers might encode certain areas of the ship that would have led to higher or lower survial changes.\n\nIn Korean, we  call it **\uc774\uc0c1\uce58 \uc81c\uac70 - removal of outlier**.","9ab6fe7e":"*Decision Tree:* One of the classifiers that's easiest to visualise. Each tree is a series of if-then-else decisions. Example: *if* sex ==  male *then* go left *else* go right. Here, *left* and *right* defines a split at a so called *node* - the decision itself. The first split can be followed up by additional ones to narrow down the decision criteria (based on the subset defined by each previous split).\n\nOne visualisation of this process is a tree trunk *branching off* into successively smaller structures. Hence: decision *tree*. Consequently, the result of the final splits are called *leaf notes* - on a tree, it doesn't get smaller than leafs.\n\nAdvantages of decision trees are that they can deal with both numerical and categorical data, are able to handle multi-output problems, and are easy to follow and interpret.\n\nDisadvantages include:\n\n- Problem: A tendency to overfitting. Solution: pruning, setting maximum depth, or PCA beforehand to find the right number of features. Visualising the tree helps to understand how well it is fitting the data.\n\n- Problem: Unstable to small variations in the data. Solution: ensembles.\n\n- Problem: Creating biased trees if some classes dominate. Solution: balance the data set by either sampling the same number of samples from each class or by adjusting the *sample_weight* parameter to normalise the sum of the class weights to the same value. Following that, parameter *min_weight_fraction_leaf* is less biased towards dominating classes.\n\n- Problem: Being just not easy to fit to certain concepts that don't lend themselves to clear yes-or-no decisions. Solution: Use a different classifier.\n\nAdditional notes:\n\n- Parameters *min_samples_split* and *min_samples_leaf* control the number of samples at a leaf note. min_samples_leaf=5 is a useful initial value. A small number will lead to overfitting, a large number prevents learning.\n\n- For sparse X convert to sparse *csc_matrix* to speed up the learning\n\nAll of the information above is digested from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/modules\/tree.html)","6ea60dcd":"* Library import for Machine Learning","6ebfdd5c":"TODO: Say something about the contributions and follow up with some ANOVA-like analysis","768cb336":"Above we are creating a kind of summary dashboard, where we collect relevant visualizations to study the distributions of the individual features.\nWe use the matplotlib *Subpot* tool to line up the individual plots in a grid.\n\nWe use overlapping histograms for ordinal features and barplots for categorical features. The barplots show the fraction of people(per group) who survived. There's a lot going on in this figure, so take your time to look at all the details.\n\n**We learn** the following things from studying individual features:\n\n* *Age*: The medians are identical.However, it's noticeable that fewer young adults have survived(ages 18-30-ish) whereas **Children younger than 1-ish had a better survival ratio**. Also, there are no obvious outliers that would indicate problematic input data. The highest ages are well consistent with the overall distribution. \nThere is a noteable shortage of teenagers compared to the crows of younger kids. But this could have natural reasons.\n\n* *Pclass*: There's a clear trend that **being 1st class passenger gives you better changes of survival.**Life just isn't fair. Money and social connection matters in case if emergency**\n\n* *SibSp & Parch*: **Having 1-3 siblings\/spouses\/parents\/children on board(SibSp = 1-2, Parch = 1-3) suggests propositionally better survival numbers that being alone(SibSp + Parch = 0) or having a large family travelling with you**.\n\n* *Embarked*: Well, that does look more interesting than expected. **Embarking at \"C\" resulted in a ligher survial rate that embarking at \"S\"**. There might be a correlation with other variables, here though.\n\n* *Fare:* This is case where a linear scaling isn't of much help because there is smaller number of more extreme numbers. A natural choice in this case is to transform the values logarithmically. For this to work we need to adjust for the zero-fare entries. The plot tell us that the **Survival chances were much lower for the cheaper cabins.**\nNavely, one would assume that those cheap cabins were mostly located deeper inside the ship, i.e. Further away from the life boats.","d83f4969":"### *Deck*","85f54092":"Now let's study the new features and see how they relate to the survival chances:","11a94bab":"<a id='encode'><\/a>","68fd891c":"<a id='model'><\/a>","a5144253":"Let's study the relation between *Fare* and *Pclass* in more detail:","7616d06c":"* Age\ud53c\ucc98\uc758 \ub110\uac12\uc744 \uc81c\uc678\ud558\uace0 historgram\uc640 Density\ubcf4\uae30","0aafce25":"Following a suggestion by [Taner](https:\/\/www.kaggle.com\/kiralt) in the comments we also use a *Confusion Matrix* to evaluate the performance of our classifier. A confusion matrix contains more information than a simple score because it shows how many data points of each class were correctly\/incorrectly classified. It's like a correlation matrix, in a sense. A plot will explain it better than 1000 words. First we define some plotting function; then we plot.","d9b2c0bb":"*AdaBoost*: A boosting classifier that fits sequences of weak learners that are progressively weighted toward those features that the previous weak learners misclassified.","cf41778f":"<a id='explore'><\/a>","126410b6":"* [numpy.log10](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.log10.html)\n* [Codetorial](https:\/\/codetorial.net\/numpy\/functions\/numpy_log10.html)","f1c48919":"The easiest method to combine different classifiers is through a **Voting Classifier**. It does exactly what the name suggests: each individual classifier makes a certain prediction and then the *majority vote* is used for each row. This majority process can either give all individual votes the same importance or assign different weights to make some classifiers have more impact than others.\n\nVoting can be more powerful when used with weights, so that several weaker classifiers can only successfully vote against one\/two stronger ones if they consistently agree on a specific prediction. This is expected to increase the accuracy of the final prediction. Read more in the extensive [Kaggle Ensemble Guide](https:\/\/mlwave.com\/kaggle-ensembling-guide\/).\n\nBelow, we decide to assign different, somewhat arbitrary weight according to how we think each classifier performs. ","8c04255b":"### *Family*","c2e98115":"* **SibSp - Sibling and Parents**","3c6cfe99":"Travelling alone appreas bar enough to be significant.","9d2bbc9b":"Before we start exploring the different models we are modifying the categorical string column types to integer. This is necessary since not all classifiers can handle string input.","a10323ef":"**Look at your data in as many different ways as possible** \n\n\nSome properties and connections will be immediately obvious. Others will require you to examine the data, or parts of it, in more specific ways. Metaphorically speaking: This is the part where the detective finds the clues.\n\nAs you know that , CBS Drama NCSI agents screen and evalute all of the details in crime scene as far as possilbe they could.\nData Exploration process are the same as NCSI agents do.\n\nFirst a broad overview. What are the types of data and their typical shape and content?\n\n\n\ucca0\ud559\uc790\uac00 \uc628\ud798\uc744 \ub2e4\ud574 \uc0ac\uc720\ud558\ub294 \uac83\uacfc \ud0d0\uc815\uc774 \ubc94\uc8c4\ud604\uc7a5\uc5d0\uc11c \uc870\uac01 \ud558\ub098\uc5d0 \uc9d1\uc911\ud558\ub294 \uac83\uc740 \ub3d9\uc77c\ud55c \uc0ac\uc720\uc758 \uacfc\uc815\uc774\ub2e4.","26070130":"[Go to the top of the page](#top)","e9021b66":"In the next step, we will try to incorporate the information from the great [Introduction to Ensembling\/Stacking in Python by Anisotropic](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) into our script.\n\nWe start out by copying the relevant parts of the script verbatim (standing on the shoulders of giants, and so on ...) and making it run in our environment. Afterwards, we will try to gradually adapt and simplify the approach, to make use of the work we have already done above for all the individual classifiers. Hopefully, this will result in a better understanding of stacking.\n\n*If you want a step by step overview then have a good look at [Anisotropic's Kernel](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) and the references therein. Seriously, you should check it out. It's great.*","29179077":"## *Model validation*","b6f7390a":"## Outline:\n\n1. [Load Data and Modules](#load) (complete)\n1. [Initial Exploration](#explore) (complete)\n1. [Relations between features](#relations) (complete)\n1. [Missing values](#missing) (complete)\n1. [Derived (engineered) features](#derived) (largely complete)\n1. [Preparing for modelling](#encode) (complete)\n1. [Modelling](#model) (medium completeness; to be extended)\n1. [Preparing our prediction for submission](#submit) (complete)","627e9e66":"### *Child*","a0cb71ae":"Sharing a ticket appreas to be goof for survival","5747d4db":"After studying the relations between the different features let's fill in a few missing values based on what we learned.\nIn my opinion, the only training feature for which it make sense to fill in the NAs is *Embarked*. Too many *Cabin* numbers are missing. And for *Age* we will choose a different approach below. We fill in the 1 missing *Fare* value in the test data frame accordingly.\n\nLet's find the two passengers and assign the most likely port based on what we found so far:","9d51a2d2":"* Find the record","3130baae":"Now we continue to examine these initial indications in more detail.\nEarlier, we had look at the *Survived statistics* of the individual features in the overview figure. Here, we want to look at correlations between ihe predictor features and how they could affect the target *Survived behaviour*.\n\nUsually, it's most interesting to start with the strong signals in the correlation plot and to examine them more detail.","c389e269":"* **Factoplot**\n\nYou can view the **FacetGrid** tutorial [here](https:\/\/seaborn.pydata.org\/generated\/seaborn.FacetGrid.html).","427bec21":"Our \"usual\" factorplot examination highlights the differences between *Pclass* (as expected) but also shows some interesting variations within the *Sex* feature. This might be related to the fact that women were more likely to share a cabin, and it would therefore indicate that the *Fare* might be a fare per cabin and not per passenger.","7b15c2b0":"This is a tricky feature there are so many missing values and the strings don't all have the same number of formatting.","af9b8c1f":"As far as I can see, there's still quite a bit of variation here.","2135a512":"It actually is. Turns out that we are more likely to know the age of higher class passengers or women, which are the strongest survival predictiors we have found. so far.(Of course, the casualty might as well go to other way, but that's not really the question here. What we want to find are the best predictors for survival.)\n\n**We learn:** There is a strong impact of *Sex and Pclass* on this new feature. This might be enough to explain all the variance in the *Age_known* variable. We should test the predictive power in our modeling.","c1a6ebc0":"As expected, *Pclass* and *Sex* have the most impact, but our engineered features are doing not bad either.","2360d38d":"## *Run and describe several different classifiers*","1fcb244f":"## *Examining\/Optimising one classifier in more detail:*","424818b9":"That's really cheap for a 1st class cabin. Maybe a transcription error in the data itself?","f7953446":"It's Mr Osen and Mr Gustafsson on Ticket 7534. Their *Fares* are close enough, though, to include them in the general treatment.\n\nNow, let's think for a moment: Identical fares could mean that the fare for a cabin was shared equally among the passengers, in which case our previous treatment would have been justified. However, it *could* also mean that the listed value is the *cumulative fare per cabin* and it was simply recorded as the same value for each passenger. Intuitively, this doesn't seem so plausible, since you typically record what is paid for a ticket and not for a cabin. But let's investigate this for a moment and check how it would transform the *Fare* distribution. For this, we create a *Fare_eff* feature above, which we derive by dividing *Fare* by the number of people sharing a ticket (*Ticket_group*; which we also newly created).","823dc3fc":"Anything above 0.05 is usually not significant and therefore solely based on these numbers we cannot say whether the SibSp = 5 sample behaves than the rest.\n\nFor larger numbers of Parch we have 4 vs 0, 4 vs 1, and 1 vs 0. Just by themselves, the last two are definitely not impressive. Combining them into Parch >= 4 gives us 9 vs 1 which is much better.\n\n**We learn:** parch >= 4 and SibSp >= 3 is bad. So is parch + SibSp = 0(i.e both 0). Parch in 1-3 and SibSp in 1-2 is good.\nAnything above 0.05 is usually not significant and therefore solely based on these numbers we cannot say whethere the SibSp = 5 sample behaves different thatn the rest.\n\nFor larger numbers of Parch we have 4 vs 0, 4 vs 1.","0008f7a4":"# 4. Filling in missing values","86f364a1":"### *Shared\\_ticket*","757e5f1b":"[Basic data type](http:\/\/blog.heartcount.io\/dd)","36329897":"We designed a number of new features, and unsurprisingly several of those are correlated with the original features we used to create them. For instance *Fare\\_cat* and *Fare*. Or *Family* and *SibSp\/Parch*. In the modelling step, we will first determine which of the features carry the most signal (*to be done*) and then use them to train a number of different classifiers.","6c4b8513":"Let's summarise briefly what we found in our data exploration:\n\n- sex and ticket class are the main factors\n\n- there seem to be additional impacts from:\n    - age: young men vs young women; (male) children\n    - relatives: parch = 1-3, sibsp = 1-2 (somewhat explained by sex but not completely)\n    - maybe the cabin deck, but not many are known\n\n- other apparent effects appear to be strongly connected to the sex\/class features:\n    - port of embarkation\n    - fare\n    - sharing a ticket\n    - large family\n    - travelling alone\n    - known cabin number\n    - known age","5c031fe7":"With these optimised parameters let's have a look at the feature importance that this classifier gives us:","715d93fe":"The file *submit.csv* will now appear in the *Output* tab of this kernel. From there you can download it and submit it by going to \"Leaderboard\" -> \"Submit Predictions\" in the tab list below the competition header. \n\nI recommend to briefly describe the details of your submission (e.g. which classifier, which meta-parameters, ...) in the corresponding text field, so that you remember the model for this score and don't have to re-submit something that you had done already.\n\n&nbsp;\n\n*Best of success and enjoy learning!*","e856231b":"In the same way, having a large family appears to be not good for survival.","57008c1f":"..there were more males among the 3rd class passengers. Possibly travelling alone?","a34960e5":"The *feature importance* tells us how much impact an individual feature has on the decisions within the classifier. Alongside the individual features we also compute a *median* importance.\n\nThe overall result is not very surprising: *Sex* and *Pclass* are the dominant features while everything else is of similar, significantly lower importance.\n\nThe devil here is in the details:\n\n- Why is *Sex* so much weaker for the boosting algorithms? And why have features like *Alone* more impact when boosted? Is it because of the lower tree depth?\n- What can we learn from these discrepancies with respect to parameter optimisation for the individual classifiers?","a332a3a7":"[Go to the top of the page](#top)","378c4835":"**p : probabily of survival**\n\n**n : the number of trial**\n\n**x : the count of being ended up to being survived.**","79fcc946":"[Go to the top of the page](#top)","2087c081":"For a view into *Pclass* vs *Sex* let's use a *mosaic plot* for a 2-dimensional overview.","3d6b781f":"**We learn:** Again, we find that having 1-3 family members works best for survival. This feature is a mix of *SibSp and Parh*, which increases the overall numbers we can work with, but might smooth out some more subtle effects.","0ad6cef2":"### *Age\\_known*","05c46f52":"Almost 100% yes. Above, we extract the standard deviation of the *Fares* among the ticket groups. A standard deviation of zero means that there's no difference. Only 2 values stand out. This is a small number that we could ignore, but we are curious, aren't we?","9d34bc71":"### *Cabin\\_known*","8dca4983":"However, we see again that a large part of this effect disappears once we control for **Sex and Pclass**.\n\n**We learn:**\nThere remains a potential trend for males and for 3rd class passengers but the uncertainities are large.\nThis feature should be tested in the modeling state.","36aec641":"Each of the individual classifiers we have used above has its strengths and weaknesses, and we should always choose the classifier that's best equipped to handle a certain problem and\/or has been found to perform with the highest accuracy. But wouldn't it be nice to combine all these different classifiers to get a more accurate overall prediction? This is possible through an approach called *Ensemble methods*. We have already encountered this strategy in our Random Forests or Bagging estimators above, where the aim was to get a more accurate estimate from combining multiple runs of a single classifier (like a Decision Tree; for instance).\n\nNow, we want to combine the results of *different kinds of classifiers* to improve our prediction.","ec9170ab":"* **Crosstab**","196b7ab0":"### *Cabin numbers*","c654d5c2":"**We can try out:**\n\nWorking hypothesis: If your group(mostly family) survived then you survived as well, unless you were a man(and presumably helped your wife\/daughter\/lady friend). We could go through the trouble here to identify families by last name. However\n\n1. Common last names might not be unique in the passenger list.\n2. As we see above a tikcket is not always shared by people with a common name\n\nTherefore, a shared ticket might actually be a stronger predictor. Of course this assumption should be tested by doing the last-name thing too.\n\nIn addition, we see that the *Fare* was identical for all the passengers in each ticket group.\nThis is something we will explore in more detail below.","2baa0968":"**eXtreme Gradient Boosting - XGBoost:**","bb476687":"TODO: Expand this section","3341c460":"*Random Forest:* As the name suggests, this classifier is using a number of decision trees instead of just a single one. Thereby, this is an *ensemble method* which combines the results of individual classifiers to improve the accuracy. Think of it as an average of estimators. An individual estimator may have a poor accuracy but if you combine several of them the resulting mean (or median) average will have a reduced uncertainty. Similar to the standard error of the mean for sampling normal distributions.\n\nThere are two types of ensemble methods: *boosting*, used below, and *averaging* (or *bagging*; see above). A random forest is an averaging classifier for which we train several estimators independently and then average over their individual predictions. Boosting works best for weak learners (e.g. decision stumps) whereas for Bagging\/Averaging to be successful we want to overfit a little\n\nThe *random* in *random forest* comes from the method of training each tree using a random bootstrap sample (i.e. one with replacement) of the original training set. Further randomness is introduced by making the node split dependent on a random subset of features instead of all of them. Here single trees are combined through the average of the prediction probabilities.\n\nIn addition to the tree parameters, the most important settings are:\n\n- n_estimators: number of trees. The larger the better, although improvements become marginal eventually\n\n- max_features: number of random features per subset. Lower numbers decrease variance and increase bias. Rule of thumb for classification: max_features = sqrt(all_features). This is the default setting.\n\nOne suggestion is to use a large number of highly overfitted trees with small split limits and no depth limit.\n\nOnce more, this info was digested from the [sklearn documentation](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forest)","908f1043":"**K Nearest Neighbours:**","13adc894":"* Descriptive Statistics","ac4e5a27":"The last plot deosn't inspire much confidence in a strong correlation between **Deck and Bad_ticket**, but maybe it will be usefule otherwise.\n\n**We learn**: **Bad_ticket** might be a lower order effect that could give us some additional accuracy.We should test it out in the modelling state.","f985c495":"The curious distribution for \"Q\" survivors somewhat follows the overall trend for 3d class passengers (which make up the vast majority of \"Q\") but is notably narrower. Not many of the children there survived but then there were not many children to begin with.\nLet's come back to this point in distribution the derved features.\n\n**We learn:** \nThere don't seemd to be strong differences in *Age* among the *Embarked* categories that would point at an imbalance that goes beyond the influence of *Pclass* and *Sex*.\n\nLet's study the relation between *Fare* and *Pclass* in more detail:","575f53a6":"* **The relationship between *Fare* and *Pclass*.**","ff689d4b":"**Ranking of models.** I've 'borrowed' that one straight from this very nice kernel, because it's a useful summary display of how our models perform:\n<https:\/\/www.kaggle.com\/startupsci\/titanic\/titanic-data-science-solutions>","04a10795":"For a final overview before the modelling stage we have another look at the correlation matrix between all old and new features:","78273b31":"[Go to the top of the page](#top)","c84a096f":"For each of these various classifiers we can have a closer look to improve their performance and understand their output. As an example we'll be using the *Extremely Randomized Trees*, but any other classifier can be substituted instead.\n\nWe will start with a *grid search algorithm* to find the best parameters to run our classifier. This is called [*tuning of the hyper-parameters*](http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html). The idea is to define a number of possible values for each hyper-parameter. Together, these sets of values define a grid (which is quite easy to visualise in two dimensions). Then, we evaluate the score of the classifier at each grid point and pick the one parameter combination that gives us the best score.","2756d4e1":"These are two women that travelled together in 1st class , were 38 and 62 years old, and had no family on board.","89f66a78":"### *Fare\\_cat*","c44a020f":"Let's remind ourselves of the distribution of *Fare* with respect to *Pclass*:","fb78502b":"*Support Vector Machine:* This classifier fits a (set of) hyper-plane(s) in the high-dimensional space of the training features so that this plane has the largest distance to any training data points. This is easy to visualise in 2 dimensions as e.g. 1 line that separates 2 classes (see the link below). In higher dimensions only mathematics can save you.\n\nThe *support vectors* are a subset of training data points used in the decision function. For unbalanced problems setting *class\\_weight='balanced'* might be helpful (compare decision tree notes).\n\nAdvantages: Effectiv in high dimensions and versatile with different kernel options.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/svm.html)","da73050a":"* binom_test","9cf88bf3":"OK.so what can we tell from the Deck(derived from the Cabin number)? First of all, the overall survial statistics much better than for the full sample, which is what we found above.\nBeyond that, the best decks for survival where B, D and E with abount 66% chance.\n\nC and F are around 60%. A and G at 50%. The only on deck T died, but that's hardly robust statitstics.\n\nThe largest number of cases we have is for B vs C. Let's see whether that's significant:","3bf6d1f7":"The [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM) is another gradient boosting tool which in 2017 was beginning to eclipse the XGBoost as Kaggle's go-to method for efficient boosting. LightGBM is often signficantly faster than XGBoost and achieves at least a similar accuracy.","76e1cd02":"Similiar to the known Cabin numbers, what about the *Passengers for which we know the age?*","7f5aade3":"## *\ud574\ub2f9 \ucee4\ub110\uc740 [Pytanic by Heads and Tails](https:\/\/www.kaggle.com\/headsortails\/pytanic)\uc744 \ucc38\uc870\ud558\uc600\uc2b5\ub2c8\ub2e4*","992753bc":"### *Title*","c1a0881c":"OK, so we have 18 differenct titles, but many of then only apply to a handful of people. The dominating ones are Mr(581), Miss(210),Mrs(170),and master(53); with the number referring to the full data. \n\nHere are the age of distributions for those.","2e7e19be":"We see that Master is capturing the male children\/teeages very well, whereas Miss applies to girls as well as younger women up to about 40.Mrs does not contain many teeagers, but has a sizeable overlap with Miss; especially in the range of 20-30 years old.\n\nNevertheless, Miss is more likely to indicate a younger woman. Overall, there is a certain amount of variance and we're not going to be able to pinpoint a certain are based on the title.\n\nTherefore, we will use 2 Age Groups, updating to the Young variabe we defined above. \nThe idea is to address the issue fo missing Age values by combining the **Age and Title features** into a single feature that should still contain some of the signal regading survial.\n\nFor this, we define everyone under 30 or with a title of **Master, Miss, or Mlle**(Mademoiselle) as Young. All the other titles we group into Not Young. This is a bit of a generalisation in terms of how **Miss and Mrs overlap**, but is might be a useful starting point. \n\nAll the other rare titles(Like Don or Lady) have average ages that are high enough to count as **Not Young**.","2629d9ca":"**Decision Tree:**","aa66219e":"# 3.Realtions between features","26aefd71":"Here we see that in the testing data set (based on our train\/test split) 12 people who survived were misclassified as dead, whereas 21 who died were misclassified as having survived. That is roughly 20% of the cases that were classified correctly. The confusion matrix plot would allow us to identify significant *imbalances* in our prediction between the false positives and the false negatives. For instance if the off-diagonal elements were 0 and 30. For our case there doesn't seem to be an imbalance.\n\nHere we use Taner's function and also include the \"official\" [sklearn example](http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html) for plotting confusion matrices. The latter one, which you can comment also includes the possibility to plot a *normalised* confusion matrix. In addition, we include the option to use a confusion matrix from [this website](http:\/\/notmatthancock.github.io\/2015\/10\/28\/confusion-matrix.html) which shows more information if we need it. Alternatively, you can use a seaborn heatmap for a quick and easy (but less pretty) plot. Just change the comment tags to switch between the options. Admittedly, 4 different ones are a bit of an overkill, but why not document what we found.","0311ae6a":"The next idea is to define new features based on the existing ones that allow for a split into survived\/not-survived with higher confidence than the existing features.\nAn example would be \"rich woman\" vs \"poor man\", but this particular distinction should be handled well by most classifiers.\nWe're looking for somethng a bit more subtle here.This is the part where the detective puts individual clues together to see whether their sum is more than its parts.\n\nThis part of the analysis is called **Feature Engineering**. I prefer the approach to list all the new features that we define together in one place, to keep an overview.\n\nEvery time we can think of a new feature, we come back here to define it and then study it further down. We compute the new feature in the combined data set, to make sure that all feature realizations are complete, and then split the combine data set again into train and test.","01a7378a":"# 2. \ub370\uc774\ud130 \ud0d0\uc0c9","b8e387a7":"Very much so. However, we have seen before that there might be imbalances in the dominating feature **Sex and Pclass** that create an apparent signal. Is this another of these cases?","b1913639":"**We learn:**\n\n* For females the survival chances appear to be higher between 18 and 40 , whereas for  men in that age range the oods are flipped.This difference between 18-40 year olds might be a better feature thatn *Sex and Age* by themselves.\n* Boys have propotional better survival chances than men, whereas girls have similiar chances as women have. Rather small numbers , though.\n\nWe study the correlation of *Age with Pclass using a violin plot*, whick is also split between survived(right half) and not survivde(left half).\nCheck out the other visualization in your forked copy.","12f66120":"* Check missing values for *Embarked* feature","a4fe3166":"**Load Python modules:** \n\n* [\ube45\uce74\uc778\uc988 \ub274\uc2a4\ub370\uc774\ud130\ub97c \uc774\uc6a9\ud55c geosciene](https:\/\/blog.daum.net\/geoscience\/1407)\n\n* [MLxtend - Machine Learnng Extendsion](https:\/\/github.com\/rasbt\/mlxtend)","6ea4a58a":"The Pclass ==1 plot looks interesting at first, but there are only 3 children in this group which makes the apparent pattern just random noise.\nTher other two passenger classes are more interesting, especially for the male children.\n\nNote, that since we are sleecting by Age, whick has many missing values, a number of children will be in the Child==False group. Nonetheless, this seems useful.\n\n**We learn:**\nMale children appear to have a survival advantage in 2nd and 3rd class.\nWe should include the Child Feature in our modeling testing.","db1d32a5":"**We learn:**\n* Both the factorplot and the mosaicplot indicate that almoset all females that died were 3rd class passengers.\n* For males being in 1st class gives a survival boost, otherwise the proportions look roughly similiar.\n* Except for 3rd class, the survival for *Embarked == Q* is close to 100% split between male and female.\n\nLet's follow up the numbers for *Pclass vs Embarked with a Pasndas crosstab* plot.","d5b1a3b4":"*Nearest Neighbours*: a non-parametric classifier that uses the training data closest to each test data point to classify it. *K* is simply the number of neighbours that are making the decision by majority vote. This is a simple yet powerful method that works well for irregular decision boundaries.\n\nImportant parameters:\n\n- n_neighbors: choosing the right *k* depends heavily on the data. Larger values suppress noise but smooth out decision boundaries. Default: 5.\n\n- weights: *uniform* assigns equal weight to each neighbour, whereas *distance* gives more weight to neighbours that are closer.\n\n[Source](http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html)","e4c3e686":"Coming soon: The next step will use the pre-packaged stacking classifier of the mlxtend package. ","07a8552e":"*Violin plots* are a modified version of boxplots*, where the shape is \"Kernel density estimate\" of the underlying distribution. These estimates are smoothed and therefore extends beyond the actual values (look closely at the dotted zero level). I have also indicated *Age == 10*, which we will use to define children (vs teenagers) in the engineering part below.\n\n**We learn:**\n\n* Age decreases progressively as Pclass decreases from 1st to 3rd.\n* Most older passengers are 1st class, but very few children are. This conflates the impact of *Age and Pclass* on the survival chances.\n* In 1st class, younger adults had better survival chances than older ones.\n* Most children in 2nd class survived, and the majority in 3rd class did too.","086f3925":"In our training data set about 60% of the passengers didn't survived. By from out predicting that everyone in the test datasets died we would get a 60% accracy.\n\nHere we also defines a consistent colour scheme for the distingushing between survived \/ non-survived.\nThis will soon be used throughout this kernel.\n\nOK. Let's go through the features one by one to see what we find. Here we will see how the distributores of survivors and non-survivors compare. \n\nPersonally, I like histograms for a first look at comparing two or more populations in case of scaled feature. For categorical features we will use *barplots* plus *Standard deviation bars*, to better judge the significance.","062ac189":"* crosstab\uc744 \uc774\uc6a9\ud55c \ud53c\ucc98\uc640 \ud0c0\uac9f\uceec\ub7fc","e51da631":"**Random Forest**","2c33d41e":"Now this is somewhat expected since it explains the difference between S - Southampton and the other ports. \nTherefore, it seems that between more 1st class passengers embarking at \"C\" - Cherbourg and more men at \"S\" - Southhampton - there doesn't seem to be much actual influence in the prot of embarkation.\n\nHowever, the last plot should alson indicate that...","a38a805a":"So well, in fact that defining new fare categories seems almost redundant because *Pclass* already captures most of this signal. Nonetheless, we'll try; because we are optimistic people at heart. We use the dashed lines in the plot above for an (empirical) division into 3 classes, which separate the cheaper *Fare_eff* of a *Pclass* group from the more expensive ones of the next one. The new feature is called *Fare_eff_cat* and behaves as follows:","04d714cf":"## *Test and select the model features*\n\nNow we are ready to model. We start with a *Logistic Regression* to assess the importance of the individual model features. We know that by definition some of our engineered features will have a *high collinearity* (i.e. behave similarly) with other new or existing features. For instance, *Young* was designed to replace *Age* and *Title* as a combination of the two. Other correlations are visible in the heatmap above. The initial modelling will allow us to decide which features are worth to take to the next step.\n\nThis is an iterative process in which you improve your model step by step, until you have found the largest feature combination which still has significant impact. Removing less important features will help you to reduce the noise in your prediction and allow your model to generalise to new data (which is our priority goal in machine learning.)\n\n*TODO: This part is still quite rudimentary and will be expanded in future versions. For now, we just continue with a rather intuitive set of important features.*  ","2c8c6430":"Passengers with more thatn 3 children + parents on board had low survival chances. However the corresponding number are not very large.\n\nFor SibSp we have 15 vs 3, 4 vs 0, and 7 vs 0.\n\nRandom outcomes with 2 possbilities(like *heads or tails* when flipping a coin) follow the [binomial distribution](https:\/\/en.wikipedia.org\/wiki\/Binomial_distribution).\n\nWe can use a **binomial test** to estimate the possibility that 5 non-survivors of of a total 5 passengers with SibSp = 6 happened due to chance assuming the overall 38% survival change for the entire sample.","5f2d7062":"Also, we will start to use **Factorplot** i.e. groups of **pointplots,** from the seaborn","2b62661f":"# \ud0c0\uc774\ud0c0\ub2c9 \ub370\uc774\ud130\ubd84\uc11d\uc744 \uc704\ud55c \ud30c\uc774\uc36c \ub69c\ubc85\uc774.","bf676938":"### *Alone*","ad25a52e":"### *Ttype and Bad\\_ticket*","f99a8f19":"As suspected, it is more likely to know the cabin of as passenger whod survived.This could be usefule."}}