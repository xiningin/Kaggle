{"cell_type":{"b1c1083a":"code","1179bd2c":"code","4e2fe80f":"code","1ffb2c29":"code","a544bfe6":"code","cfa5acff":"code","311f753c":"code","adea0dd4":"code","fc5db54a":"code","dcfe4266":"code","1284284c":"code","f7478881":"code","e9ccde97":"code","76ccd0f6":"code","cf27dade":"code","008108e0":"code","f81ab0a0":"code","9072687e":"code","b132cdde":"code","1a7a0ead":"code","60ded1ca":"code","90d42483":"code","22465143":"code","6b34cc4c":"code","62416053":"code","84fed62b":"code","93bf0086":"code","3888d16b":"code","5b414e4b":"code","67264c05":"code","334e5912":"code","ff5e0bc9":"code","a4d3baef":"code","27cc3021":"code","515fcb7a":"code","f29788f8":"code","bae798ce":"code","69520f85":"code","98bd11ce":"code","b00c7000":"code","9e484713":"code","d082822c":"code","0e13f141":"code","bb753950":"markdown","2c9ef994":"markdown","039da3f2":"markdown","f5638bc2":"markdown","1de63566":"markdown","e953d8a5":"markdown","8ed5b8b5":"markdown","ef338c7f":"markdown","6f8eb90b":"markdown","1f901178":"markdown","71b851a3":"markdown","4c6a4ecb":"markdown","da2da5c7":"markdown","3c1fec2c":"markdown","e3287536":"markdown","f16e40f3":"markdown","06f8d9e4":"markdown","e69b6049":"markdown","0ff096eb":"markdown","79c10ba2":"markdown","1887ff2c":"markdown","8ceeb5f0":"markdown","b6197f8d":"markdown","f76326a5":"markdown","fb922c79":"markdown","090534d4":"markdown","73ff3a7d":"markdown","ebbdb56d":"markdown","97cfa90e":"markdown","487440dc":"markdown","3f029113":"markdown","9eb5d0d1":"markdown","8b91d030":"markdown","46666cf3":"markdown"},"source":{"b1c1083a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1179bd2c":"import re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk # for text manipulation\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n%matplotlib inline","4e2fe80f":"df = pd.read_csv(\"..\/input\/twitter-vaccination-dataset\/vaccination2.csv\")","1ffb2c29":"print('Dataset size:',df.shape)\nprint('Columns are:',df.columns)","a544bfe6":"# use the drop columns function to streamline the dataset\ndf = df.drop(columns=['id', 'time','user_id','username','conversation_id','created_at','timezone', 'name', 'place', 'mentions', 'urls', 'photos', 'replies_count', 'likes_count', 'cashtags', 'link', 'retweet','retweets_count', 'quote_url', 'video', 'near', 'geo', 'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to', 'retweet_date'])\ndf.head(10)","cfa5acff":"print('Dataset size:',df.shape)\nprint('Columns are:',df.columns)\ndf.info()","311f753c":"#convert data to datetime and strings for manipulation.\ndf[\"tweet\"]= df[\"tweet\"].astype(str)\ndf['date']= pd.to_datetime(df['date'], infer_datetime_format=True)\ndf.info()","adea0dd4":"df = df.sample(frac=.1, random_state=1111)","fc5db54a":"import string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import re\n\nMIN_YEAR = 1900\nMAX_YEAR = 2100\n\n\ndef get_url_patern():\n    return re.compile(\n        r'(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))'\n        r'[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})')\n\n\ndef get_emojis_pattern():\n    try:\n        # UCS-4\n        emojis_pattern = re.compile(u'([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF])')\n    except re.error:\n        # UCS-2\n        emojis_pattern = re.compile(\n            u'([\\u2600-\\u27BF])|([\\uD83C][\\uDF00-\\uDFFF])|([\\uD83D][\\uDC00-\\uDE4F])|([\\uD83D][\\uDE80-\\uDEFF])')\n    return emojis_pattern\n\n\ndef get_hashtags_pattern():\n    return re.compile(r'#\\w*')\n\n\ndef get_single_letter_words_pattern():\n    return re.compile(r'(?<![\\w\\-])\\w(?![\\w\\-])')\n\n\ndef get_blank_spaces_pattern():\n    return re.compile(r'\\s{2,}|\\t')\n\n\ndef get_twitter_reserved_words_pattern():\n    return re.compile(r'(RT|rt|FAV|fav|VIA|via)')\n\n\ndef get_mentions_pattern():\n    return re.compile(r'@\\w*')\n\n\ndef is_year(text):\n    if (len(text) == 3 or len(text) == 4) and (MIN_YEAR < len(text) < MAX_YEAR):\n        return True\n    else:\n        return False\n\n\nclass TwitterPreprocessor:\n\n    def __init__(self, text: str):\n        self.text = text\n\n    def fully_preprocess(self):\n        return self \\\n            .remove_urls() \\\n            .remove_mentions() \\\n            .remove_hashtags() \\\n            .remove_twitter_reserved_words() \\\n            .remove_punctuation() \\\n            .remove_single_letter_words() \\\n            .remove_blank_spaces() \\\n            .remove_stopwords() \\\n            .remove_numbers()\n\n    def remove_urls(self):\n        self.text = re.sub(pattern=get_url_patern(), repl='', string=self.text)\n        return self\n\n    def remove_punctuation(self):\n        self.text = self.text.translate(str.maketrans('', '', string.punctuation))\n        return self\n\n    def remove_mentions(self):\n        self.text = re.sub(pattern=get_mentions_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_hashtags(self):\n        self.text = re.sub(pattern=get_hashtags_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_twitter_reserved_words(self):\n        self.text = re.sub(pattern=get_twitter_reserved_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_single_letter_words(self):\n        self.text = re.sub(pattern=get_single_letter_words_pattern(), repl='', string=self.text)\n        return self\n\n    def remove_blank_spaces(self):\n        self.text = re.sub(pattern=get_blank_spaces_pattern(), repl=' ', string=self.text)\n        return self\n\n    def remove_stopwords(self, extra_stopwords=None):\n        if extra_stopwords is None:\n            extra_stopwords = []\n        text = nltk.word_tokenize(self.text)\n        stop_words = set(stopwords.words('english'))\n\n        new_sentence = []\n        for w in text:\n            if w not in stop_words and w not in extra_stopwords:\n                new_sentence.append(w)\n        self.text = ' '.join(new_sentence)\n        return self\n\n    def remove_numbers(self, preserve_years=False):\n        text_list = self.text.split(' ')\n        for text in text_list:\n            if text.isnumeric():\n                if preserve_years:\n                    if not is_year(text):\n                        text_list.remove(text)\n                else:\n                    text_list.remove(text)\n\n        self.text = ' '.join(text_list)\n        return self\n\n    def lowercase(self):\n        self.text = self.text.lower()\n        return self","dcfe4266":"# Clean tweets and append to new column\ntweets = df['tweet']\nclean_tweets = []\nfor tweet in tweets:\n    c = TwitterPreprocessor((tweet))\n    c.fully_preprocess()\n    c = c.text\n    clean_tweets.append(c)\n    \ndf['clean_tweets'] = clean_tweets \ndf.head(5)","1284284c":"\nall_words = ' '.join([text for text in df['clean_tweets']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","f7478881":"import nltk\nnltk.download(\"vader_lexicon\")","e9ccde97":"from nltk.sentiment.vader import SentimentIntensityAnalyzer","76ccd0f6":"# create analyzer object \nanalyzer = SentimentIntensityAnalyzer()\n\n# get a list of scores and plot\nscores = [analyzer.polarity_scores(tweet)['compound'] for tweet in df['clean_tweets']]\nplt.hist(scores, bins=20)","cf27dade":"sentiment = df['clean_tweets'].apply(lambda x: analyzer.polarity_scores(x))\ndf = pd.concat([df,sentiment.apply(pd.Series)],1)\ndf.head(5)","008108e0":"df.describe()","f81ab0a0":"df.index = pd.to_datetime(df['date'])\ndf = df.sort_index()\ndf['mean'] = df['compound'].expanding().mean()\ndf['rolling'] = df['compound'].rolling('1d').mean()","9072687e":"import datetime as dt\n\nfig = plt.figure(figsize=(20,5))\nax = fig.add_subplot(111)\nax.scatter(df['date'],df['compound'], label='Tweet Sentiment')\nax.plot(df['date'],df['rolling'], color ='r', label='Rolling Mean')\nax.plot(df['date'],df['mean'], color='g', label='Expanding Mean')\n#ax.set_xlim([dt.date(2019,6,15),dt.date(2019,10,15)])\nax.set(title='Vaccination Tweets over Time', xlabel='Date', ylabel='Sentiment')\nax.legend(loc='best')\nfig.tight_layout()\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","b132cdde":"fig = plt.figure(figsize=(10,5))\nax = fig.add_subplot(111)\nax.set(title='Vaccination Tweets Sentiment Score', xlabel='Compund Sentiment Score', ylabel='Frequency')\nsns.distplot(df['compound'], bins=15, ax=ax)\nplt.show()","1a7a0ead":"ot = df.sample(frac=.05, random_state=1111)\not.sort_index(inplace=True)\n\not['mean'] = ot['compound'].expanding().mean()\not['rolling'] = ot['compound'].rolling('1d').mean()\n\nfig = plt.figure(figsize=(30,5))\nax = fig.add_subplot(111)\nax.scatter(ot['date'],ot['compound'], label='Tweet Sentiment')\nax.plot(ot['date'],ot['rolling'], color ='r', label='Rolling Mean')\nax.plot(ot['date'],ot['mean'], color='g', label='Expanding Mean')\nax.set(title='Vaccination Tweets over Time', xlabel='Date', ylabel='Sentiment')\nax.legend(loc='best')\nfig.tight_layout()\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","60ded1ca":"#save cleaned dataset with sentiment appended\n#df.to_csv('\/kaggle\/working\/df_cleaned_sent.csv',index=False)","90d42483":"def hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags\n","22465143":"# extracting hashtags from neutral tweets\n\nHT_neutral = hashtag_extract(df['hashtags'][df['compound'] == 0])\n\n# extracting hashtags from negative tweets\nHT_negative = hashtag_extract(df['hashtags'][df['compound'] < 0])\n\n# extracting hashtags from positive tweets\nHT_positive = hashtag_extract(df['hashtags'][df['compound'] > 0])\n\n# unnesting list\nHT_neutral = sum(HT_neutral,[])\nHT_negative = sum(HT_negative,[])\nHT_positive = sum(HT_positive,[])","6b34cc4c":"a = nltk.FreqDist(HT_neutral)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","62416053":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n\n# selecting top 20 most frequent hashtags     \ne = e.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","84fed62b":"c = nltk.FreqDist(HT_positive)\nf = pd.DataFrame({'Hashtag': list(c.keys()), 'Count': list(c.values())})\n\n# selecting top 20 most frequent hashtags     \nf = f.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=f, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light',\n    fontsize='x-large'  \n)\nplt.show()","93bf0086":"import sys\n# !{sys.executable} -m spacy download en\nimport re, numpy as np, pandas as pd\nfrom pprint import pprint\n\n# Gensim\nimport gensim, spacy, logging, warnings\nimport gensim.corpora as corpora\nfrom gensim.utils import lemmatize, simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# NLTK Stop words\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n","3888d16b":"def sent_to_words(sentences):\n    for sent in sentences:\n        sent = re.sub('\\S*@\\S*\\s?', '', sent) \n        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n        yield(sent)  \n","5b414e4b":"# Convert to list\ndata = df.clean_tweets.values.tolist()\ndata_words = list(sent_to_words(data))\nprint(data_words[:1])","67264c05":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\ndef process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n    texts = [bigram_mod[doc] for doc in texts]\n    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n    texts_out = []\n    nlp = spacy.load('en', disable=['parser', 'ner'])\n    for tweet in texts:\n        doc = nlp(\" \".join(tweet)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    # remove stopwords once more after lemmatization\n    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n    return texts_out\n\ndata_ready = process_words(data_words)  # processed Tweet Data!","334e5912":"data_ready[:5]","ff5e0bc9":"# Create Dictionary\nid2word = corpora.Dictionary(data_ready)\n\n# Create Corpus: Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in data_ready]\n\n# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=4, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=10,\n                                           passes=10,\n                                           alpha='symmetric',\n                                           iterations=100,\n                                           per_word_topics=True)\n\nprint(lda_model.print_topics())","a4d3baef":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    tweet_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                tweet_topics_df = tweet_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    tweet_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    tweet_topics_df = pd.concat([tweet_topics_df, contents], axis=1)\n    return(tweet_topics_df)\n\n\ndf_topic_tweet_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n\n# Format\ndf_dominant_topic = df_topic_tweet_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Tweet']\ndf_dominant_topic.head(10)","27cc3021":"# Display setting to show more characters in column\npd.options.display.max_colwidth = 100\n\ntweet_topics_sorteddf_mallet = pd.DataFrame()\ntweet_topics_outdf_grpd = df_topic_tweet_keywords.groupby('Dominant_Topic')\n\nfor i, grp in tweet_topics_outdf_grpd:\n    tweet_topics_sorteddf_mallet = pd.concat([tweet_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n                                            axis=0)\n\n# Reset Index    \ntweet_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\ntweet_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Tweet\"]\n\n# Show\ntweet_topics_sorteddf_mallet.head(10)","515fcb7a":"doc_lens = [len(d) for d in df_dominant_topic.Tweet]\n\n# Plot\nplt.figure(figsize=(16,7), dpi=160)\nplt.hist(doc_lens, bins = 100, color='navy')\nplt.text(50, 1000, \"Mean   : \" + str(round(np.mean(doc_lens))))\nplt.text(50,  2000, \"Median : \" + str(round(np.median(doc_lens))))\nplt.text(50,  3000, \"Stdev   : \" + str(round(np.std(doc_lens))))\nplt.text(50,  4000, \"1%ile    : \" + str(round(np.quantile(doc_lens, q=0.01))))\nplt.text(50,  5000, \"99%ile  : \" + str(round(np.quantile(doc_lens, q=0.99))))\n\nplt.gca().set(xlim=(0, 50), ylabel='Number of Tweets', xlabel='Tweet Word Count')\nplt.tick_params(size=16)\nplt.xticks(np.linspace(0,50,9))\nplt.title('Distribution of Tweet Word Counts', fontdict=dict(size=22))\nplt.show()","f29788f8":"import seaborn as sns\nimport matplotlib.colors as mcolors\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\nfig, axes = plt.subplots(2,2,figsize=(12,10), dpi=75, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):    \n    df_dominant_topic_sub = df_dominant_topic.loc[df_dominant_topic.Dominant_Topic == i, :]\n    doc_lens = [len(d) for d in df_dominant_topic_sub.Tweet]\n    ax.hist(doc_lens, bins = 50, color=cols[i])\n    ax.tick_params(axis='y', labelcolor=cols[i], color=cols[i])\n    sns.kdeplot(doc_lens, color=\"black\", shade=False, ax=ax.twinx())\n    ax.set(xlim=(0, 50), xlabel='Tweet Word Count')\n    ax.set_ylabel('Number of Tweets', color=cols[i])\n    ax.set_title('Topic: '+str(i), fontdict=dict(size=16, color=cols[i]))\n\nfig.tight_layout()\nfig.subplots_adjust(top=0.90)\nplt.xticks(np.linspace(0,50,9))\nfig.suptitle('Distribution of Tweet Word Counts by Dominant Topic', fontsize=22)\nplt.show()","bae798ce":"# 1. Wordcloud of Top N words in each topic\nfrom matplotlib import pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.colors as mcolors\n\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n\ncloud = WordCloud(stopwords=stop_words,\n                  background_color='white',\n                  width=2500,\n                  height=1800,\n                  max_words=10,\n                  colormap='tab10',\n                  color_func=lambda *args, **kwargs: cols[i],\n                  prefer_horizontal=1.0)\n\ntopics = lda_model.show_topics(formatted=False)\n\nfig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.flatten()):\n    fig.add_subplot(ax)\n    topic_words = dict(topics[i][1])\n    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n    plt.gca().imshow(cloud)\n    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n    plt.gca().axis('off')\n\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","69520f85":"from collections import Counter\ntopics = lda_model.show_topics(formatted=False)\ndata_flat = [w for w_list in data_ready for w in w_list]\ncounter = Counter(data_flat)\n\nout = []\nfor i, topic in topics:\n    for word, weight in topic:\n        out.append([word, i , weight, counter[word]])\n\ndf_topics = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n\n# Plot Word Count and Weights of Topic Keywords\nfig, axes = plt.subplots(2, 2, figsize=(10,8), sharey=True, dpi=100)\ncols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\nfor i, ax in enumerate(axes.flatten()):\n    ax.bar(x='word', height=\"word_count\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n    ax_twin = ax.twinx()\n    ax_twin.bar(x='word', height=\"importance\", data=df_topics.loc[df_topics.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n    ax.set_ylabel('Word Count', color=cols[i])\n    ax_twin.set_ylim(0, 0.050); ax.set_ylim(0, 1000)\n    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n    ax.tick_params(axis='y', left=False)\n    ax.set_xticklabels(df_topics.loc[df_topics.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n\nfig.tight_layout(w_pad=2)    \nfig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \nplt.show()","98bd11ce":"# Sentence Coloring of N Sentences\nfrom matplotlib.patches import Rectangle\n\ndef sentences_chart(lda_model=lda_model, corpus=corpus, start = 0, end = 13):\n    corp = corpus[start:end]\n    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n\n    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n    axes[0].axis('off')\n    for i, ax in enumerate(axes):\n        if i > 0:\n            corp_cur = corp[i-1] \n            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n\n            # Draw Rectange\n            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n\n            word_pos = 0.06\n            for j, (word, topics) in enumerate(word_dominanttopic):\n                if j < 14:\n                    ax.text(word_pos, 0.5, word,\n                            horizontalalignment='left',\n                            verticalalignment='center',\n                            fontsize=16, color=mycolors[topics],\n                            transform=ax.transAxes, fontweight=700)\n                    word_pos += .009 * len(word)  # to move the word for the next iter\n                    ax.axis('off')\n            ax.text(word_pos, 0.5, '. . .',\n                    horizontalalignment='left',\n                    verticalalignment='center',\n                    fontsize=16, color='black',\n                    transform=ax.transAxes)       \n\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n    plt.tight_layout()\n    plt.show()\n\nsentences_chart()    ","b00c7000":"# Sentence Coloring of N Sentences\ndef topics_per_document(model, corpus, start=0, end=1):\n    corpus_sel = corpus[start:end]\n    dominant_topics = []\n    topic_percentages = []\n    for i, corp in enumerate(corpus_sel):\n        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n        dominant_topics.append((i, dominant_topic))\n        topic_percentages.append(topic_percs)\n    return(dominant_topics, topic_percentages)\n\ndominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# Distribution of Dominant Topics in Each Document\ndf_dominant = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\ndominant_topic_in_each_doc = df_dominant.groupby('Dominant_Topic').size()\ndf_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# Total Topic Distribution by actual weight\ntopic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\ndf_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# Top 3 Keywords for each Topic\ntopic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n\ndf_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\ndf_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\ndf_top3words.reset_index(level=0,inplace=True)","9e484713":"from matplotlib.ticker import FuncFormatter\n\n# Plot\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n\n# Topic Distribution by Dominant Topics\nax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\nax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\ntick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\nax1.xaxis.set_major_formatter(tick_formatter)\nax1.set_title('Number of Tweets by Dominant Topic', fontdict=dict(size=10))\nax1.set_ylabel('Number of Tweets')\nax1.set_ylim(0, 20000)\n\n# Topic Distribution by Topic Weights\nax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\nax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\nax2.xaxis.set_major_formatter(tick_formatter)\nax2.set_title('Number of Tweets by Topic Weightage', fontdict=dict(size=10))\n\nplt.show()","d082822c":"# Get topic weights and dominant topics ------------\nfrom sklearn.manifold import TSNE\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\n\n# Get topic weights\n# n-1 rows each is a vector with i-1 posisitons, where n the number of documents\n# i the topic number and tmp[i] = probability of topic i\n\ntopic_weights = []\nfor i, row_list in enumerate(lda_model[corpus]):\n    topic_weights.append([w for i, w in row_list[0]])\n    \n# Array of topic weights    \narr = pd.DataFrame(topic_weights).fillna(0).values\n\n# Keep the well separated points (optional)\narr = arr[np.amax(arr, axis=1) > 0.35]\n\n# Dominant topic number in each doc\ntopic_num = np.argmax(arr, axis=1)\n\n# tSNE Dimension Reduction\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\ntsne_lda = tsne_model.fit_transform(arr)\n\n# Plot the Topic Clusters using Bokeh\noutput_notebook()\nn_topics = 4\nmycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\nplot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n              plot_width=900, plot_height=700)\nplot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\nshow(plot)","0e13f141":"import pyLDAvis.gensim\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\nvis","bb753950":"## Sentence Chart Colored by Topic\n\nEach word in the document is representative of one of the 4 topics. Let\u2019s color each word in the given documents by the topic id it is attributed to.\nThe color of the enclosing rectangle is the topic assigned to the document.","2c9ef994":"## **Story Generation and Visualization from Tweets**\n### **A) Understanding the common words used in the tweets: WordCloud**\n\nNow we want to understand the common words by plotting wordclouds.\n\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.\n\nLet\u2019s visualize all the words our data using the wordcloud plot.","039da3f2":"## **Sentiment Analysis Using VADER**\n\nCreating your own sentiment analysis model from scratch can be very difficult and tedious for a few reason. You need to find relevant data to your problem, create a LOT of labeled data for training, and you must perform data clean up and NLP pre-processing. Luckily for us, VADER is a readily available pre-trained sentiment analysis that thrives on social media data. \nSome of the big advantages include:\n1. Analysis of polarity (positive or negative sentiment) as well as valence (intensity of the sentiment \u2014 i.e. \u2018excellent\u2019 has a higher intensity than \u2018good\u2019).\n2. Handles slang (\u2018lol\u2019, \u2018sux\u2019) and emojis, which are prevalent in tweets\n3. Accounts for capital letters and punctuation (i.e. \u2018GOOD!!\u2019 is more positive than \u2018good\u2019)\n\nFor more information on VADER you can access the [github repository](https:\/\/github.com\/cjhutto\/vaderSentiment) or the [paper written by the authors](http:\/\/comp.social.gatech.edu\/papers\/icwsm14.vader.hutto.pdf). \nFor other sentiment analysis tools you can check out this [github page.](https:\/\/github.com\/laugustyniak\/awesome-sentiment-analysis).\n\nThe first analysis we are going to do is to plot a histogram of all of the sentiment scores we collected on our tweets. \n","f5638bc2":"### Cleaning The Data\nWhen dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n\nWith text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n\n### Common data cleaning steps on all text:\n\nMake text all lower case\nRemove punctuation\nRemove numerical values\nRemove common non-sensical text (\/n)\nTokenize text\nRemove stop words\nMore data cleaning steps after tokenization:\n\nStemming \/ lemmatization\nParts of speech tagging\nCreate bi-grams or tri-grams\nDeal with typos\n\n### Specific Tweet oriented cleaning using the  tweet-preprocessor module\n\n### A) Removing Twitter Handles (@user)\n\nAs mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they don\u2019t convey much information.\n\n### B) Removing Punctuations,Links, Numbers, and Special Characters\n\nAs discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles.\n\n### C) Tokenization\nTokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n\n### D) Stemming\nStemming is a rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word. For example, For example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d.\n","1de63566":"## Tokenize Sentences and Clean\nRemoving new line characters, single quotes and finally split the sentence into a list of words using gensim\u2019s `simple_preprocess()`. Setting the `deacc=True` option removes punctuations.","e953d8a5":"Let\u2019s try to tackle things one at a time here. First let\u2019s look at those tweets with a sentiment of 0. Seborn\u2019s distplot is a quick way to see the distribution of sentiment scores across our tweets.","8ed5b8b5":"## **Text PreProcessing and Cleaning**\n### **Data Inspection** \n\nLet's check out  tweets.","ef338c7f":"## **What is the Dominant topic and its percentage contribution in each document**\n\nIn LDA models, each document is composed of multiple topics. But, typically only one of the topics is dominant. The below code extracts this dominant topic for each sentence and shows the weight of the topic and the keywords in a nicely formatted output.\n\nThis way, we will know which document belongs predominantly to which topic.","6f8eb90b":"## Build the Bigram, Trigram Models and Lemmatize\nLet\u2019s form the bigram and trigrams using the Phrases model. This is passed to Phraser() for efficiency in speed of execution.\n\nNext, lemmatize each word to its root form, keeping only nouns, adjectives, verbs and adverbs.\n\nWe keep only these POS tags because they are the ones contributing the most to the meaning of the sentences. Here, we use `spacy` for lemmatization.","1f901178":"# **QUALITATIVE ANALYSIS OF TWEETS ON VACCINATION** \n\nA project geared towards a qualitative analysis of historical data from Twitter on Vaccination.\nby Dr William Kane Olwit\n\n## **WHY SHOULD WE BE CONCERNEDABOUT TWEETS ON VACCINATION?**\n\n### **The fatal impact of vaccine hesitancy**\n\nIn 2018, there were over 82,000 cases of measles confirmed in the EU \u2013 three times more than in 2017 \u2013 and measles led to 72 deaths. Cases of measles are affecting all unvaccinated groups, adults and children alike, with large numbers of cases and fatalities in countries which had previously eliminated the disease.\nVaccine hesitancy is a key reason for this worrying trend. Europe is the most vaccine-hesitant region in the world, and we are now witnessing the results. Last year\u2019s wide-ranging survey of vaccine confidence in Europe, led by Heidi Larson and her colleagues from the Vaccine Confidence Project, found that the picture in the EU is complex with varying levels of vaccine confidence between countries.\n\n### **The role of social media**\n\nAt the core of social media is the ability for us to share ideas and content with our peers. While this freedom of information is what makes social media so appealing, it is also what can make it dangerous. Social media is not the cause of vaccine hesitancy, but it has certainly played a role in making anti-vaccination arguments and pseudoscience accessible to a wider audience.\n\n### **Data Gathering:**\n\nWe collected all tweets containing  the search string: vaccination. Along with the tweet text, we downloaded the date and time when the tweet was published, and the location of the user (if provided). We also downloaded the user id, follower ids, and friends ids. The followers of a user A are those users who will receive messages from user A. The friends of a user A are those users from whom user A receives messages. Thus, information flows from a user to his followers. We collected tweets using the open source information tool, TWINT.(https:\/\/github.com\/twintproject) and a python algorithm.\n\nIn contrast to the open Twitter Search API, which only allows one to query tweets posted within the last seven days, Twint makes it possible to collect a much larger sample of Twitter posts, ranging several years. We queried Twint for different key terms that relate to the topic of vaccination ranging from the year 2006 to 30th of November 2019 and stored in an aggregated CSV file.\n\n","71b851a3":"## Word Counts of Topic Keywords\nWhen it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the tweets is also interesting to look.\n\nLet\u2019s plot the word counts and the weights of each keyword in the same chart.\n\nWe want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart we\u2019ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process.","4c6a4ecb":"## Word Clouds of Top N Keywords in Each Topic\n\nThough we\u2019ve already seen what are the topic keywords in each topic, a word cloud with the size of the words proportional to the weight is a pleasant sight. The coloring of the topics we\u2019ve taken here is followed in the subsequent plots as well.","da2da5c7":"VADER gives back 4 types of polarity scores for every call: Positive, negative, neutral or compound. \n\nIn our code, we only consider the compound score  which is a combination of the other three plus some additional rules and a normalization between -1 and 1. \n\nOne thing to notice from our histogram is that many tweets have a neutral score, and there are only slightly more positive than negative ones.\n","3c1fec2c":"Now using matplotlib, with import matplotlib.pyplot as plt, we can create a quick chart of our tweets and their sentiment over time.","e3287536":"## t-SNE Clustering Chart\nLet\u2019s visualize the clusters of documents in a 2D space using t-SNE (t-distributed stochastic neighbor embedding) algorithm.","f16e40f3":"## **The most representative sentence for each topic**\n\nSometimes you want to get samples of sentences that most represent a given topic. This code gets the most exemplar sentence for each topic.","06f8d9e4":"# **Loading Libraries**","e69b6049":"The dataset has 89,973 rows and 31 columns.\n\nFrom our case we are only interested in the `tweet`, `hashtags` and `date` columns. We shall drop the rest for now.","0ff096eb":"\nLet's read train and test datasets","79c10ba2":"## **Topic Model Analysis**\n\nTopic Modeling (TM) consists of finding the information contained in textual documents (information retrieval in English) and presenting it in the form of themes (depending on the technique used, the relative importance of the themes can also be found ).\n\nTM is therefore an unsupervised technique for classifying documents in multiple themes (Unsupervised Learning in English).\n\nFrom the point of view of the representation space, the TM is a reduction of dimensions in the vector representation of a document : instead of representing a document of a corpus by a vector in the space of the words composing the vocabulary of this corpus is represented by a vector in the space of the themes of this corpus , each value of this vector corresponding to the relative importance of the theme in this document. \n\n### **Latent Dirichlet Allocation**\n\nLatent Dirichlet Allocation (LDA) is one example of a topic model used to extract topics from a document. LDA is an unsupervised machine learning algorithm that allows a a set of textual observations to be explained by unobserved groups that explain similarities within the data. LDA represents documents as mixtures of topics that spit out words with certain probabilities.\n\n![image.png](attachment:image.png)\n\n","1887ff2c":"Let\u2019s see if we can get a little bit clearer picture of our sentiment over time. \n\nOverall our data is noisy, there is just too much of it. \nTaking a sample of our data might make it easier to see the trends happening. \nWe\u2019ll use pandas sample() function to retain just a tenth of our 89,973 tweets.","8ceeb5f0":"\n###  Understanding the impact of Hashtags on tweets sentiment","b6197f8d":"## Analyzing Sentiment\nFirst let\u2019s just call `df.describe()` and get some basic information on our dataset now .","f76326a5":"Looking at the compound score we can see on average tweets are neutral, with a mean sentiment of .005.\n\nPlotting this data will give us a better idea of what it looks like. Before we plot we make a few changes to the dataframe for ease of use, sorting all the values by timestamp so they\u2019re in order, copying the timestamp to the index to make graphing easier, and calculating an expanding and rolling mean for compound sentiment scores.","fb922c79":"## What are the most discussed topics in the documents?\n\nLet\u2019s compute the total number of documents attributed to each topic.","090534d4":"Negative tweets","73ff3a7d":"## **Build the Topic Model**\nTo build the LDA topic model using LdaModel(), you need the corpus and the dictionary. Let\u2019s create them first and then build the model. The trained topics (keywords and weights) are printed below as well.","ebbdb56d":"## pyLDAVis\n\nFinally, pyLDAVis is the most commonly used and a nice way to visualise the information contained in a topic model. Below is the implementation for `LdaModel()`.","97cfa90e":"## Data Pre-processing\n\nThe preprocessing of the text data is an essential step as it makes the raw text ready for mining, i.e., it becomes easier to extract information from the text and apply machine learning algorithms to it. If we skip this step then there is a higher chance that you are working with noisy and inconsistent data. The objective of this step is to clean noise those are less relevant to find the sentiment of tweets such as punctuation, special characters, numbers, and terms which don\u2019t carry much weightage in context to the text.\n\n### Characteristic features of Tweets \n\nFrom the perspective of Sentiment\nAnalysis, we discuss a few characteristics of Twitter:\n\n**Length of a Tweet**\n     The maximum length of a Twitter message is 140 characters. This means that we can practically consider a tweet to be a single sentence, void of complex grammatical constructs. This is a vast difference from traditional subjects of Sentiment Analysis, such as movie reviews. \n     \n**Language used**\n     Twitter is used via a variety of media including SMS and mobile phone apps. Because of this and the 140-character limit, language used in Tweets tend be more colloquial, and filled with slang and misspellings. Use of hashtags also gained popularity on Twitter and is a primary feature in any given tweet. \n     \n**Data availability**\n     Another difference is the magnitude of data available. With the Twitter API, it is easy to collect millions of tweets for training. There also exist a few datasets that have automatically and manually labelled the tweets. \n     \n**Domain of topics**\n     People often post about their likes and dislikes on social media. These are not all concentrated around one topic. ","487440dc":"Positive Tweets","3f029113":"**Let\u2019s make two plots:**\n\n1. The number of documents for each topic by assigning the document to the topic that has the most weight in that document.\n2. The number of documents for each topic by by summing up the actual weight contribution of each topic to respective documents.","9eb5d0d1":"## Frequency Distribution of Word Counts in Tweets\nLet\u2019s plot the tweet word counts distribution.","8b91d030":"**Neutral Tweets**","46666cf3":"### We will only work on a sample of the dataset to make the execeution run quicker (10% Of the total dataset)"}}