{"cell_type":{"ad1ce0c2":"code","94b5495c":"code","c07c8554":"code","97f6b50e":"code","7da1701c":"code","34c21ce7":"code","6ab25efb":"code","dbbe2230":"code","5b285cfb":"code","3cd532a4":"code","05dc7c00":"code","a1be8e5e":"code","667a4775":"code","853d4b6e":"code","36757aa0":"code","c0ce6623":"code","32a70f5b":"code","56e123a8":"code","24b11704":"code","d0cafde4":"code","b2213ee4":"code","c84f0021":"code","578348ef":"code","b9a21ea4":"code","6d587525":"code","74448692":"code","05c7286d":"code","4583f3de":"code","13b106a5":"code","e69a62eb":"code","a9b5a59b":"code","3bd7010b":"code","afd5c53d":"code","3b891728":"code","d83d1c36":"code","2d5f1707":"code","ae2ef9de":"code","ddbf3b4d":"code","fd8cf0dd":"code","11904a05":"code","7943d054":"code","30a1bd98":"code","9b59b11f":"code","320eacc8":"code","3d35b782":"code","28fb8790":"code","c2feec96":"code","51ea3253":"code","61b23966":"code","1c848cd7":"code","9cd7506c":"code","190fdca1":"code","abd2af1b":"code","ca93ae5e":"code","207e827b":"code","4289843c":"code","8ffd7f5c":"code","56449e5a":"code","319a8bd1":"code","9589ae49":"code","1508e6cd":"code","1c80d2d9":"code","270b763e":"code","99abd70e":"markdown","5c878dd4":"markdown","3c6b19ed":"markdown","1e8fb310":"markdown","a350e738":"markdown","e5076836":"markdown","1c5e4c2f":"markdown","663bd536":"markdown","3af1dde2":"markdown","20461650":"markdown","b0d9fcc1":"markdown","9abf600b":"markdown","e928cd4b":"markdown","1800718a":"markdown","2902fd38":"markdown","d65b8b2e":"markdown","75678ee5":"markdown","2393335c":"markdown","b2079b69":"markdown"},"source":{"ad1ce0c2":"### Import important libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","94b5495c":"### Read the file\n\nbike=pd.read_csv(\"\/kaggle\/input\/bikesharing\/day.csv\")\nbike.head()","c07c8554":"### Dataframe shape\n\nbike.shape","97f6b50e":"###Information\n\nbike.info()","7da1701c":"### Change season into proper format  as these have some labels\n\nbike['season']=bike['season'].replace({1:'spring', 2:'summer', 3:'fall', 4:'winter'})\n","34c21ce7":"### Change weathersit into proper format\n\nbike['weathersit']=bike['weathersit'].replace({1:'Sunny\/Partial cloudy', 2:'cloudy\/Misty', 3:'Light rainy\/Light snowy', 4:'Heavy rainy\/heavy snowy'})\n","6ab25efb":"### Change weekday into proper format\n\nbike['weekday']=bike['weekday'].replace({1:'Monday', 2:'Tuesday', 3:'Wednesday', 4:'Thrusday',5:'Friday',6:'Saturday',7:'Sunday'})","dbbe2230":"### Change weekday into proper format\n\nbike['mnth']=bike['mnth'].replace({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'})\n","5b285cfb":"###check dataframe again to check whether all above 4 codes are applied correctly or not\n\nbike.head()","3cd532a4":"### Fetching list of all numerical values\n\nbike.describe().columns.to_list()","05dc7c00":"### Outlier detection\n\n\nplt.figure(figsize=[15,18])\n\n\nbike_num=['instant',\n 'yr',\n 'holiday',\n 'workingday',\n 'temp',\n 'atemp',\n 'hum',\n 'windspeed',\n 'casual',\n 'registered',\n 'cnt']\nc=1\n\nfor i in bike_num:\n    plt.subplot(3, 4, c)\n    plt.title('{}'.format(i))\n    plt.xlabel(i)\n    sns.boxplot(bike[i])\n    c = c + 1\n    \nplt.show()","a1be8e5e":"###pairplot\n\nsns.pairplot(bike)\nplt.show()","667a4775":"### We can drop thsese column because it has no significance\n\nbike.drop(['dteday'],axis=1,inplace=True)\nbike.drop(['instant'],axis=1,inplace=True)\nbike.drop(['casual'],axis=1,inplace=True)\nbike.drop(['registered'],axis=1,inplace=True)\n","853d4b6e":"###Checkinga dataframe again\nbike.head()","36757aa0":"###Correlation using heatmap\n\nplt.figure(figsize=[11,7])\nbike_corr=bike[['yr','holiday','workingday','temp','atemp','hum','windspeed','cnt']].corr()\nsns.heatmap(data=bike_corr,annot=True)\nplt.show()","c0ce6623":"### We can drop either atemp or temp because of high correlation\n### Dropping atemp\nbike.drop(['temp'],axis=1,inplace=True)","32a70f5b":"### Looking pattern into categorical columns using boxplots\n\nplt.figure(figsize=(20, 12))\n\nplt.subplot(2,2,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike)\n\nplt.subplot(2,2,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike)\n\nplt.subplot(2,2,3)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike)\n\nplt.subplot(2,2,4)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\n\nplt.show()","56e123a8":"### Dummy variable\n\n### Season,mnth,weekdays and weathersit have some lebels\n### Change these labels into 0\/1.\n### We are dropping first column because to minimize redundancy.\nseason_d=pd.get_dummies(bike['season'],drop_first = True)\nmnth_d=pd.get_dummies(bike['mnth'],drop_first = True)\nweekday_d=pd.get_dummies(bike['weekday'],drop_first = True)\nweathersit_d=pd.get_dummies(bike['weathersit'],drop_first = True)","24b11704":"### Add the above results into original data set\n\nbike=pd.concat([bike,season_d,mnth_d,weekday_d,weathersit_d],axis=1)\nbike.head()","d0cafde4":"## Checking columns\nbike.columns","b2213ee4":"### Drop Season,mnth,weekday,weathersit columns because these are no more significant.\n\nbike=bike.drop(['season','mnth','weekday','weathersit'],axis=1)\nbike.head()","c84f0021":"from sklearn.model_selection import train_test_split\nnp.random.seed(0)\nbike_train, bike_test = train_test_split(bike, train_size = 0.7, random_state = 100)","578348ef":"### checking shape of splitted data frame\nprint(bike_train.shape)\nprint(bike_test.shape)","b9a21ea4":"bike_train.head()","6d587525":"### Importing MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler","74448692":"scaler = MinMaxScaler()","05c7286d":"\n\n\n###Variable on which scaling to be applied are,\nbike_num=['atemp','hum','windspeed','cnt']\n\n### Apply Scaler\n\nbike_train[bike_num] = scaler.fit_transform(bike_train[bike_num])","4583f3de":"bike_train.describe()","13b106a5":"### Regplot for target variable with independent variable for train data set to check linear relationship.\n\nplt.figure(figsize=[20,7])\n\nplt.subplot(1,3,1)\nsns.regplot(data=bike_train,x='atemp',y='cnt')\n\n\nplt.subplot(1,3,2)\nsns.regplot(data=bike_train,x='hum',y='cnt')\n\nplt.subplot(1,3,3)\nsns.regplot(data=bike_train,x='windspeed',y='cnt')\n\n\nplt.show()","e69a62eb":"### Separate target and independent variables\n\ny_train = bike_train.pop('cnt')\nX_train = bike_train","a9b5a59b":"### Checking shape of target and independent dataframe\nprint(y_train.shape)\nprint(X_train.shape)","3bd7010b":"### Using RFE,we will find top 10 columns\n\n#Importing libraries\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nrfe = RFE(model, 10)            \nrfe = rfe.fit(X_train, y_train)","afd5c53d":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","3b891728":"### Top 10 columns\ntop_10_col = X_train.columns[rfe.support_]\ntop_10_col","d83d1c36":"### selecting top 10 independent variable,selected using RFE method\nX_train_top_10 = X_train[top_10_col]\nX_train_top_10.shape","2d5f1707":"import statsmodels.api as sm ","ae2ef9de":"# Adding a constant variable to avoid line passing through origin\n \nX_train_cons = sm.add_constant(X_train_top_10)","ddbf3b4d":"# Creating first model \nmlr = sm.OLS(y_train, X_train_cons).fit()","fd8cf0dd":"### Fetching constant and coefficients of models\nmlr.params","11904a05":"### Fetching summaries like p-value,r-squared,adjusted r-squared etc. of model\n\nprint(mlr.summary())","7943d054":"### Importing libraries to check VIF\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","30a1bd98":"### Checking VIF value i.e multicollinearity\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_top_10.columns\nvif['VIF'] = [variance_inflation_factor(X_train_top_10.values, i) for i in range(X_train_top_10.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9b59b11f":"### First we will drop which are highly insignificant and hight VIF i.e\n### Dropping hum because of high VIF value\n\nX_dropped = X_train_top_10.drop('hum', 1)\nX_dropped.shape","320eacc8":"# Building second model\nX_train_cons = sm.add_constant(X_dropped)\n\nmlr2 = sm.OLS(y_train, X_train_cons).fit()","3d35b782":"### summaries of second model\n\nprint(mlr2.summary())","28fb8790":"### Checking VIFs for second model\n\nvif = pd.DataFrame()\nvif['Features'] = X_dropped.columns\nvif['VIF'] = [variance_inflation_factor(X_dropped.values, i) for i in range(X_dropped.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c2feec96":"### Next,We will drop 'Sunny\/Partial cloudy' because it has high VIF value'\nX_dropped = X_dropped.drop('Sunny\/Partial cloudy', 1,)\nX_dropped.shape","51ea3253":"# Building third  model\nX_train_cons = sm.add_constant(X_dropped)\n\nmlr3 = sm.OLS(y_train, X_train_cons).fit()\n","61b23966":"### Summaries of third model\n\nprint(mlr3.summary())","1c848cd7":"### Checking VIFs for third model\n\nvif = pd.DataFrame()\nvif['Features'] = X_dropped.columns\nvif['VIF'] = [variance_inflation_factor(X_dropped.values, i) for i in range(X_dropped.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9cd7506c":"X_train_cons.head()","190fdca1":"X_train_cons.columns","abd2af1b":"X_train_latest = X_train_cons.drop(['const'], axis=1)","ca93ae5e":"X_train_latest.columns","207e827b":"### Residual analysis\n\ny_train_pred = mlr3.predict(X_train_cons)","4289843c":"# plotting distribution plot to check distribution of error terms\n\nplt.figure(figsize=[8,5])\nerror = y_train - y_train_pred\nsns.distplot(error, hist=False,bins = 50)\nplt.title('Distribution of residual term')                \nplt.show()    ","8ffd7f5c":"###Variable on which scaling to be applied are,\nbike_num=['atemp','hum','windspeed','cnt']\n\n### Apply Scaler\n\nbike_test[bike_num] = scaler.transform(bike_test[bike_num])","56449e5a":"bike_test.describe()","319a8bd1":"### Splitting test data set into dependent and independent variable\ny_test = bike_test.pop('cnt')\nX_test = bike_test","9589ae49":"# Now let's use our model to make predictions.\n\n# Creating X_test_cons with columns using final X_tain\nX_test_cons = X_test[X_train_latest.columns]\n\n# Adding a constant variable to avoid line passing through origin\nX_test_cons = sm.add_constant(X_test_cons)","1508e6cd":"# Making predictions using final model i.e mlr3\ny_pred = mlr3.predict(X_test_cons)","1c80d2d9":"### Importing library to compare r-squared\n\nfrom sklearn.metrics import r2_score\n","270b763e":"### R-squared for test dataframe\nprint(r2_score(y_test, y_pred))\n\n### R-squared for train dataframe\nprint(r2_score(y_train, y_train_pred))","99abd70e":"**Scaling applied successfully**","5c878dd4":"- In spring season deamnd is less.\n- In the month of January and December demands are lowest while highest in the month.\n- In light rainy\/light snowny demand is lowest while in sunny demand is highest","3c6b19ed":"## Spliting data set into train and test","1e8fb310":"### Making prediction using mlr3\n\n\n","a350e738":"# mlr3 is the decent  model","e5076836":"***There is no null value***","1c5e4c2f":"**Errors are normally distributed with mean is equal to zero**","663bd536":"# Equation of Final model is\n\n<span style='color:green'> **Demand= 0.1175 + 0.2394(yr)-0.0823(holiday) + 0.5795(atemp) - 0.1628(windspeed) + 0.0821(summer) + 0.1070(winter) + 0.0919(sep) - 0.0658(cloud\/misty)**","3af1dde2":"# Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n## Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","20461650":"### Looking pattern into categorical columns\n\n","b0d9fcc1":"## Rescaling using MinMaxScaler","9abf600b":"### Tthe R-squared for train and test both dataframe within acceptable range\n- R-Squared_test:76.8\n- R-Squared_train :78.1","e928cd4b":"- Before dropping columns we will also ckeck VIF of independent variables\n- VIF is used to check multicollinearity between independent variables","1800718a":"# Data Prepartion","2902fd38":"- Target variable is highly correlated with registered\n- Target variable is moderately correlated with temp,atemp,year and casual\n","d65b8b2e":"**We can observe that there are some linear relationship between target variable(cnt) and some independent variables**","75678ee5":"**We can clearly see that target variable has many linear relationship with independent variables**","2393335c":"# Visualization","b2079b69":"**There is no outliers,we are good to go**"}}