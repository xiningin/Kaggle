{"cell_type":{"99028e01":"code","f070c730":"code","fe99ba92":"code","1379e5a0":"code","4806b5a6":"code","4ecfd4ee":"code","439d4dbf":"code","70c72c02":"code","670030ff":"code","5ba0eaa7":"code","6cf5b5f0":"code","008a8a13":"code","6b5beb63":"code","b7ea4db1":"code","2eadb2b5":"code","cb94d272":"code","500bcae9":"code","94fd60df":"code","151734c8":"code","9726e4c8":"code","85398a0f":"code","fcbc94a9":"code","0c22220f":"code","cc1c6456":"code","404954a1":"code","15a2a859":"code","6415285b":"code","5ceb828f":"code","b0f92b4f":"code","79c1cb69":"code","87508e8b":"code","7b99f65b":"code","fbf69037":"code","fe0ac712":"code","0ac381cc":"code","41312e4c":"code","d5d90208":"code","f6d18b53":"code","4b5b526c":"code","da1ed808":"code","cc305fd8":"code","2b5a8a0f":"code","f9a9df76":"code","c739785b":"code","c2dff52e":"code","5eb2d453":"code","226dcd85":"code","34da2d53":"code","34973abd":"code","10034eb8":"code","826945c1":"code","ee1cecc5":"code","77e2fd67":"code","17b5c6d7":"code","54abe3a0":"code","20eac93f":"code","c8fd4aaf":"code","90dfefac":"code","f145266a":"code","ac6237cd":"code","d4f5b279":"code","ce7e44b8":"code","5ec08821":"code","f9e77922":"code","cb459ee3":"code","699073b6":"code","39fe1b09":"code","19ac3fcb":"code","a5060278":"code","4e2ce995":"code","7f6a8340":"markdown","e75fa082":"markdown","226f7af3":"markdown","e5147e43":"markdown","88121953":"markdown","eb1dfd3f":"markdown","2c2021dd":"markdown","d5665e0e":"markdown","52964987":"markdown","0ce05b28":"markdown","9f53d96d":"markdown","75f5255d":"markdown","a3c8d6cb":"markdown","0d26ecdb":"markdown","5b7d3701":"markdown","86c4b94a":"markdown","d6d2fc79":"markdown","d2a47050":"markdown","0d68d8e2":"markdown","134dc845":"markdown","c2db67b9":"markdown","d696ab86":"markdown"},"source":{"99028e01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f070c730":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n\n# warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.options.display.max_columns=81","fe99ba92":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\nprint(f\"Train data shape :{train.shape}\")\nprint(f\"Test data shape :{test.shape}\")","1379e5a0":"train.head()","4806b5a6":"train = train.drop(columns=['Id'])","4ecfd4ee":"train.columns","439d4dbf":"# how many null values in feature values\nfor feature in train.columns:\n    print(f\"{feature} : {np.round(train[feature].isnull().mean(),4)}\")","70c72c02":"numerical_feature = {feature for feature in train.columns if train[feature].dtypes != 'O'}\nprint(f\"Total no of Numerical feature : {len(numerical_feature)}\")\nprint(f'Numerical Feature are: {numerical_feature}')","670030ff":"discrete_feature = {feature for feature in numerical_feature if len(train[feature].unique())<25}\nprint(f'Total number of Discrete feature : {len(discrete_feature)}')\nprint(f\"Discrete Feature are : {discrete_feature}\")","5ba0eaa7":"continuous_feature = {feature for feature in numerical_feature if feature not in discrete_feature}\nprint(f'Total number of Discrete feature : {len(continuous_feature)}')\nprint(f\"Discrete Feature are : {continuous_feature}\")","6cf5b5f0":"categorical_feature = {feature for feature in train.columns if train[feature].dtypes == 'O'}\nprint(f\"Total no of Categorical feature : {len(categorical_feature)}\")\nprint(f'Categorical Feature are: {categorical_feature}')","008a8a13":"# Finding the relationship between the categorical feature with target feature\nmissing_value = {feature for feature in categorical_feature if train[feature].isnull().sum()>1}\n\nfor feature in missing_value:\n    data = train.copy()\n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(),1, 0)\n    # let's calculate the mean SalePrice where the information is missing or present    \n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","6b5beb63":"for feature in numerical_feature:\n    data = train.copy()\n    sns.distplot(data[feature])\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","b7ea4db1":"# plotting categorical value\nfor feature in categorical_feature:\n    data = train.copy()\n    sns.barplot(train[feature], train['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('Sale Price')\n    plt.title(feature)\n    plt.show()","2eadb2b5":"plt.figure(figsize=(30,30))\nsns.heatmap(train.corr(), annot=True)","cb94d272":"train.head()","500bcae9":"for feature in train.columns:\n    print(f\"{feature} : {np.round(train[feature].isnull().mean(),2)}\")","94fd60df":"# replcing null values with median\nfor feature in numerical_feature:\n    train[feature] = train[feature].fillna(train[feature].median())","151734c8":"# list of variables that contain year information\nyear_feature = {feature for feature in numerical_feature if 'Yr' in feature or 'Year' in feature}\nfor feature in year_feature:\n    if feature!='YrSold':\n        ## We will capture the difference between year variable and year the house was sold for\n        train[feature]=train['YrSold']-train[feature]","9726e4c8":"train.head()","85398a0f":"# here we saw that there is no null value in numerical feature\nfor feature in numerical_feature:\n    print(f\"{feature} : {train[feature].isnull().sum()}\")","fcbc94a9":"for feature in categorical_feature:\n    print(f\"{feature}: {train[feature].value_counts()}\")","0c22220f":"for feature in categorical_feature:\n    train[feature] = np.where(train[feature].isnull(), 'Missing', train[feature])","cc1c6456":"for feature in categorical_feature:\n    print(f\"{feature}: {train[feature].isnull().sum()}\")","404954a1":"train.isnull().sum().sum()","15a2a859":"LEncoder = LabelEncoder()\nfor feature in categorical_feature:\n    train[feature] = LEncoder.fit_transform(train[feature])","6415285b":"train.head()","5ceb828f":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    train[feature]=np.log(train[feature])","b0f92b4f":"train.head()","79c1cb69":"feature_scaling = {feature for feature in train.columns if 'SalePrice' not in feature}","87508e8b":"MMS = MinMaxScaler()\nMMS.fit(train)\nMMS.transform(train)","7b99f65b":"X_train = train.drop(['SalePrice'], axis=1)\ny_train = train[['SalePrice']]","fbf69037":"feature_selection = SelectFromModel(Lasso(alpha=0.005, random_state=0))# remember the random state in this function\nfeature_selection.fit(X_train, y_train)","fe0ac712":"feature_selection.get_support()","0ac381cc":"# let's print the number of total and selected features\n\n# this is how we can make a list of the selected features\nselected_feature = X_train.columns[(feature_selection.get_support())]\n\n# let's print some stats\nprint(f'total features: {X_train.shape[1]}')\nprint(f'selected features: {len(selected_feature)}')","41312e4c":"selected_feature","d5d90208":"X_train = X_train[selected_feature]\nX_train.head()","f6d18b53":"y_train.head()","4b5b526c":"test.head()","da1ed808":"numerical_feature = {feature for feature in test.columns if test[feature].dtypes != 'O'}\nprint(f\"Total no of Numerical feature : {len(numerical_feature)}\")\nprint(f'Numerical Feature are: {numerical_feature}')","cc305fd8":"# replcing null values with median\nfor feature in numerical_feature:\n    test[feature] = test[feature].fillna(test[feature].median())\n    ","2b5a8a0f":"# list of variables that contain year information\nyear_feature = {feature for feature in numerical_feature if 'Yr' in feature or 'Year' in feature}\nfor feature in year_feature:\n    if feature!='YrSold':\n        ## We will capture the difference between year variable and year the house was sold for\n        test[feature]=test['YrSold']-test[feature]\n\nfor feature in categorical_feature:\n    test[feature] = np.where(test[feature].isnull(), 'Missing', test[feature])\n    \n# convert categorical data into numerical data\nLEncoder = LabelEncoder()\nfor feature in categorical_feature:\n    test[feature] = LEncoder.fit_transform(test[feature])\n\n# Any distribution into Gaussian Distribution\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\n\nfor feature in num_features:\n    test[feature]=np.log(test[feature])\n\n# Feature scaling\nMMS = MinMaxScaler()\nMMS.fit(test)\nMMS.transform(test)\n\ntest.head()","f9a9df76":"submission = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsubmission.head()","c739785b":"test['SalePrice'] = submission.SalePrice\ntest.head()","c2dff52e":"X_test = test.drop(['Id', 'SalePrice'], axis=1)\ny_test = test[['SalePrice']]\nX_test.head()","5eb2d453":"feature_selection = SelectFromModel(Lasso(alpha=0.005, random_state=0))# remember the random state in this function\nfeature_selection.fit(X_test, y_test)","226dcd85":"# let's print some stats\nprint(f'total features: {X_test.shape[1]}')\nprint(f'selected features: {len(selected_feature)}')\n\nX_test = X_test[selected_feature]\nX_test.head()","34da2d53":"X_test = X_test[selected_feature]\nX_test.head()\n#y_test.head()","34973abd":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","10034eb8":"# Instantiate a dict (+function) for storing model scores\nscores = {}\ndef get_cv_score(estimator):\n    return np.sqrt(-1 * cross_val_score(estimator, X=X_train, y=y_train, cv=5, scoring='neg_mean_squared_error', n_jobs=-1))","826945c1":"# Start with LinearRegressor Model\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_pred = lin_reg.predict(X_test)\n\nscores['linear_regression'] = get_cv_score(lin_reg)\nscores['linear_regression']","ee1cecc5":"# SVM with Linear Kernel\n\"\"\"\nlinear_svr = SVR(kernel='linear', \n                     C=10, \n                     epsilon=0.01, \n                     gamma=0.0005)\nscores['linear_svr'] = get_cv_score(linear_svr)\n\"\"\"","77e2fd67":"# SVM with RBF kernel\n\"\"\"\nsvr = SVR(kernel='rbf', \n              C=10, \n              epsilon=0.01, \n              gamma=0.0005)\nscores['svr'] = get_cv_score(svr)\n\"\"\"","17b5c6d7":"# Random Forest\nrfr = RandomForestRegressor(n_estimators=250, \n                                max_depth=15, \n                                min_samples_leaf=5, \n                                min_samples_split=5, \n                                n_jobs=-1,\n                               random_state=42)\nscores['rfr'] = get_cv_score(rfr)","54abe3a0":"# Gradient Boosting\ngbr = GradientBoostingRegressor(n_estimators=350, \n                                    learning_rate=0.1, \n                                    loss='huber',\n                                   random_state=42)\nscores['gbr'] = get_cv_score(gbr)","20eac93f":"# LGBM\nlgbr = LGBMRegressor(objective='regression',\n                        n_estimators=300,\n                        learning_rate=0.1,\n                        random_state=42)\nscores['lgbr'] = get_cv_score(lgbr)","c8fd4aaf":"# AdaBoost with DT Base Estimator\nada = AdaBoostRegressor(n_estimators=150, \n                            random_state=42)\nscores['ada'] = get_cv_score(ada)","90dfefac":"# Ending with XGBoost\nxgb = XGBRegressor(n_estimators=300,\n                      max_depth=5, \n                      learning_rate=0.1,\n                      random_state=42)\nscores['xgb'] = get_cv_score(xgb)","f145266a":"# Evaluate models before any serious Hyperparameter tuning\nprint(f\"AdaBoost: {scores['ada'].mean()}\")\nprint(f\"LGBM: {scores['lgbr'].mean()}\")\nprint(f\"GradientBoosting: {scores['gbr'].mean()}\")\nprint(f\"RandomForest: {scores['rfr'].mean()}\")\n#print(f\"Linear SVR: {scores['linear_svr'].mean()}\")\n#print(f\"Kernel SVR: {scores['svr'].mean()}\")\nprint(f\"XGBoost: {scores['xgb'].mean()}\")","ac6237cd":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n\n# Support Vector Regressor\nsvr = SVR(C= 20, epsilon= 0.008, gamma=0.0003)\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=2200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingRegressor(estimators=[\n                                ('xgboost',xgboost), \n                                ('lightgbm',lightgbm), \n                                ('svr',svr),  \n                                ('gbr',gbr), \n                                ('rf',rf)],\n                                final_estimator=xgboost,\n                                n_jobs=-1)\n","d4f5b279":"# Stacking Regressor algorithm\nstack_gen.fit(X_train, y_train)\n","ce7e44b8":"# Random Forest algorithm\nrf.fit(X_train, y_train)","5ec08821":"# XGBoost algorithm\nxgboost.fit(X_train, y_train)","f9e77922":"# Gradient Boost algorithm\ngbr.fit(X_train, y_train)","cb459ee3":"# SVR algorithm\nsvr.fit(X_train, y_train)","699073b6":"# LGBM\nlightgbm.fit(X_train, y_train)","39fe1b09":"0.1 + 0.2 + 0.2 + 0.1 + 0.05 + 0.35","19ac3fcb":"def predictions(X):\n    return ((0.1 * svr.predict(X)) + \\\n            (0.2 * gbr.predict(X)) + \\\n            (0.2 * xgboost.predict(X)) + \\\n            (0.1 * lightgbm.predict(X)) + \\\n            (0.05 * rf.predict(X)) + \\\n            (0.35 * stack_gen.predict(X)))","a5060278":"# Get the submission file ready, Remember to invert the log transform we applied earlier\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(predictions(X_test)))\n\nsubmission.to_csv(\"submission_regression.csv\", index=False)","4e2ce995":"print(\"That's All for this one\")","7f6a8340":"**Count Null values in percentage:** ","e75fa082":"### Importing neccessary packages:","226f7af3":"There is null values in our data, we cleaned all null values\n### Convert Categorical features into Numerical Feature by Encoding methods  \n    The train dataset was in an nominal categorical data, so we use nomical encoding methods to converts the categorical data into numerical values","e5147e43":"### **Fit Model With an Algorithm:** ","88121953":"### **separating numerical feature :**","eb1dfd3f":"### **Find the correlation between dependent and independent feature:**","2c2021dd":"### **Feature Scaling:**\n    In this dataset, we use MinMaxScaler for better performance bcoz we dont have negative value so use range between 0 to 1","d5665e0e":"**Importing Dataset:**","52964987":"**We make use of a StackingRegressor which takes collection of models and aggregates their predictions by having a meta-learner treat it as independent variables and the true values as the targets. This is another form of Ensembling.**","0ce05b28":"**From this analysis, there is few features are in normal distriution other are not belongs to the Gaussian distribution, we want to convert those distribution into a Gaussian distributed**","9f53d96d":"#### **Replacing categorical null values with String values:**","75f5255d":"##### **Cleaning Null values in Numerical feature:** ","a3c8d6cb":"### **Feature Engineering:** \n     In this process, cleaning all the null values with replace of mean, median, mode etc.. And replace informate values and its most important steps in machine learning. Convert the categorical feature with numerical feature for better accuray as well as fit the data into ml algorithm","0d26ecdb":"### **Perform Same method to test data set**","5b7d3701":"### **separating categorical feature:**","86c4b94a":"### Explotary Data Analysis:","d6d2fc79":"**Apply Feature Selection.First, I specify the Lasso Regression model, and I select a suitable alpha (equivalent of penalty).The bigger the alpha the less features that will be selected.Then I use the selectFromModel object from sklearn, which will select the features which coefficients are non-zero**","d2a47050":"**From this analysis we saw that there is an imbalance in the categorical feature which the value counts is higher so we not recommended to replacing null values with most frequent values**","0d68d8e2":"##### **Perform Lognormal distribution for the Numerical values which are not Normal distributed**","134dc845":"**All the categorical feature are converted into numerical feature**","c2db67b9":"### **Drop Id feature:** \n    Its not important feature so we drop it","d696ab86":"### **Hyperparameter Tunning:** \n **After Completed the feature Selection then apply into algorithm those dataset one remaining thing is to create an hyperparameter for getting more accuracy results and best fits withs the algorithm**"}}