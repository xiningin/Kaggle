{"cell_type":{"c56080c8":"code","af8bbecc":"code","2c8c8fc1":"code","d6432fb8":"code","83f6bddf":"code","7e338c13":"code","1859b483":"code","3805f640":"code","c508bb69":"code","e0697072":"code","7ca3635d":"code","45bd704a":"code","e691a410":"code","fd3d7063":"code","2186020b":"code","73713249":"code","8606f35a":"code","9efe9b9d":"code","1a2d9411":"code","92a91bc6":"code","7dd82126":"code","0117902f":"code","b737d648":"code","0098cdd9":"code","c4ee9403":"code","fc6a82b2":"code","e60cc9c1":"code","1e68023c":"code","f6e5d4cf":"code","8a35359a":"code","a2983421":"code","64a1358e":"code","8cc1eccb":"code","5f0f5137":"code","d72ea2f2":"code","39c59273":"code","2e1de6c9":"code","5ae0e650":"code","384a05bd":"code","e4df5f47":"code","654d28b6":"code","bdfba4d8":"code","91ebc5b5":"code","84578247":"code","bf72b7ed":"code","267a3a1c":"code","9309333a":"code","b2c49de6":"code","c1c79e99":"code","54c59614":"code","3741d778":"code","42dba4dc":"code","d308c544":"code","529726b2":"code","f97aec13":"code","b5fb2bca":"code","a4571609":"code","65f1f9e8":"code","3407f8b7":"code","4a23137c":"markdown","9ac94e36":"markdown","bf10478d":"markdown","3b63aeed":"markdown","029fba2f":"markdown","4e1a6863":"markdown"},"source":{"c56080c8":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier\nfrom sklearn.metrics import roc_curve, auc, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.tree import DecisionTreeClassifier\n","af8bbecc":"# data load\ndeposit_df_train = pd.read_csv('\/kaggle\/input\/devrepublik03\/training_set.csv') # set for training-testing\ntest_df = pd.read_csv('\/kaggle\/input\/devrepublik03\/validation_set.csv') # set for final validation\ndeposit_df_train.head()","2c8c8fc1":"# make target feature binary 0\/1\ndeposit_df_train['deposit'] = deposit_df_train['deposit'].apply(lambda x: 1 if x == 'yes' else 0)\ncolumns_to_keep = ['age','job','marital','education','default','balance','housing','loan','contact','day','month','poutcome']\ntest_df = test_df[columns_to_keep]\ntest_df.shape\n\n# ,,","d6432fb8":"# # \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 # month \u0417\u0430\u043c\u0435\u043d\u0430: \n# deposit_df_train.replace(to_replace=['jan', 'feb', 'dec','mar','apr','may','jun','jul','aug','sep','oct','nov'],\n#            value= ['winter','winter','winter','spring','spring','spring','summer','summer','summer','autumn','autumn','autumn'], \n#            inplace=True)","83f6bddf":"deposit_df_train['month'].value_counts()","7e338c13":"# \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 #job \u0437\u0430\u043c\u0435\u043d\u0430: \ndeposit_df_train.replace(to_replace=['unknown', 'housemaid', 'entrepreneur','unemployed','student','self-employed'],\n           value= ['other','other','other','other','other','other'], \n           inplace=True)","1859b483":"deposit_df_train['job'].value_counts()","3805f640":"import plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c508bb69":"# \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 #balance \u0437\u0430\u043c\u0435\u043d\u0430:\ndeposit_df_train.loc[(deposit_df_train['balance'] < 0), 'balance'] = 0\ndeposit_df_train.loc[(deposit_df_train['balance'] >= 0) & (deposit_df_train['balance'] < 1500), 'balance'] = 1\ndeposit_df_train.loc[(deposit_df_train['balance'] >= 1500) & (deposit_df_train['balance'] < 3000), 'balance'] = 2\ndeposit_df_train.loc[(deposit_df_train['balance'] > 3000) , 'balance'] = 3","e0697072":"## plot the distribution of  balance\n# plot the distribution of bore\nbore_mean = deposit_df_train['balance'].mean()\n\nbore_mean = deposit_df_train['balance'].mean()\nbore_median = deposit_df_train['balance'].median()\nbore_std = deposit_df_train['balance'].std()\nbore_missing = deposit_df_train['balance'].isnull().sum()\n\nplt.figure(figsize=(10,6))\nplt.subplot(211)\ndeposit_df_train['balance'].plot.hist(bins=30)\nplt.axvline(bore_mean, color='red', linestyle='--')\nplt.axvline(bore_median, color='green', linestyle='--')\nplt.axvline(bore_mean - bore_std, color='black', linestyle='--')\nplt.axvline(bore_mean + bore_std, color='black', linestyle='--')\nplt.title('Distribution of balance')\n# plt.xlabel('Age')\nplt.legend(['Mean', 'Median', 'Mean - Std', 'Mean + Std'])\n\nplt.subplot(212)\nsns.boxplot(deposit_df_train['balance'])\n\nplt.show()","7ca3635d":"print(deposit_df_train['balance'].mean())","45bd704a":"deposit_df_train['balance'].value_counts()","e691a410":"deposit_df_train.head()","fd3d7063":"deposit_df_train['deposit'].value_counts()","2186020b":"deposit_df_train.head()","73713249":"deposit_df_train.isnull().sum()","8606f35a":"deposit_df_train.shape","9efe9b9d":"#labelEncoder for all non numerical data\n# deposit_df_train_labeled = deposit_df_train.drop('deposit', axis=1).apply(LabelEncoder().fit_transform)","1a2d9411":"test_df","92a91bc6":"# # \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 #month \u0437\u0430\u043c\u0435\u043d\u0430: \n# test_df.replace(to_replace=['jan', 'feb', 'dec','mar','apr','may','jun','jul','aug','sep','oct','nov'],\n#            value= ['winter','winter','winter','spring','spring','spring','summer','summer','summer','autumn','autumn','autumn'], \n#            inplace=True)","7dd82126":"# \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 #job \u0437\u0430\u043c\u0435\u043d\u0430: \ntest_df.replace(to_replace=['unknown', 'housemaid', 'entrepreneur','unemployed','student','self-employed'],\n           value= ['other','other','other','other','other','other'], \n           inplace=True)","0117902f":"# \u0412 \u043a\u043e\u043b\u043e\u043d\u043a\u0435 #balance \u0437\u0430\u043c\u0435\u043d\u0430:\ndeposit_df_train.loc[(deposit_df_train['balance'] < 0), 'balance'] = 0\ndeposit_df_train.loc[(deposit_df_train['balance'] >= 0) & (deposit_df_train['balance'] < 1500), 'balance'] = 1\ndeposit_df_train.loc[(deposit_df_train['balance'] >= 1500) & (deposit_df_train['balance'] < 3000), 'balance'] = 2\ndeposit_df_train.loc[(deposit_df_train['balance'] > 3000) , 'balance'] = 3","b737d648":"# DROP SOME COLUMNS becouse of low correlations \/ hihgt variance and no economyc sence: pdays, previous, campaign, education\ndeposit_df_train_labeled = deposit_df_train.drop(['deposit','pdays','previous','campaign'], axis=1).apply(LabelEncoder().fit_transform)\ntest_df = test_df.apply(LabelEncoder().fit_transform)\n\n# 'marital','education'","0098cdd9":"deposit_df_train_labeled.head()","c4ee9403":"deposit_df_train_labeled['month'].value_counts()","fc6a82b2":"test_df.head()","e60cc9c1":"test_df['month'].value_counts()","1e68023c":"# train\/test split\n\nX = deposit_df_train_labeled\ny = deposit_df_train['deposit']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","f6e5d4cf":"# Adaboost\nclf_ada = AdaBoostClassifier(\n    DecisionTreeClassifier(max_depth=1),\n    n_estimators=148\n)\n\nclf_ada.fit(X_train, y_train)\ntest_y_pred = clf_ada.predict(X_test)\n\nprint(\"Sklearn AdaBoost classifier:\")\nprint(f\" - accuracy_score: {accuracy_score(y_test, test_y_pred): .5f}\")\nprint(f\" - f1_score: {f1_score(y_test, test_y_pred): .5f}\")","8a35359a":"# make predictions on a test set and get AUC score\ny_pred = clf_ada.predict_proba(X_test)\nroc_auc_score(y_test, y_pred[:,1])","a2983421":"# parameters = {\n#     'n_estimators': list(range(101, 200)) \n#    }\n\n# #  'max_depth': list(range(100, 1000)), ","64a1358e":"# base_model = AdaBoostClassifier()\n# gs_model = GridSearchCV(\n#     base_model, \n#     parameters, \n#     verbose=True,\n#     scoring='roc_auc')","8cc1eccb":"# # Adaboost\n# gs_model.fit(X_train, y_train)\n# gs_model.best_score_\n# gs_model.best_params_\n# gs_model.best_estimator_","5f0f5137":"# AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n#                    n_estimators=148, random_state=None)\n\n# roc_auc = 0.7760502293049913","d72ea2f2":"# y_pred_gs = gs_model.predict(X_test)","39c59273":"# # calculate the AUC\n# y_pred_prob_gs = gs_model.predict_proba(X_test)\n# fpr, tpr, thresholds = roc_curve(y_true=y_test, y_score=y_pred_prob_gs[:,1], pos_label=1)\n\n# roc_auc = auc(fpr, tpr)\n# roc_auc","2e1de6c9":"pip install xgboost","5ae0e650":"from xgboost import XGBClassifier","384a05bd":"test_size = 0.2                # proportion of dataset to be used as test set\ncv_size = 0.2                  # proportion of dataset to be used as cross-validation set\nN = 20                        # for feature at day t, we use lags from t-1, t-2, ..., t-N as features\n\nn_estimators = 100             # Number of boosted trees to fit. default = 100\nmax_depth = 6                  # Maximum tree depth for base learners. default = 3\nlearning_rate = 0.1            # Boosting learning rate (xgb\u2019s \u201ceta\u201d). default = 0.1\nmin_child_weight = 1           # Minimum sum of instance weight(hessian) needed in a child. default = 1\nsubsample = 1                  # Subsample ratio of the training instance. default = 1\ncolsample_bytree = 1           # Subsample ratio of columns when constructing each tree. default = 1\ncolsample_bylevel = 1          # Subsample ratio of columns for each split, in each level. default = 1\ngamma = 0                      # Minimum loss reduction required to make a further partition on a leaf node of the tree. default=0\n\nmodel_seed = 100\n\nfontsize = 14\nticklabelsize = 14\n\n\n\nmodelXGB = XGBClassifier(n_estimators=n_estimators,\n                     max_depth=max_depth,\n                     learning_rate=learning_rate,\n                     min_child_weight=min_child_weight,\n                     subsample=subsample,\n                     colsample_bytree=colsample_bytree,\n                     colsample_bylevel=colsample_bylevel,\n                     gamma=gamma)\n\nmodelXGB.fit(X_train, y_train)","e4df5f47":"test_y_pred3 = modelXGB.predict(X_test)","654d28b6":"# make predictions on a test set and get AUC score\n\ny_pred = modelXGB.predict_proba(X_test)\n\nroc_auc_score(y_test, y_pred[:,1])","bdfba4d8":"print(\"Sklearn XGBoost classifier:\")\nprint(f\" - accuracy_score: {accuracy_score(y_test, test_y_pred3): .5f}\")\n\nprint(f\" - f1_score: {f1_score(y_test, test_y_pred3): .5f}\")","91ebc5b5":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(modelXGB, X_train, y_train, cv=5).std()","84578247":"# subm = pd.DataFrame() # create an empty DF for final submissiob\n# subm['deposit'] = modelXGB.predict_proba(test_df)[:,1] # create a new column that holds probability of first (1) class\n# subm.reset_index(drop=False, inplace=True) # duplicate index as a columns\n# subm.to_csv('my_submission.csv', index=False) # save as csv file","bf72b7ed":"# subm.head() ","267a3a1c":"##","9309333a":"from catboost import CatBoostClassifier","b2c49de6":"model_cat = CatBoostClassifier(iterations=180, depth=7, learning_rate=0.1)\n\n# model_cat = CatBoostClassifier(iterations=180, depth=7, learning_rate=0.1)\nmodel_cat.fit(X_train, y_train)","c1c79e99":"test_y_pred4 = model_cat.predict(X_test)","54c59614":"# make predictions on a test set and get AUC score\n\ny_pred = model_cat.predict_proba(X_test)\n\nroc_auc_score(y_test, y_pred[:,1])","3741d778":"0.8033339760774776 - 0.80172","42dba4dc":"# Best model\n\nsubm = pd.DataFrame() # create an empty DF for final submissiob\nsubm['deposit'] = model_cat.predict_proba(test_df)[:,1] # create a new column that holds probability of first (1) class\nsubm.reset_index(drop=False, inplace=True) # duplicate index as a columns\nsubm.to_csv('my_submission.csv', index=False) # save as csv file","d308c544":"# from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n# from sklearn.ensemble import HistGradientBoostingClassifier","529726b2":"# model_hist = HistGradientBoostingClassifier(max_iter=170, max_depth=7, learning_rate = 0.11)\n# model_hist.fit(X_train, y_train)","f97aec13":"# make predictions on a test set and get AUC score\n\n# test_y_pred5_hist = model_hist.predict(X_test)\n\n# y_pred = model_hist.predict_proba(X_test)\n\n# roc_auc_score(y_test, y_pred[:,1])","b5fb2bca":"# # Analysys of DF\n# from pandas_profiling import ProfileReport\n# ProfileReport(deposit_df_train, title='Pandas Profiling Report', html={'style':{'full_width':True}})","a4571609":"# parameters = {\n#     'max_iter' : list(range(50, 300)), \n#     'max_depth': list(range(2, 10))\n#    }","65f1f9e8":"# base_model = HistGradientBoostingClassifier()\n# gs_model_hist = GridSearchCV(\n#     base_model, \n#     parameters, \n#     verbose=True,\n#     scoring='roc_auc')","3407f8b7":"# # # Hist Boost\n# gs_model_hist.fit(X_train, y_train)\n# gs_model_hist.best_score_\n# gs_model_hist.best_params_\n# gs_model_hist.best_estimator_","4a23137c":"## Model Hist Boost with Greed Search","9ac94e36":"## CatBoost","bf10478d":"Model AdaBoost with Greed Search","3b63aeed":"**Steps:**\n1. Import libraries\n2. Import dataframe\n3. Data processing\n4. Bild a model\n5. Model testing","029fba2f":"# XGBoost","4e1a6863":"## sklearn.ensemble.HistGradientBoostingClassifier"}}