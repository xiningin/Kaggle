{"cell_type":{"db8b4c51":"code","b9aad1d8":"code","75a5f9cc":"code","e06df622":"code","a3ae2073":"code","efa24000":"code","8186f2c7":"code","309f932a":"code","5ad72ac6":"code","051c7f9d":"code","d9030f84":"code","2af438e0":"code","82949258":"code","3e797d5e":"code","45853079":"code","c3b93d9d":"code","3f70a9e4":"code","8ff8faa4":"code","ba569f21":"code","195c6a2d":"code","967ffb17":"code","8ed54874":"code","e5fe1344":"code","7fd177b6":"code","a9e4261d":"code","6979b66f":"code","3fce8751":"code","e5b660bf":"markdown","9923a014":"markdown","289a9388":"markdown","6a971cec":"markdown","e15fb830":"markdown","6e64af4a":"markdown","fc5fd6b0":"markdown","356e0439":"markdown","adbfdc5f":"markdown"},"source":{"db8b4c51":"import os\nimport gc\nimport math\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport torch\nimport torch.nn as nn\n\nimport matplotlib.pyplot as plt","b9aad1d8":"# run this if using kaggle notebooks\n!cp -r ..\/input\/lux-ai-2021\/* .\n# if working locally, download the `simple\/lux` folder from here https:\/\/github.com\/Lux-AI-Challenge\/Lux-Design-2021\/tree\/master\/kits\/python\n# and we recommend following instructions in there for local development with python bots\n\n# for kaggle-environments\nfrom lux.game import Game\nfrom lux.game_map import Cell, RESOURCE_TYPES, Position\nfrom lux.constants import Constants\nfrom lux.game_constants import GAME_CONSTANTS\nfrom lux import annotate\nimport math\nimport sys\nfrom kaggle_environments import make","75a5f9cc":"class Config:\n    DAY_LENGTH  = GAME_CONSTANTS[\"PARAMETERS\"][\"DAY_LENGTH\"]\n    NIGHT_LENGTH = GAME_CONSTANTS[\"PARAMETERS\"][\"NIGHT_LENGTH\"]\n    MAX_DAYS= GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"]\n    \n    WORKER_CAPACITY = GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][\"WORKER\"]\n    CART_CAPACITY = GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][\"CART\"]\n    \n    CITY_LIGHT_UPKEEP= GAME_CONSTANTS[\"PARAMETERS\"][\"LIGHT_UPKEEP\"][\"CITY\"]\n    WORKER_LIGHT_UPKEEP= GAME_CONSTANTS[\"PARAMETERS\"][\"LIGHT_UPKEEP\"][\"WORKER\"]\n    CART_LIGHT_UPKEEP= GAME_CONSTANTS[\"PARAMETERS\"][\"LIGHT_UPKEEP\"][\"CART\"]\n    \n    \n    MAX_WOOD_AMOUNT= GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_WOOD_AMOUNT\"]\n    CITY_BUILD_COST= GAME_CONSTANTS[\"PARAMETERS\"][\"CITY_BUILD_COST\"]\n    CITY_ADJACENCY_BONUS= GAME_CONSTANTS[\"PARAMETERS\"][\"CITY_ADJACENCY_BONUS\"]\n    \n    WORKER_COLLECTION_RATE_WOOD= GAME_CONSTANTS[\"PARAMETERS\"][\"WORKER_COLLECTION_RATE\"][\"WOOD\"]\n    WORKER_COLLECTION_RATE_COAL= GAME_CONSTANTS[\"PARAMETERS\"][\"WORKER_COLLECTION_RATE\"][\"COAL\"]\n    \n    RESOURCE_TO_FUEL_RATE_WOOD= GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_TO_FUEL_RATE\"][\"WOOD\"]\n    \n    \n    #Cooldown Constants\n    WORKER_ACTION_COOLDOWN= GAME_CONSTANTS[\"PARAMETERS\"][\"UNIT_ACTION_COOLDOWN\"][\"WORKER\"]","e06df622":"def get_relative_position_from_cell(source_cell, target_cell):\n    (xsource, ysource)=(source_cell.pos.x, source_cell.pos.y)\n    (xtarget, ytarget)=(target_cell.pos.x, target_cell.pos.y)\n    dist = np.sqrt( (xsource-xtarget)**2 + (ysource-ytarget)**2 )\n    left=right=up=down=center=0\n    \n    if (xsource == xtarget) and (ysource == ytarget):\n        center=1\n    if xtarget < xsource:\n        left=1\n    if xtarget > xsource:\n        right=1\n    if ytarget < ysource:\n        down=1\n    if ytarget > ysource:\n        up=1\n    \n    return (dist, left, right, up, down, center)\n\n\ndef get_resource_cells(game_state):\n    game_map=game_state.map\n    (width, height)=(game_map.width, game_map.height)\n    resource_cells=[]\n    for i in range(width):\n        for j in range(height):\n            cell=game_map.get_cell(i, j)\n            if cell.has_resource():\n                resource_cells.append(cell)\n    return resource_cells\n\ndef get_citytile_cells(game_state):\n    game_map=game_state.map\n    (width, height)=(game_map.width, game_map.height)\n    citytiles_cells=[]\n    for i in range(width):\n        for j in range(height):\n            cell=game_map.get_cell(i, j)\n            citytile=cell.citytile\n            if citytile is None:\n                continue\n            citytiles_cells.append(citytile)\n    return citytiles_cells","a3ae2073":"def get_nearest_resources(unit, game_state, topk=5):\n    (x, y) = (unit.pos.x, unit.pos.y)\n    game_map=game_state.map\n    cell=game_map.get_cell_by_pos(unit.pos)\n    (width, height)=(game_map.width, game_map.height)\n    resource_cells=get_resource_cells(game_state)\n    nearest_resources=[]\n    \n    max_distance=np.sqrt(width**2 + height**2)\n    for rcell in resource_cells:\n        if rcell.resource.type != RESOURCE_TYPES.WOOD:\n            continue\n        (dist, left, right, up, down, center) = get_relative_position_from_cell( cell, rcell )\n        amount=rcell.resource.amount\/Config.MAX_WOOD_AMOUNT\n        dist=dist\/max_distance\n        \n        nearest_resources.append({\"dist\":  dist, \n                                  'amount': amount,\n                                  \"left\": left, \"right\": right, \"up\": up, \"down\": down, \"center\": center})\n    nearest_resources=nearest_resources.sort(key = lambda r: r['dist'])\n    return nearest_resources\n\ndef get_game_state_worker_observations(game_state):\n    team=game_state.id\n    players=game_state.players\n    unit=players[team].units[0]\n    \n    resource_obs=get_nearest_resources(unit, game_state)","efa24000":"class Convmap:\n    def __init__(self, game_state):\n        self.game_state=game_state\n        self.game_map=game_state.map\n        self.width=self.game_map.width\n        self.height = self.game_map.height\n        \n        self.players=self.game_state.players\n        self.team=self.game_state.id\n        self.turn=self.game_state.turn\n    \n    def get_city_features(self):\n        city_feats=np.zeros( (self.width, self.height, 3) )\n        for player in self.players:\n            for city_id, city in player.cities.items():\n                num_tiles=len(city.citytiles)\n                fuel=city.fuel\n                light_upkeep=city.get_light_upkeep()\n                nights_can_survive = min(10, fuel\/light_upkeep)\/10 # if can survive 3 nights value 0.3\n                \n                for citytile in city.citytiles:\n                    (x, y) = (citytile.pos.x, citytile.pos.y)\n                    if city.team == self.team:\n                        city_feats[x][y][0] = 1\n                    else:\n                        city_feats[x][y][1] = 1\n                    city_feats[x][y][2] = nights_can_survive\n        return city_feats\n    \n    def get_resource_features(self):\n        rfeats=np.zeros((self.width,self.height, 2))\n        for i in range(self.width):\n            for j in range(self.height):\n                cell=self.game_map.get_cell(i, j)\n                if not cell.has_resource():\n                    continue\n                rfeats[i][j][0] = 1\n                resource=cell.resource\n                if resource and (RESOURCE_TYPES.WOOD == resource.type):\n                    rfeats[i][j][1]= resource.amount\/Config.MAX_WOOD_AMOUNT\n        return rfeats   \n    \n    \n    def get_unit_features(self):\n        ufeats=np.zeros((self.width, self.height, 4))\n        for player in self.players:\n            for unit in player.units:\n                (x, y) = (unit.pos.x, unit.pos.y)\n                opp_team = 0 if (unit.team==self.team) else 1\n                \n                wood = unit.cargo.wood\n                coal = unit.cargo.coal\n                uranium = unit.cargo.uranium\n                total_cargo = (wood + coal + uranium)\/Config.WORKER_CAPACITY\n                \n                cooldown=unit.cooldown\/Config.WORKER_ACTION_COOLDOWN\n                \n                ufeats[x][y][0] = 1-opp_team\n                ufeats[x][y][1] = opp_team\n                ufeats[x][y][2] = total_cargo\n                ufeats[x][y][3] = cooldown\n        return ufeats\n    \n    def get_road_features(self):\n        feats=np.zeros((self.width,self.height, 1))\n        for i in range(self.width):\n            for j in range(self.height):\n                cell=self.game_map.get_cell(i, j)\n                road = cell.road\n                if (not road) or (road == 0):\n                    continue\n                feats[i][j][0] = road\/6\n        return feats\n                \n    def get_conv_features(self):\n        unit_feats=self.get_unit_features()\n        city_feats=self.get_city_features()\n        resource_feats = self.get_resource_features()\n        road_feats=self.get_road_features()\n        \n        feats=np.concatenate([unit_feats,\n                              city_feats,\n                              resource_feats,\n                              road_feats\n                             ], axis=2)\n        \n        pad_feats=np.zeros((32, 32, 10))\n        W = (32 - self.width)\/\/2\n        H = (32 - self.height)\/\/2\n        \n        pad_feats[W:-W, H:-H] = feats\n        return feats","8186f2c7":"def get_unit_relative_position(x, y, width, height):\n    dist_feats=np.zeros((width, height, 5))\n    max_dist=width *np.sqrt(2)\n    left=right=up=down=0\n    \n    dists=[]\n    for i in range(width):\n        for j in range(height):\n            dist=np.sqrt( ((x-i)**2)  + ((y-j)**2))\n            dists.append(dist)\n            if i < x:\n                left=1\n            if i > x:\n                right=1\n            if j < y:\n                down=1\n            if j > y:\n                up=1\n            dist_feats[i][j][0]=dist\/max_dist\n            dist_feats[i][j][1]=left\n            dist_feats[i][j][2]=right\n            dist_feats[i][j][3]=up\n            dist_feats[i][j][4]=down\n    dist_mean=np.mean(dists)\n    dist_std=np.std(dists)\n    \n    dist_feats[:, :, 0] = (dist_feats[:, :, 0] - dist_mean)\/dist_std\n    return dist_feats\n\ndef get_allunit_states(game_state, conv_map):\n    team=game_state.id\n    game_map = game_state.map\n    players=game_state.players\n    \n    game_completion = (1+game_state.turn)\/360\n    days_completed = min(1, (game_state.turn%40)\/30)\n    nights_completed = max(0, ((1+game_state.turn%40)-30) )\/10\n    \n    (width, height) = (game_map.width, game_map.height)\n    units_map={\n        \"cur_team\":[],\n        \"opp_team\":[]\n    }\n    \n    all_units=[]\n    #(xshift, yshift) = ((32-width)\/\/2, (32-height)\/\/2)\n    xshift=0; yshift=0;\n    for i, player in enumerate(players):\n        for unit in player.units:\n            if unit.team != game_state.id:\n                continue\n            unit_conv_map = conv_map.copy()\n            unit_feats = np.zeros(5)\n            \n            (x, y) = (unit.pos.x, unit.pos.y)\n            (xnew, ynew) = (x+xshift, y+yshift)\n            \n            cooldown=unit.cooldown\/6\n            wood=unit.cargo.wood\/100\n            dist_feats = get_unit_relative_position(xnew, ynew, width, height)\n            unit_conv_feats = np.concatenate([unit_conv_map,dist_feats], axis=2)\n            \n            unit_feats[0] = game_completion\n            unit_feats[1] = days_completed\n            unit_feats[2] = nights_completed\n            unit_feats[3] = wood\n            unit_feats[4] = cooldown\n            \n            all_units.append({\n                'unit': unit,\n                'unit_conv_feats': unit_conv_feats,\n                'unit_feats': unit_feats\n            })\n    return all_units","309f932a":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv1=nn.Conv2d(in_channels, out_channels, 3,  padding=1)\n        self.relu1=nn.ReLU6()\n        self.bn1=nn.BatchNorm2d( out_channels )\n        self.dropout1=nn.Dropout2d(0.1)\n        \n    def forward(self, x):\n        x=self.conv1(x)\n        x=self.relu1(x)\n        x=self.bn1(x)\n        x=self.dropout1(x)\n        return x\n\n\nclass ConvBackbone(nn.Module):\n    def __init__(self):\n        super(ConvBackbone, self).__init__()\n        self.pre_bn = nn.BatchNorm2d(15)\n        self.conv0  = ConvBlock(15, 64)\n        nlayers=6\n        nchannels=64\n        self.blocks = nn.ModuleList([\n            ConvBlock(nchannels, nchannels) for _ in range(nlayers)\n        ])\n        self.bn=nn.BatchNorm1d(nchannels)\n        self.dropout=nn.Dropout(0.2)\n        self.fc=nn.Linear(nchannels, 128)\n    def forward(self, x):\n        batch_size=x.shape[0]\n        x=self.pre_bn(x)\n        x=self.conv0(x)\n        for block in self.blocks:\n            x=x+block(x)\n        x=x.view(batch_size, x.shape[1], -1).sum(dim=-1)\n        x=self.bn(x)\n        x=self.dropout(x)\n        x=self.fc(x)\n        return x\n\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.backbone=ConvBackbone()\n        self.ffn=nn.Sequential(\n            nn.BatchNorm1d(5),\n            \n            nn.Linear(5, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(32, 32),\n            nn.BatchNorm1d(32),\n            nn.ReLU6(),\n            nn.Dropout(0.2),\n            \n            nn.Linear(32, 16)\n        )\n        self._policy_net=nn.Sequential(\n            nn.BatchNorm1d(128 + 16),\n            nn.Linear(128+16 ,128),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(256, 6)\n        )\n        self._value_net= nn.Sequential(\n            nn.BatchNorm1d(128+16),\n            nn.Linear(128 + 16, 128),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU6(),\n            nn.Dropout(0.1),\n            \n            nn.Linear(256, 1)\n        )\n        \n    def forward(self, x1, x2):\n        x1=self.backbone(x1)\n        x2=self.ffn(x2)\n        x=torch.cat([x1, x2], dim=-1)\n        \n        pis = self._policy_net(x).softmax(dim=-1)\n        values=self._value_net(x)\n        \n        return (pis, values)","5ad72ac6":"def has_boundary_violation(action, x, y, width, height):\n    has_violation=0\n    if (action == 0) and (y == 0):\n        has_violation=1\n    elif (action==1) and (y+1 == height):\n        has_violation=True\n    elif (action==2) and (x+1 == width):\n        has_violation=1\n    elif (action == 3) and (x==0):\n        has_violation=1\n    return has_violation\n\ndef has_opposite_collision(boundary_violation, unit, action, game_state):\n    if boundary_violation or (action==5):\n        return 0\n    violation = 0\n    (x, y) = (unit.pos.x, unit.pos.y)\n    unit_team=unit.team\n    x_next=-1; y_next=-1;\n    if action == 0:\n        y_next = y-1\n    elif action==1:\n        y_next=y+1\n    elif action==2:\n        x_next = x+1\n    elif action==3:\n        x_next = x-1\n    \n    \n    next_team=unit_team\n    next_cell=game_state.map.get_cell(x_next, y_next)\n    next_citytile=next_cell.citytile\n    if (next_citytile):\n        next_team = next_citytile.team\n    \n    for player in game_state.players:\n        for next_unit in player.units:\n            if (next_unit.pos.x == x_next) and (next_unit.pos.y == y_next):\n                next_team=next_unit.team\n                break\n    if next_team!=unit_team:\n        violation=1\n    return violation\n    \n\ndef has_build_violation(action, unit, game_state):\n    violation=0\n    if (action==5) and (not unit.can_build(game_state.map)):\n        violation=1\n    return violation\n\ndef get_game_metrics(unit, game_state, action):\n    num_tiles=0\n    team=game_state.id\n    player=game_state.players[team]\n    num_units=len(player.units)\n    unit=player.units[0]\n    cities=player.cities\n    \n    (x, y) = (unit.pos.x, unit.pos.y)\n    \n    (width, height) = (game_state.map.width, game_state.map.height)\n    boundary_violation=has_boundary_violation(action, x, y, width, height)\n    collision_violation = has_opposite_collision(boundary_violation, unit, action, game_state)\n    build_violation = has_build_violation(action, unit, game_state)\n    \n    \n    num_cities=0\n    for _, city in cities.items():\n        num_tiles+=len(city.citytiles)\n        num_cities+=1\n    return {\n        \"num_cities\": num_cities,\n        \"num_tiles\": num_tiles,\n        \"num_units\": num_units,\n        \"boundary_violation\": boundary_violation,\n        \"build_violation\": build_violation,\n        \"collision_violation\": collision_violation,\n        \"cargo_wood\": unit.cargo.wood\n    }","051c7f9d":"def calculate_rewards(game_memory, evaluate):\n    num_move_actions=0\n    num_build_actions=0\n    max_city_count=0\n    \n    num_iterations=len(game_memory)\n    for i in range(num_iterations):\n        if i==num_iterations-1:\n            break\n        \n        action=game_memory[i]['action']\n        cur_metrics=game_memory[i][\"game_metrics\"]\n        nxt_metrics=game_memory[i+1][\"game_metrics\"]\n        \n        ncities = nxt_metrics['num_tiles'] - cur_metrics['num_tiles']\n        nunits  = nxt_metrics['num_units'] - cur_metrics['num_units']\n        \n        boundary_violation=cur_metrics[\"boundary_violation\"]\n        build_violation = cur_metrics[\"build_violation\"]\n        collision_violation = cur_metrics[\"collision_violation\"]\n        \n        game_memory[i][\"total_reward\"] = (50 * nxt_metrics['num_tiles']) + (20 * nxt_metrics['num_units'])\n        if boundary_violation:\n            game_memory[i][\"total_reward\"] -= 10\n        if build_violation:\n            game_memory[i][\"total_reward\"] -= 10\n        if collision_violation:\n            game_memory[i][\"total_reward\"] -= 10\n        \n        wood_reward=(nxt_metrics['cargo_wood'] - cur_metrics['cargo_wood'])\n        game_memory[i][\"total_reward\"] += (2 * wood_reward\/20)\n        if (not boundary_violation) and (not collision_violation) and (action!=4):\n            game_memory[i][\"total_reward\"]+=1\/40\n        game_memory[i][\"rts\"] = game_memory[i][\"total_reward\"]\n        \n    last_metrics = game_memory[num_iterations-1][\"game_metrics\"]\n    if (last_metrics[\"num_units\"] == 0) or (last_metrics[\"num_tiles\"]==0):\n        game_memory[num_iterations-1][\"total_reward\"] = -50\n        game_memory[num_iterations-1][\"rts\"] = game_memory[num_iterations-1][\"total_reward\"]\n    for mem in game_memory:\n        mem['rts'] = mem.get('total_reward', 0.0)\n    \n    num_iterations=len(game_memory)\n    for i in range(num_iterations-2, -1, -1):\n        game_memory[i]['rts'] = game_memory[i]['total_reward'] + 0.9*game_memory[i+1]['rts'] ","d9030f84":"def run_policy_network(unit, unit_conv_feats, unit_feats,\n                       worker_policy,\n                       game_state,\n                       evaluate):\n    game_map = game_state.map\n    unit_conv_feats=torch.tensor(unit_conv_feats, dtype=torch.float32).transpose(0, 2).unsqueeze(0)\n    unit_feats=torch.tensor(unit_feats, dtype=torch.float32).unsqueeze(0)\n    \n    action=0\n    worker_policy.eval()\n    with torch.no_grad():\n        (actions, values)=worker_policy(unit_conv_feats, unit_feats)\n    \n    actions=actions.view(-1).numpy()\n    values=values.view(-1).numpy()\n    \n    if evaluate:\n        if (unit.can_build(game_state.map)) and (unit.cargo.wood==100):\n            action=np.argmax(actions)\n        else:\n            action=np.argmax(actions[:5])\n    else:\n        days_completed=min(30, (game_state.turn%40))\n        if (unit.cargo.wood == 100) and (unit.can_build(game_state.map)):\n            if actions[-1] < 0.17:\n                s=actions[:5].sum()\n                actions[:5] = 0.83 * actions[:5]\/(1e-8+s)\n                actions[-1]=1-actions[:5].sum()\n            action = np.random.choice(6, p=actions)\n        else:\n            s=actions[:5].sum()\n            action = np.random.choice(5, p=actions[:5]\/(s))\n    return (action, values)","2af438e0":"def play_game(worker_policy, evaluate=False, showui=False):\n    game_state=None\n    game_memory=[]\n    game_over=False\n    def get_worker_actions(unit, game_state, actions):\n        nonlocal worker_policy\n        nonlocal game_memory\n        \n        (x, y) = (unit.pos.x, unit.pos.y)\n        conv_map = Convmap(game_state).get_conv_features()\n        all_units_feats=get_allunit_states(game_state, conv_map)\n        for unit_states in all_units_feats:\n            unit=unit_states['unit']\n            unit_conv_feats= unit_states['unit_conv_feats']\n            unit_feats=unit_states['unit_feats']\n            \n            (action, values) = run_policy_network(unit, unit_conv_feats, unit_feats,\n                                                  worker_policy,\n                                                  game_state, evaluate)\n        \n            if action == 0:\n                actions.append( unit.move('n') )\n            elif action==1:\n                actions.append( unit.move('s') )\n            elif action==2:\n                actions.append( unit.move('e') )\n            elif action==3:\n                actions.append( unit.move('w') )\n            elif action==4:\n                pass\n            elif action == 5:\n                actions.append(unit.build_city())\n        \n            game_metrics = get_game_metrics(unit, game_state, action)\n            game_memory.append({\n                'conv_map': unit_conv_feats,\n                'unit_feats': unit_feats,\n                'action': action,\n                'game_metrics': game_metrics\n            })\n        \n    def agent(observation, configuration):\n        nonlocal game_state\n        nonlocal game_over\n\n        ### Do not edit ###\n        if observation[\"step\"] == 0:\n            game_state = Game()\n            game_state._initialize(observation[\"updates\"])\n            game_state._update(observation[\"updates\"][2:])\n            game_state.id = observation.player\n\n        else:\n            game_state._update(observation[\"updates\"])\n\n        ### AI Code goes down here! ###\n        actions=[]\n        player=game_state.players[game_state.id]\n        for unit in player.units:\n            if unit.can_act():\n                get_worker_actions(unit, game_state, actions)\n        # add debug statements like so!\n        return actions\n\n    \n    env=make(\"lux_ai_2021\",\n             configuration={\"seed\": random.randint(0, 100000000), \n                            \"loglevel\": 0,\n                            \"annotations\": False,\n                            \"width\": 16,\n                            \"height\": 16\n                           },\n             debug=True)\n    \n    env.run([ agent,  \"simple_agent\"])\n    if showui:\n        env.render(mode='ipython' , width=800, height=600)\n    \n    if game_state.turn < 359:\n        game_memory.append({\n            'total_reward':0,\n            'game_metrics': {\n                \"num_units\": 0,\n                \"num_cities\": 0,\n                \"num_tiles\": 0,\n                \"boundary_violation\": 0,\n                \"cargo_wood\": 0\n            }\n        })\n        \n    calculate_rewards(game_memory, evaluate)\n    for i in range(len(game_memory)-2):\n        game_memory[i]['next_conv_map'] = game_memory[i+1]['conv_map']\n        game_memory[i]['next_unit_feats'] = game_memory[i+1]['unit_feats']        \n    game_memory=game_memory[:-2]\n    return game_memory","82949258":"def collect_train_samples(worker_policy):\n    batch_size=300\n    memory=[]\n    max_city_count=0\n    num_games=0\n    while True:\n        num_games+=1\n        game_memory=play_game(worker_policy, False, False)\n        for data in game_memory:\n            conv_map = data['conv_map']\n            unit_feats = data['unit_feats']\n            \n            next_conv_map=data['next_conv_map']\n            next_unit_feats=data['next_unit_feats']\n            \n            action=data['action']\n            total_reward=data['total_reward']\n            num_cities=data[\"game_metrics\"][\"num_cities\"]\n            max_city_count=max(max_city_count, num_cities)\n            \n            memory.append({\n                'conv_map': conv_map,\n                'unit_feats':unit_feats,\n                'next_conv_map': next_conv_map,\n                'next_unit_feats': next_unit_feats,\n                'action': action,\n                'reward': total_reward,\n                'rts': data['rts']\n            })\n        if len(memory) > batch_size:\n            break\n    return (num_games, max_city_count, memory)","3e797d5e":"def evaluate(worker_policy):\n    num_games=50\n    total_tiles=0\n    total_iterations=0\n    eval_mean_reward=0\n    for _ in range(num_games):\n        game_memory=play_game(worker_policy, True, False)\n        max_tiles=0\n        num_iterations=len(game_memory)\n        total_reward=0\n        for data in game_memory:\n            num_tiles=data[\"game_metrics\"][\"num_tiles\"]\n            max_tiles=max(max_tiles, num_tiles)\n            total_reward+=data[\"total_reward\"]\n        \n        total_reward\/=num_iterations\n        total_tiles+=max_tiles\n        total_iterations += num_iterations\n        eval_mean_reward+=total_reward\n    \n    total_tiles\/=num_games\n    total_iterations\/=num_games\n    eval_mean_reward\/=num_games\n    return (total_tiles, total_iterations, eval_mean_reward)","45853079":"def get_batched_data(worker_policy):\n    conv_map=[]; next_conv_map=[];\n    unit_feats=[]; next_unit_feats=[];\n    rewards=[];\n    actions=[]\n    rts=[]\n    \n    (num_games, max_city_count, game_memory) = collect_train_samples(worker_policy)\n    for data in game_memory:\n        conv_map.append(data['conv_map'])\n        unit_feats.append(data['unit_feats'])\n\n        next_conv_map.append( data['next_conv_map'] )\n        next_unit_feats.append(data['next_unit_feats'])\n\n        actions.append( data['action'] )\n        rewards.append( data['reward'] )\n        rts.append(data['rts'])\n\n    conv_map=np.array(conv_map)\n    next_conv_map=np.array(next_conv_map);\n    unit_feats=np.array(unit_feats) \n    next_unit_feats=np.array(next_unit_feats)\n    rewards=np.array(rewards);\n    rts=np.array(rts)\n    actions=np.array(actions);\n\n    \n    conv_map=torch.tensor(conv_map, dtype=torch.float32).transpose(1, 3)\n    next_conv_map=torch.tensor(next_conv_map, dtype=torch.float32).transpose(1, 3)\n    unit_feats=torch.tensor(unit_feats, dtype=torch.float32) \n    next_unit_feats=torch.tensor(next_unit_feats, dtype=torch.float32)\n    rewards=torch.tensor(rewards, dtype=torch.float32)\n    rts=torch.tensor(rts, dtype=torch.float32)\n    actions=torch.tensor(actions, dtype=torch.long)\n    \n    return (conv_map, next_conv_map, \n            unit_feats, next_unit_feats,\n            rewards, actions, rts)","c3b93d9d":"def get_entropy(pi):\n    H = -pi * torch.log(pi)\n    H = H.sum(dim=-1)\n    return H.mean()","3f70a9e4":"def get_old_policy_evaluation(model, batch):\n    conv_map=batch[0]\n    next_conv_map=batch[1]\n    unit_feats=batch[2] \n    next_unit_feats=batch[3]\n    rewards=batch[4]\n    actions=batch[5]\n    rts=batch[6]\n    \n    model.eval()\n    with torch.no_grad():\n        (pis, values) = worker_policy(conv_map, unit_feats)\n        (_, next_values) = worker_policy(next_conv_map, next_unit_feats)\n    \n    values=values.view(-1)\n    next_values=next_values.view(-1)\n    rewards=rewards.view(-1)\n    \n    \n    \n    Aold = (rts - values)\n    old_pis = torch.gather(pis, 1,  actions.unsqueeze(-1)).view(-1)\n    old_log_pis=torch.log(old_pis + 1e-8)\n    return (old_log_pis, Aold)\n\ndef get_loss(model, optimizer, it):\n    clamp=0.1\n    batch= get_batched_data(worker_policy)\n    \n    conv_map=batch[0]\n    next_conv_map=batch[1]\n    unit_feats=batch[2] \n    next_unit_feats=batch[3]\n    rewards=batch[4]\n    actions=batch[5]\n    rts=batch[6]\n    \n    train_actor_loss=None;train_critic_loss=None;train_entropy=None\n    (old_log_pis, Aold) = get_old_policy_evaluation(model, batch)\n    \n    num_move_actions=(actions!=5).sum().item()\n    num_build_actions=(actions==5).sum().item()\n        \n        \n    for i in range(4):\n        model.train()\n        (pis, values) = worker_policy(conv_map, unit_feats)\n        (_, next_values) = worker_policy(next_conv_map, next_unit_feats)\n        \n        values=values.view(-1)\n        next_values=next_values.view(-1)\n        rewards=rewards.view(-1)\n        \n        A = ( rts - values)\n        prob_values = torch.gather(pis, 1,  actions.unsqueeze(-1)).view(-1)\n        new_log_pis=torch.log(prob_values + 1e-8)\n        \n        r = torch.exp( new_log_pis - old_log_pis )\n        surr1 = r * Aold.detach()\n        surr2 = torch.clamp(r, 1-clamp, 1+clamp) * Aold.detach()\n        \n        actor_loss = torch.min(surr1, surr2).mean()\n        critic_loss = torch.abs(A).mean()\n        \n        H = get_entropy(pis)\n        \n        loss=actor_loss - 0.8 * critic_loss + 2 * H\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if it%10==0 and i==0:\n            print(pis[0].detach().numpy())\n            \n        train_actor_loss=actor_loss.item();\n        train_critic_loss=critic_loss.item();\n        train_entropy=H.item()\n    return (loss.item(), train_actor_loss, train_critic_loss, train_entropy, num_move_actions, num_build_actions)","8ff8faa4":"def train_model(worker_policy, optimizer):\n    eval_iterations=[]\n    eval_tiles=[]\n    eval_rewards=[]\n    \n    for it in range(300):\n        (train_loss, train_actor_loss, train_critic_loss, train_entropy,\n         num_move_actions, num_build_actions) = get_loss(worker_policy, optimizer, it)\n        if it%10 == 0:\n            torch.save(worker_policy, \"policy_network_{}.pt\".format(it+1))\n            \n            print()\n            print(\"======\"*10)\n            print(\"Iteration:{} | Loss:{:.3f}\".format(it+1, train_loss))\n            print(\"Entropy:\", train_entropy)\n            print(\"Actor Loss:{:.3f} | Critic Loss:{:.3f}\".format(train_actor_loss, train_critic_loss))\n            print(\"Number Of Move Actions:\", num_move_actions)\n            print(\"Number Of Build Actions:\",num_build_actions)\n            print()\n            print(\"===========\"*10)\n            \n            print()\n            print(\"Evaluation\")\n            (avg_tiles, avg_iterations, avg_reward)=evaluate(worker_policy)\n            print(\"Avg tiles:\", avg_tiles)\n            print(\"Avg iterations:\", avg_iterations)\n            print(\"Aveg reward:\", avg_reward)\n            \n            eval_iterations.append(avg_iterations)\n            eval_tiles.append(avg_tiles)\n            eval_rewards.append(avg_reward)\n            print()\n            print()\n        gc.collect()\n        \n    return (eval_iterations, eval_tiles, eval_rewards)","ba569f21":"%%time\n\nworker_policy=ActorCritic()\noptimizer=torch.optim.Adam(worker_policy.parameters(), lr=1e-4)\n(eval_iterations, eval_tiles, eval_rewards) = train_model(worker_policy, optimizer)\n","195c6a2d":"game_memory = play_game(worker_policy, True, True)","967ffb17":"collision_violations=0\nboundary_violations=0\n\nfor mem in game_memory:\n    game_metrics=mem['game_metrics']\n    boundary_violations+=game_metrics[\"boundary_violation\"]\n    collision_violations += game_metrics[\"collision_violation\"]\n    print(mem['total_reward'])\nprint(boundary_violations, collision_violations)","8ed54874":"len(game_memory)","e5fe1344":"plt.plot(eval_iterations)\nplt.show()","7fd177b6":"plt.plot(eval_tiles)\nplt.show()","a9e4261d":"plt.plot(eval_rewards)\nplt.show()","6979b66f":"_ = play_game(worker_policy, True, True)","3fce8751":"_ = play_game(worker_policy, True, True)","e5b660bf":"# evaluate","9923a014":"# Config","289a9388":"# Utility Functions","6a971cec":"# running policy networks","e15fb830":"# model","6e64af4a":"# calculate rewards","fc5fd6b0":"# training model","356e0439":"# collect training samples","adbfdc5f":"# play game"}}