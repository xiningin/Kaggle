{"cell_type":{"29d8cdc1":"code","75e118c2":"code","8d57a084":"code","b9acec95":"code","abf0d89b":"code","3444af17":"code","acd583a4":"code","3c7eb0cf":"code","f88cba84":"code","e026835f":"code","736d4578":"code","8c62d156":"markdown","2bb90e30":"markdown","afc21460":"markdown","f337770d":"markdown","28940b5b":"markdown","36af984c":"markdown","4ff7a92b":"markdown","b5e4f7dd":"markdown","a0b15b5f":"markdown","1a117b4d":"markdown","4e18648b":"markdown"},"source":{"29d8cdc1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75e118c2":"X_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ny_train = X_train['target']\nX_train.drop(['target'], axis=1, inplace=True)\nX_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col='id')","8d57a084":"cont_cols = [cname for cname in X_train.columns if X_train[cname].dtypes != 'object']\ncat_cols = [cname for cname in X_train.columns if X_train[cname].dtypes == 'object']","b9acec95":"# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 5 and X_train[cname].dtypes == 'object']\nhigh_cardinality_cols = [cname for cname in X_train.columns if X_train[cname].nunique() >= 5 and X_train[cname].dtypes == 'object']","abf0d89b":"fig, axes = plt.subplots(int(round(len(low_cardinality_cols) \/ 2, 0)), 2, figsize=(12, 12))\nfig_idx = np.array(np.meshgrid([*range(0, axes.shape[0])], [*range(0, axes.shape[1])])).T.reshape(-1, 2)\nidx = 0\nfor col in low_cardinality_cols:\n    df = pd.concat([X_train[col], y_train], axis=1)\n    df['category'] = pd.Categorical(df[col], categories=sorted(df[col].unique()))\n    sns.violinplot(x=df['category'], y=df['target'], ax=axes[fig_idx[idx, 0], fig_idx[idx, 1]]).set(title=col, xlabel=None, ylabel=None)\n    idx += 1\n\nplt.setp(axes[-1, :], xlabel='Category')\nplt.setp(axes[:, 0], ylabel='Target')","3444af17":"fig, axes = plt.subplots(int(round(len(high_cardinality_cols) \/ 2, 0)), 2, figsize=(12, 12))\nfig_idx = np.array(np.meshgrid([*range(0, axes.shape[0])], [*range(0, axes.shape[1])])).T.reshape(-1, 2)\nidx = 0\nfor col in high_cardinality_cols:\n    df = pd.concat([X_train[col], y_train], axis=1)\n    df['category'] = pd.Categorical(df[col], categories=sorted(df[col].unique()))\n    sns.violinplot(x=df['category'], y=df['target'], ax=axes[fig_idx[idx, 0], fig_idx[idx, 1]]).set(title=col, xlabel=None, ylabel=None)\n    idx += 1\n\nplt.setp(axes[-1, :], xlabel='Category')\nplt.setp(axes[:, 0], ylabel='Target')","acd583a4":"cont_cols = [cname for cname in X_train.columns if X_train[cname].dtypes != 'object']\n\nfig, axes = plt.subplots(len(cont_cols), 1, figsize=(12, 100))\nidx = 0\nfor col in cont_cols:\n    sns.scatterplot(x=X_train[col].map(lambda x: x * 100), y=y_train, ax=axes[idx]).set(title=col, xlabel=None, ylabel='Target')\n    idx += 1\n#     sns.scatterplot(x=X_train[col].map(lambda x: round(x, 2)), y=y_train, ax=axes[idx]).set(title='{}_round'.format(col), xlabel=None, ylabel='Target')\n#     idx += 1","3c7eb0cf":"X_train_m1 = X_train.drop(cat_cols, axis=1).copy()\nX_test_m1 = X_test.drop(cat_cols, axis=1).copy()\n\nm_d = [6]\nn_e = [1000]\n\nmodels = np.array(np.meshgrid(m_d, n_e)).T.reshape(-1, 2)\nscores = []\n\nfor model in models:\n    \n    # Define model\n    model_x = XGBRegressor(max_depth=model[0], n_estimators=model[1], learning_rate=0.01, n_jobs=4, tree_method='gpu_hist')\n\n    # Bundle preprocessing and modeling code in a pipeline\n    model_x_pipeline = Pipeline(steps=[\n        ('model', model_x)\n    ])\n\n    # Preprocessing of training data, fit model\n    scores.append((-1 * cross_val_score(model_x_pipeline, X_train_m1, y_train, cv=20, scoring='neg_root_mean_squared_error')).mean())\n    \nbest_model = models[list(scores).index(np.array(scores).min())]\nprint('Best Average RMS score (across experiments):\\n', np.array(scores).min())\nprint('Best Model:\\n', best_model)\n\nmodel_final = XGBRegressor(max_depth=best_model[0], n_estimators=best_model[1], learning_rate=0.02, n_jobs=4, tree_method='gpu_hist')\n\nmodel_final_pipeline = Pipeline(steps=[\n#     ('preprocessor', preprocessor),\n    ('model', model_final)\n])\n\nmodel_final_pipeline.fit(X_train_m1, y_train)\npredictions = model_final_pipeline.predict(X_test_m1)","f88cba84":"# Bundle preprocessing\npreprocessor_m2 = ColumnTransformer(\n    transformers=[\n        ('num_low', OneHotEncoder(handle_unknown='ignore'), low_cardinality_cols),\n        ('num_high', OrdinalEncoder(), high_cardinality_cols)\n    ]\n)","e026835f":"# X_train_m2 = X_train.copy()\n# X_test_m2 = X_test.copy()\n\n# m_d = [6]\n# n_e = [1000]\n\n# models = np.array(np.meshgrid(m_d, n_e)).T.reshape(-1, 2)\n# scores = []\n\n# for model in models:\n    \n#     # Define model\n#     model_x = XGBRegressor(max_depth=model[0], n_estimators=model[1], learning_rate=0.01, n_jobs=4, tree_method='gpu_hist')\n\n#     # Bundle preprocessing and modeling code in a pipeline\n#     model_x_pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor_m2),\n#         ('model', model_x)\n#     ])\n\n#     # Preprocessing of training data, fit model\n#     scores.append((-1 * cross_val_score(model_x_pipeline, X_train_m2, y_train, cv=20, scoring='neg_root_mean_squared_error')).mean())\n    \n# best_model = models[list(scores).index(np.array(scores).min())]\n# print('Best Average RMS score (across experiments):\\n', np.array(scores).min())\n# print('Best Model:\\n', best_model)\n\n# model_final = XGBRegressor(max_depth=best_model[0], n_estimators=best_model[1], learning_rate=0.02, n_jobs=4, tree_method='gpu_hist')\n\n# model_final_pipeline = Pipeline(steps=[\n#     ('preprocessor', preprocessor),\n#     ('model', model_final)\n# ])\n\n# model_final_pipeline.fit(X_train_m2, y_train)\n# predictions = model_final_pipeline.predict(X_test_m2)","736d4578":"output = pd.DataFrame({'id': X_test.index, 'target': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","8c62d156":"### 2.2. Continous Column\n\nSecond, let's take a look on the continous data by using scatterplot relative to the target.","2bb90e30":"#### Model","afc21460":"#### Pre-Processor","f337770d":"# 2. Data Overview","28940b5b":"# Submission","36af984c":"# Model 2. Categorical (OneHot & Encoder) + Continous","4ff7a92b":"#### 2.1.1. Low Cardinality","b5e4f7dd":"#### 2.1.2. High Cardinality","a0b15b5f":"# Model 1. Drop All Categorical Columns","1a117b4d":"### 2.1. Categorical Column\n\nFirst, let's take a look on the categorical data, we can separate it into the categorical data that has low cardinality (`.nunique < 5`) and high cardinality (`.nunique \u2265 5`). To see the data more deeply, we can visualize it with the distribution plot for each columns relative to the target value.","4e18648b":"# 1. Data Preparation"}}