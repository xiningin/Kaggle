{"cell_type":{"e66ac9c9":"code","3b605f64":"code","afcde73e":"code","b4d71e92":"code","dde87adb":"code","1e073e6f":"code","96da23f4":"code","2067737a":"code","810f966a":"code","e47f7c99":"code","f5dc80b0":"code","7ccc953d":"code","bc1377d9":"code","0a38ad9e":"code","01d67e7d":"code","8fdbe3e3":"code","1329fc2d":"code","0a54a0a2":"code","34b99b36":"code","322f8ae4":"code","528dc53c":"code","4c773358":"code","52131c8b":"code","b5ce10ea":"code","6150a093":"code","b7ca26a1":"code","f6acee47":"code","e7d980c4":"code","17fd9d4f":"code","fb2ee3c1":"code","64a861e1":"code","c243e03b":"code","dd3913ac":"code","4f702d6f":"code","853336a0":"code","c30167ba":"code","32582892":"code","258ae86b":"code","7dab9266":"code","711caabc":"markdown","8e0e163d":"markdown","274d5424":"markdown","c1dedd67":"markdown","7ae03d0d":"markdown"},"source":{"e66ac9c9":"!pip install git+https:\/\/github.com\/tensorflow\/examples.git\n!pip install git+https:\/\/github.com\/surmenok\/keras_lr_finder","3b605f64":"import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport albumentations as A\nfrom tensorflow_examples.models.pix2pix import pix2pix\nfrom keras_lr_finder import LRFinder\nfrom tensorflow import keras","afcde73e":"IMG_HEIGHT = 256\nIMG_WIDTH  = 256","b4d71e92":"def Read_All_Data(path):\n    temp = []\n    updated_path = os.path.join(path,\"VOC2012\",\"ImageSets\",\"Segmentation\",\"trainval.txt\")\n    with open(updated_path,\"r\") as file_:\n        Instances = file_.read().split()\n        img_arr = []\n        seg_arr = []\n        for img in Instances:\n            path_img = os.path.join(path,\"VOC2012\",\"JPEGImages\",img+\".jpg\")\n            path_seg = os.path.join(path,\"VOC2012\",\"SegmentationClass\",img+\".png\")\n            img = np.array(Image.open(path_img).resize((IMG_WIDTH, IMG_HEIGHT)))\n            seg = np.array(Image.open(path_seg).resize((IMG_WIDTH, IMG_HEIGHT)))\n            seg[seg == 255] = 21\n            img_arr.append(img)\n            seg_arr.append(seg)\n    return np.array(img_arr), np.array(seg_arr)","dde87adb":"path = \"..\/input\/pascal-voc-2012\"\nfull_dataset = tf.data.Dataset.from_tensor_slices(Read_All_Data(path))","1e073e6f":"DATASET_SIZE = 2913","96da23f4":"train_size = int(0.7 * DATASET_SIZE)\nval_size = int(0.2 * DATASET_SIZE)\ntest_size = int(0.1 * DATASET_SIZE)\n\nfull_dataset = full_dataset.shuffle(buffer_size = DATASET_SIZE)\ntrain_dataset = full_dataset.take(train_size)\ntest_dataset = full_dataset.skip(train_size)\nval_dataset = test_dataset.skip(test_size)\ntest_dataset = test_dataset.take(test_size)","2067737a":"# def Read_Data(path,is_train = True):\n#     temp = []\n#     updated_path = os.path.join(path,\"VOC2012\",\"ImageSets\",\"Segmentation\",\"train.txt\" if is_train else \"val.txt\")\n#     with open(updated_path,\"r\") as file_:\n#         Instances = file_.read().split()\n#         img_arr = []\n#         seg_arr = []\n#         for img in Instances:\n#             path_img = os.path.join(path,\"VOC2012\",\"JPEGImages\",img+\".jpg\")\n#             path_seg = os.path.join(path,\"VOC2012\",\"SegmentationClass\",img+\".png\")\n#             img = np.array(Image.open(path_img).resize((IMG_WIDTH, IMG_HEIGHT)))\n#             seg = np.array(Image.open(path_seg).resize((IMG_WIDTH, IMG_HEIGHT)))\n#             seg[seg == 255] = 21\n#             img_arr.append(img)\n#             seg_arr.append(seg)\n#     return np.array(img_arr), np.array(seg_arr)","810f966a":"# path = \"..\/input\/pascal-voc-2012\"\n# train_data = Read_Data(path, is_train = True)\n# val_data   = Read_Data(path, is_train = False)","e47f7c99":"# TRAIN_LENGTH = train_data[0].shape[0]\n# TEST_LENGTH = val_data[0].shape[0]","f5dc80b0":"def normalize(input_image, input_mask):\n    input_image = tf.cast(input_image, tf.float32) \/ 255.0\n    return input_image, input_mask","7ccc953d":"# train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n# test_dataset = tf.data.Dataset.from_tensor_slices(val_data)","bc1377d9":"def load_image_train(input_image, input_mask):\n    input_mask = input_mask[ ..., np.newaxis]\n    \n\n    # random flip the image and mask\n    if tf.random.uniform(()) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        input_mask = tf.image.flip_left_right(input_mask)\n    \n    input_mask = input_mask[:, :, 0]\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask","0a38ad9e":"def load_image_test(input_image, input_mask):\n    input_mask = input_mask[np.newaxis, ...]\n    \n    \n    input_mask = input_mask[0]\n    input_image, input_mask = normalize(input_image, input_mask)\n\n    return input_image, input_mask","01d67e7d":"BATCH_SIZE = 32\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = train_size \/\/ BATCH_SIZE","8fdbe3e3":"train = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\nval   = train_dataset.map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\ntest = test_dataset.map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)","1329fc2d":"train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\ntest_dataset = test.batch(BATCH_SIZE)\nval_dataset = val.batch(BATCH_SIZE)","0a54a0a2":"for image, mask in train.take(2):\n    sample_image, sample_mask = image, mask\n    plt.imshow(sample_image)\n    print(sample_image.shape)\n    plt.show()\n    plt.imshow(sample_mask)\n    plt.show()","34b99b36":"base_model = tf.keras.applications.MobileNetV2(input_shape=[IMG_HEIGHT, IMG_WIDTH, 3], include_top=False)\n\n# use the output of listed layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nlayers = [base_model.get_layer(name).output for name in layer_names]\n\n# build the feature extraction model (encoder)\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n# we'll not change the parameter of the decoder\ndown_stack.trainable = False","322f8ae4":"# build the upsampling model (decoder)\nup_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]","528dc53c":"def unet_model(output_channels):\n    inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3])\n    x = inputs\n\n    # downsampling and extract features \n    skips = down_stack(x)\n    x = skips[-1]\n    # layers to establish skip connections in the encoder model\n    skips = reversed(skips[:-1])\n\n    # establish skip connections between layers of encoder and decoder\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        concat = tf.keras.layers.Concatenate()\n        x = concat([x, skip])\n\n    # the last layer \n    last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n    x = last(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","4c773358":"OUTPUT_CHANNELS = 22\nmodel = unet_model(OUTPUT_CHANNELS)\nmodel.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","52131c8b":"# x, y = next(iter(val_dataset))\n\n# for x_, y_ in val_dataset:\n#     x = tf.concat((x, x_), axis=0)\n#     y = tf.concat((y, y_), axis=0)\n    \n# # use lr_finder to draw the cycling learning rate\n# lr_finder = LRFinder(model)\n# lr_finder.find(x, y, start_lr=1e-10, end_lr=1, batch_size=32, epochs=10)\n# lr_finder.plot_loss()","b5ce10ea":"optimizer = keras.optimizers.Adam(lr=1e-4)","6150a093":"OUTPUT_CHANNELS = 22\nmodel = unet_model(OUTPUT_CHANNELS)\nmodel.compile(optimizer=optimizer,\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","b7ca26a1":"tf.keras.utils.plot_model(model, show_shapes=True)","f6acee47":"def create_mask(pred_mask):\n    pred_mask = tf.argmax(pred_mask, axis=-1)\n    pred_mask = pred_mask[..., tf.newaxis]\n    return pred_mask[0]","e7d980c4":"class DisplayCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        clear_output(wait=True)\n        show_predictions()\n        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))","17fd9d4f":"EPOCHS = 30\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = val_size\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=val_dataset)","fb2ee3c1":"def show_predictions(dataset=None, num=1):\n    if dataset:\n        for image, mask in dataset.take(num):\n            pred_mask = model.predict(image)\n            display([image[0], mask[0], create_mask(pred_mask)])\n    else:\n        display([sample_image, sample_mask,\n            create_mask(model.predict(sample_image[tf.newaxis, ...]))])","64a861e1":"def display(display_list):\n    plt.figure(figsize=(15, 15))\n\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(np.array(display_list[i]))\n        plt.axis('off')\n    plt.show()","c243e03b":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nepochs = range(EPOCHS)\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","dd3913ac":"show_predictions(train_dataset, num =10)","4f702d6f":"show_predictions(test_dataset.shuffle(buffer_size=64), num =10)","853336a0":"show_predictions(val_dataset.shuffle(buffer_size=64), num =10)","c30167ba":"model.evaluate(test_dataset)","32582892":"!mkdir source\n!mkdir mask","258ae86b":"cnt = 0\nfor image, mask in train_dataset.take(10):\n    pred_mask = model.predict(image)\n    pred_mask = create_mask(pred_mask)\n    pred_mask = (np.array(pred_mask) !=0).astype(int)\n#     print(image[0].shape)\n#     print(pred_mask.shape)\n    Image.fromarray((np.array(image[0]*255).astype(np.uint8))).save(f\"source\/source_{cnt}.jpg\")\n    Image.fromarray((pred_mask[:,:,0]*255).astype(np.uint8)).save(f\"mask\/mask_{cnt}.jpg\")\n    display([image[0], pred_mask[:, :, 0], image[0] * pred_mask])\n    cnt += 1","7dab9266":"# cat_img = np.array(Image.open(\"..\/input\/pascal-voc-2012\/VOC2012\/JPEGImages\/2007_000241.jpg\").resize((256, 256)))\n# pred_mask = model.predict(cat_img[np.newaxis,...])\n# mask = create_mask(pred_mask)\n# mask = np.array(mask!=0, dtype=int)","711caabc":"### From the above figure, 1e-4 is the optimal learning rate","8e0e163d":"### We need some agumentation while loading the training set in order to prevent overfitting ","274d5424":"### Model inference and extract mask","c1dedd67":"### show an example of image and mask","7ae03d0d":"### find the optimal learning rate"}}