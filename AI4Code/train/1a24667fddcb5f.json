{"cell_type":{"da7d15b5":"code","257a96be":"code","117f3870":"code","b4b51a8b":"code","2d3cb250":"code","e365dc9b":"code","50e1c337":"code","a6f80fd1":"code","2ce0b64d":"code","60b61d14":"code","b3b4f301":"code","88a324fc":"code","1b7ee581":"code","0ecc3cfd":"code","472d9c70":"code","30533c7d":"code","0133eecc":"code","4a21ed41":"code","036f06c8":"code","0da5b0f8":"code","21fe3f5c":"code","d4d8b069":"code","813ca8c4":"code","9e8209ea":"code","d901cb80":"code","3ec7071a":"code","880bbcfd":"code","598d28bf":"code","9f2c2757":"code","6c2bb6fa":"code","3469ddc6":"code","887a3014":"code","9b0001a9":"markdown","90645957":"markdown","c831034d":"markdown","bb2f46f5":"markdown","28fa0ca7":"markdown","1ccd8bca":"markdown"},"source":{"da7d15b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","257a96be":"# Import modules \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","117f3870":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf = data.copy()","b4b51a8b":"df.head()","2d3cb250":"df.shape","e365dc9b":"#The columns 'id' and 'Unnamed: 32' have been removed.\ndf.drop(['id','Unnamed: 32'], axis=1, inplace=True)","50e1c337":"print('Data Shape: ',df.shape)","a6f80fd1":"#The column 'diagnose' has been renamed to 'target'.\ndf = df.rename(columns={'diagnosis':'target'})","2ce0b64d":"sns.countplot(df['target'])\nprint(df.target.value_counts())\n# M = malignant, B =benign","60b61d14":"#The variable 'target' has been changed to a numeric variable.\ndf['target'] = [1 if i.strip()=='M' else 0 for i in df.target]","b3b4f301":"df.info()","88a324fc":"df.describe().T","1b7ee581":"corr = df.corr()\nsns.clustermap(corr, annot=True, fmt = '.2f',figsize=(18,13),cmap='YlGnBu') #fmt noktadan sonra 2 sayi g\u00f6rmek icin\nplt.title('Correlations')","0ecc3cfd":"th = 0.75 \nfilt = np.abs(corr['target']) > th\n\ncorr_features = corr.columns[filt].tolist()\nsns.clustermap(df[corr_features].corr(),annot=True, fmt = '.2f',figsize=(12,8),cmap='YlGnBu')\nplt.title(\"Target's correlations greater than 0.75\")","472d9c70":"#Box plot was used for information on numeric variables.\nmelt_data = pd.melt(df, id_vars =  'target',var_name='features',value_name = 'value') \n#There are two classes for the box plot, it was visualized as two classes.\n#id_vars: Column(s) to use as identifier variables.\n#var_name: Name to use for the 'variable' column.\n#value_name: Name to use for the 'value' column.\n\nplt.figure(figsize=(13,8)) \nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\", data = melt_data)\nplt.xticks(rotation = 90)\nplt.show()","30533c7d":"sns.set_style(\"darkgrid\")\nfig = sns.pairplot(data=df[corr_features], hue='target', kind='scatter', diag_kind='kde', palette='flare', corner=True)\nfig.add_legend()\n\nfig.tight_layout()\nplt.show()","0133eecc":"# Density Based Outlier Detection System (Local Outlier Factor-LOF)\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.LocalOutlierFactor.html\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/neighbors\/plot_lof_outlier_detection.html\n\nX = df.drop(['target'], axis=1) #features\ny = df['target']\ncolumns = X.columns.tolist()","4a21ed41":"from sklearn.neighbors import LocalOutlierFactor\nmodel = LocalOutlierFactor(contamination=0.02) #n_neighbors=20 default \ny_pred = model.fit_predict(X) # returns -1 for outliers and 1 for inliers\nX_scores = model.negative_outlier_factor_\nprint('y_pred[:10] ',y_pred[:10])\nprint('y_pred values and counts: ',np.unique(y_pred,return_counts=True))","036f06c8":"outliers = pd.DataFrame()\noutliers[\"values\"] = y_pred\noutliers[\"values\"]","0da5b0f8":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/neighbors\/plot_lof_outlier_detection.html\n\nplt.figure(figsize=(15,10))\nplt.title(\"Local Outlier Factor (LOF)\")\nplt.scatter(X.iloc[:,0], X.iloc[:,1], color = \"k\", s = 3, label = \"Data Points\")\n\n# plot circles with radius proportional to the outlier scores\nradius = (X_scores.max() - X_scores) \/ (X_scores.max() - X_scores.min())\nplt.scatter(\n    X.iloc[:, 0],\n    X.iloc[:, 1],\n    s=1000 * radius,\n    edgecolors=\"r\",\n    facecolors=\"none\",\n    label=\"Outlier scores\",\n)\nplt.legend()\nplt.show()","21fe3f5c":"filt = outliers[\"values\"] == -1\noutlier_index = outliers[filt].index.tolist()\n#outlier_index","d4d8b069":"#Outliers were removed from the data.\nX = X.drop(outlier_index)\ny = y.drop(outlier_index).values","813ca8c4":"X_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size = 0.25, \n                                                    random_state = 42)\nprint('X_train.shape:', X_train.shape)\nprint('X_test.shape:', X_test.shape)","9e8209ea":"#standardization \nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train) \nX_test = scaler.transform(X_test) ","d901cb80":"#Normalized trainset\nX_train_df = pd.DataFrame(X_train, columns = columns)\nX_train_df[\"target\"] = y_train","3ec7071a":"# box plot \ndata_melted = pd.melt(X_train_df, id_vars = \"target\",\n                      var_name = \"features\",\n                      value_name = \"value\")\n\nplt.figure(figsize=(12,6))\nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\", data = data_melted)\nplt.xticks(rotation = 90)\nplt.show()","880bbcfd":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train, y_train)\ny_pred_test = knn.predict(X_test)\ny_pred_train = knn.predict(X_train)\ncm = confusion_matrix(y_test, y_pred_test)\nacc_test = accuracy_score(y_test, y_pred_test)\nacc_train = accuracy_score(y_train, y_pred_train)\nscore = knn.score(X_test, y_test)\nprint(\"Test Score: {}, Train Score: {}\".format(acc_test, acc_train))\nprint(\"CM: \",cm)","598d28bf":"from sklearn.metrics import mean_squared_error\nprint('MSE (Testset):', mean_squared_error(knn.predict(X_test), y_test))\nprint('MSE (Trainset):', mean_squared_error(knn.predict(X_train), y_train))","9f2c2757":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# best k value\n\nfrom sklearn.model_selection import GridSearchCV\nknn_params = {'n_neighbors': np.arange(1,30,1)}\nknn = KNeighborsClassifier()\nknn_cv_model = GridSearchCV(knn, knn_params, cv=10)\nknn_cv_model.fit(X_train,y_train)","6c2bb6fa":"print(\"Best training score: {} with parameters: {}\".format(knn_cv_model.best_score_, knn_cv_model.best_params_))","3469ddc6":"knn_tuned = KNeighborsClassifier(knn_cv_model.best_params_['n_neighbors'])\nknn_tuned.fit(X_train, y_train)\ny_pred_test2 = knn_tuned.predict(X_test)\ny_pred_train2 = knn_tuned.predict(X_train)\ncm_test = confusion_matrix(y_test, y_pred_test2)\ncm_train = confusion_matrix(y_train, y_pred_train2)\n#cm = confusion_matrix(y_test, y_pred_test)\nacc_test = accuracy_score(y_test, y_pred_test2)\nacc_train = accuracy_score(y_train, y_pred_train2)\nprint(\"Test Score: {}, Train Score: {}\".format(acc_test, acc_train))\nprint()\nprint(\"CM Test: \",cm_test)\nprint(\"CM Train: \",cm_train)","887a3014":"from sklearn.metrics import mean_squared_error\nprint('MSE (Trainset):', mean_squared_error(knn_tuned.predict(X_train), y_train))\nprint('MSE (Testset):', mean_squared_error(knn_tuned.predict(X_test), y_test))","9b0001a9":"#### Exploratory Data Analysis and Data Preprocessing","90645957":"<img src='https:\/\/archive.ics.uci.edu\/ml\/assets\/MLimages\/Large14.jpg' width='300px;'\/>\n\nThis project is an analysis of the Breast Cancer Wisconsin (Diagnostic) Dataset. The classification used K-Nearest Neighbors (KNN), a popular machine learning algorithm, which was used to construct a model based on patient and related tumor data.\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass..\n\n#### Attribute Information:\n\n1) ID number \n2) Diagnosis (M = malignant, B = benign) \n3-32) \n \nTen real-valued features are computed for each cell nucleus: \n\na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 \/ area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  \n","c831034d":"#### Outlier detection","bb2f46f5":"- In the project, columns that were not required were initially removed from the data and the 'Diagnosis' column was renamed.\n- Since the KNN algorithm is sensitive to outliers, outliers were detected and removed from the data.\n- Values have been standardized.\n- The data set has been trained and scores have been obtained.\n- The best k value was found and the scores were obtained again with this value.\n\nOur trainset score is around 97.6 percent. This is a score from the data we trained so it looks good. Our KNN model gives a score of 0.95, which shows that our model does a good job of categorizing the diagnosis of breast cancer types","28fa0ca7":"### Model","1ccd8bca":"#### Load Data"}}