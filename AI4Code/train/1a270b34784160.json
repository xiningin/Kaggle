{"cell_type":{"fc9859de":"code","b8e4c30f":"code","92f92d0d":"code","1a875cb7":"code","e79e644a":"code","f0b9d0e6":"code","55f8a946":"code","ebdf5798":"code","fcc53ede":"code","cb2f64dd":"code","9ce0fec1":"code","4371bcd2":"code","e937b9d4":"code","915b4ebb":"code","80363562":"markdown"},"source":{"fc9859de":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8e4c30f":"# Reading our file \ndf=pd.read_table('\/kaggle\/input\/fruits-with-colors-dataset\/fruit_data_with_colors.txt')\ndf\n","92f92d0d":"# For calculating rows and columns\ndf.shape","1a875cb7":"#Checking for NaN values\ndf.isna().sum()","e79e644a":"# Analysing the co-relation between different features \ndf.corr()","f0b9d0e6":"# Column which is not contributing we have to drop it \ndf.drop('fruit_subtype',axis=1,inplace=True)","55f8a946":"# Storing features in our independent and target variable\nx=df.drop('fruit_name', axis=1)\ny=df['fruit_name']","ebdf5798":"# Spliting  our dataset into training and testing dataset\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.26)\n","fcc53ede":"# Doing Standardization for increasing the speed of our model by reducing distance.\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test =scaler.transform(x_test)\n","cb2f64dd":"# Importing our classification model\n# here n_neighbors denotes the no. of k , we have taken it as 5 because it is the most used one.\n# Then train the model\nfrom sklearn.neighbors import KNeighborsClassifier\nobj1 = KNeighborsClassifier(n_neighbors= 5)\nobj1.fit(x_train,y_train)\n","9ce0fec1":"# predicted the values with the help of testing dataset\ny_pred = obj1.predict(x_test)","4371bcd2":"#optional \n# I have used in order to see whether they are overlapping or not \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.scatter(y_pred, y_test)","e937b9d4":"# You can not end without the evaluation \n# We have to test the accuracy of our model.\nfrom sklearn.metrics import classification_report , confusion_matrix\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n","915b4ebb":"# optional \n# Although we can check our accuracy in our above step but if you want to check it more precisly\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)*100","80363562":"# AMAZING !!! Our model  accuracy is just Outstanding -- 100%"}}