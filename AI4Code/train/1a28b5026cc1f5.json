{"cell_type":{"3373f049":"code","a6b255cd":"code","803734c3":"code","af067860":"code","57897f08":"code","01d04acc":"code","ad79316e":"code","d15c1680":"code","ad19730d":"code","20194382":"code","52796274":"code","123de965":"code","c9ec7e7b":"code","e509bf2e":"code","25940f0c":"code","9883ea3f":"code","9a296133":"code","8451dad9":"code","d8be43cc":"code","d07c1802":"code","62c90979":"code","a8a065e3":"code","68c379fd":"code","b79835c0":"code","f217ab95":"code","b4f81512":"markdown","d95f2797":"markdown","f32b951c":"markdown"},"source":{"3373f049":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nprint(os.listdir(\"..\/input\"))","a6b255cd":"import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader","803734c3":"# Prepare TRAIN Dataset\n# load data\ntrain = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntrain[0:3]","af067860":"TEST0 = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')\nTEST0[0:3]","57897f08":"df=train\n\nfor c in df.columns:\n    if df[c].dtype=='object': \n        df[c] = df[c].fillna('N')\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)\n        \ntrain=df","01d04acc":"train","ad79316e":"df=TEST0\n\nfor c in df.columns:\n    if df[c].dtype=='object': \n        df[c] = df[c].fillna('N')\n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)\n        \nTERST0=df","d15c1680":"TEST0","ad19730d":"targets_numpy = train.target.values\nfeatures_numpy = train.loc[:,train.columns != \"target\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n        targets_numpy,test_size = 0.2,random_state = 42) \n\nfeaturesTrain = torch.from_numpy(features_train).float()\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test).float()\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 32\nn_iters = 10000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)","20194382":"# Prepare TEST Dataset\n\n#TEST_targets_numpy = TEST0.target.values\nTEST_features_numpy = TEST0.values\/255 # normalization\n\nfeatures_TEST = TEST_features_numpy\n#targets_TEST = TEST_targets_numpy\n\nfeaturesTEST = torch.from_numpy(features_TEST).float()\n#targetsTEST = torch.from_numpy(targets_TEST).type(torch.LongTensor) \n","52796274":"print(type(featuresTEST))","123de965":"# Create Logistic Regression Model\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(LogisticRegressionModel, self).__init__()\n        # Linear part\n        self.linear = nn.Linear(input_dim, output_dim)\n    \n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\n# Instantiate Model Class\ninput_dim = 13 # size of image px*px\noutput_dim = 2  \n\n# create logistic regression model\nmodel = LogisticRegressionModel(input_dim, output_dim)\n\n# Cross Entropy Loss  \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer \nlearning_rate = 0.001\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","c9ec7e7b":"# Traning the Model\ncount = 0\nloss_list = []\niteration_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        # Define variables\n        train = Variable(images.view(-1,13))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)   #####\n        \n        # Calculate softmax and cross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculate gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        # Prediction\n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader: \n                test = Variable(images.view(-1,13))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))","e509bf2e":"# visualization\nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Logistic Regression: Loss vs Number of iteration\")\nplt.show()","25940f0c":"model","9883ea3f":"outputsTEST = model(featuresTEST)\npredictedTEST = torch.max(outputsTEST.data,1)[1]","9a296133":"sample=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv')\nsubmit1=sample","8451dad9":"PRED1=predictedTEST.numpy()\nsubmit1['target']=PRED1\nsubmit1.to_csv('submission1.csv', index=False)\nsubmit1","d8be43cc":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 13\nhidden_dim = 150\noutput_dim = 2\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","d07c1802":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1,13))\n        labels = Variable(labels)\n        \n        optimizer.zero_grad()\n        outputs = model(train)\n        loss = error(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n                \n                test = Variable(images.view(-1,13))\n                outputs = model(test)\n                predicted = torch.max(outputs.data, 1)[1]\n                total += len(labels)\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","62c90979":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()\n\n# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","a8a065e3":"model","68c379fd":"outputsTEST = model(featuresTEST)\npredictedTEST = torch.max(outputsTEST.data,1)[1]","b79835c0":"sample=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/sample_submission.csv')\nsubmit2=sample","f217ab95":"PRED2=predictedTEST.numpy()\nsubmit2['target']=PRED2\nsubmit2.to_csv('submission2.csv', index=False)\nsubmit2","b4f81512":"# Create Logistic Regression Model","d95f2797":"# Prepare Dataset","f32b951c":"# Create ANN Model"}}