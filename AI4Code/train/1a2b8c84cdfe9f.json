{"cell_type":{"0d4fdab3":"code","8eadba35":"code","56795e15":"code","9bcae0d3":"code","91b8dc31":"code","8cb1d326":"code","15de8459":"code","fec79f7c":"code","9e25aeb5":"code","611be49a":"code","55f9c332":"code","94a84a65":"code","553a2bf5":"code","de74be78":"code","b259ab05":"code","843f5500":"code","c2592dd0":"code","b358fa83":"markdown","19d2ec1a":"markdown","91da08e2":"markdown","29feed93":"markdown","d937f8c5":"markdown","3cc83663":"markdown","e7f14364":"markdown","5d6c6ef9":"markdown","29e9185a":"markdown","98c29728":"markdown","41869bd5":"markdown","6e6c09a9":"markdown","069e41d6":"markdown","4f0a7339":"markdown","77770eb2":"markdown","5812ecb3":"markdown","c79a2212":"markdown"},"source":{"0d4fdab3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import mutual_info_score, mean_squared_error\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.linear_model import LogisticRegression, Ridge\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8eadba35":"data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndata.head()\n\n","56795e15":"features  = [\n    'neighbourhood_group',\n    'room_type',\n    'latitude',\n    'longitude',\n    'price',\n    'minimum_nights',\n    'number_of_reviews',\n    'reviews_per_month',\n    'calculated_host_listings_count',\n    'availability_365'\n]\ndf = data[features]\n\"\"\"Checking description of this project's data set\"\"\"\ndf.describe()","9bcae0d3":"from IPython.display import display\nmissing_vals = df.isnull().sum()\nprint(\"Before Imputing Missing Values\")\ndisplay(missing_vals.to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).sort_values(by = 'Missing Values', ascending = False).style.background_gradient('Reds'))\n\n\ndf.fillna(0, inplace = True)\nprint(\"After Imputing Missing Values\")\ndisplay(df.isnull().sum().to_frame().reset_index().rename({'index': 'Variables', 0: 'Missing Values'}, axis = 1).style.background_gradient('Reds'))\n","91b8dc31":"print(\"Mode for variable 'neighbourhood_group': %s\" %(df['neighbourhood_group'].value_counts().head(1)))","8cb1d326":"from sklearn.model_selection import train_test_split\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n","15de8459":"\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Correlation of Numerical Columns\"\"\"\ndisplay(df[numerical].corr())\n\n\"\"\"Heatmap of Numerical Variables\"\"\"\ndisplay(sns.heatmap(df[numerical].corr(), annot = True, lw = 0.2, square = True, cmap = 'crest'))\n","fec79f7c":"\"\"\"price variable from numeric into binary.\"\"\"\ndf_full_train['above_average'] = (df_full_train['price'] >= 152).values.astype(int)\n","9e25aeb5":"\"\"\"Mutual Information\"\"\"\ndef mutual_info_bin_score(series):\n    return mutual_info_score(series, df_full_train.above_average)\n\nmi = df_full_train[categorical].apply(mutual_info_bin_score)\nmi.round(2).sort_values(ascending=False)    ","611be49a":"\"\"\"Taking Care of Categorical variables\"\"\"\ny_train = (df_train['price'] >= 152).values.astype(int)\ny_val = (df_val['price'] >= 152).values.astype(int)\ny_test = (df_test['price'] >= 152).values.astype(int)\n\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\nnumerical.remove('price')\n\ndecision = (y_val >= 152).astype(int)\n","55f9c332":"def calculate_accuracy(features):\n    # one-hot encoding datasets\n    dv = DictVectorizer(sparse=False)\n\n    train_dict = df_train[features].to_dict(orient='records')\n    val_dict = df_val[features].to_dict(orient='records')\n\n    X_train = dv.fit_transform(train_dict)\n    X_val = dv.transform(val_dict)\n\n    \"\"\"Fitting the Model on Training Set\"\"\"\n    model = LogisticRegression(solver='liblinear', C=1.0, random_state=42)\n    model.fit(X_train, y_train)\n\n    \"\"\"Using the model on validation\"\"\"\n    y_pred = model.predict_proba(X_val)[:,1]\n\n    \"\"\"Setting up Decision Threshold to 0.5\"\"\"\n    decision = (y_pred >= 0.5)\n\n    \"\"\"Calculating accuracy\"\"\"\n    accuracy = (y_val == decision).mean()\n    \n\n    df_pred = pd.DataFrame()\n    df_pred['probability'] = y_pred\n    df_pred['prediction'] = decision\n    df_pred['actual'] = y_val\n    df_pred['correct'] = df_pred.prediction == df_pred.actual\n    return accuracy, df_pred\nacc, df_pred = calculate_accuracy(numerical+categorical)    \n\nprint(acc)\ndf_pred.head()","94a84a65":"all_vars_accuracy,_ = calculate_accuracy(numerical+categorical)\nall_vars_accuracy.round(2)","553a2bf5":"useful_features = numerical + categorical\ndiff = {}\nfor i in useful_features:\n  features = useful_features.copy()\n  features.remove(i)\n  acc,_ = calculate_accuracy(features)\n  diff[\"Difference in accuracy when removing %s\"%i] = all_vars_accuracy - acc\n\ndiff","de74be78":"pd.DataFrame(diff.values(), index = diff.keys()).rename({0: 'differences'}, axis = 1).sort_values(by = 'differences', ascending = True).style.background_gradient('Reds')","b259ab05":"\"\"\"Preparing Data for Linear Regression with Price Variable included\"\"\"\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)\n\n\"\"\"Creating List of Numerical and Categorical columns\"\"\"\ncategorical = [col for col in df.columns if df[col].dtype == 'object']\nnumerical = [col for col in df.columns if col not in categorical]\n\n\"\"\"Apply the log transformation to the price variable using the np.log1p() function.\"\"\"\ny_train = np.log1p(df_train['price'].values)\ny_val = np.log1p(df_val['price'].values)\ny_test = np.log1p(df_test['price'].values)\n\n\n\"\"\"Make sure that the target value ('price') is not in your dataframe.\"\"\"\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']\n\n\"\"\"Taking care of Categorical Variables\"\"\"\ndv = DictVectorizer(sparse=False)\nfeatures = categorical + numerical\nfeatures.remove('price')\ntrain_dict = df_train[features].to_dict(orient='records')\nval_dict = df_val[features].to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dict)\nX_val = dv.transform(val_dict)","843f5500":"scores = {}\nfor alpha in [0, 0.01, 0.1, 1, 10]:\n    model = Ridge(alpha=alpha, random_state= 42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    score = np.sqrt(mean_squared_error(y_val, y_pred))\n    scores[alpha] = score.round(3)\n    print(\"RMSE with alpha = %s and not rounding to 3 digits: %s\"%(alpha, score) )\nprint(\" \\nRMSE with rounding off to 3 digits\")\nprint(scores)\n\n","c2592dd0":"print(\"Table of RMSE rounded to 3 digits\")\npd.DataFrame(scores.values(), index = scores.keys()).rename({0: 'RMSE'}, axis = 1).sort_values(by = 'RMSE', ascending = True).style.background_gradient('Reds')","b358fa83":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Make price binary\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We need to turn the price variable from numeric into binary.<\/li>\n<li>Let's create a variable above_average which is 1 if the price is above (or equal to) 152.<\/li>\n<\/ul>\n<\/div>","19d2ec1a":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Loading and Reading Data <\/div>","91da08e2":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>This Week Questions<\/u>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 1:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>What is the most frequent observation (mode) for the column 'neighbourhood_group'?<\/li>\n<\/ul>   \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 2:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Create the correlation matrix for the numerical features of your train dataset. <\/li>\n<ul>\n<li>In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset. <\/li>    \n<\/ul> \n<li>What are the two features that have the biggest correlation in this dataset? <\/li>   \n    \n<\/ul>     \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 3:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.<\/li>\n<li>Which of these two variables has bigger score?<\/li>\n<li>Round it to 2 decimal digits using round(score, 2)<\/li>\n<\/ul> \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 4:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Now let's train a logistic regression<\/li>\n<li>Remember that we have two categorical variables in the data. Include them using one-hot encoding.<\/li>\n<li>Fit the model on the training dataset.<\/li>\n<ul>\n<li>To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:<\/li> \n<li>model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)<\/li>\n<\/ul>     \n<li>Calculate the accuracy on the validation dataset and rount it to 2 decimal digits.<\/li>\n<\/ul>    \n    \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 5:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We have 9 features: 7 numerical features and 2 categorical.<\/li>\n<li>Let's find the least useful one using the feature elimination technique.<\/li>\n<li>Train a model with all these features (using the same parameters as in Q4).<\/li>\n<li>Now exclude each feature from this set and train a model without it. Record the accuracy for each model.<\/li>\n<li>For each feature, calculate the difference between the original accuracy and the accuracy without the feature.<\/li>\n<li>Which of following feature has the smallest difference? <\/li>\n<ul>\n<li>neighbourhood_group<\/li> \n<li>room_type<\/li>\n<li>number_of_reviews<\/li> \n<li>reviews_per_month<\/li>\n<\/ul> \n<\/ul>      \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 6:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>For this question, we'll see how to use a linear regression model from Scikit-Learn<\/li>\n<li>We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.<\/li>\n<li>Fit the Ridge regression model on the training data.<\/li>\n<li>This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]<\/li>\n<li>Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.<\/li>\n<\/ul>\n\n<\/div>\n","29feed93":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 2:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Create the correlation matrix for the numerical features of your train dataset. <\/li>\n<ul>\n<li>In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset. <\/li>    \n<\/ul> \n<li>What are the two features that have the biggest correlation in this dataset? <\/li>   \n    \n<\/ul> \n<\/div>\n","d937f8c5":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">Q1. What is the most frequent observation (mode) for the column 'neighbourhood_group'?<\/div>","3cc83663":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Missing Values? Impute them with 0<\/div>","e7f14364":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 5:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>We have 9 features: 7 numerical features and 2 categorical.<\/li>\n<li>Let's find the least useful one using the feature elimination technique.<\/li>\n<li>Train a model with all these features (using the same parameters as in Q4).<\/li>\n<li>Now exclude each feature from this set and train a model without it. Record the accuracy for each model.<\/li>\n<li>For each feature, calculate the difference between the original accuracy and the accuracy without the feature.<\/li>\n<li>Which of following feature has the smallest difference? <\/li>\n<ul>\n<li>neighbourhood_group<\/li> \n<li>room_type<\/li>\n<li>number_of_reviews<\/li> \n<li>reviews_per_month<\/li>\n<\/ul> \n<\/ul>\n<\/div>\n","5d6c6ef9":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Smallest Difference:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>number_of_reviews (0.000895)<\/li> \n<\/ul><\/div>","29e9185a":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Importing Libraries<\/div>","98c29728":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 4:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Now let's train a logistic regression<\/li>\n<li>Remember that we have two categorical variables in the data. Include them using one-hot encoding.<\/li>\n<li>Fit the model on the training dataset.<\/li>\n<ul>\n<li>To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:<\/li> \n<li>model = LogisticRegression(solver='lbfgs', C=1.0, random_state=42)<\/li>\n<\/ul>     \n<li>Calculate the accuracy on the validation dataset and rount it to 2 decimal digits.<\/li>\n<\/ul> \n<\/div>","41869bd5":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 3:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Calculate the mutual information score with the (binarized) price for the two categorical variables that we have. Use the training set only.<\/li>\n<li>Which of these two variables has bigger score?<\/li>\n<li>Round it to 2 decimal digits using round(score, 2)<\/li>\n<\/ul> \n<\/div>\n","6e6c09a9":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Split the data\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>Split your data in train\/val\/test sets, with 60%\/20%\/20% distribution.<\/li>\n<li>Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.<\/li>\n<li>Make sure that the target value ('price') is not in your dataframe.<\/li>\n<\/ul>\n<\/div>\n","069e41d6":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Highest Correlation is between\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>reviews_per_month and number_of_reviews: 0.589407<\/li>\n<\/ul>\n<\/div>","4f0a7339":"<div style=\"background:#2b6684;color:white; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Features used for this Project<\/div>","77770eb2":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:yellow\">Question 6:\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n<li>For this question, we'll see how to use a linear regression model from Scikit-Learn<\/li>\n<li>We'll need to use the original column 'price'. Apply the logarithmic transformation to this column.<\/li>\n<li>Fit the Ridge regression model on the training data.<\/li>\n<li>This model has a parameter alpha. Let's try the following values: [0, 0.01, 0.1, 1, 10]<\/li>\n<li>Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.<\/li>\n<\/ul>\n<\/div>\n","5812ecb3":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">Variable room_type has the biggest score = 0.14<\/div>","c79a2212":"<div style=\"background:#2b6684; font-family:cursive;padding:0.5em;border-radius:0.2em;font-size:20px;color:white\">$ \\large alpha$ that leads to the best RMSE: 0<\/div>"}}