{"cell_type":{"a1b2831d":"code","fe1c8683":"code","e714480e":"code","bd6f0bb5":"code","7242e581":"code","00d6b58f":"code","dab9373e":"code","25dd42bf":"code","1a27ed02":"code","ba12823f":"code","368c56f6":"code","c95ce860":"code","0fd31d05":"code","fa2b3d35":"code","b76030df":"code","7731b80d":"code","9d1b3b68":"code","ad1b6236":"code","6e87d8d0":"code","0b01b633":"code","3125b97c":"code","222598db":"markdown","3218911a":"markdown","44ebe93c":"markdown","5be30931":"markdown","b8ffdbee":"markdown","dfd29048":"markdown","369529f3":"markdown","87a8ba7e":"markdown","a79b9d81":"markdown"},"source":{"a1b2831d":"# from fastbook import *\nfrom fastai.text.all import *\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast","fe1c8683":"pretrained_weights = 'gpt2'\ntokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)\nmodel = GPT2LMHeadModel.from_pretrained(pretrained_weights)","e714480e":"path = '..\/input\/poemsdataset'","bd6f0bb5":"poems = get_text_files(path, folders = ['forms','topics'])\nprint(\"There are\",len(poems),\"poems in the dataset\")","7242e581":"ballads = get_text_files(path+'\/forms', folders = ['ballad'])\nprint(\"There are\",len(ballads),\"ballads in the dataset\")","00d6b58f":"txt = poems[0].open().read(); #read the first file\nprint(txt)","dab9373e":"ballads = L(o.open().read() for o in ballads) # to make things easy we will gather all texts in one numpy array","25dd42bf":"def flatten(A):\n    rt = []\n    for i in A:\n        if isinstance(i,list): rt.extend(flatten(i))\n        else: rt.append(i)\n    return rt\n  \nall_ballads = flatten(ballads)","1a27ed02":"class TransformersTokenizer(Transform):\n    def __init__(self, tokenizer): self.tokenizer = tokenizer\n    def encodes(self, x): \n        toks = self.tokenizer.tokenize(x)\n        return tensor(self.tokenizer.convert_tokens_to_ids(toks))\n    def decodes(self, x): return TitledStr(self.tokenizer.decode(x.cpu().numpy()))","ba12823f":"splits = [range_of(70), range(100)] # use a 70\/30 split\ntls = TfmdLists(all_ballads, TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)","368c56f6":"show_at(tls.train, 0)","c95ce860":"bs,sl = 4,256\ndls = tls.dataloaders(bs=bs, seq_len=sl)","0fd31d05":"dls.show_batch(max_n=2)","fa2b3d35":"class DropOutput(Callback):\n    def after_pred(self): self.learn.pred = self.pred[0]","b76030df":"learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()","7731b80d":"learn.validate()","9d1b3b68":"learn.lr_find()","ad1b6236":"learn.fit_one_cycle(1, 1e-4)","6e87d8d0":"prompt = 'love is ridiculous' # create an initial text prompt to start your generated text\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None].cuda()\ninp.shape","0b01b633":"preds = learn.model.generate(inp, max_length=60, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(preds[0].cpu().numpy(), skip_special_tokens=True))","3125b97c":"prompt = \"I don't know what I would do\"\nprompt_ids = tokenizer.encode(prompt)\ninp = tensor(prompt_ids)[None].cuda()\npreds = learn.model.generate(inp, max_length=60, num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(preds[0].cpu().numpy(), skip_special_tokens=True))","222598db":"## Import Libraries\n","3218911a":"## Prepare the Data\n\n","44ebe93c":"## Fine-tuning the model","5be30931":"In this tutorial you will see how to fine-tune a pretrained transformer model from the transformers library by HuggingFace. It can be very simple with FastAI's data loaders. It's possible to use any of the pretrained models from HuggingFace. Below we will experiment with GPT2. ","b8ffdbee":"Adding the `num_beams` and `no_repeat_ngram_size` arguments make a huge difference. This can be explained [here](https:\/\/huggingface.co\/blog\/how-to-generate). Basically beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Without beam search you will obtain a more greedy search. Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output. Moreover, without the `no_repeat_ngram_size` you will likely obtain a repeated output. Thus we add a penalty that makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0.","dfd29048":"# Poem Generation using FastAI\n","369529f3":"## Read Data\nThis data is organized by folder. There are two main folders: forms (e.g. haiku, sonnet, etc.) and topics (e.g. love, peace, etc.). Those main folders contain subfolders for the subcategories and then the poem txt files are contained in those.\nWith fastai, it's quite easy to read the data with the the get_text_files function. You can select all folders or select specific ones.","87a8ba7e":"We'll start off with training the model on ballads. There are only 100 ballads so it won't take as long to train. However you can add more poem forms. For instance, a haiku would be very cool to experiment with and to see if it maintains the 5,7,5 syllable structure. You can also change the path to the topics folder instead of poem forms and you can try out a bunch of poem topics like love, anger, depression, etc.. ","a79b9d81":"## Poem Generation Example"}}