{"cell_type":{"029055e7":"code","d67c6737":"code","7346066d":"code","05f00a1a":"code","212a4fa1":"code","88c9eec7":"code","637375d9":"code","dfea3f5d":"code","95b32ce3":"code","44be9a59":"code","41057239":"code","6b2b29ad":"code","7faf8481":"code","a6e0358d":"code","9946ecd9":"code","14d6de79":"code","ae2768c8":"code","dbeb750c":"code","b0e5075d":"code","c0e9b354":"code","36d378c6":"code","25ee2116":"code","65b59be3":"code","16a1e7a6":"code","cc94e6da":"code","85be5ae9":"code","e700c59a":"code","4c125083":"code","93e75522":"code","244f8fd2":"code","3b995681":"code","b8d6fc00":"code","8a6a4ca4":"code","70dc403c":"markdown","25a40191":"markdown","39987859":"markdown","add51480":"markdown","b52ac664":"markdown","1406f4e3":"markdown","44f499ae":"markdown","6195ca80":"markdown","15f25eb5":"markdown","a9b69a50":"markdown"},"source":{"029055e7":"import json\nimport os\nimport random\nfrom collections import Counter, defaultdict\nfrom functools import partial\nfrom multiprocessing import Pool\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport torch\nfrom gensim.models import Word2Vec, KeyedVectors\n\n%load_ext Cython","d67c6737":"%%cython\nimport re\nfrom multiprocessing import Pool\n\nimport numpy as np\ncimport numpy as np\n\n\ncdef class StringReplacer:\n    cpdef public dict rule\n    cpdef list keys\n    cpdef list values\n    cpdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.n_rules = len(rule)\n\n    def __call__(self, str x):\n        cdef int i\n        for i in range(self.n_rules):\n            if self.keys[i] in x:\n                x = x.replace(self.keys[i], self.values[i])\n        return x\n\n    def __getstate__(self):\n        return (self.rule, self.keys, self.values, self.n_rules)\n\n    def __setstate__(self, state):\n        self.rule, self.keys, self.values, self.n_rules = state\n        \n        \ncdef class RegExpReplacer:\n    cdef dict rule\n    cdef list keys\n    cdef list values\n    cdef regexp\n    cdef int n_rules\n\n    def __init__(self, dict rule):\n        self.rule = rule\n        self.keys = list(rule.keys())\n        self.values = list(rule.values())\n        self.regexp = re.compile('(%s)' % '|'.join(self.keys))\n        self.n_rules = len(rule)\n\n    @property\n    def rule(self):\n        return self.rule\n\n    def __call__(self, str x):\n        def replace(match):\n            x = match.group(0)\n            if x in self.rule:\n                return self.rule[x]\n            else:\n                for i in range(self.n_rules):\n                    x = re.sub(self.keys[i], self.values[i], x)\n                return x\n        return self.regexp.sub(replace, x)\n    \n\ncdef class ApplyNdArray:\n    cdef func\n    cdef dtype\n    cdef dims\n    cdef int processes\n\n    def __init__(self, func, processes=1, dtype=object, dims=None):\n        self.func = func\n        self.processes = processes\n        self.dtype = dtype\n        self.dims = dims\n\n    def __call__(self, arr):\n        if self.processes == 1:\n            return self.apply(arr)\n        else:\n            return self.apply_parallel(arr)\n\n    cpdef apply(self, arr):\n        cdef int i\n        cdef int n = len(arr)\n        if self.dims is not None:\n            shape = (n, *self.dims)\n        else:\n            shape = n\n        cdef res = np.empty(shape, dtype=self.dtype)\n        for i in range(n):\n            res[i] = self.func(arr[i])\n        return res\n\n    cpdef apply_parallel(self, arr):\n        cdef list arrs = np.array_split(arr, self.processes)\n        with Pool(processes=self.processes) as pool:\n            outputs = pool.map(self.apply, arrs)\n        return np.concatenate(outputs, axis=0)\n","7346066d":"def load_qiqc(n_rows=None):\n    train_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}\/train.csv', nrows=n_rows)\n    submit_df = pd.read_csv(f'{os.environ[\"DATADIR\"]}\/test.csv', nrows=n_rows)\n    n_labels = {\n        0: (train_df.target == 0).sum(),\n        1: (train_df.target == 1).sum(),\n    }\n    train_df['target'] = train_df.target.astype('f')\n    train_df['weights'] = train_df.target.apply(lambda t: 1 \/ n_labels[t])\n\n    return train_df, submit_df\n\n\ndef build_datasets(train_df, submit_df, holdout, seed):\n    submit_dataset = QIQCDataset(submit_df)\n    if holdout:\n        # Train : Test split for holdout training\n        splitter = sklearn.model_selection.StratifiedShuffleSplit(\n            n_splits=1, test_size=0.1, random_state=seed)\n        train_indices, test_indices = list(splitter.split(\n            train_df, train_df.target))[0]\n        train_indices.sort(), test_indices.sort()\n        train_dataset = QIQCDataset(\n            train_df.iloc[train_indices].reset_index(drop=True))\n        test_dataset = QIQCDataset(\n            train_df.iloc[test_indices].reset_index(drop=True))\n    else:\n        train_dataset = QIQCDataset(train_df)\n        test_dataset = QIQCDataset(train_df.head(0))\n\n    return train_dataset, test_dataset, submit_dataset\n\n\nclass QIQCDataset(object):\n\n    def __init__(self, df):\n        self.df = df\n\n    @property\n    def tokens(self):\n        return self.df.tokens.values\n\n    @tokens.setter\n    def tokens(self, tokens):\n        self.df['tokens'] = tokens\n\n    @property\n    def positives(self):\n        return self.df[self.df.target == 1]\n\n    @property\n    def negatives(self):\n        return self.df[self.df.target == 0]\n\n    def build(self, device):\n        self._X = self.tids\n        self.X = torch.Tensor(self._X).type(torch.long).to(device)\n        if 'target' in self.df:\n            self._t = self.df.target[:, None]\n            self._W = self.df.weights\n            self.t = torch.Tensor(self._t).type(torch.float).to(device)\n            self.W = torch.Tensor(self._W).type(torch.float).to(device)\n        if hasattr(self, '_X2'):\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n        else:\n            self._X2 = np.zeros((self._X.shape[0], 1), 'f')\n            self.X2 = torch.Tensor(self._X2).type(torch.float).to(device)\n\n    def build_labeled_dataset(self, indices):\n        return torch.utils.data.TensorDataset(\n            self.X[indices], self.X2[indices],\n            self.t[indices], self.W[indices])\n    \n## Pretrained vector\n\ndef load_pretrained_vectors(names, token2id, test=False):\n    assert isinstance(names, list)\n    with Pool(processes=len(names)) as pool:\n        f = partial(load_pretrained_vector, token2id=token2id, test=test)\n        vectors = pool.map(f, names)\n    return dict([(n, v) for n, v in zip(names, vectors)])\n\n\ndef load_pretrained_vector(name, token2id, test=False):\n    loader = dict(\n        gnews=GNewsPretrainedVector,\n        wnews=WNewsPretrainedVector,\n        paragram=ParagramPretrainedVector,\n        glove=GlovePretrainedVector,\n    )\n    return loader[name].load(token2id, test)\n\n\nclass BasePretrainedVector(object):\n\n    @classmethod\n    def load(cls, token2id, test=False, limit=None):\n        embed_shape = (len(token2id), 300)\n        freqs = np.zeros((len(token2id)), dtype='f')\n\n        if test:\n            np.random.seed(0)\n            vectors = np.random.normal(0, 1, embed_shape)\n            vectors[0] = 0\n            vectors[len(token2id) \/\/ 2:] = 0\n        else:\n            vectors = np.zeros(embed_shape, dtype='f')\n            path = f'{os.environ[\"DATADIR\"]}\/{cls.path}'\n            for i, o in enumerate(\n                    open(path, encoding=\"utf8\", errors='ignore')):\n                token, *vector = o.split(' ')\n                token = str.lower(token)\n                if token not in token2id or len(o) <= 100:\n                    continue\n                if limit is not None and i > limit:\n                    break\n                freqs[token2id[token]] += 1\n                vectors[token2id[token]] += np.array(vector, 'f')\n\n        vectors[freqs != 0] \/= freqs[freqs != 0][:, None]\n        vec = KeyedVectors(300)\n        vec.add(list(token2id.keys()), vectors, replace=True)\n\n        return vec\n\n\nclass GNewsPretrainedVector(object):\n\n    name = 'GoogleNews-vectors-negative300'\n    path = f'embeddings\/{name}\/{name}.bin'\n\n    @classmethod\n    def load(cls, tokens, limit=None):\n        raise NotImplementedError\n        path = f'{os.environ[\"DATADIR\"]}\/{cls.path}'\n        return KeyedVectors.load_word2vec_format(\n            path, binary=True, limit=limit)\n\n\nclass WNewsPretrainedVector(BasePretrainedVector):\n\n    name = 'wiki-news-300d-1M'\n    path = f'embeddings\/{name}\/{name}.vec'\n\n\nclass ParagramPretrainedVector(BasePretrainedVector):\n\n    name = 'paragram_300_sl999'\n    path = f'embeddings\/{name}\/{name}.txt'\n\n\nclass GlovePretrainedVector(BasePretrainedVector):\n\n    name = 'glove.840B.300d'\n    path = f'embeddings\/{name}\/{name}.txt'\n\n    \nclass WordVocab(object):\n\n    def __init__(self, mincount=1):\n        self.counter = Counter()\n        self.n_documents = 0\n        self._counters = {}\n        self._n_documents = defaultdict(int)\n        self.mincount = mincount\n\n    def __len__(self):\n        return len(self.token2id)\n\n    def add_documents(self, documents, name):\n        self._counters[name] = Counter()\n        for document in documents:\n            bow = dict.fromkeys(document, 1)\n            self._counters[name].update(bow)\n            self.counter.update(bow)\n            self.n_documents += 1\n            self._n_documents[name] += 1\n\n    def build(self):\n        counter = dict(self.counter.most_common())\n        self.word_freq = {\n            **{'<PAD>': 0},\n            **counter,\n        }\n        self.token2id = {\n            **{'<PAD>': 0},\n            **{word: i + 1 for i, word in enumerate(counter)}\n        }\n        self.lfq = np.array(list(self.word_freq.values())) < self.mincount\n        self.hfq = ~self.lfq\n        \n        \nclass PunctSpacer(StringReplacer):\n\n    def __init__(self, edge_only=False):\n        puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u2588', '\u00bd', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u00be', '\u22c5', '\u2018', '\u221e', '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]  # NOQA\n        if edge_only:\n            rule = {\n                **dict([(f' {p}', f' {p} ') for p in puncts]),\n                **dict([(f'{p} ', f' {p} ') for p in puncts]),\n            }\n        else:\n            rule = dict([(p, f' {p} ') for p in puncts])\n        super().__init__(rule)\n        \n        \nclass NumberReplacer(RegExpReplacer):\n\n    def __init__(self, with_underscore=False):\n        prefix, suffix = '', ''\n        if with_underscore:\n            prefix += ' __'\n            suffix = '__ '\n        rule = {\n            '[0-9]{5,}': f'{prefix}#####{suffix}',\n            '[0-9]{4}': f'{prefix}####{suffix}',\n            '[0-9]{3}': f'{prefix}###{suffix}',\n            '[0-9]{2}': f'{prefix}##{suffix}',\n        }\n        super().__init__(rule)\n\n\ndef set_seed(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nclass Pipeline(object):\n\n    def __init__(self, *modules):\n        self.modules = modules\n\n    def __call__(self, x):\n        for module in self.modules:\n            x = module(x)\n        return x","05f00a1a":"%%time\nos.environ['DATADIR'] = '\/kaggle\/input'\nset_seed(0)\ntrain_df, submit_df = load_qiqc()\ndatasets = build_datasets(train_df, submit_df, holdout=False, seed=0)\ntrain_dataset, test_dataset, submit_dataset = datasets","212a4fa1":"%%time\ntokenize = Pipeline(\n    str.lower,\n    PunctSpacer(),\n    NumberReplacer(with_underscore=True),\n    str.split\n)\napply_tokenize = ApplyNdArray(tokenize, processes=2, dtype=object)\ntrain_dataset.tokens, test_dataset.tokens, submit_dataset.tokens = \\\n    [apply_tokenize(d.df.question_text.values) for d in datasets]\ntokens = np.concatenate([d.tokens for d in datasets])","88c9eec7":"%%time\nvocab = WordVocab(mincount=1)\nvocab.add_documents(train_dataset.positives.tokens, 'train-pos')\nvocab.add_documents(train_dataset.negatives.tokens, 'train-neg')\nvocab.add_documents(test_dataset.positives.tokens, 'test-pos')\nvocab.add_documents(test_dataset.negatives.tokens, 'test-neg')\nvocab.add_documents(submit_dataset.df.tokens, 'submit')\nvocab.build()","637375d9":"%%time\nglove = load_pretrained_vector('glove', vocab.token2id)\nword_vectors = {'glove': glove}\nunk = (glove.vectors == 0).all(axis=1)\nknown = ~unk","dfea3f5d":"params = dict(\n    min_count=1,\n    workers=1,\n    iter=5,\n    size=300,\n)","95b32ce3":"%%time\nmodel = Word2Vec(**params)\nmodel.build_vocab_from_freq(vocab.word_freq)\nmodel.train(tokens, total_examples=len(tokens), epochs=model.epochs)\nword_vectors['scratch'] = model.wv","44be9a59":"%%time\nmodel = Word2Vec(**params)\nmodel.build_vocab_from_freq(vocab.word_freq)\nidxmap = np.array(\n    [vocab.token2id[w] for w in model.wv.index2entity])\nmodel.wv.vectors[:] = glove.vectors[idxmap]\nmodel.trainables.syn1neg[:] = glove.vectors[idxmap]\nmodel.train(tokens, total_examples=len(tokens), epochs=model.epochs)\nword_vectors['finetune'] = model.wv","41057239":"word = 'obama'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","6b2b29ad":"word = 'lgbt'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","7faf8481":"word = 'cosx'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","a6e0358d":"word = 'brexit'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","9946ecd9":"word = 'coinbase'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","14d6de79":"word = 'tensorflow'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","ae2768c8":"word = 'cos2x'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","dbeb750c":"word = 'kubernetes'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","b0e5075d":"word = 'gdpr'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","c0e9b354":"word = '0bama'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","36d378c6":"word = 'germnay'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","25ee2116":"word = 'gogole'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","65b59be3":"word = 'javadoc'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","16a1e7a6":"word = 'cython'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","cc94e6da":"word = 'compresses'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","85be5ae9":"word = 'xgboost'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","e700c59a":"word = '2sinxcosx'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","4c125083":"word = 'germeny'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","93e75522":"word = 'bigender'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","244f8fd2":"word = 'youcanttellyourstoryfromthe'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","3b995681":"word = '5gfwdhf4rz'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","b8d6fc00":"word = '\u0961'\nprint(vocab.word_freq[word])\npd.DataFrame({name: kv.most_similar(word) for name, kv in word_vectors.items()})","8a6a4ca4":"pd.DataFrame(np.array(list(vocab.word_freq.items()))).to_csv('all.csv', index=False, sep='\\t')\npd.DataFrame(np.array(list(vocab.word_freq.items()))[unk]).to_csv('unk.csv', index=False, sep='\\t')\npd.DataFrame(np.array(list(vocab.word_freq.items()))[known]).to_csv('known.csv', index=False, sep='\\t')","70dc403c":"## Word2Vec fine-tuning (word vector & context vector)","25a40191":"## **Low** frequency words in Quora & **known** words in Glove\n\n- Glove: \u25cb\n- Scratch: \u2613\n- Finetune: \u25cb","39987859":"# Setup & preprocessing","add51480":"## **High** frequency words in Quora & **known** words in Glove\n\n- Glove: \u25cb\n- Scratch: \u25cb\n- Finetune: \u25cb","b52ac664":"# Evaluations","1406f4e3":"## Word2Vec scratch","44f499ae":"## **Low** frequency words in Quora & **unknown** words in Glove\n- Glove: \u2613\n- Scratch: \u2613\n- Finetune: \u2613","6195ca80":"## **High** frequency words in Quora & **unknown** words in Glove\n\n- Glove: -\n- Scratch: \u25cb\n- Finetune: \u25cb","15f25eb5":"# Build models & training","a9b69a50":"# Library codes"}}