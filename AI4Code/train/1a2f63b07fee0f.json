{"cell_type":{"6e24c5db":"code","89129524":"code","f2314713":"code","07a45a4f":"code","d56b9b99":"code","f31ed7b5":"code","a7b67b11":"code","7db05079":"code","705d31c6":"code","18c4eb53":"code","88f9c492":"code","d2a0fefb":"code","77146224":"code","7c6aa83a":"code","5e5af424":"code","2bf8b1ea":"code","97e36210":"code","78a35720":"code","a205975a":"code","334d2fd1":"code","d61e1ab1":"code","66d871d7":"code","7c8b4883":"code","60387789":"code","b08e8e97":"code","1860943b":"code","94fcf9c7":"code","d7038794":"code","04691370":"code","3590dd20":"code","c43a2847":"code","735ca496":"code","16b9cb5b":"code","222a2bc2":"code","1b6d89c8":"code","fb439d68":"code","533a8f5e":"code","e4cab164":"code","c5527609":"code","de51c320":"code","9d6b475c":"code","d6fdaa0a":"code","1452903b":"code","64ce437c":"code","2fe8c554":"code","467b8e53":"code","a8d4c67d":"code","5369174d":"code","2519ceb6":"code","6172c0b7":"code","ccac0dad":"code","ae596bf3":"code","1f60de35":"code","46603a91":"code","3d52855f":"markdown","50975c47":"markdown","6b04b144":"markdown","b0243ed1":"markdown","2e329777":"markdown","dd324d32":"markdown","015d6668":"markdown","9cbfb5ad":"markdown","bb856bed":"markdown","822eb8d2":"markdown","b871d513":"markdown","146d85f8":"markdown","3e382193":"markdown","8a0b502a":"markdown","57337351":"markdown","35034104":"markdown","c0ce3744":"markdown","66a10391":"markdown","7d75b8f9":"markdown","a68aaeca":"markdown","8adeee6e":"markdown","c2fca416":"markdown","0f58956f":"markdown","e8acbbd6":"markdown","17561af4":"markdown","5816d21a":"markdown","06f85262":"markdown","dffdbef8":"markdown","9bf46456":"markdown"},"source":{"6e24c5db":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')","89129524":"df = pd.read_csv('..\/input\/cardataset\/data.csv')\ndf.head()","f2314713":"df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\ndf.head()","07a45a4f":"\"\"\"\nMaking List of Categorical Columns\n\"\"\"\n\n\"\"\" \nMethod shown in mlzoomcamp \n\"\"\"\nstrings = list(df.dtypes[df.dtypes == 'object'].index)\nstrings\n\n\n# Other methods \n# list(df.select_dtypes(include = 'O').columns)\n# [col for col in df.columns if df[col].dtype == 'object']","d56b9b99":"\"\"\"\nCleaning Categorical Data in our data set\n\"\"\"\nfor col in strings:\n    df[col] = df[col].str.lower().str.replace(\" \", \"_\")\n    \ndf.head()","f31ed7b5":"for col in df.columns:\n    print(col)\n    print(df[col].unique()[:5])\n    print(df[col].nunique())\n    print('\\n')","a7b67b11":"sns.histplot(df['msrp'], bins = 50);","7db05079":"sns.histplot(df['msrp'][df['msrp'] < 100000], bins = 50);","705d31c6":"\"\"\"\nApplying log to 'msrp' column\n\"\"\"\nprice_logs = np.log1p(df['msrp'])\nsns.histplot(price_logs, bins = 50);","18c4eb53":"print(df.isnull().sum())\ndf.isnull().sum().plot(kind = 'bar')","88f9c492":"\"\"\"\nCreating length\/number of values of train, validation and test set\n\"\"\"\nn  = len(df)    \n\nn_val = int(n * 0.2)  # Creating Validation Set\nn_test = int(n * 0.2) # Creating test set\nn_train = n - n_val - n_test\n\n\"\"\"\nSetting up for randomizing the values\n\"\"\"\nidx = np.arange(n)\nnp.random.seed(2)\nnp.random.shuffle(idx)\n\n\n\"\"\"\nCreating data set for Train Valid and Test set.\nAlso to randomize the index to avoid bias.\n\"\"\"\n\ndf_train = df.iloc[idx[:n_train]]\ndf_val = df.iloc[idx[n_train:n_train+n_val]]\ndf_test = df.iloc[idx[n_train+n_val:]]\n\n\n\"\"\"\nDropping the indexes from all the datasets as they are of no use now.\n\"\"\"\n\ndf_train = df_train.reset_index(drop = True)\ndf_val = df_val.reset_index(drop = True)\ndf_test = df_test.reset_index(drop = True)\n\n\n\"\"\"\nMaking dependent variables with LOG transformation for all the datasets.\n\"\"\"\n\ny_train = np.log1p(df_train['msrp'].values)\ny_val = np.log1p(df_val['msrp'].values)\ny_test = np.log1p(df_test['msrp'].values)\n\n\"\"\"\nDeleting target \/ dependent variables from training, validation and test sets\n\"\"\"\ndel df_train['msrp']\ndel df_val['msrp']\ndel df_test['msrp']\n\n\"\"\"\nFinally Checking the Length of Datasets created\n\"\"\"\nlen(df_train), len(df_val), len(df_test)","d2a0fefb":"\"\"\"\nTaking values of 3 variables for a specific row (10): engine_hp, city_mpg, popularity\nDeclaring Biased Term and Weights for each features\n\"\"\"\nxi = [453, 11, 86] \n\n\nw0 = 0\n\n\"\"\"\nDeclaring Weights for each features\n\"\"\"\nw =  [1, 1, 1]\n\n\"\"\"\nCreating Linear Regression Function\n\"\"\"\ndef linear_regression(xi):\n    n =len(xi)                # Number of features used\n    \n    pred = w0                 # Initial \/ Base prediction\n    \n    for j in range(n):\n        pred += w[j]*xi[j]     # Formula = w0 +sigma[0:n-1]{w[j]*xi[j]}\n    \n    return pred\n\n\"\"\"\nCalling linear_regression function on xi\n\"\"\"\nlinear_regression(xi)","77146224":"\"\"\"\nChanging the bias terms and checking the prediction\n\"\"\"\n\n\"\"\"Biased Term :\"\"\"\nw0 = 7.17\n\n\"\"\"#Declaring Weights for each features\"\"\"\nw =  [0.01, 0.04, 0.002]\n\n\"\"\"\nCalling linear_regression function again on xi\n\"\"\"\nlinear_regression(xi)","7c6aa83a":"\"\"\"\nUsing exponent to undo the logrithm which we did initially\nAlso recall we did log(1 + msrp). Therefore we will need subtract 1.\n\"\"\"\nprint(\"predicted prices: \", np.expm1(12.312))","5e5af424":"\"\"\"\nDefining a function for dot product\n\"\"\"\ndef dot(xi,w):\n    n = len(xi)\n    \n    res = 0.0\n    \n    for j in range(n):\n        res += xi[j]*w[j]\n    return res\n\n\"\"\"\nImprovising linear_regression\n\"\"\"\ndef linear_regression(xi):\n    return w0 + dot(xi,w)\n\n\"\"\"\nMore Improvising : using W.T Xi = Xi W.T = w0 + dot product as done before\nWe add weight of 1 to bias term and add 1 to xis also.\n\"\"\"\nw_new = [w0] + w\ndef linear_regression(xi):\n    xi = [1] + xi\n    return dot(xi,w_new)\n\n\n\"\"\"\nchecking the function again\n\"\"\"\nlinear_regression(xi)","2bf8b1ea":"\"\"\"\nMaking final model with multiple vectors.\n\"\"\"\n\n\"\"\"Biased Term :\"\"\"\nw0 = 7.17\n\n\"\"\"#Declaring Weights for each features\"\"\"\nw =  [0.01, 0.04, 0.002]\n\n\"\"\"We add weight of 1 to bias term\"\"\"\nw_new = [w0] + w\n\n\"\"\"Making Variables\"\"\"\nx1 = [1, 148, 24, 1385]\nx2 = [1, 132, 25, 2031]\nx10 = [1, 453, 11, 86]\n\n\"\"\"Making X: independent variables array\"\"\"\nX= [x1, x2, x10]\nX = np.array(X)\n\n\"\"\"\nLinear Regression with multiple vectors\n\"\"\"\ndef linear_regression(X):\n    return X.dot(w_new)\n\n\"\"\"\nPredicting Values\n\"\"\"\nlinear_regression(X)","97e36210":"\"\"\"Making Variables\"\"\"\nX = [\n    [148, 24, 1385],\n    [132, 25, 2031],\n    [453, 11, 86],\n    [158, 24, 185],\n    [172, 25, 201],\n    [413, 11, 86],\n    [38, 54, 185],\n    [142, 25, 431],\n    [453, 31, 86]\n]\n \n\"\"\"Making X: independent variables array\"\"\"\nX = np.array(X)\n\n# \"\"\"\n# Including a biased term\n# \"\"\"\n# ones = np.ones(X.shape[0])\n# X = np.column_stack([ones, X])\n# X\n\n\"\"\"Declaring Target Variable\"\"\"\ny = [10000, 20000, 15000, 20050, 10000, 20000, 15000, 25000, 120]\n\n","78a35720":"\"\"\"\nGram Matrix\n\"\"\"\nXTX = X.T.dot(X)\n\n\"\"\"inverse of Gram Matrix\"\"\"\nXTX_inv = np.linalg.inv(XTX)\n\n\"\"\"\nNot exactly identity matrix but the numbers here are very small. Hence can be treated as 0s and 1s\n\"\"\"\n# XTX.dot(XTX_inv).round(1)\nXTX.dot(XTX_inv)\n\n\"\"\" Model\"\"\"\nw_full = XTX_inv.dot(X.T).dot(y)\n\n\n\"\"\"\nCreating Coefficients\n\"\"\"\nw0 = w_full[0]\nw = w_full[1:]\n\n\"\"\"\nprinting Coefficient\n\"\"\"\nw0, w","a205975a":"\"\"\"Creating a function for Training Linear Regression Model\"\"\"\ndef train_linear_regression(X,y):\n    \"\"\"\n    Including a biased term\n    \"\"\"\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    \"\"\"\n    Gram Matrix\n    \"\"\"\n    XTX = X.T.dot(X)\n    \n    \"\"\"inverse of Gram Matrix\"\"\"\n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]   \n\n\n\"\"\"Training the Model\"\"\"","334d2fd1":"\"\"\"\nWe need only numerical columns\n\"\"\"\ndf_train.dtypes\n\n\"\"\"\nCreating subset of only Numerical Columns\n\"\"\"\nbase = ['engine_hp','engine_cylinders','highway_mpg', 'city_mpg', 'popularity']\n\n\"\"\"\nCreating subset of Dataframe to be used as X.\n\"\"\"\nX_train = df_train[base].values\n\n\"\"\" Training model will throw error b'coz of missing values\"\"\"\ntrain_linear_regression(X_train,y_train)","d61e1ab1":"\"\"\"\nchecking missing values in our subset\n\"\"\"\ndf_train[base].isnull().sum()","66d871d7":"\"\"\"Imputing Missing values with 0 \"\"\"\nX_train = df_train[base].fillna(0).values","7c8b4883":"\"\"\"Building and Training the model \"\"\"\nw0, w = train_linear_regression(X_train,y_train)\n\n\"\"\" Prediction\"\"\"\ny_pred =  w0 + X_train.dot(w)\n\n\"\"\"Visualizing the Predictions\"\"\"\nsns.histplot(y_pred, color = 'red', alpha = 0.5, bins = 50)\nsns.histplot(y_train, color = 'blue', alpha = 0.5, bins = 50);","60387789":"def rmse(y,y_pred):\n    error  = y- y_pred\n    squared_error = error ** 2\n    mse = squared_error.mean()\n    return np.sqrt(mse)  ","b08e8e97":"rmse(y_train,y_pred)","1860943b":"\"\"\"\nCreating subset of only Numerical Columns\n\"\"\"\nbase = ['engine_hp','engine_cylinders','highway_mpg', 'city_mpg', 'popularity']\n\ndef prepare_X(df):\n    df_num = df[base]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","94fcf9c7":"X_train = prepare_X(df_train)                            # Preparing Training set\n\n\"\"\"Building and Training the model on training set\"\"\"\nw0,w = w0, w = train_linear_regression(X_train,y_train)  # Building the model on Train set\n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                 # Preparing Validation Set\ny_pred = w0 + X_val.dot(w)                                # Prediction on Validation Set\n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                       # Calculating RMSE for Validation Set ","d7038794":"\"\"\"\nModifying prepare_S function to include Feature engineering step\n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    df['age'] = max(df['year']) - df['year']\n    features = base + ['age']\n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","04691370":"X_train = prepare_X(df_train)","3590dd20":"X_train = prepare_X(df_train)                            # Preparing Training set\n\n\"\"\"Building and Training the model on training set\"\"\"\nw0,w = w0, w = train_linear_regression(X_train,y_train)  # Building the model on Train set\n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                 # Preparing Validation Set\ny_pred = w0 + X_val.dot(w)                                # Prediction on Validation Set\n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                       # Calculating RMSE for Validation Set ","c43a2847":"\n\"\"\"Visualizing the Predictions\"\"\"\nsns.histplot(y_pred, color = 'red', alpha = 0.5, bins = 50)\nsns.histplot(y_val, color = 'blue', alpha = 0.5, bins = 50);","735ca496":"\"\"\"Top 5 makers\"\"\"\nmakes = list(df['make'].value_counts().head().index)\nmakes","16b9cb5b":"\"\"\"\nModifying prepare_S function for Creating new columns for number of doors\nMaking columns for top 5 makes of the cars\n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n     \n    df['age'] = max(df['year']) - df['year']    \n    features.append('age')\n    \n    \"\"\"Making Variables for number of Doors\"\"\"\n    for v in [2,3,4]:\n        df['num_doors_%s' %v] = (df['number_of_doors'] == v).astype('int') \n        features.append('num_doors_%s' %v)\n      \n    \"\"\"Making variables for top 5 makes\"\"\"\n    for m in makes:\n        df['make_%s' %m] = (df['make'] == m).astype('int') \n        features.append('make_%s' %m)\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","222a2bc2":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            # Preparing Training set\n\n\"\"\"Building and Training the model on training set\"\"\"\nw0,w = train_linear_regression(X_train,y_train)  # Building the model on Train set\n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                 # Preparing Validation Set\ny_pred = w0 + X_val.dot(w)                                # Prediction on Validation Set\n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                       # Calculating RMSE for Validation Set ","1b6d89c8":"categorical_variables = ['make','engine_fuel_type', 'transmission_type' , 'driven_wheels', 'market_category', \n'vehicle_size', 'vehicle_style' ]           ","fb439d68":"categories = {}\n\nfor c in categorical_variables:\n    categories[c] = list(df[c].value_counts().head().index)\n","533a8f5e":"categories.items()","e4cab164":"\"\"\"\nModifying to include categorical variables \n\"\"\"\ndef prepare_X(df):\n    df = df.copy()\n    features = base.copy()\n     \n    df['age'] = max(df['year']) - df['year']    \n    features.append('age')\n    \n    \"\"\"Making Variables for number of Doors\"\"\"\n    for v in [2,3,4]:\n        df['num_doors_%s' %v] = (df['number_of_doors'] == v).astype('int') \n        features.append('num_doors_%s' %v)\n        \n    \"\"\"Making Variables for Categorical variables\"\"\"\n    for c,values in categories.items():\n        for v in values:\n            df[\"%s_%s\" %(c,v)] = (df[c] == v).astype('int') \n            features.append(\"%s_%s\" %(c,v))\n        \n    df_num = df[features]\n    df_num = df_num.fillna(0)\n    X = df_num.values\n    return X","c5527609":"\"\"\"Preparing Training set\"\"\"\nX_train = prepare_X(df_train)                            # Preparing Training set\n\n\"\"\"Building and Training the model on training set\"\"\"\nw0,w = train_linear_regression(X_train,y_train)  # Building the model on Train set\n\n\"\"\"Prediction the values of Validation Set\"\"\"\nX_val = prepare_X(df_val)                                 # Preparing Validation Set\ny_pred = w0 + X_val.dot(w)                                # Prediction on Validation Set\n\n\"\"\"Evaluation on Validation set\"\"\"\nrmse(y_val, y_pred)                                       # Calculating RMSE for Validation Set ","de51c320":"def train_linear_regression_reg(X, y, r=0.001):\n    ones = np.ones(X.shape[0])\n    X = np.column_stack([ones, X])\n    \n    XTX = X.T.dot(X)\n    \"\"\"Adding Regularization term to the diagonals\"\"\"\n    XTX = XTX + r * np.eye(XTX.shape[0])\n    \n    XTX_inv = np.linalg.inv(XTX)\n    w_full = XTX_inv.dot(X.T).dot(y)\n    \n    return w_full[0], w_full[1:]","9d6b475c":"\"\"\"Preparing X, Building, predicting and Evaluating model\"\"\"\nX_train = prepare_X(df_train)\nw0, w = train_linear_regression_reg(X_train, y_train, r=0.01)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nrmse(y_val, y_pred)","d6fdaa0a":"\"\"\"\nTraining the model for different values of r: regularization term\n\"\"\"\nfor r in [0.0, 0.00001, 0.0001, 0.001, 0.1, 1, 10]:\n    X_train = prepare_X(df_train)\n    w0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\n    X_val = prepare_X(df_val)\n    y_pred = w0 + X_val.dot(w)\n    score = rmse(y_val, y_pred)\n    \n    print(r, w0, score)","1452903b":"\"\"\"\nSelecting the best r for training our model \n\"\"\"\nr = 0.001\nX_train = prepare_X(df_train)\nw0, w = train_linear_regression_reg(X_train, y_train, r=r)\n\nX_val = prepare_X(df_val)\ny_pred = w0 + X_val.dot(w)\nscore = rmse(y_val, y_pred)\nscore","64ce437c":"\"\"\"Concatnating the datasets: train and valid\"\"\"\ndf_full_train = pd.concat([df_train, df_val])","2fe8c554":"\"\"\"Full Train dataset with index dropped. As they are of no use.\"\"\"\ndf_full_train = df_full_train.reset_index(drop=True)","467b8e53":"\"\"\"Creating X and y\"\"\"\nX_full_train = prepare_X(df_full_train)\ny_full_train = np.concatenate([y_train, y_val])","a8d4c67d":"\"\"\"Building model\"\"\"\nw0, w = train_linear_regression_reg(X_full_train, y_full_train, r=0.001)","5369174d":"\"\"\"Preparing X, Predicting and Evaluating model\"\"\"\nX_test = prepare_X(df_test)\ny_pred = w0 + X_test.dot(w)\nscore = rmse(y_test, y_pred)\nscore","2519ceb6":"\"\"\"Checking our model on random car selected\"\"\"\ncar = df_test.iloc[20].to_dict()\ncar","6172c0b7":"\"\"\"Making dataset out of given dictionary\"\"\"\ndf_small = pd.DataFrame([car])\ndf_small","ccac0dad":"\"\"\"Creating X\"\"\"\nX_small = prepare_X(df_small)","ae596bf3":"\"\"\"Predicting y\"\"\"\ny_pred = w0 + X_small.dot(w)\ny_pred = y_pred[0]\ny_pred","1f60de35":"\"\"\"Undoing Log by using exp\"\"\"\nnp.expm1(y_pred)","46603a91":"\"\"\"Checking what was the real value of that car.\"\"\"\nnp.expm1(y_test[20])","3d52855f":"[back to top](#table-of-contents)\n<a id=\"11\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">11. Car Price Baseline Model<\/div>","50975c47":"[back to top](#table-of-contents)\n<a id=\"16\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">16. Regularization<\/div>","6b04b144":"[back to top](#table-of-contents)\n<a id=\"6\"><\/a>\n<a id=\"6\"><\/a><div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">6. Missing Values<\/div>","b0243ed1":"[back to top](#table-of-contents)\n<a id=\"9\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">9. Linear Regression Vector Form<\/div>","2e329777":"[back to top](#table-of-contents)\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">Distribution of price with msrp less than 100000 <\/div>","dd324d32":"[back to top](#table-of-contents)\n<a id=\"17\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">17. Tuning the Model<\/div>","015d6668":"[back to top](#table-of-contents)\n<a id=\"10\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">10. Training Linear Regression - Normal Equationraining Linear Regression - Normal Equation<\/div>","9cbfb5ad":"[back to top](#table-of-contents)\n<a id=\"4.2\"><\/a>\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">4.2 <u>Cleaning Categorical Data in our data set<\/u> <\/div>","bb856bed":"[back to top](#table-of-contents)\n<a id=\"14\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">14. Feature Engineering<\/div>","822eb8d2":"<a id=\"9.1\"><\/a>\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">9.1 Generalized Linear Regression<\/div>","b871d513":"[back to top](#table-of-contents)\n<a id=\"18\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">18. Using the model<\/div>","146d85f8":"[back to top](#table-of-contents)\n<a id=\"12\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">12. Root Mean Squared Error (RMSE)<\/div>","3e382193":"[back to top](#table-of-contents)\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">Log(msrp +1)<\/div>","8a0b502a":"### **General Regression:** $\\large  g(X) \\approx y$\n\n### **Liner Regression for predicting price for only 1 car**  $\\Large g(x_i) \\approx y_i$\n### $\\large x_i = (x_{i1}, x_{i2}, ..... , x_{in})$ \n### $\\large y_i = g(x_{i1}, x_{i2}, ..... , x_{in})$ \n### **where $\\large x_i$ is a row in our dataset and $\\large y_i$ is the prediction we get from function** $\\large g(x_i)$\n\n### **In the following implementation we took:**\n### $\\large x_i$ **= [453, 11, 86]**\n\n### Now the formula says:\n### $\\large {g(x_i) = w_0 + w_1x_{i1} + w_2x_{i2} +w_3x_{i3}}$\n### Therefore we implement this formula $\\large g(x_i) = w_0 + \\sum_{k=0}^{n-1}w_jx_{ij}$\n### $w_0$ :  is the biased term i.e. Price of a car when we don't know anything about the car\n\n","57337351":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">2. Importing Libraries<\/div>","35034104":"[back to top](#table-of-contents)\n<a id=\"5\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">5. Exploratory Data Analysis<\/div>","c0ce3744":"[back to top](#table-of-contents)\n<a id=\"15\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">15. Categorical Variables<\/div>","66a10391":"[back to top](#table-of-contents)\n\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">Distribution of price<\/div>","7d75b8f9":"[back to top](#table-of-contents)\n<a id=\"8\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">8. Linear Regression<\/div>","a68aaeca":"[back to top](#table-of-contents)\n<a id=\"13\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">13. Validating the Model using RMSE<\/div>","8adeee6e":"<a id=\"table-of-contents\"><\/a>","c2fca416":"<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">Unique values and their numbers <\/div>","0f58956f":"[back to top](#table-of-contents)\n<a id=\"1\"><\/a>\n\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">1. Introduction\n   \n<p style=\"font-family:cursive;font-size:17px;color:white\">Cars dataset with features including make, model, year, engine, and other properties of the car used to predict its price.<\/p>\n<p style=\"font-family:cursive;font-size:17px;color:white\">Scraped from Edmunds and Twitter.<\/p>\n<p style=\"font-family:cursive;font-size:17px;color:white\"> Effects of features on the price<\/p>\n<ul style=\"font-family:cursive;font-size:17px;color:white\">\n<li> Predict the price of Cars using different Variables <\/li>\n    <\/ul>\n<\/div>\n\n","e8acbbd6":"[back to top](#table-of-contents)\n<a id=\"4.1\"><\/a>\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">4.1 Making List of Categorical Columns <\/div>","17561af4":"[back to top](#table-of-contents)\n<a id=\"3\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">3. Loading and Reading Data <\/div>\n\n","5816d21a":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>Content in this Notebook<\/u>\n\n<p style=\"font-family:cursive;font-size:17px; color:white\" >Notebook is a part of FREE ML course by Glexey Grigorev.\n<a style=\"font-family:cursive;font-size:17px; color: yellow\" href=\"https:\/\/datatalks.club\/courses\/2021-winter-ml-zoomcamp.html\" target=\"_blank\"> <u>Link for the Course<\/u><\/a><\/p>\n    \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#1\"> 1. Introduction <\/a><\/li>     \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#2\"> 2. Impoting Libraries <\/a><\/li> \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15;\" href = \"#3\"> 3. Loading and Reading Data <\/a><\/li> \n<li  style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#4\"> 4. Cleaning Strings in Column and Values<\/a><\/li> <ul>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#4.1\"> 4.1 Making List of Categorical Columns<\/a><\/li>\n    <li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#4.2\">4.2  Cleaning Categorical Data in our data set<\/a><\/li>\n    <\/ul>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#5\"> 5. Exploratory Data Analysis <\/a><\/li>     \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#6\"> 6. Missing Values <\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#7\"> 7. Validation Framework: Creating Train Validation and Test Split Manually<\/a><\/li>        \n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#8\">8. Linear Regression<\/a><\/li>\n<li style = \"line-height: 0.7\"><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#9\">9. Linear Regression Vector Form<\/a><\/li>\n<ul>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#9.1\"> 9.1 Generalized Linear Regression<\/a><\/li>\n    <li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#9\">9.2 Linear Regression with Multiple Variables<\/a><\/li><\/ul>\n        <li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#10\">10. Training Linear Regression - Normal Equation<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#11\">11. Car Price Baseline Model<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#12\">12. Root Mean Squared Error (RMSE)<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#13\">13. Validating the Model using RMSE<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#14\">14. Feature Engineering<\/a><\/li>\n    \n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#15\">15. Categorical Variables<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#16\">16. Regularization<\/a><\/li>\n<li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#17\">17. Tuning the Model<\/a><\/li>\n    <li style = \"line-height: 0.7\" ><a style=\"font-family:cursive;font-size:17px; color:#ecfe15\" href = \"#18\">18. Using the model<\/a><\/li>\n\n    \n    \n    \n    \n    \n<\/div>\n","06f85262":"[back to top](#table-of-contents)\n<a id=\"9.2\"><\/a>\n<div style=\"background:#59a1c6;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:black\">9.2 Linear Regression with Multiple Variables<\/div>","dffdbef8":"[back to top](#table-of-contents)\n<a id=\"4\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">4. Data Cleaning: Cleaning Strings in Column and values<\/div>","9bf46456":"[back to top](#table-of-contents)\n<a id=\"7\"><\/a>\n<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\">7. Validation Framework: Creating Train Validation and Test Split Manually<\/div>\n"}}