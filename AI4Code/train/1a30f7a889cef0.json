{"cell_type":{"27d8e87a":"code","bb0d9a47":"code","1e445591":"code","232daeb2":"code","4b9f3794":"code","25c86e59":"code","f05923cd":"code","2bdc87c5":"code","af71f6ad":"code","9b81f014":"code","45143499":"code","d187e7f6":"code","db4af0bf":"code","6491a5da":"code","79742ce7":"code","a422796c":"code","5099f5cb":"code","34b3b8ed":"code","6eb20b3a":"code","ed09c536":"code","4a1b7dfc":"code","d55120e9":"code","07851fce":"code","d399cfce":"code","9c2284cc":"code","0e1dbc0b":"code","ec348f82":"code","4b977437":"code","15d84417":"code","c98e1b0f":"code","5e9ac27a":"code","77bbcdcf":"markdown","9139be1e":"markdown","96b399c8":"markdown","b297cfa5":"markdown","a95cfabe":"markdown","1bd6326b":"markdown","14928d2b":"markdown","0353e51e":"markdown","9488789e":"markdown","9e172f1d":"markdown","7aa45361":"markdown","55b8e664":"markdown","1d287396":"markdown","e14597f9":"markdown"},"source":{"27d8e87a":"import numpy as np \nimport pandas as pd \nimport os\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import transforms\nfrom PIL import Image\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport matplotlib.pylab as pylab\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk import pos_tag, ne_chunk\nfrom textblob import TextBlob\nfrom textwrap import wrap\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import euclidean_distances\n","bb0d9a47":"seed = 42\nnum_epochs = 10\nlearning_rate = 0.00001\ntrain_CNN = False\nbatch_size = 32\nshuffle = True\npin_memory = True\nnum_workers = 1\n\ndef get_device():\n    if torch.cuda.is_available():\n        device = 'cuda:0'\n    else:\n        device = 'cpu'\n    return device\ndevice = get_device()","1e445591":"train_path = '..\/input\/shopee-product-matching\/train.csv'\ntest_path = '..\/input\/shopee-product-matching\/test.csv'\nsubmission_path = '..\/input\/shopee-product-matching\/sample_submission.csv'\nimage_train_path = '..\/input\/shopee-product-matching\/train_images'\nimage_test_path = '..\/input\/shopee-product-matching\/test_images'","232daeb2":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\nsub_df = pd.read_csv(submission_path)","4b9f3794":"train_df.head()","25c86e59":"print( f'{b_}train size --> {y_}{train_df.shape[0]}' )\nprint( f'{b_}test size --> {y_}{test_df.shape[0]}' )","f05923cd":"for col in ['posting_id', 'image_phash', 'label_group']:\n    print( f'{b_}{col} unique values --> {y_}{train_df[col].nunique()}' )","2bdc87c5":"class ShopeeDataset(Dataset):\n    def __init__(self, root_dir, csv_path, transform=None):\n        self.root_dir = root_dir\n        self.annotations = pd.read_csv(csv_path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        index = int( index)\n        img_id = self.annotations.iloc[index, 1]\n        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n        y_label = torch.tensor( float(self.annotations.iloc[index, -1]))\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return (img, y_label)","af71f6ad":"transform = transforms.Compose(\n        [\n            transforms.Resize((224, 224)),\n#             transforms.RandomCrop((299, 299)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )","9b81f014":"train_ds = ShopeeDataset(      root_dir = image_train_path,\n                               csv_path = train_path,  \n                               transform = transform\n                            )\ntest_ds = ShopeeDataset(\n                                root_dir = image_test_path,\n                                csv_path = test_path,\n                                transform = transform\n                            )","45143499":"def plot_sample(rows=5, cols=3):\n    \"\"\" \n        Plots sample image from training data set\n    \"\"\"\n    fig = plt.figure(figsize=(20, 20))\n    grid = plt.GridSpec(rows, cols, wspace = .25, hspace = .25)\n    for i in range(rows*cols):\n        exec (f\"plt.subplot(grid{[i]})\")\n        temp_img, temp_lab = train_ds[i]\n        plt.axis('off')\n        txt = train_df.iloc[i, -2]\n        plt.text(.5,.01,txt, bbox=dict(fill=False, edgecolor='red', linewidth=2),  horizontalalignment='center')\n        plt.imshow(temp_img.numpy().transpose((1, 2, 0)))\nplot_sample()","d187e7f6":"!wget https:\/\/i.pinimg.com\/originals\/05\/7b\/27\/057b274c134bcf92ac151758478949b3.png\n!ls","db4af0bf":"mask =  np.array(Image.open('057b274c134bcf92ac151758478949b3.png'))\nstopwords = set(STOPWORDS)\ntext = \" \".join(review for review in train_df.title)\nwordcloud = WordCloud(max_font_size=150, max_words=100, mask=mask, random_state = 42, \n                      stopwords=stopwords , \n                      background_color='white',\n                      contour_width=1,\n                      contour_color='firebrick').generate(text)\nimage_colors = ImageColorGenerator(mask)\nwordcloud.recolor(color_func=image_colors)\nplt.figure(figsize=(20, 8))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","6491a5da":"train_dl = DataLoader(    train_ds, \n                          batch_size=128, \n                          shuffle=shuffle,\n                     )\n\ntest_dl = DataLoader(    test_ds, \n                         batch_size=128, \n                         shuffle=shuffle,\n                         \n                     )","79742ce7":"def preprocess_text( sentence ):\n    \"\"\"\n        preprocess text\n        sentence: input text sentence for preprocessing\n    \"\"\"\n    sentence = str(sentence)\n    sentence = sentence.lower()\n    sentence = sentence.translate( str.maketrans('', '', string.punctuation))\n    sentence = sentence.strip()\n    sentence = word_tokenize( sentence )\n    sentence = [ word for word in sentence if not word in stopwords]\n    wl = WordNetLemmatizer()\n    sentence = [ wl.lemmatize( word) for word in sentence ]\n    sentence = ' '.join(sentence)\n    return sentence\n\ndef word_count( sentence):\n    \"\"\" \n        Counts number of words in a sentence \n        sentence: input text sentence for preprocessing\n    \"\"\"\n    wc = sentence.split(' ')\n    return len( wc )\n\ndef avg_word_count( sentence):\n    \"\"\" \n        Average Counts of words in a sentence \n        sentence: input text sentence for preprocessing\n    \"\"\"\n    avg_wc = sum(len(word) for word in sentence) \/ len(sentence.split())\n    return avg_wc\n\ndef save_preprocessed(df):\n    df.to_feather('preprocessed.fth')\n    print( 'saved preprocessed...')","a422796c":"%%time\ntrain_df['processed_title'] = train_df.title.apply(lambda x: preprocess_text(x))\ntrain_df['word_count'] = train_df.processed_title.apply(lambda x: word_count(x))\ntrain_df['avg_word_count'] = train_df.processed_title.apply(lambda x: avg_word_count(x))\nsave_preprocessed(train_df)","5099f5cb":"train_df.head()","34b3b8ed":"!pip install lightning-flash -q","6eb20b3a":"import os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nimport pytorch_lightning as pl\n\nfrom flash.core.data import download_data\nfrom flash.vision import ImageEmbedder\nfrom scipy import spatial\npl.seed_everything(7)","ed09c536":"class LitAutoEncoder(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(   nn.Linear(224 * 224, 128), \n                                        nn.ReLU(), \n                                        nn.Linear(128, 3))\n        \n        self.decoder = nn.Sequential(   nn.Linear(3, 128), \n                                        nn.ReLU(),\n                                        nn.Linear(128, 224 * 224))\n\n    def forward(self, x):\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.view(-1, 224*224 )\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log('train_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer","4a1b7dfc":"autoencoder = LitAutoEncoder()\ntrainer = pl.Trainer(gpus= 1, max_epochs=2, progress_bar_refresh_rate=20)\ntrainer.fit(autoencoder, train_dl)\n","d55120e9":"train_image_path = []\ndef get_train_image_path( num_groups=10 ):\n    \"\"\"\n        preapare groups of image paths if the number of images is greater than 10 for each group label for training\n    \"\"\"\n    group_labels = train_df.groupby('label_group')['image'].unique() # get 10 different grouplabels\n    for idx in range( len(group_labels )) :\n        image_list = group_labels.iloc[idx]\n        if  len(image_list) > 10 :\n            image_paths = [ os.path.join(image_train_path, image) for image in group_labels.iloc[ idx ] ] \n            for image in image_paths:\n                train_image_path.append( image )\n    print( f' total images in training --> {len(train_image_path)}')\n    \nget_train_image_path()","07851fce":"embedder = ImageEmbedder(backbone=\"swav-imagenet\", embedding_dim=128)\nvectors = []\nfor image_path in train_image_path[:50]:\n    embeddings = embedder.predict([ image_path ] )\n    vectors.append( embeddings)","d399cfce":"!pip install sentence-transformers -q","9c2284cc":"\nfrom sentence_transformers import SentenceTransformer\nsbert_model = SentenceTransformer('bert-base-nli-mean-tokens')","0e1dbc0b":"\ndocument_embeddings = sbert_model.encode(train_df['processed_title'])","ec348f82":"np.save('document_embeddings.npy',document_embeddings)","4b977437":"def most_similar(doc_id,similarity_matrix,matrix):\n    print (f'Document: {train_df.iloc[doc_id][\"processed_title\"]}')\n    print ('\\n')\n    print (f'Similar Documents using {matrix}:')\n    if matrix=='Cosine Similarity':\n        similar_ix=np.argsort(similarity_matrix[doc_id])[::-1]\n    elif matrix=='Euclidean Distance':\n        similar_ix=np.argsort(similarity_matrix[doc_id])\n    for ix in similar_ix:\n        if ix==doc_id:\n            continue\n        print('\\n')\n        print (f'Document: {train_df.iloc[ix][\"processed_title\"]}')\n        print (f'{matrix} : {similarity_matrix[doc_id][ix]}')","15d84417":"pairwise_similarities=cosine_similarity(document_embeddings)\npairwise_differences=euclidean_distances(document_embeddings)","c98e1b0f":"np.save('pairwise_similarities.npy',pairwise_similarities)\nnp.save('pairwise_differences.npy',pairwise_differences)","5e9ac27a":"most_similar(0,pairwise_similarities,'Cosine Similarity')","77bbcdcf":"# <center style=\"background-color:yellow\"> About the Competition <\/center>\n","9139be1e":"<img src=\"https:\/\/static.seekingalpha.com\/uploads\/2020\/11\/24\/saupload_shopee-11-11-header-2-1000x600.jpg\" align=\"center\"> <\/img>\n","96b399c8":"# Feature Generation","b297cfa5":"# <center style=\"background-color:yellow\"> Data Description <\/center>","a95cfabe":"# <center style=\"background-color:yellow\"> Load Libraries <\/center>","1bd6326b":"# <center style=\"background-color:yellow\"> Load data <\/center>","14928d2b":"# <h1 align=\"center\" style=\"background-color:powderblue;\" style=\"font-size:300%;\"> Shopee - Price Match Guarantee   <\/h1>\nsource: seekingalpha.com","0353e51e":"# <center style=\"background-color:yellow\"> Data Exploration <\/center>","9488789e":"# <center style=\"background-color:yellow\"> Data Preprocessing <\/center>","9e172f1d":"# <h1 align=\"center\"  style=\"background-color:yellow;\" style=\"font-family:verdana;\"> \u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f If you find this note book helpful. <b>please upvote!<\/b> \u2b06\ufe0f\u2b06\ufe0f\u2b06\ufe0f <\/h1>","7aa45361":"Finding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!\n\nAs this is a code competition, only the first few rows\/images of the test set are published; the remainder are only available to your notebook when it is submitted. Expect to find roughly 70,000 images in the hidden test set. The few test rows and images that are provided are intended to illustrate the hidden test set format and folder structure.\n### Files\n`train\/test`.csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.\n\n`posting_id` - the ID code for the posting.\n\n`image` - the image id\/md5sum.\n\n`image_phash` - a perceptual hash of the image.\n\n`title` - the product description for the posting.\n\n`label_group` - ID code for all postings that map to the same product. Not provided for the test set.\n\n`train\/test`images - the images associated with the postings.\n\n`sample_submission.csv` - a sample submission file in the correct format.\n\n`posting_id` - the ID code for the posting.\n\n`matches` - Space delimited list of all posting IDs that match this posting. Posts always self-match. Group sizes were capped at 50, so there's no need to predict more than 50 matches.","55b8e664":"# <center style=\"background-color:yellow\"> Set Defaults <\/center>","1d287396":"- [Competition link](https:\/\/www.kaggle.com\/c\/shopee-product-matching)\n- Objective : Determine if two products are the same by their images\n- Description <br>\n\nDo you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n\nTwo different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n\nShopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n\nIn this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.\n\nThe applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.","e14597f9":"# WIP"}}