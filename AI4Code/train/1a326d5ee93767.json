{"cell_type":{"b7f64a74":"code","04431bdc":"code","2086cca4":"code","06fa8ac7":"code","c79bdf45":"code","6c14dfec":"code","a7de9b24":"code","f26b20a7":"code","2951a285":"code","9d280c74":"code","d2374eca":"code","c31a30dd":"code","b2eb634f":"code","a6ff7985":"code","5505ee85":"code","ca380a9b":"code","5f16f4fe":"code","d9a8c396":"code","56b38e80":"code","f3b0e28e":"code","771c40e9":"code","f5cc4dbc":"code","f590edf7":"code","313268fe":"code","0e90a891":"code","45ac1b95":"code","5bc17451":"code","f47e6371":"code","1ebb2c3b":"code","c2f04fda":"code","8150fd08":"code","99831686":"code","27a798d3":"code","406fd73d":"code","3b5cdf34":"code","b794f73b":"code","a21e0a34":"code","1480a75e":"code","568bc758":"code","09e7eded":"code","57e5ebcf":"code","bb048b8b":"code","1f73f40d":"code","617cc2d1":"code","0633ac2c":"code","7b1ccbfe":"code","40caa9bd":"code","19490a9e":"markdown","0e72c365":"markdown","4e86d534":"markdown","f5a2af5d":"markdown","a8084996":"markdown","28478feb":"markdown","a8ff5392":"markdown","e9967430":"markdown","96dee4e8":"markdown","c80ccc87":"markdown","2913ea04":"markdown","1d6d41c3":"markdown","63bddb49":"markdown","57541bce":"markdown","3824bbe9":"markdown","686d0739":"markdown","d09565d6":"markdown","272ee381":"markdown","b85a4ae4":"markdown","56dd41ab":"markdown","23adf698":"markdown","04661e9f":"markdown","8a81212a":"markdown","33ca2e15":"markdown","89b21f4f":"markdown","9d894f7e":"markdown","0fd19cbf":"markdown","d2ac1c37":"markdown","a496942f":"markdown","a97f6d0b":"markdown","671f785c":"markdown","dccd17c6":"markdown","b75b862a":"markdown"},"source":{"b7f64a74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline   \n# \u7531\u4e8e %matplotlib inline \u9b54\u6cd5\u51fd\u6570\u7684\u5b58\u5728\uff0c\u5f53\u8f93\u5165plt.plot(x,y_1)\u540e\uff0c\u4e0d\u5fc5\u518d\u8f93\u5165 plt.show()\uff0c\u56fe\u50cf\u5c06\u81ea\u52a8\u663e\u793a\u51fa\u6765\n\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n","04431bdc":"train_df = pd.read_csv('\/kaggle\/input\/kaggle competitions download -c titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/kaggle competitions download -c titanic\/test.csv')\ncombine = [train_df, test_df]","2086cca4":"print(train_df.columns.values)","06fa8ac7":"# preview the data\ntrain_df.head()","c79bdf45":"train_df.tail()","6c14dfec":"train_df.info()\nprint('_'*40)\ntest_df.info()","a7de9b24":"train_df.describe()\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n# Review Parch distribution using `percentiles=[.75, .8]`\n# SibSp distribution `[.68, .69]`\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`","f26b20a7":"train_df.describe(include=['O'])","2951a285":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9d280c74":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","d2374eca":"train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c31a30dd":"train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b2eb634f":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","a6ff7985":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","5505ee85":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","ca380a9b":"# grid = sns.FacetGrid(train_df, col='Embarked', hue='Survived', palette={0: 'k', 1: 'w'})\ngrid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","5f16f4fe":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","d9a8c396":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","56b38e80":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","f3b0e28e":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","771c40e9":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","f5cc4dbc":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","f590edf7":"#grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_df, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","313268fe":"guess_ages = np.zeros((2,3))\nguess_ages","0e90a891":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","45ac1b95":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","5bc17451":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","f47e6371":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","1ebb2c3b":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c2f04fda":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","8150fd08":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","99831686":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","27a798d3":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","406fd73d":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","3b5cdf34":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","b794f73b":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","a21e0a34":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","1480a75e":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","568bc758":"test_df.head(10)","09e7eded":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n\nX_train.shape, Y_train.shape, X_test.shape","57e5ebcf":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","bb048b8b":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","1f73f40d":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","617cc2d1":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","0633ac2c":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","7b1ccbfe":"#XGBoost\u6a21\u578b\u9884\u6d4b\u5f97\u5206\nimport xgboost as xgb\n\n#\u6d4b\u8bd5\u96c6\u5360\u8bad\u7ec3\u96c620%\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_train, Y_train, test_size=0.2, random_state=123)\nxg_classifier = xgb.XGBClassifier(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 6, alpha = 10, n_estimators = 10)\n\nxg_classifier.fit(X_train,Y_train)\nxg_classifier.score(X_test,Y_test)\n","40caa9bd":"#\u7ec4\u5408\u5206\u7c7b\u6a21\u578b\u4ea4\u53c9\u68c0\u9a8c\u9884\u6d4b\u7684\u5206\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score  # \u5bfc\u5165\u4ea4\u53c9\u68c0\u9a8c\u7b97\u6cd5\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import AdaBoostClassifier, VotingClassifier, GradientBoostingClassifier, \\\n    RandomForestClassifier,  BaggingClassifier  # \u56db\u79cd\u96c6\u6210\u5206\u7c7b\u5e93\u548c\u6295\u7968\u65b9\u6cd5\u5e93\n \n\nmodel_rf = RandomForestClassifier(max_features=0.8, random_state=0)  # \u968f\u673a\u68ee\u6797\u5206\u7c7b\u6a21\u578b\u5bf9\u8c61\nmodel_adaC = AdaBoostClassifier(random_state=0)  # Adaboost\u5206\u7c7b\u6a21\u578b\u5bf9\u8c61\nmodel_BagC = BaggingClassifier(random_state=0)  # Bagging\u5206\u7c7b\u6a21\u578b\u5bf9\u8c61\nmodel_gdbc = GradientBoostingClassifier(max_features=0.8, random_state=0)  # GradientBoosting\u5206\u7c7b\u6a21\u578b\u5bf9\u8c61\nestimators = [('randomforest', model_rf), ('adaboost', model_adaC),\n              ('bagging', model_BagC), ('gradientboosting', model_gdbc)]  # \u5efa\u7acb\u7ec4\u5408\u8bc4\u4f30\u5668\u5217\u8868\nmodel_vot = VotingClassifier(estimators=estimators, voting='soft', weights=[0.9, 1.2, 1.1, 1.1],\n                             n_jobs=-1)  # \u5efa\u7acb\u7ec4\u5408\u8bc4\u4f30\u6a21\u578b\uff0c\u5bf9\u5e94\u6a21\u578b\u65b9\u6cd5\u7684\u6743\u91cd\u5360\u6bd4\u3002\ncv = StratifiedKFold(5, random_state=2)  # \u8bbe\u7f6e\u4e00\u4e2a\u6298\u4ea4\u53c9\u68c0\u9a8c\u65b9\u6cd5\ncv_score = cross_val_score(model_vot, X_train, Y_train, cv=cv)  # \u4ea4\u53c9\u68c0\u9a8c\nprint('{:*^60}'.format('Cross val scores:'))\nprint(cv_score)  # \u6253\u5370\u6bcf\u6b21\u4ea4\u53c9\u68c0\u9a8c\u5f97\u5206\nprint('Mean scores is: %.2f' % cv_score.mean())  # \u6253\u5370\u5e73\u5747\u4ea4\u53c9\u68c0\u9a8c\u5f97\u5206\nmodel_vot.fit(X_train, Y_train)  # \u6a21\u578b\u8bad\u7ec3","19490a9e":"What is the distribution of categorical features?\n\nNames are unique across the dataset (count=unique=891)\nSex variable as two possible values with 65% male (top=male, freq=577\/count=891).\nCabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\nEmbarked takes three possible values. S port used by most passengers (top=S)\nTicket feature has high ratio (22%) of duplicate values (unique=681).","0e72c365":"\u8ba9\u6211\u4eec\u4ece\u51c6\u5907\u4e00\u4e2a\u7a7a\u6570\u7ec4\u5f00\u59cb\uff0c\u5b83\u5305\u542b\u57fa\u4e8epclass x\u6027\u522b\u7ec4\u5408\u7684\u63a8\u6d4b\u5e74\u9f84\u503c\u3002","4e86d534":"**Model, predict and solve**\n\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\n\nKNN or k-Nearest Neighbors\n\nSupport Vector Machines\n\nNaive Bayes classifier\n\nDecision Tree\n\nRandom Forrest\n\nPerceptron\n\nArtificial neural network\n\nRVM or Relevance Vector Machine\n","f5a2af5d":"\u521b\u5efa\u4ece\u73b0\u6709\u7279\u5f81\u63d0\u53d6\u7684\u65b0\u7279\u5f81\n\n\u5728\u5220\u9664\u540d\u5b57\u548cpassengerid\u7279\u5f81\u4e4b\u524d\uff0c\u6211\u4eec\u60f3\u5206\u6790\u540d\u5b57\u7279\u5f81\u662f\u5426\u53ef\u4ee5\u88ab\u8bbe\u8ba1\u6765\u63d0\u53d6\u6807\u9898\u5e76\u6d4b\u8bd5\u6807\u9898\u548c\u751f\u5b58\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\n\n\n\n\u5728\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u63d0\u53d6\u6807\u9898\u7279\u5f81\u3002regex\u6a21\u5f0f\uff08\\w+\\\uff09\u4e0e\u540d\u79f0\u529f\u80fd\u4e2d\u4ee5\u70b9\u5b57\u7b26\u7ed3\u5c3e\u7684\u7b2c\u4e00\u4e2a\u5355\u8bcd\u5339\u914d\u3002expand=false\u6807\u5fd7\u8fd4\u56de\u4e00\u4e2a\u6570\u636e\u5e27\u3002\n\n\n\n\u89c2\u5bdf\n\n\n\n\u5f53\u6211\u4eec\u7ed8\u5236\u6807\u9898\u3001\u5e74\u9f84\u548c\u5b58\u6d3b\u7387\u65f6\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u4ee5\u4e0b\u89c2\u5bdf\u7ed3\u679c\u3002\n\n\n\n\u5927\u591a\u6570\u5934\u8854\u51c6\u786e\u5730\u5212\u5206\u5e74\u9f84\u7ec4\u3002\u4f8b\u5982\uff1a\u7855\u58eb\u5934\u8854\u7684\u5e74\u9f84\u5e73\u5747\u4e3a5\u5c81\u3002\n\n\u5934\u8854\u5e74\u9f84\u6bb5\u7684\u5b58\u6d3b\u7387\u7565\u6709\u4e0d\u540c\u3002\n\n\u67d0\u4e9b\u5934\u8854\u5927\u591a\u5e78\u5b58\u4e0b\u6765\uff08\u592b\u4eba\uff0c\u5148\u751f\uff09\uff0c\u6216\u8005\u6ca1\u6709\uff08\u5510\uff0c\u96f7\u592b\uff0c\u5bb9\u514b\u5e0c\u5c14\uff09\u3002\n\n\u7ed3\u8bba\n\n\n\n\u6211\u4eec\u51b3\u5b9a\u4fdd\u7559\u6a21\u578b\u57f9\u8bad\u7684\u65b0\u6807\u9898\u529f\u80fd\u3002","a8084996":"\u54ea\u4e9b\u529f\u80fd\u662f\u6df7\u5408\u6570\u636e\u7c7b\u578b\uff1f\n\n\u540c\u4e00\u529f\u80fd\u5185\u7684\u6570\u5b57\u3001\u5b57\u6bcd\u6570\u5b57\u6570\u636e\u3002\u8fd9\u4e9b\u662f\u4fee\u6b63\u76ee\u6807\u7684\u5019\u9009\u4eba\u3002\n\n\u7968\u636e\u662f\u6570\u5b57\u548c\u5b57\u6bcd\u6570\u5b57\u6570\u636e\u7c7b\u578b\u7684\u6df7\u5408\u3002\u5ea7\u8231\u662f\u5b57\u6bcd\u6570\u5b57\u3002\n\n\n\u54ea\u4e9b\u529f\u80fd\u53ef\u80fd\u5305\u542b\u9519\u8bef\u6216\u6253\u5b57\u9519\u8bef\uff1f\n\n\u5bf9\u4e8e\u5927\u578b\u6570\u636e\u96c6\uff0c\u8fd9\u5f88\u96be\u5ba1\u67e5\uff0c\u4f46\u662f\u4ece\u8f83\u5c0f\u7684\u6570\u636e\u96c6\u5ba1\u67e5\u51e0\u4e2a\u6837\u672c\u53ef\u80fd\u4f1a\u76f4\u63a5\u544a\u8bc9\u6211\u4eec\u54ea\u4e9b\u7279\u6027\u53ef\u80fd\u9700\u8981\u4fee\u6539\u3002\n\n\u540d\u79f0\u529f\u80fd\u53ef\u80fd\u5305\u542b\u9519\u8bef\u6216\u62fc\u5199\u9519\u8bef\uff0c\u56e0\u4e3a\u6709\u51e0\u79cd\u65b9\u6cd5\u53ef\u4ee5\u7528\u6765\u63cf\u8ff0\u540d\u79f0\uff0c\u5305\u62ec\u6807\u9898\u3001\u5706\u62ec\u53f7\u548c\u7528\u4e8e\u66ff\u4ee3\u6216\u77ed\u540d\u79f0\u7684\u5f15\u53f7\u3002","28478feb":"**Create new feature combining existing features**\n\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","a8ff5392":"\u8f6c\u6362\u5206\u7c7b\u7279\u5f81\n\n\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5c06\u5305\u542b\u5b57\u7b26\u4e32\u7684\u7279\u6027\u8f6c\u6362\u4e3a\u6570\u503c\u3002\u8fd9\u662f\u5927\u591a\u6570\u6a21\u578b\u7b97\u6cd5\u6240\u8981\u6c42\u7684\u3002\u8fd9\u6837\u505a\u4e5f\u5c06\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u529f\u80fd\u5b8c\u6210\u76ee\u6807\u3002\n\n\n\n\u9996\u5148\uff0c\u6211\u4eec\u5c06\u6027\u7279\u5f81\u8f6c\u6362\u4e3a\u4e00\u4e2a\u65b0\u7684\u7279\u5f81\uff0c\u5373\u6027\u522b\uff0c\u5176\u4e2d\u5973\u6027=1\uff0c\u7537\u6027=0\u3002","e9967430":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\nSex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\nInversely as Pclass increases, probability of Survived=1 decreases the most.\nThis way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\nSo is Title as second highest positive correlation.","96dee4e8":"And the test dataset.","c80ccc87":"\u6570\u5b57\u7279\u5f81\u503c\u5728\u6837\u672c\u4e2d\u7684\u5206\u5e03\u662f\u4ec0\u4e48\uff1f\n\n\n\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u5728\u65e9\u671f\u7684\u6d1e\u5bdf\u4e2d\u786e\u5b9a\u5b9e\u9645\u95ee\u9898\u9886\u57df\u7684\u57f9\u8bad\u6570\u636e\u96c6\u7684\u4ee3\u8868\u6027\u3002\n\n\u6837\u672c\u603b\u6570\u4e3a891\u4e2a\uff0c\u5360\u6cf0\u5766\u5c3c\u514b\u53f7\uff082224\uff09\u5b9e\u9645\u8f7d\u5ba2\u91cf\u768440%\u3002\n\n\u751f\u5b58\u662f\u4e00\u4e2a\u5177\u67090\u62161\u503c\u7684\u5206\u7c7b\u529f\u80fd\u3002\n\n\u7ea638%\u7684\u6837\u672c\u5b58\u6d3b\u7387\u4ee3\u8868\u4e8632%\u7684\u5b9e\u9645\u5b58\u6d3b\u7387\u3002\n\n\u5927\u591a\u6570\u4e58\u5ba2\uff08>75%\uff09\u6ca1\u6709\u548c\u7236\u6bcd\u6216\u5b69\u5b50\u4e00\u8d77\u65c5\u884c\u3002\n\n\u8fd130%\u7684\u4e58\u5ba2\u6709\u5144\u5f1f\u59d0\u59b9\u548c\/\u6216\u914d\u5076\u3002\n\n\u7968\u4ef7\u5dee\u5f02\u5f88\u5927\uff0c\u5f88\u5c11\u6709\u4e58\u5ba2\uff08<1%\uff09\u652f\u4ed8\u9ad8\u8fbe512\u7f8e\u5143\u3002\n\n\u5e74\u9f84\u572865-80\u5c81\u4e4b\u95f4\u7684\u8001\u5e74\u65c5\u5ba2\uff08<1%\uff09\u5f88\u5c11\u3002","2913ea04":"Logistic Regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference Wikipedia.\n\nNote the confidence score generated by the model based on our training dataset.","1d6d41c3":"We can also create an artificial feature combining Pclass and Age.","63bddb49":"We can not create FareBand.","57541bce":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference Wikipedia.\n\nKNN confidence score is better than Logistics Regression but worse than SVM.","3824bbe9":"Convert the Fare feature to ordinal values based on the FareBand.","686d0739":"\u8ba9\u6211\u4eec\u521b\u5efa\u5e74\u9f84\u6bb5\u5e76\u786e\u5b9a\u4e0e\u5b58\u6d3b\u7387\u7684\u76f8\u5173\u6027\u3002","d09565d6":"Assumtions based on data analysis\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions further before taking appropriate actions.\n\nCorrelating.\nWe want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\nCompleting.\nWe may want to complete Age feature as it is definitely correlated to survival.\nWe may want to complete the Embarked feature as it may also correlate with survival or another important feature.\n\nCorrecting.\nTicket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\nCabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\nPassengerId may be dropped from training dataset as it does not contribute to survival.\nName feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n\nCreating.\nWe may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\nWe may want to engineer the Name feature to extract Title as a new feature.\nWe may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\nWe may also want to create a Fare range feature if it helps our analysis.\n\nClassifying.\nWe may also add to our assumptions based on the problem description noted earlier.\nWomen (Sex=female) were more likely to have survived.\nChildren (Age<?) were more likely to have survived.\nThe upper-class passengers (Pclass=1) were more likely to have survived.\nAnalyze by pivoting features\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.\n\nPclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.\nSex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).\nSibSp and Parch These features have zero correlation for certain values. It may be best to derive a feature or a set of features from these individual features (creating #1).\n","272ee381":"\u8ba9\u6211\u4eec\u7528\u57fa\u4e8e\u8fd9\u4e9b\u6ce2\u6bb5\u7684\u5e8f\u6570\u6765\u4ee3\u66ff\u5e74\u9f84\u3002","b85a4ae4":"**Completing a categorical feature**\n\n\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","56dd41ab":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference Wikipedia.","23adf698":"\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u5b89\u5168\u5730\u4ece\u57f9\u8bad\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u5220\u9664\u540d\u79f0\u529f\u80fd\u3002\u6211\u4eec\u4e5f\u4e0d\u9700\u8981\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684passengerid\u529f\u80fd\u3002","04661e9f":"\u54ea\u4e9b\u529f\u80fd\u5305\u542b\u7a7a\u503c\u3001\u7a7a\u503c\u6216\u7a7a\u503c\uff1f\n\u8fd9\u4e9b\u9700\u8981\u7ea0\u6b63\u3002\n\u5ea7\u8231>\u5e74\u9f84>\u767b\u8239\u529f\u80fd\u5305\u542b\u8bad\u7ec3\u6570\u636e\u96c6\u987a\u5e8f\u4e2d\u7684\u82e5\u5e72\u7a7a\u503c\u3002\n\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u5ea7\u8231>\u5e74\u9f84\u4e0d\u5b8c\u6574\u3002\n\n\u5404\u79cd\u529f\u80fd\u7684\u6570\u636e\u7c7b\u578b\u662f\u4ec0\u4e48\uff1f\n\u5e2e\u52a9\u6211\u4eec\u5b9e\u73b0\u76ee\u6807\u3002\n\u4e03\u4e2a\u7279\u6027\u662f\u6574\u6570\u6216\u6d6e\u70b9\u6570\u30026\u4e2a\u7528\u4e8e\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\n\u4e94\u4e2a\u7279\u6027\u662f\u5b57\u7b26\u4e32\uff08\u5bf9\u8c61\uff09\u3002","8a81212a":"\u54ea\u4e9b\u7279\u5f81\u662f\u5206\u7c7b\u7684\uff1f\n\u8fd9\u4e9b\u503c\u5c06\u6837\u672c\u5206\u7c7b\u4e3a\u4e00\u7ec4\u76f8\u4f3c\u7684\u6837\u672c\u3002\u5728\u5206\u7c7b\u7279\u5f81\u4e2d\uff0c\u8fd9\u4e9b\u503c\u662f\u57fa\u4e8e\u540d\u4e49\u503c\u3001\u5e8f\u6570\u503c\u3001\u6bd4\u7387\u503c\u8fd8\u662f\u57fa\u4e8e\u533a\u95f4\u503c\uff1f\u9664\u6b64\u4e4b\u5916\uff0c\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u9009\u62e9\u9002\u5f53\u7684\u56fe\u5f62\u8fdb\u884c\u53ef\u89c6\u5316\u3002\n\u5206\u7c7b\uff1a\u751f\u5b58\uff0c\u6027\uff0c\u548c\u51fa\u53d1\u3002\u5e8f\u53f7\uff1aP\u7c7b\u3002\n\n\n\u54ea\u4e9b\u7279\u5f81\u662f\u6570\u5b57\u7684\uff1f\n\u54ea\u4e9b\u7279\u5f81\u662f\u6570\u5b57\u7684\uff1f\u8fd9\u4e9b\u503c\u968f\u6837\u672c\u7684\u53d8\u5316\u800c\u53d8\u5316\u3002\u5728\u6570\u5b57\u7279\u5f81\u4e2d\uff0c\u503c\u662f\u79bb\u6563\u7684\u3001\u8fde\u7eed\u7684\u8fd8\u662f\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\uff1f\u9664\u6b64\u4e4b\u5916\uff0c\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u9009\u62e9\u9002\u5f53\u7684\u56fe\u5f62\u8fdb\u884c\u53ef\u89c6\u5316\u3002\n\u7ee7\u7eed\uff1a\u5e74\u9f84\uff0c\u8f66\u8d39\u3002\u79bb\u6563\uff1asibsp\uff0cparch\u3002","33ca2e15":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference Wikipedia.\n\nThe model generated confidence score is the lowest among the models evaluated so far.","89b21f4f":"\u5b8c\u6210\u6570\u5b57\u8fde\u7eed\u7279\u5f81\n\n\u73b0\u5728\uff0c\u6211\u4eec\u5e94\u8be5\u5f00\u59cb\u4f30\u8ba1\u548c\u5b8c\u6210\u7f3a\u5c11\u6216\u7a7a\u503c\u7684\u7279\u6027\u3002\u6211\u4eec\u5c06\u9996\u5148\u4e3a\u201c\u5e74\u9f84\u201d\u529f\u80fd\u6267\u884c\u6b64\u64cd\u4f5c\u3002\n\n\n\n\u6211\u4eec\u53ef\u4ee5\u8003\u8651\u4e09\u79cd\u65b9\u6cd5\u6765\u5b8c\u6210\u4e00\u4e2a\u6570\u503c\u8fde\u7eed\u7279\u5f81\u3002\n\n\n\n\u4e00\u4e2a\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u5728\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u4e4b\u95f4\u751f\u6210\u968f\u673a\u6570\u3002\n\n\n\n\u66f4\u51c6\u786e\u7684\u731c\u6d4b\u7f3a\u5931\u503c\u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u5176\u4ed6\u76f8\u5173\u7279\u6027\u3002\u5728\u6211\u4eec\u7684\u6848\u4f8b\u4e2d\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u5e74\u9f84\u3001\u6027\u522b\u548c\u7c7b\u522b\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u4f7f\u7528\u4e0d\u540c\u7c7b\u522b\u548c\u6027\u522b\u7279\u5f81\u7ec4\u5408\u7684\u5e74\u9f84\u4e2d\u503c\u6765\u731c\u6d4b\u5e74\u9f84\u503c\u3002\u6240\u4ee5\uff0cpclass=1\uff0cgender=0\uff0cpclass=1\uff0cgender=1\u7684\u4e2d\u4f4d\u5e74\u9f84\uff0c\u7b49\u7b49\u2026\n\n\n\n\u7ed3\u5408\u65b9\u6cd51\u548c2\u3002\u56e0\u6b64\uff0c\u4e0d\u8981\u6839\u636e\u4e2d\u4f4d\u6570\u6765\u731c\u6d4b\u5e74\u9f84\u503c\uff0c\u800c\u662f\u4f7f\u7528\u5e73\u5747\u503c\u548c\u6807\u51c6\u5dee\u4e4b\u95f4\u7684\u968f\u673a\u6570\uff0c\u8fd9\u662f\u57fa\u4e8e\u7c7b\u522b\u96c6\u548c\u6027\u522b\u7ec4\u5408\u7684\u3002\n\n\n\n\u65b9\u6cd51\u548c3\u5c06\u968f\u673a\u566a\u58f0\u5f15\u5165\u6211\u4eec\u7684\u6a21\u578b\u3002\u591a\u6b21\u6267\u884c\u7684\u7ed3\u679c\u53ef\u80fd\u4f1a\u6709\u6240\u4e0d\u540c\u3002\u6211\u4eec\u66f4\u559c\u6b22\u65b9\u6cd52\u3002","9d894f7e":"**Converting categorical feature to numeric   #\u5c06\u5206\u7c7b\u7279\u5f81\u8f6c\u6362\u4e3a\u6570\u5b57**\n\n\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.","0fd19cbf":"\u6211\u4eec\u53ef\u4ee5\u628a\u5206\u7c7b\u6807\u9898\u8f6c\u6362\u6210\u5e8f\u6570\u3002","d2ac1c37":"**Quick completing and converting a numeric feature**  #\u5feb\u901f\u5b8c\u6210\u548c\u8f6c\u6362\u6570\u5b57\u529f\u80fd\n\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.","a496942f":"We can create another feature called IsAlone.","a97f6d0b":"**\u901a\u8fc7\u53ef\u89c6\u5316\u6570\u636e\u8fdb\u884c\u5206\u6790**\n\n\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u7ee7\u7eed\u4f7f\u7528\u53ef\u89c6\u5316\u5206\u6790\u6570\u636e\u6765\u786e\u8ba4\u6211\u4eec\u7684\u4e00\u4e9b\u5047\u8bbe\u3002\n\n**\u76f8\u5173\u6570\u503c\u7279\u5f81**\n\n\u8ba9\u6211\u4eec\u4ece\u7406\u89e3\u6570\u503c\u7279\u5f81\u548c\u6211\u4eec\u7684\u89e3\u51b3\u65b9\u6848\u76ee\u6807\uff08\u5b58\u6d3b\uff09\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f00\u59cb\u3002\n\u67f1\u72b6\u56fe\u6709\u52a9\u4e8e\u5206\u6790\u8fde\u7eed\u7684\u6570\u503c\u53d8\u91cf\uff0c\u5982\u5e74\u9f84\uff0c\u5728\u5e74\u9f84\u5e26\u6216\u8303\u56f4\u5c06\u6709\u52a9\u4e8e\u8bc6\u522b\u6709\u7528\u7684\u6a21\u5f0f\u3002\u67f1\u72b6\u56fe\u53ef\u4ee5\u4f7f\u7528\u81ea\u52a8\u5b9a\u4e49\u7684\u7bb1\u6216\u7b49\u8303\u56f4\u5e26\u6307\u793a\u6837\u672c\u7684\u5206\u5e03\u3002\u8fd9\u6709\u52a9\u4e8e\u6211\u4eec\u56de\u7b54\u4e0e\u7279\u5b9a\u6761\u5e26\u76f8\u5173\u7684\u95ee\u9898\uff08\u5a74\u513f\u662f\u5426\u6709\u66f4\u597d\u7684\u5b58\u6d3b\u7387\uff1f\uff09\n\u8bf7\u6ce8\u610f\uff0c\u5386\u53f2\u56fe\u50cf\u4e2d\u7684X\u8f74\u8868\u793a\u6837\u672c\u6216\u4e58\u5ba2\u7684\u8ba1\u6570\u3002\n\n\n\n**\u89c2\u5bdf**\n\n\u5a74\u513f\uff08\u5e74\u9f84<=4\uff09\u5b58\u6d3b\u7387\u9ad8\u3002\n\n\u5e74\u9f84\u6700\u5927\u7684\u4e58\u5ba2\uff0880\u5c81\uff09\u5e78\u5b58\u4e0b\u6765\u3002\n\n\u5927\u91cf15-25\u5c81\u7684\u513f\u7ae5\u6ca1\u6709\u5b58\u6d3b\u4e0b\u6765\u3002\n\n\u5927\u591a\u6570\u4e58\u5ba2\u5e74\u9f84\u572815-35\u5c81\u4e4b\u95f4\u3002\n\n\n**\u7ed3\u8bba**\n\n\u8fd9\u4e2a\u7b80\u5355\u7684\u5206\u6790\u786e\u8ba4\u4e86\u6211\u4eec\u7684\u5047\u8bbe\uff0c\u4f5c\u4e3a\u540e\u7eed\u5de5\u4f5c\u6d41\u9636\u6bb5\u7684\u51b3\u7b56\u3002\n\u6211\u4eec\u5e94\u8be5\u5728\u6a21\u578b\u57f9\u8bad\u4e2d\u8003\u8651\u5e74\u9f84\uff08\u6211\u4eec\u7684\u5047\u8bbe\u5206\u7c7b2\uff09\u3002\n\u5b8c\u6210\u7a7a\u503c\u7684\u671f\u9650\u529f\u80fd\uff08\u5b8c\u62101\uff09\u3002\n\u6211\u4eec\u5e94\u8be5\u5bf9\u5e74\u9f84\u7ec4\u8fdb\u884c\u5206\u7ec4\uff08\u521b\u5efa3\uff09\u3002","671f785c":"\u73b0\u5728\u6211\u4eec\u8fed\u4ee3sex\uff080\u62161\uff09\u548cpclass\uff081\uff0c2\uff0c3\uff09\u6765\u8ba1\u7b97\u516d\u4e2a\u7ec4\u5408\u7684\u5e74\u9f84\u4f30\u8ba1\u503c\u3002","dccd17c6":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.","b75b862a":"Next we model using Support Vector Machines(SVM) which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference Wikipedia.\n\nNote that the model generates a confidence score which is higher than Logistics Regression mod"}}