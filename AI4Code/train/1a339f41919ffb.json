{"cell_type":{"b80c385d":"code","91f44f2a":"code","836ecbae":"code","9ed4b87d":"code","844a25c4":"code","b52c064b":"code","23e746f5":"code","6b8cfe4c":"code","64681485":"code","b3dbae91":"code","70c36df2":"code","ebc92a05":"code","f2affde7":"code","3490958a":"code","4c21beee":"code","c77b4fb9":"code","62a23436":"code","8ea188d5":"code","3f597602":"code","a4f76333":"code","2dec4218":"code","ccbe6265":"code","7db3caf2":"code","e7749691":"code","b9ea580b":"code","901bbcf9":"code","6c9c6cdc":"code","2bc4e5bf":"code","57c79bce":"code","b49bda8c":"code","4bdbb42e":"code","9085d7fc":"code","9dcb5786":"code","6b5cd640":"code","42ad8486":"code","2a8c2a03":"code","ff3dd372":"code","cbbe75b6":"code","715b7407":"code","1ff81804":"code","2e287f44":"code","c33e990c":"code","42aacb62":"code","1d988445":"code","73a37e45":"code","a13422c9":"code","6a298c82":"code","322afe40":"code","bd92ce8c":"code","5f99b727":"code","878fa1f9":"code","d94e5cf9":"code","1898f460":"code","48d9146d":"code","6ea44540":"code","1664b056":"code","cdbc5713":"code","c0c63e3c":"code","a548316c":"code","e1feb92f":"code","00b6fe58":"code","ba57c472":"code","e0964a3a":"code","a0712211":"code","ae8d1397":"markdown","12e600b0":"markdown","c62af417":"markdown","abef5553":"markdown","c28f8e85":"markdown","46fafc87":"markdown","73eaa79c":"markdown","acb47162":"markdown","681a7652":"markdown","e32e02a1":"markdown","914258c9":"markdown","14df7c41":"markdown","b21418b8":"markdown","4509b81a":"markdown","57f9a613":"markdown","886ac62e":"markdown","a7d45e6e":"markdown","84c1eed8":"markdown","80140c13":"markdown","cc1f6392":"markdown","a16dadbe":"markdown","7e98810d":"markdown"},"source":{"b80c385d":"!pip install -Iv pulp==1.6.8 --quiet","91f44f2a":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.metrics import mean_squared_error, make_scorer, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n\nimport time\n\nimport scipy.stats as stats\n\nimport pulp\n\nimport unidecode","836ecbae":"## Read data\n\n# Players df contains the summary of player performance from each season\n# Each row represents one player\n\n## GWs df contains the information of each player for each gameweek\n## Each row represents a player's performance for a gameweek\n\nfolderpath = '..\/input\/fantasypremierleague\/'\n\nplayers_1617_df = pd.read_csv(folderpath+'players_raw_1617.csv')\nplayers_1718_df = pd.read_csv(folderpath+'players_raw_1718.csv')\nplayers_1819_df = pd.read_csv(folderpath+'players_raw_1819.csv')\nplayers_1920_df = pd.read_csv(folderpath+'players_raw_1920.csv')\n\ngws_1617_df = pd.read_csv(folderpath+'merged_gws_1617.csv')\ngws_1718_df = pd.read_csv(folderpath+'merged_gws_1718.csv',encoding='latin')\ngws_1819_df = pd.read_csv(folderpath+'merged_gw_1819.csv',encoding='latin')\ngws_1920_df = pd.read_csv(folderpath+'merged_gw_1920.csv',engine='python')\n\nteam_codes_df = pd.read_csv(folderpath+'teams.csv')\n","9ed4b87d":"# Clean the headers to be used later\nteam_codes_df.columns.values[2:] = team_codes_df.columns[2:].str.replace('team_', '')","844a25c4":"# remove Danny Wards from 18\/19 season\nplayers_1819_df = players_1819_df[((players_1819_df.first_name == \"Danny\") & (players_1819_df.second_name==\"Ward\"))==False]\ngws_1819_df = gws_1819_df[gws_1819_df.name.str.contains(\"Danny_Ward\")==False]","b52c064b":"player_df_list = [players_1617_df, players_1718_df, players_1819_df, players_1920_df]\ngw_df_list = [gws_1617_df, gws_1718_df, gws_1819_df, gws_1920_df]","23e746f5":"# append season and season index to dfs\n\nseasons = ['1617', '1718', '1819', '1920']\nseason_nums = list(range(len(seasons)))\n\nfor i in range(len(seasons)):\n    \n    player_df_list[i]['season'] = seasons[i]\n    gw_df_list[i]['season'] = seasons[i]\n    \n    player_df_list[i]['season_num'] = season_nums[i]\n    gw_df_list[i]['season_num'] = season_nums[i]","6b8cfe4c":"# combine dataframes from all seasons into one\n\nplayers_df = pd.concat(player_df_list)\ngws_df = pd.concat(gw_df_list)\n\nplayers_df.reset_index(inplace=True)\ngws_df.reset_index(inplace=True)","64681485":"## Get full name\n# Cleans up accents and also makes processing easier\n\ndef get_full_name_playerdf(first_name, second_name):\n    full_name = first_name +'_' + second_name\n    full_name = full_name.replace(\" \", \"_\")\n    full_name = full_name.replace(\"-\", \"_\")\n    full_name = unidecode.unidecode(full_name)\n    \n    return full_name\n\n","b3dbae91":"# Translate player positions into string for easier readability\n\npositions_dict = {\n    1: 'Keeper',\n    2: 'Defender',\n    3: 'Midfielder',\n    4: 'Forward'\n    \n}\n\n\nplayers_df['full_name'] = players_df.apply(lambda x: get_full_name_playerdf(x.first_name, x.second_name), axis=1).str.lower()\nplayers_df['position'] = players_df.element_type.map(positions_dict)\nplayers_df['starting_cost'] = players_df.now_cost - players_df.cost_change_start_fall\nplayers_df['cost_bin'] = players_df.now_cost.apply(lambda x: np.floor(x\/10))\n\n\n\ngws_df['full_name'] = gws_df.name.str.replace('_\\d+','')\ngws_df['full_name'] = gws_df['full_name'].str.replace(\" \", \"_\").str.replace(\"-\", \"_\").str.replace('_\\d+','')\ngws_df['full_name'] = gws_df['full_name'].apply(lambda x: unidecode.unidecode(x))\ngws_df['full_name'] = gws_df['full_name'].str.lower()\n","70c36df2":"    \ndef clean_gw_df(player_df, gw_df, team_codes_df):\n    \n    # Returns a df with player position, player's team name, and opponent's team name\n    \n    pdf = player_df.copy()[['full_name', 'season', 'position', 'player_team_name']]\n    gdf = gw_df.copy()\n    \n    gdf = gdf.merge(pdf, on=['full_name', 'season'], how='left')\n    \n    \n    dfs = []\n    for s, group in gdf.groupby('season'):\n\n        temp_code_df = team_codes_df[['team', s]]\n        temp_code_df = temp_code_df.dropna()\n        \n        group = group[['opponent_team']]\n        group['opponent_team_name'] = group.opponent_team.map(temp_code_df.set_index(s).team)\n        dfs.append(group[['opponent_team_name']])\n        \n    out_df = pd.concat(dfs, axis=0)\n    out_df = pd.concat([gdf, out_df], axis=1)\n\n    return out_df\n\n","ebc92a05":"gws_df.opponent_team = gws_df.opponent_team.astype(float)\nplayers_df['player_team_name'] = players_df.team_code.map(team_codes_df.set_index('team_code').team)\ngws_df = clean_gw_df(players_df, gws_df, team_codes_df)","f2affde7":"def make_available_players_df(this_season_player_df, last_season_player_df):\n    \n    \n    last_season_player_df = last_season_player_df[last_season_player_df.minutes > 0]\n    last_season_player_df = last_season_player_df[['full_name', \"total_points\"]]\n    last_season_player_df.rename(columns={'total_points': \"total_points_last_season\"},\n                                inplace=True)\n    \n    available_players_df = pd.merge(this_season_player_df,\n                                    last_season_player_df,\n                                   on='full_name', how='left')\n    \n    available_players_df.total_points_last_season = available_players_df.groupby(['position', 'cost_bin']).total_points_last_season.transform(lambda x: x.fillna(x.mean()))\n    \n    return available_players_df","3490958a":"current_season_player_df = players_df[players_df.season=='1920'] \nprevious_season_player_df = players_df[players_df.season=='1819'] \n\navailable_players_df = make_available_players_df(current_season_player_df, previous_season_player_df)","4c21beee":"def get_cheapest_players(player_df):\n    \n    cheapest_player_names = []\n    total_cost = 0\n    \n    # for each position, sort the players by cost (in ascending order)\n    # then, get the player with the most number of points\n    \n    for position, group in player_df.groupby('position'):\n        cheapest_players =  group[(group.starting_cost == group.starting_cost.min())]\n        top_cheapest_player = cheapest_players[cheapest_players.total_points == cheapest_players.total_points.max()]\n        \n        cheapest_player_name = top_cheapest_player.full_name.values[0]\n        \n        cheapest_player_names += [cheapest_player_name]\n        total_cost += top_cheapest_player.starting_cost.values[0]\n        \n        print(position, \": \", cheapest_player_name )\n        \n    return cheapest_player_names, total_cost","c77b4fb9":"bench_players, bench_cost = get_cheapest_players(available_players_df)","62a23436":"def make_decision_variables(player_df):\n    return [pulp.LpVariable(i, cat=\"Binary\") for i in player_df.full_name]","8ea188d5":"def make_optimization_function(player_df, decision_variables):\n    op_func = \"\"\n\n    for i, player in enumerate(decision_variables):\n        op_func += player_df.total_points_last_season[i]*player\n        \n    return op_func","3f597602":"def make_cash_constraint(player_df, decision_variables, available_cash):\n    total_paid = \"\"\n    for rownum, row in player_df.iterrows():\n        for i, player in enumerate(decision_variables):\n            if rownum == i:\n                formula = row['starting_cost']*player\n                total_paid += formula\n\n    return (total_paid <= available_cash)","a4f76333":"def make_player_constraint(position, n, decision_variables, player_df):\n    \n    total_n = \"\"\n    \n    player_positions = player_df.position\n    \n    for i, player in enumerate(decision_variables):\n        if player_positions[i] == position:\n            total_n += 1*player\n            \n    return(total_n == n)","2dec4218":"def add_team_constraint(prob, player_df, decision_variables):\n\n    for team, group in player_df.groupby('team_code'):\n        team_total = ''\n        \n        for player in decision_variables:\n            if player.name in group.full_name.values:\n                formula = 1*player\n                team_total += formula\n                \n        \n        prob += (team_total <= 3)","ccbe6265":"available_cash = 1000 - bench_cost\n\nprob = pulp.LpProblem('InitialTeam', pulp.LpMaximize)\n\ndecision_variables = make_decision_variables(available_players_df)\nprob += make_optimization_function(available_players_df, decision_variables)\nprob += make_cash_constraint(available_players_df, decision_variables, available_cash)\nprob += make_player_constraint(\"Keeper\", 1, decision_variables, available_players_df) \nprob += make_player_constraint(\"Defender\", 4, decision_variables, available_players_df) \nprob += make_player_constraint(\"Midfielder\", 4, decision_variables, available_players_df) \nprob += make_player_constraint(\"Forward\", 2, decision_variables, available_players_df)\n\nadd_team_constraint(prob, available_players_df, decision_variables)","7db3caf2":"## Solve\n\nprob.writeLP('InitialTeam.lp')\noptimization_result = prob.solve()\n","e7749691":"## Get initial team\n\ndef get_initial_team(prob, player_df):\n    \n    variable_names = [v.name for v in prob.variables()]\n    variable_values = [v.varValue for v in prob.variables()]\n\n    initial_team = pd.merge(pd.DataFrame({'full_name': variable_names,\n                  'selected': variable_values}),\n                                       player_df, on=\"full_name\")\n    \n    initial_team = initial_team[initial_team.selected==1.0] \n    \n    return initial_team\n\n    ","b9ea580b":"initial_team_df = get_initial_team(prob, available_players_df)\ninitial_team_df[['full_name', \"position\", \"starting_cost\", \"player_team_name\"]]","901bbcf9":"## Sanity check\n\ndef sanity_check(team_df):\n    print('Sanity check for starting 11: ')\n    print('*'*88) \n    \n    print('Number of players in each position: ')\n    for pos, group in team_df.groupby('position'):\n        print(pos, ': ', len(group), sep='')\n        \n    \n    print('*'*88)   \n    print('Number of players from each team: ')\n    print(team_df.groupby('player_team_name').position.count())\n    \n    print('*'*88)    \n    print('Total cost:', team_df.starting_cost.sum())\n    \n","6c9c6cdc":"sanity_check(initial_team_df)","2bc4e5bf":"captain = get_initial_team(prob, previous_season_player_df).sort_values(\"total_points\", ascending=False).head(1).full_name.values[0]","57c79bce":"captain = get_initial_team(prob, previous_season_player_df).sort_values(\"total_points\", ascending=False).head(1).full_name.values[0]\n\ntotal_points = current_season_player_df[current_season_player_df.full_name.isin(initial_team_df.full_name)].total_points.sum()\ntotal_points += current_season_player_df[current_season_player_df.full_name==captain].total_points\n\nprint(\"Total points for 19\/20 season:\", total_points.values[0])","b49bda8c":"def get_team_points(was_home, h_score, a_score):\n    \n    if h_score == a_score:\n        return 1\n    \n    if h_score > a_score:\n        if was_home:\n            return 3\n        else: \n            return 0\n    \n    if h_score < a_score:\n        if was_home:\n            return 0\n        else: \n            return 3","4bdbb42e":"def get_opponent_points(team_points):\n    if team_points == 1:\n        return 1\n    \n    if team_points == 3:\n        return 0\n    \n    if team_points == 0:\n        return 3","9085d7fc":"gws_df['team_points']= gws_df.apply(lambda x: get_team_points(x.was_home, x.team_h_score, x.team_a_score), axis=1)\ngws_df['opponent_points'] = gws_df.team_points.apply(lambda x: get_opponent_points(x))","9dcb5786":"def player_lag_features(gw_df, features, lags):\n    \n    out_df = gw_df.copy()\n    lagged_features = []\n    \n    for feature in features:\n            \n        for lag in lags:\n            \n            lagged_feature = 'last_' + str(lag) + '_' + feature\n            \n            if lag == 'all':\n                out_df[lagged_feature] = out_df.sort_values('round').groupby(['season', 'full_name'])[feature]\\\n            .apply(lambda x: x.cumsum() - x)\n                \n            else:\n\n                out_df[lagged_feature] = out_df.sort_values('round').groupby(['season', 'full_name'])[feature]\\\n                .apply(lambda x: x.rolling(min_periods=1, window=lag+1).sum() - x)\n\n            lagged_features.append(lagged_feature)\n    \n    return out_df, lagged_features","6b5cd640":"def team_lag_features(gw_df, features, lags):\n    out_df = gw_df.copy()\n    lagged_features = []\n    \n    for feature in features:\n\n        ## Create a df for each feature\n        ## Then, self-join so that the opponent info for that feature is included\n        ## Then, create lagged features and join the columns to the feature df\n        ## Do the same for the opponent feature\n        ## Exit loop, merge with the original df\n        \n        feature_name = feature + '_team'\n        opponent_feature_name = feature_name + '_opponent'\n        \n  \n        feature_team = out_df.groupby(['player_team_name', 'season', 'round', 'kickoff_time', 'opponent_team_name'])\\\n                        [feature].max().rename(feature_name).reset_index()\n        \n        # self join to get opponent info\n        \n        feature_team = feature_team.merge(feature_team,\n                          left_on=['player_team_name', 'season', 'round', 'kickoff_time', 'opponent_team_name'],\n                          right_on=['opponent_team_name', 'season', 'round', 'kickoff_time', 'player_team_name'],\n                          how='left',\n                          suffixes=('', '_opponent'))\n            \n        \n\n        \n        for lag in lags:\n            lagged_feature_name = 'last_' + str(lag) + '_' + feature_name\n            lagged_opponent_feature_name = 'opponent_last_' + str(lag) + '_' + feature\n            \n\n            if lag == 'all':\n                \n                feature_team[lagged_feature_name] = feature_team.sort_values('round').groupby('player_team_name')[feature_name]\\\n                                                .apply(lambda x: x.cumsum() - x)\n            \n                feature_team[lagged_opponent_feature_name] = feature_team.groupby('player_team_name')[opponent_feature_name]\\\n                                                .apply(lambda x: x.cumsum() - x)\n            else:\n                    \n                       \n                feature_team[lagged_feature_name] = feature_team.sort_values('round').groupby('player_team_name')[feature_name]\\\n                                                    .apply(lambda x: x.rolling(min_periods=1,\n                                                                              window=lag+1).sum()-x)\n\n                feature_team[lagged_opponent_feature_name] = feature_team.groupby('player_team_name')[opponent_feature_name]\\\n                                                    .apply(lambda x: x.rolling(min_periods=1,\n                                                                              window=lag+1).sum()-x)\n\n            lagged_features.extend([lagged_feature_name, lagged_opponent_feature_name])\n            \n        out_df = out_df.merge(feature_team,\n                             on=['player_team_name', 'season', 'round', 'kickoff_time', 'opponent_team_name'],\n                             how='left')\n        \n        \n        return out_df, lagged_features","42ad8486":"player_features_to_lag = [\n    'assists',\n     'bonus',\n     'bps',\n     'creativity',\n     'clean_sheets',\n     'goals_conceded',\n     'goals_scored',\n     'ict_index',\n     'influence',\n     'minutes',\n     'threat']\n\nteam_features_to_lag = ['goals_conceded', 'goals_scored', 'team_points', 'opponent_points']","2a8c2a03":"lagged_gw_df_players, lagged_player_features = player_lag_features(gws_df, player_features_to_lag, ['all', 1, 3, 5])","ff3dd372":"lagged_gw_df, lagged_team_features = team_lag_features(lagged_gw_df_players, team_features_to_lag, ['all', 1, 3, 5])","cbbe75b6":"relevant_features = ['position', 'was_home', 'minutes', 'value', 'round', 'season_num'] + \\\n    lagged_player_features + \\\n    lagged_team_features ","715b7407":"def make_dummies(df, numerical_features, categorical_features):\n    \n  \n    X_num = df[numerical_features]\n    X_cat = df[categorical_features]\n    \n    X_cat = X_cat.astype(str)\n    X_cat = pd.get_dummies(X_cat)\n    \n    # Join categorical and numerical features\n    X = pd.concat([X_num, X_cat], axis=1)\n    \n    return X","1ff81804":"categorical_features = ['was_home', 'position']\nnumerical_features = numerical_features = list(set(relevant_features) - set(categorical_features))","2e287f44":"train_df = lagged_gw_df[(lagged_gw_df.season!='1920')]\ntest_df = lagged_gw_df[(lagged_gw_df.season=='1920') ]\n\n","c33e990c":"# XGBoost handles NA values, but the other scikit learn methods (that I've chosen) do not\n\nlagged_gw_df_no_na = lagged_gw_df.dropna(subset=relevant_features + ['total_points', 'season'])\ntrain_df_no_na = lagged_gw_df_no_na[lagged_gw_df_no_na.season!='1920']\ntest_df_no_na = lagged_gw_df_no_na[lagged_gw_df_no_na.season=='1920']","42aacb62":"X_train = make_dummies(train_df[relevant_features], numerical_features, categorical_features)\ny_train = train_df.total_points\n\nX_train_no_na = make_dummies(train_df_no_na[relevant_features], numerical_features, categorical_features)\ny_train_no_na = train_df_no_na.total_points","1d988445":"X_test = make_dummies(test_df, numerical_features, categorical_features)\ny_test = test_df.total_points\n\nX_test_no_na = make_dummies(test_df_no_na, numerical_features, categorical_features)\ny_test_no_na = test_df_no_na.total_points","73a37e45":"params = {\n         'max_depth': list(range(3,7)),  \n    'min_child_weight': list(range(10,51)),\n    'learning_rate':  [0.03, 0.15, 0.3, 0.45, 0.6],\n    'subsample': stats.uniform(0.8, 0.1),\n    'colsample_bytree': [0.8, 0.1]}\n\nxgb_reg = xgb.XGBRegressor(objective='reg:squarederror')\nxgb_reg.fit(X_train, y_train)\n\n\n\nxgb_cv = RandomizedSearchCV(xgb_reg, params, cv=3, scoring='neg_root_mean_squared_error',\n                            random_state=999)\n","a13422c9":"xgb_cv.fit(X_train, y_train)\nxgb_best = xgb.XGBRegressor(objective='reg:squarederror')\nxgb_best.set_params(**xgb_cv.best_params_)\n","6a298c82":"# Initialize the models. I am doing it this way as predictions are easy to calculate so I will not be storing them until I need them\n\n\nseed = 999\nmodels = []\nmodels.append(('LinReg', LinearRegression()))\nmodels.append(('LassoReg', LassoCV()))\nmodels.append(('RidgeReg', RidgeCV()))\n","322afe40":"def get_cv_scores(models, X, y, k=5, seed=999):\n    \n    # inspired by the excellent tutorial by Jason Brownlee:\n    # https:\/\/machinelearningmastery.com\/compare-machine-learning-algorithms-python-scikit-learn\/\n    \n    names = []\n    results = []\n    print(\"Cross val scores:\")\n    \n    for name, m in models:\n        cv_results = -cross_val_score(m, X, y, cv=k, scoring='neg_root_mean_squared_error')\n        results.append(cv_results)\n        names.append(name)\n        \n        print(\"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std() ))\n        print(\"\")\n        print(\"*\"*88)\n        print(\"\")\n        \n    return names, results\n","bd92ce8c":"model_names, model_results = get_cv_scores(models, X_train_no_na, y_train_no_na)","5f99b727":"xgb_cv_scores = get_cv_scores([(\"XGB\", xgb_best)], X_train, y_train)","878fa1f9":"model_names += xgb_cv_scores[0]\nmodel_results += xgb_cv_scores[1]","d94e5cf9":"def compare_model_scores(model_names, model_results):\n    \n    fig = plt.figure()\n    fig.suptitle('Model comparison')\n    ax = fig.add_subplot(111)\n    plt.boxplot(model_results)\n    ax.set_xticklabels(model_names)\n    plt.show()","1898f460":"compare_model_scores(model_names, model_results)","48d9146d":"def make_predicted_table(y_test, y_pred, gw_df):\n    results_df = pd.DataFrame(list(zip(y_test.tolist(), y_pred.tolist())),\n                             columns=[\"actual\", \"predicted\"])\n    \n    \n    results_df.reset_index(drop=True, inplace=True)\n    gw_df.reset_index(inplace=True)\n    pred_df = pd.concat([gw_df, results_df], axis=1)\n    \n    return pred_df","6ea44540":"def get_suggested_transfer(predicted_df, team_list, current_money):\n    \n    predicted_diff = 0\n    money_change = 0\n    suggested_in = ''\n    suggested_out = ''\n    team_df = predicted_df[(predicted_df.full_name.isin(team_list))]\n    \n  \n    teams_dict = {}\n    for i, row in team_df.iterrows():\n        if row.player_team_name not in teams_dict:\n            teams_dict[row.player_team_name] = [row.full_name]\n        else:\n            teams_dict[row.player_team_name].append(row.full_name)\n            \n            \n    for position in [\"Defender\", \"Midfielder\", \"Forward\"]:\n        \n        # don't bother about keepers, variance in scores is not that great\n        # so, save the free transfer for other positions\n       \n        \n        \n        player_df = predicted_df[predicted_df.position==position].sort_values('predicted', ascending=False).reset_index()\n\n        \n        lowest_pos = 0\n        player_names = team_df[team_df.position==position].full_name.values\n        \n        # loop through the players for this position, and get the rank (row number) of the player with the lowest predicted score\n        for p in player_names:\n            player_pos = player_df[player_df.full_name==p].index[0]\n            if player_pos > lowest_pos:\n                lowest_pos = player_pos\n                potential_out = p\n                potential_out_cost = team_df[team_df.full_name==p].value.values[0]\n                potential_out_team = team_df[team_df.full_name==p].player_team_name.values[0]\n                \n            elif len(player_names) <= 1:\n                potential_out_cost = 0\n                potential_out_team = 'none'\n                potential_out = 'none'\n                \n        # get all players above this player\n        potential_players = player_df[:lowest_pos]\n        \n        \n        # only keep players that we can afford\n        potential_players = potential_players[potential_players.value <= potential_out_cost + current_money]\n        \n        # only keep players who played (need a better way of doing this)\n        potential_players = potential_players[potential_players.minutes > 0]\n\n        # get the prediction difference for each suggested player\n        # select the one with the highest difference as the suggested transfer (compare across positons)\n        \n        potential_out_predicted = team_df[team_df.full_name==p].predicted.values[0]\n        \n        for i, row in potential_players.iterrows():\n                \n            # skip if it is a player we already have\n            if row.full_name in team_list:\n                continue\n\n\n\n            # if there are no other players of the same team, it's ok to consider this player\n            # if not, check whether there are 3 players of the same team already\n            if row.player_team_name not in teams_dict:\n                pass\n            else:\n                if len(teams_dict[row.player_team_name]) == 3:\n                    # if there are already 3 players of the same team,\n                    # can't take another player of the same team\n                    # unless the suggested_out is the same team as suggested_in (direct swap)\n                   \n                    if row.player_team_name == potential_out_team:\n                        pass\n                    else:\n                        continue\n                else:\n                    pass\n                \n            \n            # check for difference in predictions\n            if row.predicted - potential_out_predicted > predicted_diff:\n                predicted_diff = row.predicted - potential_out_predicted\n                suggested_in = row.full_name\n                suggested_out = potential_out\n                \n                # calculate change in money\n                money_change = potential_out_cost - row.value\n                \n    return suggested_in, suggested_out, money_change","1664b056":"def get_score(team_list, gw_df):\n    \n    gw_score = gw_df[gw_df.full_name.isin(team_list)].actual.sum() \\\n        + gw_df[(gw_df.full_name.isin(team_list)) & (gw_df.position!='Keeper')].sort_values(\"predicted\", ascending=False).head(1).actual.values[0]\n        \n    \n    return gw_score","cdbc5713":"def get_performance(team_list, starting_money, gw_list,\n                   prediction_df):\n    \n    current_money = starting_money\n    total_score = 0\n    \n    \n    in_list = []\n    out_list = []\n    score_list = []\n    unplayed_list = []\n    \n    \n    for gw in gw_list:\n\n        gw_df = prediction_df[prediction_df.GW==gw]\n        money_change = 0\n        suggested_in = ''\n        suggested_out = ''\n        if gw > 1:\n            \n\n            suggested_in, suggested_out, money_change = get_suggested_transfer(gw_df, team_list, current_money)\n        \n            current_money += money_change\n\n            team_list.append(suggested_in)\n            team_list.remove(suggested_out)\n            \n        \n\n        ## Calculate scores\n        \n        gw_score = get_score(team_list, gw_df)\n\n        \n        out_list.append(suggested_out)\n        in_list.append(suggested_in)\n        score_list.append(gw_score)\n        \n        total_score += gw_score\n        \n    out_df = pd.DataFrame({'GW': gw_list,\n                          'player_in': in_list,\n                          'player_out': out_list,\n                          'total_score': score_list})\n    \n    print(total_score)\n    \n    return(out_df)","c0c63e3c":"## Get predictions for the two best models, Linear Regression and XGBoost\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_no_na, y_train_no_na)\nlinreg_predictions = lin_reg.predict(X_test_no_na)\n\n\nxgb_best.fit(X_train, y_train)\nxgb_predictions = xgb_best.predict(X_test)","a548316c":"predicted_df_lin_reg = make_predicted_table(y_test_no_na, linreg_predictions, test_df_no_na[relevant_features + ['full_name', 'GW', 'player_team_name', 'total_points']] )\npredicted_df_xgb = make_predicted_table(y_test, xgb_predictions, test_df[relevant_features + ['full_name', 'GW', 'player_team_name', 'total_points']] )","e1feb92f":"captain = get_initial_team(prob, previous_season_player_df).sort_values(\"total_points\", ascending=False).head(1).full_name.values[0]\n\ntotal_points = current_season_player_df[current_season_player_df.full_name.isin(initial_team_df.full_name)].total_points.sum()\ntotal_points += current_season_player_df[current_season_player_df.full_name==captain].total_points\n\nprint(\"Total points for 19\/20 season:\", total_points.values[0])","00b6fe58":"## Linear regression\n\nmy_team = list(initial_team_df.full_name)\ngameweeks = (test_df.GW).unique()\nstarting_money = 1000 - bench_cost - initial_team_df.starting_cost.sum()\n\nlin_reg_perf = get_performance(my_team, starting_money, gameweeks,\n                   predicted_df_lin_reg)","ba57c472":"## XGBoost\n\n\nmy_team = list(initial_team_df.full_name)\ngameweeks = (test_df.GW).unique()\nstarting_money = 1000 - bench_cost - initial_team_df.starting_cost.sum()\n\nxgb_cv_perf = get_performance(my_team, starting_money, gameweeks,\n                   predicted_df_xgb)","e0964a3a":"xgb_cv_perf","a0712211":"stats.ttest_rel(xgb_cv_perf.total_score, lin_reg_perf.total_score)","ae8d1397":"How does this compare to the rest of the FPL managers? Unfortunately there is no data available on historic FPL manager ranking, but with some Googling I managed to find someone with the exact same score of 1792 in the 19\/20 season (https:\/\/fantasy.premierleague.com\/entry\/299659\/history)\n\nThis manager ranked 4375672. According to https:\/\/www.premierleague.com\/news\/1252542, there were 7.6 million managers last season. This translates to a final standing of the top 57.57% of managers. Not bad, but not great either.\n\nThis initial score was calculated based on a strategy of \"set it and forget it\" - once the initial team was selected, the captain was selected as well, and I just left it to run for the whole season. No wildcards were played, no transfers were made, and no substitutions were made (so, in some weeks, a player could have had 0 points because they were not playing - but, I have yet to check if this is true for this team).\n\nNext, I will take a look at how we can improve this score by making weekly transfers!","12e600b0":"# Model comparison\n\nFirst, a reminder of the total points that would have been obtained without any transfers:","c62af417":"The XGBoost model gives a end-of-season score of 2219. A more modest, but still substantial, improvement over the linear regression model. This corresponds to a ranking of 79192, or about top 1% of all managers (https:\/\/fantasy.premierleague.com\/entry\/104879\/history). XGBoost allowed the team to climb more than 15% ranking points compared to linear regression! Furthermore, I was able to achieve a top 1% ranking! \n","abef5553":"### Optimizing for starting 11\n\nNow that I have my bench players, I'll go ahead and optimize for the starting 11. Again, here I am using the actual results of the previous season. The reasoning here is that, at least for the first Gameweek, the players who are most likely to perform well are the players who have performed well in the last season (in the absence of other information). Of course, one can also try to predict the total number of points a player will score in the current season, but in the absence of any information about the current season, I am doubtful that the prediction will be meaningful or fare better than this simple heuristic. This is something that I am planning to look into soon!\n\nFirst, a bunch of helper functions to make life easier:","c28f8e85":"Again, some helper functions to make life easier:","46fafc87":"## Picking the initial team of 15 players\n\nI decided to go with the simple approach of using the performance of players in the previous season (18\/19) as the basis of selection into the team. Using linear programming, I will optimize for the maximum number of points scored in the 18\/19 season, with the following constraints:\n\n- Total budget of 1000 \n- 1 keeper\n- 4 defenders\n- 4 midfielders\n- 2 forwards\n- not more than 3 players from the same team\n\nSince benched players do not contribute to Gameweek points, I will first pick the 4 cheapest players (1 from each position) in order to maximize the amount of money I can use for the starting 11. To do this, I simply\n\nSince I am using the 19\/20 season as the test set, I will use the 18\/19 season to pick my initial team.","73eaa79c":"# Modelling\n\n","acb47162":"## Clean and process dataframes\n\n\nFor both types of dataframes, I want to add:\n- Player position\n- Full name (since the names are inconsistent across seasons and between dataframes)\n\n\n\nAlso, it turns out that there are 2 Danny Wards in the 18\/19 season. I am still thinking of a good way to represent both. But, since the are both fringe players in that season (total of 0 points between them), I will remove them for now (sorry!)\n\n\n","681a7652":"# Approach\n\nTo get the initial team, I will rely on a simple method of picking the players with the most points based on last season.\n\nA brief recap on the current the FPL rules:\n\n1. A team must have 15 players\n2. A team must have:\n   - 2 Goalkeepers\n   - 5 Defenders\n   - 5 Midfielders\n   - 3 Forwards\n3. The total value of the initial squad must not exceed 100 million\n4. There can only be a maximum of 3 players from a single team\n5. Only the starting 11 will gain points for the FPL team\n6. A captain's score is doubled. \n7. Each gameweek, one free transfer is allowed. Each additional transfer will cost 4 points (not money!) \n\nDue to the complexity of the FPL mechanism, I have chosen to focus only on the above rules for now. There are also other rules that I plan on incorporating into my model:\n\n8. If a player in the starting 11 is unavailable (due to injury or suspension etc.), they will be substituted by the first player on the bench.\n9. If the captain is unavailable, the vice-captain's score is doubled instead.\n10. If the free transfer each week is not used, it can be carried over to the next Gameweek (for a maximum of 2 free transfers for a given Gameweek).\n11. There are a number of wildcards that can boost the points obtained in a Gameweek\n\n\nI will first use a linear programming approach to optimize the initial team. Then, I will build a regression model to forecast the predicted points for each player for each Gameweek, and use a heuristic-based algorithm to determine the best transfer (if any) to make for each Gameweek.\n","e32e02a1":"# To-do:\n\n- Add more data sources, e.g. FIFA\/Football Manager ratings, bookmaker odds etc.\n- Add wildcard strategy\n- Improve initial team selection\n- Take into account injuries and other reasons for not being able to play (i.e. minutes == 0)\n- For non-XGBoost regression, impute NA values\n- Stacked regressors\n- Re-train the model each Gameweek (and\/or use Bayesian methods instead)\n- Incorporate better strategies\/heuristics for making transfers. For example, having a set of MVPs that will not be transferred out (see https:\/\/twitter.com\/EricDFreeman\/status\/1154771281490993153)\n- Move this to github and modularize stuff","914258c9":"## Brief Intro\n\nThe Fantasty Premier League (FPL) is a fantasy football (soccer) league in which you score points based on a player's real-life performance on the football pitch. In the current instantiation of the FPL (19\/20):\n\n- \n\n![image.png](attachment:image.png)","14df7c41":"## Performance of initial team\n\nRight off the bat, how does this team fare in the 19\/20 season?\n\nNote: I selected the highest scoring player in the 18\/19 season as the captain for the 19\/20 season","b21418b8":"Since p = .009, the null hypothesis of \"no difference in the predicted points between the linear regression and XGBoost model\" is rejected (at alpha = .05). Thus, I am confident that there is a meaningful difference between the two models, and that the XGBoost provides better predicted points for the 19\/20 FPL season.","4509b81a":"It looks like XGBoost is the winner here in terms of average RMSE. The other regression models appear to be more or less the same, so I will go with the simple Linear Regression model as the other choice for comparing the transfer selection algorithm.","57f9a613":"The linear regression model gives a end-of-season score of 2041. Linear regression shows a huge improvement of about 250 points! This corresponds to a ranking of 1457242 (https:\/\/fantasy.premierleague.com\/entry\/222484\/history\/), or about the top 19% of all managers. Great improvement from 57% just with a simple linear regression model! \n\nWhat about XGBoost?","886ac62e":"# FPL Team Selection and Weekly prediction\n\nRecently I've been getting back into soccer Fantasy Premier League (FPL) again after trying it many years ago. The last I played it, I abandoned it mid-season as it was too much to keep with with - team news, player injuries, who's playing well and who's not, who's playing who... There are just too many moving parts and I had more important things to do (e.g. play FIFA).\n\nThe new season has just started and I thought - why not offload all this thinking to an ML model to predict the player points each week, and also come up with an algorithm to determine which transfer(s) I should make each week? This notebook is the result of that.\n\nUnfortunately I couldn't get the model up in time before the start of the season. Either way, I will be using the 19\/20 season to test\/validate my model, and apply it to the current season thus far (starting from GW 5).\n\n\n## Acknowledgements\n\n- I obtained the data from `vaastav` at https:\/\/github.com\/vaastav\/Fantasy-Premier-League, which is a really rich source of well-organized player and gameweek data, dating back to the 15\/17 season.\n- The team IDs were inconsistent across seasons, which is a pain, but fortunately `solpaul` has a nice file here that tracks the changes across seasons:  https:\/\/github.com\/solpaul\/fpl-prediction\/blob\/master\/data\/teams.csv\n- In terms of modeling, I drew inspiration from the following sources:\n    - https:\/\/medium.com\/@sol.paul\/how-to-win-at-fantasy-premier-league-using-data-part-1-forecasting-with-deep-learning-bf121f38643a#http:\/\/www.philipkalinda.com\/ds9.html\n    - http:\/\/www.philipkalinda.com\/ds9.html","a7d45e6e":"# Final thoughts\n\nAs Box once said, \"All models are wrong, but some are useful\". As with all models, we cannot achieve 100% accuracy (ignoring edge cases e.g. a system where the data is deterministic - input of X always gives Y). Models can and do sometimes go wrong, and it would be important to have a human in the loop to determine whether the suggested transfer makes sense or not. For example, the model sometimes makes questionable transfers, such as bringing in Dele Alli for Heung Min Son (Gameweek 28) Nicholas Pepe for Kevin de Bruyne (Gameweek 45). For someone who has been following the 19\/20 season, these would raise eyebrows given the excellent form that the \"transferred out\" players were in.  \n\nEither way, the final model seems to perform well in the 19\/20 season. I am excited to test this out in the 20\/21 season, and hopefully make it to the top 1% as predicted!","84c1eed8":"# Regression models\n\nFor a start, I have chosen to look at:\n\n- Linear regression and its variants, Lasso and Ridge\n- XGBoost\n- A simple baseline model of using group means\n\nFor the XGBoost regressor, I will first do a randomized search to tune the hyperparameters. One risk of this approach is that time and resources will be wasted on tuning the hyperparameters if it turns out that a much simpler model that doesn't require hyperparameter tuning (e.g. linear regression) performs much better than XGBoost. But, given that XGBoost has known to almost always outperform most other models, it is at least worth a shot.\n\nThen, all models will be compared and the top 2 (with the highest cross-validation score - here I use RMSE) will be used to make predictions to be fed into the algorithm to select potential transfers for each gameweek.\n\nA brief explanation on the starting values I have selected for the tuning of the hyperparameters:\n\n`max_depth`: larger values makes the model (1) more complex, (2) more likely to overfit, and (3) take longer to train. Here, I used a range from 3-6 (the default). I am wary of overfitting since the nature of the data can change dramatically across seasons (e.g. due to player transfers, improvement\/decline of play abilities, state of the club etc.). \n\n`min_child_weight`: Similar to `max_depth`, I want to reduce the complexity\/variance in the model. Here, larger values will reduce the likelihood of overfitting. I decided to use a range from 6-10\n\n`learning_rate`: The default here is 0.3. I decided to try 5 values: 1\/10 of the default, 1\/5 of the default, 1.5x the default, and 2x the default.\n\n`subsample` and `colsample_by_tree`: The default here is 1, but since I want to reduce overfitting, I decided to use a range from 0.8 - 0.9. \n\n","80140c13":"# Modeling\n\n## Approach:\n\n- Use each player's performance in previous Gameweeks to predict total points for this Gameweek.\n- Use team and opponent performance in previous Gameweeks as well\n\nThis will be done by creating lagged versions of selected features such as cumulative goals scored\/conceded by the player's team as well as their opponent. Each player's contribution will also be taken into account by creating the corresponding lagged features at the player level, as well by incorporating what FPL clas the \"ICT\" variables - intensity, creativty, and threat - which related to a player's performance on the pitch that did not directly relate to goals (but relate to creating goal-scoring opportunities and build-up play)\n\n\n- Use previous Gameweek's performance to predict total points for next gameweek\n- Calculate a rolling average\n- Also use previous season's total points as a predictor, as the predictions for the earlier gameweeks might be extremely noisy. This is important as we don't want star players to be transfered out just because they had a bad start\n- For players without data from past seasons, replace with average of players in the same position and similar cost (rounded to nearest 10m)\n\n- Only include players who had played (i.e. minutes > 0) to reduce noise","cc1f6392":"First, let's get the players who are available in the current season. This is necessary because some players would have been relegated\/promoted after the previous season, and also because players would have been transferred between clubs (of different leagues).","a16dadbe":"# Transfer selection\n\nNext, I came up with a heuristic-based algorithm that will decide which transfer, if any, to make for each Gameweek:\n\n- For each player position, find the player in the current team with the lowest predicted points. Here, I excluded the Keeper since the variance in points (and thus the potential benefit from a transfer) from Keepers is not very high compared to the other positions.\n- From the pool of players not in the current team, get all players that (1) have predicted points that are higher than this player, and (2) a cost that is lower than the cost of this player and the current available money\n- From this pool of players, a potential player will be selected. \n- Among the 3 potential players selected (1 from each position), the player with the highest predicted point difference, compared against the player that they will replace, will be selected as the final player to be transferred in. \n- The constraint of not more than 3 players from the same team will also be checked.\n- There could be cases where all the players from the current team have the highest predicted points for their respective positions. In that case, no transfers will be made.","7e98810d":"## Statistical analysis\n\nThe total points obtained by the two different models definitely different, but is this difference greater than what would be expected by random chance alone? For example, suppose that the XGBoost regressor resulted in 1 more point that the linear regression model. This difference would likely be due to chance, and we might be better off just using the simpler model.\n\nHere, I conducted a paired-samples t-test to determine whether the better performance of the XGBoost model was due to chance:"}}