{"cell_type":{"38f6d65b":"code","86889e58":"code","3bcd39d6":"code","f5a2539f":"code","b118c230":"code","ee157991":"code","752d6b6e":"code","d44a86f4":"code","6074a11b":"code","82cd7f70":"code","1b56d882":"code","7cddc6c2":"code","e68bc646":"code","a0127683":"code","a86928b2":"code","085bf968":"code","40d5b1b7":"code","cffea19d":"code","0fe19771":"code","21bae77d":"code","bf244527":"code","e6ef34cf":"code","0821bd17":"code","ce6af4db":"code","58c37388":"code","4b896636":"code","9d5fe1db":"code","5478fbf7":"code","fc3e3366":"code","384ccbcd":"code","2ad1c50a":"code","c96fb5d2":"code","5c1f9221":"code","f2933ad0":"code","30dba208":"code","329d19a9":"code","ebcee20e":"code","8330c66b":"code","36c88acd":"code","6a218e84":"code","1e6d2307":"code","230a4f96":"code","e4377d9c":"code","29a53309":"code","034c0d1c":"code","1c72604b":"code","e9415f04":"code","7e836435":"code","fe542ef8":"code","b8488b1b":"code","897f2753":"code","ff50fca6":"code","836ac9de":"code","dca28863":"code","fc52245b":"code","1bc310a0":"markdown","0b6a609d":"markdown","1100e330":"markdown","3629dc31":"markdown","2bb0e7d9":"markdown","c9ddada9":"markdown","8dd58c99":"markdown","f6fc8e0e":"markdown","9b25db20":"markdown","6580af92":"markdown","81998a78":"markdown","340aa153":"markdown","c3577352":"markdown","e89d0889":"markdown","b7bd6d7f":"markdown","20f1bee1":"markdown","c4e764b8":"markdown","b21e4297":"markdown","42a6358e":"markdown","62e52d79":"markdown","9b70baa7":"markdown","378f2ce4":"markdown","4e09a8f2":"markdown"},"source":{"38f6d65b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86889e58":"import tensorflow as tf\nimport random as rn\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(32)\nrn.seed(66)\ntf.random.set_seed(14)","3bcd39d6":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","f5a2539f":"train.head()","b118c230":"train.shape","ee157991":"train.isna().sum()","752d6b6e":"train.target.value_counts()","d44a86f4":"sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","6074a11b":"from string import punctuation","82cd7f70":"def process_hashtags(df):\n    \n    def extract_hashtags(text):\n        \n    # Split the string into words\n        words = text.split()\n    \n    # Create a list of words that are hashtags\n        hashtags = [word for word in words if word.startswith('#')]   \n    \n        return(hashtags)\n\n    df['hashtags'] = df['text'].apply(extract_hashtags)\n    \n    punc = punctuation\n    \n    def remove_punc(text):\n        \n        return text.translate(str.maketrans('', '', punc))\n    \n    # Add the hashtags to the 'keyword' column and some clean\n    df['hashtags'] = df['hashtags'].apply(lambda x: remove_punc(str(x)))\n    df['hashtags'] = df['hashtags'].apply(lambda x: x.replace(' ', ' ,'))\n    df['keyword'].fillna(' ', inplace=True)\n    df['keyword'] = df['keyword'] + ', ' + df['hashtags']\n    df['keyword'] = df['keyword'].apply(lambda x: x.strip(' ,'))\n    \n    def none(text):\n        \n        if len(text) < 1:\n            text = 'None'\n            \n        return text\n    \n    df['keyword'] = df['keyword'].apply(lambda x: none(x))\n    \n    df.drop('hashtags', axis=1, inplace=True)\n    \n    return df","1b56d882":"train_cop = train.copy()\ntest_cop = test.copy()","7cddc6c2":"train_cop = process_hashtags(train_cop)\ntest_cop = process_hashtags(test_cop)","e68bc646":"train_cop.head()","a0127683":"import spacy ","a86928b2":"nlp = spacy.load('en_core_web_sm')","085bf968":"def location(df):\n    \n    # create a column with the words identified by spacy (GPE stands for 'Geopolitical entity, i.e. countries, cities, states') \n    for i, t in enumerate(df['text']):\n        doc = nlp(t)\n        for ent in doc.ents:\n            if ent.label_ == 'GPE':\n                df.loc[i, 'loc_text'] = ent.text\n    \n    # replace missing values in 'location' by values from the new column\n    for i, v in enumerate(df['location']):\n        if v is np.nan:\n            df.loc[i, 'location'] = df.loc[i, 'loc_text']\n    \n    # add a keyword identified as GPE in 'location' if there's no value and drop it from 'keyword'\n    for i, t in enumerate(df['keyword']):\n        doc = nlp(t)\n        for ent in doc.ents:\n            if (pd.isna(df.loc[i, 'location'])) and (ent.label_ == 'GPE'):\n                df.loc[i, 'location'] = ent.text\n                df.loc[i, 'keyword'] = df.loc[i, 'keyword'].replace(ent.text, '')\n    \n    # some clean\n    df.drop('loc_text', axis=1, inplace=True)\n    df['keyword'] = df['keyword'].apply(lambda x : x.strip(' ,'))\n    \n    return df ","40d5b1b7":"# this will take some time to run\n#train_cop = location(train_cop)\n#test_cop = location(test_cop)","cffea19d":"train_cop.head()","0fe19771":"train_cop.drop('id', axis=1, inplace=True)\ntrain_cop['location'].fillna('None', inplace=True)","21bae77d":"test_cop.drop('id', axis=1, inplace=True)\ntest_cop['location'].fillna('None', inplace=True)","bf244527":"train_cop['text_count'] = train_cop['text'].apply(len)\ntest_cop['text_count'] = test_cop['text'].apply(len)","e6ef34cf":"max(train_cop.text_count)","0821bd17":"from sklearn.preprocessing import LabelEncoder","ce6af4db":"def encoder(train, test, col):\n    \n    le = LabelEncoder()\n    train[col] = le.fit_transform(train[col])\n    # change to 'unknown' words in test set which doesn't appears in train set \n    test[col] = test[col].map(lambda x: 'unknown' if x not in le.classes_ else x)\n    le.classes_ = np.append(le.classes_, 'unknown')\n    test[col] = le.transform(test[col])\n    \n    return train, test","58c37388":"train_cop, test_cop = encoder(train_cop, test_cop, 'keyword')\ntrain_cop, test_cop = encoder(train_cop, test_cop, 'location')","4b896636":"# !pip install pyspellchecker","9d5fe1db":"import re \nfrom nltk import word_tokenize, sent_tokenize \nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport nltk\n# from spellchecker import SpellChecker","5478fbf7":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)","fc3e3366":"def remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text) ","384ccbcd":"train_cop['text'] = train_cop['text'].apply(remove_urls)\ntrain_cop['text'] = train_cop['text'].apply(remove_html)\ntest_cop['text'] = test_cop['text'].apply(remove_urls)\ntest_cop['text'] = test_cop['text'].apply(remove_html)","2ad1c50a":"def clean_text(text):\n\n    text = text.translate(punctuation)\n\n    text = text.lower().split()\n\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]\n\n    text = \" \".join(text)\n    text = re.sub(r\"[^\\w\\s]\", \" \",text)\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \",text)\n\n    text = text.split()\n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(w) for w in text]\n    text = \" \".join(lemmatized_words)\n\n\n    return text","c96fb5d2":"train_cop['text'] = train_cop['text'].map(lambda x: clean_text(x))\ntest_cop['text'] = test_cop['text'].map(lambda x: clean_text(x))","5c1f9221":"\"\"\"spell = SpellChecker()\n# this will correct automatically the tweets\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\"\"\"","f2933ad0":"# this will take some time to run\n# train_cop['text'] = train_cop['text'].apply(lambda t: correct_spellings(t))           \n# test_cop['text'] = test_cop['text'].apply(lambda t: correct_spellings(t))  ","30dba208":"train_cop.to_csv('train_cleaned')                  ","329d19a9":"test_cop.to_csv('test_cleaned')        ","ebcee20e":"from keras.models import Model\nfrom keras.layers import Dense, LSTM, Embedding, Dropout, Input, Concatenate, BatchNormalization, Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.initializers import Constant\nfrom keras.utils.vis_utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","8330c66b":"train_cleaned = pd.read_csv('\/kaggle\/input\/tweetsdisaster-train-cleaned\/train_cleaned').drop('Unnamed: 0', axis=1)\ntest_cleaned = pd.read_csv('\/kaggle\/input\/tweetsdisaster-test-cleaned\/test_cleaned').drop('Unnamed: 0', axis=1)","36c88acd":"X_train, X_test, y_train, y_test = train_test_split(train_cleaned.drop('target', axis=1), train_cleaned['target'], random_state=66)","6a218e84":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train['text'])\n\nX_train['text'] = tokenizer.texts_to_sequences(X_train['text'])\nX_test['text'] = tokenizer.texts_to_sequences(X_test['text'])\ntest_cleaned['text'] = tokenizer.texts_to_sequences(test_cleaned['text'])","1e6d2307":"vocab_size = len(tokenizer.word_index) + 1","230a4f96":"var_train_text = pad_sequences(X_train['text'], padding='post', maxlen=50)\nvar_test_text = pad_sequences(X_test['text'], padding='post', maxlen=50)\ntest_set = pad_sequences(test_cleaned['text'], padding='post', maxlen=50)","e4377d9c":"var_train_num = X_train.drop('text', axis=1)\nvar_test_num = X_test.drop('text', axis=1)","29a53309":"embed_index = {}\n# Read the file and create a dict with words as keys and coefficients as values\nwith open('..\/input\/glove-42b\/glove.42B.300d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = values[1]\n        embed_index[word] = np.asarray(coefs, np.float32)","034c0d1c":"embed_matrix = np.zeros((vocab_size +1, 100))\n# Replace the value of the dict (initially 0) if the word of our vocabulary is present in the glove doc \nfor w, i in tokenizer.word_index.items():\n    embed_vector = embed_index.get(word)\n    if embed_vector is not None:\n        embed_matrix[i] = embed_vector","1c72604b":"from IPython.display import Image\nImage(\"..\/input\/tweetsdisaster-model-plt\/model_plt.png\")","e9415f04":"shape_var_num = len(X_train.drop(['text'], axis=1).columns)","7e836435":"inp_text = Input(shape=(50,))\ninp_num = Input(shape=(shape_var_num,))   \n\nnorm = BatchNormalization()(inp_num)\n\nemb_layer = Embedding(input_dim=vocab_size+1, output_dim=100, input_length=50, embeddings_initializer=Constant(embed_matrix))(inp_text)\n\nlstm = Bidirectional(LSTM(64, recurrent_dropout=0.2, return_sequences=True))(emb_layer)\nlstm_2 = Bidirectional(LSTM(64, recurrent_dropout=0.2))(lstm)\n\nconc = Concatenate()([lstm_2, norm])\ndrop = Dropout(0.2)(conc)      \n      \nhidden = Dense(128, activation='relu')(drop)\n\ndrop_2 = Dropout(0.2)(hidden)\n\nout = Dense(1, activation='sigmoid')(drop_2)\n\nmodel = Model(inputs=[inp_text, inp_num], outputs=out)\n\nmodel.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n","fe542ef8":"early_stop = EarlyStopping(patience=5, restore_best_weights=True)","b8488b1b":"model.fit([var_train_text, var_train_num], y_train, validation_data=([var_test_text, var_test_num], y_test), epochs=20, callbacks=[early_stop])     ","897f2753":"pred = model.predict([test_set, test_cleaned.drop('text', axis=1)]).round().astype(int)","ff50fca6":"pred","836ac9de":"sub['target'] = pred","dca28863":"sub['target'].value_counts()","fc52245b":"sub.to_csv('submission.csv', index=False)  ","1bc310a0":"Remove url's","0b6a609d":"# **Location**","1100e330":"Get the submission","3629dc31":"# **Model**","2bb0e7d9":"I'll use a pre trained embedding model \n\nThe one trained on tweets leads on poorer results for me so i chose the 42b (https:\/\/nlp.stanford.edu\/projects\/glove\/)","c9ddada9":"I'll extract hashtags from the text and use it to complete the keyword column","8dd58c99":"I'll use the Named Entity Recognition from Spacy  (https:\/\/spacy.io\/usage\/linguistic-features#named-entities) on tweets\n\nThe idea is to use words identified as a location to complete the column","f6fc8e0e":"I'll began by preprocess all the features but 'text'\n\nThe tweets will be treated on a second part","9b25db20":"# **Missing values**","6580af92":"# **Text**","81998a78":"# **Feature engineering**","340aa153":"Some observations : \n- The task is a binary classification\n- We have a lot of missing values\n- The text doesn't seems to be clean\n- The class to predict is pretty balanced","c3577352":"Prepare the text data","e89d0889":"# **Keyword**","b7bd6d7f":"I'll just add a column with the number of characters in each tweets","20f1bee1":"Global clean","c4e764b8":"# **Encoder**","b21e4297":"Get the numerical data","42a6358e":"Glove","62e52d79":"Correction","9b70baa7":"Our model will treat the text and the numeric features distingly \n\nFor that reason i'll use the Functional Api of keras rather than the Sequential\n\nThis is what will look like the model : ","378f2ce4":"Split the dataset","4e09a8f2":"Thanks to : \n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing\n"}}