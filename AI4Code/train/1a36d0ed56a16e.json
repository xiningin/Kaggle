{"cell_type":{"6931f859":"code","52101bf5":"code","77324b22":"code","696a2896":"code","f16c92f8":"code","83b0c66b":"code","9617acef":"code","90197241":"code","fab5036b":"code","93d1840c":"code","6aaade31":"code","0377fd85":"markdown","d7ace70d":"markdown","1f1e2b72":"markdown","d3f56c6a":"markdown","58fd7aa7":"markdown","444c9029":"markdown","a4237cad":"markdown"},"source":{"6931f859":"!pip install -U torch wandb transformers","52101bf5":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb\"\nsecret_value = UserSecretsClient().get_secret(secret_label)\n!wandb login $secret_value","77324b22":"import json\nimport math\nimport numpy as np\nimport pandas as pd \nimport random\nimport os\nfrom tqdm.notebook import tqdm\nfrom matplotlib import pyplot as plt\nimport wandb\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Sampler, Dataset, DataLoader\nimport torch.nn.init as init\nfrom torch.nn import Parameter\nfrom torch.autograd.function import InplaceFunction\n\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import AutoConfig\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AdamW\n\nimport shutil\nimport collections\n\ndevice = torch.device(\"cuda\")\nscaler = torch.cuda.amp.GradScaler()","696a2896":"class config:\n    INPUT_DIR = \"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\"\n    OUTPUT_DIR = \"\/kaggle\/working\"\n    SEED = 0\n    N_FOLDS = 5\n    SKIP_FOLD = [1, 2, 3, 4]  # Only fold-0\n    MODEL_NAME = \"deepset\/xlm-roberta-base-squad2\"\n    # tokenizer\n    TOKENIZER = AutoTokenizer.from_pretrained(MODEL_NAME)\n    NOT_WATCH_PARAM = [\"NOT_WATCH_PARAM\", \"TOKENIZER\", \"INPUT_DIR\", \"OUTPUT_DIR\"]\n    MAX_LEN = 384\n    STRIDE = 128\n    MAX_ANSWER_LEN = 30\n    CONTENT_ID= 1\n    N_BEST = 20\n    # param\n    BS = 16\n    LR = 2e-5\n    N_EPOCHS = 1\n    WARM_UP_RATIO = 0.1\n    HIDDEN_DROPOUT_PROB = 0.1\n    LAYER_NORM_EPS = 1e-5\n    WEIGHT_DECAY = 1e-6\n    ACCUMULATE = 2\n    EVAL_STEP = 50","f16c92f8":"def set_seed(seed=config.SEED):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False","83b0c66b":"# cf: https:\/\/www.kaggle.com\/thedrcat\/chaii-eda-baseline\ndef encode_train_example(example):\n    question = example['question'].lstrip()\n    context = example['context']\n    answer_text = example[\"answer_text\"]\n    answer_start = example[\"answer_start\"]\n    answer_end = answer_start + len(answer_text)\n\n    tokenized_examples = config.TOKENIZER(\n            question,\n            context,\n            truncation=\"only_second\",\n            max_length=config.MAX_LEN,\n            stride=config.STRIDE,\n            return_overflowing_tokens=True,\n            return_offsets_mapping=True,\n            return_token_type_ids=True,\n            padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_examples[\"offset_mapping\"]\n\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n    tokenized_examples['sequence_ids'] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(config.TOKENIZER.cls_token_id)\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        token_start_index = 0\n        while sequence_ids[token_start_index] != 1:\n            token_start_index += 1\n\n        token_end_index = len(input_ids) - 1\n        while sequence_ids[token_end_index] != 1:\n            token_end_index -= 1\n\n        if not (offsets[token_start_index][0] <= answer_start and offsets[token_end_index][1] >= answer_end):\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            while token_start_index < len(offsets) and offsets[token_start_index][0] <= answer_start:\n                token_start_index += 1\n            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n            while offsets[token_end_index][1] >= answer_end:\n                token_end_index -= 1\n            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n        \n        tokenized_examples['sequence_ids'].append(sequence_ids)\n\n    return tokenized_examples\n\n\ndef extract_feature_example(df):\n    tokenized_examples = {\n        'input_ids': [],\n        'attention_mask': [],\n        'token_type_ids': [],\n        'start_positions': [],\n        'end_positions': [],\n        'offset_mapping': [],\n        'sequence_ids': [],\n        'example_id': [],\n    }\n    for _, row in df.iterrows():\n        tokenized_example = encode_train_example(row)\n        tokenized_examples['input_ids'].extend(tokenized_example['input_ids'])\n        tokenized_examples['attention_mask'].extend(tokenized_example['attention_mask'])\n        tokenized_examples['token_type_ids'].extend(tokenized_example['token_type_ids'])\n        tokenized_examples['start_positions'].extend(tokenized_example['start_positions'])\n        tokenized_examples['end_positions'].extend(tokenized_example['end_positions'])\n        tokenized_examples['offset_mapping'].extend(tokenized_example['offset_mapping'])\n        tokenized_examples['sequence_ids'].extend(tokenized_example['sequence_ids'])\n        tokenized_examples['example_id'].extend([row['id'] for _ in range(len(tokenized_example['input_ids']))])\n    return tokenized_examples\n\n\nclass ChAIIDataset(Dataset):\n    \n    def __init__(self, df, train):\n        self.feature_examples = extract_feature_example(df)\n        self.train = train\n        \n    def __len__(self):\n        return len(self.feature_examples['input_ids'])\n    \n    def __getitem__(self, item):\n        d = {\n            'input_ids': torch.tensor(self.feature_examples['input_ids'][item]),\n            'attention_mask': torch.tensor(self.feature_examples['attention_mask'][item]),\n            'token_type_ids': torch.tensor(self.feature_examples['token_type_ids'][item]),\n            'start_positions': torch.tensor(self.feature_examples['start_positions'][item]),\n            'end_positions': torch.tensor(self.feature_examples['end_positions'][item]),\n            'offset_mapping': torch.tensor(self.feature_examples['offset_mapping'][item]),\n            'example_id': self.feature_examples['example_id'][item],\n        }\n        if not self.train:\n            d['sequence_ids'] = self.feature_examples['sequence_ids'][item]\n        return d","9617acef":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef eval_model(model, dset):\n    model.eval()\n    all_logits, losses = [], []\n    for d in dset:\n        with torch.no_grad():\n            outputs = model(\n                d['input_ids'].unsqueeze(0).to(device),\n                d['attention_mask'].unsqueeze(0).to(device),\n                d['token_type_ids'].unsqueeze(0).to(device),\n                start_positions=d['start_positions'].unsqueeze(0).to(device),\n                end_positions=d['end_positions'].unsqueeze(0).to(device)\n            )\n        loss = outputs['loss'].item()\n        start_logits = outputs['start_logits'].cpu()\n        end_logits = outputs['end_logits'].cpu()\n        losses.append(loss)\n        all_logits.append((start_logits, end_logits))\n    return all_logits, np.array(losses).mean()\n\n\ndef post_processing(all_logits, train_df, dset):\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(dset):\n        features_per_example[feature[\"example_id\"]].append(i)\n    \n    predicts = []\n    for example_id, feature_indices in features_per_example.items():\n        row = train_df.query(f'id==\"{example_id}\"').iloc[0]\n        context = row['context']\n        predict_answers = []\n        for feature_index in feature_indices:\n            assert dset.feature_examples['example_id'][feature_index] == example_id\n            \n            start_logits, end_logits = all_logits[feature_index]\n            offset_mapping = dset.feature_examples[\"offset_mapping\"][feature_index]\n            sequence_ids = dset.feature_examples[\"sequence_ids\"][feature_index]\n            offset_mapping = [o if i == config.CONTENT_ID else None for i, o in zip(sequence_ids, offset_mapping)]\n            \n            start_indexes = np.argsort(start_logits[0].numpy())[-config.N_BEST:]\n            end_indexes = np.argsort(end_logits[0].numpy())[-config.N_BEST:]\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if offset_mapping[start_index] is None or offset_mapping[end_index] is None:\n                        continue\n                    if len(offset_mapping) <= start_index or len(offset_mapping) <= end_index:\n                        continue\n                    if start_index > end_index or (end_index - start_index) > config.MAX_ANSWER_LEN:\n                        continue\n\n                    start_char = offset_mapping[start_index][0]\n                    end_char = offset_mapping[end_index][1]\n\n                    text = context[start_char:end_char]\n                    score = start_logits[0, start_index] + end_logits[0, end_index]\n\n                    predict_answers.append({\n                        'predict_text': text,\n                        'score': score.item()\n                    })\n            \n        if len(predict_answers) > 0:\n            best_answer = sorted(predict_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n        else:\n            best_answer = {\"predict_text\": \"\", \"score\": 0.0}\n            \n        best_answer['id'] = example_id\n        predicts.append(best_answer)\n    predict_df = pd.DataFrame(predicts)\n    return predict_df","90197241":"class ChaiiModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.transformer_config = AutoConfig.from_pretrained(config.MODEL_NAME)\n        self.transformer_config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": config.HIDDEN_DROPOUT_PROB,\n                \"layer_norm_eps\": config.LAYER_NORM_EPS,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_pretrained(config.MODEL_NAME, config=self.transformer_config)\n        self.qa_outputs = nn.Linear(self.transformer_config.hidden_size, 2)\n        self.__init_weights(self.qa_outputs)\n        \n    def __init_weights(self,module):\n        if isinstance(module,nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.transformer_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        \n    def forward(self, input_ids, attention_mask, token_type_ids=None, start_positions=None, end_positions=None):\n        transformer_out = self.transformer(\n            input_ids,\n            attention_mask,\n            token_type_ids,\n        )\n        sequence_output = transformer_out['last_hidden_state']  # 'last_hidden_state', 'pooler_output', 'hidden_states'\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n        \n        if start_positions is not None and end_positions is not None:\n            loss = self.loss_fn(start_logits, end_logits, start_positions, end_positions)\n        else:\n            loss = None\n\n        return {\n            'start_logits': start_logits,\n            'end_logits': end_logits,\n            'loss': loss,\n        }\n    \n    def loss_fn(self, start_logits, end_logits, start_positions, end_positions):\n        total_loss = None\n\n        ignored_index = start_logits.size(1)\n        start_positions = start_positions.clamp(0, ignored_index)\n        end_positions = end_positions.clamp(0, ignored_index)\n\n        loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n        start_loss = loss_fct(start_logits, start_positions)\n        end_loss = loss_fct(end_logits, end_positions)\n        total_loss = (start_loss + end_loss) \/ 2\n        \n        return total_loss","fab5036b":"def train_each_lang(exp_no, lang):\n    train_df = pd.read_csv(f'{config.INPUT_DIR}\/train.csv')\n    train_df = train_df.query(f'language==\"{lang}\"').reset_index(drop=True)\n    \n    context_len_bins = pd.qcut(train_df['context'].map(len), config.N_FOLDS, labels=range(config.N_FOLDS)).tolist()\n    skf = StratifiedKFold(n_splits=config.N_FOLDS, shuffle=True, random_state=config.SEED)\n    for f, (t_, v_) in enumerate(skf.split(X=train_df, y=context_len_bins)):\n        train_df.loc[v_, 'kfold'] = f\n\n    for fold in range(config.N_FOLDS):\n        if fold in config.SKIP_FOLD:\n            continue\n        \n        train_dataset = ChAIIDataset(train_df.query(f'kfold!={fold}'), train=True)\n        valid_dataset = ChAIIDataset(train_df.query(f'kfold=={fold}'), train=False)\n\n        set_seed()\n        train_loader = DataLoader(train_dataset, batch_size=config.BS,\n                                  pin_memory=True, shuffle=True, drop_last=True, num_workers=os.cpu_count(),\n                                  worker_init_fn=lambda x: set_seed())\n\n        model = ChaiiModel()\n        model.to(device)\n\n        optimizer = AdamW(model.parameters(), lr=config.LR, weight_decay=config.WEIGHT_DECAY)\n        max_train_steps = config.N_EPOCHS * len(train_loader) \/\/ config.ACCUMULATE\n        warmup_steps = int(max_train_steps * config.WARM_UP_RATIO)\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=warmup_steps,\n            num_training_steps=max_train_steps\n        )\n\n        # wandb\n        uniqe_exp_name = f\"{exp_no}_{lang}_f{fold}\"\n        wandb.init(project='ChAII', entity='trtd56', name=uniqe_exp_name, group=exp_no)\n        wandb_config = wandb.config\n        wandb_config.fold = fold\n        for k, v in dict(vars(config)).items():\n            if k[:2] == \"__\" or k in config.NOT_WATCH_PARAM:\n                continue\n            wandb_config[k] = v\n        wandb.watch(model)\n        os.makedirs(f'{config.OUTPUT_DIR}\/{exp_no}_{lang}', exist_ok=True)\n\n        set_seed()\n        train_iter_loss, valid_best_loss, jaccard_best_score, step_i = 999, 999, 0, 0\n        optimizer.zero_grad()\n        bar = tqdm(total=max_train_steps)\n        bar.set_description(f'{uniqe_exp_name}')\n        for epoch in range(config.N_EPOCHS):\n            for d in train_loader:\n                step_i += 1\n                model.train()\n                with torch.cuda.amp.autocast(): \n                    outputs = model(\n                        d['input_ids'].to(device),\n                        d['attention_mask'].to(device),\n                        d['token_type_ids'].to(device),\n                        start_positions=d['start_positions'].to(device),\n                        end_positions=d['end_positions'].to(device)\n                    )\n                    loss = outputs['loss'] \/ config.ACCUMULATE\n\n                step_lr = np.array([param_group[\"lr\"] for param_group in optimizer.param_groups]).mean()\n                train_iter_loss += loss.item()\n\n                scaler.scale(loss).backward()\n                if step_i % config.ACCUMULATE == 0:\n                    scaler.step(optimizer) \n                    scaler.update() \n                    optimizer.zero_grad()\n                    scheduler.step()\n                    bar.update(1)\n\n                if step_i % config.EVAL_STEP == 0 or step_i == 1 or step_i == len(train_loader):\n                    all_logits, valid_loss_avg = eval_model(model, valid_dataset)\n                    predict_df = post_processing(all_logits, train_df, valid_dataset)\n                    result_df = predict_df.merge(train_df, how='left', on='id')\n                    result_df['jaccard'] = result_df.apply(lambda x: jaccard(x['answer_text'], x['predict_text']), axis=1)\n                    jaccard_score_avg = result_df['jaccard'].mean()\n\n                    if valid_loss_avg < valid_best_loss:\n                        valid_best_loss = valid_loss_avg\n\n                    if jaccard_score_avg > jaccard_best_score:\n                        jaccard_best_score = jaccard_score_avg\n                        torch.save(model.state_dict(), f\"{config.OUTPUT_DIR}\/{exp_no}_{lang}\/chaii_f{fold}_best_jaccard_model.bin\")\n                        result_df.to_csv(f\"{config.OUTPUT_DIR}\/{exp_no}_{lang}\/chaii_f{fold}_best_jaccard_result.csv\", index=None)\n\n                    wandb.log({\n                        \"train_loss\": train_iter_loss \/ config.EVAL_STEP,\n                        \"valid_loss\": valid_loss_avg,\n                        \"valid_best_loss\": valid_best_loss,\n                        \"jaccard_score\": jaccard_score_avg,\n                        \"jaccard_best_score\": jaccard_best_score,\n                        \"learning_rate\": step_lr,\n                    })\n                    train_iter_loss = 0\n\n        wandb.finish()\n        \n        del model, optimizer, scheduler, loss\n        torch.cuda.empty_cache()","93d1840c":"def main():\n    train_each_lang(\"exp0016\", \"tamil\")\n    train_each_lang(\"exp0016\", \"hindi\")","6aaade31":"if __name__ == \"__main__\":\n    main()","0377fd85":"### functions for evaluation","d7ace70d":"set wandb to record the log during training","1f1e2b72":"### Dataloader","d3f56c6a":"### Model","58fd7aa7":"### functions for training with dividing language ","444c9029":"# ChAII train with divided Hindi and Tamil\n\n\nI interest in that whether I have to train with divide Hindi and Tamil. (I have made [discussion thread](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering\/discussion\/264749))\n\nThis notebook is training with dividing Hindi and Tamil.\n\nBefore sharing this code, I have trained with mix Hindi and Tamil.\nThe result is that:\n\n- best validation loss: 0.3081\n- best jaccard score: 0.5141\n- LB score: 0.580 (This is my best.)\n\nIt is the same parameters, fold, and random seed.\n\nI'll check which is better, mix two languages or divide.","a4237cad":"install and update libraries"}}