{"cell_type":{"d798208c":"code","992dcdc7":"code","796cb970":"code","b80e320d":"code","d3d8dc3d":"code","7fbddcdd":"code","58df5d0a":"code","f3253ebe":"code","9adb2a7b":"code","b2d6caac":"code","1a261028":"code","47841c52":"code","3ae42d1b":"code","cad749f2":"code","df34919a":"code","45038c77":"code","c1b3ec89":"code","012a9e68":"code","67c29379":"code","61689d31":"code","e8675b61":"code","0a311598":"code","66657dba":"code","eac5015a":"code","b74443e7":"markdown","dcff1017":"markdown","f7eec57c":"markdown","3128ce96":"markdown"},"source":{"d798208c":"# Importing libraries\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport category_encoders as ce\n\ndf = pd.read_csv(\"..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv\")","992dcdc7":"## clean titles\/target variable\n\n# only use most popular titles\npopular_titles = df['Q6'].value_counts() > 500\npopular_titles = popular_titles[popular_titles == True].index.tolist()\n\ndf = df[df.Q6.isin(popular_titles)]\n\n# remove rows with unlikely job titiles\nindexNames = df[df.Q6.isin({\"Student\", \"Other\", \"Not Employed\"})].index\ndf = df.drop(indexNames)","796cb970":"## clean & transform input variables\n\n# remove rows w\/out salary info\nindexNames = df[-df.Q9.str.contains('0', regex=True, na=False)].index\ndf = df.drop(indexNames)\n\n## split out predictors & target variable\n\n# all the questions we'd like to use: Q10+ are \n# multipart & encoded sepeartely\nother_variables = {\"Q6\", \"Q4\", \"Q7\", \"Q8\", \"Q9\", \n                 \"Q10\"}\ndf_subsample = df.filter(other_variables)\n\n# split into predictor & target variables\nX = df_subsample.drop(\"Q6\", axis=1)\ny = df_subsample[\"Q6\"]","b80e320d":"#df.head()\n\n# Q6 = job title\n# Q4 = level of education (lab encoder)\n# Q7 = industry employment (one hot)\n# Q8 = years of experience (make numeric)\n# Q9 = comp, want to drop \"I do not wish...\" (make numeric)\n# Q10 = degree to which business uses ML (lab encoder)\n# Q11 ... = all parts to 1-hot, drop other_text\n# Q12 ... = tools used in job (one hot)\n# Q15 ... = cloud services (one hot)\n# Q16 ... = programming languages (one hot)\n# Q19 ...  = deep learning frameworks (one hot)","d3d8dc3d":"# mulitple choice questions requiring one-hot encoding\none_hot_qs = df[df.columns[df.nunique() <= 1]].filter(regex=\"Q11\")\n\nfor i in {\"Q12\", \"Q15\", \"Q16\", \"Q19\"}:\n    new_data = df[df.columns[df.nunique() <= 1]].filter(regex=i)\n    \n    one_hot_qs = pd.concat([one_hot_qs.reset_index(drop=True), \n                            new_data.reset_index(drop=True)], \n                           axis=1)\n    \n# Q7 = industry employment (one hot)\nencoder = ce.OneHotEncoder()\n\none_hot_qs = encoder.fit_transform(one_hot_qs, y)","7fbddcdd":"# label encoding\nencoder = ce.OrdinalEncoder(cols=[\"Q4\",\"Q8\",\"Q9\",\"Q10\"])\n\nencoder.fit(X, y)\nX_cleaned = encoder.transform(X)\n\n# one hot encoding\nencoder = ce.OneHotEncoder(cols=[\"Q7\"])\n\nX_cleaned = encoder.fit_transform(X_cleaned, y)\nX_cleaned = pd.concat([one_hot_qs.reset_index(drop=True), \n                        X_cleaned.reset_index(drop=True)], \n                       axis=1)\n\n# encode target variable\nencoder = ce.OrdinalEncoder()\ny_encoded = encoder.fit_transform(y)","58df5d0a":"# Splitting data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X_cleaned, y,\n                                                    train_size=0.80, test_size=0.20)\n\n# vesion w\/ y numerically encoded\nX_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_cleaned, y_encoded,\n                                                    train_size=0.80, test_size=0.20)\n","f3253ebe":"# look at classes & imbalances\ny.value_counts()","9adb2a7b":"from xgboost import XGBClassifier\n\nmy_model = XGBClassifier()\n# Add silent=True to avoid printing out updates with each cycle\nmy_model.fit(X_train, y_train, verbose=False)","b2d6caac":"from sklearn.metrics import auc, accuracy_score, confusion_matrix\n\n# make predictions\nxgb_predictions = my_model.predict(X_test)\n\n# accuracy... but this is an imbalanced multi-class prob\nprint(accuracy_score(y_test, xgb_predictions))","1a261028":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    fig.set_figheight(15)\n    fig.set_figwidth(15)\n    return ax","47841c52":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(xgb_predictions, y_test, \n                      classes=unique_labels(y_test),\n                      normalize=True,\n                      title='Confusion matrix, without normalization')","3ae42d1b":"from xgboost import plot_importance\n\n# plot feature importance\nplt.rcParams[\"figure.figsize\"] = (14, 14)\n\nplot_importance(my_model)\n\n# Q4 = level of education (lab encoder)\n# Q7 = industry employment (one hot)\n# Q8 = years of experience (make numeric)\n# Q9 = comp, want to drop \"I do not wish...\" (make numeric)\n# Q10 = degree to which business uses ML (lab encoder)\n# Q11 ... = role at work (1 = Analyze and understand data to influence product or business decisions,\n# 5 = Do research that advances the state of the art of machine learning)\n# Q12 ... = tools used in job (one hot)\n# Q15 ... = cloud services (one hot)\n# Q16 ... = programming languages (one hot)\n# Q19 ... = deep learning frameworks (one hot)","cad749f2":"from tpot import TPOTClassifier\n\ntpot = TPOTClassifier(generations=8, population_size=20, \n                      verbosity=2, early_stop=2)\ntpot.fit(X_train_encoded, y_train_encoded)\n\nprint(\"Accuracy is {}%\".format(tpot.score(X_test_encoded, y_test_encoded)*100))","df34919a":"tpot.export('tpot_pipeline.py')","45038c77":"!cat tpot_pipeline.py","c1b3ec89":"tpot_predictions = tpot.predict(X_test_encoded)\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(tpot_predictions, y_test_encoded[\"Q6\"], \n                      classes=unique_labels(y_test),\n                      normalize=True,\n                      title='Confusion matrix, without normalization')","012a9e68":"import h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init()","67c29379":"# convert data to h20Frame\ntrain_data = h2o.H2OFrame(X_train)\ntest_data = h2o.H2OFrame(list(y_train))\n\ntrain_data = train_data.cbind(test_data)\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=10, seed=1)\n# y is the name of the H20 frame with the \naml.train(y=\"C1\", training_frame=train_data)","61689d31":"# View the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)  # Print all rows instead of default (10 rows)\n\n# The leader model is stored here\naml.leader","e8675b61":"# convert data to h20Frame\ntrain_data = h2o.H2OFrame(X_test)\ntest_data = h2o.H2OFrame(list(y_test))\n\ntesting_data = train_data.cbind(test_data)\n\nh2o_predictions = aml.predict(testing_data)","0a311598":"h2o_predictions","66657dba":"h2o_predictions = h2o_predictions[\"predict\"].as_data_frame()\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(h2o_predictions, y_test, \n                      classes=unique_labels(y_test),\n                      normalize=True,\n                      title='Confusion matrix, without normalization')","eac5015a":"print(\"Vanilla XGBoost:\")\nprint( (y_test, xgb_predictions))\nprint(\"TPOT:\")\nprint(accuracy_score(y_test_encoded, tpot_predictions))\nprint(\"H20 AutoML:\")\nprint(accuracy_score(y_test, h2o_predictions))","b74443e7":"# Vanilla XGBoost","dcff1017":"# Overall accuracy","f7eec57c":"# TPOT\n\nDocumentation: https:\/\/epistasislab.github.io\/tpot\/","3128ce96":"# H20.ai AutoML\n\nDocumentation: http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html"}}