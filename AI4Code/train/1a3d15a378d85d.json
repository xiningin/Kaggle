{"cell_type":{"afea4a8c":"code","56e517d4":"code","15a64a97":"code","0891e534":"code","56e6c8c9":"code","ccc94de4":"code","5004931f":"code","62818c39":"code","886f89ab":"code","a4ed1248":"code","41e1dc5d":"code","5fafc139":"code","e554d373":"code","dbb8cc73":"code","5480d8d0":"code","2bb5d741":"code","5edac0e7":"code","723912b9":"code","f681177f":"code","76bfe65f":"code","4d94d7d1":"code","340be936":"code","1f4d8e44":"code","e6c62451":"code","b90564b7":"code","a24e3962":"code","6da5ee0a":"code","d54b768b":"code","34e6f7fc":"code","3786c8af":"code","28d50e84":"code","9d65a6d4":"code","9e35caec":"code","0f1db491":"code","f2ac8631":"code","f16dfc51":"code","f426da4c":"code","54524de7":"code","e511be9b":"code","de0d1bc8":"markdown","8be8c4ce":"markdown","4ae765bd":"markdown","d27ae9e7":"markdown","0ba5abc8":"markdown","f0bfb927":"markdown","e74864c6":"markdown","7440b077":"markdown"},"source":{"afea4a8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","56e517d4":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression","15a64a97":"df=pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf","0891e534":"df.info()","56e6c8c9":"df.isnull().sum()","ccc94de4":"df[\"quality\"].value_counts()","5004931f":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfor i in range(0,len(df)):\n    if df[\"quality\"][i]>6.5:\n        df[\"quality\"][i]=1\n    else:\n        df[\"quality\"][i]=0","62818c39":"df[\"quality\"].value_counts()","886f89ab":"df.hist(bins=20,figsize=(16,10))\nplt.show()","a4ed1248":"plt.figure(figsize=(12,8))\ncorr=df.corr()\nsns.heatmap(corr,annot=True)","41e1dc5d":"sns.pairplot(df,hue=\"quality\")\nplt.show()","5fafc139":"df.loc[df.duplicated()]","e554d373":"df.drop_duplicates()","dbb8cc73":"df[\"quality\"].value_counts()","5480d8d0":"X=df.iloc[:,:-1]\ny=df.iloc[:,-1]\nsmote=SMOTE(k_neighbors=4)\nX,y=smote.fit_resample(X,y)","2bb5d741":"X","5edac0e7":"y.value_counts()","723912b9":"sc=StandardScaler()\nX=sc.fit_transform(X)","f681177f":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=100)","76bfe65f":"model=LogisticRegression()\nmodel.fit(X_train,y_train)\npred=model.predict(X_test)\nacc_score=accuracy_score(y_test,pred)\nprint(\"accuracy_score = \",acc_score)","4d94d7d1":"cm=confusion_matrix(y_test,pred)\nprint(\"confusion matrix of Logistic Regression model \\n\",cm)","340be936":"cls=classification_report(y_test,pred)\nprint(\"classification_report \\n\",cls)","1f4d8e44":"fold=KFold(n_splits=10,shuffle=True,random_state=20)\nscore=cross_val_score(model,X,y,cv=fold)\n#print(\"10 KFold scores \\n\",score)\nmean=np.array(score).mean()\nprint(\"K-fold of Logistic regression model \\n\",mean)","e6c62451":"ada=AdaBoostClassifier()\nada.fit(X_train,y_train)\npred=ada.predict(X_test)\nacc_score=accuracy_score(y_test,pred)\nprint(acc_score)","b90564b7":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,pred)\nprint(\"confusion matrix of AdaBoostClassifier model \\n\",cm)","a24e3962":"cls=classification_report(y_test,pred)\nprint(\"classification_report \\n\",cls)","6da5ee0a":"fold=KFold(n_splits=10,shuffle=True,random_state=20)\nscore=cross_val_score(ada,X,y,cv=fold)\n#print(\"10 KFold scores \\n\",score)\nmean=np.array(score).mean()\nprint(\"K-Fold of AdaBoostClasifier model \\n\",mean)","d54b768b":"gbc=GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\npred=gbc.predict(X_test)\nacc_score=accuracy_score(y_test,pred)\nprint(acc_score)","34e6f7fc":"cm=confusion_matrix(y_test,pred)\nprint(\"confusion matrix of GradientBoostingClassifier model \\n\",cm)","3786c8af":"cls=classification_report(y_test,pred)\nprint(\"classification_report \\n\",cls)","28d50e84":"fold=KFold(n_splits=10,shuffle=True,random_state=20)\nscore=cross_val_score(gbc,X,y,cv=fold)\n#print(score)\nmean=np.array(score).mean()\nprint(\"K-Fold of GradientBoostingClassifier model\\n\",mean)","9d65a6d4":"rfc=RandomForestClassifier()\nrfc.fit(X_train,y_train)\npred=rfc.predict(X_test)\nacc_score=accuracy_score(y_test,pred)\nprint(acc_score)","9e35caec":"cm=confusion_matrix(y_test,pred)\nprint(\"confusion matrix of RandomForestClassifier model \\n\",cm)","0f1db491":"cls=classification_report(y_test,pred)\nprint(\"classification_report \\n\",cls)","f2ac8631":"fold=KFold(n_splits=10,shuffle=True,random_state=20)\nscore=cross_val_score(rfc,X,y,cv=fold)\n#print(score)\nmean=np.array(score).mean()\nprint(\"K-Fold of RandomForestClassifier model \\n\",mean)","f16dfc51":"dc=DecisionTreeClassifier()\ndc.fit(X_train,y_train)\npred=dc.predict(X_test)\nacc_score=accuracy_score(y_test,pred)\nprint(acc_score)","f426da4c":"cm=confusion_matrix(y_test,pred)\nprint(\"confusion matrix of DecisionTreeClassifier model \\n\",cm)","54524de7":"cls=classification_report(y_test,pred)\nprint(\"classification_report \\n\",cls)","e511be9b":"fold=KFold(n_splits=10,shuffle=True,random_state=20)\nscore=cross_val_score(rfc,X,y,cv=fold)\n#print(score)\nmean=np.array(score).mean()\nprint(\"K-Fold of DecisionTreeClassifier model \\n\",mean)","de0d1bc8":"**Models  ----------------  Accuracy**\n* **1. Logistic Regression------- 80.96%**\n* **2. AdaBoostClassifier ------- 86.28%**\n* **3. GradientBoostingClassifier ------- 90.15%**\n* **4. RandomForestClassifier ------- 94.68%**\n* **5. DecisionTreeClassifier ------- 94.60%**","8be8c4ce":"**We need to check if their are any duplicates. If we found then we need to drop them**","4ae765bd":"**If You Guys Liked or found something Interesting and learn't something new.**\n# please upvote my work","d27ae9e7":"**We are now done with the scaling part. Now we can train the model and start testing.**","0ba5abc8":"**The above data are duplicates. So we need to drop them if not they may impact our analysis**","f0bfb927":"**Now, our data set is balanced. So we are ready with our data and now we can start the process.**","e74864c6":"**So, As i told you before this is an imbalanced dataset. We need to do some kind of sampling to get good prediction.**","7440b077":"**So we had removed all the duplicates i.e, around 240 rows.**"}}