{"cell_type":{"7fd515bd":"code","13c6e4a0":"code","7f24ef56":"code","78e844c4":"code","fab711cd":"code","75625a6b":"code","447fd790":"code","5491fba3":"code","562511dd":"code","d9e34bbc":"code","91ef9179":"code","8d610145":"code","fe080581":"code","abef0204":"code","46ebfd82":"code","50258015":"code","111f9fc6":"code","585d05e0":"code","1773b71f":"code","884704c6":"code","891fd70c":"code","1d38fed2":"code","fa26938d":"code","ca0560c7":"code","e468862c":"code","a4d5c60e":"code","eda649c0":"code","fb5d1d39":"code","c6b4d7be":"code","7b845cae":"code","fa0b54cc":"code","0fb6e980":"code","05456912":"code","1ed0f098":"code","6bff08fd":"code","05b64d2b":"code","52a81127":"code","d85da479":"code","ed177396":"code","57964045":"code","e1898d85":"code","b098dd5e":"code","a0a6b670":"code","40f41e9a":"code","a3f9f542":"code","b0d28bcb":"code","66ff40ec":"code","ad7808c5":"code","0e0dfb38":"code","ae149fa7":"code","345b902e":"code","baa904be":"code","36c3da82":"code","1b79c63b":"markdown","e3cfcaff":"markdown","a48cad27":"markdown","41fbb2c1":"markdown","5f629ef8":"markdown","59097e4e":"markdown","f1900ddb":"markdown","b2580ea6":"markdown","3eda2bd3":"markdown","2b1a9e78":"markdown","3b652c9b":"markdown","a6d6e391":"markdown","afe9badf":"markdown","f5550c7b":"markdown","48ae8bcc":"markdown","05712d8a":"markdown","53998a59":"markdown","d3c98dbc":"markdown","b4598e68":"markdown","731bc3fe":"markdown","431a95a9":"markdown","4ef6b6bd":"markdown","e982b6ed":"markdown","fe8602e7":"markdown","6e89b1b1":"markdown","404f636e":"markdown","d806a5ef":"markdown","c5ece147":"markdown","e50be0c3":"markdown","ccfeeae0":"markdown","5d43a7cf":"markdown","4106f835":"markdown","222fd3ea":"markdown","65a6f57a":"markdown","a9b70cf8":"markdown","b9e755ec":"markdown","588e8139":"markdown","d8567ee7":"markdown","1b2115d2":"markdown","258c1e04":"markdown","05cf742b":"markdown","a335a8e2":"markdown","905b1a49":"markdown","9ce3ea58":"markdown","9d11e474":"markdown","cd2802ad":"markdown","9afc5efd":"markdown","de7814f2":"markdown","26539ee1":"markdown","dae5f275":"markdown"},"source":{"7fd515bd":"import numpy as np\nimport pandas as pa\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","13c6e4a0":"data = pa.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndata.info()","7f24ef56":"data.describe()","78e844c4":"sn.countplot(data['Outcome'])\nplt.title('Diabetes Count')\n## It's quite imbalanced  ( U can apply SMOTE to resample it but here I will be not resampling it)","fab711cd":"from sklearn.preprocessing import Imputer\noutcome = data['Outcome']\nfill_values = Imputer(missing_values=0,strategy=\"mean\",axis=0) # Replace missing values with mean\ndata = fill_values.fit_transform(data)\n\ndata = pa.DataFrame({'Pregnancies': data[:, 0], 'Glucose': data[:, 1],'BloodPressure':data[:, 2],\n                        'SkinThickness':data[:, 3],'Insulin':data[:, 4],'BMI':data[:, 5],'DiabetesPedigreeFunction':data[:, 6],\n                        'Age':data[:, 7],'Outcome':outcome})","75625a6b":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['Glucose'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='Glucose',data=data).set_title('Box Plot ')","447fd790":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['BloodPressure'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='BloodPressure',data=data).set_title('Box Plot ')","5491fba3":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['Insulin'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='Insulin',data=data).set_title('Box Plot ')","562511dd":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['Age'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='Age',data=data).set_title('Box Plot ')","d9e34bbc":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['BMI'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='BMI',data=data).set_title('Box Plot ')","91ef9179":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\nsn.distplot(data['Pregnancies'],ax=axis1).set_title('Probabilty density Function')\nsn.boxplot(x='Outcome',y='Pregnancies',data=data).set_title('Box Plot ')","8d610145":"y = data['Outcome']\ndata_x = data\ndata_x=data_x.drop('Outcome',axis=1)\n\ndata_n_2 = (data_x - data_x.mean())\/(data_x.std())\ndataz = pa.concat([y,data_n_2.iloc[:,0:9]],axis=1)\n\ndataz = pa.melt(dataz,id_vars=\"Outcome\",\n                    var_name=\"features\",\n                    value_name='value')\n#dataz.drop(dataz[dataz.features == 0].index,inplace=True)\n\nplt.figure(figsize=(10,10))\nsn.violinplot(x=\"features\", y=\"value\", hue=\"Outcome\", data=dataz,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","fe080581":"plt.figure(figsize=(20,8))\nsn.violinplot(x='Pregnancies',y='Age',hue='Outcome',data=data)\nplt.xticks(rotation=90)\n## Pregnencies + Age + Outcome\nplt.figure(figsize=(20,8))\nsn.boxplot(x='Pregnancies',y='Age',hue='Outcome',data=data)\nplt.xticks(rotation=90)","abef0204":"plt.figure(figsize=(20,8))\nsn.boxplot(x='Pregnancies',y='BloodPressure',data=data)\nplt.xticks(rotation=90)","46ebfd82":"sn.jointplot(data['BloodPressure'],data['Age'],kind='regg',color=\"#ce1414\")","50258015":"fig = plt.figure(figsize=(20, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\nsn.boxplot(x='Age',y='BloodPressure',data=data,ax=ax_main)\nsn.boxplot(data['BloodPressure'],ax=ax_right)\nsn.boxplot(data['Age'],ax=ax_bottom)","111f9fc6":"fig = plt.figure(figsize=(20, 10), dpi= 80)\ngrid = plt.GridSpec(4, 4, hspace=0.5, wspace=0.2)\n\nax_main = fig.add_subplot(grid[:-1, :-1])\nax_right = fig.add_subplot(grid[:-1, -1], xticklabels=[], yticklabels=[])\nax_bottom = fig.add_subplot(grid[-1, 0:-1], xticklabels=[], yticklabels=[])\n\nsn.boxplot(x='BloodPressure',y='Insulin',data=data,ax=ax_main)\nsn.boxplot(data['Insulin'],ax=ax_right)\nsn.boxplot(data['BloodPressure'],ax=ax_bottom)","585d05e0":"sn.jointplot(data['BMI'],data['SkinThickness'],kind='regg',color=\"#ce1414\")","1773b71f":"features = list(data.columns)\nfeatures.remove('Outcome')","884704c6":"plt.figure(figsize=(18,10), dpi= 80)\nsn.pairplot(data,kind='scatter',hue='Outcome')","891fd70c":"sn.set(style='whitegrid',palette='muted')\ny = data['Outcome']\ndata_x = data\ndata_x=data_x.drop('Outcome',axis=1)\n\ndata_n_2 = (data_x - data_x.mean())\/(data_x.std())\ndataz = pa.concat([y,data_n_2.iloc[:,0:9]],axis=1)\n\ndataz = pa.melt(dataz,id_vars=\"Outcome\",\n                    var_name=\"features\",\n                    value_name='value')\n\nplt.figure(figsize=(15,9))\nsn.swarmplot(x='features',y='value',hue='Outcome',data=dataz)\nplt.xticks(rotation=90)","1d38fed2":"plt.figure(figsize=(10,7))\nsn.heatmap(data.corr(),annot=True,linewidths=.5, fmt= '.2f',cmap='BuPu')","fa26938d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.model_selection import train_test_split","ca0560c7":"radm_state = 42","e468862c":"train = data[features][0:760]\ny = data['Outcome'][0:760]\ntest = data[features][760:]\nactual_test = data['Outcome'][760:]","a4d5c60e":"X_train,X_test,Y_train,Y_test = train_test_split(train,y,random_state=radm_state,test_size=0.2)","eda649c0":"radm_classifier= RandomForestClassifier(random_state=42)\nradm_model = radm_classifier.fit(X_train,Y_train)\n\naccuracy = accuracy_score(Y_test,radm_classifier.predict(X_test))\nprint(\"Accuracy{}\".format(accuracy))\n\ncm = confusion_matrix(Y_test,radm_classifier.predict(X_test))\nsn.heatmap(cm,annot=True,fmt=\"d\")","fb5d1d39":"from pdpbox import pdp, info_plots\npdp_ = pdp.pdp_isolate(\n    model=radm_model, dataset=X_train, model_features=features, feature='Glucose'\n)\nfig, axes = pdp.pdp_plot(\n    pdp_isolate_out=pdp_, feature_name='Glucose', center=True, \n     plot_lines=True, frac_to_plot=100\n)","c6b4d7be":"import shap\nX_train = X_train.reset_index(drop=True)\nestimator = radm_model\nshap_explain = shap.TreeExplainer(estimator)\nshap_values = shap_explain.shap_values(X_train.iloc[589])\n\nshap.initjs()\nshap.force_plot(shap_explain.expected_value[1], shap_values[1],X_train.iloc[589])","7b845cae":"select_feature = SelectKBest(chi2,k=5).fit(X_train,Y_train)","fa0b54cc":"#print(select_feature.scores_)\n#print(X_train.columns)\nx_train_2 = select_feature.transform(X_train)\nx_test_2 = select_feature.transform(X_test)\n\nradm_classifier= RandomForestClassifier(random_state=42)\nradm_model = radm_classifier.fit(x_train_2,Y_train)","0fb6e980":"accuracy = accuracy_score(Y_test,radm_classifier.predict(x_test_2))\nprint(\"Accuracy{}\".format(accuracy))\n\ncm = confusion_matrix(Y_test,radm_classifier.predict(x_test_2))\nsn.heatmap(cm,annot=True,fmt=\"d\")","05456912":"from sklearn.feature_selection import RFECV","1ed0f098":"random_classifer = RandomForestClassifier(random_state=42)\nrfecv = RFECV(estimator=random_classifer,step=1,cv=5,scoring='accuracy')\n\nrfcev_model = rfecv.fit(X_train,Y_train)\n\nprint(\"Optimal No. of features:\",rfcev_model.n_features_)\nprint(\"Best Features:\",X_train.columns[rfcev_model.support_])","6bff08fd":"optimal_features =['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'BMI',\n       'DiabetesPedigreeFunction', 'Age']\n\nradm_model_test = random_classifer.fit(X_train[optimal_features],Y_train)\naccuracy = accuracy_score(Y_test,random_classifer.predict(X_test[optimal_features]))\nprint(\"Accuracy{}\".format(accuracy))\ncm = confusion_matrix(Y_test,random_classifer.predict(X_test[optimal_features]))\nsn.heatmap(cm,annot=True,fmt=\"d\")","05b64d2b":"# Plot number of features VS. cross-validation scores\nimport matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfcev_model.grid_scores_) + 1), rfcev_model.grid_scores_)\nplt.show()","52a81127":"clf_rf_5 = RandomForestClassifier(random_state=42)      \nclr_rf_5 = clf_rf_5.fit(X_train,Y_train)\nimportances = clr_rf_5.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf_rf_5.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n\nplt.figure(1, figsize=(14, 13))\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n       color=\"g\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices],rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","d85da479":"from sklearn.preprocessing import StandardScaler\nstandard_scaler = StandardScaler()\ntrain = standard_scaler.fit_transform(train[optimal_features])\ntest = standard_scaler.transform(test[optimal_features])","ed177396":"from sklearn.model_selection import cross_val_score,StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn import metrics","57964045":"X_train,X_test,Y_train,Y_test = train_test_split(train,y,test_size=0.2,random_state=42)\ncv = StratifiedShuffleSplit(n_splits=2,test_size=0.2,random_state=42)\n\nscoring = 'roc_auc'","e1898d85":"model_scores = pa.DataFrame(columns=['Name','Best Parameters','Best Score','Test Score','CV Mean','CV Std'])","b098dd5e":"color = sn.color_palette()\nsn.set_style('darkgrid')","a0a6b670":"def helper_function(name,model):\n    global model_scores\n    \n    model_lf = model.best_estimator_.fit(X_train,Y_train)\n    scores = cross_val_score(model.best_estimator_,X_train,Y_train,cv=5,scoring=scoring,verbose=0)\n    \n    cross_mean  = scores.mean()\n    cross_std = scores.std()\n    \n    test_score = model.score(X_test,Y_test)\n    \n    model_scores = model_scores.append({'Name':name,'Best Parameters':model.best_params_,\n                                        'Best Score':model.best_score_,'Test Score':test_score,\n                                        'CV Mean':cross_mean,'CV Std':cross_std },ignore_index=True)\n    \n    fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,6))\n    \n    \n    ## Draw Confusion Matrix.\n    \n    \n    predicted_value = model.best_estimator_.predict(X_test)\n    cm = metrics.confusion_matrix(Y_test,predicted_value)\n    sn.heatmap(cm,annot=True,fmt=\".2f\",cmap='BuPu',ax=axis1).set_title(\"Confusion Matrix\")\n    \n    ## Draw Roc Curve\n    \n    test_results_df = pa.DataFrame({'actual':Y_test})\n    test_results_df = test_results_df.reset_index()\n    \n    predict_probabilites = pa.DataFrame(model.best_estimator_.predict_proba(X_test))\n    test_results_df['chd_1'] = predict_probabilites.iloc[:,1:2]\n    \n    fpr,tpr,thresholds = metrics.roc_curve(test_results_df.actual,test_results_df.chd_1,drop_intermediate=False)\n    \n    auc_score = metrics.roc_auc_score(test_results_df.actual,test_results_df.chd_1)\n    \n    plt.plot(fpr,tpr,label=\"ROC Curve (area = %.2f)\"% auc_score)\n    plt.plot([0,1],[0,1],'k--')\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0,1.05])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc='lower right')\n    \n    ## print classification rreport\n    \n    print(metrics.classification_report(Y_test,predicted_value))\n    pass","40f41e9a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier,RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nimport lightgbm as lgm","a3f9f542":"logit_model = LogisticRegression()\n\nparam_grid = {'C':[0.001,0.01,0.05,1,100],'penalty':['l1','l2']}\nlogit_grid = GridSearchCV(logit_model,param_grid,cv=cv,scoring=scoring)\nlogit_grid.fit(X_train,Y_train)\nhelper_function(\"Logistic Regression\",logit_grid)","b0d28bcb":"param_grid = {'n_neighbors':[x for x in range(1,40)],'weights':['uniform','distance']}\n\nknn_model =  KNeighborsClassifier()\n\nknn_grid  = RandomizedSearchCV(knn_model,param_grid,cv=cv,scoring=scoring,random_state=42)\nknn_grid.fit(X_train,Y_train)\nhelper_function(\"K-Nearest-Neighbors\",knn_grid)","66ff40ec":"n_estimators = [int(x) for x in np.linspace(start=200,stop=2000,num=10)] #Boosting parameters\nmax_features = ['auto', 'sqrt']# Boosting Parameters\nmax_depth = [int(x) for x in np.linspace(10,200,num=20)] #Max depth of the tree\nmax_depth.append(None)\nbootstrap = [True,False] # Bootstrap here means how the samples will be chosen with or without replacement\n\n# Total Combination 10*2*20*2 = 800 !\n\nparam_grid = {'n_estimators':n_estimators,\n              'max_features':max_features,\n              'max_depth':max_depth,\n              'bootstrap':bootstrap}\n\nrandom_model = RandomForestClassifier()\ngrid_random = RandomizedSearchCV(radm_model,param_grid,cv=cv,scoring=scoring,n_iter=100,random_state=42)\n\ngrid_random.fit(X_train,Y_train)\nhelper_function(\"RADNOM FOREST\",grid_random)","ad7808c5":"ada_model = AdaBoostClassifier()\n\nparam_grid = {'n_estimators':[int(x) for x in np.linspace(start=20,stop=300,num=15)],\n              'learning_rate':np.arange(.1,4,.3)}\n\nada_grid = RandomizedSearchCV(ada_model,param_grid,cv=cv,scoring=scoring,n_iter=100,random_state=42)\n\nada_grid.fit(X_train,Y_train)\nhelper_function(\"ADA Boost\",ada_grid)","0e0dfb38":"n_estimators = [int(x) for x in np.linspace(start=20,stop=120,num=6)]\nlearning_rate = [0.1,0.01,0.05,0.001]\nmax_depth= np.arange(2,5,1)\n\n\nparam_grid = {'n_estimators':n_estimators,'learning_rate':learning_rate,'max_depth':max_depth}\n\ngrad_model = GradientBoostingClassifier()\n\ngrid_grad = GridSearchCV(grad_model,param_grid,cv=cv,scoring=scoring)\ngrid_grad.fit(X_train,Y_train)\n\nhelper_function(\"Gradient Boosting\",grid_grad)","ae149fa7":"from xgboost.sklearn import XGBClassifier\n\nparam_grid = {'max_depth':range(3,8,2),'min_child_weight':range(1,10,2),'gamma':[0.5,1,1.5,2,5],\n              'subsample':[0.6,0.8,1.0],'colsample_bytree':[0.6,0.8,1.0]}\n\nxgboost_model = XGBClassifier(learning_rate=0.025,n_estimators=600,objective='binary:logistic',silent=True,nthread=1)\nxgboost_grid = RandomizedSearchCV(xgboost_model,param_grid,cv=cv,scoring=scoring,n_iter=100,random_state=42)\nxgboost_grid.fit(X_train,Y_train)\nhelper_function(\"XGBOOST\",xgboost_grid)","345b902e":"plt.figure(figsize=(12,6))\nsn.barplot(x='Name',y='CV Mean',data=model_scores)","baa904be":"#### Traiing the XGBoost  Model\n\nxtreme_gradient_boost_model =xgboost_grid.best_estimator_\nxtreme_gradient_boost_model.fit(X_train,Y_train)\n\nprint(metrics.classification_report(Y_train,xtreme_gradient_boost_model.predict(X_train)))","36c3da82":"print(metrics.classification_report(actual_test,xtreme_gradient_boost_model.predict(test)))","1b79c63b":"##### Pregnencies + BloodPressure","e3cfcaff":"##### Observation from the above plots\nWe can see that glucose is quite symmetric to normal distribution and average people having a glucose level of 100 to 120 & we also noticed that the person with a high glucose level have more likely to have diabetes.\n\nAverage blood pressure level of perople is around 70, and if we notice at the box plot, bloodpressure has not a significat realtion with 'Outcome'.But the preson with a high blood pressure i.e hypertension is more likely to have diabetes as high blood pressure can damage kidneys,eye and other body oragans.\n\nNext from feature insulin we noticed it's totally right skewed (later we we normalize it) and it can be a important feature(We will check it later in feature selection).\n\nAge is aslo right skewed but from box plot we can notice that it's perfomring well in classification,old age person are more likely to have diabetes.\n\nWomans having high BMI are more likely to have diabetes. Pregrancies is also doing a great job in classification the chance of getting a diabetes increases as no of pregnancies increases. ","a48cad27":"#### Checking the Outcome whether it's balanced or not","41fbb2c1":"###### Swarm Plot For identifying importance of features whether the feature is important for classification","5f629ef8":"##### Plotting Violin Plot ","59097e4e":"### Import Libraries","f1900ddb":"### Feature Selection-(Univariate Feature Selection, Recursive Feature Elimination, Tree based Feature selection )","b2580ea6":"##### Gradient Boosting<a id='12'><\/a>","3eda2bd3":"##### Insulin","2b1a9e78":"#### Prediction<a id='15'><\/a>","3b652c9b":"##### Age\n","a6d6e391":"##### Logistic Regression<a id='8'><\/a>","afe9badf":"Here from feature Blood pressure we can see that the median of this feature is seperated so it can be imporntant for classification\nand also same for Pregnancies and Glucose.","f5550c7b":"Glucose BMI DiabetesPedigreeFunction and Age are the 4 best features(70% variance is explained by this 4 features).","48ae8bcc":"#### Recursive feature Elimination with cross validation and random forest classification<a id='6'><\/a>","05712d8a":"##### Random Forest<a id='10'><\/a>","53998a59":"Glucose,Pregnancies,BMI,Age has a strong corelation with Outcome (Target Variable) and there is no probelm of multicolinearity.","d3c98dbc":"##### BMI","b4598e68":"1.Logistic Regression                                                                                          \n2.KNN                                                                                                              \n3.Random Forest                                                                                             \n4.AdaBoost                                                                                             \n5.Gradient Boosting                                                                                             \n6.XGBoost                                                                                                          ","731bc3fe":"### SHAP<a id='4'><\/a> \nSHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.Check the documentation here https:\/\/shap.readthedocs.io\/en\/latest\/\n\n![](http:\/\/shap.readthedocs.io\/en\/latest\/_images\/shap_diagram.png)","431a95a9":"#### Scaling<a id='6'><\/a>\n\n##### Here I will be applying standarization to starndarize the data \nHere I am doing Standarization because parametric models like linear regression,logistic regression, KNN,requires the data to be normalize else it will perfom very badly.Whereas in ensemble models it does not require the data to be standarize, if standarized then also it's not a problem.","4ef6b6bd":"#### Partial Dependence Plots<a id='3'><\/a>\n\nThe partial dependence plot (short PDP or PD plot) shows the marginal effect one or two features have on the predicted outcome of a machine learning model. A partial dependence plot can show whether the relationship between the target and a feature is linear, monotonic or more complex.","e982b6ed":"##### Pregnencies + Age + Outcome","fe8602e7":"After applying our model on 5 best features the accuracy is more or less same","6e89b1b1":"#### Compare Models<a id='14'><\/a>","404f636e":"Random forest model to see the accuracy score becuause we will need this score later to comapre how our model performs after feature selection. So,here this model is able to give 71% accuracy.","d806a5ef":"I used swarm plot to identify which feature is performing well in classification.Glucose is really a important feature as in the swarm plot we can see that it is seperating both the class(0 \/ 1).Other feature like SkinThickness,BloodPressure etc are not doing the classification well., I am not saying the other feature is important I will later use some feature seletion techniques for indentifying the best features. ","c5ece147":"#### Modelling","e50be0c3":"##### KNN<a id='9'><\/a>","ccfeeae0":"Here we can see that all the models are predicting the person who is not diabetic much better than the person who is diabetic, so this can be a problem because of our chosen cut-of 0.5. In this problem, we are more likely to give more preference on predicting the person who has diabetic correctly this problem can be solved by using a Cost-Based Approach just we have to give more penalty to false positive. As the cost of false negatives and false positives is not the same, the optimal classification cut-off probability can also be determined using the cost-based approach, which finds the cut-off where the total cost is minimum. In the cost-based approach, we assign penalty cost for misclassification of positive and negative and find the total cost for a cut-off probability.\n\nThe performance of all the models can be much more improved by tuning the parameters. \n\nHere, I am going to choose  XGBoost over all the models as it performing better in classifying diabetic persons, and the cross-validation mean is also fairly good.","5d43a7cf":"#### Missing Values <a id='2'><\/a>","4106f835":"After training our model with that 7 features it is giving a accuracy of 73.68% which is slighlty more than previous one.","222fd3ea":"##### Blood Pressure","65a6f57a":"# <font color='blue' size=5 >Diabetes Prediction<\/font>\n\n![](http:\/\/www.news-medical.net\/image.axd?picture=2018%2f3%2fshutterstock_665792917.jpg&ts=20180302122441&ri=674)\n\n#### This Notebook Contains (In a nutshell) -\n\n1- **EDA: **                                                                                                            \n   -> [Descriptive Analytics](#1)                                                                                                                        \n   -> [Missing Values](#2)\n\n2- [Partial Dependence Plots](#3)                                                                                                                               \n3- [SHAP](#4)\n\n4- - **Feature Selection: **                                                                                                                      \n    ->[Univariate Feature Selection](#5)                                                                                                             \n    ->[Recursive Feature Elimination with cross vaidation](#6)                                                                      \n    ->[Feature Importance](#7)                                                                                                                                                                                  \n    \n5- **ML Models: **                                                                                                                                                  \n         ->[LR](#8)                                                                                                                                                \n         ->[KNN](#9)  \n         ->[Random Forest](#10)                                                                                                                                     \n         ->[AdaBoost](#11)                                                                                                                                           \n         ->[Gradient Boosting](#12)   \n         ->[XGBoost](#13)\n\n6- [Compare Models](#14)                                                                                                                                                       \n7- [Prediction](#15)\n\n<font color='red' size=3>If u think this notebook is worth reading and u gained some knowledge then please upvote my kernal. <\/font>","a9b70cf8":"From here we can see that if the person is having a glucose level more than 100 then his chance of having a diabetes also increases ,I am doing this PDP plots for only feature Glucose we can do also on other features.\nFor more information ->https:\/\/christophm.github.io\/interpretable-ml-book\/pdp.html","b9e755ec":"##### Pregnancies\n","588e8139":"##### Pairplot","d8567ee7":"> Here I am predicting the test data that I have created at the beginning which contains only 8 values 6 of which are diabetic and 2 are not diabetic our model can correctly classify both of them but as it's a small test data it's giving very good prediction but if the dataset is large then the accuracy should be varying.","1b2115d2":"Feature causing increase in prediction are the pink ones and feature causing decrease in prediction are the blue ones , \nhere the base value is is 0.34 and output value is 0.8.This person is classified as diabetic person features that have significant influence  are SkinThickness 43,DiabetesPedigreeFunction 1.39, BMI = 49.9,Glucose =199.\nThe value of Age and Pregnancies is quite low and it  tries to neglate the effect but couldn't because the combined total effect of the pink features is much more.","258c1e04":"#### Algorithms","05cf742b":"#### Descriptive Analytics <a id='1'><\/a>","a335a8e2":"#### Univariate feature Selection<a id='5'><\/a>","905b1a49":"##### Observations from above plots:\n\nHaving a high glucose level, the chance of having diabetes is more &\nAs age increases the chance of having diabetes also increases\n.I also noticed that if the woman having a high body mass index then the skin thickness of that person is also high,  whereas having excess BMI is more likely to having diabetes.","9ce3ea58":"> ##### AdaBoost<a id='11'><\/a>","9d11e474":"#### Feature Importance<a id='7'><\/a>","cd2802ad":"The optimal no of features are 7 they are 'Pregnancies', 'Glucose', 'Insulin', 'SkinThickness', 'BMI',\n'DiabetesPedigreeFunction', 'Age'. Recursive feature Elimination with cross validation  tells us that the accuracy of our model will be best if we train our model with this features.","9afc5efd":"##### XGBoost<a id='13'><\/a>","de7814f2":"##### Glucose","26539ee1":"#### Checking the accuracy before applying feature selection","dae5f275":"People from age group 30 to 40 have a blood pressure level between 60 to 90 and after that age peoples are having higher blodd pressure (more than 100)."}}