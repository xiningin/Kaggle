{"cell_type":{"735c4e7a":"code","88e78d8e":"code","f836d80e":"code","53ae4252":"code","cad20363":"code","dab0ad5f":"code","366d6337":"code","7ab313b1":"code","6065c886":"code","ab516278":"code","c1e9eaea":"code","b885b4c2":"code","3801b64d":"code","9660bf82":"code","efacdef5":"code","df495e9b":"code","bd5c3782":"markdown","141d3e0f":"markdown","9f0fcdb7":"markdown","67c3b5b5":"markdown","b7165852":"markdown","67d5efff":"markdown","a742e329":"markdown","c64409e6":"markdown"},"source":{"735c4e7a":"import random \nrandom.seed(123)\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nimport sklearn.metrics as skm\nfrom sklearn.model_selection import train_test_split\nimport operator as op\n\nfrom sklearn.impute import SimpleImputer\n\nimport seaborn as sns\nsns.set(rc={'figure.figsize': (12,8)})\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","88e78d8e":"df = pd.read_csv('\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt',sep=\";\")\ndf.head()","f836d80e":"df.info()","53ae4252":"# Count the number of null values\ndf.isnull().sum()","cad20363":"df.isnull().any(axis = 1).sum()","dab0ad5f":"m, n = df.shape\ndf_per = (df.isnull().sum().sum())\/m\ncol_pers = {}\nfor i in df.columns:\n    col_pers[i] = (df[i].isnull().sum())\/m\n\nprint(df_per)\nprint(col_pers)","366d6337":"# Feature Modification\ndf['Date'] = df['Date'].astype(str)\ndf['Time'] = df['Time'].astype(str)\ndf.replace(['?', 'nan', np.nan], -1, inplace=True) \nnum_vars= ['Global_active_power', 'Global_reactive_power', 'Voltage', \n           'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\nfor i in num_vars:\n    df[i] = pd.to_numeric(df[i])\nimp = SimpleImputer(missing_values=-1, strategy='mean')\ndf[num_vars] = imp.fit_transform(df[num_vars])\ndf.info()","7ab313b1":"# Target Variable\neq1 = (df['Global_active_power']*1000\/60) \neq2 = df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3']\ndf['power_consumption'] = eq1 - eq2\ndf.head()","6065c886":"# Distribution of the target variables\nsns.histplot(data=df, x='power_consumption', bins=15, kde=True)\nplt.show()","ab516278":"corr = np.corrcoef(df.corr())\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(df.corr(), annot=True, mask=mask)\nplt.show()","c1e9eaea":"models = {}\ndf1 = df\ndf1.head()","b885b4c2":"class linmodel():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n\n    def pre_processing(self):\n        cat = ['Date', 'Time', 'power_consumption']\n        X = self.df.drop(cat+[self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 2)\n        return self\n\n    def fit_pred_acc(self):\n        reg = LinearRegression()\n        reg.fit(self.X_train, self.Y_train)\n        pred = reg.predict(self.X_test)\n        mae = round(skm.mean_absolute_error(self.Y_test, pred), 2)\n        rmse = round(skm.mean_squared_error(self.Y_test, pred, squared=False), 2)\n        r2_score = round(skm.r2_score(self.Y_test, pred), 4)\n        ev = round(skm.explained_variance_score(self.Y_test, pred), 4)\n    \n        return [mae, rmse, r2_score, ev]\n    \nlin = linmodel(df, 'Global_active_power')\nlin = lin.pre_processing()\nmodels[\"Mult. Reg\"] = lin.fit_pred_acc()","3801b64d":"class rdgmodel():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n\n    def pre_processing(self):\n        cat = ['Date', 'Time', 'power_consumption']\n        X = self.df.drop(cat+[self.target], axis=1).values\n        Y = self.df[self.target].values\n        \n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 2)\n        return self\n\n    def fit_pred_acc(self):\n        reg = Ridge(alpha=0.0001, normalize=True)\n        reg.fit(self.X_train, self.Y_train)\n        pred = reg.predict(self.X_test)\n        mae = round(skm.mean_absolute_error(self.Y_test, pred), 2)\n        rmse = round(skm.mean_squared_error(self.Y_test, pred, squared=False), 2)\n        r2_score = round(skm.r2_score(self.Y_test, pred), 4)\n        ev = round(skm.explained_variance_score(self.Y_test, pred), 4)\n    \n        return [mae, rmse, r2_score, ev]\n    \nrdg = rdgmodel(df, 'Global_active_power')\nrdg = rdg.pre_processing()\nmodels[\"Ridge Reg\"] = rdg.fit_pred_acc()","9660bf82":"class lasmodel():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n\n    def pre_processing(self):\n        cat = ['Date', 'Time', 'power_consumption']\n        X = self.df.drop(cat+[self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.3, \n                                                            random_state = 72)\n        return self\n\n    def fit_pred_acc(self):\n        reg = Lasso()\n        reg.fit(self.X_train, self.Y_train)\n        pred = reg.predict(self.X_test)\n        mae = round(skm.mean_absolute_error(self.Y_test, pred), 2)\n        rmse = round(skm.mean_squared_error(self.Y_test, pred, squared=False), 2)\n        r2_score = round(skm.r2_score(self.Y_test, pred), 4)\n        ev = round(skm.explained_variance_score(self.Y_test, pred), 4)\n    \n        return [mae, rmse, r2_score, ev]\n    \nlas = lasmodel(df, 'Global_active_power')\nlas = las.pre_processing()\nmodels[\"Lasso Reg\"] = las.fit_pred_acc()","efacdef5":"class polymodel():\n    def __init__(self, df, target):\n        self.df = df\n        self.target = target\n\n    def pre_processing(self):\n        cat = ['Date', 'Time', 'power_consumption']\n        X = self.df.drop(cat+[self.target], axis=1).values\n        Y = self.df[self.target].values\n\n        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(X, Y, \n                                                            test_size = 0.2, \n                                                            random_state = 42)\n        return self\n\n    def fit_pred_acc(self):\n        reg = LinearRegression(normalize=True)\n        pol_feat = PolynomialFeatures(2)\n        X_train_transf = pol_feat.fit_transform(self.X_train)\n        X_test_transf = pol_feat.fit_transform(self.X_test)\n        model = reg.fit(X_train_transf, self.Y_train)\n        pred = model.predict(X_test_transf)\n        mae = round(skm.mean_absolute_error(self.Y_test, pred), 2)\n        rmse = round(skm.mean_squared_error(self.Y_test, pred, squared=False), 2)\n        r2_score = round(skm.r2_score(self.Y_test, pred), 4)\n        ev = round(skm.explained_variance_score(self.Y_test, pred), 4)\n    \n        return [mae, rmse, r2_score, ev]\n    \npoly = polymodel(df, 'Global_active_power')\npoly = poly.pre_processing()\nmodels[\"Poly Reg\"] = poly.fit_pred_acc()","df495e9b":"models_df = pd.DataFrame.from_dict(models, orient='index',\n                  columns=['MAE', 'RMSE', 'R_sq', 'Expl. Var.'])\nmodels_df","bd5c3782":"## 4. Polynomial Multiple Regression","141d3e0f":"# 1. Data Importing","9f0fcdb7":"## 2. Shrinkage Technique: Ridge","67c3b5b5":"## 3. Shrinkage Technique: Lasso","b7165852":"# Exploratory Data Analysis","67d5efff":"## 1. Multiple Linear Regression","a742e329":"# 2. Data Pre-Processing","c64409e6":"# Model Building & Evaluation"}}