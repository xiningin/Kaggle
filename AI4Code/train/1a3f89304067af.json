{"cell_type":{"e807d351":"code","fe2f1e30":"code","bff72276":"code","4c58cc3e":"code","8c0c1a6d":"code","0cef8e73":"code","a3a23efa":"code","e9d6e73e":"code","30a9174a":"code","41ec1b0e":"code","ed1265b1":"code","2c09c267":"code","0368f03c":"code","c54dd08c":"code","565e6cf4":"code","40983c63":"code","c5e04b58":"code","9b669277":"code","105c4930":"code","1b3aa51f":"code","d811bb5a":"code","ee870a24":"code","62f127d6":"code","56d8e54b":"code","d9d16aaa":"code","c3789c0d":"code","4fcc3597":"code","319e5dc9":"code","b56e4999":"code","d81315f6":"code","ba4e74c9":"code","af94e231":"code","4356c7ca":"code","cb3865ee":"code","dfb8c792":"code","1f44e21f":"code","5368ec66":"code","69cebda8":"code","8461c2e4":"code","fac29f61":"code","8cd3d3f1":"code","72822495":"code","7d8febc3":"code","fc44a6cc":"code","17220f77":"code","71ea3076":"code","98c60c69":"code","e88a8041":"code","3d4470e4":"code","7d832655":"code","54b1dd22":"code","cea9285e":"code","85a56524":"code","270d7798":"code","11361f3d":"code","f361cb93":"code","4ce74365":"code","bb20aa01":"code","1fe0fab2":"code","10de959f":"code","3209186d":"code","dcafddb7":"code","806669e5":"code","688e9456":"code","4f54ae79":"code","a8108513":"code","59d2e14f":"code","3b9e28aa":"code","04112485":"markdown","7666262a":"markdown","504c56c0":"markdown","df3be729":"markdown","13fc9b00":"markdown","59de84b9":"markdown","c37b4ee1":"markdown","201e4687":"markdown","61d5bab9":"markdown","71289b71":"markdown","73f0165e":"markdown","3ecbfc31":"markdown","b656e9cd":"markdown","92d7be3d":"markdown","120ec8f0":"markdown","39a42c41":"markdown","26ac6de5":"markdown","e8997e5c":"markdown","b293d1a3":"markdown","dcf97bb5":"markdown","cb9cbfe2":"markdown","66e7e3a0":"markdown","ec781d69":"markdown","5f39996b":"markdown","41f27095":"markdown","54bb2aa0":"markdown","34bfbbc1":"markdown","0dc2c33d":"markdown","d048506b":"markdown","dcbe6792":"markdown","c0699770":"markdown","1e817ce1":"markdown","fb702f78":"markdown","3ddfbac9":"markdown","91255c03":"markdown","6eae2512":"markdown","d0bfc91d":"markdown","048dc3f2":"markdown","5aed2294":"markdown"},"source":{"e807d351":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.pandas.set_option('display.max_columns', 100)","fe2f1e30":"# Importing Dataset\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","bff72276":"train.head()","4c58cc3e":"print('Shape of Train: {}'.format(train.shape))\nprint('Shape of Test: {}'.format(test.shape))","8c0c1a6d":"missing_features = [feature for feature in train.columns if train[feature].isnull().sum()>1]\nprint(missing_features)","0cef8e73":"# Let's see the percentage of missing values\nfor feature in missing_features:\n    print('{}:{}'.format(feature, round(100*train[feature].isnull().sum()\/train.shape[0],2)))","a3a23efa":"# Let's capture missing values in each feature.\nfor feature in missing_features:\n    data = train.copy()\n    data[feature] = np.where(data[feature].isnull(),1,0)\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","e9d6e73e":"# Let's identify numerical variables\nnum_features = [feature for feature in train.columns if train[feature].dtypes!='O']\nprint(num_features)","30a9174a":"temporal_features = [feature for feature in train.columns if 'Year' in feature or 'Yr' in feature]\nprint(temporal_features)","41ec1b0e":"train.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('YrSold')\nplt.ylabel('SalePrice')\nplt.title('SalePrice with Time')","ed1265b1":"for feature in temporal_features:\n    if feature != 'YrSold':\n        df = train.copy()\n        df[feature] = df['YrSold'] - df[feature]\n        sns.scatterplot(df[feature], df['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalePrice')\n        plt.title(feature)\n        plt.show()","2c09c267":"# We need to remove temporal variables from numerical variables\nnum_features = set(num_features).difference(temporal_features)","0368f03c":"# Let's see how we can separate them. To do this we need to know the cardinality of numerical features in the data.\nfor feature in num_features:\n    print('{}:{}'.format(feature, len(train[feature].unique())))","c54dd08c":"discrete_features = [feature for feature in num_features if len(train[feature].unique())<25]\nprint(discrete_features)","565e6cf4":"# Let's see the relationship between Discrete variables and Sale Price\nfor feature in discrete_features:\n    train.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()","40983c63":"continuous_features = [feature for feature in num_features if feature not in discrete_features and feature not in ['Id']]\nlen(continuous_features)","c5e04b58":"# Let's see the relationship between continuous features and Sale Price\nfor feature in continuous_features:\n    sns.scatterplot(train[feature], train['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","9b669277":"for feature in continuous_features:\n    train[feature].plot.box()\n    plt.show()","105c4930":"train.rename({'FireplaceQu':'FireplaceQC'}, axis=1, inplace=True)\ntest.rename({'FireplaceQu':'FireplaceQC'}, axis=1, inplace=True)","1b3aa51f":"categorical_features = [feature for feature in train.columns if train[feature].dtypes=='O']\nprint(categorical_features)","d811bb5a":"ordinal_features = [feature for feature in categorical_features if 'Cond' in feature or 'Qual' in feature or 'QC' in feature]\nprint(ordinal_features)","ee870a24":"# Let's see the unique values of each ordinal features\nfor feature in ordinal_features:\n    print('{}:{}'.format(feature, train[feature].unique()))","62f127d6":"features_to_remove = ['Condition1', 'Condition2', 'SaleCondition']\nordinal_features = set(ordinal_features).difference(set(features_to_remove))\nordinal_features","56d8e54b":"# Removing ordinal features from categorical features\ncategorical_features = set(categorical_features).difference(set(ordinal_features))\nlen(categorical_features)","d9d16aaa":"# Let's see the cardinalty of each categorical features\nfor feature in categorical_features:\n    print('{}:{}'.format(feature, len(train[feature].unique())))","c3789c0d":"# Let's look the relationship between categorical and target variable\nfor feature in categorical_features:\n    train.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","4fcc3597":"# Let's see the relationship between ordinal variable and target variable\nfor feature in ordinal_features:\n    train.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","319e5dc9":"missing_features = [feature for feature in train.columns if train[feature].isnull().sum()>1]\nprint(missing_features)","b56e4999":"for feature in missing_features:\n    print('{}:{}'.format(feature, train[feature].isnull().sum()))","d81315f6":"# Function to capture the missingness of features\ndef capture_missing(data):\n    data = data.copy()\n    data['Alley_nan'] = np.where(data['Alley'].isnull(),1,0)\n    data['MasVnr_nan'] = np.where(data['MasVnrType'].isnull(),1,0)\n    data['Bsmt_nan'] = np.where(data['BsmtExposure'].isnull(),1,0)\n    data['Fireplace_nan'] = np.where(data['FireplaceQC'].isnull(),1,0)\n    data['Garage_nan'] = np.where(data['GarageQual'].isnull(),1,0)\n    data['Pool_nan'] = np.where(data['PoolQC'].isnull(),1,0)\n    data['Fence_nan'] = np.where(data['Fence'].isnull(),1,0)\n    return data","ba4e74c9":"# Capturing missingness in train and test data\ntrain = capture_missing(train)\ntest = capture_missing(test)","af94e231":"num_missing_col = [feature for feature in missing_features if train[feature].dtypes!='O']\ncat_missing_col = [feature for feature in missing_features if train[feature].dtypes=='O']","4356c7ca":"num_missing_col","cb3865ee":"print(cat_missing_col)","dfb8c792":"# Function for imputing missing, values in categorical features\ndef cat_missing_imputer(data, cat_cols):\n    data = data.copy()\n    for feature in cat_cols:\n        data[feature].fillna('Missing', inplace=True)\n    return data","1f44e21f":"# Imputing categorical features\ntrain[cat_missing_col] = cat_missing_imputer(train[cat_missing_col], cat_missing_col)\ntest[cat_missing_col] = cat_missing_imputer(test[cat_missing_col], cat_missing_col)","5368ec66":"train[cat_missing_col].head()","69cebda8":"# Function for imputing numerical variable\ndef num_missing_imputer(data, cat_cols):\n    data = data.copy()\n    for feature in cat_cols:\n        data[feature].fillna(0, inplace=True)\n    return data","8461c2e4":"# Imputing `LotFrontage`, `MasVnrArea` with 0\ntrain[['LotFrontage', 'MasVnrArea']] = num_missing_imputer(train[['LotFrontage', 'MasVnrArea']], \n                                                           ['LotFrontage', 'MasVnrArea'])\ntest[['LotFrontage', 'MasVnrArea']] = num_missing_imputer(test[['LotFrontage', 'MasVnrArea']], \n                                                           ['LotFrontage', 'MasVnrArea'])","fac29f61":"# Function for ageing variable\ndef ageing(data, date_variables):\n    data = data.copy()\n    for feature in date_variables:\n        if feature !='YrSold':\n            data[feature] = data['YrSold'] - data[feature]\n            \n    return data","8cd3d3f1":"# Changing year features into ageing features\ntrain = ageing(train, temporal_features)\ntest = ageing(test, temporal_features)","72822495":"train[temporal_features].head()","7d8febc3":"# Let's check the missing value in the dataset\nplt.figure(figsize=(18,9))\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","fc44a6cc":"train['Electrical'].isnull().sum()","17220f77":"# Let's check the entries of this feature\ntrain['Electrical'].value_counts()","71ea3076":"# Let's impute the missing value with SBrkr\ntrain['Electrical'].fillna('SBrkr', inplace=True)\ntest['Electrical'].fillna('SBrkr', inplace=True)\n\n# Imputing 0 in GarageYrBuilt\ntrain['GarageYrBlt'].fillna(0, inplace=True)\ntest['GarageYrBlt'].fillna(0, inplace=True)","98c60c69":"# Let's see the unique values of ordinal features again!\nfor feature in ordinal_features:\n    print('{}:{}'.format(feature, train[feature].unique()))","e88a8041":"# Encoding ordinal features\nencoding_dict = {'Missing':0,\n                'Po':1,\n                'Fa':2,\n                'TA':3,\n                'Gd':4,\n                'Ex':5}\n\ndef encoding(data, features_to_be_encoded, encoding_dict):\n    data = data.copy()\n    for feature in features_to_be_encoded:\n        data[feature].replace(encoding_dict, inplace=True)\n        \n    return data","3d4470e4":"# Encoding ordinal features in train and test dataset\ntrain = encoding(train, ordinal_features, encoding_dict)\ntest = encoding(test, ordinal_features, encoding_dict)","7d832655":"train.head()","54b1dd22":"print(categorical_features)","cea9285e":"# Function for rare label\ndef rare_label(data, categorical_features):\n    data = data.copy()\n    for feature in categorical_features:\n        temp = data.groupby(feature).size()\/data.shape[0]\n        temp_df = temp[temp>0.01].index\n        data[feature] = np.where(data[feature].isin(temp_df), data[feature], 'rare_var')\n        \n    return data","85a56524":"# Creating rare label in train and test dataset\ntrain = rare_label(train, categorical_features)\ntest = rare_label(test, categorical_features)","270d7798":"# Let's see the cardinality of categorical features\nfor feature in categorical_features:\n    print('{}:{}'.format(feature, len(train[feature].unique())))","11361f3d":"for feature in ['Exterior1st', 'Exterior2nd', 'Neighborhood']:\n    train.groupby(feature)['SalePrice'].median().sort_values(ascending=False).plot.bar()\n    plt.xlabel(feature)\n    plt.show()","f361cb93":"# Let's create dummy variable\ndef dummy_creator(data, categorical_features):\n    data_dummy = pd.DataFrame()\n    for feature in categorical_features:\n        temp = pd.get_dummies(data[feature], prefix=feature, drop_first=True)\n        data_dummy = pd.concat([data_dummy, temp], axis=1)\n        \n    return data_dummy","4ce74365":"# Creating dummy features in train and test dataset\ntrain_dummy = dummy_creator(train[categorical_features], categorical_features)\ntest_dummy = dummy_creator(test[categorical_features], categorical_features)","bb20aa01":"# Dropping categorical features and adding dummy features in train and test sets\ntrain.drop(categorical_features, axis=1, inplace=True)\ntrain = pd.concat([train, train_dummy], axis=1)\n\ntest.drop(categorical_features, axis=1, inplace=True)\ntest = pd.concat([test, test_dummy], axis=1)","1fe0fab2":"train.isnull().sum().sort_values(ascending=False)","10de959f":"test.isnull().sum().sort_values(ascending=False)","3209186d":"# Let's see features missing in test dataset\nmis_features = [feature for feature in test.columns if test[feature].isnull().sum()>0]\nmis_features","dcafddb7":"# We will impute these missing values from train dataset\ntest.loc[test.BsmtFinSF1.isnull(), 'BsmtFinSF1'] = train.BsmtFinSF1.mean()\ntest.loc[test.BsmtFinSF2.isnull(), 'BsmtFinSF2'] = train.BsmtFinSF2.mean()\ntest.loc[test.BsmtUnfSF.isnull(), 'BsmtUnfSF'] = train.BsmtUnfSF.mean()\ntest.loc[test.TotalBsmtSF.isnull(), 'TotalBsmtSF'] = train.TotalBsmtSF.mean()\ntest.loc[test.GarageArea.isnull(), 'GarageArea'] = train.GarageArea.mean()\ntest.loc[test.BsmtFullBath.isnull(), 'BsmtFullBath'] = train.BsmtFullBath.mode()\ntest.loc[test.BsmtHalfBath.isnull(), 'BsmtHalfBath'] = train.BsmtHalfBath.mode()\ntest.loc[test.GarageCars.isnull(), 'GarageCars'] = train.GarageCars.mode()\ntest.loc[test.KitchenQual.isnull(), 'KitchenQual'] = train.KitchenQual.mode()\n","806669e5":"# Let's drop id column from the data\ntrain.drop('Id', axis=1, inplace=True)","688e9456":"# Storing Id column of test data set\ntest_id = test['Id']\ntest.drop('Id', axis=1, inplace=True)","4f54ae79":"# Normalising all features in train and test dataset except\ntrain_normal_features = [feature for feature in train.columns if feature not in train_dummy.columns]\ntest_normal_features = [feature for feature in test.columns if feature not in test_dummy.columns]","a8108513":"# Function for log_transformation\ndef log_transform(data, normal_features):\n    data = data.copy()\n    for feature in normal_features:\n        if 0 in data[feature]:\n            pass\n        else:\n            data[feature] = np.log(data[feature])\n            \n    return data","59d2e14f":"# Performing log transform\ntrain[train_normal_features] = log_transform(train[train_normal_features], train_normal_features)\ntest[test_normal_features] = log_transform(train[test_normal_features], test_normal_features)","3b9e28aa":"# Saving preprocessed train and test files\ntrain.to_csv('train_preprocess.csv')\ntest.to_csv('test_preprocess.csv')","04112485":"Let's see if there is any relationship between `YrSold` and `SalePrice`.","7666262a":"Now that we are done with missing values, let's perform label encoding on `ordinal features` and `YrSold`.","504c56c0":"### Data Analysis","df3be729":"To make all numerical features follow normal distribution, we will perform log transformation.","13fc9b00":"Let's separate these features in numerical features and categorical features. So, that we can determine the imputation mechanism.","59de84b9":"### Missing Values","c37b4ee1":"### Continuous Numerical Features","201e4687":"### Categorical Features","61d5bab9":"From above list, we can say that there are following facilities, which are missing in some houses:\n\n* Alley\n* MasVnr\n* Basement\n* Fireplace\n* Garage\n* Pool\n* Fence\n\nAs we have seen that, missing one of these features impact the selling price of the house, therefore we should capture the missingness of these features.","71289b71":"Since we need to determine the sales price of house, therefore, how old the construction, how old the modification is of importance. Therefore we will calculate the difference of each time feature form `YrSold`.","73f0165e":"Here, we will identify only features with missing values.","3ecbfc31":"### Numerical Variables","b656e9cd":"As it is evident from the graph, that there is a significant relationship between missing values and target variable.Therefore, we will impute something meaningful.","92d7be3d":"### Discrete Numerical Features","120ec8f0":"As it is evident form the graph, that older construction has lower value. Thereforeit's a good idea, to create the ageing feature. We will do it in `Feature Engineering` part.","39a42c41":"Now we will create ageing variables.","26ac6de5":"Most of missing values are in `GarageYrBlt`, we will impute it with 0. Also let's check the missing value in `Electrical`","e8997e5c":"### Missing Value Treatment","b293d1a3":"Let's see if we have any missing values in train or test set or we have any categorical feature left","dcf97bb5":"## Feature Engineering","cb9cbfe2":"As we can see that `Condition1`, `Condition2` and `SaleCondition` are not ordinal features, therefore, we will remove these features from the list.","66e7e3a0":"In discrete variables the concept of outliers don't exist. We will look outliers in continuous variables.","ec781d69":"### Outliers","5f39996b":"Let's see the relationship of features with missing values and the SalePrice","41f27095":"### Handling of Temporal Variables\n\nWe have some date columns, from this we will extract the duration or ageing.","54bb2aa0":"It can be understand that, many small houses don't have pools or basement or garages. So, if these features are missing, therefore, we should label all missing values in these features as `missing`.","34bfbbc1":"All we have left is categorical features. We need to create dummy variables, but before that, let's try to reduce some cardinality of categorical features.\n\nIn order to do so, we will lable those categories with 'rare' which have `<1%` of data of that feature.","0dc2c33d":"As we have seen earlier that after 2007, house prices crashed. Therefore, we should create a label for each year. By this way, we will be able to capture this information.","d048506b":"Now we have left with 3 features, which have highest cardinality, namely:\n* Exterior1st\n* Neighborhood\n* Exterior2nd\n\nLet's see if they make any impact on SalePrice at all.","dcbe6792":"It is evident that we can choose discrete features, having less than 25.","c0699770":"## Machine Learning Pipeline\n\n1. Data Analysis\n2. Feature Engineering\n3. Feature Selection\n4. Model Building\n5. Model Deployment","1e817ce1":"In data analysis, we will do the following:\n* Distribution of Numerical Values\n* Categorical Variables\n* Cardinality of Categorical Variables\n* Outliers\n* Relationship between Independent and Dependent variable","fb702f78":"### Types of Numerical Features\n\nThere are two types of numerical features.\n1. Discrete Numerical Features\n2. Continuous Numerical Features\n\nWe will look into each of them seperately.","3ddfbac9":"From the graph, we can see, there are missing values in most of the features. Also we can see some outliers, which we will\nstudy in later section.","91255c03":"### LabelEncoding","6eae2512":"Now, if we include these features, then due to their high cardinality, we will have high number of columns in our data, after dummy variables creation. But, as it is clear that these features do have an impact on the target feature. Therefore, we will include them and will do feature selection with more advance algorithms.","d0bfc91d":"For missing values in numerical features, we will impute 0, as 2 of the 3 features with missing values are area of some kind. If that particular quality is missing in the house, then their area will also be missing. Therefore, we will impute it as `0`.","048dc3f2":"It can be consulted in the data discription:\n* Ex : Excellent\n* Gd : Good\n* TA : Average\/Typical\n* Fa : Fair\n* Po : Poor\n\nAnd Missing stands for Missing.\n\nTherefore, I will encode this in following fashion:\n* Missing : 0\n* Po      : 1\n* Fa      : 2\n* TA      : 3\n* Gd      : 4\n* Ex      : 5","5aed2294":"There are some features which are ordinal features, so we should separate them. Let's identify them:"}}