{"cell_type":{"47d57cf9":"code","f853c417":"code","97776975":"code","1af11ba5":"code","3c1193b7":"code","95a22b2e":"code","04706f11":"code","7bb8cfb2":"code","1b876ff1":"code","0e65a650":"code","fbeaf11b":"code","be5055c3":"code","19b9c3af":"code","9ff48cd6":"code","dc36ac7f":"code","1516b19d":"code","7527272a":"code","303a568b":"code","91210dac":"code","a54b988f":"code","64f6ba35":"code","cc88f476":"code","1e167c9d":"code","d25cf7f2":"code","9b474ae0":"code","46083290":"code","eb193302":"code","d054900a":"code","006382ed":"code","4eb993cb":"code","b227e26f":"code","1837df79":"code","a4241493":"code","5253c7de":"code","0672b1eb":"code","f4d63b39":"code","0b8339d5":"code","01a42858":"code","47693db5":"code","2b1ff28b":"code","44536fd8":"code","aafd7488":"code","90adb287":"code","74571154":"code","15df0be2":"code","b4f34904":"code","41e78d29":"markdown","2113f035":"markdown","3d5fd37f":"markdown","76d68630":"markdown","df5913e2":"markdown","b4540c57":"markdown","b33c74d2":"markdown","61ff5cc8":"markdown","298441da":"markdown","5d801345":"markdown","409feee7":"markdown","5be9ff4e":"markdown","de7ebb1b":"markdown","e5660de7":"markdown","8347bbfa":"markdown","0e8058c3":"markdown","fd4a9478":"markdown","ce95f07f":"markdown","166ec390":"markdown","614abea7":"markdown","5baf8433":"markdown","15e371d7":"markdown","e53ce3cf":"markdown","ac02a56d":"markdown","91ef45a5":"markdown","18db43ca":"markdown","fcbe0104":"markdown","40a69db7":"markdown","137b8b7f":"markdown","20564924":"markdown","014922e6":"markdown","e3a6bc4b":"markdown","415c6c15":"markdown","00da8277":"markdown","1afc079b":"markdown","79086948":"markdown","994a7fe3":"markdown","6ef60219":"markdown","0e7eaf36":"markdown","283a457b":"markdown","53cbb29b":"markdown","9b7c596a":"markdown","7f18bf6b":"markdown","c6b446c0":"markdown","f94539d4":"markdown","ea29fd58":"markdown","5cacdec3":"markdown","0c95b5bb":"markdown","98607051":"markdown","61c5890f":"markdown","3967abb8":"markdown","97034f12":"markdown","33add391":"markdown","bdfeea57":"markdown","f825eb31":"markdown","32089684":"markdown","8fde9982":"markdown","530641e9":"markdown","d37f99c8":"markdown","7b33a2a5":"markdown","d59bd766":"markdown","67d0e89f":"markdown","c35e93c1":"markdown","af3e2227":"markdown","7a93099f":"markdown","c9277197":"markdown","4e5d7459":"markdown","a2727539":"markdown","3f4d025c":"markdown","f199d829":"markdown","6efd7bf0":"markdown","1911b68f":"markdown","33dabc5b":"markdown","5375cd64":"markdown","92e49b53":"markdown","359b669a":"markdown"},"source":{"47d57cf9":"# import libraries for data exploration and basic statistical functions\n\nimport pandas as pd\nimport numpy as np","f853c417":"data = pd.read_csv('..\/input\/HW1-data.csv')\ndata","97776975":"# Top movies by mean score\nmeans = data.iloc[:, 2:].mean().sort_values(ascending=False)\nmeans","1af11ba5":"# Counts\ncounts = data.iloc[:, 2:].count()\ncounts.sort_values(ascending=False)","3c1193b7":"# Top movies by percentage of positive marks\ncounts_positive = data.iloc[:, 2:][data.iloc[:, 2:] >= 4].count()\ncounts_positive.sort_values(ascending=False)\n(counts_positive \/ counts).sort_values(ascending=False)","95a22b2e":"# Percentage of people who watched Toy Story also watched...\nassociative_product = '1: Toy Story (1995)'\n\nwatched_product = data.iloc[:, 2:][data[associative_product].notnull()].count()\n(watched_product \/ data[associative_product].count()).sort_values(ascending=False)","04706f11":"# Correlation between Toy Story and other movie ratings\ndata.iloc[:, 2:].corrwith(data[associative_product]).sort_values(ascending=False)","7bb8cfb2":"# Means separate by gender\ngender_column_name = 'Gender (1 =F, 0=M)'\nmale_means = data.iloc[:, 2:][data[gender_column_name] == 0].mean()\nfemale_means = data.iloc[:, 2:][data[gender_column_name] == 1].mean()","1b876ff1":"# Male means\nmale_means.sort_values(ascending=False)","0e65a650":"#Female means\nfemale_means.sort_values(ascending=False)","fbeaf11b":"# Overall mean ratings \nmale_average_mean = data.iloc[:, 2:][data[gender_column_name] == 0].sum().sum() \/ data.iloc[:, 2:][data[gender_column_name] == 0].count().sum()\nfemale_average_mean = data.iloc[:, 2:][data[gender_column_name] == 1].sum().sum() \/ data.iloc[:, 2:][data[gender_column_name] == 1].count().sum()\nprint(\"Male avg. mean: {} Female avg. mean: {}\".format(male_average_mean, female_average_mean))","be5055c3":"# Movies that female users rate highest above male raters\n\n(female_means - male_means).sort_values(ascending=False)","19b9c3af":"# Movies that male users rate highest above female raters\n\n(male_means - female_means).sort_values(ascending=False)","9ff48cd6":"# Positive (> 4) ratings by male\n\ncounts_positive_male = data.iloc[:, 2:][(data >= 4)][data[gender_column_name] == 0].count()\ncounts_positive_male.sort_values(ascending=False)","dc36ac7f":"# Percentage of positive ratings by male\n\ncounts_male = data.iloc[:, 2:][data[gender_column_name] == 0].count()\npercentage_positive_male = (counts_positive_male \/ counts_male)\npercentage_positive_male.sort_values(ascending=False)","1516b19d":"# Positive (> 4) ratings by female\n\ncounts_positive_female = data.iloc[:, 2:][(data >= 4)][data[gender_column_name] == 1].count()\ncounts_positive_female.sort_values(ascending=False)","7527272a":"# Percentage of positive ratings by female\n\ncounts_female = data.iloc[:, 2:][data[gender_column_name] == 1].count()\npercentage_positive_female = (counts_positive_female \/ counts_female)\npercentage_positive_female.sort_values(ascending=False)","303a568b":"# Female-male difference in the liking percentage\n\n(percentage_positive_female - percentage_positive_male).sort_values(ascending=False)","91210dac":"# Male-female difference in liking percentage\n(percentage_positive_male - percentage_positive_female).sort_values(ascending=False)","a54b988f":"# Difference between the average rating overall\n\nfemale_average_mean - male_average_mean","64f6ba35":"# importing raw data from excel file\n\nraw_data = pd.read_excel(\"..\/input\/cbf.xls\")\nraw_data","cc88f476":"docs = raw_data.loc['doc1':'doc20', 'baseball':'family']\ndocs","1e167c9d":"user_ranks = raw_data.loc['doc1':'doc20', 'User 1':'User 2']\nuser_ranks.fillna(0, inplace=True)\nuser_ranks","d25cf7f2":"user_profiles = np.array(docs).T @ np.array(user_ranks)\npd.DataFrame(user_profiles, docs.columns, user_ranks.columns)","9b474ae0":"user_preferences = np.matmul(np.array(docs), user_profiles)\nupdf = pd.DataFrame(user_preferences, docs.index, user_ranks.columns)\nupdf","46083290":"updf.loc[:, 'User 1'].sort_values(ascending=False)","eb193302":"updf.loc[:, 'User 2'].sort_values(ascending=False)","d054900a":"normalized_docs = docs.div(docs.sum(axis=1).apply(np.sqrt), axis=0)\nnormalized_docs","006382ed":"normalized_profiles = np.matmul(np.array(normalized_docs).T, np.array(user_ranks))\npd.DataFrame(normalized_profiles, docs.columns, user_ranks.columns)","4eb993cb":"normalized_preferences = np.matmul(np.array(normalized_docs), normalized_profiles)\nnpdf = pd.DataFrame(normalized_preferences, docs.index, user_ranks.columns)\nnpdf","b227e26f":"npdf.loc[:, 'User 1'].sort_values(ascending=False)","1837df79":"docs","a4241493":"DF = docs.sum(axis=0)\nIDF = 1.0 \/ DF\nnp.array(IDF)","5253c7de":"weighted_preferences = np.matmul(np.array(normalized_docs), np.multiply(np.array(normalized_profiles).T, np.array(IDF)).T)\npd.DataFrame(weighted_preferences, docs.index, user_ranks.columns)","0672b1eb":"# library for visualization\nimport seaborn","f4d63b39":"data = pd.read_excel(\"..\/input\/data.xls\")\ndata","0b8339d5":"# user correlations\ncorrelations = pd.DataFrame(data.transpose(), data.columns, data.index).corr()\nseaborn.heatmap(correlations)","01a42858":"# selecting 6 neighbors \n\nneighbours_3867 = correlations[3867].sort_values(ascending=False)[1:6]","47693db5":"recommendations = data.fillna(0)","2b1ff28b":"recommendations.loc[neighbours_3867.index]","44536fd8":"# calculations for top-5 movies\n\n(recommendations.loc[neighbours_3867.index].multiply(\n    neighbours_3867, axis=0).sum(axis=0) \/ (recommendations.iloc[:, :] != 0).multiply(\n    neighbours_3867, axis=0).sum(axis=0)).sort_values(ascending=False)[:5]","aafd7488":"from scipy import spatial\nfrom sklearn.metrics.pairwise import cosine_similarity","90adb287":"ratings_raw_data = pd.read_excel('..\/input\/iicf.xls')\ndata = pd.DataFrame(ratings_raw_data.iloc[0:20, 1:21])\ndata.index = ratings_raw_data.iloc[0:20,0]\ndata_means = data.mean(axis=0)\ndata","74571154":"data.fillna(0, inplace=True)\nsimilarities = pd.DataFrame(cosine_similarity(data.T, data.T), data.columns, data.columns)\nseaborn.heatmap(similarities)","15df0be2":"# Yeah, this probably can be done better with Pandas one-liners.\n\npredictions = data.copy()\nfor user in data.index: #every user in index\n    for movie in data.columns: #every movie for user\n        mean = data_means[movie] #mean rate of this movie\n        similar_movies = similarities[movie] # similar movies to this movie\n        numerator = 0\n        weights_sum = 0\n        for sm in similar_movies.index: # for every similar movie\n            weight = similar_movies[sm]\n            rating = data.loc[user, sm]\n            if weight > 0 and rating > 0: #which is non-negative (sim) and rated by user\n                numerator += weight * (rating - mean)\n                weights_sum += weight\n        predictions.loc[user, movie] = mean + (numerator \/ weights_sum)","b4f34904":"predictions.loc[755].sort_values(ascending=False)","41e78d29":"$$P(i|j) = \\frac{P(i \\vee j)}{P(j)} = \\frac{|U_{i} \\cap U{j}|\/|U|}{|U_{j}| \/ |U|}$$","2113f035":"### 3.1 Content-based filtering","3d5fd37f":"### 3.2 Collaborative filtering","76d68630":"Personalized-based recommender systems also have common subtypes:\n\n - Collaborative filtering recommender systems\n - Content-based recommender systems","df5913e2":"In item-item collaborative filtering, we provide a recommendation based on other items similar to us. The benefits of it, compared to user-user collaborative filtering, is that we usually need much less similarity computations (in most cases, there are much more users in systems than items). The most common pitfall - system provides can provide very obvious recommendations.","b4540c57":"## Contents\n\n1. Introduction\n2. Non-Personalized Recommender Systems\n3. Personalized Recommender Systems\n\n    3.1 Content-Based Filtering  \n    3.2 Collaborative Filtering\n    \n        3.2.1 User-user collaborative filtering\n        3.2.2 Item-item collaborative filtering\n        \n    3.3 Matrix factorization  \n    3.4 Hybrid Recommender Systems\n4. Conclusion","b33c74d2":"We estimate probabilities by counting: $P (i)$ is the fraction of users in the system who\npurchased item i; $P(i\\vee j)$ is the fraction that purchased both $i$ and $j$","61ff5cc8":"## 4. Conclusion","298441da":"Recommender systems are utilized in a variety of areas including movies, music, news, books, research articles, search queries, social tags, and products in general. There are also recommender systems for experts, collaborators, jokes, restaurants, garments, financial services, life insurance, romantic partners (online dating), and Twitter pages.","5d801345":"In hybrid recommender systems, recommendation is made usually based on scores provided by multiple recommender systems. The most common technique is to represent the final score as a linear combination of scores provided by other recommenders with according weights. \n\nAnother option is so-called \"switch\" recommender system. Given some input, system decides, which of the available recommender engines is better to use for a recommendation in this particular situation. Such algorithm helps to overcome problems that exist in each recommender separately.\n\nWe also can use so-called \"cascade\" hybrid recommenders - the system where outputs of one recommendtion algorithm are used as inputs to other. \n\nThere are dozens of ways to use hybrid recommender systems, and there are no common way for applying them to real world problem. Design and architecture of each of such systems depends on data available, domain field and requirements for a particular system.","409feee7":"Collaborative filtering, also referred to as social filtering, filters information by using the recommendations of other people. It is based on the idea that people who agreed in their evaluation of certain items in the past are likely to agree again in the future. A person who wants to see a movie for example, might ask for recommendations from friends. The recommendations of some friends who have similar interests are trusted more than recommendations from others. This information is used in the decision on which movie to see.","5be9ff4e":"Recommender systems typically produce a list of recommendations in one of two ways:\n\n - Non-personalized approach\n - Personalized approach","de7ebb1b":"Using the same math, calculate new normalized user preferences:","e5660de7":"For this example, we will make predictions for user 3867.\n\nOur 'neighborhood' for a user - users with N highest correlations","8347bbfa":"The association rule formula is derived from Bayes theorem:\n\n$$P(i|j) = \\frac{P(i \\vee j)}{P(j)}$$","0e8058c3":"Let's use basic matrix multiplication to predict user interest in particular topic","fd4a9478":"Beside association, we can also define similarity by basically measuring products corellation:","ce95f07f":"Score provided by item-item filtering is computed using the following formula:\n\n$$ s(i,u) = \\mu_{i} + \\frac{\\Sigma_{j \\in I_{u}}w_{ij}(r_{u,j}-\\mu_{j})}{\\Sigma_{j \\in I_{u}}|w_{ij}|}  $$","166ec390":"The following Python code produces some example of non-personalized data analysis based on Movie Lens movie ratings dataset:","614abea7":"All the personalized recommendation require certain amount of data collected about users. Data could either be collected implicitly (products user click on, see) and explicitly (in forms of ratings, surveys, polls). Both methods are used widely and can be combined together depending on the system restrictions and type of recommendation system provide.","5baf8433":"However, there are common techniques that are commonly used by themselves and in combination. Modern recommender systems still use collaborative filtering and content filtering techniques, although nowadays this algorithms are used in combination and with application of such more advanced techniques as matrix factorization, neural networks and hybrid recommender systems.","15e371d7":"$$ p_{u,i} = \\mu_{u} + \\frac{\\Sigma_{v \\in N(u;i)}cos(u,v)(r_{v,i}-\\mu_{v})}{\\Sigma_{v \\in N(u;i)}|cos(u,v)|}  $$","e53ce3cf":"#### 3.2.1 User-user collaborative filtering","ac02a56d":"Finally, we filled a table with our prediction. Let's see the results for user 755:","91ef45a5":"### Scoring and ordering","18db43ca":"#### Associative rule recommendation:","fcbe0104":"## 1. Introduction","40a69db7":"(some background information and Wikipedia text)\n\nA *recommender system* or a *recommendation system* (sometimes replacing \"system\" with a synonym such as platform or engine) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item","137b8b7f":"You may have noticed that in our computation an article that had many attributes checked could have more influence on the overall profile than one that had only a few. doc 1 and doc 19 each have five attributes, while doc6, doc7, and doc18 only have 2 attributes each.\n\nTo fight this problem, we might want to normalize our ratings first.","20564924":"We have rating of two users. \n\nThe value of 1.0 means the user liked the document, the value of 0 - disliked.\n\nNaN means that the user never seen the document (and we have to predict rating)","014922e6":"Let's imagine we watched the movie \"Toy Story\" and we want to have a list of relevant movies to watch next. We can apply association rule here:","e3a6bc4b":"And rating counts:","415c6c15":"#### 3.2.2 Item-item collaborative filtering","00da8277":"As we can see, preferences changed after normalization.\n\nAnother popular and very common approach is to apply TF-IDF technique to our documents.\n\nTFIDF score is caclucated as a product of TF (term frequency) and IDF (inverse documents frequency), which makes more important things that appear frequently in this document, but rarely appear in other documents.","1afc079b":"All the techniques mentioned above have their own problems and pitfalls, which developers face creating and applying recommender systems to real-world problems. They must be taken into account while designing system architecture, and will be covered later in this work. Though the field of recommendation itself is relatively old, there are still no solutions that work perfectly for every case. Designing and evaluating a recommender system is hard, and requires a deep understanding of domain knowledge and data available, as well as constant experimenting and modification. First recommender systems appeared long time ago in 1990's, but the intense research started quite recently with the availability of better computational power and tremendous amounts of data coming from all different sources in internet.","79086948":"We need a way to talk about users, items, and the ratings matrix\n\n$I$ - the set of items.<br>\n\n$U$ - the set of users.<br>\n\n$R$ - the ratings matrix or set of ratings.<br>\n\n$u, v \\in U$ - individual user.<br>\n\n$i, j \\in I$ - individual item.<br>\n\n$r_{ui} \\in R$ - a rating given by user $u$ for item $i$.<br>\n\n$R_{u} \\subset R$ - the set of ratings given by user $u$.<br>\n\n$R_{i} \\subset R$ - the set of ratings given for item $i$.<br>\n\n$\\vec{r_{u}}$ or $r_{u}$ - the ratings given by user $u$, as a vector with missing values for unrated items.\nWe will often work with a normalized vector $\\hat{r_{u}}$.<br>\n\n$\\vec{r_{i}}$ or $r_{i}$ - the ratings given for item $i$, as a vector with missing values for unrated items.<br>","994a7fe3":"This approach is used to recommend items that are related to chosen one (\"People who buy this also bought...\") and, therefore, uses *reference item* to provide recommendations.","6ef60219":"The example of content-based filtering applied to documents:","0e7eaf36":"Next step is to calculate matrix of predicted user preferences for documents.","283a457b":"Making recommendations above, we did not use data about user's gender. Statistically, men and women tend to like or dislike different kinds of movies, so, in order to make non-personalized recommendations more precise, we can take this information into account and see the difference:","53cbb29b":"Let's first calculate top movies by mean score:","9b7c596a":"Sometimes, we do not need to know precise rating to make recommendation. Therefore, we can define some ratings as positive (in this example, all the ratings >= 4):","7f18bf6b":"Building a good recommender system is not an easy task. Although some algorithms are considered \"best practices\", they all have their strenghs and weaknesses. Developing a recommender systems requires good understanding of domain users, data that can be collected, and purposes of our recommendation. Without knowing all the things mentioned above, it is impossible to design a good recommender system, no matter how complicated are algorithms you use. ","c6b446c0":"Example of item-item recommendation:","f94539d4":"We are going to use it to describe scoring algorithms.\n\n$s(i; u)$ - the score for item $i$ for user $u$.<br>\n\n$s(i; u,q,x)$ - the score for item $i$ for user $u$ with query $q$ in context $x$.<br>\n\n$O(I; u,q,x)$ - the ordering for items $I$ for user $u$ with query $q$ in context $x$.<br>","ea29fd58":"Collaborative filtering often uses the concept of **neighbourhood** (the amount of people\/items we base our prediction on). Making neighbourhoods too small results in not enough information for accurate prediction, and making them too big results in high computational complexity and letting noize in systems. Neighborhood size is a hyperparameter which needs to be tuned in every system. Distance between neighbors can be defined using such metrics as cosine similarity.","5cacdec3":"For convenience, we are going to use the same notation across this Kernel:","0c95b5bb":"One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system\n\nThis competition energized the search for new and more accurate algorithms. The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction:\n\n>Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique. Consequently, our solution is an ensemble of many methods.","98607051":"Though non-personalized recommenders are rarely used in modern systems by themselves, they are still very powerful in combination with other algorithms, and, sometimes, the only available option. \n\nHow can we make a recommendation for a user that we have little or no data about? \n\nThat's where stereotype-based recommendations can be made, and most of the times we can take into account:\n\n - items popularity\n - user demographic data \n - user actions during that particular session (for example, items in online-shop basket)","61c5890f":"One of the common approaches we can use is mean-based recommendation.\n\nBasic mean is computed using the following formula:\n$$\\mu = \\frac{\\Sigma_{u \\in U_{i}}r_{ui}}{|U_{i}|}$$","3967abb8":"And can be used for recommending items with the highest rating. However, in order to make our recommendations more stable, we can use \"damped\" mean algorithm and add some \"fake\" global mean rating to our score. ","97034f12":"Notation for SVD and other decomposition techniques.\n\n$R = P\\Sigma Q^{T}$ - a factorization of the ratings matrix $R$ into a $|U| \\times k$ user-feature preference matrix $P$ and a $|I| \\times k$ item-feature relevance matrix $Q$.<br>\n\n$\\vec{p_{u}}$ or $p_{u}$ - the user feature vector over latent features.<br>\n\n$\\vec{q_{i}}$ or $q_{i}$ - the item feature vector.<br>","33add391":"Beside classical approaches to recommendation with techniques described above, there are a lot of different cases that require modifications or special settings:\n\n - Group recommender systems\n - Context-aware recommender systems\n - Risk-aware recommender systems","bdfeea57":"In user-user collaborative filtering, we provide a recommendation based on tastes of other users similar to us. The problem with that algorithm is that we need a lot of information about other people to provide correct recommendations, but the main benefits are effectivness and ability to provide new, unexpected, and, yet, good recommendatons.","f825eb31":"### 3.4 Hybrid recommender systems","32089684":"An example of User-user collaborative filtering:","8fde9982":"Content-based filtering, also referred to as cognitive filtering, recommends items based on a comparison between the content of the items and a user profile. The content of each item is represented as a set of descriptors or terms, typically the words that occur in a document. The user profile is represented with the same terms and built up by analyzing the content of items which have been seen by the user.\n\nSeveral issues have to be considered when implementing a content-based filtering system. First, terms can either be assigned automatically or manually. When terms are assigned automatically a method has to be chosen that can extract these terms from items. Second, the terms have to be represented such that both the user profile and the items can be compared in a meaningful way. Third, a learning algorithm has to be chosen that is able to learn the user profile based on seen items and can make recommendations based on this user profile.\n\nThe greatest advantage in content-based filtering systems is that the recommendations provided can easily be interpreted to user, because we always know what \"features\" about particular item made algorithm rate it higher.\n\nWhen we have the representation of our item or user as a vector of features, we can measure use metrics such as cosine distance to measure similarity between user profile vector and item feature vector:\n\n$$cos(p_{u}, q_{i}) = \\frac{\\Sigma_{t}p_{it}q_{ut}}{\\sqrt{\\Sigma_{t}q_{ut}^{2}}\\sqrt{\\Sigma_{t}p_{it}^{2}}}$$","530641e9":"The purpose of this Kernel is to explore the basics of Recommender Systems and to give the beginners some intuition with code examples. It covers some popular algorithms and strategies, but does not get deeply into advanced techniques or evaluation metrics (yet).  This notebook is inspired by Recommender Systems Course by University of Minessota.\n\nI did not take huge datasets and tried to make everything easy-to-understand, but if you have any suggestions about making this Kernel better, please, share!","d37f99c8":"## Common notation","7b33a2a5":"One of the most common problems all collaborative filtering recommender systems face - a so called \"cold start\" problem, when we either:\n\n - do not have enough ratings for a new user to find neighbours\n - do not have enough ratings for a new item to find neighbours\n - have a completely new system without any data to make recommendations\n \nIn each of those cases, problems might be solved differently depending on the particular case and options available.","d59bd766":"## 3. Personalized Recommendation","67d0e89f":"We can see the predicted \"ratings\" of documents for User 1 & User 2","c35e93c1":"Normalized profiles now:","af3e2227":"The benefits of those techniques are that they can dramatically improve system performance by reducing the necessary amount of space. Collaborative techniques can be later applied on decomposed matrices. \n\nThis work does not cover factorization techniques in depth. (at least yet)","7a93099f":"### Basic objects","c9277197":"### Matrix factorization","4e5d7459":"The advanced version of this rule computes how much more likely someone is to rate an item $i$ when they rated $j$ than they would have if we do not know anything about whether they have rated $j$:\n\n$$P(i|j) = \\frac{P(i \\vee j)}{P(i)P(j)}$$","a2727539":"In order to account for user's tendecy to give higher\/lower ratings, we will use normalization again. Algorithm for providing score based on user-user collaborative filtering is defined as:","3f4d025c":"In matrix factorization techniques, we usually represent the rating matrix as a product of 3 other matrices.\n$$R = P\\Sigma Q^{T}$$","f199d829":"### 3.3 Matrix factorization","6efd7bf0":"#### Mean-based recommendation:","1911b68f":"The field of recommender systems is constantly developing, providing us with new studies on context-based recommendation, risk-aware and group recommendations, as well as research in different evaluation methods and iterative factorization techniques. There are dozens of ways to design a recommender, and choosing \"the best\" approach is up to people who know why and how they want to make recommendations.","33dabc5b":"### 2. Non-personalized Recommendation","5375cd64":"$$s(i) = \\frac{\\Sigma_{u \\in U_{i}}r_{ui} + \\alpha\\mu}{|U_{i}| + \\alpha}$$","92e49b53":"In this case, *j* is the *reference item*, and *i* is an item to be scored.","359b669a":"Where $\\alpha$ is a damping parameter, which represents the number of \"fake\" ratings we are adding. Because of that damping factor, recommender become less tend to make extreme recommendations."}}