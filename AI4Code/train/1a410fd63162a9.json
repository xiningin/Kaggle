{"cell_type":{"936c716e":"code","2f863001":"code","142689e5":"code","5257294e":"code","83299dd3":"code","45331bee":"code","d0555906":"code","4bb6a2df":"code","1a5f2fca":"code","a4240af8":"code","85fa391c":"code","766c317e":"code","d0ddc68f":"code","71ef4c14":"code","7d953455":"code","9f80cae4":"code","1ad913d9":"code","ba8787fa":"code","b5f9d8af":"code","aa231773":"code","75c30ed9":"code","7395473f":"code","a60f6a08":"code","592d36a1":"code","ceaebb23":"code","9e695d8b":"code","1344b44d":"markdown","9afa9711":"markdown","e1919f1d":"markdown","6bdef3e7":"markdown","bcd51af8":"markdown","c8a1be00":"markdown","50ff6977":"markdown","3fa4525d":"markdown","7c638297":"markdown","01739408":"markdown","31def829":"markdown","a8abfe73":"markdown","82eeac86":"markdown","de073e1b":"markdown","2ef2cd8b":"markdown","71727bc8":"markdown"},"source":{"936c716e":"import ast\nimport glob\nimport os\nimport yaml\n\nimport numpy as np\nimport pandas as pd\n\n\nfrom IPython.display import Image, display\nfrom IPython.core.magic import register_line_cell_magic\nfrom shutil import copyfile\nfrom tqdm import tqdm\ntqdm.pandas()","2f863001":"HOME_DIR = '\/kaggle\/working'\nCOTS_DATASET_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images'","142689e5":"# I just used spllited dataset by @julian3833 - Reef - A CV strategy: subsequences! \n# https:\/\/www.kaggle.com\/julian3833\/reef-a-cv-strategy-subsequences \n\ndf = pd.read_csv(\"..\/input\/reef-cv-strategy-subsequences-dataframes\/train-validation-split\/train-0.1.csv\")\ndf.head(3)","5257294e":"def add_path(row):\n    return f\"{COTS_DATASET_PATH}\/video_{row.video_id}\/{row.video_frame}.jpg\"\n\ndef num_boxes(annotations):\n    annotations = ast.literal_eval(annotations)\n    return len(annotations)\n\ndf['path'] = df.apply(lambda row: add_path(row), axis=1)\ndf['num_bbox'] = df['annotations'].apply(lambda x: num_boxes(x))\nprint(\"New path and annotations preprocessing completed\")","83299dd3":"df = df[df.num_bbox > 0]\n\nprint(f'Dataset images with annotations: {len(df)}')","45331bee":"def add_new_path(row):\n    if row.is_train:\n        return f\"{HOME_DIR}\/yolor_dataset\/images\/train\/{row.image_id}.jpg\"\n    else: \n        return f\"{HOME_DIR}\/yolor_dataset\/images\/valid\/{row.image_id}.jpg\"\n    \n\ndf['new_path'] = df.apply(lambda row: add_new_path(row), axis=1)\nprint(\"New image path for train\/valid created\")","d0555906":"df.head(3)","4bb6a2df":"os.makedirs(f\"{HOME_DIR}\/yolor_dataset\/images\/train\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/images\/valid\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/labels\/train\")\nos.makedirs(f\"{HOME_DIR}\/yolor_dataset\/labels\/valid\")\nprint(f\"Directory structure yor YoloR created\")","1a5f2fca":"def copy_file(row):\n  copyfile(row.path, row.new_path)\n\n_ = df.progress_apply(lambda row: copy_file(row), axis=1)","a4240af8":"IMG_WIDTH, IMG_HEIGHT = 1280, 720\n\ndef get_yolo_format_bbox(img_w, img_h, box):\n    w = box['width'] \n    h = box['height']\n    \n    if (bbox['x'] + bbox['width'] > 1280):\n        w = 1280 - bbox['x'] \n    if (bbox['y'] + bbox['height'] > 720):\n        h = 720 - bbox['y'] \n        \n    xc = box['x'] + int(np.round(w\/2))\n    yc = box['y'] + int(np.round(h\/2)) \n\n    return [xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]\n    \n\nfor index, row in tqdm(df.iterrows()):\n    annotations = ast.literal_eval(row.annotations)\n    bboxes = []\n    for bbox in annotations:\n        bbox = get_yolo_format_bbox(IMG_WIDTH, IMG_HEIGHT, bbox)\n        bboxes.append(bbox)\n        \n    if row.is_train:\n        file_name = f\"{HOME_DIR}\/yolor_dataset\/labels\/train\/{row.image_id}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n    else:\n        file_name = f\"{HOME_DIR}\/yolor_dataset\/labels\/valid\/{row.image_id}.txt\"\n        os.makedirs(os.path.dirname(file_name), exist_ok=True)\n        \n    with open(file_name, 'w') as f:\n        for i, bbox in enumerate(bboxes):\n            label = 0\n            bbox = [label]+bbox\n            bbox = [str(i) for i in bbox]\n            bbox = ' '.join(bbox)\n            f.write(bbox)\n            f.write('\\n')\n                \nprint(\"Annotations in YoloR format for all images created.\")","85fa391c":"data_yaml = dict(\n    train = f'{HOME_DIR}\/yolor_dataset\/images\/train',\n    val = f'{HOME_DIR}\/yolor_dataset\/images\/valid',\n    nc = 1,\n    names = ['sf']\n)\n\n\nwith open(f'{HOME_DIR}\/YoloR-data.yaml', 'w') as outfile:\n    yaml.dump(data_yaml, outfile, default_flow_style=True)\n\nprint(f'Dataset configuration file for YoloR created')","766c317e":"!git clone https:\/\/github.com\/WongKinYiu\/yolor","d0ddc68f":"!pip install torchvision --upgrade -q\n!pip install wandb --upgrade","71ef4c14":"%cd yolor\n!pip install -qr requirements.txt","7d953455":"%cd ..\n!git clone https:\/\/github.com\/JunnYu\/mish-cuda\n%cd mish-cuda\n!git reset --hard 6f38976064cbcc4782f4212d7c0c5f6dd5e315a8\n!python setup.py build install\n%cd ..","9f80cae4":"!git clone https:\/\/github.com\/fbcotter\/pytorch_wavelets\n%cd pytorch_wavelets\n!pip install .\n%cd ..","1ad913d9":"%cd yolor\n!bash scripts\/get_pretrain.sh","ba8787fa":"# more about Secrets -> https:\/\/www.kaggle.com\/product-feedback\/114053\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\") \nwandb.login(key=wandb_api)\nwandb.login(anonymous='must')","b5f9d8af":"@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","aa231773":"%%writetemplate \/kaggle\/working\/yolor\/data\/coco.yaml\n\nnc: 1\nnames: ['starfish',]","75c30ed9":"%%writetemplate \/kaggle\/working\/yolor\/data\/coco.names\n\nstarfish","7395473f":"%%writetemplate \/kaggle\/working\/hyp-yolor.yaml\n\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\nmomentum: 0.937  # SGD momentum\/Adam beta1\nweight_decay: 0.0005  # optimizer weight decay 5e-4\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\nwarmup_momentum: 0.8  # warmup initial momentum\nwarmup_bias_lr: 0.1  # warmup initial bias lr\nbox: 0.05  # box loss gain\ncls: 0.5  # cls loss gain\ncls_pw: 1.0  # cls BCELoss positive_weight\nobj: 1.0  # obj loss gain (scale with pixels)\nobj_pw: 1.0  # obj BCELoss positive_weight\niou_t: 0.20  # IoU training threshold\nanchor_t: 4.0  # anchor-multiple threshold\n# anchors: 3  # anchors per output layer (0 to ignore)\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\nhsv_h: 0.0  # image HSV-Hue augmentation (fraction)\nhsv_s: 0.0  # image HSV-Saturation augmentation (fraction)\nhsv_v: 0.0  # image HSV-Value augmentation (fraction)\ndegrees: 0.0  # image rotation (+\/- deg)\ntranslate: 0.5  # image translation (+\/- fraction)\nscale: 0.0  # image scale (+\/- gain)\nshear: 0.0  # image shear (+\/- deg)\nperspective: 0.0  # image perspective (+\/- fraction), range 0-0.001\nflipud: 0.0  # image flip up-down (probability)\nfliplr: 0.5  # image flip left-right (probability)\nmosaic: 0.95  # image mosaic (probability)\nmixup: 0.3  # image mixup (probability)","a60f6a08":"!python train.py \\\n --batch-size 4 \\\n --img 1280 768 \\\n --data '{HOME_DIR}\/YoloR-data.yaml' \\\n --cfg '.\/cfg\/yolor_p6.cfg' \\\n --weights '.\/yolor_p6.pt' \\\n --device 0 \\\n --name yolor_p6 \\\n --hyp '\/kaggle\/working\/hyp-yolor.yaml' \\\n --epochs 8","592d36a1":"INFER_PATH = f\"{HOME_DIR}\/yolor_dataset\/infer\"\nos.makedirs(INFER_PATH)\n\ndf_infer = df.query(\"~is_train and num_bbox > 4\").sample(n = 15)\n\ndef copy_file(row):\n    new_location = INFER_PATH + '\/' + row.image_id + '.jpg'\n    copyfile(row.path, new_location)\n\n_ = df_infer.progress_apply(lambda row: copy_file(row), axis=1)","ceaebb23":"!python detect.py \\\n    --source {INFER_PATH} \\\n    --cfg .\/cfg\/yolor_p6.cfg \\\n    --weights '\/kaggle\/working\/yolor\/runs\/train\/yolor_p6\/weights\/best_overall.pt' \\\n    --conf 0.05 \\\n    --img-size 1280 \\\n    --device 0 ","9e695d8b":"for img in glob.glob('\/kaggle\/working\/yolor\/inference\/output\/*.jpg'): \n    display(Image(filename=img))\n    print(\"\\n\")","1344b44d":"We got an error - but it is connected with w&b integrations. Looking for solution.","9afa9711":"## 4. CREATE YoloR DATASET CONFIGURATION FILE","e1919f1d":"## 6. INFERENCE USING YoloR ","6bdef3e7":"<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/unifued_network.png\"\/><\/div>\n\n<div align=\"center\"><img width=\"640\" src=\"https:\/\/github.com\/WongKinYiu\/yolor\/raw\/main\/figure\/performance.png\"\/><\/div>","bcd51af8":"### 4E. CONFIGURE WEIGHTS AND BIASES FOR EXPERIMENT LOGGING ","c8a1be00":"## 5. TRAIN YoloR","50ff6977":"### 4D. DWONLOAD LATEST CHECKPOINT FROM YoloR MODEL HUB \n\nIn this notebook we take P6 model (because I want to show only how to train YoloR model on Kaggle) but you can experiment with other YoloR models: https:\/\/github.com\/WongKinYiu\/yolor","3fa4525d":"### 4B. INSTALL MISH CUDA","7c638297":"### 4F. CONFIGURE YoloR HYPERPARAMETERS ","01739408":"## 4. INSTALL YoloR\n\n### 4A. CLONE YoloR GIT REPOSITORY ","31def829":"## Train YoloR on COTS dataset (PART 1 - TRAINING) - as easy as possible to help people start with YoloR and develop this notebook\nThis notebook introduces YOLOR on Kaggle and TensorFlow - Help Protect the Great Barrier Reef competition. It shows how to train custom object detection model (COTS dataset) using YoloR. It could be good starting point for build own custom model based on YoloR detector. Full github repository you can find here - [YOLOR](https:\/\/github.com\/WongKinYiu\/yolor)\n\nSteps covered in this notebook:\n\n* Prepare COTS dataset for YoloR training\n* Install YoloR (YoloR, MISH CUDA, pytorch_wavelets)\n* Download Pre-Trained Weights for YoloR HUB\n* Prepare configuration files (YoloR hyperparameters and dataset)\n* Weights and Biases configuration for training logging\n* YoloR training\n* Run YoloR inference on test images\n\n<div class=\"alert alert-warning\">I found that there is no reference custom model training YoloR notebook on Kaggle. Since we have such an opportunity this is my contribution to this competition. Feel free to use it and enjoy! I really appreciate if you upvote this notebook. Thank you!<\/div>\n\n<div class=\"alert alert-success\" role=\"alert\">\nI introduced YoloX in TensorFlow - Help Protect the Great Barrier Reef competition as well. You can find these notebooks here:      \n    <ul>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-full-training-pipeline-for-cots-dataset\">YoloX full training pipeline for COTS dataset<\/a><\/li>\n        <li> <a href=\"https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507\">YoloX detections submission made on COTS dataset<\/a><\/li>\n    <\/ul>\n    \n<\/div>","a8abfe73":"### 4C. INSTALL PYTORCH WAVELETS ","82eeac86":"## 0. IMPORT MODULES","de073e1b":"## 1. PREPARE DATASET","2ef2cd8b":"## 3. CREATE YoloR ANNOTATIONS","71727bc8":"## 2. CREATE DATASET FILE STRUCTURE"}}