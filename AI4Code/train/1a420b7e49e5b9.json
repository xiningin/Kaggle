{"cell_type":{"3f74186e":"code","e91c9b58":"code","7438954a":"code","2319b56f":"code","45af8389":"code","48331498":"code","e7f798c0":"code","0daeecb3":"code","8acff847":"code","f9137738":"code","87789c17":"code","258afd9c":"code","125a5e8f":"code","dad6b657":"code","2d6e780b":"code","5cab238e":"code","25058041":"code","a0bd5a30":"code","772b2a92":"code","3bde0273":"code","82a6f1ad":"markdown","0cae7923":"markdown","363f844d":"markdown"},"source":{"3f74186e":"import numpy as np\nimport pandas as pd\npd.set_option(\"max_colwidth\", 90)\n\nfrom string import punctuation\nfrom bs4 import BeautifulSoup\nimport re\nimport calendar\nimport textwrap\n\nfrom nltk import word_tokenize, pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer","e91c9b58":"stop_words = stopwords.words('english') \\\n                + [\"can't\"] \\\n                + [x.lower() for x in calendar.month_name[1:]] \\\n                + [x.lower() for x in calendar.month_abbr[1:]] \\\n                + ['utc', 'wikipedia', 'wiki']\n\n\ndef text_preprocessor(text: str, max_len: int = 600) -> str:\n    \"\"\" Cutting and cleaning the text. \"\"\"\n    text = text.strip()\n    text = textwrap.shorten(text, width=max_len, placeholder='')\n    text = text.replace('\\n', ' ')\n    text = text.lower()\n\n    text = re.sub(r'image|file|jpg|jpeg', '', text)\n    text = re.sub(r'\\d{1,4}\\.\\d{1,4}\\.\\d{1,4}\\.\\d{1,4}', '', text)\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', text)\n\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text()\n    \n    text_cleaned = [w.strip(punctuation) for w in text.split() if not w.isdigit()]\n    text = \" \".join(text_cleaned)\n    \n    return text\n\n\ndef toxic_preprocessor(string: str) -> float:\n    \"\"\" Get value of toxic text. \"\"\"\n    return sum(model.predict(string).values())","7438954a":"validation_data_path = \"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\"\nvalidation_data = pd.read_csv(validation_data_path)\nvalidation_data.shape","2319b56f":"%%capture\n!pip install detoxify","45af8389":"from detoxify import Detoxify\n\nmodel = Detoxify('original')","48331498":"# model.predict(\"this article sucks woo woo wooooooo\")\n# {'toxicity': 0.9875552,\n#  'severe_toxicity': 0.05354331,\n#  'obscene': 0.924013,\n#  'threat': 0.0024955713,\n#  'insult': 0.18830173,\n#  'identity_attack': 0.0021805398}\n# 2.1580893732607365","e7f798c0":"# model.predict(\"what wher is your sexy pic gone from your main page put it back\")\n# {'toxicity': 0.75664175,\n#  'severe_toxicity': 0.001743382,\n#  'obscene': 0.12342469,\n#  'threat': 0.001059845,\n#  'insult': 0.06852302,\n#  'identity_attack': 0.0021121758}\n# 0.9535048543475568","0daeecb3":"# shorten data to speed up debugging\nvalidation_data.loc[::100].shape","8acff847":"check_data = validation_data.copy()\ncheck_data","f9137738":"%%time\nclean_data = check_data.copy()\nclean_data['less_toxic'] = clean_data['less_toxic'].apply(text_preprocessor)\nclean_data['more_toxic'] = clean_data['more_toxic'].apply(text_preprocessor)","87789c17":"clean_data","258afd9c":"%%time\ntoxic_data = check_data.copy()  # === without text_preprocessor ===\ntoxic_data['less_toxic'] = toxic_data['less_toxic'].apply(toxic_preprocessor)\ntoxic_data['more_toxic'] = toxic_data['more_toxic'].apply(toxic_preprocessor)","125a5e8f":"toxic_data","dad6b657":"# 0.6912780656303973 < validation_data\ntoxic_data.eval('less_toxic < more_toxic').mean()","2d6e780b":"correct_predict = toxic_data.eval('less_toxic < more_toxic')","5cab238e":"diff_toxic_data = toxic_data.loc[~correct_predict] \\\n                    .assign(diff=lambda x: x.less_toxic - x.more_toxic)\ndiff_toxic_data","25058041":"diff_toxic_data['diff'].hist(bins=100, figsize=(12,6));","a0bd5a30":"clean_data.loc[~correct_predict]","772b2a92":"# The indicies of incorrect preditions\nclean_data.loc[~correct_predict].reset_index()['index'].hist(bins=100, figsize=(12,6));","3bde0273":"pd.DataFrame({'all': clean_data.loc[:, 'worker'].value_counts(),\n              'correct': clean_data.loc[correct_predict, 'worker'].value_counts(),\n              'incorrect': clean_data.loc[~correct_predict, 'worker'].value_counts()}) \\\n                    .fillna(0).sort_values(by='incorrect', ascending=False) \\\n                    .rename_axis(index='worker', columns='predict')","82a6f1ad":"# 2. Get score","0cae7923":"# 1. Import & Load","363f844d":"# 3. Check result"}}