{"cell_type":{"51ec77a6":"code","f2e59013":"code","ec493cdd":"code","a05f93d6":"code","2dd0b87f":"code","acd91d72":"code","881df2db":"code","9bea540a":"code","0c40047f":"code","2afee39f":"code","a41cbb26":"code","7e902e8c":"code","68d93c0d":"code","7293421f":"code","a7b9cb7d":"code","06dcaf10":"code","c9f78e0e":"code","0702132e":"code","52397d9d":"code","3fbb9bb6":"code","b0780a3e":"code","7c869647":"code","8b598d3d":"markdown","0d2c96ea":"markdown","1817d1ab":"markdown","657e91b7":"markdown","b74e2866":"markdown","53966bed":"markdown","f387cb5d":"markdown","85e060af":"markdown","2e8d8789":"markdown","97abe399":"markdown","0083d68e":"markdown","ef6f8de6":"markdown","ed21ac1b":"markdown","4aa5e25d":"markdown","969ac421":"markdown","4bd6bf43":"markdown","4cc4e650":"markdown","a0a10d2b":"markdown","f3aa9183":"markdown","17c8d3d0":"markdown","3ea236f4":"markdown"},"source":{"51ec77a6":"# imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom numpy.random import seed\nfrom itertools import chain\nfrom tensorflow.keras import Model,Input\nfrom tensorflow.keras.layers import LSTM,Embedding,Dense\nfrom tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional\n\nplt.style.use('seaborn')","f2e59013":"# getting the data\nfile = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\ndata = pd.read_csv(file, encoding = \"latin1\")\n\n# drop the POS column because we dont need it\ndata = data.drop('POS', 1)","ec493cdd":"data.head()","a05f93d6":"# Fill na\ndata = data.fillna(method = 'ffill')","2dd0b87f":"words = list(set(data[\"Word\"].values))\nwords.append(\"ENDPAD\")\nnum_words = len(words)\n\nprint(f\"Total number of unique words in dataset: {num_words}\")","acd91d72":"tags = list(set(data[\"Tag\"].values))\nnum_tags = len(tags)\nnum_tags\nprint(\"List of tags: \" + ', '.join([tag for tag in tags]))\nprint(f\"Total Number of tags {num_tags}\")","881df2db":"class Get_sentence(object):\n    def __init__(self,data):\n        self.n_sent = 1\n        self.data = data\n        agg_func = lambda s:[(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n                                                    s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]","9bea540a":"getter = Get_sentence(data)\nsentence = getter.sentences\nsentence[10]","0c40047f":"plt.figure(figsize=(14,7))\nplt.hist([len(s) for s in sentence],bins = 50)\nplt.xlabel(\"Length of Sentences\")\nplt.show()","2afee39f":"plt.figure(figsize=(14, 7))\ndata.Tag[data.Tag != 'O']\\\n    .value_counts()\\\n    .plot\\\n    .barh();","a41cbb26":"word_idx = {w : i + 1 for i ,w in enumerate(words)}\ntag_idx =  {t : i for i ,t in enumerate(tags)}","7e902e8c":"tag_idx","68d93c0d":"max_len = 50\nX = [[word_idx[w[0]] for w in s] for s in sentence]\nX = pad_sequences(maxlen = max_len, sequences = X, padding = 'post', value = num_words - 1)\n\ny = [[tag_idx[w[1]] for w in s] for s in sentence]\ny = pad_sequences(maxlen = max_len, sequences = y, padding = 'post', value = tag_idx['O'])\ny = [to_categorical(i, num_classes = num_tags) for i in  y]","7293421f":"x_train,x_test,y_train,y_test = train_test_split(X, y,test_size = 0.1, random_state = 1)","a7b9cb7d":"input_word = Input(shape = (max_len,))\nmodel = Embedding(input_dim = num_words, output_dim = max_len, input_length = max_len)(input_word)\nmodel = SpatialDropout1D(0.1)(model)\nmodel = Bidirectional(LSTM(units = 100,return_sequences = True, recurrent_dropout = 0.1))(model)\nout = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\nmodel = Model(input_word,out)\n\nmodel.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])\nmodel.summary()","06dcaf10":"plot_model(model, show_shapes = True)","c9f78e0e":"model.fit(x_train, np.array(y_train), batch_size = 64, verbose = 1, epochs = 3, validation_split = 0.2)","0702132e":"model.evaluate(x_test, np.array(y_test))","52397d9d":"rand_sent = np.random.randint(0, x_test.shape[0]) # get a random sentense\np = model.predict(np.array([x_test[rand_sent]]))\np = np.argmax(p, axis = -1)\n\ny_true = np.argmax(np.array(y_test), axis = -1)[rand_sent] # get actual tags for random sentense\n\nprint(\"{:20}{:20}\\t{}\\n\".format(\"Word\", \"True\", \"Pred\"))\nprint(\"-\" * 55)\n\nfor (w, t, pred) in zip(x_test[rand_sent], y_true, p[0]):\n    print(\"{:20}{:20}\\t{}\".format(words[w - 1], tags[t], tags[pred]))","3fbb9bb6":"def create_test_input_from_text(text):\n    word_list = text.split(\" \")\n    x_new = []\n    for word in word_list:\n        x_new.append(word_idx[word])\n        \n    p = model.predict(np.array([x_new]))\n    p = np.argmax(p, axis = -1)\n    print(\"{:20}\\t{}\\n\".format(\"Word\", \"Prediction\"))\n    print(\"-\" * 35)\n\n    for (w, pred) in zip(range(len(x_new)), p[0]):\n        print(\"{:20}\\t{}\".format(word_list[w], tags[pred]))","b0780a3e":"\ntest_inputs = \"the weather in London is very hot\"\ncreate_test_input_from_text(test_inputs)","7c869647":"\ntest_inputs = \"my friend Mohammed is travelling to Oman\"\ncreate_test_input_from_text(test_inputs)","8b598d3d":"as you can see the testing dataset accuracy is very high as well, confirming that the model is not overfitted, now let\u2019s try to tag random sentences from our training dataset and printing the original values compared to the values predicted by our model","0d2c96ea":"after the model has been trained the final loss is 0.439, and the final accuracy is 0.9845 which is 98.45%\nvery high accuracy, I don\u2019t think the model is overfitted because as we will see later on while predicting it has no problem working with foreign data.\n\nFinally, let\u2019s evaluate the model using our testing dataset\n","1817d1ab":"most of our sentenses have a length of 20 words, the longest sentense is around 63 words","657e91b7":"# Model Evaluation","b74e2866":"now lets split the data into training and testing, we will use a testing dataset size of 10%, i belive that should be enough","53966bed":"We will implement a functional model rather than a sequential one, the reason behind this is a functional model provided us with better accuracy in the current situation, we should always use the model that works best for the job, for example, a sequential Max Pooled LSTM did give me better results when doing a project about predicting COVID-19 cases, another reason is that a functional model is more flexible and allows us to have multiple inputs or output layers (thats more of a Keras API thing).\n\nour loss measurement is categorical cross-entropy due to the prediction output and input being categorical labels in the end, for the optimizer, we will stick with adam because it works in most cases, we could adjust the learning rate for it but I don\u2019t think that's necessary.\n","f387cb5d":"from the tags graph, we can observe that B-geo places are overrepresented, that might confuse the model, and also I-nat and I-gpe are almost non-existent in the dataset, we can already predict that the model might have issues classifying these 2 tags because of not enough training data for them.","85e060af":"now lets create a function so anyone can input their data and the model will do entity recognition on it!","2e8d8789":"# Building the Model","97abe399":"lets finally pad our sentenses using a max length of 50","0083d68e":"# Preparing Data","ef6f8de6":"# Data Exploration ","ed21ac1b":"the data has some na values, those values should be the number of the sentese, lets fix that","4aa5e25d":"seperate the words and tags into their own lists, later on this will be used for various actions including making the training and testing datasets and also when doing the prediction, another important number we need is the total number of tags which will be used for output sizes ","969ac421":"Finally, we now train our model with these parameters\n\n- batch size: the number of words the model will train on at each time, the reason we chose 64 is sort of arbitrary, however, 64 * 540(total number of batches) = 34560 which is very close to our total number of words and tags, a higher batch number might speed up the training but will also reduce the accuracy.\n\n\n- epochs: the number of times the model will train through all the data, a higher number of epochs will not necessarily improve the accuracy. in fact, it might cause overfitting that is why we will stick to only 3 epochs.\n\n\n- validation split: the amount of data which will be used to validate the model during training, we will use 20% of our training dataset (do not confuse validation with testing, both are totally different things)\n","4bd6bf43":"When training a machine learning model we need to use sentences of an equal length, we are going to pad each of our sentences to a total length of 50 words, this might cut some sentences short but I believe thats fine since those are only a very few. an index mapping is a way we can link the tags and words numerically, each word has an index number and each tag has an index number as well, we can pass these number to our neural network and it will be able to learn with it efficiently, when we are doing prediction we use the predicted number as an index in our tags, words index list and get the actual tag, word.","4cc4e650":"after building the model, the summary function shows us all the layers of the model and their parameters, inputs, and outputs, a better way to show such information is by using the plot_model method\n\nwe can see the final TimeDistributed layer outputs 50 tags of 17 types\n","a0a10d2b":"# Data Visualization \nlets take a look at the distrbution of our words and tags in graphs which are easier to understand","f3aa9183":"The goal of this project is to implement a bi-directional LSTM functional neural network that can classify named entities. the dataset has been extracted from GMB corpus and it is structured in a way that makes it easier to train a model for named entity recognition or part of speech tagging, however, we will be making use of only the named entity recognition part.\n\nincluded entities:\n\n- geo = Geographical Entity\n- org = Organization\n- per = Person\n- gpe = Geopolitical Entity\n- tim = Time indicator\n- art = Artifact\n- eve = Event\n- nat = Natural Phenomenon\n","17c8d3d0":"lets make a class that will get get us a full sentense from our data, this is just for data exploration","3ea236f4":"first lets take a look at our dataset by using the head function "}}