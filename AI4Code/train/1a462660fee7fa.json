{"cell_type":{"98ba0b0a":"code","19801bde":"code","424c9224":"code","ea045f39":"code","c414f932":"code","d8ab03fb":"code","36a35c0c":"code","c43e7b86":"code","d108eb1f":"code","67134505":"code","a61c9cd3":"code","19e5e72e":"code","a3fc0e27":"code","3a18dcae":"code","587a7262":"code","aabc12b5":"code","0f7c3c77":"code","68e00a9d":"code","b07543f1":"code","9b58e6c0":"code","50593a4b":"code","f556985a":"code","211d80cc":"code","7f66335d":"code","470a5ab1":"code","3c7a10c6":"code","28d4450d":"code","ab2440ab":"code","aa8d60f7":"code","b6aa587e":"code","a2addde2":"code","bc71c6c2":"markdown","5e285333":"markdown","2464293c":"markdown","d6674e40":"markdown","767105cf":"markdown","5b3c5db2":"markdown","66b9dfe7":"markdown","6c32e991":"markdown","366e45c7":"markdown","4cb95132":"markdown","3e1d90e3":"markdown"},"source":{"98ba0b0a":"import numpy as np\nimport pandas as pd","19801bde":"import os\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.filterwarnings('ignore')","424c9224":"#importing the training data\ndf=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\nprint(df.shape)\ndf.head(10)","ea045f39":"df['sentiment'] = np.where(df['sentiment'] == 'positive', 1, 0)","c414f932":"df.head()","d8ab03fb":"df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']","36a35c0c":"import spacy\nspacy.load('en_core_web_sm')","c43e7b86":"import torch\nimport torch.nn.functional as F\nimport torchtext\nimport time\nimport random\nimport pandas as pd","d108eb1f":"# general Settings\n\nRANDOM_SEED = 123\ntorch.manual_seed(RANDOM_SEED)\n\nVOCABULARY_SIZE = 20000\nLEARNING_RATE = 0.005\nBATCH_SIZE = 128\nNUM_EPOCHS = 50\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nEMBEDDING_DIM = 128\nHIDDEN_DIM = 256\nNUM_CLASSES = 2","67134505":"# Define feature processing\nTEXT = torchtext.legacy.data.Field(tokenize = 'spacy', tokenizer_language = 'en_core_web_sm')","a61c9cd3":"# Define Label processing\n\nLABEL = torchtext.legacy.data.LabelField(dtype = torch.long)","19e5e72e":"df.to_csv('moviedata.csv', index = None)\ndf = pd.read_csv('moviedata.csv')\ndf.head()","a3fc0e27":"# process the dataset\n\nfields = [('TEXT_COLUMN_NAME', TEXT), ('LABEL_COLUMN_NAME', LABEL)]\n\ndataset = torchtext.legacy.data.TabularDataset(\n                    path = 'moviedata.csv',\n                    format = 'csv',\n                    skip_header = True,\n                    fields = fields\n)","3a18dcae":"# Split dataset into train and test set\n\ntrain_data, test_data = dataset.split(split_ratio = [0.8, 0.2], random_state = random.seed(RANDOM_SEED))\n\nprint('Length of train data', len(train_data))\nprint('Length of test data', len(test_data))","587a7262":"train_data, val_data = train_data.split(split_ratio = [0.85, 0.15], random_state = random.seed(RANDOM_SEED))\n\nprint('Length of train data', len(train_data))\nprint('Length of valid data', len(val_data))","aabc12b5":"# Look at first traning example\n\nprint(vars(train_data.examples[2000]))","0f7c3c77":"# Build Vocabulary\n\nTEXT.build_vocab(train_data, max_size = VOCABULARY_SIZE)\nLABEL.build_vocab(train_data)\n\nprint(f'vocabulary size: {len(TEXT.vocab)}')\nprint(f'Label Size: {len(LABEL.vocab)}')","68e00a9d":"# Lets look at most common words\n\nprint(TEXT.vocab.freqs.most_common(30))","b07543f1":"# Token corresponding to first 10 Indices\n\nprint(TEXT.vocab.itos[:20]) #itos = Integer to string","9b58e6c0":"print(TEXT.vocab.stoi['top'])","50593a4b":"# class labels to Integer from string\n\nprint(LABEL.vocab.stoi)","f556985a":"LABEL.vocab.freqs","211d80cc":"# Define Dataloader\n\ntrain_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n        (train_data, val_data, test_data),\n        batch_size = BATCH_SIZE,\n        sort_within_batch = False,\n        sort_key = lambda x : len(x.TEXT_COLUMN_NAME),\n        #device = DEVICE\n    )","7f66335d":"# Testing the iterators (note that the number of rows depends on the longest document in the respective batch):\n\nprint('Train')\nfor batch in train_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break\n    \nprint('\\nValid:')\nfor batch in valid_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break\n    \nprint('\\nTest:')\nfor batch in test_loader:\n    print(f'Text matrix size: {batch.TEXT_COLUMN_NAME.size()}')\n    print(f'Target vector size: {batch.LABEL_COLUMN_NAME.size()}')\n    break","470a5ab1":"train_loader, valid_loader, test_loader = torchtext.legacy.data.BucketIterator.splits(\n        (train_data, val_data, test_data),\n        batch_size = BATCH_SIZE,\n        sort_within_batch = False,\n        sort_key = lambda x : len(x.TEXT_COLUMN_NAME),\n        device = DEVICE\n    )","3c7a10c6":"class RNN(torch.nn.Module):\n    \n    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n        super().__init__()\n\n        self.embedding = torch.nn.Embedding(input_dim, embedding_dim)\n        \n        self.rnn = torch.nn.LSTM(embedding_dim,\n                                 hidden_dim)        \n        \n        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n        \n\n    def forward(self, text):\n        # text dim: [sentence length, batch size]\n        \n        embedded = self.embedding(text)\n        # embedded dim: [sentence length, batch size, embedding dim]\n        \n        output, (hidden, cell) = self.rnn(embedded)\n        # output dim: [sentence length, batch size, hidden dim]\n        # hidden dim: [1, batch size, hidden dim]\n\n        hidden.squeeze_(0)\n        # hidden dim: [batch size, hidden dim]\n        \n        output = self.fc(hidden)\n        return output","28d4450d":"torch.manual_seed(RANDOM_SEED)\nmodel = RNN(input_dim=len(TEXT.vocab),\n            embedding_dim=EMBEDDING_DIM,\n            hidden_dim=HIDDEN_DIM,\n            output_dim=NUM_CLASSES # could use 1 for binary classification\n)\n\nmodel = model.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)","ab2440ab":"def compute_accuracy(model, data_loader, device):\n\n    with torch.no_grad():\n\n        correct_pred, num_examples = 0, 0\n\n        for i, (features, targets) in enumerate(data_loader):\n\n            features = features.to(device)\n            targets = targets.float().to(device)\n\n            logits = model(features)\n            _, predicted_labels = torch.max(logits, 1)\n\n            num_examples += targets.size(0)\n            correct_pred += (predicted_labels == targets).sum()\n    return correct_pred.float()\/num_examples * 100","aa8d60f7":"start_time = time.time()\n\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    for batch_idx, batch_data in enumerate(train_loader):\n        \n        text = batch_data.TEXT_COLUMN_NAME.to(DEVICE)\n        labels = batch_data.LABEL_COLUMN_NAME.to(DEVICE)\n\n        ### FORWARD AND BACK PROP\n        logits = model(text)\n        loss = F.cross_entropy(logits, labels)\n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        ### UPDATE MODEL PARAMETERS\n        optimizer.step()\n        \n        ### LOGGING\n        if not batch_idx % 50:\n            print (f'Epoch: {epoch+1:03d}\/{NUM_EPOCHS:03d} | '\n                   f'Batch {batch_idx:03d}\/{len(train_loader):03d} | '\n                   f'Loss: {loss:.4f}')\n\n    with torch.set_grad_enabled(False):\n        print(f'training accuracy: '\n              f'{compute_accuracy(model, train_loader, DEVICE):.2f}%'\n              f'\\nvalid accuracy: '\n              f'{compute_accuracy(model, valid_loader, DEVICE):.2f}%')\n        \n    print(f'Time elapsed: {(time.time() - start_time)\/60:.2f} min')\n    \nprint(f'Total Training Time: {(time.time() - start_time)\/60:.2f} min')\nprint(f'Test accuracy: {compute_accuracy(model, test_loader, DEVICE):.2f}%')","b6aa587e":"import spacy\n\n\nnlp = spacy.blank(\"en\")\n\ndef predict_sentiment(model, sentence):\n\n    model.eval()\n    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n    length = [len(indexed)]\n    tensor = torch.LongTensor(indexed).to(DEVICE)\n    tensor = tensor.unsqueeze(1)\n    length_tensor = torch.LongTensor(length)\n    prediction = torch.nn.functional.softmax(model(tensor), dim=1)\n    return prediction[0][1].item()\n\nprint('Probability positive:')\npredict_sentiment(model, \"This is such an awesome movie, I really love it!\")","a2addde2":"print('Probability positive:')\npredict_sentiment(model, \"I really hate this movie. It is really bad and sucks!\")","bc71c6c2":"# Data Split","5e285333":"# Data Preparation for Batch wise Implimentation","2464293c":"# Model Building","d6674e40":"# Text & label Preparation","767105cf":"# Data Preparation","5b3c5db2":"# Model Testing","66b9dfe7":"# Data Observation after Tokenization","6c32e991":"# Model Run","366e45c7":" 2 extra value in vocabulary is because added (unknown) and (padding)","4cb95132":"# Define Accuracy","3e1d90e3":"# Import Required Libraries & Data Loading"}}