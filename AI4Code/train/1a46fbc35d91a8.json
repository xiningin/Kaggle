{"cell_type":{"e6355d86":"code","63e69591":"code","223c7cbd":"code","11625292":"code","95261b0f":"code","51d480a7":"code","917b99b0":"code","4a84ab3b":"code","b44396f0":"code","5273dc37":"code","22a9c63d":"code","27a5abfd":"code","5a64612e":"code","15430608":"code","c9c590f6":"code","b01adc78":"code","d7be68f6":"code","a1c54e1d":"code","dc68dac5":"code","8fc4e55b":"code","bd5e713c":"code","ea4a15e9":"code","ff9a258b":"code","2d19218e":"code","dfc158fc":"code","53a9543e":"code","b8527477":"code","3914edd9":"code","b84a05ba":"code","5180b4dc":"code","8b659e57":"code","aac1ebbf":"code","0acee573":"code","9cf6ff6e":"code","97a20fe5":"code","ec4e299e":"code","b039040e":"code","94b41992":"code","40bdc85d":"code","43405cc7":"code","c20b6490":"code","52feb59e":"code","a8f6eb77":"code","29a37e48":"code","d05f92d3":"code","7177cccd":"code","e43c8f4e":"code","c95019c7":"code","b1f79fad":"code","7a440aeb":"code","7160ae42":"code","a1a739f1":"code","b52c065d":"code","dbaa8762":"code","1acf3b92":"code","bb9d1821":"code","55ececa9":"code","279d4b7f":"code","9f8227da":"code","fc0c82d4":"code","3a48c97b":"code","68903b74":"code","1c37141d":"code","e64b1314":"code","491811da":"code","12b0b770":"code","fb8cf13a":"code","fd9bb523":"code","0217943d":"code","11f541d9":"code","8c9877a8":"code","f7d2e498":"code","10475115":"code","d9f0a7b2":"code","15b23e18":"code","10290ac9":"code","35fc3bab":"code","4f4935ab":"code","69ae8b46":"code","d15c30a6":"code","51d4eb3c":"code","bbddc3cf":"code","8f85b744":"code","f3362f25":"code","7cbecd89":"code","4aa60c93":"code","3392e705":"code","38b21c8c":"code","523c7fba":"code","5d475378":"code","bd3d3665":"code","dce47ea3":"code","f32ed4af":"code","c258deaa":"markdown","cea86517":"markdown","dcb2ad1f":"markdown","928068af":"markdown","b00d504e":"markdown","8f779707":"markdown","42c6ff3f":"markdown","93902b21":"markdown","1e5d1dea":"markdown","7bba2000":"markdown","4e1201b8":"markdown","4cd6b7bc":"markdown","772cfea9":"markdown","9d8565f4":"markdown","af2fb268":"markdown","11f6cf46":"markdown","ef03b659":"markdown","820c06b5":"markdown","131da85e":"markdown","f0d39c5f":"markdown","f77ba01e":"markdown","0a27e603":"markdown","975d4ea3":"markdown","25b03d3b":"markdown","830e6dab":"markdown","ca0befc8":"markdown","8fb6a0c4":"markdown","896f316c":"markdown","1fe9eef1":"markdown","9d83e700":"markdown","9cd8b981":"markdown","1aa4fff6":"markdown","5a212c6e":"markdown","d083891f":"markdown","fb7c51aa":"markdown","13352fcc":"markdown","d523a75c":"markdown","37671f08":"markdown","8a78236a":"markdown","0a66c29f":"markdown","4a419895":"markdown","02b4bd8d":"markdown","495c2f2d":"markdown","370de9c8":"markdown","3859b41d":"markdown","689b2a7d":"markdown","5b9cd766":"markdown","dd3fb516":"markdown","c0ca6289":"markdown","fcb3185a":"markdown","d5e6b9db":"markdown","11cbe211":"markdown","31c52d83":"markdown","4debaf84":"markdown","3db33892":"markdown","8776f9b3":"markdown","c28cca4b":"markdown","cc1f0060":"markdown","6cee5bec":"markdown","a2c0285f":"markdown","1768e28a":"markdown","f8c3f003":"markdown","86107e07":"markdown","35994d6a":"markdown","28331b64":"markdown"},"source":{"e6355d86":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport platform\nimport warnings\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, VotingClassifier)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.preprocessing import (StandardScaler, OrdinalEncoder)\n\nfrom sklearn.metrics import (classification_report, confusion_matrix)\n\nfrom sklearn.model_selection import (GridSearchCV, learning_curve,\n                                     cross_val_score, train_test_split)\n\n\nwarnings.filterwarnings('ignore')\n\nprint('Python version: {}'.format(platform.python_version()))\nprint('NumPy version: {}'.format(np.__version__))\nprint('pandas version: {}'.format(pd.__version__))","63e69591":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","223c7cbd":"total_num = train_data.shape[0] + test_data.shape[0]\ntest_pct = round(test_data.shape[0] * 100 \/ total_num)\n\nprint('Number of entries in the training dataset: {}'.format(train_data.shape[0]))\nprint('Number of entries in the test dataset: {}'.format(test_data.shape[0]))\n\nprint('Percentage of entries for testing: {}%'.format(test_pct))","11625292":"train_data.head()","95261b0f":"# List of features to view descriptive statistics\nfeatures = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']","51d480a7":"train_data[features].describe()","917b99b0":"train_data[features].describe(include=['O'])","4a84ab3b":"test_data.head()","b44396f0":"test_data[features].describe()","5273dc37":"test_data[features].describe(include=['O'])","22a9c63d":"train_data['Survived'].value_counts(normalize=True)","27a5abfd":"train_data.isna().sum()","5a64612e":"def num_isna(feature):\n    return round(train_data[feature].isna().sum() * 100 \/ train_data.shape[0], 3)\n\nfeatures_na = train_data.columns[train_data.isna().any()].tolist()\n\nfor feature in features_na:\n    print('Percentage of NA values for {}: {}%'.format(feature, num_isna(feature)))","15430608":"test_data.isna().sum()","c9c590f6":"features_na = test_data.columns[test_data.isna().any()].tolist()\n\nfor feature in features_na:\n    print('Percentage of NA values for {}: {}%'.format(feature, num_isna(feature)))","b01adc78":"sns.catplot(x='Sex', y='Survived', kind='bar', data=train_data)","d7be68f6":"sns.distplot(train_data['Age'])","a1c54e1d":"sns.countplot(train_data['Parch'])","dc68dac5":"sns.countplot(train_data['SibSp'])","8fc4e55b":"sns.countplot(train_data['Pclass'])","bd5e713c":"sns.catplot(x='Sex', y='Survived', hue='Pclass', kind='point', data=train_data)","ea4a15e9":"sns.catplot(x='Sex', y='Survived', hue='Embarked', kind='bar', data=train_data)","ff9a258b":"sns.catplot(x='Parch', y='Survived', hue='Sex', kind='point', data=train_data)\nsns.catplot(x='SibSp', y='Survived', hue='Sex', kind='point', data=train_data)","2d19218e":"sns.violinplot(x='Sex', y='Age', hue='Survived', data=train_data)","dfc158fc":"sns.catplot(x='Pclass', y='Age', hue='Sex', kind='swarm', data=train_data)","53a9543e":"sns.catplot(x='Embarked', y='Age', hue='Sex', kind='swarm', data=train_data)\nsns.catplot(x='Embarked', y='Age', hue='Pclass', kind='swarm', data=train_data)","b8527477":"sns.catplot(x='Sex', y='Fare', hue='Survived', kind='boxen', data=train_data)\nsns.catplot(x='Pclass', y='Fare', hue='Survived', kind='boxen', data=train_data)\nsns.catplot(x='Embarked', y='Fare', hue='Survived', kind='boxen', data=train_data)","3914edd9":"sns.relplot(x='Age', y='Fare', hue='Survived', size='Pclass', data=train_data)","b84a05ba":"features_na = ['Cabin', 'Age', 'Embarked', 'Fare']","5180b4dc":"# We calculate the values and apply the tranpose to invert the rows\/columns for easier manipulation\ntrain_data_stats = train_data.describe().T\ntest_data_stats = test_data.describe().T","8b659e57":"train_data_stats.head()","aac1ebbf":"test_data_stats.head()","0acee573":"def age_dist(df, stats):\n    return np.random.randint(stats.at['Age', 'mean'] - stats.at['Age', 'std'],\n                           stats.at['Age', 'mean'] + stats.at['Age', 'std'],\n                           size=df['Age'].isna().sum())\n\n\ntrain_data['Cabin'] = train_data['Cabin'].fillna('N')\n\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n\ntrain_data['Age'][train_data['Age'].isna()] = age_dist(train_data, train_data_stats)","9cf6ff6e":"train_data.isna().sum()","97a20fe5":"test_data[test_data['Fare'].isna()]","ec4e299e":"train_data[train_data['Pclass'] == 3].describe()","b039040e":"test_data['Cabin'] = test_data['Cabin'].fillna('N')\n\ntest_data['Fare'] = test_data['Fare'].fillna(test_data[test_data['Pclass'] == 3]['Fare'].median())\n\ntest_data['Age'][test_data['Age'].isna()] = age_dist(test_data, test_data_stats)","94b41992":"test_data.isna().sum()","40bdc85d":"pd.cut(train_data['Age'].astype(int), 5).cat.categories","43405cc7":"pd.qcut(train_data['Fare'], 4).cat.categories","c20b6490":"for df in [train_data, test_data]:\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n\n    df.loc[ df['Age'] <= 16, 'Age'] = 0\n    df.loc[(df['Age'] > 16) & (df['Age'] <= 32), 'Age'] = 1\n    df.loc[(df['Age'] > 32) & (df['Age'] <= 48), 'Age'] = 2\n    df.loc[(df['Age'] > 48) & (df['Age'] <= 64), 'Age'] = 3\n    df.loc[df['Age'] > 64, 'Age'] = 4\n    df['Age'] = df['Age'].astype(int)\n\n    df.loc[df['Fare'] <= 7.91, 'Fare'] = 0\n    df.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\n    df.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare'] = 2\n    df.loc[df['Fare'] > 31, 'Fare'] = 3\n    df['Fare'] = df['Fare'].astype(int)","52feb59e":"train_data.head()","a8f6eb77":"for df in [train_data, test_data]:\n    df['Surname'] = df['Name'].str.split(',')\n    df['Surname'] = df['Surname'].apply(lambda x: list(x)[0])\n    df['Family'] = df.agg('{0[Surname]}:{0[FamilySize]}'.format, axis=1)","29a37e48":"train_data.head()","d05f92d3":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return ''\n\nfor df in [train_data, test_data]:\n    df['Title'] = df['Name'].apply(get_title)\n\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')","7177cccd":"for df in [train_data, test_data]:\n    df['AdultMale'] = ((df['Age'] > 0) & (df['Sex'] == 'male')).astype(int)","e43c8f4e":"for df in [train_data, test_data]:\n    df['Deck'] = df['Cabin'].str[0]\n    df['Cabin_Extra'] = df['Cabin'].str.contains(' ').astype(int)","c95019c7":"train_data[train_data['Cabin'].str.startswith('F ')]","b1f79fad":"train_data[train_data['Cabin'] != 'N']['Cabin'].values","7a440aeb":"train_data.groupby('Deck')['Survived'].value_counts(normalize=True).unstack()","7160ae42":"sns.catplot(x='Deck', y='Survived', kind='bar', data=train_data)","a1a739f1":"train_data.groupby(['Cabin_Extra'])['Survived'].value_counts(normalize=True).unstack()","b52c065d":"train_data.groupby(['Sex', 'Deck'])['Survived'].value_counts(normalize=True).unstack()","dbaa8762":"train_data.groupby(['Sex', 'Deck', 'Cabin_Extra'])['Survived'].value_counts(normalize=True).unstack()","1acf3b92":"train_data.head()","bb9d1821":"test_data.head()","55ececa9":"plt.figure(figsize=(14,12))\nsns.heatmap(train_data.corr(), annot=True)\nplt.show()","279d4b7f":"def encode_features(df):\n    df['Deck'] = df['Deck'].apply(ord)\n\n    return df\n\nfor df in [train_data, test_data]:\n    df = encode_features(df)","9f8227da":"train_data = pd.get_dummies(train_data, columns=['Sex', 'Pclass', 'Embarked', 'Title', 'Age', 'Fare'])\ntest_data = pd.get_dummies(test_data, columns=['Sex', 'Pclass', 'Embarked', 'Title', 'Age', 'Fare'])","fc0c82d4":"PassengerId = test_data['PassengerId']","3a48c97b":"drop_elements = ['PassengerId', 'Name', 'Family', 'Ticket', 'Cabin', 'Cabin_Extra', 'Surname', 'Family']\n\ntrain_data = train_data.drop(drop_elements, axis=1)\ntest_data = test_data.drop(drop_elements, axis=1)","68903b74":"train_data.head()","1c37141d":"X = train_data.drop('Survived', axis=1)\ny = train_data['Survived']\nX_test = test_data.copy()","e64b1314":"X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)","491811da":"# Copy dataframe to later inspect and compare true values and predictions\nX_validate_df = X_validate.copy()\n\nstandard_scaler = StandardScaler()\nstandard_scaler.fit(X_train)\n\nX_train = standard_scaler.transform(X_train)\nX_validate = standard_scaler.transform(X_validate)","12b0b770":"model_results = pd.DataFrame(columns=['Score', 'Cross-validation score'])","fb8cf13a":"# Parameters obtained with grid search\nparams = {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n\nrandom_forest = RandomForestClassifier(random_state=0, n_jobs=-1).fit(X_train, y_train)\n\nscore = random_forest.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(random_forest, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Random Forest'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","fd9bb523":"decision_tree = DecisionTreeClassifier(random_state=0).fit(X_train, y_train)\n\nscore = decision_tree.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(decision_tree, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Decision Tree'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","0217943d":"# Parameters obtained with grid search\nparams = {'C': 10, 'dual': True, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 0.0001}\n\nlogistic_regression = LogisticRegression(random_state=0, n_jobs=-1).fit(X_train, y_train)\n\nscore = logistic_regression.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(logistic_regression, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Logistic Regression'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","11f541d9":"# Parameters obtained with grid search\nparams = {'n_neighbors': 8, 'weights': 'uniform'}\n\nknn = KNeighborsClassifier().fit(X_train, y_train)\n\nscore = knn.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(knn, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['K-nearest Neighbors'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","8c9877a8":"gaussian_nb = GaussianNB().fit(X_train, y_train)\n\nscore = gaussian_nb.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(gaussian_nb, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Gaussian Naive Bayes'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","f7d2e498":"# Parameters obtained with grid search\nparams = {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n\nsvc = SVC(random_state=0).fit(X_train, y_train)\n\nscore = svc.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(svc, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Support Vector Machine'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","10475115":"# Parameters obtained with grid search\nparams = {'activation': 'tanh', 'early_stopping': False, 'learning_rate': 'constant', 'learning_rate_init': 0.001}\n\nmlp = MLPClassifier(early_stopping=True, random_state=0, max_iter=300).fit(X_train, y_train)\n\nscore = mlp.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(mlp, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Multi-layer Perceptron'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","d9f0a7b2":"# Parameters obtained with grid search\nparams = {'colsample_bytree': 0.7, 'max_depth': 5, 'n_estimators': 200,'objective': 'binary:logistic', 'reg_alpha': 1.2, 'reg_lambda': 1.3, 'subsample': 0.8}\n\nxgboost_model = XGBClassifier(random_state=0, nthread=-1).fit(X_train, y_train)\n\nscore = xgboost_model.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(xgboost_model, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['XGBoost'] = [score, cross_val_score_mean]\n\nprint([score, cross_val_score_mean])","15b23e18":"model_results","10290ac9":"selected_estimator = svc","35fc3bab":"use_feature_selection = False\n\nselected_features = None\n\nif use_feature_selection:\n    selector = SelectFromModel(estimator=selected_estimator)\n    selector = selector.fit(X_train, y_train)\n\n    selected_features = X.columns[selector.get_support()].to_list()\n\n    print('Best features: {}'.format(selected_features))\n\n    X_train = selector.transform(X_train)\n    X_validate = selector.transform(X_validate)","4f4935ab":"# hyper parameters for Random Forest\nforest_grid = {\n  'n_estimators': [100, 200, 500, 1000],\n  'criterion': ['gini', 'entropy'],\n  'max_depth': [2, 4, 6, 8, None],\n  'min_samples_split': [5, 2, 1, 0.2, .05],\n  'min_samples_leaf': [5, 1, 0.2, .05],\n}\n\n# hyper parameters for K-nearest Neighbors\nknn_grid = {'n_neighbors': list(range(1,11)), 'weights': ['uniform', 'distance']}\n\n# hyper parameters for Support Vector Machine\nsvm_grid = {'C':[1,10,100,1000], 'gamma':[1,0.1,0.001,0.0001], 'kernel':['linear','rbf']}\n\n# hyper parameters for Logistic Regression\nlogistic_grid = {\n  'C':[1,10,100,1000], 'penalty':['l1','l2','elasticnet'],\n  'dual': [False, True], 'tol':[1e-4, 1e-5],\n  'solver': ['newton-cg','lbfgs','liblinear','sag','saga']\n}\n\n# hyper parameters for Multi-layer Perceptron\nmlp_grid = {\n  'activation':['identity','logistic','tanh','relu'],\n  'learning_rate': ['constant','invscaling','adaptive'],\n  'learning_rate_init': [0.003, 0.001, 0.0001],\n  'early_stopping':[False, True]\n}\n\n# hyper parameters for XGBoost\nxgboost_grid = {\n  'n_estimators': [200, 500, 1000, 2000],\n  'colsample_bytree': [0.7, 0.8],\n  'max_depth': [5, 10, 15, 20],\n  'reg_alpha': [1.1, 1.2, 1.3],\n  'reg_lambda': [1.1, 1.2, 1.3],\n  'subsample': [0.7, 0.8, 0.9],\n  'objective': ['binary:logistic'],\n}","69ae8b46":"def grid_search(estimator, param_grid, X_train, X_test, y_train, y_test):\n\n    # We can re-run the grid search with the other parameter grids\n    tune_model = GridSearchCV(estimator, param_grid=param_grid, cv=10, n_jobs=-1)\n    tune_model.fit(X_train, y_train)\n\n    print(type(estimator))\n\n    print(\"\\nGrid scores on development set:\\n\")\n\n    means = tune_model.cv_results_['mean_test_score']\n    stds = tune_model.cv_results_['std_test_score']\n\n    print(\"%0.3f (+\/-%0.03f) for %r\\n\" % \n        (means[tune_model.best_index_], stds[tune_model.best_index_] * 2, tune_model.cv_results_['params'][tune_model.best_index_]))\n\n    print(\"Detailed classification report:\\n\")\n    y_pred = tune_model.predict(X_test)\n\n    print(classification_report(y_test, y_pred, target_names=['Died', 'Survived']))\n\n    return tune_model.best_estimator_","d15c30a6":"selected_estimator = grid_search(selected_estimator, svm_grid, X_train, X_validate, y_train, y_validate)","51d4eb3c":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure(facecolor=(1, 1, 1), figsize=(12, 8))\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n      estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std, alpha=0.1,\n                    color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n            label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n            label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n\n    return plt","bbddc3cf":"plot_learning_curve(selected_estimator, 'Learning Curve', X_train, y_train, (0.7, 1.01), cv=10, n_jobs=-1)\nplt.show()","8f85b744":"voting_estimators = [('rf', random_forest), ('lreg', logistic_regression), ('knn', knn), ('svm', svc), ('mlp', mlp), ('xgboost', xgboost_model)]\n\nvoting = VotingClassifier(voting_estimators, 'hard').fit(X_train, y_train)\n\nscore = voting.score(X_train, y_train)\ncross_val_score_mean = cross_val_score(voting, X_train, y_train, cv=10, n_jobs=-1).mean()\n\nmodel_results.loc['Voting Hard'] = [score, cross_val_score_mean]\n\nmodel_results","f3362f25":"final_score = cross_val_score(selected_estimator, X_train, y_train, cv=10)\n\nprint('Accuracy: {:.3f} (+\/- {:.2f})'.format(final_score.mean(), final_score.std() * 2))","7cbecd89":"y_pred = selected_estimator.predict(X_validate)\n\n# Confusion matrix for the Selected Model\nsns.heatmap(confusion_matrix(y_validate, y_pred), annot=True, fmt='d', cmap='Blues')","4aa60c93":"final_score = cross_val_score(voting, X_train, y_train, cv=10)\n\nprint('Accuracy: {:.3f} (+\/- {:.2f})'.format(final_score.mean(), final_score.std() * 2))","3392e705":"y_pred = voting.predict(X_validate)\n\n# Confusion matrix for the Voting Hard Model\nsns.heatmap(confusion_matrix(y_validate, y_pred), annot=True, fmt='d', cmap='Blues')","38b21c8c":"y_pred = selected_estimator.predict(X_validate)","523c7fba":"X_validate_df['Survived'] = y_validate\nX_validate_df['Prediction'] = y_pred","5d475378":"X_validate_df[(X_validate_df['Survived'] != X_validate_df['Prediction']) & (X_validate_df['Survived'] == 1)].head()","bd3d3665":"X_validate_df[(X_validate_df['Survived'] != X_validate_df['Prediction']) & (X_validate_df['Survived'] == 0)].head()","dce47ea3":"if use_feature_selection:\n    X = X[selected_features]\n    X_test = X_test[selected_features]","f32ed4af":"X = standard_scaler.fit_transform(X)\nX_test = standard_scaler.transform(X_test)","c258deaa":"To fill in the missing *Fare* value we will look at the median fare cost for the class of this specific passenger.","cea86517":"### Further Improvements\n\nThere is still a lot of different things that can be done to try to improve our score. Some examples may be:\n- Improve the cross-validation strategy to find the optimal parameters to train our models\n- Try out different types of classifiers\n- Expand the feature engineering\n- Experiment with other techniques like stacking different classifiers ","dcb2ad1f":"The output of the models scores and cross-validation score means show that the **Random Forest** model has the best results.","928068af":"We will create an age distribution in order to fill in the missing age values. For the *Cabin* feature we will assign a value *N* and for the *Embarked* feature because there are only a few, we will assume the most common value.","b00d504e":"### Gaussian Naive Bayes Model","8f779707":"**Is our training data balanced? Do we have a reasonable distribution of passengers who survived and those who did not?**\n\nLooks like it is fairly balanced. Around 61% died while approximately 38% survived.","42c6ff3f":"First, we will import all the necessary libraries","93902b21":"Lets check for any missing values in our data.","1e5d1dea":"Descriptive statistics for numeric and categorical features of the training data:","7bba2000":"For categorical features with many unique values, we will encode them as numeric values using the function *ord()* will return an integer representing the Unicode code point of the character.","4e1201b8":"**What is the distribution of passengers parents\/children aboard?**","4cd6b7bc":"**Which places of embark have the most survivors? Comparing gender**\n\nIt seems passengers that embarked in *Cherbourg (C)* have a slightly higher survival rate, both male and female. The same may not be true for the other two places of embark.","772cfea9":"**False Negative predictions**","9d8565f4":"**What is the distribution of passengers siblings\/spouses aboard?**","af2fb268":"## Modelling","11f6cf46":"We can also add information about if the passenger was an adult male and also if the passenger was travelling alone. We saw earlier when visualizing the data that adult males had a lower survival rate.","ef03b659":"Next, we will look at passengers name and try to infer their title and social status information.","820c06b5":"We will discard some features that are no longer necessary, either were transformed or are not relevant to our modelling step.","131da85e":"Two features we haven't explore in greater detail are the *Ticket* and *Cabin*. First we can extract the first letter of the existing Cabins and called *Deck*.","f0d39c5f":"At this stage we are ready to separate our features from the labels then we split our data into random train and test subsets, with a proportion of 70% of training data and 30% of testing data to fit the models. Before fitting each model we will normalize the input data.","f77ba01e":"### Handling missing values","0a27e603":"Normalize the data. It is important to only use the train dataset to fit the scaler to avoid data leakage issues.","975d4ea3":"**Which age group range have the most survivors? Comparing gender**","25b03d3b":"### Random Forest Model","830e6dab":"We will concatenate the training and testing data this time, to cover all the data, because we want to calculate the descriptive statistics values. We will use these values to fill in some of the missing information.","ca0befc8":"### Decision Tree Model","8fb6a0c4":"**Which classes of passengers survived the most? Comparing gender**\n\nIt looks like the higher their passenger class the better chance of survival and female passengers of upper classes may have a higher survival rate too.","896f316c":"**What is the distribution of passengers in terms of *Sex*, *Pclass*, *Embarked* place and *Age*?**","1fe9eef1":"We will transform the *Fare* and *Age* features into group bins.","9d83e700":"To recap, we have 4 features with missing values: *Cabin*, *Age*, *Embarked* and *Fare*.","9cd8b981":"In the end, we can look at the confusion matrix and check our results.","1aa4fff6":"For categorical features with few unique values, we will use one-hot encoding leveraging the built-in pandas function *pd.get_dummies()*.","5a212c6e":"### XGBoost Gradient Boosting Model","d083891f":"## Exploratory Data Analysis\n\nFirst, we will look at our initial data and observe the distribution of the features and visualize them in the shape of different plots. We will also check for missing values and look for any statistical relationships in the data.","fb7c51aa":"### K-nearest Neighbors Model","13352fcc":"Lets take a look at our data with the new added features.","d523a75c":"We will define a small helper function to calculate the percentage of missing values for a given feature.","37671f08":"**What are the different values for the *Cabin* feature? Does the deck where the passengers were affects survival rate?**","8a78236a":"Finally, we combine *Fare*, *Age* and *Pclass* to observe the distribution of survivors.","0a66c29f":"**Which features contain missing values in the training data?**\n\nCabin, Age and Embarked have missing values.","4a419895":"**False Positive predictions**","02b4bd8d":"## Feature engineering","495c2f2d":"Confusion Matrix for voting estimator","370de9c8":"# Titanic: Machine Learning from Disaster\n\n**Last updated:** 2020\/09\/12\n\n![Titanic](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/640px-RMS_Titanic_3.jpg)\n\n## Introduction\n\nIn this notebook we will try to predict the outcome of the RMS Titanic passengers, which ones survived the shipwreck, by exploring the Titanic dataset and creating a machine learning model. This will be a **classification** problem and we try out different models and ensemble methods and see what works best in this scenario.\n\nThe titanic is a classic introductory dataset and it will allow us to practice a lot of different tasks in the context of Data Science and Machine Learning.\n\nWe will begin by doing some data exploration and visualization to gather a better understanding of the dataset.\n\nNext, we will perform some feature engineering to extract some new insights and add some extra information to feed to our model. We will also deal with any potential missing values.\n\nFinally, we will try to predict the outcome by using different types of models and compare their performances in order to find the most suitable one for this project. In the end, we will attempt to improve our results by optimizing our final model parameters.\n\nTo sum up, in this notebook we will:\n1. Load and perform exploratory data analysis of the Titanic dataset\n2. Feature engineering\n3. Modelling\n4. Optimize and fine-tuning model parameters \n5. Predict the outcome\n\n\nFeel free to share your comments, any tips how to improve this work.\n\nHave a nice day!","3859b41d":"### Support Vector Machine Model","689b2a7d":"**Does family size has any impact on the survival chances? Comparing gender**\n\nIn both plots smaller families had a considerable higher survival rate compared to bigger ones.","5b9cd766":"**Feature selection**\n\nNot all estimators support this method of selecting features since is based on importance weights.","dd3fb516":"Descriptive statistics for numeric and categorical features of the testing data:","c0ca6289":"### Extracting new features\n\n**Look for passengers surname, how many families were travelling together? Is there any connection between family members and survival rate?**","fcb3185a":"### Fine-tuning and hyper parameters optimization\n\nHaving picked our best model, now we will experiment different sets of parameters to check if we can improve its performance. We will look at at three different ways to validate and fine-tune our final model:\n- **Feature selection** - Find out which is the optimal number of features.\n- **Grid Search** - Hyper parameters optimization\n- **Learning curve** - Evaluate training and cross-validation scores, look for possible underfitting or overfitting situations","d5e6b9db":"## Loading the data","11cbe211":"**Did one gender survided more than the other?**\n\nWe can confirm that a significant higher number of female passengers survived compared to the male passengers.","31c52d83":"**Grid Search**\n\nThis might take a while since there are a few combinations to test.","4debaf84":"### Logistic Regression Model","3db33892":"Confusion Matrix for selected estimator","8776f9b3":"**Learning Curve**","c28cca4b":"Correlation of our features.","cc1f0060":"**What is the likelyhood of survival by *Fare* price?**","6cee5bec":"**What is the distribution of passengers classes?**","a2c0285f":"**Which features contain missing values in the testing data?**\n\nCabin, Age and Fare have missing values.","1768e28a":"**What is the distribution of passengers ages?**","f8c3f003":"We will make a helper DataFrame to store each model's performance score.","86107e07":"### Visualizing the data \n\nWe will plot the data distribution and statistical relationships.","35994d6a":"### Predictions\n\nComparing our predictions with the true values can give us more insights how to further improve our results in the future.","28331b64":"### Multi-layer Perceptron Model"}}