{"cell_type":{"c85111a7":"code","d4acdd05":"code","9ee7501a":"code","f243516c":"code","f641e03e":"code","ffead4f8":"code","afdb7587":"code","2af13a79":"code","ead1cfe8":"code","355f31db":"code","b615e6d3":"code","8c229489":"code","affd3fea":"code","81159856":"code","b87359ce":"code","f9fe7e0a":"code","8a111cd9":"code","d6fda4b4":"code","498fd205":"code","68110c6a":"code","6aee6136":"code","8ec92517":"code","c19cb05b":"code","c5cd5a0d":"code","cd99aac2":"code","54498003":"code","8e5b7c74":"code","b252b978":"code","a5be0d95":"code","3573e061":"code","3b37cdf7":"code","901b872a":"code","1e5c2870":"code","03d443b9":"code","7d065df0":"code","ce609cae":"code","fe2dbc81":"code","885468c5":"code","0de302d5":"code","282c0436":"code","6aa04a2e":"code","1240c6e1":"code","3b5f1138":"code","db2b0ee3":"code","853d6cfd":"markdown","393685fa":"markdown","c1a0da5f":"markdown","a71dff2b":"markdown","f16441e5":"markdown","b359d143":"markdown","148bf7ff":"markdown","e18b8f1e":"markdown","e618b225":"markdown","7b6dbfb4":"markdown","183afc9e":"markdown","07c4881b":"markdown","ae018c73":"markdown"},"source":{"c85111a7":"## General Utilities\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport json\nimport re\nimport os\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n## Sklearn Utilities\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n## Tqdm Utilities\nfrom tqdm import tqdm_notebook, tnrange\nfrom tqdm.auto import tqdm\ntqdm.pandas(desc='Progress')\n\n## Bokeh Utilities\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.layouts import column\nfrom bokeh.models import RadioButtonGroup\nfrom bokeh.models import TextInput\nfrom bokeh.layouts import gridplot\nfrom bokeh.models import Div\nfrom bokeh.models import Paragraph\nfrom bokeh.layouts import column, widgetbox\n\n## IPython Utilities\nfrom IPython.display import HTML\n\nimport notebook as widgets\nfrom ipywidgets import interact, interactive, fixed, interact_manual, interactive_output, VBox\n\nfrom IPython.html import widgets\nfrom IPython.display import display, Image, HTML, Markdown, clear_output","d4acdd05":"## Install flair library\n!pip install flair","9ee7501a":"## Install allennlp library\n\n!pip install allennlp","f243516c":"!python -m spacy download en_core_web_md","f641e03e":"## Load Spacy Utilities:\nimport spacy\nimport en_core_web_md\nnlp = en_core_web_md.load()","ffead4f8":"## Flair Utilities\nfrom flair.embeddings import ELMoEmbeddings, PooledFlairEmbeddings, Sentence, DocumentPoolEmbeddings\nfrom typing import List","afdb7587":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head()","2af13a79":"## Information about Metadata:\nmeta_df.info()","ead1cfe8":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nprint(len(all_json))","355f31db":"def cstr(s, color='blue'):\n    return \"<text style=color:{}>{}<\/text>\".format(color, s)\n\ndef printmd(string):\n    display(Markdown(cstr(string)))","b615e6d3":"## JSON File Reader Class\nclass FileReader:\n    \"\"\"FileReader adds break after every words when character length reach to certain amount.\"\"\"\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'","8c229489":"first_row = FileReader(all_json[0])\nprint(first_row)","affd3fea":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","81159856":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])","b87359ce":"df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","f9fe7e0a":"dict_ = None","8a111cd9":"## Adding word count columns for both abstract and body_text\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))","d6fda4b4":"df_covid.head()","498fd205":"## Remove Duplicates\ndf_covid.drop_duplicates(['abstract', 'body_text'], inplace=True)","68110c6a":"## Remove NA's from data\ndf_covid.dropna(inplace=True)\ndf_covid.info()","6aee6136":"## Taking only 12000 articles for analysis:\ndf_covid = df_covid.head(12000)","8ec92517":"## Remove punctuation from each text:\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\ndf_covid['title'] = df_covid['title'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))","c19cb05b":"## Convert each text to lower case:\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))\ndf_covid['abstract'] = df_covid['abstract'].apply(lambda x: lower_case(x))\ndf_covid['title'] = df_covid['title'].apply(lambda x: lower_case(x))","c5cd5a0d":"## Considering body of articles only:\ntext = df_covid[[\"title\"]]","cd99aac2":"text.head()","54498003":"## Converting text dataframe into array:\ntext_arr = text.stack().tolist()\nlen(text_arr)","8e5b7c74":"## Considering only 500 articles for analysis:\nrequire_text = text_arr[:500]","b252b978":"## Using Spacy module for Sentence Tokenization:\nsentences = []\nfor body in tqdm(require_text):\n    doc = nlp(body)\n    for i in doc.sents:\n        if len(i)>10:\n            ## Taking those sentences only which have length more than 10\n            sentences.append(i.string.strip())\n\nprint(len(sentences))","a5be0d95":"## Creating Document Pool Embeddings using Stacked of PooledFlairEmbeddings('pubmed-forward'), PooledFlairEmbeddings('pubmed-backward') & ELMoEmbeddings('pubmed')\ndocument_embeddings = DocumentPoolEmbeddings([PooledFlairEmbeddings('pubmed-forward'),\n                                             PooledFlairEmbeddings('pubmed-backward'),\n                                             ELMoEmbeddings('pubmed')],\n                                             pooling='min')","3573e061":"## Getting sentence embeddings for each sentence and storing those into flair_elmo_ls:\nflair_elmo_ls = []\n\nfor _sent in tqdm(sentences):\n    example = Sentence(_sent)\n    document_embeddings.embed(example)\n    flair_elmo_ls.append(example.get_embedding())","3b37cdf7":"## Converting embeddings into numpy array :\nflair_elmo_arr = [emb.cpu().detach().numpy() for emb in flair_elmo_ls]","901b872a":"tsne = TSNE(verbose=1, perplexity=5)\nX_embedded = tsne.fit_transform(flair_elmo_arr)","1e5c2870":"from sklearn.cluster import MiniBatchKMeans\n\nk = 20\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(flair_elmo_arr)\ny = y_pred","03d443b9":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport random \n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# let's shuffle the list so distinct colors stay next to each other\npalette = sns.hls_palette(20, l=.4, s=.9)\nrandom.shuffle(palette)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\nplt.title(\"t-SNE Covid-19 Articles - Clustered (K-Means) - Flair & Elmo Biomedical Embeddings\")\nplt.show()","7d065df0":"output_notebook()\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= df_covid['title'],\n    authors = df_covid['authors'],\n    journal = df_covid['journal'],\n    abstract = df_covid['abstract_summary'],\n    labels = [\"C-\" + str(x) for x in y_labels]\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n],\n                 point_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\np = figure(plot_width=800, plot_height=800, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"t-SNE Covid-19 Articles, Clustered(K-Means), Flair & Elmo Biomedical Embeddings\", \n           toolbar_location=\"right\")\n\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\n\n# add callback to control \ncallback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var radio_value = cb_obj.active;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            labels = data['desc'];\n            \n            if (radio_value == '20') {\n                for (i = 0; i < x.length; i++) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                }\n            }\n            else {\n                for (i = 0; i < x.length; i++) {\n                    if(labels[i] == radio_value) {\n                        x[i] = x_backup[i];\n                        y[i] = y_backup[i];\n                    } else {\n                        x[i] = undefined;\n                        y[i] = undefined;\n                    }\n                }\n            }\n\n\n        source.change.emit();\n        \"\"\")\n\n# callback for searchbar\nkeyword_callback = CustomJS(args=dict(p=p, source=source), code=\"\"\"\n            \n            var text_value = cb_obj.value;\n            var data = source.data; \n            \n            x = data['x'];\n            y = data['y'];\n            \n            x_backup = data['x_backup'];\n            y_backup = data['y_backup'];\n            \n            abstract = data['abstract'];\n            titles = data['titles'];\n            authors = data['authors'];\n            journal = data['journal'];\n\n            for (i = 0; i < x.length; i++) {\n                if(abstract[i].includes(text_value) || \n                   titles[i].includes(text_value) || \n                   authors[i].includes(text_value) || \n                   journal[i].includes(text_value)) {\n                    x[i] = x_backup[i];\n                    y[i] = y_backup[i];\n                } else {\n                    x[i] = undefined;\n                    y[i] = undefined;\n                }\n            }\n            \n\n\n        source.change.emit();\n        \"\"\")\n\n# option\noption = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n                                  \"C-3\", \"C-4\", \"C-5\",\n                                  \"C-6\", \"C-7\", \"C-8\",\n                                  \"C-9\", \"C-10\", \"C-11\",\n                                  \"C-12\", \"C-13\", \"C-14\",\n                                  \"C-15\", \"C-16\", \"C-17\",\n                                  \"C-18\", \"C-19\", \"All\"], \n                          active=20, callback=callback)\n\n# search box\nkeyword = TextInput(title=\"Search:\", callback=keyword_callback)\n\n#header\nheader = Div(text=\"\"\"<h1>COVID-19 Articles Cluster<\/h1>\"\"\")\n\n# show\nshow(column(header, widgetbox(option, keyword),p))","ce609cae":"def get_similarity(search_string, results_returned = 3):\n    example_text = Sentence(search_string)\n    document_embeddings.embed(example_text)\n    search_vect = example_text.get_embedding()\n    search_vect = search_vect.cpu().detach().numpy()\n    cosine_similarities = pd.Series(cosine_similarity([search_vect], flair_elmo_arr).flatten())\n    output =\"\"\n    for i,j in cosine_similarities.nlargest(int(results_returned)).iteritems():\n        output +='<p style=\"font-family:verdana; font-size:110%;\"> '\n        for i in sentences[i].split():\n            if i.lower() in search_string:\n                output += \" <b>\"+str(i)+\"<\/b>\"\n            else:\n                output += \" \"+str(i)\n        output += \"<\/p><hr>\"\n\n    output = '<h3>Results:<\/h3>'+output\n    display(HTML(output))\n\ntext = widgets.Text(\n    value='virus genetics, origin, and evolution',\n    placeholder='Paste ticket description here!',\n    description='Query:',\n    disabled=False,\n    layout=widgets.Layout(width='50%', height='50px')\n)\n\nout = widgets.Output()\n\ndef callback(_):\n    with out:\n        clear_output()\n        # what happens when we press the button\n        printmd(\"**<font color=orange> -------------------------------------------------------------------------------------------------------- <\/font>**\")        \n        printmd(f\"**<font color=blue>Semantic Search has Started <\/font>**\")\n        get_similarity(text.value)\n        printmd(\"**<font color=orange> -------------------------------------------------------------------------------------------------------- <\/font>**\")        \n\ntext.on_submit(callback)\n# displaying button and its output together\nwidgets.VBox([text, out])","fe2dbc81":"# Install an End-To-End Closed Domain Question Answering System\n!pip install cdqa","885468c5":"## Load Cdqa Utilities:\nfrom ast import literal_eval\n\nfrom cdqa.utils.filters import filter_paragraphs\nfrom cdqa.pipeline import QAPipeline\nfrom cdqa.utils.download import download_model","0de302d5":"## Download BERT Squad 1.1 Pretrained Q&A Model\ndownload_model(model='bert-squad_1.1', dir='.\/models')","282c0436":"## Converting body_text into different paragraphs :\ndf_covid[\"paragraphs\"] = [x.split('\\n') for x in df_covid[\"body_text\"]]","6aa04a2e":"df = filter_paragraphs(df_covid)\ndf.head()","1240c6e1":"cdqa_pipeline = QAPipeline(reader='.\/models\/bert_qa.joblib')\ncdqa_pipeline.fit_retriever(df=df)","3b5f1138":"def get_cdqa_prediction(x):\n    prediction = cdqa_pipeline.predict(x)\n    question = '<h3>Question:<\/h3>'+x\n    answer = '<h3>Answer:<\/h3>'+prediction[0]\n    title = '<h3>Title:<\/h3>'+prediction[1]    \n    paragraph = '<h3>Paragraph:<\/h3>'+prediction[2]    \n    \n    display(HTML(question))\n    display(HTML(answer))\n    display(HTML(title))\n    display(HTML(paragraph))","db2b0ee3":"text = widgets.Text(\n    value='What do we know about diagnostics and surveillance?',\n    placeholder='Paste ticket description here!',\n    description='Question:',\n    disabled=False,\n    layout=widgets.Layout(width='50%', height='50px')\n)\n\nout = widgets.Output()\n\ndef callback(_):\n    with out:\n        clear_output()\n        # what happens when we press the button\n        printmd(\"**<font color=orange> ------------------------------------------------------------------------------------------------------------------------------- <\/font>**\")        \n        printmd(f\"**<font color=blue>COVID-19 (Question & Answering System)<\/font>**\")\n        get_cdqa_prediction(text.value)\n        printmd(\"**<font color=orange> ------------------------------------------------------------------------------------------------------------------------------- <\/font>**\")        \n\ntext.on_submit(callback)\n# displaying button and its output together\nwidgets.VBox([text, out])","853d6cfd":"**Sentence Tokenization**","393685fa":"**An End-To-End Closed Domain Question Answering System (CdQA)**","c1a0da5f":"**Data Pre-Processing\/Cleaning**","a71dff2b":"**Semantic Search**","f16441e5":"**Dimensionality Reduction with t-SNE**","b359d143":"**Helper Functions**","148bf7ff":"**Convert the Data into Pandas DataFrame**","e18b8f1e":"**Create Clusters (K-Means) of Sentence Embeddings**","e618b225":"**Loading Metadata Information**","7b6dbfb4":"**Loading Required Libraries**","183afc9e":"**Loading Flair & Elmo Contextual Biomedical Embeddings**","07c4881b":"## Contents:\n* [Loading Required Libraries](#first-bullet)\n* [Loading Metadata Information](#second-bullet)\n* [Fetching all the JSON Files](#third-bullet)\n* [Helper Functions](#third-bullet)\n* [Data Pre-Processing\/Cleaning](#fourth-bullet)\n* [Sentence Tokenization](#fifth-bullet)\n* [Loading Flair & Elmo Contextual Biomedical Embeddings](#sixth-bullet)\n* [Dimensionality Reduction with t-SNE](#seventh-bullet)\n* [Create Clusters (K-Means) of Sentence Embeddings](#eigth-bullet)\n* [Semantic Search](#ninth-bullet)\n* [An End-To-End Closed Domain Question Answering System (CdQA)](#tenth-bullet)","ae018c73":"**Fetching all the JSON files**"}}