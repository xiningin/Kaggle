{"cell_type":{"8eb4506d":"code","50699743":"code","0060c37a":"code","7cbd4b61":"code","34d73f72":"code","0969d3bf":"code","f070c5b0":"code","f04503cf":"code","f3063fbf":"code","9ff57c1a":"code","110d6d87":"code","78be63c5":"code","b9d9d758":"code","cea32bc8":"code","ee23b79f":"code","1f200667":"markdown","687cc0e4":"markdown","2a88f2bb":"markdown","c6fdaccf":"markdown","b85befe5":"markdown","6742185f":"markdown","57d96ec5":"markdown","d10f3928":"markdown","3411bdbc":"markdown","a031b48d":"markdown","ac996880":"markdown","9b36f0d5":"markdown","a6c478cd":"markdown","76608ba0":"markdown","001e13d9":"markdown","3a8cfcb8":"markdown"},"source":{"8eb4506d":"!pip install transformers==3.5.1 --quiet\n!pip install livelossplot --quiet\n!pip install git+https:\/\/github.com\/PetrochukM\/PyTorch-NLP.git --quiet\n!pip install emoji --quiet","50699743":"!nvidia-smi","0060c37a":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\n\nfrom transformers import BertForSequenceClassification,BertTokenizerFast,AdamW,logging\nlogging.set_verbosity_error()\nimport torch\n\nfrom livelossplot import PlotLosses\n\npd.options.display.max_colwidth = 1000\npd.set_option('display.expand_frame_repr', False)\n\nimport re,emoji\nimport imageio,glob","7cbd4b61":"torch.manual_seed(1)\nprint()","34d73f72":"!test -d \/tmp\/tweeteval || git clone https:\/\/github.com\/cardiffnlp\/tweeteval \/tmp\/tweeteval","0969d3bf":"def load_df(text_path,label_path):\n    with open(text_path,'rt') as fi:\n        texts = fi.read().strip().split('\\n')\n    text_dfs = pd.Series(data=texts,name='text',dtype='str')\n    labels_dfs = pd.read_csv(label_path,names=['label'],index_col=False).label\n    ret_df = pd.concat([text_dfs,labels_dfs],axis=1)\n    return ret_df\n\ntrain_df = load_df('\/tmp\/tweeteval\/datasets\/hate\/train_text.txt','\/tmp\/tweeteval\/datasets\/hate\/train_labels.txt').head(1500)\nval_df = load_df('\/tmp\/tweeteval\/datasets\/hate\/val_text.txt','\/tmp\/tweeteval\/datasets\/hate\/val_labels.txt').head(1500)\ntest_df = load_df('\/tmp\/tweeteval\/datasets\/hate\/test_text.txt','\/tmp\/tweeteval\/datasets\/hate\/test_labels.txt').head(1500)\nprint()","f070c5b0":"def encode_urls(row):\n    row.text = re.sub(r\"(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))\",\"HTTPURL\", row.text)\n    return row\n\ndef encode_mentions_hashtags(row):\n    row.text = row.text.replace('@',' @')\n    row.text = re.sub(r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9-_]+)\",\"@USER\", row.text)\n    row.text = row.text.replace('#',' ')\n    return row\n\ndef encode_emojis(row):\n    row.text = emoji.demojize(row.text)\n    return row\n\ndef remove_extra_spaces(row):\n    row.text = ' '.join(row.text.split())\n    return row\n\ndef lower_text(row):\n    row.text = row.text.lower()\n    return row\n\ndef preprocess_data_df(df):\n    df = df.apply(encode_urls,axis=1)\n    df = df.apply(encode_mentions_hashtags,axis=1)\n    df = df.apply(encode_emojis,axis=1)\n    df = df.apply(remove_extra_spaces,axis=1)\n    df = df.apply(lower_text,axis=1)\n    return df","f04503cf":"train_df = preprocess_data_df(train_df)\nval_df = preprocess_data_df(val_df)\ntest_df = preprocess_data_df(test_df)","f3063fbf":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\ndef get_bert_encoded_data_in_batches(df,batch_size = 0,max_seq_length = 50):\n    global tokenizer\n    data = [(row.text,row.label,) for _,row in df.iterrows()]\n    sampler = torch.utils.data.sampler.SequentialSampler(data)\n    batch_sampler = torch.utils.data.BatchSampler(sampler,batch_size=batch_size if batch_size > 0 else len(data), drop_last=False)\n    for batch in batch_sampler:\n        encoded_batch_data = tokenizer.batch_encode_plus([data[i][0] for i in batch],max_length = max_seq_length,pad_to_max_length=True,truncation=True)\n        seq = torch.tensor(encoded_batch_data['input_ids'])\n        mask = torch.tensor(encoded_batch_data['attention_mask'])\n        yield (seq,mask),torch.LongTensor([data[i][1] for i in batch])","9ff57c1a":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ndevice","110d6d87":"EPOCHS = 5 #@param {type:\"slider\", min:0, max:10, step:1}\nLEARNING_RATE = 0.00005 #@param [1e-5,5e-5,1e-4] {type:\"raw\"}\nBATCH_SIZE = 16 #@param [8,16,32,64] {type:\"raw\"}\nMAX_SEQ_LEN = 50   #@param [50,100,512] {type:\"raw\"}","78be63c5":"dim_reducer = TSNE(n_components=2)\n#dim_reducer = PCA(n_components=2)\n\ndef visualize_layerwise_embeddings(hidden_states,masks,ys,epoch,title,layers_to_visualize=[0,1,2,3,8,9,10,11]):\n    print('visualize_layerwise_embeddings for',title,'epoch',epoch)\n    global dim_reducer\n    !mkdir -p \/tmp\/plots\/{title}\n    num_layers = len(layers_to_visualize)\n    fig = plt.figure(figsize=(24,(num_layers\/4)*6)) #each subplot of size 6x6\n    ax = [fig.add_subplot(num_layers\/4,4,i+1) for i in range(num_layers)]\n    ys = ys.numpy().reshape(-1)\n    for i,layer_i in enumerate(layers_to_visualize):#range(hidden_states):\n        layer_hidden_states = hidden_states[layer_i]\n        averaged_layer_hidden_states = torch.div(layer_hidden_states.sum(dim=1),masks.sum(dim=1,keepdim=True))\n        layer_dim_reduced_vectors = dim_reducer.fit_transform(averaged_layer_hidden_states.numpy())\n        df = pd.DataFrame.from_dict({'x':layer_dim_reduced_vectors[:,0],'y':layer_dim_reduced_vectors[:,1],'label':ys})\n        df.label = df.label.astype(int)\n        sns.scatterplot(data=df,x='x',y='y',hue='label',ax=ax[i])\n        fig.suptitle(f\"{title}: epoch {epoch}\")\n        ax[i].set_title(f\"layer {layer_i+1}\")\n    plt.savefig(f'\/tmp\/plots\/{title}\/{epoch}',format='png',pad_inches=0)\n    print()","b9d9d758":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\").to(device)\nprint(model)","cea32bc8":"%%time\nloss_function = torch.nn.NLLLoss()\noptimizer = AdamW(lr=LEARNING_RATE,params=model.parameters())\nliveloss = PlotLosses()\nfor epoch in range(EPOCHS+1):\n    print(f'epoch = {epoch}')\n    logs = {}\n    if epoch:   #do not train on 0th epoch, only visualize on it\n        model.train(True)   #toggle model in train mode\n        train_correct_preds,train_total_preds,train_total_loss = 0,0,0.0\n        for x,y in get_bert_encoded_data_in_batches(train_df,BATCH_SIZE,MAX_SEQ_LEN):\n            model.zero_grad()\n            sent_ids,masks = x\n            sent_ids = sent_ids.to(device)\n            masks = masks.to(device)\n            y = y.to(device)\n            model_out = model(sent_ids,masks,return_dict=True)\n            log_probs = torch.nn.functional.log_softmax(model_out.logits, dim=1)\n            loss = loss_function(log_probs, y)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #gradient clipping to prevent exploding gradients\n            optimizer.step()\n\n    model.train(False)  #toggle model in eval mode\n    with torch.no_grad():\n        train_correct_preds,train_total_preds,train_total_loss = 0,0,0.0\n        train_masks,train_ys = torch.zeros(0,MAX_SEQ_LEN),torch.zeros(0,1)\n        train_hidden_states = None\n        for x,y in get_bert_encoded_data_in_batches(train_df,BATCH_SIZE,MAX_SEQ_LEN):\n            sent_ids,masks = x\n            sent_ids = sent_ids.to(device)\n            masks = masks.to(device)\n            y = y.to(device)\n            model_out = model(sent_ids,masks,output_hidden_states=True,return_dict=True)\n            log_probs = torch.nn.functional.log_softmax(model_out.logits, dim=1)\n            loss = loss_function(log_probs, y)\n            hidden_states = model_out.hidden_states[1:]\n            \n            train_total_loss += (loss.detach() * y.shape[0])\n            train_preds = torch.argmax(log_probs,dim=1)\n            train_correct_preds += (train_preds == y).float().sum()\n            train_total_preds += train_preds.shape[0]\n\n            train_masks = torch.cat([train_masks,masks.cpu()])\n            train_ys = torch.cat([train_ys,y.cpu().view(-1,1)])\n\n            if type(train_hidden_states) == type(None):\n                train_hidden_states = tuple(layer_hidden_states.cpu() for layer_hidden_states in hidden_states)\n            else:\n                train_hidden_states = tuple(torch.cat([layer_hidden_state_all,layer_hidden_state_batch.cpu()])for layer_hidden_state_all,layer_hidden_state_batch in zip(train_hidden_states,hidden_states))\n        \n        visualize_layerwise_embeddings(train_hidden_states,train_masks,train_ys,epoch,'train_data')\n\n        train_acc = train_correct_preds.float() \/ train_total_preds\n        train_loss = train_total_loss \/ train_total_preds\n        logs['loss'] = train_loss.item()\n        logs['acc'] = train_acc.item()\n        #\n        val_correct_preds,val_total_preds,val_total_loss = 0,0,0.0\n        val_masks,val_ys = torch.zeros(0,MAX_SEQ_LEN),torch.zeros(0,1)\n        val_hidden_states = None\n        for x,y in get_bert_encoded_data_in_batches(val_df,BATCH_SIZE,MAX_SEQ_LEN):\n            sent_ids,masks = x\n            sent_ids = sent_ids.to(device)\n            masks = masks.to(device)\n            y = y.to(device)\n            model_out = model(sent_ids,masks,output_hidden_states=True,return_dict=True)\n            log_probs = torch.nn.functional.log_softmax(model_out.logits, dim=1)\n            loss = loss_function(log_probs, y)\n            hidden_states = model_out.hidden_states[1:]\n            #logging logic\n            val_total_loss += (loss.detach() * y.shape[0])\n            val_preds = torch.argmax(log_probs,dim=1)\n            val_correct_preds += (val_preds == y).float().sum()\n            val_total_preds += val_preds.shape[0]\n\n            val_masks = torch.cat([val_masks,masks.cpu()])\n            val_ys = torch.cat([val_ys,y.cpu().view(-1,1)])\n\n            if type(val_hidden_states) == type(None):\n                val_hidden_states = tuple(layer_hidden_states.cpu() for layer_hidden_states in hidden_states)\n            else:\n                val_hidden_states = tuple(torch.cat([layer_hidden_state_all,layer_hidden_state_batch.cpu()])for layer_hidden_state_all,layer_hidden_state_batch in zip(val_hidden_states,hidden_states))\n        \n        visualize_layerwise_embeddings(val_hidden_states,val_masks,val_ys,epoch,'val_data')\n        val_acc = val_correct_preds.float() \/ val_total_preds\n        val_loss = val_total_loss \/ val_total_preds\n        logs['val_loss'] = val_loss.item()\n        logs['val_acc'] = val_acc.item()\n    if epoch:   #no need to learning-curve plot on 0th epoch\n        liveloss.update(logs)\n        liveloss.send()","ee23b79f":"val_images = []\nfor filename in sorted(glob.glob('\/tmp\/plots\/val_data\/*')):\n    print('appending for',filename)\n    val_images.append(imageio.imread(filename))\nimageio.mimsave('\/tmp\/plots\/val.gif', val_images,duration=len(val_images))\n\ntrain_images = []\nfor filename in sorted(glob.glob('\/tmp\/plots\/train_data\/*')):\n    print('appending for',filename)\n    train_images.append(imageio.imread(filename))\nimageio.mimsave('\/tmp\/plots\/train.gif', train_images,duration=len(train_images)\/2)","1f200667":"## Data preprocessing\n1.   preprocess_data_df: Preprocesses the text of passed dataframe in-place.  \nComment out the operations that you think aren't required for your data *or* go ahead and add more that suit you!\n\n","687cc0e4":"# An unseen way of visualizing sequence embeddings across BERT encoder\u00a0layers\n## A hands-on tutorial \ud83d\udcbb \ud83e\udd13\nWelcome to the accompanying code for [my article](https:\/\/tanmay17061.medium.com\/1d6a351e4568)!  \nGo through the short article once to understand about BERT layers and the motivation behind this notebook. Then come back here and finish this notebook to explore those cool ideas yourself!\ud83c\udd92\n\n### Important: Do not forget to turn on internet connectivity for your notebook before executing the cells!","2a88f2bb":"# Time to draw some inferences!\ud83d\udcaf\u2714\ufe0f\nFinally, try to draw inferences from the 2 *.gif*s!   \nFocus on the embeddings from each layer at a time and try to understand the patterns they show\ud83d\udcc8 .  \n**Download the 2 *.gif* files from *\/tmp\/plots\/* directory.**  \nMine look like this:  \n### Visualizations for train data across layers of a BERT model:\n![\/tmp\/plots\/train.gif](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*h4zTHVFbyDA6WUtinh8zjQ.gif)\n\n### Visualizations for validation data across layers of a BERT model:\n![\/tmp\/plots\/val.gif](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*7fo8LJcGvVmtuODPW8lF3A.gif)","c6fdaccf":"### Dimensionality Reduction\nWe use scikit-learn's [t-SNE](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html) dimension reduction implementation to visualize embeddings on a 2D plane.  \nYou can also use [PCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html) depending on which suits better to your dataset.","b85befe5":"## Initializing model\nInitialize our pre-trained *bert-base-uncased* model.  \n\n**Note:** you can also plug in other transformer-encoder architectures like *RobertaForSequenceClassification*, *DistilBertForSequenceClassification*, etc.  \n\nLet's also print its architecture to understand the kind of giant we are trying to tame here!\ud83d\ude28","6742185f":"Make sure that your environment has GPU support.  \nTransformer based models are huge and become a pain to work with on CPUs \ud83d\ude2d.","57d96ec5":"## Convert your data into a \ud83e\udd17Transformers BERT compatible form\n*get_bert_encoded_data_in_batches*: helper function to provide data in appropriate format to the BERT model (in batches)  \nWe will be using the pre-trained 'bert-base-uncased' tokenizer that comes bundled with the transformers library.","d10f3928":"# Train pipeline \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f\nTime to lift some weights (pun intended).","3411bdbc":"# Data pipeline \ud83d\udcca\nAll the data handling goes here.","a031b48d":"## Stitch the *.png*s into a *.gif* \nLet's take a step further and stitch these *.png*s together into a *.gif* to aid the visualization (make it look awesome!\ud83c\udd92 )","ac996880":"Let's train our BERT model on the dataset satisfactorily!  \nThe below cell will train the model on *train_df* according to the hypeparameters supplied.  \nWe can extract the *train* and *val* layer embeddings by passing the *output_hidden_states=True* argument to the [forward pass](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertForSequenceClassification.forward) of our [BertForSequenceClassification](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#bertforsequenceclassification) model.","9b36f0d5":"The following snippets load the data:\n1. from *train_text.txt* and *train_labels.txt* into train_df\n1. from *val_text.txt* and *val_labels.txt* into val_df\n1. from *test_text.txt* and *test_labels.txt* into test_df\n\nWe will pick our dataset from [this repository](https:\/\/github.com\/cardiffnlp\/tweeteval).  \nFeel free to modify this logic for loading your own dataset! \ud83e\udd29  ","a6c478cd":"Tune hyper-parameters for your BERT model here.","76608ba0":"Check if GPU is available in the environment. If yes, it would be a good idea to use it to train our BERT model! \ud83c\udfc3\u200d\u2642\ufe0f \u23f0","001e13d9":"\ud83d\udca1 **Note:** It is a good idea to manually seed RNGs for reproducibility of your results.","3a8cfcb8":"# Environment Setup \u2699\ufe0f\nInstall all required libraries and import modules\/utilities.  \nWe'll be working with the *BertForSequenceClassification* model from the transformers library in this notebook to visualize how the various layer embeddings evolve over multiple epochs!"}}