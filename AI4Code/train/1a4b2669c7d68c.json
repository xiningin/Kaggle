{"cell_type":{"e3810272":"code","876a014b":"code","21491b7d":"code","b4c2517e":"code","fc24e22e":"code","51875e06":"code","6acd8f24":"code","77dbe951":"code","c1da8b75":"code","3763264a":"code","afc94dc1":"code","94ca7218":"code","10328444":"code","ac5ea63b":"code","b8380413":"code","e051bf28":"code","17e5fdd7":"code","03477305":"code","d244870a":"code","376eba49":"code","e00858ac":"code","e2ea18e9":"code","9be9f800":"code","f80de749":"code","8458983d":"code","fcf98979":"code","2db27b67":"code","a04bcae7":"code","e79dd7bd":"code","b51d50dc":"code","7d8a9fb9":"code","144f254e":"code","1bd807ec":"code","9dad762b":"code","d4e95f21":"code","d1465f33":"code","3134c310":"code","1960f788":"code","517ded34":"code","979806a1":"code","8d7cbacf":"code","77db9509":"code","f21a17d5":"code","0cd5a57c":"code","c63a355f":"code","576ba6f1":"code","2a5ef9d1":"code","1bc0433d":"code","ab849737":"code","e339ff85":"code","514a7e45":"code","207dd121":"code","d479535e":"code","aa6f55f6":"code","88bc94e1":"code","3967d005":"code","245d2123":"code","a8814e50":"code","adb086cb":"code","bef1132b":"code","69dfde15":"code","c6635041":"code","732d9f9f":"code","1ad4911a":"code","ee839d54":"code","d45cbff4":"code","98001af1":"code","f7cf6f0d":"code","d8f93ce7":"code","20ba9609":"code","9a08c0a5":"code","2013d026":"code","f205af13":"code","74f7ae5d":"code","8ab127d4":"code","69b1389f":"code","71baf59c":"code","edfdd082":"code","b18c327a":"code","37c98d77":"code","8227e87e":"code","88463487":"code","d9946dd0":"code","710f4e9b":"code","58d88faa":"code","d56a472d":"code","75e033ad":"code","8c4f36a8":"code","6fa39e8e":"markdown","94008845":"markdown","df3b8a60":"markdown","ad6d18c8":"markdown","eb83c7f4":"markdown","3a2586dc":"markdown","7bffbd64":"markdown","ddf019c0":"markdown","9deb19e2":"markdown","b6c36436":"markdown","6540d674":"markdown","c972ca8f":"markdown","ae55c2e5":"markdown","1367e1d7":"markdown","8048783b":"markdown","674b6fcf":"markdown","25e8673d":"markdown","02c3eda4":"markdown","85334cc6":"markdown","29be4887":"markdown","7f3ff99a":"markdown","09742d90":"markdown","06326361":"markdown","92d6fe08":"markdown","8f3f80e1":"markdown","45d1a92a":"markdown","1df17c13":"markdown","ddc1020d":"markdown","720cb39a":"markdown","acf91748":"markdown","680184f8":"markdown","a10d86fb":"markdown"},"source":{"e3810272":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import  mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV","876a014b":"folder = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\nout_dir = '\/kaggle\/working'\n\n# local run\n# folder = 'data'\n# out_dir = 'output\/'\n\ntrain = pd.read_csv(os.path.join(folder, 'train.csv'))\ntest = pd.read_csv(os.path.join(folder, 'test.csv'))\n\nprint(train.shape)\nprint(test.shape)","21491b7d":"# concat train and test sets st we always perform transformation on both sets\ntest['SalePrice'] = 0\ndata = pd.concat([train, test])\n\nprint(data.shape)\n\n# lowercase all column names for convenience\ndata.columns = [str.lower(cc) for cc in data.columns]\n\n# sale price per square feet is also interested\n# data['sale_price_per_sf'] = data['saleprice'] \/ data['grlivarea']","b4c2517e":"def cal_age_from_built(row):\n    return row['yrsold'] - row['yearbuilt']\n\ndef cal_age_from_remodel(row):\n    return row['yrsold'] - row['yearremodadd']\n\n\ndef fold_zone_type(ms_zone):\n    if ms_zone in ['FV', 'RH', 'C (all)']:\n        return 'Other'\n    else:\n        return ms_zone\n#         return {'RL': 'Residential Low Density'.lower() , \n#                 'RM': 'Residential Medium Density'.lower(),\n#                 None: 'NA'\n#                }[ms_zone]    \n\ndef to_adjacency(cond):\n    if 'RR' in cond:\n        return 'Railroad'\n    if 'Pos' in cond:\n        return 'Positive feature'\n    return {\n        'Artery': 'Arterial street',\n        'Feedr': 'Feeder street',\n        'Norm': 'Normal'    \n        }[cond]","fc24e22e":"def onehot_encode(cat_feat, data, dummy_na=False):\n    # given a categorical column,\n    # perform onehot encode and return encoded DF together with names of new binary columns\n    categories = data[cat_feat].unique()\n    print('there are', len(categories), 'categories as follows:')\n    print(categories)\n    \n    encoded = pd.get_dummies(data[cat_feat], prefix=cat_feat, dummy_na=dummy_na)\n    res = pd.concat([data.drop(columns=[cat_feat]), encoded], axis='columns')\n    new_feat_names = ['_'.join([cat_feat, cc]) for cc in categories]\n    return res, new_feat_names\n\ndef encode_cat_feats(data, cat_feats, dummy_na=False):\n    print('Onehot encode categorical features: ', cat_feats)\n\n    encoded_df = data.copy()\n    # encode 1 cat feature at a time\n    for cf in cat_feats:\n        encoded_df, _ = onehot_encode(cf, encoded_df, dummy_na=dummy_na)\n\n    return encoded_df","51875e06":"def list_numeric_columns(data):\n    return list(data.columns[np.where(data.dtypes != 'object')])\n\ndef list_string_columns(data):\n    return list(data.columns[np.where(data.dtypes == 'object')])\n\ndef split_train_valid(data, target):\n    y = data.pop(target)\n    X = data\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=.1, \n                                                          random_state=1\n                                                         )\n    return X_train, X_valid, y_train, y_valid\n\ndef check_na(data):\n    # check if any NA left\n    na_count = [sum(data[ff].isnull()) for ff in data.columns]\n    return pd.DataFrame({'column': data.columns, 'na_count': na_count}).\\\n              query('na_count > 0')   ","6acd8f24":"def to_quantitative(text_feat, df, scoring):\n    '''\n    Given a feature stored in data as text but actually a quantitative feat, convert it to numerical values\n    via given encoding\n    :param scoring:\n    :param text_feat:\n    :return:\n    '''\n    n_na = sum(df[text_feat].isnull())\n    print('\\t Feature {0} has {1} NAs, they will be filled by 0'.format(text_feat, n_na))\n\n    res = df.copy()\n    res[text_feat].fillna(\"NA\", inplace=True)\n    res[text_feat] = res[text_feat].apply(lambda form: scoring[form])\n    return res\n\ndef quant_to_scores(quant_feats, data, scorings):\n    print('\\n Converting quantitative text features to scores...')\n    score_dict = dict(zip(quant_feats, scorings))\n    \n    for tf in quant_feats:  \n        data = to_quantitative(text_feat=tf, df=data, scoring=score_dict[tf])\n\n    return data","77dbe951":"def make_output(y_pred):\n    test_index = range(len(train) + 1, len(data) + 1)\n    return pd.DataFrame({'Id': test_index, 'SalePrice': y_pred})","c1da8b75":"def get_train_tests(data, target):\n    train_part = data.loc[data[target] > 0]\n    test_part = data.loc[data[target] == 0]\n    return train_part, test_part","3763264a":"target = 'saleprice'\ny_train = data.loc[data[target] > 0][target]","afc94dc1":"num_vars = list_numeric_columns(data)\nna_checker = check_na(data[num_vars] ).sort_values('na_count', ascending=False).\\\n    reset_index(drop=True)\nprint(na_checker)","94ca7218":"to_fill = na_checker['column'].values[3:]\nto_fill","10328444":"data[to_fill] = data[to_fill].fillna(data[to_fill].mean())","ac5ea63b":"data['masvnrarea'].fillna(data['masvnrarea'].mean(), inplace=True)","b8380413":"train_part, test_part = get_train_tests(data, target)","e051bf28":"# columns_with_lots_of_na = na_checker.head(11)['column']\n# print(columns_with_lots_of_na)\n# data = data.drop(columns=columns_with_lots_of_na)","17e5fdd7":"feats0 = ['overallqual',\n          'yearbuilt', 'mosold', 'yrsold', 'grlivarea', 'lotarea'\n         ]\n# overallcond","03477305":"y_train = train_part[target]","d244870a":"X_train = train_part[feats0]\nX_test = test_part[feats0]","376eba49":"lr = LinearRegression()\nlr.fit(X_train, y_train)\nprint('score of linear regressor', lr.score(X_train, y_train))\n\nalphas = np.linspace(-3, 0, 4)\nridge = RidgeCV(alphas=alphas, cv=5)\nridge.fit(X_train, y_train)\nprint('score of ridge regressor', ridge.score(X_train, y_train))","e00858ac":"num_cols = list_numeric_columns(data)\nprint(num_cols)","e2ea18e9":"check_na(data[num_cols]).sort_values('na_count', ascending=False)","9be9f800":"feats0 = ['overallqual',\n          'yearbuilt', 'mosold', 'yrsold', 'grlivarea', 'lotarea'\n         ]\n# overallcond","f80de749":"lr = LinearRegression()","8458983d":"# features for bathrooms, bedrooms\nroom_feats = ['bedroomabvgr', 'fullbath', 'halfbath',\n              'kitchenabvgr', 'totrmsabvgrd'\n             ] # total_bath","fcf98979":"X_train = train_part[feats0 + room_feats]\nX_test = test_part[feats0 + room_feats]\n\nlr.fit(X_train, y_train)\nprint('score of linear regressor', lr.score(X_train, y_train))","2db27b67":"# area-related features\narea_feats = ['1stflrsf', '2ndflrsf', 'lowqualfinsf', 'masvnrarea']","a04bcae7":"X_train = train_part[feats0 + room_feats + area_feats]\nX_test = test_part[feats0 + room_feats + area_feats]\n\nlr.fit(X_train, y_train)\nprint('score of linear regressor', lr.score(X_train, y_train))","e79dd7bd":"check_na(data[['bsmtfinsf1', 'bsmtunfsf',]])","b51d50dc":"# basement features\n# a potential feature is ratio between unfinished basement area and total area\n\n# data['bsmt_unfinished_ratio'] = data['bsmtunfsf'] \/ data['totalbsmtsf']\nbsmt_feats = [ 'totalbsmtsf', \n             ] \n# 'bsmtfullbath', 'bsmthalfbath',  \n# 'bsmtunfsf', 'bsmtfinsf2', 'bsmtfinsf1'","7d8a9fb9":"X_train = train_part[feats0 + room_feats  + bsmt_feats + area_feats]\nX_test = test_part[feats0 + room_feats  + bsmt_feats + area_feats]\n\nlr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","144f254e":"make_output(lr.predict(X_test)).to_csv(os.path.join(out_dir, 'lin_res.csv') , index=False)","1bd807ec":"# garage feats\ngar_feats = [ 'garagecars', 'garagearea'] # garageyrblt: many NA","9dad762b":"X_train = train_part[feats0 + room_feats + area_feats + bsmt_feats + gar_feats]\nX_test = test_part[feats0 + room_feats + area_feats + bsmt_feats + gar_feats]","d4e95f21":"check_na(X_train)","d1465f33":"lr.fit(X_train, y_train)\nround(lr.score(X_train, y_train), 4)","3134c310":"make_output(lr.predict(X_test)).to_csv(os.path.join(out_dir, 'lin_res_2.csv') , index=False)","1960f788":"feats = feats0 + room_feats  + bsmt_feats + area_feats","517ded34":"!pip install dython","979806a1":"from dython.nominal import correlation_ratio\n\nfrom dython.nominal import associations","8d7cbacf":"cat_feats = list_string_columns(data)\nprint('# cat feats: ', len(cat_feats))\nprint(cat_feats)","77db9509":"eta_corrs = [correlation_ratio(train_part[cf], train_part[target]) for cf in cat_feats]\ncorr_target_cat_feats = pd.DataFrame({'cat_feat': cat_feats, 'corr_with_target': eta_corrs\n                                     }).sort_values('corr_with_target', ascending=False)\ncorr_target_cat_feats","f21a17d5":"qual_feats = ['exterqual', 'kitchenqual', 'bsmtqual']\ndata[qual_feats].describe()","0cd5a57c":"data['exterqual'].unique()\ndata['kitchenqual'].unique()\ndata['bsmtqual'].unique()","c63a355f":"six_scale = {\"Ex\": 5, \"Gd\": 4, \"TA\": 3, \"Fa\": 2, \"Po\": 1, \"NA\": 0}\nscorings = [six_scale]*len(qual_feats)\n\ndata = quant_to_scores(qual_feats, data, scorings)\ntrain_part, test_part = get_train_tests(data, target)","576ba6f1":"feats += qual_feats\nX_train = train_part[feats]\nX_test = test_part[feats]","2a5ef9d1":"lr.fit(X_train, y_train)\nlr_score = round(lr.score(X_train, y_train), 4)","1bc0433d":"make_output(lr.predict(X_test)).to_csv(os.path.join(out_dir, 'lin_res_2.csv') , index=False)","ab849737":"# plot corr between 10 cat features with highest corr to target\ntop10_feats = corr_target_cat_feats.head(10)['cat_feat']\nassociations(data[top10_feats], theil_u=True, figsize=(10, 10))","e339ff85":"# to reduce minority \ndata['zone_type'] = data['mszoning'].apply(fold_zone_type)\ndata['adjacency'] = data['condition1'].apply(to_adjacency)\ntrain_part, test_part = get_train_tests(data, target)","514a7e45":"[correlation_ratio(train_part[cf], train_part[target]) \n for cf in ['zone_type', 'adjacency']\n]","207dd121":"check_na(data[['neighborhood']])","d479535e":"# nbhood\nencoded_data, nbh_feats = onehot_encode('neighborhood', data)\nprint(encoded_data.shape)","aa6f55f6":"encoded_data[nbh_feats] .head()","88bc94e1":"train_part, test_part = get_train_tests(encoded_data, target)","3967d005":"feats += nbh_feats\nX_train = train_part[feats]\nX_test = test_part[feats]","245d2123":"to_drop = ['saletype', 'salecondition']","a8814e50":"base_rf = RandomForestRegressor(n_estimators=100, max_features=1.0, n_jobs=-1,\n                               random_state=1,\n                               )","adb086cb":"base_rf.fit(X_train, y_train)\nrf_score = round(base_rf.score(X_train, y_train), 4)\nrf_score","bef1132b":"y_pred = base_rf.predict(X_test)\nmake_output(y_pred).to_csv(os.path.join(out_dir, 'rf_res.csv') , index=False)","69dfde15":"100 * (rf_score - lr_score)\/lr_score","c6635041":"# adding foundation\ndata.foundation.value_counts()","732d9f9f":"check_na(data[['foundation']])","1ad4911a":"encoded_data, fdn_feats = onehot_encode('foundation', encoded_data)","ee839d54":"train_part, test_part = get_train_tests(encoded_data, target)","d45cbff4":"feats += fdn_feats\nX_train = train_part[feats]\nX_test = test_part[feats]","98001af1":"base_rf.fit(X_train, y_train)\nrf_score = round(base_rf.score(X_train, y_train), 4)\nrf_score","f7cf6f0d":"from pprint import pprint\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n# Number of features to consider at every split\nmax_features = [1., 'auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(2, 20, num = 10)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","d8f93ce7":"# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrandom_rf = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                               n_iter = 100, cv = 3, verbose=2, random_state=42, \n                               n_jobs = -1)\n\n\nrandom_rf.fit(X_train, y_train)\n\npprint(random_rf.best_params_)","20ba9609":"random_search_score = random_rf.best_estimator_.score(X_train, y_train)\nprint(round(random_search_score, 4))","9a08c0a5":"!pip install xgboost","2013d026":"!pip install lightgbm","f205af13":"!pip install category_encoders","74f7ae5d":"import category_encoders as ce","8ab127d4":"# concat train and test sets st we always perform transformation on both sets\ntest['SalePrice'] = 0\ndata = pd.concat([train, test])\n\nprint(data.shape)\n\n# lowercase all column names for convenience\ndata.columns = [str.lower(cc) for cc in data.columns]\n\n# sale price per square feet is also interested\n# data['sale_price_per_sf'] = data['saleprice'] \/ data['grlivarea']","69b1389f":"cat_feats = list_string_columns(data)\nprint('# categ feats:', len(cat_feats))","71baf59c":"# try with nominal feats with high correlation with target first\n# eta_corrs = [correlation_ratio(train_part[cf], train_part[target]) for cf in cat_feats]\n# corr_target_cat_feats = pd.DataFrame({'cat_feat': cat_feats, 'corr_with_target': eta_corrs\n#                                      }).sort_values('corr_with_target', ascending=False)\n# corr_target_cat_feats","edfdd082":"?pd.DataFrame.join","b18c327a":"train.columns = [str.lower(cc) for cc in train.columns]\ntest.columns = [str.lower(cc) for cc in test.columns]","37c98d77":"target_enc = ce.TargetEncoder(cols=cat_feats)\ntarget_enc.fit(train[cat_feats] , train[target])\nprint(target_enc.transform(train[cat_feats]).head())","8227e87e":"# Transform the features, \n# rename the columns with _target suffix, and join to dataframe\n# also remove old categ vars\ntrain_TE = train.join(target_enc.transform(train[cat_feats]).add_suffix('_target')).drop(columns=cat_feats)\ntest_TE = test.join(target_enc.transform(test[cat_feats]).add_suffix('_target')).drop(columns=cat_feats)","88463487":"features = train_TE.columns.drop([target, 'id'])\nprint(features)","d9946dd0":"base_rf = RandomForestRegressor()","710f4e9b":"print(check_na(train_TE))\nprint(check_na(test_TE))","58d88faa":"num_feats = list_numeric_columns(train_TE)\ntrain_TE[num_feats] = train_TE[num_feats].fillna(train_TE[num_feats].mean())\ntest_TE[num_feats] = test_TE[num_feats].fillna(test_TE[num_feats].mean())","d56a472d":"print(check_na(train_TE))\nprint(check_na(test_TE))","75e033ad":"base_rf.fit(train_TE[features], train_TE[target])\nbase_rf.score(train_TE[features], train_TE[target])","8c4f36a8":"y_pred = base_rf.predict(test_TE[features])\nmake_output(y_pred).to_csv(os.path.join(out_dir, 'rf_res_TE.csv') , index=False)","6fa39e8e":"## Preprocessing","94008845":"## Define target","df3b8a60":"Fill NAs in certain numeric vars by their means.","ad6d18c8":"# Simple model\nLInear regressors with no derived features.","eb83c7f4":"### Target encoding","3a2586dc":"__note__: adding var \"overallcond\" pull down prediction perf a lot, so this var has problem.","7bffbd64":"As expected: \n+ \"neighborhood\" has very high correlation with target, as it is purely categorical feats, I will deal with it later\n+ \"exterqual\" and \"kitchenqual\" also has high correlation with target, so I will encode them by proper scales.","ddf019c0":"There are two approaches:\n+ predict directly sale price\n+ predict price per SF, then multiply with living area to estimate sale price\n\nI plan to try both, but first we need some helpers.","9deb19e2":"## Numeric features","b6c36436":"## Try other encoding schemes for nominal vars","6540d674":"## Tuning RF \n\nI will tune RF via randomized then grid searches. \n+ A randomized search allows a quick search over hyperparameter space, \n+ which then suggests a clearer direction for an exhaustive grid search.","c972ca8f":"### CatBoost encoding","ae55c2e5":"+ adding garage features not help, so drop them","1367e1d7":"# Random forest\n\n","8048783b":"+ as expected \"foundation\" not help at all, it even pulls down perf.","674b6fcf":"+ adding neighborhood has a whooping effect. It boosts score by >20% and advanced me >450 places on leaderboard.\n+ now we add the next relevant categorical var, \"foundation\" (this is because kitchen and exterior qualities are already added). But should expect that the impact may be marginal, as the correlation is not that high.","25e8673d":"## TODO: drop non-important feats","02c3eda4":"TODO: Drop columns with lots of NAs.","85334cc6":"### Handle missing values","29be4887":"### Random grid search","7f3ff99a":"+ for correlation between 2 categorical vars, check method theils_u in dython","09742d90":"+ adding \"masvnrarea\" boost prediction a bit more","06326361":"Ridge and base linear regressors have same score. So we only need linear regressor.","92d6fe08":"## Helpers for categorical correlaton","8f3f80e1":"## Encode categorical features\n\nI will perform onehot encoding incrementally, starting with features having highest correlations with target.","45d1a92a":"# Categorical features\n\nTo pick good features among categorical vars, we need a way to measure the correlation between a categorical var and our\ncontinuous target. Here comes eta correlation.","1df17c13":"## Helper methods","ddc1020d":"# Summary\n\nI will start with linear regression as base model. \n\n+ I add quantitative features incrementally to see their impact on prediction perf\n+ I will also figure out what is the best perf of a linear model\n\nThen we will switch to ensemble models:\n+ random forest\n+ tune hyperparams of RF to achieve a better model. ","720cb39a":"+ Adding \"bsmtunfsf\" and \"bsmtfinsf1\" helps reach better score on train, but pull down prediction perf on test set. This maybe overfitting.\n\n+ bsmt_unfinished_ratio has NA","acf91748":"We will pick features based on analysis from my [EDA notebook](https:\/\/www.kaggle.com\/victor191\/eda-and-smart-feature-engineer).","680184f8":"# Others","a10d86fb":"## Base RF\n\nNo tuning yet."}}