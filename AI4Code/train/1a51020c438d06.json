{"cell_type":{"f5d68c66":"code","4753c4e6":"code","10c08e6d":"code","634fb0c7":"code","10dbaec4":"code","4b98931b":"code","b7d0df4f":"code","e71cc9c3":"code","d4df75c6":"code","8e0e1d46":"code","9a86331a":"code","d63c0cb3":"code","18f4811d":"code","5706a0d5":"code","4916140f":"code","6e6290ff":"code","0c152b23":"code","cd61764a":"code","19cae4d9":"code","5212739e":"code","2e6515ec":"markdown","39d1741b":"markdown","8918e8df":"markdown","eced5956":"markdown","cbf51922":"markdown","4f3df0db":"markdown","5b58b6a6":"markdown","db471e46":"markdown","bc32333c":"markdown","e32fe865":"markdown","12876459":"markdown","c361f7be":"markdown","3fccaa71":"markdown","79baaea7":"markdown","7cc951f7":"markdown","36d712ce":"markdown","3682eb8f":"markdown","51a511d8":"markdown","c3932de4":"markdown","02607cdf":"markdown","f1a58df5":"markdown","61a76ed3":"markdown"},"source":{"f5d68c66":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/nlp-getting-started'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4753c4e6":"train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint(train.shape, test.shape)\ntrain.sample(10, random_state=26)","10c08e6d":"import re\n\n# Preprocess text\ndef preprocess(df):\n    df_new = df.copy(deep=True)\n    \n    # Get rid of twitter handles, and take lowercase text\n    df_new['text'] = df.apply(lambda row: re.sub('@[A-z0-9]', '', row['text']).lower(), axis=1)\n    \n    # Training feature - keyword + text\n    df_new['text_w_kword'] = df_new.apply(lambda row: 'keyword: ' + str(row['keyword']) + '. '+ str(row['text']), axis=1)\n    return df_new\n\ntrain_prep = preprocess(train)\ntest_prep = preprocess(test)\n\ntrain_prep[40:60]","634fb0c7":"from sklearn.model_selection import train_test_split\n\n# Split data\nX_train, X_valid, y_train, y_valid = train_test_split(train_prep['text_w_kword'], # Inputs\n                                                    train_prep['target'], # Labels\n                                                    test_size=0.1, \n                                                    random_state=1)","10dbaec4":"# Load tokenizer\nfrom transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('\/kaggle\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased\/')\n\n# Get encodings\n# For this dataset, the largest set of tokens was 88 long, so max_length = 100 is fine here\n# Padding means for any dataset shorter than 100, it adds 0s to reach 100 in length\n# Truncation does the opposite, it'll cut any tokens past 100 - not needed here, but good practice!\ntrain_encodings = tokenizer(list(X_train), truncation=True, padding='max_length', max_length=100)\nvalid_encodings = tokenizer(list(X_valid), truncation=True, padding='max_length', max_length=100)","4b98931b":"# Convert to dataset object\nimport tensorflow as tf\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    y_train.values.astype('float32').reshape((-1,1)) # To make it 2 dimensional\n))\n\nvalid_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(valid_encodings),\n    y_valid.values.astype('float32').reshape((-1,1)) # To make it 2 dimensional\n))\n\ntrain_dataset","b7d0df4f":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nes = EarlyStopping(monitor='val_loss', \n                   verbose=1, # Prints outputs\n                   patience=4, # Waits 4 epochs for an improvement\n                   restore_best_weights=True) # Restores model back to best epoch","e71cc9c3":"from tensorflow.keras.optimizers.schedules import PolynomialDecay\nbatch_size = 64 \nnum_epochs = 15\n# The number of training steps in total is the number of samples in the dataset, divided by the batch size \n# then multiplied by the total number of epochs\nnum_train_steps = (X_train.shape[0] \/\/ batch_size) * num_epochs\n\n# This is actually a linear decay from 5x10^-5 to 1x10^-5, not actually polynomial!\nlr_scheduler = PolynomialDecay(\n    initial_learning_rate=5e-5, # Start point\n    end_learning_rate=1e-5, # End point\n    decay_steps=num_train_steps \n    )\n\nfrom tensorflow.keras.optimizers import Adam\nnew_opt = Adam(learning_rate=lr_scheduler)","d4df75c6":"from keras import backend as K\n\ndef f1_score(true, pred): \n\n    ground_positives = K.sum(true, axis=0) + K.epsilon()       # = TP + FN\n    pred_positives = K.sum(pred, axis=0) + K.epsilon()         # = TP + FP\n    true_positives = K.sum(true * pred, axis=0) + K.epsilon()  # = TP\n    \n    precision = true_positives \/ pred_positives \n    recall = true_positives \/ ground_positives\n\n    f1 = 2 * (precision * recall) \/ (precision + recall + K.epsilon())\n    \n    return f1","8e0e1d46":"# Fine tune with native Tensorflow\nfrom transformers import TFDistilBertForSequenceClassification\n\n# Load model, set up for regression\nmodel = TFDistilBertForSequenceClassification.from_pretrained('\/kaggle\/input\/huggingface-bert-variants\/distilbert-base-uncased\/distilbert-base-uncased\/',\n                                                              num_labels=2)\n\nmodel.compile(\n    optimizer=new_opt,\n    # Very important - model outputs logits, not probabilities!\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n)\n\nhistory = model.fit(train_dataset.batch(batch_size),\n                    validation_data=valid_dataset.batch(batch_size),\n                    epochs=num_epochs,\n                    callbacks=[es])","9a86331a":"test_encodings = tokenizer(list(test_prep['text_w_kword']), truncation=True, padding='max_length', max_length=100)\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings)\n))","d63c0cb3":"test_preds = model.predict(test_dataset.batch(1))","18f4811d":"test_prep","5706a0d5":"test_preds.logits[:40]","4916140f":"class_preds = np.argmax(test_preds.logits, axis=1)\nclass_preds","6e6290ff":"valid_preds = model.predict(valid_dataset.batch(batch_size))","0c152b23":"valid_class_preds = np.argmax(valid_preds.logits, axis=1)","cd61764a":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_valid, valid_class_preds))","19cae4d9":"df_submission = pd.DataFrame({'id':test['id'].values,\n                             'target':class_preds})\n\ndf_submission","5212739e":"df_submission.to_csv('submission.csv', index=False)","2e6515ec":"Split training data into train and validation data (used to stop overfitting).","39d1741b":"First, apply same preprocessing steps to our test data - of course without target labels!","8918e8df":"# Modelling","eced5956":"This cell is currently unused, but as the competition is scored using F1 score it might be a good idea to trial using this as a loss function rather than cross entropy loss!","cbf51922":"Crucial step - if we don't set up our encoded sequences as tensorflow datasets we won't be able to train our model.","4f3df0db":"See outputs","5b58b6a6":"Set up function to stop our model overfitting, and restore the model to the cycle (epoch) where it performed best.","db471e46":"Get rid of twitter handles (@sometext98), take the lowercase text, and add in an extra feature for training - the keyword.\n\nFeel free to edit this and see if you can get a better score! What happens if you remove stopwords? Can you use location at all? Does the keyword variable matter at all?\n\nThe reason I included the keyword in the text itself is because BERT only takes text inputs, so we need everything to be in text format that's going to go into the model.","bc32333c":"This next step comes from the Huggingface course, it sets up a learning rate (how much the model changes) and how it should decrease over time.\n\nAgain, feel free to tweak this and see if you can get a better score with different setups!","e32fe865":"# Hi everyone!\n\nIn this notebook I'll walk you through how to fine tune DistilBERT - a transformer encoder - for classification of disaster tweets.\n\nI'll give you a brief overview of transformers & BERT\/DistilBERT, feel free to skip this if you want to just get to the code.\n\n**Note: you'll need in the settings on the right hand side to turn the accelerator on to GPU!**\n\nAll of this theory, and where I learnt about fine tuning transformers, can be found in [this fantastic free course by Huggingface](https:\/\/huggingface.co\/course\/chapter1). [This page](https:\/\/huggingface.co\/transformers\/custom_datasets.html) was also super helpful for using your own datasets for fine tuning.\n\n------------------------\n### What are transformers?\n\nNo, not the shapeshifting robots, transformers are a state of the art architecture used in Natural Language Processing (NLP).\n\nThe transformer architecture consists of 2 components:\n- **Encoder**: something that turns sequences (passages of text) into vectors (lists of numbers), where the numbers represent the meaning of the word, the position of the word in the sequence, and most importantly the **context** of the word relative to other words in the sentence. An encoder **learns language**.\n- **Decorder**: something that can take encoder inputs, and a prompt, and **generate an output**, an example could be language translation or summarising a large piece of text.\n\nThis architecture was [proposed by Google in late 2017](https:\/\/arxiv.org\/pdf\/1706.03762.pdf).\n\nThe (arguably) most important part of this architecture lies in the encoder section - that it learns **context** of words relative to other words.\n\nAs an example, consider the sentences \"I walked down a *bank*\" and \"The *bank* robbers escaped\". In previous, simpler NLP tools such as [TF-IDF](https:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089#:~:text=TF%2DIDF%20stands%20for%20%E2%80%9CTerm,Information%20Retrieval%20and%20Text%20Mining.) 'bank' would be represented the same way as a vector in both sentences, despite having 2 clearly different meanings.\n\nThe encoder learns context using a mechanism called [attention](https:\/\/jalammar.github.io\/illustrated-transformer\/), which effectively figures out how much a token (a token can be a word, a character, or part of a word depending on your approach) relates to other tokens in a sentence.\n\nIn the 2nd sentence, *bank* heavily relates to 'robbers', whereas 'the' and 'escaped' don't relate much to *bank* as they refer to the robbers specifically.\n\nIn short, encoders are really, really good at learning language. Which is great for us - as we can train one that is specifically good at understanding tweets and whether they are disaster related or not. Under the hood an encoder has the same weights and biases you'd find in a deep learning model - and they can be trained accordingly.\n\nThere are plenty of resources out there if you want to learn more on encoders, but hopefully that's a good overview.\n\n\n-----\n### What is DistilBERT? What even is BERT for that matter?\n\nOn the theme above of wanting to use just the encoder part of the transformer architecture, [BERT - or Binary Encoder Representations from Transformers](https:\/\/arxiv.org\/pdf\/1810.04805.pdf), is another creation of Google that was trained on a huge amount of data (including 2,500 million words from Wikipedia!) to learn English language.\n\nIt achieved state of the art results on a variety of NLP tasks, and a lot of it's performance was attributed to it's ability to learn using a whole passage of text at once ('bidirectional' is a bit misleading, it doesn't learn left-to-right or right-to-left) which made it faster to train and gave it superior context compared to one directional models used previously (LSTM, RNNs).\n\nFun fact, most of the reason it was called 'BERT' was because the predecessor of sorts was called [ELMo](https:\/\/arxiv.org\/pdf\/1802.05365.pdf), so you can fully expect a sesame street themed successor...\n\nMore importantly, it can be fine tuned for a variety of tasks by only adding one additional output layer, which is handy for us! Fine tuning just refers to the process of training a model where [you aren't completely starting from scratch](https:\/\/stats.stackexchange.com\/questions\/331369\/what-is-meant-by-fine-tuning-of-neural-network).\n\nDistilBERT is a [smaller, faster version of BERT](https:\/\/arxiv.org\/pdf\/1910.01108.pdf) that is 40% smaller than BERT, but retains 97% of it's performance and is 60% faster to train. This is what we're going to use!\n","12876459":"The Huggingface course goes into far more detail - but the DistilBERT tokenizer returns a dict with 2 things:\n1. input IDs, integers that each token corresponds to, padding tokens are typically 0s\n2. attention masks, 1s indicate a token that should be used by the model and 0s mean the opposite - for any padding tokens we use for sequences (bits of text, in this case tweets) less than 100 tokens long (which will be all of them) we ignore them in the model","c361f7be":"Just for sanity checking - see how validation data predictions look.","3fccaa71":"But taking the largest logit value works just as well - argmax will return the index from a list that has the largest value.\n\nAs a higher logit relates to a higher probability, and the index position is the same as the target labels, we don't need to convert to probabilities to get our labels.","79baaea7":"Load csv files","7cc951f7":"The important bit - setting up our DistilBERT encoder and training it.\n\n**Make sure you have your GPU accelerator turned on here!**\n\nThis cell may take 5-10 minutes to run, but it'll update you on progress.\n\nAgain if you want to use a different model then there are plenty of variants to use (not just BERT related either), but be warned that with the larger models (such as RoBERTa) memory issues are quite common and training can fail.","36d712ce":"Create output dataframe","3682eb8f":"Inspect the logits, [these can be converted to probabilities if you want to](https:\/\/stackoverflow.com\/questions\/46416984\/how-to-convert-logits-to-probability-in-binary-classification-in-tensorflow).","51a511d8":"# Data prep","c3932de4":"And submit! Run this cell, download the outputs, and that should be it.","02607cdf":"Predict test labels (1 = disaster, 0 = not)","f1a58df5":"# Predictions","61a76ed3":"Load tokenizer, a tokenizer just breaks sentences down into words \/ sub words \/ characters depending on which one you use.\n\nDistilBERT uses word pieces (sub words), so small words like 'dog' would stay as they are but larger words and plurals may get broken down further. Learn more [here](https:\/\/huggingface.co\/course\/chapter2\/4?fw=tf).\n\n**This section relies on an external dataset, just click 'add data' in the top right, and search 'huggingface-bert-variants' and you should be able to run the below fine!** \n\nFeel free to try other models as well, but don't forget to change the class you import (e.g. from transformers import YourModelTokenizer."}}