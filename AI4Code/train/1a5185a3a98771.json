{"cell_type":{"7d45aade":"code","9ab075bb":"code","ae3d459b":"code","178ca3c6":"code","680cce8a":"code","f9634cc7":"code","9aa55df9":"code","a300a825":"code","816ecc34":"code","0de09229":"code","8006b645":"code","f760a5b4":"code","7d307973":"code","534d849b":"code","ca246768":"markdown","7816d857":"markdown","6b9267f7":"markdown","6e62ea2b":"markdown","5d2442d5":"markdown","1f5461d6":"markdown","82a37815":"markdown","bbcfb8b6":"markdown","653e8274":"markdown","9a06f731":"markdown","4e8f2eeb":"markdown","49474a92":"markdown","826e7d24":"markdown","71262469":"markdown","475a95d5":"markdown"},"source":{"7d45aade":"import surprise\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9ab075bb":"raw=pd.read_csv('..\/input\/ratings.csv')\nraw.drop_duplicates(inplace=True)\nprint('we have',raw.shape[0], 'ratings')\nprint('the number of unique users we have is:', len(raw.user_id.unique()))\nprint('the number of unique books we have is:', len(raw.book_id.unique()))\nprint(\"The median user rated %d books.\"%raw.user_id.value_counts().median())\nprint('The max rating is: %d'%raw.rating.max(),\"the min rating is: %d\"%raw.rating.min())\nraw.head()","ae3d459b":"#swapping columns\nraw=raw[['user_id','book_id','rating']] \nraw.columns = ['n_users','n_items','rating']\n\nrawTrain,rawholdout = train_test_split(raw, test_size=0.25 )\n# when importing from a DF, you only need to specify the scale of the ratings.\nreader = surprise.Reader(rating_scale=(1,5)) \n#into surprise:\ndata = surprise.Dataset.load_from_df(rawTrain,reader)\nholdout = surprise.Dataset.load_from_df(rawholdout,reader)","178ca3c6":"kSplit = surprise.model_selection.split.KFold(n_splits=10, shuffle=True) # split data into folds. \n","680cce8a":"sim_options = sim_options = {'name': 'cosine',\n               'user_based': False  # compute  similarities between items\n               }\ncollabKNN = surprise.KNNBasic(k=40,sim_options=sim_options) #try removing sim_options. You'll find memory errors. \nrmseKNN = []\nrmseSVD = []\nrmseCo = []\nrmseSlope = []\nfor trainset, testset in kSplit.split(data): #iterate through the folds.\n    collabKNN.fit(trainset)\n    predictionsKNN = collabKNN.test(testset)\n    rmseKNN.append(surprise.accuracy.rmse(predictionsKNN,verbose=True))#get root means squared error\n    \n","f9634cc7":"funkSVD = surprise.prediction_algorithms.matrix_factorization.SVD(n_factors=30,n_epochs=10,biased=True)","9aa55df9":"min_error = 1\nfor trainset, testset in kSplit.split(data): #iterate through the folds.\n    funkSVD.fit(trainset)\n    predictionsSVD = funkSVD.test(testset)\n    rmseSVD.append(surprise.accuracy.rmse(predictionsSVD,verbose=True))#get root means squared error\n    \n    ","a300a825":"coClus = surprise.prediction_algorithms.co_clustering.CoClustering(n_cltr_u=4,n_cltr_i=4,n_epochs=25) \nfor trainset, testset in kSplit.split(data): #iterate through the folds.\n    coClus.fit(trainset)\n    predictionsCoClus = coClus.test(testset)\n    rmseCo.append(surprise.accuracy.rmse(predictionsCoClus,verbose=True))#get root means squared error\n","816ecc34":"slopeOne = surprise.prediction_algorithms.slope_one.SlopeOne()\n","0de09229":"for trainset, testset in kSplit.split(data): #iterate through the folds.\n    slopeOne.fit(trainset)\n    predictionsSlope = slopeOne.test(testset)\n    rmseSlope.append(surprise.accuracy.rmse(predictionsSlope,verbose=True))#get root means squared error","8006b645":"#plotting the prediction data:\nimport matplotlib.pyplot as plt\nfor prediction in compiledPredictions:\n    modelPrediction = plt.plot(rmseKNN,label='knn')\n    modelPrediction = plt.plot(rmseSVD,label='svd')\n    modelPrediction = plt.plot(rmseCo,label='cluster')\n    modelPrediction = plt.plot(rmseSlope,label='slope')\n\n    modelPrediction = plt.xlabel('folds')\n    modelPrediction = plt.ylabel('accuracy')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","f760a5b4":"class HybridFacto(surprise.AlgoBase):\n    def __init__(self,epochs, learning_rate,num_models):\n        self.alpha = np.array([1\/len(num_models)]*len(num_models))\n        self.epochs = epochs\n        self.learning_rate = learning_rate\n    \n    def fit(self,holdout):\n        holdout=holdout.build_full_trainset().build_testset()\n        for epoch in range(self.epochs):\n            \n            predictions = np.array([collabKNN.test(holdout),funkSVD.test(holdout),coClus.test(holdout),slopeOne.test(holdout)])\n            maeGradient = [surprise.accuracy.mae(prediction) for prediction in predictions]\n            newalpha = self.alpha - learning_rate * maeGradient  \n            #convergence check:\n            if newalpha - self.alpha < 0.001:\n                break\n            self.alpha = newalpha\n            \n    def estimate(self,u,i):\n        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n            raise PredictionImpossible('User and\/or item is unkown.')\n        algoResults = np.array([collabKNN.predict(u,i),funkSVD.predict(u,i),coClus.predict(u,i),slopeOne.predict(u,i)])\n        return np.sum(np.dot(self.alpha,algoResults))\n        ","7d307973":"hybrid = HybridFacto(epochs=10,0.05,4)\nhybrid.fit(holdout)\nrmseHyb = []\nfor trainset, testset in kSplit.split(data): #iterate through the folds.\n    predhybrid = Hyhybrid.test(testset)\n    rmseHyb.append(surprise.accuracy.rmse(predhybrid))\n","534d849b":"#plotting the prediction data:\nfor prediction in compiledPredictions:\n    modelPrediction = plt.plot(rmseKNN,label='knn')\n    modelPrediction = plt.plot(rmseSVD,label='svd')\n    modelPrediction = plt.plot(rmseCo,label='cluster')\n    modelPrediction = plt.plot(rmseSlope,label='slope')\n    modelPrediction = plt.plot(rmseHyb,label='Hybrid')\n\n    modelPrediction = plt.xlabel('folds')\n    modelPrediction = plt.ylabel('accuracy')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","ca246768":"Beautiful, we now have four recommender systems begging to be placed into an Ensemble Method. This is where the fun begins. First, lets plot each one to see how they performed. \n\nThen, lets implemented the Ensemble algorithm outlined above.\n","7816d857":"## Hybrid Recommender Systems with Suprise: A weighted appproach\n### We're using Suprise(a sci-kit package for recommender systems) to ensure that the recommender systems that we are using are optimized-- so then we can ensemble them and not worry about flaws in individual implementation. ","6b9267f7":"First, lets read and clean the dataset so we know what we are working with. This I copied from my other kernel regarding Probablistic Matrix Factorization. ","6e62ea2b":"Implementing Collaborative Filtering, Number one on our list:\nCollaborative filtering is a recommender system that recommends based off of similiarity between items. The big idea is that items that are similiar should be similiarly liked by the same user. For example, if you liked Alien, and you really liked Predator, there's a good chance you'll enjoy Alien Versus Predator. We're just doing the same thing with books here. \nIf you'd like to read more, read up here: http:\/\/courses.ischool.berkeley.edu\/i290-dm\/s11\/SECURE\/a1-koren.pdf ","5d2442d5":"Now we'll make a class in Surprise and inherit it from Algobase.","1f5461d6":"### First, lets pick some algorithms to include into our ensemble. We'll choose four. \n1. Collaborative Filtering\n2. Matrix Factorization\n3. collaborative filtering with co-clustering\n4. Collaborative Filtering based on the popular Slope One Algorithm","82a37815":"### Beautiful, lets train more. You're welcome to edit this notebook and try different hyperparameters. The main purpose of this notebook is to show you the ensemble methods, but you can use Suprise's Grid Search CV to find the best possible Hyperparameters. ","bbcfb8b6":"lets test it out:","653e8274":"Perfect, It looks like our KNN is outperforming the rest. Lets try to hybridize the models so we can get the best parts of every model. To do this, we're going to use Suprise to make a new algorithm, and make it out-perform the rest. ","9a06f731":"and lets plot it!\n","4e8f2eeb":"Whenever you're loading a dataset into Surprise, you can use their dataset Reader class, which alleviates a great deal of pain. You can specify a lot of file formats-- but for pandas dataframes, which we're using, you can specify the ratings and the Dataframe.","49474a92":"### Second, lets train our Matrix Factorization Algorithm. \nThis algorithm was created by Simon Funk during the Netflix Prize, and it is called FunkSVD. The big idea behind this algorithm is you try to estimate the best latent factors for the ratings. So, if you have a 100k users and 10k books, you factor the 100k x 10k matrix into the number of factors. In turn, you would be making two 100k x 30 and 30 x 10k matrices. You multiply them together to get the predicted rating. This lets us optimize on the latent factors between users, such as users that are similiar together because they all rated action films, and latent factors between items, like book series like Goosebumps and Steven King. We multiply each of these to get the predicted rating. \n\nIf you'd like to read more, look it up here: https:\/\/papers.nips.cc\/paper\/3208-probabilistic-matrix-factorization.pdf ","826e7d24":"Beautiful, lets train a recommender system using co-clustering collaborative filtering. \nCo-clustering is where you cluster users and items together, using clustering techniques. You identify three clusters. You'll have to sum three things to get a predicted rating:\n    1. You find the cluster for the specified rating of user u and item i, and identify the mean of that cluster. So you find the mean of cluster u_i.\n    2. find the mean of the cluster of item i and subtract that from the average rating of that item.\n    3. find the mean of cluster of user u and substract that from the average rating of that user. \n    \nFor most of these, you'll find that the RSME remains the same for all of the K-Folds. \n\nIf you want to learn more about Co-Clustering, read more here: https:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.113.6458&rep=rep1&type=pdf ","71262469":"# In Pseudo Code, our Algorithm is as follows: \n1. We split the dataset into 10 folds, where we train on 9 of the folds and test on the remaining one, which randomly alternates.. \n2. We run several recommender systems on the dataset, and optimize the recommender systems on the 75% system.\n3. intialize a weighted variable alpha to be 1\/q, where q is the number of recommender systems we use. \n4. let the rated matrix equal alpha * sum(predicted Ratings Matrices) and compare that with the real rating. \n5. Using Gradient Descent, optimize the alpha term over parameter space to be able to optimize to give the most weight to the model which can represent the best prediction.","475a95d5":"Training our last model, we will use the Slope One Collaborative Filtering Algorithm. This algorithm computes the slope of each of the relevant items rated by a user, finds the difference, then computes the prediction. Its a blunt instrument, but its a good heuristic that might improve our ensemble method.  You can read more here: https:\/\/arxiv.org\/abs\/cs\/0702144 "}}