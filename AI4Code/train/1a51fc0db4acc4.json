{"cell_type":{"bdfb60a4":"code","bf40c749":"code","c8e20969":"code","f3e0cd61":"code","5a08c5fc":"code","c40c26e6":"code","70037772":"code","3f923fdc":"code","5e93a41d":"code","c0b90bc5":"code","dbf8e5b1":"code","026dc1c1":"code","7afffd8b":"code","c118a8be":"code","b2883ccc":"code","c1789a28":"code","faa2392f":"code","2abe0d9d":"code","9c4e5127":"code","7a182e1b":"code","ef0f24f1":"code","e900c007":"code","1c0dc6d1":"code","34dff30d":"code","096d1e74":"code","7feed0a8":"code","85c80064":"code","cfcc405c":"code","4615b8bf":"code","6984d76b":"code","bffdbdff":"code","71891b6c":"code","e42ec4d7":"code","5af3828e":"code","a87d1010":"code","98aaa196":"code","ec3de708":"code","d1895d6d":"code","af36d1a2":"code","89b6a250":"code","9f994c9b":"code","97840048":"code","8bcfcebe":"code","4a701929":"markdown","7cc3a8bd":"markdown","54c91979":"markdown","d188a4a5":"markdown","9231ac16":"markdown","abc68a6b":"markdown","83583528":"markdown","e5c928b5":"markdown","0ed23161":"markdown","b9034b27":"markdown","450f5801":"markdown","e065dd0d":"markdown","d1e7f62e":"markdown","69e97607":"markdown","af708b58":"markdown","bd4e6dc2":"markdown","38c01215":"markdown","ce27a4e3":"markdown","5e1cdd62":"markdown","8ff27b47":"markdown","83ec2132":"markdown","5199d90e":"markdown","74679991":"markdown","81a393c8":"markdown","df7619c4":"markdown","32a76a38":"markdown","4a5dcfd9":"markdown","ad9e398d":"markdown"},"source":{"bdfb60a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf40c749":"# Libraries\n!pip install openpyxl\n\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","c8e20969":"# FILE PATH\nTRAIN_PATH = \"\/kaggle\/input\/predicting-food-delivery-time\/Participants Data\/Data_Train.xlsx\"\nTEST_PATH = \"\/kaggle\/input\/predicting-food-delivery-time\/Participants Data\/Data_Test.xlsx\"","f3e0cd61":"df = pd.read_excel(TRAIN_PATH)\ndf.head()","5a08c5fc":"df.info()","c40c26e6":"df.describe()","70037772":"df[\"Restaurant\"]","3f923fdc":"df[\"Location\"]","5e93a41d":"df[\"Location\"].value_counts()","c0b90bc5":"df_location = pd.get_dummies(df, columns=[\"Location\"])\ndf_location.head()","dbf8e5b1":"df_location.columns","026dc1c1":"df[\"Cuisines\"]","7afffd8b":"df_cuisines = df[\"Cuisines\"].str.get_dummies(\", \")\ndf_cuisines","c118a8be":"df_cuisines.columns","b2883ccc":"df[\"Average_Cost\"]","c1789a28":"df[\"Average_Cost\"].value_counts()","faa2392f":"df_average_cost = df[df[\"Average_Cost\"] != \"for\"]\ndf_average_cost = df_average_cost[\"Average_Cost\"].str.replace(\"\u20b9\", \"\")\ndf_average_cost = df_average_cost.str.replace(\",\", \"\")\ndf_average_cost = pd.to_numeric(df_average_cost)\ndf_average_cost","2abe0d9d":"df[\"Minimum_Order\"]","9c4e5127":"df[\"Minimum_Order\"].value_counts()","7a182e1b":"df_minimum_order = df.copy()\nminimum_order = df_minimum_order[\"Minimum_Order\"].str.replace(\"\u20b9\", \"\")\nminimum_order = pd.to_numeric(minimum_order)\nminimum_order","ef0f24f1":"df[\"Rating\"]","e900c007":"df[\"Rating\"].value_counts()","1c0dc6d1":"rating_str = [\"-\", \"NEW\", \"Opening Soon\", \"Temporarily Closed\"]\ndf_rating_num = df[~df[\"Rating\"].isin(rating_str)]\nrating_num = pd.to_numeric(df_rating_num[\"Rating\"])\nmedian = rating_num.median()\nmedian","34dff30d":"df_rating = df.copy()\ndf_rating.loc[df[\"Rating\"].isin(rating_str), \"Rating\"] = str(median)\ndf_rating[\"Rating\"] = pd.to_numeric(df_rating[\"Rating\"])\ndf_rating[\"Rating\"]","096d1e74":"df[\"Votes\"]","7feed0a8":"df_votes = df.copy()\ndf_votes.loc[df[\"Votes\"] == '-', \"Votes\"] = str(0)\ndf_votes[\"Votes\"] = pd.to_numeric(df_votes[\"Votes\"])\ndf_votes[\"Votes\"]","85c80064":"df_votes[\"Votes\"].value_counts()","cfcc405c":"df[\"Reviews\"]","4615b8bf":"df[\"Reviews\"].value_counts()","6984d76b":"df_reviews = df.copy()\ndf_reviews.loc[df[\"Reviews\"] == '-', \"Reviews\"] = str(0)\ndf_reviews[\"Reviews\"] = pd.to_numeric(df_reviews[\"Reviews\"])\ndf_reviews[\"Reviews\"]","bffdbdff":"df_reviews[\"Reviews\"].value_counts()","71891b6c":"df[\"Delivery_Time\"]","e42ec4d7":"df[\"Delivery_Time\"].value_counts()","5af3828e":"df_delivery_time = df.copy()\ndelivery_time = df_delivery_time[\"Delivery_Time\"].str.replace(\" minutes\", \"\")\ndelivery_time = pd.to_numeric(delivery_time)\ndelivery_time","a87d1010":"delivery_time.value_counts()","98aaa196":"def copy(df):\n    return df.copy()\n\ndef drop(df, col):\n    new_df = df.drop(col, axis=1)\n    return new_df\n\ndef drop_row(df, col, find):\n    new_df = df[df[col] != find]\n    return new_df\n\ndef get_dummies(df, col):\n    return pd.get_dummies(df, columns=[col])\n\ndef get_dummies_comma(df, col):\n    new_df = df[col].str.get_dummies(\", \")\n    new_df = pd.concat([df, new_df], axis = 1)\n    new_df.drop(\"Cuisines\", axis=1, inplace=True)\n    return new_df\n\ndef replace(df, col):\n    replace_list = [\"\u20b9\", \",\", \" minutes\"]\n    new_df = df.copy()\n    for rep in replace_list:\n        new_df[col] = new_df[col].str.replace(rep, \"\")\n    return new_df\n\ndef to_numeric(df, col):\n    new_df = df.copy()\n    new_df[col] = pd.to_numeric(new_df[col])\n    return new_df\n\ndef fill(df, col, filled):\n    find = [\"-\", \"NEW\", \"Opening Soon\", \"Temporarily Closed\"]\n    if filled == 'median':\n        num = df[~df[col].isin(find)]\n        num = pd.to_numeric(num[col])\n        filled = str(num.median())\n    new_df = df.copy()\n    new_df.loc[df[col].isin(find), col] = filled\n    return new_df\n\ndef train_pipeline(df):\n    return (df.pipe(replace, \"Delivery_Time\")\n              .pipe(to_numeric, \"Delivery_Time\"))\n    \ndef pipeline(df):\n    return (df.pipe(copy)\n            .pipe(drop, \"Restaurant\")\n            .pipe(get_dummies, \"Location\")\n            .pipe(get_dummies_comma, \"Cuisines\")\n            .pipe(drop_row, \"Average_Cost\", \"for\")\n            .pipe(replace, \"Average_Cost\")\n            .pipe(to_numeric, \"Average_Cost\")\n            .pipe(replace, \"Minimum_Order\")\n            .pipe(to_numeric, \"Minimum_Order\")\n            .pipe(fill, \"Reviews\", \"0\")\n            .pipe(to_numeric, \"Reviews\")\n            .pipe(fill, \"Votes\", \"0\")\n            .pipe(to_numeric, \"Votes\")\n            .pipe(fill, \"Rating\", \"median\")\n            .pipe(to_numeric, \"Rating\"))\n\ntrain = pipeline(df)\ntrain = train_pipeline(train)\ntrain","ec3de708":"train_data = drop(train, \"Delivery_Time\")\ntrain_data","d1895d6d":"train_data_numpy = train_data.to_numpy()\ntrain_data_numpy","af36d1a2":"train_label = train[\"Delivery_Time\"]\ntrain_label","89b6a250":"forest = RandomForestRegressor(n_estimators=10, random_state=42)\nforest.fit(train_data_numpy, train_label)","9f994c9b":"forest_scores = cross_val_score(forest, train_data_numpy, train_label, scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n    \ndisplay_scores(forest_rmse_scores)","97840048":"pd.read_excel(TEST_PATH).head()","8bcfcebe":"test_df = pd.read_excel(TEST_PATH)\n\ndataset = pd.concat([df, test_df], axis=0)\ndataset = pipeline(dataset)\n\ntest = dataset[len(train):] # for 1-removed row\ntest = test.drop(\"Delivery_Time\", axis=1)\n\nprediction = forest.predict(test)\n                            \nsubmission = pd.DataFrame({\"Delivery_Time\": prediction})\nsubmission.to_csv('submission.csv', index=False)","4a701929":"`Average_Cost` column is about the Average cost of the delivery. It is basically the `int` values with the prefix `\u20b9`. So we need to remove the prefix. And there are data with comma like `1,000`, `1,500`. Also we need to remove the rows that has `for` value. Why `for`? Maybe there's some error. And then, we need to convert to `int`","7cc3a8bd":"Now with prepared data, I will train the model with simple `Scikit-Learn` based Machine-Learning. I'll use `RandomForestRegressor` which is very powerful for regression.","54c91979":"RMSE mean for `10-fold cross validation`: 10.172799971500904","d188a4a5":"Let's transform all this sequence with `pipeline`.","9231ac16":"There are only 35 locations in `Location` column. Some parent address like `Kolkata` or `Pune` appears more that one time. But just let them all as different classes. At here, We can use `One-hot Encoding`.","abc68a6b":"# Let's predict the Delivery Time with simple ML\n\n1. Prepare the data\n2. Select a model and train\n3. Solve the problem","83583528":"For this `multi-labeled One-hot Encoding`, we need to process commas at strings while makeing dummies","e5c928b5":"## 3. Solve the problem","0ed23161":"We can see that the `Location` columns seperate into its values and only get 0 or 1 values. 35 columns are newly added because `Location` column has 35 different unique values.","b9034b27":"It may over-generate the columns with encoder. So, How about this methods?","450f5801":"Now get the test data and check the model precision.","e065dd0d":"Cuisines are the collection of the foods they sell. Some restaurants have many kinds of cuisines, and some are not. Like `Location` Column, we can just simply make `One-hot encoding` from here. But think about:","d1e7f62e":"`Restaurant` column is about the ID of the restaurants. Because it doesn't look useful that much, I will drop this column.","69e97607":"ce cream -> [1, 0]\n\nDesserts -> [0, 1]\n\nIce cream, Desserts -> [1, 1]","af708b58":"`Rating` column is basically `float` types but there are some strings, `-`, `NEW`, `Opening Soon` and `Temporarily Closed`. They counts roughly 2k, so rather remove the rows, let's convert the values into `median`","bd4e6dc2":"Let's check the error using cross validation especially the `K-fold`. The `cross_val_score` split the data into `train data` and `validation data`. `K` means we loop this things for `k` times. Set `k = 10`.","38c01215":"`Reviews` column has only `-`, and the others are all `int` types. for `Reviews`, I assume that the default value for reviews is 0. No one reviews.","ce27a4e3":"Unlike some csv files, this xlsx files contains all values as `object`. To predict with machine-learning, we need to convert this value to numeric values like `int`, or `float`.","5e1cdd62":"`Votes` column has only `-`, and the others are all `float` types. for `Votes`, I assume that the default value for votes is 0. No one votes. This is the different.","8ff27b47":"## 1. Prepare the data","83ec2132":"`Minimum_Order` column is similar with the `Average_Cost` column.","5199d90e":"Ice cream -> [1, 0, 0]\n\nDesserts -> [0, 1, 0]\n\nIce cream, Desserts -> [0, 0, 1]","74679991":"Read CSV files using pandas and openpyxl. And then, just take a look.","81a393c8":"train data","df7619c4":"`Location` column is about the location information with words and commas. For more details, Lets see the unique value.","32a76a38":"## 2. Select a model and train","4a5dcfd9":"`Pandas Dataframe` should be converted into `numpy array` to make a model","ad9e398d":"`Delivery_Time` column is the target value we try to predict. For regression, we should convert the `object` value to `int`."}}