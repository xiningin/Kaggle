{"cell_type":{"449718e3":"code","7564bd11":"code","7c437622":"code","c5ff0725":"code","00aa16c8":"code","cddc857f":"code","b4fa354c":"code","748c705b":"code","8c2ae613":"code","1e04ada9":"code","de69a446":"code","8106e74e":"code","05fd5508":"code","84ebc775":"code","936d2a3d":"code","0d3f3088":"code","5e15c6fc":"markdown","cfb62e0a":"markdown","2791d85b":"markdown","c541cc06":"markdown","24560be2":"markdown","08e07efb":"markdown","bd107d43":"markdown","3259f4dc":"markdown","20235a26":"markdown"},"source":{"449718e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport PIL\nfrom sklearn.model_selection import train_test_split","7564bd11":"ROOT_DIR = '\/kaggle\/input\/digit-recognizer'\ntrain_df = pd.read_csv(os.path.join(ROOT_DIR, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(ROOT_DIR, 'test.csv'))","7c437622":"print('Train size: {}, test size: {}'.format(train_df.shape, test_df.shape))","c5ff0725":"train_df.head()","00aa16c8":"# convert DataFrame to numpy array\n\nfeature_train = train_df.to_numpy()[:,1:]\nlabel_train = train_df.to_numpy()[:,0]\n\nfeature_test = test_df.to_numpy()","cddc857f":"feature_train, feature_val, label_train, label_val = train_test_split(feature_train, label_train, test_size=0.2, stratify=label_train)","b4fa354c":"print('Train size: {}, validation size: {}'.format(len(label_train), len(label_val)))","748c705b":"feature_train = feature_train.reshape(-1, 28, 28, 1)\nfeature_val = feature_val.reshape(-1, 28, 28, 1)\n\nfeature_test = feature_test.reshape(-1, 28, 28, 1)","8c2ae613":"# visualize single image\ndef visualize(image):\n    plt.imshow(image[:,:,0], cmap='gray')\n    plt.show()\n    \n# visualize images from different classes\ndef visualize_samples(input_arr, target_arr, num_classes=10):\n    \"\"\"\n    Inputs:\n    - input_arr: our features array\n    - target_arr: our labels array\n    - num_classes: number of classes we want to visualize, default = 10 (maximum).\n    \"\"\"\n    assert (num_classes > 0 and num_classes <= 10), 'Number of classes must be in range [1-10]'\n    fig, axs = plt.subplots(1, num_classes, figsize=(1.5*num_classes, 2))\n    if num_classes == 1:\n        axs.imshow(input_arr[target_arr==0][0][:,:,0], cmap='gray')\n        axs.axis('off')\n        axs.set_title('Digit 0')\n    else:\n        for i in range(num_classes):\n            axs[i].imshow(input_arr[target_arr==i][0][:,:,0], cmap='gray')\n            axs[i].axis('off')\n            axs[i].set_title('Digit {}'.format(i))","1e04ada9":"visualize_samples(feature_train, label_train, 10)","de69a446":"# create a generator\ntrain_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=25,\n    height_shift_range=3,\n    width_shift_range=3\n)","8106e74e":"BATCH_SIZE = 64 # use whichever batch size you want\n\ntrain_ds = train_datagen.flow(feature_train, label_train, batch_size=BATCH_SIZE)  # train_ds is a generator\n\n# val_ds and test_ds are Dataset objects\nval_ds = tf.data.Dataset.from_tensor_slices((feature_val, label_val)).batch(BATCH_SIZE).prefetch(1)\ntest_ds = tf.data.Dataset.from_tensor_slices(feature_test).batch(BATCH_SIZE).prefetch(1)","05fd5508":"layers = [\n    tf.keras.layers.Conv2D(16, kernel_size=3, strides=1, padding='same', input_shape=(28,28,1)),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.Conv2D(64, kernel_size=5, strides=1, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Dropout(0.25),\n    \n    tf.keras.layers.MaxPool2D(strides=2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Dense(10, activation='softmax')\n]\n\nmodel = tf.keras.Sequential(layers=layers)\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, decay_steps=3000, decay_rate=0.95)\noptimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(train_ds, epochs=40, validation_data=val_ds)","84ebc775":"# plot training history of our model\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylabel('Accuracy')\n    plt.ylim([min(plt.ylim()),1])\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.ylabel('Cross Entropy')\n    plt.ylim([0,1.0])\n    plt.title('Training and Validation Loss')\n    plt.xlabel('epoch')\n    plt.show()\n","936d2a3d":"plot_history(history)","0d3f3088":"pred_test = np.argmax(model.predict(test_ds), axis=1)\nout_df = pd.DataFrame({'ImageId': np.arange(1, len(pred_test)+1), 'Label': pred_test})\nout_df.to_csv('output.csv', index=False)","5e15c6fc":"Val accuracy seems to be slightly higher than train accuracy, which is due to data augmentation (and Dropout at train time). Overall, our simple model performs quite well and is not prone to overfitting.","cfb62e0a":"To use with CNN, convert our flatten features from format (N, M) to (N, H, W, C).","2791d85b":"Splitting training-validation set","c541cc06":"### Load data","24560be2":"Plot our learning curves","08e07efb":"## Digit recognizer with simple ConvNet\nFirst, import necessary libraries","bd107d43":"### Make predictions","3259f4dc":"### Data augmentation\nCreate a ImageDataGenerator object which rotates, shifts our images. You can add more transformations, or change the maximum ranges of below transformations.","20235a26":"### Define model and train"}}