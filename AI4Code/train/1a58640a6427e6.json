{"cell_type":{"3d0ad0a9":"code","c925e04c":"code","f0daaa34":"code","e7ee2876":"code","747f5d1d":"code","001e4686":"code","1e940049":"code","55e7ca1d":"code","7b5f395a":"code","dfffbd65":"markdown","fd80509d":"markdown","8a6533a1":"markdown","5c73466a":"markdown","fee1f1e1":"markdown","45ac7937":"markdown","bb426f7a":"markdown","e9308959":"markdown","ed0fe730":"markdown","6dee9a93":"markdown","be4af2ad":"markdown","49869e96":"markdown","5a46d1da":"markdown","6aa0409c":"markdown","a6cf16f6":"markdown","a192c8b7":"markdown"},"source":{"3d0ad0a9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tqdm.notebook\nfrom tqdm.notebook import tqdm\nimport re\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nimport xgboost\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\nfor p in [np, pd, plt, sklearn, xgboost,sns,plt,re]:\n    print (p.__name__, p.__version__)","c925e04c":"import numpy as np \nimport pandas as pd \nimport catboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:#tqdm(df.columns):\n        col_type = df[col].dtypes\n\n        if col_type=='object':\n            df[col] = df[col].astype('category')\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef display_status(message):\n    from IPython.display import display, clear_output\n    import time\n    display(message) # print string\n    clear_output(wait=True)\n    \n\nprint('Available datasets:')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nstart = time.time()\ndisplay_status('Load aggregated data (1\/3)')\nagg_data = pd.read_csv('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/agg_data.csv')\nagg_data = reduce_mem_usage(agg_data)\n\n\nX_test = agg_data[agg_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\nX_test.fillna(0,inplace=True)\n\ndel agg_data\ngc.collect()\n\nbasepath= '..\/input\/competitive-data-science-predict-future-sales\/'\ntest = pd.read_csv( basepath+\"test.csv\" )\n\ndisplay_status('Load serialized models (2\/3) --- Time elapsed:{0} secs'.format(time.time()-start))\nrf = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/rf_pred_30_10.model', 'rb'))\nxgb = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/xgb_pred_100_10.model', 'rb'))\nmeta_model = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/metamodel.model', 'rb'))\n\ndisplay_status('Final prediction (3\/3) --- Time elapsed:{0} secs'.format(time.time()-start))\nmeta_test = pd.DataFrame({'ID':test.index,'pred_1':rf.predict(X_test),'pred_2':xgb.predict(X_test)})\nX_meta_test = meta_test[['pred_1','pred_2']]\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": meta_model.predict(X_meta_test).clip(0, 20)\n})\nsubmission.to_csv('meta_model_pred.csv', index=False)\nsubmission.to_csv('submission.csv', index=False)\nprint('Final prediction submission.csv file has been generated. Thanks for your patience')\n\ndel rf, xgb, meta_model, meta_test, X_meta_test, submission, X_test\ngc.collect()","f0daaa34":"import numpy as np \nimport pandas as pd \nimport catboost\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport re\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nimport time\nimport os\nimport gc\nimport pickle\nfrom xgboost import XGBRegressor\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\npd.set_option('display.max_columns',100)\nimport os\n\n\nbasepath= '..\/input\/competitive-data-science-predict-future-sales\/'\n\nitems = pd.read_csv(basepath+\"items.csv\")\nitem_categories = pd.read_csv(basepath+\"item_categories.csv\")\nshops = pd.read_csv(basepath+\"shops.csv\")\ntrain = pd.read_csv( basepath+\"sales_train.csv\" )\n#train = train.sample(20000) #for quick Proof of Concept, sample 20000\ntest = pd.read_csv( basepath+\"test.csv\" )\nprint('All data has been loaded.')","e7ee2876":"agg_data = pd.read_csv('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/agg_data.csv')\ncols = ['holiday_num','item_cnt_month','next_month_holiday_num','vacation_period']\n\nagg_data[cols].groupby([\"holiday_num\"], as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='holiday_num',y='item_cnt_month',\n                                  title='Current Month Holidays __VS__ Sales', subplots=True, rot=90)\n\nagg_data[cols].groupby([\"next_month_holiday_num\"] , as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='next_month_holiday_num',y='item_cnt_month',\n                                  title='Anticipation of num holidays next month __VS__ Sales', subplots=True, rot=90)\n\n# Remapped to string since the original agg_data was already mean-encoded\ndays = pd.Series(['winter','etc','spring','etc','may',  'summer','summer','summer','etc','winter','winter','winter'])\nagg_data[\"vacation_period\"] = agg_data[\"month\"].map(days).astype('category')\nagg_data[cols].groupby([\"vacation_period\"] , as_index=False \n                      ).agg({\"item_cnt_month\": [\"sum\"] }\n                           ).plot(kind='bar',x='vacation_period',y='item_cnt_month',\n                                  title='Vacation period __VS__ Sales. May is special and deserves its own category', subplots=True, rot=90)\n\ndel agg_data, days\ngc.collect()","747f5d1d":"alldata = None\nalldata = train.merge(items, how='inner', on='item_id' )\nalldata= alldata.merge(item_categories, how='inner', on='item_category_id' )\nalldata = alldata.merge(shops,how='inner', on='shop_id')\nalldata['year'] = pd.to_datetime(alldata['date']).dt.year\nalldata['month'] = pd.to_datetime(alldata['date']).dt.month\nalldata['year_month'] = alldata['year'].astype(str) + '-' + alldata['month'].astype(str)\npopular_game_oct2015 = alldata.groupby(['year_month','item_category_name','item_name'],as_index=False).sum()\ncols=['date',\t'date_block_num',\t'item_name', \t'item_cnt_day', 'shop_id']\n\ntop_100 = popular_game_oct2015[popular_game_oct2015['year_month']=='2015-10'].sort_values(by=['item_cnt_day'], ascending=False).iloc[:100,:]\ntop_100[['item_name','item_cnt_day']].head(15).plot.bar(x='item_name', rot=-90)\n\n\n#free memory\ndel alldata,popular_game_oct2015,top_100\ngc.collect()","001e4686":"# Let sample some of the data and plot its kernel distribution between ('shop_id','item_id') pair\nprint('Train unique:\\n{0}    \\n\\nTest unique:\\n{1}'.format( train.nunique(), test.nunique()))\ntrain_g = sns.jointplot(x=\"shop_id\", y=\"item_id\", data=train[['shop_id','item_id']].sample(10000), kind=\"kde\")\nplt.subplots_adjust(top=0.9)\ntrain_g.fig.suptitle('Train Data KDE(sampled randomly)')\ntest_g = sns.jointplot(x=\"shop_id\", y=\"item_id\", data=test[['shop_id','item_id']].sample(10000), kind=\"kde\")\n\nplt.subplots_adjust(top=0.9)\ntest_g.fig.suptitle('Test Data KDE(sampled randomly)')\n\ndel test_g, train_g\ngc.collect()","1e940049":"num_debut = len(set(test['item_id']) - set(train['item_id']))\ntotal_test = len(set(test['item_id']))\npctg = (num_debut\/total_test)*100\nprint ('num of debut items: {0} ({1}%) out of {2} items'.format(num_debut,pctg,total_test))","55e7ca1d":"# Hyperparameter\ngc.collect()\nagg_data = pd.read_csv('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/agg_data.csv')\n\nX_train = agg_data[agg_data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\ny_train = agg_data[agg_data.date_block_num < 33]['item_cnt_month']\nX_val = agg_data[agg_data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\ny_val = agg_data[agg_data.date_block_num == 33]['item_cnt_month']\nX_test = agg_data[agg_data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\nX_train.fillna(0,inplace=True)\nX_val.fillna(0,inplace=True)\nX_test.fillna(0,inplace=True)\n\nrf_filename = 'rf_pred_30_10.model'\nrf = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/'+rf_filename, 'rb'))\n\ndel agg_data\ngc.collect()\n\n\npredictions = []\nfor tree in rf.estimators_:\n    predictions.append(tree.predict(X_val)[None, :])\n\nfrom sklearn.metrics import mean_squared_error\npredictions = np.vstack(predictions)\nscores = []\n\nacc = []\ntemp = [0] * len(predictions[0])\nfor i in range(len(predictions)):\n    temp += predictions[i] \n    acc.append(temp\/(i+1))\n\nscores=[]\nfor pred in acc:\n    scores.append(np.sqrt(mean_squared_error(y_val, pred)))\n\nscores\nplt.figure(figsize=(10, 6))\nplt.plot(scores, linewidth=3)\nplt.xlabel('num_trees')\nplt.ylabel('RMSE')\n\ndel scores,predictions, rf\ngc.collect()","7b5f395a":"def plot_feature_imp(model,model_name):\n    global X_train\n    df = pd.DataFrame({'feature':X_train.columns,\n                       'feature_importances_':model.feature_importances_}).sort_values(by='feature_importances_', ascending=False)\n    df.plot(x='feature',kind='bar',title='Feature importance for '+model_name)\n\n\nxgb = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/xgb_pred_100_10.model', 'rb'))\nplot_feature_imp(xgb,'XGBoost')\n\nrf = pickle.load(open('\/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/rf_pred_30_10.model', 'rb'))\nplot_feature_imp(rf,'Random Forest')\n\ndel xgb, rf\ngc.collect()","dfffbd65":"### Popular games are bestsellers","fd80509d":"## Thought on Data Leaks\nLet us profile the numbers of unique data and plot pairs of (item_id, plot_id)","8a6533a1":"# > Run the next cell to load preprocessed data, serialized models, and predict\nPlease expect to wait for 10 minutes for ETL, load model, and predict.\nFor recreating model from the scratch, please check this [notebook](https:\/\/www.kaggle.com\/rrrrrikimaru\/create-model-simple-e-to-e-eda-to-ensemble) ","5c73466a":"# 5. Feature Importances","fee1f1e1":"# 1. tldr; Summary\nThe numbers of holidays for each month are added as I hypothesize they affect sales.\n1. Thanks to [Schrodinger](https:\/\/www.kaggle.com\/lonewolf45) for the excellent [notebook](https:\/\/www.kaggle.com\/lonewolf45\/coursera-final-project) that performs extraordinary ETL & XGB modelling. I learn a lot from this, parts of the source code there are reused and refactored to fit the author's coding style. \n\n2. The following are the original contributions by the author:\n> * EDA\n> * Augmenting Russia holiday information to the data\n> * Augmenting popular titles\/keywords to the item category such as 'Call of Duty', 'Star Wars'\n> * Apply Mean encoding to vacation 'season' relative to the target\n> * Ensemble using Linear Regression","45ac7937":"# Recreate models and predict from Scratch\n# > Please check this link >> [notebook](https:\/\/www.kaggle.com\/rrrrrikimaru\/create-model-simple-e-to-e-eda-to-ensemble)  <<\n\n### Thank you for reading till the end.\nAny feedback is very welcomed.","bb426f7a":"### Load Data","e9308959":"as shown above, given just a month away from Nov 2015(the period to predict), the top-15 sales have repeated popular game products such as Assassin's Creed, Uncharted, GTA V contributing to the sales. Capitalizing this on facts, some keywords from the item_name will be used as category.","ed0fe730":"Both plots above of shows different importances between XGB and RF. Although both are still Tree based methods, both are ensembled anyway using Linear Regression. (please refer to the[ source code](https:\/\/www.kaggle.com\/rrrrrikimaru\/create-model-simple-e-to-e-eda-to-ensemble) for the details).","6dee9a93":"In terms of RMSE, we can see above that the num_trees round 10 trees was quite optimum.","be4af2ad":"The data mostly consists of PC, Entertainment, Game related. It can be suspected that the sales will be seasonal based on popularity and holidays. Here are my 3 hypotheses:\n1. Holidays affect sales\n2. Popular titles (ex.: games) are bestsellers & deserve their own categories\n3. Data leaks are unlikely for test data for the following reasons: (a) there are some debut items \n, (b) the target variables to guess by LB probing are massive (between -1 to clipped 20)\n, (c) shops may sell 'classical' items (items that has long been not sold, suddenly appears back in certain period of time)\n\n### First suspect: holidays affect sales","49869e96":"# 4. Hyperparameter Optimization\nThe optimum number of trees for Random Forest was approximated below","5a46d1da":"# 2. Exploratory Data Analysis","6aa0409c":"The distributions of item_id and shop_id vary and it is inconclusive that data leakage exists, however since target value is not binary value of 0\/1, the possibility target values (from -1 to upper-clipped 20) to guess based on LB probing(submit & check score up\/down) are massive. I decided to proceed to the ETL instead. In addition there are some debut items (item_id) in the test set as shown below:","a6cf16f6":"# 3. How the Data was Transformed and ETL-ed\n* The data will be group and merged by these 5 columns and some of their combination: \"date_block_num\", \"shop_id\",\"item_id\",'subtype_code','shop_city'. This is to extract their local, group, and global aggregated operation results.\n* The data was also transformed using scikit transformer. This will save memory since the data will be mostly stored in int format (The author followed the way of the original source code mentioned above).","a192c8b7":"### Libraries Requirement Check\nThis notebook was created using Kaggle Notebook & the following libraries at the time of creation.\n* numpy 1.18.1\n* pandas 1.0.3\n* matplotlib.pylab 1.18.1\n* sklearn 0.22.2.post1\n* xgboost 1.0.2\n* seaborn 0.10.0\n* matplotlib.pylab 1.18.1\n* re 2.2.1\n\nThe following files should exist at the  directory \/kaggle\/input\/create-model-simple-e-to-e-eda-to-ensemble\/ (you should be able to access this by default)\n1. agg_data.csv\n2. rf_pred_30_10.model\n3. rf_pred_30_10.csv\n4. metamodel.model\n5. xgb_pred_100_10.csv\n6. xgb_pred_100_10.model"}}