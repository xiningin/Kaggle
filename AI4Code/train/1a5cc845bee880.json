{"cell_type":{"7f349fa7":"code","b2a3c36a":"code","b659b3c2":"code","45b1f31a":"code","c4eccf7b":"code","bd6b9c8c":"code","9265d638":"code","ab549796":"code","85b29d27":"code","ed8b03a6":"code","54b5941c":"code","4e253d7b":"code","d5be6f0f":"code","5a8b04f1":"code","fd548563":"code","21b3574e":"code","46acf2c8":"code","b43c17e0":"code","9fe81429":"code","406dd523":"code","2e989016":"code","52afd25b":"code","5f2ab4ea":"code","8367a256":"code","013ff227":"code","9654faf5":"code","78b5d9e2":"code","382dacc4":"code","38a2c1e2":"markdown","ce06e5e3":"markdown","07982407":"markdown","dc58d0b2":"markdown","e11fbfb8":"markdown","e80ea56e":"markdown","1e662822":"markdown","ffeeed6b":"markdown","0efc86d7":"markdown"},"source":{"7f349fa7":"import os\nimport glob\nimport warnings\nimport cv2\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.applications import VGG16\nfrom keras.applications import inception_v3\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Sequential, Model\nfrom keras.models import InputLayer\nfrom keras.models import load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import SGD\n\nimport tensorflow as tf\n\n%matplotlib inline\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)","b2a3c36a":"# set data path\n\ndata_path = \"\/kaggle\/input\/intel-image-classification\/\"\ndata_path_train = data_path + \"seg_train\/seg_train\/\"\ndata_path_validation = data_path + \"seg_test\/seg_test\/\"\ndata_path_prediction = data_path + \"seg_pred\/seg_pred\/\"\n","b659b3c2":"# get all image file path\nfile_names = [(os.path.abspath(x), os.path.relpath(x, data_path), os.path.basename(x)) \n              for x in Path(data_path).glob(\"**\/*.jpg\")]\n\n# store image meta-data into dataframe\ndf = pd.DataFrame(\n    file_names, \n    columns=[\"full_file_name\", \"rel_file_name\", \"file_name\"])\n\n# set image data type\ndf[\"image_type\"] = df['rel_file_name'].map(\n    lambda x: \n        \"pred\" if \"pred\" in x \n        else x.split(\"\/\")[0].replace(\"seg_\", \"\"))\n\n# set image classification\ndf[\"image_classification\"] = df['rel_file_name'].map(\n    lambda x: \n        \"?\" if \"pred\" in x \n        else x.split(\"\/\")[2].replace(\"seg_\", \"\"))\n\ndf.sample(5)\n","45b1f31a":"# visualize it\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.color_palette(\"dark\", 4)\n\npalette = sns.color_palette(\"muted\", 6)\nsns.set_palette(palette)\n\nsns.countplot(\n    data = df, x = \"image_type\", \n    order = [\"train\", \"test\", \"pred\"], \n    ax = ax[0])\n\nsns.countplot(\n    data = df.loc[df[\"image_classification\"] != \"?\", :], \n    x = \"image_type\", hue = \"image_classification\", \n    order = [\"train\", \"test\", \"pred\"], \n    hue_order = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"],\n    ax = ax[1])\n","c4eccf7b":"# Inspect images\n\nmosaic_size = (20, 20)\nmosaic_number_of_images = 80\nmosaic_number_of_columns = 8\nmosaic_number_of_rows = int(mosaic_number_of_images \/ mosaic_number_of_columns)\n\nmosaic_tile_text_fontColor = (0, 255, 0)\ndisplay_image_size = (768, 768, 3)\n\ndf_display_items = df.loc[df[\"image_type\"] != \"pred\"].sample(mosaic_number_of_images)\nmosaic_image = np.zeros(\n    (mosaic_number_of_rows * (display_image_size[0] + 2), \n     mosaic_number_of_columns * (display_image_size[1] + 2), \n     3))\n\ncounter = 0\nfor i in range(mosaic_number_of_rows):\n    \n    y_start = i * (display_image_size[1] + 2)\n    y_end = y_start + display_image_size[1]\n    \n    j = 0\n    while j < mosaic_number_of_columns:\n        \n        x_start = j * (display_image_size[0] + 2)\n        x_end = x_start + display_image_size[0]\n        \n        image_path = df_display_items.iloc[counter, :][\"full_file_name\"]\n        image_classification = df_display_items.iloc[counter, :][\"image_classification\"]\n\n        img = image.load_img(image_path, target_size = display_image_size)\n        img = image.img_to_array(img) \/ 255\n \n        # print category into the image itself\n        cv2.putText(\n            img = img, \n            text = image_classification,\n            org = (10, 150),\n            fontFace = cv2.FONT_HERSHEY_PLAIN,\n            fontScale = 8,\n            color = mosaic_tile_text_fontColor,\n            thickness = 8,\n            lineType = 8)\n       \n        mosaic_image[y_start:y_end, x_start:x_end, :] = img[:,:,:]\n\n        j += 1\n        counter += 1\n\nplt.figure(figsize = mosaic_size)\nplt.imshow(mosaic_image[:, :, :], cmap='seismic')","bd6b9c8c":"parameter_batch_size_train = 250\nparameter_batch_size_val = 250\n\nparameter_image_data_rotation_range = 0.4\nparameter_image_data_horizontal_flip = True\n\nimg_height = 299\nimg_width = 299\nimg_channels = 3\n","9265d638":"# contruct train image data generator\n\ntrain_datagen = ImageDataGenerator(\n    rotation_range = parameter_image_data_rotation_range,\n    horizontal_flip = parameter_image_data_horizontal_flip,\n    preprocessing_function = inception_v3.preprocess_input)  \n\nX_train_gen = train_datagen.flow_from_directory(\n    data_path_train,\n    target_size = (img_height, img_width),\n    batch_size = parameter_batch_size_train)\n","ab549796":"# contruct validation image data generator\n\nval_datagen = ImageDataGenerator(\n    preprocessing_function = inception_v3.preprocess_input)\n\nX_val_gen = val_datagen.flow_from_directory(\n    data_path_validation,\n    target_size = (img_height, img_width),\n    batch_size = parameter_batch_size_val)\n","85b29d27":"# get Google's inception V3 as pre-trainned model\n\nmodel_base = inception_v3.InceptionV3(\n    input_shape = (img_height, img_height, img_channels), \n    include_top = False)\n\n#model_base.summary()","ed8b03a6":"# build a new model with the featurizer and a new classifier\n\nmodel_classifier = Sequential()\n\nmodel_classifier.add(InputLayer(input_shape=(8, 8, 2048))) # InceptionV3\nmodel_classifier.add(Flatten())\nmodel_classifier.add(Dense(32, activation='relu'))\nmodel_classifier.add(Dropout(0.5))\nmodel_classifier.add(Dense(6, activation='softmax'))\n\nmodel_classifier.summary()","54b5941c":"# build a combined model, consists of incepction V3 as base model and our new classifier model\n\nmodel_classifier_output = model_classifier(model_base.output) # classifier consumes the output of the CNN\n\nmodel_combined = Model(\n    inputs  = model_base.input,      # CNN without classifier\n    outputs = model_classifier_output)     # Classifier\n\nmodel_combined.summary()","4e253d7b":"# Gets all layers except classifier (the last one)\n# Freeze layers up to the classifier\n# This will preserve the weights for layers during backpropagation\n\nfor layer in model_combined.layers[:-1]:\n    layer.trainable = False # freeze\n\n#model_combined.summary()","d5be6f0f":"parameter_cnn_learning_rate = 1e-3\nparameter_cnn_epochs = 20\nparameter_cnn_steps_per_epoch = 20\nparameter_cnn_validation_steps = 20\n","5a8b04f1":"sgd = SGD(lr = parameter_cnn_learning_rate)\n\nmodel_combined.compile(\n    optimizer = sgd,                   \n    loss = 'categorical_crossentropy',\n    metrics=['acc'])","fd548563":"import time\ntb = TensorBoard(log_dir='logs'.format(int(time.time())))\nes = EarlyStopping(patience=3)\nmc = ModelCheckpoint('transfer.{epoch:02d}-{val_loss:.2f}.hdf5')\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n","21b3574e":"#%load_ext tensorboard.notebook\n#%tensorboard --logdir logs\n\nhistory = model_combined.fit_generator(\n    X_train_gen,\n    steps_per_epoch = parameter_cnn_steps_per_epoch,\n    epochs = parameter_cnn_epochs,\n    validation_data = X_val_gen,\n    validation_steps = parameter_cnn_validation_steps)","46acf2c8":"#model_path = 'transfer.02-0.51.hdf5' # update model path\n#best_model = load_model(model_path)\n\nbest_model = model_combined","b43c17e0":"fig, ax = plt.subplots(ncols=2, figsize=(10, 5))\n\nax[0].plot(history.history['acc'], label='train')\nax[0].plot(history.history['val_acc'], label='val')\nax[0].set_ylabel('accuracy')\nax[0].set_xlabel('epochs')\nax[0].legend()\n\nax[1].plot(history.history['loss'], label='train')\nax[1].plot(history.history['val_loss'], label='val')\nax[1].set_ylabel('loss')\nax[1].set_xlabel('epochs')\nax[1].legend()\n\nplt.show()","9fe81429":"X_val, y_val = X_val_gen.next()\ny_val_classes = y_val.argmax(axis=1)","406dd523":"# probabilities\npred = best_model.predict(X_val)\n\n# probabilities to classes\npred_classes = pred.argmax(axis=1)\n#pred_classes","2e989016":"print(classification_report(y_val_classes, pred_classes))","52afd25b":"print(confusion_matrix(y_val_classes, pred_classes))","5f2ab4ea":"labels = X_val_gen.class_indices\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in pred_classes]\n\nlabels\n#predictions","8367a256":"# fully adapted \n# http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n# https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\n    \nimport itertools\n\ndef plot_confusion_matrix(\n    cm,\n    target_names,\n    title = 'Confusion matrix',\n    cmap = None,\n    normalize = True):\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(5, 5))\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n\n    plt.show()","013ff227":"plot_confusion_matrix(\n    cm = confusion_matrix(y_val_classes, pred_classes), \n    normalize = True,\n    target_names = labels.items(),\n    title = \"Normalized Confusion Matrix\")","9654faf5":"df_view = df.loc[df[\"image_type\"] == \"test\"]\n\ndf_view[\"image_classification_predict\"] = \"\"\n\nfor index, row in df_view.iterrows():\n   \n    img = image.load_img(row[\"full_file_name\"], target_size=(img_height, img_width)) \n    \n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = inception_v3.preprocess_input(x)    \n    \n    predict = best_model.predict(x).argmax(axis=1)\n    predict_decode = [labels[k] for k in predict]    \n    \n    row[\"image_classification_predict\"] = predict_decode[0]\n    ","78b5d9e2":"df_show = df_view.loc[df_view[\"image_classification_predict\"] != df_view[\"image_classification\"], :]\n\ndf_show.shape\ndf_view.shape","382dacc4":"\n# Inspect images # TODO REFACTOR THIS\nmosaic_size = (20, 20)\nmosaic_number_of_images = 80\nmosaic_number_of_columns = 8\nmosaic_number_of_rows = int(mosaic_number_of_images \/ mosaic_number_of_columns)\nmosaic_tile_text_fontColor = (0, 255, 0)\ndisplay_image_size = (768, 768, 3)\ndf_display_items = df_show.sample(mosaic_number_of_images)\nmosaic_image = np.zeros(\n    (mosaic_number_of_rows * (display_image_size[0] + 2), \n     mosaic_number_of_columns * (display_image_size[1] + 2), \n     3))\ncounter = 0\nfor i in range(mosaic_number_of_rows):\n    \n    y_start = i * (display_image_size[1] + 2)\n    y_end = y_start + display_image_size[1]\n    \n    j = 0\n    while j < mosaic_number_of_columns:\n        \n        x_start = j * (display_image_size[0] + 2)\n        x_end = x_start + display_image_size[0]\n        \n        image_path = df_display_items.iloc[counter, :][\"full_file_name\"]\n        image_classification = df_display_items.iloc[counter, :][\"image_classification\"]\n        image_classification_prediction = df_display_items.iloc[counter, :][\"image_classification_predict\"]\n        img = image.load_img(image_path, target_size = display_image_size)\n        img = image.img_to_array(img) \/ 255\n \n        # print category into the image itself\n        cv2.putText(\n            img = img, \n            text = image_classification,\n            org = (10, 150),\n            fontFace = cv2.FONT_HERSHEY_PLAIN,\n            fontScale = 8,\n            color = mosaic_tile_text_fontColor,\n            thickness = 8,\n            lineType = 8)\n        \n        # print category into the image itself\n        cv2.putText(\n            img = img, \n            text = image_classification_prediction,\n            org = (10, 650),\n            fontFace = cv2.FONT_HERSHEY_PLAIN,\n            fontScale = 8,\n            color = (255, 0, 0),\n            thickness = 8,\n            lineType = 8)        \n       \n        mosaic_image[y_start:y_end, x_start:x_end, :] = img[:,:,:]\n        j += 1\n        counter += 1\nplt.figure(figsize = mosaic_size)\nplt.imshow(mosaic_image[:, :, :], cmap='seismic')\n","38a2c1e2":"### Model training","ce06e5e3":"#### Build and visualize image data dictionary","07982407":"## Model engineering","dc58d0b2":"### Model measurement","e11fbfb8":"### Visualize image collection","e80ea56e":"## Data enginering","1e662822":"## Introduction\nThe purpose of this work is about creating an image classifier to identify natural scenes around the world, based on the data initially published by Intel to host a Challenge. A dataset of 25K images are used to train a classifier to categorized images into buildings, forest, glacier, mountains, sea and street.\n\n## Approach\nThe approach is to build CNN that can identify objects in images, on top of pre-built and pre-trained model applying transfer learning. \n\n## Model specifications\n\n- Model: CNN\n- Pre-Trained model: INCEPTION V3\n- NN Library: KERAS on top TensorFlow\n\n","ffeeed6b":"### Model construction","0efc86d7":"#### Model - training validation accuracy and cost"}}