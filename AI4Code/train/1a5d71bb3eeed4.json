{"cell_type":{"f330e4fb":"code","77f1d604":"code","e35f6bbf":"code","e1130b32":"code","0f807408":"code","13639716":"code","8a5b3fd0":"code","0d239a74":"code","c4f34dd4":"code","6dac395e":"code","4951ce7e":"code","824e161f":"code","a25de64f":"code","139433f5":"code","3454b637":"code","9c43e5d2":"code","5c8b149c":"code","760e844f":"code","44a657f6":"code","9c3e2c80":"code","89698b4d":"code","c6963668":"code","1b7d5e6d":"code","adcc225d":"code","1b1d15ca":"code","eb2cf07a":"code","32bc25f5":"code","05a9033d":"code","12136455":"code","c3bffe05":"code","e6166e6f":"code","a19b0213":"code","57e3e75b":"code","4c57a413":"code","5b671165":"code","bd06e376":"code","191593bb":"code","a9deae51":"code","e3295560":"code","5427afea":"code","0e1ef37b":"code","763729a0":"code","d8b61603":"code","4d64380f":"code","89884f99":"code","a805d570":"code","a2bd6685":"code","8fb9d8d8":"code","98595a2f":"code","84624167":"markdown","481beaa3":"markdown","f01f4f89":"markdown","dbbce9bf":"markdown","86d85e2d":"markdown","9525ea81":"markdown","16636368":"markdown","d75c28ab":"markdown","54d96534":"markdown","1be75632":"markdown","3ca44632":"markdown","5bcec788":"markdown","927d7843":"markdown","d871a4b6":"markdown","5f5c992f":"markdown","40e3cd1e":"markdown","c8367d11":"markdown","9881f68e":"markdown","317a36fd":"markdown","ef1a5ce3":"markdown","623e50ea":"markdown","d2c79439":"markdown","0c162e64":"markdown","098f0c53":"markdown"},"source":{"f330e4fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import statements required for Plotly \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, log_loss, classification_report, matthews_corrcoef\nimport xgboost\n\n# Import and suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')","77f1d604":"df_train = pd.read_csv(\"..\/input\/student-shopee-code-league-marketing-analytics\/train.csv\")\ndf_users = pd.read_csv(\"..\/input\/student-shopee-code-league-marketing-analytics\/users.csv\")\ndf_test = pd.read_csv(\"..\/input\/student-shopee-code-league-marketing-analytics\/test.csv\")\nsample_sub = pd.read_csv(\"..\/input\/student-shopee-code-league-marketing-analytics\/sample_submission_0_1.csv\")\n\ndf_test.shape","e35f6bbf":"df_traintest = df_train.append(df_test)\ndf_traintest = df_traintest.merge(df_users, on='user_id', how='left')","e1130b32":"cat_features = ['country_code','dayofweek','domain']\nnumerical_features = [ 'subject_line_length',\n       'last_open_day', 'last_login_day', 'last_checkout_day',\n       'open_count_last_10_days', 'open_count_last_30_days',\n       'open_count_last_60_days', 'login_count_last_10_days',\n       'login_count_last_30_days', 'login_count_last_60_days',\n       'checkout_count_last_10_days', 'checkout_count_last_30_days',\n       'checkout_count_last_60_days','attr_1', 'attr_2',\n       'attr_3', 'age']\nused_cols = cat_features + numerical_features","0f807408":"def time_to_categorical_series(df,type=\"hour\"):\n    if type == \"hour\":\n        return df['date_time'].dt.hour.astype('float64')\n    elif type == \"dayofweek\":\n        return df['grass_date'].dt.dayofweek.astype('float64')\n    elif type == \"month\":\n        return df['date_time'].dt.month.astype('float64')\n    else:\n        return None\n    \ndef time_to_categorical(df):\n#     hour_series = time_to_categorical_series(df,type='hour')\n    dayofweek_series = time_to_categorical_series(df,type='dayofweek')\n#     month_series = time_to_categorical_series(df,type='month')\n\n#     df['hour'] = hour_series\n    df['dayofweek'] = dayofweek_series\n#     df['month'] = month_series","13639716":"df_traintest['grass_date'] = pd.to_datetime(df_traintest['grass_date'])\ntime_to_categorical(df_traintest)\n\ndf_traintest","8a5b3fd0":"df_traintest = df_traintest[['row_id', 'open_flag'] + used_cols ]\n\ndf_traintest","0d239a74":"def fill_ints(data):\n    try:\n        return int(data)\n    except:\n        return np.nan\n\nnoise_col = ['last_open_day', 'last_login_day','last_checkout_day']\n\nfor col in noise_col:\n    df_traintest[col] = pd.to_numeric(df_traintest[col].apply(fill_ints))\n","c4f34dd4":"df_traintest[used_cols].isnull().sum()","6dac395e":"df_traintest[used_cols].describe()","4951ce7e":"df_traintest[used_cols].quantile([0,.01, .25, .50])","824e161f":"current_cols = [ \n    'subject_line_length',\n    'age',\n]\ncurrent_threshold = [\n    17, 17\n]\n\nfor col, val in zip(current_cols, current_threshold):\n    print(\"Change\", (df_traintest[col] < val).sum(), \"Rows from\", col)\n    df_traintest.loc[df_traintest[col] < val, col] = val","a25de64f":"df_traintest[used_cols].quantile([.75,.95, .999, 1.0])","139433f5":"current_cols = [ \n    'last_open_day',\n    'last_login_day',\n    'last_checkout_day',\n    'open_count_last_10_days', \n    'open_count_last_30_days', \n    'open_count_last_60_days',\n    'login_count_last_10_days',\n    'login_count_last_30_days', \n    'login_count_last_60_days',\n    'checkout_count_last_10_days',\n    'checkout_count_last_30_days', \n    'checkout_count_last_60_days',\n]\ncurrent_threshold = [\n    587.599, 920, 1049,\n    12, 30, 56,\n    121.492\t, 342.492, 645,\n    35, 90, 168\n]\n\nfor col, val in zip(current_cols, current_threshold):\n    print(\"Change\", (df_traintest[col] > val).sum(), \"Rows from\", col)\n    df_traintest.loc[df_traintest[col] > val, col] = val","3454b637":"df_traintest[used_cols] = df_traintest[used_cols].fillna(df_traintest.median())\n# df_traintest[used_cols] = df_traintest[used_cols].fillna(df_traintest.mean())\n\ndf_traintest","9c43e5d2":"year_now = 2020\ngen_boomer = 1964\ngen_x = 1980\ngen_y = 1996\ngen_z = year_now\n\ndf_traintest.loc[df_traintest['age'] >= year_now - gen_boomer, 'generation'] = 'B'\ndf_traintest.loc[(df_traintest['age'] >= year_now - gen_x) & (df_traintest['age'] < year_now - gen_boomer), 'generation'] = 'X'\ndf_traintest.loc[(df_traintest['age'] >= year_now - gen_y) & (df_traintest['age'] < year_now - gen_x), 'generation'] = 'Y'\ndf_traintest.loc[(df_traintest['age'] >= year_now - gen_z) & (df_traintest['age'] < year_now - gen_y), 'generation'] = 'Z'\ndf_traintest","5c8b149c":"qcut_features = [\n#     'last_open_day', 'last_login_day', 'last_checkout_day',\n#     'open_count_last_10_days', 'open_count_last_30_days','open_count_last_60_days', \n#     'login_count_last_10_days','login_count_last_30_days', 'login_count_last_60_days',\n#     'checkout_count_last_10_days', 'checkout_count_last_30_days','checkout_count_last_60_days',\n#     '20_interval_open', '30_interval_open','50_interval_open', \n#     '20_interval_login', '30_interval_login', '50_interval_login', \n#     '20_interval_checkout', '30_interval_checkout','50_interval_checkout'\n#     'subject_line_length',\n]\n\ncut_features = [\n#     'subject_line_length',\n#     'age'\n]","760e844f":"for feature in cut_features:\n    df_traintest[feature+'_bin'] = pd.cut(df_traintest[feature].astype(int), 10)\n    print('Feature:', feature)\n    print('Unique:',len(df_traintest[feature+'_bin'].unique()))\n    print(df_traintest[feature+'_bin'].unique())\n    print('='*50)","44a657f6":"for feature in qcut_features:\n    print('Feature:', feature)\n    df_traintest[feature+'_bin'] = pd.qcut(df_traintest[feature].rank(method='first').astype(int),10)\n    print('Unique:',len(df_traintest[feature+'_bin'].unique()))\n    print(df_traintest[feature+'_bin'].unique())\n    print('='*50)","9c3e2c80":"from sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\nfor feature in cut_features + qcut_features:\n#     df_traintest[feature+'_bin'] = label.fit_transform(df_traintest[feature+'_bin'])\n    df_traintest[feature] = label.fit_transform(df_traintest[feature+'_bin'])\n    df_traintest = df_traintest.drop(feature+'_bin', axis=1)\n\ndf_traintest['domain'] = label.fit_transform(df_traintest['domain'])\ndf_traintest['generation'] = label.fit_transform(df_traintest['generation'])\n\ndf_traintest","89698b4d":"X = df_traintest.drop(['row_id', 'open_flag'], axis=1)","c6963668":"X['20_interval_open'] = X['open_count_last_30_days'] - X['open_count_last_10_days'] \nX['30_interval_open'] = X['open_count_last_60_days'] - X['open_count_last_30_days'] \nX['50_interval_open'] = X['open_count_last_60_days'] - X['open_count_last_10_days'] \n\nX['20_interval_login'] = X['login_count_last_30_days'] - X['login_count_last_10_days'] \nX['30_interval_login'] = X['login_count_last_60_days'] - X['login_count_last_30_days'] \nX['50_interval_login'] = X['login_count_last_60_days'] - X['login_count_last_10_days'] \n\nX['20_interval_checkout'] = X['checkout_count_last_30_days'] - X['checkout_count_last_10_days'] \nX['30_interval_checkout'] = X['checkout_count_last_60_days'] - X['checkout_count_last_30_days'] \nX['50_interval_checkout'] = X['checkout_count_last_60_days'] - X['checkout_count_last_10_days'] ","1b7d5e6d":"cat_features = ['country_code','dayofweek','domain', 'generation']","adcc225d":"encoded_features = []\nfor feature in cat_features:\n    encoded_feat = OneHotEncoder().fit_transform(X[feature].astype('float64').values.reshape(-1, 1)).toarray()\n    n = X[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(1, n + 1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = X.index\n    encoded_features.append(encoded_df)\n    \nX = pd.concat([X, *encoded_features], axis=1)\n\nX","1b1d15ca":"numerical_outliner_features = [\n    'age'\n]","eb2cf07a":"numerical_features = ['20_interval_checkout', '20_interval_login', '20_interval_open',\n            '30_interval_checkout', '30_interval_login', '30_interval_open',\n            '50_interval_checkout', '50_interval_login', '50_interval_open',\n            'attr_1', 'attr_2', 'attr_3', 'checkout_count_last_10_days',\n            'checkout_count_last_30_days', 'checkout_count_last_60_days',\n            'last_checkout_day', 'last_login_day','last_open_day', 'login_count_last_10_days',\n            'login_count_last_30_days', 'login_count_last_60_days',\n            'open_count_last_10_days', 'open_count_last_30_days',\n            'open_count_last_60_days', 'subject_line_length']\n","32bc25f5":"numerical_onehot_features = ['country_code_1', 'country_code_2', 'country_code_3',\n            'country_code_4', 'country_code_5', 'country_code_6',\n            'country_code_7', 'dayofweek_1', 'dayofweek_2', 'dayofweek_3',\n            'dayofweek_4', 'dayofweek_5', 'dayofweek_6', 'dayofweek_7',\n            'domain_1', 'domain_10', 'domain_11', 'domain_2', 'domain_3',\n            'domain_4', 'domain_5', 'domain_6', 'domain_7', 'domain_8',\n            'domain_9', 'generation_1', 'generation_2', 'generation_3',\n            'generation_4']","05a9033d":"df_traintest = df_traintest[['row_id', 'open_flag']]","12136455":"X = pd.DataFrame(X)\n\ndf_traintest = pd.concat([df_traintest.reset_index(drop = True), X], axis = 1)","c3bffe05":"train = df_traintest.loc[~df_traintest.open_flag.isnull()]\ntest = df_traintest.loc[df_traintest.open_flag.isnull()]\n\ntrain.shape, test.shape","e6166e6f":"from sklearn.model_selection import train_test_split\n\nX_full = train.drop([\"open_flag\", \"row_id\"], axis = 1)\ny_full = train.open_flag\n\nX_full.shape","a19b0213":"X_test = test.drop(['row_id',\"open_flag\"], axis = 1)\n\nX_test.shape","57e3e75b":"# Definition of the CategoricalEncoder class, copied from PR #9151.\n# Just run this cell, or copy it to your code, no need to try to\n# understand every line.\n# Code reference Hands on Machine Learning with Scikit Learn and Tensorflow by Aurelien Geron.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","4c57a413":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# A class to select numerical or categorical columns \n# since Scikit-Learn doesn't handle DataFrames yet\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","5b671165":"from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest, VarianceThreshold\nfrom sklearn.impute  import SimpleImputer\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import matthews_corrcoef\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","bd06e376":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n\n# Making pipelines\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector(numerical_features)),\n    (\"std_scaler\", MinMaxScaler()),\n    ('poly_feature', PolynomialFeatures(degree=2, include_bias= False, interaction_only= True))\n])\n\nnumerical_outliner_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector(numerical_outliner_features)),\n    (\"std_scaler\", RobustScaler()),\n    ('poly_feature', PolynomialFeatures(degree=3, include_bias= False, interaction_only= True))\n])\n\nnumerical_onehot_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector(numerical_onehot_features)),\n    (\"std_scaler\", StandardScaler()),\n])\ncategorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector(cat_features)),\n    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n])\n\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"numerical_outliner_pipeline\", numerical_outliner_pipeline),\n        (\"numerical_onehot_pipeline\", numerical_onehot_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","191593bb":"train_features = preprocess_pipeline.fit_transform(X_full)\ntrain_labels = y_full\n\nX_test = preprocess_pipeline.fit_transform(X_test)\n\ntrain_features.shape, X_test.shape","a9deae51":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = X_train, X_valid, y_train, y_valid = train_test_split(train_features, train_labels, test_size=0.1, random_state=12345)\n\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","e3295560":"from sklearn.model_selection import KFold\nimport time\n\nlgb_param = {'num_leaves': 10,\n             'min_data_in_leaf': 5, \n             'objective':'binary',\n             'max_depth': -1,\n             'learning_rate': 0.005,\n             \"boosting\": \"gbdt\",\n             \"feature_fraction\": 0.8,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.8 ,\n             \"bagging_seed\": 2020,\n             \"metric\": 'binary_logloss',\n             \"tree_learner\": \"voting\",\n             \"lambda_l1\": 0.3,\n             \"random_state\": 2020,\n#              'device': 'gpu',\n             \"verbosity\": -1}\n\nmax_iter = 5\nfolds = KFold(n_splits=max_iter, shuffle=True, random_state=2020)\noof = np.zeros(len(X_train))\npredictions = np.zeros(len(X_valid))\npredictions_test = np.zeros(len(X_test))\n\nstart = time.time()\nstart_time= time.time()\nscore = [0 for _ in range(folds.n_splits)]\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    new_X_train = pd.DataFrame(data=X_train)\n    new_y_train = pd.DataFrame(data=y_train)\n    trn_data = lgb.Dataset(new_X_train.iloc[trn_idx],\n                           label=new_y_train.iloc[trn_idx],\n                          )\n    val_data = lgb.Dataset(new_X_train.iloc[val_idx],\n                           label=new_y_train.iloc[val_idx],\n                          )\n\n    num_round = 10000\n    clf = lgb.train(lgb_param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=500,\n                    early_stopping_rounds = 300)\n    \n    oof[val_idx] = (clf.predict(new_X_train.iloc[val_idx], num_iteration=clf.best_iteration) > 0.5).astype(int)\n    \n    # print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n    score[fold_] = matthews_corrcoef(new_y_train.iloc[val_idx], oof[val_idx])\n    \n    predictions += clf.predict(pd.DataFrame(data=X_valid), num_iteration=clf.best_iteration)\n    predictions_test += clf.predict(pd.DataFrame(data=X_test), num_iteration=clf.best_iteration)\n    if fold_ == max_iter - 1: break\n        \nif (folds.n_splits == max_iter):\n    print(\"CV score: {:<8.5f}\".format(matthews_corrcoef(y_train, oof)))\nelse:\n    print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))\n        ","5427afea":"best_score, best_threshold = 0, 0\nscores = []\nfor i in np.linspace(.5, .8, 40):\n    score = matthews_corrcoef(y_valid, ((predictions \/ ((fold_ + 1) \/ 2) ) > i).astype(int) )\n    scores.append(score)\n    if score > best_score:\n        best_score = score\n        best_threshold = i\n\nprint('Best Score:', best_score)\nprint('Best Threshold: ', best_threshold)\nplt.plot(scores)\nplt.show()","0e1ef37b":"predictions_test_end = ((predictions_test \/ ((fold_ + 1) \/ 2) ) > best_threshold).astype(int) ","763729a0":"# Get our predictions\nlgb_predictions = np.array(((predictions \/ ((fold_ + 1) \/ 2) ) > best_threshold).astype(int))\nprint(\"Predictions have finished\")","d8b61603":"print('Accuracy score: ', accuracy_score(y_valid, lgb_predictions))\nprint(\"=\"*80)\nprint(classification_report(y_valid, lgb_predictions))\nprint(\"=\"*80)\nprint('MCC Score = ',matthews_corrcoef(y_valid, lgb_predictions))","4d64380f":"# feature importance\nprint(\"Features Importance...\")\ngain = clf.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':clf.feature_name(), \n                   'split':clf.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:50])","89884f99":"%%time\n\nlgb_scores = lgb.cv(params=lgb_param, train_set=lgb.Dataset(new_X_train,label=new_y_train))\nlgb_mean = np.array(lgb_scores['auc-mean']).mean()\nprint('LGB Done')","a805d570":"d = {'Classifiers': ['LGB'], \n    'Crossval Mean Scores': [lgb_mean]}\n\nresult_df = pd.DataFrame(data=d)\n\nresult_df = result_df.sort_values(by=['Crossval Mean Scores'], ascending=False)\nresult_df","a2bd6685":"lgb_submission = sample_sub.copy()\nlgb_submission.open_flag = predictions_test_end\nlgb_submission.to_csv('submission_lgb.csv', index=False)","8fb9d8d8":"def plot_distribution_result(df):\n    ax = sns.countplot(df['open_flag'])\n\n    for val in ax.patches:\n        pct = '{:.2f}%'.format(100 * val.get_height() \/ df.shape[0])\n        xpos = val.get_x() + val.get_width() \/ 2.\n        ypos = val.get_height()\n        ax.annotate(pct, (xpos, ypos), ha='center', va='center', fontsize=14, xytext=(0, 12), textcoords='offset points')\n\n    plt.title('Open Flag comparison', fontsize=24, pad=15)\n    plt.xlabel('open flag', labelpad=18)\n    plt.tight_layout()\n    plt.show()","98595a2f":"plot_distribution_result(lgb_submission)","84624167":"## Split into train and test again","481beaa3":"## Change never to nan in 3 columns","f01f4f89":"## Add Interval Between Count","dbbce9bf":"## Re-Declare Feature","86d85e2d":"## Read the data","9525ea81":"## Final Process Data using FeautureUnion","16636368":"## Import Libraries","d75c28ab":"## Split data","54d96534":"## Add Generation Feature","1be75632":"### Training using LGBM + KFold","3ca44632":"## Merge Dataset\n1. Merge between train and test dataset\n2. Merge with user dataset","5bcec788":"## Submission","927d7843":"## Process using cut and qcut\n- cut based on value\n- qcut based on frequency","d871a4b6":"## Process Date","5f5c992f":"### See the result's distribution","40e3cd1e":"## Add Label Encoder\n- For cut and qcut feature\n- Change old value with encoder from cut and qcut from pandas","c8367d11":"## Initial Feature","9881f68e":"### Score the predictions","317a36fd":"## Fill NaN Value with median","ef1a5ce3":"## Notebook in Nutshell\n> Best Score From Ensemble of:\n* [Version 4](https:\/\/www.kaggle.com\/stevenevan99\/marketing-analytic-revised-v2?scriptVersionId=40302181)\n* [Version 3](https:\/\/www.kaggle.com\/stevenevan99\/marketing-analytic-revised-v2?scriptVersionId=40294101)\n* [Version 2](https:\/\/www.kaggle.com\/stevenevan99\/marketing-analytic-revised-v2?scriptVersionId=40281169)","623e50ea":"### Check Features Importance","d2c79439":"## Change value\n- Using .01 quantile threshold for 0 quantile\n- Using .99 quantile threshold for 1 quantile","0c162e64":"## Modelling","098f0c53":"### Cross val score"}}