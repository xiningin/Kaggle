{"cell_type":{"5a32a85d":"code","8d1cd3ed":"code","a09d6d80":"code","0f1f39a9":"code","5e02a2e7":"code","5ca93991":"code","0312fb38":"code","f7b57adf":"code","6c824197":"code","4fb510d8":"code","e3755fbc":"code","b0018f40":"code","dee629f9":"code","123dd2b0":"code","0506533b":"code","42044216":"code","bb2d8d67":"code","b29f22ac":"code","b9dba36b":"markdown","57368cee":"markdown","209cb1bc":"markdown","08cd68a5":"markdown","782cec6b":"markdown","aeac371f":"markdown","dfef469a":"markdown","0d767b9b":"markdown","f7f526c6":"markdown","40698aac":"markdown","ed4955e8":"markdown","809abf12":"markdown","b1322908":"markdown","015099f0":"markdown","7c68ea31":"markdown","53254510":"markdown","e889c55d":"markdown","3cd73a96":"markdown","d68316f6":"markdown"},"source":{"5a32a85d":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784',version=1)  ","8d1cd3ed":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX,y = mnist[\"data\"],mnist[\"target\"].astype(np.int) \nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50000)\n\n# For quick calculation, reduce the size of data set\nX_train, y_train = X_train[:5000], y_train[:5000]\nX_test, y_test = X_train[:2500], y_train[:2500]\n\nprint(\"X_train :\",len(X_train))\nprint(\"y_train :\",len(y_train))\nprint(\"X_test  :\",len(X_test))\nprint(\"y_test  :\",len(y_test))","a09d6d80":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n\ndef report(model) :\n    y_pred     = cross_val_predict(model, X_test, y_test, cv=3)     \n    clf_report = classification_report(y_test,y_pred, output_dict =True)\n    accuracy   = clf_report[\"accuracy\"]                # accuracy\n    precision  = clf_report['macro avg']['precision']  # precision\n    recall     = clf_report['macro avg']['recall']     # recall\n    confusion  = confusion_matrix(y_test, y_pred)      # confusion_matrix\n    print(\"accuracy : \",accuracy)\n    print(\"precision :\", precision)\n    print(\"recall :\",recall)\n    print(\"Confusion Matrix :\\n\"+str(confusion))\n    \n    \ndef show_auc(y_true,y_score):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')    \n    plt.legend(loc=\"lower right\")\n\n    \ndef Pretreatment_ROC(model, X, y):\n    y_prob = model.predict_proba(X)\n    y_pred = cross_val_predict(model, X, y)\n    y_true = np.array(y == y_pred)\n    y_score = np.array([y_prob[i][int(y[i])] for i in range(len(y))])\n    return y_true, y_score\n\n\ndef Print_clf(clf):\n    clf.fit(X_train,y_train)\n    report(clf)\n\n    y_true, y_score = Pretreatment_ROC(clf,X_test,y_test)\n    show_auc(y_true,y_score)","0f1f39a9":"from sklearn.neighbors    import KNeighborsClassifier\nfrom sklearn.ensemble     import ExtraTreesClassifier\nfrom sklearn.ensemble     import RandomForestClassifier\nfrom sklearn.ensemble     import BaggingClassifier\nfrom sklearn.ensemble     import AdaBoostClassifier\nfrom sklearn.ensemble     import VotingClassifier\n\nrnd = 1234 #random_state\n\nknn_clf = KNeighborsClassifier(n_neighbors=15)\next_clf = ExtraTreesClassifier(n_estimators=20,random_state=rnd)\nrdf_clf = RandomForestClassifier(n_estimators=10, random_state=rnd)\n\nbag_clf = BaggingClassifier(\n    ExtraTreesClassifier(n_estimators=20,random_state=rnd), \n    n_jobs=-1,\n    n_estimators=5,\n    random_state=12\n)\n\nada_clf = AdaBoostClassifier(\n    ExtraTreesClassifier(n_estimators=20,random_state=rnd),\n    n_estimators=50,\n    learning_rate=0.2, \n    algorithm=\"SAMME.R\", \n    random_state=12\n)\n\n\nvot_clf = VotingClassifier(\n    estimators= [        \n        (\"ext_clf\",ext_clf),\n        (\"rdf_clf\",rdf_clf),\n        (\"bag_clf\",bag_clf),\n        (\"ada_clf\",ada_clf)\n    ] , voting='soft'\n)","5e02a2e7":"Print_clf(knn_clf)","5ca93991":"Print_clf(ext_clf)","0312fb38":"Print_clf(rdf_clf)","f7b57adf":"Print_clf(bag_clf)","6c824197":"Print_clf(ada_clf)","4fb510d8":"Print_clf(vot_clf)","e3755fbc":"import matplotlib\nimport matplotlib.pyplot as plt\nimport random\n\ndef show(y, img_ord,img_pca):\n    index = random.randint(1,100)\n    image_ord = img_ord[index].reshape(28, 28)\n    image_rcd = img_pca[index].reshape(28, 28)\n\n    plt.figure(figsize=(7, 4))\n    pos = 121\n    for img in [image_ord, image_rcd] :\n        plt.subplot(pos)\n        plt.title(f\"y = {y[index]}\",fontsize = 15)\n        plt.imshow(img, cmap = matplotlib.cm.binary,interpolation=\"nearest\")\n        plt.axis(\"off\"); pos += 1    \n    plt.tight_layout()\n    ","b0018f40":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=40, whiten=True)\n\ntrain_reduced = pca.fit_transform(X_train)\ntrain_recovered = pca.inverse_transform(train_reduced)\n\ntest_reduced = pca.fit_transform(X_test)\ntest_recovered = pca.inverse_transform(test_reduced)\n\nshow(y_train,X_train,train_recovered)\n\n\ndef Print_clf_PCA(clf):\n    print(\"ord dimention :\",X_train.shape[1])\n    print(\"pca dimention : \",test_reduced.shape[1])\n\n    clf.fit(train_reduced,y_train)\n    report(clf)\n\n    y_true, y_score = Pretreatment_ROC(clf,test_reduced,y_test)\n    show_auc(y_true,y_score)","dee629f9":"Print_clf_PCA(knn_clf)","123dd2b0":"Print_clf_PCA(ext_clf)","0506533b":"Print_clf_PCA(rdf_clf)","42044216":"Print_clf_PCA(bag_clf)","bb2d8d67":"Print_clf_PCA(ada_clf)","b29f22ac":"Print_clf_PCA(vot_clf)","b9dba36b":"## 2.Model comparison","57368cee":"### 2.3. RandomForestClassifier","209cb1bc":"### 3.2. PCA-ExtraTreesClassifier","08cd68a5":"# MNIST\n\n1. import MNIST data set \n1. Model comparison  \n    2.1. KNeighborsClassifier  \n    2.2. ExtraTreesClassifier  \n    2.3. RandomForestClassifier  \n    2.4. BaggingClassifier  \n    2.5. AdaBoostClassifier  \n    2.6. VotingClassifier  \n    - print Confusion Matrix \n    - plot ROC curve  \n1. compare with PCA data","782cec6b":"### 3.4. PCA-BaggingClassifier","aeac371f":"### Define models","dfef469a":"### 3.6. PCA-VotingClassifier","0d767b9b":"### For quick calculation, reduce the size of data set","f7f526c6":"### 2.1. KNeighborsClassifier","40698aac":"## 1. import MNIST datase","ed4955e8":"### 3.1. PCA-KNeighborsClassifier","809abf12":"### 2.4. BaggingClassifier","b1322908":"# 3. compare with PCA data","015099f0":"### 3.3. PCA-RandomForestClassifier","7c68ea31":"### 2.6. VotingClassifier","53254510":"### Define the function to use","e889c55d":"### 3.5. PCA-AdaBoostClassifier","3cd73a96":"### 2.2. ExtraTreesClassifier","d68316f6":"### 2.5. AdaBoostClassifier"}}