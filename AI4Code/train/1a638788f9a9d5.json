{"cell_type":{"0a90e57f":"code","bc097c57":"code","d1248b92":"code","d30e736e":"code","483d6ec8":"code","a5803c88":"code","3f7a5ee4":"code","e4f03bf6":"code","f280073b":"code","41a011d5":"code","e81e83cf":"code","ca4881ff":"code","25d0fa73":"code","03086f55":"code","9b1722e4":"code","a930b8a8":"code","3656f2eb":"code","ff255678":"code","09d0660e":"code","7d38f960":"code","ce932e75":"code","230184ab":"code","a7eeb907":"code","d16d0f27":"code","ee859c03":"code","288b2c66":"code","72a1b470":"code","db6913d6":"code","dc4f2910":"code","d69f3141":"code","19af9ea5":"code","22d3da57":"code","af7b693e":"code","d3bb5aa3":"code","2b1d144b":"code","c231a8a0":"code","4458b549":"code","64eb66d1":"code","92f6536d":"code","34a1b0d6":"code","25fe3a5f":"code","a8c497c2":"code","a61a6d97":"code","6341aef4":"code","af252dc2":"code","f080a2bb":"code","ea517a4f":"code","dee852a7":"code","ea8abe9e":"code","9ca63764":"code","4822c431":"code","01b0a67a":"code","339503d8":"code","4123efbb":"code","1022c500":"code","53b4d898":"code","b97a5bce":"code","86568864":"code","46070e80":"code","d154092e":"code","673f45fd":"code","9106470a":"code","89374a3c":"code","6a8a2b6a":"code","6b8cb01b":"code","07a0eeb1":"markdown","971b9b23":"markdown","4ee27581":"markdown","fac36411":"markdown","9dc682a9":"markdown","e0327802":"markdown","a0cf368a":"markdown","69cd8a66":"markdown","fe3afda1":"markdown","723f7d52":"markdown","9a12ae94":"markdown","a3010e71":"markdown","8b592679":"markdown","0dbe4e48":"markdown","1def08f7":"markdown","b5d577bd":"markdown","cf94946a":"markdown","75fff3c0":"markdown","d827d2fa":"markdown","71c15f8f":"markdown"},"source":{"0a90e57f":"## Preparing the tools, Importing All tools.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve,plot_roc_curve","bc097c57":"df = pd.read_csv(\"..\/input\/heart-disease\/heart-disease.csv\")\ndf.head()","d1248b92":"df.shape","d30e736e":"df.head()","483d6ec8":"df.tail()","a5803c88":"df.target.value_counts()","3f7a5ee4":"df[\"target\"].value_counts().plot(kind=\"bar\",color=[\"salmon\",\"blue\"])","e4f03bf6":"df.info()","f280073b":"# Missing value ?\ndf.isna().sum()","41a011d5":"df.describe()","e81e83cf":"df.sex.value_counts()","ca4881ff":"# Compare target column with sex column\npd.crosstab(df.target,df.sex)","25d0fa73":"# Create a plot of crosstab\npd.crosstab(df.target,df.sex).plot(kind=\"bar\",figsize=(10,6),color=[\"pink\",\"lightblue\"])\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease ,1 = Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"]);\nplt.xticks(rotation=0);","03086f55":"df[\"thalach\"].value_counts()","9b1722e4":"# Create another figure\nplt.figure(figsize=(10,6))\n# scatter with positive examples\nplt.scatter(df.age[df.target==1],df.thalach[df.target==1],c=\"salmon\")\n# Scatter with negative examples\nplt.scatter(df.age[df.target==0],df.thalach[df.target==0],c=\"green\")\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\" Max Heart Rate\")\nplt.legend([\"Disease\",\"No Disease\"])","a930b8a8":"# Check distribution of the age column with historigram\ndf.age.plot.hist(color=\"lightblue\");","3656f2eb":"pd.crosstab(df.cp,df.target)","ff255678":"# Make crosstab more visual\npd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(10,6),color=[\"salmon\",\"lightblue\"])\nplt.title(\"Heart Disease Frequency per Chain pain type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\",\"Disease\"])\nplt.xticks(rotation=0);","09d0660e":"# Make a correlation matrix\ndf.corr()","7d38f960":"# Let's make a pretty correlation matrix\ncorr_matrix = df.corr()\nfig,ax = plt.subplots(figsize=(19,15))\nax = sns.heatmap(corr_matrix,annot=True,linewidths=0.5,fmt=\".2f\",cmap=\"YlGnBu\")","ce932e75":"df.head()","230184ab":"# Plit the data into X and y\nX = df.drop(\"target\",axis=1)\ny = df[\"target\"]\n","a7eeb907":"y","d16d0f27":"# Split data into train and test sets\nnp.random.seed(42)\n# Split into train & test set\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)","ee859c03":"y_train","288b2c66":"# Put models in a dictionary\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"KNN\": KNeighborsClassifier(),\n    \"Random Forest\": RandomForestClassifier()\n}\n# Create a function to fit and score model\ndef fit_and_score(models,X_train,X_test,y_train,y_test):\n    np.random.seed(42)\n    # Make a dictionary to keep model score\n    model_scores = {}\n    # Loop through models\n    for name,model in models.items():\n        model.fit(X_train,y_train)\n        #Evalute the model and append its score to model scores\n        model_scores[name] = model.score(X_test,y_test)\n    return model_scores","72a1b470":"model_scores = fit_and_score(models=models,X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","db6913d6":"model_compare = pd.DataFrame(model_scores,index=[\"Accuracy\"])\nmodel_compare.T.plot.bar()","dc4f2910":"# Let's tune KNN\ntrain_scores = []\ntest_scores = []\n# Create a list of different values for n_neighbors\nneighbors = range(1,21)\n# setup KNN instante\nknn = KNeighborsClassifier()\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    # Fit the algorithm\n    knn.fit(X_train,y_train)\n    # Update the training scores list\n    train_scores.append(knn.score(X_train,y_train))\n    # Update the test scores list\n    test_scores.append(knn.score(X_test,y_test))","d69f3141":"train_scores","19af9ea5":"test_scores","22d3da57":"plt.plot(neighbors,train_scores,label=\"Train score\")\nplt.plot(neighbors,test_scores,label=\"Test score\")\nplt.xticks(np.arange(0,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\nprint(f\"Maximum KNN Score is : {max(test_scores)*100:.2f}%\")","af7b693e":"# Create hyperparamter grid for LogisticRegressor\nlog_reg_grid = {\n    \"C\": np.logspace(-4, 4, 20),\n    \"solver\": [\"liblinear\"]\n}\n# Create hyperparameter grid for RandomForestClassifier\nrf_grid = {\n    \"n_estimators\": np.arange(10,1000,50),\n    \"max_depth\": [None,3,5,10],\n    \"min_samples_split\":np.arange(2,20,2),\n    \"min_samples_leaf\": np.arange(2,20,2)\n}","d3bb5aa3":"# Tune LogisticRegression\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression()\n                                ,param_distributions=log_reg_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n# Fit random hyperparamter search model for LogisticRegression\nrs_log_reg.fit(X_train,y_train)","2b1d144b":"rs_log_reg.best_params_","c231a8a0":"rs_log_reg.score(X_test,y_test)","4458b549":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1)\n                                ,param_distributions=rf_grid,\n                               cv=5,\n                               n_iter=20,\n                               verbose=True)\n# Fit random hyperparamter search model for LogisticRegression\nrs_rf.fit(X_train,y_train)","64eb66d1":"rs_rf.best_params_","92f6536d":"rs_rf.score(X_test,y_test)","34a1b0d6":"model_scores","25fe3a5f":"# Different hyperparameter for our  LogisticRegression model\nlog_reg_grid = {\n    \"C\": np.logspace(-4,4,30),\n    \"solver\":[\"liblinear\"]\n}\n# Setup grid hyperparamter search for LG\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train,y_train);","a8c497c2":"# Check the best parameters \ngs_log_reg.best_params_","a61a6d97":"# Evaluate the GridSearch LG model\ngs_log_reg.score(X_test,y_test)","6341aef4":"model_scores","af252dc2":"# Make predictions with Tuned model\ny_preds = gs_log_reg.predict(X_test)","f080a2bb":"y_preds","ea517a4f":"# Plot roc curve and calculate AUC metric\nplot_roc_curve(gs_log_reg,X_test,y_test)","dee852a7":"# Confusion matrix\nprint(confusion_matrix(y_test,y_preds))","ea8abe9e":"sns.set(font_scale=1.5)\ndef plot_conf_mat(y_test,y_preds):\n    fig,ax = plt.subplots(figsize=(3,3))\n    ax = sns.heatmap(confusion_matrix(y_test,y_preds),annot=True,cbar=False)\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \nplot_conf_mat(y_test,y_preds)\n    ","9ca63764":"print(classification_report(y_test,y_preds))","4822c431":"# Check best hyperparameter\ngs_log_reg.best_params_","01b0a67a":"\" Create a new classifier with best parameters\"\nclf = LogisticRegression(C=0.20433597178569418,solver = \"liblinear\")","339503d8":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf,X,y,cv=5,scoring=\"accuracy\")\ncv_acc","4123efbb":"cv_acc = np.mean(cv_acc)\ncv_acc","1022c500":"# Cross-validated precision\ncv_prec = cross_val_score(clf,X,y,cv=5,scoring=\"precision\")\ncv_prec = np.mean(cv_prec)\ncv_prec","53b4d898":"# Cross-validated recall\ncv_recall = cross_val_score(clf,X,y,cv=5,scoring=\"recall\")\ncv_recall = np.mean(cv_recall)\ncv_recall","b97a5bce":"# Cross-validated f1\ncv_f1 = cross_val_score(clf,X,y,cv=5,scoring=\"f1\")\ncv_f1 = np.mean(cv_f1)\ncv_f1","86568864":"# Visualize Cross-Validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\":cv_acc,\n                          \"Precision\":cv_prec,\n                          \"Recall\":cv_recall,\n                          \"F1\":cv_f1},index=[0])\ncv_metrics.plot.bar()","46070e80":"df.head()","d154092e":"# Fit an instance of LG\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","673f45fd":"# Check coef\nclf.coef_","9106470a":"# Macth coef's of features to columns\nfeature_dict = dict(zip(df.columns,list(clf.coef_[0])))\nfeature_dict","89374a3c":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict,index=[0])\nfeature_df.T.plot.bar(title=\"Feature Importance\",legend=False);","6a8a2b6a":"pd.crosstab(df.sex,df.target)","6b8cb01b":"pd.crosstab(df.slope,df.target)","07a0eeb1":"### Heart Disease Frequency per Chain pain type\ncp chest pain type:\n\n0: Typical angina: chest pain related decrease blood supply to the heart\n\n 1: Atypical angina: chest pain not related to heart\n \n 2: Non-anginal pain: typically esophageal spasms (non heart related)\n \n 3: Asymptomatic: chest pain not showing signs of disease","971b9b23":"### Modeling ","4ee27581":"Now we've got a ROC curve, an AUC metric and a confusion matrix, let's get a classification report as well as cross-validated precision, recall and f1-score.\n\n","fac36411":"### Feature Importance\n\nFeature importance is another as asking, \"which features contributed most to the outcomes of the model and how did they contribute?\"\n\nFinding feature importance is different for each machine learning model. One way to find feature importance is to search for \"(MODEL NAME) feature importance\".\n\nLet's find the feature importance for our LogisticRegression model...","9dc682a9":"### Model Comparaison","e0327802":"# Predicting heart disease using machine learning\n\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting whether or not someone has heart disease based on their medical attributes.\nIf you find that interesting , please vote :).\n\nWe're going to take the following approach:\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n## 1. Problem Definition\n\nIn a statement,\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n\n## 2. Data\n\nThe original data came from the Cleavland data from the UCI Machine Learning Repository. https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+Disease\n\nThere is also a version of it available on Kaggle. https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n## 3. Evaluation\n\n> If we can reach 95% accuracy at predicting whether or not a patient has heart disease during the proof of concept, we'll pursue the project.\n\n## 4. Features\n\nThis is where you'll get different information about each of the features in your data. You can do this via doing your own research (such as looking at the links above) or by talking to a subject matter expert (someone who knows about the dataset).\n\n**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","a0cf368a":"### Heart Disease Frequency according to sex","69cd8a66":"## Hyperparamter Tuning with GridSearchCV\n\nSince our LogisticRegression model provides the best scores so far, we'll try and improve them again using GridSearchCV...","fe3afda1":"Now we've got hyperparameter grids setup for each of our models, let's tune them using RandomizedSearchCV...","723f7d52":"### Calculate evaluation metrics using cross-validation\n\nWe're going to calculate accuracy, precision, recall and f1-score of our model using cross-validation and to do so we'll be using `cross_val_score()`.","9a12ae94":"# Now we've got our data split into training and test sets, it's time to build a machine learning model.\n\nWe'll train it (find the patterns) on the training set.\n\nAnd we'll test it (use the patterns) on the test set.\n\nWe're going to try 3 different machine learning models:\n1. Logistic Regression \n2. K-Nearest Neighbours Classifier\n3. Random Forest Classifier","a3010e71":"slope - the slope of the peak exercise ST segment\n* 0: Upsloping: better heart rate with excercise (uncommon)\n* 1: Flatsloping: minimal change (typical healthy heart)\n* 2: Downslopins: signs of unhealthy heart","8b592679":"### Age vs. Max Heart Rate for Heart Disease","0dbe4e48":"## 6. Experimentation\n\nIf you haven't hit your evaluation metric yet... ask yourself...\n\n* Could you collect more data?\n* Could you try a better model? Like CatBoost or XGBoost?\n* Could you improve the current models? (beyond what we've done so far)\n* If your model is good enough (you have hit your evaluation metric) how would you export it and share it with others?","1def08f7":"### Hyperparameter tuning with RandomizedSearchCV\n\nWe're going to tune:\n\n* LogisticRegression()\n\n* RandomForestClassifier()\n\n... using RandomizedSearchCV","b5d577bd":"## Evaluting our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score\n\n... and it would be great if cross-validation was used where possible.\n\nTo make comparisons and evaluate our trained model, first we need to make predictions.","cf94946a":"Now we've tuned LogisticRegression(), let's do the same for RandomForestClassifier()...","75fff3c0":"## Data Exploration\nThe goal here is to find out more about the data and become a subject matter export on the dataset you're working with. \n\n1. What question(s) are you trying to solve?\n2. What kind of data do we have and how do we treat different types?\n3. What's missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data?","d827d2fa":"Now we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n* Hyperparameter tuning\n* Feature importance\n* Confusion matrix\n* Cross-validation\n* Precision\n* Recall\n* F1 score\n* Classification report\n* ROC curve\n* Area under the curve (AUC)\n\n### Hyperparameter tuning (by hand)","71c15f8f":"## Load data"}}