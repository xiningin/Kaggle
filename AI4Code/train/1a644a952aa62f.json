{"cell_type":{"376756b0":"code","f1127a8b":"code","9f808d4b":"code","4890ea92":"code","3cd7494a":"code","d7abe6ba":"code","412a23df":"code","5fc2856a":"code","4c42e809":"code","c34f75c7":"code","ad56d491":"code","fd10ecfa":"code","c9930465":"code","a42e55c0":"code","0a946e7b":"code","5c81618b":"code","0abe0f55":"code","3a057aba":"code","4b0bca97":"code","a82bd44e":"markdown","a85c9648":"markdown","a75f252d":"markdown"},"source":{"376756b0":"# define constants\nsplitYear = 2014\nlr = 0.01","f1127a8b":"# constants\ntrainDim = 11\nnum_epochs = 10\nminibatch_size = 64\nseed = 0","9f808d4b":"# okay let's actually set up a tensor flow graph\nimport tensorflow as tf\n\n# placeholders\ntf.reset_default_graph()\ninputs = tf.placeholder(tf.float32, shape=(None, trainDim), name='inputs')\nlabels = tf.placeholder(tf.float32, shape=(None, 1), name='labels')\n\n# First layer\nhidden_size = 128\nW1 = tf.get_variable(\"W1\", shape=[trainDim, hidden_size],\\\n           initializer=tf.contrib.layers.xavier_initializer())\nb1 = tf.get_variable(\"b1\", shape=[1, hidden_size],\\\n           initializer=tf.zeros_initializer())\nW2 = tf.get_variable(\"W2\", [hidden_size, 1],\\\n            initializer = tf.contrib.layers.xavier_initializer())\nb2 = tf.get_variable(\"b2\", [1, 1], initializer = tf.zeros_initializer())\n\n","4890ea92":"# set up the relationships\nZ1 = tf.add(tf.matmul(inputs, W1), b1)\nA1 = tf.nn.sigmoid(Z1)\nZ2 = tf.add(tf.matmul(A1, W2), b2) \ncost = tf.reduce_mean(tf.losses.hinge_loss(labels = labels, logits = Z2))","3cd7494a":"# optimizer and cost\noptimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost)\ninit = tf.global_variables_initializer()","d7abe6ba":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain","412a23df":"# import module we'll need to import our custom module\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/lossespy\/Losses.py\", dst = \"..\/working\/Losses.py\")\n\n# import all our functions\nfrom Losses import *","5fc2856a":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","4c42e809":"(market_train_df, _) = env.get_training_data()","c34f75c7":"cat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                    'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                    'returnsOpenPrevMktres10']\n\nfrom sklearn.preprocessing import StandardScaler\n \nmarket_train_df[num_cols] = market_train_df[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train_df[num_cols] = scaler.fit_transform(market_train_df[num_cols])\n\n#col_mean = np.mean(market_train_df.returnsOpenPrevMktres10)\n#market_train_df['returnsOpenPrevMktres10'].fillna(col_mean, inplace=True)","ad56d491":"# data formatting and splitting\nmarket_train_df['y'] = ((market_train_df.returnsOpenNextMktres10 > 0).values).astype(int)\nmarket_train_df['year'] = pd.to_datetime(market_train_df.time).dt.year\ntrain = market_train_df[market_train_df.year <= splitYear]\ntest = market_train_df[market_train_df.year > splitYear]","fd10ecfa":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\ntrain_indices = np.where(market_train_df.year <= splitYear)[0] \ntest_indices = np.where(market_train_df.year > splitYear)[0]\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\nX_test, y_test, r_test, u_test, d_test = get_input(market_train_df, test_indices)","c9930465":"# mini-batch generation function\nimport math\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation, :]\n    shuffled_Y = Y[permutation, :].reshape((m, Y.shape[1]))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","a42e55c0":"# make the mini-batches\nX = X_train['num']\ny = np.expand_dims(y_train, 1).astype(int)\nminibatches = random_mini_batches(X, y)","0a946e7b":"# Start the session to compute the tensorflow graph\ntestPreds = None\nwith tf.Session() as sess:\n\n    # Run the initialization\n    sess.run(init)\n        \n    # Do the training loop\n    for epoch in range(num_epochs):\n        \n        epoch_cost = 0.\n                \n        # iterate through the minibatches\n        for minibatch in minibatches:\n            \n            # Select a minibatch\n            (minibatch_X, minibatch_Y) = minibatch\n            \n            # IMPORTANT: The line that runs the graph on a minibatch.\n            # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n            _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={inputs: minibatch_X, labels: minibatch_Y})\n            epoch_cost += minibatch_cost \n\n        # Print the cost every epoch\n        print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n\n    # make the predictions on the outcome\n    trainPreds = sess.run([Z2], feed_dict={inputs: X_train['num']})\n    testPreds = sess.run([Z2], feed_dict={inputs: X_test['num']})","5c81618b":"# get the test and train predictions\ntemp_train = trainPreds[0].flatten()\nprobs_train = np.exp(temp_train)\/(1 + np.exp(temp_train))\nconfidence_train = 2*(probs_train - 0.5)\n\ntemp_test = testPreds[0].flatten()\nprobs_test = np.exp(temp_test)\/(1 + np.exp(temp_test))\nconfidence_test = 2*(probs_test - 0.5)","0abe0f55":"def computeSigmaScore(preds, r, u, d):\n    x_t_i = preds * r * u\n    data = {'day' : d, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score_valid = mean \/ std\n    return(score_valid)\n    \ndef computeCrossEntropyLoss(probs, r, eps = 1e-7):\n    labels = (r >= 0).astype(int)\n    probs_clipped = np.clip(probs, eps, 1.0-eps)\n    return(np.mean(labels*np.log(probs_clipped) + (1-labels)*np.log(1-probs_clipped)))","3a057aba":"[computeSigmaScore(confidence_test, r_test, u_test, d_test), \n -computeCrossEntropyLoss(probs_test, r_test)]","4b0bca97":"[computeSigmaScore(confidence_train, r_train, u_train, d_train), \n -computeCrossEntropyLoss(probs_train, r_train)]","a82bd44e":"# Make the TensorFlow Graph","a85c9648":"# Load in the Data","a75f252d":"# Train neural net model"}}