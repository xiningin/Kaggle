{"cell_type":{"67dd8293":"code","d1d4a543":"code","23595117":"code","f0945a27":"code","630b1da8":"code","c80beb17":"code","068ef733":"code","f8a6fd24":"code","4ad1cf47":"code","fc528933":"code","ed0d00cf":"code","1c778d86":"code","98dda1b1":"code","ac407337":"code","b667e450":"code","1e7d3cb2":"code","b7e6b6bc":"code","9c6f025d":"markdown","278bbad2":"markdown","b39f742e":"markdown","5eedf1d8":"markdown","9686b1d1":"markdown","ce70ac0c":"markdown","d76cb6d9":"markdown","351029c5":"markdown","ff5b2a0d":"markdown","481faba4":"markdown"},"source":{"67dd8293":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1d4a543":"!pip install torchinfo","23595117":"import zipfile\nimport glob\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, transforms\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\n\nimport wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb\")\n\nwandb.init(project='Cat-vs-Dog-CNN', save_code=True)\n\n# https:\/\/github.com\/TylerYep\/torchinfo\nfrom torchinfo import summary # conda install -c conda-forge torchinfo","f0945a27":"train_dir = 'train'\ntest_dir = 'test'\nwith zipfile.ZipFile('\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/train.zip') as train_zip:\n    train_zip.extractall('')\n    \nwith zipfile.ZipFile('\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/test.zip') as test_zip:\n    test_zip.extractall('')\ntrain_list = glob.glob(os.path.join(train_dir,'*.jpg'))\ntest_list = glob.glob(os.path.join(test_dir, '*.jpg'))\nprint(f\"Train Data: {len(train_list)}\")\nprint(f\"Test Data: {len(test_list)}\")","630b1da8":"labels = [path.split('\/')[-1].split('.')[0] for path in train_list]","c80beb17":"random_idx = np.random.randint(1, len(train_list), size=9)\nfig, axes = plt.subplots(3, 3, figsize=(16, 12))\n\nfor idx, ax in enumerate(axes.ravel()):\n    img = Image.open(train_list[idx])\n    ax.set_title(labels[idx])\n    ax.imshow(img)","068ef733":"# we reserve 20% of the training set for the validation and the remaining 80% for training\nVALIDATION_RATIO = 0.2\n\ntrain_list, valid_list = train_test_split(train_list, \n                                          test_size=VALIDATION_RATIO,\n                                          stratify=labels,\n                                          random_state=0)\n\ntraining_size = len(train_list)\nvalidation_size = len(valid_list)\ntesting_size = len(test_list)\n\nprint(f\"Train Data: {training_size}\")\nprint(f\"Validation Data: {validation_size}\")\nprint(f\"Test Data: {testing_size}\")","f8a6fd24":"train_transforms = transforms.Compose([\n#         transforms.Resize(128), # makes it easier for the GPU\n        transforms.Resize((227, 227)),\n#         transforms.RandomResizedCrop(112),\n#         transforms.RandomResizedCrop(227, 227),\n#         transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\nval_transforms = transforms.Compose([\n#         transforms.Resize(128),\n        transforms.Resize((227, 227)),\n#         transforms.CenterCrop(112),\n#         transforms.CenterCrop(227, 227),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n\n\ntest_transforms = transforms.Compose([\n#         transforms.Resize(128),\n        transforms.Resize((227, 227)),\n#         transforms.CenterCrop(112),\n#         transforms.CenterCrop(227, 227),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])","4ad1cf47":"class CatsDogsDataset(Dataset):\n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n        self.filelength = len(file_list)\n\n    def __len__(self):\n        return self.filelength\n\n    def __getitem__(self, idx):\n        img_path = self.file_list[idx]\n        img = Image.open(img_path)\n        img_transformed = self.transform(img)\n        label = img_path.split(\"\/\")[-1].split(\".\")[0]\n        label = 1 if label == \"dog\" else 0\n        return img_transformed, label","fc528933":"train_data = CatsDogsDataset(train_list, transform=train_transforms)\nvalid_data = CatsDogsDataset(valid_list, transform=test_transforms)\ntest_data = CatsDogsDataset(test_list, transform=test_transforms)","ed0d00cf":"batch_size = 32\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)","1c778d86":"dataiter = iter(train_loader)\nimages, labels = dataiter.next()\n\nprint(images.shape) # (number_of_images, batch, image_width, image_height)\nprint(torch.unique(labels).size(dim=0))","98dda1b1":"plt.imshow(images[0].permute(1, 2, 0).cpu().squeeze())\nplt.title(\"Ground Truth: {}\".format(labels[0]))\nplt.show()","ac407337":"# If you think 224x224 is the right input size for AlexNet please\n# @see see https:\/\/cs231n.github.io\/convolutional-networks\/\n# @see https:\/\/stackoverflow.com\/questions\/36733636\/number-of-neurons-in-alexnet\n# @see https:\/\/datascience.stackexchange.com\/questions\/29245\/what-is-the-input-size-of-alex-net\n# @see https:\/\/learnopencv.com\/understanding-alexnet\/\nclass AlexNet(nn.Module):\n    def __init__(self, num_classes: int):\n        super(AlexNet, self).__init__()\n        \n        #----------------------------\n        # CONVOLUTIONAL LAYERS\n        #----------------------------\n        self.feature_extraction = nn.Sequential(\n            # 1st Convolutional Layer\n            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2, bias=False),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n            \n            # 2nd Convolutional Layer\n            nn.Conv2d(in_channels=96, out_channels=192, kernel_size=5, stride=1, padding=2, bias=False),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n            \n            # 3rd Convolutional Layer\n            nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.ReLU(inplace=True),\n\n            # 4th Convolutional Layer\n            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.ReLU(inplace=True),\n            \n            # 5th Convolutional Layer\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=0),\n        )\n\n        #----------------------------\n        # FULLY CONNECTED LAYERS\n        #----------------------------\n        self.classifier = nn.Sequential(\n            # 1st Fully Connected Layer\n            nn.Linear(in_features=(256 * 6 * 6), out_features=4096),\n            nn.ReLU(inplace=True),\n            # Dropout to prevent overfitting\n            nn.Dropout(p=0.5),\n\n            # 2nd Fully Connected Layer\n            nn.Linear(in_features=4096, out_features=4096),\n            nn.ReLU(inplace=True),\n            # Dropout to prevent overfitting\n            nn.Dropout(p=0.5),\n\n            # 3rd Fully Connected Layer\n            nn.Linear(in_features=4096, out_features=num_classes),\n        )\n\n    # Here we actually _build_ the net.    \n    def forward(self,x) -> torch.Tensor:\n        # Convolutional layers\n        x = self.feature_extraction(x)\n        # Flatten\n        x = x.view(-1, 256 * 6 * 6)\n        # Fully connected layers\n        x = self.classifier(x)\n        return x","b667e450":"model = AlexNet(2)\n# print(model)\n\nsummary(model, input_size=(batch_size, 3, 227, 227))","1e7d3cb2":"# cuda:0, in case of multiple GPUs we will use the first one (0)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Detected device: {}'.format(device))\n# Put model to device\nmodel.to(device)\n# batch.to(device)\nprint('Training on {}!'.format(device))","b7e6b6bc":"# Defines the learning rates for the parameter updates\nlr_rate = 1e-3 # e.q to 0.003, you can change it if needed\n# To update the hyperparameters of the model.\noptimizer = torch.optim.SGD(model.parameters(), lr=lr_rate) # momentum=0.5 decreased the accuracy of almost 2%\ncriterion = nn.CrossEntropyLoss()\n\nEPOCHS = 50\n\nfor epoch in range(EPOCHS):\n    # TRAINING LOOP\n    training_loss = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        # -------------\n        # Forward Pass\n        # -------------\n        # Clear the gradients as we don't want any gradient from previous epoch\n        # to carry forward: don't want to cummulate gradients.\n        optimizer.zero_grad()\n        # Forward Pass\n        output = model(images)\n        # Find the Loss\n        loss = criterion(output, labels)\n        # Calculate gradients\n        loss.backward()\n        # Update Weights\n        optimizer.step()\n        training_loss += loss.item()\n    \n    # VALIDATION LOOP\n    with torch.no_grad(): # we don't need gradients in the validation phase\n        validation_loss = 0\n        correct_classified = 0\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            # -------------\n            # Forward Pass\n            # -------------\n            output = model(images)\n            # Returns the maximum value of all elements in the input tensor (predictions).\n            # dim: the dimension to reduce the tensor to.\n            # _: prediction confidence\n            # predicted: prediction label\n            _, predicted = torch.max(output, dim=1)\n            # Find the Loss\n            loss = criterion(output, labels)\n            # Calculate Loss\n            validation_loss += loss.item()\n            \n            # Comparing the prediction (predicted) with the ground truth (labels)\n            correct_classified += int(predicted.eq(labels).sum().item())\n\n    validation_accuracy = correct_classified \/ validation_size\n    print('Epoch {}'.format(epoch+1),\n          \"\\t training_loss: \", training_loss,\n          '\\t validation_loss: ', validation_loss,\n          '\\t validation_accuracy: ', validation_accuracy),\n    wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss, 'validation_accuracy': validation_accuracy})","9c6f025d":"# Convolutional Neural Network","278bbad2":"## Use Sklearn to split data","b39f742e":"## Plot random image with their label","5eedf1d8":"## Unzip datasets","9686b1d1":"We will discuss this in more detail in a near future...","ce70ac0c":"Create dataloader, you can modify the batch size if needed","d76cb6d9":"# Run NN on CPU or GPU?\nInit the model and put it on GPU","351029c5":"Define the dataset using PIL to read image","ff5b2a0d":"Kaggle link: https:\/\/www.kaggle.com\/c\/dogs-vs-cats-redux-kernels-edition","481faba4":"## Import everything needed"}}