{"cell_type":{"21d84169":"code","00d40281":"code","22b7be7a":"code","43a65756":"code","6c3d0d2f":"code","ddeba7b1":"code","acd6b89e":"code","e0c784a3":"code","bd7bdaa1":"code","20b036a9":"code","540c12cb":"code","8e4175bd":"code","653d104a":"code","bb7ead0a":"code","678ea182":"code","ec762b2a":"code","ca9ace83":"code","957fba6c":"markdown"},"source":{"21d84169":"# RBF SVC STEPS","00d40281":"#related libraries","22b7be7a":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","43a65756":"#import and split the data","6c3d0d2f":"diabetes = pd.read_csv(\"..\/input\/diabetes\/diabetes.csv\")\ndf = diabetes.copy()\ndf = df.dropna()\ny = df[\"Outcome\"]\nX = df.drop(['Outcome'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.30, \n                                                    random_state=42)","ddeba7b1":"#set and fit the model","acd6b89e":"svc_model=SVC(kernel=\"rbf\",gamma='auto').fit(X_train,y_train)   # Actually,we don't have to indicate \"rbf\" \n                                                   #because rbf is default setting for SVC kernel. ","e0c784a3":"svc_model","bd7bdaa1":"y_pred= svc_model.predict(X_test)\naccuracy_score(y_test,y_pred)","20b036a9":"#model tuning","540c12cb":"# we will tune \"c value \" and \"gamma\" which are hiperparameters for this model","8e4175bd":"svc_params={\"C\": [0.0001,0.001,0.01,0.1,1,5,10,50,100],\"gamma\":[0.0001,0.001,0.01,0.1,1,5,10,50,100]}","653d104a":"svc_cv_model= GridSearchCV(svc_model,svc_params,cv=10,n_jobs=-1,verbose=2)\nsvc_cv_model.fit(X_train,y_train)","bb7ead0a":"svc_cv_model.best_params_","678ea182":"svc_final_model=SVC(kernel=\"rbf\", C=10,gamma=0.0001).fit(X_train,y_train)","ec762b2a":"y_pred=svc_final_model.predict(X_test)\naccuracy_score(y_test,y_pred)","ca9ace83":"# We found 0.766 by Logistic Regression\n#          0.775 by Naive Bayes \n#          0.731 by KNN\n#          0.744 by Linear SVC\n#And now,  0.735 by Nonlinear SVC Steps","957fba6c":"#Thanks to https:\/\/github.com\/mvahit\/DSMLBC"}}