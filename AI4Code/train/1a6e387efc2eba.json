{"cell_type":{"385611a0":"code","a9c6e306":"code","7e400aff":"code","e33c25d4":"code","6e234e76":"code","c34135af":"code","fb79dc56":"code","8e0e6467":"code","6df47efa":"code","98569570":"code","48e1025f":"code","6adb3b6e":"code","9288d27f":"code","a6cc9621":"code","bd13502e":"code","66ee04ce":"code","df876bdb":"code","442da003":"code","ace19416":"code","0b4314d4":"code","84897371":"code","f4b8b246":"code","ffca7484":"code","80a9e56f":"code","83636f75":"code","034b666a":"code","cae2f716":"code","0c5ea3c8":"code","a9231eb2":"code","b5066084":"code","aed62543":"code","70dbc2ad":"code","091f4a39":"code","4395ce8c":"code","5d3a6355":"code","474375c0":"code","b4cf08ae":"code","e06ed216":"code","c25875e4":"code","60c7238b":"code","215c6612":"code","1f6a0b67":"code","082569b8":"code","5527acf2":"code","eb3d53b5":"code","b4405b0f":"code","70faad1d":"markdown","94f953f3":"markdown","fabefd24":"markdown","84b9eeac":"markdown","1bb84214":"markdown","ce9d270b":"markdown","a77956ed":"markdown","1295b89f":"markdown","0c072b47":"markdown","ce314de7":"markdown","a9b83064":"markdown","c91f2d3b":"markdown","d45b18b8":"markdown","da48a003":"markdown","5fe3be4d":"markdown","671df43a":"markdown","eebb4789":"markdown","9aefb128":"markdown","ccf65147":"markdown","1c656dc1":"markdown","af1e1df6":"markdown","01835aa8":"markdown","0c30bab4":"markdown","e9b3ec37":"markdown","e58aa943":"markdown","fe7aaadd":"markdown","f8577f26":"markdown","a953c706":"markdown","f0ae68b2":"markdown","5497517a":"markdown","4cd4eb61":"markdown","952896f3":"markdown","2f420d79":"markdown","bd658049":"markdown","701965b9":"markdown","f4cd8a26":"markdown","5d115741":"markdown","8923549a":"markdown","2de47636":"markdown","e64cb538":"markdown","1c0c65f6":"markdown"},"source":{"385611a0":"# Read files \nimport os\n\n# Data wrangling and data visualistion \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Processing texts\nimport nltk\nimport re\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\n# Others\nimport numpy as np\nfrom collections import Counter\nimport time\nfrom statistics import mean","a9c6e306":"# Read data \ndata = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\", encoding = \"latin-1\")\ndata = data.dropna(how = \"any\", axis = 1)\ndata.columns = ['label','body_text']\ndata.head()","7e400aff":"print(f\"Input data has {len(data)} rows and {len(data.columns)} columns.\")","e33c25d4":"print(f\"Out of {len(data)} rows, {len(data[data.label == 'spam'])} are spam and {len(data[data.label == 'ham'])} are ham.\")","6e234e76":"total = len(data)\nplt.figure(figsize = (5, 5))\nplt.title(\"Number of spam vs ham messages\")\nax = sns.countplot(x = 'label', data = data)\nfor p in ax.patches:\n    percentage = '{0:.0f}%'.format(p.get_height() \/ total * 100)\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height() + 20\n    ax.annotate(percentage, (x, y), ha = 'center')\nplt.show()","c34135af":"print(f\"Number of null in label: {data.label.isnull().sum()}\")\nprint(f\"Number of null in text: {data.body_text.isnull().sum()}\")","fb79dc56":"# body_len\ndata['body_len'] = data.body_text.apply(lambda x: len(x) - x.count(\" \"))\n\n# punct%\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3) * 100\ndata['punct%'] = data.body_text.apply(lambda x: count_punct(x))\n\ndata.head()","8e0e6467":"# Summary statistics\ndata[['body_len', 'punct%']].describe().transpose()","6df47efa":"# Text with maximum body_len\nlist(data.loc[data.body_len == 740, 'body_text'])","98569570":"# Text with maximum punct%\nlist(data.loc[data['punct%'] == 100, 'body_text'])","48e1025f":"# Plot body_len distribution for ham and spam messages \nbins = np.linspace(0, 200, 40)\ndata.loc[data.label == 'spam', 'body_len'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'spam')\ndata.loc[data.label == 'ham', 'body_len'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'ham')\nplt.legend(loc = 'best')\nplt.xlabel(\"body_len\")\nplt.title(\"Body length ham vs spam\")\nplt.show()","6adb3b6e":"# Plot punct% for ham and spam messages \nbins = np.linspace(0, 50, 40)\ndata.loc[data.label == 'spam', 'punct%'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'spam')\ndata.loc[data.label == 'ham', 'punct%'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'ham')\nplt.legend(loc = 'best')\nplt.xlabel(\"punct%\")\nplt.title(\"Punctuation percentage ham vs spam\")\nplt.show()","9288d27f":"\"NLP\" == \"nlp\"","a6cc9621":"\"NLP\".lower() == \"nlp\"","bd13502e":"\"I love NLP\" == \"I love NLP.\"","66ee04ce":"# List of punctuations in the string library \nstring.punctuation","df876bdb":"# Remove punctuation \ntext = 'OMG! Did you see what happened to her? I was so shocked when I heard the news. :('\nprint(text)\ntext = \"\".join([word for word in text if word not in string.punctuation])\nprint(text)","442da003":"# Available commands in the re library \ndir(re)","ace19416":"messy_text = 'This-is-a-made\/up.string*to>>>>test----2\"\"\"\"\"\"different~regex-methods'\nre.split('\\W+', messy_text)","0b4314d4":"re.findall('\\w+', messy_text)","84897371":"# Examples of stopwords \nstopwords = nltk.corpus.stopwords.words('english')\nstopwords[0:500:25]","f4b8b246":"print(text)\nprint(text.lower().split())\nprint([word for word in text.lower().split() if word not in stopwords])","ffca7484":"ps = nltk.PorterStemmer()\nwn = nltk.WordNetLemmatizer()","80a9e56f":"print(ps.stem('goose'))\nprint(ps.stem('geese'))","83636f75":"print(wn.lemmatize('goose'))\nprint(wn.lemmatize('geese'))","034b666a":"# Create function for cleaning text  \ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.findall('\\S+', text)\n    # text = [ps.stem(word) for word in tokens if word not in stopwords]\n    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]\n    return text\n\n# Apply function to body_text \ndata['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))\ndata[['body_text', 'cleaned_text']].head(10)","cae2f716":"# Collect ham words\nham_words = list(data.loc[data.label == 'ham', 'cleaned_text'])\n\n# Flatten list of lists\nham_words = list(np.concatenate(ham_words).flat)\n\n# Create dictionary to store word frequency\nham_words = Counter(ham_words)\npd.DataFrame(ham_words.most_common(50), columns = ['word', 'frequency'])","0c5ea3c8":"# Collect spam words\nspam_words = list(data.loc[data.label == 'spam', 'cleaned_text'])\n\n# Flatten list of lists\nspam_words = list(np.concatenate(spam_words).flat)\n\n# Create dictionary to store word frequency\nspam_words = Counter(spam_words)\npd.DataFrame(spam_words.most_common(50), columns = ['word', 'frequency'])","a9231eb2":"# Define extra stopwords\nextra_stopwords = ['u', 'im', '2', 'ur', 'ill', '4', 'lor', 'r', 'n', 'da', 'oh']\n\n# Remove extra stopwords  \ndata['cleaned_text'] = data['cleaned_text'].apply(lambda x: [word for word in x if word not in extra_stopwords])","b5066084":"# Organise ham words data\nham_words = list(data.loc[data.label == 'ham', 'cleaned_text'])\nham_words = list(np.concatenate(ham_words).flat)\nham_words = Counter(ham_words)\nham_words = pd.DataFrame(ham_words.most_common(30), columns = ['word', 'frequency'])\n\n# Plot most common harm words\nfig, ax = plt.subplots(figsize = (15, 5))\nsns.barplot(x = 'word', y = 'frequency', data = ham_words, ax = ax)\nplt.xticks(rotation = '90')\nplt.title(\"30 most common ham words\")","aed62543":"# Organise spam words data\nspam_words = list(data.loc[data.label == 'spam', 'cleaned_text'])\nspam_words = list(np.concatenate(spam_words).flat)\nspam_words = Counter(spam_words)\nspam_words = pd.DataFrame(spam_words.most_common(30), columns = ['word', 'frequency'])\n\n# Plot most common harm words\nfig, ax = plt.subplots(figsize = (15, 5))\nsns.barplot(x = 'word', y = 'frequency', data = spam_words, ax = ax)\nplt.xticks(rotation = '90')\nplt.title(\"30 most common spam words\")","70dbc2ad":"# CountVectorizer\ncorpus = ['I love bananas', 'Bananas are so amazing!', 'Bananas go so well with pancakes']\ncount_vect = CountVectorizer()\ncorpus = count_vect.fit_transform(corpus)\ncount_vect.get_feature_names()","091f4a39":"pd.DataFrame(corpus.toarray(), columns = count_vect.get_feature_names())","4395ce8c":"# TfidfTransformer\ntfidf_transformer = TfidfTransformer()\ncorpus = tfidf_transformer.fit_transform(corpus)\npd.DataFrame(corpus.toarray(), columns = count_vect.get_feature_names())","5d3a6355":"# TfidfVectorizer\ncorpus = ['I love bananas', 'Bananas are so amazing!', 'Bananas go so well with pancakes']\ntfidf_vect = TfidfVectorizer()\ncorpus = tfidf_vect.fit_transform(corpus)\npd.DataFrame(corpus.toarray(), columns = tfidf_vect.get_feature_names())","474375c0":"data.head()","b4cf08ae":"# Train test split\nX_train, X_test, Y_train, Y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data.label, random_state = 42, test_size = 0.2)\n\n# Check shape \nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"Y_train shape: {Y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"Y_test shape: {Y_test.shape}\")","e06ed216":"# Instantiate and fit TfidfVectorizer\ntfidf_vect = TfidfVectorizer(analyzer = clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n\n# Use fitted TfidfVectorizer to transform body text in X_train and X_test\ntfidf_train = tfidf_vect.transform(X_train['body_text'])\ntfidf_test = tfidf_vect.transform(X_test['body_text'])\n\n# Recombine transformed body text with body_len and punct% features\nX_train = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop = True), pd.DataFrame(tfidf_train.toarray())], axis = 1)\nX_test = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop = True), pd.DataFrame(tfidf_test.toarray())], axis = 1)\n\n# Check shape\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"Y_train shape: {Y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"Y_test shape: {Y_test.shape}\")","c25875e4":"# Default random forest \nprint(RandomForestClassifier())","60c7238b":"# Manual grid search for random forest \ndef explore_rf_params(n_est, depth):\n    rf = RandomForestClassifier(n_estimators = n_est, max_depth = depth, n_jobs = -1, random_state = 42)\n    rf_model = rf.fit(X_train, Y_train)\n    Y_pred = rf_model.predict(X_test)\n    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')\n    print(f\"Est: {n_est} \/ Depth: {depth} ---- Precision: {round(precision, 3)} \/ Recall: {round(recall, 3)} \/ Accuracy: {round((Y_pred==Y_test).sum() \/ len(Y_pred), 3)}\")\n    \nfor n_est in [50, 100, 150]:\n    for depth in [10, 20, 30, None]:\n        explore_rf_params(n_est, depth)","215c6612":"# Instantiate RandomForestClassifier with optimal set of hyperparameters \nrf = RandomForestClassifier(n_estimators = 100, max_depth = None, random_state = 42, n_jobs = -1)\n\n# Fit model\nstart = time.time()\nrf_model = rf.fit(X_train, Y_train)\nend = time.time()\nfit_time = end - start\n\n# Predict \nstart = time.time()\nY_pred = rf_model.predict(X_test)\nend = time.time()\npred_time = end - start\n\n# Time and prediction results\nprecision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')\nprint(f\"Fit time: {round(fit_time, 3)} \/ Predict time: {round(pred_time, 3)}\")\nprint(f\"Precision: {round(precision, 3)} \/ Recall: {round(recall, 3)} \/ Accuracy: {round((Y_pred==Y_test).sum() \/ len(Y_pred), 3)}\")","1f6a0b67":"# Confusion matrix for RandomForestClassifier\nmatrix = confusion_matrix(Y_test, Y_pred)\nsns.heatmap(matrix, annot = True, fmt = 'd')","082569b8":"# Default gradient boosting \nprint(GradientBoostingClassifier())","5527acf2":"# Instantiate GradientBoostingClassifier\ngb = GradientBoostingClassifier(random_state = 42)\n\n# Fit model\nstart = time.time()\ngb_model = gb.fit(X_train, Y_train)\nend = time.time()\nfit_time = end - start\n\n# Predict \nstart = time.time()\nY_pred = gb_model.predict(X_test)\nend = time.time()\npred_time = end - start\n\n# Time and prediction results\nprecision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')\nprint(f\"Fit time: {round(fit_time, 3)} \/ Predict time: {round(pred_time, 3)}\")\nprint(f\"Precision: {round(precision, 3)} \/ Recall: {round(recall, 3)} \/ Accuracy: {round((Y_pred==Y_test).sum() \/ len(Y_pred), 3)}\")","eb3d53b5":"# Confusion matrix for GradientBoostingClassifier\nmatrix = confusion_matrix(Y_test, Y_pred)\nsns.heatmap(matrix, annot = True, fmt = 'd')","b4405b0f":"# Instantiate TfidfVectorizer, RandomForestClassifier and GradientBoostingClassifier \ntfidf_vect = TfidfVectorizer(analyzer = clean_text)\nrf = RandomForestClassifier(random_state = 42, n_jobs = -1)\ngb = GradientBoostingClassifier(random_state = 42)\n\n# Make columns transformer\ntransformer = make_column_transformer((tfidf_vect, 'body_text'), remainder = 'passthrough')\n\n# Build two separate pipelines for RandomForestClassifier and GradientBoostingClassifier \nrf_pipeline = make_pipeline(transformer, rf)\ngb_pipeline = make_pipeline(transformer, gb)\n\n# Perform 5-fold cross validation and compute mean score \nrf_score = cross_val_score(rf_pipeline, data[['body_text', 'body_len', 'punct%']], data.label, cv = 5, scoring = 'accuracy', n_jobs = -1)\ngb_score = cross_val_score(gb_pipeline, data[['body_text', 'body_len', 'punct%']], data.label, cv = 5, scoring = 'accuracy', n_jobs = -1)\nprint(f\"Random forest score: {round(mean(rf_score), 3)}\")\nprint(f\"Gradient boosting score: {round(mean(gb_score), 3)}\")","70faad1d":"## 4.3 Tokenize words \n\nTokenizing involves splitting a string or sentence into a list of characters and we can do so by utilising the **regular expression (re)** library in Python. ","94f953f3":"The key hyperparameters to pay attention to are:\n- **max_depth** (maximum depth of each decision tree)\n- **n_estimators** (how many parallel decision trees to build)\n- **random_state** (for reproducibility purpose)\n- **n_jobs** (number of jobs to run in parallel) \n\nLet's now manually build a simple grid search using nested for loops in order to explore the most optimal set of hyperparameters.","fabefd24":"# 1. Import and read data","84b9eeac":"As humans, we can almost immediately see that the two texts above are exactly the same. However, Python fails to realise this. It is therefore important that we remove all punctuations in order to allow Python to interpret the text more clearly. \n\nThese are the punctuations that are stored in the **string** library in Python. ","1bb84214":"We can use [list comprehension](https:\/\/www.w3schools.com\/python\/python_lists_comprehension.asp) to remove all punctuations in a string of characters. ","ce9d270b":"As we can see, spam messages have a longer body length i.e. contain more words. \n\nWhat about punctuation percentage?","a77956ed":"# Conclusion\n\nTo wrap up, we have successfully completed an end-to-end natural language processing (NLP) project which involves building a binary classifier capable of classifiying a given text message as spam or ham.\n\nWe started off the project by exploring the dataset, followed by feature engineering where we created two new features: *body_len* and *punct%*. We then moved on to performing some preprocessing steps that are specific to the NLP pipeline such as removing punctuations and stopwords, tokenizing and stemming \/ lemmatization. After that, we performed vectorization using **TfidfVectorizer** in order to encode text and turn them into feature vectors for machine learning. Finally, we were able to build two separate prediction models: **RandomForestClassifier** and **GradientBoostingClassifier** as well as compare their accuracy and overall performance.\n\nI would like to give a final thanks to [Derek Jedamski](https:\/\/www.linkedin.com\/in\/derek-jedamski-8a887045\/?trk=lil_course) and his course, [NLP with Python for Machine Learning Essential Training](https:\/\/www.linkedin.com\/learning\/nlp-with-python-for-machine-learning-essential-training) for introducing to the world of NLP in a way that was straightforward and easy-to-follow. I would highly encourage checking out his other courses on machine learning in Python. \n\nFollow me:\n- [Twitter](http:\/\/www.twitter.com\/chongjason914)\n- [LinkedIn](http:\/\/www.linkedin.com\/in\/chongjason914)\n- [Medium](http:\/\/www.medium.com\/@chongjason)\n- [YouTube](http:\/\/www.youtube.com\/jasonchong914)","1295b89f":"The two distributions don't look awfully different although ham messages do appear to have a longer tail i.e. ham messages tend to have a higher punctuation percentage.","0c072b47":"As we can see, we get exactly the same result as section 5.1.\n\nSince the results are the same, we will use TfidfVectorizer in this notebook. ","ce314de7":"Precision is constant at 1 for all cases. Recall and accuracy, on the other hand, improves as max_depth increases with None max_depth giving the best results. There is very little additional improvement adding 50 more trees after the 100th tree so we will set n_estimators = 100.","a9b83064":"Unfortunately, nltk did not account for all the variations to stopwords. Here, I will define some extra stopwords to further clean up our corpus of words. \n- u = you\n- im = I'm\n- 2 = to\n- ur = your\n- ill = I'll\n- 4 = for\n- lor (slang word)\n- r = are\n- n = and\n- da = the\n- oh (slang word) ","c91f2d3b":"## 6.1.2 GradientBoostingClassifier\n\n[GradientBoostingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html) is an ensemble learning method (boosting) that takes an iterative approach to combining weak learners to create a strong learner by focusing on mistakes of prior iterations. ","d45b18b8":"## 5.2 How TfidfVectorizer works\n\n[TfidfVectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html) is equivalent to CountVectorizer followed by TfidfTransformer. ","da48a003":"The key hyperparameters to pay attention to are:\n- **learning_rate** (weight of each sequential tree on the final prediction)\n- **max_depth** (maximum depth of each decision tree)\n- **n_estimators** (number of sequential trees)\n- **random_state** (for reproducibility purpose)\n\nUnfortunately, grid search for gradient boosting will take a long time so let's just use the default hyperparameters for now.","5fe3be4d":"# Introduction \n\nNatural language processing (NLP) refers to the field within artificial intelligence that deals with the interaction between computers and humans using the natural language. This includes enabling computers to manipulate, analyse, interpret and generate human language. \n\nThis notebook is heavily inspired by [NLP with Python for Machine Learning Essential Training](https:\/\/www.linkedin.com\/learning\/nlp-with-python-for-machine-learning-essential-training), a course given by [Derek Jedamski](https:\/\/www.linkedin.com\/in\/derek-jedamski-8a887045\/?trk=lil_course) on LinkedIn. My goal with this notebook is to summarise what I have learned from the course as well as recreate the results of a spam classifier, a binary classifier (prediction model) capable of classifying a given text message as *ham* or *spam*. \n\nThe [SMS spam collection dataset](https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset) is a set of SMS tagged messages that have been collected for SMS spam research. It contains a set of 5,572 SMS messages in English, tagged according to ham (legitimate) or spam. \n\nFor ease of reference, this notebook is structured as follows:\n0. Import libraries\n1. Import and read data\n2. Exploratory data analysis (EDA)\n3. Feature engineering\n4. Cleaning text\n5. Vectorization\n6. Modelling (using RandomForestClassifier and GradientBoostingClassifier)\n\nAgain, I would like to give full credits and reference to [Derek Jedamski](https:\/\/www.linkedin.com\/in\/derek-jedamski-8a887045\/?trk=lil_course) for his work and I recommend checking out his courses on LinkedIn.\n\nWithout further ado, let's begin!","671df43a":"Let's have a look at the text with maximum body_len as well as the text with maximum punct%.","eebb4789":"We have successfully removed the following stopwords from the original text:\n- did\n- you\n- what\n- to\n- her\n- i\n- was\n- so\n- when\n- the\n\nNotice that we first turned the original text into a list of lowercase words before running our list comprehension. This is because words are stored in lowercase letters in the nltk library. ","9aefb128":"# 3. Feature engineering\n\nFeature engineering is the process of creating new features and\/or transforming existing features to get the most out of your data. \n\nIn this section, I will create two new features:\n- body_len (length of the body text excluding spaces)\n- punct% (percentage of punctuation in the body text) \n\nThere are many other features that you can create to analyse a string or sentence, here are some [ideas](https:\/\/www.analyticsvidhya.com\/blog\/2021\/04\/a-guide-to-feature-engineering-in-nlp\/) to get started.","ccf65147":"There are significantly more ham messages (87%) than there are spam messages (13%). ","1c656dc1":"Both commands have not only removed all punctuations but more importantly tokenized the text into a list of words. ","af1e1df6":"## 5.1 How (Countvectorizer + TfidfTransformer) works\n\n[CountVectorizer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) creates a document-term matrix where the entry of each cell will be a count of the number of times that word occured in that document. \n\n[TfidfTransformer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html) is similar to that of a CountVectorizer but instead of the cells representing the count, the cells represent a weighting that is meant to identify how important a word is to an individual text message. The formula to compute the weighting for each cell is as follows:\n![image.png](attachment:image.png)","01835aa8":"# 4. Cleaning text \n\nIn order to better manage our messy text messages, we will perform the following steps to clean up the input data:\n\n- Turn words into lowercase letters only \n- Remove punctuation\n- Tokenize words\n- Remove stopwords\n- Stemming vs lemmatization (text normalization)","0c30bab4":"## 6.1 Train-test-split approach","e9b3ec37":"While both models, in this particular example, have returned a very similar prediction result, it is important to bear in mind the trade-offs that may occur in other scenarios where this is not the case. More specifically, it is worth considering the business context and the overall purpose in which the model is built for.\n\nFor example, in spam classification, it is better to optimise for precision as we can probably deal with some spam messages in our inbox here and there but we definitely don't want our model to classify an important message as spam. In contrast, in fraud detection, it is much better to optimise for recall as it is more costly if our model fails to identify a real threat (false negative) than it is if it identifies a false threat (false positive). ","e58aa943":"As we can see, stemming takes a more crude approach than lemmatizing by simply chopping off the end of a word using heuristics, without any understanding of the context in which a word is used. As a result,stemming can sometimes not return an actual word in the dictionary unlike lemmatizing which will always return a dictionary word. \n\nLemmatizing, on the other hand, considers multiple factors before simplifying a given word and is generally considered more accurate compared to stemming. However, this comes at the expense of being slower and more computationally expensive than stemming. ","fe7aaadd":"## 4.6 Putting everything together into a single clean_text function \n\nNow, we want to summarise everything that we have learned about text cleaning into a single function that we can apply to our original text messages data. ","f8577f26":"# 5. Vectorization \n\nVectorizing is the process of encoding text as integers to create feature vectors. ","a953c706":"# 2. Exploratory data analysis (EDA)\n\nExploratory data analysis is the process of performing initial investigations on data so as to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations (definition credits to Prasad Patil on \"[What is Exploratory Data Analysis?](https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15)\").  ","f0ae68b2":"## 4.2 Remove punctuation\n\nThe rationale behind removing punctuation is that punctuation does not hold any meaning in a text. We want Python to only focus on the words in a given text and not worry about the punctuations that are involved. ","5497517a":"## 4.1 Turn words into lowercase letters only\n\nPython does not see all characters as equal. Thus, we will need to convert all words into lowercase letters for consistency. ","4cd4eb61":"# 6. Modelling\n\nNow that our data is ready, we can finally move on to modelling, that is building a binary classifier to classify a given text as ham or spam.\n\nHere, we will consider two approach: **train-test-split** and **pipeline** as well as two types of machine learning models, or more specifically, ensemble methods: **random forest** and **gradient boosting**.\n\nEnsemble method is a technique that creates multiple models and then combinese them to produce better results than any of the single models individually. ","952896f3":"Yay, no missing data!","2f420d79":"Let's look at an example of tokenization using two different commands:\n- re.split\n- re.findall","bd658049":"## 6.2 Pipeline approach","701965b9":"## 4.5 Stemming vs lemmatization (word normalization)\n\n**Stemming:** The process of reducing inflection or derived words to their word stem or root by crudely chopping off the ends of a word to leave only the base.\n**Lemmatizing:** The process of grouping together inflected forms of a word so they can be analyzed as a single term. \n\nBroadly speaking, both stemming and lemmatizing serve the purpose of condensing the variations of the same word down to its root form. This is to prevent the computer from storing every single unique word it sees in a corpus of words but instead only take note of a word in its most basic form and correlate other words with similar meanings. \n\nFor example, *grew*, *grown*, *growing* and *growth* are all simply variations to the word *grow*. In this case, the computer only needs to remember the word *grow* and not the rest. ","f4cd8a26":"Now, use the newly created features to explore the text distribution.","5d115741":"Let's again apply list comprehension to remove all stopwords in a given string. ","8923549a":"## 6.1.1 RandomForestClassifier\n\n[RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) is an ensemble learning method (bagging) that constructs a collection of decision trees and then aggregates the predictions of each tree to determine the final prediction.","2de47636":"## 4.4 Remove stopwords\n\nStopwords are commonly used words in the English language like *but*, *if* and *the* that don't contribute much to the overall meaning of a sentence. For this reason, stopwords are usually removed in order to reduce the number of tokens Python needs to process when building our model. \n\nStopwords are stored in **nltk.corpus.stopwords** and here are some examples.","e64cb538":"Taadaa, all punctuations have been removed!","1c0c65f6":"# 0. Import libraries"}}