{"cell_type":{"07a7b46d":"code","4e014d0c":"code","aab58047":"code","39a0d6da":"code","da1af9ef":"code","74017fb5":"code","04305afa":"code","fd69c800":"code","223b6724":"code","fd9cfcd7":"code","09b899f8":"code","b1a300e5":"code","4e72d808":"code","5329a77b":"code","b1da9e0c":"code","3ec9179a":"code","1d8f3ffb":"code","ee9ae0fe":"code","735ac675":"code","4f063113":"code","b01f90cf":"code","1709289a":"code","e84a1421":"code","3cd7f015":"code","8a39eb02":"code","061448ed":"code","731d72a0":"code","6d31c319":"markdown","6896ddd4":"markdown","af164739":"markdown","a1ec078b":"markdown","d3d215fe":"markdown","175ffbb6":"markdown","44d07f5f":"markdown","983b5860":"markdown","3e18e693":"markdown","3f73a1db":"markdown","f053c78b":"markdown"},"source":{"07a7b46d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4e014d0c":"# Importing Necessary libraries\nimport re\nimport nltk\nimport spacy","aab58047":"# Loading Data\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","39a0d6da":"train.head()","da1af9ef":"train.isnull().sum()","74017fb5":"train=train.dropna()\ntarget = train['target']","04305afa":"train = train['keyword'] + ' ' + train['location'] + \" \" + train['text'] \ntest = test['keyword'] + \" \" + test['location'] + \" \" + test['text'] \ntrain.head()","fd69c800":"import string","223b6724":"train = train.str.lower()","fd9cfcd7":"train.head()","09b899f8":"target.head()","b1a300e5":"text=train","4e72d808":"text.head()","5329a77b":"def remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\ntext_clean = text.apply(lambda text: remove_punctuation(text))","b1da9e0c":"text_clean.head()","3ec9179a":"from nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))","1d8f3ffb":"def stopwords_(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntext_clean = text_clean.apply(lambda text: stopwords_(text))","ee9ae0fe":"text_clean.head()","735ac675":"\", \".join(stopwords.words('english'))","4f063113":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatizer_(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])","b01f90cf":"lemmatizer.lemmatize(\"wrote\", \"v\")","1709289a":"lemmatizer.lemmatize(\"written\", \"v\")","e84a1421":"text_clean = text_clean.apply(lambda text: lemmatizer_(text))\n\ntrain = text_clean","3cd7f015":"# train = train.drop(['id'],axis=1)\ntrain.head()","8a39eb02":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfig, (ax1) = plt.subplots(1, figsize=[12, 12])\nwordcloud = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(text_clean))\nax1.imshow(wordcloud)\nax1.axis('off')\nax1.set_title('Frequent Words',fontsize=16);","061448ed":"from sklearn.linear_model import LogisticRegression\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nprint(train.shape)\ntrain_vectors = vectorizer.fit_transform(train)\ntest_vectors = vectorizer.transform(test)\nprint(train_vectors.shape)\nclf = LogisticRegression(random_state=0).fit(train_vectors, target)\n\nprint(clf.predict(test_vectors))\n\nprint(clf.score(train_vectors,target))","731d72a0":"df=clf.predict(test_vectors)\nresult=pd.DataFrame(df)\nx=pd.concat([test['id'], result],axis=1)\nx.to_csv('mycsvfile.csv',index=False)\n","6d31c319":"In the earlier preview, you can find a mixture of upper and lower case letters. Now, you can see that whole of the text is in lower case. This forms the first step of text preprocessing. Let us now split our dataset for the target and text variables. ","6896ddd4":"Here we will remove keyword and location variables. They donot have much of a value here. In addition to that, there are many NaN values. So it is better to remove those variables. ","af164739":"Hope you find this useful. If you like this kernel, please consider upvoting it. ","a1ec078b":"Yeah. you have now completed the first phase of the text preprocessing. Now let us proceed to the next one. \n\nLemmatization is the process of reducing the words to their roots. Let us take a look at an example for better understanding. ","d3d215fe":"You can find that the functuations are removed. Now we will remove the so called stopwords. They are highly repetitive words in the text but do not posses a greater value for their presence. So we will remove it. ","175ffbb6":"Do I need to explain further. Hahaha. Not at all necesary. It is self explanatory. But if you have any doubts donot hesitate to comment in the comment section. \n\nLet us apply this to our text. ","44d07f5f":"![](http:\/\/)![](http:\/\/)If you find anything unnecessary or to be removed, you can do so by appending it to the stopwords or remove it manually.\n\nSometime later, I will share a kernel on how to solve this problem in an efficient manner. ","983b5860":"Now we have everything required for us to get started. Here id is the reference like an index. 'text' is our asset. We will be working on that. 'target' is the target variable. ","3e18e693":"You can find ityourself right. The use of removing these words. Want to know what are those words. Take a look at it. ","3f73a1db":"The above data has punctuation with it and they do not have any semantic meaning in our data. So we will remoce it. The following is a better way of removing it. ","f053c78b":"All of these can also be done by in-built packages. But it is a good parctice in the beginning to understand our data better. \n\nNow for the fun part, we will look at the most used words in the cleaned text. We will use wordcloud library for that. "}}