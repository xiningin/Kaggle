{"cell_type":{"eacd66f1":"code","d6ad6701":"code","7c03729f":"code","5a597222":"code","2cbb2753":"code","7d590d55":"code","afddebf7":"code","cc5b698f":"markdown","2e217269":"markdown","ed74a6be":"markdown","ed0459aa":"markdown","384651b7":"markdown","e197e060":"markdown","406d5344":"markdown","f697b7a1":"markdown","ff57fea1":"markdown","6b76b037":"markdown","947ea04b":"markdown","488b2042":"markdown","a7a9cec4":"markdown","a26b0349":"markdown","8e76d8ac":"markdown","a5487df2":"markdown","f9c36a6f":"markdown","8d3b62f8":"markdown","8b8ca599":"markdown","14200543":"markdown","61ff583e":"markdown","142bf005":"markdown","e38dda3f":"markdown","459d40d1":"markdown","b43b056b":"markdown","d3db477e":"markdown","2bd20f4d":"markdown","db57e848":"markdown"},"source":{"eacd66f1":"\n# Load libraries\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create logistic regression\nlogistic = linear_model.LogisticRegression()\n# Create range of candidate penalty hyperparameter values\npenalty = ['l1', 'l2']\n# Create range of candidate regularization hyperparameter values\nC = np.logspace(0, 4, 10)\n# Create dictionary hyperparameter candidates\nhyperparameters = dict(C=C, penalty=penalty)\n\n# Create grid search\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\nnp.logspace(0, 4, 10)\n\n\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\n# Predict target vector\nbest_model.predict(features)","d6ad6701":"# Load libraries\nfrom scipy.stats import uniform\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import RandomizedSearchCV\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create logistic regression\nlogistic = linear_model.LogisticRegression()\n# Create range of candidate regularization penalty hyperparameter values\npenalty = ['l1', 'l2']\n# Create distribution of candidate regularization hyperparameter values\nC = uniform(loc=0, scale=4)\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n# Create randomized search\nrandomizedsearch = RandomizedSearchCV(\nlogistic, hyperparameters, random_state=1, n_iter=100, cv=5, verbose=0,\nn_jobs=-1)\n# Fit randomized search\nbest_model = randomizedsearch.fit(features, target)\n\n# Define a uniform distribution between 0 and 4, sample 10 values\nuniform(loc=0, scale=4).rvs(10)\n\n# View best hyperparameters\nprint('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\nprint('Best C:', best_model.best_estimator_.get_params()['C'])\n\n# Predict target vector\nbest_model.predict(features)","7c03729f":"# Load libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n# Set random seed\nnp.random.seed(0)\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create a pipeline\npipe = Pipeline([(\"classifier\", RandomForestClassifier())])\n# Create dictionary with candidate learning algorithms and their hyperparameters\nsearch_space = [{\"classifier\": [LogisticRegression()],\n\"classifier__penalty\": ['l1', 'l2'],\n\"classifier__C\": np.logspace(0, 4, 10)},\n{\"classifier\": [RandomForestClassifier()],\n\"classifier__n_estimators\": [10, 100, 1000],\n\"classifier__max_features\": [1, 2, 3]}]\n# Create grid search\ngridsearch = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\n\n# View best model\nbest_model.best_estimator_.get_params()[\"classifier\"]\n\n# Predict target vector\nbest_model.predict(features)","5a597222":"# Load libraries\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n# Set random seed\nnp.random.seed(0)\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create a preprocessing object that includes StandardScaler features and PCA\npreprocess = FeatureUnion([(\"std\", StandardScaler()), (\"pca\", PCA())])\n# Create a pipeline\npipe = Pipeline([(\"preprocess\", preprocess),\n(\"classifier\", LogisticRegression())])\n# Create space of candidate values\nsearch_space = [{\"preprocess__pca__n_components\": [1, 2, 3],\n\"classifier__penalty\": [\"l1\", \"l2\"],\n\"classifier__C\": np.logspace(0, 4, 10)}]\n# Create grid search\nclf = GridSearchCV(pipe, search_space, cv=5, verbose=0, n_jobs=-1)\n# Fit grid search\nbest_model = clf.fit(features, target)\n\n# View best model\nbest_model.best_estimator_.get_params()['preprocess__pca__n_components']","2cbb2753":"# Load libraries\nimport numpy as np\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n\n# Create logistic regression\nlogistic = linear_model.LogisticRegression()\n# Create range of candidate regularization penalty hyperparameter values\npenalty = [\"l1\", \"l2\"]\n# Create range of candidate values for C\nC = np.logspace(0, 4, 1000)\n# Create hyperparameter options\nhyperparameters = dict(C=C, penalty=penalty)\n# Create grid search\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=1)\n# Fit grid search\nbest_model = gridsearch.fit(features, target)\n\n# Create grid search using one core\nclf = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=1, verbose=1)\n# Fit grid search\nbest_model = clf.fit(features, target)","7d590d55":"# Load libraries\nfrom sklearn import linear_model, datasets\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create cross-validated logistic regression\nlogit = linear_model.LogisticRegressionCV(Cs=100)\n# Train model\nlogit.fit(features, target)","afddebf7":"# Load libraries\nimport numpy as np\nfrom sklearn import linear_model, datasets\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n# Load data\niris = datasets.load_iris()\nfeatures = iris.data\ntarget = iris.target\n# Create logistic regression\nlogistic = linear_model.LogisticRegression()\n# Create range of 20 candidate values for C\nC = np.logspace(0, 4, 20)\n# Create hyperparameter options\nhyperparameters = dict(C=C)\n# Create grid search\n\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, n_jobs=-1, verbose=0)\n# Conduct nested cross-validation and outut the average score\ncross_val_score(gridsearch, features, target).mean()\n\ngridsearch = GridSearchCV(logistic, hyperparameters, cv=5, verbose=1)\n\nbest_model = gridsearch.fit(features, target)\n\nscores = cross_val_score(gridsearch, features, target)","cc5b698f":"# <b style=\"color:green\">Supported Discussion <\/b>\nIn the recipes of this chapter, we have kept the number of candidate models small to\nmake the code complete quickly. However, in the real world we will often have many\nthousands or tens of thousands of models to train. The end result is that it can take\nmany hours to find the best model. To speed up the process, scikit-learn lets us train\nmultiple models simultaneously. Without going into too much technical detail, scikitlearn\ncan simultaneously train models up to the number of cores on the machine.\nMost modern laptops have four cores, so (assuming you are currently on a laptop) we\ncan potentially train four models at the same time. This will dramatically increase the\nspeed of our model selection process. The parameter n_jobs defines the number of\nmodels to train in parallel.","2e217269":"# <b style=\"color:red\">Problem\nYou want to select the best model by searching over a range of learning algorithms\nand their respective hyperparameters.<\/b>","ed74a6be":"# <b style=\"color:blue\">Solution: \nCreate a dictionary of candidate learning algorithms and their hyperparameters:<\/b>","ed0459aa":"# 2. Selecting Best Models Using Randomized Search","384651b7":"# 4. Selecting Best Models When Preprocessing","e197e060":"# <b style=\"color:blue\">Solution:\nIf you are using a select number of learning algorithms, use scikit-learn\u2019s modelspecific\ncross-validation hyperparameter tuning. For example, LogisticRegres\nsionCV:<\/b>","406d5344":"# <b style=\"color:red\">Problem\nYou want to include a preprocessing step during model selection.<\/b>","f697b7a1":"# <b style=\"color:green\">Supported Discussion <\/b>\nNested cross-validation during model selection is a difficult concept for many people\nto grasp the first time. Remember that in k-fold cross-validation, we train our model\non k\u20131 folds of the data, use this model to make predictions on the remaining fold,\nand then evaluate our model best on how well our model\u2019s predictions compare to the\ntrue values. We then repeat this process k times.\nIn the model selection searches described in this chapter (i.e., GridSearchCV and Ran\ndomizedSearchCV), we used cross-validation to evaluate which hyperparameter values\nproduced the best models. However, a nuanced and generally underappreciated problem\narises: since we used the data to select the best hyperparameter values, we cannot\nuse that same data to evaluate the model\u2019s performance. The solution? Wrap the\ncross-validation used for model search in another cross-validation! In nested crossvalidation,\nthe \u201cinner\u201d cross-validation selects the best model, while the \u201couter\u201d crossvalidation\nprovides us with an unbiased evaluation of the model\u2019s performance. In\nour solution, the inner cross-validation is our GridSearchCV object, which we then\nwrap in an outer cross-validation using cross_val_score.","ff57fea1":"# <b style=\"color:blue\">Solution\nUse scikit-learn\u2019s RandomizedSearchCV:<\/b>","6b76b037":"# <b style=\"color:red\">Problem\nYou want to evaluate the performance of a model found through model selection.<\/b>","947ea04b":"# <b style=\"color:green\">Supported Discussion <\/b>\nIn the previous two recipes we found the best model by searching over possible\nhyperparameter values of a learning algorithm. However, what if we are not certain\nwhich learning algorithm to use? Recent versions of scikit-learn allow us to include\nlearning algorithms as part of the search space. In our solution we define a search\nspace that includes two learning algorithms: logistic regression and random forest\nclassifier. Each learning algorithm has its own hyperparameters, and we define their\ncandidate values using the format classifier__[hyperparameter name].","488b2042":"# 5. Speeding Up Model Selection with Parallelization","a7a9cec4":"# <b style=\"color:red\">Problem\nYou want a computationally cheaper method than exhaustive search to select the best\nmodel.<\/b>","a26b0349":"# <b style=\"color:red\">Problem : \nYou want to select the best model by searching over a range of hyperparameters.<\/b>","8e76d8ac":"# 6. Speeding Up Model Selection Using Algorithm-Specific Methods","a5487df2":"# 7. Evaluating Performance After Model Selection","f9c36a6f":"# <b style=\"color:blue\">Solution: \nUse scikit-learn\u2019s GridSearchCV:<\/b>","8d3b62f8":"# <b style=\"color:blue\">Solution:\nUse all the cores in your machine by setting n_jobs=-1:<\/b>","8b8ca599":"# <b style=\"color:green\">Supported Discussion <\/b>\nSometimes the characteristics of a learning algorithm allow us to search for the best\nhyperparameters significantly faster than either brute force or randomized model\nsearch methods. In scikit-learn, many learning algorithms (e.g., ridge, lasso, and elastic\nnet regression) have an algorithm-specific cross-validation method to take advantage\nof this. For example, LogisticRegression is used to conduct a standard logistic\nregression classifier, while LogisticRegressionCV implements an efficient crossvalidated\nlogistic regression classifier that has the ability to identify the optimum\nvalue of the hyperparameter C.","14200543":"# <b style=\"color:red\">Problem\nYou need to speed up model selection.<\/b>","61ff583e":"# 3. Selecting Best Models from Multiple Learning Algorithms","142bf005":"# <b style=\"color:blue\">Solution:\nCreate a pipeline that includes the preprocessing step and any of its parameters:<\/b>","e38dda3f":"# <b style=\"color:red\">Problem\nYou need to speed up model selection.<\/b>","459d40d1":"# <b style=\"color:green\">Supported Discussion <\/b>\nhave to be careful to properly handle preprocessing when conducting model selection.\nFirst, GridSearchCV uses cross-validation to determine which model has the\nhighest performance. However, in cross-validation we are in effect pretending that\nthe fold held out, as the test set is not seen, and thus not part of fitting any preprocessing\nsteps (e.g., scaling or standardization). For this reason, we cannot preprocess\nthe data and then run GridSearchCV. Rather, the preprocessing steps must be a part\nof the set of actions taken by GridSearchCV. While this might appear complex, the\nreality is that scikit-learn makes it simple. FeatureUnion allows us to combine multi\u2010\nple preprocessing actions properly. In our solution we use FeatureUnion to combine\ntwo preprocessing steps: standardize the feature values (StandardScaler) and Principal\nComponent Analysis (PCA). This object is called preprocess and contains both of\nour preprocessing steps. We then include preprocess into a pipeline with our learning\nalgorithm. The end result is that this allows us to outsource the proper (and confusing)\nhandling of fitting, transforming, and training the models with combinations\nof hyperparameters to scikit-learn.","b43b056b":"# 1. Selecting Best Models Using Exhaustive Search","d3db477e":"# <b style=\"color:blue\">Solution\nUse nested cross-validation to avoid biased evaluation:<\/b>","2bd20f4d":"# <b style=\"color:green\">Supported Discussion <\/b>\n\nGridSearchCV is a brute-force approach to model selection using cross-validation.\nSpecifically, a user defines sets of possible values for one or multiple hyperparameters,\nand then GridSearchCV trains a model using every value and\/or combination of values.\nThe model with the best performance score is selected as the best model. For\nexample, in our solution we used logistic regression as our learning algorithm, containing\ntwo hyperparameters: C and the regularization penalty. Don\u2019t worry if you\ndon\u2019t know what C and regularization mean; we cover them in the next few chapters.\nJust realize that C and the regularization penalty can take a range of values, which\nhave to be specified prior to training. For C, we define 10 possible values:","db57e848":"# <b style=\"color:green\">Supported Discussion <\/b>\nIn Recipe 1, we used GridSearchCV on a user-defined set of hyperparameter values\nto search for the best model according to a score function. A more efficient method\nthan GridSearchCV\u2019s brute-force search is to search over a specific number of random\ncombinations of hyperparameter values from user-supplied distributions (e.g., normal,\nuniform). scikit-learn implements this randomized search technique with Ran\ndomizedSearchCV."}}