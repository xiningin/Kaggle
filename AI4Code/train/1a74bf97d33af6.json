{"cell_type":{"607efa88":"code","0e1c101a":"code","2f273b75":"code","0fa78756":"code","691a2a3f":"code","d385e0ee":"code","4fc4c29f":"code","0236c663":"code","06edff0e":"code","f0b6f5b2":"code","606ea339":"code","b4032342":"code","4c64d33d":"code","32ba2a49":"code","81356cca":"code","df8b2944":"code","9d4de5bb":"code","e1425cdf":"code","72a1fc6d":"code","d7190e09":"code","56834040":"code","93f60cd2":"code","9767dedf":"code","25b3e54c":"code","4d7b4811":"code","f55c022c":"code","6bebb493":"code","53dbcd5e":"code","e623cf40":"code","e6e9e275":"code","03993934":"code","44ecbeb3":"code","ad98a621":"code","5683906b":"code","f2956d90":"code","92ba097d":"code","cf3c0b8f":"code","c523bb84":"code","c930fd9e":"code","6f16c1c1":"code","1a2d3da6":"code","e7769078":"code","9b42931d":"code","69c06973":"code","a64325dc":"code","4146d170":"code","ba4f81e1":"code","11162969":"code","f1e437d7":"code","1e324482":"markdown","207ce80c":"markdown","39891605":"markdown","e30c73c3":"markdown","f7e497b0":"markdown","94c8f457":"markdown","ae8eece9":"markdown","88424c5e":"markdown","979f2f4e":"markdown","b06b44f0":"markdown","f50283c8":"markdown","5ddbbd95":"markdown","7380d978":"markdown","a629390c":"markdown","43b7d664":"markdown","204caa33":"markdown","865dff01":"markdown","5e96b2bd":"markdown","4fb61c7b":"markdown","df54b272":"markdown","49760d12":"markdown","b2d25d45":"markdown","58e51f3c":"markdown","b8fedb1f":"markdown"},"source":{"607efa88":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(context = 'notebook', palette = sns.color_palette(\"Set2\"))\nimport numpy as np\nimport sklearn as sk","0e1c101a":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","2f273b75":"df","0fa78756":"feature=['Age','RestingBP','Cholesterol','MaxHR','Oldpeak']\nfor f in feature:\n    fig=plt.figure(figsize=(9,6))\n    sns.boxplot(x=df[f], hue=df.HeartDisease)","691a2a3f":"df = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\") # To make sure that we do not double down on this pre-processing\n\nfor f in feature:\n    q1, q3= np.percentile(df[f],[25,75])\n    iqr = q3 - q1\n    lower_bound = q1 -(1.5 * iqr) \n    upper_bound = q3 +(1.5 * iqr)\n    print(\"Feature: {}\".format(f))\n    print(\"lower_bound\",lower_bound)\n    print(\"upper_bound\",upper_bound)\n    df = df[(df[f]>=lower_bound) & (df[f]<=upper_bound)]\n    print(\"=-=-=-=-=-=-=-=-=-=-=-=-=\")","d385e0ee":"sns.countplot(data = df, x= \"Sex\")","4fc4c29f":"sns.displot(df, x= \"Age\", hue=\"Sex\")","0236c663":"sns.displot(df, x=\"RestingBP\", hue=\"Sex\")","06edff0e":"sns.boxplot(data= df, x=\"RestingBP\", y=\"Sex\")","f0b6f5b2":"sns.boxplot(data= df, x=\"Age\", y=\"Sex\")","606ea339":"sns.catplot(data=df, x=\"ChestPainType\", y=\"Age\", kind=\"swarm\", hue=\"Sex\")","b4032342":"df.ChestPainType = df.ChestPainType.replace({'ASY': 'Asymptomatic',\n                                             'ATA': 'Atypical angina',\n                                             'NAP': 'Non-Anginal Pain',\n                                              'TA': 'Typical Angina'})","4c64d33d":"for var in df.ChestPainType.value_counts().index:\n    print(var)\n    print(\"Male : Female \")\n    print(len(df[(df['ChestPainType'] == var) & (df['Sex'] == \"M\")]), \" : \", len(df[(df['ChestPainType'] == var) & (df['Sex'] == \"F\")]))","32ba2a49":"print(\"Total sample: {}\".format(len(df)))\nprint(\"Male: {}\".format(len(df[df['Sex'] == \"M\"])))\nprint(\"Female: {}\".format(len(df[df['Sex'] == \"F\"])))","81356cca":"for var in df.ChestPainType.value_counts().index:\n    print(var)\n    print(\"Male : Female \")\n    print(len(df[(df['ChestPainType'] == var) & (df['Sex'] == \"M\")])\/len(df[df['Sex'] == \"M\"]), \" : \", len(df[(df['ChestPainType'] == var) & (df['Sex'] == \"F\")])\/len(df[df['Sex'] == \"F\"]))","df8b2944":"sns.displot(df[(df['ChestPainType'] ==\"Atypical angina\")], x=\"Age\", hue=\"Sex\")","9d4de5bb":"from scipy import stats","e1425cdf":"rng = np.random.default_rng()\nMale_group = df[\"HeartDisease\"][(df[\"Sex\"] == \"M\") & (df[\"ChestPainType\"] == \"Atypical angina\")]\nFemale_group = df[\"HeartDisease\"][(df[\"Sex\"] == \"F\") & (df[\"ChestPainType\"] == \"Atypical angina\")]\nstats.ttest_ind(Male_group, Female_group)","72a1fc6d":"rng = np.random.default_rng()\nMale_group = df[\"HeartDisease\"][(df[\"Sex\"] == \"M\") & (df[\"ChestPainType\"] == \"Non-Anginal Pain\")]\nFemale_group = df[\"HeartDisease\"][(df[\"Sex\"] == \"F\") & (df[\"ChestPainType\"] == \"Non-Anginal Pain\")]\nstats.ttest_ind(Male_group, Female_group)","d7190e09":"df","56834040":"sns.catplot(data=df, x=\"HeartDisease\", y=\"Age\", kind=\"swarm\", hue=\"Sex\")","93f60cd2":"fig=plt.figure(figsize=(9,6))\nsns.boxplot(data= df, y=\"Age\", x=\"HeartDisease\", hue = \"Sex\")","9767dedf":"'''for var1 in [\"M\", \"F\"]:\n    for var2 in [0, 1]:\n\n        q1, q3= np.percentile(df[\"Age\"][(df[\"HeartDisease\"] == var2) & (df[\"Sex\"] == var1)], [25,75])\n        iqr = q3 - q1\n        lower_bound = q1 -(1.5 * iqr) \n        upper_bound = q3 +(1.5 * iqr)\n        print(\"Feature Gender ({}) and HeartDisease ({})\".format(var1, var2))\n        print(\"lower_bound\",lower_bound)\n        print(\"upper_bound\",upper_bound)\n        #df = df[(df[f]>=lower_bound) & (df[f]<=upper_bound)]\n        df = df[df[\"Age\"][(df[\"HeartDisease\"] == var2) & (df[\"Sex\"] == var1)]>=lower_bound]\n        df = df[df[\"Age\"][(df[\"HeartDisease\"] == var2) & (df[\"Sex\"] == var1)]<=upper_bound]\n        print(\"=-=-=-=-=-=-=-=-=-=-=-=-=\")'''","25b3e54c":"df","4d7b4811":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","f55c022c":"# DO FEATURE SELECTION","6bebb493":"#Using Pearson Correlation\n\nplt.figure(figsize=(12,10))\ncor = df.corr(method=\"spearman\").abs()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nprint(\"Spearman correlation matrix\")\nplt.show()\n","53dbcd5e":"for f in [\"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"ST_Slope\", \"Sex\"]:\n\n    one_hot = pd.get_dummies(df[f])\n    # Drop column B as it is now encoded\n    df = df.drop(f,axis = 1)\n    # Join the encoded df\n    df = df.join(one_hot)\n\n","e623cf40":"df","e6e9e275":"df[\"Age\"]=(df[\"Age\"]-df[\"Age\"].mean())\/df[\"Age\"].std()","03993934":"for f in [\"Age\", \"RestingBP\", \"Cholesterol\", \"MaxHR\", \"Oldpeak\"]:\n    df[f]=(df[f]-df[f].mean())\/df[f].std()","44ecbeb3":"from sklearn.model_selection import train_test_split","ad98a621":"X_train, X_test, y_train, y_test = train_test_split(df[ df.columns[df.columns !='HeartDisease'] ], df[\"HeartDisease\"], test_size=0.25)","5683906b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import svm, model_selection, metrics\nfrom sklearn.metrics import plot_confusion_matrix","f2956d90":"# setup all the parameters and models\nexps = {\n    'svm-lin': {\n        'paramgrid': {'C': np.logspace(-2,3,10)},\n        'clf': svm.SVC(kernel='linear') },\n    'svm-rbf': {\n        'paramgrid': {'C': np.logspace(-2,3,10), 'gamma': np.logspace(-4,3,10) },\n        'clf': svm.SVC(kernel='rbf') },\n    'svm-poly': {\n        'paramgrid': {'C': np.logspace(-2,3,10), 'degree': [2, 3, 4] },\n        'clf': svm.SVC(kernel='poly') },\n    }","92ba097d":"clf_1 = model_selection.GridSearchCV(estimator=exps['svm-lin']['clf'], param_grid=exps['svm-lin']['paramgrid'])\nclf_1.fit(X_train, y_train)\n\nclf_2 = model_selection.GridSearchCV(estimator=exps['svm-rbf']['clf'], param_grid=exps['svm-rbf']['paramgrid'])\nclf_2.fit(X_train, y_train)\n\nclf_3 = model_selection.GridSearchCV(estimator=exps['svm-poly']['clf'], param_grid=exps['svm-poly']['paramgrid'])\nclf_3.fit(X_train, y_train)\n\nmodel = {'svm-lin': clf_1, 'svm-rbf': clf_2, 'svm-poly': clf_3}","cf3c0b8f":"print('SVM-Lin Train')\nprint(metrics.accuracy_score(y_train, model['svm-lin'].predict(X_train)))\nprint('SVM-Lin Test')\nprint(metrics.accuracy_score(y_test, model['svm-lin'].predict(X_test)))\n\nprint('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n\nprint('SVM-rbf Train')\nprint(metrics.accuracy_score(y_train, model['svm-rbf'].predict(X_train)))\nprint('SVM-rbf Test')\nprint(metrics.accuracy_score(y_test, model['svm-rbf'].predict(X_test)))\n\nprint('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n\nprint('SVM-poly Train')\nprint(metrics.accuracy_score(y_train, model['svm-poly'].predict(X_train)))\nprint('SVM-poly Test')\nprint(metrics.accuracy_score(y_test, model['svm-poly'].predict(X_test)))","c523bb84":"from sklearn.metrics import confusion_matrix","c930fd9e":"cm = confusion_matrix(y_test, model['svm-rbf'].predict(X_test))\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16})","6f16c1c1":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","1a2d3da6":"array = []\n\nfor depth in range(1, 20):\n\n    rcf = RandomForestClassifier(max_depth=depth, random_state=0)\n    rcf.fit(X_train, y_train)\n    array.append(metrics.accuracy_score(y_test, rcf.predict(X_test)))\n    \nfig=plt.figure(figsize=(9,6))\nsns.lineplot(x=range(1, 20), y=array)","e7769078":"rcf = RandomForestClassifier(max_depth=array.index(max(array)), random_state=0)\nrcf.fit(X_train, y_train)\nprint(metrics.accuracy_score(y_test, rcf.predict(X_test)))","9b42931d":"cm = confusion_matrix(y_test, rcf.predict(X_test))\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16})","69c06973":"array = []\n\nfor n in range(1, 1000, 5):\n\n    gb_model = GradientBoostingClassifier(n_estimators=n, learning_rate=0.01, random_state=0)\n    gb_model.fit(X_train, y_train)\n    array.append(metrics.accuracy_score(y_test, gb_model.predict(X_test)))\n    \nfig=plt.figure(figsize=(9,6))\nsns.lineplot(x=range(1, 1000, 5), y=array)","a64325dc":"gb_model = GradientBoostingClassifier(n_estimators=1+array.index(max(array))*5, learning_rate=0.01, random_state=0)\ngb_model.fit(X_train, y_train)\n\nprint(metrics.accuracy_score(y_test, gb_model.predict(X_test)))","4146d170":"cm = confusion_matrix(y_test, gb_model.predict(X_test))\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16})","ba4f81e1":"array = []\n\nfor n in range(1, 1000, 5):\n\n    gb_model = AdaBoostClassifier(n_estimators=n, learning_rate=0.01, random_state=0)\n    gb_model.fit(X_train, y_train)\n    array.append(metrics.accuracy_score(y_test, gb_model.predict(X_test)))\n    \nfig=plt.figure(figsize=(9,6))\nsns.lineplot(x=range(1, 1000, 5), y=array)","11162969":"ab_model = AdaBoostClassifier(n_estimators=1+array.index(max(array))*5, learning_rate=0.01, random_state=0)\nab_model.fit(X_train, y_train)\n\nprint(metrics.accuracy_score(y_test, gb_model.predict(X_test)))","f1e437d7":"cm = confusion_matrix(y_test, ab_model.predict(X_test))\ndf_cm = pd.DataFrame(cm, columns=np.unique(y_test), index = np.unique(y_test))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nsns.heatmap(df_cm, cmap=\"Greens\", annot=True,annot_kws={\"size\": 16})","1e324482":"- Version 1 - Uploaded\n- Version 2 - Fixed the incoherency in the explanations, grammar mistakes and made a clearer division between the stages of the mini project.","207ce80c":"The dataset is unbalanced - more males than females.","39891605":"Unnormalized comparison between male and female","e30c73c3":"# Prediction Modelling","f7e497b0":"Need to remove the anomalies *To be completed*","94c8f457":"From this comparison, we can see that:\n- Men are more likely to experience Asymptomatic chest pain. \n- Women are significantly more likely to have Atypical Angina relative to men. \n- It is also noticable is that women seem to be more likely to have non-anginal pain but p-value and power of that observation should be found. \n- Typical Angina seem to have no significant difference between genders, however we do not have enough observations to make a definitive conclusion.","ae8eece9":"From this simple SVM model, we were able to make a good prediction of people with heart disease with only ~5% of people being a False Negative.","88424c5e":"We also have to normalize the continuous data to prevent high computational costs and also make the difference between each value less - so that the model doesn't choose higher values to be more important than the other.","979f2f4e":"- H0: There is no statistical significant difference between male and female in experiencing Atypical Angina. \n- Ha: There is a statistical significant difference between male and female in experiencing Atypical Angina.","b06b44f0":"We have categorical data which should be converted into numerical form to make it easeir for the model to interpert. However, we cannot keep categorical in the integer format in the same coolumnas the model might potentially interpret one category more important than the other as category #1 might be assigned value of 1 and category #2 might be assigned value 2; 2 > 1 which doesn't make sense as its categorical data. We can convert this column into many separate columns to either contain particular cateogry or not. This can be done with onehot-encoders.","f50283c8":"# Feature selection and data wrangling","5ddbbd95":"# Feature engineering","7380d978":"# Bivariate Visualization","a629390c":"Both Sex classes seem to be normally distributed.","43b7d664":"Resting BP average, lower and upper quartiles are near identical","204caa33":"Remove outliers - outliers will make the distribution vary more and therefore lower the statistical power.","865dff01":"**Context**\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n**Attribute Information**\n\n    Age: age of the patient [years]\n    Sex: sex of the patient [M: Male, F: Female]\n    ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n    RestingBP: resting blood pressure [mm Hg]\n    Cholesterol: serum cholesterol [mm\/dl]\n    FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n    RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n    MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n    ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n    Oldpeak: oldpeak = ST [Numeric value measured in depression]\n    ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n    HeartDisease: output class [1: heart disease, 0: Normal]\n\n**Source**\n\nThis dataset was created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes. The five datasets used for its curation are:\n\n    Cleveland: 303 observations\n    Hungarian: 294 observations\n    Switzerland: 123 observations\n    Long Beach VA: 200 observations\n    Stalog (Heart) Data Set: 270 observations\n\nTotal: 1190 observations\nDuplicated: 272 observations\n\nFinal dataset: 918 observations\n\nEvery dataset used can be found under the Index of heart disease datasets from UCI Machine Learning Repository on the following link: https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/heart-disease\/\n\n**Citation**\n\nfedesoriano. (September 2021). Heart Failure Prediction Dataset. Retrieved [Date Retrieved] from https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction.\n\n**Acknowledgements**\n\nCreators:\n\n    Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.\n    University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.\n    University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.\n    V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.\n\nDonor:\nDavid W. Aha (aha '@' ics.uci.edu) (714) 856-8779","5e96b2bd":"Spearman correlation was used as there is potentially non-linear correlation between different variables. As we can see none of the features are strongly collinear.","4fb61c7b":"We can be 90% confident that there is a significant difference between the two populations when it comes to atypical angina.","df54b272":"# Learn general Information about the dataset \n\n- We most likely had to do extra preprocessing so that the model is more reliable. Furthermore it will give us more insight about what feature are important for the correct prediction.","49760d12":"We can be 95% (nearly almost 99%) confident that there is a significant difference between the two populations when it comes to non-anginal pain.\n","b2d25d45":"Average age of a male set in this dataset is higher than of a female set","58e51f3c":"Normalized comparison between male and female","b8fedb1f":"We can observe that both genders follow same distribution for Atypical angina and currently it does not "}}