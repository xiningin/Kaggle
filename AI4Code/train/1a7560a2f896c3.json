{"cell_type":{"ce066ba8":"code","58085f7e":"code","4bb2cb92":"code","a0a67ba3":"code","d49c5c99":"code","3bf16a65":"code","15c02a33":"code","941a6cb9":"code","fad7f38c":"code","93848020":"code","206b2a42":"code","429dee52":"code","6b314280":"code","4da5e62e":"code","730e3071":"code","354bfb21":"code","c4d9448c":"code","f423734b":"code","2bd92a24":"code","6185f2c3":"code","647e92d6":"code","98bb2524":"code","5fba04d1":"code","98678867":"code","c30951b8":"code","dfb3741c":"markdown","d016bbac":"markdown","c12c7cc5":"markdown","3b8abc45":"markdown"},"source":{"ce066ba8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model,tree,ensemble\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score","58085f7e":"df_train = pd.read_csv('..\/input\/socc-ai-competition-1\/train.csv')\ndf_test = pd.read_csv('..\/input\/socc-ai-competition-1\/test.csv')","4bb2cb92":"X_train=df_train.drop(['shares','url'],1)\ny_train=df_train['shares']\nX_test=df_test.drop(['url'],1)","a0a67ba3":"kf =KFold(n_splits=5, shuffle=True, random_state=42)\n\ncnt = 1\n# split()  method generate indices to split data into training and test set.\nfor train_index, test_index in kf.split(X_train, y_train):\n    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n    cnt += 1","d49c5c99":"def rmse(score):\n    rmse = np.sqrt(-score)\n    print(f'rmse= {\"{:.2f}\".format(rmse)}')","3bf16a65":"score = cross_val_score(linear_model.LinearRegression(), X_train, y_train, cv= kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold: {score}')\nrmse(score.mean())","15c02a33":"score = cross_val_score(tree.DecisionTreeRegressor(random_state= 42), X_train, y_train, cv=kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold: {score}')\nrmse(score.mean())","941a6cb9":"score = cross_val_score(ensemble.RandomForestRegressor(random_state= 42), X_train, y_train, cv= kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold are: {score}')\nrmse(score.mean())","fad7f38c":"score = cross_val_score(ensemble.GradientBoostingRegressor(random_state= 42), X_train, y_train, cv= kf, scoring=\"neg_mean_squared_error\")\nprint(f'Scores for each fold are: {score}')\nrmse(score.mean())","93848020":"!pip install pytorch-tabnet","206b2a42":"'''y = df_train['shares']\nX = df_train.drop(columns=['url','shares'],axis=1)\n\nimport numpy as np\nX = np.array(X)\ny = np.array(y).reshape(-1,1)'''\n\n\n'''from pytorch_tabnet.tab_model import TabNetRegressor\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\npredictions_array =[]\nCV_score_array    =[]\nfor train_index, test_index in kf.split(X):\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    regressor = TabNetRegressor(verbose=0,seed=42)\n    regressor.fit(X_train=X_train, y_train=y_train,\n              eval_set=[(X_valid, y_valid)],\n              patience=300, max_epochs=30,\n              eval_metric=['rmse'])\n    CV_score_array.append(regressor.best_cost)\n    predictions_array.append(np.expm1(regressor.predict(X_test)))\n\npredictions = np.mean(predictions_array,axis=0)'''","429dee52":"#U can aslo run the above but the below one is the simple way\n\nX_train=df_train.drop(['shares','url'],1)\ny_train=df_train['shares']\n\nimport numpy as np\nX_train = np.array(X_train)\ny_train = np.array(y_train).reshape(-1,1)\n\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nrg = TabNetRegressor()  \nrg.fit(\n  X_train, y_train,max_epochs=200\n)","6b314280":"#pred = rg.predict(X_test)\n\n#use this for prediction ","4da5e62e":"import pandas as pd\ndf_train = pd.read_csv('..\/input\/socc-ai-competition-1\/train.csv')\ndf_test = pd.read_csv('..\/input\/socc-ai-competition-1\/test.csv')","730e3071":"X_train=df_train.drop(['shares','url'],1)\ny_train=df_train['shares']\nX_test=df_test.drop(['url'],1)","354bfb21":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(max_depth=10, max_features=5, n_estimators=25, n_jobs=-1)\nrf.fit(X_train, y_train)\ny_pred= rf.predict(X_test)","c4d9448c":"from sklearn.model_selection import GridSearchCV\nforest = ensemble.RandomForestRegressor(n_jobs=-1)\nparam_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\ngrid_search_forest = GridSearchCV(forest, param_grid, cv=10, scoring='neg_mean_squared_error')\ngrid_search_forest.fit(X_train, y_train)","f423734b":"cvres = grid_search_forest.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","2bd92a24":"#find the best model of grid search\ngrid_search_forest.best_estimator_","6185f2c3":"grid_best= grid_search_forest.best_estimator_.predict(X_train)\nerrors = abs(grid_best - y_train)\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ y_train))\n# Calculate and display accuracy\naccuracy = 100 - mape    \n#print result\nprint('The best model from grid-search has an accuracy of', round(accuracy, 2),'%')","647e92d6":"from sklearn.metrics import mean_squared_error\ngrid_mse = mean_squared_error(y_train, grid_best)\ngrid_rmse = np.sqrt(grid_mse)\nprint('The best model from the grid search has a RMSE of', round(grid_rmse, 2))","98bb2524":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten","5fba04d1":"model = Sequential()\nmodel.add(Dense(32, input_dim=X_train.shape[1], kernel_initializer='glorot_uniform', activation='relu'))\nmodel.add(Dense(16, kernel_initializer='glorot_uniform', activation='relu'))  \nmodel.add(Dense(8, kernel_initializer='glorot_uniform', activation='relu'))  \nmodel.add(Dense(1, kernel_initializer='glorot_uniform'))\nmodel.compile(loss='mse', optimizer='adam', metrics=['mse'])","98678867":"history = model.fit(X_train, y_train, epochs=30, verbose=0)\ny_pred= model.predict(X_test)","c30951b8":"from sklearn.neural_network import MLPRegressor\nregr = MLPRegressor(random_state=1).fit(X_train, y_train)\ny_pred = regr.predict(X_test)","dfb3741c":"# Hyperparameter tuning","d016bbac":"# Nueral models","c12c7cc5":"# K_FOLD Cross validation\n\n**Using normal regression models**\n1. linear regression\n2. RandomForestRegressor\n3. DecisionTreeRegressor\n4. GradientBoostingRegressor\n5. KNeighborsRegressor\n6. XGB regressor\n\nsome other regression algorithms like\n**Ridge,Lasso,BayesianRidge,TweedieRegressor,KernelRidge**","3b8abc45":"# Pytorch tabnet regressor"}}