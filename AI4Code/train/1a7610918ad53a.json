{"cell_type":{"0d4f19c2":"code","76fbf4ed":"code","f7063b80":"code","708565e0":"code","2f5afe66":"code","903e0542":"code","c3260cba":"code","001aaa9f":"code","760a5fe9":"code","186b6263":"code","1bfdcbf2":"code","18fd571b":"code","857da3b7":"code","b5672277":"code","3f2911ac":"code","f0bd9d27":"markdown","f484b78c":"markdown","67106d76":"markdown","405f1da5":"markdown","f22b16e5":"markdown","c1d486e0":"markdown"},"source":{"0d4f19c2":"import numpy as np # linear algebra\nimport os, sys\n\nfrom gensim.models import Word2Vec, KeyedVectors\nfrom gensim.test.utils import datapath\n\nfrom torchtext import vocab, data\nimport torchtext","76fbf4ed":"from tqdm import tqdm_notebook\ntorchtext.vocab.tqdm = tqdm_notebook # Replace tqdm to tqdm_notebook in module torchtext","f7063b80":"# define root path\nPATH = '..\/input'\n\n# Get path to file\nvertor_size = 512\nglove_file = os.path.join(PATH, f'glovereddit120b\/GloVe.Reddit.120B.{vertor_size}D.txt')\n\n# Load with gensim\nmodel = KeyedVectors.load_word2vec_format(glove_file)","708565e0":"# Typical test\nsimilarities = model.most_similar(positive=['woman', 'king'], negative=['man'])\nprint(similarities[0])\n\nsimilarities = model.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\nprint(similarities[0])","2f5afe66":"# Election influence on vectors \nsimilarities = model.most_similar('Trump')\nfor i in similarities:\n    print(i)","903e0542":"# Check some trends:\nsimilarities = model.most_similar('SJWs')\nfor i in similarities:\n    print(i)","c3260cba":"def print_similarities(word, n=5):\n    print(f'Similarities for word: {word}')\n    similarities = model.most_similar(word)\n    for i in similarities[:n]:\n        print(f'\\t{i}')\n    \nprint_similarities('fuck')\nprint_similarities('sh*t')\n","001aaa9f":"analogy_scores = model.evaluate_word_analogies(datapath('questions-words.txt'))\nprint(f'Analogy score: {analogy_scores[0]:.4f}')","760a5fe9":"categories = analogy_scores[1:][0][:-1]\nfor category in categories:\n    print(f\"In {category['section']}, correct: {len(category['correct'])}, incorrect: {len(category['incorrect'])}\")","186b6263":"# Load with torchtext:\nvec = vocab.Vectors(glove_file, cache='.\/')\n\n# clean tmp file\n!rm *.pt","1bfdcbf2":"print(f'Words in embedding: {vec.vectors.size(0)}, dim_size: {vec.vectors.size(1)}')","18fd571b":"## Add path to file\nutils_path = '..\/input'\nsys.path.insert(0, utils_path)","857da3b7":"from clean_text import RegExCleaner","b5672277":"cleaner = RegExCleaner.reddits()","3f2911ac":"# separate 's from words\ntext = \"This's world\"\nprint(f'Before: {text}, After: {cleaner(text)}')\n\n# shrink repeated letters to 2\ntext = \"This's wwwwwwwwwworld\"\nprint(f'Before: {text}, After: {cleaner(text)}')","f0bd9d27":"### english obscene words","f484b78c":"## Extra: Cleaner text, whitch used for create corpus","67106d76":"## Check some vectors","405f1da5":"### Get analogy scores","f22b16e5":"## Load libs","c1d486e0":"## Introduction\nDataset contained Global Vectors for Word Representation based on Reddit comments.\n<br> All files converted to word2vec format\n<br> Vector size of embeddings: 64, 300, 512, 768 (zipped)"}}