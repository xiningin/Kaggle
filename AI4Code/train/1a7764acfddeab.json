{"cell_type":{"975ffc43":"code","6b72d452":"code","23d8c3c8":"code","4026ebee":"code","260cf389":"code","5d77ac02":"code","4f6bd167":"code","1b61cf22":"code","7d5466d5":"code","95322123":"code","84ba36f8":"code","20c46d4f":"code","b8e50666":"code","d1390981":"code","e33bc98f":"code","eed265ae":"code","77668e9a":"code","269fa357":"code","f64253d5":"code","d9b33815":"code","2cfa9539":"code","06e225d9":"code","03ca7cde":"code","d1ba7e9c":"code","5ef54547":"code","ee1458af":"code","af31f07b":"code","50177f05":"code","98d0a757":"markdown","2827e841":"markdown","2abcd391":"markdown","88ad4073":"markdown","59d12cc7":"markdown","6eb87536":"markdown","e7bb319e":"markdown","19918d7d":"markdown","e41af8f0":"markdown","4306ed7b":"markdown","ec10b174":"markdown","f679d6be":"markdown","82d2f6a8":"markdown","8d3dd5b6":"markdown","7e363aba":"markdown","087b6649":"markdown","1917c477":"markdown","147d5fa1":"markdown","41134acc":"markdown"},"source":{"975ffc43":"import pandas as pd\nfrom transformers import TFBertModel\nseed_value = 42\nimport os\nos.environ['PYTHONHASHSEED'] = str(seed_value)\nimport random\nrandom.seed(seed_value)\nimport numpy as np\nnp.random.seed(seed_value)\nnp.set_printoptions(precision=2)\nimport tensorflow as tf\ntf.random.set_seed(seed_value)\n#import tensorflow_addons as tfa\nimport tensorflow.keras\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import ModelCheckpoint","6b72d452":"print(tf.test.gpu_device_name())\n# See https:\/\/www.tensorflow.org\/tutorials\/using_gpu#allowing_gpu_memory_growth\n","23d8c3c8":"from transformers import AutoTokenizer,TFAutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\nbert = TFAutoModel.from_pretrained(\"bert-base-cased\")","4026ebee":"# Import the datasets\ntrain_raw= pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_raw = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","260cf389":"train_raw.head()","5d77ac02":"# Perform EDA on the train\ntrain_raw.shape ,train_raw.dtypes","4f6bd167":"from transformers import AutoTokenizer\n\nSEQ_LEN = 128  # we will cut\/pad our sequences to a length of 128 tokens\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ndef tokenize(sentence):\n    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n    return tokens['input_ids'], tokens['attention_mask']\n\n# initialize two arrays for input tensors\nXids = np.zeros((len(train_raw), SEQ_LEN))\nXmask = np.zeros((len(train_raw), SEQ_LEN))\n\nfor i, sentence in enumerate(train_raw['excerpt']):\n    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n    if i % 10000 == 0:\n        print(i)  # do this so we can see some progress","1b61cf22":"Xids","7d5466d5":"Xmask","95322123":"labels = train_raw['target'].values  # take sentiment column in df as array","84ba36f8":"labels","20c46d4f":"import tensorflow as tf\n\nBATCH_SIZE = 32  # we will use batches of 32\n\n# load arrays into tensorflow dataset\ndataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n\n# create a mapping function that we use to restructure our dataset\ndef map_func(input_ids, masks, labels):\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n\n# using map method to apply map_func to dataset\ndataset = dataset.map(map_func)\n\n# shuffle data and batch it\ndataset = dataset.shuffle(100000).batch(BATCH_SIZE)","b8e50666":"type(dataset)","d1390981":"# get the length of the batched dataset\nDS_LEN = len([0 for batch in dataset])\nSPLIT = 0.9  # 90-10 split\n\ntrain = dataset.take(round(DS_LEN*SPLIT))  # get first 90% of batches\nval = dataset.skip(round(DS_LEN*SPLIT))  # skip first 90% and keep final 10%\n\ndel dataset  # optionally, delete dataset to free up disk-space","e33bc98f":"from transformers import AutoModel\n\n# initialize cased BERT model\nbert = TFAutoModel.from_pretrained('bert-base-cased')\n\ninput_ids = tf.keras.layers.Input(shape=(128,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(128,), name='attention_mask', dtype='int32')\n\n# we consume the last_hidden_state tensor from bert (discarding pooled_outputs)\nembeddings = bert(input_ids, attention_mask=mask)[0]\n\nX = tf.keras.layers.LSTM(64)(embeddings)\nX = tf.keras.layers.BatchNormalization()(X)\nX = tf.keras.layers.Dense(64, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.1)(X)\ny = tf.keras.layers.Dense(1, name='outputs')(X)\n\n# define input and output layers of our model\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n\n# freeze the BERT layer - otherwise we will be training 100M+ parameters...\nmodel.layers[2].trainable = False","eed265ae":"model.summary()","77668e9a":"tf.keras.utils.plot_model(model=model, show_shapes=True, dpi=76, )","269fa357":"from tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\noptimizer = tf.keras.optimizers.Adam(0.01)\n#loss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot\nrmse = RootMeanSquaredError()\nbest_weights_file = \".\/weights.h5\"\nbatch_size = 16\nmax_epochs= 1000\nm_ckpt = ModelCheckpoint(best_weights_file, monitor='val_auc', mode='max', verbose=2,\n                             save_weights_only=True, save_best_only=True)\nes = EarlyStopping(monitor='loss',min_delta=0.0000000000000000001, patience=10)\n\nmodel.compile(optimizer=optimizer, loss='mse', metrics=[rmse])","f64253d5":"# fit model using our gpu\nwith tf.device('\/gpu:0'):\n    history = model.fit(\n            train,\n            validation_data=val,\n            epochs=max_epochs,\n            batch_size=batch_size,\n            callbacks=[m_ckpt,es],\n            verbose=2\n        )","d9b33815":"loss, root_mean_squared_error = model.evaluate( val, verbose=0)\nprint('root_mean_squared_error_model: %f' % (accuracy *100))\nprint('loss_model: %f' % (loss *100))","2cfa9539":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\ndef plot_history(history):\n    acc = history.history['root_mean_squared_error']\n    val_acc = history.history['val_root_mean_squared_error']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training root_mean_squared_error')\n    plt.plot(x, val_acc, 'r', label='Validation root_mean_squared_error')\n    plt.title('Training and validation root_mean_squared_error')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\nplot_history(history)","06e225d9":"#model = load_model()\n#tokenizer= DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\npredictions = model.predict(val)","03ca7cde":"test_raw = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_raw.head()","d1ba7e9c":"SEQ_LEN = 128 # we will cut\/pad our sequences to a length of 128 tokens\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n\ndef tokenize(sentence):\n    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n    return tokens['input_ids'], tokens['attention_mask']\n\n# initialize two arrays for input tensors\nXids = np.zeros((len(test_raw), SEQ_LEN))\nXmask = np.zeros((len(test_raw), SEQ_LEN))\n\nfor i, sentence in enumerate(test_raw['excerpt']):\n    Xids[i, :], Xmask[i, :] = tokenize(sentence)\n    if i % 10000 == 0:\n        print(i)  # do this so we can see some progress\nBATCH_SIZE = 1  # we will use batches of 1\n\n# load arrays into tensorflow dataset\ntest = tf.data.Dataset.from_tensor_slices((Xids, Xmask, [0]*len(test_raw)))\n\n# create a mapping function that we use to restructure our dataset\ndef map_func(input_ids, masks, labels):\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n\n# using map method to apply map_func to dataset\nUnseen_test_prep = test.map(map_func)\n\n# shuffle data and batch it\nUnseen_test_prep = Unseen_test_prep.shuffle(100000).batch(BATCH_SIZE)   \npreds = model.predict(Unseen_test_prep)     ","5ef54547":"preds","ee1458af":"test_raw['id'].values","af31f07b":"my_submission = pd.DataFrame({'id': test_raw.id, 'target': preds.ravel()})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)\n\n","50177f05":"my_submission","98d0a757":"Entry IDs are a list of integers uniquely related to a specific word.\n\nThe attention mask is a list of 1s and 0s that match the IDs in the entry ID array - BERT reads this and only applies attention to IDs that match a mask value of attention of 1. This allows us to avoid drawing attention to the filler tokens.\n\nOur encode_plus arguments are:\n\n    Our award. It is simply a string representing a text.\n    The max_length of our encoded outputs. We use a value of 32 which means that each output tensor has a length of 32.\n    We cut the sequences that are more than 32 tokens in length with truncation = True.\n    For sequences shorter than 32 tokens, we pad them with zeros up to a length of 32 using padding = 'max_length'.\n    BERT uses several special tokens, to mark the start \/ end of sequences, for padding, unknown words and mask words. We add those using add_special_tokens = True.\n    BERT also takes two inputs, the input_idset attention_mask. We extract the attention mask with return_attention_mask = True.\n    By default the tokenizer will return a token type ID tensor - which we don't need, so we use return_token_type_ids = False.\n    Finally, we use TensorFlow, so we return the TensorFlow tensors using return_tensors = 'tf'. If you are using PyTorch, use return_tensors = 'pt'.\n\n# Lables preparation\n\n","2827e841":"Here we first import the transformers library and initialize a tokenizer for the bert-base-casedmodel used. A list of models can be found here. We then define a tokenize function which manages the tokenization.\n\nWe use the encode_plus method of our BERT tokenizer to convert a sentence into input_ids and attention_masktensors.","2abcd391":"\n# Prepare Submission File\n\nWe make submissions in CSV files. Your submissions usually have two columns: an ID column and a prediction column. The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the housing data is the string 'Id'). The prediction column will use the name of the target field.\n\nWe will create a DataFrame with this data, and then use the dataframe's to_csv method to write our submission file. Explicitly include the argument index=False to prevent pandas from adding another column in our csv file.\n","88ad4073":"This whole process can take some time. I like to save the encoded tables so that we can recover them in case of problems or for future tests.","59d12cc7":"# Preparation and Feature Extraction ","6eb87536":"# Definition of the model\n\nOur data is now ready and we can define our model architecture. We'll use BERT, followed by an LSTM layer and a few simple NN layers. These last layers after BERT are our classifier.\n\nOur classifier uses the hidden state tensors output from BERT - using them to predict our target.\n![image.png](attachment:image.png)","e7bb319e":"with open('xids.npy', 'wb') as f:\n    np.save(f, Xids)\nwith open('xmask.npy', 'wb') as f:\n    np.save(f, Xmask)\nwith open('labels.npy', 'wb') as f:\n    np.save(f, labels)","19918d7d":"# Evaluate :","e41af8f0":"# Train validation separation\n\nThe last step before training our model is to divide our dataset into training, validation, and (optionally) testing sets. We will stick to a simple 90\u201310 train-validation separation here.","4306ed7b":"Now that we have all the arrays encoded, we load them into a TensorFlow dataset object. Using the dataset, we easily restructure, mix and group the data.","ec10b174":"# Simple EDA ","f679d6be":"# Predict Unseen Data :","82d2f6a8":"When inserting textual data into our model, there are a few things to consider. First, we need to use tokenizer.encode_plus (...) to convert our text to input ID and attention mask tensors (we'll talk about that later).\n\nBERT expects these two tensors as inputs. One mapped to \"input_ids\" and another to \"attention_mask\".\n\nAt the other end of the spectrum, BERT generates two default tensors (more are available). These are \"last_hidden_state\" and \"pooler_output\".\n\nThe output of the pooler is simply the last hidden state, processed slightly further by a linear layer and a Tanh activation function - this also reduces its dimensionality from 3D (last hidden state) to 2D (output of pooler).\n\nLater we will consume the last hidden state tensor and remove the output from the pooler.\n\n# Classification\n## Data\n\nCommonLit Readability Prize . We can download and extract it programmatically, like this:","8d3dd5b6":"# Train Data\n## feature Extraction X :","7e363aba":"\n# Create a natural language classifier with Bert and Tensorflow\n\nHigh performance transformer models such as BERT and GPT-3 transform a wide range of previously menial and language-based tasks into a few clicks job, saving a lot of time.\n\nIn most industries, the latest wave of language optimization is just beginning - taking its first steps. But these plants are widespread and grow quickly.\n\nMuch of this adoption is due to the incredibly low barrier to entry. If you know the basics of TensorFlow or PyTorch and take some time to familiarize yourself with the Transformers library, you're already halfway there.\n\nWith the Transformers library, it only takes three lines of code to initialize a cutting-edge ML model - a model built from billions of research dollars spent by Google, Facebook, and OpenAI.\n\nThis article will walk you through the steps to create a classification model that harnesses the power of transformers, using Google's BERT. \n- Finding Models\n- Initializing\n- Bert Inputs and Outputs\nClassification\n- The Data\n- Tokenization\n- Data Prep\n- Train-Validation Split\n- Model Definition\n- Train\nResults\n# Transformateurs\n## Find models\n\nWe will be using BERT, possibly the most well-known transformer architecture.\n\nTo understand what we need to use BERT, we head to the HuggingFace template page (HuggingFace built the Transformer framework).\n\nOnce there, we will find both bert-base-cased and bert-base-uncased on the first page. cased means that the pattern distinguishes between uppercase and lowercase, whereas this uncase does not.\n## Initialization\n\nIf we click on the model we find more specific details. On this page we can see the model initialization code. Because we are using TensorFlow, our code will use TFAutoTokenizer and TFAutoModel instead of AutoTokenizer and AutoModel, respectively:\n","087b6649":"def plot_horsepower(x, y):\n  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n  plt.plot(x, y, color='k', label='Predictions')\n  plt.xlabel('Horsepower')\n  plt.ylabel('MPG')\n  plt.legend()\n","1917c477":"# Tokenization\nWe have our text data in the textcolumn, which we now need to tokenize. We will use the BERT tokenizer, because we will use a BERT transformer later.","147d5fa1":"# Plot prediction ","41134acc":"# Coaching\n\nWe can now train our model. First, we configure our optimizer (Adam), our loss function, and our precision metric. Then we compile the model and practice!"}}