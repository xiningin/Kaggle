{"cell_type":{"01259ca0":"code","d4074cc6":"code","f9f9d2a0":"code","b82911fe":"code","06337de9":"code","a56f5843":"code","52cf1381":"code","af56104b":"code","19c086f5":"code","be687bc8":"code","e0b30fad":"code","12b61756":"code","22773014":"code","d6b3e284":"code","0c5562d7":"code","f9bc36fe":"code","43c4ccdf":"code","e406518e":"code","307c4b4e":"code","57ad2083":"code","ed4a614a":"code","56d5812c":"code","2db509dd":"code","ba557c41":"code","e7fe3c52":"markdown","bc3edda3":"markdown","be7da8ad":"markdown","c5515604":"markdown","a65713ab":"markdown","89b8ffa0":"markdown","2e654a7c":"markdown","4b245241":"markdown","4a6d1d83":"markdown","331118bc":"markdown","ea75332a":"markdown","bd23cc5c":"markdown","a47155c9":"markdown","fbd74fb4":"markdown","8103f2bc":"markdown","c88985c2":"markdown","ebb93af0":"markdown","f32d7af4":"markdown","32979319":"markdown","d60beed2":"markdown","b7cb3d3d":"markdown","9e2ef5a1":"markdown","8741570a":"markdown","f0ea3f34":"markdown","46f577be":"markdown","2201ac45":"markdown","6d6733a6":"markdown","1eea84cf":"markdown"},"source":{"01259ca0":"import pandas as pd\nX_full = pd.read_csv('..\/input\/housepricingmlpipeline\/train.csv', index_col='Id')\nX_test_full = pd.read_csv('..\/input\/housepricingmlpipeline\/test.csv', index_col='Id')","d4074cc6":"X_full.head()","f9f9d2a0":"X_full.shape","b82911fe":"X_test_full.head()","06337de9":"X_test_full.shape","a56f5843":"X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X_full.SalePrice\nX_full.drop(['SalePrice'], axis=1, inplace=True)","52cf1381":"y.head()","af56104b":"y.shape","19c086f5":"X_full.head()","be687bc8":"X_full.shape","e0b30fad":"from sklearn.model_selection import train_test_split\n \nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","12b61756":"s = (X_full.dtypes == 'object')\ncurrent_cat = list(s[s].index)\nlen(current_cat)","22773014":"categorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype == \"object\"]","d6b3e284":"len(categorical_cols)","0c5562d7":"# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]","f9bc36fe":"len(numerical_cols)","43c4ccdf":"# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","e406518e":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder","307c4b4e":"numerical_transformer = SimpleImputer(strategy='constant')","57ad2083":"categorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","ed4a614a":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","56d5812c":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","2db509dd":"clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])","ba557c41":"clf.fit(X_train, y_train)","e7fe3c52":"As we can see, there was no NaN in the column `SalePrice`","bc3edda3":"![purple-divider](https:\/\/user-images.githubusercontent.com\/7065401\/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)\n\n## PIPELINE SET UP.","be7da8ad":"Now, let's pick only the ones with low Cardinality (number of unique values in a column), picking the value of 10, which is convenient but arbitrary.","c5515604":"### B.4 - Bundle preprocessing for numerical and categorical data:","a65713ab":"![green-divider](https:\/\/user-images.githubusercontent.com\/7065401\/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n\n### D- Create and Evaluate the Pipeline:\n\nFinally, we use the Pipeline class to define a pipeline that bundles the preprocessing and modeling steps. There are a few important things to notice:\n\n\n* With the pipeline, we preprocess the training data and fit the model in a single line of code. (In contrast, without a pipeline, we have to do imputation, one-hot encoding, and model training in separate steps. This becomes especially messy if we have to deal with both numerical and categorical variables!)\n\n\n* With the pipeline, we supply the unprocessed features in **`X_valid`** to the predict() command, and the pipeline automatically preprocesses the features before generating predictions. (However, without a pipeline, we have to remember to preprocess the validation data before making predictions.)","89b8ffa0":"### D.1 - Bundle preprocessing and modeling code in a pipeline:","2e654a7c":"### A.5. SELECTION OF CATEGORICAL AND NUMERICAL COLUMNS:","4b245241":"### A.4. Break off validation set from training data:","4a6d1d83":"### A.2.  Taking a glance at the data:","331118bc":"### D.3 - Preprocessing of validation data, get predictions:","ea75332a":"### B.1 - Loading libraries: ","bd23cc5c":"### D.2 - Preprocessing of training data, fit model:","a47155c9":"As we can see, the column `SalePrice` was already remove from **X_full**","fbd74fb4":"Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step. Pipelines have some important benefits, those include:\n\n* **Cleaner Code**: Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.\n* **Fewer Bugs**: There are fewer opportunities to misapply a step or forget a preprocessing step.\n* **Easier to Productionize**: It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.\n* **More Options for Model Validation**: You will see an example in the next tutorial, which covers cross-validation.","8103f2bc":"![green-divider](https:\/\/user-images.githubusercontent.com\/7065401\/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n\n### B- PREPROCESSING:","c88985c2":"Similar to how a pipeline bundles together preprocessing and modeling steps, we use the **`ColumnTransformer`** class to bundle together different preprocessing steps, such as: \n* imputes missing values in numerical data, and\n* imputes missing values and applies a one-hot encoding to categorical data.","ebb93af0":"![green-divider](https:\/\/user-images.githubusercontent.com\/7065401\/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n\n### C- MODEL:\n\nWe define a `random forest` model with the familiar **`RandomForestRegressor`** class","f32d7af4":"### A.6. FINAL SET OF COLUMNS:","32979319":"### B.2 - Preprocessing for numerical data:","d60beed2":"We can see that **X_full** has one column more than **X_test_full**: **`SalePrice`**. we have to remove from **X_full** this column and setting it as **y**. \n\nAlso, we can see a lot of NAN data. We are going to delete any row containing a NaN in the target (`SalePrice`)","b7cb3d3d":"### A.1. Reading the data:","9e2ef5a1":"Doing the math wit the number of columns (36+40+3), we get 79, which is the original number of columns of the set. Hence the process was correct.","8741570a":"**_We construct the full pipeline in four steps:_**","f0ea3f34":"![green-divider](https:\/\/user-images.githubusercontent.com\/7065401\/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)\n\n### A- DATA PREPARATION:","46f577be":"### A.3. Removing rows with NaN in the target columns and separating target from predictors:","2201ac45":"![rmotr](https:\/\/user-images.githubusercontent.com\/7065401\/52071918-bda15380-2562-11e9-828c-7f95297e4a82.png)\n<hr style=\"margin-bottom: 40px;\">\n\n<img src=\"https:\/\/www.xenonstack.com\/hubfs\/Imported_Blog_Media\/xenonstack-machine-learning-pipeline-2-1-1.png\"\n    style=\"width:400px; float: right; margin: 0 40px 40px 40px;\"><\/img>\n\n# Housing Prices - ML PIPELINES\n\nIn this project, we will build a ML PIPELINE for predicting Sale Price of of residential homes in Ames, Iowa. The model will be evaluated with the Mean Absolute Error (MAE).\n\n\nThe dataset was taken from Kaggle's Housing Prices Competition (Visit [Kaggle](https:\/\/www.kaggle.com)). It has 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. The objective is to predict the final price of each home.\n","6d6733a6":"### B.3 - Preprocessing for categorical data:","1eea84cf":"Let's first calculated the amount of current categorical columns:"}}