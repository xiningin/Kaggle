{"cell_type":{"5ae86c26":"code","d65f6c72":"code","3f5c83bd":"code","22352ece":"code","fef90f5e":"code","c64778dd":"code","9572fd88":"code","ccf4e213":"code","3eaa2107":"code","0c7d00af":"code","b2853912":"code","7f6c6766":"code","03ac34a4":"code","a17244d5":"code","61782a70":"code","4bab48f9":"code","4f0d0188":"code","66825ba5":"code","541c0b58":"code","b2ad9f87":"code","eb170bee":"code","5e661ed9":"code","57bb2351":"code","ad300472":"code","afeba6b6":"code","7ff6ab43":"code","90f51237":"code","be7b9457":"code","2ac6ed97":"code","fd8491cd":"code","67fe3d74":"code","4bbd208e":"code","4bd705e8":"code","ae914a80":"code","423c6744":"code","8caf85c6":"code","06d5e2a1":"code","fda08ee4":"code","983b4f49":"code","a1fa9368":"code","5b0cdf90":"code","7e550569":"code","41f4ad0c":"code","162a5d10":"code","2a25731a":"code","50729c23":"code","e70901ef":"code","af7a24a9":"code","ef6fa67d":"code","711752f7":"code","4f90245d":"code","d35dd9e1":"markdown","e73263dd":"markdown","aadf0d51":"markdown","8a63c78d":"markdown","16a8747e":"markdown","982dfe49":"markdown","a93964f4":"markdown","8a633172":"markdown","02e24586":"markdown","505215a0":"markdown","10bea5d4":"markdown","6585b7d7":"markdown","a7d6254d":"markdown","acd81e03":"markdown","76590cac":"markdown","6362810b":"markdown","d0f71b5e":"markdown","d5f9a5f2":"markdown","d0c84e69":"markdown"},"source":{"5ae86c26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d65f6c72":"!pwd","3f5c83bd":"import seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score,recall_score,confusion_matrix\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier","22352ece":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv\")\nprint(train.shape)\ntrain.head()","fef90f5e":"train.tail()","c64778dd":"test = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv\")\nprint(test.shape)\ntest.head()","9572fd88":"submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv\")\nsubmission.head()","ccf4e213":"sub_id = submission[['PassengerId']]","3eaa2107":"# Find the missing values as a percentage in each column\ndef findMissingPercent(training):\n    percent_missing = training.isnull().sum() * 100 \/ len(training)\n    missing_value_df = pd.DataFrame({'column_name': training.columns,\n                                     'percent_missing': percent_missing})\n    missing_value_df.sort_values(by = 'percent_missing', ascending = False, inplace= True)\n    \n    return missing_value_df.reset_index(drop=True)\n\nmissing = findMissingPercent(train)\nmissing","0c7d00af":"missing_test = findMissingPercent(test)\nmissing_test","b2853912":"print(\"The Number of Unique Elements : {}\".format(len(train['Cabin'].unique())))\npd.DataFrame(train['Cabin'].value_counts()).head()\n\n#There are a lot of distinct numbers so decided to use the first character by filling \"Other:XX\" class for null rows. \n#\u00a0So that we will have less number of category. ","7f6c6766":"# fill missing value as a new category\ntrain['Cabin'] = train['Cabin'].fillna(\"XX\")\n\n# make that column as string\ntrain['Cabin'] = train['Cabin'].astype(str)\n\n#\u00a0extract the first digit\ntrain['Cabin_fd'] = train['Cabin'].apply(lambda x: (x[:1]))\n\n# take value counts\ntrain['Cabin_fd'].value_counts()","03ac34a4":"#\u00a0Apply the same strategy for the test set. \ntest['Cabin'] = test['Cabin'].fillna(\"XX\")\n\n# make that column as string\ntest['Cabin'] = test['Cabin'].astype(str)\n\n#\u00a0extract the first digit\ntest['Cabin_fd'] = test['Cabin'].apply(lambda x: (x[:1]))\n\n# take value counts\ntest['Cabin_fd'].value_counts()","a17244d5":"# I could not find any idea for the ticket no. I'll skip for now :) \nprint(\"The Number of Unique Elements : {}\".format(len(train['Ticket'].unique())))\npd.DataFrame(train['Ticket'].value_counts()).head()","61782a70":"print(\"The Number of Unique Elements : {}\".format(len(train['Age'].unique())))\npd.DataFrame(train['Age'].value_counts()).head()\n\n# I will fill null rows in the Age column by looking at pcass and gender information. ","4bab48f9":"#\u00a0Generate a reference for missing age value \nmissing_age = train.groupby(['Pclass', 'Sex']).agg({'Age':'mean'})\nmissing_age","4f0d0188":"# Fill based on the reference table. \ntrain['Age_filled'] = train.apply(\n    lambda row: missing_age['Age'][(row['Pclass'], row['Sex'])] if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)","66825ba5":"# Apply the same method for test set. \ntest['Age_filled'] = test.apply(\n    lambda row: missing_age['Age'][(row['Pclass'], row['Sex'])] if np.isnan(row['Age']) else row['Age'],\n    axis=1\n)","541c0b58":"print(\"The Number of Unique Elements : {}\".format(len(train['Embarked'].unique())))\npd.DataFrame(train['Embarked'].value_counts()).head()\n\n# There are small number of categories. I will fill the missing data with the most frequent element \"S\". \ntrain['Embarked']","b2ad9f87":"# Fill based on the reference table. \ntrain['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode().iloc[0])","eb170bee":"# Apply the same method for test set. \ntest['Embarked'] = test['Embarked'].fillna(train['Embarked'].mode().iloc[0])","5e661ed9":"#\u00a0Plot the histogram \ntrain.hist(column='Fare', bins = 50)","57bb2351":"train.head()","ad300472":"#\u00a0Generate a reference for missing Fare value \nmissing_fare = train.groupby(['Pclass', 'Embarked']).agg({'Fare':'mean'})\nmissing_fare","afeba6b6":"# Fill based on the reference table. \ntrain['Fare_filled'] = train.apply(\n    lambda row: missing_fare['Fare'][(row['Pclass'], row['Embarked'])] if np.isnan(row['Fare']) else row['Fare'],\n    axis=1\n)\ntrain.head()","7ff6ab43":"# Apply the same method for the test set. \ntest['Fare_filled'] = test.apply(\n    lambda row: missing_fare['Fare'][(row['Pclass'], row['Embarked'])] if np.isnan(row['Fare']) else row['Fare'],\n    axis=1\n)\ntest.head()","90f51237":"train.head()","be7b9457":"train.corr(method ='pearson')\n# There is a correlation for Fare (positive) and Age (positive) and Pclass (negative)\n#\u00a0We may use this information to generate new features. ","2ac6ed97":"\n# calculate the correlation matrix\ncorr = train.corr(method ='pearson')\n\n# plot the heatmap\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","fd8491cd":"X = train[['Pclass','Sex','Age_filled','SibSp', 'Parch','Fare_filled', 'Cabin_fd','Embarked']]\ny = train[['Survived']]\nprint(\"Training Data Feature Shape {}\".format(X.shape))\nprint(\"Training Data Target Shape {}\".format(y.shape))\n\nsubmission = test[['Pclass','Sex','Age_filled','SibSp', 'Parch','Fare_filled', 'Cabin_fd','Embarked']]\n\nprint(\"Submission Data Shape {}\".format(submission.shape))\n","67fe3d74":"X.head()","4bbd208e":"def applyOneHot(X,col):\n\n    #\u00a0Apply one-hot encoding for the training data. \n    enc_dow = OneHotEncoder(handle_unknown='ignore')\n\n    enc_df_dow = pd.DataFrame(enc_dow.fit_transform(X[[col]]).toarray())\n    enc_df_dow.columns = enc_dow.get_feature_names([col])\n\n    print(\"Encoded Shape is {} for {} \\n\".format(enc_df_dow.shape, col))\n    \n    \n    # merge with main df bridge_df on key values\n    #X_ = X.join(enc_df)\n    X = pd.concat([X, enc_df_dow], axis = 1 )\n    \n    return enc_dow, X\n\n\none_sex, X = applyOneHot(X,'Sex')\ndel X['Sex']\nX.head()","4bd705e8":"one_cabin, X = applyOneHot(X,'Cabin_fd')\ndel X['Cabin_fd']\n\none_embark, X = applyOneHot(X,'Embarked')\ndel X['Embarked']\n\nX.head()","ae914a80":"#\u00a0Apply one-hot encoding for the test data. \n\ndef encodeTestSet(enc, df, col):\n    \n    enc_df_dow_test = pd.DataFrame(enc.transform(df[[col]]).toarray())\n    enc_df_dow_test.columns = enc.get_feature_names([col])\n    print(\"Encoded Shape is {} \\n\".format(enc_df_dow_test.shape))\n\n    # Merge with main df bridge_df on key values\n    df = pd.concat([df, enc_df_dow_test], axis = 1 )\n    return df \n\n# Encode Gender\nsubmission = encodeTestSet(one_sex, submission, 'Sex')\n\n# Encode Cabin\nsubmission = encodeTestSet(one_cabin, submission, 'Cabin_fd')\n\n# Encode Gender\nsubmission = encodeTestSet(one_embark, submission, 'Embarked')\n\ndel submission['Cabin_fd']\ndel submission['Embarked']\ndel submission['Sex']\n\nsubmission.head()","423c6744":"# Apply test train and validation split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=121)\n\nprint(\"Training Ratio   : {}%\".format(X_train.shape[0]*100 \/X.shape[0]))\nprint(\"Test Ratio       : {}%\".format(X_test.shape[0]*100 \/X.shape[0] ))\n\n#\u00a0We obtained %85-%15 distribution. ","8caf85c6":"#\u00a0There is no imbalance between ones and zeros in both dataset. \nprint(y_test[y_test['Survived'] == 0].shape[0]*100 \/ y_test.shape[0])\nprint(y_train[y_train['Survived'] == 0].shape[0]*100 \/ y_train.shape[0])","06d5e2a1":"# I will test the number of estimators and max_depth. \n#\u00a0You can also use GridSearch or RandomSearch in this step. \n\nn_estimators = [50,100,500]\nmax_depth= [5,10,20]\n\nfinalScores = pd.DataFrame(columns = ['n_estimators','max_depth', 'mean_accuracy', 'std_accuracy'])\n\ndef performRF(finalScores, X_train=X_train, y_train=y_train, n_splits = 5):\n    # I will apply cross validation to get model performance from the training set. \n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    cnt = 1\n    \n    for i in n_estimators: \n        for j in max_depth: \n            \n            rf = RandomForestClassifier(\n                n_estimators=i,\n                max_depth=j\n            )\n\n            score = cross_val_score(rf, X_train, y_train, cv= kf, scoring=\"accuracy\")\n            #print(f'Scores for each fold are: {score}')\n            #print(\"----\")\n            #print(\"n_estimators : {} and max_depth : {}\".format(i,j))\n            #print(f'Average score: {\"{:.2f}\".format(score.mean())}')  \n            #print(\"----\")\n            \n            series_obj = pd.Series( [i,j,score.mean(),score.std()], \n                        index=finalScores.columns )\n            \n            # Add a series as a row to the dataframe  \n            finalScores = finalScores.append(series_obj,\n                                    ignore_index=True)\n            \n            print(\"No {} completed. (Out of {} )\".format(cnt, len(n_estimators)*len(max_depth)))\n\n            cnt = cnt + 1 \n            \n            \n    return finalScores\n\nfinalScores = performRF(finalScores, X_train=X_train, y_train=y_train, n_splits = 5)","fda08ee4":"finalScores","983b4f49":"#\u00a0create an RF Model \nrf = RandomForestClassifier(\n    n_estimators=100,\n    max_depth=10,\n    max_features= 10\n)\n\n# Fit the model \nrf.fit(X_train, y_train)\n\n\ny_pred = rf.predict(X_test)\ny_pred_tr = rf.predict(X_train)\n\n#\u00a0save prediction probabilities \npreds = rf.predict_proba(X_test)[:,1]\n\n\nprint(\"Training Accuracy : {}\".format(accuracy_score(y_train,y_pred_tr)))\nprint(\"Test Accuracy : {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"-- \")\nprint(\"Test Recall : {}\".format(recall_score(y_test,y_pred)))\nprint(\"Test Confusion Matrix : \\n {}\".format(confusion_matrix(y_test,y_pred)))\n\n\n# We obtained a low accuracy with RF. Let's optimize it with more model parameters. \n","a1fa9368":"def plotROC(y_test, preds, name):\n    \n    # generate a no skill prediction (majority class)\n    ns_probs = [0 for _ in range(len(preds))]\n\n    # calculate scores\n    ns_auc = roc_auc_score(y_test, ns_probs)\n    lr_auc = roc_auc_score(y_test, preds)\n    \n    # summarize scores\n    print('Random Guess: ROC AUC={0:.2f}'.format(ns_auc)) \n    \n    print(name + ': ROC AUC={0:.2f}'.format(lr_auc))\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, preds)\n    # plot the roc curve for the model\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='Random Guess')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label=name)\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    # show the grid\n    plt.grid(True)\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n    \nplotROC(y_test , preds, 'Random Forest')","5b0cdf90":"# I will test the number of estimators and max_depth. \n#\u00a0You can also use GridSearch or RandomSearch in this step. \n\nnum_leaves = [200,500]\nmax_depth= [3,5,10]\n\n    \nfinalScores = pd.DataFrame(columns = ['num_leaves','max_depth', 'mean_accuracy', 'std_accuracy'])\n\ndef performLGBM(finalScores, X_train=X_train, y_train=y_train, n_splits = 5,\n                num_leaves=num_leaves,max_depth =max_depth ):\n    # I will apply cross validation to get model performance from the training set. \n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    cnt = 1\n    \n    for i in num_leaves: \n        for j in max_depth: \n            \n            lg = lgb.LGBMClassifier(\n                num_leaves=i,\n                max_depth=j\n            )\n\n\n            score = cross_val_score(lg, X_train, y_train, cv= kf, scoring=\"accuracy\")\n            #print(f'Scores for each fold are: {score}')\n            #print(\"----\")\n            #print(\"n_estimators : {} and max_depth : {}\".format(i,j))\n            #print(f'Average score: {\"{:.2f}\".format(score.mean())}')  \n            #print(\"----\")\n            \n            series_obj = pd.Series( [i,j,score.mean(),score.std()], \n                        index=finalScores.columns )\n            \n            # Add a series as a row to the dataframe  \n            finalScores = finalScores.append(series_obj,\n                                    ignore_index=True)\n            \n            print(\"No {} completed. (Out of {} )\".format(cnt, len(num_leaves)*len(max_depth)))\n\n            cnt = cnt + 1 \n            \n            \n    return finalScores\n\nfinalScores = performLGBM(finalScores, X_train=X_train, y_train=y_train, n_splits = 5)","7e550569":"finalScores","41f4ad0c":"# build the lightgbm model\nlgbm = lgb.LGBMClassifier(num_leaves=200,max_depth=3,learning_rate = 0.1, num_iterations = 1000)\nlgbm.fit(X_train, y_train)\n\n\ny_pred = lgbm.predict(X_test)\ny_pred_tr = lgbm.predict(X_train)\n\n#\u00a0save prediction probabilities \npreds = lgbm.predict_proba(X_test)[:,1]\n\n\nprint(\"Training Accuracy : {}\".format(accuracy_score(y_train,y_pred_tr)))\nprint(\"Test Accuracy : {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"-- \")\nprint(\"Test Recall : {}\".format(recall_score(y_test,y_pred)))\nprint(\"Test Confusion Matrix : \\n {}\".format(confusion_matrix(y_test,y_pred)))\nprint(\"--\\n\")\n\n# We obtained a low accuracy with LightGBM. Let's optimize it with more model parameters. \n\nplotROC(y_test , preds, 'LightGBM')","162a5d10":"#import warnings\n#warnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef plotImp(model, X , num = 20):\n    feature_imp = pd.DataFrame({'Value':model.feature_importances_,'Feature':X.columns})\n    plt.figure(figsize=(40, 21))\n    sns.set(font_scale = 4)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                        ascending=False)[0:num])\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    #plt.savefig('lgbm_importances-01.png')\n    plt.show()\n\n\nplotImp(lgbm, X_train , num = 30)","2a25731a":"import shap\n#shap.initjs()\n\n# compute SHAP values\nexplainer = shap.Explainer(lgbm, X_train)\nshap_values = explainer(X_train)","50729c23":"# summarize the effects of all the features\nshap.plots.beeswarm(shap_values , max_display = 20)","e70901ef":"shap.plots.bar(shap_values , max_display = 20)","af7a24a9":"submission_test = lgbm.predict(submission) ","ef6fa67d":"sub_id['Survived'] = submission_test","711752f7":"sub_id.head()","4f90245d":"sub_id.to_csv(\"submission__.csv\", index = False, encoding = 'utf-8')","d35dd9e1":"#F5 Fare ","e73263dd":"#F4 Embarked","aadf0d51":"Define a RF Model","8a63c78d":"Feature Analysis","16a8747e":"#Perform Random Forest Model","982dfe49":"Import Data","a93964f4":"Deal With Categorical Features ","8a633172":"#Find the correlation between all features and target","02e24586":"#F2 Ticket","505215a0":"#Perform LightGBM","10bea5d4":"I used some part of my code from the below links.\n\n#ref : https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/","6585b7d7":"Until now, we have completed the missing value problem. In the next section, we may add new features. I skipped this part for now. Firsty, I will create a baseline model. ","a7d6254d":"Prepare Submission File","acd81e03":"Prepare the Training Data, Test and Validation Data","76590cac":"I obtained similar results via RF and LightGBM. I will use the LightGBM for test. ","6362810b":"Also, you can observe the most important features. This can help us to improve our model. ","d0f71b5e":"Let's check the null ratio at first. Then, handle the missing rows. ","d5f9a5f2":"#F1 Cabin","d0c84e69":"#F3 Age"}}