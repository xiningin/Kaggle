{"cell_type":{"b7572543":"code","5edc028d":"code","36c24c1d":"code","bf314383":"code","f096c4e2":"code","88d25a7d":"code","2e6e16e3":"code","359ab98b":"code","5fb6a671":"code","8814f7e9":"code","280a6ec3":"code","b6fefe52":"code","965a3c49":"code","29ad9d5b":"code","ea647161":"code","eaabc349":"code","311bef88":"code","b4635f49":"code","37024e38":"code","b9769d3c":"markdown","733e7c7c":"markdown","e6dafeda":"markdown"},"source":{"b7572543":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5edc028d":"test_data_sub=pd.read_csv('..\/input\/test.csv')\ntrain_data=pd.read_csv('..\/input\/train.csv')\nreviews=train_data['review'].get_values()\nlabels=train_data['sentiment'].get_values()\ninput_test=test_data_sub['review'].get_values()\ny_test=list()","36c24c1d":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nstop_words","bf314383":"appos = {\n\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"that's\" : \"that is\",\n\"there's\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"\n}","f096c4e2":"from string import punctuation\ndef review_formatting(reviews):\n    all_reviews=list()\n    for text in reviews:\n        lower_case = text.lower()\n        words = lower_case.split()\n        reformed = [appos[word] if word in appos else word for word in words]\n        reformed_test=list()\n        for word in reformed:\n            if word not in stop_words:\n                reformed_test.append(word)\n        reformed = \" \".join(reformed_test) \n        punct_text = \"\".join([ch for ch in reformed if ch not in punctuation])\n        all_reviews.append(punct_text)\n    all_text = \" \".join(all_reviews)\n    all_words = all_text.split()\n    return all_reviews, all_words","88d25a7d":"from collections import Counter \n# Count all the words using Counter Method\nall_reviews, all_words=review_formatting(reviews)\ncount_words = Counter(all_words)\ntotal_words=len(all_words)\nsorted_words=count_words.most_common(total_words)\nvocab_to_int={w:i+1 for i,(w,c) in enumerate(sorted_words)}","2e6e16e3":"def encode_reviews(reviews):\n    \"\"\"\n    encode_reviews function will encodes review in to array of numbers\n    \"\"\"\n    all_reviews=list()\n    for text in reviews:\n        text = text.lower()\n        text = \"\".join([ch for ch in text if ch not in punctuation])\n        all_reviews.append(text)\n    encoded_reviews=list()\n    for review in all_reviews:\n        encoded_review=list()\n        for word in review.split():\n            if word not in vocab_to_int.keys():\n                encoded_review.append(0)\n            else:\n                encoded_review.append(vocab_to_int[word])\n        encoded_reviews.append(encoded_review)\n    return encoded_reviews","359ab98b":"def pad_sequences(encoded_reviews, sequence_length=250):\n    ''' \n    Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n    '''\n    features=np.zeros((len(encoded_reviews), sequence_length), dtype=int)\n    \n    for i, review in enumerate(encoded_reviews):\n        review_len=len(review)\n        if (review_len<=sequence_length):\n            zeros=list(np.zeros(sequence_length-review_len))\n            new=zeros+review\n        else:\n            new=review[:sequence_length]\n        features[i,:]=np.array(new)\n    return features","5fb6a671":"def preprocess(reviews):\n    \"\"\"\n    This Function will tranform reviews in to model readable form\n    \"\"\"\n    formated_reviews, all_words = review_formatting(reviews)\n    encoded_reviews=encode_reviews(formated_reviews)\n    features=pad_sequences(encoded_reviews, 250)\n    return features","8814f7e9":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nencoded_reviews=encode_reviews(reviews)\nreview_len=[len(encoded_review) for encoded_review in encoded_reviews]\npd.Series(review_len).hist()\nplt.show()\npd.Series(review_len).describe()","280a6ec3":"#split_dataset into 80% training , 10% test and 10% Validation Dataset\nfeatures=preprocess(reviews)\ntrain_x=features[:int(0.90*len(features))]\ntrain_y=labels[:int(0.90*len(features))]\nvalid_x=features[int(0.90*len(features)):]\nvalid_y=labels[int(0.90*len(features)):]\nprint(len(train_y), len(valid_y))","b6fefe52":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset\n\n#create Tensor Dataset\ntrain_data=TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\nvalid_data=TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n\n#dataloader\nbatch_size=50\ntrain_loader=DataLoader(train_data, batch_size=batch_size, shuffle=True)\nvalid_loader=DataLoader(valid_data, batch_size=batch_size, shuffle=True)","965a3c49":"# obtain one batch of training data\ndataiter = iter(train_loader)\nsample_x, sample_y = dataiter.next()\nprint('Sample input size: ', sample_x.size()) # batch_size, seq_length\nprint('Sample input: \\n', sample_x)\nprint()\nprint('Sample label size: ', sample_y.size()) # batch_size\nprint('Sample label: \\n', sample_y)","29ad9d5b":"import torch.nn as nn\n \nclass SentimentalLSTM(nn.Module):\n    \"\"\"\n    The RNN model that will be used to perform Sentiment analysis.\n    \"\"\"\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):    \n        \"\"\"\n        Initialize the model by setting up the layers\n        \"\"\"\n        super().__init__()\n        self.output_size=output_size\n        self.n_layers=n_layers\n        self.hidden_dim=hidden_dim\n        \n        #Embedding and LSTM layers\n        self.embedding=nn.Embedding(vocab_size, embedding_dim)\n        self.lstm=nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        \n        #dropout layer\n        self.dropout=nn.Dropout(0.3)\n        \n        #Linear and sigmoid layer\n        self.fc1=nn.Linear(hidden_dim, 64)\n        self.fc2=nn.Linear(64, 16)\n        self.fc3=nn.Linear(16,output_size)\n        self.sigmoid=nn.Sigmoid()\n        \n    def forward(self, x, hidden):\n        \"\"\"\n        Perform a forward pass of our model on some input and hidden state.\n        \"\"\"\n        batch_size=x.size()\n        \n        #Embadding and LSTM output\n        embedd=self.embedding(x)\n        lstm_out, hidden=self.lstm(embedd, hidden)\n        \n        #stack up the lstm output\n        lstm_out=lstm_out.contiguous().view(-1, self.hidden_dim)\n        \n        #dropout and fully connected layers\n        out=self.dropout(lstm_out)\n        out=self.fc1(out)\n        out=self.dropout(out)\n        out=self.fc2(out)\n        out=self.dropout(out)\n        out=self.fc3(out)\n        sig_out=self.sigmoid(out)\n        \n        sig_out=sig_out.view(batch_size, -1)\n        sig_out=sig_out[:, -1]\n        \n        return sig_out, hidden\n    \n    def init_hidden(self, batch_size):\n        \"\"\"Initialize Hidden STATE\"\"\"\n        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden\n        ","ea647161":"# Instantiate the model w\/ hyperparams\nvocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\noutput_size = 1\nembedding_dim = 400\nhidden_dim = 256\nn_layers = 2\n\nnet = SentimentalLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\nprint(net)","eaabc349":"# loss and optimization functions\nlr=0.001\n\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\n\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\n# training params\n\nepochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n\ncounter = 0\nprint_every = 100\nclip=5 # gradient clipping\n\n# move model to GPU, if available\nif(train_on_gpu):\n    net.cuda()\n\nnet.train()\n# train for some number of epochs\nfor e in range(epochs):\n    # initialize hidden state\n    h = net.init_hidden(batch_size)\n\n    # batch loop\n    for inputs, labels in train_loader:\n        counter += 1\n\n        if(train_on_gpu):\n            inputs=inputs.cuda()\n            labels=labels.cuda()\n        # Creating new variables for the hidden state, otherwise\n        # we'd backprop through the entire training history\n        h = tuple([each.data for each in h])\n\n        # zero accumulated gradients\n        net.zero_grad()\n\n        # get the output from the model\n        output, h = net(inputs, h)\n\n        # calculate the loss and perform backprop\n        loss = criterion(output.squeeze(), labels.float())\n        loss.backward()\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n        nn.utils.clip_grad_norm_(net.parameters(), clip)\n        optimizer.step()\n\n        # loss stats\n        if counter % print_every == 0:\n            # Get validation loss\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs, labels in valid_loader:\n\n                # Creating new variables for the hidden state, otherwise\n                # we'd backprop through the entire training history\n                val_h = tuple([each.data for each in val_h])\n\n                inputs, labels = inputs.cuda(), labels.cuda()  \n                output, val_h = net(inputs, val_h)\n                val_loss = criterion(output.squeeze(), labels.float())\n\n                val_losses.append(val_loss.item())\n\n            net.train()\n            print(\"Epoch: {}\/{}...\".format(e+1, epochs),\n                  \"Step: {}...\".format(counter),\n                  \"Loss: {:.6f}...\".format(loss.item()),\n                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))","311bef88":"def test_model(input_test):\n    output_list=list()\n    batch_size=50   \n    net.eval()\n    with torch.no_grad():\n        test_review=preprocess(input_test)\n        for review in test_review:\n            # convert to tensor to pass into your model\n            feature_tensor = torch.from_numpy(review).view(1,-1)\n            if(train_on_gpu):\n                feature_tensor= feature_tensor.cuda()\n            batch_size = feature_tensor.size(0)\n            # initialize hidden state\n            h = net.init_hidden(batch_size)\n            # get the output from the model\n            output, h = net(feature_tensor, h)\n            pred = torch.round(output.squeeze()) \n            output_list.append(pred)\n        labels=[int(i.data.cpu().numpy()) for i in output_list]\n        return labels\nlabels=test_model(input_test)","b4635f49":"output = pd.DataFrame()\noutput['Id'] = test_data_sub['Id']\noutput['sentiment'] = labels\noutput.to_csv('submission.csv', index=False)","37024e38":"output","b9769d3c":"# Analyze The Review Length","733e7c7c":"#  2. TEXT PRE-PROCESSING","e6dafeda":"# 1. LOAD THE TRAINING TEXT"}}