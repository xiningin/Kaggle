{"cell_type":{"e1493590":"code","4e5dab8c":"code","b288b457":"code","4830e642":"code","82abec5f":"code","e6fb99e2":"code","1e96dcb5":"code","f4ed1b49":"code","d7722218":"code","d00a1c91":"code","df58ff2c":"code","90007643":"code","b9506cb9":"code","d2ef9c6f":"code","57d3d5a3":"code","808dfac5":"code","c7002efe":"code","b98598d1":"code","3bdb07a2":"code","260aa0c1":"code","935f6d64":"code","58203a53":"code","0e79bea5":"code","ab9a6e0e":"code","bdd5c3b1":"code","db082be4":"code","4055dcca":"code","649d2e39":"code","ccfaead3":"code","f8d8b4de":"code","e6a41399":"code","c14cc66b":"code","b61c5adc":"markdown","c01ac9d4":"markdown","0d52a2a3":"markdown","3a4dd651":"markdown","4ae2ca3d":"markdown","9f450fb8":"markdown","c3596ce5":"markdown","f40d49e5":"markdown","29b48d44":"markdown","0c7424b1":"markdown","b7095b2e":"markdown","f96f8712":"markdown","94bfae7a":"markdown","4b346328":"markdown","ddd80236":"markdown","bfa9330c":"markdown","5571e325":"markdown","d07fe3c9":"markdown","75839997":"markdown","181a7d6f":"markdown","ebbae964":"markdown","b6a28f75":"markdown","98bd7965":"markdown","a0c29f2e":"markdown","8619fabc":"markdown","82b3b96f":"markdown","5af9ed77":"markdown"},"source":{"e1493590":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('darkgrid')\nfrom matplotlib import pyplot","4e5dab8c":"df=pd.read_csv('..\/input\/Admission_Predict_Ver1.1.csv')","b288b457":"df.head(10).T","4830e642":"df=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","82abec5f":"l = df.columns\nprint('The columns are: ',l)","e6fb99e2":"print(df.isnull().sum())\nprint('\\n\\nNo null values')","1e96dcb5":"df.describe().T","f4ed1b49":"def detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \noutliers_to_drop=detect_outliers(df,2,['GRE Score', 'TOEFL Score', 'University Rating', 'SOP',\n       'LOR ', 'CGPA', 'Research'])","d7722218":"df.loc[outliers_to_drop] # Show the outliers rows","d00a1c91":"cols=df.drop(labels='Serial No.',axis=1)\ncols.head().T","df58ff2c":"corr = cols.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(9, 7))\n    ax = sns.heatmap(corr,mask=mask,square=True,annot=True,fmt='0.2f',linewidths=.8,cmap=\"hsv\")\n","90007643":"plt.rcParams['axes.facecolor'] = \"#e6ffed\"\nplt.rcParams['figure.facecolor'] = \"#e6ffed\"\ng = sns.pairplot(data=cols,hue='Research',markers=[\"^\", \"v\"],palette='inferno')","b9506cb9":"plt.rcParams['axes.facecolor'] = \"#ffe5e5\"\nplt.rcParams['figure.facecolor'] = \"#ffe5e5\"\nplt.figure(figsize=(6,6))\nplt.subplot(2, 1, 1)\nsns.distplot(df['GRE Score'],bins=34,color='Red',  kde_kws={\"color\": \"y\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 2,\"alpha\": 0.3 })\nplt.subplot(2, 1, 2)\nsns.distplot(df['TOEFL Score'],bins=12,color='Blue' ,kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"},hist_kws={\"linewidth\": 7,\"alpha\": 0.3 })","d2ef9c6f":"sns.scatterplot(x='University Rating',y='CGPA',data=df,color='Red', marker=\"^\", s=100)","57d3d5a3":"co_gre=df[df[\"GRE Score\"]>=300]\nco_toefel=df[df[\"TOEFL Score\"]>=100]","808dfac5":"fig, ax = pyplot.subplots(figsize=(15,8))\nsns.barplot(x='GRE Score',y='Chance of Admit',data=co_gre, linewidth=1.5,edgecolor=\"0.1\")\nplt.show()","c7002efe":"fig, ax = pyplot.subplots(figsize=(15,8))\nsns.barplot(x='TOEFL Score',y='Chance of Admit',data=co_toefel, linewidth=3.5,edgecolor=\"0.8\")\nplt.show()","b98598d1":"s = df[df[\"Chance of Admit\"] >= 0.75][\"University Rating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ns.plot(kind='bar',figsize=(20, 10),linestyle='dashed',linewidth=5)\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","3bdb07a2":"print(\"Average GRE Score :{0:.2f} out of 340\".format(df['GRE Score'].mean()))\nprint('Average TOEFL Score:{0:.2f} out of 120'.format(df['TOEFL Score'].mean()))\nprint('Average CGPA:{0:.2f} out of 10'.format(df['CGPA'].mean()))\nprint('Average Chance of getting admitted:{0:.2f}%'.format(df['Chance of Admit'].mean()*100))","260aa0c1":"toppers=df[(df['GRE Score']>=330) & (df['TOEFL Score']>=115) & (df['CGPA']>=9.5)].sort_values(by=['Chance of Admit'],ascending=False)\ntoppers","935f6d64":"# reading the dataset\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\n\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","58203a53":"X=df.drop('Chance of Admit',axis=1)\ny=df['Chance of Admit']","0e79bea5":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n#Normalisation works slightly better for Regression.\nX_norm=preprocessing.normalize(X)\nX_train,X_test,y_train,y_test=train_test_split(X_norm,y,test_size=0.20,random_state=101)","ab9a6e0e":"from sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error","bdd5c3b1":"regressors=[['Linear Regression :',LinearRegression()],\n       ['Decision Tree Regression :',DecisionTreeRegressor()],\n       ['Random Forest Regression :',RandomForestRegressor()],\n       ['Gradient Boosting Regression :', GradientBoostingRegressor()],\n       ['Ada Boosting Regression :',AdaBoostRegressor()],\n       ['Extra Tree Regression :', ExtraTreesRegressor()],\n       ['K-Neighbors Regression :',KNeighborsRegressor()],\n       ['Support Vector Regression :',SVR()]]\nreg_pred=[]\nprint('Results...\\n')\nfor name,model in regressors:\n    model=model\n    model.fit(X_train,y_train)\n    predictions = model.predict(X_test)\n    rms=np.sqrt(mean_squared_error(y_test, predictions))\n    reg_pred.append(rms)\n    print(name,rms)","db082be4":"y_ax=['Linear Regression' ,'Decision Tree Regression', 'Random Forest Regression','Gradient Boosting Regression', 'Ada Boosting Regression','Extra Tree Regression' ,'K-Neighbors Regression', 'Support Vector Regression' ]\nx_ax=reg_pred","4055dcca":"sns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.1\")","649d2e39":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=101)","ccfaead3":"#If Chance of Admit greater than 80% we classify it as 1\ny_train_c = [1 if each > 0.8 else 0 for each in y_train]\ny_test_c  = [1 if each > 0.8 else 0 for each in y_test]","f8d8b4de":"classifiers=[['Logistic Regression :',LogisticRegression()],\n       ['Decision Tree Classification :',DecisionTreeClassifier()],\n       ['Random Forest Classification :',RandomForestClassifier()],\n       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n       ['Ada Boosting Classification :',AdaBoostClassifier()],\n       ['Extra Tree Classification :', ExtraTreesClassifier()],\n       ['K-Neighbors Classification :',KNeighborsClassifier()],\n       ['Support Vector Classification :',SVC()],\n       ['Gausian Naive Bayes :',GaussianNB()]]\ncla_pred=[]\nfor name,model in classifiers:\n    model=model\n    model.fit(X_train,y_train_c)\n    predictions = model.predict(X_test)\n    cla_pred.append(accuracy_score(y_test_c,predictions))\n    print(name,accuracy_score(y_test_c,predictions))","e6a41399":"y_ax=['Logistic Regression' ,\n      'Decision Tree Classifier',\n      'Random Forest Classifier',\n      'Gradient Boosting Classifier',\n      'Ada Boosting Classifier',\n      'Extra Tree Classifier' ,\n      'K-Neighbors Classifier',\n      'Support Vector Classifier',\n       'Gaussian Naive Bayes']\nx_ax=cla_pred","c14cc66b":"sns.barplot(x=x_ax,y=y_ax,linewidth=1.5,edgecolor=\"0.8\")\nplt.xlabel('Accuracy')","b61c5adc":"### Outlier Detection","c01ac9d4":"Ratings of university increase with the increase in the CGPA","0d52a2a3":"Preparing the data for Regression ","3a4dd651":"## Lets check out the toppers","4ae2ca3d":"# 3.Data Analysis\n<a class=\"anchor\" id=\"3\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","9f450fb8":"## Now lets set some cut-off scores and try to analyse scores above the cut-off","c3596ce5":"Since outliers can have a dramatic effect on the prediction (especially for regression problems), I chose to manage them.\n\nI used the Tukey method (Tukey JW., 1977) to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n\nI decided to detect outliers from the numerical values features (GRE Score, TOEFL Score, University Rating, SOP, LOR , CGPA, Research). Then, i considered outliers as rows that have at least two outlied numerical values","f40d49e5":"From the above 2 graphs its clear that people tend to score above 310 in GRE and above 100 in TOEFL","29b48d44":"<font color=\"red\" size=4>Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","0c7424b1":"Here we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are also correlated.","b7095b2e":"<font color=\"red\" size=5><center>GRADUATE ADMISSION CHANCES<\/center><\/font>\n<center><img src=\"https:\/\/blogs.colum.edu\/marginalia\/files\/2017\/03\/screen-shot-2017-03-09-at-2.20.49-pm.png\"><\/center>","f96f8712":"## Regression\n<a class=\"anchor\" id=\"4.1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","94bfae7a":"Inferences from the above pairplot:\n* GRE score TOEFL score and CGPA all are linearly related to each other \n* Research Students tend to Score higher by all means","4b346328":"Taking a look at our dataset for understanding","ddd80236":"So the winner in Regression is : **Linear Regression**\n\n\nAnd the winner in Classification is : **Extra Tree Classifier**","bfa9330c":"The above two graphs make it clear that higher the Scores better the chance of admit","5571e325":"The column name `Chance of Admit` had a space at the end so I renamed it","d07fe3c9":"<font color=\"chocolate\" size=+2.5><b>My Other Kernels<\/b><\/font>\n\nClick on the button to view kernel...\n\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/fifa-in-depth-analysis-with-linear-regression\" class=\"btn btn-success\" style=\"color:white;\">FIFA In-Depth Analysis<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-1\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 1<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 2<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/students-performance-in-exams-eda-in-depth\" class=\"btn btn-success\" style=\"color:white;\">Students performance in Exams- EDA in depth \ud83d\udcca\ud83d\udcc8<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/pulmonary-embolism-dicom-preprocessing-eda\" class=\"btn btn-success\" style=\"color:white;\">\ud83e\ude7aPulmonary Embolism Dicom preprocessing & EDA\ud83e\ude7a<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/first-kaggle-submission\" class=\"btn btn-success\" style=\"color:white;\">Titanic: Machine Learning from Disaster<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/graduate-admission-chances\" class=\"btn btn-success\" style=\"color:white;\">\ud83d\udcd6 Graduate Admission Chances \ud83d\udcd5 \ud83d\udcd4<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/flower-classification-augmentations-eda\" class=\"btn btn-success\" style=\"color:white;\">Flower_Classification+Augmentations+EDA<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/storytelling-with-gwd-pre-print-data\" class=\"btn btn-success\" style=\"color:white;\">Storytelling with GWD pre_print data<\/a><br><br>\n\n\n### If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote<\/b><\/font>.<br>\n\n<a href=\"#top\" class=\"btn btn-primary\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","75839997":"<a class=\"anchor\" id=\"toc\"><\/a>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 600px;\">\n<h1>Contents<\/h1>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#1\">1 Introduction<\/a><\/li>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#2\">2 Understanding data<\/a><\/li>\n\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#3\">3 Data Analysis<\/a><\/li>\n     \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#4\">4 Modelling<\/a><\/li>\n    <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">  \n        <li style=\"list-style: outside none none !important;\"><a href=\"#4.1\">4.1 Regression<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#4.2\">4.2 Classification<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#4.3\">4.3 Blending (Coming soon)<\/a><\/li>\n        \n\n<\/ul>\n<\/div>","181a7d6f":"<font color=\"red\" size=5><center>\ud83d\udea7WORK UNDER PROGRESS\ud83d\udea7<\/center><\/font>","ebbae964":"There are no outliers because all the values are inside a fixed range and none of them go lower\/beyond that range which therefore produces no outliers","b6a28f75":"# 1. Introduction\n<a class=\"anchor\" id=\"1\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","98bd7965":"The above table gives us some intuition about all the columns and and some of their statistics","a0c29f2e":"This kernel mainly focuses on what parameters are important for a student to get into a graduate school.\n\nBy the end of this kernel it will be clear of what are the scores required for different tests to have better admission chances and get into a good graduate school.","8619fabc":"# 2.Understanding Data\n<a class=\"anchor\" id=\"2\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","82b3b96f":"## Classification\n<a class=\"anchor\" id=\"4.2\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>","5af9ed77":"# 4.Modelling\n<a class=\"anchor\" id=\"4\"><\/a>\n<a href=\"#toc\"><img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/20\/Circle-icons-arrow-up.svg\/1200px-Circle-icons-arrow-up.svg.png\" style=\"width:20px;hight:20px;float:left\" >        Back to the table of contents<\/a>"}}