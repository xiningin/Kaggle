{"cell_type":{"78350ef4":"code","c806cee6":"code","1961f3e9":"code","2ef44a8f":"code","183f0128":"code","b1db2a56":"code","32d19bd5":"code","4749035c":"code","7b752a6d":"code","0b517141":"code","1c815ccd":"code","49d3c3b4":"code","a4ddf0b3":"code","ed70dcce":"code","cb290285":"code","bbe076dd":"code","011cc8a4":"code","c4b2f203":"code","dbd18b85":"code","91cf6049":"code","4ad23c42":"code","f8835060":"code","17e2c3c9":"code","7b6bdd88":"code","f0f13898":"code","86290c01":"code","a556fba4":"code","0bbff712":"code","cf838b7f":"code","4ffa7b1e":"code","1ef90288":"code","026e33ad":"code","bd401544":"code","3f6bdc47":"code","f3177cc3":"code","4c725e7b":"code","90fcd78b":"code","deba84fb":"code","d65a1ba2":"code","cb8cf9bb":"code","d37f25fa":"code","111d8cdc":"code","498e7d3d":"code","e423d443":"code","e292dafe":"code","f23dc82f":"code","3879777c":"code","e773b68a":"code","5a34ec57":"code","c22eb557":"code","8bac5ca5":"code","51f1d09e":"code","56c9dc9d":"code","285e6944":"code","98559767":"code","bbeaa278":"code","18af6b76":"code","56eb8537":"code","66187d84":"code","76863fe0":"code","0fee2dc1":"code","6bb0d396":"markdown","482c815c":"markdown","1e073cc7":"markdown","5771caff":"markdown","86b6273a":"markdown","97671690":"markdown","5032e26f":"markdown","379e9510":"markdown","532fd139":"markdown","8a716149":"markdown","08f03f68":"markdown","b3f9954b":"markdown","f1e85ce3":"markdown","ace08a88":"markdown","9e8fd4ff":"markdown","c7744e9a":"markdown","f7294578":"markdown","4008aca1":"markdown","beb9cd2c":"markdown","074801c3":"markdown","9b4f12fd":"markdown","5e41a1eb":"markdown","56e1144b":"markdown","a19766c4":"markdown","36cb436a":"markdown","a04234f0":"markdown","8a01136a":"markdown","dcc7fe74":"markdown","5b3a77e5":"markdown","11315df4":"markdown","f97b4345":"markdown","63c0f05d":"markdown","dfd14b42":"markdown","55f7247a":"markdown","8790a2b1":"markdown","5ce9cfbc":"markdown"},"source":{"78350ef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c806cee6":"# Let's import the libraries\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sys\nimport os\nfrom scipy.stats import ttest_ind\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import GridSearchCV","1961f3e9":"# preparations\n# get the dataframe in a variable\ndf_estates = pd.read_csv(\"\/kaggle\/input\/real-estate-dataset\/data.csv\")","2ef44a8f":"# create a copy of the initial dataframe\ndf_data = df_estates.copy()","183f0128":"# view the five first rows of the dataset\ndf_data.head()","b1db2a56":"# Let's identify the target \ndf_data.columns","32d19bd5":"# Shape of the dataframe\ndf_data.shape","4749035c":"df_data.dtypes","7b752a6d":"df_data.info()","0b517141":"df_data[\"CHAS\"] = df_data[\"CHAS\"].astype(\"category\")","1c815ccd":"df_data[\"CHAS\"].unique()","49d3c3b4":"# Counts the proportions of class 0 and class 1 in the CHAS variable\ndf_data[\"CHAS\"].value_counts(normalize = True)","a4ddf0b3":"# Let's have a view on the proportions of the CHAS variable\ndf_data[\"CHAS\"].value_counts().plot.pie(labels = [\"class 0\", \"class 1\"])","ed70dcce":"df_data.duplicated().sum()","cb290285":"# Counting the number of null values for each column\ndf_data.isna().sum()","bbe076dd":"# Counting the number of null values\ndf_data.isna().sum().sum()","011cc8a4":"profile = ProfileReport(df_data, title = \"Pandas Profiling Report\")","c4b2f203":"profile","dbd18b85":"# Get another correlation matrix with plotly express\npx.imshow(df_data.corr())","91cf6049":"for column in df_data.select_dtypes(np.number).columns:\n    plt.figure(figsize=(7, 7))\n    sns.boxplot(data = df_data, x=\"CHAS\", y = column)","4ad23c42":"sns.pairplot(df_data, hue = \"CHAS\")","f8835060":"df_chas_0, df_chas_1 = df_data[df_data[\"CHAS\"] == 0], df_data[df_data[\"CHAS\"] == 1]","17e2c3c9":"# Let's define a function that can do the student test for the different classes.\ndef student_test(class_0, class_1, alpha, column):\n    class_0, class_1 = class_0[column], class_1[column]\n    class_0 = class_0.sample(class_1.shape[0])\n    stat, p = ttest_ind(class_0, class_1)\n    if p < alpha:\n        print(f\"{column:-<30} : H0 rejected\")\n    else:\n        print(f\"{column:-<30} : H0 accepted\")","7b6bdd88":"for column in df_data.select_dtypes(np.number).columns:\n    student_test(df_chas_0, df_chas_1, 0.025, column)","f0f13898":"class Preprocess:\n    '''This class will help us to preprocess the data and train the model\n    \n    Attributes:\n        dataframe(pd.Dataframe): The dataset before any preprocessing\n        target(str): The name of the target\n    '''\n    \n    def __init__(self, dataframe, target:str):\n        '''Initialize the attributes (the data frame and the name of the target)\n        '''\n        if target in dataframe.columns:            \n            self.__dataframe = dataframe.copy()\n            self.target = target\n        else:\n            raise ValueError(\"The target must be a existing column.\")\n    \n    '''Define the dataframe property\n    '''\n    @property\n    def dataframe(self):\n        return self.__dataframe\n    \n    def split_data(self):\n        '''Define a method which can split the data into training set and testing set\n        '''\n        train_data, test_data = train_test_split(self.dataframe, test_size = 0.2, random_state = 12)\n        return train_data, test_data\n    \n    def clean_data(self, data, column, mode):\n        '''Define a column that will clean the dataframe\n        '''\n        for col in column:\n            if mode == \"replace\":\n                replace_by = \"me\"\n                #replace_by = input(\"Do you replace the na values by mean(me), by max(ma), by min(mi) or by 0(z)\")\n                if replace_by == \"me\":\n                    replace = data[col].mean()\n                elif replace_by == \"ma\":\n                    replace = data[col].max()\n                elif replace_by == \"mi\":\n                    replace = data[col].min()\n                elif replace_by == \"z\":\n                    replace = 0\n                else:\n                    raise ValueError(\"You doesn't choose the right replace mode. Retry again please.\")\n                data[col] = data[col].fillna(replace)\n            elif mode == \"drop\":\n                axis = input(\"Provide the drop axis please: 0 or 1\")\n                if axis == '0':\n                    data.dropna(axis = 0)\n                elif axis == '1':\n                    data.dropna(axis = 1)\n                else:\n                    raise ValueError(\"You doesn't provide a valid axis. Retry again please.\")\n            else:\n                raise ValueError(\"The chosen mode doesn't exist.\")\n        return data\n    \n    def normalization(self, data):\n        '''Define a normalization function\n        '''\n        normalizer = RobustScaler()\n        normalized_data = normalizer.fit_transform(data)\n        return normalized_data\n    \n    def delete_columns(self, columns_to_delete, axis = 1):\n        '''Drop directly columns\n        '''\n        column = columns_to_delete\n        if type(column) is str:\n            column = [column]\n        elif type(column) is list:\n            column = column\n        else:\n            raise TypeError(\"The column specified as parameter must be of type string or list\")\n        for col in self.dataframe.columns:\n            if col in column:\n                self.dataframe.drop(col, axis = axis, inplace = True)\n    \n    def features_selection(self, model, X_train, y_train):\n        '''Features selection with SelectFromModel and RidgeRegression model\n        '''\n        select = SelectFromModel(model)\n        select.fit(X_train, y_train)\n        print(f\"chosed columns :\\n\",X_train.columns[select.get_support()])\n        return select\n    \n    def preprocessing(self, data, columns, cleaning_mode = \"replace\"):\n        '''Define a preprocessing function\n        '''\n        column = columns\n        if type(column) is str:\n            column = [column]\n        elif type(column) is list:\n            column = column\n        else:\n            raise TypeError(\"The column specified as parameter must be of type string or list\")\n        \n        # clean the dataframe\n        data = self.clean_data(data, column, cleaning_mode)\n        \n        # get the target\n        y = data[self.target]\n        \n        # get the features\n        X = data.drop(columns = [self.target], axis = 1)\n        \n        return X, y\n    \n    def simple_train(self, X_train, y_train, X_test, y_test, model_type = \"regressor\"):\n        '''This function have to train a LassoRegression model or a DecisionTreeClassifier \n        depending on the value of type parameter.\n        '''\n        if model_type == \"regressor\":\n            model = Lasso(alpha = 0.1)\n            model.fit(X_train, y_train)\n        elif model_type == \"classifier\":\n            model = DecisionTreeClassifier(random_state = 12)\n            model.fit(X_train, y_train)\n        else:\n            raise ValueError(\"Type {} is not define !\".format(model_type))\n        \n        return model\n    \n    def evaluation(self, model, model_name, X_test, y_test, X_train, y_train):\n        \"\"\"Define a scoring function\n        \"\"\"\n        y_pred = model.predict(X_test)\n        print(f\"Mean squared error : {mean_squared_error(y_test, y_pred)}\")\n        print(f\"Mean absolute error : \\n{mean_absolute_error(y_test, y_pred)}\")\n        print(f\"Model score : \\n{model.score(X_test, y_test)}\")\n        N, train_score, test_score = learning_curve(model, X_train, y_train, cv = 5, train_sizes = np.linspace(0.1, 1, 10))\n        plt.figure(figsize = (13, 14))\n        plt.plot(N, train_score.mean(axis = 1), label = \"train_score\")\n        plt.plot(N, test_score.mean(axis = 1), label = \"test_score\")\n        plt.title(f\"Evaluation model {model_name}\")\n        plt.legend()","86290c01":"class RegressionModel(Preprocess):\n    '''This class will help us to train a regression model. Children of Preprocess class\n    \n    Attributes:\n        dataframe(pd.Dataframe): The dataset before any preprocessing\n        target(str): The name of the target\n    '''\n    \n    def __init__(self, dataframe, target:str):\n        super().__init__(dataframe, target)\n    \n    def multiple_model(self, X_train, y_train, X_test, y_test, discretize = False):\n        \"\"\"Define a function that can do multiple training with different models\n        and evaluate each model.\n        \"\"\"\n        from sklearn.preprocessing import KBinsDiscretizer\n        from sklearn.pipeline import make_pipeline\n        from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n        from sklearn.svm import SVR\n        from sklearn.linear_model import Ridge\n        \n        numerical_features = make_column_selector(dtype_include=np.number)\n        \n        if discretize == True:\n            numerical_pipeline = make_pipeline(KBinsDiscretizer(10, encode = \"onehot\"))\n        else:\n            numerical_pipeline = make_pipeline(PolynomialFeatures(10), RobustScaler())\n        \n        preprocessor = make_column_transformer((numerical_pipeline, numerical_features))\n        \n        Forest = make_pipeline(preprocessor, RandomForestRegressor(random_state = 12))\n        Adaboost = make_pipeline(preprocessor, AdaBoostRegressor(random_state = 12))\n        Svr = make_pipeline(preprocessor, SVR())\n        Ridge = make_pipeline(preprocessor, Ridge(alpha = 0.1))\n        \n        models_dict = {\"Forest\" : Forest,\n                       \"Adaboost\" : Adaboost,\n                       \"SVC\" : Svr,\n                       \"Ridge\" : Ridge}\n        \n        for key, value in models_dict.items():\n            print(f\"{'-'*15}\\nFor Model {key} :\")\n            value.fit(X_train, y_train)\n            self.evaluation(value, key, X_test, y_test, X_train, y_train)\n            yield key, value\n   ","a556fba4":"class ClassificationModel(Preprocess):\n    '''This class will help us to train a classification model. Children of Preprocess class\n    \n    Attributes:\n        dataframe(pd.Dataframe): The dataset before any preprocessing\n        target(str): The name of the target\n    '''\n    \n    def __init__(self, dataframe, target:str):\n        super().__init__(dataframe, target)\n    \n    def multiple_model(self, X_train, y_train, X_test, y_test, discretize = False):\n        \"\"\"Define a function that can do multiple training with different models\n        and evaluate each model.\n        \"\"\"\n        from sklearn.preprocessing import KBinsDiscretizer\n        from sklearn.pipeline import make_pipeline\n        from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n        from sklearn.svm import SVC\n        from sklearn.neighbors import KNeighborsClassifier\n        from sklearn.linear_model import LogisticRegression\n        \n        numerical_features = make_column_selector(dtype_include=np.number)\n        \n        if discretize == True:\n            numerical_pipeline = make_pipeline(KBinsDiscretizer(10, encode = \"onehot\"))\n        else:\n            numerical_pipeline = make_pipeline(PolynomialFeatures(10), RobustScaler())\n        \n        preprocessor = make_column_transformer((numerical_pipeline, numerical_features))\n        \n        Forest = make_pipeline(preprocessor, RandomForestClassifier(random_state = 12))\n        Adaboost = make_pipeline(preprocessor, AdaBoostClassifier(random_state = 12))\n        Svc = make_pipeline(preprocessor, SVC(random_state = 12))\n        Knn = make_pipeline(preprocessor, KNeighborsClassifier())\n        LogReg = make_pipeline(preprocessor, LogisticRegression(random_state = 12))\n        \n        models_dict = {\"Forest\" : Forest,\n                       \"Adaboost\" : Adaboost,\n                       \"SVC\" : Svc,\n                       \"Knn\": Knn,\n                       \"LogReg\": LogReg\n                      }\n        \n        for key, value in models_dict.items():\n            print(f\"{'-'*15}\\nFor Model {key} :\")\n            value.fit(X_train, y_train)\n            self.evaluation(value, key, X_test, y_test, X_train, y_train)\n            yield key, value\n    ","0bbff712":"preprocess = RegressionModel(df_data, \"MEDV\")","cf838b7f":"# drop ZN and RAD variable from the dataset\npreprocess.delete_columns([\"ZN\", \"RAD\", \"CRIM\", \"B\"])","4ffa7b1e":"train_set, test_set = preprocess.split_data()","1ef90288":"# shapes of the sets\ntrain_set.shape, test_set.shape","026e33ad":"X_train, y_train = preprocess.preprocessing(train_set, \"RM\")","bd401544":"X_test, y_test = preprocess.preprocessing(test_set, \"RM\")","3f6bdc47":"model = preprocess.simple_train(X_train, y_train, X_test, y_test)","f3177cc3":"preprocess.evaluation(model, \"Lasso\", X_test, y_test, X_train, y_train)","4c725e7b":"columns = X_train.columns\ncoefs = model.coef_\npx.pie(values = coefs, names = columns)","90fcd78b":"select = preprocess.features_selection(Ridge(alpha = 0.1), X_train, y_train)","deba84fb":"chosen_variables = ['CHAS', 'NOX', 'RM']","d65a1ba2":"X_train, X_test = X_train[chosen_variables], X_test[chosen_variables]","cb8cf9bb":"model = preprocess.simple_train(X_train, y_train, X_test, y_test)\npreprocess.evaluation(model, \"Lasso\", X_test, y_test, X_train, y_train)","d37f25fa":"generate_models = preprocess.multiple_model(X_train, y_train, X_test, y_test)","111d8cdc":"models = dict()","498e7d3d":"for key, model in generate_models:\n    models[key] = model","e423d443":"# Let's see the params of the model\nmodels[\"Ridge\"].get_params()","e292dafe":"# initialize the params to test\nhyper_params = {\n    'columntransformer__pipeline__polynomialfeatures__degree': np.arange(8, 13),\n    'columntransformer__pipeline__polynomialfeatures__include_bias': [True, False],\n    'columntransformer__pipeline__polynomialfeatures__interaction_only': [True, False],\n    'ridge__alpha': np.linspace(0.1, 1, 10),\n    'ridge__copy_X': [True, False],\n    'ridge__normalize': [True, False],\n    'ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']\n}","f23dc82f":"# make mae scorer\nmae = make_scorer(mean_absolute_error, greater_is_better = False)","3879777c":"# create a gridsearchcv\ngrid = GridSearchCV(models[\"Ridge\"], param_grid = hyper_params, scoring = mae, cv = 5)","e773b68a":"# train the model with different hyper-params values\ngrid.fit(X_train, y_train)","5a34ec57":"# Let's see the best params\ngrid.best_params_","c22eb557":"# get the best model\nmodel = grid.best_estimator_","8bac5ca5":"# Let's evaluate the model\npreprocess.evaluation(model, \"Ridge\", X_test, y_test, X_train, y_train)","51f1d09e":"generate_models = preprocess.multiple_model(X_train, y_train, X_test, y_test, True)","56c9dc9d":"models = dict()","285e6944":"for key, model in generate_models:\n    models[key] = model","98559767":"# Let's see the params of the model\nmodels[\"Ridge\"].get_params()","bbeaa278":"# initialize the params to test\nhyper_params = {\n    'columntransformer__pipeline__kbinsdiscretizer__encode': ['onehot', 'onehot-dense', 'ordinal'],\n    'columntransformer__pipeline__kbinsdiscretizer__n_bins': np.arange(5, 13),\n    'columntransformer__pipeline__kbinsdiscretizer__strategy': ['uniform', 'quantile', 'kmeans'],\n    'ridge__alpha': np.linspace(0.1, 1, 10),\n    'ridge__copy_X': [True, False],\n    'ridge__normalize': [True, False]\n}","18af6b76":"# create a gridsearchcv\ngrid = GridSearchCV(models[\"Ridge\"], param_grid = hyper_params, scoring = mae, cv = 5)","56eb8537":"# train the model with different hyper-params values\ngrid.fit(X_train, y_train)","66187d84":"# Let's see the best params\ngrid.best_params_","76863fe0":"# get the best model\nmodel = grid.best_estimator_","0fee2dc1":"# Let's evaluate the new model\npreprocess.evaluation(model, \"Ridge\", X_test, y_test, X_train, y_train)","6bb0d396":"## Cross validation and grid search","482c815c":"We maintain the ridge model ","1e073cc7":"The CHAS variable doesn't influence the target but rather the AGE and PTRATIO variables.","5771caff":"# Let's get ready for the EDA \n- Objectives: Explore the dataset of housing values in suburbs of Boston and predict the prices. We must reach more than 60 % of the score. \n- First tasks: \n  - Identify the target: The target is the median value of owner-occupied homes (float) in 1000's dollars. We can discretize the variable to use a classification model or use directly a linear regression model.\n  - Verify the shape of the data frame: The data frame contains 511 observations and 14 variables (including the MEDV variable).\n  - Identify which columns are of type float or int or object: We identify three columns of type int and the other columns are of type float. The column CHAS is, as we learned in the description, a categorical variable (the class 0 represent a percentage of 93% and, class 1, 6%). RAD and TAX columns are discrete variables but not categorical variables. Other columns are continuous variables.\n  - Inspect if the dataset contains duplicated values: The data frame doesn't contain duplicated rows.\n  - Inspect if the dataset contains null values: alone the column RM contains null values (5). We can fill null values by the average value of the RM variable.\n  - Profile the dataset by using the pandas-profiling library :\n      - The ZN variable contains null values (73.8% of the column is equal to 0).\n      - The CHAS variable is interesting because it contains imbalanced binary classes (important for the training).\n      - We see that some columns don't have many distinct values inside so we can use the ridge regression to penalize those columns (most investigations are required).\n      - The RM column seems to have a normal distribution like the target.\n      - We remark that the target has a linear correlation with the RM variable but not with other variables.\n  - Guess if some columns are strongly correlated: The most correlated variables (>90% of correlation) is TAX and RAD variables but we identify other correlated variables with more than 70% of correlation (INDUCE is correlated with NOX and TAX variables with respectively -76% and 72% of correlation and DIS is correlated with NOX and AGE with respectively -76% and -74% of correlation).  \n  - Verify if the dataset contains some abnormal values : We identify some variables containing abnormal values\n      - CRIME variable contains many abnormal values but more on the class 0 of CHAS variable (one abnormal on class 1 of CHAS variable);\n      - We remark the same thing about the ZN variable but with 3 abnormal values on class 1 of CHAS variable at this time;\n      - RM contains like CRIME 1 abnormal value on class 1 of CHAS variable but many abnormal values on the class 0 (around the box);\n      - DIS contains three abnormal values on class 0 of CHAS variable;\n      - RAD and TAX contain only one abnormal value on class 1 of CHAS variable;\n      - PTRATIO contains two abnormal values on class 0 of CHAS variable;\n      - the B variable has 3 abnormal values on class 1 of CHAS variable and many abnormal values on class 0;\n      - the LSTAT variable contains 6 abnormal values on class 0 of CHAS variable;\n      - the target contains many abnormal values on class 0 of CHAS variable.\n  - Move to the first conclusion : \n      - RM variable is the more correlated variable to the target. So we have to maintain this variable for training the model;\n      - Some variables don't contain much variability along with the dataset. Maybe we have to delete them from the dataset later;\n      - ZN variable doesn't contain great values because of the percentage of null values. Another fact is that the no correlation with any variable. We have to remove it from the dataset.\n      - Some variables like TAX and RAD have a great relationship. So we need only one of a couple of variables.\n      - We detect some variables having many abnormal values on class 0 of the CHAS variable and only a few or none of the abnormal data on class 1 of the CHAS variable. If we delete all of those abnormal values it will delete any other data that may be necessary for training the model. So we have to keep them for the moment.\n- Second Tasks:\n  - Verify if features are correlated to the target: Histograms and other plots\n  - Verify if the CHAS variable adds information to the dataset: CHAS variable doesn't add great information to the dataset. The distributions of continuous variables are the same for different classes of the CHAS variable. But we see that the AGE distributions are almost differents. So we can keep the CHAS variable for the moment.\n  - Move to the second conclusion: The CHAS variable seems to doesn't influence the target. We can investigate it by doing a student test.\n- Third Tasks:\n  - Hypothesis test H0: Let's test if the real estate distributions are the same for different classes of CHAS variables.\n      - The CHAS variable doesn't influence any of the variables excepted the AGE and PTRATIO variables. \n- General conclusion: We have a small number of observations, and we will certainly obtain a high bias with a regression model. We can train at the first time regression model, and nextly we will discretize the target and retry.\n","86b6273a":"RM and CHAS are the more important variables. But let's a better features selection by using SelectFromModel.","97671690":"separating the dataset depending on the classes of CHAS variable","5032e26f":"CHAS variable doesn't add great information to the dataset. The distributions of continuous variables are the same for different classes of the CHAS variable.","379e9510":"We identify some variables with abnormal values after tracing a plot box of each variable","532fd139":"## Split the data","8a716149":"## View of the distributions and correlations with pair plot","08f03f68":"## Cross validation and grid search","b3f9954b":"## Let's use pandas profiling before moving to more strength explorations","f1e85ce3":"## Delete some columns from the dataset","ace08a88":"## Type of the variables","9e8fd4ff":"## Identify the target :","c7744e9a":"## Let's see what variables are more important to the model.","f7294578":"The best model with no overfitting is the ridge regressor. We obtain underfitting on another side with a score of 57% on the testing outcome.","4008aca1":"# Model selection: \nWe have to choose between a few regression models, the best one with no overfitting and no underfitting. The selected regression models for this step are the SVR, the Random forest regressor, the Adaboost regressor, and the ridge regression.","beb9cd2c":"# model selection under features discretization","074801c3":"## Verify if the dataset contains abnormal values by tracing the box plot of each column","9b4f12fd":"## General preprocess for each set","5e41a1eb":"## Hypothesis test","56e1144b":"## First training with a Lasso model","a19766c4":"So the class 0 represent a percentage of 93% and, class 1, 6%","36cb436a":"change the type of the column CHAS to a category in the data frame","a04234f0":"## Verify the shape of dataframe","8a01136a":"Let's use a generator to train the multiple models","dcc7fe74":"Only CHAS NOX and RM were chosen","5b3a77e5":"The data frame doesn't contain duplicated rows","11315df4":"The more correlated variables realize a blue (negative correlation) or yellow (positive correlation) color for the correlation coefficient between there.","f97b4345":"## Begin the preprocess","63c0f05d":"Let's do the test for each non categorical column","dfd14b42":"# Preprocessing: Use a class for a more helps process\n#### After the first preprocess: We obtained 53% of the score for the testing set and 61% for the training set. Overfitting has occurred. Let's delete the CRIM and B variables to obtain a better result because those variables contain many abnormal values.\n#### After the second preprocess: We obtained 52% of the score for the testing set and 59% for the training set. We have to do more preprocessing. Let's verify what variables are more important to the model and delete those which don't add much information. We will need SelectFromModel to do that.\n#### After the third preprocess: We obtained 46% of the score for the testing set and 50% for the training set. The chosen variables were CHAS, NOX, RM with a Ridge model. We can go the next step : model selection.","55f7247a":"## Let's verify if the dataset contains null values","8790a2b1":"The target here is the column MEDV (Median value of owner-occupied homes) which is expressed as type float in 1000's $","5ce9cfbc":"## Inspect if the data frame contains duplicated rows (duplicated columns are for later)"}}