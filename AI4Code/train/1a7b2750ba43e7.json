{"cell_type":{"6ad63654":"code","e53c5250":"code","1979dbd0":"code","49867c45":"code","f0c6954d":"code","621db5c7":"code","e387f33a":"code","97153c1d":"code","9418e03a":"code","d57e58a9":"code","5fe37047":"code","4e171755":"code","58fa4775":"code","38ce90fb":"code","6c96ee1d":"code","3211e9bd":"code","40c6e984":"code","9ce55f33":"code","b04a6430":"code","ac26af4d":"code","d92b026e":"code","c7809e4b":"markdown","809914dc":"markdown","cee2a1a4":"markdown","cafb32fa":"markdown","83e95891":"markdown","5f0d3588":"markdown","8a35476a":"markdown","5a50c036":"markdown","61240480":"markdown","75582fe8":"markdown","ef1184c9":"markdown","97449a42":"markdown"},"source":{"6ad63654":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e53c5250":"file = '..\/input\/Iris.csv'\ndf = pd.read_csv(file)\ndf = df.drop('Id',axis=1)\ndf = df.rename(columns={'Species':'Label'})","1979dbd0":"def train_test_split(df,test_size):\n    if isinstance(test_size, float):\n        test_size = int(test_size*len(df))\n\n    indices = list(df.index)\n    test_indices = random.sample(indices, test_size)\n    test_df = df.loc[test_indices]\n    train_df = df.drop(test_indices)\n    return train_df, test_df","49867c45":"train_df, test_df = train_test_split(df, 0.1)\ndata = train_df.values           #making a numpy array for faster \nsns.lmplot(data=train_df, x = \"PetalWidthCm\", y = \"PetalLengthCm\", hue=\"Label\", fit_reg=False)","f0c6954d":"def check_purity(data):\n    label_column = data[:,-1]\n    unique_classes = np.unique(label_column)\n    \n    if len(unique_classes) == 1:\n        return True\n    else:\n        return False","621db5c7":"def classify_data(data):\n    label_column = data[:,-1]\n    unique_classes, count_unique_classes = np.unique(label_column, return_counts=True)\n    classification = unique_classes[count_unique_classes.argmax()]\n    return classification","e387f33a":"def get_potential_splits(data):\n    \n    potential_splits = {}\n    _,n_columns = data.shape\n    for index in range(n_columns-1):\n        potential_splits[index] = []\n        values = np.unique(data[:,index])\n        for i in range(1,len(values)):\n            prev_val = values[i-1]\n            this_val = values[i]\n            mid_val = (prev_val+this_val)\/2\n            potential_splits[index].append(mid_val)\n    return potential_splits","97153c1d":"splits = get_potential_splits(train_df.values)\nsns.lmplot(data=train_df, x = \"PetalWidthCm\", y = \"PetalLengthCm\", hue=\"Label\", fit_reg=False, height = 6, aspect = 1.5)\nplt.vlines(x=splits[3], ymin = 0, ymax = 7)","9418e03a":"def split_data(data, split_column, split_value):\n    \n    split_column_value = data[:, split_column]\n    \n    data_below = data[split_column_value <= split_value]\n    data_above = data[split_column_value > split_value]\n    \n    return data_below, data_above","d57e58a9":"# data_below, data_above = split_data(data, 3, 0.8)\n# t = pd.DataFrame(data_below)\n# t.columns = train_df.columns\n# sns.lmplot(data=t, x = \"PetalWidthCm\", y = \"PetalLengthCm\", hue=\"Label\", fit_reg=False, height = 6, aspect = 1.5)\n# plt.vlines(x = 0.8, ymin = 0, ymax = 7.5)\n# plt.xlim(0,2.6)","5fe37047":"def calculate_entropy(data):\n    label_column = data[:,-1]\n    _, counts = np.unique(label_column, return_counts = True)\n    total = counts.sum()\n    probabilities = counts\/total\n    entropy = sum(-probabilities*np.log2(probabilities))\n    return entropy","4e171755":"def calculate_overall_entropy(data_below, data_above):\n    \n    total = len(data_below)+len(data_above)\n    p = len(data_below)\/total\n    q = len(data_above)\/total\n\n    overall_entropy = p*calculate_entropy(data_below) + q*calculate_entropy(data_above)\n    \n    return overall_entropy","58fa4775":"def determine_best_split(data, potential_splits):\n    overall_entropy = 1000\n    for column in splits:\n        for value in splits[column]:\n            data_below, data_above = split_data(data, column, value)\n            current_overall_entropy = calculate_overall_entropy(data_below, data_above)\n\n            if current_overall_entropy <= overall_entropy:\n                overall_entropy = current_overall_entropy\n                best_split_column = column\n                best_split_value = value\n    return best_split_column, best_split_value","38ce90fb":"determine_best_split(data,splits)","6c96ee1d":"def decision_tree_algorithm(df, counter = 0, min_samples = 2):\n    \n    if counter == 0:\n        global COLUMN_HEADERS\n        COLUMN_HEADERS = df.columns\n        data = df.values\n    else:\n        data = df\n    \n    #base case\n    if (check_purity(data)) or (len(data) < min_samples):\n        classification = classify_data(data)\n        return classification\n    \n    else:\n        counter += 1\n        potential_splits = get_potential_splits(data)\n        split_column, split_value = determine_best_split(data, potential_splits)\n        data_below, data_above = split_data(data, split_column, split_value)\n        \n        question = \"{} <= {}\".format(COLUMN_HEADERS[split_column], split_value)\n        subtree = {question:[]}\n        \n        yes_answer = decision_tree_algorithm(data_below, counter, min_samples)\n        no_answer = decision_tree_algorithm(data_above, counter, min_samples)\n        \n        if yes_answer == no_answer:\n            subtree = yes_answer\n        else:\n            subtree[question].append(yes_answer)\n            subtree[question].append(no_answer)\n        \n        return subtree","3211e9bd":"tree = decision_tree_algorithm(train_df, min_samples = 2)\npprint(tree)","40c6e984":"def classify_example(example, tree):\n    question = list(tree.keys())[0]\n    feature, comparison, value = question.split()\n\n    if example[feature] <= float(value):\n        answer = tree[question][0]\n    else:\n        answer = tree[question][1]\n\n    if not isinstance(answer, dict):\n        return answer\n    else:\n        return classify_example(example, answer)\n    \nquestion = list(tree.keys())[0]\nfeature, comparison, value = question.split()\nprint(question)\nprint(feature)\nprint(comparison)\nprint(value)","9ce55f33":"example = train_df.loc[1]\nprint(example)","b04a6430":"# Classifying using our tree\n\nclassify_example(example, tree)","ac26af4d":"def calc_accuracy(df, tree):\n    \n    df[\"Classification\"] = df.apply(classify_example, axis = 1, args = (tree,))\n    df['Correct_Classification'] = df.Classification == df.Label\n\n    accuracy = df.Correct_Classification.mean()\n    \n    return accuracy","d92b026e":"print(\"Train data accuracy = \",calc_accuracy(train_df, tree))\nprint(\"Test data accuracy = \",calc_accuracy(test_df, tree))","c7809e4b":"**Classify**","809914dc":"**Check for Data Purity**","cee2a1a4":"**Decision Tree Algorithm**","cafb32fa":"**Train-Test-Split Function**","83e95891":"**Preparing Data**","5f0d3588":"**Consider an example**","8a35476a":"**The IRIS DataSet**","5a50c036":"**Split Data**","61240480":"**Classification**","75582fe8":"**Potential Splits**","ef1184c9":"**Lowest Overall Entropy**","97449a42":"**Accuracy**"}}