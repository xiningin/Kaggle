{"cell_type":{"98d45291":"code","78c2992f":"code","29ac0190":"code","6aeda769":"code","4fa81d05":"code","964884c9":"code","fd60964d":"code","36a2b453":"code","c1df79f7":"code","7b6f60d9":"code","faff969a":"code","9683af79":"code","14942d9a":"code","fde6b9ad":"code","e6340c31":"code","02b595cf":"code","b57c6a5f":"code","3ace5a66":"code","edcf2513":"code","cf9688a5":"code","2a143959":"code","2e846992":"code","f130cb25":"code","975ca443":"code","4a4b43a7":"code","e94e024f":"code","fdb2162f":"code","8d9f5105":"code","c533e3f8":"code","3d924f19":"code","f0282ee0":"code","9dad7eb3":"code","777ca358":"code","43514faf":"code","faa6756c":"code","ebc6cdf0":"code","44185d19":"code","67cfc090":"code","83666f4f":"code","1395c33e":"code","f4a815d1":"code","2069cc0d":"code","0ef30487":"code","c51c28f6":"code","9c503195":"code","de4cc90a":"code","0b143ec0":"code","8768f213":"code","74593a5c":"code","5528fd4e":"code","8a458437":"code","bc76efa0":"code","a4fc0762":"code","80d4e55f":"code","73e255a7":"code","749ca205":"code","ce0b06a9":"code","6d373db7":"code","90533b84":"code","f5ccad81":"code","e71f2034":"code","44b94d8e":"code","ae984bce":"code","04c823aa":"code","740067a2":"code","5a138bcc":"code","fb052c78":"code","545dfa88":"code","866bc759":"code","d4fc8139":"code","1d60110c":"code","e84cec47":"code","c21cd18c":"code","11094e2c":"code","cbf98fe8":"code","1cc7bc17":"code","723480d6":"code","8c8572a7":"markdown","3e831ab9":"markdown"},"source":{"98d45291":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport sklearn\nfrom datetime import date, timedelta\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb","78c2992f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","29ac0190":"# Change your location\ntrain = pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/test_QkPvNLx.csv')\nsample = pd.read_csv('\/kaggle\/input\/womenintheloop-data-science-hackathon\/sample_submission_pn2DrMq.csv')","6aeda769":"# Columns of the data set\nprint(list(train.columns))\nprint(list(test.columns))","4fa81d05":"print(train.shape)\nprint(test.shape)","964884c9":"# We notice that User Traffic in the train set is missing in the test data\n# Lets analyse each parameter","fd60964d":"train.ID.nunique()","36a2b453":"train.apply(lambda x : len(x.unique()))","c1df79f7":"# Notice that ID is unique for each data point, So removing can remove it ","7b6f60d9":"del train['ID']\ndel test['ID']","faff969a":"# Sales is the target variable and its a regression problem, lets further analyse sales","9683af79":"train.Sales.describe() # ","14942d9a":"# From the description of sales, we find that minimum sales is 0 which is not possible","fde6b9ad":"train[['Sales']].boxplot()","e6340c31":"sns.distplot(train['Sales'])","02b595cf":"# From the distribution of we can find that\n# Sales do not follow normal distribution\n# Sales is postively skewed","b57c6a5f":"print(\"Skewness= \", train['Sales'].skew())\nprint(\"Kurtosis= \", train['Sales'].kurt())","3ace5a66":"# Since kutosis is > 3, we can conclude that Distribution is longer, has Many Outliers","edcf2513":"# Analysis of Day_Number","cf9688a5":"# plotting on sample of dataset coz data is huge \nsampletrain= train.sample(1000)\nsns.regplot(x='Day_No',y='Sales',data= sampletrain)","2a143959":"# As we can see from the above graph, We can say that Day_No is not a good estimator","2e846992":"train[['Course_ID','Sales']].boxplot()","f130cb25":"train[['Course_Domain','Sales']].boxplot()","975ca443":"train[['Short_Promotion','Sales']].boxplot()","4a4b43a7":"# From the above graphs we can conclude that data can be normalised before fiting it into a models","e94e024f":"# Lets find the correlation\nplt.subplots(figsize=(10,8))\nsns.heatmap(train.corr(), annot= True)","fdb2162f":"train.corr().unstack().sort_values().drop_duplicates()","8d9f5105":"sns.countplot(train['Course_Domain'])","c533e3f8":"sns.countplot(train['Long_Promotion'])","3d924f19":"sns.countplot(train['Short_Promotion'])","f0282ee0":"sample_train = train[train.Sales>0] # Sales cannot be zero\nsample_train[sample_train['Course_ID']==132][['Day_No','Sales']].plot(x='Day_No',y='Sales',figsize=(16,4))","9dad7eb3":"avg_sales = train.groupby(['Course_ID','Course_Domain','Course_Type','Short_Promotion','Public_Holiday','Long_Promotion'])['Sales'].mean().reset_index()","777ca358":"def day_to_date(dataset):\n    start = date(2018,12,31)\n    dataset['Date'] = dataset['Day_No'].apply(lambda x: start + timedelta(x)) \n\ndef day_month_year(dataset): \n    dataset['Day'] = dataset['Date'].apply(lambda x: x.day)\n    dataset['Month'] = dataset['Date'].apply(lambda x: x.month)\n    dataset['Year'] = dataset['Date'].apply(lambda x: x.year)","43514faf":"day_to_date(train)\nday_month_year(train)\nday_to_date(test)\nday_month_year(test)","faa6756c":"train.drop('User_Traffic',axis =1, inplace = True)","ebc6cdf0":"print(train.columns)\nprint(test.columns)","44185d19":"# merge train and test\ndf = train.append(test)","67cfc090":"df.isna().sum()","83666f4f":"df['Competition_Metric'].fillna(df['Competition_Metric'].median(), inplace = True)","1395c33e":"df1=pd.get_dummies(df,columns=['Course_Domain','Course_Type'],drop_first=True)","f4a815d1":"df1.columns\n","2069cc0d":"df1.drop(['Day_No','Date'],axis = 1,inplace = True)","0ef30487":"#splitting train and test from df\ntrain1= df1[df1['Sales'].isnull()!= True]\ntest1= df1[df1['Sales'].isnull()== True].drop(['Sales'], axis=1)\nprint(train1.shape)\nprint(test1.shape)","c51c28f6":"print(train1.columns)\nprint(test1.columns)","9c503195":"X_train = train1.drop('Sales',axis = 1)\nY_train = train.Sales\nX_test = test1","de4cc90a":"print(X_train.columns)\nprint(Y_train)\nprint(X_test.columns)\n","0b143ec0":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 890)","8768f213":"print(X_train.columns)\nprint(X_test.columns)","74593a5c":"lgb_train = lgb.Dataset(X_tr, y_tr)\nlgb_val = lgb.Dataset(X_val, y_val)","5528fd4e":"from hyperopt import STATUS_OK \nfrom hyperopt import hp \nfrom hyperopt import tpe \nfrom hyperopt import fmin \nfrom hyperopt import Trials\n\nN_FOLDS = 5","8a458437":" from sklearn.metrics import mean_squared_log_error","bc76efa0":"def rmsle(preds, lgb_train): \n    eval_name = 'rmsle' \n    eval_result = np.sqrt(mean_squared_log_error(preds, lgb_train.get_label())) \n    return (eval_name, eval_result*1000, False)","a4fc0762":"def objective(params, n_folds = N_FOLDS): \n    cv_results = lgb.cv(params, lgb_train, num_boost_round = 1000, nfold = 5, feval = rmsle, early_stopping_rounds = 10, seed = 50) \n    best_score = min(cv_results['rmsle-mean']) \n    return {'loss': best_score, 'params': params, 'status': STATUS_OK}","80d4e55f":"space = { 'task': hp.choice('task', ['train']), 'objective': hp.choice('objective', ['gamma']), 'metric' : hp.choice('metric', ['None']), 'boosting': hp.choice('boosting', ['gbdt']), 'learning_rate': hp.loguniform('learning_rate',np.log(0.003), np.log(0.5)), 'num_leaves': hp.choice('num_leaves', range(2, 100, 5)), 'max_depth': hp.choice('max_depth', range(1, 30, 5)), 'bagging_fraction': hp.uniform('bagging_fraction', 0.1, 1.0), 'bagging_freq': hp.choice('bagging_freq', range(1, 10, 1)), 'feature_fraction': hp.uniform('feature_fraction', 0.1, 1.0), 'max_bin': hp.choice('max_bin', range(200, 256, 5)), 'min_data_in_leaf': hp.choice('min_data_in_leaf', range(10, 1000, 1)), 'subsample': hp.uniform('subsample', 0.1, 1.0), 'bagging_seed': hp.choice('bagging_seed', range(1, 10, 1)), 'feature_fraction_seed': hp.choice('feature_fraction_seed', range(1, 10, 1)), }","73e255a7":"evals_result = {} \nparams = {\n        'task': 'train',\n        'objective': 'gamma',\n        'metric' : 'None',\n        'boosting': 'gbdt',\n        'learning_rate': 0.03,\n        'num_leaves': 100,\n        'bagging_fraction': 0.85,\n        'bagging_freq': 1,\n        'bagging_seed': 1,\n        'feature_fraction': 0.9,\n        'feature_fraction_seed': 1,\n        'max_bin': 256,\n        'n_estimators': 1000,\n    }\ndef rmsle(preds, lgb_train):\n    eval_name = \"rmsle\"\n    eval_result = np.sqrt(mean_squared_log_error(preds, lgb_train.get_label()))\n    return (eval_name, eval_result*1000, False)\n\n\ncv_results = lgb.cv(params, lgb_train, num_boost_round = 1000, nfold = 5, feval = rmsle, early_stopping_rounds = 10, verbose_eval = 100, seed = 50)\n\nlgbm_model = lgb.train(params, train_set = lgb_train, valid_sets = lgb_val, feval = rmsle,  evals_result = evals_result, verbose_eval = 100)","749ca205":"predictions = lgbm_model.predict(X_test)\n","ce0b06a9":"len(predictions)","6d373db7":"sample['Sales'] = predictions","90533b84":"sample # Submission 1","f5ccad81":"from sklearn.ensemble import RandomForestRegressor\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor","e71f2034":"def rmsle1000(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true + 1) - np.log1p(y_pred + 1), 2))) *1000","44b94d8e":"from sklearn.model_selection import GridSearchCV,StratifiedKFold","ae984bce":"#xgb\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\n\nX_train = train1.drop('Sales',axis = 1)\nY_train = train.Sales\nX_test = test1\n\ncv_score =[]\ni=1\nfor train_index,test_index in kf.split(X_train, Y_train):\n    print('Fold no. = ', i)\n    \n    x_train, x_test = X_train.loc[train_index], X_train.loc[test_index]\n    y_train, y_test = Y_train.loc[train_index], Y_train.loc[test_index]\n    \n    #model\n    xgb = XGBRegressor(n_estimators= 500)\n    xgb.fit(x_train, y_train)\n    y_pred= xgb.predict(x_test)\n    score = rmsle1000(y_test, y_pred)\n    print('RMSLE score:',score)\n    cv_score.append(score)    \n    \n    i+=1","04c823aa":"xgb.feature_importances_","740067a2":"xgb = XGBRegressor(n_estimators= 500)\nxgb.fit(X_train,Y_train)\nxgb_preds = xgb.predict(X_test)","5a138bcc":"# for submission\nlgbm = LGBMRegressor(n_estimators= 500)\nlgbm.fit(X_train, Y_train)\nlgbm_preds = lgbm.predict(X_test)","fb052c78":"print(xgb_preds)","545dfa88":"print(lgbm_preds)","866bc759":"print(train1.columns)\nprint(test1.columns)","d4fc8139":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()\ntrain = h2o.H2OFrame(train1)\ntest = h2o.H2OFrame(test1)\n","1d60110c":"y = \"Sales\"\nx = list(X_train.columns)\naml = H2OAutoML(max_models = 30, max_runtime_secs=300, seed = 1)\naml.train(x = x, y = y, training_frame = train)\nlb = aml.leaderboard\nlb.head()\nlb.head(rows=lb.nrows)\npreds = aml.predict(test)\nwater_preds=h2o.as_list(preds) ","e84cec47":"water_preds","c21cd18c":"sample.rename(columns = {'Sales':'Sales_by_lgbm'},inplace = True)","11094e2c":"sample['XG Boost'] = xgb_preds","cbf98fe8":"sample['LightGBM'] = lgbm_preds","1cc7bc17":"sample['H20'] = water_preds","723480d6":"#Predictions of various modles\nsample","8c8572a7":"Adding a new feature by using Day_No ","3e831ab9":"Since test doesnt contain User Traffic, we are removing it"}}