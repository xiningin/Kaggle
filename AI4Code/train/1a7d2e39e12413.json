{"cell_type":{"d4bfaab6":"code","f45c9678":"code","bfa6c274":"code","332f81ca":"code","15219cd0":"code","37280e7c":"code","f4ee29c5":"code","3aa43c85":"code","31867e3d":"code","580e969b":"code","a1292802":"code","cebf85a9":"code","224e5f05":"markdown","b838db29":"markdown","a7097cd2":"markdown","3d8a5efa":"markdown"},"source":{"d4bfaab6":"!pip install tensorflow-gpu==1.15","f45c9678":"import tensorflow as tf\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport bz2\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","bfa6c274":"stream = bz2.BZ2File('\/kaggle\/input\/amazonreviews\/train.ft.txt.bz2')\n\ndata = stream.readlines()\nprint(data[:5])","332f81ca":"corpus = []\nlabels = []\nfor s in data[:10000]:\n    s = s.decode('utf-8')\n    labels.append(s[:10])\n    corpus.append(s[11:-1])\n    \nprint(corpus[:5])\nprint(labels[:5])\n    ","15219cd0":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus).toarray()","37280e7c":"(xrows, xcols) = X.shape\nprint(xrows, xcols)","f4ee29c5":"Y = []\nfor l in labels:\n    if(l=='__label__1'):\n        Y.append(0)\n    else:\n        Y.append(1)\nY = np.array(Y)\n(yrows,) = Y.shape\nY = Y.reshape(yrows,1)\nprint(yrows)","3aa43c85":"x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.3)","31867e3d":"import tensorflow as tf\nwith tf.name_scope('placeholders'):\n    x = tf.placeholder(tf.float32, (None, xcols))\n    y = tf.placeholder(tf.float32, (None,1))\n    \nwith tf.name_scope('weights'):\n    W = tf.Variable(tf.random.normal((xcols,1)))\n    b = tf.Variable(tf.random_normal((1,)))\n\nwith tf.name_scope('prediction'):\n    y_logit = tf.add(tf.matmul(x,W),b)\n    y_one_prob = tf.sigmoid(y_logit)\n    y_pred = tf.round(y_one_prob)\n\nwith tf.name_scope('loss'):\n    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit, labels=y)\n    l = tf.reduce_sum(entropy)\n    \nwith tf.name_scope('optimizer'):\n    train_op = tf.train.AdamOptimizer(1).minimize(l)\n\n    \nn_steps = 10\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    # Train model\n    for i in range(n_steps):\n        feed_dict = {x: x_train, y: y_train}\n        _,loss = sess.run([train_op, l], feed_dict=feed_dict)\n        print(\"loss: \",loss)\n    # Test model\n    from sklearn.metrics import accuracy_score\n    feed_dict = {x: x_test}\n    pred_op = sess.run(y_pred, feed_dict=feed_dict)\n    score = accuracy_score(pred_op, y_test)\n    print('Score: ',score)\n\n    ","580e969b":"from numpy import array\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\ndef max_length(reviews):\n    return max(len(d.split()) for d in reviews)\n\ndef create_io(tokenizer, max_length, reviews, labels):\n    X, Y = list(), list()\n    # walk through each review\n    i=0\n    for desc in reviews:\n        # encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        # split one sequence into multiple X,y pairs\n        # pad input sequence\n        in_seq = pad_sequences([seq], maxlen=max_length)[0]\n        # store\n        X.append(in_seq)\n        if(labels[i]=='__label__1'):\n            Y.append([1,0])\n        else:\n            Y.append([0,1])\n        i+=1\n    return array(X), array(Y)\n\ndef create_tokenizer(reviews):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(reviews)\n    return tokenizer\n\nrevs = corpus.copy()\n\nmax_len = int(max_length(revs))\n\ntokenizer = create_tokenizer(revs)\n\nX, Y = create_io(tokenizer, max_len, revs, labels)\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.3)\n\nprint('Done!')","a1292802":"vocab_size = len(tokenizer.word_index) + 1\nembed_size = 100\ntime_steps = max_len\nhidden_layers = 500\nclasses = 2\n\ndef rnn_step(previous_hidden_state,x):\n    current_hidden_state = tf.tanh(\n    tf.matmul(previous_hidden_state, Wh) +\n    tf.matmul(x, Wx) + b_rnn)\n    return current_hidden_state\n\nwith tf.name_scope('input'):\n    x = tf.placeholder(tf.int64, (None, max_len))\n    batchSize = tf.shape(x)[0]\n    print('batch size: ', batchSize)\n    y = tf.placeholder(tf.float32, (None, classes))\n\nwith tf.name_scope('embedding'):\n    embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_size]))\n    inputs = tf.nn.embedding_lookup(embedding, x)\n\nwith tf.name_scope('rnn_weights'):\n    with tf.name_scope(\"W_x\"):\n        Wx = tf.Variable(tf.zeros([embed_size, hidden_layers]))\n    with tf.name_scope(\"W_h\"):\n        Wh = tf.Variable(tf.zeros([hidden_layers, hidden_layers]))\n    with tf.name_scope(\"Bias\"):\n        b_rnn = tf.Variable(tf.zeros([hidden_layers]))\n        \nwith tf.name_scope('rnn_step'):\n    processed_input = tf.transpose(inputs, perm=[1, 0, 2])\n    initial_hidden = tf.zeros([batchSize,hidden_layers])\n    all_hidden_states = tf.scan(rnn_step,\n                        processed_input,\n                        initializer=initial_hidden,\n                        name='states')\n    \ndef get_linear_layer(hidden_state):\n    return tf.matmul(hidden_state, Wl) + bl\n\nwith tf.name_scope('linear_layer_weights') as scope:\n    with tf.name_scope(\"W_linear\"):\n        Wl = tf.Variable(tf.truncated_normal([hidden_layers,\n        classes],\n        mean=0,stddev=.01))\n    with tf.name_scope(\"Bias_linear\"):\n        bl = tf.Variable(tf.truncated_normal([classes],\n        mean=0,stddev=.01))\n    all_outputs = tf.map_fn(get_linear_layer, all_hidden_states)\n    output = all_outputs[-1]\n    \nwith tf.name_scope('cross_entropy'):\n    cross_entropy = tf.reduce_mean(\n    tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n    \nwith tf.name_scope('train'):\n    train_step = tf.train.AdamOptimizer()\\\n    .minimize(cross_entropy)\n    \nwith tf.name_scope('accuracy'):\n    correct_prediction = tf.equal(\n    tf.argmax(y,1), tf.argmax(output,1))\n    accuracy = (tf.reduce_mean(\n    tf.cast(correct_prediction, tf.float32)))*100\n    \nepochs = 10\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    # Train model\n    for i in range(epochs):\n        feed_dict = {x: x_train, y: y_train}\n        _, acc, loss = sess.run([train_step, accuracy, cross_entropy], feed_dict=feed_dict)\n        print('Training loss: ',loss, ' accuracy: ', acc)\n    \n    # Test Model\n    feed_dict = {x:x_test, y:y_test}\n    acc, loss = sess.run([accuracy, cross_entropy], feed_dict=feed_dict)\n    print('Test loss: ', loss, ' accuracy: ',acc)\n    \nprint('Done!')","cebf85a9":"vocab_size = len(tokenizer.word_index) + 1\nembed_size = 100\ntime_steps = max_len\nhidden_layers = 50\nclasses = 2\nlstm_layers = 10\n\ng = tf.Graph()\n\nwith g.as_default():\n    with tf.name_scope('input'):\n        x = tf.placeholder(tf.int64, (None, max_len))\n        print(x)\n        batchSize = tf.shape(x)[0]\n        print('batch size: ', batchSize)\n        y = tf.placeholder(tf.float32, (None, classes))\n        print(y)\n\n    with tf.name_scope('embedding'):\n        embedding = tf.Variable(tf.truncated_normal([vocab_size, embed_size]))\n        print(embedding)\n        inputs = tf.nn.embedding_lookup(embedding, x)\n        print(inputs)\n\n    with tf.name_scope('lstm_cell'):\n        lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layers,\n        forget_bias=1.0)\n        outputs, states = tf.nn.dynamic_rnn(lstm_cell, inputs,                                    \n        dtype=tf.float32)\n        print(outputs, ', ',states)\n\n    with tf.name_scope('linear_layer_weights') as scope:\n        with tf.name_scope(\"W_linear\"):\n            Wl = tf.Variable(tf.truncated_normal([hidden_layers,\n            classes],\n            mean=0,stddev=.01))\n            print(Wl)\n        with tf.name_scope(\"Bias_linear\"):\n            bl = tf.Variable(tf.truncated_normal([classes],\n            mean=0,stddev=.01))\n            print(bl)\n        output = tf.matmul(states[1],Wl) + bl\n        print(output)\n\n    with tf.name_scope('cross_entropy'):\n        cross_entropy = tf.reduce_mean(\\\n        tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y))\n        print(cross_entropy)\n\n    with tf.name_scope('train'):\n        train_step = tf.train.AdamOptimizer()\\\n        .minimize(cross_entropy)\n\n    with tf.name_scope('accuracy'):\n        correct_prediction = tf.equal(\n        tf.argmax(y,1), tf.argmax(output,1))\n        print(correct_prediction)\n        accuracy = (tf.reduce_mean(\n        tf.cast(correct_prediction, tf.float32)))*100\n        print(accuracy)\n\n    epochs = 10\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        # Train model\n        for i in range(epochs):\n            feed_dict = {x: x_train, y: y_train}\n            _, acc, loss = sess.run([train_step, accuracy, cross_entropy], feed_dict=feed_dict)\n            print('Training loss: ',loss, ' accuracy: ', acc)\n\n        # Test Model\n        feed_dict = {x:x_test, y:y_test}\n        acc, loss = sess.run([accuracy, cross_entropy], feed_dict=feed_dict)\n        print('Test loss: ', loss, ' accuracy: ',acc)\n\nprint('Done!')","224e5f05":"### Data Preprocessing for RNN","b838db29":"### Logistic Regression - Benchmarking","a7097cd2":"## LSTM Model","3d8a5efa":"### Vanilla RNN Model"}}