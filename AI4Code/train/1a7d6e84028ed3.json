{"cell_type":{"264580fd":"code","91c6af6b":"code","1757bada":"code","6756ace6":"code","74fe7b2c":"code","2d60d980":"code","320c1f67":"code","17211bec":"code","8b363866":"code","9a88865e":"code","7f886a08":"code","dc1db08f":"code","945f461b":"code","1e7615fb":"code","2a787138":"code","4e4a5bbc":"code","69181eed":"code","c3835a1e":"code","a9045a54":"code","0b549098":"code","01ba6962":"code","aa0950fc":"code","e994f169":"code","2f22dead":"code","d05cdf71":"code","337a0815":"code","507ce9fe":"code","ed24913f":"code","c879ae8c":"code","70b8c993":"code","67e27540":"code","b793cb4d":"code","a60ce4fb":"code","a95c333a":"code","09d80f39":"code","f9b53105":"code","3ec68e52":"markdown","97f0ab8e":"markdown","5ee29f66":"markdown","78a3864d":"markdown","e48db247":"markdown","88f7a9d1":"markdown","80de3f91":"markdown","85370fa6":"markdown","d76afc48":"markdown","a4740f77":"markdown","39023ca9":"markdown","0e5f2a98":"markdown","dd3841b0":"markdown","cbc23066":"markdown"},"source":{"264580fd":"import sys\nsys.path.append('..\/input\/autokeras-april-2021')","91c6af6b":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom scipy.stats import pearsonr, spearmanr, kendalltau\nfrom tqdm.autonotebook import tqdm\nfrom collections import Counter\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport string\nimport re\npd.set_option('display.max_columns', None)\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\n\n# NLP\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n\n# Machine Learning\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, RepeatedKFold\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#Models\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom lightgbm import LGBMRegressor\nimport autokeras as ak\n#Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything()","1757bada":"data_dir = '..\/input\/commonlitreadabilityprize'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","6756ace6":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","74fe7b2c":"train_df.sample(10)","2d60d980":"train_df.describe().T","320c1f67":"test_df.head()","17211bec":"sub_df.head()","8b363866":"word_count = [len(x.split()) for x in train_df['excerpt'].tolist()]\nbarplot_dim = (12, 6)\nax = plt.subplots(figsize =barplot_dim);\nax = sns.distplot(word_count, kde=False);\nax.set_ylabel('No. of Observations', size=15)\nax.set_xlabel('No. of Words', size=15)\nax.set_title('Word Count Distribution', size=20);","9a88865e":"num_bins = 1 + (3.322*np.log10(train_df.shape[0])) #Sturge\u2019s Rule\nprint(f'Number of bins: {num_bins}')\ntrain_df['target_binned'] = pd.cut(train_df['target'], bins=int(num_bins), labels=False)","7f886a08":"barplot_dim = (20, 30)\nplt.figure(figsize=barplot_dim)\nfor i in range(int(num_bins)):\n    temp_df = train_df[train_df['target_binned'] == i]\n    word_count = [len(x.split()) for x in temp_df['excerpt'].tolist()]\n    plt.subplot(4, 3, i+1)\n    ax = sns.distplot(word_count, kde=False);\n    ax.set_ylabel('No. of Observations', size=15)\n    ax.set_xlabel('No. of Words', size=15)\n    ax.set_title(f'Word Count Distribution (Bin: {i})', size=20);\n    plt.xlim([140, 220])\nplt.show();","dc1db08f":"train_df['excerpt_word_count'] = train_df['excerpt'].apply(lambda x: len(x.split()))\n\npearson_corr, _ = pearsonr(train_df['excerpt_word_count'], train_df['target'])\nspearman_corr, _ = spearmanr(train_df['excerpt_word_count'], train_df['target'])\ntau_corr, _ = kendalltau(train_df['excerpt_word_count'], train_df['target'])\n\nprint('Pearsons correlation: %.3f' % pearson_corr)\nprint('Spearmans correlation: %.3f' % spearman_corr)\nprint('Kendall Tau correlation: %.3f' % tau_corr)","945f461b":"temp_df = train_df[(train_df['target_binned'] >= 0) & (train_df['target_binned'] < 3)]\n\ntext = ' '.join(temp_df['excerpt'])\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","1e7615fb":"temp_df = train_df[train_df['target_binned'] >= 9]\n\ntext = ' '.join(temp_df['excerpt'])\nwordcloud = WordCloud(background_color='white', stopwords=STOPWORDS, width=2560, height=1440).generate(text)\n\nbarplot_dim = (15, 15)\nax = plt.subplots(figsize=barplot_dim, facecolor='w')\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","2a787138":"def text_cleaning(text):\n    '''\n    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n    text - Sentence that needs to be cleaned\n    '''\n    text = ''.join([k for k in text if k not in string.punctuation])\n    text = str(text).lower()\n    text = re.sub('[^a-zA-Z]', ' ', text)\n    text = re.sub(' +', ' ', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    return text","4e4a5bbc":"tqdm.pandas()\ntrain_df['excerpt'] = train_df['excerpt'].progress_apply(text_cleaning)","69181eed":"test_df['excerpt'] = test_df['excerpt'].progress_apply(text_cleaning)","c3835a1e":"def prepare_text(text, nlp=nlp):\n    '''\n    Returns the text after stop-word removal and lemmatization.\n    text - Sentence to be processed\n    nlp - Spacy NLP model\n    '''\n    doc = nlp(text)\n    lemma_list = [token.lemma_ for token in doc if not token.is_stop]\n    lemmatized_sentence = ' '.join(lemma_list)\n        \n    return lemmatized_sentence","a9045a54":"train_df['excerpt'] = train_df['excerpt'].progress_apply(prepare_text)","0b549098":"test_df['excerpt'] = test_df['excerpt'].progress_apply(prepare_text)","01ba6962":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nNUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.target_binned.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","aa0950fc":"train_df = train_df[['id', 'excerpt', 'target', 'kfold']]","e994f169":"doc = nlp.pipe(train_df['excerpt'])\nx_train_stv = np.array([text.vector for text in doc])\ndoc = nlp.pipe(test_df['excerpt'])\nx_test_stv = np.array([text.vector for text in doc])","2f22dead":"def get_stacking():\n    level0 = []\n    level0.append(('knn', KNeighborsRegressor()))\n    level0.append(('svr', SVR()))\n    level0.append(('Ridge', Ridge()))\n    level0.append(('LR', LinearRegression()))\n    level0.append(('RF', RandomForestRegressor(random_state=42)))\n    level0.append(('Lgbm', LGBMRegressor(metric='rmse',\n                                         objective='regression',\n                                         learning_rate=0.01,\n                                         seed=42)))\n    \n    level1 = LinearRegression()\n    model = StackingRegressor(estimators=level0, final_estimator=level1, cv=None)\n    return model","d05cdf71":"def get_models():\n    models = dict()\n    models['knn'] = KNeighborsRegressor()\n    models['svr'] = SVR()\n    models['Ridge'] = Ridge()\n    models['LR'] = LinearRegression()\n    models['RF'] = RandomForestRegressor()\n    models['Lgbm'] = LGBMRegressor()\n    models['Stacked'] = get_stacking()\n    \n    return models","337a0815":"def evaluate_model(model, X, y):\n    cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","507ce9fe":"%%time\n\nX = x_train_stv\ny = np.array(train_df['target'])\n\nmodels = get_models()\nresults = []\nnames = []\n\nfor name, model in models.items():\n    scores = -evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} : {round(np.mean(scores),3)} ({round(np.std(scores),3)})')","ed24913f":"ax = plt.subplots(figsize=(12, 6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","c879ae8c":"mean_scores = []\nfor score in results:\n    mean_scores.append(round(np.mean(score),3))\nmin_index = mean_scores.index(min(mean_scores))\nmodel_name = names[min_index]","70b8c993":"print(f'Best Score: {mean_scores[min_index]}')\nprint(f'Best Model: {model_name}')","67e27540":"%%time\n\nscores = []\n\nfor i in range(NUM_SPLITS):\n    train = train_df[train_df['kfold'] != i].copy()\n    valid = train_df[train_df['kfold'] == i].copy()\n    \n    train['excerpt'] = train['excerpt'].apply(lambda x: '<START> ' + x + ' <END>')\n    valid['excerpt'] = valid['excerpt'].apply(lambda x: '<START> ' + x + ' <END>')\n    \n    auto_reg = ak.TextRegressor(overwrite=True, max_trials=4)\n    auto_reg.fit(train['excerpt'].values, train['target'].values, epochs=10)\n    preds = auto_reg.predict(valid['excerpt'].values)\n    \n    loss = mean_squared_error(valid['target'].values, preds, squared=False)\n    scores.append(loss)\n    print(f'Fold {i+1}: {loss}')\nprint('')\nprint(f'Mean Loss: {np.mean(scores)}')","b793cb4d":"mean_scores = []\nfor score in results:\n    mean_scores.append(round(np.mean(score),3))\nmin_index = mean_scores.index(min(mean_scores))\nmodel_name = names[min_index]","a60ce4fb":"print(f'Best Score: {mean_scores[min_index]}')\nprint(f'Best Model: {model_name}')","a95c333a":"%%time\n\n#ML\ndoc = nlp.pipe(train_df['excerpt'])\nx_train_stv = np.array([text.vector for text in doc])\ndoc = nlp.pipe(test_df['excerpt'])\nx_test_stv = np.array([text.vector for text in doc])\n\nmodels = get_models()\nreg = models[model_name]\nX = x_train_stv\ny = np.array(train_df['target'])\nreg.fit(X, y)\npreds_ml = reg.predict(x_test_stv)\n\n#Auto Keras\ntrain_df['excerpt'] = train_df['excerpt'].apply(lambda x: '<START> ' + x + ' <END>')\ntest_df['excerpt'] = test_df['excerpt'].apply(lambda x: '<START> ' + x + ' <END>')\nauto_reg = ak.TextRegressor(overwrite=True, max_trials=4)\nauto_reg.fit(train_df['excerpt'].values, train_df['target'].values, epochs=10)\npreds_ak = auto_reg.predict(test_df['excerpt'].values)\n\n# Final Prediction\nweights = [5, 1]\npreds = (weights[0]*preds_ml + weights[1]*preds_ak.squeeze())\/(sum(weights))\n\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['target'] = preds","09d80f39":"submission.head()","f9b53105":"submission.to_csv(\"submission.csv\",index=False)","3ec68e52":"## 2. Auto Keras","97f0ab8e":"# EDA","5ee29f66":"# Vectorization","78a3864d":"Let's see how the word count varies across each range of readability (target):-","e48db247":"## Word Count Distribution","88f7a9d1":"# Submission","80de3f91":"# Text Preparation","85370fa6":"So there isn't anything apparent when we look at the words in isolation as well.  \nThus, a **sequential method will work on this dataset as comapred to any bag-of-words method**.","d76afc48":"**As we can infer from this, the word count is not that much relevant to the readability of the excerpt.**  \nLet's look at excerpts with high scores and low scores in word cloud and see if something jumps out...","a4740f77":"# About This Notebook\nThis is a first run through the competition data to try and understand the datatset and realise the problem at hand with some quick EDA and classical ML methods.  \n**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a\n\n# Problem Statement\n* Currently, most educational texts are matched to readers using traditional readability methods or commercially available formulas.\n* Tools like Flesch-Kincaid Grade Level are based on weak proxies of text decoding (i.e., characters or syllables per word) and syntactic complexity (i.e., number or words per sentence).\n* They lack construct and theoretical validity.\n* Commercially available formulas, such as Lexile, can be cost-prohibitive, lack suitable validation studies, and suffer from transparency issues when the formula's features aren't publicly available.\n\n# Why this competition?\nAs evident from the problem statement, this competition prsents an interesting angle to the use of NLP and has the potential to make real life contribution\/change.  \n\nIf successful, you'll aid administrators, teachers, and students. Literacy curriculum developers and teachers who choose passages will be able to quickly and accurately evaluate works for their classrooms. Plus, these models will become more accessible for all. Perhaps most importantly, students will benefit from feedback on the complexity and readability of their work, making it far easier to improve essential reading skills.\n\n# Expected Outcome\nLoosely speaking, ***Given an excerpt of text, we need to rate the complexity of reading passages for grade 3-12 classroom use.***\n\n# Data Description\nThe training file contains the following features:-\n* `id` - unique ID for excerpt\n* `url_legal` - URL of source - this is blank in the test set.\n* `license` - license of source material - this is blank in the test set.\n* `excerpt` - text to predict reading ease of\n* `target` - reading ease\n* `standard_error` - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n# Grading Metric\nSubmissions are scored on the root mean squared error. RMSE is defined as:  \n$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n\nwhere $ \\hat{y} $ is the predicted value, $ y $ is the original value, and $ n $ is the number of rows in the test data.\n\n# Problem Category:-\nFrom the data and objective its is evident that this is a **Regression Problem** in the NLP Domain.\n\nSo without further ado, let's now start with some basic imports to take us through this:-","39023ca9":"**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","0e5f2a98":"# KFolds","dd3841b0":"# Models\n\n## 1. Classical ML Models\nAlthough we have already established that sequence models will perform better on this task, let's create a quick and dirty bag-of-words model just as a baseline.","cbc23066":"# Text Cleaning"}}