{"cell_type":{"8891829a":"code","788e2944":"code","9f95bb70":"code","f1b050f1":"code","c618b3e9":"code","68eb14c1":"code","a00f2cbb":"code","aa34893b":"code","7024dbdc":"code","f4097916":"code","d308587f":"code","acd34854":"code","671f8bc8":"code","b168c879":"code","00413877":"code","b96023ee":"code","ad880c2f":"code","672f13d5":"code","bc27f302":"code","3d31f8a5":"code","7e23fd11":"code","db1e37d9":"code","f9d5ae09":"code","598913b5":"markdown","b0f480cf":"markdown","fd65df5e":"markdown","e60e4533":"markdown","475f0d1f":"markdown","8d9e2baf":"markdown","499d92b1":"markdown","5988b0c6":"markdown","302eef76":"markdown","f7cc59a0":"markdown","474c072d":"markdown","704d1946":"markdown","ce489bd8":"markdown","4991944b":"markdown","11d2b68e":"markdown","fec8d9a5":"markdown","325ca5ef":"markdown","1fdf3e1a":"markdown","1f8cc6d4":"markdown","98746067":"markdown","c09074f6":"markdown","ffd0edbb":"markdown","52962fbc":"markdown","ce0df1d0":"markdown","5bc5351b":"markdown"},"source":{"8891829a":"# Credits: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nnb_epoch = 12","788e2944":"# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","9f95bb70":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n \nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","f1b050f1":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","c618b3e9":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(5, 5),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (5, 5), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","68eb14c1":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","a00f2cbb":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(7, 7),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (7, 7), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","aa34893b":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","7024dbdc":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","f4097916":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","d308587f":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(5, 5)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","acd34854":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","671f8bc8":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","b168c879":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","00413877":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","b96023ee":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","ad880c2f":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.6))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","672f13d5":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","bc27f302":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(20, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(40, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","3d31f8a5":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","7e23fd11":"%%time\nmodel = Sequential()\nmodel.add(Conv2D(40, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory = model.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))","db1e37d9":"score = model.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\nfig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,nb_epoch+1))\n\nvy = history.history['val_loss']\nty = history.history['loss']\nplt_dynamic(x, vy, ty, ax)","f9d5ae09":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Model description\",  \"test loss\", \"test accuracy\"]\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 2x2 \\n Dropout 0.25 \\n Flatten \\n Dense Dropout 0.5\", 0.0302, .9904])\nx.add_row([\"Conv 2D 5x5 32\\n Conv 2D 5x5 64\\n Maxpool 2x2 \\n Dropout 0.25 \\n Flatten \\n Dense Dropout 0.5\", 0.0231, .9926])\nx.add_row([\"Conv 2D 7x7 32\\n Conv 2D 7x7 64\\n Maxpool 2x2 \\n Dropout 0.25 \\n Flatten \\n Dense Dropout 0.5\", 0.0227, .9937])\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 3x3 \\n Dropout 0.25 \\n Flatten \\n Dense Dropout 0.5\", 0.0275, .9924])\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 5x5 \\n Dropout 0.25 \\n Flatten \\n Dense Dropout 0.5\", 0.0284, .9925])\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 2x2 \\n Dropout 0.40 \\n Flatten \\n Dense Dropout 0.5\", 0.0294, .9908])\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 2x2 \\n Dropout 0.50 \\n Flatten \\n Dense Dropout 0.5\", 0.0305, .9908])\nx.add_row([\"Conv 2D 3x3 32\\n Conv 2D 3x3 64\\n Maxpool 2x2 \\n Dropout 0.60 \\n Flatten \\n Dense Dropout 0.5\", 0.0285, .9904])\nx.add_row([\"Conv 2D 3x3 20\\n Conv 2D 3x3 40\\n Maxpool 2x2 \\n Dropout 0.60 \\n Flatten \\n Dense Dropout 0.5\", 0.0285, .9904])\nx.add_row([\"Conv 2D 3x3 40\\n Conv 2D 3x3 20\\n Maxpool 2x2 \\n Dropout 0.60 \\n Flatten \\n Dense Dropout 0.5\", 0.0285, .9904])\n\nprint(x)","598913b5":"- Increased dropout rate after the first maxpool and it acted negetively again\n- The performance is bad compared to old model with 0.25 dropout rate","b0f480cf":"- Test loss seems to reduce eventually\n- At the end of 10 epochs test loss slightly increased and again reduced.","fd65df5e":"- There is a slight decrease in loss after increasing the convolution kernel size from 5x5 to 7x7","e60e4533":"- Increased dropout rate after the first maxpool and it acted negetively.\n- The performance is bad compared to old model with 0.25 dropout rate","475f0d1f":"## Conclusion","8d9e2baf":"- Increaseing the maxpool kernel from 2x2 to 3x3 has improved the accuracy and reduced loss significantly","499d92b1":"- Reduced the number of neurons in the first and second convolution layer. \n- The performance has reduced compared to the first model.\n- Valiation loss is worse compared to all prior models.","5988b0c6":"## Experiment 4\n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 3x3 + Droup out 0.25 + Flatten + Drop out 0.5","302eef76":"- Increased the number of neurons in the first and reduced the number of neurons in the second convolution layer. \n- The performance has improved slightly compared to the previous model..\n- Valiation loss is inproved compared to prior model.","f7cc59a0":"- Increasing convolutional kernel size has shown an increase in accuray and better reducution in validation loss\n- Increasing maxpool kernel size has negative impact on loss and accuracy\n- Increasing the dropout rate wont simply improve the performance of the model. I has negative effect. Need to do hyper parameter tuning to find best value.\n- More nuber of neurons in convolution layers show better performance than less number of neurons\n","474c072d":"## Experiment 7\n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Droup out 0.6 + Flatten + Drop out 0.5","704d1946":"- Increased dropout rate after the first maxpool and it acted negetively\n- Loss and accuracy is worse than all previous models\n- The performance is bad compared to old model with 0.25 dropout rate","ce489bd8":"## Experiment 2 \n### 2D Conv 5x5 Kernel o\/p 32 + 2D Conv 5x5 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Droup out 0.25 + Flatten + Drop out 0.5","4991944b":"## Experiment 9\n### 2D Conv 3x3 Kernel o\/p 40 + 2D Conv 3x3 Kernel o\/p 20 + Max Pooling Kernel 2x2 + Droup out 0.25 + Flatten + Drop out 0.5","11d2b68e":"## Importing libraries","fec8d9a5":"There are several parameters to a convolutional neural network. I am experimenting with some parameters of the CNN like kernel size of convolutional layer, maxpool layer, dropouts and number of neurons in each convolutional layer to see if there is any change in the performance of the model with a change in above mentioned parameters.\n\nI have done similar experements in simple mlp  you can see them in the below link. Feel free to share your thoughts about these experements.\n\n[https:\/\/www.kaggle.com\/vishnurapps\/experimenting-with-neural-networks](https:\/\/www.kaggle.com\/vishnurapps\/experimenting-with-neural-networks)\n","325ca5ef":"- Increaseing the maxpool kernel from 3x3 to 5x5 has no advantage\n- There is no change in the loss or accuracy compared with the 2x2 maxpool kernel","1fdf3e1a":"## Experiment 8\n### 2D Conv 3x3 Kernel o\/p 20 + 2D Conv 3x3 Kernel o\/p 40 + Max Pooling Kernel 2x2 + Droup out 0.25 + Flatten + Drop out 0.5","1f8cc6d4":"## Experiment 1 \n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Drop out 0.25 + Flatten + Drop out 0.5","98746067":"![image.png](attachment:image.png)","c09074f6":"- There is a slight imporvement in accuracy and slight decrease in loss after increasing the convolution kernel size from 3x3 to 5x5","ffd0edbb":"## Experiment 3\n### 2D Conv 7x7 Kernel o\/p 32 + 2D Conv 7x7 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Droup out 0.25 + Flatten + Drop out 0.5","52962fbc":"## Experiment 5\n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Droup out 0.4 + Flatten + Drop out 0.5","ce0df1d0":"## Experiment 5\n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 5x5 + Droup out 0.25 + Flatten + Drop out 0.5","5bc5351b":"## Experiment 6\n### 2D Conv 3x3 Kernel o\/p 32 + 2D Conv 3x3 Kernel o\/p 64 + Max Pooling Kernel 2x2 + Droup out 0.5 + Flatten + Drop out 0.5"}}