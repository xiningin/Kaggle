{"cell_type":{"b0cebe63":"code","5cb0faaa":"code","7f59422c":"code","b7786919":"code","acbdf5b7":"code","18411844":"code","b8c6fb2e":"code","d2ed2d7a":"code","10d32ca2":"code","2a2b587e":"code","b39c0e22":"code","aae7f2ac":"code","86ffae21":"code","848cee4e":"code","cec0505b":"code","5cb54832":"code","925b0dc9":"code","444971be":"code","216eadfc":"code","2e82fde8":"code","9354c2c4":"markdown"},"source":{"b0cebe63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5cb0faaa":"from pathlib import Path\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Dropout, Dense, Flatten, BatchNormalization, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.applications.resnet50 import ResNet50\n\nfrom sklearn.model_selection import train_test_split","7f59422c":"lr_schedule = LearningRateScheduler(lambda epoch: 1e-6 * 2**(epoch\/2))","b7786919":"train_path = Path.cwd()\/'..'\/'input'\/'Kannada-MNIST'\/'train.csv'\n\nall_data = pd.read_csv(train_path)","acbdf5b7":"y = all_data.label\nx = all_data.iloc[:,1:].values\nx = x.reshape(x.shape[0], 28, 28, 1)","18411844":"x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=10000,\n                                                     random_state=42, shuffle=True,\n                                                     stratify=y)","b8c6fb2e":"train_datagen = ImageDataGenerator(rescale=1.\/255.,\n                                   rotation_range=10,\n                                   width_shift_range=0.25,\n                                   height_shift_range=0.25,\n                                   shear_range=0.1,\n                                   zoom_range=0.25,\n                                   horizontal_flip=False)\n\nvalid_datagen = ImageDataGenerator(rescale=1.\/255.)\n\ntrain_generator = train_datagen.flow(x_train, y_train, batch_size=1024)\nvalid_generator = valid_datagen.flow(x_valid, y_valid)","d2ed2d7a":"def plot_history(history):\n    #-----------------------------------------------------------\n    # Retrieve a list of list results on training and test data\n    # sets for each training epoch\n    #-----------------------------------------------------------\n    acc      = history.history[     'accuracy' ]\n    val_acc  = history.history[ 'val_accuracy' ]\n    loss     = history.history[    'loss' ]\n    val_loss = history.history['val_loss' ]\n\n    epochs   = range(len(acc)) # Get number of epochs\n\n    #------------------------------------------------\n    # Plot training and validation accuracy per epoch\n    #------------------------------------------------\n    plt.plot  ( epochs,     acc , label='Training')\n    plt.plot  ( epochs, val_acc , label='Validation')\n    plt.xlabel ('Epoch')\n    plt.ylabel ('Accuracy')\n    plt.legend ()\n    plt.title ('Training and validation accuracy')\n    plt.figure()\n\n    #------------------------------------------------\n    # Plot training and validation loss per epoch\n    #------------------------------------------------\n    plt.plot  ( epochs,     loss, label='Training')\n    plt.plot  ( epochs, val_loss, label='Validation')\n    plt.xlabel ('Epoch')\n    plt.ylabel ('Loss')\n    plt.legend ()\n    plt.title ('Training and validation loss'   )","10d32ca2":"def report(model, history=None, validation_generator=None):\n    if history is not None:\n        plot_history(history)\n    \n    if validation_generator is not None:\n        # Evaluate trained model on validation set\n        validation_generator.reset()\n        [val_loss, val_acc] = model.evaluate_generator(validation_generator)\n        print('Model evaluation')\n        print(f'val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n        print()","2a2b587e":"def plot_learning_rate_history(history, steps, rate_sched, axisrange):\n    lrs = [rate_sched(n) for n in range(steps)]\n    plt.semilogx(lrs, history.history[\"loss\"])\n    plt.axis(axisrange)\n    plt.xlabel('Learning rate')\n    plt.ylabel('Loss')\n    plt.show()","b39c0e22":"from tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\n\nclass LR_Updater(Callback):\n    '''This callback is utilized to log learning rates every iteration (batch cycle)\n    it is not meant to be directly used as a callback but extended by other callbacks\n    ie. LR_Cycle\n    '''\n    def __init__(self, epoch_iterations):\n        '''\n        iterations = training batches\n        epoch_iterations = number of batches in one full training cycle\n        '''\n        self.epoch_iterations = epoch_iterations\n        self.trn_iterations = 0.\n        self.history = {}\n    def on_train_begin(self, logs={}):\n        self.trn_iterations = 0.\n        logs = logs or {}\n    def on_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        self.trn_iterations += 1\n        K.set_value(self.model.optimizer.lr, self.setRate())\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\nclass LR_Cycle(LR_Updater):\n    '''This callback is utilized to implement cyclical learning rates\n    it is based on this pytorch implementation https:\/\/github.com\/fastai\/fastai\/blob\/master\/fastai\n    and adopted from this keras implementation https:\/\/github.com\/bckenstler\/CLR\n    '''\n    \n    def __init__(self, iterations, cycle_mult = 1):\n        '''\n        iterations = initial number of iterations in one annealing cycle\n        cycle_mult = used to increase the cycle length cycle_mult times after every cycle\n        for example: cycle_mult = 2 doubles the length of the cycle at the end of each cy$\n        '''\n        self.min_lr = 0\n        self.cycle_mult = cycle_mult\n        self.cycle_iterations = 0.\n        super().__init__(iterations)\n    \n    def setRate(self):\n        self.cycle_iterations += 1\n        if self.cycle_iterations == self.epoch_iterations:\n            self.cycle_iterations = 0.\n            self.epoch_iterations *= self.cycle_mult\n        decay_phase = np.pi*self.cycle_iterations\/self.epoch_iterations\n        decay = (np.cos(decay_phase) + 1.) \/ 2.\n        return self.max_lr * decay\n    \n    def on_train_begin(self, logs={}):\n        super().on_train_begin(logs={})\n        self.cycle_iterations = 0.\n        self.max_lr = K.get_value(self.model.optimizer.lr)","aae7f2ac":"def build_model(optimizer=Adam()):\n    model = Sequential()\n\n    model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size=5, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(128, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, kernel_size=5, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    model.add(Conv2D(256, kernel_size=3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.2))\n\n    model.add(Flatten())\n    model.add(Dense(256))\n    model.add(BatchNormalization())\n    model.add(Dense(128))\n    model.add(BatchNormalization())\n    model.add(Dense(10, activation='softmax'))\n    \n    model.compile(loss='sparse_categorical_crossentropy',\n              optimizer=optimizer,\n              metrics=['accuracy'])\n    \n    return model","86ffae21":"model = build_model(Adam(learning_rate=1e-3))\n\nmodel.summary()","848cee4e":"cyclic_lr = LR_Cycle(49, cycle_mult=2)","cec0505b":"history = model.fit_generator(train_generator,\n                              epochs=31,\n                              validation_data=valid_generator,\n                                callbacks=[cyclic_lr])","5cb54832":"report(model, history, valid_generator)","925b0dc9":"test_path = Path.cwd()\/'..'\/'input'\/'Kannada-MNIST'\/'test.csv'\n\nall_data_test = pd.read_csv(test_path)","444971be":"x_test = all_data_test.iloc[:,1:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)","216eadfc":"predictions = model.predict_classes(x_test\/255.)","2e82fde8":"output = pd.DataFrame({'id': all_data_test.id,\n                       'label': predictions})\noutput.to_csv(\"submission.csv\",index=False)","9354c2c4":"Borrowing the CNN architecture from this excellent notebook:\nhttps:\/\/www.kaggle.com\/kenanajk\/understanding-cnns-with-kannada-mnist"}}