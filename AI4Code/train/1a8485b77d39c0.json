{"cell_type":{"b00c7d46":"code","a8f64f63":"code","d9f3733d":"code","33f258d1":"code","22e180b4":"code","f93252dc":"code","27f26047":"code","14db1d5a":"code","b5a3a83a":"code","9a5d5640":"code","2f2c3531":"code","494f8f85":"code","4c2d3d4f":"code","d948d1e1":"code","06f1be88":"code","208ffbf2":"code","3e7fdef5":"code","52208618":"code","00f58fa8":"code","a53c0e21":"code","afda62d6":"code","2b52feed":"code","036b127c":"code","b8ff2c08":"code","4a79af3d":"code","4b749e97":"code","11029b58":"code","0d191f74":"code","fee84f55":"code","cb6b43bb":"code","cb0a3955":"code","7ba8c838":"code","1d01464f":"code","de6ed173":"code","27e98c74":"code","3c4dff26":"code","200a30b3":"code","fb549d0f":"code","33850588":"code","c3e9cc90":"code","e22bb0c0":"code","4c5dc2d0":"code","62482257":"code","f048574f":"code","9d59f05d":"markdown","f00292f6":"markdown","7da3fd7c":"markdown","1dc9a284":"markdown","715c25e7":"markdown","4a27dc74":"markdown","daf52925":"markdown","c0432fac":"markdown","6be46c60":"markdown","222fe1e5":"markdown","7541d9bc":"markdown","9b145f33":"markdown","bcd6820e":"markdown","f31528b0":"markdown","d897a336":"markdown","89130e22":"markdown","12179e36":"markdown","f1a3d08d":"markdown","7da99b64":"markdown","433c17be":"markdown","e1023c90":"markdown","3ef69140":"markdown","ea954eed":"markdown","bb4eddd5":"markdown","246bb4c4":"markdown","87608edc":"markdown","c1aa16cc":"markdown","2c6b9e19":"markdown","03aec534":"markdown","a28089f7":"markdown","efe926a8":"markdown","8cef31f9":"markdown","53855078":"markdown","a7b39356":"markdown","a53311d0":"markdown","79b33613":"markdown","859a3c11":"markdown","4e4d8a39":"markdown","113af73d":"markdown","64efc893":"markdown","af323725":"markdown","af613a51":"markdown","953dee7b":"markdown","b4473c71":"markdown","27a71be1":"markdown","77b8320c":"markdown","e82e12c9":"markdown","40711e91":"markdown","03a264c0":"markdown","64fd1c8b":"markdown","ec0453d8":"markdown","fe0c046a":"markdown","dfffe4c3":"markdown","9af9f033":"markdown","c50ec32f":"markdown","d73bd40c":"markdown","bc15bd3b":"markdown","9bf561f8":"markdown","92109831":"markdown","ee2304ab":"markdown","a387fb5d":"markdown","03d7b92b":"markdown","b5c674b6":"markdown","aa2ab7a0":"markdown","952572a3":"markdown","3c6e43f3":"markdown","c835232e":"markdown","e6c019ca":"markdown","a0596a27":"markdown","7b016080":"markdown","32bb4012":"markdown"},"source":{"b00c7d46":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns\nimport scipy.stats as sc\nimport gc\nimport warnings","a8f64f63":"plt.rcParams['figure.figsize'] = 15,8\nsns.set(rc={'figure.figsize':(15,8)})\npd.options.display.float_format = '{:.2f}'.format\nwarnings.filterwarnings('ignore')\ngc.enable()","d9f3733d":"train = pd.read_csv('..\/input\/train_V2.csv')\ntest = pd.read_csv('..\/input\/test_V2.csv')\ntrain.head()","33f258d1":"train.info()","22e180b4":"display(train[train.isnull().any(1)])\ndisplay(test[test.isnull().any(1)])","f93252dc":"train.drop(2744604, inplace=True)","27f26047":"train.describe()","14db1d5a":"train['winPlacePerc'].hist(bins=25);","b5a3a83a":"print(sc.normaltest(train['winPlacePerc']))\nprint('Skew: ', sc.skew(train['winPlacePerc']))","9a5d5640":"def featStat(featureName, constrain,plotType):\n    feat = train[featureName][train[featureName]>0]\n    data = train[[featureName,'winPlacePerc']].copy()\n    q99 = int(data[featureName].quantile(0.99))\n    plt.rcParams['figure.figsize'] = 15,5;   \n    \n    if constrain!=None:\n        feat = feat[feat<constrain]\n    if plotType == 'hist':\n        plt.subplot(1,2,1)\n        feat.hist(bins=50);\n        plt.title(featureName);\n        \n        n = 20\n        cut_range = np.linspace(0,q99,n)\n        cut_range = np.append(cut_range, data[featureName].max())\n        data[featureName] = pd.cut(data[featureName],\n                                         cut_range,\n                                         labels=[\"{:.0f}-{:.0f}\".format(a_, b_) for a_, b_ in zip(cut_range[:n], cut_range[1:])],\n                                         include_lowest=True\n                                        )\n        ax = plt.subplot(1,2,2)\n        sns.boxplot(x=\"winPlacePerc\", y=featureName, data=data, ax=ax, color=\"#2196F3\")\n        ax.set_xlabel('winPlacePerc', size=14, color=\"#263238\")\n        ax.set_ylabel(featureName, size=14, color=\"#263238\")\n        plt.gca().xaxis.grid(True)\n        plt.tight_layout()\n           \n    if plotType == 'count':        \n        plt.subplot(1,2,1)\n        sns.countplot(feat, color=\"#2196F3\");\n        \n        plt.subplot(1,2,2)\n        data.loc[data[featureName] > q99, featureName] = q99+1\n        x_order = data.groupby(featureName).mean().reset_index()[featureName]\n        x_order.iloc[-1] = str(q99+1)+\"+\"\n        data[featureName][data[featureName] == q99+1] = str(q99+1)+\"+\"\n        \n        ax = sns.boxplot(x=featureName, y='winPlacePerc', data=data, color=\"#2196F3\", order = x_order);\n        ax.set_xlabel(featureName, size=14, color=\"#263238\")\n        ax.set_ylabel('WinPlacePerc', size=14, color=\"#263238\")\n    plt.tight_layout()","2f2c3531":"featStat('kills',15,'count');\nplt.show();\nfeatStat('longestKill',400,'hist');\nplt.show();\nfeatStat('damageDealt',1000,'hist');","494f8f85":"featStat('heals',20,'count')\nplt.show()\nfeatStat('boosts',12,'count')","4c2d3d4f":"featStat('walkDistance',5000,'hist')\nplt.show()\nfeatStat('swimDistance',500,'hist')\nplt.show()\nfeatStat('rideDistance',12000,'hist')","d948d1e1":"featStat('weaponsAcquired',15,'count')\nplt.show()\nfeatStat('vehicleDestroys',None,'count')","06f1be88":"features = ['kills', 'longestKill', 'damageDealt', 'heals', 'boosts', 'walkDistance', 'swimDistance', 'rideDistance', 'weaponsAcquired', 'vehicleDestroys']\nzeroPerc = ((train[features] == 0).sum(0) \/ len(train)*100).sort_values(ascending = False)\nsns.barplot(x=zeroPerc.index , y=zeroPerc, color=\"#2196F3\");\nplt.title(\"Percentage of zero values\")\nplt.tight_layout()","208ffbf2":"df = train.drop(columns=['Id','matchId','groupId','matchType']+features)\ndf[(df>0) & (df<=df.quantile(0.99))].hist(bins=25,layout=(5,5),figsize=(15,15));\nplt.tight_layout()","3e7fdef5":"f,ax = plt.subplots(figsize=(15, 13))\nsns.heatmap(df.corr(), annot=True, fmt= '.1f',ax=ax,cbar=False)\nplt.show()","52208618":"f,ax = plt.subplots(figsize=(11, 11))\ncols = abs(train.corr()).nlargest(6, 'winPlacePerc')['winPlacePerc'].index\nhm = sns.heatmap(np.corrcoef(train[cols].values.T), annot=True, square=True, fmt='.2f',  yticklabels=cols.values, xticklabels=cols.values)\nprint(\", \".join(cols[1:]), \" most correlate with target feature\")\nplt.show()","00f58fa8":"sns.set(font_scale=2)\nsns.pairplot(train, y_vars=[\"winPlacePerc\"], x_vars=cols[1:],height=8);\nsns.set(font_scale=1)","a53c0e21":"print(\"Number of match in train dataset:\",train['matchId'].nunique())","afda62d6":"playersJoined = train.groupby('matchId')['matchId'].transform('count')\nsns.countplot(playersJoined[playersJoined>=75])\nplt.title('playersJoined');","2b52feed":"ngroupsByMatch = train.groupby('matchId')['groupId'].nunique()\nax = sns.countplot(ngroupsByMatch)\nplt.title('Number of groups by match');\nax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%d'))\nax.xaxis.set_major_locator(ticker.MultipleLocator(base=5)) #Starts from 0 not from 1:(","036b127c":"train.matchDuration.hist(bins=50);","b8ff2c08":"plt.rcParams['figure.figsize'] = 18,7;\ntypes = train.groupby('matchType').size().sort_values(ascending=False)\nsns.barplot(x=types.index,y=types.values);\nplt.title(\"Number of players by matchType\");\nplt.tight_layout()","4a79af3d":"def _min(x):\n    return x.value_counts().values.min()\ndef _max(x):\n    return x.value_counts().values.max()\ndef _avg(x):\n    return x.value_counts().values.mean()\ndef _med(x):\n    return np.median(x.value_counts().values)\ndef _num(x):\n    return x.nunique()\ninfostat = train.groupby('matchType').agg({\n    \"matchId\": [np.size, _num, _min,_med,_max], #np.size - number of players, _num - number of matches\n    \"groupId\": [_min,_med,_max],\n    \"matchDuration\": [min,np.median, max], \n    \"maxPlace\": [min,np.median,max],\n    \"numGroups\":[min,np.median,max]\n    }).sort_values(by=('matchId','size'),ascending=False)  \ndisplay(infostat)","4b749e97":"import lightgbm as lgb","11029b58":"# Function, which reduce memory usage. \n# This function I took from ready kernel (https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage)\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","0d191f74":"def initial_preparing(df, Debug):\n    if Debug:\n        df = df[df['matchId'].isin(df['matchId'].unique()[:2000])]\n    # Drop next columns. *Points features don't correlate with target feature, need\n    # more EDA to understand how they work.\n    df.drop(columns=['killPoints','rankPoints','winPoints','matchType','maxPlace','Id'],inplace=True)\n    X = df.groupby(['matchId','groupId']).agg(np.mean)\n    X = reduce_mem_usage(X)\n    y = X['winPlacePerc']     \n    X.drop(columns=['winPlacePerc'],inplace=True)\n    X_ranked = X.groupby('matchId').rank(pct=True)\n    X = X.reset_index()[['matchId','groupId']].merge(X_ranked, how='left', on=['matchId', 'groupId'] )\n    X.drop(['matchId','groupId'],axis=1, inplace=True)\n    X = reduce_mem_usage(X)\n    return X, y","fee84f55":"X_train, y = initial_preparing(train.copy(),False)","cb6b43bb":"from sklearn.model_selection import train_test_split\nX_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)","cb0a3955":"from sklearn.model_selection import cross_val_score\nimport sklearn.metrics\nfrom sklearn.model_selection import GridSearchCV","7ba8c838":"%%time\nlgtrain = lgb.Dataset(X_train, label=y_train.reset_index(drop=True))\nres = lgb.cv({'metric': 'mae'},lgtrain, nfold=5,stratified=False,seed=666)\nprint(\"Mean score:\",res['l1-mean'][-1])\ngc.collect()","1d01464f":"team_features = {\n        'assists': [sum, np.mean, np.size], #np.size - size of team\n        'boosts' : [sum, np.var, np.mean], \n        'heals': [sum, np.var, np.mean],\n        'damageDealt': [np.var,min,max,np.mean],\n        'DBNOs': [np.var,max,np.mean],\n        'headshotKills': [max,np.mean],\n        'killPlaceScall':[sum, min,max, np.var, np.mean],\n        'kills': [ sum, max, np.var,np.mean],\n        'killStreaks': [max,np.var,np.mean],\n        'longestKill': [max, np.mean, np.var],\n        'revives': sum,\n        'rideDistance': [sum, np.mean,np.var],\n        'swimDistance': [np.var],\n        'teamKills': sum,\n        'vehicleDestroys': sum,\n        'walkDistance': [np.var,np.mean],\n        'weaponsAcquired': [np.mean],\n        'damageRate': [np.var,min,max,np.mean],\n        'headshotRate': [np.var,max,np.mean],\n        'killStreakRate': [np.var,np.max, np.mean],\n        'healthItems': [np.var, np.mean],\n        'healsOnKill': [ np.var, np.mean],\n        'sniper': [ np.var, np.mean],\n        'totalDistance': [sum, np.var, np.mean],\n        'totalDistancePen': [ sum ,np.var, np.mean],\n        'killsPerRoadDist': [np.mean],\n        'killsPerWalkDist': [np.mean],\n        'killsPerDist': [np.mean],\n        'distance_over_weapons': [np.mean],\n        'walkDistance_over_heals': [np.mean],\n        'skill': [np.var,np.mean]\n}","de6ed173":"def featuring(df, isTrain, Debug):\n    y=None\n    if Debug:\n        df = df[df['matchId'].isin(df['matchId'].unique()[:2000])]\n \n    #Creating new features\n    #_________________________________________________________________________________________\n\n    nplayers = df.groupby('matchId')['matchId'].transform('count')\n    df['killPlaceScall'] = df['killPlace'] \/ nplayers\n    df['damageRate'] = df['kills']\/(0.01*df['damageDealt'])\n    df['headshotRate'] = df['headshotKills']\/df['kills']\n    df['killStreakRate'] = df['killStreaks']\/df['kills']\n    df['healthItems'] = df['heals'] + df['boosts']\n    df['healsOnKill'] = df['healthItems']\/df['kills']\n    df['sniper'] = df['longestKill']\/100*df['weaponsAcquired']\n    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n    df['totalDistancePen'] = df['rideDistance']\/5 + df[\"walkDistance\"] + df[\"swimDistance\"]*10\n    df['killsPerRoadDist'] = df['roadKills'] \/ (df['rideDistance']+1)\n    df['killsPerWalkDist'] = (df['kills']-df['roadKills']) \/ (df['walkDistance']+1)\n    df['killsPerDist'] = df['kills']\/(df['totalDistance']+1)\n    df['distance_over_weapons'] = df['totalDistance'] \/ df['weaponsAcquired']\n    df['walkDistance_over_heals'] = df['walkDistance']\/100\/df['heals']\n    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"] - df['teamKills'] \n    df.fillna(0,inplace=True)\n    df.replace(np.inf, 0, inplace=True)\n    #_________________________________________________________________________________________\n    \n    ids = df[['matchId','groupId','Id']]\n    df.drop(columns=['killPlace','killPoints','rankPoints','winPoints','matchType','maxPlace','Id'],inplace=True)\n    \n    tfeatures = team_features.copy()\n    if isTrain:\n        tfeatures['winPlacePerc'] = max\n    X = df.groupby(['matchId','groupId']).agg(tfeatures)\n    X.fillna(0,inplace=True)\n    X.replace(np.inf, 1000000, inplace=True)\n    X = reduce_mem_usage(X)    \n    if isTrain:\n        y = X[('winPlacePerc','max')]     \n        X.drop(columns=[('winPlacePerc','max')],inplace=True)\n             \n   \n    #Group dataset by matches. To each match apply ranking \n    X_ranked = X.groupby('matchId').rank(pct=True)    \n    X = X.reset_index()[['matchId','groupId']].merge(X_ranked, suffixes=[\"\", \"_rank\"], how='left', on=['matchId', 'groupId'] )\n\n    ids_after = X[['matchId','groupId']]\n    ids_after.columns = ['matchId','groupId']\n    \n    X = X.drop(['matchId','groupId'],axis=1)\n    X.columns = [a+\"_\"+b for a,b in X.columns]\n    X = reduce_mem_usage(X)\n    \n    return X, y, ids,ids_after","27e98c74":"%%time\nX_train, y, _,_ = featuring(train,True,False)\nX_test, _,ids_init,ids_after = featuring(test,False,False)","3c4dff26":"X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y, test_size=0.2, random_state=666)","200a30b3":"%%time\nlgtrain = lgb.Dataset(X_train, label=y_train.reset_index(drop=True))\nres = lgb.cv({'metric': 'mae'},lgtrain, nfold=5,stratified=False,seed=666)\nprint(\"Mean score:\",res['l1-mean'][-1])","fb549d0f":"gridParams = {\n    'num_leaves': [30,50,100], 'max_depth': [-1,8,15], \n    'min_data_in_leaf': [100,300,500], 'max_bin': [250,500], \n    'lambda_l1': [0.01], 'num_iterations': [5], \n    'nthread': [4], 'seed': [666],\n    'learning_rate': [0.05], 'metric': ['mae'],\n    \"bagging_fraction\" : [0.7], \"bagging_seed\" : [0], \"colsample_bytree\" : [0.7]\n    }\nmodel = lgb.LGBMRegressor()\ngrid = GridSearchCV(model, gridParams,\n                    verbose=1,\n                    cv=5)","33850588":"%%time\ngrid.fit(X_train.iloc[:500000,:], y_train.iloc[:500000])","c3e9cc90":"print(\"Best params:\", grid.best_params_)\nprint(\"\\nBest score:\", grid.best_score_)\nparams = grid.best_params_","e22bb0c0":"from sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import learning_curve\nmodel = lgb.LGBMRegressor(learning_rate=0.05,nthread=4)\n\ndef plot_with_err(x, data, **kwargs):\n    mu, std = data.mean(1), data.std(1)\n    lines = plt.plot(x, mu, '-', **kwargs)\n    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n    facecolor=lines[0].get_color(), alpha=0.2)\n    \ndef plot_learning_curve():\n    train_sizes = [1000,5000,10000,50000,100000,500000]\n    N_train, val_train, val_test = learning_curve(model,\n    X_train, y_train, train_sizes=train_sizes, cv=5,\n    scoring='neg_mean_absolute_error')\n    plot_with_err(N_train, abs(val_train), label='training scores')\n    plot_with_err(N_train, abs(val_test), label='validation scores')\n    plt.xlabel('Training Set Size'); plt.ylabel('MAE')\n    plt.legend()\n\nplot_learning_curve()","4c5dc2d0":"def iter_vs_score(num_iterations):\n    val_train, val_test = validation_curve(model, X_train[:500000], y_train[:500000],\n        'num_iterations', num_iterations, cv=4,scoring='neg_mean_absolute_error', verbose=1)\n    plot_with_err(num_iterations, abs(val_test), label='validation scores')\n    plot_with_err(num_iterations, abs(val_train), label='training scores')\n    plt.xlabel('Number of iterations'); plt.ylabel('MAE')\n    plt.legend();\n    plt.show();\n\nnum_iterations_small = [5,10,20,30,100,200]\niter_vs_score(num_iterations_small)\nnum_iterations_big = [500,1000,5000,10000]\niter_vs_score(num_iterations_big)","62482257":"%%time\nlgtrain = lgb.Dataset(X_train, label=y_train)\nlgval = lgb.Dataset(X_holdout, label=y_holdout)\n\nparams['num_iterations'] = 5000\nmodel = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)","f048574f":"pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n\nids_after['winPlacePerc'] = pred_test\npredict = ids_init.merge(ids_after, how='left', on=['groupId',\"matchId\"])['winPlacePerc']\ndf_sub = pd.read_csv(\"..\/input\/sample_submission_V2.csv\")\ndf_test = pd.read_csv(\"..\/input\/test_V2.csv\")\ndf_sub['winPlacePerc'] = predict\ndf_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission.csv\", index=False)","9d59f05d":"There are only one row with nan value, so let's drop it","f00292f6":"## 12. Conclusions","7da3fd7c":"### Feature correlations ","1dc9a284":"Split our train dataset again","715c25e7":"We have 4.5 millions of player stats records! <br\/>\n<br\/>\nNow check dataset for missing values","4a27dc74":"**Some stats by matchType**","daf52925":"## 10. Plotting training and validation curves","c0432fac":"Pvalue is zero, so this distribution is not normal <br\/>\nSkew is close to zero, so distribution is almostly symmetric","6be46c60":"**The task**:  using player statistic during the match, predict final placement of this player, where 0 is last place and 1 is winner winner, chicken dinner. \n<br\/><br\/>\nDataset contains over 65,000 games' worth of anonymized player data, which you can download from [kaggle](https:\/\/www.kaggle.com\/c\/pubg-finish-placement-prediction\/data) website. Each row of data is player stats at the end of the match.  <br\/> \nThe data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group. <br\/>\nStatistics can be like - player kills,  his\/her match, group and personal ID,  amount walked distance and etc...\n<br\/> **WinPlacePerc** - is a target feature on a scale from 1 (first place) to 0 (last place) - percentile winning placement.\n <br\/>  <br\/>\nA solution of the task can be valuable for PUBG players, for understanding, what parameters are important, which tactic to choose. Also using [PUBG Developer API](https:\/\/developer.pubg.com\/) we can collect our own data with more features. So it makes real to create a lot of different apps, which will help players. For example, app with personal assisstant, who will give a tips, what skill you should to train . \n <br\/>  <br\/>\nLet's look to the data","222fe1e5":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/importLGB.jpg\" width=\"500\" height=\"190\"> \n","7541d9bc":"## 8-9. Cross-validation and adjustment of model hyperparameters. Creation of new features and description of this process","9b145f33":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/maxresdefault.jpg\" width=\"1000\" height=\"600\"> ","bcd6820e":"Lets add new features and make ranking again.","f31528b0":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/pubg-bike.jpg\" width=\"1200\" height=\"800\"> ","d897a336":"As we can see, at small sizes of trainset, we have a big difference in train and validation scores. The reason is overfitting of train set and lack of data.\n<br\/> But with increasing size, this curves converge. With 500 000 train size this difference is very small. That's why I took 500 000 instead of all trainset in GridSearchCV.","89130e22":"## 1. Feature and data explanation","12179e36":"Split our train dataset to part, which we are going to train (X_train; same name), and to part with which we will check an error.","f1a3d08d":"Now look at distrubution of features with upper limit (to get rid of outliers)  and without zero values (because of lots of zero values)\n<br\/>Also make boxplots to see correlation target feature from feature values","7da99b64":"*Game zones*\n![game zones](https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Circle%20zones.png)","433c17be":"### Match statistics","e1023c90":"**New features** <br\/>\n<br\/> &nbsp;&nbsp; &nbsp; `killPlaceScall` - scaled `killPlace` feature. Just divide `killPlace` on number of players in a match.\n<br\/> &nbsp;&nbsp; &nbsp; `damageRate` - ratio `kills` and `damageDealt\/100`. If `damageRate`>1, player killed enemies, who was already damaged. So it was more easies to kill them.\nIf this feature <1, it means that player deal more damage than he\/she kill - player had  a difficult battle or just a little damage  some players, whose he\/she don't kill. \n<br\/>  &nbsp;&nbsp; &nbsp;`headshotRate` - percentage of headshot kills. Shows skill of player\n<br\/>  &nbsp;&nbsp; &nbsp;`killStreakRate` - percentage of killStreak from all kills. Also shows player skill\n<br\/>  &nbsp;&nbsp; &nbsp;`healthItems` - total number of health items (heals+boosts). \n<br\/>  &nbsp;&nbsp; &nbsp;`healsOnKill` - equal to `healsItems`\/`kills`. It shows how good player was in a battle. If player don't use heals after kill, it probably means, that he\/she don't take damage.\n<br\/>  &nbsp;&nbsp; &nbsp;`sniper` - equal to `longestKill`\/100*`weaponsAcquired`. It shows player sniper skill. Usually snipers have a good weapon. To find this weapon, player more likeky need acquired a lot other weapons. Yea, it's strange feature.\n<br\/>  &nbsp;&nbsp; &nbsp;`totalDistance` -  `rideDistance` + `walkDistance` + `swimDistance`.  Big distance means that player survived for long period of time, so he\/she will take a good final place.\n<br\/>  &nbsp;&nbsp; &nbsp;`totalDistancePen` - penalized `totalDistance`. It's needed to predict time of player game . So vehicle speed is approximately 5 times higher than player walk speed and swim speed is approximately 10 times lower than player walk speed.\n<br\/>  &nbsp;&nbsp; &nbsp;`killsPerRoadDist` - kills per distance. This feature can show your skill too. It's difficult to kill enemy using vehicle. \n<br\/>  &nbsp;&nbsp; &nbsp;`killsPerWalkDist` - represent player style. It shows you are camper or always in moving.\n<br\/>  &nbsp;&nbsp; &nbsp;`killsPerDist` - just combination of `killsPerRoadDist` and `killsPerWalkDist`\n<br\/>  &nbsp;&nbsp; &nbsp;`distance_over_weapons` - low values can represent that player try to find loot by yourself and\/or he\/she don't satisfied his\/her equipment, high values can mean that player just take loot from killed people and\/or he\/she has good equipment. Of course, it's not always true.\n<br\/>  &nbsp;&nbsp; &nbsp;`walkDistance_over_heals` - may represent player battles per distance.\n<br\/>  &nbsp;&nbsp; &nbsp;`skill` - equal to `headshotKills` + `roadKills` + `teamKills`. Just one of the indicator of player skill.","3ef69140":"We get 0.0291 for holdout set and 0.0242 for train set. It's obviously better than our previous scores. Now make a prediction for test set and put results to file `submission.csv`","ea954eed":"## 5. Metrics selection","bb4eddd5":"As we can see, with increasing of value of this features, probability to win also increase. So features, described above, good correlate with target feature. <br\/>\nPlot remaining features","246bb4c4":"At first, tell something about the game. **PlayerUnknown's Battlegrounds (PUBG)** -  an online multiplayer battle royale game.  Up to 100 players are dropped onto an island empty-handed and must explore, scavenge, loot and eliminate other players until only one is left standing, all while the play zone continues to shrink.  <br\/>\nBattle Royale-style video games have taken the world by storm. So PUBG becomes very popular. With over 50 million copies sold, it's the fifth best selling game of all time, and has millions of active monthly players.<br\/>\n","87608edc":"## 6. Model selection","c1aa16cc":"## 2-3 Primary data analysis and visual data analysis","2c6b9e19":"I choose 5 folds in cross-validation. We have a big dataset, so it's not necessary to set more folds, 80% for train part is enough to train model.  Moreover, If I choose higher number, than it will take a lot of time to compute. ","03aec534":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Best%20win.gif\" width=\"600\" height=\"400\"> ","a28089f7":"Best score is worse than after cross-validation, because there was taken 5 iterations,  and in cross-validation - 100 iterations. But it's will be OK, when we set higher number of iterations with parameters, which we find now.","efe926a8":"Take features, which most correlate with target feature","8cef31f9":"When we aggregate dataset by groupId, we create \"new\" features, i.e. we can aggregate by different ways. <br\/>\nFor example, 'boosts':sum - total number of using boosts in one team.","53855078":"So, people usually play in squads or pairs. (or maybe just data collected in this way) <br\/><br\/>\nAt the end,  some numbers, which describe each type of game by number of players, groups, matches and etc.\n<br\/>  In this table np.size - number of players and  _num - number of matches. We can see that maxPlace and numGroups are almostly the same.","a7b39356":"For small number of iterations, error fall down quickly. For large iterations error goes down, but slowly. Also we  can notice, that validation and training scores are approximetly the same for small number of iterations. For big number of iterations we can see, that score for training set continue to go down,  for validation set it goes down too, but much more slower. So curves diverge, but there are no overfitting, because validation score continue go down.","a53311d0":"Efremov Ivan","79b33613":"Let's make pairplots. We can clearly see correlation with winPlacePerc (but maybe only with weaponsAcquired it's difficult to see)","859a3c11":"&nbsp;&nbsp; &nbsp;We get a good score. But, of course, it can be better. There are lot of ways to do this. For example, I deleted `killPoints`,`rankPoints`and `winPoints` features. They may be helpfull, if correctly iterpret them. Also there are lots of cheaters in a game. So cheaters should be processed. I just slightly tuned LightGBM, we can find better parameters or even try other model.\n<br\/>&nbsp;&nbsp; &nbsp;In the begining, I mentioned about [PUBG Developer API](https:\/\/developer.pubg.com\/). We can get more features with it, so can make more complex model. It will be cool to create app, which give you tips in real time during the match. This solution can bring closer this idea or just help PUBG community by other way.\n","4e4d8a39":"We are going to tune `num_leaves`,  `max_depth`,  `min_data_in_leaf` and `max_bin`, because it's main parameters in LightGBM. \n<br\/> &nbsp;&nbsp; &nbsp;`num_leaves` -  max number of leaves in one tree. It's the main parameter to control the complexity of the tree model.\n<br\/> &nbsp;&nbsp; &nbsp;`max_depth` - limit the max depth for tree model. This is used to deal with over-fitting. -1 means no limit\n<br\/> &nbsp;&nbsp; &nbsp;`min_data_in_leaf` - minimal number of data in one leaf. This is a very important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and <br\/> &nbsp;&nbsp; &nbsp;`num_leaves`.\n<br\/> &nbsp;&nbsp; &nbsp;`max_bin` - max number of bins that feature values will be bucketed in. Small number of bins may reduce training accuracy but may increase general power (deal with over-fitting)","113af73d":"We get a significant improvement (almost in 2 times). So new features really help to understand the data.\n<br\/>\n<br\/> \nNow let's tune LightGBM. To do this, we are going to use GridSearchCV, which helps find the best parameters.","64efc893":"As I already mentioned, we are going to group player statistics to teams (by groupId).","af323725":"# PUBG Finish Placement Prediction","af613a51":"Now let's plot learning curve with different sizes of trainsets.","953dee7b":"![](https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/banner.png)","b4473c71":"**Distance**","27a71be1":"## 4. Insights and found dependencies","77b8320c":"**Some other features**","e82e12c9":"I decided to use **LightGBM**. With LightGBM convenient to work with large datasets (our case). LightGBM more faster than, for example, XGBoost and give good score at the same time. There are lots of parameters to tune (main problem) and it supports solving regression problems.","40711e91":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/player-unknown-battlegrounds-freefall.jpg\" width=\"1000\" height=\"600\"> ","03a264c0":"![](https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Love%20MAE.png)","64fd1c8b":"Now look how score depends from number of iterations.","ec0453d8":"We can notice, that 0 and values are more than others.  It's because first and last place exists in every match) <br\/>\nWinPlacePerc has obviously uniform distribution, but let's check target feature for normality and skewness of distribution (becouse of task) ","fe0c046a":"&nbsp;&nbsp;&nbsp;This task is regression problem. For regression problem we know only `mean absolute error` (MAE), `mean squared error` (MSE; also root MSE exists  ) and `mean squared log error` (MSLE; root MSLE exists too). <br\/>\n&nbsp;&nbsp;&nbsp;Our target have uniform distribution with range from 0 to 1. But MSLE more appropriate for not uniform distribution and MSE usually use, when large errors are particularly undesirable. We are not in this situations, so **MAE** will be convenient for us.","dfffe4c3":" &nbsp;&nbsp;&nbsp;&nbsp; We found, that walkDistance, killPlace, boosts, weaponsAcquired and damageDealt - is most correlated features. It easy to guess why.\n<br\/>     &nbsp;&nbsp;&nbsp;&nbsp;  If you are close to the top, more likely, that you walk greater distance, because you have to be in the circle (game zone). More likely, that you find a good weapon and\/or kill somebody. If you kill somebody, your enemy can hurt you, so then it's better to use boost. Near each killed enemy, you can find his\/her loot and ,probably, you acquire some his\/her weapons.\n<br\/>\n &nbsp;&nbsp;&nbsp;&nbsp; Also we can see, that a lot of people play in squads or duos (play in groups). Players in one team have the same finish placement. Finish result depends from team work. So it's better to see general statistic by team, not by separate player.","9af9f033":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Car%20destroy.gif\" width=\"600\" height=\"400\"> ","c50ec32f":"Thank you for reading and sorry for my English)","d73bd40c":"Let's train LightGBM model with params, which we had found with GridSearchCV.  In the same time we will compute error on hold-out set every 1000 iterations. Total number of iterations is 5000, it's should me enough. If we take higher number of iterations, we won't get significant improvement or can even get overfitting.","bc15bd3b":"## 11. Prediction for test and hold-out samples","9bf561f8":"General info about aech column","92109831":"We can already guess, that the target feature has uniform distribution. It's because winPlacePerc is already scaled feature and after every match player can have only one place.","ee2304ab":"## 7. Data preprocessing","a387fb5d":"We can see 3 peaks on second plot and 2 peaks in match duration plot. Presumably, it depends from match type.","03d7b92b":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Car%20kill.gif\" width=\"600\" height=\"400\"> ","b5c674b6":"So, our score is 0.0644. It's not bad. It means that our model error is +-6.42 placements (if there are 100 players on server)","aa2ab7a0":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Boosters.png\" width=\"800\" height=\"600\"> ","952572a3":"**Heals and boosts**","3c6e43f3":"**Kills and damage**","c835232e":" &nbsp;&nbsp;&nbsp;&nbsp;In next steps we will create new features. That's why this step will repeat again. So, at this stage, make simple data preparation. We just group all by team and than make a ranking in each match.\n<br\/>\n >&nbsp; Ranking - scaling, where the lowest value in initial table replace to value about zero (depends from distribution; no lower than 0) and maximum value replace to value about 1 (no higher than 1).","e6c019ca":"There we take only 500 000 teams out of 1 500 000. As we will see further (on learning curve), it's enough to find best params.","a0596a27":"### Data fields\n\n* **DBNOs** - Number of enemy players knocked.\n* **assists** - Number of enemy players this player damaged that were killed by teammates.\n* **boosts** - Number of boost items used.\n* **damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n* **headshotKills** - Number of enemy players killed with headshots.\n* **heals** - Number of healing items used.\n* **Id** - Player\u2019s Id\n* **killPlace** - Ranking in match of number of enemy players killed.\n* **killPoints** - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is * a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a \u201cNone\u201d.\n* **killStreaks** - Max number of enemy players killed in a short amount of time.\n* **kills** - Number of enemy players killed.\n* **longestKill** - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat.\n* **matchDuration** - Duration of match in seconds.\n* **matchId** - ID to identify match. There are no matches that are in both the training and testing set.\n* **matchType** - String identifying the game mode that the data comes from. The standard modes are \u201csolo\u201d, \u201cduo\u201d, \u201csquad\u201d, \u201csolo-fpp\u201d, \u201cduo-fpp\u201d, and \u201csquad-fpp\u201d; other modes are from events or custom matches.\n* **rankPoints** - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API\u2019s next version, so use with caution. Value of -1 takes place of \u201cNone\u201d.\n* **revives** - Number of times this player revived teammates.\n* **rideDistance** - Total distance traveled in vehicles measured in meters.\n* **roadKills** - Number of kills while in a vehicle.\n* **swimDistance** - Total distance traveled by swimming measured in meters.\n* **teamKills** - Number of times this player killed a teammate.\n* **vehicleDestroys** - Number of vehicles destroyed.\n* **walkDistance** - Total distance traveled on foot measured in meters.\n* **weaponsAcquired** - Number of weapons picked up.\n* **winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a \u201cNone\u201d.\n* **groupId** - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n* **numGroups** - Number of groups we have data for in the match.\n* **maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n* **winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match","7b016080":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Winner-winner.jpg\" width=\"600\" height=\"600\"> ","32bb4012":"<img src=\"https:\/\/github.com\/4ku\/PUBG-prediction\/raw\/master\/pictures\/Heals.gif\" width=\"600\" height=\"400\"> "}}