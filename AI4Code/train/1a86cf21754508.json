{"cell_type":{"02ccd6e6":"code","bd50b7d9":"code","5258c280":"code","d886ce5e":"code","39364f0f":"code","d1ecddae":"code","4499f414":"code","3541d529":"code","bda88f83":"code","747a99b8":"code","04dbd322":"code","8266fa24":"code","25ea3d8b":"code","6bd856b0":"code","ebf37393":"code","c0547afc":"code","f23710ac":"code","568df04d":"code","bdf9ab06":"code","841583e0":"code","2fabb7ac":"code","d4dc9844":"code","51f75685":"code","1624080a":"code","fd88fc2d":"code","9acc16a5":"code","3f7d466c":"code","215dd849":"code","8f29f9d8":"code","e025922c":"code","1ef05d32":"markdown","a407c63b":"markdown","fbfce0f3":"markdown","ef9ac9a4":"markdown","9316d7bc":"markdown","d2170e67":"markdown","27d196af":"markdown","ed4af603":"markdown","8b851c32":"markdown","15df210d":"markdown","c09d3f39":"markdown","5297d04b":"markdown","b98cc40f":"markdown","72d21456":"markdown","a293090b":"markdown","32a77159":"markdown","cf94fa2a":"markdown","de1d8020":"markdown","e29c82c0":"markdown","3d46e72b":"markdown","cc921b15":"markdown","dfca0bf2":"markdown","18647e0c":"markdown","9e6c72f6":"markdown","5fbd7b09":"markdown","5778dec0":"markdown","f7ccada6":"markdown","426abfc0":"markdown","327027d5":"markdown","d591f15d":"markdown","4874a751":"markdown","e1ea3287":"markdown","cdfaf6c3":"markdown","2bd7d7c8":"markdown","d63b170e":"markdown","b2e8b0c0":"markdown","2d3af87f":"markdown","5c188800":"markdown","4d1ffb1e":"markdown","e9140f21":"markdown","4e58be39":"markdown","2e662523":"markdown","9dbfb4ea":"markdown","4387b77f":"markdown","1f0af9a9":"markdown","0719f18f":"markdown","be91d3d2":"markdown","3c18ed9c":"markdown"},"source":{"02ccd6e6":"# always start with checking out the files!\n!ls ..\/input\/*","bd50b7d9":"# Basic packages\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rd # generating random numbers\nimport datetime # manipulating date formats\n# Viz\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\n\n\n# TIME SERIES\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","5258c280":"# Import all of them \nsales=pd.read_csv(\"..\/input\/sales_train.csv\")\n\n# settings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nitem_cat=pd.read_csv(\"..\/input\/item_categories.csv\")\nitem=pd.read_csv(\"..\/input\/items.csv\")\nsub=pd.read_csv(\"..\/input\/sample_submission.csv\")\nshops=pd.read_csv(\"..\/input\/shops.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")","d886ce5e":"#formatting the date column correctly\nsales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\n# check\nprint(sales.info())","39364f0f":"# Aggregate to monthly level the required metrics\n\nmonthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})\n\n## Lets break down the line of code here:\n# aggregate by date-block(month),shop_id and item_id\n# select the columns date,item_price and item_cnt(sales)\n# Provide a dictionary which says what aggregation to perform on which column\n# min and max on the date\n# average of the item_price\n# sum of the sales","d1ecddae":"# number of items per cat \n#notice that the analisis tupla is simply x\n\nx=item.groupby(['item_category_id']).count()\n\nx=x.sort_values(by='item_id',ascending=False)\nx=x.iloc[0:10].reset_index()\nx\n# #plot\nplt.figure(figsize=(8,4))\nax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)\nplt.title(\"Items per Category\")\nplt.ylabel('# of items', fontsize=12)\nplt.xlabel('Category', fontsize=12)\nplt.show()","4499f414":"sales.head()","3541d529":"monthly_sales.head()","bda88f83":"#declaration of time-series kind of variable ts with groupby, but the structure is simple\n\nts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\n\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","747a99b8":"#with ts.rolling we are computing the mean and std of sales grouped in windows of size 12 in units of time (block times, indeed)\n\nplt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","04dbd322":"import statsmodels.api as sm\n# multiplicative\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"multiplicative\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","8266fa24":"# Additive model\nres = sm.tsa.seasonal_decompose(ts.values,freq=12,model=\"additive\")\n#plt.figure(figsize=(16,12))\nfig = res.plot()\n#fig.show()","25ea3d8b":"# R version ported into python  \n\n# alas ! rpy2 does not exist in Kaggle kernals :( \n# from rpy2.robjects import r\n# def decompose(series, frequency, s_window, **kwargs):\n#     df = pd.DataFrame()\n#     df['date'] = series.index\n#     s = [x for x in series.values]\n#     length = len(series)\n#     s = r.ts(s, frequency=frequency)\n#     decomposed = [x for x in r.stl(s, s_window, **kwargs).rx2('time.series')]\n#     df['observed'] = series.values\n#     df['trend'] = decomposed[length:2*length]\n#     df['seasonal'] = decomposed[0:length]\n#     df['residual'] = decomposed[2*length:3*length]\n#     return df","6bd856b0":"from fbprophet import Prophet\n\n# adding the dates to the Time-series as index\n#normally, ts is already created before since we have to at least plot the data the we want to analyze and this is done above\n#ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n\nts.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nts=ts.reset_index()\nts.head()\n\n#prophet reqiures a pandas df at the below config \n# ( date column named as DS and the value column as Y)\n","ebf37393":"ts.columns=['ds','y']\nmodel = Prophet( yearly_seasonality=True) #instantiate Prophet with only yearly seasonality as our data is monthly \nmodel.fit(ts) #fit the model with your dataframe","c0547afc":"# predict for five months in the furure and MS - month start is the frequency\nfuture = model.make_future_dataframe(periods = 5, freq = 'MS')  \n# now lets make the forecasts\nforecast = model.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","f23710ac":"forecast.head()","568df04d":"model.plot(forecast)","bdf9ab06":"model.plot_components(forecast)","841583e0":"test.head()","2fabb7ac":"#Function that generate the input for prophet for any shop_id x item_id combination, which are read as free parameters\n\ndef sales_shop_item_prophet_input(shop_id_history, item_id_history):\n   \n    sales_shop_item = sales[(sales.shop_id==shop_id_history) & (sales.item_id == item_id_history)]\n    monthly_sales_shop_item = sales_shop_item.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\n    \n    dates = pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\n    nullhistory = pd.DataFrame({\"ds\":dates,\"y\":[0]*dates.shape[0]},columns=[\"ds\",\"y\"])\n    \n    if (sales_shop_item.shape[0] > 0):\n        for con in range(monthly_sales_shop_item.shape[0]): \n            index_date_block = monthly_sales_shop_item.index[con]\n            count_date_block = monthly_sales_shop_item.iloc[con]\n    \n            nullhistory.iloc[index_date_block,1] = count_date_block\n    \n    return nullhistory    ","d4dc9844":"sales_shop_item_prophet_input(test.iloc[0,1], test.iloc[0,2])","51f75685":"nodeToForecast = sales_shop_item_prophet_input(test.iloc[0,1], test.iloc[0,2])\n","1624080a":"nodeToForecast","fd88fc2d":"nodeToForecast.iloc[:,1].mean()","9acc16a5":"range(len(test))","3f7d466c":"import time\nstart_time=time.time()\n\n# Bottoms up\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nlist_of_nodes =[]\nlist_of_predictions = []\n\nfor node in range(len(test)):\n#for node in range(40):\n    # take the date-column and the col to be forecasted\n\n    test_shop_id_index = test.iloc[node,1]\n    test_item_id_index = test.iloc[node,2]\n  \n    #nodeToForecast = sales_shop_item_prophet_input(test_shop_id_index, test_item_id_index)\n\n    #growth = 'linear'\n    #m = Prophet(growth, yearly_seasonality=True)\n    #m.fit(nodeToForecast)\n    #future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    #forecastsDict[node] = m.predict(future)\n    \n    #location_of_prediction_yhat = forecastsDict[node].shape[1] - 1\n    #location_of_prediction_month = forecastsDict[node].shape[0] - 1\n    \n    #prediction_sales_next_month = forecastsDict[node].iloc[location_of_prediction_month,location_of_prediction_yhat]\n    \n    #print(node,test_shop_id_index,test_item_id_index,prediction_sales_next_month)\n    \n    #prediction_sales_next_month = nodeToForecast.iloc[:,1].mean()\n    \n    prediction_sales_next_month = 0.5\n    \n    list_of_nodes.append(node)\n    list_of_predictions.append(prediction_sales_next_month)\n    \n    if (node % 1000 == 0):\n        end_time=time.time()\n        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n        start_time=end_time\n       # break","215dd849":" dfsubmission = pd.DataFrame({\"ID\":list_of_nodes,\"item_cnt_month\":list_of_predictions},columns=[\"ID\",\"item_cnt_month\"])","8f29f9d8":"dfsubmission","e025922c":"dfsubmission.to_csv('submission_file.csv',index=False)","1ef05d32":"## MA(1) process -- has ACF cut off at lag=1","a407c63b":"## Basics of TS:\n\nCollation of different basic concepts of the different traditional time-series models and some basic intuition behind them\n\n## Objective:\nThis kernel was made to serve as repository of various time-series concepts for beginners and I hope it would be useful as a refresher to some of the experts too :)\n\n## Table of contents:\n* Competition and data overview\n* Imports ( data and packages )\n* Basic exploration\/EDA\n* Single time-series \n    * Stationarity\n    * Seasonality , Trend and Remainder\n    * AR , MA , ARMA , ARIMA\n    * Selecting P and Q using AIC\n    * ETS\n    * Prophet \n    * UCM\n* Hierarchical time-series\n    * Bottom's up\n    * AHP\n    * PHA \n    * FP \n    \n    \n## Competition and data overview:\n\nIn this playground competition, we are provided with the challenge of predicting total sales for every product and store in the next month for Russian Software company-[1c company](http:\/\/1c.ru\/eng\/title.htm). \n\n**What does the IC company do?:**\n\n1C: Enterprise 8 system of programs is intended for automation of everyday enterprise activities: various business tasks of economic and management activity, such as management accounting, business accounting, HR management, CRM, SRM, MRP, MRP, etc.\n\n**Data**:\nWe are provided with daily sales data for each store-item combination, but our task is to predict sales at a monthly level.\n\n## Imports:\n","fbfce0f3":"# now testing the stationarity again after de-seasonality\ntest_stationarity(new_ts)","ef9ac9a4":"forecastsDict[32]","9316d7bc":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(arma22, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n","d2170e67":"# Simulate an AR(2) process\n\nn = int(1000)\nalphas = np.array([.444, .333])\nbetas = np.array([0.])\n\n# Python requires us to specify the zero-lag value which is 1\n# Also note that the alphas for the AR model must be negated\n# We also set the betas for the MA equal to 0 for an AR(p) model\n# For more information see the examples at statsmodels.org\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nar2 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \n_ = tsplot(ar2, lags=12,title=\"AR(2) process\")","27d196af":"start_time=time.time()\n\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_shop_sales)):\n    \n    dates_column = monthly_shop_sales.iloc[:,0]\n    sales_column = monthly_shop_sales.iloc[:,node+1]\n    \n    nodeToForecast = pd.DataFrame({'ds': dates_column, 'y': sales_column}, columns=['ds', 'y'])\n    \n    #take the date-column and the col to be forecasted\n    #nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n    #print(nodeToForecast.head())  # just to check\n    # rename for prophet compatability\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)\n    ","ed4af603":"# Second time that we apply prophet, \n\n## This time we run over node (shop_id x item_id) time series. Simply because the submission file is required in this direction of predictions, not the previous one","8b851c32":"ts=sales.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,16))\nplt.subplot(311)\nplt.title('Original')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts)\n\nplt.subplot(312)\nplt.title('After De-trend')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts)\nplt.plot(new_ts)\nplt.plot()\n\nplt.subplot(313)\nplt.title('After De-seasonalization')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nnew_ts=difference(ts,12)       # assuming the seasonality is 12 months long\nplt.plot(new_ts)\nplt.plot()","15df210d":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh',title=''):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title(title)\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","c09d3f39":"Of course, there is a lot more that we can explore in this dataset, but let's dive into the time-series part.\n\n# Single series:\n\nThe objective requires us to predict sales for the next month at a store-item combination.\n\nSales over time of each store-item is a time-series in itself. Before we dive into all the combinations, first let's understand how to forecast for a single series.\n\nI've chosen to predict for the total sales per month for the entire company.\n\nFirst let's compute the total sales per month and plot that data.\n","5297d04b":"we assume an additive model, then we can write\n\n> yt=St+Tt+Et \n\nwhere yt is the data at period t, St is the seasonal component at period t, Tt is the trend-cycle component at period tt and Et is the remainder (or irregular or error) component at period t\nSimilarly for Multiplicative model,\n\n> yt=St  x Tt x Et \n\n## Stationarity:\n\n![q](https:\/\/static1.squarespace.com\/static\/53ac905ee4b003339a856a1d\/t\/5818f84aebbd1ac01c275bac\/1478031479192\/?format=750w)\n\nStationarity refers to time-invariance of a series. (ie) Two points in a time series are related to each other by only how far apart they are, and not by the direction(forward\/backward)\n\nWhen a time series is stationary, it can be easier to model. Statistical modeling methods assume or require the time series to be stationary.\n\n\nThere are multiple tests that can be used to check stationarity.\n* ADF( Augmented Dicky Fuller Test) \n* KPSS \n* PP (Phillips-Perron test)\n\nLet's just perform the ADF which is the most commonly used one.\n\nNote: [Step by step guide to perform dicky fuller test in Excel](http:\/\/www.real-statistics.com\/time-series-analysis\/stochastic-processes\/dickey-fuller-test\/)\n\n[Another Useful guide](http:\/\/www.blackarbs.com\/blog\/time-series-analysis-in-python-linear-models-to-garch\/11\/1\/2016#AR) \n\n[good reference](https:\/\/github.com\/ultimatist\/ODSC17\/blob\/master\/Time%20Series%20with%20Python%20(ODSC)%20STA.ipynb)\n","b98cc40f":"**Quick observations:**\nThere is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".\n\nLet's check that with a quick decomposition into Trend, seasonality and residuals.\n","72d21456":"## Foot-notes:\n\nI'm not a stats major, so please do let me know in the comments if you feel that I've left out any important technique or if there was any mistake in the content.\n\nI plan to add another kernel about Time-series here which would be about adapting the open-source solutions from the recent time-series competitions ( Favorita, Recruit,etc. ) to this playground dataset.\n\nDo leave a comment\/upvote :) ","a293090b":"# Prophet: \n\nRecently open-sourced by Facebook research. It's a very promising tool, that is often a very handy and quick solution to the frustrating **flatline** :P\n\n![FLATLINE](https:\/\/i.stack.imgur.com\/fWzyX.jpg)\n\nSure, one could argue that with proper pre-processing and carefully tuning the parameters the above graph would not happen. \n\nBut the truth is that most of us don't either have the patience or the expertise to make it happen.\n\nAlso, there is the fact that in most practical scenarios- there is often a lot of time-series that needs to be predicted.\nEg: This competition. It requires us to predict the next month sales for the **Store - item level combinations** which could be in the thousands.(ie) predict 1000s of parameters!\n\nAnother neat functionality is that it follows the typical **sklearn** syntax.\n\nAt its core, the Prophet procedure is an additive regression model with four main components:\n* A piecewise linear or logistic growth curve trend. Prophet automatically detects changes in trends by selecting changepoints from the data.\n* A yearly seasonal component modeled using Fourier series.\n* A weekly seasonal component using dummy variables.\n* A user-provided list of important holidays.\n\n**Resources for learning more about prophet:**\n* https:\/\/www.youtube.com\/watch?v=95-HMzxsghY\n* https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html#python-api\n* https:\/\/research.fb.com\/prophet-forecasting-at-scale\/\n* https:\/\/blog.exploratory.io\/is-prophet-better-than-arima-for-forecasting-time-series-fa9ae08a5851","32a77159":"Awesome. The trend and seasonality from Prophet look similar to the ones that we had earlier using the traditional methods.\n\n## UCM:\n\nUnobserved Components Model. The intuition here is similar to that of the prophet. The model breaks down the time-series into its components, trend, seasonal, cycle and regresses them and then predicts the next point for the components and then combines them.\n\nUnfortunately, I could not find a good package\/code that can perform this model in Python :( \n\nR version of UCM: https:\/\/bicorner.com\/2015\/12\/28\/unobserved-component-models-in-r\/\n\n# Hierarchical time series:\n\nThe [Forecasting: principles and practice](https:\/\/www.otexts.org\/fpp\/9\/4) , is the ultimate reference book for forecasting by Rob J Hyndman.\n\nHe lays out the fundamentals of dealing with grouped or Hierarchical forecasts. Consider the following simple scenario.\n\n![](https:\/\/www.otexts.org\/sites\/default\/files\/resize\/fpp\/images\/hts1-550x274.png)\n\nHyndman proposes the following methods to estimate the points in this hierarchy. I've tried to simplify the language to make it more intuitve.\n\n### Bottom up approach:\n* Predict all the base level series using any method, and then just aggregate it to the top.\n* Advantages: Simple , No information is lost due to aggregation.\n* Dis-advantages: Lower levels can be noisy\n\n### Top down approach:\n* Predict the top level first. (Eg: predict total sales first)\n* Then calculate **weights** that denote the proportion of the total sales that needs to be given to the base level forecast(Eg:) the contribution of the item's sales to the total sales \n* There are different ways of arriving at the \"weights\". \n    * **Average Historical Proportions** - Simple average of the item's contribution to sales in the past months\n    * **Proportion of historical averages** - Weight is the ratio of average value of bottom series by the average value of total series (Eg: Weight(item1)= mean(item1)\/mean(total_sales))\n    * **Forecasted Proportions** - Predict the proportion in the future using changes in the past proportions\n* Use these weights to calcuate the base -forecasts and other levels\n\n### Middle out:\n* Use both bottom up and top down together.\n* Eg: Consider our problem of predicting store-item level forecasts.\n    * Take the middle level(Stores) and find forecasts for the stores\n    * Use bottoms up approach to find overall sales\n    * Dis-integrate store sales using proportions to find the item-level sales using a top-down approach\n    \n### Optimal combination approach:\n* Predict for all the layers independently\n* Since, all the layers are independent, they might not be consistent with hierarchy\n    * Eg: Since the items are forecasted independently, the sum of the items sold in the store might not be equal to the forecasted sale of store  or as Hyndman puts it \u201caggregate consistent\u201d\n* Then some matrix calculations and adjustments happen to provide ad-hoc adjustments to the forecast to make them consistent with the hierarchy\n\n\n### Enough with the theory. Lets start making forecasts! :P\nThe problem at hand here, has 22170 items and 60 stores . This indicates that there can be around a **million** individual time-series(item-store combinations) that we need to predict!\n\nConfiguring each of them would be nearly impossible. Let's use Prophet which does it for us.\n\nStarting off with the bottoms up approach.\n\nThere are some other points to consider here: \n* Not all stores sell all items\n* What happens when a new product is introduced? \n* What if a product is removed off the shelves?","cf94fa2a":"# Simulate MA(2) process with betas 0.6, 0.4\nn = int(1000)\nalphas = np.array([0.])\nbetas = np.array([0.6, 0.4])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\nma3 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n)\n_ = tsplot(ma3, lags=12,title=\"MA(2) process\")","de1d8020":"### Now after the transformations, our p-value for the DF test is well within 5 %. Hence we can assume Stationarity of the series\n\nWe can easily get back the original series using the inverse transform function that we have defined above.\n\nNow let's dive into making the forecasts!\n\n# AR, MA and ARMA models:\nTL: DR version of the models:\n\nMA - Next value in the series is a function of the average of the previous n number of values\nAR - The errors(difference in mean) of the next value is a function of the errors in the previous n number of values\nARMA - a mixture of both.\n\nNow, How do we find out, if our time-series is an AR process or MA process?\n\nLet's find out!","e29c82c0":"# Simulate an MA(1) process\nn = int(1000)\n# set the AR(p) alphas equal to 0\nalphas = np.array([0.])\nbetas = np.array([0.8])\n# add zero-lag and negate alphas\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\nma1 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n) \nlimit=12\n_ = tsplot(ma1, lags=limit,title=\"MA(1) process\")","3d46e72b":"## Now things get a little hazy. Its not very clear\/straight-forward.\n\nA nifty summary of the above plots:\n\nACF Shape\t| Indicated Model |\n-- | -- |\nExponential, decaying to zero |\tAutoregressive model. Use the partial autocorrelation plot to identify the order of the autoregressive model |\nAlternating positive and negative, decaying to zero\tAutoregressive model. |  Use the partial autocorrelation plot to help identify the order. |\nOne or more spikes, rest are essentially zero | Moving average model, order identified by where plot becomes zero. |\nDecay, starting after a few lags |\tMixed autoregressive and moving average (ARMA) model. | \nAll zero or close to zero | Data are essentially random. |\nHigh values at fixed intervals | Include seasonal autoregressive term. |\nNo decay to zero |\tSeries is not stationary |\n\n\n## Let's use a systematic approach to finding the order of AR and MA processes.","cc921b15":"## We've correctly identified the order of the simulated process as ARMA(2,2) (this is just a test of the previous routine !!!)\n\n\n### Lets use it for the sales time-series.\n","dfca0bf2":"# get the unique combinations of item-store from the sales data at monthly level\nmonthly_sales=sales.groupby([\"shop_id\",\"item_id\",\"date_block_num\"])[\"item_cnt_day\"].sum()\n# arrange it conviniently to perform the hts \nmonthly_sales=monthly_sales.unstack(level=-1).fillna(0)\nmonthly_sales=monthly_sales.T\ndates=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nmonthly_sales.index=dates\nmonthly_sales=monthly_sales.reset_index()\nmonthly_sales.head()","18647e0c":"Definition of the variable monthly_sales, which is not necessarily useful for the run of fit and predictions but seems to be useful to have a different view of the file data by using the groupby option in a complicated way. It is maybe useful for the stat analysis, but I have my doubts since the main object for the analysis is mostly given by ts kind of variable with columns date_block and item_count","9e6c72f6":"Now, we declare a new variable to define the data to analyze in terms of time series predictions\nbasically, the item_cnt_day added in differents directions, such as date_block_num or date_block_num x shop_id x item_id, etc.","5fbd7b09":"## AR(1) process -- has ACF tailing out and PACF cutting off at lag=1","5778dec0":"monthly_item_sales=sales.groupby([\"date_block_num\",\"item_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_item_sales=monthly_item_sales.unstack(level=1)\nmonthly_item_sales=monthly_item_sales.fillna(0)\nmonthly_item_sales.index=dates\nmonthly_item_sales=monthly_item_sales.reset_index()\nmonthly_item_sales.head()","f7ccada6":"monthly_shop_sales=sales.groupby([\"date_block_num\",\"shop_id\"])[\"item_cnt_day\"].sum()\n# get the shops to the columns\nmonthly_shop_sales=monthly_shop_sales.unstack(level=1)\nmonthly_shop_sales=monthly_shop_sales.fillna(0)\nmonthly_shop_sales.index=dates\nmonthly_shop_sales=monthly_shop_sales.reset_index()\nmonthly_shop_sales.head()","426abfc0":"test","327027d5":"\nForecast of Monthly Sales per Item","d591f15d":"~16s for 10 predictions. We need a million predictions. This would not work out.\n\n# Middle out:\nLet's predict for the store level","4874a751":"# to remove trend\nfrom pandas import Series as Series\n# create a differenced series\ndef difference(dataset, interval=1):\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset[i] - dataset[i - interval]\n        diff.append(value)\n    return Series(diff)\n\n# invert differenced forecast\ndef inverse_difference(last_ob, value):\n    return value + last_ob\n\n","e1ea3287":"predictions_unknown=predictions[-1]\npredictions_unknown","cdfaf6c3":"## AR(2) process -- has ACF tailing out and PACF cutting off at lag=2","2bd7d7c8":"start_time=time.time()\n\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\nfor node in range(len(monthly_item_sales)):\n    \n    dates_column = monthly_item_sales.iloc[:,0]\n    sales_column = monthly_item_sales.iloc[:,node+1]\n    \n    nodeToForecast = pd.DataFrame({'ds': dates_column, 'y': sales_column}, columns=['ds', 'y'])\n    \n    #take the date-column and the col to be forecasted\n    #nodeToForecast = pd.concat([monthly_shop_sales.iloc[:,0], monthly_shop_sales.iloc[:, node+1]], axis = 1)\n    #print(nodeToForecast.head())  # just to check\n    # rename for prophet compatability\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)\n    ","d63b170e":"# Simulate an ARMA(2, 2) model with alphas=[0.5,-0.25] and betas=[0.5,-0.3]\nmax_lag = 12\n\nn = int(5000) # lots of samples to help estimates\nburn = int(n\/10) # number of samples to discard before fit\n\nalphas = np.array([0.8, -0.65])\nbetas = np.array([0.5, -0.7])\nar = np.r_[1, -alphas]\nma = np.r_[1, betas]\n\narma22 = smt.arma_generate_sample(ar=ar, ma=ma, nsample=n, burnin=burn)\n_ = tsplot(arma22, lags=max_lag,title=\"ARMA(2,2) process\")","b2e8b0c0":"# pick best order by aic \n# smallest aic value wins\nbest_aic = np.inf \nbest_order = None\nbest_mdl = None\n\nrng = range(5)\nfor i in rng:\n    for j in rng:\n        try:\n            tmp_mdl = smt.ARMA(new_ts.values, order=(i, j)).fit(method='mle', trend='nc')\n            tmp_aic = tmp_mdl.aic\n            if tmp_aic < best_aic:\n                best_aic = tmp_aic\n                best_order = (i, j)\n                best_mdl = tmp_mdl\n        except: continue\n\n\nprint('aic: {:6.5f} | order: {}'.format(best_aic, best_order))\n","2d3af87f":"import time\nstart_time=time.time()\n\n# Bottoms up\n# Calculating the base forecasts using prophet\n# From HTSprophet pachage -- https:\/\/github.com\/CollinRooney12\/htsprophet\/blob\/master\/htsprophet\/hts.py\nforecastsDict = {}\ndates_column = monthly_sales.iloc[:,0]\n\nfor node in range(len(monthly_sales)):\n    # take the date-column and the col to be forecasted\n\n  \n    sales_column = monthly_sales.iloc[:,node+1]\n    \n    nodeToForecast = pd.DataFrame({'ds': dates_column, 'y': sales_column}, columns=['ds', 'y'])\n    \n    #nodeToForecast = pd.concat([monthly_sales.iloc[:,0], monthly_sales.iloc[:, node+1]], axis = 1)\n    #print(nodeToForecast.head())  # just to check\n    #rename for prophet compatability\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[0] : 'ds'})\n    #nodeToForecast = nodeToForecast.rename(columns = {nodeToForecast.columns[1] : 'y'})\n    \n    growth = 'linear'\n    m = Prophet(growth, yearly_seasonality=True)\n    m.fit(nodeToForecast)\n    future = m.make_future_dataframe(periods = 1, freq = 'MS')\n    forecastsDict[node] = m.predict(future)\n    if (node== 10):\n        end_time=time.time()\n        print(\"forecasting for \",node,\"th node and took\",end_time-start_time,\"s\")\n        break\n    ","5c188800":"Simply use best_mdl.predict() to predict the next values","4d1ffb1e":"# Stationarity tests\ndef test_stationarity(timeseries):\n    \n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(timeseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)\n\ntest_stationarity(ts)\n","e9140f21":"#predictions = np.zeros([len(forecastsDict[0].yhat),1]) \nnCols = len(list(forecastsDict.keys()))+1\nfor key in range(0, nCols-1):\n    f1 = np.array(forecastsDict[key].yhat)\n    f2 = f1[:, np.newaxis]\n    if key==0:\n        predictions=f2.copy()\n       # print(predictions.shape)\n    else:\n       predictions = np.concatenate((predictions, f2), axis = 1)","4e58be39":"# take a peak\nmonthly_sales.head(20)","2e662523":"Forecast of Monthly Sales per Shop","9dbfb4ea":"# First application of Prophet, using the full but simple ts dataframe","4387b77f":"predictions","1f0af9a9":"# Simulate an AR(1) process with alpha = 0.6\nnp.random.seed(1)\nn_samples = int(1000)\na = 0.6\nx = w = np.random.normal(size=n_samples)\n\nfor t in range(n_samples):\n    x[t] = a*x[t-1] + w[t]\nlimit=12    \n_ = tsplot(x, lags=limit,title=\"AR(1)process\")","0719f18f":"monthly_item_sales","be91d3d2":"## MA(2) process -- has ACF cut off at lag=2","3c18ed9c":"## Under construction...........\n\n### Unconventional techniques: converting TS into a regression problem\n\n### Dealing with Hierarchy\n### Codes for top down, optimal ,etc\n\n"}}