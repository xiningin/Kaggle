{"cell_type":{"668e792d":"code","7ec68899":"code","e9c2b58d":"code","c27b19ca":"code","11368676":"code","78a3530e":"code","2d61a196":"code","77af358c":"code","79796837":"code","afc992a0":"code","d2b36a18":"code","4506b996":"code","02e2414c":"code","5fa476e2":"code","f58c1b46":"code","7f0e4e4c":"code","080394cb":"code","3247de8e":"code","e79bc045":"code","35ed93c1":"markdown","0c4ec992":"markdown","08b52d81":"markdown","9f0e2a69":"markdown","1552f543":"markdown","fc2fd4a4":"markdown","a98d7e5b":"markdown","ccbe94e1":"markdown","10caa592":"markdown","85248bf0":"markdown","0669ca42":"markdown","4dfd25de":"markdown","ebacd04a":"markdown","aefe23c6":"markdown","b4bc609b":"markdown","8ddf53ff":"markdown","f86a471d":"markdown","f1ec2a89":"markdown","b45e136f":"markdown","616d65ab":"markdown","30c876f6":"markdown","0372fa21":"markdown","2f38a049":"markdown","1bc7c732":"markdown","2753cc8c":"markdown","35691355":"markdown"},"source":{"668e792d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom zipfile import ZipFile\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path=os.path.join(dirname, filename)\n        if 'train' in path:\n            __training_path=path\n        elif 'test' in path:\n            __test_path=path","7ec68899":"#loaded files\nprint(f'Training path:{__training_path}\\nTest path:{__test_path}')","e9c2b58d":"# Kaggle Environment Prepration\n#update kaggle env\nimport sys\n#you may update the environment that allow you to run the whole code\n!{sys.executable} -m pip install --upgrade scikit-learn==\"0.24.2\"","c27b19ca":"#record this information if you need to run the Kernel internally\nimport sklearn; sklearn.show_versions()","11368676":"def proc_multifiles_zip(path, condition, delimiter=','):\n    zip_file = ZipFile(path)\n    for text_file in zip_file.infolist():\n        if condition in text_file.filename:\n            return pd.read_csv(zip_file.open(text_file.filename), delimiter=delimiter)","78a3530e":"def __load__data(__training_path, __test_path, concat=False):\n\t\"\"\"load data as input dataset\n\tparams: __training_path: the training path of input dataset\n\tparams: __test_path: the path of test dataset\n\tparams: if it is True, then it will concatinate the training and test dataset as output\n\treturns: generate final loaded dataset as dataset, input and test\n\t\"\"\"\n\t# LOAD DATA\n\timport pandas as pd\n\t__train_dataset = pd.read_csv(__training_path, delimiter=',')\n\t__test_dataset = pd.read_csv(__test_path, delimiter=',')\n\treturn __train_dataset, __test_dataset\n__train_dataset, __test_dataset = __load__data(__training_path, __test_path, concat=True)\n__train_dataset.head()","2d61a196":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 4))\naxes = axes.ravel()\nfor index,col in enumerate(['Age', 'Fare']):\n\tsns.histplot(__train_dataset[col],ax=axes[index])\n\taxes[index].tick_params(axis=\"x\", rotation=90)\n\taxes[index].set_title(\"Distribution of %s\" %col )\nfig.tight_layout();\n\n\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.ravel()\nfor index,col in enumerate(['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']):\n\tsns.countplot(x = __train_dataset[col],ax=axes[index])\n\taxes[index].tick_params(axis=\"x\", rotation=90)\n\taxes[index].set_title(\"Distribution of %s\" %col )\nfor index in range(-3,0):\n\taxes[index].set_visible(False)\nfig.tight_layout();\n\n\nplt.figure(figsize=(5,5))\nsns.countplot(x = __train_dataset[\"Survived\"])\nplt.xticks(rotation=90)\nplt.title(\"Distribution Plots for Target Column\")\nfig.tight_layout();","77af358c":"# STORE SUBMISSION RELEVANT COLUMNS\n__test_dataset_submission_columns = __test_dataset['PassengerId']","79796837":"# DISCARD IRRELEVANT COLUMNS\n__train_dataset.drop(['PassengerId'], axis=1, inplace=True)\n__test_dataset.drop(['PassengerId'], axis=1, inplace=True)","afc992a0":"# PREPROCESSING-1\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n_NUMERIC_COLS_WITH_MISSING_VALUES = ['SibSp', 'Age', 'Fare', 'Pclass', 'Parch']\nfor _col in _NUMERIC_COLS_WITH_MISSING_VALUES:\n    __simple_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n    __train_dataset[_col] = __simple_imputer.fit_transform(__train_dataset[_col].values.reshape(-1,1))[:,0]\n    if _col in __test_dataset:\n        __test_dataset[_col] = __simple_imputer.transform(__test_dataset[_col].astype(__train_dataset[_col].dtypes).values.reshape(-1,1))[:,0]","d2b36a18":"# PREPROCESSING-2\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n_STRING_COLS_WITH_MISSING_VALUES = ['Cabin', 'Embarked', 'Ticket', 'Name', 'Sex']\nfor _col in _STRING_COLS_WITH_MISSING_VALUES:\n    __simple_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    __train_dataset[_col] = __simple_imputer.fit_transform(__train_dataset[_col].values.reshape(-1,1))[:,0]\n    if _col in __test_dataset:\n        __test_dataset[_col] = __simple_imputer.transform(__test_dataset[_col].astype(__train_dataset[_col].dtypes).values.reshape(-1,1))[:,0]","4506b996":"# PREPROCESSING-3\nfrom sklearn.preprocessing import OrdinalEncoder\n_CATEGORICAL_COLS = ['Sex', 'Embarked']\n_ohe = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n__train_dataset[_CATEGORICAL_COLS] = pd.DataFrame(_ohe.fit_transform(__train_dataset[_CATEGORICAL_COLS]), columns=_CATEGORICAL_COLS)\n__test_dataset[_CATEGORICAL_COLS] = pd.DataFrame(_ohe.transform(__test_dataset[_CATEGORICAL_COLS]), columns=_CATEGORICAL_COLS)","02e2414c":"# PREPROCESSING-4\nimport nltk\nimport re\nimport string\n_TEXT_COLUMNS = ['Name', 'Ticket', 'Cabin']\ndef process_text(__dataset):\n    for _col in _TEXT_COLUMNS:\n        process_text = [t.lower() for t in __dataset[_col]]\n        # strip all punctuation\n        table = str.maketrans('', '', string.punctuation)\n        process_text = [t.translate(table) for t in process_text]\n        # convert all numbers in text to 'num'\n        process_text = [re.sub(r'\\d+', 'num', t) for t in process_text]\n        __dataset[_col] = process_text\n    return __dataset\n__train_dataset = process_text(__train_dataset)\n__test_dataset = process_text(__test_dataset)","5fa476e2":"_COLS_FOR_HEATMAP = ['Age', 'Fare', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'Survived']\nplt.figure(figsize=(20,20))\nplt.title(\"Pearson Correlation HeatMap of Features\")\nsns.heatmap(__train_dataset[_COLS_FOR_HEATMAP].corr(),annot=True)","f58c1b46":"# DETACH TARGET\n__feature_train = __train_dataset.drop(['Survived'], axis=1)\n__target_train =__train_dataset['Survived']\n__feature_test = __test_dataset","7f0e4e4c":"# PREPROCESSING-5\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport scipy.sparse as sparse\nfrom scipy.sparse import hstack, csr_matrix\n_TEXT_COLUMNS = ['Ticket', 'Name', 'Cabin']\n__temp_train_data = __feature_train[_TEXT_COLUMNS]\n__feature_train.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_train_object_array = []\n__temp_test_data = __feature_test[_TEXT_COLUMNS]\n__feature_test.drop(_TEXT_COLUMNS, axis=1, inplace=True)\n__feature_test_object_array = []\nfor _col in _TEXT_COLUMNS:\n    __tfidfvectorizer = TfidfVectorizer(max_features=3000)\n    vector_train = __tfidfvectorizer.fit_transform(__temp_train_data[_col])\n    __feature_train_object_array.append(vector_train)\n    vector_test = __tfidfvectorizer.transform(__temp_test_data[_col])\n    __feature_test_object_array.append(vector_test)\n__feature_train = sparse.hstack([__feature_train] + __feature_train_object_array).tocsr()\n__feature_test = sparse.hstack([__feature_test] + __feature_test_object_array).tocsr()","080394cb":"# MODEL\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n__model = RandomForestClassifier()\n__model.fit(__feature_train, __target_train) \n__y_pred = __model.predict(__feature_test)","3247de8e":"# SUBMISSION\nsubmission = pd.DataFrame(columns=['PassengerId'], data=__test_dataset_submission_columns)\nsubmission = pd.concat([submission, pd.DataFrame(__y_pred, columns=['Survived'])], axis=1)\nsubmission.head()","e79bc045":"# save submission file\nsubmission.to_csv(\"kaggle_submission.csv\", index=False)","35ed93c1":"<b> Is there any null value?<\/b> \nThe answer is <b>Yes<\/b>; let's review top 3 of those columns with the number of Null values.\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Column<\/th>\n      <th>#Null<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Cabin<\/td>\n      <td>687<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>Age<\/td>\n      <td>177<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Embarked<\/td>\n      <td>2<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\nAs partial of the results shown above, there are total <b>3<\/b> columns with Null values.","0c4ec992":"## Imputation Transformer\nWe will use which is an imputation transformer for completing missing values.\nWe can use out-of-the-box imputation transformer from Scikit-Learn packages. The detail and the list of complete parameters can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html)","08b52d81":"# Training and Prediction\nFirst, we will train a model based on pre-processed the training dataset.\nSecond, let's predict test values based on the trained model.","9f0e2a69":"## Skewness\nIn probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n\nMore detail can be found [here](https:\/\/en.wikipedia.org\/wiki\/Skewness) and the [Probability and Statistics Tables and Formulae](http:\/\/tomlr.free.fr\/Math%E9matiques\/Math%20Complete\/Probability%20and%20statistics\/CRC%20-%20standard%20probability%20and%20Statistics%20tables%20and%20formulae%20-%20DANIEL%20ZWILLINGER.pdf) by Zwillinger and Kokoska.\n\nHere are two samples Skewness data for positive and negative skew data.<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/f8\/Negative_and_positive_skew_diagrams_%28English%29.svg\" alt=\"source:Wikimedia\">\n\nFirst, we we will calculate the skewness for each columns in Titanic Dataset if each selected column has positive float\/integer values.\n\nSecond, we will review the following conditions.\n\n- if $Skewness>1$ or $Skewness<-1$, we will consider it as highly skewed;\n\n- if $0.5<Skewness<=1$ or $-0.5<Skewness<=-1$, we will consider it as moderate skewed;\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Column<\/th>\n      <th>Skewness<\/th>\n      <th>Skewness_Type<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Pclass<\/td>\n      <td>-0.630548<\/td>\n      <td>moderately skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>SibSp<\/td>\n      <td>3.695352<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Parch<\/td>\n      <td>2.749117<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>Fare<\/td>\n      <td>4.787317<\/td>\n      <td>highly skewed<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","1552f543":"## Remove Missing Values in Categorical Columns","fc2fd4a4":"## Encoding Ordinal Categorical Features\nLet's transfer categorical features as an integer array.\nWe will use Ordinal Encoder as explained [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html).\n\nIn the given input dataset there are <b>2<\/b> columns that can be transfered to integer and it includes:* Sex,Embarked *.","a98d7e5b":"### Discard Irrelevant Columns\nIn the given input dataset there are <b>1<\/b> column that can be removed as follows:* PassengerId *.","ccbe94e1":"# Load Competition Dataset","10caa592":"# Finding Intresting Datapoints\nLet's process each field by their histogram frequency and check if there is any intresting data point.\n\nThere are <b>4<\/b> number of intresting values in the following columns.\nThe below table shows each <b>Value<\/b> of each <b>Field<\/b>(column) with their total frequencies, <b>Lower<\/b> shows the lower frequency of normal distribution, <b>Upper<\/b> shows the upper bound frequency of normal distribution, and <b>Criteria<\/b> shows if the frequnecy passed <b>Upper bound<\/b> or <b>Lower bound<\/b>.\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Field<\/th>\n      <th>Value<\/th>\n      <th>Frequency<\/th>\n      <th>Lower<\/th>\n      <th>Upper<\/th>\n      <th>Criteria<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>Age<\/td>\n      <td>24.00<\/td>\n      <td>30<\/td>\n      <td>0.0088<\/td>\n      <td>29.9736<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>SibSp<\/td>\n      <td>0.00<\/td>\n      <td>608<\/td>\n      <td>5.0012<\/td>\n      <td>607.7606<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>Parch<\/td>\n      <td>0.00<\/td>\n      <td>678<\/td>\n      <td>1.0018<\/td>\n      <td>677.6640<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>Fare<\/td>\n      <td>8.05<\/td>\n      <td>43<\/td>\n      <td>1.0000<\/td>\n      <td>42.9753<\/td>\n      <td>Upper<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n\n\nFor example, in the <b>Age<\/b> column the value of <b>24.0<\/b> has <b>30<\/b> repeatation but this number is not between Lower bound(0.0088) and Upper bound(29.97360000000002).\n\n\nLet     $C_0=24.0$   and   $Freq(C_0)=30$     ,   $Upper(C_0)=29.97360000000002$     ,   $Lower(C_0)=0.0088$\n\n$Freq(C_0) > Upper(C_0)$.","85248bf0":"# Text Vectorizer\nIn the next step, we will transfer pre-processed text columns to a vector representation. The vector representations allows us to train a model based on numerical representations.\nWe will use TfidfVectorizer and more detail can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html).","0669ca42":"### Visualization for data distribution of columns","4dfd25de":"Let's review the dataset description:\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>count<\/th>\n      <th>mean<\/th>\n      <th>std<\/th>\n      <th>min<\/th>\n      <th>25%<\/th>\n      <th>50%<\/th>\n      <th>75%<\/th>\n      <th>max<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>PassengerId<\/th>\n      <td>891.0<\/td>\n      <td>446.000000<\/td>\n      <td>257.353842<\/td>\n      <td>1.00<\/td>\n      <td>223.5000<\/td>\n      <td>446.0000<\/td>\n      <td>668.5<\/td>\n      <td>891.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Survived<\/th>\n      <td>891.0<\/td>\n      <td>0.383838<\/td>\n      <td>0.486592<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>1.0<\/td>\n      <td>1.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Pclass<\/th>\n      <td>891.0<\/td>\n      <td>2.308642<\/td>\n      <td>0.836071<\/td>\n      <td>1.00<\/td>\n      <td>2.0000<\/td>\n      <td>3.0000<\/td>\n      <td>3.0<\/td>\n      <td>3.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Age<\/th>\n      <td>714.0<\/td>\n      <td>29.699118<\/td>\n      <td>14.526497<\/td>\n      <td>0.42<\/td>\n      <td>20.1250<\/td>\n      <td>28.0000<\/td>\n      <td>38.0<\/td>\n      <td>80.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>SibSp<\/th>\n      <td>891.0<\/td>\n      <td>0.523008<\/td>\n      <td>1.102743<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>1.0<\/td>\n      <td>8.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Parch<\/th>\n      <td>891.0<\/td>\n      <td>0.381594<\/td>\n      <td>0.806057<\/td>\n      <td>0.00<\/td>\n      <td>0.0000<\/td>\n      <td>0.0000<\/td>\n      <td>0.0<\/td>\n      <td>6.0000<\/td>\n    <\/tr>\n    <tr>\n      <th>Fare<\/th>\n      <td>891.0<\/td>\n      <td>32.204208<\/td>\n      <td>49.693429<\/td>\n      <td>0.00<\/td>\n      <td>7.9104<\/td>\n      <td>14.4542<\/td>\n      <td>31.0<\/td>\n      <td>512.3292<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","ebacd04a":"There is <b>2<\/b> unique value in <b>Survived<\/b> column which is a target column.\nlet's see frequency of values for the target column of {col}:\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>Survived<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>549<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>342<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","aefe23c6":"## Remove Missing Values in Numerical Columns\n\nIn the given input dataset there are <b>5 columns <\/b> with  missing data as follows:<b>Cabin, Embarked, Ticket, Name, Sex<\/b>\n\nThe following code removes the missing values from those columns. We use average value (median) of each column to replace the null values.","b4bc609b":"## Input Dataset","8ddf53ff":"### Target Column\nWe need to predict the target column.\nTherefore, we need to detach the target column in prediction.\nNote that if we don't drop the field, it will generate a model with high accuracy on training and worst accuracy on test (because the value in test dataset is Null).\nHere is the list of *target column*: <b>Survived<\/b>","f86a471d":"Competition dataset located in \"\/kaggle\/input\"; This path defined by Kaggle to access the competition file. We will list two files from this path as input files.","f1ec2a89":"In the given input dataset there are <b>5 columns<\/b> with  missing data as follows:<b>SibSp, Age, Fare, Pclass, Parch<\/b>\nThe following code removes the missing values from those columns. We use an average value (median) of each column to replace the Null values.","b45e136f":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/81\/Titanic_in_color.png\/320px-Titanic_in_color.png\"\/>\n<center>Image Source: Fidodog14 and SandyShores03, Public domain, via Wikimedia Commons<\/center>","616d65ab":"### Visualization for feature heatmap","30c876f6":"## Random Forest Classifier\nWe will use *RandomForestClassifier* which is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\nThe sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\nMore details about *RandomForestClassifier* can be found [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html).","0372fa21":"# Submission File\nWe have to maintain the target columns in \"submission.csv\" which will be submitted as our prediction results.","2f38a049":"# Exploratory Data Analysis (EDA)\n## General Structure\nTitanic Dataset includes <b>12<\/b> columns and <b>891<\/b> rows.\nThere are <b>3<\/b> different data types as follows: *int64, object, float64*.","1bc7c732":"# Text Processing\nThe dataset has <b>3<\/b> text values as follows: <b>Name,Ticket,Cabin<\/b>.\nNow, let's covert the text as follows.\n\n- First, convert text to lowercase;\n\n- Second, strip all punctuations;\n\n- Finally, convert all numbers in text to 'num'; therefore, in the next step our model will use a single token instead of valriety of tokens of numbers.","2753cc8c":"# About Competition\nTitanic ML competition is the best first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n Machine learning can predict which passengers survived the Titanic shipwreck. The competition is simple: use machine learning to create a model that predicts which survivors survived the wreck.Competition file is available [here](https:\/\/www.kaggle.com\/c\/titanic).","35691355":"# Input Dataset"}}