{"cell_type":{"cef0ca1d":"code","3fcaed37":"code","acd0f39b":"code","3c81babc":"code","1430d52d":"code","77550ff9":"code","4d341c05":"code","74371125":"code","c64db024":"code","0a3edd7d":"code","3238cdeb":"code","052d76bc":"code","90b42c94":"code","98860674":"code","b0d42c87":"code","a096b167":"code","ff574956":"code","cc42fa83":"code","a512f772":"code","53788b07":"code","5c406bf7":"code","104c4fe1":"code","c8f2b173":"code","e01978d2":"code","96095146":"code","a3fbaf99":"code","ea86d669":"code","18e3ecdf":"code","d557bf52":"code","6dd56b45":"code","d8eb6024":"code","efb42a86":"code","25daee50":"code","5da028e0":"code","24c1e337":"code","dac11670":"code","0433e3bb":"code","4a753f8e":"code","4c7dc240":"code","688c41c0":"code","575eccae":"code","2f6691e0":"code","1581d71d":"code","8eb73049":"code","7640a2ea":"code","b537d72f":"code","efa95684":"code","500fce3a":"code","10da924a":"code","236fbb23":"code","ca0c5514":"code","ecc85b7c":"code","e6c2cbba":"code","af5b3a01":"code","31514353":"code","23144456":"code","ccc7021b":"code","96fb7d66":"code","91bd2d48":"code","6cef56c7":"code","788a13bf":"code","59699d0f":"code","37d3aee0":"code","f5d77c11":"code","44390ca3":"code","c3a03840":"code","df8cb7b0":"code","ba8055c3":"markdown","9c0c3343":"markdown","cde96994":"markdown","613f06ef":"markdown","8f35d9f3":"markdown","1790732c":"markdown","a1f04435":"markdown","1a106ea2":"markdown","4494e74e":"markdown","faa0160c":"markdown","7fe927b2":"markdown","b16bc49c":"markdown","2d02f904":"markdown","bbafa622":"markdown","ffe866e9":"markdown","748acfa4":"markdown","974a6848":"markdown","fb2a1c9f":"markdown","87a50a55":"markdown","9cded6a9":"markdown","f12adc7a":"markdown","d505a72c":"markdown","fd6a9f6d":"markdown","373a9053":"markdown","14654082":"markdown","af7f7a5e":"markdown","a54a3ec2":"markdown","29e90c21":"markdown","6e009167":"markdown","9c71a204":"markdown","cd9a356b":"markdown","93805f3d":"markdown","5dba015f":"markdown","656ce50f":"markdown","8369325f":"markdown","125b7454":"markdown","3365171c":"markdown","430d1ca8":"markdown","af1d341d":"markdown","7fbc796e":"markdown","f7ff1f9d":"markdown","84773bcf":"markdown","bd2275eb":"markdown","4bafa28b":"markdown","72a0aca2":"markdown","516ecf3b":"markdown","075bbf42":"markdown","5ffc77a7":"markdown","b8baf98e":"markdown","6df29f24":"markdown","5b1a4530":"markdown"},"source":{"cef0ca1d":"\n\n!pip install -U git+https:\/\/github.com\/albu\/albumentations\n\n","3fcaed37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imread\nfrom mpl_toolkits.mplot3d import Axes3D\nimport scipy.ndimage as ndimage\nimport cv2\nfrom tqdm import tqdm#_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom functools import reduce\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom scipy.optimize import minimize\nimport cv2\nimport glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom torchvision import transforms, utils\n\nfrom albumentations import Compose, ShiftScaleRotate, RandomCrop, \\\n    ToGray, CLAHE, GaussNoise, RandomGamma, \\\n    RandomBrightnessContrast, RGBShift, HueSaturationValue,Normalize\nfrom albumentations import *\nfrom albumentations.pytorch import ToTensor\n\n\nfrom tqdm import tqdm_notebook\n\nPATH = '..\/input\/pku-autonomous-driving\/'\nos.listdir(PATH)\nfrom skimage.io import imread\nimport os\nfrom math import sin, cos","acd0f39b":"DATASET_DIR = '\/kaggle\/input\/pku-autonomous-driving\/'","3c81babc":"from albumentations import (HorizontalFlip,OpticalDistortion,VerticalFlip,GridDistortion,RandomBrightnessContrast,OneOf,ElasticTransform,RandomGamma,IAAEmboss,Blur,RandomRotate90,Transpose, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)","1430d52d":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/1W9q5SjaJTc\"rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","77550ff9":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\"  src=\"https:\/\/www.youtube.com\/embed\/GlbZA62eXpU\" rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","4d341c05":"print(os.listdir(PATH))","74371125":"train = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'sample_submission.csv')","c64db024":"test_size = len(test)\ntest_size","0a3edd7d":"public_test = int(test_size * 0.09)","3238cdeb":"print(public_test,'samples for testing public lb!!')","052d76bc":"imageFile =  PATH + 'train_images\/ID_60565cf6d.jpg'\nmat = imread(imageFile)\nmat = mat[:,:,0] # get the first channel\nrows, cols = mat.shape\nxv, yv = np.meshgrid(range(cols), range(rows)[::-1])\n     \nblurred = ndimage.gaussian_filter(mat, sigma=(5, 5), order=0)\nfig = plt.figure(figsize=(40,40))\n     \nax = fig.add_subplot(221)\nax.imshow(mat, cmap='gray')\n     \nax = fig.add_subplot(222, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, mat)\n     \nax = fig.add_subplot(223)\nax.imshow(blurred, cmap='gray')\n     \nax = fig.add_subplot(224, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, blurred)\nplt.show()","90b42c94":"imageFile =  PATH + 'test_masks\/ID_f64a00c44.jpg'\nmat = imread(imageFile)\nmat = mat[:,:,0] # get the first channel\nrows, cols = mat.shape\nxv, yv = np.meshgrid(range(cols), range(rows)[::-1])\n     \nblurred = ndimage.gaussian_filter(mat, sigma=(5, 5), order=0)\nfig = plt.figure(figsize=(50,50))\n     \nax = fig.add_subplot(221)\nax.imshow(mat, cmap='gray')\n     \nax = fig.add_subplot(222, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, mat)\n     \nax = fig.add_subplot(223)\nax.imshow(blurred, cmap='gray')\n     \nax = fig.add_subplot(224, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, blurred)\nplt.show()","98860674":"imageFile =  PATH + 'train_images\/ID_01021ce21.jpg'\nmat = imread(imageFile)\n\nfrom numpy import asarray\nfrom PIL import Image\n# load image\n#image = mat #Image.open('sydney_bridge.jpg')\n#image = Image.open(PATH + 'train_images\/ID_60565cf6d.jpg')\nimage = cv2.imread(PATH + 'train_images\/ID_60565cf6d.jpg')\nimage = cv2.resize(image, (1536, 512))    \n#print(image.shape)\npixels = asarray(image)\n# convert from integers to floats\npixels = pixels.astype('float32')\n# calculate per-channel means and standard deviations\nmeans = pixels.mean(axis=(0,1), dtype='float64')\nstds = pixels.std(axis=(0,1), dtype='float64')\nprint('Means: %s, Stds: %s' % (means, stds))\n# per-channel standardization of pixels\npixels = (pixels - means) \/ stds\n# confirm it had the desired effect\nmeans = pixels.mean(axis=(0,1), dtype='float64')\nstds = pixels.std(axis=(0,1), dtype='float64')\nprint('Means: %s, Stds: %s' % (means, stds))","b0d42c87":"image = cv2.imread(PATH + 'train_images\/ID_60565cf6d.jpg').astype(np.float32) \/ 255\nimg = image\nimg = cv2.resize(img, (1536, 512))\nprint(image.shape)\nprint(image.mean())\nprint(image.std())\n\nimg -= img.mean()\nimg \/= img.std()\nprint('-------------------------------')\nprint(img.shape)\nprint(img.mean())\nprint(img.std())","a096b167":"h = 1536\nw = 512\n","ff574956":"def compute_sample_mean(im_dir, image_files):\n    \"\"\"\n    Parameters:\n        im_dir: The directory that contains images.\n        image_files: List of image files you need.\n\n    Returns:\n        sample-mean\n    \"\"\"\n\n    if len(image_files) > 0:\n        image_sum = cv2.imread(os.path.join(im_dir, image_files[0]))\n        image_sum = cv2.resize(image_sum, (h, w))  \n        image_sum = cv2.cvtColor(image_sum, cv2.COLOR_BGR2RGB).astype(np.float64)\n\n        for image_file in tqdm_notebook(image_files[1:]):\n            img = cv2.imread(os.path.join(im_dir, image_file))\n            img = cv2.resize(img, (h, w))  \n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            image_sum += img\n\n        return image_sum\/(255*len(image_files))\n\n    return None\n\n\ndef compute_pixel_mean(im_dir, image_files, sample_mean=None):\n    \"\"\"\n    Parameters:\n        im_dir: The directory that contains images.\n        image_files: List of image files you need.\n        sample_mean: sample-mean value.\n\n    Returns:\n        pixel-mean\n    \"\"\"\n\n    if sample_mean is None:\n        sample_mean = compute_sample_mean(im_dir, image_files)\n\n    return np.mean(sample_mean, axis=(0, 1))\n\n\ndef compute_sample_std(im_dir, image_files, sample_mean=None):\n    \"\"\"\n    Parameters:\n        im_dir: The directory that contains images.\n        image_files: List of image files you need.\n        sample_mean: sample-mean value.\n\n    Returns:\n        sample-std\n    \"\"\"\n\n    if len(image_files) > 0:\n        if sample_mean is None:\n            sample_mean = compute_sample_mean(im_dir, image_files)\n\n        square_diff_sum = 0\n        for image_file in tqdm_notebook(image_files):\n            img = cv2.imread(os.path.join(im_dir, image_file))\n            img = cv2.resize(img, (h, w))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float64)\n            img \/= 255.\n            square_diff_sum += np.square(np.abs(img - sample_mean))\n\n        return np.sqrt(square_diff_sum\/len(image_files))\n\n    return 0\n\n\ndef compute_pixel_std(im_dir, image_files, sample_mean=None, pixel_mean=None):\n    \"\"\"\n    Parameters:\n        im_dir: The directory that contains images.\n        image_files: List of image files you need.\n        sample_mean: sample-mean value.\n        pixel_mean: pixel-mean value.\n\n    Returns:\n        pixel-std\n    \"\"\"\n\n    if len(image_files) > 0:\n        if pixel_mean is None:\n            if sample_mean is not None:\n                pixel_mean = compute_pixel_mean(im_dir, image_files, sample_mean)\n            else:\n                pixel_mean = compute_pixel_mean(im_dir, image_files)\n\n        square_diff_sum = 0\n        for image_file in tqdm_notebook(image_files):\n            img = cv2.imread(os.path.join(im_dir, image_file))\n            img = cv2.resize(img, (h, w))\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float64)\n            img \/= 255.\n            square_diff_sum += np.sum(np.square(np.abs(img - pixel_mean)), axis=(0, 1))\n\n        return np.sqrt(square_diff_sum\/(len(image_files)*img.shape[0]*img.shape[1]))\n\n    return 0","cc42fa83":"train_images_dir = os.path.join(DATASET_DIR, 'train_images')\nimage_files = os.listdir(train_images_dir)","a512f772":"sample_mean = compute_sample_mean(train_images_dir, image_files)\nsample_std = compute_sample_std(train_images_dir, image_files, sample_mean=sample_mean)\npixel_mean = compute_pixel_mean(train_images_dir, image_files, sample_mean=sample_mean)\npixel_std = compute_pixel_std(train_images_dir, image_files, pixel_mean=pixel_mean)","53788b07":"print('Sample mean:', sample_mean)","5c406bf7":"print('Sample std:', sample_std)","104c4fe1":"print('Pixel mean:', pixel_mean)\nprint('Pixel std:', pixel_std)","c8f2b173":"train_images_dir = os.path.join(DATASET_DIR, 'test_images')\nimage_files = os.listdir(train_images_dir)","e01978d2":"sample_mean = compute_sample_mean(train_images_dir, image_files)\nsample_std = compute_sample_std(train_images_dir, image_files, sample_mean=sample_mean)\npixel_mean = compute_pixel_mean(train_images_dir, image_files, sample_mean=sample_mean)\npixel_std = compute_pixel_std(train_images_dir, image_files, pixel_mean=pixel_mean)","96095146":"print('Sample mean:', sample_mean)","a3fbaf99":"print('Sample std:', sample_std)","ea86d669":"print('Pixel mean:', pixel_mean)\nprint('Pixel std:', pixel_std)","18e3ecdf":"imageFile =  PATH + 'test_masks\/ID_d17f58f89.jpg'\nmat = imread(imageFile)\nmat = mat[:,:,0] # get the first channel\nrows, cols = mat.shape\nxv, yv = np.meshgrid(range(cols), range(rows)[::-1])\n     \nblurred = ndimage.gaussian_filter(mat, sigma=(5, 5), order=0)\nfig = plt.figure(figsize=(50,50))\n     \nax = fig.add_subplot(221)\nax.imshow(mat, cmap='gray')\n     \nax = fig.add_subplot(222, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, mat)\n     \nax = fig.add_subplot(223)\nax.imshow(blurred, cmap='gray')\n     \nax = fig.add_subplot(224, projection='3d')\nax.elev= 75\nax.plot_surface(xv, yv, blurred)\nplt.show()","d557bf52":"# taken from : https:\/\/www.kaggle.com\/robikscube\/autonomous-driving-introduction-data-review\n\ntrain_expanded = pd.concat([train, train['PredictionString'].str.split(' ', expand=True)], axis=1)\ntrain_expanded = train_expanded.rename(columns={0 : '1_model_type', 1 : '1_yaw', 2 : '1_pitch',\n                                                3 : '1_roll', 4 : '1_x', 5 : '1_y', 6 : '1_z'})\ntrain_expanded.drop('PredictionString', axis=1).head()","6dd56b45":"new_train_objects = train_expanded\nplot = sns.jointplot(x=new_train_objects['1_x'][:1000], y=new_train_objects['1_y'][:1000], kind='kde', color='blueviolet')\nplot.set_axis_labels('1_x', '1_y', fontsize=16)\nplt.show()","d8eb6024":"center_x = []\nab = new_train_objects['1_x']\nfor i in range(len(ab)):\n    center_x.append(float(ab[i])) \n    \n","efb42a86":"center_y = []\nab = new_train_objects['1_y']\nfor i in range(len(ab)):\n    center_y.append(float(ab[i])) ","25daee50":"center_z = []\nab = new_train_objects['1_z']\nfor i in range(len(ab)):\n    center_z.append(float(ab[i])) \n","5da028e0":"pitch = []\nab = new_train_objects['1_pitch']\nfor i in range(len(ab)):\n    pitch.append(float(ab[i])) \n","24c1e337":"roll = []\nab = new_train_objects['1_roll']\nfor i in range(len(ab)):\n    roll.append(float(ab[i])) \n    ","dac11670":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(center_z, color='red', ax=ax).set_title('center_z', fontsize=16)\nplt.xlabel('center_z', fontsize=15)\nplt.show()","0433e3bb":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(pitch, color='black', ax=ax).set_title('pitch', fontsize=16)\nplt.xlabel('pitch', fontsize=15)\nplt.show()","4a753f8e":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(roll, color='purple', ax=ax).set_title('Roll', fontsize=16)\nplt.xlabel('Roll', fontsize=15)\nplt.show()","4c7dc240":"\nfig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(center_x, color='darkorange', ax=ax).set_title('center_x and center_y', fontsize=16)\nsns.distplot(center_y, color='purple', ax=ax).set_title('center_x and center_y', fontsize=16)\nplt.xlabel('center_x and center_y', fontsize=15)\nplt.show()","688c41c0":"yaw = []\nab = new_train_objects['1_yaw']\nfor i in range(len(ab)):\n    yaw.append(float(ab[i])) ","575eccae":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.distplot(yaw, color='darkgreen', ax=ax).set_title('yaw', fontsize=16)\nplt.xlabel('yaw', fontsize=15)\nplt.show()","2f6691e0":"#from car_models_json file\ncar_models = ('guangqi-chuanqi-GS4-2015.json', 'sikeda-jingrui.json', 'dazhong-SUV.json', \n'biyadi-qin.json', 'jipu-3.json', 'qirui-ruihu.json', 'leikesasi.json', 'biaozhi-liangxiang.json',\n'rongwei-750.json', 'fengtian-puladuo-06.json', 'changanbenben.json', 'changan-cs5.json',\n'lingmu-SX4-2012.json', 'biaozhi-508.json', 'linken-SUV.json', 'Skoda_Fabia-2011.json', 'dazhong.json',\n'yiqi-benteng-b50.json', 'baoma-X5.json', 'jili-boyue.json', 'biyadi-2x-F0.json', 'biaozhi-3008.json',\n'qiya.json', 'sikeda-SUV.json', '036-CAR01.json', 'fengtian-liangxiang.json', 'jilixiongmao-2015.json',\n'fengtian-SUV-gai.json', 'dongfeng-xuetielong-C6.json', 'fengtian-MPV.json', 'xiandai-suonata.json',\n'baoma-530.json', 'baojun-510.json', 'baoma-330.json', 'biyadi-F3.json', '037-CAR02.json',\n'lingmu-aotuo-2009.json', 'baojun-310-2017.json', 'fengtian-weichi-2006.json', 'bieke-yinglang-XT.json', \n'biaozhi-408.json', 'lufeng-X8.json', 'yingfeinidi-qx80.json', 'changcheng-H6-2016.json', 'fute.json', \n'mazida-6-2015.json', 'bieke.json', 'oubao.json', 'bieke-kaiyue.json', 'feiyate.json', 'haima-3.json',\n'baoshijie-kayan.json', 'yingfeinidi-SUV.json', 'dongfeng-yulong-naruijie.json', \n'dongfeng-fengguang-S560.json', 'dazhongmaiteng.json', 'lingmu-swift.json', 'benchi-ML500.json', \n'sanling-oulande.json', 'bentian-fengfan.json', 'baoshijie-paoche.json', 'MG-GT-2015.json', \n'beiqi-huansu-H3.json', 'biyadi-tang.json', 'jianghuai-ruifeng-S3.json', 'rongwei-RX5.json', \n'dongnan-V3-lingyue-2011.json', 'kaidilake-CTS.json', '019-SUV.json', 'benchi-SUR.json',\n'dongfeng-fengxing-SX6.json', 'dihao-EV.json', 'aodi-a6.json', 'xiandai-i25-2016.json', \n'dongfeng-DS5.json', 'changan-CS35-2012.json', 'benchi-GLK-300.json', 'supai-2016.json', 'aodi-Q7-SUV.json')\n","1581d71d":"suv = twox = threex = 0\nfor i in range(len(car_models)):\n    with open('..\/input\/pku-autonomous-driving\/car_models_json\/'+car_models[i]) as json_file:\n        data = json.load(json_file)\n        print('car_type: '+data['car_type'])\n        if(data['car_type'] == 'SUV'):\n            suv+=1\n        elif(data['car_type'] == '2x'):\n            twox+=1\n        else:\n            threex+=1\n        \n","8eb73049":"print('Total SUV:', suv)       \nprint('Total 2x:', twox)  \nprint('Total 3x:', threex)  ","7640a2ea":"#taken from here  : https:\/\/www.kaggle.com\/ebouteillon\/load-a-3d-car-model\n\nwith open('..\/input\/pku-autonomous-driving\/car_models_json\/'+car_models[0]) as json_file:\n    data = json.load(json_file)\n    vertices = np.array(data['vertices'])\n    triangles = np.array(data['faces']) - 1\n    plt.figure(figsize=(20,10))\n    ax = plt.axes(projection='3d')\n    ax.set_title('car_type: '+data['car_type'])\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n    ax.set_zlim([0, 3])\n    ax.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='green')\n\nwith open('..\/input\/pku-autonomous-driving\/car_models_json\/'+car_models[1]) as json_file:\n    data = json.load(json_file)\n    vertices = np.array(data['vertices'])\n    triangles = np.array(data['faces']) - 1\n    plt.figure(figsize=(20,10))\n    ax = plt.axes(projection='3d')\n    ax.set_title('car_type: '+data['car_type'])\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n    ax.set_zlim([0, 3])\n    ax.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='red')\n    \nwith open('..\/input\/pku-autonomous-driving\/car_models_json\/'+car_models[3]) as json_file:\n    data = json.load(json_file)\n    vertices = np.array(data['vertices'])\n    triangles = np.array(data['faces']) - 1\n    plt.figure(figsize=(20,10))\n    ax = plt.axes(projection='3d')\n    ax.set_title('car_type: '+data['car_type'])\n    ax.set_xlim([-3, 3])\n    ax.set_ylim([-3, 3])\n    ax.set_zlim([0, 3])\n    ax.plot_trisurf(vertices[:,0], vertices[:,2], triangles, -vertices[:,1], shade=True, color='yellow')\n","b537d72f":"print(os.listdir(PATH + 'camera'))","efa95684":"file=open(PATH + '\/camera\/camera_intrinsic.txt', \"r\").read().split('\\n')\nfor i in range(len(file)):\n    print(file[i])","500fce3a":"#Taken From : https:\/\/www.kaggle.com\/hocop1\/centernet-baseline\n# From camera.zip\ncamera_matrix = np.array([[2304.5479, 0,  1686.2379],\n                          [0, 2305.8757, 1354.9849],\n                          [0, 0, 1]], dtype=np.float32)\ncamera_matrix_inv = np.linalg.inv(camera_matrix)\n\ndef str2coords(s, names=['id', 'yaw', 'pitch', 'roll', 'x', 'y', 'z']):\n    '''\n    Input:\n        s: PredictionString (e.g. from train dataframe)\n        names: array of what to extract from the string\n    Output:\n        list of dicts with keys from `names`\n    '''\n    coords = []\n    for l in np.array(s.split()).reshape([-1, 7]):\n        coords.append(dict(zip(names, l.astype('float'))))\n        if 'id' in coords[-1]:\n            coords[-1]['id'] = int(coords[-1]['id'])\n    return coords\n\ndef get_img_coords(s):\n    '''\n    Input is a PredictionString (e.g. from train dataframe)\n    Output is two arrays:\n        xs: x coordinates in the image\n        ys: y coordinates in the image\n    '''\n    coords = str2coords(s)\n    xs = [c['x'] for c in coords]\n    ys = [c['y'] for c in coords]\n    zs = [c['z'] for c in coords]\n    P = np.array(list(zip(xs, ys, zs))).T\n    img_p = np.dot(camera_matrix, P).T\n    img_p[:, 0] \/= img_p[:, 2]\n    img_p[:, 1] \/= img_p[:, 2]\n    img_xs = img_p[:, 0]\n    img_ys = img_p[:, 1]\n    img_zs = img_p[:, 2] # z = Distance from the camera\n    return img_xs, img_ys\n\nplt.figure(figsize=(20,20))\nplt.imshow(imread(PATH + 'train_images\/' + train['ImageId'][1] + '.jpg'))\nplt.scatter(*get_img_coords(train['PredictionString'][1]), color='red', s=100);","10da924a":"a = train\na.head()\n","236fbb23":"a.PredictionString[0]","ca0c5514":"i = 0\nfor img in glob.glob(PATH + \"train_images\/*.jpg\"):\n    i+=1\n    if(i==6):\n        break\n    img = cv2.imread(img)\n    plt.figure(figsize=(15,8))\n    plt.imshow(img);","ecc85b7c":"i = 0\nfor img in glob.glob(PATH + \"train_masks\/*.jpg\"):\n    i+=1\n    if(i==6):\n        break\n    img = cv2.imread(img)\n    plt.figure(figsize=(15,8))\n    plt.imshow(img);","e6c2cbba":"i = 0\nfor img in glob.glob(PATH + \"test_images\/*.jpg\"):\n    i+=1\n    if(i==11):\n        break\n    img = cv2.imread(img)\n    plt.figure(figsize=(15,8))\n    plt.imshow(img);","af5b3a01":"i = 0\nfor img in glob.glob(PATH + \"test_masks\/*.jpg\"):\n    i+=1\n    if(i==11):\n        break\n    img = cv2.imread(img)\n    plt.figure(figsize=(15,8))\n    plt.imshow(img);","31514353":"train = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'sample_submission.csv')\nbad_list = ['ID_1a5a10365',\n'ID_1db0533c7',\n'ID_53c3fe91a',\n'ID_408f58e9f',\n'ID_4445ae041',\n'ID_bb1d991f6',\n'ID_c44983aeb',\n'ID_f30ebe4d4']\ntrain = train.loc[~train['ImageId'].isin(bad_list)]\n# From camera.zip\ncamera_matrix = np.array([[2304.5479, 0,  1686.2379],\n                          [0, 2305.8757, 1354.9849],\n                          [0, 0, 1]], dtype=np.float32)\ncamera_matrix_inv = np.linalg.inv(camera_matrix)\n\ntrain.head()","23144456":"def imread(path, fast_mode=False):\n    img = cv2.imread(path)\n    if not fast_mode and img is not None and len(img.shape) == 3:\n        img = np.array(img[:, :, ::-1])\n    return img\n\nimg = imread(PATH + 'train_images\/ID_8a6e65317' + '.jpg')\nIMG_SHAPE = img.shape\n\nplt.figure(figsize=(15,8))\nplt.imshow(img);","ccc7021b":"def rotate(x, angle):\n    x = x + angle\n    x = x - (x + np.pi) \/\/ (2 * np.pi) * 2 * np.pi\n    return x\n","96fb7d66":"IMG_WIDTH = 2048\nIMG_HEIGHT = 512\nMODEL_SCALE = 8\n\ndef _regr_preprocess(regr_dict, flip=False):\n    if flip:\n        for k in ['x', 'pitch', 'roll']:\n            regr_dict[k] = -regr_dict[k]\n    for name in ['x', 'y', 'z']:\n        regr_dict[name] = regr_dict[name] \/ 100\n    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n    regr_dict.pop('pitch')\n    regr_dict.pop('id')\n    return regr_dict\n\ndef _regr_back(regr_dict):\n    for name in ['x', 'y', 'z']:\n        regr_dict[name] = regr_dict[name] * 100\n    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n    \n    pitch_sin = regr_dict['pitch_sin'] \/ np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n    pitch_cos = regr_dict['pitch_cos'] \/ np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n    return regr_dict\n\ndef preprocess_image(img, flip=False):\n    img = img[img.shape[0] \/\/ 2:]\n    bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n    bg = bg[:, :img.shape[1] \/\/ 6]\n    img = np.concatenate([bg, img, bg], 1)\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    if flip:\n        img = img[:,::-1]\n    return (img \/ 255).astype('float32')\n\ndef get_mask_and_regr(img, labels, flip=False):\n    mask = np.zeros([IMG_HEIGHT \/\/ MODEL_SCALE, IMG_WIDTH \/\/ MODEL_SCALE], dtype='float32')\n    regr_names = ['x', 'y', 'z', 'yaw', 'pitch', 'roll']\n    regr = np.zeros([IMG_HEIGHT \/\/ MODEL_SCALE, IMG_WIDTH \/\/ MODEL_SCALE, 7], dtype='float32')\n    coords = str2coords(labels)\n    xs, ys = get_img_coords(labels)\n    for x, y, regr_dict in zip(xs, ys, coords):\n        x, y = y, x\n        x = (x - img.shape[0] \/\/ 2) * IMG_HEIGHT \/ (img.shape[0] \/\/ 2) \/ MODEL_SCALE\n        x = np.round(x).astype('int')\n        y = (y + img.shape[1] \/\/ 6) * IMG_WIDTH \/ (img.shape[1] * 4\/3) \/ MODEL_SCALE\n        y = np.round(y).astype('int')\n        if x >= 0 and x < IMG_HEIGHT \/\/ MODEL_SCALE and y >= 0 and y < IMG_WIDTH \/\/ MODEL_SCALE:\n            mask[x, y] = 1\n            regr_dict = _regr_preprocess(regr_dict, flip)\n            regr[x, y] = [regr_dict[n] for n in sorted(regr_dict)]\n    if flip:\n        mask = np.array(mask[:,::-1])\n        regr = np.array(regr[:,::-1])\n    return mask, regr","91bd2d48":"class CarDataset(Dataset):\n    \"\"\"Car dataset.\"\"\"\n\n    def __init__(self, dataframe, root_dir, training=True, transform=True):\n        self.df = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.training = training\n        if self.transform :\n            if self.training :\n                self.aug = Compose([\n                    \n                            #RandomGamma(p=0.2),\n                            #HorizontalFlip(),\n\n                            Cutout(num_holes=20,p=0.1),\n                        \n                            ])\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # Get image name\n        idx, labels = self.df.values[idx]\n        img_name = self.root_dir.format(idx)\n        \n        # Read image\n        img0 = imread(img_name, True)\n        img = preprocess_image(img0)\n        img = np.rollaxis(img, 2, 0)\n        # Get mask and regression maps\n        if self.training:\n            mask, regr = get_mask_and_regr(img0, labels)\n            regr = np.rollaxis(regr, 2, 0)\n        else:\n            mask, regr = 0, 0\n        if self.transform:\n            augmented = self.aug(image=img, mask=mask,regr=regr)\n            img, mask,regr = augmented['image'],augmented['mask'] ,augmented['regr']\n            \n        return [img, mask, regr]","6cef56c7":"train_images_dir = PATH + 'train_images\/{}.jpg'\ntest_images_dir = PATH + 'test_images\/{}.jpg'\n\ndf_train, df_dev = train_test_split(train, test_size=0.1, random_state=63)\ndf_test = test\n\n# Create dataset objects\ntrain_dataset = CarDataset(df_train, train_images_dir)\ndev_dataset = CarDataset(df_dev, train_images_dir)\ntest_dataset = CarDataset(df_test, test_images_dir)","788a13bf":"img, mask, regr = dev_dataset[10]\n\nplt.figure(figsize=(10,10))\nplt.imshow(np.rollaxis(img, 0, 3))\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(mask)\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(regr[-2])\nplt.show()","59699d0f":"class CarDataset(Dataset):\n    \"\"\"Car dataset.\"\"\"\n\n    def __init__(self, dataframe, root_dir, training=True, transform=True):\n        self.df = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.training = training\n        if self.transform :\n            if self.training :\n                self.aug = Compose([\n                    \n                            #RandomGamma(p=0.2),\n                            HorizontalFlip(),\n\n                         \n                        \n                            ])\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # Get image name\n        idx, labels = self.df.values[idx]\n        img_name = self.root_dir.format(idx)\n        \n        # Read image\n        img0 = imread(img_name, True)\n        img = preprocess_image(img0)\n        img = np.rollaxis(img, 2, 0)\n        # Get mask and regression maps\n        if self.training:\n            mask, regr = get_mask_and_regr(img0, labels)\n            regr = np.rollaxis(regr, 2, 0)\n        else:\n            mask, regr = 0, 0\n        if self.transform:\n            augmented = self.aug(image=img, mask=mask,regr=regr)\n            img, mask,regr = augmented['image'],augmented['mask'] ,augmented['regr']\n            \n        return [img, mask, regr]","37d3aee0":"train_images_dir = PATH + 'train_images\/{}.jpg'\ntest_images_dir = PATH + 'test_images\/{}.jpg'\n\ndf_train, df_dev = train_test_split(train, test_size=0.1, random_state=63)\ndf_test = test\n\n# Create dataset objects\ntrain_dataset = CarDataset(df_train, train_images_dir)\ndev_dataset = CarDataset(df_dev, train_images_dir)\ntest_dataset = CarDataset(df_test, test_images_dir)","f5d77c11":"img, mask, regr = dev_dataset[10]\n\nplt.figure(figsize=(10,10))\nplt.imshow(np.rollaxis(img, 0, 3))\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(mask)\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(regr[-2])\nplt.show()","44390ca3":"import albumentations as A\nA.MultiplicativeNoise()","c3a03840":"\n\nclass CarDataset(Dataset):\n    \"\"\"Car dataset.\"\"\"\n\n    def __init__(self, dataframe, root_dir, training=True, transform=True):\n        self.df = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.training = training\n        if self.transform :\n            if self.training :\n                self.aug = Compose([\n                    \n                            #RandomGamma(p=0.2),\n                            #A.ISONoise(),\n                            A.IAAPerspective(p=1,keep_size=False),\n                            A.GaussNoise()\n                         \n                        \n                            ])\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # Get image name\n        idx, labels = self.df.values[idx]\n        img_name = self.root_dir.format(idx)\n        \n        # Read image\n        img0 = imread(img_name, True)\n        img = preprocess_image(img0)\n        img = np.rollaxis(img, 2, 0)\n        # Get mask and regression maps\n        if self.training:\n            mask, regr = get_mask_and_regr(img0, labels)\n            regr = np.rollaxis(regr, 2, 0)\n        else:\n            mask, regr = 0, 0\n        if self.transform:\n            augmented = self.aug(image=img, mask=mask,regr=regr)\n            img, mask,regr = augmented['image'],augmented['mask'] ,augmented['regr']\n            \n        return [img, mask, regr]\n    \n    \ntrain_images_dir = PATH + 'train_images\/{}.jpg'\ntest_images_dir = PATH + 'test_images\/{}.jpg'\n\ndf_train, df_dev = train_test_split(train, test_size=0.1, random_state=63)\ndf_test = test\n\n# Create dataset objects\ntrain_dataset = CarDataset(df_train, train_images_dir)\ndev_dataset = CarDataset(df_dev, train_images_dir)\ntest_dataset = CarDataset(df_test, test_images_dir)\n\nimg, mask, regr = dev_dataset[10]\n\nplt.figure(figsize=(20,10))\nplt.imshow(np.rollaxis(img, 0, 3))\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(mask)\nplt.show()\n\nplt.figure(figsize=(8,8))\nplt.imshow(regr[-2])\nplt.show()","df8cb7b0":"train_dataset = CarDataset(df_train, train_images_dir, transform=False)\nBATCH_SIZE = 2\n\n# Create data generators - they will produce batches\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,drop_last=True)\ndev_loader = DataLoader(dataset=dev_dataset, batch_size=2, shuffle=False, num_workers=2,drop_last=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)","ba8055c3":"**Lets Visualize one SUV model,one 2x model and one 3x model car below**","9c0c3343":"**we can see inconsistency in pitch and roll distribution**","cde96994":"**Converting ID_60565cf6d.jpg image from train_images directory to 3D**","613f06ef":"# what to do in this competition?","8f35d9f3":"**Lets visualize a messy mask from test_masks directory**","1790732c":"IF YOU SPOT ANY ERROR,PLEASE LET ME KNOW IN THE COMMENT SECTION AND IT WILL BE HIGHLY APPRECIATED\n\n**Thanks for reading**","a1f04435":"# lets try GaussNoise ","1a106ea2":"**REF : https:\/\/www.kaggle.com\/hocop1\/centernet-baseline**","4494e74e":"**Imports**","faa0160c":"**Directories**","7fe927b2":"<h1 style=\"color:blue;\">\nIf you find this kernel interesting, please drop an  <font color=\"red\">UPVOTE<\/font>. It motivates me to produce more quality content :)\n<\/h1>","b16bc49c":"**size of test set**","2d02f904":"*the code below shows the image as a surface in 3D using MatPlotLib, numpy, and ndimage (for filtering).If the image is grayscale, the gray values of each pixel can determine the height of the surface.It may be necessary to blur (i.e. filter) the image to smooth out spikes that will occur due to adjacent pixels with very different values.The meshgrid function is used to set up a mesh for the surface. The \u2018[::-1]\u2019 reverses the meshgrid rows so the surface has the same orientation as the image.*\n\n[source link](https:\/\/www.quora.com\/How-do-I-convert-a-single-2D-image-to-3D-using-python)","bbafa622":"**Distribution of Roll**","ffe866e9":"**Before we begin,lets watch a video first**","748acfa4":"**Converting center_x,center_y,center_z,roll and pitch values from string to float**","974a6848":"**We understand ImageId column contains id of each image but why PredictionString?**","fb2a1c9f":"# After Applying HorizontalFlip","87a50a55":"**Distribution of center_z**","9cded6a9":"# KDE Plot\n","f12adc7a":"# means and stds (samples vs pixels) from test images directory","d505a72c":"# Means and stds of single image","fd6a9f6d":"Lets take a look at first row of PredictionString ","373a9053":"**From the video above you can see Makers of Baidu's new driverless car believe it's safer than a human driver and hope to have more on the street within the next 10 years. CNN's Matt Rivers reports.**","14654082":"**PredictionString containing a collection of poses and confidence scores.**","af7f7a5e":"# This is going to be my first ever EDA kernel\n\n**I will gradually update this kernel**","a54a3ec2":"**public lb uses 9% of 2021 samples means**","29e90c21":"**Visualizing first 5 train images from train_images directory**","6e009167":"**I would also like you to recommend watching another video**","9c71a204":"\n# Expanding out the prediction string for the first vehicle\n\nWe know the order of each value in the prediction string. We can expand it out for the first vehicle and see some statistics for this first vehicle position.\n","cd9a356b":"# Without HorizontalFlip","93805f3d":"**stolen from here :** https:\/\/www.kaggle.com\/phunghieu\/cloud-mean-std-calculation","5dba015f":"**I have borrowed some code from this beautiful kernel[ Lyft Competition : Understanding the data](https:\/\/www.kaggle.com\/tarunpaparaju\/lyft-competition-understanding-the-data) for generating few plots**","656ce50f":"**Distribution of pitch**","8369325f":"# means and stds (samples vs pixels) from train images directory","125b7454":"> Officer: Sir have you been drinking?\n\n> \u201cOf course, but not the car\u201d ;)","3365171c":"**Now lets visualize a good looking one from test_masks directory**","430d1ca8":"**The dataset contains photos of streets, taken from the roof of a car. We're attempting to predict the position and orientation of all un-masked cars in the test images. we should also provide a confidence score indicating how sure we are of your prediction.**","af1d341d":"**Lets read what inside that camera folder**\n\ninside camera folder we have got camera_intrinsic.txt file,we will read that file in cell below","7fbc796e":"**What's Inside train.csv?**","f7ff1f9d":"# Distributions of center_x and center_y\n","84773bcf":"**Visualizing first 10 test_masks from test_masks directory**","bd2275eb":"It can be seen in the video above - on the front camera of that Self-Driving Car we have **drive net** detecting obstacles the bounding boxes around the cars we have **weight net** detecting the intersection the yellow box around everything, *weight net*  then also detecting traffic lights and traffic signs and light that is classifying traffic light staid correctly is red we also have sign classification going on using **sign net** at the same time *drive net* is detecting pedestrians in the cyan bounding boxes on the far side of the intersection we also have **open road net** tracing out the free space around obstacles on the scene and on top of that we have our object tracking from frame to frame you see the track IDs on the top of each bounding box we also have our **camera base DNN distance estimation** running so you see the distance and metres displayed at the bottom of each box.**clear sight net** is also running in the background assessing whether and how well the cameras can see in our four cameras around perception set up on our embedded EDX platform and all of these rich perception functionality is what our planning and control software is going to use to execute the autonomous driving maneuvers that you will see if you see the video above.\n\n","4bafa28b":"from the diagram above,\n\n**darkorange = center_x**\n\n**purple = center_y**","72a0aca2":"* > COMPARE OUTPUT OF ABOVE 2 IMAGES(messy mask and good looking mask) on your own!","516ecf3b":"**Visualizing first 5 train_masks from train_masks directory**","075bbf42":"**Distribution of yaw**","5ffc77a7":"*181 samples for for public lb is very very low so DON\"T TRUST YOUR PUBLIC LB SCORE AT ALL!*","b8baf98e":"**There are actually 3 types of car models provided in car_models_json file.**","6df29f24":"**Visualizing first 10 test_images from test_images directory**","5b1a4530":"# FOR Understanding data please check [this link](https:\/\/www.kaggle.com\/c\/pku-autonomous-driving\/data)"}}