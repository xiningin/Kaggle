{"cell_type":{"62772e3b":"code","cd69c345":"code","d5c80f9c":"code","5087d2f8":"code","51ca761b":"code","d634b005":"code","a9835790":"code","515648b0":"code","9637ef91":"code","37701914":"code","1711e3bc":"code","32c276f2":"code","66c7e5ea":"code","2fc7064a":"code","b87796bd":"code","933a4cb6":"code","f790b01b":"code","3ba54f76":"code","d5cb0dc3":"code","d724beef":"code","16b193ec":"code","26ec1d73":"code","5a0ade07":"code","52bc879a":"code","0b2249be":"code","bb53c1c6":"code","ec2f4c04":"code","95dd4e81":"code","7ada013d":"code","f293c2f4":"code","2b73ac5f":"code","cb131bf4":"code","2a8bc807":"code","d69807b2":"code","60258165":"code","ef612348":"code","f6d58242":"code","8845d6e2":"code","20e52b28":"code","37d16edb":"code","0143e78e":"code","711189b6":"code","a1a02ff2":"code","19ea5635":"code","9b195d28":"markdown","873e2878":"markdown","3d560658":"markdown","caff0cfd":"markdown","11ec6d5e":"markdown","7d118598":"markdown","5981e30c":"markdown","62170018":"markdown","d4d2cb40":"markdown","379a84b7":"markdown","8aebdf10":"markdown","26e22515":"markdown","4b4fe4e1":"markdown","dfba802b":"markdown"},"source":{"62772e3b":"import numpy as np\nimport pandas as pd","cd69c345":"df = pd.read_csv('..\/input\/pump-sensor-data\/sensor.csv')","d5c80f9c":"df.shape\ndf.columns\n100*df.isna().sum()\/len(df)","5087d2f8":"#Remove sensor_15 and sensor_50, too many missing values","51ca761b":"df = df.drop(['sensor_15','sensor_50','Unnamed: 0'],axis=1)","d634b005":"100*df['machine_status'].value_counts()\/len(df)","a9835790":"columns = [col for col in df.columns if not col.find('sensor')]\nfor col in columns:\n    df[col] = df[col].fillna(df[col].mean())","515648b0":"df.isna().sum()","9637ef91":"corr = df.corr()","37701914":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\ncorr80 = corr[abs(corr)> 0.8]\nsns.heatmap(corr80)","1711e3bc":"from sklearn.decomposition import PCA\n\ndata = df[[sensor for sensor in df.columns.tolist() if not sensor.find('sensor')]]\n\npca = PCA(n_components=2)\ndata_reduce = pca.fit_transform(data)","32c276f2":"print(pca.explained_variance_ratio_)","66c7e5ea":"import matplotlib.pyplot as plt\nplt.plot(data_reduce)","2fc7064a":"data_reduce","b87796bd":"from statsmodels.tsa.api import VAR","933a4cb6":"model = VAR(pd.DataFrame(data_reduce))","f790b01b":"results = model.fit(2)","3ba54f76":"results.summary()","d5cb0dc3":"results.plot()","d724beef":"results.plot_acorr()","16b193ec":"lag_order = results.k_ar\nprint(lag_order)\nresults.plot()","26ec1d73":"plt.plot(results.forecast(pd.DataFrame(data_reduce).values[-lag_order:], 5))","5a0ade07":"results.plot_forecast(200000)","52bc879a":"!pip install pyjanitor","0b2249be":"import datetime\n\nformat_str = '%Y-%m-%d %H:%M:%S'  # The format\n\ndf['timestamp'] = pd.DataFrame([datetime.datetime.strptime(date, format_str) for date in df['timestamp']])\n#y = df.groupby(df.timestamp.dt.year).count()['Date'].values\n#x = df.groupby(df.timestamp.dt.year).count()['Date'].index","bb53c1c6":"import janitor\n\ndef calculate_rul(df):\n    \n    df_broken = df[df['machine_status'] == 'BROKEN']\n    L=[]\n    d = df['timestamp'].iloc[0]\n    for date_broken in df_broken['timestamp']:\n        \n        df_filtered = df.filter_date('timestamp',d, date_broken)\n        df_filtered['rul'] = date_broken - df_filtered['timestamp']\n        \n        L.append(df_filtered)\n        d = date_broken\n    \n    #L.append(df.filter_date('timestamp',df_broken['timestamp'].iloc[-1], df['timestamp'].iloc[-1]))\n    result = pd.concat(L)\n    \n\n    return result.loc[~result.index.duplicated(keep='first')] #remove duplicates indices\n\ndf_rul = calculate_rul(df)","ec2f4c04":"df_rul['rul']\n\ndelta_t = np.timedelta64(4, 'D')\ndelta_t_2 = np.timedelta64(2, 'D')","95dd4e81":"# Label1 indicates a failure will occur within the next 30 cycles.\n# 1 indicates failure, 0 indicates healthy \ndf_rul['label1'] = np.where(df_rul['rul'] <= delta_t, 1, 0 )","7ada013d":"from sklearn.preprocessing import MinMaxScaler\n\ncols_normalize = df_rul.columns.difference(['id','timestamp','rul','label1','label2','machine_status'])\n\nscaler = MinMaxScaler()\n\ndf_rul[cols_normalize] = scaler.fit_transform(df_rul[cols_normalize])","f293c2f4":"df_rul[\"rul\"] = df_rul[\"rul\"].apply(lambda x: x.total_seconds()\/3600) #converting to hours","2b73ac5f":"#separate in train and test dataset\ndf_rul[df_rul['machine_status'] ==\"BROKEN\"]\n\ndf_rul['time_norm']= df_rul['timestamp'].values.tolist()\ndf_rul[\"time_norm\"] = (df_rul[\"time_norm\"]-df_rul[\"time_norm\"].min())\/(df_rul[\"time_norm\"].max()-df_rul[\"time_norm\"].min())\n\ntrain_df = df_rul.iloc[:77790,:].drop(['machine_status',\"timestamp\"],axis=1)\ntrain_df[\"time_norm\"] = (train_df[\"time_norm\"]-train_df[\"time_norm\"].min())\/(train_df[\"time_norm\"].max()-train_df[\"time_norm\"].min())\n\n\ntest_df = df_rul.iloc[77791:,:].drop(['machine_status',\"timestamp\"],axis=1)\ntest_df[\"time_norm\"] = (test_df[\"time_norm\"]-test_df[\"time_norm\"].min())\/(test_df[\"time_norm\"].max()-test_df[\"time_norm\"].min())\n\ntrain_df['rul'] = train_df['rul'].values.tolist()","cb131bf4":"test_df['rul'].describe()","2a8bc807":"features_col_name=[column for column in df.columns if not column.find('sensor')]\ntarget_col_name='label1'\n\nfeatures_col_name","d69807b2":"# window size\nseq_length=100\nseq_cols=features_col_name","60258165":"def gen_sequence(id_df, seq_length, seq_cols):\n    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n    we can use shorter ones \"\"\"\n    # for one id I put all the rows in a single matrix\n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n    # Iterate over two lists in parallel.\n    # For example id1 have 192 rows and sequence_length is equal to 50\n    # so zip iterate over two following list of numbers (0,112),(50,192)\n    # 0 50 -> from row 0 to row 50\n    # 1 51 -> from row 1 to row 51\n    # 2 52 -> from row 2 to row 52\n    # ...\n    # 111 191 -> from row 111 to 191\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_matrix[start:stop, :]\n\n# function to generate labels\ndef gen_labels(id_df, seq_length, label):\n    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n    we can use shorter ones \"\"\"\n    # For one id I put all the labels in a single matrix.\n    # For example:\n    # [[1]\n    # [4]\n    # [1]\n    # [5]\n    # [9]\n    # ...\n    # [200]] \n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n    # I have to remove the first seq_length labels\n    # because for one id the first sequence of seq_length size have as target\n    # the last label (the previus ones are discarded).\n    # All the next id's sequences will have associated step by step one label as target.\n    return data_matrix[seq_length:num_elements, :]","ef612348":"# generate X_train for LSTM 3D dimension array\nseq_gen = [list(gen_sequence(train_df, seq_length, seq_cols))]\n\nseq_array = np.concatenate(list(seq_gen)).astype(np.float32)\nprint(seq_array.shape)","f6d58242":"label_gen = [gen_labels(train_df, seq_length, ['label1'])]\n\nlabel_array = np.concatenate(label_gen).astype(np.float32)\nlabel_array.shape","8845d6e2":"from sklearn.metrics import confusion_matrix, recall_score, precision_score\nfrom keras.models import Sequential\nfrom sklearn import datasets\nfrom keras.layers import Dense, Dropout, LSTM, Activation","20e52b28":"from sklearn.metrics import fbeta_score\nfrom keras import backend as K\n\n\ndef fbeta(y_true, y_pred, threshold_shift=0):\n    beta = 10\n\n    # just in case of hipster activation at the final layer\n    y_pred = K.clip(y_pred, 0, 1)\n\n    # shifting the prediction threshold from .5 if needed\n    y_pred_bin = K.round(y_pred + threshold_shift)\n\n    tp = K.sum(K.round(y_true * y_pred_bin)) + K.epsilon()\n    fp = K.sum(K.round(K.clip(y_pred_bin - y_true, 0, 1)))\n    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n\n    beta_squared = beta ** 2\n    return (beta_squared + 1) * (precision * recall) \/ (beta_squared * precision + recall + K.epsilon())","37d16edb":"from sklearn.metrics import fbeta_score\n\n# build the LSTM network\n# Feature weights\nnb_features = seq_array.shape[2]\nnb_out = label_array.shape[1]\n\n# LSTM model\nmodel = Sequential()\n\n# The first layer\nmodel.add(LSTM(\n         input_shape=(seq_length, nb_features),\n         units=100,\n         return_sequences=True))\n\n# Plus a 20% dropout rate\nmodel.add(Dropout(0.2))\n\n# The second layer\nmodel.add(LSTM(\n          units=50,\n          return_sequences=False))\n\n# Plus a 20% dropout rate\nmodel.add(Dropout(0.2))\n\n# Dense sigmoid layer\nmodel.add(Dense(units=nb_out, activation='sigmoid'))\n\n# With adam optimizer and a binary crossentropy loss. We will opimize for model accuracy.\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=[fbeta])\n\n# Verify the architecture \nprint(model.summary())","0143e78e":"import keras\nimport time\n\n\nt0 = time.time()\n# fit the network\nmodel.fit(seq_array, # Training features\n          label_array, # Training labels\n          epochs=10,   # We'll stop after 10 epochs\n          batch_size=200, # \n          validation_split=0.10, # Use 10% of data to evaluate the loss. (val_loss)\n          verbose=1, #\n          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', # Monitor the validation loss\n                                                     min_delta=0,    # until it doesn't change (or gets worse)\n                                                     patience=5,  # patience > 1 so it continutes if it is not consistently improving\n                                                     verbose=0, \n                                                     mode='auto')])\n\nprint(\"Training took \"+str(time.time() - t0)+\" seconds\")","711189b6":"# training metrics\nscores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\nprint('Training Accurracy: {}'.format(scores[1]))","a1a02ff2":"# make predictions and compute confusion matrix\ny_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\ny_true = label_array\nprint('Training Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\ncm = confusion_matrix(y_true, y_pred)\ncm","19ea5635":"from sklearn.metrics import fbeta_score\n\n# compute precision and recall\nprecision = precision_score(y_true, y_pred)\nrecall = recall_score(y_true, y_pred)\nf1 = 2 * (precision * recall) \/ (precision + recall)\nprint( 'Training Precision: ', precision, '\\n', 'Training Recall: ', recall, '\\n', 'Training F1 Score:', f1)\nprint(fbeta_score(y_true, y_pred,beta=0.5))","9b195d28":"Transform timestamp to datetime type","873e2878":"73% of the variance is explained by the first component","3d560658":"# AR models","caff0cfd":"Let's now shape the data for a LSTM network. I inspire from this notebook that explain how to use LSTM for predictive maintenance.\nhttps:\/\/github.com\/Azure-Samples\/MachineLearningSamples-DeepLearningforPredictiveMaintenance\/blob\/master\/Code\/2_model_building_and_evaluation.ipynb","11ec6d5e":"Train the model","7d118598":"Scale the data","5981e30c":"\n# Pump sensors predictive maintenance","62170018":"For the LSTM approach, we have to calculate remaining useful life i.e time before broken for earch rows","d4d2cb40":"Let's now define a label to define if we are 4 days before a failure. I choose 4 days because of the regularity of the failure. This is the target label we want to predict.","379a84b7":"We can notice two groups of sensors highly correlated (sensor 15-25) and (sensors 14-29)","8aebdf10":"ARIMA results are not convicing, let's try another approach.","26e22515":"# Correlations","4b4fe4e1":"# LSTM models","dfba802b":"Let's define the network:\n\nThis is a network of only 2 layers, with dropout. The first LSTM layer with 100 units, one for each input sequence, followed by another LSTM layer with 50 units. We will also apply dropout each LSTM layer to control overfitting. The final dense output layer employs a sigmoid activation corresponding to the binary classification requirement.\n\nWe choose the Fbeta metrics for this kind of problems."}}