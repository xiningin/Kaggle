{"cell_type":{"0a056e03":"code","1604af19":"code","1d5221ab":"code","15ec2748":"code","007cc11e":"code","2633399a":"code","9f4a86f9":"code","a4ac48dd":"code","138880cb":"code","bc092e97":"code","56bd2a83":"code","74dc5563":"code","446e7fd6":"code","5b1b9515":"code","284b0e9a":"code","dd2fb0f2":"code","e5d54623":"code","43bf1139":"code","c85268e5":"code","be7123cc":"code","d6f681bd":"code","10eb29be":"code","bac65058":"code","d73bbb7d":"code","4c9d0fdd":"code","4539df65":"code","aeb0d90b":"code","98170f48":"code","7a8fe4f8":"code","b9937df9":"code","200051de":"code","042deedb":"markdown","67c3ea1a":"markdown","27224d57":"markdown","a6815a31":"markdown","d41bcc40":"markdown","261c642d":"markdown","a818b7da":"markdown","0292cbb0":"markdown","790bedec":"markdown","73d00eb1":"markdown","e148145b":"markdown","f64f6739":"markdown","14861ad0":"markdown","01cc6c6d":"markdown","c3b04d82":"markdown","5791c2da":"markdown","41397e70":"markdown","11ba9303":"markdown","167bad44":"markdown","9652b043":"markdown","1c7c0b82":"markdown"},"source":{"0a056e03":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom scipy.stats import shapiro","1604af19":"data = pd.read_csv(\"..\/input\/simple-regression\/data_regression.csv\")","1d5221ab":"data.head()","15ec2748":"data.describe()","007cc11e":"data.corr()","2633399a":"plt = sns.lmplot(x = \"Height\", y = \"Weight\", data = data);","9f4a86f9":"stat, p = shapiro(data.Weight)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))","a4ac48dd":"sns.distplot(a = data[\"Weight\"], hist = False);","138880cb":"sns.boxplot(x = data[\"Weight\"]);","bc092e97":"independent_variable = data[[\"Height\"]]\nindependent_variable.head()","56bd2a83":"dependent_variable = data[[\"Weight\"]]\ndependent_variable.head()","74dc5563":"linear_model = sm.OLS(dependent_variable,independent_variable)","446e7fd6":"model = linear_model.fit()","5b1b9515":"model.summary()","284b0e9a":"predicted_data = pd.DataFrame({\"Weight\" : data.Weight, \"Predicted_Weight\" : model.fittedvalues})\npredicted_data","dd2fb0f2":"predicted_data.plot(kind='bar',figsize=(20,10))\nplt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\nplt.show()","e5d54623":"sns.regplot(data.Height, data.Weight);","43bf1139":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression","c85268e5":"data = pd.read_csv(\"..\/input\/simple-regression\/data_regression.csv\")","be7123cc":"X = data[[\"Height\"]]\ny = data[[\"Weight\"]]","d6f681bd":"reg = LinearRegression()","10eb29be":"model = reg.fit(X,y)","bac65058":"model.intercept_","d73bbb7d":"model.coef_","4c9d0fdd":"model.score(X,y)","4539df65":"final_df = pd.DataFrame({\"Weight\" : data.Weight, \"Pedicted_Weight\" : reg.predict(X)[:, 0]})\nfinal_df","aeb0d90b":"plt = sns.regplot(data.Height, data.Weight)\nplt.set_title(\"Height = (-39.0449657) + (61.26229747) * Weight\");","98170f48":"from sklearn.metrics import mean_squared_error, r2_score\nmean_squared_error(y, model.predict(X))","7a8fe4f8":"final_df[\"error\"] = final_df[\"Weight\"] - final_df[\"Pedicted_Weight\"]\nfinal_df","b9937df9":"stat, p = shapiro(final_df.error)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))","200051de":"sns.distplot(a = final_df[\"error\"], hist = False);","042deedb":"**Height = (-39.0449657) + (61.26229747) * Weight**","67c3ea1a":"The value obtained from model.intercept_ is equal to \"beta 0\" value.","27224d57":"The correlation value between the dependent variable \"Weight\" and the independent variable \"Height\" was quite high.","a6815a31":"# NORMALITY TEST","d41bcc40":"In statistics, the mean squared error (MSE) of an estimator measures the average of the squares of the errors\u2014that is, the average squared difference between the estimated values and the actual value.","261c642d":"In addition, when we look at histogram and density graphs, we see that our variable shows a distribution suitable for normal distribution.","a818b7da":"When we want to examine the suitability of our data for normal distribution, we use the \"normality test\". I preferred to apply shapiro-wilk because of our data size is small. P-value is 0.697 so we can not rejected H0 hypothesis (H0 : The distribution of the weight variable does not differ from the normal distribution.) We have provided the normality assumption.","0292cbb0":"p-values > 0.05 so error variable shows a distribution suitable for normal distribution.","790bedec":"We can read data using the pandas library. Our data is a csv file, so we can use \"pd.read_csv()\". CSV (comma-separated value) files are a common file format for transferring and storing data.","73d00eb1":"The value obtained from model.coef_ is equal to \"beta 1\" value.","e148145b":"# LINEAR REGRESSION with STATSMODEL LIBRARY","f64f6739":"R square is 0.9891","14861ad0":"In statistics, simple linear regression is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variables. The adjective simple refers to the fact that the outcome variable is related to a single predictor.\n\nThere are four assumptions associated with a \"Linear Regression Model\":\n\n* Linearity: The relationship between X and the mean of Y is linear.\n* Homoscedasticity: The variance of residual is the same for any value of X.\n* Independence: Observations are independent of each other.\n* Normality: For any fixed value of X, Y is normally distributed.","01cc6c6d":"\"R-squared(R2)\" is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n\nThe R-Square value of the model was calculated as \"0.988\". This value is very satisfactory for the success of the model.\n\nThe high R-square value does not always indicate a successful model, because the model can give successful results by memorizing the data set and this situation is called overfitting. (In case of overfitting, the R-square value is very high and misleading.)\n\nWhen we look at the prob(F - statistics) value, we see that we have established a statistically significant model.","c3b04d82":"# DATA","5791c2da":"# LINEAR REGRESSION with sklearn Library","41397e70":"# LIBRARIES","11ba9303":"Since the size of the data is small and it is a simple linear regression application, I did not divide the data into two. However, when applying machine learning, the data is divided into two as training and test data.","167bad44":"As a result of the model we built with sklearn, \"mean squared error\" value was calculated as 0.5.","9652b043":"# CORRELATION","1c7c0b82":"We can see the first 5 observation values with the help of the \"head()\" function. Observing the first observations helps to recognize variables."}}