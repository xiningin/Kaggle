{"cell_type":{"c5762455":"code","5429c60c":"code","d3a90a98":"code","a44338d4":"code","969cc0ea":"code","d2290b05":"code","daf45c36":"code","c493f76e":"code","fa68e3a1":"code","ff30df3a":"code","11265a10":"code","4bb4881c":"code","5dbefdb5":"code","2d3e6fee":"code","697c8088":"code","79123a6e":"code","3b624caa":"code","2ced19b0":"code","ee41cc63":"code","901a5f0f":"code","017d2749":"code","aa600091":"code","b10d8da1":"code","23bfcbd5":"code","6c800efc":"code","0bdf3212":"code","b4224ec1":"code","da143546":"code","a5b8541f":"code","4d920021":"code","8668aea1":"code","8243fe09":"code","83166aed":"code","41877c7f":"code","9521c4f0":"code","5eca3afc":"code","289cff17":"code","0c17c67c":"code","2eeab9de":"code","bfc6bc9a":"code","9acfe98e":"code","c8348e11":"code","579c3ffe":"code","05b6a051":"code","0b9591be":"code","a7b50f0b":"code","94258845":"code","58f952c8":"code","e66494ec":"code","dd1658d9":"code","61f27214":"code","02e192aa":"code","7e37b02d":"code","5d995286":"markdown","f95db1ac":"markdown","30c0ef8c":"markdown","4e3a5a5d":"markdown","1660b7a1":"markdown","3ae417aa":"markdown","037e9f25":"markdown","6f185104":"markdown","9abc263d":"markdown","2c0b4737":"markdown","7d96f3a2":"markdown","963fc130":"markdown","0a8a6e6b":"markdown","4a75cca7":"markdown","3d08e705":"markdown","f88cc230":"markdown","92b0220a":"markdown","117c3137":"markdown","92711a8c":"markdown","469e3f9c":"markdown","7b11a355":"markdown","8136ccb1":"markdown","4f76263b":"markdown","e0672692":"markdown","f7dd0bf9":"markdown","55ce5061":"markdown","af60d512":"markdown","84c8ac2e":"markdown","f3773dc7":"markdown","f4989568":"markdown","ec9b2cb9":"markdown","26da61c8":"markdown","0cd047dd":"markdown","bf7df904":"markdown","78d4d053":"markdown","9ed13bfd":"markdown","b6dce84a":"markdown","ec75b22b":"markdown","460d94a6":"markdown","a1257201":"markdown","6b663d00":"markdown","0100bf82":"markdown","330d7c34":"markdown","5d07a898":"markdown","774dbcb0":"markdown","9839ab09":"markdown","71787cc4":"markdown","285d3f49":"markdown","4bfc5cea":"markdown","204a3036":"markdown","53b7d0ff":"markdown","ef911841":"markdown","827a873d":"markdown","9356e310":"markdown","7f9465ed":"markdown","55f68c78":"markdown","81d363a4":"markdown","a8d87de8":"markdown","d84cca9c":"markdown","bd1fb3f0":"markdown","53f9865e":"markdown","baf0e122":"markdown","ee5c7e71":"markdown","fdf70193":"markdown","86242ee1":"markdown","becf4e56":"markdown","f66f590c":"markdown","6a0e5ede":"markdown","b5d8a054":"markdown","f4cf78d0":"markdown","e9fb9e1b":"markdown","e09218d5":"markdown","77afc18e":"markdown","021c4da9":"markdown","47b3411b":"markdown","57bd74ea":"markdown","0f86ed03":"markdown","6dec9ac6":"markdown","fec4885c":"markdown","4e082585":"markdown","935a6246":"markdown","0043e7c9":"markdown","6513787b":"markdown","f0a03dd4":"markdown","3191299c":"markdown","bec9f332":"markdown","c2b9b6f6":"markdown","5ba883e4":"markdown","21f6245a":"markdown","255dcc10":"markdown","a79ff579":"markdown","4211aa87":"markdown","2a93f93f":"markdown","ca3f23a2":"markdown","0fed4d44":"markdown","c2867c84":"markdown","542ecb32":"markdown","44235b57":"markdown","a7e71c46":"markdown","ec8ad495":"markdown","7a43e13d":"markdown","030c8402":"markdown","580c1e2b":"markdown"},"source":{"c5762455":"!pip install pandas-profiling","5429c60c":"# Linear Algebra\nimport numpy as np \n# Data Processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\n# Pandas Profiling\nimport pandas_profiling as pp\n\n# Data Visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","d3a90a98":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","a44338d4":"train_df.info()","969cc0ea":"train_df.describe()","d2290b05":"pp.ProfileReport(train_df, title= 'Pandas Profile report of \"Train\" set', html= {'style':{'full_width': True}})","daf45c36":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","c493f76e":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nwomen = train_df[train_df['Sex'] == 'female']\nmen = train_df[train_df['Sex'] == 'male']\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(), bins= 18, label= survived, ax= axes[0], kde= False)\nax.legend()\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins= 40, label= not_survived, ax= axes[0], kde= False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(), bins= 18, label= survived, ax= axes[1], kde= False)\nax.legend()\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins= 40, label= not_survived, ax= axes[1], kde= False)\nax.legend()\nax.set_title('Male')","fa68e3a1":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","ff30df3a":"sns.barplot(x='Pclass', y='Survived', data=train_df)","11265a10":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","4bb4881c":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\n    # print(dataset)\n    \ntrain_df['not_alone'].value_counts()\n","5dbefdb5":"axes = sns.factorplot('relatives','Survived', data=train_df, aspect = 2.5, )","2d3e6fee":"train_df = train_df.drop(['PassengerId'], axis=1)","697c8088":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    \n# We can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","79123a6e":"train_df","3b624caa":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df['Age'].mean()\n    std = test_df['Age'].std()\n    is_null = dataset['Age'].isnull().sum()\n    # Compute random number between mean , std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, is_null)\n    age_slice = dataset['Age'].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset['Age'] = age_slice\n    dataset['Age'] = train_df['Age'].astype(int)\n    \ntrain_df['Age'].isnull().sum()","2ced19b0":"train_df['Embarked'].describe()","ee41cc63":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)\n    \ntrain_df['Embarked'].isnull().sum()","901a5f0f":"train_df.info()","017d2749":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","aa600091":"data = [train_df, test_df]\ntitles = {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Rare': 5}\n\nfor dataset in data:\n    # Extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([a-zA-Z]+)\\.', expand= False)\n    # Replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess', 'Cap', 'Col', 'Don', 'Dr'\\\n                                                 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n    # Convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # Filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Title'] = dataset['Title'].astype(int)\n    \ntrain_df = train_df.drop(['Name'], axis= 1)\ntest_df = test_df.drop(['Name'], axis= 1)","b10d8da1":"genders = {'male': 0, 'female': 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","23bfcbd5":"train_df['Ticket'].describe()","6c800efc":"train_df = train_df.drop(['Ticket'], axis= 1)\ntest_df = test_df.drop(['Ticket'], axis= 1)","0bdf3212":"ports = {'S': 0, 'C': 1, 'Q': 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","b4224ec1":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[dataset['Age'] > 66, 'Age'] = 6\n    \n# let's see how it's distributed \ntrain_df['Age'].value_counts()","da143546":"train_df.head()","a5b8541f":"data = [train_df, test_df]\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n# let's see how it's distributed\ntrain_df['Fare'].value_counts()","4d920021":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class'] = dataset['Age'] * dataset['Pclass']","8668aea1":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare'] \/ (dataset['relatives'] + 1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n    \n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)","8243fe09":"X_train = train_df.drop(['Survived'], axis= 1)\nY_train = train_df['Survived']\nX_Test = test_df.drop(['PassengerId'], axis= 1).copy()","83166aed":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_Test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","41877c7f":"logreg = LogisticRegression(solver='liblinear')\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_Test)\n\nacc_logreg = round(logreg.score(X_train, Y_train) * 100, 2)","9521c4f0":"# KNN\nknn = KNeighborsClassifier(n_neighbors= 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_Test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","5eca3afc":"results = pd.DataFrame({'Model': ['KNN', 'Logestic Regresion', 'Random Forest'],\\\n                       'Score': [acc_knn, acc_logreg, acc_random_forest]})\nresult_df = results.sort_values(by= 'Score', ascending= False)\nresult_df = result_df.set_index('Score')\nresult_df.head()","289cff17":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators= 100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring= \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","0c17c67c":"importances = pd.DataFrame({'feature': X_train.columns, 'importance': np.round(random_forest.feature_importances_, 3)})\nimportances = importances.sort_values('importance', ascending= False).set_index('feature')\n\nimportances.head(15)","2eeab9de":"importances.plot.bar()","bfc6bc9a":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","9acfe98e":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score= True)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_Test)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n\nprint(round(acc_random_forest, 2), '%')","c8348e11":"print(\"oob score:\", round(random_forest.oob_score_, 4) * 100, '%')","579c3ffe":"# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n#from sklearn.model_selection import GridSearchCV, cross_val_score \n\n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\n# clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n\n# clf.fit(X_train, Y_train)\n\n# clf.best_params_","05b6a051":"random_forest = RandomForestClassifier(criterion= 'gini', min_samples_leaf= 1, min_samples_split= 10,\n                                       n_estimators= 100, max_features= 'auto', oob_score= True,\n                                       random_state= 1, n_jobs= -1)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_Test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","0b9591be":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\n\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv= 3)\nconfusion_matrix(Y_train, predictions)","a7b50f0b":"plot_confusion_matrix(random_forest, X_train, Y_train)","94258845":"from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))\nprint(\"F1-Score:\", f1_score(Y_train, predictions))","58f952c8":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\n\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","e66494ec":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","dd1658d9":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","61f27214":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","02e192aa":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","7e37b02d":"submission.to_csv('submission.csv', index=False)","5d995286":"### __Feature Importance__","f95db1ac":"Above you can see that _\u2018Fare\u2019_ is a float and we have to deal with 4 categorical features: _Name, Sex, Ticket and Embarked_. Lets investigate and transfrom one after another.","30c0ef8c":"Now that we have a proper model, we can start evaluating it\u2019s performace in a more accurate way. Previously we only used accuracy and the oob score, which is just another form of accuracy. The problem is just, that it\u2019s more complicated to evaluate a classification model than a regression model. We will talk about this in the following section.","4e3a5a5d":"__Embarked:__","1660b7a1":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","3ae417aa":"![HyperparameterTuning.png](attachment:HyperparameterTuning.png)","037e9f25":"Above we can see that 5 of the features are numeric and 7 of categorical. From the table above, we can note a few things.\nFirst of all, that we **need to convert a lot of features into numeric** ones later on, so that the machine learning algorithms\ncan process them. Furthermore, we can see that the **features have widly different ranges**, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values(NaN = Not a Number), that we need to deal with.","6f185104":"![Olympic_&_Titanic_cutaway_diagram.png](attachment:Olympic_&_Titanic_cutaway_diagram.png)","9abc263d":"For the \u2018Fare\u2019 feature, we need to do the same as with the \u2018Age\u2019 feature. But it isn\u2019t that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \u201cqcut()\u201d function, that we can use to see, how we can form the categories.","2c0b4737":"We will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.","7d96f3a2":"### __Precision, Recall and F-Score:__","963fc130":"__Test new Parameters:__","0a8a6e6b":"#### __2. Fare per Person__","4a75cca7":"We will now create categories within the following features:","3d08e705":"Another way is to plot the precision and recall against each other:","f88cc230":"__Cabin:__\n\nAs a reminder, we have to deal with Cabin (687), Embarked (2) and Age (177). First I thought, we have to delete the \u2018Cabin\u2019 variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the __letter refers to the deck__. Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G.","92b0220a":"You can __combine precision and recall into one score__, which is called the __F-score__. The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. As a result of that, the classifier will only get a high F-score, if both recall and precision are high.","117c3137":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.","92711a8c":"To say it in simple words: __Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction__.\n\n__One big advantage of random forest is, that it can be used for both classification and regression problems__, which form the majority of current machine learning systems. With a few exceptions a random-forest classifier has all the hyperparameters of a decision-tree classifier and also all the hyperparameters of a bagging classifier, to control the ensemble itself.","469e3f9c":"__Sex:__","7b11a355":"  * #### __Missing Data__","8136ccb1":"First, I will __drop \u2018PassengerId\u2019__ from the train set, because it does not contribute to a persons survival probability. I __will not drop it from the test set__, since it is required there for the submission.","4f76263b":"__Ticket:__","e0672692":"According tables of __\"Missing Values\"__ in pandas profile and above table, the __Embarked__ feature has only __2 missing values__, which can easily be filled. It will be much more tricky, to deal with the __\u2018Age\u2019__ feature, which has __177 missing values__. The __\u2018Cabin\u2019__ feature needs further investigation, but it looks like that we might want to drop it from the dataset, since __77 % of it are missing__.","f7dd0bf9":"### 3. __K Nearest Neighbor:__","55ce5061":" * ### **Dataset information (Pandas Profiling)**","af60d512":"### __Confusion Matrix:__","84c8ac2e":"__The random-forest algorithm brings extra randomness into the model, when it is growing the trees__. Instead of __searching for the best feature while splitting a node, it searches for the best feature among a random subset of features__. This process creates a __wide diversity__, which __generally results in a better model__. Therefore when you are growing a tree in random forest, only a random subset of the features is considered for splitting a node. You can even make trees more random, by using random thresholds on top of it, for each feature rather than searching for the best possible thresholds (like a normal decision tree does).","f3773dc7":"### __ROC AUC Curve__","f4989568":"### __ROC AUC Score__","ec9b2cb9":"There we have it, a __77 % F-score__. __The score is not that high__, because we have a __recall of 74%__. But unfortunately the F-score is not perfect, because it favors classifiers that have a similar precision and recall. This is a problem, because you sometimes want a high precision and sometimes a high recall. The thing is that an increasing precision, sometimes results in an decreasing recall and vice versa (depending on the threshold). This is called the precision\/recall tradeoff.","26da61c8":"##### __Overview__\n\nThe data has been split into two groups:\n\n* training set (train.csv)\n* test set (test.csv)\n\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, as an example of what a submission file should look like.","0cd047dd":"#### __1. Age and Sex__","bf7df904":"# __Predicting the Survival of Titanic Passengers__\n\n### __Titanic - Machine Learning from Disaster__\n#### Start here! Predict survival on the Titanic and get familiar with ML basics","78d4d053":"Our model predicts __82%__ of the time, a passengers survival correctly (__precision__). The __recall__ tells us that it predicted the survival of __74 %__ of the people who actually survived.","9ed13bfd":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","b6dce84a":"### __Table of content__\n\n---\n\n* __Part 1 - Data Preprocessing__\n    1. Importing Libraries\n    2. Importing Datasets\n    3. Exploration Data Analysis (EDA)\n        * Dataset Information (Pandas Profiling)\n        * Data Preprocessing\n            * Missing data\n            * Converting Features\n        * Creating Categories\n        * Creating New Features\n        \n* __Part 2 - Building and Training the Classification model__\n    1. Random Forest\n    2. Logestic Regresion\n    3. K Nearest Neighbor\n    4. Accuracy Score\n    \n* __Part 3 - Creating a submission.csv__\n\n---","ec75b22b":"You are now able to choose a threshold, that gives you the best precision\/recall tradeoff for your current machine learning problem. If you want for example a precision of __80%__, you can easily look at the plots and see that you would need a threshold of around 0.4. Then you could train a model with exactly that threshold and would get the desired accuracy.","460d94a6":"* ### __Data Preprocessing__","a1257201":"##### __Data Dictionary__\n| Variable |\tDefinition\t                               | Key                       |\n|----------|:----------------------------------------------|:--------------------------|\n|survival  |\tSurvival                                   |\t0 = No, 1 = Yes        |\n|pclass    |\tTicket class                               |\t1 = 1st, 2 = 2nd, 3 = 3rd |\n|sex       |\tSex                                        |\t                       |\n|Age       |\tAge in years                               |\t                       |\n|sibsp     |\t# of siblings \/ spouses aboard the Titanic |\t|\n|parch     |\t# of parents \/ children aboard the Titanic |\t|\n|ticket    |\tTicket number                              |\t                       |\n|fare      |\tPassenger fare                             |\t                       |\n|cabin     |\tCabin number                               |\t                       |\n|embarked  |\tPort of Embarkation                        |\tC = Cherbourg, Q = Queenstown, S = Southampton|","6b663d00":"Embarked seems to be correlated with survival, depending on the gender.\n\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\n\nPclass also seems to be correlated with survival. We will generate another plot of it below.","0100bf82":"__another run__","330d7c34":"## __Which is the best Model ?__","5d07a898":"Now we can start tuning the hyperameters of random forest.","774dbcb0":"Below you can see the code of the __hyperparamter tuning__ for the parameters __criterion, min_samples_leaf, min_samples_split__ and __n_estimators__.\n\nI put this code into a comment, because it takes a long time to run it. Directly underneeth it, I put a screenshot of the gridsearch's output.","9839ab09":"The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\n\nA classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5.","71787cc4":"__Fare:__","285d3f49":"__The training-set has 891 examples and 11 features + the target variable (survived).__  2 of the features are floats, 5 are integers and 5 are objects.","4bfc5cea":"Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other. Later on, we will use cross validation.","204a3036":"Nice ! I think that score is good enough to submit the predictions for the test-set to the Kaggle leaderboard.","53b7d0ff":"**Now we need to convert the \u2018age\u2019 feature. First we will convert it from float into integer. Then we will create the new \u2018AgeGroup\u201d variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don\u2019t want for example that 80% of your data falls into group 1.**","ef911841":"#### __3. Pclass__","827a873d":"# __In conclusion__","9356e310":"Another way to evaluate and compare your binary classifier is provided by the __ROC AUC Curve__. This curve plots the __true positive rate (also called recall)__ against the __false positive rate (ratio of incorrectly classified negative instances)__, instead of plotting the precision versus the recall.","7f9465ed":"__Embarked:__\n\nSince the Embarked feature has only 2 missing values, we will just fill these with the most common one.","55f68c78":"You can see that men have a high probability of survival when they are between 18 and 40 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\n\nFor men the probability of survival is very low between the age of 5 and 18, but that isn\u2019t true for women. Another thing to note is that infants also have a little bit higher probability of survival.\n\nSince there seem to be __certain ages, which have increased odds of survival__ and because I want every feature to be roughly on the same scale, I will create age groups later on.","81d363a4":"Above you can clearly see that the __recall is falling of rapidly at a precision__ of around __82%__. Because of that you may want to select the precision\/recall tradeoff before that \u2014 maybe at around __75 %__.","a8d87de8":"Our random forest model predicts as good as it did before. A general rule is that, the __more features you have, the more likely your model will suffer from overfitting__ and vice versa. But I think our data __looks fine for now__ and hasn't too much features.","d84cca9c":"To me it would make sense if everything except __\u2018PassengerId\u2019__, __\u2018Ticket\u2019__ and __\u2018Name\u2019__ would be correlated with a high survival rate.","bd1fb3f0":"# __Part 2 - Building and Training the Classification model__","53f9865e":"* ### __Creating new Features__","baf0e122":"## __Further Evaluation__","ee5c7e71":"Here we can see that you had a __high probabilty of survival with 1 to 3 realitves__, but a __lower one if you had less than 1 or more than 3__ (except for some cases with 6 relatives).","fdf70193":"I will add two new features to the dataset, that I compute out of other features.","86242ee1":"### 1. __Random Forest:__","becf4e56":"+ ### __Creating Categories:__","f66f590c":"We will plot the precision and recall with the threshold using matplotlib:","6a0e5ede":"#### __1. Age times Class__","b5d8a054":"### __Random Forest__","f4cf78d0":"### __Precision Recall Curve__","e9fb9e1b":"* #### __Converting Features:__","e09218d5":"## __1. Importing Libraries__","77afc18e":"![HyperparameterTuning2.png](attachment:HyperparameterTuning2.png)","021c4da9":"__Another great quality of random forest is that they make it very easy to measure the relative importance of each feature__. Sklearn measure a features importance by looking at how much the treee nodes, that use that feature, __reduce impurity on average (across all trees in the forest)__. It computes this score automaticall for each feature after training and scales the results so that the __sum of all importances is equal to 1__. We will acces this below:","47b3411b":"## **3. Exploration Data Analysis**","57bd74ea":"As we can see, the __Random Forest classifier goes on the first place__. But first, let us check, how random-forest performs, when we use __cross validation__.","0f86ed03":"### __Hyperparameter Tuning__","6dec9ac6":"### __Conclusion:__\n\n\n__not_alone__ and __Parch__ __doesn\u2019t play a significant role in our random forest classifiers prediction process__. Because of that __I will drop them__ from the dataset and train the classifier again. We could also remove more or less features, but this would need a more detailed investigation of the features effect on our model. But I think it\u2019s just fine to remove only Alone and Parch.","fec4885c":" ### 2. __Logistic Regression:__","4e082585":"Above we can see that __38% out of the training-set servived the titanic.__\nWe can also see that the passenger ages range from __0.4 to 80__.","935a6246":"# __Part 3 - Creating a submission.csv__","0043e7c9":"Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:","6513787b":"__Fare:__","f0a03dd4":"## __K-Fold Cross Validation:__","3191299c":"__Training random forest again:__","bec9f332":"#### __2. Embarked, Pclass and Sex__","c2b9b6f6":"__Let\u2019s take a more detailed look at what data is actually missing:__","5ba883e4":"# __If you liked my work then please upvote, Thank you.__","21f6245a":"Convert \u2018Sex\u2019 feature into numeric.","255dcc10":"SibSp and Parch would make __more sense as a combined feature__, that shows the __total number of relatives, a person has on the Titanic__. I will create it below and also a feature that sows if someone is not alone.","a79ff579":"# __Part 1 - Data Preprocessing__","4211aa87":"##### __Variable Notes__\n\n__pclass__: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n\n__age__ : Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n\n__sibsp__ : The dataset defines family relations in this way\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n\n__parch__ : The dataset defines family relations in this way\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them","2a93f93f":"Convert \u2018Embarked\u2019 feature into numeric.","ca3f23a2":"#### __4. SibSp and Parch__","0fed4d44":"__Age:__\n\nNow we can tackle the issue with the age features missing values. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","c2867c84":"__There is also another way to evaluate a random-forest classifier, which is probably much more accurate than the score we used before__. What I am talking about is the __out-of-bag samples__ to estimate the generalization accuracy. I will not go into details here about how it works.","542ecb32":"K-Fold Cross Validation randomly splits the training data into __K subsets called folds__.\nThe code below perform K-Fold Cross Validation on our random forest model, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.","44235b57":"__There are many things for a greater chance to survive. Being a female or a child will increase you chances. If you have a higher class ticket you have the more chance of surviving than a third class ticket. As for a man, you are more likely to survive if embark in Cherbourg compare to Southampton or Queenstown. If you also travel with 1 or 3 people than 0 or more than 3 your survival chances are greater. The younger you are will also make your survival chance. So it comes down to many things to surivive on the titanic.__","a7e71c46":"__Age:__","ec8ad495":"Installation __'pandas-profiling'__ for generates profile reports from a pandas DataFrame","7a43e13d":"### __Data Description__\n\n---","030c8402":"This looks much more realistic than before. Our model has a __average accuracy of 82% with a standard deviation of 4 %__. The standard deviation shows us, how precise the estimates are .\n\nThis means in our case that the accuracy of __our model can differ + - 4%__.\n\nI think the accuracy is still really good and since random forest is an easy to use model, we will try to increase it\u2019s performance even further in the following section.","580c1e2b":"## __RMS Titanic__"}}