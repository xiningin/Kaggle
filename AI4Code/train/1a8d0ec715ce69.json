{"cell_type":{"5be1306c":"code","c99d501d":"code","fae62e42":"code","fcbad7a7":"code","b428d128":"code","59d19bb8":"code","0826b208":"code","21c12d3d":"code","2cc37b78":"code","1882538f":"code","2b96e727":"code","2adf3892":"code","4a0724b9":"code","cdda1944":"code","9788e726":"code","2bc5dbc6":"code","bf61d1c3":"code","a3db3047":"code","e3fed568":"code","2b8d5ad2":"code","a9be9b64":"code","784030e7":"code","8faf5730":"code","12d8e2ed":"code","1d1338ce":"code","c402dd62":"code","e283414a":"code","110b5e96":"code","b382754f":"code","0fb3e67c":"code","37a40cc3":"code","d043f335":"code","43b84faa":"code","4eeb657b":"code","c41f47a4":"code","565e2e57":"code","4440011e":"code","a00925c2":"code","59d03c35":"code","b7aa76e6":"code","f28ed9ca":"code","52fe3f9e":"code","3c0318d2":"code","1d96c5eb":"code","65964231":"code","7645186a":"code","965fe52d":"code","2c359ab8":"code","4efcfb0c":"code","de24644a":"code","dbe50895":"code","6557b37c":"code","274efd64":"code","6c91842e":"code","69ffa799":"code","b4153f24":"code","bd46216a":"code","2e956b42":"code","fb9c10c4":"code","d4ba1dab":"code","917f5e10":"code","16c2be35":"code","cd72f216":"code","4e378ad6":"code","00961fa6":"code","c61334b8":"code","b4fc8f23":"code","f96e9bc6":"code","095e43c5":"code","c3136810":"code","a617814d":"code","584068e8":"code","9086c9cd":"code","31901089":"code","3fbdc972":"code","c267d75b":"code","a807601c":"code","328e8a31":"code","2f1a810a":"code","0e7c56cf":"code","5c8c715b":"code","81c6932d":"code","2677bfc5":"code","ac761028":"code","269dfade":"code","60e5893c":"code","8cae3880":"code","62a6f9b2":"code","1600972b":"code","4be1eb03":"code","fc5334e1":"code","f81a54b5":"code","a57e8588":"code","ce274b28":"code","d5446701":"code","8681c365":"code","a72b5c34":"code","efa917a1":"code","ce551c7f":"code","5cc579f0":"code","ee019fd9":"code","9e56034b":"code","3295be04":"code","cbe2e1c2":"code","d55a08c3":"code","889959b1":"code","9f905af3":"code","b9604cf0":"code","2d063618":"code","a4ba1568":"code","3aaf03d2":"code","d50d11db":"code","86a78c7e":"code","084ef8e2":"markdown","eb0f6443":"markdown","e8e446a6":"markdown","3a429ad0":"markdown","07c064ab":"markdown","81d2493a":"markdown","dedbcde8":"markdown","f73f96d1":"markdown","8f540e58":"markdown","d4559d5b":"markdown","ef0c7421":"markdown","a98194a7":"markdown","3dda1921":"markdown","e3e35519":"markdown","b8ed932b":"markdown","eb23ff9a":"markdown","55c7112b":"markdown","7a106e57":"markdown","9a15f8f7":"markdown","038c9dab":"markdown","4b2f835b":"markdown","b8ec9917":"markdown","676c0726":"markdown","3f797b0e":"markdown","2d0633c7":"markdown","7649547a":"markdown","424ac18a":"markdown","ed34a143":"markdown","5e673806":"markdown","d0e74202":"markdown","4b1fa060":"markdown","07f07880":"markdown","b2c1627e":"markdown","5d8ac519":"markdown","135dccdf":"markdown","e627f3e9":"markdown","bd6631d8":"markdown","88a5a738":"markdown"},"source":{"5be1306c":"!pip install pyspark\nimport pandas as pd\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nimport numpy as np\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport numpy as np\n\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext\n\n\nfrom pyspark.ml import feature, regression,classification, Pipeline, evaluation \nfrom pyspark.sql import functions as fn, Row\nfrom pyspark import sql\n\nimport matplotlib.pyplot as plt\nimport pandas as pd","c99d501d":"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)","fae62e42":"df = pd.read_csv('..\/input\/diabetes\/diabetic_data.csv')\n","fcbad7a7":"df","b428d128":"import pandas_profiling","59d19bb8":"df.profile_report()","0826b208":"df.columns","21c12d3d":"df[\"readmitted\"].value_counts()","2cc37b78":"df = pd.get_dummies(df,columns = [df.columns.values[i] for i in range(24,47) ], prefix=[df.columns.values[i] for i in range(24,47)], prefix_sep='_',drop_first=True) \n##Dummy reference Medication Down","1882538f":"df.shape","2b96e727":"df.columns","2adf3892":"df['readmitted'] = df['readmitted'].map({'NO': 0, '<30': 1, \">30\":2})\ndf['readmittedbinary'] = df['readmitted'].map({0: 0, 1: 1, 2:1})","4a0724b9":"df = pd.get_dummies(df, columns=[\"change\",'max_glu_serum','A1Cresult','diabetesMed'], prefix = [\"change\",'max_glu_serum','A1Cresult','diabetesMed'],prefix_sep='_',drop_first=True)\n## Dummy Reference A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch","cdda1944":"df['age'] = df['age'].map({'[0-10)':5,'[10-20)':15, '[20-30)':25,'[30-40)':35,'[40-50)':45,'[50-60)':55,'[60-70)':65,'[70-80)':75,'[80-90)':85,'[90-100)':95})","9788e726":"df.drop(['encounter_id','patient_nbr','weight','admission_type_id','discharge_disposition_id','admission_source_id','medical_specialty','payer_code'],axis=1,inplace=True)\n\n","2bc5dbc6":"df=df.loc[df['gender'].isin(['Male','Female'])]#df.loc[df['B'].isin(['one','three'])]","bf61d1c3":"df.replace('?', np.nan, inplace = True)","a3db3047":"df= df.dropna()##Clean pandas df without dummy variables ","e3fed568":"df.columns","2b8d5ad2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a9be9b64":"a = sns.countplot(x = df['age'], hue= df['readmitted'])","784030e7":"d = sns.countplot(x = df['readmitted'])","8faf5730":"b = sns.countplot(x = df['gender'], hue= df['readmitted'])","12d8e2ed":"c = sns.countplot(x = df['race'], hue= df['readmitted'])","1d1338ce":"count_of_y = df[\"age\"].groupby(df[\"readmitted\"]).value_counts().rename(\"counts\").reset_index()\ncount_of_y\nfig = sns.lineplot(x=\"age\", y=\"counts\", hue=\"readmitted\", data=count_of_y)","c402dd62":"sns.heatmap(df.corr())","e283414a":"plt.figure(figsize=(25, 8))\na = df.corr()\nb = a['readmitted']\nc= b.to_frame()\ntype(c)\nc.sort_values(by = ['readmitted'], ascending = False , inplace = True)\npos = c.head(8)\nc.sort_values(by = ['readmitted'], ascending = True , inplace = True)\nneg = c.head(8)\nneg\n","110b5e96":"pos.index.name = 'feature'\npos.reset_index(inplace=True)\npos\n","b382754f":"neg.index.name = 'feature'\nneg.reset_index(inplace=True)\nneg","0fb3e67c":"pos=pos.drop(pos.index[0:2])\npos","37a40cc3":"posplot = sns.barplot(x='feature', y=\"readmitted\", data=pos)\nposplot.set_xticklabels(posplot.get_xticklabels(),rotation=30)","d043f335":"negplot = sns.barplot(x='feature', y=\"readmitted\", data=neg)\nnegplot\nnegplot.set_xticklabels(negplot.get_xticklabels(),rotation=40)","43b84faa":"df.to_csv(\"clean.csv\")","4eeb657b":"sorted(df.columns)","c41f47a4":"df","565e2e57":"spark_df = spark.read.csv('clean.csv', header=True, inferSchema=True)","4440011e":"spark_df.show()","a00925c2":"#spark_df=spark_df.withColumnRenamed(\"glimepiride-poglitazone\",\"glimepiridepoglitazone\").withColumnRenamed(\"glyburide-metformin\",\"glyburidemetformin\").withColumnRenamed(\"glipizide-metformin\",\"glipizidemetformin\").withColumnRenamed(\"glimepiride-pioglitazone\",\"glimepiridepioglitazone\").withColumnRenamed(\"metformin-rosiglitazone\",\"metforminrosiglitazone\").withColumnRenamed(\"metformin-pioglitazone\",\"metforminpioglitazone\")","59d03c35":"def amit(row):    \n    ma=0\n    mb=0\n    md=0\n    me=0\n    sa=0\n    dr=dd=ddt=di=dm=dg=dn=dr2=dd2=ddt2=di2=dm2=dg2=dn2=dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n    \n    if \"V\" in row.diag_1 or \"E\" in row.diag_1:\n        dr=dd=ddt=di=dm=dg=dn=0\n    #elif 390 <= float(row.diag_1) <= 459 or float(row.diag_1) == 785: #DUMMY DROPPED REFERENCE\n    #    dc =1 ##Circulatory\n    elif 460 <= float(row.diag_1) <= 519 or float(row.diag_1) == 786:\n        dr =1 #Respiratory\n    elif 520 <= float(row.diag_1) <= 579 or float(row.diag_1) == 787:\n        dd =1 #Digestive\n    elif 250 <= float(row.diag_1) <= 250.999:\n        ddt =1 #Diabetes\n    elif 800 <= float(row.diag_1) <= 999:\n        di =1 #Injury\n    elif 710 <= float(row.diag_1) <= 739:\n        dm =1 #musculoskeletal\n    elif 580 <= float(row.diag_1) <= 629 or float(row.diag_1) == 788:\n        dg =1 #Genitourinary\n    elif 140 <= float(row.diag_1) <= 239:\n        dn =1 #Neoplasms\n    else:\n        dr=dd=ddt=di=dm=dg=dn=0\n        #do=1#others\n        \n    if \"V\" in row.diag_2 or \"E\" in row.diag_2:\n        #do2=1\n        dr2=dd2=ddt2=di2=dm2=dg2=dn2=0\n    #elif 390 <= float(row.diag_2) <= 459 or float(row.diag_2) == 785: #DUMMY DROPPED REFERENCE\n    #    dc2 =1 ##Circulatory\n    elif 460 <= float(row.diag_2) <= 519 or float(row.diag_2) == 786:\n        dr2 =1 #Respiratory\n    elif 520 <= float(row.diag_2) <= 579 or float(row.diag_2) == 787:\n        dd2 =1 #Digestive\n    elif 250 <= float(row.diag_2) <= 250.999:\n        ddt2 =1 #Diabetes\n    elif 800 <= float(row.diag_2) <= 999:\n        di2 =1 #Injury\n    elif 710 <= float(row.diag_2) <= 739:\n        dm2 =2 #musculoskeletal\n    elif 580 <= float(row.diag_2) <= 629 or float(row.diag_2) == 788:\n        dg2 =1 #Genitourinary\n    #elif 140 <= float(row.diag_2) <= 239:\n    #   dn2 =1 #Neoplasms\n    else:\n        #do2=1#others\n        dr2=dd2=ddt2=di2=dm2=dg2=dn2=0\n        \n    if \"V\" in row.diag_3 or \"E\" in row.diag_3:\n        #do3=1\n        dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n    #elif 390 <= float(row.diag_3) <= 459 or float(row.diag_3) == 785:#DUMMY DROPPED REFERENCE\n    #    dc3 =1 ##Circulatory\n    elif 460 <= float(row.diag_3) <= 519 or float(row.diag_3) == 786:\n        dr3 =1 #Respiratory\n    elif 520 <= float(row.diag_3) <= 579 or float(row.diag_3) == 787:\n        dd3 =1 #Digestive\n    elif 250 <= float(row.diag_3) <= 250.999:\n        ddt3 =1 #Diabetes\n    elif 800 <= float(row.diag_3) <= 999:\n        di3 =1 #Injury\n    elif 710 <= float(row.diag_3) <= 739:\n        dm3 =1 #musculoskeletal\n    elif 580 <= float(row.diag_3) <= 629 or float(row.diag_3) == 788:\n        dg3 =1 #Genitourinary\n    elif 140 <= float(row.diag_3) <= 239:\n        dn3 =1 #Neoplasms\n    else:\n        dr3=dd3=ddt3=di3=dm3=dg3=dn3=0\n        #do3=1#others  \n\n    \n    if row.race == \"Caucasian\":\n        ma = 1\n        #prfloat(\"A\")\n    elif row.race == \"Asian\":\n        mb = 1\n    #elif row.race == \"AfricanAmerican\": #DUMMY DROPPED REFERENCE\n    #    mc = 1\n    elif row.race ==\"Hispanic\":\n        me = 1\n    else:# :\n        ma=0\n        mb=0\n        me = 0\n\n\n    if row.gender == \"Male\":\n        sa = 1\n    #elif row.gender == \"Female\": #DROPPED DUMMY REFERENCE\n    #    sb = 1\n    \n    \n    r = Row(Caucasian=int(ma) ,Asian=int(mb) ,Hispanic=int(me),male=float(sa),\n            Respiratory=dr,\n            Digestive= dd,\n            Diabetes = ddt,\n            Injury= di,\n            Muscuskeletal= dm,\n            Neoplasms=dn,\n            Genitourinary = dg,\n            \n            Respiratory2=dr2,\n            Digestive2= dd2,\n            Diabetes2 = ddt2,\n            Injury2= di2,\n            Muscuskeletal2= dm2,\n            Neoplasms2=dn2,\n            Genitourinary2 = dg2,\n            \n            Respiratory3=dr3,\n            Digestive3= dd3,\n            Diabetes3 = ddt3,\n            Injury3= di3,\n            Muscuskeletal3= dm3,\n            Neoplasms3=dn3,\n            Genitourinary3 = dg3,\n            \n      )\n    return(r)","b7aa76e6":"dummy_df = spark.createDataFrame(spark_df.rdd.map(amit))","f28ed9ca":"spark_df.show()","52fe3f9e":"dummy_df.show()","3c0318d2":"from pyspark.sql.functions import monotonically_increasing_id, row_number\nfrom pyspark.sql.window import Window\n# since there is no common column between these two dataframes add row_index so that it can be joined\nspark_df=spark_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\ndummy_df=dummy_df.withColumn('row_index', row_number().over(Window.orderBy(monotonically_increasing_id())))\n\ndummy_df = dummy_df.join(spark_df, on=[\"row_index\"]).drop(\"row_index\")\ndummy_df.show()\n","1d96c5eb":"dummy_df = dummy_df.drop(\"diag_1\",\"diag_2\",\"diag_3\",\"gender\",\"race\")","65964231":"dummy_df = dummy_df.drop(\"_c0\")","7645186a":"#Checking dummies","965fe52d":"dummy_df.select('male',\"Caucasian\",\"Hispanic\",\"Asian\").show()#,'Male','Female', 'Circulatory','Circulatory2','Circulatory3').show()","2c359ab8":"dummy_df.printSchema()","4efcfb0c":"training_df, validation_df, testing_df = dummy_df.randomSplit([0.6, 0.3, 0.1], seed=0)","de24644a":"dummy_df.columns","dbe50895":"featlist = ['Asian',\n 'Caucasian',\n 'Diabetes',\n 'Diabetes2',\n 'Diabetes3',\n 'Digestive',\n 'Digestive2',\n 'Digestive3',\n 'Genitourinary',\n 'Genitourinary2',\n 'Genitourinary3',\n 'Hispanic',\n 'Injury',\n 'Injury2',\n 'Injury3',\n 'Muscuskeletal',\n 'Muscuskeletal2',\n 'Muscuskeletal3',\n 'Neoplasms',\n 'Neoplasms2',\n 'Neoplasms3',\n 'Respiratory',\n 'Respiratory2',\n 'Respiratory3',\n 'male',\n 'age',\n 'time_in_hospital',\n 'num_lab_procedures',\n 'num_procedures',\n 'num_medications',\n 'number_outpatient',\n 'number_emergency',\n 'number_inpatient',\n 'number_diagnoses',\n 'metformin_No',\n 'metformin_Steady',\n 'metformin_Up',\n 'repaglinide_No',\n 'repaglinide_Steady',\n 'repaglinide_Up',\n 'nateglinide_No',\n 'nateglinide_Steady',\n 'nateglinide_Up',\n 'chlorpropamide_No',\n 'chlorpropamide_Steady',\n 'chlorpropamide_Up',\n 'glimepiride_No',\n 'glimepiride_Steady',\n 'glimepiride_Up',\n 'acetohexamide_Steady',\n 'glipizide_No',\n 'glipizide_Steady',\n 'glipizide_Up',\n 'glyburide_No',\n 'glyburide_Steady',\n 'glyburide_Up',\n 'tolbutamide_Steady',\n 'pioglitazone_No',\n 'pioglitazone_Steady',\n 'pioglitazone_Up',\n 'rosiglitazone_No',\n 'rosiglitazone_Steady',\n 'rosiglitazone_Up',\n 'acarbose_No',\n 'acarbose_Steady',\n 'acarbose_Up',\n 'miglitol_No',\n 'miglitol_Steady',\n 'miglitol_Up',\n 'troglitazone_Steady',\n 'tolazamide_Steady',\n 'tolazamide_Up',\n 'insulin_No',\n 'insulin_Steady',\n 'insulin_Up',\n 'glyburide-metformin_No',\n 'glyburide-metformin_Steady',\n 'glyburide-metformin_Up',\n 'glipizide-metformin_Steady',\n 'glimepiride-pioglitazone_Steady',\n 'metformin-rosiglitazone_Steady',\n 'metformin-pioglitazone_Steady',\n 'change_No',\n 'max_glu_serum_>300',\n 'max_glu_serum_None',\n 'max_glu_serum_Norm',\n 'A1Cresult_>8',\n 'A1Cresult_None',\n 'A1Cresult_Norm',\n 'diabetesMed_Yes']","6557b37c":"model1 = Pipeline(stages=[feature.VectorAssembler(inputCols=featlist,\n                                        outputCol='features'),feature.StandardScaler(inputCol='features',outputCol = 'sdfeatures'),\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","274efd64":"pipe_model = model1.fit(training_df)","6c91842e":"pipe_modeldf = pipe_model.transform(validation_df).select(\"readmittedbinary\",\"prediction\")\npipe_modeldf.show()","69ffa799":"tp = pipe_modeldf[(pipe_modeldf.readmittedbinary == 1) & (pipe_modeldf.prediction == 1)].count()\ntn = pipe_modeldf[(pipe_modeldf.readmittedbinary == 0) & (pipe_modeldf.prediction == 0)].count()\nfp = pipe_modeldf[(pipe_modeldf.readmittedbinary == 0) & (pipe_modeldf.prediction == 1)].count()\nfn = pipe_modeldf[(pipe_modeldf.readmittedbinary == 1) & (pipe_modeldf.prediction == 0)].count()\nprint (\"True Positives:\", tp)\nprint (\"True Negatives:\", tn)\nprint (\"False Positives:\", fp)\nprint (\"False Negatives:\", fn)\nprint (\"Total\", dummy_df.count())\n\nr = (tp)\/(tp + fn)\nprint (\"recall\", r)\n\np = float(tp) \/ (tp + fp)\nprint (\"precision\", p)","b4153f24":"specificity = tn\/(tn+fp)\nprint(\"specificity\",specificity)","bd46216a":"evaluator = evaluation.BinaryClassificationEvaluator(labelCol='readmittedbinary')\nAUC1 = evaluator.evaluate(pipe_model.transform(validation_df))","2e956b42":"AUC1","fb9c10c4":"pd.DataFrame(list(zip(featlist, pipe_model.stages[-1].coefficients.toArray())),\n            columns = ['column', 'Coefficients']).sort_values('Coefficients',ascending = False).head(10)","d4ba1dab":"print(\"Intercept: \" + str(pipe_model.stages[-1].intercept))","917f5e10":"prob = 1\/(1+np.exp(-pipe_model.stages[-1].intercept))\nprob","16c2be35":"np.exp(0.4582)","cd72f216":"pd.DataFrame(list(zip(featlist, pipe_model.stages[-1].coefficients.toArray())),\n            columns = ['column', 'Coefficients']).sort_values('Coefficients').head(10)","4e378ad6":"np.exp(-0.068755)","00961fa6":"beta = pipe_model.stages[-1].coefficients\nplt.plot(beta)\nplt.ylabel('Coefficients')\nplt.show()","c61334b8":"trainingSummary = pipe_model.stages[-1].summary\nroc = trainingSummary.roc.toPandas()\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))\nroc","b4fc8f23":"fMeasure = trainingSummary.fMeasureByThreshold\nmaxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\nbestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n    .select('threshold').head()['threshold']\nmaxFMeasure","f96e9bc6":"pr = trainingSummary.pr.toPandas()\nplt.plot(pr['recall'],pr['precision'])\nplt.ylabel('Precision')\nplt.xlabel('Recall')\nplt.show()\n#pr['recall']","095e43c5":"from pyspark.ml.tuning import ParamGridBuilder, CrossValidator","c3136810":"va =feature.VectorAssembler(inputCols=featlist ,  outputCol='features')","a617814d":"sd = feature.StandardScaler(inputCol='features',outputCol = 'sdfeatures')","584068e8":"lr = classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')","9086c9cd":"pipe_model2 = Pipeline(stages=[va,sd, lr])","31901089":"paramGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.1, 0.3, 0.5]).addGrid(lr.elasticNetParam, [0.2, 0.8, 0.5]).addGrid(lr.maxIter, [15, 30, 50]).build())","3fbdc972":"cv = CrossValidator(estimator=pipe_model2, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=5)","c267d75b":"cvModel = cv.fit(training_df)","a807601c":"AUC2 = evaluator.evaluate(cvModel.transform(validation_df))\nAUC2","328e8a31":"param_dict = cvModel.bestModel.stages[-1].extractParamMap()\n\nsane_dict = {}\nfor k, v in param_dict.items():\n    #print(k)\n    sane_dict[k.name] = v\n\nbest_reg = sane_dict[\"regParam\"]\nbest_elastic_net = sane_dict[\"elasticNetParam\"]\nbest_max_iter = sane_dict[\"maxIter\"]\nprint(best_reg)\nprint(best_elastic_net)\nprint(best_max_iter)","2f1a810a":"modelrf = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\")])","0e7c56cf":"modelrffit= modelrf.fit(training_df)","5c8c715b":"modelrfdf = modelrffit.transform(validation_df)","81c6932d":"modelrfdf","2677bfc5":"AUCrf = evaluator.evaluate(modelrffit.transform(validation_df))","ac761028":"AUCrf","269dfade":"pd.DataFrame(list(zip(featlist, modelrffit.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","60e5893c":"tp = modelrfdf[(modelrfdf.readmittedbinary == 1) & (modelrfdf.prediction == 1)].count()\ntn = modelrfdf[(modelrfdf.readmittedbinary == 1) & (modelrfdf.prediction == 0)].count()\nfp = modelrfdf[(modelrfdf.readmittedbinary == 0) & (modelrfdf.prediction == 1)].count()\nfn = modelrfdf[(modelrfdf.readmittedbinary == 0) & (modelrfdf.prediction == 0)].count()\nprint (\"True Positives:\", tp)\nprint (\"True Negatives:\", tn)\nprint (\"False Positives:\", fp)\nprint (\"False Negatives:\", fn)\n#print (\"Total\", df.count())\n\nr = (tp)\/(tp + fn)\nprint (\"recall\", r)\n\np = float(tp) \/ (tp + fp)\nprint (\"precision\", p)","8cae3880":"sensitivity = tn\/(tn+fp)\nprint(\"Sensitivity\",sensitivity)","62a6f9b2":"beta = modelrffit.stages[-1].featureImportances\nplt.plot(beta)\nplt.ylabel('Importance')\nplt.show()","1600972b":"rf=classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\")","4be1eb03":"mrf = Pipeline(stages=[va,rf])","fc5334e1":"paramGrid = (ParamGridBuilder()\n             .addGrid(rf.numTrees, [40]).addGrid(rf.maxDepth,[5,10,15,30]).build())\n\n","f81a54b5":"cvrf = CrossValidator(estimator=mrf, \\\n                    estimatorParamMaps=paramGrid, \\\n                    evaluator=evaluator, \\\n                    numFolds=3)","a57e8588":"cvrf1 = cvrf.fit(training_df)","ce274b28":"AUCn = evaluator.evaluate(cvrf1.transform(validation_df))\nAUCn","d5446701":"param_dict1 = cvrf1.bestModel.stages[-1].extractParamMap()\n\nsane_dict1 = {}\nfor k, v in param_dict1.items():\n    #print(k)\n    sane_dict1[k.name] = v\n\n\nbest_max_depth = sane_dict1[\"maxDepth\"]\nprint(best_max_depth)","8681c365":"modelrfmc = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmitted', featuresCol=\"features\",maxDepth=15, numTrees=30)])","a72b5c34":"modelrffitmc= modelrfmc.fit(training_df)","efa917a1":"evaluatormc = evaluation.MulticlassClassificationEvaluator(labelCol='readmitted',predictionCol=\"prediction\",metricName=\"accuracy\")\nAUCrfmc = evaluatormc.evaluate(modelrffitmc.transform(validation_df))","ce551c7f":"AUCrfmc","5cc579f0":"pd.DataFrame(list(zip(featlist, modelrffitmc.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","ee019fd9":"model_new_lr2 = Pipeline(stages=[feature.VectorAssembler(inputCols=['number_inpatient', 'number_emergency', 'number_diagnoses', 'Diabetes', 'number_outpatient','time_in_hospital','diabetesMed_Yes','age','rosiglitazone_Steady','Caucasian'],\n                                        outputCol='features'),sd,\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","9e56034b":"pipe_model3 = model_new_lr2.fit(training_df)\nAUC5 = evaluator.evaluate(pipe_model3.transform(validation_df))\nAUC5","3295be04":"modelrfselected = Pipeline(stages= [feature.VectorAssembler(inputCols=['number_inpatient', 'number_emergency', 'number_diagnoses', 'Diabetes', 'number_outpatient','time_in_hospital','diabetesMed_Yes','age','rosiglitazone_Steady','Caucasian'],\n                                        outputCol='features'), classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\",maxDepth=15, numTrees=30)])","cbe2e1c2":"modelrffitselected= modelrfselected.fit(training_df)","d55a08c3":"AUCrfselected = evaluator.evaluate(modelrffitselected.transform(validation_df))\nAUCrfselected","889959b1":"model_new_lr3 = Pipeline(stages=[feature.VectorAssembler(inputCols=['number_inpatient','num_medications','num_lab_procedures','number_diagnoses','time_in_hospital','age','number_emergency', 'number_outpatient','num_procedures','male'],\n                                        outputCol='features'),sd,\n                 classification.LogisticRegression(labelCol='readmittedbinary', featuresCol='sdfeatures')])\n","9f905af3":"pipe_model4 = model_new_lr3.fit(training_df)\nAUC6 = evaluator.evaluate(pipe_model4.transform(validation_df))\nAUC6","b9604cf0":"best_model = Pipeline(stages= [va, classification.RandomForestClassifier(labelCol='readmittedbinary', featuresCol=\"features\",maxDepth=10, numTrees=40)])","2d063618":"bestmodel_fit= best_model.fit(training_df)","a4ba1568":"AUCfinal = evaluator.evaluate(bestmodel_fit.transform(testing_df))","3aaf03d2":"AUCfinal","d50d11db":"pd.DataFrame(list(zip(featlist, bestmodel_fit.stages[1].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight', ascending = False).head(10)","86a78c7e":"bestmodel_final_fit= best_model.fit(dummy_df)","084ef8e2":"Let us talk about coefficient of number_inpatient. It has a value of 0.4582. We interpret as: For one unit increase in number_inpatient, the logit of being readmitted increases by 0.4582, everything else being constant. We can compute odd ratio by taking the exponent of the coefficient.","eb0f6443":"### Logistic Regression on top of Random Forest features","e8e446a6":"Every one unit increase in num_procedures decreases the odds of being readmitted by about 7 percent.","3a429ad0":"### Feature Engineering","07c064ab":"### Logistic model 2 with all features and reg param","81d2493a":"Interesting! Our model correctly predicts 79% of people who will not require readmittance correctly.","dedbcde8":"We can say that with everything else being zero, an African American female with diag1,diag2,diag3 as Circulatory and and all medications as Down, and the following characteristics: A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch, has 46.73% probability of being readmitted to the hospital. ","f73f96d1":"### Best model","8f540e58":"Testing the best model so far on Testing data","d4559d5b":"## *Will a patient be readmitted to a hospital within 30 days?*","ef0c7421":"# Splitting into training, testing and validation","a98194a7":"\n### Importing required libraries","3dda1921":"![](https:\/\/dsuqs7rf8y6gn.cloudfront.net\/wp-content\/uploads\/2019\/03\/patient-based.png)","e3e35519":"### Random forest model 1","b8ed932b":"Let us look at coefficient of num_procedures. ","eb23ff9a":"### Data Exploration and Data Cleaning","55c7112b":"Now that we have all the coefficients and intercept, lets try to interpret their meaning. Remember that in logistic regression we do not predict the actual value like linear regression. Instead, we try to predict the probabilty of getting 1 or 0; readmitted or not readmitted. To do this we fit the data to sigmoid function. To explain y as a linear combination of x variables, we take logit, which  is log(p\/(1-p)). Hence, the coefficients in logistic regression are in terms of log odds. \nLet us look at the intercept first. The intercept has value of -0.1306. Remember we have chosen dummies as reference. So we interpret the intercept as : The log odds or logit of an African American female with diag1,diag2,diag3 as Circulatory and and all medications as Down, and the following characteristics: A1Cresult_>7,max_glu_serum>200,diabetesMed=No, change=ch, is -0.1306. We can find the probability by plugging the value in sigmoid function.","7a106e57":"### Model Building and Evaluation","9a15f8f7":"### Random Forest on top features of original Logistic Regression","038c9dab":"# Logistic model 1 with all features no parameter tuning","4b2f835b":"Let us closely look at our results here. The confusion matrix here shows that there are 5742 true positives. This means that the model _correctly_ predicted 5742 patients who were readmitted to the hospital as readmitted. There are 12317 true negatives. This means that the model _correctly_ predicted 12317 patients who were NOT readmitted to hospital as NOT readmitted. Similarly, the model _incorrectly_ predicted 3351 people who were NOT readmitted to hospital as readmitted. Also, the model _incorrectly_ predicted that 7962 who were readmitted as NOT readmitted.\nLooking at the recall value we can say that our our model correctly predicts 42% of readmitted cases (positives). Lets calculate specificity","b8ec9917":"# Looking at our final columns","676c0726":"# Patient Readmittance Analysis","3f797b0e":"### Data Visualization","2d0633c7":"### Creating a Spark DataFrame","7649547a":"Odd ratio can be thought of as odds of being readmitted when number_inpatients is n+1 by odds of being readmitted when number_inpatients is n. Hence we can say that, holding all the other variables fixed, by increasing number_inpatient by one, we expect to see the odds of getting readmitted increase by about 58%.","424ac18a":"# Model interpretation (Intercept and Coefficients)","ed34a143":"## In this notebook, I have tried to analyze if a patient will be readmitted to a hostipal given certain medical and demographic data. To understand this data, certain degree of healthcare domain knowledge is needed, like ICD9 diagnosis codes. I have used pandas for EDA and data preprocessing and pyspark for modelling. Models used: Logistic Regression, Random Forest.I have also extracted the coefficients and important features from both models to find out which parameters have most weight on determining if a patient will be readmitted. ","5e673806":"Here, I created a function to deal with ICD-9 codes. Please read the data description for more information about these codes.","d0e74202":"This is our target variable. We convert it to 0 and 1 for Binary Classification.","4b1fa060":"### Multiclass classification for Readmitted","07f07880":"Columns 24 to 47 are medications which are categories. Most of the columns are nominal and categorical. We will deal with them by creating dummies. Notice that we are dropping one dummy. So during our interpretation we will have to interpret coefficients of other dummies with reference to the dropped dummy!","b2c1627e":"I am joining the two spark dataframes here on row number. This will be our final data with all dummy variables.","5d8ac519":"### Cross-Validation Random Forest Model","135dccdf":"Dropping these columns as we already have their dummies","e627f3e9":"### *If you have any questions or feedback, please comment! And if you find this notebook helpful, do leave an  UPVOTE*","bd6631d8":"### Logistic Regression on top features of original Logistic Regression","88a5a738":"Fit the model to entire data"}}