{"cell_type":{"f82c8fb9":"code","e3bf9517":"code","f5ffc819":"code","236a4920":"code","443f7fdb":"code","4c7e1c62":"code","fa128d4d":"code","23871559":"code","67827397":"code","335c20d5":"code","950e4a01":"code","5aff3d38":"code","7a4241cb":"code","800e5e18":"code","2eab7334":"code","deb02bf3":"code","a57d3d7b":"code","011e4f52":"code","ef850ed6":"code","2572e4cb":"code","44f973b8":"code","ff555d7d":"code","05a4b380":"code","42678975":"code","82d73a3c":"code","c93d627a":"code","9a1d825c":"code","e6ac505e":"code","adb06097":"code","c2bf387d":"code","1b166ac8":"code","e7d2bbdd":"code","403db6b1":"code","9e3ce4a6":"code","875dcdf6":"code","4a6b6710":"code","af9f977a":"code","d7d2411d":"code","d591aedf":"code","fb517481":"code","b95fe5b9":"code","ea7d7468":"code","17a4fe25":"code","5cf7057e":"code","8a0a4ad4":"code","3d3d5c17":"code","d4250406":"code","873105a6":"code","2c6babea":"code","5753408b":"code","0b7b8bb9":"code","52f1c5ca":"code","b80533df":"code","b4539641":"code","3b202697":"code","7aedec85":"code","5f75accf":"code","983d188c":"code","659f968d":"code","adb66f4b":"code","6b0c6f1b":"code","bad3740f":"markdown","4e2be1d9":"markdown","6c223a2d":"markdown","64792bda":"markdown","17118ec5":"markdown","3b90a1a2":"markdown","3d41b8da":"markdown","a092e497":"markdown","b545ea49":"markdown","a3b85d64":"markdown","f1dabd81":"markdown","a1b50a18":"markdown","934aa21c":"markdown","3ae2ad99":"markdown","3950797c":"markdown","8cbb118c":"markdown","112e7755":"markdown"},"source":{"f82c8fb9":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# # For example, here's several helpful packages to load in \n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# # Input data files are available in the \"..\/input\/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # Any results you write to the current directory are saved as output.","e3bf9517":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer , PowerTransformer\n\n\nfrom keras.layers import Dense , LSTM\nfrom keras.models import Sequential\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\ncmap = cm.get_cmap('Spectral') # Colour map (there are many others)\n\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import r2_score","f5ffc819":"# loading triain_FD001 file\ntrain_file = \"\/kaggle\/input\/nasa-cmaps\/CMaps\/train_FD001.txt\" \ntest_file = \"\/kaggle\/input\/nasa-cmaps\/CMaps\/test_FD001.txt\"\nRUL_file = \"\/kaggle\/input\/nasa-cmaps\/CMaps\/RUL_FD001.txt\"\n\ndf = pd.read_csv(train_file,sep=\" \",header=None)\ndf.head()","236a4920":"df.drop(columns=[26,27],inplace=True)","443f7fdb":"columns = [\"Section-{}\".format(i)  for i in range(26)]\ndf.columns = columns\ndf.head()","4c7e1c62":"# Names \nMachineID_name = [\"Section-0\"]\nRUL_name = [\"Section-1\"]\nOS_name = [\"Section-{}\".format(i) for i in range(2,5)]\nSensor_name = [\"Section-{}\".format(i) for i in range(5,26)]\n\n# Data in pandas DataFrame\nMachineID_data = df[MachineID_name]\nRUL_data = df[RUL_name]\nOS_data = df[OS_name]\nSensor_data = df[Sensor_name]\n\n# Data in pandas Series\nMachineID_series = df[\"Section-0\"]\nRUL_series = df[\"Section-1\"]","fa128d4d":"MachineID_series.unique()","23871559":"grp = RUL_data.groupby(MachineID_series)\nmax_cycles = np.array([max(grp.get_group(i)[\"Section-1\"]) for i in MachineID_series.unique()])\nmax_cycles","67827397":"print(\"Max Life >> \",max(max_cycles))\nprint(\"Mean Life >> \",np.mean(max_cycles))\nprint(\"Min Life >> \",min(max_cycles))","335c20d5":"df.info()","950e4a01":"df.describe()","5aff3d38":"# Now, Initially visulizing the Operation Settings 2,3,4\ndf.plot(x=RUL_name[0], y= OS_name[0], c='k'); df.plot(x=RUL_name[0], y=OS_name[0], kind= \"kde\") \ndf.plot(x=RUL_name[0], y=OS_name[1], c='k'); df.plot(x=RUL_name[0], y=OS_name[1], kind='kde')\ndf.plot(x=RUL_name[0], y=OS_name[2], c='k')#; df.plot(x=RUL_name[0], y=OS_name[2], kind='kde') ","7a4241cb":"for name in Sensor_name:\n    df.plot(x=RUL_name[0], y=name, c='k')","800e5e18":"data = pd.concat([RUL_data,OS_data,Sensor_data], axis=1)\ndata.drop(data[[\"Section-4\", # Operatinal Setting\n                \"Section-5\", # Sensor data\n                \"Section-9\", # Sensor data\n                \"Section-10\", # Sensor data\n                \"Section-14\",# Sensor data\n                \"Section-20\",# Sensor data\n                \"Section-22\",# Sensor data\n                \"Section-23\"]], axis=1 , inplace=True)","2eab7334":"def Normalize(dataframe):\n    gen = MinMaxScaler(feature_range=(0, 1))\n    gen_data = gen.fit_transform(dataframe)\n    return gen_data\n\ndef reshaping(train_X, train_y):\n    abc = np.array(train_X).reshape(-1,1)\n    asdsad= np.reshape(abc, (abc.shape[0], 1, abc.shape[1]))\n    return asdsad, np.array(train_y)","deb02bf3":"# Making the funtion which Outputs RUL Dataframe\ndef RUL_df():\n    rul_lst = [j  for i in MachineID_series.unique() for j in np.array(grp.get_group(i)[::-1][\"Section-1\"])]\n    rul_col = pd.DataFrame({\"rul\":rul_lst})\n    return rul_col\n\nRUL_df().head()","a57d3d7b":"#  Now, getting the data & Split it \nnormalize_labels = Normalize(np.array(RUL_df()).reshape(-1,1)).reshape(1,-1)[0]\n# print(normalize_labels.reshape(1,-1)[0])\n\ntrain_X , test_X , train_Y , test_Y = train_test_split(Normalize(data),normalize_labels , test_size = 0.2)","011e4f52":"#### Moving towards the model making and it's requiste\ndef lstm_reshaping(train_X, train_y):\n    train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n    train_Y = np.array(train_y).reshape(1,-1)[0]\n    return train_X, train_Y","ef850ed6":"# Making data according to model input requirements:\ntrainX, trainY = lstm_reshaping(train_X, train_Y)\ntestX, testY = lstm_reshaping(test_X, test_Y)\n\n# Examining\nprint(trainX.shape)\nprint(testX.shape)","2572e4cb":"look_back = 17\n# Model\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(1, look_back)))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])","44f973b8":"model.summary()","ff555d7d":"history = model.fit(trainX, trainY, epochs=10, batch_size=4, validation_data=(testX, testY))","05a4b380":"print(history.history.keys())","42678975":"# Accuracy Graph\nplt.plot(history.epoch, history.history['accuracy'] , label=\"accuracy\")\nplt.plot(history.epoch, history.history['val_accuracy'] , label = \"val_accuracy\")\nplt.legend()\nplt.show()","82d73a3c":"# Loss Graph\nplt.plot(history.epoch, history.history['loss'] , label = \"loss\")\nplt.plot(history.epoch, history.history['val_loss'] , label = \"val_loss\")\nplt.legend()\nplt.show()","c93d627a":"# Now Making the Prediction and Evaluating the model\nscore = model.evaluate(testX, testY, verbose = 0)\nprint(\"%s: %.2f%%\" % (\"acc\", score[1]*100))","9a1d825c":"# Checking the model prediction on graphical form \nprint(len(testY))\nprint(len(testX))\n\nplt.plot(testY, label = \"Actual\")\nplt.xlabel(\"Data\");plt.ylabel(\"RUL\")\n\nval = model.predict(testX)\nplst = [i[0] for i in val]\nplt.plot(plst, c='k', label=\"Predict\")\nplt.legend();plt.show()","e6ac505e":"def rolling_mean(pandas_df):\n    data = pandas_df.rolling(20).mean()\n    return data\n\ndef method_PCA(df):\n    pca = PCA(n_components=1)\n    data = pca.fit_transform(Normalize(df))\n    return data\n\ndef transform_data(data):\n    pt = PowerTransformer()\n    transform_data = pt.fit_transform(data)\n    return transform_data ","adb06097":"# grouping w.r.t MID (Machine ID)\ncol_names = data.columns\ndef grouping(datafile, mid_series):\n    data = [x for x in datafile.groupby(mid_series)]\n    return data  ","c2bf387d":"# APPLYING PCA WITH RESPECT TO THERE MID DATA\ndef data_processing(dataframe, grp_mid_series):\n    pca_data = grouping(dataframe, grp_mid_series)\n    process_lst_of_lst =[]\n    jk =1\n    for i in pca_data:\n        dfs = i[1] \n\n        time = dfs[\"Section-1\"]\n        data = method_PCA(dfs)\n\n        print(\"----------------MachineID-{}----------------\".format(jk))\n\n        data = transform_data(data)\n        process_lst_of_lst.append(data)\n\n        plt.plot(time, data)\n        plt.show()\n\n        jk = jk+1\n    return process_lst_of_lst\n\nprocess_lst_of_lst = data_processing(df, MachineID_series)","1b166ac8":"### Visulization of all 100 machine data all togther after applying PCA\nfor i in process_lst_of_lst:\n    val = [j for j in range(len(i))]\n    plt.plot(val,i)","e7d2bbdd":"process_data_lst = [j for i in process_lst_of_lst for j in i.reshape(1,-1)[0]]\nprocess_df = pd.DataFrame({\"MID\":MachineID_series, 'data':process_data_lst, 'rul':list(RUL_df()[\"rul\"])})\nprocess_df.head()","403db6b1":"train_X , test_X , train_y , test_y = train_test_split(process_df['data'], process_df['rul'] , test_size = 0.01)","9e3ce4a6":"# Making data according to model input requirements:\ntrainX, trainY = reshaping(train_X, train_y)\ntestX, testY = reshaping(test_X, test_y)\n\n# Examining\nprint(trainX.shape)\nprint(testX.shape)","875dcdf6":"look_back = 1\n# Model\nmodel = Sequential()\nmodel.add(LSTM(50, input_shape=(1, look_back)))\nmodel.add(Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam',metrics=['mae', 'mse'])","4a6b6710":"model.summary()","af9f977a":"history = model.fit(trainX, trainY, epochs=5, batch_size=16, validation_data=(testX, testY))","d7d2411d":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","d591aedf":"print(history.history.keys())","fb517481":"plt.plot(history.epoch, history.history['mae'] , label=\"mae\")\nplt.plot(history.epoch, history.history['val_mae'] , label = \"val_mae\")\nplt.legend()\nplt.show()","b95fe5b9":"plt.plot(history.epoch, history.history['mse'] , label=\"mse\")\nplt.plot(history.epoch, history.history['val_mse'] , label = \"val_mse\")\nplt.legend()\nplt.show()","ea7d7468":"print(len(testY))\nprint(len(testX))\n\nplt.plot(testY, label = \"Actual\")\nplt.xlabel(\"Data\");plt.ylabel(\"RUL\")\nval = model.predict(testX)\nplst = [i[0] for i in val]\nplt.plot(plst, c='k', label=\"Predict\")\nplt.legend();plt.show()","17a4fe25":"process_df.head()","5cf7057e":"from sklearn.linear_model import LinearRegression\nX = np.array(process_df['data']).reshape(-1,1)\n\ny = np.array(process_df['rul']).reshape(-1,1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20) \n\nreg = LinearRegression()\nreg.fit(X_train, y_train)","8a0a4ad4":"## Accuracy of Linear Regression\nprint(\"Acc of lR >> \",reg.score(X_test, y_test))","3d3d5c17":"plt.scatter(X_train, y_train, c='k', label='training data')\nplt.scatter(X_test, y_test, label='testing data') \npred = reg.predict(X_test)\nplt.plot(X_test, pred, c='r' , label='regression line')\nplt.xlabel(\"Sensor Data\"); plt.ylabel(\"RUL values\")\nplt.legend()\nplt.show()","d4250406":"# Now, trying to check this in all machine-id\ndef graphs(model_instance):\n    for i in range(1,101):\n        dataset = process_df.groupby('MID').get_group(i)\n        print(\"MID-{}\".format(i));\n        testX = np.array(dataset['data']).reshape(-1,1)\n        testy = np.array(dataset['rul']).reshape(-1,1)\n\n        # score\n        print(\"acc at MID-{} >> \".format(i), model_instance.score(testX, testy))\n\n        # graph\n        pred = model_instance.predict(testX)\n        plt.scatter(testX, testy) \n        plt.plot(testX, pred, c='r')\n        plt.show()\n\ngraphs(reg)","873105a6":"print(\"R2 Score >>\", r2_score(y_test, pred))\nprint(\"explained_variance_score >> \", explained_variance_score(y_test, pred))\nprint(\"mean_squared_error >> \", mean_squared_error(y_test, pred))\nprint(\"mean_absolute_error >>\",mean_absolute_error(y_test, pred))\n# print('cv score >> ',cross_val_score(reg, testX, testy, cv=3))","2c6babea":"plt.plot(y_test)\nplt.plot(pred,c='k')","5753408b":"# trying dession tree regreesor\nfrom sklearn.tree import DecisionTreeRegressor\n\ndt_reg = DecisionTreeRegressor()\ndt_reg.fit(X_train, y_train)\nprint(\"acc of DTS >> \",dt_reg.score(X_test, y_test))","0b7b8bb9":"plt.scatter(X_train, y_train, c='k', label='training data')\nplt.scatter(X_test, y_test, label='testing data') \ndt_pred = dt_reg.predict(X_test)\nplt.scatter(X_test, dt_pred, c='r' , label='prediction')\nplt.xlabel(\"Sensor Data\"); plt.ylabel(\"RUL values\")\nplt.legend()\nplt.show()","52f1c5ca":"print(\"R2 Score >>\", r2_score(y_test, dt_pred))\nprint(\"explained_variance_score >> \", explained_variance_score(y_test, dt_pred))\nprint(\"mean_squared_error >> \", mean_squared_error(y_test, dt_pred))\nprint(\"mean_absolute_error >>\",mean_absolute_error(y_test, dt_pred))\n# print('cv score >> ',cross_val_score(dt_reg, testX, testy, cv=3))","b80533df":"# Now, trying to check this in all machine-id        \ngraphs(dt_reg)","b4539641":"plt.plot(y_test)\nplt.plot(dt_pred,c='k')","3b202697":"df_test = pd.read_csv(test_file, sep=\" \",header=None)\ndf_test.drop(columns=[26,27],inplace=True)\ndf_test.columns = columns\ndf_test.head()","7aedec85":"df_rul = pd.read_csv(RUL_file, names=['rul'])\ndf_rul.head()","5f75accf":"df_test.drop(df_test[[\"Section-4\", # Operatinal Setting\n                \"Section-5\", # Sensor data\n                \"Section-9\", # Sensor data\n                \"Section-10\", # Sensor data\n                \"Section-14\",# Sensor data\n                \"Section-20\",# Sensor data\n                \"Section-22\",# Sensor data\n                \"Section-23\"]], axis=1 , inplace=True)\n\ntest_mid = df_test[\"Section-0\"]\ndf_test.head()","983d188c":"test_lst=data_processing(df_test,test_mid)","659f968d":"test_process_data_lst = [j for i in test_lst for j in i.reshape(1,-1)[0]]\ntest_process_df = pd.DataFrame({\"MID\":test_mid, 'data':test_process_data_lst})\ntest_process_df.head()","adb66f4b":"val = test_process_df.groupby('MID')\nval = [np.array(val.get_group(i)['data']).reshape(-1,1) for i in range(1,101)]\n# print(val)\n# print(np.array(test_rul_csv)[2][0])","6b0c6f1b":"# lr prediction\ncount = 0\nrul = np.array(df_rul)\nfor mid_val in val:\n    lr_predict = reg.predict(mid_val)\n    plt.plot([i for i in range(len(mid_val))], [rul[count][0] for i in range(len(mid_val))], c='r')\n    plt.plot(lr_predict)\n    count = count +1\n    plt.show()","bad3740f":"#### Plotting accuracy & loss graph\n","4e2be1d9":"### CAN BE DONE MORE BETTER FROM HERE","6c223a2d":"### Now, Visulizing the sensor's value w.r.t its Cycles so, to find is there any deviation with mean in it or not \n- 'NOTE:' that all MID is taken all together for this","64792bda":"### Testing on the test_data (test_FD001.txt)","17118ec5":"### Workflow:\n\n- remove noise\n- standatize or normalize the dataset mainly normalize it\n- now, split the dataset and apply RNN on it\n- test the model on split data\n- import the test file, original rul file together and find all prediced rul value\n- now, get the last three rul value of prediction and average it\n- now, compare that avg value with orgginal rul.\n- In second, aproach can apply PCA and then do the same or can find model for individual MID (OPTIONAL)","3b90a1a2":"### Model Making","3d41b8da":"From the above vizulization its clear that \n- Section-4 (Oprational Setting-3)\n- Section-5 (Sensor-1)\n- Section-9  (Sensor-5)\n- Section-14 (Sensor-10)\n- Section-20 (Sensor-16)\n- Section-22 (Sensor-18)\n- Section-23 (Sensor-19)\n\nDoes not play a vital role in variation of data and there std is also almost 0 so, these sensor data is useless for us hence, we can drop this coloumn data","a092e497":"### Now, implementing Dession tree regreesor on PCA dataset","b545ea49":"# METHOD-2","a3b85d64":"#### Now, moving ahead in our approach of workflow data cleaing\n","f1dabd81":"Info about data:\n- Section-0 is MachineID\n- Section-1 is time in, Cycles\n- Section-2...4 is Opertional Settings\n- Section-5...21 is sensor's data \n\n\n- Data Set: FD001\n- Train trjectories: 100\n- Test trajectories: 100\n- Conditions: ONE (Sea Level)\n- Fault Modes: ONE (HPC Degradation)\n\nData sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine \u2013 i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n\nThe engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective of the competition is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate. Also provided a vector of true Remaining Useful Life (RUL) values for the test data.\n","a1b50a18":"### Removing noise by rolling mean for each machine ID\n","934aa21c":"### Now, Concatenating the two df (Operating Settings & Sensor) and removing the above sensor data","3ae2ad99":"From the above cell we can conclude that the machine can have on an average life upto 200(approx.)cycles & in worst case it can be near about 120(approx.) cycles & in best case it can be about 360(approx.) cycles","3950797c":"### Now, implementing linear regrssion on PCA dataset","8cbb118c":"# METHOD-1\n","112e7755":"#### Now, converting the test data like train data for prediction "}}