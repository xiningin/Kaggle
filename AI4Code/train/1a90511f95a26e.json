{"cell_type":{"1b39f0c5":"code","65dc27af":"code","d130e82f":"code","fa68b403":"code","a8684bca":"code","8f3bb00f":"code","01e65ac6":"code","04907268":"code","4fa34c66":"code","c2ed8812":"code","cf31a1e1":"code","1626b105":"code","d6af93a0":"code","9af8022d":"code","d1d03c44":"code","13bba71c":"code","c79ec1b7":"code","e2e36f94":"code","8906aa8d":"code","c10943c1":"code","908b7e7e":"code","39931988":"code","79d4faa0":"code","7177448b":"code","c5d79311":"code","11983ed5":"code","1c218513":"code","e7576acd":"code","235fedd0":"code","caea651a":"code","1957920a":"code","72bc8db4":"code","b570d2bd":"code","7fde4422":"code","60485db6":"code","5b2eea8d":"code","37a2971c":"code","cb8d89ea":"code","22b54fcc":"code","34d4b13a":"code","6ec14550":"code","4c80abca":"code","be8d5d08":"code","23d94820":"code","d53ff6c5":"code","3f6c6534":"code","b02502f1":"code","88685cb3":"code","93535a8e":"code","8b1a26cd":"code","0e4c2580":"code","d54e31f7":"code","2788ec05":"code","1b7f0cef":"code","a607024e":"code","f5775d05":"code","d57f87df":"code","5dc8e6ae":"code","2ed0ef1a":"code","c502aee3":"code","dff11d6a":"code","825655b6":"code","192f1d51":"code","be527214":"code","c8227e05":"code","fb62fee8":"code","0df31f5d":"code","6a1543a7":"code","9ce3e1a4":"code","8e237cbb":"code","c4e80f2f":"code","aaa41ac2":"code","a95b5523":"code","ab0fd203":"code","f8bcf435":"code","1b3bccd7":"code","87dc3464":"code","ac11a8b1":"code","e572e792":"code","53ab12ad":"code","e13448a9":"code","4636104d":"code","cc53ca5e":"code","a10ab03d":"code","dc2e303a":"code","c4402568":"code","855ab1eb":"code","b9b03ffc":"code","80a09e78":"code","4b3e65e0":"code","9f30b2da":"code","a0dda4a1":"code","1889921d":"code","ceffdc00":"code","e9e48cc2":"code","31303b58":"code","8053ce92":"code","d78657de":"code","64f02760":"code","de8121ff":"code","ed11686f":"code","3c7eb8e2":"code","ad65fe2c":"code","b1b1a9fb":"code","09fdecc2":"code","0f9fa4e9":"code","7ccd7056":"code","f1edfde3":"code","976f5ecd":"code","96988b59":"code","c0c79cdb":"code","e9899f46":"code","bb894826":"code","a17467f3":"code","d18cca14":"code","dd6cf143":"code","9d4528f8":"code","3f63a2c0":"code","04fb76e5":"code","01adf79c":"code","4043c3ab":"code","24206010":"code","bb8c83a2":"code","52ef6a68":"code","27bffb8b":"code","4721a3f8":"code","bbb4a754":"code","dd7d2d94":"code","aa34b02e":"code","3a7da34a":"code","afd8cafb":"code","056c4083":"code","eb90aa93":"code","d6a73766":"code","d0e6b0fb":"code","bfac9cd4":"code","c740e4ec":"code","5070a61a":"code","0fb2eb78":"code","ee208430":"code","ab72a36d":"code","f6c06f11":"code","9cc6147a":"code","6c45f03c":"code","5cc691e4":"code","7a19b53d":"code","c5eb516e":"code","3938682c":"markdown","d34b1646":"markdown","ebafe17d":"markdown","2a1ab0f5":"markdown","2c68d052":"markdown","fb0833f3":"markdown","b155c215":"markdown","8bedb8e6":"markdown","abc1c735":"markdown","93e759b9":"markdown","c303ff57":"markdown","4aac2743":"markdown","c382d3d7":"markdown","9dbb840c":"markdown","3e39bf71":"markdown","e4088d35":"markdown","72a91fe9":"markdown","575f1da7":"markdown","95b3ce74":"markdown","8ec9bbc8":"markdown","226588db":"markdown","f394b1bb":"markdown","2a7f7851":"markdown","28a42062":"markdown","4d209a4a":"markdown","7b966df8":"markdown","1d21352d":"markdown","70284482":"markdown","ed28264f":"markdown","aa1dd664":"markdown","50e8a724":"markdown","3f5420a0":"markdown","dd4e24e5":"markdown","1253acde":"markdown","e86056ad":"markdown","25ce1bee":"markdown","16022c88":"markdown","e145a1d1":"markdown","2bdf9ab2":"markdown","d0c2061b":"markdown","6bd97661":"markdown","ef1e9d54":"markdown","0cfe5dc9":"markdown","17bc2c9e":"markdown","878d89fa":"markdown"},"source":{"1b39f0c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nplt.style.use(\"seaborn-whitegrid\")       \nimport pandas_profiling as pp \n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65dc27af":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","d130e82f":"train_df.head()","fa68b403":"train_df.shape","a8684bca":"train_df.describe().T","8f3bb00f":"train_df.info()","01e65ac6":"\nprofile_report = pp.ProfileReport(train_df)","04907268":"profile_report","4fa34c66":"train_df.isnull().sum()","c2ed8812":"#Cabin \n\ntrain_df.drop(\"Cabin\", axis = 1, inplace = True)\n","cf31a1e1":"#Embarked\n\ntrain_df[train_df[\"Embarked\"].isnull()]","1626b105":"train_df.boxplot(column=\"Fare\",by = \"Embarked\")\nplt.show()\n","d6af93a0":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\n","9af8022d":"#Age\n\nname = train_df[\"Name\"]\ntrain_df[\"Name_Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","d1d03c44":"train_df[\"Name_Title\"].value_counts()","13bba71c":"train_df['Name_Title'].replace( ['Mlle','Mme','Ms','Dr','Major','Lady','the Countess','Jonkheer',\n'Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other',\n'Other','Mr','Mr','Mr'],inplace=True)","c79ec1b7":"sns.countplot(x=\"Name_Title\", data = train_df);\nplt.xticks(rotation = 90);\n","e2e36f94":"train_df.groupby('Name_Title')['Age'].mean()","8906aa8d":"train_df.loc[(train_df[\"Age\"].isnull())&(train_df[\"Name_Title\"]=='Mr'),'Age']=33\ntrain_df.loc[(train_df[\"Age\"].isnull())&(train_df[\"Name_Title\"]=='Mrs'),'Age']=36\ntrain_df.loc[(train_df[\"Age\"].isnull())&(train_df[\"Name_Title\"]=='Master'),'Age']=5\ntrain_df.loc[(train_df[\"Age\"].isnull())&(train_df[\"Name_Title\"]=='Miss'),'Age']=22\ntrain_df.loc[(train_df[\"Age\"].isnull())&(train_df[\"Name_Title\"]=='Other'),'Age']=46","c10943c1":"train_df.isnull().sum()","908b7e7e":"test_df.isnull().sum()","39931988":"#Cabin\n\ntest_df.drop(\"Cabin\", axis = 1, inplace = True)","79d4faa0":"#Fare\n\ntest_df[test_df[\"Fare\"].isnull()]","7177448b":"test_df[[\"Embarked\",\"Fare\"]].groupby([\"Embarked\"],as_index = False).mean() ","c5d79311":"test_df[\"Fare\"].fillna(66, inplace = True)","11983ed5":"#Age\n\nname = test_df[\"Name\"]\ntest_df[\"Name_Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]","1c218513":"test_df['Name_Title'].replace( ['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer',\n'Col','Rev','Capt','Sir','Don','Dona'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other',\n'Other','Mr','Mr','Mr','Other'],inplace=True)","e7576acd":"test_df.groupby('Name_Title')['Age'].mean()","235fedd0":"test_df.loc[(test_df[\"Age\"].isnull())&(test_df[\"Name_Title\"]=='Mr'),'Age']=32\ntest_df.loc[(test_df[\"Age\"].isnull())&(test_df[\"Name_Title\"]=='Mrs'),'Age']=38\ntest_df.loc[(test_df[\"Age\"].isnull())&(test_df[\"Name_Title\"]=='Master'),'Age']=7\ntest_df.loc[(test_df[\"Age\"].isnull())&(test_df[\"Name_Title\"]=='Miss'),'Age']=21\ntest_df.loc[(test_df[\"Age\"].isnull())&(test_df[\"Name_Title\"]=='Other'),'Age']=42","caea651a":"test_df.isnull().sum()","1957920a":"#Pclass - Survived\nsns.barplot(train_df[\"Pclass\"], train_df[\"Survived\"]);","72bc8db4":"train_df[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"],\nas_index = False).mean().sort_values(by=\"Survived\",ascending = False)","b570d2bd":"#Sex - Survived\nsns.barplot(train_df[\"Sex\"], train_df[\"Survived\"]);","7fde4422":"train_df[[\"Sex\",\"Survived\"]].groupby([\"Sex\"],\nas_index = False).mean().sort_values(by=\"Survived\",ascending = False)","60485db6":"#SibSp - Survived\nsns.barplot(train_df[\"SibSp\"], train_df[\"Survived\"]);\n","5b2eea8d":"train_df[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],\nas_index = False).mean().sort_values(by=\"Survived\",ascending = False)","37a2971c":"#Parch - Survived\nsns.barplot(train_df[\"Parch\"], train_df[\"Survived\"]);\n","cb8d89ea":"train_df[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],\nas_index = False).mean().sort_values(by=\"Survived\",ascending = False)","22b54fcc":"#Age - Survived\n\ng = sns.FacetGrid(train_df, col = \"Survived\")\ng.map(sns.distplot, \"Age\", bins = 25)\nplt.show()","34d4b13a":"#Pclass - Survived - Age\n\ng = sns.FacetGrid(train_df, col = \"Survived\", row = \"Pclass\", size = 2)\ng.map(plt.hist, \"Age\", bins = 25)\ng.add_legend()\nplt.show()","6ec14550":"#Embarked - Sex - Pclass - Survived\n\ng = sns.FacetGrid(train_df, row = \"Embarked\", size = 2)\ng.map(sns.pointplot, \"Pclass\",\"Survived\",\"Sex\")\ng.add_legend()\nplt.show()","4c80abca":"#Embarked - Sex - Fare - Survived\n\ng = sns.FacetGrid(train_df, row = \"Embarked\", col = \"Survived\", size = 2.3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","be8d5d08":"#HeatMap\nsns.heatmap(train_df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Fare\",\"Pclass\",\"Survived\"]].corr(), annot = True)\nplt.show()","23d94820":"train_df.head(2)","d53ff6c5":"test_df.head(2)","3f6c6534":"train_test = [train_df, test_df]","b02502f1":"train_df['Age_Band'] = pd.cut(train_df['Age'], 5)\ntrain_df[['Age_Band', 'Survived']].groupby(['Age_Band'], \n        as_index=False).mean().sort_values(by='Age_Band', ascending=True)","88685cb3":"for dataset in train_test:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_df.head()","93535a8e":"for dataset in train_test:\n    dataset['Sex'].replace(['male','female'],[0,1],inplace=True)\n    dataset['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n    dataset['Name_Title'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\n\ntrain_df.head()","8b1a26cd":"\nfor dataset in train_test:\n    dataset['FamilySize'] = dataset['SibSp'] +  dataset['Parch'] + 1\n\nprint (train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","0e4c2580":"for dataset in train_test:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \nprint (train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","d54e31f7":"train_df['Fare_Band'] = pd.qcut(train_df['Fare'], 4)\nprint (train_df[['Fare_Band', 'Survived']].groupby(['Fare_Band'], as_index=False).mean())\ntrain_df.head()","2788ec05":"for dataset in train_test:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain_df.head(2)","1b7f0cef":"test_df.head(2)","a607024e":"train_df.drop([\"Name\", \"SibSp\", \"Parch\", \"Ticket\", \"PassengerId\", \"Age_Band\", \"Fare_Band\"], axis = 1, inplace = True)\ntest_df.drop([\"Name\",\"SibSp\", \"Parch\",\"Ticket\",\"PassengerId\"], axis = 1, inplace = True)","f5775d05":"train_df.head(2)","d57f87df":"test_df.head(2)","5dc8e6ae":"sns.heatmap(train_df.corr(),annot=True,linewidths=0.2)\n\nplt.show()","2ed0ef1a":"train = train_df\ntrain.to_csv(\"titanic_train.csv\", index = False)\n\ntest = test_df\ntest.to_csv(\"titanic_test.csv\", index = False)","c502aee3":"train_df.head()","dff11d6a":"train_df1 = train_df.copy()\ntest_df1 = test_df.copy()\n\n","825655b6":"train_test1 = [train_df1,test_df1]","192f1d51":"for dataset in train_test1:\n    dataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\n    dataset[\"Sex\"] = dataset[\"Sex\"].astype(\"category\")\n    dataset[\"Fare\"] = dataset[\"Fare\"].astype(\"category\")\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].astype(\"category\")\n    dataset[\"Name_Title\"] = dataset[\"Name_Title\"].astype(\"category\")\n    dataset[\"Age\"] = dataset[\"Age\"].astype(\"category\")\n    dataset[\"IsAlone\"] = dataset[\"IsAlone\"].astype(\"category\")\n    dataset[\"FamilySize\"] = dataset[\"FamilySize\"].astype(\"category\")","be527214":"#train_df1=pd.get_dummies(train_df1,drop_first=True)\n\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Sex\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Pclass\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Fare\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Name_Title\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Age\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"FamilySize\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"IsAlone\"])\ntrain_df1 = pd.get_dummies(train_df1, columns=[\"Embarked\"])","c8227e05":"#test_df1=pd.get_dummies(test_df1,drop_first=True)\n\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Sex\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Pclass\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Fare\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Name_Title\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Age\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"FamilySize\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"IsAlone\"])\ntest_df1 = pd.get_dummies(test_df1, columns=[\"Embarked\"])","fb62fee8":"train_df1.head()","0df31f5d":"test_df1.head()","6a1543a7":"X_train = train_df.drop([\"Survived\"], axis = 1)\ny_train = train_df[\"Survived\"]\nX_test = test_df\n\nX_train1 = train_df1.drop([\"Survived\"], axis = 1)\ny_train1 = train_df1[\"Survived\"]\nX_test1 = test_df1","9ce3e1a4":"X_train.head()","8e237cbb":"X_train1.head()","c4e80f2f":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score,roc_curve","aaa41ac2":"from sklearn.linear_model import LogisticRegression\nlog = LogisticRegression(solver = \"liblinear\")\nlog_model = log.fit(X_train,y_train)\nlog_model","a95b5523":"confusion_matrix(y_train, log_model.predict(X_train))\nprint(classification_report(y_train, log_model.predict(X_train)))","ab0fd203":"accuracy_score(y_train, log_model.predict(X_train))\ncross_val_score(log_model, X_train, y_train, cv = 10).mean()","f8bcf435":"logit_roc_auc = roc_auc_score(y_train, log_model.predict(X_train))\n\nfpr, tpr, thresholds = roc_curve(y_train, log_model.predict_proba(X_train)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive ')\nplt.ylabel('True Positive ')\nplt.title('ROC')\nplt.show()","1b3bccd7":"from sklearn.naive_bayes import GaussianNB\n\n\nnb = GaussianNB()\nnb_model = nb.fit(X_train, y_train)\nnb_model","87dc3464":"accuracy_score(y_train, nb_model.predict(X_train))","ac11a8b1":"cross_val_score(nb_model, X_train, nb_model.predict(X_train), cv = 10).mean()","e572e792":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn_model = knn.fit(X_train, y_train)\nknn_model","53ab12ad":"\naccuracy_score(y_train, knn_model.predict(X_train))","e13448a9":"knn_params = {\"n_neighbors\": np.arange(1,20)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, knn_params, cv=10)\nknn_cv.fit(X_train, y_train)","4636104d":"print(\"Best KNN score:\" + str(knn_cv.best_score_))\nprint(\"Best KNN parameter: \" + str(knn_cv.best_params_))","cc53ca5e":"knn = KNeighborsClassifier(10)\nknn_tuned = knn.fit(X_train, y_train)","a10ab03d":"\naccuracy_score(y_train, knn_tuned.predict(X_train))","dc2e303a":"d = {'Accuracy in KNN before GridSearchCV ': [0.84], 'Accuracy in KNN After GridSearchCV': [0.84]}\nknn_data = pd.DataFrame(data=d)\nknn_data","c4402568":"from sklearn.svm import SVC\n\n\nsvm_model = SVC(kernel = \"rbf\").fit(X_train, y_train)\n\naccuracy_score(y_train, svm_model.predict(X_train))","855ab1eb":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\n\nsvc = SVC()\nsvc_cv_model = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model.fit(X_train, y_train)","b9b03ffc":"print(\"Best Params: \" + str(svc_cv_model.best_params_))","80a09e78":"svc_tuned = SVC(C = 10, gamma = 0.1).fit(X_train, y_train)\n\naccuracy_score(y_train, svc_tuned.predict(X_train))","4b3e65e0":"d = {'Accuracy in SVM before GridSearchCV ': [0.83], 'Accuracy in SVM After GridSearchCV': [0.85]}\nsvm_data = pd.DataFrame(data=d)\nsvm_data","9f30b2da":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier().fit(X_train, y_train)\n\naccuracy_score(y_train, rf_model.predict(X_train))","a0dda4a1":"rf_params = {\"max_depth\": [2,5,8],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\n\nrf_model = RandomForestClassifier()\n\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) \n\nrf_cv_model.fit(X_train, y_train)","1889921d":"print(\"Best Params: \" + str(rf_cv_model.best_params_))","ceffdc00":"rf_tuned = RandomForestClassifier(max_depth = 5, \n                                  max_features = 2, \n                                  min_samples_split = 2,\n                                  n_estimators = 1000)","e9e48cc2":"rf_tuned.fit(X_train, y_train)\n\naccuracy_score(y_train, rf_tuned.predict(X_train))","31303b58":"confusion_matrix(y_train, rf_tuned.predict(X_train))\nprint(classification_report(y_train, rf_tuned.predict(X_train)))","8053ce92":"Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");","d78657de":"d = {'Accuracy in RF before GridSearchCV ': [0.88], 'Accuracy in RF After GridSearchCV': [0.83]}\nrf_data = pd.DataFrame(data=d)\nrf_data","64f02760":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm_model = GradientBoostingClassifier().fit(X_train, y_train)\n\naccuracy_score(y_train, gbm_model.predict(X_train))","de8121ff":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv.fit(X_train, y_train)","ed11686f":"print(\"Best Params: \" + str(gbm_cv.best_params_))","3c7eb8e2":"gbm = GradientBoostingClassifier(learning_rate = 0.01, \n                                 max_depth = 3,\n                                min_samples_split = 10,\n                                n_estimators = 500)\n\ngbm_tuned =  gbm.fit(X_train,y_train)","ad65fe2c":"\naccuracy_score(y_train, gbm_tuned.predict(X_train))","b1b1a9fb":"confusion_matrix(y_train, gbm_tuned.predict(X_train))\nprint(classification_report(y_train, gbm_tuned.predict(X_train)))","09fdecc2":"Importance = pd.DataFrame({\"Importance\": gbm_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");","0f9fa4e9":"d = {'Accuracy in GBM before GridSearchCV ': [0.8473], 'Accuracy in GBM After GridSearchCV': [0.8451]}\ngbm_data = pd.DataFrame(data=d)\ngbm_data","7ccd7056":"from catboost import CatBoostClassifier\ncat_model = CatBoostClassifier().fit(X_train, y_train)\n\n\naccuracy_score(y_train, cat_model.predict(X_train))","f1edfde3":"catb_params = {\n    'iterations': [200,500],\n    'learning_rate': [0.01,0.05, 0.1],\n    'depth': [3,5,8] }\n\ncatb = CatBoostClassifier()\ncatb_cv_model = GridSearchCV(catb, catb_params, cv=5, n_jobs = -1, verbose = 2)\ncatb_cv_model.fit(X_train, y_train)","976f5ecd":"catb_cv_model.best_params_\n","96988b59":"catb = CatBoostClassifier(iterations = 200, \n                          learning_rate = 0.01, \n                          depth = 5)\n\ncatb_tuned = catb.fit(X_train, y_train)\n\naccuracy_score(y_train, catb_tuned.predict(X_train))","c0c79cdb":"models = [\n    knn_tuned,\n    log_model,\n    svc_tuned,\n    nb_model,\n    rf_tuned,\n    gbm_tuned,\n    catb_tuned,\n    \n]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_train)\n    accuracy = accuracy_score(y_train, y_pred)\n    print(\"-\"*28)\n    print(name + \":\" )\n    print(\"Accuracy: {:.4%}\".format(accuracy))","e9899f46":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Accuracy\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_train)\n    accuracy = accuracy_score(y_train, y_pred)    \n    result = pd.DataFrame([[name, accuracy*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'Accuracy', y = 'Models', data=results, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('accuracy rate of models'); ","bb894826":"log = LogisticRegression(solver = \"liblinear\")\nlog_model_onehot = log.fit(X_train1,y_train1)\nlog_model_onehot","a17467f3":"\nprint(classification_report(y_train1, log_model_onehot.predict(X_train1)))","d18cca14":"accuracy_score(y_train, log_model_onehot.predict(X_train1))\ncross_val_score(log_model, X_train1, y_train1, cv = 10).mean()","dd6cf143":"nb = GaussianNB()\nnb_model_onehot = nb.fit(X_train1, y_train1)\nnb_model_onehot","9d4528f8":"accuracy_score(y_train1, nb_model_onehot.predict(X_train1))","3f63a2c0":"cross_val_score(nb_model_onehot, X_train1, nb_model_onehot.predict(X_train1), cv = 10).mean()","04fb76e5":"knn = KNeighborsClassifier()\nknn_model_onehot = knn.fit(X_train1, y_train1)\nknn_model_onehot","01adf79c":"accuracy_score(y_train1, knn_model_onehot.predict(X_train1))","4043c3ab":"knn_params = {\"n_neighbors\": np.arange(1,20)}\nknn = KNeighborsClassifier()\nknn_cv_onehot = GridSearchCV(knn, knn_params, cv=10)\nknn_cv_onehot.fit(X_train1, y_train1)","24206010":"print(\"Best KNN parameter: \" + str(knn_cv_onehot.best_params_))","bb8c83a2":"knn = KNeighborsClassifier(10)\nknn_tuned_onehot = knn.fit(X_train1, y_train1)\naccuracy_score(y_train1, knn_tuned_onehot.predict(X_train1))","52ef6a68":"d = {'Accuracy in KNN before GridSearchCV ': [0.81], 'Accuracy in KNN After GridSearchCV': [0.82]}\nknn_data = pd.DataFrame(data=d)\nknn_data","27bffb8b":"from sklearn.svm import SVC\n\n\nsvm_model_onehot = SVC(kernel = \"rbf\").fit(X_train1, y_train1)\n\naccuracy_score(y_train1, svm_model_onehot.predict(X_train1))","4721a3f8":"svc_params = {\"C\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\n\nsvc = SVC()\nsvc_cv_model_onehot = GridSearchCV(svc, svc_params, \n                         cv = 10, \n                         n_jobs = -1,\n                         verbose = 2)\n\nsvc_cv_model_onehot.fit(X_train1, y_train1)","bbb4a754":"print(\"Best Params: \" + str(svc_cv_model_onehot.best_params_))","dd7d2d94":"svc_tuned_onehot = SVC(C = 1, gamma = 0.1).fit(X_train1, y_train1)\n\naccuracy_score(y_train1, svc_tuned_onehot.predict(X_train1))","aa34b02e":"d = {'Accuracy in SVM before GridSearchCV ': [0.84], 'Accuracy in SVM After GridSearchCV': [0.83]}\nsvm_data = pd.DataFrame(data=d)\nsvm_data","3a7da34a":"rf_model_onehot = RandomForestClassifier().fit(X_train1, y_train1)\n\naccuracy_score(y_train1, rf_model_onehot.predict(X_train1))","afd8cafb":"rf_params = {\"max_depth\": [2,5,8],\n            \"max_features\": [2,5,8],\n            \"n_estimators\": [10,500,1000],\n            \"min_samples_split\": [2,5,10]}\n\nrf_model = RandomForestClassifier()\n\nrf_cv_model_onehot = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = 10, \n                           n_jobs = -1, \n                           verbose = 2) \n\nrf_cv_model_onehot.fit(X_train1, y_train1)","056c4083":"print(\"Best Params: \" + str(rf_cv_model_onehot.best_params_))","eb90aa93":"rf_tuned_onehot = RandomForestClassifier(max_depth = 5, \n                                  max_features = 5, \n                                  min_samples_split = 10,\n                                  n_estimators = 500)","d6a73766":"rf_tuned_onehot.fit(X_train1, y_train1)\n\naccuracy_score(y_train1, rf_tuned_onehot.predict(X_train1))","d0e6b0fb":"confusion_matrix(y_train1, rf_tuned_onehot.predict(X_train1))\nprint(classification_report(y_train1, rf_tuned_onehot.predict(X_train1)))","bfac9cd4":"d = {'Accuracy in RF before GridSearchCV ': [0.88], 'Accuracy in RF After GridSearchCV': [0.83]}\nrf_data = pd.DataFrame(data=d)\nrf_data","c740e4ec":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm_model_onehot = GradientBoostingClassifier().fit(X_train1, y_train1)\n\naccuracy_score(y_train, gbm_model_onehot.predict(X_train1))","5070a61a":"gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n             \"n_estimators\": [100,500,100],\n             \"max_depth\": [3,5,10],\n             \"min_samples_split\": [2,5,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv_onehot = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv_onehot.fit(X_train1, y_train1)","0fb2eb78":"print(\"Best Params: \" + str(gbm_cv_onehot.best_params_))","ee208430":"gbm = GradientBoostingClassifier(learning_rate = 0.01, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 100)\n\ngbm_tuned_onehot =  gbm.fit(X_train1,y_train1)","ab72a36d":"accuracy_score(y_train1, gbm_tuned_onehot.predict(X_train1))","f6c06f11":"d = {'Accuracy in GBM before GridSearchCV ': [0.84], 'Accuracy in GBM After GridSearchCV': [0.82]}\ngbm_data = pd.DataFrame(data=d)\ngbm_data","9cc6147a":"models = [\n    \n    knn_tuned_onehot,\n    log_model_onehot,\n    svc_tuned_onehot,\n    nb_model_onehot,\n    rf_tuned_onehot,\n    gbm_tuned_onehot,\n    \n]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_train1)\n    accuracy = accuracy_score(y_train1, y_pred)\n    print(\"-\"*28)\n    print(name + \":\" )\n    print(\"Accuracy: {:.4%}\".format(accuracy))","6c45f03c":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Accuracy\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_train1)\n    accuracy = accuracy_score(y_train1, y_pred)    \n    result = pd.DataFrame([[name, accuracy*100]], columns= [\"Models\",\"Accuracy\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'Accuracy', y = 'Models', data=results, color=\"r\")\nplt.xlabel('Accuracy %')\nplt.title('accuracy rate of models'); ","5cc691e4":"df = [(\"KNN\",84),\n      (\"KNN(onehot)\",82),\n      (\"Logreg\", 80),\n      (\"logreg(onehot)\",82),\n      (\"SVC\", 85),\n      (\"SVC(onehot)\",83),\n      (\"NB\", 80),\n      (\"NB(onehot)\",44),\n      (\"RF\",83),\n      (\"RF(onehot)\",83),\n      (\"GBM\", 85),\n      (\"GBM(onehot)\",82)]\nmodel = pd.DataFrame(df, columns=['Model' , 'Accuracy %'])\nmodel","7a19b53d":"sns.barplot(x= 'Accuracy %', y = 'Model', data=model, color=\"r\");","c5eb516e":"test_survived = pd.Series(gbm_tuned.predict(X_test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)","3938682c":" As you can see above(Pandas Profile);\n   *  histogram distributions of variables\n   *  value_count() all variables ","d34b1646":"* Missing Embarked value is 1 st Pclass and Fare is 80.So, I looked Fare-Embarked relationships and I filled \"C\".","ebafe17d":"* if sibsp == 0 or 1 or 2, passenger has more chance to survive.","2a1ab0f5":"## 5.2.2.Naive Bayes(One-Hot Encoding)","2c68d052":"## 4.1.Age Feature","fb0833f3":"## 4.2.Sex - Embarked - Name_Title Feature","b155c215":"* As you can see above rich people are always much more lucky.","8bedb8e6":"After preparing the data for modeling, I wondered how the results would change if I applied one-hot encoding. So I decided to try both. My first data is without one-hot encoding other is one-hot encoding. I applied one-hot encoding to the data I edited with feature engineering. We will see the effect of this change on the results.","abc1c735":"* Fare is the most important feature for survived.","93e759b9":"## 5.2.6.GBM(One Hot Encoding)","c303ff57":"* small familes have more chance to survive.","4aac2743":"## Variable Description\n\n* PassengerId: unique id number to each passenger\n* Survived: passenger survive(1) or died(0)\n* Pclass: passenger class\n* Name: name\n* Sex: gender of passenger\n* Age: age of passenger\n* SibSp: number of siblings\/spouses\n* Parch: number of parents\/children\n* Ticket: ticket number\n* Fare: amount of money spent on ticket\n* Cabin: cabin category\n* Embarked: port where passenger embarked (C = Cherbourg, Q = Queenstown, S = Southampton)","c382d3d7":"## 4.3.Fare Feature","9dbb840c":"## 5.2.1. Logistic Regression (One-Hot Encoding)","3e39bf71":"## 5.1.5.Random Forests(One Hot Encoding)","e4088d35":"# 5.Modelling","72a91fe9":"## 5.1.3.KNN","575f1da7":"## 5.1.1.Logistic Regression","95b3ce74":"## 5.1.4.SVC","8ec9bbc8":"* Best accuracy is SVC(kernel = rbf). ","226588db":"* age <= 10 and oldest pasengers have a high survival rate. \n* Range of 20-25 years old did not survive.","f394b1bb":"## 5.1.2.Naive Bayes","2a7f7851":"## 4.4.One-Hot Encoding","28a42062":"* I applied the same changes to the test data in order to filled missing value.","4d209a4a":"# 4.Feature Engineering","7b966df8":"* After modelling, You can see that one hot encoding does not have much effect on the results.\n\n* Best accuracy is that the database without using one hot encoding(train_df) with SVC model. but my personal experience is that sometimes svc(rbf) can be overfitting. That's why I used gbm in my prediction.","1d21352d":"* As you can see above, Pclass is important feature for Survived.","70284482":"* The Age feature has 177 null values. We know that max, min, mean, median value of Age. But this data is not enough for filling null values.\n\n* I checked the Name feature and I noticed that all names have a personal titles (Mr, miss etc.). So, I used mean value of personal titles in order to filled missing Age value.","ed28264f":"## 5.1.7 CatBoost","aa1dd664":"## 5.1.Modelling without One-Hot Encoding","50e8a724":"* Since 692 of 891 data is missing, I droped cabin column.","3f5420a0":"* Age Feature\n* Sex - Embarked - Name_Title Feature\n* Family Size \/ IsAlone - (New Feature includes SibSp and Parch) \n* Fare Feature \n","dd4e24e5":"* Best accuracy is SVM(kernel = rbf).","1253acde":"# 2.Missing Value\n\n* train_df Missing Value\n* test_df Missing Value","e86056ad":"* As you can see above importance of Sex is decrase. ","25ce1bee":"## 5.2.3.KNN (One-Hot Encoding)","16022c88":"## 2.1.Train Missing Value","e145a1d1":"## 5.2.4.SVC","2bdf9ab2":"## 5.1.6.GBM","d0c2061b":"# 1.Load and Check Data","6bd97661":"# 3.Visualization\n* Pclass - Survived\n* Sex - Survived\n* SibSp - Survived\n* Parch - Survived\n* Age - Survived\n* Pclass - Survived - Age\n* Embarked - Sex - Pclass - Survived\n* Embarked - Sex - Fare - Survived","ef1e9d54":"* After feature engineering, there is an increase in correlation.","0cfe5dc9":"## 5.2.Modelling with One-Hot Encoding ","17bc2c9e":"## 5.1.5.Random Forests","878d89fa":"## 2.2.Test Missing Value"}}