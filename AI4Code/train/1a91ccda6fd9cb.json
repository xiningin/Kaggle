{"cell_type":{"ea887038":"code","8b0ebd6a":"code","9bf7ab7a":"code","945a5a9c":"code","ad72fdc0":"code","344e5849":"code","12674c13":"code","f21e90aa":"code","9a82e81e":"code","9915b107":"code","f753848f":"code","a804916d":"code","e5daedb9":"code","03325160":"code","6f669165":"code","1815bee9":"code","c8a30903":"code","7d79cfbf":"code","0ea89b35":"code","87c103b7":"code","8eaede81":"code","7b9256ce":"code","1028539b":"code","f8d800ce":"code","2467c85e":"code","46f79865":"code","2007530b":"code","46e79b5b":"code","e4ff2f88":"code","13f17c83":"code","2008aeb3":"markdown","ef3948cd":"markdown","bfe172b9":"markdown","47644230":"markdown","ae5f7cbb":"markdown","1fdc6fcb":"markdown","679163ee":"markdown","c37aca92":"markdown","08c0b5c7":"markdown","cf51f217":"markdown","8117940b":"markdown","d0a39b0a":"markdown","f539921c":"markdown","14767b9f":"markdown","44bf9de8":"markdown","0201abd2":"markdown","12e34ba0":"markdown","179d65c8":"markdown","d5d9dfef":"markdown","d357d24e":"markdown","5f2da530":"markdown","d52d28fd":"markdown","130e674e":"markdown","e4876079":"markdown","aeee77a1":"markdown","51b30525":"markdown"},"source":{"ea887038":"from tensorflow import keras\nfrom tensorflow.train import Example, Feature, Features\nfrom tensorflow.train import BytesList, FloatList, Int64List\nfrom contextlib import ExitStack\nfrom itertools import chain,cycle\n\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport math","8b0ebd6a":"df_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_test.csv\")\nlen(df_train), len(df_test)","9bf7ab7a":"X_train, y_train = df_train.iloc[7000:,1:], df_train.iloc[7000:,:1]\nX_valid, y_valid = df_train.iloc[:7000,1:], df_train.iloc[:7000,:1]\nX_test, y_test = df_test.iloc[:, 1:], df_test.iloc[:, :1]","945a5a9c":"len(X_train), len(X_valid), len(X_test)","ad72fdc0":"X_train.dtypes, y_train.dtypes","344e5849":"X_train = X_train.to_numpy(dtype='uint8')\ny_train = y_train.to_numpy(dtype='int64')\n\nX_valid = X_valid.to_numpy(dtype='uint8')\ny_valid = y_valid.to_numpy(dtype='int64')\n\nX_test = X_test.to_numpy(dtype='uint8')\ny_test = y_test.to_numpy(dtype='uint64')","12674c13":"X_train = X_train.reshape(len(X_train), 28,28)\nX_valid = X_valid.reshape(len(X_valid), 28,28)\nX_test = X_test.reshape(len(X_test), 28,28)\n\nX_train.shape, X_valid.shape, X_test.shape","f21e90aa":"class_names = [\"T-shirt\/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]","9a82e81e":"def plot_images(X_train, y_train, n_rows = 4, n_cols = 10):\n    plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n    for row in range(n_rows):\n        for col in range(n_cols):\n            index = n_cols * row + col\n            plt.subplot(n_rows, n_cols, index + 1)\n            plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n            plt.axis('off')\n            plt.title(class_names[y_train[index][0]], fontsize=12)\n    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n    plt.show()\n\n    # This function is taken from:\n    #   https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/10_neural_nets_with_keras.ipynb\n    \nplot_images(X_train, y_train)","9915b107":"plot_nulls(y_valid, y_train, y_test, height=1.8, titles=['valid','train','test'])","f753848f":"train_set = tf.data.Dataset.from_tensor_slices((X_train,y_train)).shuffle(len(X_train))\nvalid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\ntest_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))","a804916d":"def create_example(image, label):\n    image_data = tf.io.serialize_tensor(image)\n    return Example(\n        features=Features(\n            feature={\n                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n                \"label\": Feature(int64_list=Int64List(value=[label])),\n            }\n        )\n    )","e5daedb9":"for image, label in valid_set.take(1):\n    print(create_example(image,label))","03325160":"from contextlib import ExitStack\n\ndef write_records(name: str, dataset, n_shards=10, directory=\"\"):\n    # Create the paths.\n    paths= [\"{}.tfrecord-{:05d}-of-{:05d}\"\n            .format(name, index, n_shards) for index in range(n_shards)]\n    ## Hold the filenames and save them into csv files...\n    path_dataframe = pd.DataFrame(paths)\n    path_dataframe.to_csv(name+\"_filepaths\"+\".csv\")\n    ##\n    if not os.path.exists(directory):\n        ## Check if the specified directory exists. (default=\"\") \n        os.makedirs(directory)\n    with ExitStack() as stack:\n        writers = [stack.enter_context(tf.io.TFRecordWriter(\n                                        os.path.join(directory,path))) for path in paths]\n        for index, (image,label) in dataset.enumerate():\n            shard = index % n_shards\n            example = create_example(image,label)\n            writers[shard].write(example.SerializeToString())\n        return paths","6f669165":"## Write the Example objects that are gained from tensors into the files in\n## specified directories.\nwrite_records(\"fashion_mnist_train\", train_set, directory=\"TRAIN_TFRECORD\")\nwrite_records(\"fashion_mnist_valid\", valid_set, directory=\"VALID_TFRECORD\")\nwrite_records(\"fashion_mnist_test\", test_set, directory=\"TEST_TFRECORD\")","1815bee9":"# Get the filepaths of the .tfrecord files from the .csv lists\n\ntrain_filepaths = pd.read_csv('fashion_mnist_train_filepaths.csv',\n                              header=0, index_col=0).values.tolist()\nvalid_filepaths = pd.read_csv('fashion_mnist_valid_filepaths.csv',\n                              header=0, index_col=0).values.tolist()\ntest_filepaths = pd.read_csv('fashion_mnist_test_filepaths.csv',\n                             header=0, index_col=0).values.tolist()\ntrain_filepaths","c8a30903":"\ndef _parse_image_function(example_proto):\n  \"\"\"\n  This function parses image and label objects from a proto.\n  Args:\n      example_proto (TFRecordDataset): Take the raw tfrecord file.\n  Returns:\n      Tensor: Return the tfrecord objects as tensor.\n  \"\"\"\n    # Create a dictionary describing the features.\n  image_feature_description = {\n      'image': tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n      'label': tf.io.FixedLenFeature([], tf.int64, default_value=-1),}\n    ##\n    # Parse the input tf.train.Example proto using the dictionary above.\n  example = tf.io.parse_single_example(example_proto, image_feature_description)\n  image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n    ##\n    ## NORMALIZATION\n  dividing = tf.constant([255], dtype=tf.uint8)\n  image = tf.divide(image,dividing)\n    ##\n  image = tf.reshape(image, shape=[28,28])\n  return image, example[\"label\"]\n\n\ndef get_data(filepaths, directory=\"\", n_threads=2, \n             shuffle_buffer_size=4096, batch_size=4096):\n    \"\"\"\n    This function gets the data from the filepaths that we have given.\n          Args:\n              filepaths (a list of strings): Take the filepaths of\n                                              tfrecord files.\n              directory (string): The dir that filepaths be in.\n              n_threads (int): Number of processor threads that will\n                            be used.\n              shuffle_buffer_size (int): Size of buffer to shuffle\n                                          the data.\n              batch_size (int): Max amount of rows that will be fetched\n                                from the files at a time.\n\n          Returns:\n              Tensors: Return the tfrecord objects as tensor.\n    \"\"\"\n    ## JOIN THE DIRECTORY NAME (IF SPECIFIED) WITH THE GIVEN FILEPATHS.\n    def join_dir(filepath):\n        return os.path.join(directory, filepath[0])\n    ##\n    filepaths = list(map(join_dir, filepaths))\n    dataset = tf.data.Dataset.list_files(filepaths)\n    ## num_parallel_reads is for using multiple processor threads to accelerate\n    ## the job\n    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=n_threads)\n    dataset = dataset.map(_parse_image_function, num_parallel_calls=n_threads)\n    ## Shuffling might be necessary for the data that are not distributed randomly.\n    dataset = dataset.shuffle(shuffle_buffer_size)\n\n    ## IMPORTANT::\n    ## Take the data batch by batch. So that, you prevent your RAM to be overfilled.\n    dataset = dataset.batch(batch_size)\n    ## The 'prefetch' function prepares the next batch just after you call a batch. \n    ##  In this way, you can reach the next batch faster.\n    return dataset.prefetch(1)","7d79cfbf":"train_set = get_data(train_filepaths, directory=\"TRAIN_TFRECORD\")\nvalid_set = get_data(valid_filepaths, directory=\"VALID_TFRECORD\")\ntest_set = get_data(test_filepaths, directory=\"TEST_TFRECORD\")","0ea89b35":"for element in train_set.take(1).as_numpy_iterator():\n  print('max=',element[0].max(), 'min=',element[0].min())","87c103b7":"keras.backend.clear_session()\ntf.random.set_seed(44)\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Flatten(input_shape=[28,28]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(800, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(600, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(600, kernel_initializer='lecun_normal', activation='selu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(10, activation=\"softmax\")\n])","8eaede81":"model.summary()","7b9256ce":"tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True, dpi=60)","1028539b":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('k8fTYJPd3_I', width=800, height=300)","f8d800ce":"opt_list = {\n    \"sgd\": tf.keras.optimizers.SGD(learning_rate=0.01,\n                            momentum=0.95,\n                            nesterov=True),\n    \"adam\": tf.keras.optimizers.Adam(learning_rate=0.01,\n                            beta_1=0.9,\n                            beta_2=0.999),\n    \"nadam\": tf.keras.optimizers.Nadam(learning_rate=0.01,\n                              beta_1=0.9,\n                              beta_2=0.999,\n                              epsilon=1e-07),\n    \"rmsprop\":tf.keras.optimizers.RMSprop(learning_rate=0.01,\n                                rho=0.9),\n           }","2467c85e":"reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                                              patience=30, min_lr=0.0001)","46f79865":"cp_val_accuracy, cp_val_loss = checkpoint_generator('mnist')\n\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer= opt_list[\"adam\"], # You can view the opt_list above.\n              metrics=[\"accuracy\"],)","2007530b":"plot_losses = PlotLosses()\n\nhistory = model.fit(train_set,\n          epochs=10,\n          validation_data=valid_set,\n          callbacks=[plot_losses,\n                     reduce_lr,\n                     # These callbacks save the model with the best val_accuracy,\n                     cp_val_accuracy, \n                     cp_val_loss,])  # and the model with the best val_loss...","46e79b5b":"history = model.fit(train_set,\n          epochs=100,\n          validation_data=valid_set,\n          callbacks=[plot_losses,\n                     reduce_lr,\n                     # I have added 2 additional callbacks\n                     cp_val_accuracy, # These callbacks hold the model with the best val_accuracy,\n                     cp_val_loss,])     # and the model with the best val_loss...","e4ff2f88":"checkpoint_evaluate('mnist', x=test_set)","13f17c83":"model.evaluate(test_set)","2008aeb3":"# Callbacks\n\n<h3>These are the callbacks that I&#39;ve used in my model:<\/h3>\n\n<ul>\n\t<li><span style=\"font-size:16px\">ReduceLROnPlateau<\/span><\/li>\n\t<li><span style=\"font-size:16px\">Checkpoints (my <strong>original <\/strong>functions to save the models with best val_accuracy and val_loss. and evaluate them)<\/span><\/li>\n\t<li><span style=\"font-size:16px\">Plot Losses (a library that I&#39;ve edited to plot accuracy and loss during training time.)<\/span><\/li>\n<\/ul>\n\n<p><strong><span style=\"font-size:16px\">a. ReduceLROnPlateau<\/span><\/strong><\/p>\n\n<p><span style=\"font-size:16px\">If the <strong>val_loss <\/strong>value of a model does not improve by <strong>30 epochs<\/strong>, multiply the <strong>learning_rate <\/strong>by<strong> 0.5<\/strong>.<\/span><\/p>\n\n<p><span style=\"font-size:16px\"><strong>b. Checkpoints<\/strong><\/span><\/p>\n\n<p><strong>You can see the function in this link:&nbsp;<a href=\"https:\/\/github.com\/shiny-apricot\/My-Machine-Learning-Works\/blob\/372cfa4421638bbe3b4bf9044cea348dafe7defa\/My_Library\/tf_checkpoint.py\">shiny-apricot\/tf_checkpoint.py<\/a><\/strong><\/p>\n\n<p><span style=\"font-size:16px\"><strong>c. Plot Accuracy-Loss During Training Phase<\/strong><\/span><\/p>\n\n<p><strong>You can see the function in this link:&nbsp;<a href=\"https:\/\/github.com\/shiny-apricot\/My-Machine-Learning-Works\/blob\/372cfa4421638bbe3b4bf9044cea348dafe7defa\/My_Library\/tf_model_plot_loss.py\">shiny-apricot\/tf_plot_loss.py<\/a><\/strong><\/p>\n\n<p>&nbsp;<\/p>\n","ef3948cd":"<p><span style=\"font-size:17px\"><strong>As you can see, with the Normalization that we applied in the function &#39;get_data&#39;, our pixel values are stayed between 0 and 1 instead of 0-255.<\/strong><\/span><\/p>\n","bfe172b9":"## Null Check\n\n<p><span style=\"font-size:20px\">To go to the function, visit: <a href=\"https:\/\/github.com\/shiny-apricot\/My-Machine-Learning-Works\/blob\/1fd0f319d3384a9d9155de7b6ce0a5048f6645b2\/My_Library\/plot_null_values.py\">shiny-apricot\/plot_null_values.py<\/a><\/span><\/p>\n\n<p><span style=\"font-size:20px\">It works both with numpy arrays and dataframes.<\/span><\/p>\n","47644230":"<p><span style=\"font-size:20px\">Every single pixel in our data have <strong>int64<\/strong> datatype. <strong>Int64 <\/strong>datatype can hold values between&nbsp;<\/span><strong>&nbsp;-9,223,372,036,854,775,808 to positive&nbsp;9,223,372,036,854,775,807.&nbsp; <\/strong><span style=\"font-size:20px\">But our values are just between <u>0 and 255<\/u>. So, we will convert them to <strong>uint8<\/strong>, which holds numbers between <strong>0-255<\/strong>. This is exactly what we want!&nbsp;&nbsp;<\/span><\/p>\n","ae5f7cbb":"#### Plot Losses\n\n<p><strong>You can see the function in this link:&nbsp;<a href=\"https:\/\/github.com\/shiny-apricot\/My-Machine-Learning-Works\/blob\/372cfa4421638bbe3b4bf9044cea348dafe7defa\/My_Library\/tf_model_plot_loss.py\">shiny-apricot\/tf_plot_loss.py<\/a><\/strong><\/p>\n","1fdc6fcb":"<p><span style=\"font-size:18px\">&gt; To begin, let&#39;s convert the numpy arrays into tensorflow 'Tensor' objects first.<\/span><\/p>\n","679163ee":"# Intro:\n<p><span style=\"font-size:18px\">This notebook contains the structure of a <strong>DNN <\/strong>(Deep Neural Network) Model. It <strong>does not contain a CNN<\/strong> (Convolutional Neural Network).<\/span><\/p>\n\n<p><span style=\"font-size:19px\">The accuracy rate <\/span><span style=\"font-size:22px\"><strong>on the test set<\/strong><\/span><span style=\"font-size:20px\"> reached by the model is <strong>90.4%<\/strong><\/span><\/p>\n\n<p><span style=\"font-size:18px\">In this notebook, some state-of-the-art methods like:<\/span><\/p>\n\n<ul>\n\t<li><strong><span style=\"font-size:18px\">Batch Normalization<\/span><\/strong><\/li>\n\t<li><strong><span style=\"font-size:18px\">Dropout<\/span><\/strong><\/li>\n\t<li><strong><span style=\"font-size:18px\">Callbacks<\/span><\/strong><\/li>\n\t<li><strong><span style=\"font-size:18px\">Different Optimizers<\/span><\/strong><\/li>\n\t<li><strong><span style=\"font-size:18px\">TFRecord Filetype<\/span><\/strong><\/li>\n<\/ul>\n\n<p><span style=\"font-size:19px\">Has been used.<\/span><\/p>\n\n<p>&nbsp;<\/p>\n","c37aca92":"<p><span style=\"font-size:18px\">There is no null values. GREAT!<\/span><\/p>\n","08c0b5c7":"### Reduce LR\n<p><span style=\"font-size:18px\"><strong>If &#39;val_loss&#39; does not improve for &#39;30&#39; epochs, then multiply the current learning rate by &#39;0.5&#39;.<\/strong><\/span><\/p>\n\n","cf51f217":"<p style=\"text-align:right\"><strong><span style=\"font-size:20px\">~yasin inal<\/span><\/strong><\/p>\n\n<p><span style=\"font-size:20px\">My GitHub Page:&nbsp;<a href=\"https:\/\/github.com\/shiny-apricot\">github.com\/shiny-apricot<\/a><\/span><\/p>\n\n<p><span style=\"font-size:20px\">My LinkedIn Profile:&nbsp;<\/span><span style=\"font-size:16px\"><a href=\"https:\/\/www.linkedin.com\/in\/yasin-inal-abb41876\/\">Yasin Inal | LinkedIn<\/a><\/span><\/p>\n","8117940b":"# Model Architecture\n\n<p><span style=\"font-size:16px\">Some concepts that are used in this architecture:<\/span><\/p>\n\n<ul>\n\t<li><span style=\"font-size:16px\">Batch Normalization<\/span><\/li>\n\t<li><span style=\"font-size:16px\">Selu Activation Function and &#39;Lecun Normal&#39; Kernel Initializer<\/span><\/li>\n\t<li><span style=\"font-size:16px\">Dropout<\/span><\/li>\n<\/ul>\n\n## Batch Normalization\n\n<ul>\n\t<li><span style=\"font-size:16px\">In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called <em>Batch Normalization (BN)<\/em>. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer.<\/span><\/li>\n\t<br \/>\n\t<li><span style=\"font-size:16px\">This operation simply <strong>zero centers<\/strong> and <strong>normalizes <\/strong>each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, <em>the operation lets the model learn the optimal scale and mean of each of the layer&rsquo;s inputs<\/em>.<\/span><\/li>\n\t<br \/>\n\t<li><span style=\"font-size:16px\">In many cases, if you add a BN layer as the very first layer of your neural network,<strong> you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you<\/strong>&nbsp;(well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature)<\/span><\/li>\n<\/ul>\n\n<p style=\"text-align:right\">Aurelion Geron&nbsp;~ Hands on Machine Learning with Scikit Keras and Tensorflow (2019)<\/p>\n\n## SELU\n\n<h3><span style=\"font-size:18px\"><span style=\"color:#d35400\"><strong>Selu Activation Function and Kernel Initializers<\/strong><\/span><\/span><\/h3>\n\n<ul>\n\t<li><span style=\"font-size:16px\">Then, a 2017 paper by G&uuml;nter Klambauer et al. introduced the <strong>Scaled ELU (SELU) <\/strong>activation function: as its name suggests, it is a scaled variant of the ELU activation function. <\/span><\/li>\n\t<li><span style=\"font-size:16px\">The authors showed that if you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will<em><strong> self-normalize<\/strong><\/em>: the output of <u><em>each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing\/exploding gradients problem<\/em><\/u>. <\/span><\/li>\n\t<li><span style=\"font-size:16px\">As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones).<\/span><\/li>\n<\/ul>\n\n<p style=\"text-align:right\">Aurelion Geron&nbsp;~ Hands on Machine Learning with Scikit Keras and Tensorflow (2019)<\/p>\n\n## Dropout\n\n<p><span style=\"color:#d35400\"><strong><span style=\"font-size:16px\">What is Dropout?<\/span><\/strong><\/span><\/p>\n\n<p><span style=\"font-size:16px\">According to Wikipedia &mdash; *The term &ldquo;dropout&rdquo; refers to dropping out units (both hidden and visible) in a neural network*. Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. By &ldquo;ignoring&rdquo;, I mean these units are not considered during a particular forward or backward pass.<\/span><\/p>\n\n#### Why do we need Dropout?\n\n<ul>\n\t<li><span style=\"font-size:16px\">The answer to these questions is &ldquo;to prevent over-fitting&rdquo;.<\/span><\/li>\n\t<li><span style=\"font-size:16px\">A fully connected layer occupies most of the parameters, and hence, neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data.<\/span><\/li>\n<\/ul>\n\n<p>For more information=&gt; <a href=\"https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5\">https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5<\/a><\/p>\n\n<p><span style=\"color:#ffffff\"><span style=\"font-size:16px\"><span style=\"background-color:#e74c3c\">There is an improved version of the Dropout layer named &quot;MCDropout&quot;.<\/span><\/span><\/span><\/p>\n\n<p><span style=\"color:#ffffff\"><span style=\"font-size:16px\"><span style=\"background-color:#e74c3c\">However, it reduced the accuracy of my model by about 2%. I&#39;m not sure if I did something wrong or did it really reduce the accuracy.<\/span><\/span><\/span><\/p>\n\n<p>&nbsp;<\/p>\n\n<p><span style=\"color:#d35400\"><strong><span style=\"font-size:16px\">Which activation function should you use for the hidden layers of your deep neural networks?<\/span><\/strong><\/span><\/p>\n\n<p><span style=\"font-size:16px\">Although your mileage will vary, in general <em><strong>SELU &gt; ELU &gt; leaky ReLU (and its variants) &gt; ReLU &gt; tanh &gt; logistic.<\/strong><\/em> <\/span><\/p>\n\n<p><span style=\"font-size:16px\">If the network&rsquo;s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0). If you care a lot about runtime latency, then you may prefer leaky ReLU. If you don&rsquo;t want to tweak yet another hyperparameter, you may use the default &alpha; values used by Keras (e.g., 0.3 for leaky ReLU).<\/span><\/p>\n\n<p style=\"text-align:right\"><span style=\"font-size:16px\">Aurelion Geron&nbsp;~ Hands on Machine Learning with Scikit Keras and Tensorflow (2019)<\/span><\/p>\n\n<p><span style=\"font-size:16px\">However, in this model, I tried many of them and found out that <em>SELU performed better<\/em> than the other activation functions.<\/span><\/p>\n\n<p>&nbsp;<\/p>\n\n<p><span style=\"font-size:18px\"><strong>Activation Function Examples<\/strong><\/span><\/p>\n\n<p><strong><span style=\"color:#c0392b\"><span style=\"font-size:14px\">Give attention to different kernel_initializers.<\/span><\/span><\/strong><\/p>\n\n<ol>\n\t<li>\n\t<pre>\n<code class=\"language-python\">tf.keras.layers.Dense(200, kernel_initializer='he_normal', activation='relu'),\ntf.keras.layers.Dense(200, kernel_initializer='he_normal', activation='elu'),\ntf.keras.layers.Dense(200, kernel_initializer='lecun_normal', activation='selu'),\ntf.keras.layers.LeakyReLU(alpha=0.2),<\/code><\/pre>\n\t<\/li>\n<\/ol>\n\n<p><span style=\"color:#d35400\"><span style=\"font-size:16px\"><strong>Which Kernel Initializer Should Be Used For an Activation Function?<\/strong><\/span><\/span><\/p>\n\n<p><span style=\"font-size:16px\">Here is a table that answers this question.<\/span><\/p>\n\n![Screenshot_2.png](attachment:ed6bf38d-2ad2-4acb-94f6-42522ed2e2be.png)\n\n<p>&nbsp;<\/p>\n\n<p><span style=\"color:#d35400\"><span style=\"font-size:16px\"><strong>Why I Have Used So Much Neurons (800+600+600) ?<\/strong><\/span><\/span><\/p>\n\n<p><span style=\"font-size:16px\">Vincent Vanhoucke, a scientist at Google, has dubbed this the <strong>&ldquo;stretch pants&rdquo; <\/strong>approach: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size. With this approach, you avoid <em><strong>bottleneck <\/strong><\/em>layers that could ruin your model. On the flip side, <em>if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs (e.g., a layer with two neurons can only output 2D data, so if it processes 3D data, some information will be lost). No matter how big and powerful the rest of the network is, that information will never be recovered<\/em>.<\/span><\/p>\n\n<p><span style=\"font-size:16px\">So, in my opinion,<em><strong> if the latency of the final model is not a huge problem for us, using more neurons than we need is usually better for accuracy.<\/strong> <\/em><strong>Plus, it will probably not affect the training time! <\/strong>Because, despite each epoch time will slow down with more neurons, the model will reach the optima with fewer epochs. Eventually, total training time will probably not change significantly.<\/span><\/p>\n\n<p>&nbsp;<\/p>\n","d0a39b0a":"<p><span style=\"font-size:18px\">&gt; &nbsp;The function below will help us to convert the <strong>Tensors <\/strong>into <strong>Examples <\/strong>(some TFRecord objects).<\/span><\/p>\n","f539921c":"## TFRecord Filetype Conversion","14767b9f":"<p><strong><span style=\"font-size:18px\">Seperate the data into validation, training and test set. Also take the x and y (label) values.<\/span><\/strong><\/p>\n\n","44bf9de8":"### Why I am Using TFRecord Filetype in This Notebook?\n\n<p><span style=\"font-size:16px\">In fact, using TFRecord in a small dataset like Fashion MNIST has almost no contribution to the performance. However, this notebook is an <strong>example<\/strong> to show the implementation of TFRecord since it is recommended to use it in large datasets.&nbsp;<\/span><\/p>\n\n<p>&nbsp;<\/p>\n\n### What is TFRecord? \n<p><span style=\"font-size:16px\">Tensorflow Records or TFRecords in short, is the recommended binary file format for high throughput data ingestion, which is optimized for data ingestion for training ML models using Tensorflow.<\/span><\/p>\n\n<p>&nbsp;<\/p>\n\n<p><span style=\"font-size:16px\">If you are working with large datasets, using a <strong>binary file format<\/strong> for storage of your data can have a significant impact on the performance of your import pipeline and as a consequence on the training time of your model. Binary data <strong>takes up less space on disk<\/strong>, <strong>takes less time to copy<\/strong> and <strong>can be read much more efficiently<\/strong>&nbsp;from disk. This is especially true if your data is stored on spinning disks, due to the much lower read\/write performance in comparison with SSDs.<\/span><\/p>\n\n<p><span style=\"font-size:16px\">For more information, you can visit: <a href=\"https:\/\/medium.com\/mostly-ai\/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564\">https:\/\/medium.com\/mostly-ai\/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564<\/a><\/span><\/p>\n\n<p>&nbsp;<\/p>\n","0201abd2":"<p><span style=\"font-size:20px\"><strong>90.41% ACCURACY. THAT IS GREAT FOR A DNN !!<\/strong><\/span><\/p>\n\n<p><span style=\"font-size:16px\">To dispel the doubts, let&#39;s try this with the model in our RAM::<\/span><\/p>\n","12e34ba0":"# Optimizers\n\n<p><span style=\"font-size:16px\">Classical optimizers like gradient descent or stochastic gradient descent (SGD) are very powerful techniques. Nevertheless, they are very old and more powerful techniques are being invented by the time. In the list below, I have created 4 optimizers as <em>&quot;<strong>sgd<\/strong>&quot;, &quot;<strong>adam<\/strong>&quot;, &quot;<strong>nadam<\/strong>&quot; and &quot;<strong>rmsprop<\/strong>&quot;<\/em>. But before them, it is better to learn some concepts like <em>momentum optimization<\/em>, <em>nesterov accelerated gradient.<\/em><\/span><\/p>\n\n<p><span style=\"font-size:16px\"><em>Here is a YouTube video from Andrew NG that explains momentum optimization:<\/em><\/span><\/p>\n<p>&nbsp;<\/p>\n","179d65c8":"<h1><span style=\"font-size:16px\">In my experiment, I have seen that &quot;<strong>adam<\/strong>&quot; performed better than the others. <\/span><\/h1>\n\n<h1><span style=\"font-size:16px\">The<em> &#39;beta_1&#39;&nbsp; <\/em>argument in the optimizer refers to Momentum decay hyperparameter.<\/span><\/h1>\n\n<p><span style=\"font-size:16px\">The<em> &#39;beta_2&#39;<\/em> argument refers to Scaling decay hyperparameter.<\/span><\/p>\n\n<p>&nbsp;<\/p>\n\n<p><span style=\"font-size:14.9925pt\"><span style=\"font-family:Arial-BoldMT\"><span data-darkreader-inline-color=\"\" style=\"--darkreader-inline-color:#ce7d7d; color:#c67171\"><strong>WARNING <\/strong><\/span><\/span><\/span><\/p>\n\n<p><span style=\"font-size:16px\">Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. However, a 2017 paper by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. So when you are disappointed by your model&rsquo;s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it&rsquo;s moving fast.<\/span><\/p>\n\n<p style=\"text-align:right\">Aurelion Geron&nbsp;~ Hands on Machine Learning with Scikit Keras and Tensorflow (2019)<\/p>","d5d9dfef":"> The function below **writes** the Example objects into files by **seperating** the data into different parts.","d357d24e":"## Fetch The Data","5f2da530":"<p><span style=\"font-size:18px\">Since the first epochs start with huge loss numbers, it may disrupt our plot image that we will create simultaneously. So, we will train the first 10 epochs seperately.<\/span><\/p>\n","d52d28fd":"#### Checkpoint Generator\n\n<p><strong>You can see the function in this link:&nbsp;<a href=\"https:\/\/github.com\/shiny-apricot\/My-Machine-Learning-Works\/blob\/372cfa4421638bbe3b4bf9044cea348dafe7defa\/My_Library\/tf_checkpoint.py\">shiny-apricot\/tf_checkpoint.py<\/a><\/strong><\/p>","130e674e":"# TESTING\n\n<p><span style=\"font-size:18px\">Let&#39;s test our saved models with the test set using my own function.<\/span><\/p>\n","e4876079":"<p><span style=\"font-size:18px\">&gt; You can see below the first element of the <strong>valid_set <\/strong>after being converted to an Example.<br \/>\nThe first key is the image that is converted to binary format and the second key contains is its <strong>label.<\/strong><\/span><\/p>\n\n<p><span style=\"color:#ffffff\"><span style=\"font-size:18px\"><strong><span style=\"background-color:#c0392b\">&nbsp;IMPORTANT:::<\/span><\/strong><\/span><\/span><\/p>\n\n<p><span style=\"font-size:18px\">Since we are using uint8 datatype, it is not holding so much space. But if we would first normalize the data and squeeze them between 0 and 1, we may had to use float32 or float64 datatype, which would cause significant increase in the disk usage. So, we will first save the data and normalize it after fetching from the disk.<\/span><\/p>\n","aeee77a1":"> We have written the tfrecord files successfully. Now, it is time to read and turn them into tensorflow Tensor objects. <\/br>\n#### REMEMBER: Using tfrecord files in this notebook is just to show an example about its implementation. Otherwise, it would be sufficient just to take the data from the kaggle datasets.","51b30525":"<p><span style=\"font-size:18px\">You can clearly see the decrease of the thickness of the line at the end of the image. It refers to the decrease of the learning rate which we have specified with our &quot;ReduceLROnPlateou&quot; Callback.&nbsp;<\/span><\/p>\n"}}