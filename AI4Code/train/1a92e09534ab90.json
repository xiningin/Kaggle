{"cell_type":{"057c684d":"code","3effc95c":"code","76d2cf09":"code","f0e74a30":"code","567a1373":"code","27720dc0":"code","0f04fb3c":"code","72b2b408":"code","cd192ede":"code","66db5a69":"code","fe4a186f":"code","7a99e690":"code","0a5cbc4a":"code","1542f505":"code","ff62ee4c":"code","7b237cea":"code","8e0b87a2":"markdown"},"source":{"057c684d":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev # --version nightly \n\n!apt install aptitude -y\n!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n!pip install mecab-python3==0.996.6rc2\n\n!pip install unidic-lite\n!pip install -U \"transformers==2.11.0\"","3effc95c":"!pip freeze | grep transformers","76d2cf09":"import gc\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\n\nimport MeCab\nimport random\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.utils.data\n\nfrom tqdm.autonotebook import tqdm\nimport pickle\n\nfrom transformers import *\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f0e74a30":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef reduce_fn(vals):\n    return sum(vals) \/ len(vals)\n\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","567a1373":"class config:\n    #DATA_DIR = \"\/kaggle\/input\/aio-make-token-ids-selection\"\n    DATA_DIR = \"\/kaggle\/input\/aio-make-tokenid-with\"\n    #DATA_DIR = \"\/kaggle\/input\/aio-bert-tokenids-nnlm-v5\"\n    SEED = 412\n    TRAIN_BATCH_SIZE = 1\n    VALID_BATCH_SIZE = 2\n    OPTIONS = 20\n    EPOCHS = 3\n    LEARNING_RATE = 5e-6\n    WARM_UP_RATIO = 0.0\n    MODEL_TYPE = \"cl-tohoku\/bert-base-japanese-whole-word-masking\"\n    TOKENIZER = BertJapaneseTokenizer.from_pretrained(MODEL_TYPE)","27720dc0":"with open(f\"{config.DATA_DIR}\/train.pkl\", \"rb\") as f:\n    train = pickle.load(f)\nwith open(f\"{config.DATA_DIR}\/dev1.pkl\", \"rb\") as f:\n    dev1 = pickle.load(f)\nwith open(f\"{config.DATA_DIR}\/dev2.pkl\", \"rb\") as f:\n    dev2 = pickle.load(f)","0f04fb3c":"with open(f\"\/kaggle\/input\/aio-make-new-token-ids-from-wiki\/original_train.pkl\", \"rb\") as f:\n    original_train = pickle.load(f)","72b2b408":"#with open(f\"\/kaggle\/input\/aio-make-all-wiki-org-train\/original_train_all_0.pkl\", \"rb\") as f:\n#    original_train_all = pickle.load(f)","cd192ede":"print(\"befor:\", len(train))\ntrain = train + original_train + dev2\nprint(\"after:\", len(train))\n\ndel original_train\ngc.collect()","66db5a69":"def loss_fn(outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets)\n\nclass BertForAIO(nn.Module):\n    def __init__(self):\n        super(BertForAIO, self).__init__()\n\n        bert_conf = BertConfig(config.MODEL_TYPE)\n        bert_conf.output_hidden_states = True\n        bert_conf.vocab_size = config.TOKENIZER.vocab_size\n        #bert_conf.attention_probs_dropout_prob = 0.2\n        #bert_conf.hidden_dropout_prob = 0.2\n\n        self.n_use_layer = 1\n        self.dropout_sample = 5\n        self.dropout_ratio = 0.2\n\n        self.bert = AutoModel.from_pretrained(config.MODEL_TYPE, config=bert_conf)\n        \n        self.dropout = nn.Dropout(self.dropout_ratio)\n        \n        n_weights = bert_conf.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        \n        self.dense1 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        self.dense2 = nn.Linear(bert_conf.hidden_size*self.n_use_layer, bert_conf.hidden_size*self.n_use_layer)\n        \n        self.dropouts = nn.ModuleList([nn.Dropout(self.dropout_ratio) for _ in range(self.dropout_sample)])\n    \n        self.fc = nn.Linear(bert_conf.hidden_size*self.n_use_layer, 1)\n\n\n    def forward(self, ids, mask, token_type_ids):\n        n_choice = ids.shape[1]\n        \n        ids = ids.view(-1, ids.size(-1))\n        mask = mask.view(-1, mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n\n        _, _, h = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n\n        cat_output = torch.stack([self.dropout(layer[:, 0, :]) for layer in h], dim=2)\n        cat_output = (torch.softmax(self.layer_weights, dim=0) * cat_output).sum(-1)\n        #cat_output = (torch.sigmoid(self.layer_weights) * cat_output).sum(-1)\n\n        cat_output = self.dense1(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        cat_output = self.dense2(cat_output)\n        cat_output = nn.ReLU()(cat_output)\n        logits = sum([self.fc(dropout(cat_output)) for dropout in self.dropouts])\/self.dropout_sample\n\n        logits = logits.view(-1, n_choice)\n\n        return logits","fe4a186f":"class JaqketDataset:\n    def __init__(self, data, optinons=20):\n        self.data = data\n        self.optinons = optinons\n        self.negative_sample = list(range(1, 20))\n    \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, item):\n        d = self.data[item]\n        if self.optinons == 20:\n            return {\n                'ids': torch.tensor(d[\"input_ids\"][:self.optinons], dtype=torch.long),\n                'mask': torch.tensor(d[\"attention_mask\"][:self.optinons], dtype=torch.long),\n                'token_type_ids': torch.tensor(d[\"token_type_ids\"][:self.optinons], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}\n        else:\n            random.shuffle(self.negative_sample)\n            sample_idx = [0] + self.negative_sample[:self.optinons]\n            return {\n                'ids': torch.tensor([d[\"input_ids\"][i] for i in sample_idx], dtype=torch.long),\n                'mask': torch.tensor([d[\"attention_mask\"][i] for i in sample_idx], dtype=torch.long),\n                'token_type_ids': torch.tensor([d[\"token_type_ids\"][i] for i in sample_idx], dtype=torch.long),\n                'targets': torch.tensor(d[\"label\"], dtype=torch.long)}","7a99e690":"def train_fn(data_loader, model, optimizer, device, num_batches, scheduler=None):\n    model.train()\n    for bi, d in enumerate(data_loader):\n        ids = d[\"ids\"].to(device, dtype=torch.long)\n        mask = d[\"mask\"].to(device, dtype=torch.long)\n        token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n        targets = d[\"targets\"].to(device, dtype=torch.long)\n\n        model.zero_grad()\n        pred = model(ids, mask, token_type_ids)\n        loss = loss_fn(pred, targets)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        \n        #if bi % 25 == 0:\n        #    print(bi, loss)\n\n        \ndef eval_fn(data_loader, model, device):\n    model.eval()\n    acc_scores = AverageMeter()\n    with torch.no_grad():\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"].to(device, dtype=torch.long)\n            mask = d[\"mask\"].to(device, dtype=torch.long)\n            token_type_ids = d[\"token_type_ids\"].to(device, dtype=torch.long)\n            targets = d[\"targets\"].to(device, dtype=torch.long).cpu().numpy()\n\n            pred = model(ids, mask, token_type_ids)\n            pred = pred.cpu().detach().numpy().argmax(1)\n\n            acc = accuracy_score(targets, pred)\n            acc_scores.update(acc, ids.size(0))\n\n    return acc_scores.avg","0a5cbc4a":"def run():\n    device = xm.xla_device()\n    model = MX.to(device)\n\n    # set train dataloder\n    train_dataset = JaqketDataset(train, optinons=config.OPTIONS)\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=2\n    )\n\n    # set dev1 dataloder\n    dev1_dataset = JaqketDataset(dev1, optinons=20)\n    dev1_sampler = torch.utils.data.distributed.DistributedSampler(\n        dev1_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    dev1_data_loader = torch.utils.data.DataLoader(\n        dev1_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        sampler=dev1_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    # set dev2 dataloder\n    dev2_dataset = JaqketDataset(dev2, optinons=20)\n    dev2_sampler = torch.utils.data.distributed.DistributedSampler(\n        dev2_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    dev2_data_loader = torch.utils.data.DataLoader(\n        dev2_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        sampler=dev2_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n\n    # optimizer setting\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    \n    optimizer_parameters = []\n    max_lrs = []\n    learning_rate = config.LEARNING_RATE * xm.xrt_world_size()\n    for param in param_optimizer:\n        # weight_decay\n        if any(n in param[0] for n in no_decay):\n            weight_decay = 0.0\n        else:\n            weight_decay = 0.1\n        \n        # learning rate\n        if param[0].find(\"bert.encoder.layer\") != -1:\n            n_diff_last = 11 - int(param[0].split(\".\")[3])\n            lr = learning_rate*0.9**n_diff_last\n        elif \"embeddings\" in param[0]:\n            lr = learning_rate*0.9**11\n        else:\n            lr = learning_rate\n        \n        max_lrs.append(lr)\n        d = {\"params\": param[1], \"weight_decay\": weight_decay}\n        optimizer_parameters.append(d)\n\n    optimizer = AdamW(\n        optimizer_parameters,\n        lr=config.LEARNING_RATE * xm.xrt_world_size(),\n    )\n\n    # scheduler setting\n    num_train_steps = int(\n        len(train) \/ config.TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * config.EPOCHS\n    )\n    n_warmup = int(num_train_steps * config.WARM_UP_RATIO)\n\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        #max_lr=max_lrs,\n        num_warmup_steps=n_warmup,\n        num_training_steps=num_train_steps\n    )\n  \n    num_batches = int(len(train) \/ (config.TRAIN_BATCH_SIZE * xm.xrt_world_size()))\n  \n\n    for param in model.named_parameters():\n      if \"fc\" in param[0]:\n        continue\n      if \"dense\" in param[0]:\n        continue\n      param[1].requires_grad = False\n    \n    for epoch in range(config.EPOCHS):\n        if epoch == 1:\n            for param in model.named_parameters():\n              param[1].requires_grad = True\n    \n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_fn(\n            para_loader.per_device_loader(device), \n            model, \n            optimizer, \n            device,\n            num_batches,\n            scheduler\n        )\n\n        para_loader = pl.ParallelLoader(dev1_data_loader, [device])\n        dev1_score = eval_fn(para_loader.per_device_loader(device), model, device)\n        dev1_score = xm.mesh_reduce('acc_reduce', dev1_score, reduce_fn)\n        \n        para_loader = pl.ParallelLoader(dev2_data_loader, [device])\n        dev2_score = eval_fn(para_loader.per_device_loader(device), model, device)\n        dev2_score = xm.mesh_reduce('acc_reduce', dev2_score, reduce_fn)\n        \n        xm.master_print(f'Epoch={epoch}, dev1={dev1_score}, dev2={dev2_score}')\n        xm.save(model.state_dict(), f\"model_{epoch}.bin\")\n    \n\ndef _mp_fn(rank, flags):\n    seed_everything(config.SEED)\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = run()","1542f505":"MX = BertForAIO()\nMX.load_state_dict(torch.load(f\"\/kaggle\/input\/aio-original-train-data\/model_1.bin\", map_location=torch.device('cpu')))\n\n#w_dic = torch.load(f\"\/kaggle\/input\/aio-my-prediction\/model_squad.bin\", map_location=torch.device('cpu'))\n#del w_dic[\"qa_outputs.weight\"]\n#del w_dic[\"qa_outputs.bias\"]\n#w_dic = {\".\".join(k.split(\".\")[1:]): v for k, v in w_dic.items()}\n#MX.bert.load_state_dict(w_dic)\n\ngc.collect()","ff62ee4c":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","7b237cea":"!ls","8e0b87a2":"# \u5b9f\u9a13\u8a73\u7d30\n\ndev2 train"}}