{"cell_type":{"a7d6e34b":"code","3dfc275a":"code","e6af92be":"code","2e7a1f47":"code","fa3ef551":"code","b1c8172c":"code","0cc30645":"code","e1509683":"code","bfb48a6e":"code","49c557e1":"code","0235c9cc":"code","55ef713b":"code","6fc33237":"code","d01e2216":"code","ad4a7707":"code","24890bf7":"markdown","95bb5538":"markdown","ad3c7722":"markdown","59a6d096":"markdown","5da2efd5":"markdown","c58d074b":"markdown","d4265021":"markdown"},"source":{"a7d6e34b":"import numpy as np\nimport pandas as pd\nimport os\nimport warnings\nimport random\nimport torch \nfrom torch import nn\nimport torch.optim as optim\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nfrom transformers import RobertaModel, RobertaConfig\nwarnings.filterwarnings('ignore')","3dfc275a":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\n#\u00a0TODO: Average across many seeds. Yes, later.\nseed = 4444\nseed_everything(seed)","e6af92be":"# Using the fine-tuned RoBERTa on the twitter training dataset\n# No fine-tuning here...\nBASE_ROBERTA = \"..\/input\/roberta-base\"","2e7a1f47":"class TweetDataset(torch.utils.data.Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        # TODO: Change with the fine-tuned model.\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab_file=f'{BASE_ROBERTA}\/vocab.json', \n            merges_file=f'{BASE_ROBERTA}\/merges.txt', \n            lowercase=True,\n            add_prefix_space=True)\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx\n        \ndef get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = torch.utils.data.DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = torch.utils.data.DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    dataloaders_dict = {\"train\": train_loader, \"val\": val_loader}\n\n    return dataloaders_dict\n\ndef get_test_loader(df, batch_size=32):\n    loader = torch.utils.data.DataLoader(\n        TweetDataset(df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)    \n    return loader","fa3ef551":"class TweetModel(nn.Module):\n    def __init__(self):\n        super(TweetModel, self).__init__()\n        # TODO: Change with the fine-tuned roberta\n        config = RobertaConfig.from_pretrained(\n           f'{BASE_ROBERTA}\/config.json', output_hidden_states=True)\n        self.config = config\n        self.roberta = RobertaModel.from_pretrained(\n            f'{BASE_ROBERTA}\/pytorch_model.bin', config=self.config)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)\n\n        # +1 for the bias term\n        n_weights = config.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = nn.Parameter(weights_init)\n\n        self.intermediate_classifier = nn.Linear(config.hidden_size, config.hidden_size \/\/ 2)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n        \n        # TODO: Is this the bug? Yes it was.\n        # self.roberta.init_weights()\n        nn.init.normal_(self.classifier.weight, std=0.02)\n        nn.init.normal_(self.classifier.bias, 0)\n\n    def forward(self, input_ids, attention_mask):\n        _, _, hidden_layers = self.roberta(input_ids, attention_mask)\n        \n        \n        \"\"\"\n        x = torch.stack([hidden_layers[-1], hidden_layers[-2]],\n                                  dim=3)\n        \n        x = torch.mean(x, dim=3)\n        \"\"\"\n        \n        x = torch.stack([self.dropout(layer) for layer in hidden_layers],\n                                  dim=3)\n        \n        # Weighted mean.\n        x = (torch.softmax(self.layer_weights, dim=0) * x).sum(-1)\n        \n        # x = torch.mean(x, -1)\n        \n        # x = self.intermediate_classifier(x)\n        \n        x = torch.mean(torch.stack([\n            self.classifier(self.high_dropout(x))\n            for _ in range(5)\n        ], dim=0), dim=0)\n        \n        \n        # x = self.classifier(self.dropout(x))\n        \n        \n        start_logits, end_logits = x.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n                \n        return start_logits, end_logits","b1c8172c":"model = TweetModel()","0cc30645":"output = model(torch.tensor([0, 1313, 2, 2, 42, 16, 41, 6344, 3545, 328, 2], dtype=torch.long).unsqueeze(0), \n      torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.long).unsqueeze(0), \n)\nstart_pos_logit = output[0]\nend_pos_logit = output[1]\nprint(start_pos_logit)\nprint(end_pos_logit)","e1509683":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    ce_loss = nn.CrossEntropyLoss()\n    start_loss = ce_loss(start_logits, start_positions)\n    end_loss = ce_loss(end_logits, end_positions)    \n    total_loss = start_loss + end_loss\n    return total_loss","bfb48a6e":"def get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2, smooth=0): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return (float(len(c)) + smooth) \/ (len(a) + len(b) - len(c) + smooth)\n\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n    start_pred = np.argmax(start_logits)\n    end_pred = np.argmax(end_logits)\n    if start_pred > end_pred:\n        pred = text\n    else:\n        pred = get_selected_text(text, start_pred, end_pred, offsets)\n        \n    true = get_selected_text(text, start_idx, end_idx, offsets)\n    \n    return jaccard(true, pred)","49c557e1":"#\u00a0Load trained model folds and predict\nMODEL_FOLDS_BASE_FOLDER = \"..\/input\/tweeter-stratification-no-fine-tuning-seed-4444\/\"","0235c9cc":"%%time\n\n\n#\u00a0TODO: Remove incorrect selected_text or complete it?\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntest_df['text'] = test_df['text'].astype(str)\ntest_loader = get_test_loader(test_df)\npredictions = []\nmodels = []\n# TODO: Make this work with many seeds later. \nfor fold in range(5):\n    model = TweetModel()\n    model.cuda()\n    model.load_state_dict(torch.load(f'{MODEL_FOLDS_BASE_FOLDER}\/roberta_fold{fold+1}_{seed}.pth'))\n    model.eval()\n    models.append(model)\ncount = 0\nfor data in test_loader:\n    ids = data['ids'].cuda()\n    masks = data['masks'].cuda()\n    tweet = data['tweet']\n    offsets = data['offsets'].numpy()\n\n    start_logits = []\n    end_logits = []\n    for model in models:\n        with torch.no_grad():\n            output = model(ids, masks)\n            start_logits.append(output[0].cpu().detach().numpy())\n            end_logits.append(output[1].cpu().detach().numpy())\n\n    start_logits = np.array(start_logits)\n    end_logits = np.array(end_logits)\n    start_logits = np.mean(start_logits, axis=0)\n    end_logits = np.mean(end_logits, axis=0)\n    for i in range(len(ids)):    \n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        #\u00a0TODO: Better post-processing?\n        if start_pred > end_pred:\n            count += 1\n            pred = tweet[i]\n        else:\n            pred = get_selected_text(tweet[i], start_pred, end_pred, offsets[i])\n        predictions.append(pred)","55ef713b":"print(count)\nlen(predictions)","6fc33237":"pred_length = [len(p) for p in predictions]","d01e2216":"pd.Series(pred_length).value_counts().hist(bins=50)","ad4a7707":"sub_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\nsub_df['selected_text'] = predictions\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\nsub_df['selected_text'] = sub_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\nsub_df.to_csv('submission.csv', index=False)\nsub_df.head()","24890bf7":"# Data Loader","95bb5538":"# Loss Function","ad3c7722":"# Seed","59a6d096":"# Inference","5da2efd5":"# Evaluation Function","c58d074b":"# Model","d4265021":"# Submission"}}