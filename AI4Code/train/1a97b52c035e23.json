{"cell_type":{"d4d95a48":"code","780246a2":"code","57125203":"code","ab1aa971":"code","8cd150b1":"code","bd731c3a":"code","fb044e35":"code","156058b3":"code","507a99b8":"code","cded4251":"code","f8eafb46":"code","6c043158":"code","4426eb36":"code","b56574d8":"code","a7896141":"code","3225651b":"code","2f1a148c":"code","ea630b76":"code","26f377fb":"code","284b2e30":"code","bb8ae00e":"code","11357f7e":"code","26d7ecb7":"code","bfc1afe5":"code","cb54f1ec":"code","a591f1a0":"code","6b6b95b5":"code","b24e8ca4":"code","ddb938e4":"code","935907db":"code","9e8885ed":"code","3bc74de4":"code","a0a31ed0":"code","16f55858":"markdown","d6558214":"markdown","93b0fd53":"markdown","3dc618c1":"markdown","61a4376a":"markdown","fd043b45":"markdown","41fab3ef":"markdown","cf8f7906":"markdown","6cdcbe8f":"markdown","f3d8cf15":"markdown","bffa0c98":"markdown","cfe65e65":"markdown","a841fc6a":"markdown","d84e027e":"markdown","ccd0fcd4":"markdown"},"source":{"d4d95a48":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings  as ws\nws.filterwarnings(\"ignore\")","780246a2":"df = pd.read_csv(\"..\/input\/diabetes\/diabetes-dataset.csv\")","57125203":"df.head()","ab1aa971":"df.shape","8cd150b1":"sns.set()\nsns.countplot(df[\"Outcome\"])\nplt.show()","bd731c3a":"#CHECKING FOR CORRELATION\n\ncorrmat = df.corr()\ntop_corr_features=corrmat.index\nplt.figure(figsize=(15,15))\ng=sns.heatmap(df[top_corr_features].corr(),annot=True, cmap=\"RdYlGn\")","fb044e35":"#HISTOGRAM TO CHECK FOR GRAPH SHAPE\n\nplt.figure(figsize=(20,20))\ndf.hist()","156058b3":"df.isna().sum()","507a99b8":"from sklearn.preprocessing import StandardScaler","cded4251":"# Split the data \nfrom sklearn.model_selection import train_test_split\nX = df.drop(columns =[ \"Outcome\", \"Pregnancies\", \"DiabetesPedigreeFunction\"])\ny = df[\"Outcome\"]\nX_train , X_test, y_train, y_test = train_test_split (X, y, test_size = 0.2, random_state = 42, stratify = y)","f8eafb46":"# Scaling the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","6c043158":"# Fitting the model to the training data\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score","4426eb36":"model=[]\nmodel_accuracy=[]","b56574d8":"\nlog = LogisticRegression()\nmodel.append('LogisticRegression')\nlog.fit(X_train_scaled, y_train)\ny_pred= log.predict(X_test_scaled)\nlog_accuracy= accuracy_score(y_pred, y_test)\nmodel_accuracy.append(log_accuracy)\n# Evaluating the Random Forest Model\nprint (\"Accuracy on test set is \", round(log.score(X_test_scaled, y_test) * 100, 2), \"%\")\nprint (\"Accuracy on train set is \", round(log.score(X_train_scaled, y_train) * 100, 2), \"%\")","a7896141":"rfe = RandomForestClassifier()\nmodel.append('RandomForestClassifier')\n\nrfe.fit(X_train_scaled, y_train)\ny_pred= rfe.predict(X_test_scaled)\nrfe_accuracy= accuracy_score(y_pred, y_test)\nmodel_accuracy.append(rfe_accuracy)\n# Evaluating the Random Forest Model\nprint (\"Accuracy on test set is \", round(rfe.score(X_test_scaled, y_test) * 100, 2), \"%\")\nprint (\"Accuracy on train set is \", round(rfe.score(X_train_scaled, y_train) * 100, 2), \"%\")","3225651b":"dec = DecisionTreeClassifier()\nmodel.append('DecisionTreeClassifier')\n\ndec.fit(X_train_scaled, y_train)\ny_pred= dec.predict(X_test_scaled)\ndec_accuracy= accuracy_score(y_pred, y_test)\nmodel_accuracy.append(dec_accuracy)\n\n# Evaluating the Random Forest Model\nprint (\"Accuracy on test set is \", round(dec.score(X_test_scaled, y_test) * 100, 2), \"%\")\nprint (\"Accuracy on train set is \", round(dec.score(X_train_scaled, y_train) * 100, 2), \"%\")","2f1a148c":"svc = SVC()\nmodel.append('SVC')\n\nsvc.fit(X_train_scaled, y_train)\ny_pred= rfe.predict(X_test_scaled)\nsvc_accuracy= accuracy_score(y_pred, y_test)\nmodel_accuracy.append(svc_accuracy)\n\n# Evaluating the Random Forest Model\nprint (\"Accuracy on test set is \", round(svc.score(X_test_scaled, y_test) * 100, 2), \"%\")\nprint (\"Accuracy on train set is \", round(svc.score(X_train_scaled, y_train) * 100, 2), \"%\")","ea630b76":"knn = KNeighborsClassifier()\nmodel.append('KNeighborsClassifier')\n\nknn.fit(X_train_scaled, y_train)\ny_pred= rfe.predict(X_test_scaled)\nknn_accuracy= accuracy_score(y_pred, y_test)\nmodel_accuracy.append(knn_accuracy)\n\n# Evaluating the Random Forest Model\nprint (\"Accuracy on test set is \", round(knn.score(X_test_scaled, y_test) * 100, 2), \"%\")\nprint (\"Accuracy on train set is \", round(knn.score(X_train_scaled, y_train) * 100, 2), \"%\")","26f377fb":"fig,ax=plt.subplots(figsize=(10,8))\nsns.barplot(model,model_accuracy)\nax.set_title(\"Accuracy of Models on Test Data\",pad=20)\nax.set_xlabel(\"Models\",labelpad=20)\nax.set_ylabel(\"Accuracy\",labelpad=20)\nplt.xticks(rotation=90)\n\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x+0.25, y + height + 0.01))","284b2e30":"bagging_models=[]\nbagging_accuracy=[]\n\nmodels_scores=[]","bb8ae00e":"bag_dec= BaggingClassifier(base_estimator=dec)\nbagging_models.append('bagging decisionTree')\nbag_dec.fit(X_train_scaled, y_train)\nbag_dec_pred= bag_dec.predict(X_test_scaled)\nbag_dec_accuracy= accuracy_score(bag_dec_pred, y_test)\nbagging_accuracy.append(bag_dec_accuracy)\nprint(\"The accuracy for the training set is \", bag_dec.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", bag_dec.score(X_test_scaled, y_test))\nprint(\"\\n\")\n\nmodels_scores.append(['Bagging_DecisionTree', bag_dec_accuracy])","11357f7e":"bag_ref= BaggingClassifier(base_estimator=rfe)\nbagging_models.append('bagging RandomForest')\n\nbag_ref.fit(X_train_scaled, y_train)\n\nbag_ref_pred= bag_ref.predict(X_test_scaled)\nbag_ref_accuracy= accuracy_score(bag_ref_pred, y_test)\nbagging_accuracy.append(bag_ref_accuracy)\nprint(\"The accuracy for the training set is \", bag_ref.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", bag_ref.score(X_test_scaled, y_test))\n\nmodels_scores.append(['Bagging_RandomForest', bag_ref_accuracy])","26d7ecb7":"bag_svc= BaggingClassifier(base_estimator=svc)\nbagging_models.append('bagging svc')\n\nbag_svc.fit(X_train_scaled, y_train)\n\nbag_svc_pred= bag_ref.predict(X_test_scaled)\nbag_svc_accuracy= accuracy_score(bag_svc_pred, y_test)\nbagging_accuracy.append(bag_svc_accuracy)\nprint(\"The accuracy for the training set is \", bag_svc.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", bag_svc.score(X_test_scaled, y_test))\n\nmodels_scores.append(['Bagging_SupportVector', bag_svc_accuracy])","bfc1afe5":"bag_knn= BaggingClassifier(base_estimator=knn)\nbagging_models.append('bagging knn')\n\nbag_knn.fit(X_train_scaled, y_train)\n\nbag_knn_pred= bag_knn.predict(X_test_scaled)\nbag_knn_accuracy= accuracy_score(bag_knn_pred, y_test)\nbagging_accuracy.append(bag_knn_accuracy)\nprint(\"The accuracy for the training set is \", bag_knn.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", bag_knn.score(X_test_scaled, y_test))\n\nmodels_scores.append(['Bagging_knn', bag_knn_accuracy])","cb54f1ec":"bag_log= BaggingClassifier(base_estimator=log)\nbagging_models.append('bagging logisticRegression')\n\nbag_log.fit(X_train_scaled, y_train)\n\nbag_log_pred= bag_knn.predict(X_test_scaled)\nbag_log_accuracy= accuracy_score(bag_log_pred, y_test)\nbagging_accuracy.append(bag_log_accuracy)\nprint(\"The accuracy for the training set is \", bag_log.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", bag_log.score(X_test_scaled, y_test))","a591f1a0":"fig,ax=plt.subplots(figsize=(10,8))\nsns.barplot(bagging_models,bagging_accuracy)\nax.set_title(\"Accuracy of Models on Test Data\",pad=20)\nax.set_xlabel(\"Models\",labelpad=20)\nax.set_ylabel(\"Accuracy\",labelpad=20)\nplt.xticks(rotation=90)\n\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x+0.25, y + height + 0.01))","6b6b95b5":"boosting_models=[]\nboosting_accuracy=[]","b24e8ca4":"#boosting\nada_dec= AdaBoostClassifier(base_estimator=dec)\nada_dec.fit(X_train_scaled, y_train)\nada_dec_pred= ada_dec.predict(X_test_scaled)\nboost_dec_accuracy= accuracy_score(ada_dec_pred, y_test)\n\nada_ref= AdaBoostClassifier(base_estimator=rfe)\nboosting_models.append('boosting random forest')\nada_ref.fit(X_train_scaled, y_train)\nada_ref_pred= ada_ref.predict(X_test_scaled)\nboost_ref_accuracy= accuracy_score(ada_ref_pred, y_test)\nboosting_accuracy.append(boost_ref_accuracy)\nmodels_scores.append(['Adaboosting_RandomForest', boost_ref_accuracy])\n\nada_svc= AdaBoostClassifier(base_estimator=svc, algorithm='SAMME')\nboosting_models.append('boosting svc')\nada_svc.fit(X_train_scaled, y_train)\nada_svc_pred= ada_svc.predict(X_test_scaled)\nboost_svc_accuracy= accuracy_score(ada_svc_pred, y_test)\nboosting_accuracy.append(boost_svc_accuracy)\nmodels_scores.append(['Adaboosting_SupportVector', boost_svc_accuracy])\n\nfig,ax=plt.subplots(figsize=(10,8))\nsns.barplot(boosting_models,boosting_accuracy)\nax.set_title(\"Accuracy of Models on Test Data\",pad=20)\nax.set_xlabel(\"Models\",labelpad=20)\nax.set_ylabel(\"Accuracy\",labelpad=20)\nplt.xticks(rotation=90)\n\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.0%}'.format(height), (x+0.25, y + height + 0.01))","ddb938e4":"estimators=[]\n\nmodel1= SVC()\nestimators.append((\"svc\", model1))\n\nmodel2= DecisionTreeClassifier()\nestimators.append((\"dec\", model2))\n\nmodel3= model3 = RandomForestClassifier()\nestimators.append((\"rad\", model3))\n\nmodel4= LogisticRegression()\nestimators.append((\"log\", model4))\n\nmodel5= KNeighborsClassifier()\nestimators.append((\"knn\", model5))\n\n# create the ensemble model\nensemble = VotingClassifier(estimators, voting=\"hard\")\nensemble.fit(X_train_scaled, y_train)\n\nvot_pred= ensemble.predict(X_test_scaled)\nscore_vote= accuracy_score(vot_pred, y_test)\nprint(score_vote*100)\nprint(\"\\n\")\nprint(\"The accuracy for the training set is \", ensemble.score(X_train_scaled, y_train))\nprint(\"The accuracy for the test set is \", ensemble.score(X_test_scaled, y_test))\nprint(\"\\n\")\nmodels_scores.append(['voting classifier', score_vote])","935907db":"\npd.DataFrame(models_scores).sort_values(by=[0], ascending=True)","9e8885ed":"bag_dec = BaggingClassifier(base_estimator=dec)\nbagging_models.append('bagging decisionTree')\nbag_dec.fit(X_train_scaled, y_train)\nbag_dec_pred = bag_dec.predict([[100, 20, 50,60,0.78, 40]]) #labels=[1=\"Diabetes\", 0=\"Not Diabetes\"]\nif bag_dec_pred == 1:\n    print(\"Diabetic\")\nelse:\n    print(\"Non Diabetic\")","3bc74de4":"import pickle\n\nPkl_Filename = \"Final_Diabetes.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(bag_dec, file)","a0a31ed0":"# Load the Model back from file\nwith open(Pkl_Filename, 'rb') as file:  \n    Pickled_LR_Model = pickle.load(file)\n\nPickled_LR_Model","16f55858":"# DATA PREPROCESSING","d6558214":"\n# IMPORTING PACKAGES\n","93b0fd53":"# **Combine multiple models of various types**\n\n**\u2022Vote Classification**\n\n**\u2022Blending or Stacking**","3dc618c1":"   # BAGGING      \n   ![Ensemble_Learning_Bagging.png](attachment:Ensemble_Learning_Bagging.png)","61a4376a":"   # BOOSTING\n   \n   ![Ensemble_Learning_Boosting.png](attachment:Ensemble_Learning_Boosting.png)","fd043b45":"# MODEL BUILDING OR TRAINING","41fab3ef":"   # VOTE CLASSIFICATION","cf8f7906":"# IMPORTING MODELS","6cdcbe8f":"Above graph clearly shows the class imbalance ","f3d8cf15":"# SUMMARY OF MODELS ACCURACY","bffa0c98":"# **Combine multiple models of similar type**\n\n\n**Bagging (Bootstrap aggregation)**\n\n**Boosting**","cfe65e65":"# READING THE DATA","a841fc6a":"# DATA VISUALIZATION","d84e027e":"# ENSEMBLE METHODS","ccd0fcd4":"# MODEL SAVING\n"}}