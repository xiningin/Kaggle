{"cell_type":{"845295ba":"code","55f1f1db":"code","7603470d":"code","d4a96361":"code","99dc297d":"code","ec24383d":"code","dee9519d":"code","e1d725db":"code","af9c3c73":"code","1210e0ec":"code","f1682885":"code","1a434ff2":"code","6489011d":"code","24a8c0a5":"code","1fa334cf":"code","db9042bf":"code","e2e8ffd3":"code","40810d89":"code","81b8603a":"code","da940c3c":"code","6e50181e":"code","f62bc7fe":"code","c523e804":"code","1217b717":"code","324509d5":"code","193b1768":"code","9594abe5":"code","a16d47cf":"code","8462a1b3":"code","11ede6d9":"code","06ea3f82":"code","602e0f1d":"code","0d40f9bd":"code","0782df82":"code","b1fa1260":"code","f7fe8002":"code","71bc3f25":"code","51dbc2b7":"code","e99fa344":"code","f6c4933d":"code","9a293015":"code","a0c7903d":"code","a8d7031e":"code","6baf7a6e":"code","5f3830fc":"code","42c11e17":"code","24549cc5":"code","f53cc506":"code","59e3c7f0":"code","d969101d":"code","d0f4eb88":"markdown","327ccba5":"markdown","fbc47dbd":"markdown","9b10c244":"markdown","a7a8b231":"markdown","1adcf76e":"markdown","16bbb2b6":"markdown","7f1b8443":"markdown","145981ac":"markdown","bdd1f0ce":"markdown","45d50ed8":"markdown","5facc3c4":"markdown","44f7e8b4":"markdown","dcfbf29e":"markdown","2c8bd316":"markdown","b4cc6559":"markdown","95450100":"markdown","a7c7de3f":"markdown","e9f7411e":"markdown","44c773cc":"markdown","7a6e5b18":"markdown","7dda6f93":"markdown","5460a4b0":"markdown","81efc693":"markdown","253fd896":"markdown","965e6b78":"markdown","00c70903":"markdown","4628587c":"markdown"},"source":{"845295ba":"# import tensorflow library to use GPU into this code for faster processing\nimport tensorflow","55f1f1db":"# import the required libraries\n\nimport zipfile  #read the csv file from zip format without extracting it in drive, we save space\nimport numpy as np  #linear algebra computations and transformations\nimport pandas as pd  #read the dataframe and dataframe operations\nimport matplotlib.pyplot as plt  #visualization of data\nimport seaborn as sns  #visualization of data\nimport re  #support for regular expressions\n\n# pd.set_option('display.max_columns', 500)  #set the default option to show all columns when we want to show data\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nfrom scipy import stats","7603470d":"# read contents of zip file\nzf = zipfile.ZipFile('creditcard.csv.zip')\n\n# read the data from csv file into pandas dataframe\ncc_fraud = pd.read_csv(zf.open('creditcard.csv'))","d4a96361":"# create a copy of original dataframe so as to avoid reading from drive again\ndf = cc_fraud.copy()\nprint(df.shape)\ndf.head()","99dc297d":"# We perform descriptive statistics to check the mean and std of each variable. \ndf.describe()","ec24383d":"print(df.Class.value_counts())\nprint(df.Class.value_counts(normalize=True))","dee9519d":"sns.pairplot(df)\nplt.show()","e1d725db":"print(f'skewness in Time column: {df.Time.skew():.2f}')\nplt.subplots(figsize=(8,6))\n# plt.subplot(121)\nsns.distplot(df.Time)\n# plt.subplot(122)\n# stats.probplot(df.Time, plot=plt)\nplt.xlabel('Time elapsed in seconds', fontsize=12)\nplt.show()","af9c3c73":"sns.boxplot(df.Time)\nplt.show()","1210e0ec":"bins = [0,25000,50000,75000,100000,125000,150000,175000]\ntime_bin = pd.cut(df.Time, bins, right=False)\ndf['Time_Bins'] = time_bin\ntime_table = pd.crosstab(index=df.Time_Bins, columns=df.Class)\n\ndf['Hour'] = df['Time'].apply(lambda x: int(np.ceil(float(x)\/3600) % 24))+1\nhour_table = df.pivot_table(values='Amount',index='Hour',columns='Class',aggfunc='count', margins=True)\n","f1682885":"time_table.head()\nhour_table.sort_values(by=[1], ascending=False).head(10)","1a434ff2":"hour_table.plot(kind='bar', stacked=True, figsize=(8,6))\nplt.xticks(np.arange(0,25), rotation=0)\nplt.xlabel('Hour of the day', fontsize=12)\nplt.ylabel('Number of Transactions', fontsize=12)\nplt.show()","6489011d":"max_amount_0 = df[df.Class==0].groupby(by='Hour', observed=True).max()['Amount']\nmin_amount_0 = df[df.Class==0].groupby(by='Hour', observed=True).min()['Amount']\ncount_0 = df.Hour[df.Class==0].value_counts().sort_index()","24a8c0a5":"max_amount_1 = df[df.Class==1].groupby(by='Hour', observed=True).max()['Amount']\nmin_amount_1 = df[df.Class==1].groupby(by='Hour', observed=True).min()['Amount']\ncount_1 = df.Hour[df.Class==1].value_counts().sort_index()","1fa334cf":"df_time = pd.DataFrame({('0','min_amount_0'):min_amount_0,\n                        ('0','max_amount_0'):max_amount_0,\n                        ('0','count_0'):count_0,\n                        ('1','min_amount_1'):min_amount_1,\n                        ('1','max_amount_1'):max_amount_1,\n                        ('1','count_1'):count_1}, index=count_0.index)","db9042bf":"df_time.sort_values(by=('1','count_1'), ascending=False)","e2e8ffd3":"df.drop(['Time_Bins','Hour'], axis=1, inplace=True)","40810d89":"plt.subplots(figsize=(15,10))\nplt.subplot(221)\nsns.distplot(df.Amount)\nplt.title('Distribution plot of Amount')\nplt.subplot(222)\nstats.probplot(df.Amount, plot=plt)\n\nplt.subplot(223)\nsns.distplot(np.log1p(df.Amount))\nplt.title('Distribution plot of Amount after Log Transformation')\nplt.subplot(224)\nstats.probplot(np.log1p(df.Amount), plot=plt)\n\nplt.show()","81b8603a":"np.quantile(a=df.Amount, q=[0.25,0.5,0.75])\n\nLW = max(5.6 - (77.165-5.6), 0)\nprint('LW: ',LW)\n\nUW = 77.165+(77.165-5.6)\nprint('UW: ',UW)","da940c3c":"df1 = df[df.Amount <= 148]\ndf1.Class.value_counts()","6e50181e":"df2 = df1[df1.Amount <= 55]","f62bc7fe":"plt.subplots(figsize=(15,8))\nplt.subplot(131)\nsns.boxplot(df.Amount, orient='vertical')\nplt.title('BoxPlot of original Amount Data')\nplt.subplot(132)\nsns.boxplot(df1.Amount, orient='vertical')\nplt.title('BoxPlot of Amount Data after outlier handling')\nplt.subplot(133)\nsns.boxplot(df2.Amount, orient='vertical')\nplt.title('BoxPlot after handling the ourliers in Data')\nplt.show()","c523e804":"df1 = df.copy()\ndf1.Amount = np.log1p(df1.Amount)\n\ndf2 = df1[df1.Amount <= 8]","1217b717":"df2.Class.value_counts()","324509d5":"plt.subplots(figsize=(12,8))\nplt.subplot(131)\nsns.boxplot(df.Amount, orient='vertical')\nplt.title('Given Amount Column')\nplt.subplot(132)\nsns.boxplot(df1.Amount, orient='vertical')\nplt.title('Amount after Log Transformation')\nplt.subplot(133)\nsns.boxplot(df2.Amount, orient='vertical')\nplt.title('Amount after outlier handling')\nplt.show()","193b1768":"correlation_matrix = df.corr()\nfig = plt.figure(figsize=(15,9))\nsns.heatmap(correlation_matrix, vmax=0.8, square = True)\nplt.show()","9594abe5":"# import necessary modules\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nfrom sklearn.metrics import roc_curve, precision_recall_curve, average_precision_score  #comparison metrics\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor  #feature selection\nfrom statsmodels.tools.tools import add_constant #feature selection computing VIF","a16d47cf":"ss = StandardScaler()\ndf2.Time = ss.fit_transform(np.array(df2.Time).reshape(-1,1))","8462a1b3":"X = add_constant(df2)\n# X.drop(['Amount'], axis=1, inplace=True)\n# X = pd.get_dummies(X)\nX.drop('const', axis=1, inplace=True)\na = pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)","11ede6d9":"b = pd.DataFrame(a, columns=['VIF'])","06ea3f82":"b.sort_values(by='VIF', ascending=False)","602e0f1d":"X = df2.drop(['Class'], axis=1)\ny = df2.Class","0d40f9bd":"X_train = X.sample(frac=0.8, random_state=10)","0782df82":"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.2, random_state=10)","b1fa1260":"y_train.value_counts(normalize=True)","f7fe8002":"y_test.value_counts(normalize=True)","71bc3f25":"%%time\nrf = RandomForestClassifier(max_depth=15, n_estimators=100, oob_score=True, class_weight='balanced_subsample')\nrf.fit(X_train, y_train)","51dbc2b7":"y_pred_rf = rf.predict(X_test)\ny_pred_train = rf.predict(X_train)","e99fa344":"print('Train Confusion matrix:\\n', confusion_matrix(y_train, y_pred_train))\nprint('Test Confusion matrix:\\n', confusion_matrix(y_test, y_pred_rf))\nprint('Classification Report:\\n', classification_report(y_test, y_pred_rf))\nprint(f'\\nROC Score: {roc_auc_score(y_test, y_pred_rf):.4f}')","f6c4933d":"print(pd.crosstab(y, y_pred_rf, rownames=['Actual'], colnames=['Predicted'], margins=True))","9a293015":"max_depth = [int(x) for x in np.linspace(5,20,4)]\n# max_features = ['auto', 'sqrt']\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# min_samples_split = [2, 5, 10]\n# min_samples_leaf = [1, 2, 4]\n# bootstrap = [True, False]\n\nrandom_grid = {'max_depth': max_depth}","a0c7903d":"%%time\nrf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\nrandom_rf = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=10, cv=5, n_jobs=-1, scoring='roc_auc')","a8d7031e":"%%time\nrandom_rf.fit(X,y)","6baf7a6e":"print(f'grid best params: {random_rf.best_params_}')\nprint(f'grid best: {random_rf.best_score_}')","5f3830fc":"%%time\nparam = {'max_depth':[13,15,18]\n         }\nxgb = XGBClassifier(subsample=0.7, colsample_bytree=0.8)\ngrid_xgb = GridSearchCV(estimator=xgb, param_grid=param, scoring='roc_auc', n_jobs=4, cv=5)\ngrid_xgb.fit(X,y)","42c11e17":"print(f'grid best params: {grid_xgb.best_params_}')\nprint(f'grid best: {grid_xgb.best_score_}')","24549cc5":"%%time\nxgb=XGBClassifier(max_depth=18, subsample=0.7, scale_pos_weight=1, colsample_bytree=0.8)\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)\ny_pred_train = xgb.predict(X_train)","f53cc506":"print('Train Confusion matrix:\\n', confusion_matrix(y_train, y_pred_train))\nprint('Test Confusion matrix:\\n', confusion_matrix(y_test, y_pred_xgb))\nprint('Classification Report:\\n', classification_report(y_test, y_pred_xgb))\nprint(f'\\nROC Score: {roc_auc_score(y_test, y_pred_xgb):.4f}')","59e3c7f0":"rf_p,rf_r,rf_t = precision_recall_curve(y_test,y_pred_rf)\nrf_fpr,rf_tpr,rf_thr = roc_curve(y_test,y_pred_rf)\n\nxgb_p,xgb_r,xgb_t = precision_recall_curve(y_test,y_pred_xgb)\nxgb_fpr,xgb_tpr,xgb_thr = roc_curve(y_test,y_pred_xgb)","d969101d":"plt.figure(figsize=(18,8))\nplt.subplot(121)\nplt.title('ROC Curve', fontsize=16)\nplt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_test, y_pred_rf)))\nplt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_test, y_pred_xgb)))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.axis([-0.01, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n            arrowprops=dict(facecolor='#6E726D', shrink=0.05))\nplt.legend(loc='lower right', fontsize=12)\n\nplt.subplot(122)\nplt.title('Precision Recall Curve', fontsize=16)\nplt.plot(rf_r, rf_p, label='Random Forest Classifier Score: {:.4f}'.format(average_precision_score(y_test, y_pred_rf)))\nplt.plot(xgb_r, xgb_p, label='XGBoost Classifier Score: {:.4f}'.format(average_precision_score(y_test, y_pred_xgb)))\nplt.axis([0, 1.01, 0, 1.01])\nplt.xlabel('Recall', fontsize=14)\nplt.ylabel('Precision', fontsize=14)\nplt.legend(fontsize=12)\n\nplt.show()","d0f4eb88":"##### Normality check of Time","327ccba5":"We can see many outliers in Amount column, but we cannot remove them as higher values can mean that a fraud has happened. Also if we try to keep only fraud records and cap them then we change the meaning of transaction and disturb the pattern observed.","fbc47dbd":"transactions are lesser at early hours of the day, and increases during the day","9b10c244":"##### Hyperparameter Tuning of RF","a7a8b231":"Bimodal indicates that there are lesser transactions at certain time of day.","1adcf76e":"## Objective\n##### to create a model for banks and automize the fraud detection among credit card users.\nAll the columns in the dataset are output of PCA applied on original dataset. Hence all columns are numerical and standardized in given form. We also have no missing values in the dataset.","16bbb2b6":"    Random Forest\n    All data with Amount log transformed: ROC=0.9192, Recall=0.92, (413,79)\n    original Amount: ROC=0.9167, Recall=0.92, (410,82)\n    max_depth=10, n_estimators=50, ROC=0.9177, Recall=0.92, (411,81)\n    n_estimators=100, ROC=0.9197, Recall=0.91, (412,80)\n    ROC=0.9042, Recall=0.90, (76,18)","7f1b8443":"#### Visualize the pair plot of given data","145981ac":"Predicted variable contains 284315 records of negative class (i.e. no fraud recorded) and 492 positive records (i.e. fraud recorded). There is high skewness with class distribution seen to be 99.82%:0.17%","bdd1f0ce":"##### Random Forest Classifier","45d50ed8":"Cost sensitive learning - We assign costs of FN and FP misclassifications and try to reduce this cost","5facc3c4":"## Exploratory Data Analysis","44f7e8b4":"We create a new dataset df1 with log transformation applied on Amount column","dcfbf29e":"##### Outlier detection in Amount","2c8bd316":"##### Precision Recall Curve and ROC Curve","b4cc6559":"To improve results we need to gather more data, mostly of minority class. Data generation and feature engineering usually has higher payoff in terms of time invested and improved performance. \n\nOversampling, Undersampling, smote cannot be useful here as the positive class is only 0.17% and taking it up to 30-50% data means either adding lot of noise, redundant data or losing lots of information from negative class. This way the model will be forced to learn incorrect patterns in the data and the model cannot generalize.\n\nWe do not have scope for either hence we move towards the model hyperparameter tuning","95450100":"    default parameters: ROC=0.5, FN=492\n    after gridsearchcv: ROC=0.9329, Recall=0.93, max_depth=5, colsample_bytree=0.8, scale_pos_weight = 1, subsample=0.5 (426,66)\n    ROC=0.9360, Recall=0.94, max_depth=5, colsample_bytree=0.8, subsample=0.7, (429,63)\n    ROC=0.9553, Recall=0.96, max_depth=15, (448,44)\n    ROC=0.9096, Recall=0.91, max_depth=18, (77,17)","a7c7de3f":"Using correlation matrix we can understand that since the features given are from PCA, so these are uncorrelated and only correlation can be observed with Time and Amount columns which were not transformed with PCA earlier","e9f7411e":"### Descriptive Analytics","44c773cc":"##### Normality check of Amount","7a6e5b18":"Since it is skewed data we need to grow the tree to sufficient depth so as to learn the patterns in minority class records. More number of trees will help to reduce the overfit resulting from full\/higher depth of trees.","7dda6f93":"We have 2.84L records with 30 possible predictors and 1 predicted variable.","5460a4b0":"Since the data is highly imbalanced our first choice would be Random Forest. RF will not overfit the data","81efc693":"### Check for class imbalance","253fd896":"##### XGBoost Classifier","965e6b78":"we can see that there are no outliers in Time feature, hence no treatment needed on Time column","00c70903":"##### hour wise transaction","4628587c":"## Import libraries, read data and perform descriptive statistics"}}