{"cell_type":{"e8f9a13b":"code","ca2470c1":"code","bf530fd7":"code","c8c16c5a":"code","416cd4f2":"code","912c4039":"code","f793943a":"code","4fde59c9":"code","0830b9f4":"code","15de2aad":"code","b1d826d3":"code","4d41debf":"code","1faffa12":"code","93c5187a":"code","75352217":"code","5d368b63":"code","53fa2f3a":"code","1f764376":"code","83873aea":"code","337251bd":"code","27e36092":"code","e5066a51":"code","aadb5b38":"code","a1466b2b":"code","45ca6a42":"code","426b09ab":"code","6b4076b8":"code","d14ef879":"code","297d22da":"code","2f465cd4":"code","a210d0a5":"code","5427224d":"code","0df71e00":"code","5e36abc3":"code","8453e130":"code","61c93d38":"code","3b2573e6":"code","2fdf1ac0":"code","0f723b4e":"code","c7ad71d1":"code","6e7e35c3":"code","b53dc124":"code","c2eec1b6":"markdown","c39e4c75":"markdown","c85299ba":"markdown"},"source":{"e8f9a13b":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set_style('darkgrid')\nsns.set(rc={'figure.figsize':(12, 10)})\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","ca2470c1":"train_df = pd.read_csv('..\/input\/train\/train.csv')\ntest_df = pd.read_csv('..\/input\/test\/test.csv')\ny = train_df['AdoptionSpeed']\ntrain_df['dataset_type'] = 'train'\ntest_df['dataset_type'] = 'test'\ntrain_id = train_df['PetID']\ntest_id = test_df['PetID']\nall_data0 = pd.concat([train_df, test_df])\nall_data = all_data0.drop(columns = 'AdoptionSpeed')\ntrain_df.head()","bf530fd7":"print(train_df.shape)\nprint(test_df.shape)","c8c16c5a":"display(train_df.describe())\ndisplay(train_df.info())","416cd4f2":"train_df['AdoptionSpeed'].value_counts().sort_index(ascending=False).plot(kind='barh', \n                                                                          figsize=(15,6))\nplt.title('Adoption Speed (Target)', fontsize=18)","912c4039":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.countplot(x='Type', data=train_df).set(xticklabels=['Dog', 'Cat'])\nplt.title('Type(train)', fontsize=18)\n\nplt.subplot(122)\nsns.countplot(x='Type', data=test_df).set(xticklabels=['Dog', 'Cat'])\nplt.title('Type(test)', fontsize=18)","f793943a":"plt.figure(figsize=(10, 7))\nsns.countplot(x='Type', data=train_df, hue=\"AdoptionSpeed\").set(xticklabels=['Dog', 'Cat'])\nplt.title('Type\/AdoptionSpeed(train)', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","4fde59c9":"plt.figure(figsize=(10, 7))\nsns.countplot(x='Gender', data=train_df, hue=\"AdoptionSpeed\")\nplt.title('Gender\/AdoptionSpeed(train)', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","0830b9f4":"train_df['Mixed_Breed'] = train_df.apply(lambda x: 0 if x.Breed2==0 and x.Breed1!=307 else 1, axis=1)\ntrain_df['Num_Color'] = train_df.apply(lambda x:  3-sum([y==0 for y in [x.Color1, x.Color2, x.Color3]]), axis=1)\ntrain_df['Description'].fillna(\"\", inplace=True)\ntrain_df['Description_Length'] = train_df.Description.map(len)","15de2aad":"test_df['Mixed_Breed'] = test_df.apply(lambda x: 0 if x.Breed2==0 and x.Breed1!=307 else 1, axis=1)\ntest_df['Num_Color'] = test_df.apply(lambda x:  3-sum([y==0 for y in [x.Color1, x.Color2, x.Color3]]), axis=1)\ntest_df['Description'].fillna(\"\", inplace=True)\ntest_df['Description_Length'] = test_df.Description.map(len)","b1d826d3":"plt.figure(figsize=(10, 7))\nsns.countplot(x='Mixed_Breed', data=train_df, hue=\"AdoptionSpeed\")\nplt.title('Mixed_Breed\/AdoptionSpeed(train)', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","4d41debf":"plt.figure(figsize=(10, 7))\nsns.countplot(x='Num_Color', data=train_df, hue=\"AdoptionSpeed\")\nplt.title('Num_Color\/AdoptionSpeed(train)', fontsize=18)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)","1faffa12":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.distplot(train_df['Age'], fit=norm)\n\nplt.subplot(122)\nres = stats.probplot(train_df['Age'], plot=plt)\n\ndisplay(train_df['Age'].skew())","93c5187a":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.distplot((train_df['PhotoAmt']), fit=norm)\n\nplt.subplot(122)\nres = stats.probplot((train_df['PhotoAmt']), plot=plt)\n\ndisplay((train_df['PhotoAmt']).skew())","75352217":"plt.figure(figsize=(15,7))\nplt.subplot(121)\nsns.distplot((train_df['Description_Length']), fit=norm)\n\nplt.subplot(122)\nres = stats.probplot((train_df['Description_Length']), plot=plt)\n\ndisplay((train_df['Description_Length']).skew())","5d368b63":"import json\n#getting description sentiment analyses\n    \ndef get_desc_anly(type, recalc):\n    if recalc == 1:\n        if type == \"train\":\n            path = \"..\/input\/train_sentiment\/\"#..\/input\/train_sentiment\/\n        elif type == \"test\":\n            path = \"..\/input\/test_sentiment\/\"#..\/input\/test_sentiment\/\n        print(\"Getting description sentiment analysis for\",type+\"_sentiment.csv\")\n        files = [f for f in os.listdir(path) if (f.endswith('.json') & os.path.isfile(path+f))]\n\n        df = pd.DataFrame(columns=[\"PetID\", \"DescScore\", \"DescMagnitude\"])\n        i = 0\n        for f in files:\n            #print(path + f)\n            with open(path+f, encoding=\"utf8\") as json_data:\n                data = json.load(json_data)\n            \n            df.loc[i]= [f[:-5],data[\"documentSentiment\"][\"score\"],data[\"documentSentiment\"][\"magnitude\"]]\n            i = i+1\n        df.to_csv(type+\"_sentiment.csv\", index=False)\n    elif recalc == 0:\n        df = pd.read_csv(type+\"_sentiment.csv\")\n    return df\n\n    \ntrain_snt = get_desc_anly(\"train\", 1)\ntest_snt = get_desc_anly(\"test\", 1)\n    \ntrain_df = train_df.set_index(\"PetID\").join(train_snt.set_index(\"PetID\")).reset_index()\ntest_df = test_df.set_index(\"PetID\").join(test_snt.set_index(\"PetID\")).reset_index()\n\ntrain_df[\"DescScore\"].fillna(0, inplace=True)\ntrain_df[\"DescMagnitude\"].fillna(0, inplace=True)\n\ntrain_df[\"Name\"].fillna(\"none\", inplace=True)\ntest_df[\"DescScore\"].fillna(0, inplace=True)\ntest_df[\"DescMagnitude\"].fillna(0, inplace=True)\n\ntest_df[\"Name\"].fillna(\"\", inplace=True)\n\ntrain_df[\"NameLength\"] = train_df[\"Name\"].str.len()\ntest_df[\"NameLength\"] = test_df[\"Name\"].str.len()\n","53fa2f3a":"vertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('..\/input\/train_metadata\/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain_df.loc[:, 'vertex_x'] = vertex_xs\ntrain_df.loc[:, 'vertex_y'] = vertex_ys\ntrain_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain_df.loc[:, 'dominant_blue'] = dominant_blues\ntrain_df.loc[:, 'dominant_green'] = dominant_greens\ntrain_df.loc[:, 'dominant_red'] = dominant_reds\ntrain_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain_df.loc[:, 'dominant_score'] = dominant_scores\ntrain_df.loc[:, 'label_description'] = label_descriptions\ntrain_df.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('..\/input\/test_metadata\/' + pet + '-1.json', 'r') as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest_df.loc[:, 'vertex_x'] = vertex_xs\ntest_df.loc[:, 'vertex_y'] = vertex_ys\ntest_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntest_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest_df.loc[:, 'dominant_blue'] = dominant_blues\ntest_df.loc[:, 'dominant_green'] = dominant_greens\ntest_df.loc[:, 'dominant_red'] = dominant_reds\ntest_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest_df.loc[:, 'dominant_score'] = dominant_scores\ntest_df.loc[:, 'label_description'] = label_descriptions\ntest_df.loc[:, 'label_score'] = label_scores","1f764376":"train_df_num = train_df.drop(columns = train_df.dtypes[train_df.dtypes=='object'].index)","83873aea":"test_df_num = test_df.drop(columns = test_df.dtypes[test_df.dtypes=='object'].index)","337251bd":"train_df_num.head()","27e36092":"train_df_num.shape","e5066a51":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, cohen_kappa_score, make_scorer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle","aadb5b38":"x = train_df_num.drop(columns = 'AdoptionSpeed')\ny = train_df_num.AdoptionSpeed","a1466b2b":"x_tst = test_df_num","45ca6a42":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0, shuffle=True)","426b09ab":"X_train.shape, X_test.shape, x_tst.shape","6b4076b8":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,\\\n                            ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nRANDOM_STATE = 0","d14ef879":"# some classifiers for test\nclassifiers = [KNeighborsClassifier(n_jobs=-1),\n               \n               LogisticRegression(),\n               LogisticRegression(C=0.005, penalty='l1', class_weight=None, fit_intercept=False, max_iter=100, tol=0.01),\n               LogisticRegression(C=0.0001, penalty='l2', class_weight=None, fit_intercept=True, max_iter=150, tol=1e-06),\n               LogisticRegression(C=96, penalty='l1', class_weight='balanced', fit_intercept=True, max_iter=50, tol=0.0001),\n               LogisticRegression(C=54, penalty='l2', class_weight='balanced', fit_intercept=False, max_iter=450, tol=0.001),\n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.005, max_depth=12, max_features=0.8, min_samples_leaf=2, subsample=0.2),\n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.01, max_depth=5, max_features=0.6, min_samples_leaf=10, subsample=0.8),\n               GradientBoostingClassifier(n_estimators=200, learning_rate=0.001, max_depth=90, max_features=0.5, min_samples_leaf=20, subsample=0.2),\n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.01, max_depth=20, max_features=0.6, min_samples_leaf=24, subsample=0.7),\n               GradientBoostingClassifier(n_estimators=300, learning_rate=0.01, max_depth=90, max_features=0.5, min_samples_leaf=20, subsample=0.2),\n               RandomForestClassifier(n_jobs=-1),\n               RandomForestClassifier(n_estimators=225, bootstrap=True, max_depth=83, max_features='auto', min_samples_leaf=5, min_samples_split=5,  n_jobs=-1),\n               RandomForestClassifier(criterion='gini', n_estimators=100, min_samples_split=12, min_samples_leaf=1, oob_score=True, n_jobs=-1),\n               ExtraTreesClassifier(n_jobs=-1),\n               ExtraTreesClassifier(n_estimators=200, bootstrap=False, max_depth=80, max_features='auto', min_samples_leaf=6, min_samples_split=5, n_jobs=-1),\n               AdaBoostClassifier(),\n               AdaBoostClassifier(n_estimators=225, algorithm='SAMME.R', learning_rate=0.2),\n               DecisionTreeClassifier(),\n               DecisionTreeClassifier(criterion='entropy', max_depth=110, max_features='auto', min_samples_leaf=6, min_samples_split=330)\n               ]\nclassifiers_names = ['knn1',\n                     'lr1', 'lr2', 'lr3', 'lr4', 'lr5',\n                     'gb1', 'gb2', 'gb3', 'gb4', 'gb5', \n                     'rf1', 'rf2', 'rf3',\n                     'et1', 'et2', \n                     'adb1', 'adb2',\n                     'dt1','dt2',\n                    ]","297d22da":"classifiers_predictions = pd.DataFrame()\nfor name, classifier in zip(classifiers_names, classifiers):\n    classifier.fit(X_train, y_train)\n    train_predictions = pd.Series(classifier.predict(X_train))\n    test_predictions = classifier.predict(X_test)\n    \n    classifiers_predictions[name] = test_predictions\n    print('{0}: ({1} - {2})'.format(name,\n                                    cohen_kappa_score(y_train, train_predictions, weights='quadratic') ,\n                                    cohen_kappa_score(y_test, test_predictions, weights='quadratic')))","2f465cd4":"classifiers_predictions = classifiers_predictions[['knn1', \n                     'lr1', 'lr2', 'lr3', 'lr4', 'lr5',\n                     'gb1', 'gb2', 'gb3', 'gb4', 'gb5', \n                     'rf1', 'rf2', 'rf3',\n                     'et1', 'et2', \n                     'adb1', 'adb2',\n                     'dt1','dt2',\n                    ]]","a210d0a5":"sns.heatmap(classifiers_predictions.corr(), linewidths=.5);\nplt.yticks(rotation=0);\nplt.xticks(rotation=30);\nsns.set(font_scale=2)","5427224d":"# some classifiers for test\nclassifiers = [\n               LogisticRegression(C=96, penalty='l1', class_weight='balanced', fit_intercept=True, max_iter=50, tol=0.0001),\n               \n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.005, max_depth=12, max_features=0.8, min_samples_leaf=2, subsample=0.2),\n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.01, max_depth=5, max_features=0.6, min_samples_leaf=10, subsample=0.8),\n               GradientBoostingClassifier(n_estimators=200, learning_rate=0.001, max_depth=90, max_features=0.5, min_samples_leaf=20, subsample=0.2),\n               GradientBoostingClassifier(n_estimators=50, learning_rate=0.01, max_depth=20, max_features=0.6, min_samples_leaf=24, subsample=0.7),\n               RandomForestClassifier(n_estimators=225, bootstrap=True, max_depth=83, max_features='auto', min_samples_leaf=5, min_samples_split=5,  n_jobs=-1),\n               RandomForestClassifier(criterion='gini', n_estimators=100, min_samples_split=12, min_samples_leaf=1, oob_score=True, n_jobs=-1),\n               ExtraTreesClassifier(n_estimators=200, bootstrap=False, max_depth=80, max_features='auto', min_samples_leaf=6, min_samples_split=5, n_jobs=-1),\n               AdaBoostClassifier(),\n               AdaBoostClassifier(n_estimators=225, algorithm='SAMME.R', learning_rate=0.2),\n               DecisionTreeClassifier(criterion='entropy', max_depth=110, max_features='auto', min_samples_leaf=6, min_samples_split=330)\n               ]\nclassifiers_names = ['lr4', \n                     'gb1','gb2', 'gb3', 'gb4', \n                     'rf2', 'rf3',\n                     'et2',  \n                     'adb1','adb2',\n                     'dt2',\n                    ]","0df71e00":"def simple_blending(basic_algorithms, meta_algorithm, X_train, X_test, y_train, test_df, part1_ratio=0.9, random_state=None):\n    tr = pd.DataFrame()\n    tst = pd.DataFrame()\n    y = pd.DataFrame()\n    X_train_part1, X_train_part2,\\\n    y_train_part1, y_train_part2 = train_test_split(X_train, y_train, test_size=1-part1_ratio, random_state=random_state)\n    \n    for index, basic_algorithm in enumerate(basic_algorithms):\n        #print(index)\n        basic_algorithm.fit(X_train_part1, y_train_part1)\n\n        part2_predictions = basic_algorithm.predict(X_train_part2)\n        tr[index] = part2_predictions\n\n        test_predictions = basic_algorithm.predict(X_test)\n        tst[index] = test_predictions\n        \n        test_pred = basic_algorithm.predict(test_df)\n        y[index] = test_pred\n        \n    meta_algorithm.fit(tr, y_train_part2)\n    \n    return meta_algorithm.predict(tst), meta_algorithm.predict(y)","5e36abc3":"r = pd.DataFrame()\nexperiments = list()\nfor i in range(1, 10):\n    simple_blending_predictions, result = simple_blending(classifiers,\n                                              LogisticRegression(C=5, random_state=RANDOM_STATE),\n                                              X_train, X_test, y_train, x_tst,\n                                              part1_ratio=0.9,\n                                              random_state=i)\n    r[i] = result\n    #print(simple_blending_predictions)\n    print(cohen_kappa_score(y_test, simple_blending_predictions, weights='quadratic'))\n    experiments.append(cohen_kappa_score( y_test, simple_blending_predictions, weights='quadratic'))\nprint('mean kappa: {0}\\nstd: {1}'.format(round(np.mean(experiments), 4), round(np.std(experiments), 5)))","8453e130":"r['avg'] = r.mean(axis=1).round(0).astype(int)\nr.avg.head()","61c93d38":"def simple_blending_features(basic_algorithms, meta_algorithm, X_train, X_test, y_train, test_df, part1_ratio=0.5, random_state=None):\n    tr = pd.DataFrame()\n    tst = pd.DataFrame()\n    y = pd.DataFrame()\n    \n    X_train_part1, X_train_part2,\\\n    y_train_part1, y_train_part2 = train_test_split(X_train, y_train, test_size=1-part1_ratio, random_state=random_state)\n    \n    tr = tr.append(X_train_part2)\n    tst = tst.append(X_test)\n    y = y.append(test_df)\n    \n    for index, basic_algorithm in enumerate(basic_algorithms):\n        #print(index)\n        basic_algorithm.fit(X_train_part1, y_train_part1)\n\n        part2_predictions = basic_algorithm.predict(X_train_part2)\n        tr[index] = part2_predictions\n\n        test_predictions = basic_algorithm.predict(X_test)\n        tst[index] = test_predictions\n        \n        test_pred = basic_algorithm.predict(test_df)\n        y[index] = test_pred\n    \n    meta_algorithm.fit(tr, y_train_part2)\n\n    return meta_algorithm.predict(tst), meta_algorithm.predict(y)","3b2573e6":"r2 = pd.DataFrame()\nexperiments = list()\nfor i in range(1, 10):\n    simple_blending_features_predictions, result2 = simple_blending_features(classifiers,\n                                              LogisticRegression(C=5, random_state=RANDOM_STATE),\n                                              X_train, X_test, y_train, x_tst,\n                                              part1_ratio=0.9,\n                                              random_state=i)\n    r2[i] = result2\n    #print(simple_blending_predictions)\n    print(cohen_kappa_score(y_test, simple_blending_features_predictions, weights='quadratic'))\n    experiments.append(cohen_kappa_score( y_test, simple_blending_features_predictions, weights='quadratic'))\nprint('mean kappa: {0}\\nstd: {1}'.format(round(np.mean(experiments), 4), round(np.std(experiments), 5)))","2fdf1ac0":"r2['avg'] = r2.mean(axis=1).round(0).astype(int)\nr2.avg.head()","0f723b4e":"from pandas.util.testing import assert_series_equal\nassert_series_equal(r['avg'], r2['avg'])","c7ad71d1":"submission = pd.DataFrame(data={'PetID' : test_df['PetID'], 'AdoptionSpeed' : r['avg']})\nsubmission.to_csv('submission.csv', index=False)\n","6e7e35c3":"# check submission\nsubmission.head(5)","b53dc124":"# Plot 1\nplt.figure(figsize=(15,4))\nplt.subplot(211)\ntrain_df['AdoptionSpeed'].value_counts().sort_index(ascending=False).plot(kind='barh')\nplt.title('Target Variable distribution in training set')\n\n# Plot 2\nplt.subplot(212)\nsubmission['AdoptionSpeed'].value_counts().sort_index(ascending=False).plot(kind='barh')\nplt.title('Target Variable distribution in predictions')\n\nplt.subplots_adjust(top=2)","c2eec1b6":"### ** in work**\n","c39e4c75":"### AdoptionSpeed  \nContestants are required to predict this value. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way:   \n0 - Pet was adopted on the same day as it was listed.   \n1 - Pet was adopted between 1 and 7 days (1st week) after being listed.   \n2 - Pet was adopted between 8 and 30 days (1st month) after being listed.   \n3 - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.  \n4 - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).","c85299ba":"### Data Fields  \n* PetID - Unique hash ID of pet profile\n* AdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict. See below section for more info.\n* Type - Type of animal (1 = Dog, 2 = Cat)\n* Name - Name of pet (Empty if not named)\n* Age - Age of pet when listed, in months\n* Breed1 - Primary breed of pet (Refer to BreedLabels dictionary)\n* Breed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary)\n* Gender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets)\n* Color1 - Color 1 of pet (Refer to ColorLabels dictionary)\n* Color2 - Color 2 of pet (Refer to ColorLabels dictionary)\n* Color3 - Color 3 of pet (Refer to ColorLabels dictionary)\n* MaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified)\n* FurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified)\n* Vaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure)\n* Dewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure)\n* Sterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure)\n* Health - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified)\n* Quantity - Number of pets represented in profile\n* Fee - Adoption fee (0 = Free)\n* State - State location in Malaysia (Refer to StateLabels dictionary)\n* RescuerID - Unique hash ID of rescuer\n* VideoAmt - Total uploaded videos for this pet\n* PhotoAmt - Total uploaded photos for this pet\n* Description - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese."}}