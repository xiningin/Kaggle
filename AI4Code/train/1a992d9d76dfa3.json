{"cell_type":{"6d0568c3":"code","4a0db359":"code","8b131cf7":"code","4fd32856":"code","6bd45ef4":"code","a610c2a9":"code","ca3eb9fa":"code","b2b1b0e8":"code","5794bae3":"code","0050c2ea":"code","71ce0e42":"code","e789d3dc":"code","fa3d8d4a":"code","863e01e4":"code","775fd40d":"code","e1eacf12":"code","adc20f94":"code","67835d31":"code","7fba79c3":"code","f0570054":"code","04fdf81d":"code","7549dc08":"code","c763a224":"code","63cd82da":"code","6923b80b":"code","028b6568":"code","8eee0e58":"code","4a1480c9":"code","f2cb5226":"code","cce25c44":"code","dff4e49f":"code","eb418c63":"code","8303ebe4":"code","2e18e467":"code","51e1e7e6":"code","bb5b2bea":"code","bf7098ab":"code","1eea9aaa":"code","ae62441a":"code","8dca75ff":"code","77862e3f":"code","c243c3b9":"code","6b2facc3":"code","37ba50c2":"code","76a25b32":"code","eb0206a6":"code","2cebd49f":"code","a274c3ae":"code","cd6844d8":"code","bec129cc":"code","5e151b64":"code","f6923a97":"code","f7f81dab":"code","06bb27aa":"code","dda52d1e":"code","9cfc8b3a":"code","43c0f1f7":"code","d9367a74":"code","f7d0c815":"code","03f6eec0":"code","e90582e0":"code","fcf69e37":"code","2476d44b":"code","247dd17d":"code","6d5b517d":"code","772b65fa":"code","24d50cfa":"code","1f08d52e":"code","a4e0d7d4":"code","e9fd4212":"code","179f71c1":"code","e3fc6e7e":"code","21986919":"code","4a22e3e1":"code","046491f2":"code","c298be77":"code","bc28c14e":"code","24aa8ad2":"code","6a227b41":"code","53fea5da":"code","79d00d43":"code","b701011c":"code","baa1c8ff":"code","8025fc43":"code","7ce6dcaf":"markdown","c6f29d45":"markdown","3ea70908":"markdown","5cbd0040":"markdown","516f0b0d":"markdown","46da0489":"markdown","e8b205a0":"markdown","3570c2b1":"markdown","fbcc845a":"markdown","00fbd2db":"markdown","0667c7d3":"markdown","f7a88b3a":"markdown","3689a948":"markdown","5cbdf30f":"markdown","d25a4536":"markdown","7da0981e":"markdown","6fffa9b0":"markdown","885e2f0e":"markdown","9e638976":"markdown","e425fbc3":"markdown"},"source":{"6d0568c3":"# 22p22c0589_Naratip_W2H2_27092020","4a0db359":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b131cf7":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test","4fd32856":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ndf","6bd45ef4":"df_age = df[['Age','Survived']]\ndf_age['Age'] = df_age['Age'].apply(np.ceil)\n\ndf_fare = df[['Fare','Survived','Pclass']]\ndf_fare['Fare'] = df_fare['Fare'].apply(np.ceil)","a610c2a9":"df_age","ca3eb9fa":"# !pip install seaborn --upgrade","b2b1b0e8":"# !pip list","5794bae3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# plt.figure(figsize=(14,12))\n# sns.distplot(df_age,label = 'Survived',color=\"green\")\n\nsurvived = 'survived'\nnot_survived = 'not survived'\n\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(22, 8))\nax = sns.distplot(df_age[df_age['Survived']==1].Age, ax = axes[0], bins=18, label = survived, kde =False, color=\"green\")\nax = sns.distplot(df_age[df_age['Survived']==0].Age, ax= axes [0], bins=40, label = not_survived, kde =False, color=\"red\")\nax.legend()\nax.set_title('age')\n\nax = sns.distplot(df_age[df_age['Survived']==1].Age.dropna(), ax = axes[1],bins = 80, label = survived, kde =False, color=\"green\")\nax = sns.distplot(df_age[df_age['Survived']==0].Age.dropna(), ax= axes [1],bins = 80, label = not_survived, kde =False, color=\"red\")\nax.legend()\nax.set_title('age')","0050c2ea":"df_fin = pd.concat([df, df_test],axis = 0).reset_index()\ndf_fin","71ce0e42":"# df_fin.info()","e789d3dc":"# Clean data\ndf_fin = df_fin[['Name','PassengerId','Survived', 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\ndf_fin['Age'] = df_fin['Age'].fillna(df_fin['Age'].mode()[0])\ndf_fin['Embarked'] = df_fin['Embarked'].fillna(df_fin['Embarked'].mode()[0])\ndf_fin['Fare'] = df_fin['Fare'].fillna(df_fin[(df_fin['Pclass'] == 3)].Fare.mean())","fa3d8d4a":"df_fin[(df_fin['Fare'].isnull())]","863e01e4":"df_fin[(df_fin['Pclass'] == 3)& (df_fin['PassengerId'] == 1044)]","775fd40d":"df_fin['Fare']","e1eacf12":"df_fin.info()","adc20f94":"# df_fin.sort_values(by=['Age'])","67835d31":"# x = df_fin[\"Age\"].value_counts().reset_index().sort_values(by=['index'])","7fba79c3":"# x","f0570054":"# import seaborn as sns\n# ax = sns.barplot(x=\"index\", y=\"Age\", data=x)","04fdf81d":"df_fin['family_size'] = df_fin['SibSp'] + df_fin['Parch'] + 1\n\ndf_fin['Alone'] = df_fin['family_size'].apply(lambda x: 0 if x > 1 else 1)\ndf_fin['not_child'] = df_fin['Age'].apply(lambda x: 0 if x <= 16 else 1)\n\n#replace age group\ndf_fin[\"age_group\"] = df_fin[\"Age\"]\ndf_fin[\"age_group\"] = df_fin[\"age_group\"].replace(\n    [df_fin[(df_fin[\"age_group\"] <=16)&(df_fin[\"age_group\"] <= 16)].Age.unique().tolist()], 0) #replace to condition to 0\n\ndf_fin[\"age_group\"] = df_fin[\"age_group\"].replace(\n    [df_fin[(df_fin[\"age_group\"] >16)&(df_fin[\"age_group\"] <= 36)].Age.unique().tolist()], 1) #replace to condition to 1\n\ndf_fin[\"age_group\"] = df_fin[\"age_group\"].replace(\n    [df_fin[(df_fin[\"age_group\"] >36)&(df_fin[\"age_group\"] <= 56)].Age.unique().tolist()], 2) #replace to condition to 2\n\ndf_fin[\"age_group\"] = df_fin[\"age_group\"].replace(\n    [df_fin[(df_fin[\"age_group\"] >56)].Age.unique().tolist()], 3) #replace to condition to 3\n\n\n\ndf_fin['fareperage'] = df_fin['Fare']\/df_fin['Age']\ndf_fin['fareperPclass'] = df_fin['Fare']\/df_fin['Pclass']\ndf_fin['fareMinusPclass'] = df_fin['Fare']*df_fin['Pclass']\ndf_fin\n","7549dc08":"# df_test  = df_fin\n# df_test[\"Age\"] = df_test[\"Age\"].replace(\n#     [df_test[(df_test[\"Age\"] >21)&(df_test[\"Age\"] < 23)].Age.unique().tolist()], 1) #replace to condition to 1","c763a224":"# df_test\n# df_test[(df_test[\"Age\"] >21)&(df_test[\"Age\"] < 23)].Age.unique().tolist()","63cd82da":"# df_fin[['Name']].head(20)","6923b80b":"df_fin['title'] = df_fin['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ndf_fin.title.unique()","028b6568":"df_fin[\"title\"] = df_fin[\"title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf_fin[\"title\"] = df_fin[\"title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndf_fin[\"title\"] = df_fin[\"title\"].astype(int)","8eee0e58":"df_fin","4a1480c9":"df_fin.info()","f2cb5226":"#One hot encoding\n# df = pd.get_dummies(df, columns=['Sex', 'Embarked'],drop_first = False)\ndf_fin = pd.get_dummies(df_fin, columns=['title', 'Sex', 'Embarked'],drop_first = False)\ndf_fin","cce25c44":"list(df_fin)","dff4e49f":"# split data 2 column for eazy to nor\n\ndf_norm = df_fin[[\n#     'Name',\n#  'PassengerId',\n#  'Survived',\n 'Pclass',\n 'Age',\n 'SibSp',\n 'Parch',\n 'Fare',\n 'family_size',\n#  'Alone',\n#  'not_child',\n#  'age_group',\n 'fareperage',\n 'fareperPclass',\n 'fareMinusPclass',\n#  'title_0',\n#  'title_1',\n#  'title_2',\n#  'title_3',\n#  'Sex_female',\n#  'Sex_male',\n#  'Embarked_C',\n#  'Embarked_Q',\n#  'Embarked_S'\n]]\n\ndf_bi = df_fin[[ # have ID, label\n#     'Name',\n 'PassengerId',\n 'Survived',\n#  'Pclass',\n#  'Age',\n#  'SibSp',\n#  'Parch',\n#  'Fare',\n#  'family_size',\n 'Alone',\n 'not_child',\n 'age_group',\n#  'fareperage',\n#  'fareperPclass',\n#  'fareMinusPclass',\n 'title_0',\n 'title_1',\n 'title_2',\n 'title_3',\n 'Sex_female',\n 'Sex_male',\n 'Embarked_C',\n 'Embarked_Q',\n 'Embarked_S'\n]]","eb418c63":"# Standardization in some coulumn\nfrom sklearn import preprocessing\n\ncols = list(df_norm)\npt = preprocessing.PowerTransformer(method='yeo-johnson', standardize=True) # support only positive value\nmat = pt.fit_transform(df_norm[cols])\n\nX = pd.DataFrame(mat, columns=cols)\n\n# comnine data that do Standardization with data that do one hot encode\nX = pd.concat([X, df_bi], axis=1)","8303ebe4":"X","2e18e467":"df_test = X[(X['PassengerId'] >= 892)]\ndf_train = X[(X['PassengerId'] <= 891)]\n","51e1e7e6":"import seaborn as sns\n# sns.heatmap(df_train.corr())\nwith pd.option_context('display.max_rows', 30, 'display.max_columns', 30): \n    display(df_train.corr().Survived)","bb5b2bea":"list(df_train)","bf7098ab":"data = df_train[['Pclass',\n 'Age',\n 'SibSp',\n 'Parch',\n 'Fare',\n 'family_size',\n 'fareperage',\n 'fareperPclass',\n 'fareMinusPclass',\n#  'PassengerId',\n#  'Survived',\n 'Alone',\n 'not_child',\n 'age_group',\n 'title_0',\n 'title_1',\n 'title_2',\n 'title_3',\n 'Sex_female',\n 'Sex_male',\n 'Embarked_C',\n 'Embarked_Q',\n 'Embarked_S']\n               ]\ntarget = df_train[['Survived']]\n\nprint(data.shape, target.shape)","1eea9aaa":"from sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.30, random_state = 40) \nY_test = np.array(Y_test)\n\nprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)","ae62441a":"from sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\nfrom sklearn.metrics import precision_score, recall_score\n\ndef evaluation(y_true, y_pred):\n    l = len(y_pred)\n    tp = 0\n    fp = 0\n    tn = 0\n    fn = 0\n\n    for i in range (l):\n        if y_pred[i] == y_true[i] and y_pred[i] == 1 : #model 1 real 1\n            tp += 1\n        elif y_pred[i] != y_true[i] and y_pred[i] == 1 : #model 1 real 0\n            fp +=1\n\n        elif y_pred[i] == y_true[i] and y_pred[i] == 0: #model 0 real 1\n            tn += 1\n        elif y_pred[i] != y_true[i] and y_pred[i] == 0: #model 0 real 0\n            fn +=1  \n\n    p = tp \/ (tp+fp)\n    r = tp\/(tp+fn)\n    f1 = 2*p*r\/(p+r)\n    \n    print(\"precision : {0}, recall : {1}, f1 : {2}\".format(p,r,f1))\n\n    return f1","8dca75ff":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nresult = model.fit(X_train,Y_train)\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))\n","77862e3f":"from sklearn.naive_bayes import GaussianNB\n\nbmodel = GaussianNB()\nresult = bmodel.fit(X_train,Y_train)\ny_pred = bmodel.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))\n","c243c3b9":"from sklearn.neural_network import MLPClassifier\nmlp_model = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n\nresult = mlp_model.fit(X_train,Y_train)\ny_pred = mlp_model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))\n","6b2facc3":"import catboost\nfrom catboost import CatBoostClassifier, Pool","37ba50c2":"best_params = {\n#     'bagging_temperature': 0.8,\n#                'depth': 10,\n               'iterations': 50000,\n#                'l2_leaf_reg': 30,\n               'learning_rate': 0.0005,\n               'random_strength': 0.8\n              }\n\nmodel = CatBoostClassifier(\n        **best_params,\n#         loss_function='MultiClassOneVsAll',\n#         eval_metric='AUC',         \n#         task_type=\"GPU\",\n#         auto_class_weights = 'Balanced',\n        nan_mode='Min',\n        verbose=False,\n        \n)\n\nmodel.fit(\n        X_train, Y_train,\n        verbose_eval=1000, \n        early_stopping_rounds=1000,\n        eval_set=(X_test, Y_test),\n        use_best_model=False,\n#         plot=True\n)\n\nprint(model)","76a25b32":"model = CatBoostClassifier()\nmodel.fit(X_train, Y_train,eval_set=(X_test, Y_test))\n","eb0206a6":"# Grid search find tune parameter\n# model = CatBoostClassifier(\n# #     task_type=\"GPU\"\n# )\n\n# grid = {'learning_rate': [0.03, 0.1],\n#         'depth': [4, 6, 10],\n#         'l2_leaf_reg': [1, 3, 5, 7, 9]\n#        }\n       \n\n# randomized_search_result = model.randomized_search(grid,\n#                                                    X=X_train,\n#                                                    y=Y_train,\n#                                                    plot=True)","2cebd49f":"# y_pred = model.predict(X_test)\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))\n# y_pred","a274c3ae":"# y_pred","cd6844d8":"import shap\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(Pool(X_train, Y_train))\nshap.summary_plot(shap_values, X_train, plot_type=\"bar\")","bec129cc":"import xgboost as xgb\nmodel = xgb.XGBClassifier()\n\nmodel.fit(X_train, Y_train, \n          eval_set=[(X_test, Y_test)],\n          early_stopping_rounds=40,\n         )\n# y_pred = model.predict(X_test)\n# y_pred = model.predict(X)","5e151b64":"\ny_pred = model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))","f6923a97":"# y_pred","f7f81dab":"# grid search\n\nfrom scipy.stats import uniform, randint\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\n\nparams = {\n    \"colsample_bytree\": uniform(0.7, 0.3),\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}\n\nsearch = RandomizedSearchCV(model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n\nsearch.fit(X_train, Y_train)\n\n# report_best_scores(search.cv_results_, 1)","06bb27aa":"y_pred = search.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))","dda52d1e":"feature_important = model.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.plot(kind='barh')","9cfc8b3a":"from lightgbm import LGBMClassifier\n\nmodel = LGBMClassifier()\n\nmodel.fit(X_train, Y_train)\ny_pred = model.predict(X_test)","43c0f1f7":"print(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))","d9367a74":"from sklearn.model_selection import KFold\nimport numpy as np\n\ndef evaluate_model(model, X_train, Y_train, X_test, Y_test):\n    \n    result = model.fit(X_train,Y_train)\n    y_pred = model.predict(X_test)\n    \n    print(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\n    \n    f_score = evaluation(Y_test, y_pred)\n    \n    return f_score\n\n\n\nkf = KFold(n_splits=5,random_state=None, shuffle=False)\ncount = 1\nf_measure = [['Decision Tree'],['Naive Bays'],['Neuron Network']]\n\n\n          \nfor train_index, test_index in kf.split(data):\n    print(test_index)\n    print(\"\\n\\n\\n\\nFold: {0}\\n\".format(count))\n    count = count + 1\n    f_score = 0\n    X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n    Y_train, Y_test = target.iloc[train_index], target.iloc[test_index]\n    \n    Y_test = np.array(Y_test)\n\n    \n    dmodel = DecisionTreeClassifier()\n    bmodel = GaussianNB()\n    mlp_model = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n    \n    \n    print(\"Decision Tree: \")\n    f_score = evaluate_model(dmodel, X_train, Y_train, X_test, Y_test)\n    f_measure[0].append(f_score)\n    \n    print(\"\\nNaive Bays:\")\n    f_score = evaluate_model(bmodel, X_train, Y_train, X_test, Y_test)\n    f_measure[1].append(f_score)\n    \n    print(\"\\nNeuron Network:\")\n    f_score = evaluate_model(mlp_model, X_train, Y_train, X_test, Y_test)\n    f_measure[2].append(f_score)","f7d0c815":"import numpy as np\nmeanf = []\nfor i in range(0,len(f_measure)):\n    meanf.append([f_measure[i][0], np.mean(f_measure[i][1:])])\n\npd.DataFrame(meanf,columns=['Predictor','f_measure_mean'])","03f6eec0":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport copy\n\n\ndef all_extract_feature():\n    \n    kf = KFold(n_splits=5,random_state=40,shuffle=True)\n#     idx = []\n#     all_new_feature = []\n    model_1st = []\n    \n    \n    model_name = [LGBMClassifier(),\n                  CatBoostClassifier(),\n                  xgb.XGBClassifier(),\n                  DecisionTreeClassifier(),\n                  GaussianNB(),\n                  MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)] # \n    \n    for i, model in enumerate(model_name):\n        all_new_feature = []\n        idx = []\n        for train_index, test_index in kf.split(data): # test_index = index from test data\n#             print(test_index)\n\n            X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n            Y_train, Y_test = target.iloc[train_index], target.iloc[test_index]\n\n#             model = CatBoostClassifier()\n            output = extract_feature_each_Fold(model ,X_train, X_test,Y_train, Y_test)\n    \n            mod = output[1]\n            new_feature = output[0]\n\n            all_new_feature = all_new_feature + new_feature.tolist()\n            idx = idx + test_index.tolist() #collect index of data\n\n        feature = pd.DataFrame(data = {'idx': idx, 'feature_pred'+str(i) : all_new_feature}) #create table that have index and output in each index\n        feature = feature.set_index(\"idx\")\n        \n        print(feature)\n        if i == 0:\n            ex_feature = feature\n            \n        else:\n       \n            ex_feature = pd.concat([ex_feature, feature], axis = 1, join='inner')\n            \n        model_1st.append(mod)\n        print(len(model_1st))\n    return ex_feature, model_1st\n\n\n\ndef extract_feature_each_Fold(model, X_train, X_test,Y_train, Y_test):\n    \n    result = model.fit(X_train,Y_train)\n    y_pred = model.predict(X_test)\n\n    return y_pred, model\n\n\noutput = all_extract_feature()\nnew_feature = output[0]\nfirst_model = output[1]\nnew_feature = new_feature.reset_index().sort_values(by=['idx'], ascending=True).set_index(\"idx\")","e90582e0":"# new_feature = new_feature.reset_index().sort_values(by=['idx'], ascending=True).set_index(\"idx\")\nnew_feature","fcf69e37":"# target","2476d44b":"from sklearn.model_selection import train_test_split, GridSearchCV\nX_train, X_test, Y_train, Y_test = train_test_split(new_feature, target, test_size=0.18, random_state = 2) \nY_test = np.array(Y_test)\n\nprint(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)","247dd17d":"X_test","6d5b517d":"from lightgbm import LGBMClassifier\n\nsecond_model = LGBMClassifier()\n\nsecond_model.fit(X_train, Y_train)\ny_pred = second_model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))","772b65fa":"import xgboost as xgb\nsecond_model = xgb.XGBClassifier()\n\nsecond_model.fit(X_train, Y_train, \n#           eval_set=[(X_test, Y_test)],\n#           early_stopping_rounds=40,\n         )\ny_pred = second_model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))","24d50cfa":"# from sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\n\nsecond_model = LogisticRegression()\nsecond_model.fit(X_train, Y_train)\n\ny_pred = second_model.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(Y_test, y_pred))\nprint(evaluation(Y_test, y_pred))\n","1f08d52e":"test_index = df_test[['PassengerId']].reset_index(drop=True)\ntest = df_test[list(data)].reset_index(drop=True)\ntest\n","a4e0d7d4":"test_index","e9fd4212":"pred = model.predict(test)\n# pred = search.predict(test)\n# pred","179f71c1":"submission_col =  [ 'Survived'\n              ] \n\nsubmission = pd.DataFrame(pred, columns=submission_col)\nsubmission","e3fc6e7e":"ans = pd.concat([test_index, submission], axis=1, join='inner')\nans['Survived'] = ans['Survived'].astype('int')\nans","21986919":"# aa = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n# aa","4a22e3e1":"ans.to_csv('light_GBM_boost.csv', index=  False)","046491f2":"list(data)","c298be77":"test_index = df_test[['PassengerId']].reset_index(drop=True)\ntest = df_test[list(data)].reset_index(drop=True)\ntest","bc28c14e":"test.info()","24aa8ad2":"first_model","6a227b41":"for idx, model_first in enumerate(first_model):\n    pred = model_first.predict(test)\n    pred = pred.tolist()\n    \n    \n    if idx == 0:\n        second_feature = pd.DataFrame(data = {'Pred' + str(idx): pred}) #create table that have index and output in each index\n\n    else:\n        pred_df = pd.DataFrame(data = {'Pred' + str(idx): pred})\n        second_feature = pd.concat([second_feature,pred_df], axis = 1, join='inner')\n\n        \n# second_feature\n# feature = pd.DataFrame(data = {'idx': idx, 'feature_pred'+str(i) : all_new_feature}) #create table that have index and output in each index\n#         feature = feature.set_index(\"idx\")\n        \n#         print(feature)\n#         if i == 0:\n#             ex_feature = feature\n            \n#         else:\n       \n#             ex_feature = pd.concat([ex_feature, feature], axis = 1, join='inner')\n# first_model\n# pred = first_model.predict(test)\nsecond_feature","53fea5da":"# new_feature = pd.DataFrame(pred, columns=['feature_pred'])\n# test = pd.concat([test,new_feature], axis = 1, join='inner')\n# test","79d00d43":"pred = second_model.predict(second_feature)\npred","b701011c":"submission_col =  ['Survived'] \n\nsubmission = pd.DataFrame(pred, columns=submission_col)\nans = pd.concat([test_index, submission], axis=1, join='inner')\nans['Survived'] = ans['Survived'].astype('int')\nans.to_csv('Stacking_1st-many_2nd-Logistic2.csv', index=  False)\n","baa1c8ff":"# ans.to_csv('stacking_model_catandligh.csv', index=  False)","8025fc43":"ans","7ce6dcaf":"# Choose feature\/label","c6f29d45":"logistic","3ea70908":"# Preprocessing","5cbd0040":"# Stacking layer model","516f0b0d":"xgboost","46da0489":"2nd model predict Y","e8b205a0":"# Model","3570c2b1":"# KFold crossvalidation","fbcc845a":"light gbm","00fbd2db":"Decision Tree","0667c7d3":"# Train\/Test Split","f7a88b3a":"\n# Real world data","3689a948":"Naive Bayes","5cbdf30f":"1st model extract new feature","d25a4536":"# Output average f-measure","7da0981e":"light","6fffa9b0":"xgboost","885e2f0e":"**Neuron Network**","9e638976":"Cat boost","e425fbc3":"# real world_ stacking method"}}