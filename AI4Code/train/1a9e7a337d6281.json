{"cell_type":{"97efa356":"code","6552403d":"code","3a2610e9":"code","64d4265d":"code","9fdfdb6c":"code","6bc40f18":"code","69e1da2c":"code","1f1f4b99":"code","5237fea6":"code","53b6bd3a":"code","f10ae1fd":"code","f8578fc5":"code","0b260d16":"code","4db0fbb1":"code","781703c0":"code","03c6112a":"code","71d5863c":"code","dd10e704":"code","0107c268":"code","7e9fc645":"code","1b8a381f":"code","d7649bd7":"markdown","5f19e393":"markdown","68fe9808":"markdown","cd831c10":"markdown","2803a270":"markdown","0fd2a798":"markdown","6209a2ce":"markdown","acb3626c":"markdown","2f8ae686":"markdown","ac1c58bc":"markdown"},"source":{"97efa356":"import csv\nimport gc\nimport glob\nimport json\nimport math\nimport operator\nimport os\nimport pprint\nimport re\nimport string\nimport time\nfrom collections import OrderedDict\nfrom math import floor\n\nimport joblib\nimport keras\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom gensim.models import KeyedVectors\nfrom keras import backend as K\nfrom keras import constraints, initializers, layers, optimizers, regularizers\nfrom keras.callbacks import *\nfrom keras.engine.topology import Layer\nfrom keras.layers import *\nfrom keras.layers import (GRU, LSTM, Activation, Add, Bidirectional,\n                          Concatenate, Conv1D, CuDNNGRU, CuDNNLSTM, Dense,\n                          Dropout, Embedding, Flatten, GlobalMaxPool1D, Input)\nfrom keras.models import Model\nfrom keras.optimizers import *\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom sklearn.metrics import log_loss\nfrom tqdm import tqdm\n\n%matplotlib inline","6552403d":"plt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')","3a2610e9":"files = sorted(glob.glob('..\/input\/gendered-pronoun-resolution\/*.tsv'))\npprint.pprint(files)\n\nsubmission = pd.read_csv('..\/input\/gendered-pronoun-resolution\/sample_submission_stage_1.csv')\nprint('submission head:')\nprint(submission.head())\n\n# Download data from GAP Google dataset\n# From there, -test and -validation will serve as training set\ngap_test = pd.read_csv('..\/input\/kaggle-gap\/gap-test.tsv', sep='\\t')\ngap_valid = pd.read_csv('..\/input\/kaggle-gap\/gap-validation.tsv', sep='\\t')\ntrain = pd.concat([gap_test, gap_valid], ignore_index=True, sort=False)\n\n# gap-development is public test set on Kaggle\ntest = pd.read_csv('..\/input\/kaggle-gap\/gap-development.tsv', sep='\\t')\n\nprint('\\ntrain shape: {}'.format(train.shape))\nprint('test shape: {}'.format(test.shape))","64d4265d":"train_samples = np.random.choice(train.Text.values, 15)\ntest_samples = np.random.choice(test.Text.values, 15)\n\nprint('\\ttrain samples inspection:\\n')\nfor i in range(len(train_samples)):\n    print('sample {}:\\n{}\\n'.format(i, train_samples[i]))\n\nprint('\\n\\ttest samples inspection:\\n')\nfor i in range(len(test_samples)):\n    print('sample {}:\\n{}\\n'.format(i, test_samples[i]))","9fdfdb6c":"train_lens = train.Text.apply(lambda x: len(x.split(' ')))\ntest_lens = test.Text.apply(lambda x: len(x.split(' ')))\n\nplt.hist(train_lens.values, bins=25, color='r')\nplt.hist(test_lens.values, bins=25, color='b')\nplt.title('train & test length distribution:')\nplt.show()","6bc40f18":"def build_vocab(sentences, verbose=True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable=(not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n\ndef check_coverage(vocab, embeddings_index):\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\n\n# Functions to clean the text:\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    return x\n\n\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef add_lower(embedding, vocab):\n    count = 0\n    for word in vocab:\n        if word in embedding and word.lower() not in embedding:  \n            embedding[word.lower()] = embedding[word]\n            count += 1\n    print(f\"Added {count} words to embedding\")","69e1da2c":"# First step is to concatenate train and test data to act on whole set during processing:\ndf_full = pd.concat([train, test], ignore_index=True, sort=False)","1f1f4b99":"# Load GoogleNews embedding\nnews_path = '..\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)","5237fea6":"# Initial trial:\nvocab = build_vocab(df_full.Text.apply(lambda x: x.split(' ')).tolist())\noov_words = check_coverage(vocab, embeddings_index)\n\n# When applied on raw text, we see that embeddings for only 48% of words are available.\n# This is too low, let's try to improve upon this!","53b6bd3a":"# Lowercase:\nvocab = build_vocab(df_full.Text.apply(lambda x: x.lower().split(' ')).tolist())\noov_words = check_coverage(vocab, embeddings_index)\n\n# After lowering the words, number of embeddings found has dropped.\n# For this embedding, words should be kept in their original case.","f10ae1fd":"# Clean punctuation:\nvocab = build_vocab(df_full.Text.apply(lambda x: clean_text(x).split(' ')).tolist())\noov_words = check_coverage(vocab, embeddings_index)\n\n# After cleaning punctuation, a significant gain is achieved, nice!","f8578fc5":"# Clean punctuation & numbers:\nvocab = build_vocab(df_full.Text.apply(lambda x: clean_numbers(clean_text(x)).split(' ')).tolist())\noov_words = check_coverage(vocab, embeddings_index)\n\n# When cleaning numbers, there is an additional subtle gain of almost 3%.\n# Not very much but still useful! \n# Now we have almost 93% of words covered, that's very good fraction.","0b260d16":"# Clean punctuation & numbers, in addition replace all non-alphanumeric characters:\nregex = re.compile('[^a-zA-Z]')\n\nsentences_clean = df_full.Text.apply(lambda x: clean_numbers(clean_text(x)).split(' ')).tolist()\nsentences_clean = list(map(lambda y: [regex.sub('', x) for x in y], sentences_clean))\n\nvocab = build_vocab(sentences_clean)\noov_words = check_coverage(vocab, embeddings_index)\n\n# Removing all non-alphanumeric characters does not bring an improvement.\n# Let's stick with the previous version, where punctuation and numbers are cleaned.","4db0fbb1":"# Final vocabulary:\nfinal_vocab = build_vocab(df_full.Text.apply(lambda x: clean_numbers(clean_text(x)).split(' ')).tolist())\n# oov_words = check_coverage(final_vocab, embeddings_index)\n\n\ndel embeddings_index\ngc.collect()","781703c0":"# From https:\/\/www.kaggle.com\/shujian\/single-rnn-with-4-folds-clr\ndef load_glove(word_index):\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix\n\n\nmax_features = 40000","03c6112a":"# Process embedding matrix once and save it.\n# Afterwards, is the file exists, matrix will be loaded instead of processed again.\n# Saves a lot of time when performing experiments, because embedding loading takes quite a while!\n\nif os.path.isfile('glove_embedding.joblib'):\n    embedding_matrix = joblib.load('glove_embedding.joblib')\n    print('embedding matrix loaded.')\nelse:\n    # Load embedding:\n    EMBEDDING_FILE = '..\/input\/glove-embeddings\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): \n        return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n    \n    embedding_matrix = load_glove(final_vocab)\n    joblib.dump(embedding_matrix, 'glove_embedding.joblib')\n    print('embedding matrix processed.')\n    del embeddings_index\n    gc.collect()\n\n\n# We set max_features to number equal to all words from vocabulary,\n# because the number isn't very high - less than 37k.\nmax_features = embedding_matrix.shape[0]\nembed_size = embedding_matrix.shape[1]\nprint('embedding matrix shape: {}'.format(embedding_matrix.shape))\nprint('max features: {}'.format(max_features))","71d5863c":"# https:\/\/www.kaggle.com\/keyit92\/coref-by-mlp-cnn-coattention\n# Function to create labels out of original df columns:\ndef _row_to_y(row):\n    if row.loc['A-coref']:\n        return 0\n    if row.loc['B-coref']:\n        return 1\n    return 2\n\n\n# Process data to tokenized form:\nmaxlen = 128\n\n\nX = df_full.Text.apply(lambda x: clean_numbers(clean_text(x)).split(' '))\n\n# max_features = 40000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X))\n\nX = tokenizer.texts_to_sequences(X)\n\nX_train = X[:train.shape[0]]\nX_valid = X[train.shape[0]:]\n\nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_valid = pad_sequences(X_valid, maxlen=maxlen)\n\ny_train = train.apply(_row_to_y, axis=1)\ny_valid = test.apply(_row_to_y, axis=1)\n\ny_train = to_categorical(y_train)\ny_valid = to_categorical(y_valid)\n\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)","dd10e704":"# https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-652-lb\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n\n\ndef LstmBasic(embedding_matrix, dropout=0.0):\n\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    # x = Embedding(max_features, embed_size)(inp)\n    x = SpatialDropout1D(0.1)(x)\n\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    # x = PReLU()(x)\n    identity = x\n    x = Dropout(dropout)(x)\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    # x = PReLU()(x)\n    x = Add()([x, identity])\n    identity = x\n    x = Dropout(dropout)(x)\n\n    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n    # x = PReLU()(x)\n    x = Add()([x, identity])\n    x = Attention(maxlen)(x)\n    x = Dropout(dropout)(x)\n\n    x = Dense(128)(x)\n    x = PReLU()(x)\n    x = Dense(3, activation=\"softmax\")(x)\n\n    model = Model(inputs=inp, outputs=x)\n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=optimizers.Adam(lr=1e-4),\n        metrics=['accuracy'])\n\n    return model","0107c268":"# Run parameters\nN_BAGS = 3\nN_EPOCHS = 5\nTO_MONITOR = 'val_loss'\nRUN_NAME = 'LstmAttention'\nmonitor_mode = 'min'\n\n\n# Create checkpoints dir\nif not os.path.isdir('.\/checkpoints\/'):\n    os.mkdir('.\/checkpoints\/')\n\n\nval_preds = []\n    \n# Train bagged model\nfor b in range(N_BAGS):\n    \n    print('\\trunning bag: {}'.format(b))\n    \n    # Initialize model:\n    model = LstmBasic(embedding_matrix)\n\n    # Set of training callbacks\n    ckpt_name = 'checkpoints\/{0}_bag_{1}.h5'.format(RUN_NAME, b)\n    logger = CSVLogger('.\/checkpoints\/{0}_bag_{1}.log'.format(RUN_NAME, b))\n\n    ckpt = ModelCheckpoint(ckpt_name, \n                           save_best_only=True,\n                           save_weights_only=True, \n                           verbose=1, \n                           monitor=TO_MONITOR, \n                           mode=monitor_mode)\n    reduce_lr = ReduceLROnPlateau(monitor=TO_MONITOR,\n                                  mode=monitor_mode, \n                                  patience=10)\n    early_stop = EarlyStopping(monitor=TO_MONITOR,\n                               mode=monitor_mode, \n                               patience=20)\n\n    history = model.fit(X_train, y_train, batch_size=256, epochs=N_EPOCHS, \n            validation_data=(X_valid, y_valid), verbose=0,\n            callbacks=[ckpt, reduce_lr, early_stop, logger])\n\n    val_loss_min = min(history.history['val_loss'])\n    print('best model val loss: {:.4f}'.format(val_loss_min))\n    \n    print('loading best weights')\n    model.load_weights(ckpt_name)\n    print('predict validation set with best weights')\n    val_pred = model.predict(X_valid, batch_size=256)\n    val_preds.append(val_pred)\n    \n\n# Average over bags:\nval_pred = np.mean(val_preds, axis=0)\nval_logloss = log_loss(y_valid, val_pred)\nprint('averaged logloss: {:.4f}'.format(val_logloss))","7e9fc645":"plt.hist(val_pred[:, 0], bins=50, color='r', label='A')\nplt.hist(val_pred[:, 1], bins=50, color='b', label='B')\nplt.hist(val_pred[:, 2], bins=50, color='g', label='Neither')\nplt.legend()\nplt.title('validation prediction distribution:')\nplt.show()","1b8a381f":"submission_ = submission.copy()\nsubmission_.iloc[:, 1:] = val_pred\nsubmission_.to_csv('lstm_baseline_loss_{:.4f}.csv'.format(val_logloss), index=False)","d7649bd7":"### create embedding matrix for model:\n\nNow, to switch things a little, we'll use GloVe embedding for model training.","5f19e393":"### explore pretrained embedding:\n\nNext step is exploration of pretrained embedding. I will make use of ideas and code containd in great [kernel by Dieter](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings) and also in another very useful [kernel by Theo Viel](https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing) and in some cases introducte subtle changes.","68fe9808":"### data:\n\nFor this competition, data is sourced from [GAP Dataset](https:\/\/github.com\/google-research-datasets\/gap-coreference).\nIt consists of three sets - development, test, validation. First one corresponds to stage 1 test set on Kaggle, so will be used for validation in our setting. This will effectively mimic public LB.\nModels will be trained on concatenated data from test and validation.\n\nAfter data is loaded, we can inspect number of samples.\nOur training set consists of 2454 examples and test set of 2000. This is a very small number of samples to train model properly. This will be a difficult problem!\n\nOne of key choices will be to make use of pretrained word embeddings.\nSet of those is available in [Quora dataset](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/data) in `embeddings.zip` file.\n\n### task:\n\nAccording to the competition description:\n\n_In this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither._\n\n\nSo this will be a multiclass problem, where we have to decide between 3 classes - A, B or neither.\nIt is reflected in submission, where for each sample a probability distribution over classes should be submitted.\n\nCompetition metric is _multi-class logarithmic loss_, which can be optimized directly.","cd831c10":"# LSTM with Attention - Keras","2803a270":"### examine prediction distribution:","0fd2a798":"### build LSTM model with Attention:","6209a2ce":"### inspect samples:","acb3626c":"### tokenize text:","2f8ae686":"### inspect length of sentences distribution:\n\nBased on the length histograms, it seems like the distributions are very similar between train and test.\nThis is a good sign, as significantly shorter or longer sentences in test could cause the models to generalize bad. ","ac1c58bc":"### generate submission:"}}