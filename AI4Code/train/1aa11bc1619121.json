{"cell_type":{"c17380c9":"code","28e8c99c":"code","2a322eb5":"code","545f23d2":"code","fbebc71b":"code","393194a8":"code","1c3eafd2":"code","0b927f6f":"code","64350253":"code","526f566f":"code","79a64cf2":"code","a91e039f":"code","31a4347a":"code","a96368da":"code","728e8cb3":"code","33d8ce6b":"code","cf66d789":"code","27edc957":"code","633756a4":"code","84f0a8a2":"code","87fcf738":"code","343bf20d":"code","fabce1a9":"code","bcd3b2a4":"code","cf47e6b9":"code","a2c74572":"code","67cc70c5":"code","59045976":"code","cd686e42":"code","9a46619a":"code","8016b970":"code","87284a7f":"code","163c42d7":"markdown","6f47adf4":"markdown","5c1f539c":"markdown","fb68d215":"markdown","b717fc51":"markdown","a6a2a303":"markdown","407e785b":"markdown","98c28ba2":"markdown","a140fe70":"markdown","835a20e5":"markdown","8a385e93":"markdown","56780cf2":"markdown","9c48ac88":"markdown","22d620e9":"markdown","c8e4b81b":"markdown","5339216a":"markdown","7c9669f5":"markdown","354b8a97":"markdown","897287bf":"markdown","27ba8211":"markdown","41007464":"markdown","204e01ef":"markdown"},"source":{"c17380c9":"%load_ext autoreload\n%autoreload 2\nimport os\n\n%matplotlib inline","28e8c99c":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom scipy.stats import norm, skew\n\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n#from sklearn.ensemble import RandomForestClassifier\n\nimport category_encoders as ce\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler, RobustScaler\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import roc_curve, auc\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox\n\nfrom datetime import datetime\n\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","2a322eb5":"!ls ..\/input","545f23d2":"PATH = \"..\/input\/cleaned-data\/\"","fbebc71b":"# Loading data cleaned at Step 1 - links can be find on top of notebook\ndf_train=pd.read_csv(f'{PATH}train_clean.csv')\ndf_test=pd.read_csv(f'{PATH}test_clean.csv')","393194a8":"print('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['SalePrice'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))","1c3eafd2":"#remember where to divide train and test\nntrain = df_train.shape[0]\nntest = df_test.shape[0]","0b927f6f":"def concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set on axis 0\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\ndf_all.shape","64350253":"#Dividing Target column (Y)\ny_train_full = df_train.SalePrice.values\ndf_all.drop(['SalePrice'], axis=1, inplace=True)\n#df_all.drop('Id',axis=1,inplace=True)","526f566f":"y_train_full","79a64cf2":"df_all.head()","a91e039f":"df_all.shape","31a4347a":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nskewness = df_all.select_dtypes(include=numerics).apply(lambda x: skew(x))\nskew_index = skewness[abs(skewness) >= 0.85].index\nskewness[skew_index].sort_values(ascending=False)","a96368da":"'''BoxCox Transform'''\nlam = 0.15\n\nfor column in skew_index:\n    df_all[column] = boxcox1p(df_all[column], lam)\n","728e8cb3":"# Evaluation after working with skewed data\n#evaluate(df_all)","33d8ce6b":"df_all=pd.get_dummies(df_all)","cf66d789":"df_all.shape","27edc957":"\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n# split Validational\/Test set from Training set after Categorical Value Engeneering\n#def original_train_test(df_all):\nX_test=df_all.iloc[ntrain:] # Test set\nX_train_full=df_all.iloc[:ntrain] # Train set\nX_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)","633756a4":"# Saving all features for future comparison.\nall_features = df_all.keys()\n# Removing features.\ndf_all = df_all.drop(df_all.loc[:,(df_all==0).sum()>=(df_all.shape[0]*0.984)],axis=1)\ndf_all = df_all.drop(df_all.loc[:,(df_all==1).sum()>=(df_all.shape[0]*0.984)],axis=1) \n# Getting and printing the remaining features.\nremain_features = df_all.keys()\nremov_features = [st for st in all_features if st not in remain_features]\nprint(len(remov_features), 'features were removed:', remov_features)","84f0a8a2":"# Evaluation after dropping not important features\n#evaluate(df_all)","87fcf738":"scaler = RobustScaler()\ndf_all = pd.DataFrame(scaler.fit_transform(df_all))","343bf20d":"# Evaluation after Normalization\n#evaluate(df_all)","fabce1a9":"\"\"\"Dividing working DataFrame back to Train and Test\"\"\"\n# split Validational\/Test set from Training set after Categorical Value Engeneering\n#def original_train_test(df_all):\nX_test=df_all.iloc[ntrain:] # Test set\nX_train_full=df_all.iloc[:ntrain] # Train set\nX_train, X_valid, y_train, y_valid = train_test_split(pd.get_dummies(X_train_full), y_train_full)","bcd3b2a4":"# Defining evaluation functions\ndef rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m,X_train=X_train, X_valid=X_valid, y_train=y_train, y_valid=y_valid):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","cf47e6b9":"# ElasticNet Lasso\nprint('ElasticNet Lasso')\ndef lasso_score(X,y):\n    lasso = ElasticNet(random_state=1)\n    param = {'l1_ratio' : [0],\n             'alpha' : [0.017]}\n    lasso = GridSearchCV(lasso, param, cv=5, scoring='neg_mean_squared_error')\n    lasso.fit(X,y)\n    print('Lasso:', np.sqrt(lasso.best_score_*-1))\n    return lasso\nlasso_score(X_train, y_train)\n\n# XGBoost\nprint('XGBoost')\nm_xgb = XGBRegressor(n_estimators=160, learning_rate=0.05)\n# using early_stop to find out where validation scores don't improve\n#m_xgb.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_valid, y_valid)], verbose=False)\nm_xgb.fit(X_train, y_train)\nprint_score(m_xgb,X_train, X_valid, y_train, y_valid)\n\n\n# Random Forest\nprint('Random Forest')\nm_rf = RandomForestRegressor(n_estimators=160, min_samples_leaf=1, max_features=0.5, n_jobs=-1, oob_score=True)\nm_rf.fit(X_train, y_train)\nprint_score(m_rf,X_train, X_valid, y_train, y_valid)\n","a2c74572":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X_train_full=X_train_full):\n    rmse = np.sqrt(-cross_val_score(model, X_train_full, y_train_full,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)\n\n\n# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n                                       \n\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\n                                \n\nprint('TEST score on CV')\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","67cc70c5":"# Fit models\nprint('StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X_train_full), np.array(y_train_full))\nelastic_model_full_data = elasticnet.fit(X_train_full, y_train_full)\nlasso_model_full_data = lasso.fit(X_train_full, y_train_full)\nridge_model_full_data = ridge.fit(X_train_full, y_train_full)\nsvr_model_full_data = svr.fit(X_train_full, y_train_full)\ngbr_model_full_data = gbr.fit(X_train_full, y_train_full)\nxgb_model_full_data = xgboost.fit(X_train_full, y_train_full)\nlgb_model_full_data = lightgbm.fit(X_train_full, y_train_full)","59045976":"def blend_models_predict(X_train_full):\n    return ((0.1 * elastic_model_full_data.predict(X_train_full)) + \\\n            (0.05 * lasso_model_full_data.predict(X_train_full)) + \\\n            (0.1 * ridge_model_full_data.predict(X_train_full)) + \\\n            (0.1 * svr_model_full_data.predict(X_train_full)) + \\\n            (0.1 * gbr_model_full_data.predict(X_train_full)) + \\\n            (0.15 * xgb_model_full_data.predict(X_train_full)) + \\\n            (0.1 * lgb_model_full_data.predict(X_train_full)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X_train_full))))","cd686e42":"print('RMSLE score on train data:')\nprint(rmsle(y_train_full, blend_models_predict(X_train_full)))","9a46619a":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))","8016b970":"submission.head()","87284a7f":"submission.to_csv(\"submission.csv\", index=False)\nprint('Save submission', datetime.now(),)","163c42d7":"# Machine Learning","6f47adf4":"## Dealining with Scewed data","5c1f539c":"# Initial Setup and Data Load","fb68d215":"## Predictions for submission","b717fc51":"**So let us start with Step 1.1. This simple Step 1.1, which is used same data cleaned at Step 1 which we have loaded to this kernel and se StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor, for prediction and submission. This approach gave us score of Top 8% = 0.11469**\n","a6a2a303":"**This is Step 1.1** - simple notebook on \"**House Prices: Advanced Regression Techniques**\"\n\nIf you like this work - upvotes and feedback are highly appreciated :)\n\n**This is Step 1.1** - same as corresponding Step 1 but instead of simple ElasticNet model we used StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor, for prediction and submission. And it gave us \u00b10.1146 (Top 8%)\n\n\n**Step 1** you can find in our public kernels\nhttps:\/\/www.kaggle.com\/eiosifov\/house-adv-step-1-data-cleaning-elastic\n\n**As we have stated at the beginning:**\nMain purpose is learning and evaluating how different data cleaning, feature encoding, feature creation, skew, normalization, external feature engineering, affects different modeling and stacking techniques and how different modeling techniques affect performance of predictions.\n\n**We will explore this competition in a few steps. **\n\nEach next step we decided just to use CSV data from the previous step:\n\n**Step 1 - Data cleaning** - simple data cleaning + Scew + Normalization + ElasticNet. Without any feature engineering, external feature creation, and any advanced modeling techniques. One important thing we have done at this step is using ML to impute missing values rather then just use our logical thinking.\n\n**Step 2 - Basic Feature Engineering** - basic feature engineering + Scew + Normalization + ElasticNet. At this step, we will test different encoding techniques for our data (ordinal, one-hot, label, binary, ...). We will not create any new features. And we will measure heavily effect of each change on different models. \n\n**Step 3 - Advanced Feature Engineering** - at this step, we will create and encode new features, use feature importance to drop not important features, etc. And we will measure heavily effect of each change on different models.\n\n**Step 1.2, 2.2, 3.2** - same as corresponding 1, 2, 3 but instead of simple ElasticNet model we will use StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor, for prediction and submission.","407e785b":"## Dropping low variance features","98c28ba2":"# Preparing clean data for ML","a140fe70":"## Normalization","835a20e5":"As next Step (2) we will work on basic Feature Engineering (heavily on data encoding techniques).","8a385e93":"#Normalization, the Sigmoid, Log, Cube Root and the Hyperbolic Tangent. \n#It all depends on what one is trying to accomplish.","56780cf2":"# Conclusins before Step 2","9c48ac88":"### Y (target value) to Log, as stated at Kaggle Evaluation page","22d620e9":"As ML algorithms for prediction we will use:\n\nStep 1 - simple ElasticNet\n\nStep 1.1 - same data, StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor\n\nStep 2 - basic feature engineering, ML Elastic Net\n\nStep 2.2 - basic feature engineering, ML StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor\n\nStep 3 - advanced feature engineering, ML Elastic Net\n\nStep 3.2 - advanced feature engineering, ML StackingCVRegressor (ridge, lasso, elasticnet, gbr, xgboost, lightgbm) with XGBoost as meta_regressor","c8e4b81b":"# Submission","5339216a":"## Dummies","7c9669f5":"\"\"\"\"\nFeatures that have too low variance can negatively impact the model, so we need to remove them by the number of repetitive equal values. In this case, we used a threshold of 99.2% (not 0 or 1 values). Therefore, if any feature has more than \u02dc99% reps of 1 or 0 it will be excluded. When doing this,\n\"\"\"","354b8a97":"**Wow!!!\nAs we can see with more complex modeling and Stacking techniques and even without any Feature Engineering we  can achieve score 0.11469 that is about Top 8%**\n\nFrom our experience we can say, that some techniques of Feature Engineering and Feature Creation can negatively affect accuracy of model and hence the score. We will see this at Step 2 and Step 2.1","897287bf":"Great! We are ready to Train real model","27ba8211":"## DataFrame concatination and Y separation","41007464":"## Pred ML Evaluation","204e01ef":"After this step we drastically improved scores, especially ElasticNet"}}