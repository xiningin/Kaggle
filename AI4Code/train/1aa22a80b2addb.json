{"cell_type":{"dd063058":"code","f746471f":"code","79ecd278":"code","b6609097":"code","a2a3f55e":"code","00c31130":"code","c9f4d6d1":"code","695fc90b":"code","b3f409c6":"code","94689092":"code","7df4fc63":"code","7f7162c7":"code","91f9d00b":"code","50b20b7b":"code","af7d5b3a":"code","4dec6e23":"code","b04a8af1":"code","dcfe029b":"code","d9b9eec6":"code","4a94b3f3":"code","45c38790":"code","457681ef":"code","0c0e3e61":"code","af2742f8":"code","4f6b8529":"markdown","f95403e8":"markdown","22c0b759":"markdown","411a28c5":"markdown","cfd7c27a":"markdown","a24d0426":"markdown","222729be":"markdown","bf0da714":"markdown","4798fdcb":"markdown","71490e56":"markdown","6ef00165":"markdown"},"source":{"dd063058":"# !pip install -U sentence-transformers\n!pip install torchsummary\n# !pip install torchtext\n# !pip install spacy\n# !python3 -m spacy download en","f746471f":"import os\nimport gc\nimport time\nimport random\nimport math\nimport re\nimport json\nimport csv\nimport pickle\nimport datetime\nfrom contextlib import contextmanager\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\n# from tqdm.notebook import tqdm_notebook as tqdm\n\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n# import torchtext\n# from torchtext.data.utils import get_tokenizer\nfrom torchsummary import summary\n\nfrom keras.preprocessing.text import Tokenizer\n# import spacy\n# from bs4 import BeautifulSoup\n# from sentence_transformers import SentenceTransformer\n\nfrom sklearn.decomposition import PCA\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n\nplt.style.use('dark_background')\nsns.set_style(\"darkgrid\")\n\npd.set_option('max_rows', 1000)\npd.set_option('max_columns', 1000)","79ecd278":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    # tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print(f\"[{name}] done in {time.time() - t0:.0f} s\")\n    \n\ndef load_df(path):\n    basename = os.path.basename(path)\n    ext = path.split('.')[-1]\n    \n    if ext == 'csv':\n        df = pd.read_csv(path)\n    elif ext == 'pkl':\n        df = pd.read_pickle(path)\n    else:\n        raise IOError(f'Not Accessable Format .{ext}')\n\n    print(f\"{basename} shape \/ {df.shape}\")\n    \n    return df\n\n\n# sending message to slack channel (requiring slack_notificate.sh)\ndef send2slack(message):\n    if os.path.isfile('.\/slack_notificate.sh'):\n        print(message)\n        MESSAGE=message\n        !echo $MESSAGE | .\/slack_notificate.sh\n    else:\n        print(message)","b6609097":"# ref: https:\/\/discuss.pytorch.org\/t\/rmsle-loss-function\/67281\/2\nclass RMSLELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self, pred, actual):\n        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n    \n\ndef split_cat(text):\n    try: return text.split(\"\/\")\n    except: return (\"missing\", \"missing\", \"missing\")\n\n        \n# NaN\u3092 `missing` \u3067\u57cb\u3081\u308b\ndef handle_missing_inplace(df):\n    # df['category_name'].fillna(value='missing', inplace=True)\n    df['brand_name'].fillna(value='missing', inplace=True)\n    df['item_description'].fillna(value='missing', inplace=True)","a2a3f55e":"INPUT_DIR = '..\/input\/ailab-ml-training-2\/'\n\ndf_path_dict = {\n    'train': os.path.join(INPUT_DIR, 'train.csv'),\n    'test': os.path.join(INPUT_DIR, 'test.csv'),\n    'sample_submission': os.path.join(INPUT_DIR, 'sample_submission.csv'),\n}\n\nSEED = 42\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nMULTI_GPU = False\nBATCH_SIZE = 30000 if MULTI_GPU else 10000\nEPOCHS = 1\nLEARNING_RATE = 0.001\nDROPOUT = 0.1\nPATIENCE = -1  # -1\u306e\u6642\uff0cPATIENCE\u306f\u4ed5\u4e8b\u3057\u306a\u3044\n\nseed_everything(seed=SEED)","00c31130":"with timer('Data Loading'):\n    train_df = load_df(df_path_dict['train'])\n    test_df = load_df(df_path_dict['test'])\n    sample_submission = load_df(df_path_dict['sample_submission'])","c9f4d6d1":"with timer('Handling missing values'):\n    handle_missing_inplace(train_df)\n    handle_missing_inplace(test_df)","695fc90b":"with timer('Split Category'):\n    train_df['general_cat'], train_df['subcat_1'], train_df['subcat_2'] = \\\n    zip(*train_df['category_name'].apply(lambda x: split_cat(x)))\n    \n    test_df['general_cat'], test_df['subcat_1'], test_df['subcat_2'] = \\\n    zip(*test_df['category_name'].apply(lambda x: split_cat(x)))\n\n    train_df.drop('category_name', inplace=True, axis=1)\n    test_df.drop('category_name', inplace=True, axis=1)","b3f409c6":"# Categorical Data\nwith timer('Handling categorical data'):\n    le = LabelEncoder()\n    le.fit(np.hstack([train_df['general_cat'], test_df['general_cat']]))\n    train_df['general_cat'] = le.transform(train_df['general_cat'])\n    test_df['general_cat'] = le.transform(test_df['general_cat'])\n    \n    le = LabelEncoder()\n    le.fit(np.hstack([train_df['subcat_1'], test_df['subcat_1']]))\n    train_df['subcat_1'] = le.transform(train_df['subcat_1'])\n    test_df['subcat_1'] = le.transform(test_df['subcat_1'])\n    \n    le = LabelEncoder()\n    le.fit(np.hstack([train_df['subcat_2'], test_df['subcat_2']]))\n    train_df['subcat_2'] = le.transform(train_df['subcat_2'])\n    test_df['subcat_2'] = le.transform(test_df['subcat_2'])\n\n    le = LabelEncoder()\n    le.fit(np.hstack([train_df['brand_name'], test_df['brand_name']]))\n    train_df['brand_name'] = le.transform(train_df['brand_name'])\n    test_df['brand_name'] = le.transform(test_df['brand_name'])\n    \n    del le\n    gc.collect()\n\ntrain_df.head()","94689092":"# Target Scaling\ntrain_df['target'] = np.log1p(train_df['price'])\ntarget_scalar = MinMaxScaler(feature_range=(-1, 1))\ntrain_df['target'] = target_scalar.fit_transform(train_df['target'].values.reshape(-1, 1))","7df4fc63":"train_df[['target']].hist()","7f7162c7":"with timer('Tokenize Texts'):\n    raw_text = np.hstack([\n        train_df['item_description'].str.lower(),\n        train_df['name'].str.lower(),\n    ])\n\n    token_raw = Tokenizer()\n    token_raw.fit_on_texts(raw_text)\n\n    train_df['item_description'] = token_raw.texts_to_sequences(train_df['item_description'].str.lower())\n    test_df['item_description'] = token_raw.texts_to_sequences(test_df['item_description'].str.lower())\n\n    train_df['name'] = token_raw.texts_to_sequences(train_df['name'].str.lower())\n    test_df['name'] = token_raw.texts_to_sequences(test_df['name'].str.lower())","91f9d00b":"MAX_NAME = np.max([\n    np.max(train_df['name'].apply(lambda x: len(x))),\n    np.max(test_df['name'].apply(lambda x: len(x)))\n])\n\nMAX_DESC = np.max([\n    np.max(train_df['item_description'].apply(lambda x: len(x))),\n    np.max(test_df['item_description'].apply(lambda x: len(x)))\n])\n\nMAX_GEN_CAT = np.max([train_df['general_cat'].max(), test_df['general_cat'].max()]) + 1\nMAX_SUBCAT1 = np.max([train_df['subcat_1'].max(), test_df['subcat_1'].max()]) + 1\nMAX_SUBCAT2 = np.max([train_df['subcat_2'].max(), test_df['subcat_2'].max()]) + 1\nMAX_BRAND = np.max([train_df['brand_name'].max(), test_df['brand_name'].max()]) + 1\nMAX_CONDITION = np.max([train_df['item_condition_id'].max(), test_df['item_condition_id'].max()]) + 1\n\nMAX_TEXT = len(token_raw.word_index) + 1","50b20b7b":"# TODO memory check\nwith timer('padding'):\n    train_df['item_description'] = \\\n    train_df['item_description'].apply(lambda x: np.pad(x, (0, MAX_DESC - len(x))))\n    test_df['item_description'] = \\\n    test_df['item_description'].apply(lambda x: np.pad(x, (0, MAX_DESC - len(x))))\n    train_df['name'] = train_df['name'].apply(lambda x: np.pad(x, (0, MAX_NAME - len(x))))\n    test_df['name'] = test_df['name'].apply(lambda x: np.pad(x, (0, MAX_NAME - len(x))))","af7d5b3a":"dev_data, val_data = train_test_split(train_df, random_state=SEED, train_size=0.8)\nprint(f'training data shape: {dev_data.shape}')\nprint(f'validation data shape: {val_data.shape}')\n\ndel train_df\ngc.collect()","4dec6e23":"class MyDataset(Dataset):\n    def __init__(self, data, train=True):\n        if train:\n            self.data = data.drop('target', axis=1)\n            self.target = torch.Tensor(data['target'].values)\n        else:\n            self.data = data\n            # self.target = torch.zeros(data.shape[0])\n            self.target = torch.Tensor(data['test_id'].values)\n        \n    def __len__(self):\n        return self.data.shape[0]\n    \n    def __getitem__(self, idx):\n        tmp = self.data.iloc[idx]\n        # TODO Long Tensor\u306b\u3059\u308b\n        tmp = torch.Tensor(np.concatenate([\n            tmp['name'],\n            tmp['item_description'],\n            [tmp['brand_name']],\n            [tmp['general_cat']],\n            [tmp['subcat_1']],\n            [tmp['subcat_2']],\n            [tmp['item_condition_id']],\n            [tmp['shipping']],\n        ]))\n        return tmp, self.target[idx]","b04a8af1":"print(f'MAX_NAME - {MAX_NAME}')\nprint(f'MAX_DESC - {MAX_DESC}')\nprint(f'MAX_GENCAT - {MAX_GEN_CAT}')\nprint(f'MAX_SUBCAT1 - {MAX_SUBCAT1}')\nprint(f'MAX_SUBCAT2 - {MAX_SUBCAT2}')\nprint(f'MAX_BRAND - {MAX_BRAND}')\nprint(f'MAX_CONDITION - {MAX_CONDITION}')\nprint(f'MAX_TEXT - {MAX_TEXT}')","dcfe029b":"class Net(nn.Module):\n    def __init__(self, dropout=0.1):\n        super(Net, self).__init__()\n        # Individual Embedding Layers\n        self.embed_desc = nn.Embedding(MAX_TEXT, 64)\n        self.embed_name = nn.Embedding(MAX_TEXT, 64)\n        self.embed_brnd = nn.Embedding(MAX_BRAND, 16)\n        self.embed_cate = nn.Embedding(MAX_GEN_CAT, 4)\n        self.embed_sub1 = nn.Embedding(MAX_SUBCAT1, 8)\n        self.embed_sub2 = nn.Embedding(MAX_SUBCAT2, 8)\n        self.embed_cond = nn.Embedding(MAX_CONDITION, 4)\n        self.embed_ship = nn.Embedding(2, 4)\n        \n        # Individual GRU\n        self.gru_desc = nn.Sequential(\n            nn.GRU(64, 16, batch_first=True),\n            # nn.Tanh()\n        )\n        self.gru_name = nn.Sequential(\n            nn.GRU(64, 8, batch_first=True),\n            # nn.Tanh()\n        )\n        \n        # Core Network\n        self.main_net = nn.Sequential(\n            # input - concat all results\n            # name - desc - brnd - cate - sub1 - sub2 - cond - ship\n            nn.Linear(16 + 8 + 16 + 4 + 8 + 8 + 4 + 4, 128),\n            # nn.ReLU(),\n            nn.Dropout(dropout),\n            \n            # hidden\n            nn.Linear(128, 64),\n            # nn.ReLU(),\n            nn.Dropout(dropout),\n            \n            # output\n            nn.Linear(64, 1),\n            # nn.Sigmoid()\n        )\n        \n    def forward(self, data):\n        bsize = data.shape[0]\n        name = self.embed_name(data[:, :MAX_NAME].to(torch.long))\n        _, name = self.gru_name(name)\n        name = F.tanh(name)\n        desc = self.embed_desc(data[:, MAX_NAME:MAX_NAME + MAX_DESC].to(torch.long))\n        _, desc = self.gru_desc(desc)\n        desc = F.tanh(desc)\n        # print(f'[DEBUG] name shape: {name.shape}')\n        \n        brnd = self.embed_brnd(data[:, MAX_NAME + MAX_DESC].to(torch.long))\n        cate = self.embed_cate(data[:, MAX_NAME + MAX_DESC + 1].to(torch.long))\n        sub1 = self.embed_sub1(data[:, MAX_NAME + MAX_DESC + 2].to(torch.long))\n        sub2 = self.embed_sub2(data[:, MAX_NAME + MAX_DESC + 3].to(torch.long))\n        cond = self.embed_cond(data[:, MAX_NAME + MAX_DESC + 4].to(torch.long))\n        ship = self.embed_ship(data[:, MAX_NAME + MAX_DESC + 5].to(torch.long))\n        pred = self.main_net(torch.cat((name[0], desc[0], brnd, cate, sub1, sub2, cond, ship), dim=1))\n        return pred","d9b9eec6":"model = Net(dropout=DROPOUT).to(DEVICE)\nsummary(model, [[292]])   # x.shape[1]  <-- x, y = next(iter(dataloader))","4a94b3f3":"dev_dataset = MyDataset(dev_data)\nval_dataset = MyDataset(val_data)\ndev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nif MULTI_GPU:\n    model = torch.nn.DataParallel(model)\n\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n# criterion = RMSLELoss()\ncriterion = nn.MSELoss()\n\nbest_loss = 1e6\nskip_count = 0\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    dev_loss_list = []\n        \n    for x, y in tqdm(dev_dataloader):\n        x = x.to(DEVICE)\n        y = y.to(DEVICE)\n        optim.zero_grad()\n        \n        y_pred = model(x)\n        loss = criterion(y_pred.squeeze(1), y)\n        loss.backward()\n        optim.step()\n        \n        dev_loss_list.append(loss.item())\n        \n    model.eval()\n    val_loss_list = []\n\n    for x, y in tqdm(val_dataloader):\n        x = x.to(DEVICE)\n        y = y.to(DEVICE)\n        \n        with torch.no_grad():\n            y_pred = model(x)\n            loss = criterion(y_pred, y)\n        \n        val_loss_list.append(loss.item())\n        \n    send2slack('epoch: {}\/{} - loss: {:.5f} - val_loss: {:.5f}'.format(\n        epoch, \n        EPOCHS,\n        np.mean(dev_loss_list),\n        np.mean(val_loss_list)\n    ))\n    \n    if np.mean(val_loss_list) < best_loss:\n        skip_count = 0\n        best_loss = np.mean(val_loss_list)\n        state_dict = model.state_dict()\n    else:\n        skip_count += 1\n        if skip_count == PATIENCE:\n            break","45c38790":"send2slack('Done')","457681ef":"test_dataset = MyDataset(test_df, train=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel.eval()\npredictions = []\ntmp = pd.DataFrame()\n\nfor x, test_ids in tqdm(test_dataloader):\n    x = x.to(DEVICE)\n    test_ids = test_ids.numpy().astype(np.int64)\n    \n    with torch.no_grad():\n        y_pred = model(x)\n        \n    y_pred = np.expm1(target_scalar.inverse_transform(y_pred.detach().cpu().numpy())).astype(np.float64)\n    tmp = pd.concat([tmp, pd.DataFrame({'test_id': test_ids, 'price': y_pred.squeeze(1)})])\n\nsample_submission = sample_submission[['test_id']].merge(tmp, on='test_id')\n    ","0c0e3e61":"sample_submission[['price']].hist()","af2742f8":"sample_submission.to_csv('submission.csv', index=False)","4f6b8529":"# Training\n","f95403e8":"# Modeling","22c0b759":"# Data Preparation","411a28c5":"# Libraries","cfd7c27a":"# Data Loading","a24d0426":"## Utils","222729be":"# NN Simple Baseline Notebook\n- ref: https:\/\/www.kaggle.com\/knowledgegrappler\/a-simple-nn-solution-with-keras-0-48611-pl?scriptVersionId=1794537\n\n- \u3053\u306enotebook\u306fPyTorch\u3092\u4f7f\u7528\u3057\u305fNeuralNetwork\u3092\u7d39\u4ecb\u3059\u308b\u3082\u306e\u3067\u3059\n- commit\u306e\u305f\u3081\u3060\u3051\u306e\u30d5\u30a1\u30a4\u30eb\u306a\u306e\u30671epoch\u3057\u304b\u56de\u3057\u3066\u307e\u305b\u3093(5epoch\u56de\u3059\u3053\u3068\u3067\uff0cleader_board\u306eNN\u306e\u30b9\u30b3\u30a2\u306f\u51fa\u308b\u306f\u305a\u3067\u3059)\n\n## FYI:Next Step\n- valid RMLSE\u3092\u51fa\u3059\u3088\u3046\u306b\u3057\u3066\u307f\u308b (\u3053\u306enotebook\u306e\u5b9f\u88c5\u3067\u306f`target`\u306e\u5024\u57df\u3092\u6b63\u898f\u5316\u3057\u3066\u3044\u308b\u7406\u7531\u306a\u3069\u3067Nan\u3068\u306a\u3063\u3066\u3057\u307e\u3046\u306e\u3067\u5b66\u7fd2\u305d\u306e\u3082\u306e\u306b\u306f\u4f7f\u7528\u3067\u304d\u307e\u305b\u3093\uff0e)\n- Stratified k-fold\n- \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\n\n---\n- \u30b3\u30fc\u30c9\u306e\u6539\u826f\u70b9\u30fb\u30d0\u30b0\u306b\u6c17\u4ed8\u3044\u305f\u4eba\u306f\u30b3\u30e1\u30f3\u30c8\u3044\u305f\u3060\u3051\u308c\u3070\u5e78\u3044\u3067\u3059","bf0da714":"# Sentence Features\n- `name`, `item_description` \u306e\u6271\u3044\u304c\u4eca\u56de\u306e\u30ad\u30fc\u306b\u306a\u308b\u3088\u3046\u306a\u6c17\u304c\u3057\u3066\u3044\u308b\n   - BERT\u3092\u7528\u3044\u305fSentence Vector\u306f\u3042\u307e\u308a\u3046\u307e\u304f\u884c\u3063\u3066\u306a\u3044 (\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u6570\u306b\u5bfe\u3057\u3066758\u6b21\u5143\u304c\u5927\u304d\u3059\u304e\u3066\u3046\u307e\u304f\u6271\u3048\u3066\u3044\u306a\u3044?)\n   - \u672cnotebook\u306ftext\u3092\u30c8\u30fc\u30af\u30f3\u5316\u3057\u3066\uff0c\u30c8\u30fc\u30af\u30f3\u3092GRU\u306b\u6295\u3052\u5165\u308c\u3066\u3044\u308b\n   - \u3044\u308d\u3044\u308d\u3046\u307e\u304f\u3044\u304b\u306a\u3059\u304e\u3066\u53c2\u7167\u5143\u306b\u5408\u308f\u305b\u3088\u3046\u3068\u7121\u7406\u3084\u308akeras\u306eTokenizer()\u4f7f\u7528\u3057\u3066\u3044\u305f\u308a\u3059\u308b","4798fdcb":"# Target","71490e56":"# Prediction","6ef00165":"# PreProcessing"}}