{"cell_type":{"5b0a3c4b":"code","0015e61e":"code","29528197":"code","2251a710":"code","06d41b79":"code","cf712a14":"code","e77624a5":"code","2c85620d":"code","017f24d0":"code","95849113":"code","5a415641":"code","e314bbf3":"code","ba58fb54":"code","962642f0":"code","6ff9ba9c":"code","a6bd45ec":"code","6675aec6":"code","809574f3":"code","0dd4f49e":"code","bc2ccee6":"code","3c103c77":"code","de987886":"code","0c1b0bbe":"code","72eef6c5":"code","0501911f":"code","b6d495d3":"code","e310e6ae":"code","0e3e998f":"code","02aee52a":"code","a10e543a":"code","595e7848":"code","66640a64":"code","8fbcf4a2":"code","cdd5ff19":"code","64abafb1":"code","d87f9613":"code","10dcc670":"code","9f42a627":"code","a13f2a0c":"code","67869754":"code","81e48d2a":"code","4228a08c":"code","aa285376":"code","ffb730ce":"code","77eba002":"code","154fd724":"code","2652c3ff":"code","78755259":"code","6c85ed16":"code","052fdc97":"code","71631211":"code","e61bdb0a":"code","fba35756":"code","4da01f43":"code","af137520":"code","07d3249d":"code","782ca303":"code","fdcf58fa":"code","ac614491":"code","776baa0e":"code","f1817d60":"code","73146837":"code","f8c3bfdb":"code","d0d638ab":"code","ce4720cd":"code","2a3d5613":"code","e7fea36b":"code","5434c3ac":"code","00ef0a0b":"code","fdb05d91":"code","ad8e81f4":"code","8943d23c":"code","36b9e8dd":"markdown","4b01c347":"markdown","afac883a":"markdown","fa5a729d":"markdown","18d7aac7":"markdown","c4324829":"markdown","bf1c9e39":"markdown","36bb88a1":"markdown","905d973f":"markdown","8657842c":"markdown","3858cb87":"markdown","82d84d5a":"markdown","a6d7c48c":"markdown","6ca6bf9d":"markdown","1bc024a5":"markdown","7b907a8b":"markdown","eeb43f50":"markdown","82b1412e":"markdown","e6784914":"markdown","0e97db2a":"markdown","9ed35652":"markdown","ea22a650":"markdown","cad353af":"markdown","cea80092":"markdown","6dc4c5b5":"markdown","0de0f400":"markdown","7dc73ed9":"markdown","74bb581d":"markdown","7712d46f":"markdown","9c0ad0e0":"markdown","f36af35a":"markdown","03c94648":"markdown","2d5de278":"markdown","3a6d4594":"markdown","59ccaeae":"markdown","9c0864fd":"markdown","b681cc96":"markdown","411c22ef":"markdown","ffba70a2":"markdown","c70045de":"markdown","c001efb7":"markdown","57db7bb0":"markdown","bcc39aa5":"markdown","76e8e172":"markdown","0faa800c":"markdown","d2991970":"markdown","8133bb28":"markdown","3402bd37":"markdown","81b2bea0":"markdown","ef876822":"markdown","e5860b1e":"markdown","3d8050ca":"markdown","7a2283a2":"markdown","f4831a2f":"markdown","1d999808":"markdown","55e5d947":"markdown","4fd8359e":"markdown","c9c63cd8":"markdown","02ca2535":"markdown","e4c58fa2":"markdown","976be45f":"markdown","82695e76":"markdown","1df269e6":"markdown","e03a1608":"markdown","6d6a4619":"markdown","c506dad5":"markdown"},"source":{"5b0a3c4b":"# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0015e61e":"train_dataset_path = \"\/kaggle\/input\/Dataset\/Train.csv\"\ntest_dataset_path  = \"\/kaggle\/input\/Dataset\/Test.csv\"","29528197":"# Data manipulation libraries\nimport pandas as pd\nimport numpy as np\n\n# Data visualistaion libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.plotting.register_matplotlib_converters()\n%matplotlib inline\n\n# Machine learning libraries\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\n\n# Machine Learning Models\nfrom sklearn.ensemble import RandomForestRegressor","2251a710":"# Both training and testing data\ntraining_dataframe = pd.read_csv(train_dataset_path,index_col=\"Employee_ID\")\ntesting_dataframe  = pd.read_csv(test_dataset_path,index_col =\"Employee_ID\")","06d41b79":"training_dataframe.describe()","cf712a14":"## Data Distribution\ntraining_dataframe.hist(bins = 50, figsize = (20,15))","e77624a5":"print(training_dataframe)","2c85620d":"print(training_dataframe.Travel_Rate)","017f24d0":"# All the columns\nprint(training_dataframe.columns)","95849113":"training_dataframe.head()","5a415641":"training_dataframe.tail()","e314bbf3":"## Columns with missing values\ncols_with_missing = [col for col in training_dataframe.columns if training_dataframe[col].isnull().any()]\nprint(cols_with_missing)","ba58fb54":"# Get a list of categorical columns\n\ns = (training_dataframe.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(object_cols)","962642f0":"# Remove rows with missing target, separate target from predictors in training data\nX_train_full = training_dataframe\n\nX_train_full.dropna(axis=0, subset=['Attrition_rate'], inplace=True, how = \"any\")\ny_train_full = X_train_full.Attrition_rate\n# X_train_full.drop(['Attrition_rate'], axis=1, inplace=True)\n\n\nX_train_full.head()","6ff9ba9c":"X_valid_full = testing_dataframe\n\nlabelEncoder = LabelEncoder()\nfor col in object_cols:\n    X_train_full[col] = labelEncoder.fit_transform(X_train_full[col])\n    X_valid_full[col] = labelEncoder.transform(X_valid_full[col])\n    ","a6bd45ec":"X_train_full.head()","6675aec6":"X_valid_full.head()","809574f3":"si = SimpleImputer(strategy='most_frequent')\nX_train_imputed = pd.DataFrame(si.fit_transform(X_train_full))\n# It is important to fit_transform the training data\n\n\n# Imputation removed column names; they have to be put back\nX_train_imputed.columns = X_train_full.columns\n# X_train_imputed.index = X_train_full.index\nprint(\"shape of X_train_imputed = \",X_train_imputed.shape)\nX_train_imputed.head()","0dd4f49e":"print(\"shape of y_train_full = \",y_train_full.shape)","bc2ccee6":"print(\"Shape of X_valid_full = \",X_valid_full.shape )","3c103c77":"X_train_imputed.head()\n","de987886":"data = X_train_imputed\nind_var = y_train_full\n\nplt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs Age\")\n\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Age\"])","0c1b0bbe":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs time_since_promotion\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Time_since_promotion\"])","72eef6c5":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs time_of_service\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Time_of_service\"])","0501911f":"sns.jointplot(x=data[\"Time_of_service\"],y=data[\"Attrition_rate\"],kind=\"kde\")","b6d495d3":"plt.figure(figsize= (8,6))\nplt.title(\"Attrition Rate Vs Gender\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Gender\"])\nplt.legend(title='Gender', loc='lower left', labels=['Male : 1', 'Female : 0'])","e310e6ae":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs eductaion_level\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Education_Level\"])","0e3e998f":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs Work life balance\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Work_Life_balance\"])","02aee52a":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs work_life_balance\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Work_Life_balance\"])","a10e543a":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs growth rate(%)\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"growth_rate\"])","595e7848":"sns.jointplot(x=data[\"growth_rate\"],y=data[\"Attrition_rate\"],kind=\"kde\")","66640a64":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs Job Unit\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Unit\"])","8fbcf4a2":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs Pay scale\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Pay_Scale\"])","cdd5ff19":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs Pay_scale\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Pay_Scale\"])","64abafb1":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs decision making skills\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Decision_skill_possess\"])","d87f9613":"plt.figure(figsize=(20,6))\nplt.title(\"Attrition Rate Vs travel rate\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"Travel_Rate\"])","10dcc670":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs Travel rate\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Travel_Rate\"])","9f42a627":"plt.figure(figsize= (15,10))\nplt.title(\"Attrition Rate Vs Compensation_and_Benefits\")\nsns.barplot(y=data[\"Attrition_rate\"],x=data[\"Compensation_and_Benefits\"])","a13f2a0c":"plt.title(\"Attrition Rate Vs Anominised variables\")\nsns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR1\"])","67869754":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR2\"])","81e48d2a":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR3\"])","4228a08c":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR4\"])","aa285376":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR5\"])","ffb730ce":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR6\"])","77eba002":"sns.lineplot(y=data[\"Attrition_rate\"],x=data[\"VAR7\"])","154fd724":"corr_matrix = data.corr()\ncorr_matrix[\"Attrition_rate\"].sort_values(ascending = False)","2652c3ff":"sample_data = data\nplt.figure(figsize = (30,10))\ncorr = sample_data.corr()\nax = sns.heatmap(corr,vmin = -0.03,vmax = 0.03, center = 0,cmap=sns.diverging_palette(20, 220, n=200), square=True, linewidths = 0.5)\n\nax.set_xticklabels( ax.get_xticklabels(),rotation=45, horizontalalignment='right')","78755259":"features_1 = [\"Compensation_and_Benefits\",\"Travel_Rate\",\"Pay_Scale\",\n              \"Unit\",\"growth_rate\",\"Education_Level\",\"Time_of_service\",\"Age\"]\n\nfeatures_2 = [\"Gender\",\"Relationship_Status\",\"Hometown\",\"Unit\",\n              \"Decision_skill_possess\",\"Time_since_promotion\",\n              \"growth_rate\",\"Post_Level\",\"Work_Life_balance\"]     #Strong positive correaltion\n\nfeatures_3 = data.columns[:-1]\n\nfeatures_4 =[\"Gender\",\"Unit\",\"Work_Life_balance\",\"Decision_skill_possess\",\"Post_Level\",\"growth_rate\",\"Time_since_promotion\",\"Travel_Rate\",                 \n\"VAR4\",\"Age\",\"Pay_Scale\", \"VAR7\",\"Time_of_service\", \"VAR2\",\"Compensation_and_Benefits\"]      # Strong negative and positive correaltion\n\nfeatures_5 =[\"Gender\",\"Unit\",\"Work_Life_balance\",\"Decision_skill_possess\",\"Post_Level\",\"growth_rate\",\"Time_since_promotion\",\"Travel_Rate\",                 \n\"VAR4\",\"Age\",\"Pay_Scale\", \"VAR7\",\"Time_of_service\", \"VAR2\",\"Compensation_and_Benefits\",\"Relationship_Status\"\n            ,\"Education_Level\",\"VAR1\"] \n\nX = data[features_4]\ny = data[\"Attrition_rate\"]\n\n# Separating validation from training data\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,train_size=0.7, test_size=0.3,random_state = 0)\n\n","6c85ed16":"# Model selection and Training\n\nrandom_forest_model = RandomForestRegressor(random_state = 1)\nrandom_forest_model.fit(train_X,train_y)\nmodel_rf_preds = random_forest_model.predict(val_X)\nmae_score_rf = mean_absolute_error(val_y,model_rf_preds)\nrmse_rf = mean_squared_error(val_y, model_rf_preds, squared=False)\nprint(\"Mean absolute error with Random Forest = \",mae_score_rf)\nprint(\"Root mean square error with Random Forest = \",rmse_rf)\nprint(\"Final Score for comp = \",100*(1-rmse_rf))","052fdc97":"# Model selection and training\n\ndef xgb_n_estimators_selection(x):\n    xgb_model = XGBRegressor(n_estimators = x)\n    xgb_model.fit(train_X,train_y)\n    model_xgb_preds = xgb_model.predict(val_X)\n    mae_score_xgb = mean_absolute_error(val_y,model_xgb_preds)\n    return mae_score_xgb\n","71631211":"mae_list=[]\nn_min = 0;\nfor i in range(1,50):\n    mae_list.append(xgb_n_estimators_selection(i))\n    plt.plot(i,mae_list[i-1],'bo')\n    \nplt.title(\"MAE vs n_estimators fo XGBoost\")\nplt.xlabel(\"n_estimators\")\nplt.ylabel(\"MAE\")","e61bdb0a":"print(\"The best score is {val} at n_estimator of {n}\".format(val=min(mae_list),n= mae_list.index(min(mae_list))))","fba35756":"xgbModel_updated = XGBRegressor(n_estimators = 500, learning_rate = 0.1,early_stopping_rounds = 20,\n                  eval_set=[(val_X,val_y)], verbose = False)\n\nxgbModel_updated.fit(train_X,train_y)\n\n\npredictions_xgbModel_updated = xgbModel_updated.predict(val_X)\nmae_xgbModel_updated = mean_absolute_error(predictions_xgbModel_updated,val_y)\nprint(\"MAE on updated XGB model wiht early stopping= \",mae_xgbModel_updated)\nxgbModel_updated_rmse = mean_squared_error(predictions_xgbModel_updated,val_y,squared = False)\nprint(\"RMSE on xgbModel with early stopping= \",xgbModel_updated_rmse)\nprint(\"Final Score for comp = \",100*(1-xgbModel_updated_rmse))","4da01f43":"from sklearn import linear_model\nlr_model = linear_model.Lasso(alpha = 0.01)\nlr_model.fit(train_X,train_y)\nlr_model_prediction = lr_model.predict(val_X)\nlr_model_mae = mean_absolute_error(lr_model_prediction,val_y)\nprint(\"MAE on Lasso regression model= \",lr_model_mae)\nlr_model_rmse = mean_squared_error(lr_model_prediction,val_y,squared = False)\nprint(\"RMSE on lasso regression model= \",lr_model_rmse)\nprint(\"Final Score for comp = \",100*(1-lr_model_rmse))","af137520":"from sklearn.linear_model import ElasticNet\nen_model = ElasticNet(alpha = 0.01, l1_ratio = 0.8)       # l1_ratio is the ratio of ridge and lasso regression. Here 20:80\nen_model.fit(train_X,train_y)\nen_model_prediction = en_model.predict(val_X)\nen_model_mae = mean_absolute_error(en_model_prediction,val_y)\nprint(\"MAE on Elastic Net regression model= \",en_model_mae)\nen_model_rmse = mean_squared_error(en_model_prediction,val_y,squared = False)\nprint(\"RMSE on Elastic Net regression model= \",en_model_rmse)\nprint(\"Final Score for comp = \",100*(1-en_model_rmse))","07d3249d":"# Linear Support Vector Regression\n\nfrom sklearn.svm import LinearSVR\nlinear_svr_model = LinearSVR(epsilon = 1.5)\nlinear_svr_model.fit(train_X,train_y)\nlinear_svr_model_pred = linear_svr_model.predict(val_X)\nlinear_svr_model_mae = mean_absolute_error(linear_svr_model_pred,val_y)\nprint(\"MAE on Linear Support Vector regression model= \",linear_svr_model_mae)\nlinear_svr_model_rmse = mean_squared_error(linear_svr_model_pred,val_y,squared = False)\nprint(\"RMSE on Linear Support Vector regression model= \",linear_svr_model_rmse)\nprint(\"Final Score for comp = \",100*(1-linear_svr_model_rmse))","782ca303":"# SVR with poly kernel\n\nfrom sklearn.svm import SVR\nsvr_model = SVR(kernel = \"poly\",epsilon = 0.1)\nsvr_model.fit(train_X,train_y)\nsvr_model_pred = svr_model.predict(val_X)\nsvr_model_mae = mean_absolute_error(svr_model_pred,val_y)\nprint(\"MAE on Support Vector regression model with poly kernel= \",svr_model_mae)\nsvr_model_rmse = mean_squared_error(svr_model_pred,val_y,squared = False)\nprint(\"RMSE on Support Vector regression model with poly kernel= \",svr_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-svr_model_rmse)))","fdcf58fa":"from sklearn.linear_model import PoissonRegressor\n\npr_model = PoissonRegressor(max_iter=300)\npr_model.fit(train_X,train_y)\npr_model_pred = pr_model.predict(val_X)\npr_model_mae = mean_absolute_error(pr_model_pred,val_y)\nprint(\"MAE on Poisson regression model = \",pr_model_mae)\npr_model_rmse = mean_squared_error(pr_model_pred,val_y,squared = False)\nprint(\"RMSE on Poisson regression model = \",pr_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-pr_model_rmse)))","ac614491":"\ndef alpha_tuning_pr(x):\n    test_model  = PoissonRegressor(alpha = x,max_iter = 300)\n    test_model.fit(train_X,train_y)\n    test_pred = test_model.predict(val_X)\n    test_rmse = mean_squared_error(pr_model_pred,val_y,squared = False)\n    return test_rmse\n    ","776baa0e":"rmse_list_pr= []\nalpha_list_pr = []\na = 0;\nfor i in np.arange(1e-15,9e-15, 1e-15):\n    rmse_list_pr.append(alpha_tuning_pr(i))\n    alpha_list_pr.append(i)\n    plt.plot(i,alpha_tuning_pr(i),'bo')\n    \nplt.title(\"RMSE vs alpha for Poisson Regression\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"RMSE\")","f1817d60":"from sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\ngbr_model =  HistGradientBoostingRegressor(loss=\"poisson\",max_leaf_nodes=2)\ngbr_model.fit(train_X,train_y)\ngbr_model_pred = gbr_model.predict(val_X)\ngbr_model_mae = mean_absolute_error(gbr_model_pred,val_y)\nprint(\"MAE on gbr_model = \",gbr_model_mae)\ngbr_model_rmse = mean_squared_error(gbr_model_pred,val_y,squared = False)\nprint(\"RMSE on gbr_model = \",gbr_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-gbr_model_rmse)))","73146837":"def maxLeafNodes_tuning_gbr(x):\n    test_model  = HistGradientBoostingRegressor(loss=\"poisson\",max_leaf_nodes=x)\n    test_model.fit(train_X,train_y)\n    test_pred = test_model.predict(val_X)\n    test_rmse = mean_squared_error(test_pred,val_y,squared = False)\n    return test_rmse","f8c3bfdb":"rmse_list_gbr= []\nleaf_list_gbr = []\na = 0;\nfor i in range(10,200,10):\n    rmse_list_gbr.append(maxLeafNodes_tuning_gbr(i))\n    leaf_list_gbr.append(i)\n    plt.plot(i,maxLeafNodes_tuning_gbr(i),'bo')\n    \nplt.title(\"RMSE vs Max Leaf nodes for GBR\")\nplt.xlabel(\"Max Leaf nodes for GBR\")\nplt.ylabel(\"RMSE\")","d0d638ab":"from sklearn.linear_model import TweedieRegressor\ntr_model = TweedieRegressor(power=0, alpha=1, link='log')\ntr_model.fit(train_X,train_y)\ntr_model_pred = tr_model.predict(val_X)\ntr_model_mae = mean_absolute_error(tr_model_pred,val_y)\nprint(\"MAE on TweedieRegressor_model = \",tr_model_mae)\ntr_model_rmse = mean_squared_error(tr_model_pred,val_y,squared = False)\nprint(\"RMSE on TweedieRegressor_model = \",tr_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-tr_model_rmse)))","ce4720cd":"from sklearn.linear_model import PassiveAggressiveRegressor\npar_model = PassiveAggressiveRegressor(max_iter=100, random_state=0,tol=1e-3)\npar_model.fit(train_X,train_y)\npar_model_pred = par_model.predict(val_X)\npar_model_mae = mean_absolute_error(par_model_pred,val_y)\nprint(\"MAE on Passive Agressive Regressor model = \",par_model_mae)\npar_model_rmse = mean_squared_error(par_model_pred,val_y,squared = False)\nprint(\"RMSE on Passive Agressive Regressor model = \",par_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-par_model_rmse)))","2a3d5613":"from sklearn.linear_model import OrthogonalMatchingPursuit\nomp_model = OrthogonalMatchingPursuit()\nomp_model.fit(train_X,train_y)\nomp_model_pred = omp_model.predict(val_X)\nomp_model_mae = mean_absolute_error(omp_model_pred,val_y)\nprint(\"MAE on OMP model = \",omp_model_mae)\nomp_model_rmse = mean_squared_error(omp_model_pred,val_y,squared = False)\nprint(\"RMSE on OMP model = \",omp_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-omp_model_rmse)))","e7fea36b":"from sklearn import linear_model\nbr_model = linear_model.BayesianRidge()\nbr_model.fit(train_X,train_y)\nbr_model_pred = br_model.predict(val_X)\nbr_model_mae = mean_absolute_error(br_model_pred,val_y)\nprint(\"MAE on Bayesian Ridge model = \",br_model_mae)\nbr_model_rmse = mean_squared_error(br_model_pred,val_y,squared = False)\nprint(\"RMSE on BR model = \",br_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-br_model_rmse)))","5434c3ac":"from sklearn import linear_model\nard_reg_model = linear_model.ARDRegression()\nard_reg_model.fit(train_X,train_y)\nard_reg_model_pred = ard_reg_model.predict(val_X)\nard_reg_model_mae = mean_absolute_error(ard_reg_model_pred,val_y)\nprint(\"MAE on ARD regresssor model = \",ard_reg_model_mae)\nard_reg_model_rmse = mean_squared_error(ard_reg_model_pred,val_y,squared = False)\nprint(\"RMSE on ARD Regressor model = \",ard_reg_model_rmse)\nprint(\"Final Score for comp = \",100*(max(0,1-ard_reg_model_rmse)))","00ef0a0b":"# final_model =  XGBRegressor(n_estimators = 11)\nfinal_model   =  pr_model\ncvScores = -1 * cross_val_score(final_model, X, y, cv = 5, scoring = \"neg_root_mean_squared_error\" )\nprint(\"RMSE scores = : \", cvScores)\ncomp_scores = 100 * (1 - cvScores)\nprint(\"Actual score =\",comp_scores)","fdb05d91":"print(\"Worst score = \", comp_scores.min())\nprint(\"Best score = \", comp_scores.max())\nprint(\"Average score = \",comp_scores.mean())","ad8e81f4":"# final_model.fit(X,y)\npr_model.fit(X,y)\nX_valid_for_submission = X_valid_full[features_4]\n\n# X_valid_for_submission.head()\n\n# Generating prediction on the testing data\nnew_X_valid = X_valid_for_submission.fillna(X_valid_for_submission.median())\nfull_prediction = pr_model.predict(new_X_valid)","8943d23c":"output = pd.DataFrame({ 'Employee_ID':X_valid_full.index,\n                       'Attrition_rate' : full_prediction\n                      })\noutput.to_csv('submission21.csv', index = False)","36b9e8dd":"**From the plot it is evident that attrition rates increases with a low level of education but the curve flatens out with an increase in the level of education. This might an important feature for predicting our target variable.**","4b01c347":"### 1. First let's checkout the relation between attrition rate and Age","afac883a":"### Missing Values","fa5a729d":"## Model_11 : Bayesian Ridge","18d7aac7":"### Model_2 : XGBoost","c4324829":"**Let's split the training_dataframe into training and testing subsets.**","bf1c9e39":"**Now we will save the data to a CSV file if asked so that we can successfully submit it to competitons**","36bb88a1":"## Model_4 : Elastic Net","905d973f":"**As we see there is not much variation with the attrition rates in comparision with the decision making skills like conceptual, analytical and directive(Label encoded) of the employees.**","8657842c":"## Model_12 : ARD Regressor","3858cb87":"## Building the ML model","82d84d5a":"**This relationship may be better understood using a 2D KDE plot**","a6d7c48c":"## Model_7 : Gradient Boosting Regression Trees for Poisson Regression","6ca6bf9d":"### 11. Next let's check the effect of travel on attrition rates","1bc024a5":"### 8. **Relationship between attrition rates and the job unit.**","7b907a8b":"***Imputation removes column names so we had to put them back***","eeb43f50":"### 2. Let us consider the factor of time since last promotion which might help us gain further insights to justify our previous assumption.","82b1412e":"**This plot shows a non linear relation between attrition Rate and growth rate. This is in sharp contrast to our earlier assumption.This can be further understood using a 2D Kde plot**","e6784914":"**We need to plot the correlation matrix to study the Anominised variables**","0e97db2a":"We will try to perform imputation on the missing values. But before imputation we will have to check for categorical values.","9ed35652":"### Fianlly the columns selected for our model would be:\n***From the last row of the correlation heat map we see the columns highly correlated to Attrition_rate are:***\n* Gender\n* Relationship_Status\n* Hometown\n* Unit\n* Decision_skill_possess\n* Time_since_promotion\n* growth_rate\n* Post_Level\n* Work_Life_balance\n* Compensation_and_Benefits\n* VAR 2\n* VAR 7\n* Time of service\n* Pay scale\n* age\n* Travel Rate","ea22a650":"**Generally the RSME score doesnt change much with respect to the alpha value of the Poisson Regression model**","cad353af":"**But by the Gradient descent method we found out that the best mean_absolute_error is found out at n_estimators value of 11. So we will use that value to train out xgb_model.**","cea80092":"**From this bar plot it is evident that that Male are more likely to leave an organisation than their female counterparts. Still, the difference is not huge enough to have any signifigant contribution to the attrition rates.**","6dc4c5b5":"To deal with columns having missing values we will perform imputation on the column with missing data. We might have dropped the column entirely but in that case the model might lost a lot of data, unless most values of the dropped column are missing.\nWe will first of all find out the colunms with missing data.","0de0f400":"## Model_10 : Orthogonal Matching Pursuit","7dc73ed9":"**From this 2D KDE plot it is evident that attrition rate is maximum between a growth rate of 20 - 40 and 50 - 70. Hence this would be an important variable in the prediction of the the target.**","74bb581d":"Now we will label encode the categorical columns. Fitting a label encoder to a column in the training data creates a corresponding integer-valued label for each unique value that appears in the training data. In case, the validation data contains values that don't also appear in the training data, the encoder will throw an error, because these values won't have an integer assigned to them.","7712d46f":"**From this trend it is evident that employee attrition rate is more profound between the age of 35 to 40**. This may be due to a variety of factors such as low work life ballance or less job satisfaction","9c0ad0e0":"**This shows that compensation and benefits are an important factor in deciding employee attrition rates**","f36af35a":"### 10. Next is the realtionship with decision making skills of the employess.","03c94648":"* Mean absolute error with Random Forest with feature_1 =  0.13961009653174605\n* Mean absolute error with Random Forest with feature_2 =  0.13989972828117914\n* Mean absolute error with Random Forest with feature_3 =  0.1365071338095238","2d5de278":"**Thus the job department or unit such as IT, logistics, Quality control and HR is a good feature for prediction of our target**","3a6d4594":"**According to our assumption work life balance does count towards the attrition rates of employees. There is a slight increase in the rates with an increase in work_life_balance value. There is a linear relation between the two**","59ccaeae":"### Cross validation on the final model","9c0864fd":"### 5. **Next let us understand the effects of eductaion level on our target variable i.e attrition rate. We may expect a linear realtionship.**","b681cc96":"**Initially there is a increase in attrition rates with respect to pay scale (till 3) then it slowly decreases and becomes stable over rest of the values and decreases beyond 8. Pay scale will be an interesting feature for our model.**","411c22ef":"## 12. Relationship with Compensation and benefits","ffba70a2":"### Categorical Values\nTo deal with categorical values we can drop the column if not necessary, label encode the variables or perform ohe hot encoding.","c70045de":"## Model_9 : Pasive Agressive Regressor","c001efb7":"**Using Gradient Descent to find the n_estimators**","57db7bb0":"## Model_8 : TweedieRegressor","bcc39aa5":"**We see that the attrition rate linearly dependent on the time since last promotion but increases a little bit if gap increases more than 3.5 years**","76e8e172":"### Looking for correaltions","0faa800c":"## Model_5 : Support Vector Regressor","d2991970":"### Let's predict the testing dataset on the final model and see the result\nThe trained model is completely unaware of the testing dataframe. By observing the RMSE values of all the above models we are choosing the Poisson Regressor algortihm for the final model.","8133bb28":"### Let's develop the Correlation Matrix with heatmap\nCorrelation states how the features are related to each other or the target variable. Heatmap makes it easy to identify which features are most related to the target variable","3402bd37":"### 7. Now we should analyze the growth rate of an employee in an organisation. Let's assume that the attrition rates will be inversely proportional to the growth rate","81b2bea0":"### 13. Now taking into account the various Anominised variables (VAR1, VAR2, VAR3, VAR4, VAR5, VAR6, VAR7)\n","ef876822":"### We will test out various machine learning algortihms and finally select the best out of them.","e5860b1e":"### 9. **Next in our list is a very important factor: Pay Scale.**","3d8050ca":"### To gain a insight into the data","7a2283a2":"### 3. **Next let's see the effect of time of sevice on the attrition rate of the employee**","f4831a2f":"## Imputation","1d999808":"**Therefore we will train our xgb model at n_estimators of 11. Another method will be to use `early_topping_rounds`**A small learning rate and large number of estimators will yield more accurate XGBoost models, though it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets learning_rate=0.1","55e5d947":"**This plot shows a strong relation between the two variables. Atrrition rate remains very low for low years of service but changes on increasing the term of service. This will be a very nice feature variable for our prediction**","4fd8359e":"### 6. **Let us understand the most important factor in prediction the attrition rates in a company. This may be the work life balance score. We may assume the trend to be linear. Let's see what the stats have to say about this**","c9c63cd8":"#### First and last few rows","02ca2535":"The values which are close to +1 show a very strong positive correaltion with the target variable,i.e directly proportional and for values close to -1 it shows a strong negative correlation ,i.e inversely proportional. For values lying near zero show that there is no linear correlation with the variable.","e4c58fa2":"> ## Model_3: Lasso Regression","976be45f":"### 4. **Lets us also compare the atrition rates for male and female employees.**","82695e76":"## Preprocessing and Data Wrangling","1df269e6":"## Model_6 : Poission Regressor","e03a1608":"<h1 align=\"center\"><u><b> Employee Attrition Rate Predictor<\/b><\/u><\/h1>\n<h3 align=\"center\"> Hackerearth machine learning challenge <\/h3>\n\n### Problem Statement:\nEmployees are the most important part of an organization. Successful employees meet deadlines, make sales, and build the brand through positive customer interactions.\n\nEmployee attrition is a major cost to an organization and predicting such attritions is the most important requirement of the Human Resources department in many organizations. In this problem, your task is to predict the attrition rate of employees of an organization.","6d6a4619":"## Data Visualization and analysis","c506dad5":"### Model_1 : Random Forest"}}