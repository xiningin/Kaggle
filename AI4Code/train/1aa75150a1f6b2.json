{"cell_type":{"f665578c":"code","806cde39":"code","ad548297":"code","1bf590c7":"code","56c60d75":"code","0ae8e131":"code","430ef276":"code","dbbc2e7e":"code","6c4c61db":"code","72ac7ac8":"code","a8911407":"code","1fd9850d":"code","16c19e8f":"code","069acaeb":"code","7bfbeb7d":"code","29a33633":"code","039e70cd":"code","8bc3d9af":"code","35c17d6b":"code","d2e051b3":"code","e77536ec":"code","e4217e6f":"code","a32755f1":"code","702fb047":"code","704db80f":"markdown","ee5088ee":"markdown","4911f616":"markdown","33cda146":"markdown","fd1b19f1":"markdown","cc6decd7":"markdown","1e303636":"markdown","560b39c2":"markdown","bac17aff":"markdown"},"source":{"f665578c":"import os\nGPU_id = 0\nos.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)","806cde39":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport time\nimport math\nfrom tqdm import tqdm\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torch import nn,optim\nimport torch.nn.functional as F\n\n%matplotlib inline","ad548297":"USE_GPU = torch.cuda.is_available()\nif USE_GPU:\n    print('Use GPU')\nelse:\n    print('Use CPU')","1bf590c7":"def show_mnist_batch(sample_batched):\n    \"\"\"Show image for a batch of samples.\"\"\"\n    images_batch, labels_batch = \\\n            sample_batched['image'], sample_batched['label']\n\n    grid = utils.make_grid(images_batch)\n    plt.imshow(grid.numpy().transpose((1, 2, 0)))","56c60d75":"def cross_entropy(y,yp):\n    # y is the ground truch\n    # yp is the prediction\n    yp[yp>0.99999] = 0.99999\n    yp[yp<1e-5] = 1e-5\n    return np.mean(-np.log(yp[range(yp.shape[0]),y.astype(int)]))\n\ndef accuracy(y,yp):\n    return (y==np.argmax(yp,axis=1)).mean()\n\ndef softmax(score):\n    score = np.asarray(score, dtype=float)\n    score = np.exp(score-np.max(score))\n    score = score\/(np.sum(score, axis=1).reshape([score.shape[0],1]))#[:,np.newaxis]\n    return score","0ae8e131":"class MnistDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, df, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = df\n        self.transform = transform\n        if 'label' in df.columns:\n            self.labels = df['label'].values\n            self.images = df.drop('label',axis=1).values\n        else:\n            self.labels = np.zeros(df.shape[0])\n            self.images = df.values\n        self.images = (self.images\/255.0).astype(np.float32).reshape(df.shape[0],28,28)\n        \n    \n    def head(self):\n        return self.df.head()\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        label = np.array(self.labels[idx])\n        image = self.images[idx]\n        sample = {'image': image, 'label': label}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample","430ef276":"class ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n        # torch image: [C, H, W]\n        return {'image': torch.from_numpy(image).unsqueeze(0),\n                'label': torch.from_numpy(label)}","dbbc2e7e":"class Logistic_Model(nn.Module):\n    def __init__(self,num_fea,num_class):\n        super().__init__()\n        #nn.Linear(input_dim, output_dim)\n        self.lin = nn.Linear(num_fea,num_class)\n\n    def forward(self, xb):\n        B = xb.size()[0]\n        if len(xb.size())>2:\n            xb = xb.view(B,-1) # 4D tensor of B,C,H,W -> 2D tensor B,CxHxW\n        return self.lin(xb)","6c4c61db":"class SimpleCNN(torch.nn.Module):\n    \n    #Our batch shape for input x is (3, 32, 32)\n    \n    def __init__(self,h,w,c,num_class):\n        super(SimpleCNN, self).__init__()\n        \n        #Input channels = 3, output channels = 18\n        self.h = h\n        self.w = w\n        self.c = c\n        self.num_class = num_class\n        \n        self.conv1 = torch.nn.Conv2d(c, 18, kernel_size=3, stride=1, padding=1)\n        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        \n        #4608 input features, 64 output features (see sizing flow below)\n        self.fc1 = torch.nn.Linear(18 * h\/\/2 * w\/\/2, 64)\n        \n        #64 input features, 10 output features for our 10 defined classes\n        self.fc2 = torch.nn.Linear(64, 10)\n        \n    def forward(self, x):\n        \n        #Computes the activation of the first convolution\n        #Size changes from (3, 32, 32) to (18, 32, 32)\n        x = F.relu(self.conv1(x))\n        \n        #Size changes from (18, 32, 32) to (18, 16, 16)\n        x = self.pool(x)\n        \n        #Reshape data to input to the input layer of the neural net\n        #Size changes from (18, 16, 16) to (1, 4608)\n        #Recall that the -1 infers this dimension from the other given dimension\n        x = x.view(-1, 18 * self.w\/\/2 * self.h\/\/2)\n        \n        #Computes the activation of the first fully connected layer\n        #Size changes from (1, 4608) to (1, 64)\n        x = F.relu(self.fc1(x))\n        \n        #Computes the second fully connected layer (activation applied later)\n        #Size changes from (1, 64) to (1, 10)\n        x = self.fc2(x)\n        return(x)","72ac7ac8":"class Learner(object):\n    \n    def __init__(self,model,**params): \n        self.model = model\n        if USE_GPU:\n            self.model.cuda()\n        self.params = params\n        \n    def predict(self,test_dl):\n        yps = []\n        for batch in tqdm(test_dl):\n            xb, yb = batch['image'],batch['label']\n            if USE_GPU:\n                xb, yb = xb.cuda(),yb.cuda()\n            pred = self.model(xb)\n            if USE_GPU:\n                yps.append(pred.cpu().detach().numpy())\n            else:\n                yps.append(pred.detach().numpy())\n        yps = np.vstack(yps)\n        yps = softmax(yps)\n        return yps\n        \n    def fit(self,train_dl,valid_dl=None,\n            epochs=10,lr=0.001,wd=0.1):\n        opt_type = self.params.get('opt','SGD')\n        if opt_type == 'SGD':\n            opt = optim.SGD(self.model.parameters(), lr=lr)\n        for epoch in range(epochs):\n            train_loss = 0\n            for batch in tqdm(train_dl):\n                xb, yb = batch['image'],batch['label']\n                if USE_GPU:\n                    xb, yb = xb.cuda(),yb.cuda()\n                pred = self.model(xb)\n                loss = F.cross_entropy(pred, yb)\n                if USE_GPU:\n                    train_loss += loss.cpu().detach().numpy()\n                else:\n                    train_loss += loss.detach().numpy()\n                loss.backward()\n                opt.step()\n                opt.zero_grad()\n            if valid_dl is None:\n                print('Epoch %d Training Loss:%.4f'%(epoch,\n                            train_loss\/len(train_dl)))\n                continue\n            yps = []\n            yrs = []\n            for batch in tqdm(valid_dl):\n                xb, yb = batch['image'],batch['label']\n                if USE_GPU:\n                    xb, yb = xb.cuda(),yb.cuda()\n                pred = self.model(xb)\n                if USE_GPU:\n                    yps.append(pred.cpu().detach().numpy())\n                    yrs.append(yb.cpu().detach().numpy())\n                else:\n                    yps.append(pred.detach().numpy())\n                    yrs.append(yb.detach().numpy())\n            yps = np.vstack(yps)\n            yps = softmax(yps)\n            yrs = np.concatenate(yrs)\n            ce = cross_entropy(yrs,yps)\n            acc = accuracy(yrs,yps)\n            print('Epoch %d Training Loss:%.4f Valid ACC: %.4f Cross Entropy:%4f'%(epoch,\n                            train_loss\/len(train_dl),acc,ce))\n            \n        self.opt = opt","a8911407":"%%time\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint(train_df.shape, test_df.shape)","1fd9850d":"train_df.head()","16c19e8f":"%%time\n\nval_pct = 0.2 # use 20% train data as local validation\nis_valid = np.random.rand(train_df.shape[0])<val_pct\ntrain_df, valid_df = train_df.loc[~is_valid], train_df.loc[is_valid]\nprint(train_df.shape, valid_df.shape)","069acaeb":"%%time\ntrain_dataset = MnistDataset(df=train_df,\n                            transform=transforms.Compose([\n                                               ToTensor()\n                                           ]))\nvalid_dataset = MnistDataset(df=valid_df,\n                            transform=transforms.Compose([\n                                               ToTensor()\n                                           ]))\ntest_dataset = MnistDataset(df=test_df,\n                            transform=transforms.Compose([\n                                               ToTensor()\n                                           ]))","7bfbeb7d":"fig = plt.figure(figsize=(20,8))\n\nfor i in range(len(train_dataset)):\n    sample = train_dataset[i]\n\n    print(i, sample['image'].shape)\n\n    ax = plt.subplot(1, 4, i + 1)\n    plt.tight_layout()\n    ax.set_title('Sample #{} Label {}'.format(i,sample['label']), fontsize=30)\n    ax.axis('off')\n    plt.imshow(sample['image'].numpy()[0],cmap='gray')\n\n    if i == 3:\n        plt.show()\n        break","29a33633":"%%time\n\nbatch_size = 128\ncpu_workers = 8\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n                        shuffle=True, num_workers=cpu_workers,\n                        drop_last=True)\n\nvalid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,\n                        shuffle=False, num_workers=cpu_workers,\n                        drop_last=False)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size,\n                        shuffle=False, num_workers=cpu_workers,\n                        drop_last=False)","039e70cd":"for i_batch, sample_batched in enumerate(train_dataloader):\n    print(i_batch, sample_batched['image'].size(),\n          sample_batched['label'].size())\n\n    plt.figure(figsize=(10,10))\n    show_mnist_batch(sample_batched)\n    plt.axis('off')\n    plt.ioff()\n    plt.show()\n    break","8bc3d9af":"model = SimpleCNN(h=28,w=28,c=1,num_class=10)\nlearn = Learner(model)","35c17d6b":"%%time\nlearn.fit(train_dl=train_dataloader,\n          valid_dl=valid_dataloader,\n          lr=0.01,\n          epochs=50)","d2e051b3":"%%time\nyp = learn.predict(valid_dataloader)","e77536ec":"%%time\nacc = accuracy(valid_df.label.values,yp)\nce = cross_entropy(valid_df.label.values,yp)\nprint('Valid ACC: %.4f Cross Entropy:%4f'%(acc,ce))","e4217e6f":"%%time\nyp = learn.predict(test_dataloader)","a32755f1":"sub = pd.DataFrame()\nsub['ImageId'] = np.arange(yp.shape[0])+1\nsub['Label'] = np.argmax(yp,axis=1)\nsub.head()","702fb047":"from datetime import datetime\nclock = \"{}\".format(datetime.now()).replace(' ','-').replace(':','-').split('.')[0]\nout = 'pytorch_%s_acc_%.4f_ce_%.4f.csv'%(clock,acc,ce)\nprint(out)\nsub.to_csv(out,index=False)","704db80f":"### A model is a subclass of nn.Module which defines a computing graph","ee5088ee":"## Inspect datasets","4911f616":"### A learner has functions fit() and predict(), like the sklearn model","33cda146":"## Function and class definitions","fd1b19f1":"### Read csv","cc6decd7":"### Illustrate the first batch","1e303636":"## Training","560b39c2":"### Data loader generates batch of samples with multi-thread functions.","bac17aff":"### Predict and write submission"}}