{"cell_type":{"8a955dda":"code","14989f6d":"code","cd7fbd89":"code","9ecc91ab":"code","6e9831a5":"code","022462ed":"code","78c9fb34":"code","f0676f58":"code","70d596d6":"code","5ad9c9f0":"code","84f27077":"code","999028d3":"code","33a9ec19":"code","78630693":"code","ac23c4c5":"code","71aecf70":"code","78220533":"code","6560f56b":"code","1ef929db":"code","ed9c7c79":"code","0264900f":"code","862cdb45":"code","c944e060":"code","9a632fcd":"code","e5d48710":"code","353c02f2":"code","a6824598":"code","f63156fd":"code","1dc50f99":"code","2662efac":"code","9c3a3c55":"code","12a46c25":"code","55ddf3b9":"code","93baecc2":"code","7e404a73":"code","0c9d8125":"code","6e43f989":"code","5af4340d":"code","25293b74":"code","ab310ab9":"code","2bc39b55":"code","35969c01":"markdown","e546af50":"markdown","1f35ad4a":"markdown","48e083dc":"markdown","a8c3bba7":"markdown","ec4de9bd":"markdown","4c0d45eb":"markdown","2d1a6718":"markdown","736d7645":"markdown","a3140cc3":"markdown","fe4bff49":"markdown","c0cd5c64":"markdown","22b8a00c":"markdown","7e13e2c6":"markdown"},"source":{"8a955dda":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport string\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score","14989f6d":"website_df=pd.read_csv('..\/input\/website-classification\/website_classification.csv')\nwebsite_df.head()","cd7fbd89":"print('Shape of dataframe:',website_df.shape)\nprint('Columns in dataframe:',website_df.columns)","9ecc91ab":"website_df.drop('Unnamed: 0',axis=1,inplace=True)   #Dropping the unnecessary column\nwebsite_df.head()","6e9831a5":"website_df.isnull().sum()   #Checking whether there are any null values.","022462ed":"df_a=website_df.groupby('Category').count().sort_values(by='Category',ascending=True)\ndf_a.index","78c9fb34":"df_a","f0676f58":"sns.countplot(y=website_df['Category'],order=website_df['Category'].value_counts().index[:16])","70d596d6":"#More frequently occuring words appear larger.\n\n\nfrom wordcloud import WordCloud,STOPWORDS\ncontent=''\nstopwords_df=set(STOPWORDS)\n\nfor i in website_df['cleaned_website_text']:\n    tokens=i.split(' ')\n    \n    content=content+' '.join(tokens)+' '\n\nwordcloud_df=WordCloud(width=800,height=600,background_color='white',stopwords=stopwords_df).generate(content)\nplt.figure(figsize=(8,8))\nplt.axis('off')\nplt.imshow(wordcloud_df)","5ad9c9f0":"#Education\nsum_ed=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Education'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_ed=sum_ed+a\nprint('Total number of words including all the educational websites: ', sum_ed)\n\n\n\n#Business\/Corporate\nsum_business=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Business\/Corporate'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_business=sum_business+a\nprint('Total number of words including all the Business\/Corporate website: ', sum_business)\n\n\n#Travel\nsum_travel=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Travel'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_travel=sum_travel+a\nprint('Total number of words including all the travel websites: ', sum_travel)\n\n\n#Streaming services\nsum_stream=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Streaming Services'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_stream=sum_stream+a\nprint('Total number of words including all the Streaming services websites: ', sum_stream)\n\n\n#Sports\nsum_sports=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Sports'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_sports=sum_sports+a\nprint('Total number of words including all the sports websites: ', sum_sports)\n\n\n#E-commerce\nsum_commerce=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='E-Commerce'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_commerce=sum_commerce+a\nprint('Total number of words including all the E-commerce website: ', sum_commerce)\n\n\n\n#Games\nsum_game=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Games'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_game=sum_game+a\nprint('Total number of words including all the games websites: ', sum_game)\n\n\n\n#News\nsum_news=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='News'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_news=sum_news+a\nprint('Total number of words including all the News websites: ', sum_news)\n\n\n\n#Health and Fitness\nsum_health=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Health and Fitness'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_health=sum_health+a\nprint('Total number of words including all the Health and Fitness websites: ', sum_health)\n\n\n\n#Computers and Technology\nsum_comp=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Computers and Technology'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_comp=sum_comp+a\nprint('Total number of words including all the Computers and Technology website: ', sum_comp)\n\n\n\n#Photography\nsum_photog=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Photography'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_photog=sum_photog+a\nprint('Total number of words including all the Photography websites: ', sum_photog)\n\n\n\n#Food\nsum_food=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Food'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_food=sum_food+a\nprint('Total number of words including all the Food websites: ', sum_food)\n\n\n\n#Law and Government\nsum_law=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Law and Government'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_law=sum_law+a\nprint('Total number of words including all the Law and Government websites: ', sum_law)\n\n\n\n#Social Networking and Messaging\nsum_social=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Social Networking and Messaging'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_social=sum_social+a\nprint('Total number of words including all the Social Networking and Messaging website: ', sum_social)\n\n\n\n#Adult\nsum_ad=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Adult'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_ad=sum_ad+a\nprint('Total number of words including all the adult websites: ', sum_ad)\n\n\n\n#Forums\nsum_forum=0\nfor i in website_df['Category'].index:\n    if (website_df['Category'][i]=='Forums'):\n        a= len(website_df['cleaned_website_text'][i])\n        sum_forum=sum_forum+a\nprint('Total number of words including all the Forums websites: ', sum_forum)\n\n\nl=[sum_ed,sum_forum,sum_news,sum_sports,sum_health,sum_comp,sum_ad,sum_photog,sum_social,sum_law,sum_food,sum_game,sum_stream,sum_travel,sum_business,sum_commerce]\nx1=pd.Series(l).to_frame()\nx2=pd.Series(['Education','Business\/Corporate','Travel','Streaming Services','Sports','E-Commerce','Games','News','Health & Fitness','Computer and Technology','Photography','Food','Law & Government','Social Neworking & Messaging','Adult','Forums']).to_frame()\nx3=pd.concat([x2,x1],axis=1,ignore_index=True)\nx3.columns=['Website category','Sum of length of all the words on the site']\nx3.sort_values(by='Sum of length of all the words on the site',ascending=False)","84f27077":"sns.set_style('whitegrid')\nplt.figure(figsize=(20,6))\nsns.barplot(x='Website category',y='Sum of length of all the words on the site',data=x3)\nplt.xticks(rotation=75)\nplt.title('Category wise total number of characters on all the websites (combined)',fontweight='bold',fontsize=15)","999028d3":"X=website_df['cleaned_website_text']      \ny=website_df['Category']   ","33a9ec19":"def text_process(value):\n    nopunc=[char for char in value if char not in string.punctuation]   #Removes punctuation\n    nopunc=''.join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]  #Removes stopwords","78630693":"X.apply(text_process).head()    #Getting the cleaned text (head of the dataframe displayed here)","ac23c4c5":"from sklearn.feature_extraction.text import CountVectorizer \nX=CountVectorizer().fit_transform(X)","71aecf70":"from sklearn.feature_extraction.text import TfidfTransformer    \nX=TfidfTransformer().fit_transform(X)               ","78220533":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=101)","6560f56b":"from sklearn.naive_bayes import MultinomialNB","1ef929db":"website_class=MultinomialNB()","ed9c7c79":"website_class.fit(X_train,y_train)","0264900f":"predictions=website_class.predict(X_test)","862cdb45":"predictions","c944e060":"print(classification_report(predictions,y_test))","9a632fcd":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(predictions,y_test),cmap='YlGn_r',annot=True)","e5d48710":"print(accuracy_score(predictions,y_test))","353c02f2":"from sklearn.ensemble import RandomForestClassifier","a6824598":"rfc=RandomForestClassifier()","f63156fd":"rfc.fit(X_train,y_train)","1dc50f99":"predictions_rfc=rfc.predict(X_test)","2662efac":"predictions_rfc","9c3a3c55":"print(classification_report(predictions_rfc,y_test))","12a46c25":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(predictions_rfc,y_test),cmap='PiYG',annot=True)","55ddf3b9":"print(accuracy_score(predictions_rfc,y_test))","93baecc2":"from sklearn.svm import SVC","7e404a73":"svc=SVC()","0c9d8125":"svc.fit(X_train,y_train)","6e43f989":"predictions_svc=svc.predict(X_test)","5af4340d":"predictions","25293b74":"print(classification_report(predictions_svc,y_test))","ab310ab9":"plt.figure(figsize=(10,6))\nsns.heatmap(confusion_matrix(predictions_rfc,y_test),cmap='YlOrBr',annot=True)","2bc39b55":"print(accuracy_score(predictions_svc,y_test))","35969c01":"#### Categories of websites in the dataset","e546af50":"![image.png](attachment:122986e3-9079-45a6-861e-a7e1504de990.png)","1f35ad4a":"#### Removing stopwords","48e083dc":"From the plot it can be seen that the **top 5 website categories are:**\n    \n 1.Educational\n\n 2.Business\/Corporate\n    \n 3.Travel\n\n 4.Streaming Services\n    \n 5.Sports","a8c3bba7":"### Naive Bayes classifier","ec4de9bd":"#### It can be seen that Naive Bayes classifier gives maximum accuracy (approx. 88%)","4c0d45eb":"### Applying algorithm (Bag of Words model)","2d1a6718":"#### Total length of words on all websites (combined) of different categories ","736d7645":"### Exploratory data analysis","a3140cc3":"### Random Forest Classifier","fe4bff49":"### Importing libraries and getting the data","c0cd5c64":"The dataframe x3 and the plot above indicate the **total number of characters that have been used to make all the the websites of a particular category** or in other words, **the total length of the words that are there in all the websites of a particular category.**\n\n\n**For example**- All the 107 travel websites have used 1020091 characters in total. \n\nThe **top 5 website categories that have used maximum number of characters** are:\n\n1.Travel\n\n2.Streaming Services\n\n3.Education\n\n4.Social Networing & Messaging\n\n5.Sports\n","22b8a00c":"#### Wordcloud","7e13e2c6":"### Support Vector Machines"}}