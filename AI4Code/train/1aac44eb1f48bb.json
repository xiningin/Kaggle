{"cell_type":{"34391e35":"code","57f75bdf":"code","74748a97":"code","1c80dae0":"code","f59f30b4":"code","91574f94":"code","5a2707ea":"code","86adf871":"code","fedc83a6":"code","c178f8eb":"code","ab6696aa":"code","f6deaf0a":"code","435290f0":"code","ee66498e":"code","71df1e8b":"code","477c5706":"code","1e7cf259":"code","20d0b63e":"code","7c1da564":"code","6c26a503":"markdown","5bbf26a4":"markdown","6e94c621":"markdown"},"source":{"34391e35":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom fastai.vision import *\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","57f75bdf":"from pathlib import Path\npath=Path('..\/input')\ndf_trn=pd.read_csv(path\/'X_train.csv')\ndf_label=pd.read_csv(path\/'y_train.csv')\ndf_test=pd.read_csv(path\/'X_test.csv')\n","74748a97":"df_all=pd.concat([df_trn,df_test])\ndf_all['train']=[0]*len(df_trn)+[1]*len(df_test)\ndf_all.series_id[df_all.train==1]+=len(df_trn)\ndf_label=pd.DataFrame(data=[0]*len(np.unique(df_trn.series_id))+[1]*len(np.unique(df_test.series_id)),columns=['train'])","1c80dae0":"from tqdm import tqdm\n\nimport seaborn as sns","f59f30b4":"sns.pairplot(df_all.sample(frac=0.1),hue='train',vars=['orientation_X','orientation_Y','orientation_Z','orientation_W'])","91574f94":"sns.pairplot(df_all.sample(frac=0.1),hue='train',vars=['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z'])","5a2707ea":"sns.pairplot(df_all.sample(frac=0.1),hue='train',vars=['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z'])","86adf871":"cols=['linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\nfor col in cols:\n\n    df_all[col]=(df_all[col])\/(85)\ncols=['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\n","fedc83a6":"def get(self,i):\n    return tensor(np.append((self.items[i][1]['measurement_number'][:,None].astype(np.float32)-64)\/512,self.items[i][1][cols].values.astype(np.float32),axis=1))","c178f8eb":"sample_list=df_all.groupby('series_id')","ab6696aa":"ItemList.get=get","f6deaf0a":"src=(ItemList(sample_list,inner_df=df_label).split_by_rand_pct(0.2).label_from_df(cols='train'))\ndata=src.databunch(bs=32)\n","435290f0":"class LSTMClassifier(nn.Module):\n\n    def __init__(self, in_dim, hidden_dim, num_layers, dropout, bidirectional, num_classes, batch_size):\n        super(LSTMClassifier, self).__init__()\n        self.in_dim = in_dim\n        self.hidden_dim = hidden_dim\n        self.batch_size = batch_size\n        self.bidirectional = bidirectional\n        self.num_dir = 2 if bidirectional else 1\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.lstm = nn.LSTM(input_size=self.in_dim, hidden_size=self.hidden_dim, num_layers=self.num_layers, dropout=self.dropout, bidirectional=self.bidirectional,\n                            batch_first=True)\n        self.gru = nn.GRU(self.hidden_dim * 2, self.hidden_dim, bidirectional=self.bidirectional, batch_first=True)\n        self.fc = nn.Sequential(\n            nn.Linear(2048, hidden_dim),\n            nn.ReLU(True),\n            nn.Dropout(p=dropout),\n            nn.Linear(hidden_dim, num_classes),\n        )\n\n    def forward(self, x):\n\n        lstm_out, _ = self.lstm(x)\n        gru_out, _ = self.gru(lstm_out)\n        avg_pool_l = torch.mean(lstm_out, 1)\n        max_pool_l, _ = torch.max(lstm_out, 1)\n        \n        avg_pool_g = torch.mean(gru_out, 1)\n        max_pool_g, _ = torch.max(gru_out, 1)\n        x = torch.cat((avg_pool_g, max_pool_g, avg_pool_l, max_pool_l), 1)\n        y = self.fc(x)\n        return y","ee66498e":"model = LSTMClassifier(11, 256, 2, 0.5, True, 2, 32)\n\nlearn=Learner(data,model,metrics=accuracy)\n\nlearn.lr_find(num_it=200)\n\nlearn.recorder.plot()\n\n","71df1e8b":"src_list=ItemList(sample_list,inner_df=df_label)\n#for i,(idx_train,idx_val) in enumerate(sss.split(np.unique(df_trn.series_id), df_label.surface)):\nfrom sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n                                     StratifiedKFold, GroupShuffleSplit,\n                                     GroupKFold, StratifiedShuffleSplit)\nsss = StratifiedKFold(n_splits=5, random_state=0)\nsss.get_n_splits(sample_list, df_label.train)","477c5706":"for i,(idx_train,idx_val) in enumerate(sss.split(sample_list, df_label.train)):\n    print(df_label.train[idx_train].mean(),df_label.train[idx_val].mean())","1e7cf259":"for i,(idx_train,idx_val) in enumerate(sss.split(sample_list, df_label.train)):\n    src=(src_list.split_by_idxs(idx_train,idx_val).label_from_df(cols='train'))\n    data=src.databunch(bs=32)\n    model = LSTMClassifier(11, 256, 2, 0.5, True, 2, 32)\n    learn=Learner(data,model,metrics=accuracy)\n    learn.fit_one_cycle(15,1e-3)\n    learn.recorder.plot_losses()\n    learn.recorder.plot_metrics()\n    x,y=learn.get_preds()\n    ","20d0b63e":"preds,y,losses = learn.get_preds(with_loss=True)\ninterp = ClassificationInterpretation(learn, preds, y, losses)","7c1da564":"interp.plot_confusion_matrix(figsize=(16,9))","6c26a503":"Not too much information difference in train and testset I guess","5bbf26a4":"# Adversarial Validation\nI appendes all samples from training and test set and then tried to predict if the sample is from the train or test set.\nI case the test and train set vary this should result in a high accuracy. However I got a accuracy between 0.5 and 0.6.\nI guess this means they differ a bit but not significantly.\nThe CV in my CNN and LSTM approach (both ~0.8 CV and 0.6 LB) vary quite a bit though.\nI'm not sure what to make of it. Maybe my models overfit on some structure related to group_id?\n\n## Any Idea what I could make of it?\n I was thinking of testing a group_id validation scheme with this method to see if I can get it even closer to 0.5\n alternatively one could take the samples from the training set that were most like the test set and only use those to train\n","6e94c621":"Yep its stratified"}}