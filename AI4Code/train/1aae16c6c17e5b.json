{"cell_type":{"4a64a3c6":"code","ebc61eb7":"code","9c9422b5":"code","b4541c80":"code","201a849c":"code","76ca8765":"code","9f179fd6":"code","7cf2275c":"code","97023b64":"code","2496cb78":"code","b75af2b8":"code","3a55a088":"code","1a3c868a":"code","cb459d63":"code","f51161ef":"code","b218f9ec":"code","c9bbb6e5":"code","950adc12":"code","e0d1a99c":"code","1f2ab614":"code","4965c6a1":"code","3ec2937d":"code","d869ce74":"code","d09df924":"code","407db8dd":"code","7c8d4c45":"code","9eef4cf1":"code","fa32f169":"markdown","47acf5be":"markdown","e1a445f3":"markdown","e835c4c6":"markdown","b4081548":"markdown","334c4cf2":"markdown","27fe2e60":"markdown","381eff91":"markdown","db446601":"markdown","01c021b7":"markdown","2037974a":"markdown","097d295a":"markdown","d5e19369":"markdown","43a51498":"markdown"},"source":{"4a64a3c6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nimport string\nimport nltk\n\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import FrenchStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping","ebc61eb7":"nltk.download('stopwords')\nnltk.download('punkt')","9c9422b5":"dataset_train = pd.read_csv('..\/input\/frenchfakenewsdetector\/datafake_train.csv',delimiter=';', encoding='utf-8')\ndataset_test = pd.read_csv('..\/input\/frenchfakenewsdetector\/datafake_test.csv',delimiter=';', encoding='utf-8')\ndataset_train.head(5)","b4541c80":"def reformatDataset(df):\n  df = df.drop([\"media\"], axis=1)\n  df = df.rename(columns={'fake': 'label', 'post': 'text'})\n  return df\n\ndataset_train = reformatDataset(dataset_train)\ndataset_test = reformatDataset(dataset_test)","201a849c":"dataset_train.head()","76ca8765":"def remove_punct(_str):\n  _str = re.sub('['+string.punctuation+']', ' ', _str)\n  _str = re.sub('[\\r]', ' ', _str)\n  _str = re.sub('[\\n]', ' ', _str)\n  _str = re.sub('[\u00ab\"]', ' ', _str)\n  return _str","9f179fd6":"dataset_train['text'][0]","7cf2275c":"remove_punct(dataset_train['text'][0])","97023b64":"dataset_train['text'] = dataset_train['text'].apply(remove_punct)\ndataset_test['text'] = dataset_test['text'].apply(remove_punct)","2496cb78":"french_stopwords = set(stopwords.words('french'))\nfiltre_stopfr =  lambda text: [token for token in text if token.lower() not in french_stopwords]","b75af2b8":"filtre_stopfr(word_tokenize(dataset_train['text'][0]) )","3a55a088":"def remove_stop_words_fr(_str):\n  return filtre_stopfr(word_tokenize(_str))\n\ndataset_train['text'] = dataset_train['text'].apply(remove_stop_words_fr)\ndataset_test['text'] = dataset_test['text'].apply(remove_stop_words_fr)","1a3c868a":"dataset_train","cb459d63":"tokenizer = Tokenizer(num_words=60000, lower=None)\ntokenizer.fit_on_texts(dataset_train['text'])\nseq_train = tokenizer.texts_to_sequences(dataset_train['text'])\nseq_test = tokenizer.texts_to_sequences(dataset_test['text'])","f51161ef":"print(\"Nb of texts: \" , tokenizer.document_count)\n#print(\"Word indexes: \" , tokenizer.word_index)\nprint(\"Word\/token counts: \" , len(tokenizer.word_counts))\nnb_token = len(tokenizer.word_index)","b218f9ec":"ds_train = pad_sequences(seq_train)\ntrain_seq_len = ds_train.shape[1]\nds_train.shape","c9bbb6e5":"ds_test = pad_sequences(seq_test, maxlen=train_seq_len)\nds_test.shape","950adc12":"y_train = dataset_train['label'].astype('int') \ny_test = dataset_test['label'].astype('int') ","e0d1a99c":"train_seq_len, nb_token","1f2ab614":"ds_train","4965c6a1":"ds_train.shape","3ec2937d":"mon_cnn = tf.keras.Sequential()\n\nmon_cnn.add(Input(shape=(train_seq_len,)))\n\nmon_cnn.add(Embedding(nb_token + 1, 20))\n\nmon_cnn.add(Conv1D(32, 3, activation='relu'))\nmon_cnn.add(MaxPooling1D(3))\n\nmon_cnn.add(Conv1D(64, 3, activation='relu'))\nmon_cnn.add(MaxPooling1D(3))\n\nmon_cnn.add(Conv1D(128, 3, activation='relu'))\nmon_cnn.add(GlobalMaxPooling1D())\n\nmon_cnn.add(Dense(1, activation='sigmoid'))","d869ce74":"mon_cnn.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","d09df924":"mon_cnn.summary()","407db8dd":"early_stop = EarlyStopping(monitor='val_loss', patience=2)\nmon_cnn.fit(x=ds_train, \n            y=y_train, \n            validation_data=(ds_test, y_test), \n            epochs=10\n            ,callbacks=[early_stop]\n            )","7c8d4c45":"losses = pd.DataFrame(mon_cnn.history.history)\nlosses[['accuracy', 'val_accuracy']].plot()","9eef4cf1":"losses[['loss', 'val_loss']].plot()","fa32f169":"Data comes from Kaggle  \nhttps:\/\/www.kaggle.com\/hgilles06\/frenchfakenewsdetector\/code","47acf5be":"Create The CNN Network using Tensorflow","e1a445f3":"Prepare data for CNN Tensorflow creation:  \n1) Tokenize first the two dataset   \n2) Pad datasets  ","e835c4c6":"## Datasets creation","b4081548":"Use NLTK to remove french stopwords","334c4cf2":"Fit the Model & use earlystopping function to accelerate the fit step & prevent from overfitting","27fe2e60":"## CNN Modeling","381eff91":"## Dataset (train & test) acquisition","db446601":"## Work on text data first","01c021b7":"### Remove punctuation","2037974a":"Just take a look with a sample ...","097d295a":"# Fake News Detector (French data)","d5e19369":"### Remove stop words (french)","43a51498":"Reformat dataset to make it more readable:"}}