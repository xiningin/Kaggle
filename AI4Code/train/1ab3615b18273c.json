{"cell_type":{"7585044c":"code","1faaf519":"code","20b11db5":"code","cbe3c4e9":"code","13902443":"code","6884067f":"code","4bd99702":"code","d30d1de4":"code","efb803a3":"code","f71f617c":"code","85f9179c":"code","472c35bd":"code","88fc13cd":"code","75525ff6":"code","1f698f9e":"code","48406d07":"code","8d0add0b":"code","2e295082":"code","3677d02c":"code","545cc20a":"code","316e2bd8":"code","335215f6":"code","8e5bc255":"code","1c5c4c6d":"code","32f7abd6":"code","df4abb40":"code","ca13d767":"code","12fae97d":"code","193da727":"code","2530e1dd":"code","28ce6a48":"code","d8247d5b":"code","bd5ebebc":"code","8ec1307c":"code","3013752d":"code","0ebe8687":"code","f3b43836":"code","0983bc6f":"markdown","f3889e7e":"markdown","6b3581c6":"markdown","83e60d98":"markdown","6f04d513":"markdown","bc532067":"markdown","89689be5":"markdown","4abd3382":"markdown","9d52eca6":"markdown","06ce1ba8":"markdown","c75dac5d":"markdown","ac157cc5":"markdown","4f69b0ba":"markdown","4b1829a2":"markdown","89311fc5":"markdown","11f367b0":"markdown","f8c9f7a3":"markdown","b5840824":"markdown","9d220fc1":"markdown","39d68a4d":"markdown","e012458e":"markdown","7b10270d":"markdown","1ae818f6":"markdown","4ab4b1a8":"markdown","b9a1cdc9":"markdown","302b91a1":"markdown","f2d27a6e":"markdown","2c310f08":"markdown","c979a178":"markdown","88e451cb":"markdown","bbf913eb":"markdown","19422920":"markdown"},"source":{"7585044c":"import os\nimport re\nimport ssl\nimport glob\nimport zipfile\nimport pandas as pd\nimport string, unicodedata\nfrom git.repo.base import Repo\nimport nltk\nimport pyLDAvis.sklearn\nfrom nltk.stem import WordNetLemmatizer \nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer","1faaf519":"\"\"\"\nUseful helper function to disable the SSL\n\"\"\"\ndef disable_SSL_Check():\n    try:\n        _create_unverified_https_context = ssl._create_unverified_context\n    except AttributeError:\n        pass\n    else:\n        ssl._create_default_https_context = _create_unverified_https_context","20b11db5":"\"\"\"\nThis function get the repository containing the folder name\n\"\"\"\ndef collect_data_from_url(url, data_folder, dest_folder):\n    \"\"\"\n    @param:\n        url: the URL used to collect the PDF files\n        data_folder: the folder containing the data to be analyzed\n        dest_folder: the folder that will contain the cloned folder\n        \n    @return\n        the path to the folder containing all the files\n    \"\"\"\n    \n    if not os.path.exists(dest_folder):\n        Repo.clone_from(url, dest_folder) # clone the repo\n        folder_name = dest_folder+'\/'+ data_folder\n        \n        if(zipfile.is_zipfile(folder_name)):\n            with zipfile.ZipFile(folder_name, 'r') as zip_ref:\n                zip_ref.extractall(dest_folder)\n                \n        # The name of the file without any extension\n        return os.path.splitext(folder_name)[0]\n\n    print(\"The folder exists\")\n    # The folder containing the data will \n    ","cbe3c4e9":"# Call the collector\nurl = \"https:\/\/github.com\/keitazoumana\/lda-tutorial.git\"\ndata_folder = \"topic_modeling.zip\"\ndest_folder = \"\/kaggle\/working\/text_files\"\n\ncollect_data_from_url(url, data_folder, dest_folder)","13902443":"\"\"\"\nThis function extracts the content of each file and create a final pandas dataframe\n\"\"\"\ndef extract_txt_into_df(path_to_folder, extension):\n    \"\"\"\n    @params:\n        path_to_folder: the path to the folder containing the .txt files\n        extention: the extension of the files in the folder (.txt in our case)\n        \n    @return:\n        the pandas dataframe of the files\n    \"\"\"\n    list_of_dict = []\n    all_files = glob.glob(path_to_folder+'\/*'+ extension)\n    \n    for file in all_files: \n        parsed_file = {}\n        \n        with open(file, \"r\") as current_file:\n            parsed_file[\"file_name\"] = file\n            parsed_file[\"file_content\"] = current_file.read()\n            list_of_dict.append(parsed_file)\n        \n    text_df = pd.DataFrame(list_of_dict) # convert the list of dictionary to pandas dataframe\n    \n    return text_df","6884067f":"txt_df = extract_txt_into_df(\"\/kaggle\/working\/text_files\/topic_modeling\", \".txt\")","4bd99702":"txt_df.head()","d30d1de4":"txt_df.shape","efb803a3":"# look at the text data of the second file in the dataframe\ntxt_df.loc[2][\"file_content\"]","f71f617c":"\"\"\"\n1. Removes URLs from the given text\n\"\"\"\ndef remove_url_links(text):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n\n    @return:\n        the text without URLs\n    \"\"\"\n    \n    #text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    return re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE).lstrip()","85f9179c":"\"\"\"\n2. Remove non-ASCII characters from the text\n\"\"\"\ndef remove_non_ascii(text):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n\n    @return:\n        the text without non-ASCII characters\n    \"\"\"\n    new_words = []\n    for word in text.split():\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_word = ''.join(i for i in new_word if not i.isdigit())\n        new_words.append(new_word.lower())\n    return \" \".join(new_words)","472c35bd":"\"\"\"\n3. removes specific characters like: *-_\n\"\"\"\ndef remove_specifis_characters(text):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n\n    @return:\n        the text without the bellow characters\n    \"\"\"\n    text = re.sub(\"\\*\", \"\", text)\n    text = re.sub(\"\\-\", \"\", text)\n    text = re.sub(\"\\_\", \"\", text)\n    text = re.sub(\"\\#\", \"\", text)\n    text = re.sub(\"\\@\", \"\", text)\n    text = re.sub(\"\\[\", \"\", text)\n    text = re.sub(\"\\]\", \"\", text)\n    text = re.sub(\"\\(\", \"\", text) \n    text = re.sub(\"\\)\", \"\", text)\n    \n    return text","88fc13cd":"\"\"\"\n4. removes words of less than a given number of characters from the text\n\"\"\"\ndef remove_words_given_lenght(text, lenght=3):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n        lenght: the number of characters  \n    @return:\n        the text without the bellow characters\n    \"\"\"\n    new_words = []\n    \n    for word in text.split():\n        if(len(word)>lenght):\n            new_words.append(word)\n            \n    return \" \".join(new_words)","75525ff6":"\"\"\"\n5. perform the lemmatization task\n\"\"\"\ndef lemmatize_data(text):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n        lenght: the number of characters  \n    @return:\n        the lemmatized format of the text\n    \"\"\"\n    lemmatizer = WordNetLemmatizer()\n    word_list = nltk.word_tokenize(text)\n    lemmatized_text = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n    \n    return lemmatized_text","1f698f9e":"\"\"\"\nFinal function combining all the previous ones\n\"\"\"\ndef clean_normalize_text(text):\n    \"\"\"\n    @param:\n        text: the corresponding text to preprocess\n\n    @return:\n        the text after applying the previous transformations\n    \"\"\"\n    text = remove_url_links(text)\n    text = remove_non_ascii(text)\n    text = remove_specifis_characters(text)\n    text = remove_words_given_lenght(text)\n    text = lemmatize_data(text)\n    \n    return text","48406d07":"\"\"\"\nThis function performs the preprocessing operation on the text column of the dataframe\n\"\"\"\ndef data_processor(dataframe, col_name):\n    \"\"\"\n    @param:\n        dataframe: the dataframe to preprocess\n        col_name: the name of the column to perform the processing on\n    @return:\n        the dataframe after applying the clean_normalize_text function to the \"text\" column\n    \"\"\"\n    dataframe[\"preprocessed\"] = dataframe[col_name].apply(lambda text: clean_normalize_text(str(text)))\n    return dataframe","8d0add0b":"# Calling the Processor\nprecessed_df = data_processor(txt_df, \"file_content\")","2e295082":"# Lets check the result\nprint(\"Before the Processing\")\nprint(txt_df.loc[2][\"file_content\"]+'\\n')\n\nprint(\"#\"*100)\nprint(\"After the Processing\")\nprint(txt_df.loc[2][\"preprocessed\"])","3677d02c":"cv = CountVectorizer(max_df=0.95, min_df=2, stop_words=\"english\")  # removes stopwords by using the \"stop_words\" parameter.\ndtm = cv.fit_transform(txt_df['preprocessed'])","545cc20a":"dtm","316e2bd8":"feature_names = cv.get_feature_names()\nlen(feature_names) # show the total number of distinct words","335215f6":"feature_names[7400:7420]","8e5bc255":"NUM_TOPICS = 7 \nLDA_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=30, random_state=42)","1c5c4c6d":"LDA_model.fit(dtm)","32f7abd6":"import random \nfor index in range(15):\n    random_word_ID = random.randint(0, 6506)\n    print(cv.get_feature_names()[random_word_ID])","df4abb40":"# Pick a single topic \na_topic = LDA_model.components_[0]\n\n# Get the indices that would sort this array\na_topic.argsort()","ca13d767":"# The word least representative of this topic\na_topic[597]","12fae97d":"# The word most representative of this topic\na_topic[3598]","193da727":"top_10_words_indices = a_topic.argsort()[-10:]\n\nfor i in top_10_words_indices:\n    print(cv.get_feature_names()[i])","2530e1dd":"for i, topic in enumerate(LDA_model.components_):\n    print(\"THE TOP {} WORDS FOR TOPIC #{}\".format(10, i))\n    print([cv.get_feature_names()[index] for index in topic.argsort()[-10:]])\n    print(\"\\n\")","28ce6a48":"final_topics = LDA_model.transform(dtm)\nfinal_topics.shape","d8247d5b":"final_topics[0]","bd5ebebc":"final_topics[0].argmax()","8ec1307c":"txt_df[\"Topic N\u00b0\"] = final_topics.argmax(axis=1)","3013752d":"txt_df.head()","0ebe8687":"pyLDAvis.enable_notebook() # To enable the visualization on the notebook","f3b43836":"panel = pyLDAvis.sklearn.prepare(LDA_model, dtm, cv, mds='tsne') # Create the panel for the visualization\npanel","0983bc6f":"### <span style=\"font-size:larger;\">4. Vectorizer<\/span>","f3889e7e":"### <span style=\"font-size:larger;\">1. Collector<\/span>","6b3581c6":"This looks like scientific Article. Let's have a look at all the 7 topics found.\n","83e60d98":"#### END:  If you liked it, give it an upvote\ud83d\udc4d \ud83d\ude01\nMade with \u2764\ufe0f\u2764\ufe0f\u2764\ufe0f   \n##### Of course, I am open to any suggestion ","6f04d513":"By looking at the previous text data, we can see that a lot of cleaning tasks need to be performed such as: \n* 1. removing url links\n* 2. removing non ascii characters \n* 3. removing some specific characters\n* 4. removing words with less than 3 characters\n* 5. lemmatization\n* etc...     \n\n### Focus on the lemmatization   \nThis is a process of grouping together the different inflected forms of a word so they can be analyzed as a single item. It helps in reducing the dimensionality of the document. An example can be seen below. change (in green) is the lemma of the words in the left side. \n![image.png](attachment:image.png)\n\n\nWe will be able to perform those tasks with the following helper functions","bc532067":"### Attach Discovered Topic Labels to Original News","89689be5":"\n\nAccording to our LDA model:\n\n* the first document belongs to 1st topic.   \n* the second document belongs to 5th topic.    \n* the third document belongs to 3rd topic.   \n* etc.    \n \n","4abd3382":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#008000;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">This value (1) means that our LDA model thinks that the 3rd document belongs to the 1st topic.\n<\/p>\n<\/div>\n\n","9d52eca6":"**Note**:   \n* The corresponding observation of each experiment is given in the green rectangles.    \n* The elements in the orange box highlight the main ","06ce1ba8":"Let's have a look at some of the features that have been extracted from the documents. Here we print the words from the 7400th to the 7400th position\n","c75dac5d":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#008000;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\nWe have a total of 189 files and 2 columns \n\n<\/p>\n<\/div>\n","ac157cc5":"### <span style=\"font-size:larger;\">0. Libraries<\/span>","4f69b0ba":"## B- Possible solution   \n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#e6aa20;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">You can use a tool that helps you deal with the problem, so that you can easily identify the underlying domain of interest of each File you will have to process. In our case we will build a LDA tool that will determine the underlying topic (domain of expertise) for a given File from internet.  \n<\/p>\n<\/div>","4b1829a2":"### Combination with the original data \nLet's create a new column called **Topic N\u00b0** that will correspond to the topic value to which each document belongs to.","89311fc5":"With all these information, **we are now ready to implement our tool**! \ud83d\udcaa\ud83c\udffc","11f367b0":"#### Some Comments On The Graphic   \n* By selecting a particular term on the right, we can see which topic(s) it belongs.\n* Vice-versa, by choosing a topic on the left, we can see all the terms, from most to least relevant term.\n","f8c9f7a3":"Here, we are not interested in the **file_name** column, since we will only be using **file_content** data.\n\n*max_df*: float in range [0.0, 1.0] or int, default=1.0\nWhen building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\n*min_df*: float in range [0.0, 1.0] or int, default=1\nWhen building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n\nBe defining the **CountVectorizer** object as below, we ignore:\n\n* all terms that occur over 95% times in our document corpus. We say in this case that the terms occuring more than this threshold are not significant, most of them are stopwords.\n\n* all the terms that occur fewer than twice in the entire corpus.\n","b5840824":"### Show Stored Words.\nLet's randomnly have a look at some words of that have been stored.","9d220fc1":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#008000;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">We can observe that our Document X Term Matrix (dtm) has: 189 documents, and 7480 distinct words <br\/>\n                We can also get all those words using the get_feature_names() function\n<\/p>\n<\/div>","39d68a4d":"### Top Words Per Topic","e012458e":"### <span style=\"font-size:larger;\">3. Processor<\/span>","7b10270d":"Let have a look at the top 10 words for the topic we previously took","1ae818f6":"### LDA\nThere are different technics of topic modeling, but in our case we will be specifically be using **LDA**\n\nFrom our DTM matrix, we can now build our LDA to extract topics from the underlined texts.    \nThe number of topic to be extracted is a hyperparameter, so we do not know it a a glance. In our case, we will be using 7 topics.\nLDA is an iterative algorithm, we will have 30 iterations in our case, but the default value is 10.\n","4ab4b1a8":"# END TO END LDA SYSTEM FROM PROBLEM STATEMENT TO SOLUTION DEVELOPMENT   \n<p style=\"text-align:center;\"> Zoumana KEITA, Data Scientist <\/p>","b9a1cdc9":"### Some Visualization     \nWe will be using the **pyldavis** module to visualize the topics associated to our documents.","302b91a1":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#008000;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">For the 3rd function (remove_specifis_characters), we can reduce the number of line by     \n    using regular expressions like in the 1st function (remove_url_links).    \n    But I just want to use it the way it is in order show a different way of performing special characters.    \n    That might help most people understand the idea behind it \ud83d\ude09       \n\n<\/p>\n<\/div>","f2d27a6e":"## C- Solution Architecture   \nHere is the global architecture for the problem. This architecture is contains main components:    \n\nCapture d\u2019e\u0301cran 2020-12-18 a\u0300 23.41.37![image.png](attachment:image.png)","2c310f08":"### <span style=\"font-size:larger;\">2. Extractor<\/span>","c979a178":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#008000;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">final_topics contains, for each of our 189 documents, the probability score of how likely a document belongs to each of the 7 topics. This is a Document X Topics matrix. For example, below is the probability values for the first document.\n\n<\/p>\n<\/div>","88e451cb":"### <span style=\"font-size:larger;\">5. Topic Building<\/span>","bbf913eb":"## A- Problem Statement   \n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#e6aa20;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">Imagine your company receives text files related to some domains of expertise such sport, nutrition, science, etc. In the beginning you were able to manually manage the task, but your business kept growing which brings you much more client, which means much more files analyze. You can to keep being accurate in your analysis, without hiring a lot of additional staff. In the other hand, you've a good new: your domain of expertise did not change.  \n<\/p>\n<\/div>\n  ","19422920":"1. **Collector**: collect all the .txt Files from the corporate website. The corporte website can be an internal plateforme used to collect the files    \n2. **Extractor**: extract the .txt Files content in order to be used for further analysis by creating a pandas dataframe     \n2. **Processor**: perform the preprocessing task in order to clean the data by using the tasks such as stopword removal. \n    We might need specific analysis depending on the observation made on the data        \n3. **Vectorizer**: vertorize the documents so that the LDA model can perform the modeling tasks     \n4. **Topic Builder**: build the LDA model by associating to each document, the corresponding topic and provide a tool for dynamic visualization     "}}