{"cell_type":{"d6623d22":"code","6e30d9bc":"code","ad7b27ea":"code","97076734":"code","279a6906":"code","ffebe143":"code","3b8c2981":"code","9444eb61":"code","bd1e78f9":"code","2f107b2a":"code","5b81bd9b":"code","c86f5a0f":"code","dced4d98":"code","01e62d2c":"code","1663bc3c":"code","75c68eec":"code","e8a7091e":"code","d6c0c332":"code","b691f955":"code","e25f520b":"code","854c38cb":"code","f5ffc370":"code","acb398bb":"code","329c6bb2":"code","5a863d58":"code","5ce84949":"code","63561871":"code","33f918c6":"code","18aa7ba0":"code","45225c94":"code","55be9a4a":"code","d57d3046":"code","674d4e8d":"code","1b45e1f8":"code","3123e147":"markdown","62c457a7":"markdown","d18e069d":"markdown"},"source":{"d6623d22":"import numpy as np\nimport pandas as pd\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import text\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport pickle\nimport gc\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split, KFold\nimport torch.nn.functional as F\nimport os\nimport random\nimport time\nimport pickle\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler\nimport operator\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nimport spacy\nfrom spacy.lang.en import English\nfrom scipy.stats import spearmanr\nimport re\ntqdm.pandas()","6e30d9bc":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","ad7b27ea":"CRAWL_EMBEDDING_PATH = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nGLOVE_EMBEDDING_PATH = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\ntrain_csv_path       = '..\/input\/google-quest-challenge\/train.csv'\ntest_csv_path        = '..\/input\/google-quest-challenge\/test.csv'\nseed                 = 0\nepochs               = 50\nmax_features         = 100000\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nseed_everything(seed)\nnlp = English()  # just the language with no model\nsentencizer = nlp.create_pipe(\"sentencizer\")\nnlp.add_pipe(sentencizer)","97076734":"train = pd.read_csv(train_csv_path)\ntest  = pd.read_csv(test_csv_path)","279a6906":"def get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\ndef load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr","ffebe143":"tree_tokenizer = TreebankWordTokenizer()\ndef handle_contractions(x):\n    x = tree_tokenizer.tokenize(x)\n    x = ' '.join(x)\n    return x","3b8c2981":"for col in ['question_body', 'question_title', 'answer']:\n    train[col] = train[col].apply(lambda x: handle_contractions(x))\n    test[col] = test[col].apply(lambda x: handle_contractions(x))","9444eb61":"tokenizer = text.Tokenizer(lower=False)","bd1e78f9":"X_train_question = train['question_body']\nX_train_title    = train['question_title']\nX_train_answer   = train['answer']\n\nX_test_question  = test['question_body']\nX_test_title     = test['question_title']\nX_test_answer    = test['answer']","2f107b2a":"tokenizer.fit_on_texts(list(X_train_question) + \\\n                       list(X_train_answer) + \\\n                       list(X_train_title) + \\\n                       list(X_test_question) + \\\n                       list(X_test_answer) + \\\n                       list(X_test_title))","5b81bd9b":"def split_document(texts):\n    \n    all_sents = []\n    max_number_sentense = 0.0\n    for text in tqdm(texts):\n        doc  = nlp(text)\n        sents = []\n        for idx, sent in enumerate(doc.sents):\n            sents.append(sent.text)\n        all_sents.append(sents)\n    \n    return all_sents\n\nX_train_question = split_document(X_train_question)\nX_train_answer   = split_document(X_train_answer)\n\nX_test_question  = split_document(X_test_question)\nX_test_answer    = split_document(X_test_answer)","c86f5a0f":"def build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n                        \n    return embedding_matrix, unknown_words","dced4d98":"crawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, GLOVE_EMBEDDING_PATH)\nprint('n unknown words (glove): ', len(unknown_words_glove))","01e62d2c":"embedding_matrix = np.concatenate([crawl_matrix, glove_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","1663bc3c":"def add_question_metadata_features(text):\n    doc = nlp(text)\n    indirect = 0\n    question_count = 0\n    reason_explanation_words = 0\n    choice_words = 0\n\n    for sent in doc.sents:\n        if '?' in sent.text and '?' == sent.text[-1]:\n            question_count += 1                  # -> question_multi_intent\n            for token in sent:\n                if token.text.lower() == 'why':  # question_type_reason_explanation e.g index->102\n                    reason_explanation_words += 1\n                elif token.text.lower() == 'or':\n                    choice_words += 1            # question_type_choice\n    if question_count == 0:\n        indirect = 1\n\n    return [indirect, question_count, reason_explanation_words, choice_words]","75c68eec":"ans_user_category = train[train[['answer_user_name', 'category']].duplicated()][['answer_user_name', 'category']].values.tolist()\nprint(len(ans_user_category))","e8a7091e":"def question_answer_author_same(df):\n    \n    q_username = df['question_user_name']\n    a_username = df['answer_user_name'] \n    \n    author_same = []\n    for i in range(len(df)):\n        if q_username[i] == a_username[i]:\n            author_same.append(int(1))\n        else:\n            author_same.append(int(0))\n            \n    return author_same\n\ndef add_external_features(df):\n    \n    #If the question is longer, it may be more clear, which may help users give a more \n    df['question_body']      = df['question_body'].progress_apply(lambda x:str(x))\n    df['question_num_words'] = df.question_body.str.count('\\S+')\n    \n    #The assumption here is that longer answer could bring more useful detail\n    df['answer']            = df['answer'].progress_apply(lambda x:str(x))\n    df['answer_num_words']  = df.answer.str.count('\\S+')\n    \n    #if the question is long and the answer is short, it may be less relevant\n    df[\"question_vs_answer_length\"] = df['question_num_words'] \/  df['answer_num_words']\n    \n    #if answer's author is the same as the corresponding question's author,\n    #Why he\/she asked question.. :)\n    df[\"q_a_author_same\"] = question_answer_author_same(df)\n    \n    #answers which was posted by users who answer one category more than one times, they may have read more similar questions.\n    #thus, the answers by this type of user will more relevent to question.\n    ans_user_cat = []\n    for x in tqdm(df[['answer_user_name', 'category']].values.tolist()):\n        if x in ans_user_category:\n            ans_user_cat.append(int(1))\n        else:\n            ans_user_cat.append(int(0))\n    df['ans_user_with_cat'] = ans_user_cat\n    \n    handmade_features = []\n\n    for idx, text in enumerate(df['question_body'].values):\n        handmade_features.append(add_question_metadata_features(text))\n        \n\n    return df, np.array(handmade_features)","d6c0c332":"train, train_handmade_features = add_external_features(train)\ntest, test_handmade_features   = add_external_features(test)","b691f955":"num_words_scaler = MinMaxScaler()\nnum_words_scaler.fit(train[['question_num_words', 'answer_num_words']].values)\ntrain[['question_num_words', 'answer_num_words']]= num_words_scaler.transform(train[['question_num_words', 'answer_num_words']].values)\ntest[['question_num_words', 'answer_num_words']] = num_words_scaler.transform(test[['question_num_words', 'answer_num_words']].values)","e25f520b":"train_external_features = train[['question_num_words', 'answer_num_words',\n                                 \"question_vs_answer_length\", \"q_a_author_same\",\n                                 \"ans_user_with_cat\"]].values\ntest_external_features  = test[['question_num_words', 'answer_num_words',\n                                \"question_vs_answer_length\", \"q_a_author_same\", \n                                \"ans_user_with_cat\"]].values","854c38cb":"train_external_features = np.hstack((train_external_features, train_handmade_features))\ntest_external_features = np.hstack((test_external_features, test_handmade_features))","f5ffc370":"def tokenizer_to_index(texts, max_number_sentence, maxlen):\n    \n    all_seqs = []\n    \n    for text in tqdm(texts):\n        seqs = []\n        for sent in text:\n            sent = tokenizer.texts_to_sequences(pd.Series(sent))\n            sent = pad_sequences(sent, maxlen=maxlen)\n            if len(sent) == 0:\n                seqs.append([0]*maxlen)\n            else:\n                seqs.append(sent[0])\n        if len(seqs) < max_number_sentence:\n            gap = max_number_sentence - len(seqs)\n            pad_zeros = [[0]*maxlen for g in range(gap)]\n            seqs = pad_zeros + seqs # pad -> pre\n        elif len(seqs) > max_number_sentence:\n            seqs = seqs[:max_number_sentence]\n            \n        all_seqs.append(np.array(seqs))\n    return np.stack(all_seqs, 0)\n    ","acb398bb":"X_train_question = tokenizer_to_index(X_train_question, max_number_sentence=20, maxlen=50)\nX_train_answer   = tokenizer_to_index(X_train_answer, max_number_sentence=20, maxlen=50)\n\n\nX_test_question = tokenizer_to_index(X_test_question, max_number_sentence=20, maxlen=50)\nX_test_answer   = tokenizer_to_index(X_test_answer, max_number_sentence=20, maxlen=50)","329c6bb2":"X_train_title    = tokenizer.texts_to_sequences(X_train_title)\nX_train_title    = pad_sequences(X_train_title, maxlen=30)","5a863d58":"X_test_title    = tokenizer.texts_to_sequences(X_test_title)\nX_test_title    = pad_sequences(X_test_title, maxlen=30)","5ce84949":"# the assumption here is that the question comment relevance might depend on the category of the question\n\nunique_categories = list(set(train['category'].unique().tolist() + test['category'].unique().tolist()))\ncategory_dict = {i + 1: e for i, e in enumerate(unique_categories)}\ncategory_dict_reverse = {v: k for k, v in category_dict.items()}\n\nunique_hosts = list(set(train['host'].unique().tolist() + test['host'].unique().tolist()))\nhost_dict = {i + 1: e for i, e in enumerate(unique_hosts)}\nhost_dict_reverse = {v: k for k, v in host_dict.items()}\n\ntrain_host = train['host'].apply(lambda x: host_dict_reverse[x]).values\ntrain_category = train['category'].apply(lambda x: category_dict_reverse[x]).values\n\ntest_host = test['host'].apply(lambda x: host_dict_reverse[x]).values\ntest_category = test['category'].apply(lambda x: category_dict_reverse[x]).values\n\nn_cat = len(category_dict) + 1\ncat_emb = min(np.ceil((len(category_dict)) \/ 2), 50)\nn_host = len(host_dict) + 1\nhost_emb = min(np.ceil((len(host_dict)) \/ 2), 50)","63561871":"class QuestDataset(Dataset):\n\n    def __init__(self, df, questions, answers, titles, hosts, categories, external_features):\n        self.df = df\n        self.questions         = questions\n        self.answers           = answers\n        self.titles            = titles\n        self.hosts             = hosts\n        self.categories        = categories\n        self.external_features = external_features\n\n        self.question_cols = ['question_asker_intent_understanding',\n                              'question_body_critical', 'question_conversational',\n                              'question_expect_short_answer', 'question_fact_seeking',\n                              'question_has_commonly_accepted_answer',\n                              'question_interestingness_others', 'question_interestingness_self',\n                              'question_multi_intent', 'question_not_really_a_question',\n                              'question_opinion_seeking', 'question_type_choice',\n                              'question_type_compare', 'question_type_consequence',\n                              'question_type_definition', 'question_type_entity',\n                              'question_type_instructions', 'question_type_procedure',\n                              'question_type_reason_explanation', 'question_type_spelling',\n                              'question_well_written']\n        self.answer_cols = ['answer_helpful', 'answer_level_of_information',\n                            'answer_plausible', 'answer_relevance',\n                            'answer_satisfaction', 'answer_type_instructions',\n                            'answer_type_procedure', 'answer_type_reason_explanation',\n                            'answer_well_written']\n\n        self.label = self.df[self.question_cols + self.answer_cols].values\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        answer = self.answers[idx]\n        title = self.titles[idx]\n        host = self.hosts[idx]\n        category = self.categories[idx]\n        external_features = self.external_features[idx]\n\n        labels = self.label[idx]\n\n        return [question, answer, title, host, category, external_features], labels","33f918c6":"class QuestDataset_test(Dataset):\n    \n    def __init__(self, questions, answers, titles, hosts, categories, external_features):\n        \n        self.questions         = questions\n        self.answers           = answers\n        self.titles            = titles\n        self.hosts             = hosts\n        self.categories        = categories\n        self.external_features = external_features\n\n    def __len__(self):\n        return self.questions.shape[0]\n\n    def __getitem__(self, idx):\n        \n        question = self.questions[idx]\n        answer   = self.answers[idx]\n        title    = self.titles[idx]\n        host = self.hosts[idx]\n        category = self.categories[idx]\n        external_features = self.external_features[idx]\n        \n        return [question, answer, title, host, category, external_features]","18aa7ba0":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\nclass Attention(nn.Module):\n    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n\n        self.supports_masking = True\n\n        self.bias = bias\n        self.feature_dim = feature_dim\n        self.step_dim = step_dim\n        self.features_dim = 0\n\n        weight = torch.zeros(feature_dim, 1)\n        nn.init.xavier_uniform_(weight)\n        self.weight = nn.Parameter(weight)\n\n        if bias:\n            self.b = nn.Parameter(torch.zeros(step_dim))\n\n    def forward(self, x, mask=None):\n        feature_dim = self.feature_dim\n        step_dim = self.step_dim\n\n        eij = torch.mm(\n            x.contiguous().view(-1, feature_dim),\n            self.weight\n        ).view(-1, step_dim)\n\n        if self.bias:\n            eij = eij + self.b\n\n        eij = torch.tanh(eij)\n        a = torch.exp(eij)\n\n        if mask is not None:\n            a = a * mask\n\n        a = a \/ torch.sum(a, 1, keepdim=True) + 1e-10\n        weighted_input = x * torch.unsqueeze(a, -1)\n        \n        return torch.sum(weighted_input, 1)\n\nclass rnn_Layer(nn.Module):\n    \n    def __init__(self, input_dim, output_dim, max_len):\n        super().__init__()\n        self.lstm_1 = nn.LSTM(input_dim, output_dim, bidirectional=True, batch_first=True)\n        self.atten  = Attention(output_dim * 2, max_len)\n        \n    def forward(self, x):\n        \n        lstm_output, _ = self.lstm_1(x)\n        \n        return self.atten(lstm_output)","45225c94":"class QuestModel(nn.Module):\n\n    def __init__(self, embedding_matrix, n_cat, cat_emb, n_host, host_emb):\n        super().__init__()\n\n        LSTM_UNITS = 128\n        embed_size = embedding_matrix.shape[1]\n        DENSE_HIDDEN_UNITS = LSTM_UNITS * 4\n        #max_features = config.MAX_FEATURES\n\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n\n        self.category_embedding = nn.Embedding(n_cat, int(cat_emb))\n        self.host_embedding = nn.Embedding(n_host, int(host_emb))\n\n        ##########################################################\n        # LSTM\n        ##########################################################\n        self.lstm_q_1 = rnn_Layer(embed_size, LSTM_UNITS, max_len=50)\n        self.lstm_q_2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n\n        self.lstm_a_1 = rnn_Layer(embed_size, LSTM_UNITS, max_len=50)\n        self.lstm_a_2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n\n        self.lstm_t_1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n\n        self.p_fc1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS),\n                                   nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                   nn.ReLU(inplace=True),\n                                   nn.Dropout(0.5))\n        self.a_fc1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS),\n                                   nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                   nn.ReLU(inplace=True),\n                                   nn.Dropout(0.5))\n        self.t_fc1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS),\n                                   nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                   nn.ReLU(inplace=True),\n                                   nn.Dropout(0.5))\n\n        ######################################\n        # Q-branch\n        ######################################\n        self.q_t_consine = nn.CosineSimilarity(dim=1)\n        self.q_fc1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS*2 + int(cat_emb) + int(host_emb) + 6, DENSE_HIDDEN_UNITS),\n                                   nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                   nn.ReLU(inplace=True),\n                                   nn.Dropout(0.5))\n        self.q_fc2 = nn.Linear(DENSE_HIDDEN_UNITS, 21)\n\n        ######################################\n        # QA-branch\n        ######################################\n\n        self.aq_bil = nn.Bilinear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        self.aq_fc1 = nn.Sequential(nn.Linear(DENSE_HIDDEN_UNITS * 4 + 4, DENSE_HIDDEN_UNITS),\n                                    nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n                                    nn.ReLU(inplace=True),\n                                    nn.Dropout(0.5))\n\n        self.aq_fc2 = nn.Linear(DENSE_HIDDEN_UNITS, 9)\n\n    def forward(self, question, answer, title, host, category, external_features):\n\n        _, q_sentence_num, q_max_len = question.size()\n        _, a_sentence_num, a_max_len = answer.size()\n        category_embed  = self.category_embedding(category)\n        host_embed      = self.host_embedding(host)\n        question_length  = external_features[:, 0].unsqueeze(-1)\n        answer_length    = external_features[:, 1].unsqueeze(-1)\n        q_vs_a           = external_features[:, 2].unsqueeze(-1)\n        qa_same_author   = external_features[:, 3].unsqueeze(-1)\n        a_with_cat       = external_features[:, 4].unsqueeze(-1)\n        \n        indirect         = external_features[:, 5].unsqueeze(-1)\n        num_question     = external_features[:, 6].unsqueeze(-1)\n        reasonal_explain = external_features[:, 7].unsqueeze(-1)\n        choice           = external_features[:, 8].unsqueeze(-1)\n\n        #######################################\n        # Question\n        #######################################\n        q_reps = []\n        for i in range(q_sentence_num):\n            question_sentence = question[:, i, :].long()  # (batch_size, max_len)\n            question_embedding = self.embedding(question_sentence)\n            question_embedding = self.embedding_dropout(question_embedding)  # (batch_size, max_len, embed_size)\n            q_sentence_reps = self.lstm_q_1(question_embedding)  # (batch_size, output_dim*2) #Word-level-attention\n            q_sentence_reps = torch.unsqueeze(q_sentence_reps, dim=1)  # (batch_size, 1, LSTM_UNITS*2)\n            q_reps.append(q_sentence_reps)\n\n        q_reps     = torch.cat(q_reps, dim=1)  #(batch_size, sentence_num, LSTM_UNITS*2)\n        q_lstm2, _ = self.lstm_q_2(q_reps)\n\n        q_avg_pool = torch.mean(q_lstm2, 1)\n        q_max_pool, _ = torch.max(q_lstm2, 1)\n\n        #######################################\n        # answer\n        #######################################\n        a_reps = []\n        for j in range(a_sentence_num):\n            answer_sentence = answer[:, j, :].long()  # (batch_size, max_len)\n            answer_embedding = self.embedding(answer_sentence)\n            answer_embedding = self.embedding_dropout(answer_embedding)  # (batch_size, max_len, embed_size)\n            a_sentence_reps = self.lstm_a_1(answer_embedding)  # (batch_size, LSTM_UNITS*2)\n            a_sentence_reps = torch.unsqueeze(a_sentence_reps, dim=1)  # (batch_size, 1, DENSE_HIDDEN_UNITS)\n            a_reps.append(a_sentence_reps)\n\n        a_reps = torch.cat(a_reps, dim=1)  # (batch_size, sentence_num, DENSE_HIDDEN_UNITS)\n        a_lstm2, _ = self.lstm_a_2(a_reps)\n\n        a_avg_pool = torch.mean(a_lstm2, 1)\n        a_max_pool, _ = torch.max(a_lstm2, 1)\n\n        #######################################\n        # title\n        #######################################\n\n        title_embedding = self.embedding(title.long())\n        title_embedding = self.embedding_dropout(title_embedding)\n\n        t_lstm1, _ = self.lstm_t_1(title_embedding)\n\n        t_avg_pool = torch.mean(t_lstm1, 1)\n        t_max_pool, _ = torch.max(t_lstm1, 1)\n\n        q_features = self.p_fc1(\n            torch.cat((q_max_pool, q_avg_pool), 1))  # (batch_size, LSTM_UNITS*4) -> (batch_size, LSTM_UNITS)\n        a_features = self.a_fc1(\n            torch.cat((a_max_pool, a_avg_pool), 1))  # (batch_size, LSTM_UNITS*4) -> (batch_size, LSTM_UNITS)\n        t_features = self.t_fc1(\n            torch.cat((t_max_pool, t_avg_pool), 1))  # (batch_size, LSTM_UNITS*4) -> (batch_size, LSTM_UNITS)\n        ######################################\n        # Q-branch\n        ######################################\n        cosine_q_t = self.q_t_consine(q_features, t_features).unsqueeze(-1)\n        hidden_q   = self.q_fc1(torch.cat((q_features, t_features, category_embed, host_embed,\n                                           cosine_q_t, question_length, indirect, num_question,\n                                           reasonal_explain, choice), 1))\n        q_result   = self.q_fc2(hidden_q)\n        ######################################\n        # QA-branch\n        ######################################\n        bil_sim   = self.aq_bil(q_features, a_features)\n        hidden_aq = self.aq_fc1(torch.cat((q_features, t_features, a_features, \n                                           bil_sim, answer_length, q_vs_a, \n                                           qa_same_author, a_with_cat), 1))\n        aq_result = self.aq_fc2(hidden_aq)\n\n        return torch.cat((q_result, aq_result), 1)\n\n","55be9a4a":"def train_model(train_loader, optimizer, criterion):\n    \n    model.train()\n    avg_loss = 0.\n    \n    for idx, (inputs, labels) in enumerate(train_loader):\n        questions, answers, titles, hosts, categories, external_features = inputs \n        questions, answers, titles, hosts, categories = questions.cuda(), answers.cuda(), titles.cuda(), hosts.long().cuda(), categories.long().cuda()\n        external_features = external_features.float().cuda()\n        labels = labels.float().cuda()\n        \n        optimizer.zero_grad()\n        output_train = model(questions, answers, titles, hosts, categories, external_features)\n        loss = criterion(output_train,labels)\n        loss.backward() \n        optimizer.step()\n        avg_loss += loss.item() \/ len(train_loader)\n        \n    return avg_loss\n\ndef val_model(val_loader):\n    avg_val_loss = 0.\n    model.eval() #\u5b9f\u884c\u30e2\u30fc\u30c9\n    preds = []\n    original = []\n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(val_loader):\n            questions, answers, titles, hosts, categories, external_features = inputs \n            questions, answers, titles, hosts, categories = questions.cuda(), answers.cuda(), titles.cuda(), hosts.long().cuda(), categories.long().cuda()\n            external_features = external_features.float().cuda()\n            labels = labels.float().cuda()\n            \n            \n            output_val = model(questions, answers, titles, hosts, categories, external_features)\n            avg_val_loss += criterion(output_val, labels).item() \/ len(val_loader)\n            preds.append(output_val.cpu().numpy())\n            original.append(labels.cpu().numpy())\n        \n        score = 0\n        for i in range(30):\n            score += np.nan_to_num(\n                spearmanr(np.concatenate(original)[:, i], np.concatenate(preds)[:, i]).correlation \/ 30)\n        \n    return avg_val_loss, score\n\n\ndef predict_result(model, test_loader, batch_size=64):\n    \n    output = np.zeros((len(test_set), 30))\n    model.eval()\n    with torch.no_grad():\n        for idx, inputs in enumerate(test_loader):\n            start_index = idx * batch_size\n            end_index   = min(start_index + batch_size, len(test_set))\n            questions, answers, titles, hosts, categories, external_features = inputs \n            questions, answers, titles, hosts, categories = questions.cuda(), answers.cuda(), titles.cuda(), hosts.long().cuda(), categories.long().cuda()\n            external_features = external_features.float().cuda()\n            predictions = model(questions, answers, titles, hosts, categories, external_features)\n            predictions = torch.sigmoid(predictions)\n            output[start_index:end_index, :] = predictions.detach().cpu().numpy()\n            \n    return output","d57d3046":"kf = KFold(n_splits=5, shuffle=True, random_state=seed)\ntest_set     = QuestDataset_test(X_test_question,  \n                                 X_test_answer, \n                                 X_test_title,\n                                 test_host,\n                                 test_category,\n                                 test_external_features,\n                                )\ntest_loader  = DataLoader(test_set, batch_size=64, shuffle=False)\nresult = np.zeros((len(test), 30))\n\nfor fold, (train_index, val_index) in enumerate(kf.split(range(len(train)))):\n    print(\"fold:\", fold)\n    train_df = train.iloc[train_index]\n    val_df   = train.iloc[val_index]\n    \n    train_set    = QuestDataset(train_df,\n                                X_train_question[train_index],\n                                X_train_answer[train_index],\n                                X_train_title[train_index],\n                                train_host[train_index],\n                                train_category[train_index],\n                                train_external_features[train_index])\n    train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n    \n    val_set      = QuestDataset(val_df,\n                                X_train_question[val_index],\n                                X_train_answer[val_index],\n                                X_train_title[val_index],\n                                train_host[val_index],\n                                train_category[val_index],\n                                train_external_features[val_index])\n    val_loader   = DataLoader(val_set, batch_size=64, shuffle=False)\n    \n\n    model = QuestModel(embedding_matrix, n_cat, cat_emb, n_host, host_emb)\n    model.to(device)\n    \n    best_avg_loss   = 100.0\n    best_score      = 0.0\n    best_param_loss = None\n    best_param_score = None \n    i = 0\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = nn.BCEWithLogitsLoss()\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n\n    \n    for epoch in range(epochs):\n        \n        if i == 5: break\n        start_time   = time.time()\n        avg_loss     = train_model(train_loader, optimizer, criterion)\n        avg_val_loss, score = val_model(val_loader)\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t score={:.6f} \\t time={:.2f}s'.format(epoch + 1, epochs, avg_loss, avg_val_loss, score, elapsed_time))\n    \n        if best_avg_loss > avg_val_loss:\n            i = 0\n            best_avg_loss = avg_val_loss \n            best_param_loss = model.state_dict()\n        if best_score < score:\n            best_score = score\n            best_param_score = model.state_dict()\n        else:\n            i += 1\n        scheduler.step(avg_val_loss)\n        \n\n    model.load_state_dict(best_param_score)\n    result += predict_result(model, test_loader)\n    \n    torch.cuda.empty_cache()\n    del train_df\n    del val_df\n    del model\n    gc.collect()","674d4e8d":"result \/= 5","1b45e1f8":"submission = pd.read_csv(\"\/kaggle\/input\/google-quest-challenge\/sample_submission.csv\")\nsubmission.loc[:, 'question_asker_intent_understanding':] = result\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","3123e147":"## Preparing Train","62c457a7":"## Useful Function","d18e069d":"## Model"}}