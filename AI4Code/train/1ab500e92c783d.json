{"cell_type":{"1cc6b167":"code","3ebd7954":"code","f80f1309":"code","2e594f64":"code","58182298":"code","77861cc4":"code","fd767b39":"code","23f36d3f":"code","1964dbf6":"code","5b76bc24":"code","150a7f49":"code","fc289727":"code","955dd7e2":"code","7ab29170":"code","e556cce4":"code","b45e52a9":"code","84c7418c":"code","c780bf43":"code","1933f06b":"code","332ac4a7":"code","f90e8050":"code","1ba289a6":"code","26987b11":"code","17a5aa99":"code","5f2f4071":"code","a2f42098":"code","a58265c0":"code","4c2084bb":"code","0dbd31bc":"code","9ae92f13":"code","cb0fec31":"code","56419644":"code","4530c0e6":"code","eecc6ee7":"code","d0b78af6":"code","d81be157":"code","ad61a6b7":"code","b6f6b4f1":"code","f0fba781":"markdown","1ef33ba4":"markdown","00ff75f1":"markdown","90f09e20":"markdown","5974dfeb":"markdown","60e8ed82":"markdown"},"source":{"1cc6b167":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory","3ebd7954":"data_dir = '..\/input\/doom-crossing\/'","f80f1309":"batch_size = 16\nimg_height = 224\nimg_width = 224","2e594f64":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.3,\n  subset=\"training\",\n  seed=100,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","58182298":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=100,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","77861cc4":"class_names = train_ds.class_names\nprint(class_names)","fd767b39":"plt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","23f36d3f":"val_batches = tf.data.experimental.cardinality(val_ds)\ntest_ds = val_ds.take(val_batches \/\/ 5)\nval_ds = val_ds.skip(val_batches \/\/ 5)","1964dbf6":"print('Number of training batches: %d' % tf.data.experimental.cardinality(train_ds))\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(val_ds))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_ds))","5b76bc24":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = train_ds.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = val_ds.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_ds.prefetch(buffer_size=AUTOTUNE)","150a7f49":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","fc289727":"for image, _ in train_dataset.take(1):\n  plt.figure(figsize=(10, 10))\n  first_image = image[0]\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n    plt.imshow(augmented_image[0] \/ 255)\n    plt.axis('off')","955dd7e2":"#Rescaling\npreprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\nrescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5, offset= -1)","7ab29170":"weights = '..\/input\/keras-pretrain-model-weights\/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5'\nIMG_SHAPE = (img_height, img_width) + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights=weights)","e556cce4":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","b45e52a9":"base_model.trainable = False","84c7418c":"base_model.summary()","c780bf43":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","1933f06b":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","332ac4a7":"inputs = tf.keras.Input(shape=(224, 224, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.3)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","f90e8050":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","1ba289a6":"model.summary()","26987b11":"len(model.trainable_variables)","17a5aa99":"initial_epochs = 20\n\nloss0, accuracy0 = model.evaluate(validation_dataset)","5f2f4071":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","a2f42098":"history = model.fit(train_dataset,\n                    epochs=initial_epochs,\n                    validation_data=validation_dataset)","a58265c0":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.ylabel('Accuracy')\nplt.ylim([min(plt.ylim()),1])\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.ylabel('Cross Entropy')\nplt.ylim([0,1.0])\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","4c2084bb":"base_model.trainable = True","0dbd31bc":"# Let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the base model: \", len(base_model.layers))\n\n# Fine-tune from this layer onwards\nfine_tune_at = 100\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False","9ae92f13":"#As you are training a much larger model and want to readapt the pretrained weights, \n#it is important to use a lower learning rate at this stage. Otherwise, your model could overfit very quickly.\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/20),\n              metrics=['accuracy'])","cb0fec31":"model.summary()","56419644":"len(model.trainable_variables)","4530c0e6":"fine_tune_epochs = 20\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_dataset,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1],\n                         validation_data=validation_dataset)","eecc6ee7":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']","d0b78af6":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.7, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","d81be157":"loss, accuracy = model.evaluate(test_dataset)\nprint('Test accuracy :', accuracy)","ad61a6b7":"#Retrieve a batch of images from the test set\nimage_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\n\n# Apply a sigmoid since our model returns logits\npredictions = tf.nn.sigmoid(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(10, 10))\nfor i in range(9):\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(image_batch[i].astype(\"uint8\"))\n  plt.title(class_names[predictions[i]])\n  plt.axis(\"off\")","b6f6b4f1":"# Get all predictions and labels\nfinal_images = []\nfinal_predictions = []\nfinal_labels = []\n\nfor x in test_dataset.as_numpy_iterator():\n    image_batch, label_batch = x\n    predictions = model.predict_on_batch(image_batch).flatten()\n\n    # Apply a sigmoid since our model returns logits\n    predictions = tf.nn.sigmoid(predictions)\n    predictions = tf.where(predictions < 0.5, 0, 1)\n    final_images.append(image_batch)\n    final_predictions = np.append(final_predictions,predictions.numpy())\n    final_labels = np.append(final_labels, label_batch)\n    \nfinal_images = np.reshape(final_images, (4*16,224,224,3))\n\n#Check which images were difficult for the model\nMislabeled = [x for x in range(len(final_predictions)) if final_predictions[x] != final_labels[x]]\nprint(Mislabeled)\nplt.figure(figsize=(10, 10))\nfor i in range(min(9,len(Mislabeled))):\n  ax = plt.subplot(3, 3, i + 1)\n  plt.imshow(final_images[Mislabeled[i]].astype(\"uint8\"))\n  plt.title(class_names[final_predictions[Mislabeled[i]].astype(\"uint8\")])\n  plt.axis(\"off\")","f0fba781":"## Future Works\nI think the model can be made slightly more accurate. The validation accuracy is not quite smooth from epoch to epoch so I think there could be some work done to regularize the model a little bit better. Overall I am quite happy, I believe there is a not so insignificant amount of memes that would be hard for even a human to distinguish whether it is a doom or animal crossing meme because there are a few memes that include both doom and animal crossing characters. A future improvement may be to include the caption as extra information, however I am not sure how much this will improve things. \n\nThere is also a fair amount of memes that are text heavy and may benefit from a different model other than a MobileNet CNN. \n","1ef33ba4":"## Fine Tuning\nThe weights of the pre-trained model were not changed at all, and so fine tuning these weights to our training set may have a [positive affect](https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning#fine_tuning) on the accuracy of the model.","00ff75f1":"## MobileNet_v2\nWe will be using the MobileNet_v2 infrastructure because it is a very powerful network pre-trained on a huge amount of data with 1000 different classes. We need to chooes which layer to use for feature extraction, and typicaly the top layer is not very good because it is too specific and not very useful. More info on this [here](https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning#create_the_base_model_from_the_pre-trained_convnets).","90f09e20":"## Animal Crossing\/ Doom Classifier\nThis classifier is a fun project I picked up to gain some practical ML experience. This project is based off of [tensorflow docs](https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning) aimed at transfer learning.  ","5974dfeb":"## Autotune Optimization\nAutotune optimization is done for non-blocking I\/O.  This allows training to occur while data is being loaded and vice-versa. More information can be read [here](https:\/\/www.tensorflow.org\/guide\/data_performance).","60e8ed82":"## Data Augmentation\nThis dataset is quite small, so data augmentation is very important for this model to improve."}}