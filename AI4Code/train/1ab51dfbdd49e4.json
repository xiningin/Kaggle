{"cell_type":{"f7256860":"code","a5029664":"code","119a4039":"code","89122c40":"code","4f07d183":"code","8bca9b9f":"code","d8abed63":"code","c270ea97":"code","ed0eb98f":"code","44052332":"code","721f5e4c":"code","61eaa81a":"code","ddaffefb":"code","eb3ed466":"code","de2c4ccc":"code","ce3bf58d":"markdown","95f8f04e":"markdown","b5795323":"markdown","4a05bf15":"markdown","2ec89a24":"markdown","22c3e285":"markdown","39bda453":"markdown","e756762b":"markdown","26159438":"markdown","9673e8d9":"markdown","7bbbf08b":"markdown","6f46da13":"markdown","b9809163":"markdown","e90aa465":"markdown"},"source":{"f7256860":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\nimport os\nprint(os.listdir(\"..\/input\"))","a5029664":"df_trn = pd.read_csv('..\/input\/train.csv')\ndf_tst = pd.read_csv('..\/input\/test.csv')","119a4039":"print('Train and test shapes are: {}, {}'.format(df_trn.shape, df_tst.shape))\nprint('Train and test memory footprint: {:.2f} MB, {:.2f} MB'\n      .format(df_trn.memory_usage(deep=True).sum()\/ 1024**2,\n              df_tst.memory_usage(deep=True).sum()\/ 1024**2)\n     )\nw_pos = df_trn['target'].sum()\/df_trn.shape[0]\nprint('Fraction of positive target (insencere) = {:.4f}'.format(w_pos))","89122c40":"df_trn.head()","4f07d183":"from nltk.corpus import stopwords\nstops = set(stopwords.words(\"english\"))","8bca9b9f":"import string\ndef remove_punctuation(s):\n    s = ''.join([i for i in s if i not in frozenset(string.punctuation)])\n    return s\n\nX_trn = (df_trn['question_text']\n         .apply(remove_punctuation)\n         .apply(lambda x: ' '.join([w.lower() for w in x.split(' ') if w.lower() not in stops]))\n        )\nX_trn2 = (df_trn['question_text']\n         .apply(remove_punctuation)\n         #.apply(lambda x: ' '.join([w.lower() for w in x.split(' ') if w.lower() not in stops]))\n        )\nX_tst = (df_tst['question_text']\n         .apply(remove_punctuation)\n         #.apply(lambda x: ' '.join([w.lower() for w in x.split(' ') if w.lower() not in stops]))\n        )\ny_trn = df_trn['target']\n\ndel df_trn, df_tst","d8abed63":"X_trn.head()","c270ea97":"import vowpalwabbit as vw\nfrom vowpalwabbit.sklearn_vw import VWClassifier\n\n# VW uses 1\/-1 target variables for classification instead of 1\/0, so we need to apply mapping\ndef convert_labels_sklearn_to_vw(y_sklearn):\n    return y_sklearn.map({1:1, 0:-1})\n\n# The function to create VW-compatible inputs from the text features and the target\ndef to_vw(X, y=None, namespace='Name', w=None):\n    labels = '1' if y is None else y.astype(str)\n    if w is not None:\n        labels = labels + ' ' + np.round(y.map({1: w, -1: 1}),5).astype(str)\n    prefix = labels + ' |' + namespace + ' '\n    if isinstance(X, pd.DataFrame):\n        return prefix + X.apply(lambda x: ' '.join(x), axis=1)\n    elif isinstance(X, pd.Series):\n        return prefix + X","ed0eb98f":"mdl_inputs = {\n# VW1x is analogous to the configuration from the kernel cited in the intro\n#                 'VW1x': [VWClassifier(quiet=False, convert_to_vw=False, \n#                                      passes=1, link='logistic',\n#                                      random_seed=314),\n#                              {'pos_threshold':0.5, \n#                               'b':29, 'ngram':2, 'skips': 1, \n#                               'l1':3.4742122764e-09, 'l2':1.24232077629e-11},\n#                              {},\n#                              None,\n#                              None,\n#                              1.\/w_pos\n#                             ],\n                'VW1': [VWClassifier(quiet=False, convert_to_vw=False, \n                                     passes=3, link='logistic',\n                                     random_seed=314),\n                             {'pos_threshold':0.5},\n                             {},\n                             None,\n                             None,\n                             1.\/w_pos\n                            ],\n#                 'VW2': [VWClassifier(quiet=False, convert_to_vw=False, \n#                                      passes=5, link='logistic',\n#                                      random_seed=314),\n#                              {'pos_threshold':0.5},\n#                              {},\n#                              None,\n#                              None,\n#                              1.\/w_pos\n#                             ],\n#                 'VW3': [VWClassifier(quiet=False, convert_to_vw=False, \n#                                      passes=10, link='logistic',\n#                                      random_seed=314),\n#                              {'pos_threshold':0.5},\n#                              {},\n#                              None,\n#                              None,\n#                              1.\/w_pos\n#                             ],\n         }\n\n# for i in [22]:\n#     mdl_inputs['VW_passes3_thrs{}'.format(i)] = mdl_inputs['VW_passes3'].copy()\n#     mdl_inputs['VW_passes3_thrs{}'.format(i)][1] = {'pos_threshold':i\/100.}","44052332":"from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.base import clone, ClassifierMixin, RegressorMixin\n\ndef train_single_model(clf_, X_, y_, random_state_=314, opt_parameters_={}, fit_params_={}):\n    '''\n    A wrapper to train a model with particular parameters\n    '''\n    c = clone(clf_)\n    \n    param_dict = {}\n    if 'VW' in type(c).__name__:\n        # we need to get ALL parameters, as the VW instance is destroyed on set_params\n        param_dict = c.get_params()\n        # the threshold is lost in the cloning\n        param_dict['pos_threshold'] = clf_.pos_threshold\n        param_dict.update(opt_parameters_)\n        # the random_state is random_seed so far\n        param_dict.update({'random_seed': random_state_})\n        if hasattr(c, 'fit_'):\n            # reset VW if it has already been trained\n            c.get_vw().finish()\n            c.vw_ = None \n    else:\n        param_dict = opt_parameters_\n        param_dict['random_state'] = random_state_\n    # Set pre-configured parameters\n    c.set_params(**param_dict)\n    #print('Threshold = ',c.pos_threshold)\n    \n    return c.fit(X_, y_, **fit_params_)\n\ndef train_model_in_CV(model, X, y, metric, metric_args={},\n                            model_name='xmodel',\n                            seed=31416, n=5,\n                            opt_parameters_={}, fit_params_={},\n                            verbose=True,\n                            groups=None, \n                            y_eval=None,\n                            w_=1.):\n    # the list of classifiers for voting ensable\n    clfs = []\n    # performance \n    perf_eval = {'score_i_oof': 0,\n                 'score_i_ave': 0,\n                 'score_i_std': 0,\n                 'score_i': []\n                }\n\n    cv = KFold(n, shuffle=True, random_state=seed) #Stratified\n\n    scores = []\n    clfs = []\n\n    for n_fold, (trn_idx, val_idx) in enumerate(cv.split(X, (y!=0).astype(np.int8), groups=groups)):\n        X_trn, y_trn = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n        X_trn_vw = to_vw(X_trn, convert_labels_sklearn_to_vw(y_trn), w=w_).values\n        X_val_vw = to_vw(X_val, convert_labels_sklearn_to_vw(y_val), w=w_).values\n\n        #display(y_trn.head())\n        clf = train_single_model(model, X_trn_vw, None, 314+n_fold, opt_parameters_, fit_params_)\n        #plt.hist(clf.decision_function(X_val_vw), bins=50)\n        \n        if 'VW' in type(clf).__name__:\n            x_thres = np.linspace(0.05, 0.95, num=37)\n            y_f1    = []\n            for thres in x_thres:\n                # predict on the validation sample\n                y_pred_tmp = (clf.decision_function(X_val_vw) > thres).astype(int)\n                y_f1.append(metric(y_val, y_pred_tmp, **metric_args))\n            i_opt = np.argmax(y_f1)\n\n            clf.pos_threshold = x_thres[i_opt]\n            #print('Optimal threshold = {:.4f}'.format(clf.pos_threshold))\n        \n        # predict on the validation sample\n        y_pred_tmp = (clf.decision_function(X_val_vw) > clf.pos_threshold).astype(int)\n        #store evaluated metric\n        scores.append(metric(y_val, y_pred_tmp, **metric_args))\n        \n        # store the model\n        clfs.append(('{}{}'.format(model_name,n_fold), clf))\n        \n        #cleanup\n        del X_trn, y_trn, X_val, y_val, y_pred_tmp, X_trn_vw, X_val_vw\n\n    #plt.show()\n    perf_eval['score_i_oof'] = 0\n    perf_eval['score_i'] = scores            \n    perf_eval['score_i_ave'] = np.mean(scores)\n    perf_eval['score_i_std'] = np.std(scores)\n\n    return clfs, perf_eval, None\n\ndef print_perf_clf(name, perf_eval, fmt='.4f'):\n    print('Performance of the model:')    \n    print('Mean(Val) score inner {} Classifier: {:{fmt}}+-{:{fmt}}'.format(name, \n                                                                       perf_eval['score_i_ave'],\n                                                                       perf_eval['score_i_std'],\n                                                                       fmt=fmt\n                                                                     ))\n    print('Min\/max scores on folds: {:{fmt}} \/ {:{fmt}}'.format(np.min(perf_eval['score_i']),\n                                                            np.max(perf_eval['score_i']),\n                                                            fmt=fmt\n                                                           ))\n    print('OOF score inner {} Classifier: {:{fmt}}'.format(name, perf_eval['score_i_oof'], fmt=fmt))\n    print('Scores in individual folds: [{}]'\n          .format(' '.join(['{:{fmt}}'.format(c, fmt=fmt) \n                            for c in perf_eval['score_i']\n                           ])\n                 )\n         )","721f5e4c":"%%time\nfrom sklearn.metrics import f1_score\n\nmdls = {}\nresults = {}\ny_oofs = {}\nfor name, (mdl, mdl_pars, fit_pars, y_, g_, w_) in mdl_inputs.items():\n    print('--------------- {} -----------'.format(name))\n    mdl_, perf_eval_, y_oof_ = train_model_in_CV(mdl, X_trn2.iloc[:],\n                                                  y_trn.iloc[:], f1_score, \n                                                  metric_args={},\n                                                  model_name=name, \n                                                  opt_parameters_=mdl_pars,\n                                                  fit_params_=fit_pars, \n                                                  n=5,\n                                                  verbose=500, \n                                                  groups=g_, \n                                                  y_eval=None if 'LGBMRanker' not in type(mdl).__name__ else y_rnk_eval,\n                                                  w_=w_\n                                                )\n    results[name] = perf_eval_\n    mdls[name] = mdl_\n    y_oofs[name] = y_oof_\n    print_perf_clf(name, perf_eval_)","61eaa81a":"%%time\ny_subs= {}\nX_tst_vw = to_vw(X_tst, None).values\nfor c in mdl_inputs:\n    mdls_= mdls[c]\n    y_sub = np.zeros(X_tst_vw.shape[0])\n    for mdl_ in mdls_:\n        y_sub += mdl_[1].decision_function(X_tst_vw)\n    y_sub \/= len(mdls_)\n    \n    y_subs[c] = y_sub","ddaffefb":"df_sub = pd.read_csv('..\/input\/sample_submission.csv')","eb3ed466":"s= 'VW1'#VW_passes3_w\ndf_sub['prediction'] = (y_subs[s] > np.median([mdl_[1].pos_threshold for mdl_ in mdls[s]])).astype(int)\ndf_sub.to_csv('submission.csv', index=False)","de2c4ccc":"!head submission.csv","ce3bf58d":"## Prepare submission","95f8f04e":"--------------- VW1_l1_1em4 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l1_1em4 Classifier: 0.3987+-0.0173\nMin\/max scores on folds: 0.3705 \/ 0.4185\nOOF score inner VW1_l1_1em4 Classifier: 0.0000\nScores in individual folds: [0.4185 0.3911 0.3705 0.3993 0.4142]\n--------------- VW1_l1_1em3 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l1_1em3 Classifier: 0.1171+-0.0024\nMin\/max scores on folds: 0.1132 \/ 0.1203\nOOF score inner VW1_l1_1em3 Classifier: 0.0000\nScores in individual folds: [0.1203 0.1169 0.1132 0.1185 0.1164]\n--------------- VW1_l2_1em4 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l2_1em4 Classifier: 0.4889+-0.0128\nMin\/max scores on folds: 0.4659 \/ 0.5017\nOOF score inner VW1_l2_1em4 Classifier: 0.0000\nScores in individual folds: [0.4848 0.4981 0.4659 0.5017 0.4939]\n--------------- VW1_l2_1em3 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l2_1em3 Classifier: 0.3869+-0.0157\nMin\/max scores on folds: 0.3606 \/ 0.4039\nOOF score inner VW1_l2_1em3 Classifier: 0.0000\nScores in individual folds: [0.3950 0.3777 0.3606 0.3972 0.4039]\n--------------- VW2_l1_1em4 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l1_1em4 Classifier: 0.1171+-0.0024\nMin\/max scores on folds: 0.1132 \/ 0.1203\nOOF score inner VW2_l1_1em4 Classifier: 0.0000\nScores in individual folds: [0.1203 0.1169 0.1132 0.1185 0.1164]\n--------------- VW2_l1_1em3 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l1_1em3 Classifier: 0.1171+-0.0024\nMin\/max scores on folds: 0.1132 \/ 0.1203\nOOF score inner VW2_l1_1em3 Classifier: 0.0000\nScores in individual folds: [0.1203 0.1169 0.1132 0.1185 0.1164]\n--------------- VW2_l2_1em4 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l2_1em4 Classifier: 0.4441+-0.0153\nMin\/max scores on folds: 0.4260 \/ 0.4705\nOOF score inner VW2_l2_1em4 Classifier: 0.0000\nScores in individual folds: [0.4367 0.4501 0.4260 0.4370 0.4705]\n--------------- VW2_l2_1em3 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l2_1em3 Classifier: 0.2886+-0.0157\nMin\/max scores on folds: 0.2693 \/ 0.3157\nOOF score inner VW2_l2_1em3 Classifier: 0.0000\nScores in individual folds: [0.2866 0.3157 0.2783 0.2932 0.2693]\n","b5795323":"Print data stats","4a05bf15":"Stop words","2ec89a24":"Display a couple of first entries","22c3e285":"Extract the training features and the target variable","39bda453":"Actual training of the model","e756762b":"Helper functions for Vowpal Wabbit","26159438":"The function to do training in a cross-validation loop and evaluate performance of the model","9673e8d9":"# Linear model using Vowpal Wabbit\n\n<img src=\"https:\/\/cdn.dribbble.com\/users\/261617\/screenshots\/3146111\/vw-dribbble.png\" alt=\"drawing\" width=\"400\"\/>\n\n## What is Vowpal Wabbit?\nThis is a package to perform fast training of linear models. It is a very sophysticated tool, that allows to use many different advanced algorithms. I encourage you to check out their [Wiki on github](https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/wiki)\n\nFor the baseline the two key issues are:\n\n- it is **fast**. In fact, the package allows online learning, i.e. sequential processing of training examples one-by-one, similar to what SGDClassifier\/SGDRegressor models in sklearn aim to achieve. But in command-line mode one truelly reads only a single line  from an input file into memory, thus one can train a model on a dataset that does not fit into memory.\n- it **applies hashing on text features**. \n   - This means that we do not need to run much of pre-processing and can let the machine to do the learning. That's what is implemented in this baseline- we directly feed the message text into the training removing punctuation and stopwords only (the latter is not needed in fact). \n   - This also means that the text features are stored in a more compact form that the naive OHE (=BoW) representation, thus memory footprint is reduced.\n   \nIn the following kernel the sklearn API of VW is used. This allows to use the same data and the same methods to be used in VW as well as in other ML tools. A small technical note: the dataset had to be slightly processed, as the VW internal functions can not properly handle text inputs in a DataFrame.\n\nThe ideas of using class weighting for disbalance problem, threshold optimisation and ngrams come from https:\/\/www.kaggle.com\/hippskill\/vowpal-wabbit-starter-pack (check it out- it is a solid piece of work)","7bbbf08b":"Define the model that will be trained: our `VW_passes3` model will do 3 iterations (=passes) over the data using the prepared text format as the input. **The threshold to apply for 0\/1 label assignment was tuned on the data to get the best F1 score**","6f46da13":"--------------- VW1 -----------\nPerformance of the model:\nMean(Val) score inner VW1 Classifier: 0.4984+-0.0122\nMin\/max scores on folds: 0.4783 \/ 0.5154\nOOF score inner VW1 Classifier: 0.0000\nScores in individual folds: [0.4952 0.4978 0.4783 0.5154 0.5054]\n--------------- VW1_l1_1em7 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l1_1em7 Classifier: 0.4987+-0.0123\nMin\/max scores on folds: 0.4787 \/ 0.5161\nOOF score inner VW1_l1_1em7 Classifier: 0.0000\nScores in individual folds: [0.4956 0.4981 0.4787 0.5161 0.5050]\n--------------- VW1_l1_1em5 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l1_1em5 Classifier: 0.5009+-0.0104\nMin\/max scores on folds: 0.4853 \/ 0.5149\nOOF score inner VW1_l1_1em5 Classifier: 0.0000\nScores in individual folds: [0.4983 0.4853 0.4964 0.5097 0.5149]\n--------------- VW1_l2_1em7 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l2_1em7 Classifier: 0.4985+-0.0121\nMin\/max scores on folds: 0.4787 \/ 0.5154\nOOF score inner VW1_l2_1em7 Classifier: 0.0000\nScores in individual folds: [0.4952 0.4978 0.4787 0.5154 0.5054]\n--------------- VW1_l2_1em5 -----------\nPerformance of the model:\nMean(Val) score inner VW1_l2_1em5 Classifier: 0.5037+-0.0108\nMin\/max scores on folds: 0.4875 \/ 0.5200\nOOF score inner VW1_l2_1em5 Classifier: 0.0000\nScores in individual folds: [0.5030 0.4990 0.4875 0.5200 0.5094]\n--------------- VW2 -----------\nPerformance of the model:\nMean(Val) score inner VW2 Classifier: 0.4430+-0.0058\nMin\/max scores on folds: 0.4373 \/ 0.4536\nOOF score inner VW2 Classifier: 0.0000\nScores in individual folds: [0.4390 0.4373 0.4444 0.4406 0.4536]\n--------------- VW2_l1_1em7 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l1_1em7 Classifier: 0.4461+-0.0059\nMin\/max scores on folds: 0.4401 \/ 0.4569\nOOF score inner VW2_l1_1em7 Classifier: 0.0000\nScores in individual folds: [0.4401 0.4413 0.4450 0.4469 0.4569]\n--------------- VW2_l1_1em5 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l1_1em5 Classifier: 0.4768+-0.0102\nMin\/max scores on folds: 0.4640 \/ 0.4936\nOOF score inner VW2_l1_1em5 Classifier: 0.0000\nScores in individual folds: [0.4701 0.4744 0.4640 0.4820 0.4936]\n--------------- VW2_l2_1em7 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l2_1em7 Classifier: 0.4432+-0.0070\nMin\/max scores on folds: 0.4361 \/ 0.4555\nOOF score inner VW2_l2_1em7 Classifier: 0.0000\nScores in individual folds: [0.4361 0.4371 0.4420 0.4452 0.4555]\n--------------- VW2_l2_1em5 -----------\nPerformance of the model:\nMean(Val) score inner VW2_l2_1em5 Classifier: 0.4501+-0.0124\nMin\/max scores on folds: 0.4330 \/ 0.4707\nOOF score inner VW2_l2_1em5 Classifier: 0.0000\nScores in individual folds: [0.4330 0.4495 0.4439 0.4532 0.4707]","b9809163":"Read in the data","e90aa465":"Models, that were trained on k sets of k-1 folds are averaged before application of the decision threshold"}}