{"cell_type":{"13c4faa0":"code","52da0d45":"code","5c6028e6":"code","69e368af":"code","e38497d1":"code","1beface4":"code","f9c20fb2":"code","4e5306ef":"code","575574db":"code","fbd6a677":"code","c19c4e52":"code","95fe9c19":"code","b5c6ae44":"code","8aa8f8ae":"code","f5abd97e":"code","01ceef93":"code","7fa0786d":"code","434a7ae1":"code","b5aa6cc9":"code","bb1d1a61":"code","57f305b4":"code","1dcb2e0f":"code","834c7073":"code","3fe9e026":"code","e3a82676":"code","6b7d354f":"code","288be7e6":"markdown","e6410c1e":"markdown","1b5cd19f":"markdown","d8867e27":"markdown","ce6c27cb":"markdown","c8565d8d":"markdown","34cf0162":"markdown","62a8e4e4":"markdown","c17bc686":"markdown","26035876":"markdown","3649d14f":"markdown","911752f4":"markdown","f99afd78":"markdown"},"source":{"13c4faa0":"#Importing the libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline\n\n#scikit learn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cross_validation import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n","52da0d45":"df = pd.read_csv(\"..\/input\/data.csv\")","5c6028e6":"df.head()","69e368af":"df.info()","e38497d1":"#We can see Unnamed:32 has all null values hence we cannot use this column for our analysis and id will also be of no use for analysis\ndf.drop('Unnamed: 32', axis  = 1, inplace=True)\ndf.drop('id', axis = 1, inplace= True)\n","1beface4":"#Let us convert 'Malign' and 'Benign' to 1 and 0 respectively so it will be easier for analysis\n\ndf['diagnosis'] = df['diagnosis'].map({'M':1,'B':0})","f9c20fb2":"df.describe()","4e5306ef":"sns.countplot(df['diagnosis'])","575574db":"df.columns","fbd6a677":"#The mean, standard error, and \"worst\" or largest (mean of the three largest values) of these features were computed for each image,resulting in 30 features.\n#For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n#more info at https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer-wisconsin\/wdbc.names\n\nfirst = list(df.columns[1:10])\nsecond = list(df.columns[11:21])\nthird =  list(df.columns[21:30])\n","c19c4e52":"#Let us find the correlation between different attributes\ncorr1 = df[first].corr()","95fe9c19":"#Let us visualize with a heatmap\nplt.figure(figsize=(14,10))\nsns.heatmap(corr1, cmap='coolwarm', xticklabels = first,  yticklabels = first, annot=True)","b5c6ae44":"#Let us perform analysis on the mean features\n\nmelign = df[df['diagnosis'] == 1][first]\nbening = df[df['diagnosis'] == 0][first]\n","8aa8f8ae":"melign.columns","f5abd97e":"for columns in melign.columns:\n    plt.figure()\n    sns.distplot(melign[columns], kde=False, rug= True)\n    sns.distplot(bening[columns], kde=False, rug= True)\n    sns.distplot\nplt.tight_layout()\n","01ceef93":"color_function = {0: \"green\", 1: \"red\"}\ncolors = df[\"diagnosis\"].map(lambda x: color_function.get(x))\n\npd.plotting.scatter_matrix(df[first], c=colors, alpha = 0.4, figsize = (15, 15));\n","7fa0786d":"#We divide the data into Training and test set \ntrain, test = train_test_split(df, test_size = 0.25)","434a7ae1":"# I have created a function to perform k folds cross validation which helps in obtaining a better insight to test the accuracy of the model\n# More info at https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/\n\ndef classification_model(model, data, predictors, outcome):\n  #Fit the model:\n  model.fit(data[predictors],data[outcome])\n  \n  predictions = model.predict(data[predictors])\n  \n  accuracy = metrics.accuracy_score(predictions,data[outcome])\n  print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n\n  #Perform k-fold cross-validation with 5 folds\n  kf = KFold(data.shape[0],n_folds= 5)\n  error = []\n  for train, test in kf:\n    # Filter the training data\n    train_predictors = (data[predictors].iloc[train,:])\n    train_target = data[outcome].iloc[train]\n    model.fit(train_predictors, train_target)\n    \n    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n    \n    print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n    \n  model.fit(data[predictors],data[outcome]) ","b5aa6cc9":"#Using Logistic regression on the top five features\n#more info at https:\/\/en.wikipedia.org\/wiki\/Logistic_regression\n\npredictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\noutcome_var='diagnosis'\nmodel=LogisticRegression()\nclassification_model(model,train,predictor_var,outcome_var)","bb1d1a61":"#Let us check the accuracy on test data\nclassification_model(model, test,predictor_var,outcome_var)","57f305b4":"#Let us try to classify using a decision tree classifier \npredictor_var = ['radius_mean','perimeter_mean','area_mean','compactness_mean','concave points_mean']\nmodel = DecisionTreeClassifier()\nclassification_model(model,train,predictor_var,outcome_var)","1dcb2e0f":"classification_model(model, test,predictor_var,outcome_var)","834c7073":"predictor_var = first\nmodel = RandomForestClassifier()\nclassification_model(model, train,predictor_var,outcome_var)","3fe9e026":"#Let us find the most important features used for classification model\n\nfeatimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\nprint(featimp)","e3a82676":"predictor_var = ['concave points_mean','area_mean','radius_mean','perimeter_mean','concavity_mean']\nmodel = RandomForestClassifier()\nclassification_model(model,train,predictor_var,outcome_var)","6b7d354f":"# I think we get a better prediction with all the features now let us try it on test data!\npredictor_var = first\nmodel = RandomForestClassifier()\nclassification_model(model, test,predictor_var,outcome_var)","288be7e6":"# Breast cancer detection\ndataset source: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)\n","e6410c1e":"We are getting 100% accuracy! Is it overfitting let us try it on test data\n","1b5cd19f":"**We can see that radius, perimeter and area are highly correlated as seen from the heatmap.** <br>\n**Also compactness_mean, concavepoint_mean and concavity_mean are highly correlated**\n","d8867e27":"## Performing Exploratory Data Analysis\n","ce6c27cb":"We can see that the mean values of perimeter, area, concavity, compactness, radius and concave points can be used for classification as these parameters show a correlation. <br>\nWhile parameters such as smoothness, symmetry, fractual dimension and texture don't show much seperation and is of not much use for classification.\n","c8565d8d":"## Conclusion\n\nHence we can see detailed exploratory data analysis of breast cancer data and implementation of classification algorithms to train a model in detecting whether the cancer is benign or malign.\n","34cf0162":"Let us try using random forest","62a8e4e4":"**Attribute Information:**\n<br>\n1) ID number <br>\n2) Diagnosis (M = malignant, B = benign) <br>\n3-32) <br>\n\nTen real-valued features are computed for each cell nucleus: <br>\n\na) radius (mean of distances from center to points on the perimeter) <br>\nb) texture (standard deviation of gray-scale values) <br>\nc) perimeter <br>\nd) area <br>\ne) smoothness (local variation in radius lengths) <br>\nf) compactness (perimeter^2 \/ area - 1.0) <br>\ng) concavity (severity of concave portions of the contour) <br>\nh) concave points (number of concave portions of the contour) <br>\ni) symmetry <br>\nj) fractal dimension (\"coastline approximation\" - 1) <br>","c17bc686":"## Machine learning\n","26035876":"**Using a scatter matrix we can see a well seperation of malign and benign cancer with green points indication benign cancer cells and red points indicating malign cancer cells.**\n","3649d14f":"We have 30 different attributes from images extracted, Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. <br>\nWe predict the Stage of Breast Cancer B (Bengin) or M (malignant).","911752f4":"We can see there are almost double number patients with benign cancer","f99afd78":"## Let us learn more about the data "}}