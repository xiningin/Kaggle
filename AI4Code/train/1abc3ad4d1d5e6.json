{"cell_type":{"5bd40d45":"code","30f3ff04":"code","ccb72e12":"code","f143b042":"code","a5431fee":"code","9132fb67":"code","bc61dbcb":"code","00fbf323":"code","eaa8a14b":"code","3e8eef65":"code","d9ec7cda":"code","02f65715":"code","dad2f0db":"code","6004a413":"code","886f8194":"markdown"},"source":{"5bd40d45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30f3ff04":"! pip install simplet5 -q","ccb72e12":"from sklearn.model_selection import train_test_split\nimport random\nimport torch\nimport re\nimport os\nimport string\n\nimport pandas as pd\n\nfrom simplet5 import SimpleT5","f143b042":"class Settings:\n#     PROJ_NAME = 'Text-Summarization-Using-T5'\n#     root_path = os.getcwd().split(PROJ_NAME)[0] + PROJ_NAME + \"\\\\\"\n#     APPLICATION_PATH = root_path + \"backend\\\\services\\\\text_summarization\\\\application\\\\\"\n    # setting up logs path\n#     LOGS_DIRECTORY = root_path + \"backend\\\\services\\\\text_summarization\\\\logs\\\\logs.txt\"\n\n    MODEL_TYPE = \"t5\"\n    MODEL_NAME = \"t5-base\"\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # training data directory\n    TRAIN_DATA = \"\/kaggle\/input\/news-summary\/news_summary.csv\"\n\n    Columns = ['headlines', 'text']\n\n    USE_GPU = None\n    if str(DEVICE) == \"cuda\":\n        USE_GPU=True\n    else:\n        USE_GPU = False\n\n    EPOCHS = 5\n\n    encoding = 'latin-1'\n    columns_dict = {\"headlines\": \"target_text\", \"text\": \"source_text\"}\n    df_column_list = ['source_text', 'target_text']\n    SUMMARIZE_KEY = \"summarize: \"\n    SOURCE_TEXT_KEY = 'source_text'\n    TEST_SIZE = 0.2\n    BATCH_SIZE = 8\n    source_max_token_len = 128\n    target_max_token_len = 50\n    train_df_len = 5000\n    test_df_len = 100","a5431fee":"class Preprocess:\n    def __init__(self):\n        self.settings = Settings\n\n    def clean_text(self, text):\n        text = text.lower()\n        text = re.sub('\\[.*?\\]', '', text)\n        text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n        text = re.sub('<.*?>+', '', text)\n        text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n        text = re.sub('\\n', '', text)\n        text = re.sub('\\w*\\d\\w*', '', text)\n        return text\n\n    def preprocess_data(self, data_path):\n        df = pd.read_csv(data_path, encoding=self.settings.encoding, usecols=self.settings.Columns)\n        # simpleT5 expects dataframe to have 2 columns: \"source_text\" and \"target_text\"\n        df = df.rename(columns=self.settings.columns_dict)\n        df = df[self.settings.df_column_list]\n        # T5 model expects a task related prefix: since it is a summarization task, we will add a prefix \"summarize: \"\n        df[self.settings.SOURCE_TEXT_KEY] = self.settings.SUMMARIZE_KEY + df[self.settings.SOURCE_TEXT_KEY]\n\n        return df\n","9132fb67":"class T5Model:\n    def __init__(self, model_type, model_name):\n        self.model = SimpleT5()\n        self.model.from_pretrained(model_type=model_type,\n                                   model_name=model_name)\n\n    def load_model(self, model_type, model_path, use_gpu: bool):\n        try:\n            self.model.load_model(\n                model_type=model_type,\n                model_dir=model_path,\n                use_gpu=use_gpu\n            )\n\n        except BaseException as ex:\n            print(\"error occurred while loading model \", str(ex))","bc61dbcb":"class Train:\n    def __init__(self):\n        # initialize required class\n        self.settings = Settings\n        self.preprocess = Preprocess()\n\n        # initialize required variables\n        self.t5_model = None\n\n    def __initialize(self):\n        try:\n            self.t5_model = T5Model(model_name=self.settings.MODEL_NAME,\n                                    model_type=self.settings.MODEL_TYPE)\n\n        except BaseException as ex:\n            print(\"error occurred while loading model \", str(ex))\n\n    def set_seed(self, seed_value=42):\n        random.seed(seed_value)\n        np.random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n\n    def train(self, df):\n        try:\n            train_df, test_df = train_test_split(df, test_size=self.settings.TEST_SIZE)\n\n            self.t5_model.model.train(train_df=train_df[:self.settings.train_df_len],\n                                      eval_df=test_df[:self.settings.test_df_len],\n                                      source_max_token_len=self.settings.source_max_token_len,\n                                      target_max_token_len=self.settings.target_max_token_len,\n                                      batch_size=self.settings.BATCH_SIZE, max_epochs=self.settings.EPOCHS,\n                                      use_gpu=self.settings.USE_GPU)\n\n        except BaseException as ex:\n            print(\"error occurred while loading model \", str(ex))\n\n    def run(self):\n        try:\n            print(\"Loading and Preparing the Dataset-----!! \")\n            df = self.preprocess.preprocess_data(self.settings.TRAIN_DATA)\n            print(df.head())\n            print(\"Dataset Successfully Loaded and Prepared-----!! \")\n            print(\"Loading and Initializing the T5 Model -----!! \")\n            self.__initialize()\n            print(\"Model Successfully Loaded and Initialized-----!! \")\n\n            print(\"------------------Starting Training-----------!!\")\n            self.set_seed()\n            self.train(df)\n            print(\"Training complete-----!!!\")\n\n        except BaseException as ex:\n            print(\"Following Exception Occurred---!! \", str(ex))","00fbf323":"print(Settings.USE_GPU)\nprint(Settings.DEVICE)","eaa8a14b":"t= Train()\nt.run()","3e8eef65":"! ( cd outputs; ls )","d9ec7cda":"#src = https:\/\/www.thehindu.com\/business\/Industry\/twitter-interim-grievance-officer-for-india-quits\/article35004295.ece\ntext_to_summarize=\"\"\"summarize: Twitter\u2019s interim resident grievance officer for India has stepped down, leaving the micro-blogging site without a grievance official as mandated by the new IT rules to address complaints from Indian subscribers, according to a source.\n\nThe source said that Dharmendra Chatur, who was recently appointed as interim resident grievance officer for India by Twitter, has quit from the post.\n\nThe social media company\u2019s website no longer displays his name, as required under Information Technology (Intermediary Guidelines and Digital Media Ethics Code) Rules 2021.\n\nTwitter declined to comment on the development.\n\nThe development comes at a time when the micro-blogging platform has been engaged in a tussle with the Indian government over the new social media rules. The government has slammed Twitter for deliberate defiance and failure to comply with the country\u2019s new IT rules.\n\"\"\"","02f65715":"best_weight = \".\/outputs\/SimpleT5-epoch-4-train-loss-0.6182\"","dad2f0db":"t5_model = T5Model(model_name=Settings.MODEL_NAME,model_type=Settings.MODEL_TYPE)\nt5_model.load_model(model_type=Settings.MODEL_TYPE,\n                   use_gpu=Settings.USE_GPU,\n                   model_path=best_weight\n                   )","6004a413":"t5_model.model.predict(text_to_summarize)","886f8194":"## Inference"}}