{"cell_type":{"80575112":"code","ded0a2d5":"code","975b71e6":"code","abe7fc65":"code","fe36ce31":"code","3f13e7e3":"code","ff26ef39":"code","5555bf34":"code","362a858c":"code","468c77e7":"code","5d90b445":"code","fc56755b":"code","d598df82":"code","b0abbff7":"code","0d2a7983":"code","bd7ca9d1":"code","7cadccc4":"code","652c2745":"code","1c124d26":"code","d998301c":"code","39a3e402":"code","369235bd":"code","3dcb7191":"code","da435db8":"markdown","269660a9":"markdown","a991d65b":"markdown","ea04167b":"markdown","1e88663e":"markdown","c8e60259":"markdown","956c0bf3":"markdown","60be00e0":"markdown","a4677dc3":"markdown","0f1cd12c":"markdown","608438ef":"markdown","a20a55e8":"markdown","9bae7f26":"markdown"},"source":{"80575112":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten","ded0a2d5":"data = pd.read_csv(\"..\/input\/churn-modelling\/Churn_Modelling.csv\")\ndata.head()","975b71e6":"# check the shape of the data\ndata.shape","abe7fc65":"# independent variable\nX=data.drop(['RowNumber','CustomerId','Surname','Exited'],axis=1)\n#dependent variable\ny=data['Exited']\n\n# checking the shape of the data\nX.shape, y.shape","fe36ce31":"X.head()","3f13e7e3":"# import the encoder\nfrom sklearn.preprocessing import LabelEncoder","ff26ef39":"# inputing the labelencoder into a variable 'label1'\nlabel1 = LabelEncoder()","5555bf34":"# transforming the geography column of the dataset\nX['Geography']=label1.fit_transform(X['Geography'])\nX.head()","362a858c":"# transforming the Gender column of the dataset\nX['Gender']=label1.fit_transform(X['Gender'])\nX.head()","468c77e7":"X=pd.get_dummies(X,drop_first=True,columns=['Geography'])\nX.head()","5d90b445":"# dividing the data into training and testing data\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=99,stratify=y)\n\n# checking the shape of the training and testing data\nX_train.shape,X_test.shape,y_train.shape,y_test.shape\n","fc56755b":"X_train.head()","d598df82":"# importing the standard scaler\nfrom sklearn.preprocessing import StandardScaler","b0abbff7":"scaler=StandardScaler()\n# scaling the training data\nX_train=scaler.fit_transform(X_train)\n# scaling the testing data\nX_test=scaler.fit_transform(X_test)\n\n# check\nX_train","0d2a7983":"# import model\nmodel=Sequential()\n# import layers\nmodel.add(Dense(X.shape[1],activation='relu',input_dim=X.shape[1]))\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))","bd7ca9d1":"# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","7cadccc4":"y_train=y_train.values\ny_train\ny_test=y_test.values\ny_test","652c2745":"%%time\n# fitting the model\nhistory=model.fit(X_train,y_train,batch_size=10,epochs=20)","1c124d26":"# prediction\ny_pred=model.predict_classes(X_test)\ny_pred","d998301c":"# check the accuracy\nx_loss,x_acc=model.evaluate(X_test,y_test)\nprint(x_loss,x_acc)","39a3e402":"# import the libraries\nfrom sklearn.metrics import confusion_matrix,accuracy_score","369235bd":"# accuracy score\nconfusion_matrix(y_test,y_pred)","3dcb7191":"pd.DataFrame(history.history).plot(figsize=(8,5))\nplt.grid(True)\nplt.show()","da435db8":"We can see that our data is varied in a long range, so we need to scale the data in a specific range.\n\nWe will use Standard Scaler to scale the data here","269660a9":"This dataset contains the customers details of the bank and here the target variable is in binary form stating that the customer has left the bank or he continues to be the customer","a991d65b":"Checking the accuracy score by using the confusion matrix,accuracy score","ea04167b":"Here we have details of the 10k customers into 14 columns.\n\nWe can clearly make out from the dataset that the columns RowNumber, CustomerId and Surname does not have any correlation with the dataset. So we will drop that first.\n\nWe will now divide the dataset into dependent and independent dataset , also the 'Exited' column is our dependent variable which we will  drop that to.\n","1e88663e":"Here we converted the string into numerical values for the Gender and Geography dataset, but as it is a categorical value it doest mean the value '2' to is greater than '1' or its greater than '0'. So in order to give equal importance to the categorical values we will convert the values by Onehotencoding or by getdummies","c8e60259":"Importing the dataset","956c0bf3":"We can see that from our dataset that , our Geography and gender columns contains the categorical values, we will need to encode the data","60be00e0":"Importing the important libraries","a4677dc3":"Here batch size specifies the no of inputs we want to evaluate a weight, for example here for every 10 inputs our weights will get updated.\n\nNote: if the batch size is 1 then for every input the weights will get updated, which is not feasible If the batch size is equal to the size of the total data(rows) then the stochastic gradient will become kind of batch gradient","0f1cd12c":"Now our data is preprocessed properlly we will build the model","608438ef":"Here we added the no neurons we want to run as the no features in our X dataset and input dimension(11) which is equal to 11 * 11 also we gave the same no of features. this is for the first input layer\n\nthen for the second layer which is our only hidden layer we are giving 128 no of neurons with relu activation\n\nthen for the third layer which is our output layer we are 1 as the value as the output will contain 0 or 1 and so the activation associated with the binary is sigmoid","a20a55e8":"Here our y_train was in a series format so we converted the same into an array like X_train","9bae7f26":"Here we used the getdummies library on X data and the column we used on is Geography, but take notice that we used drop_first=True, this will drop one column from geopgraphy after creating dummy columns to reduce the redundancy.\n\nAlso we did not use the get dummies on Gender columns as they are already of the value 0 and 1 only unlike Geography had 0,1,2\n\nNow we will divide our data into training and testing data"}}