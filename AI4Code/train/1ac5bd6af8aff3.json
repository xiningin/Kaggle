{"cell_type":{"1692dbed":"code","93d84c5e":"code","d272e041":"code","f5e42b8c":"code","77d176d4":"code","ba77ef14":"code","fcb18cff":"code","80e4414f":"code","78d5faf5":"code","40f41b90":"code","d7a5c92d":"code","a86e0240":"code","9fc26b29":"code","8c0f2e3c":"code","3a0d7f84":"code","283e48ce":"code","ebb34e33":"markdown","90ac7973":"markdown","580d3257":"markdown"},"source":{"1692dbed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93d84c5e":"import pandas as pd\n\ntrain_df = pd.read_csv('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\ntrain_df.sample(5)","d272e041":"import json\nfile = open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/{}'.format(train_df['Id'].iloc[1])+'.json')\ntext_json = json.load(file)","f5e42b8c":"# text_json  # [{'section_title':'', 'text':''}]\n# len(text_json)\n# text_json[8].get('section_title')","77d176d4":"# file = open('coleridgeinitiative-show-us-the-data\/test\/2100032a-7c33-4bff-97ef-690822c43466.json')\n# text_json = json.load(file)\n# text_json","ba77ef14":"def get_all_text(json_file):\n    \n    file = open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/{}'.format(json_file)+'.json')\n    text_json = json.load(file)\n    length = len(text_json)\n    text_list = []\n    \n    for i in range(length):\n        text_list.append(text_json[i].get('text'))\n    text = ' '.join(text_list)\n    \n    return text\n\ndef get_all_section_titles(json_file):\n    \n    file = open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/train\/{}'.format(json_file)+'.json')\n    text_json = json.load(file)\n    length = len(text_json)\n    section_titles_list = []\n    \n    for i in range(length):\n        section_titles_list.append(text_json[i].get('section_title'))\n    section_titles = ' '.join(section_titles_list)\n    \n    return section_titles","fcb18cff":"train_df['text'] = train_df['Id'].apply(get_all_text)\ntrain_df['section_titles'] = train_df['Id'].apply(get_all_section_titles)","80e4414f":"train_df.drop(columns=['dataset_label','Id'],inplace=True) ","78d5faf5":"import re\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","40f41b90":"train_df['section_titles'] = train_df['section_titles'].apply(clean_text)\ntrain_df['text'] = train_df['text'].apply(clean_text)\ntrain_df['dataset_title'] = train_df['dataset_title'].apply(clean_text)\ntrain_df['pub_title'] = train_df['pub_title'].apply(clean_text)","d7a5c92d":"train_df.sample(5)","a86e0240":"from collections import Counter\n\nprint('No of Data Titles:',len(Counter(train_df['dataset_title'])))\nprint('No of Data Labels:',len(Counter(train_df['cleaned_label'])))","9fc26b29":"import os\n\ntest_data = []\n\nfor i in os.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/'):\n    \n    file = open('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/test\/{}'.format(i))\n    text_json = json.load(file)\n    length = len(text_json)\n    text_list = []\n    section_titles_list=[]\n    \n    for j in range(length):\n        text_list.append(text_json[j].get('text'))\n        section_titles_list.append(text_json[j].get('section_title'))\n        text = ' '.join(text_list)\n        section_titles=' '.join(section_titles_list)\n        \n    test_data.append([i.split('.')[0],text,section_titles])\n    \ntest_df = pd.DataFrame(columns = ['Id','text','section_titles'], data =test_data)","8c0f2e3c":"test_df['text'] = test_df['text'].apply(clean_text)\ntest_df['section_titles'] = test_df['section_titles'].apply(clean_text)","3a0d7f84":"test_df.drop(columns=['Id'],inplace=True)","283e48ce":"test_df","ebb34e33":"# Columns\n- id - publication id - note that there are multiple rows for some training documents, indicating multiple mentioned datasets\n- pub_title - title of the publication (a small number of publications have the same title)\n- dataset_title - the title of the dataset that is mentioned within the publication\n- dataset_label - a portion of the text that indicates the dataset\n- cleaned_label - the dataset_label, as passed through the clean_text function from the Evaluation page","90ac7973":"# Data Description\nThe objective of the competition is to identify the mention of datasets within scientific publications. Your predictions will be short excerpts from the publications that appear to note a dataset. Predictions that more accurately match the precise words used to identify the dataset within the publication will score higher. Predictions should be cleaned using the clean_text function from the Evaluation page to ensure proper matching.\n\nPublications are provided in JSON format, broken up into sections with section titles.\n\nThe goal in this competition is not just to match known dataset strings but to generalize to datasets that have never been seen before using NLP and statistical techniques. A percentage of the public test set publications are drawn from the training set - not all datasets have been identified in train, so these unidentified datasets have been used as a portion of the public test labels. These should serve as guides for the difficult task of labeling the private test set.\n\nNote that the hidden test set has roughly ~8000 publications, many times the size of the public test set. Plan your compute time accordingly.","580d3257":"# Files\n- train - the full text of the training set's publications in JSON format, broken into sections with section titles\n- test - the full text of the test set's publications in JSON format, broken into sections with section titles\n- train.csv - labels and metadata for the training set\n- sample_submission.csv - a sample submission file in the correct format"}}