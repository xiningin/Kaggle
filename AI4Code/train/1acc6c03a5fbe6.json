{"cell_type":{"93bea943":"code","50016e08":"code","0e83a8bc":"code","c6a5001b":"code","925fbff2":"code","dc1884cf":"code","beda7fac":"code","58969177":"code","1c0817ff":"code","82dc5886":"code","7c9ac99d":"code","5d124eea":"code","4db978fd":"code","55911bfd":"code","06b3355b":"markdown"},"source":{"93bea943":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","50016e08":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB","0e83a8bc":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","c6a5001b":"train = pd.read_csv(\"..\/input\/processed-ag-news\/processed_train.csv\")\ntest = pd.read_csv(\"..\/input\/processed-ag-news\/processed_test.csv\")","925fbff2":"train.columns","dc1884cf":"train[\"processed_title\"].fillna(\"\", inplace=True)\ntrain[\"Class Index\"] = train[\"Class Index\"].apply(lambda x: x-1)","beda7fac":"y = train[\"Class Index\"]\nxtrain, xvalid, ytrain, yvalid = train_test_split(train.processed_title.values, y, \n                                                  stratify=y, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)","58969177":"print (xtrain.shape)\nprint (xvalid.shape)","1c0817ff":"tfv = TfidfVectorizer(min_df=3, analyzer=\"word\", ngram_range=(1,3), sublinear_tf=1, use_idf=1, smooth_idf=1)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv = tfv.transform(xtrain)\nxvalid_tfv = tfv.transform(xvalid)","82dc5886":"tfv.transform([train[\"processed_title\"].iloc[0]])","7c9ac99d":"nb_tfv = MultinomialNB()\nnb_tfv.fit(xtrain_tfv, ytrain)\nprediction = nb_tfv.predict_proba(xvalid_tfv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(yvalid, prediction))","5d124eea":"ctv = CountVectorizer(min_df=3, ngram_range=(1,3), analyzer=\"word\")\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv = ctv.transform(xtrain)\nxvalid_ctv = ctv.transform(xvalid)","4db978fd":"ctv.transform([train[\"processed_title\"].iloc[0]])","55911bfd":"nb_ctv = MultinomialNB()\nnb_ctv.fit(xtrain_ctv, ytrain)\nprediction = nb_ctv.predict_proba(xvalid_ctv)\n\nprint(\"logloss: %0.3f\" % multiclass_logloss(yvalid, prediction))","06b3355b":"We are trying to evaluate using multiiclass log loss. This is implemented in the follow way (taken from: https:\/\/github.com\/dnouri\/nolearn\/blob\/master\/nolearn\/lasagne\/util.py)"}}