{"cell_type":{"bec3b0ad":"code","bf5e3282":"code","7be2db6f":"code","1fdb8b35":"code","0410f4af":"code","b96edbc6":"code","856a165f":"code","ced09e4d":"code","e594444d":"code","cc1bae74":"code","65276f7a":"code","159dfeda":"code","112a8d8e":"code","44e0d45c":"code","c456bf19":"code","96f85ec7":"code","d0349f96":"code","6f7eabc2":"code","c1293bb4":"code","b25f21b3":"code","449a8615":"code","8e4f1312":"code","80ba4813":"code","e9fd8e5d":"code","cbc4a139":"code","40b17496":"code","26034ae8":"code","8754242d":"code","a1a1166c":"code","b5f6e9f5":"code","2c8c3d6c":"code","78f588d4":"code","cf99e216":"code","ebf059c9":"code","129f5dae":"code","53307785":"code","876a53a1":"code","ead7563f":"code","5e8f2a7f":"code","6af8583c":"markdown"},"source":{"bec3b0ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Sklearn Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bf5e3282":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","7be2db6f":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","1fdb8b35":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)","0410f4af":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","b96edbc6":"train_data.info()","856a165f":"train_data.describe()","ced09e4d":"train_data.isnull().sum()","e594444d":"corr = train_data.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True,\n    annot=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","cc1bae74":"sns.barplot(x='Pclass', y='Survived', data=train_data)","65276f7a":"sns.countplot(x='Survived', data=train_data)","159dfeda":"y = train_data['Survived']\nX= train_data.copy()\nX = X.drop(columns=['Survived', 'Name', 'Ticket', 'Cabin', 'PassengerId'],axis=1)\ntest_data = test_data.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],axis=1)\n\n\n#Dealing with Age and Embarked missing values\ndata = [X, test_data]\n\nfor dataset in data:\n    mean = X[\"Age\"].mean()\n    std = test_data[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = X[\"Age\"].astype(int)\n    \nX[\"Age\"].isnull().sum()","112a8d8e":"X.isnull().sum()","44e0d45c":"test_data.isnull().sum()","c456bf19":"X['Embarked'].describe()","96f85ec7":"common_value = 'S'\nX['Embarked'] =X['Embarked'].fillna(common_value)\nX.isnull().sum()","d0349f96":"test_data[\"Fare\"] = test_data[\"Fare\"].fillna(value=test_data[\"Fare\"].median())\ntest_data.isnull().sum()","6f7eabc2":"X_encoded = pd.get_dummies(X, drop_first=True)\ntitanic_test_encoded = pd.get_dummies(test_data, drop_first=True)","c1293bb4":"X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=101)\nX_train.head()","b25f21b3":"#Stochastic Gradient Descent (SGD):\n\nsgd = linear_model.SGDClassifier(max_iter=10, tol=None)\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nsgd.score(X_train, y_train)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\n\n#Random Forest:\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\n#Extreme Gradient Boosting:\nxgb = XGBClassifier( max_depth= 100, n_estimators= 500, learning_rate=0.29, random_state= 0, n_jobs=5)\nxgb.fit(X_train, y_train)\nY_pred = xgb.predict(X_test)\nacc_xgb = round(xgb.score(X_train, y_train) * 100, 2)\n\n#K Nearest Neighbor:\n# KNN \nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n\n#Gaussian Naive Bayes:\n\ngaussian = GaussianNB() \ngaussian.fit(X_train, y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\n\n#Perceptron:\n\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\n\n#Linear Support Vector Machine:\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\n#Decision Tree\n\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\n#Which is the best Model ?\n\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'XGB' ,\n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_xgb, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","449a8615":"# param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n# from sklearn.model_selection import GridSearchCappendcross_val_score\n# rf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n# clf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n# clf.fit(X_train, y_train)","8e4f1312":"# clf.best_params_","80ba4813":"random_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=400, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","e9fd8e5d":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","cbc4a139":"#Precision and Recall\nfrom sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","40b17496":"#ROC AUC\nfrom sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","26034ae8":"# param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35 ] ,\n#  \"max_depth\"        : [ 20, 50, 100, 150, 200],\n#  \"min_child_weight\" : [ 1, 3, 5, 7 ],\n#  \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ] }\n# from sklearn.model_selection import GridSearchCV, cross_val_score\n# xgb = XGBClassifier( max_depth= 100, n_estimators= 500, learning_rate=0.29, random_state= 0, n_jobs=5, eval_metric='logloss')\n# clf = GridSearchCV(estimator=xgb, param_grid=param_grid, n_jobs=10)\n# clf.fit(X_train, y_train)","8754242d":"# clf.best_params_","a1a1166c":"xgb = XGBClassifier(gamma = 0.0, \n                    learning_rate = 0.15,\n                    max_depth = 20,\n                    min_child_weight = 7 ,\n                    random_state=1, \n                    n_jobs=5)\n\nxgb.fit(X_train, y_train)\nY_prediction = xgb.predict(X_test)\n\nxgb.score(X_train, y_train)\nacc_xgb = round(xgb.score(X_train, y_train) * 100, 2)","b5f6e9f5":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(xgb, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","2c8c3d6c":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","78f588d4":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = xgb.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nfrom sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","cf99e216":"# dt = DecisionTreeClassifier(random_state=42)\n# # Create the parameter grid based on the results of random search \n# params = {\n#     'max_depth': [2, 3, 5, 10, 20],\n#     'min_samples_leaf': [5, 10, 20, 50, 100],\n#     'criterion': [\"gini\", \"entropy\"]\n# }\n# grid_search = GridSearchCV(estimator=dt, \n#                            param_grid=params, \n#                            cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")\n# grid_search.fit(X_train, y_train)","ebf059c9":"# grid_search.best_params_","129f5dae":"dt = DecisionTreeClassifier(random_state=42,criterion='entropy', max_depth= 3, min_samples_leaf= 20)\n\ndt.fit(X_train, y_train)\nY_prediction = dt.predict(X_test)\n\ndt.score(X_train, y_train)\nacc_dt = round(dt.score(X_train, y_train) * 100, 2)","53307785":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(dt, X_train, y_train, cv=3)\nconfusion_matrix(y_train, predictions)","876a53a1":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","ead7563f":"y_pred1 = random_forest.predict(titanic_test_encoded)\ny_pred2 = xgb.predict(titanic_test_encoded)\ny_pred3 = dt.predict(titanic_test_encoded)\ny_pred = []\n\nfor i in range(len(y_pred1)):\n    if y_pred1[i]+y_pred2[i]+y_pred3[i]>1:\n        y_pred.append(1)\n    else:\n        y_pred.append(0)","5e8f2a7f":"\ntitanic_test_original = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\noutput = pd.DataFrame({'PassengerId':  titanic_test_original.PassengerId, 'Survived': y_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","6af8583c":"# Taking the higher vote from best 3 models"}}