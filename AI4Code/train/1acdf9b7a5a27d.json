{"cell_type":{"c8224f60":"code","8eeac0ba":"code","fa4fd030":"code","a2052c9d":"code","722ee0ac":"code","96aad088":"code","3fbbdf48":"code","f06d7efa":"code","f8b66e7a":"code","bae5e97d":"code","1999293b":"code","2a0984fb":"code","e1cf4804":"code","f14164a9":"code","14ecf363":"code","a118f660":"code","d4db7167":"code","e3e87489":"code","21736c2d":"code","740c5556":"code","22273bee":"code","a935a4f5":"code","a6943b51":"code","5e0a5a7d":"code","f392f4d2":"code","b30cac5f":"markdown","68511b55":"markdown","e61a5f20":"markdown","7f459520":"markdown","ec29856d":"markdown","cde15bff":"markdown","480c7298":"markdown","95c840af":"markdown","b92a704b":"markdown","14a643fa":"markdown","eb53b084":"markdown","02e4b588":"markdown","98c156e9":"markdown","904fdfce":"markdown","c8fdbb84":"markdown","098506fe":"markdown","6c916d90":"markdown","6fbab2c0":"markdown","9b2b0bdc":"markdown","0725b06b":"markdown","95fe876d":"markdown","e8d9a160":"markdown","5bf45cbb":"markdown","f685dc70":"markdown","c55e58c1":"markdown"},"source":{"c8224f60":"# EDA and Preparing Data libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Spliting data and creating model libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.models import Sequential #initialize neural network library\nfrom keras.layers import Dense #build our layers library\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8eeac0ba":"data_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_train.head()","fa4fd030":"data_train.info()","a2052c9d":"sns.countplot(data_train[\"Survived\"])\nplt.show()","722ee0ac":"sns.countplot(data_train[\"Pclass\"])","96aad088":"data_train['Title'] = data_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ndata_train['Title'] = data_train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndata_train['Title'] = data_train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndata_train['Title'] = data_train['Title'].replace('Mlle', 'Miss')\ndata_train['Title'] = data_train['Title'].replace('Ms', 'Miss')\ndata_train['Title'] = data_train['Title'].replace('Mme', 'Mrs')\n\nplt.figure(figsize=(10,7))\nsns.countplot(data_train.Title)\nplt.title(\"data_train Passanger Name\",color = 'blue',fontsize=15)\nplt.show()","3fbbdf48":"sns.countplot(data_train[\"Sex\"])","f06d7efa":"g = sns.FacetGrid(data_train,row=\"Sex\",col=\"Pclass\")\ng.map(sns.countplot,\"Survived\")\nplt.show()","f8b66e7a":"data_train[\"Age\"].describe()","bae5e97d":"sns.distplot(data_train[\"Age\"])","1999293b":"sns.countplot(data_train[\"SibSp\"])","2a0984fb":"sns.countplot(data_train[\"Parch\"])","e1cf4804":"data_train[\"Fare\"].describe()","f14164a9":"fare = ['above100$' if i>=100 else '32between100$' if (i<100 and i>=32) else 'Free' if i==0 else 'below32$' for i in data_train[\"Fare\"]]\nplt.figure(figsize=(10,7))\nsns.countplot(fare)\nplt.title(\"data_train Passanger Fare\",color = 'blue',fontsize=15)\nplt.show()","14ecf363":"sns.countplot(data_train[\"Embarked\"])","a118f660":"data_train.head()","d4db7167":"#missing value\nprint(pd.isnull(data_train).sum())","e3e87489":"# drop name and ticket\ndata_train = data_train.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1)\n\n# Sex\ndata_train[\"Sex\"] = data_train[\"Sex\"].replace(\"male\",1)\ndata_train[\"Sex\"] = data_train[\"Sex\"].replace(\"female\",2)\n\n# Age\ndata_train[\"Age\"] = data_train[\"Age\"].replace(np.nan,data_train[\"Age\"].median())\n\n# Fare\ndata_train[\"Fare\"] = data_train[\"Fare\"].replace(np.nan,data_train[\"Fare\"].median())\n\n# Cabin\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'A', 'Cabin'] = 1\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'B', 'Cabin'] = 2\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'C', 'Cabin'] = 3\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'D', 'Cabin'] = 4\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'E', 'Cabin'] = 5\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'F', 'Cabin'] = 6\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'G', 'Cabin'] = 7\ndata_train.loc[data_train[\"Cabin\"].str[0] == 'T', 'Cabin'] = 8\ndata_train[\"Cabin\"] = data_train[\"Cabin\"].fillna(data_train[\"Cabin\"].mean())\n\n# Embarked\ndata_train[\"Embarked\"] = data_train[\"Embarked\"].replace(\"S\",1)\ndata_train[\"Embarked\"] = data_train[\"Embarked\"].replace(\"C\",2)\ndata_train[\"Embarked\"] = data_train[\"Embarked\"].replace(\"Q\",3)\ndata_train[\"Embarked\"] = data_train[\"Embarked\"].replace(np.nan,data_train[\"Embarked\"].median())\n\n# Title\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Mr\",1)\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Mrs\",2)\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Miss\",3)\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Master\",4)\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Rare\",5)\ndata_train[\"Title\"] = data_train[\"Title\"].replace(\"Royal\",6)\n\n#Family Size\ndata_train['FamilySize'] = data_train['SibSp'] + data_train['Parch']\n\ndata_train.drop([\"SibSp\",\"Parch\"],axis=1,inplace=True)","21736c2d":"data_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndata_test.head()","740c5556":"print(pd.isnull(data_test).sum())","22273bee":"#Title\ndata_test['Title'] = data_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ndata_test['Title'] = data_test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ndata_test['Title'] = data_test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ndata_test['Title'] = data_test['Title'].replace('Mlle', 'Miss')\ndata_test['Title'] = data_test['Title'].replace('Ms', 'Miss')\ndata_test['Title'] = data_test['Title'].replace('Mme', 'Mrs')\n\n# Sex\ndata_test[\"Sex\"] = data_test[\"Sex\"].replace(\"male\",1)\ndata_test[\"Sex\"] = data_test[\"Sex\"].replace(\"female\",2)\n\n# Age\ndata_test[\"Age\"] = data_test[\"Age\"].replace(np.nan,data_test[\"Age\"].median())\n\n# Fare\ndata_test[\"Fare\"] = data_test[\"Fare\"].replace(np.nan,data_test[\"Fare\"].median())\n\n# Cabin\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'A', 'Cabin'] = 1\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'B', 'Cabin'] = 2\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'C', 'Cabin'] = 3\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'D', 'Cabin'] = 4\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'E', 'Cabin'] = 5\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'F', 'Cabin'] = 6\ndata_test.loc[data_test[\"Cabin\"].str[0] == 'G', 'Cabin'] = 7\ndata_test.loc[data_test[\"Cabin\"].astype(str).str[0] == 'T', 'Cabin'] = 8\ndata_test[\"Cabin\"] = data_test[\"Cabin\"].fillna(int(data_test[\"Cabin\"].mean()))\n\n# Embarked\ndata_test[\"Embarked\"] = data_test[\"Embarked\"].replace(\"S\",1)\ndata_test[\"Embarked\"] = data_test[\"Embarked\"].replace(\"C\",2)\ndata_test[\"Embarked\"] = data_test[\"Embarked\"].replace(\"Q\",3)\n\n# Title\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Mr\",1)\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Mrs\",2)\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Miss\",3)\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Master\",4)\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Rare\",5)\ndata_test[\"Title\"] = data_test[\"Title\"].replace(\"Royal\",6)\n\n#Family Size\ndata_test['FamilySize'] = data_test['SibSp'] + data_test['Parch']\n\n# drop passenger id, name,ticket, sibsp and parch\ndata_test_x = data_test.drop([\"PassengerId\",\"Name\",\"Ticket\",\"SibSp\",\"Parch\"],axis=1)","a935a4f5":"X = data_train.drop([\"Survived\"],axis=1)\nY = data_train[\"Survived\"]\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)\nprint(\"x_train shape: \",x_train.shape)\nprint(\"y_train shape: \",y_train.shape)\nprint(\"x_test shape: \",x_test.shape)\nprint(\"y_test shape: \",y_test.shape)","a6943b51":"classifier = Sequential() # initialize neural network\nclassifier.add(Dense(units = 128, activation = 'relu', input_dim = X.shape[1]))\nclassifier.add(Dense(units = 32, activation = 'relu'))\nclassifier.add(Dense(units = 16, activation = 'relu'))\nclassifier.add(Dense(units = 8, activation = 'relu'))\nclassifier.add(Dense(units = 4, activation = 'relu'))\nclassifier.add(Dense(units = 1, activation = 'sigmoid')) #output layer\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nmodel = classifier.fit(x_train,y_train,epochs=600)\nmean = np.mean(model.history['accuracy'])\nprint(\"Accuracy mean: \"+ str(mean))","5e0a5a7d":"y_predict = classifier.predict(x_test)\ncm = confusion_matrix(y_test,np.argmax(y_predict, axis=1))\n\nf, ax = plt.subplots(figsize=(5, 5))\nsns.heatmap(cm, annot=True, fmt=\"d\", linewidths=.5, ax=ax)","f392f4d2":"ids = data_test['PassengerId']\npredict = classifier.predict(data_test_x)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': np.argmax(predict,axis=1)})\noutput.to_csv('submission.csv', index=False)","b30cac5f":"# Survived Predict with Artifical Neural Networks\n\nHi everybody! This kernel I'm gonna analyze Titanic Disaster Dataset by using ANN(Artifical Neural Network) I'm gonna use the Python language. I hope it's benefit for you. If this kernel is useful for you, please don't forget upvote it. I will be waiting your comment this kernel. Your comment is important in order to improving myself. So Let's go! :)\n\n<img src=\"https:\/\/i.milliyet.com.tr\/MolatikDetayBig\/2020\/04\/14\/fft371_mf33115214.Jpeg\"\/>\n\n\n## CONTENTS\n\n[1. Libraries](#1) <br\/>\n[2. Exploratory Data Analysis](#2) <br\/>\n[3. Preparing Data](#3) <br\/>\n[4. Train and Test Split](#4) <br\/>\n[5. Create Artifical Neural Network Model](#5) <br\/>\n[6. Model Evaluation using Confusion Matrix](#6) <br\/>\n[7. Conclusion](#7)","68511b55":"<a id=\"1\"><\/a>\n## Libraries","e61a5f20":"<a id=\"4\"><\/a>\n## Train and Test Split\n\nOur train data splitted as train and test data in order to educate correctly. Train data creates %80 percent of main train data and test data creates %20 percent of main train data. In the below, you see number of passenger train and test datas. You can set percentage datas according to yourself model.","7f459520":"#### Age","ec29856d":"<a id=\"2\"><\/a>\n## Exploratory Data Analysis\n\n\nIn this section, We will analyze feature of disaster data. We will visualize our datas and will realize some information extractions. ","cde15bff":"Mean train fare is 32$. In the graph above we see most passanger who buy ticket below 32$. Probably this passangers can be 3th. class. ","480c7298":"### Test Dataset","95c840af":"<a id=\"2.2.2\"><\/a>\n#### Pclass\n\nWaoow, There are the most third class passenger. ","b92a704b":"### Train Dataset","14a643fa":"#### Sex","eb53b084":"<a id=\"2.1.1\"><\/a>\n#### Survived\n\nWe can observe number of death more than number of survive. Well Is number of death\/survived related to passenger class? ","02e4b588":"#### Fare","98c156e9":"In the graphs above, we see survived number according to sex and passenger class. We can observe death man number more than death woman. Also, passanger class is effective in the death number. You know the disaster. First class passengers are saved, Third class passengers are sunk into water. :(","904fdfce":"In this part, We gonna prepare train and test dataset.","c8fdbb84":"#### SibSp","098506fe":"<a id=\"2.2.3\"><\/a>\n#### Title \/ Name\n\n","6c916d90":"<a id=\"6\"><\/a>\n## Model Evaluation using Confusion Matrix","6fbab2c0":"The section code in above, You can find some information. There are 12 feature in the disaster data. It has information about 891 people. Five feature are integer type and other five feature are object type. Lastly, two feature are float type. Some features contain missing value.  ","9b2b0bdc":"#### Parch","0725b06b":"<a id=\"3\"><\/a>\n## Preparing Data","95fe876d":"You can find the libraries which I use. :)","e8d9a160":"#### Embarked","5bf45cbb":"As we see SibSp and Parch features. The number of alone passangers are quite more. Parch and SibSp represent parent, child(Parch) and sibling,spouse(SibSp). ","f685dc70":"## Conclusion\n\nArtifical Neural Netwok succeed quite prediction survived.    ","c55e58c1":"<a id=\"5\"><\/a>\n## Create Artifical Neural Network Model\n\nI used to artifical neural network model. I wrote about deep learning beginner tutorial on Kaggle. You can reach it when you click [this](https:\/\/www.kaggle.com\/ecemboluk\/deep-learning-tutorial-on-sign-language-digits). I used to 6 layer together input and output layer. My model has 4 hidden layer. My hidden layers contain 60 units in total. As activation function I prefered relu. Because it is more faster than other activation function. I prefer adam algorithm as optimizer algorithm. Epochs, layer number, unit number are hyperparameters. Nobody don't know which values is true for their model. You can find this by trying. :)"}}