{"cell_type":{"48dbfa8e":"code","2158fdea":"code","63349c55":"code","72e29017":"code","84ecd564":"code","6de47690":"code","9a2354af":"code","c6c7eea0":"code","1185c428":"code","f18b1a25":"code","93f19968":"code","a465d8a2":"code","c45a4426":"code","ca874f06":"code","8b9bc82c":"code","7b488308":"code","a2a3dc9a":"markdown","c03838d4":"markdown","6539134a":"markdown","7e060f8f":"markdown","9339c4a8":"markdown","ff6aa182":"markdown","630153dd":"markdown","4e324d65":"markdown","516b9e6a":"markdown","db0e4ba4":"markdown","913a1934":"markdown","16b2ef64":"markdown","52dc0390":"markdown"},"source":{"48dbfa8e":"import os\n\nimport numpy as np\nimport pandas as pd\n\ndata = pd.read_csv('..\/input\/StudentsPerformance.csv')\n\n# Convert to categorical!\nfor i in range(data.shape[1]):\n    if i not in [5,6,7]:\n        data.iloc[:, i] = data.iloc[:,i].astype(\"category\")\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\nfor i in range(data.shape[1]):\n    if i not in [1,2,5,6,7]:\n        data.iloc[:, i] = label_encoder.fit_transform(data.iloc[:,i])\n\n\n\ndata = pd.get_dummies(data)\ndata.head()","2158fdea":"\nfrom sklearn.model_selection import train_test_split \n\ntrain, test = train_test_split(data, random_state = 123)\n\nprint(train.shape)\nprint(test.shape)\n\ncol_names = list(data.columns)","63349c55":"math_pred_feats = [x for i, x in enumerate(col_names) if i in list(range(0,data.shape[1])) and i not in [3]]\nmath_pred_label = col_names[3]\ntrain_math = train[math_pred_feats]\ny_train_math = train[math_pred_label]\ntest_math = test[math_pred_feats]\ny_test_math = test[math_pred_label]\n\n\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\n\n\nxgb_model = XGBRegressor(max_depth = 5,\n                n_estimators=500,\n                n_jobs=4,\n                subsample=1.0,\n                colsample_bytree=0.7,\n                random_state=1302)\nxgb_params = xgb_model.get_xgb_params()\n\nxgb_model.fit(train_math, y_train_math, verbose = True)\n\ntrain_math.head()\n","72e29017":"test_math.head()","84ecd564":"preds = xgb_model.predict(test_math)\nfrom sklearn.metrics import mean_squared_error\n\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test_math,preds)))","6de47690":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform, randint\n\n\nparam_dist = {\n    'colsample_bytree':uniform(0.1,0.9),\n    'gamma':reciprocal(1e-5,1),\n    'min_child_weight':[1,3],\n    'learning_rate':reciprocal(1e-4,1),\n    'max_depth':randint(2,6),\n    'n_estimators':randint(100,1000),\n    'reg_alpha':[1e-5, 0.1],\n    'reg_lambda':[1e-5, 0.1],\n    'subsample':[0.8]\n}\n\nrand_search = RandomizedSearchCV(estimator = xgb_model, param_distributions = param_dist, n_iter = 3, n_jobs=3, iid=False,verbose=True, scoring = 'neg_mean_squared_error', random_state = 123)\nprint(\"Fitting model...\")\nrand_search.fit(train_math, y_train_math)\nprint(\"Model fitted\")\nprint(\"Best score: \")\nprint(rand_search.best_score_)\nprint(\"Best model: \")\nprint(rand_search.best_params_)","9a2354af":"best_model = XGBRegressor(**rand_search.best_params_)\nbest_model.fit(train_math, y_train_math)\n\ny_preds = best_model.predict(test_math)\nprint(\"RMSE: \",np.sqrt(mean_squared_error(y_test_math,y_preds)))","c6c7eea0":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n    \"max_depth\":randint(3,5),\n    \"max_features\":randint(3,5),\n    \"bootstrap\":[True,False],\n    \"min_samples_split\":randint(2,7),\n    'n_estimators':randint(10,500)\n}\n\nrand_search = RandomizedSearchCV(rf_model, param_distributions = param_dist, n_jobs=5, cv = 5, verbose = True, n_iter=90, random_state = 123)\nrand_search.fit(train_math, y_train_math)\n","1185c428":"print(rand_search.best_score_)\nprint(rand_search.best_params_)","f18b1a25":"best_rf_model = RandomForestRegressor(**rand_search.best_params_)\nbest_rf_model.fit(train_math,y_train_math)\n\nrf_preds = best_rf_model.predict(test_math)\nprint(\"MSE:\", mean_squared_error(y_test_math,y_preds))","93f19968":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nparam_dist = {'criterion':['mse'], \n              'max_depth':[1], \n              'max_features':[None],\n           'max_leaf_nodes':[None], \n              'min_impurity_decrease':uniform(0,1),\n           'min_impurity_split':[None], \n              'min_samples_leaf':uniform(1e-9,(0.5-1e-9)),\n           'min_samples_split':randint(2,5), \n              'min_weight_fraction_leaf':reciprocal(1e-7,0.5),\n           'presort':[False], \n              'random_state':[123], \n              'splitter':['best']\n             }\n\nrand_search = RandomizedSearchCV(DecisionTreeRegressor(), param_distributions = param_dist, n_jobs=5, cv = 5, verbose = True, n_iter=90)\nrand_search.fit(train_math, y_train_math)\n\nadaboost_model = AdaBoostRegressor(DecisionTreeRegressor(**rand_search.best_params_), n_estimators=50).fit(train_math, y_train_math)\n\nprint(adaboost_model)\ny_preds = adaboost_model.predict(test_math)\nprint(\"MSE:\", mean_squared_error(y_test_math, y_preds))\nprint(\"RMSE:\", np.sqrt(mean_squared_error(y_test_math, y_preds)))","a465d8a2":"data.head()","c45a4426":"from sklearn.preprocessing import scale\nmath_score_mean = np.mean(data['math score'])\nmath_score_sd = np.std(data['math score'])\ndata.iloc[:,0:3] = data.iloc[:,0:3].replace({1:1, 0:-1})\ndata.iloc[:,3:6] = data.iloc[:,3:6].apply(lambda x: (x - np.mean(x)) \/ np.std(x), axis=0)\ndata.iloc[:,6:17] = data.iloc[:,6:17].replace({1:1, 0:-1})\n\n\ntrain, test = train_test_split(data, random_state = 123)\ndata.head()\n\ntrain_math_gp = train.drop(['math score','reading score','writing score'], axis = 1)\ny_train_math_gp = train['math score']\n\ntest_math_gp = test.drop(['math score','reading score', 'writing score'], axis = 1)\ny_test_math_gp = test['math score']","ca874f06":"print(test_math_gp.shape)\ntest_math_gp.head()","8b9bc82c":"print(train_math_gp.shape)\ntrain_math_gp.head()","7b488308":"from sklearn.gaussian_process import GaussianProcessRegressor\n\ngp_model = GaussianProcessRegressor(n_restarts_optimizer = 50)\ngp_model.fit(train_math_gp, y_train_math_gp)\ny_preds = gp_model.predict(test_math_gp)\nprint(\"RMSE: \", np.sqrt(mean_squared_error(y_test_math_gp, y_preds)))\nprint(\"MSE: \", mean_squared_error(y_test_math_gp, y_preds))\n\nfrom sklearn.model_selection import cross_val_score\nprint(cross_val_score(gp_model,cv=5,X = data.drop(['math score','reading score','writing score'], axis = 1), y = data['math score'], scoring = 'neg_mean_squared_error'))\n\n# Rescale it. \n\nMSE = mean_squared_error(y_test_math_gp, y_preds)*math_score_sd + math_score_mean\n\nprint(\"MSE\", MSE)","a2a3dc9a":"Okay, this was not that nice to be honest - not a particularly good score. It seems like XGBoost was the best in this case, achieving quite a low MSE.","c03838d4":"So this is the data. What do we do with it now? We could try a logistic regression analysis, since we have many classes. The classes are mostly binary or tertiary, apart from parental level of education. Random Forest and XGBoost are also great options, so I will implement these with cross-validation. For each score, I will create one of each model mentioned above and compute their MSE scores. But first, we divide into training and test data. ","6539134a":"# Predicting students' math performances through the data set\n\nIn this kernel, I am just focusing on predicting the math scores. I reach quite a low RMSE with XGBoost (~5 ish) which is not too shabby. ","7e060f8f":"## Predicting Math score\n\nFirst, we predict the math score. ","9339c4a8":"Not very good either. Better can be achieved. \n\n## Adaboost with decision stumps\n\nLet's try decision stumps, and see if they work well. ","ff6aa182":"## Gaussian process regression\n\nHow about Gaussian Process regression? We have very few dimensions, which definitely makes it suitable. We just want to scale the data of course, to ensure a mean of 0 on all variables which is needed for Gaussian Processes. Of course, it probably won't be a mean of 0 as it is unbalanced between -1 and 1, but let's try. ","630153dd":"Nice. Now, let's try randomforest. For speed, let's just do a quick Randomized search. ","4e324d65":"First, we load in the data and inspect it. ","516b9e6a":"Now use the best model to fit, and check how much the MSE has been reduced. ","db0e4ba4":"Quite a nice RMSE. Let's see if we can reduce this by performing a randomized search. Randomized searches are quicker and thus more convenient, and usually performs more or less as good as exhaustive grid searches. ","913a1934":"Now, we have a model. Now, we use that to predict. ","16b2ef64":"Okay, these decision stumps did not work that well. Let's try something else.  ","52dc0390":"Now use the best model!"}}