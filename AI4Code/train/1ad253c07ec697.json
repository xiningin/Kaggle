{"cell_type":{"d23e2253":"code","9727b6ee":"code","4d03c172":"code","03e23ef8":"code","5d34b99d":"code","448606bd":"code","b0793fd7":"markdown","95045ae4":"markdown","47ba452d":"markdown","c665d030":"markdown","f86e43d4":"markdown","fb129901":"markdown","f4f4e419":"markdown"},"source":{"d23e2253":"import pandas as pd\nimport numpy  as np\nimport matplotlib.pyplot as plt\nfrom termcolor import colored\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Make the dataset:\n# Create a noisy parabola\nx_parabola = 50 * np.random.default_rng(100).random((50,))\ny_parabola = ((x_parabola-15)**2) + (np.random.default_rng(30).random((50,))-0.5)*100\n# add a few outliers\nx_outliers = 50 * np.random.default_rng(80).random((10,))\ny_outliers = ((x_outliers-15)**2) + (np.random.default_rng(500).random((10,))-0.5)*750\n# concatenate the two groups of points together\nx_points   = np.concatenate((x_parabola, x_outliers), axis=0)\ny_points   = np.concatenate((y_parabola, y_outliers), axis=0)\n\n# Now the plots and fits\n###########################################################\n# The 'underfitting' plot\n###########################################################\nfig = plt.figure(figsize=(7, 8))\nax = fig.add_subplot(3, 1, 1)\nfig_1 = plt.scatter(x=x_points, y=y_points)\nplt.axis('off')\nx = np.linspace(0,50,400)\n#ax.set_title (\"underfitting\", fontsize=18)\nax.text(10,450,'underfitting', fontsize=18)\nfit = (np.polyfit(x_points,y_points, 1 ))\nm = fit[0]\nc = fit[1]\nunderfit = (m*x + c)\nfig_1 = plt.plot(x, underfit,color='orange',linewidth=3 )\nax.set(xlim=(0, 45), ylim=(-200, 850))\n\n###########################################################\n# The 'good' plot\n###########################################################\nax = fig.add_subplot(3, 1, 2)\n#ax.set_title (\"a good fit\", fontsize=18)\nax.text(15,350,'a good fit', fontsize=18)\nfit = (np.polyfit(x_points,y_points, 2 ))\na = fit[0]\nc = fit[2]\nm = fit[1]\ngoodfit = (a*x**2 + m*x + c)\nfig_1 = plt.plot(x,goodfit,color='orange',linewidth=3)\nfig_1 = plt.scatter(x=x_points, y=y_points)\nax.set(xlim=(0, 45), ylim=(-200, 850))\nplt.axis('off')\n\n###########################################################\n# The 'overfitting' plot\n###########################################################\nax = fig.add_subplot(3, 1, 3)\n# overfit\n#ax.set_title (\"overfitting\", fontsize=18)\nax.text(15,350,'overfitting', fontsize=18)\nfit = (np.polyfit(x_points,y_points, 50 ))\noverfit = np.poly1d(fit)\nfig_1 = plt.plot(x,overfit(x),color='orange',linewidth=3)\nfig_1 = plt.scatter(x=x_points, y=y_points)\nax.set(xlim=(0, 45), ylim=(-200, 850))\nplt.axis('off')\nplt.show()","9727b6ee":"# Make the dataset:\nn_points = 75\nmu, sigma = 0, 0.4\n# The 'zeros' class (in blue)\nzeros = np.zeros((n_points), dtype=int)\nnp.random.seed(1)\nx_zeros = 0.7 + (np.random.normal(mu, sigma, n_points))*0.6\nnp.random.seed(220)\ny_zeros = 0.3 + (np.random.normal(mu, sigma, n_points))*0.6\n# The 'ones' class (in orange)\nones = np.ones((n_points), dtype=int)\nnp.random.seed(500)\nx_ones = 0.3 + (np.random.normal(mu, sigma, n_points))*0.6\nnp.random.seed(5000)\ny_ones = 0.7 + (np.random.normal(mu, sigma, n_points))*0.6\n# Make a dataset dataframe\ndf_zeros = pd.DataFrame({'x': x_zeros,'y': y_zeros,'class' : zeros})\ndf_ones = pd.DataFrame({'x': x_ones,'y': y_ones,'class' : ones})\ndf = pd.concat([df_zeros,df_ones],ignore_index=True,axis=0)\n# create the training data\nX_train = df[['x','y']]\ny_train = df[['class']]\n\n# now for the classification:\nfrom sklearn.tree import DecisionTreeClassifier\n# underfit\nunderfit = DecisionTreeClassifier(max_depth=1,random_state=4)\nunderfit.fit(X_train, y_train)\n# a good fit\ngoodfit = DecisionTreeClassifier(max_depth=2,random_state=4)\ngoodfit.fit(X_train, y_train)\n# overfit\noverfit = DecisionTreeClassifier(max_depth=5,random_state=4)\noverfit.fit(X_train, y_train)\n\n# produce a map of the predictions for a grid of points\nxx, yy = np.meshgrid(np.arange(-0.3, 1.3, 0.001),np.arange(-0.3, 1.3, 0.001))\nZ_underfit = underfit.predict(np.array([xx.ravel(), yy.ravel()]).T).reshape(xx.shape)\nZ_goodfit  = goodfit.predict(np.array([xx.ravel(), yy.ravel()]).T).reshape(xx.shape)\nZ_overfit  = overfit.predict(np.array([xx.ravel(), yy.ravel()]).T).reshape(xx.shape)\n\n# and now the plots:\nfig = plt.figure(figsize=(15, 4))\n###########################################################\n# The 'underfitting' plot\n###########################################################\nax = fig.add_subplot(1, 3, 1)\nax.text(-0.25,-0.2,'underfitting', fontsize=18)\nplt.scatter(x=x_zeros, y=y_zeros)\nplt.scatter(x=x_ones, y=y_ones)\n# we shall use the 'plasma' cmap as the two extremes of plasma are blue and orange\nplt.contourf(xx, yy, Z_underfit, alpha=0.4, cmap='plasma')\nplt.axis('off')\n\n###########################################################\n# The 'good' plot\n###########################################################\nax = fig.add_subplot(1, 3, 2)\nax.text(-0.25,-0.2,'a good fit', fontsize=18)\nplt.scatter(x=x_zeros, y=y_zeros)\nplt.scatter(x=x_ones, y=y_ones)\nplt.contourf(xx, yy, Z_goodfit, alpha=0.4, cmap='plasma')\nplt.axis('off')\n\n###########################################################\n# The 'overfitting' plot\n###########################################################\nax = fig.add_subplot(1, 3, 3)\nax.text(-0.25,-0.2,'overfitting', fontsize=18)\nplt.scatter(x=x_zeros, y=y_zeros)\nplt.scatter(x=x_ones, y=y_ones)\nplt.contourf(xx, yy, Z_overfit, alpha=0.4, cmap='plasma')\nplt.axis('off')\n\nplt.show();","4d03c172":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data  = pd.read_csv('..\/input\/titanic\/test.csv')\nsolution   = pd.read_csv('..\/input\/submission-solution\/submission_solution.csv')\n\n#===========================================================================\n# select some features\n#===========================================================================\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\n#===========================================================================\n# for the features that are categorical we use pd.get_dummies:\n# \"Convert categorical variable into dummy\/indicator variables.\"\n#===========================================================================\nX_train       = pd.get_dummies(train_data[features])\ny_train       = train_data[\"Survived\"]\nfinal_X_test  = pd.get_dummies(test_data[features])\n\n#===========================================================================\n# perform the classification and the fit\n#===========================================================================\nclassifier = DecisionTreeClassifier(random_state=4)\nclassifier.fit(X_train, y_train)\n\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n\nK_splits = 11\n#===========================================================================\n# calculate the scores\n#===========================================================================\nprint(colored(\"The mean accuracy score of the train data is %.5f\" % classifier.score(X_train, y_train), color='blue'))\nCV_scores = cross_val_score(classifier, X_train, y_train, cv=K_splits)\nprint(\"The individual cross-validation scores are: \\n\",CV_scores)\nprint(\"The minimum cross-validation score is %.3f\" % min(CV_scores))\nprint(\"The maximum cross-validation score is %.3f\" % max(CV_scores))\nprint(colored(\"The mean  cross-validation   score is %.5f \u00b1 %0.2f\" % (CV_scores.mean(), CV_scores.std() * 2), color='yellow'))\nprint(colored(\"The test (i.e. leaderboard)  score is %.5f\" % accuracy_score(solution['Survived'],predictions), color='red'))","03e23ef8":"from yellowbrick.model_selection import CVScores\nfrom sklearn.model_selection import StratifiedKFold\n# Create a cross-validation strategy\ncv = StratifiedKFold(n_splits=K_splits)\nvisualizer = CVScores(classifier, cv=cv, scoring='f1_weighted',size=(1200, 400))\nvisualizer.fit(X_train, y_train)\nvisualizer.show();","5d34b99d":"classifier = DecisionTreeClassifier(max_depth=1,max_features=1,random_state=4)","448606bd":"classifier.fit(X_train, y_train)\n#===========================================================================\n# use the model to predict 'Survived' for the test data\n#===========================================================================\npredictions = classifier.predict(final_X_test)\n#===========================================================================\n# calculate the scores\n#===========================================================================\nprint(colored(\"The mean accuracy score of the train data is %.5f\" % classifier.score(X_train, y_train), color='blue'))\nCV_scores = cross_val_score(classifier, X_train, y_train, cv=K_splits)\nprint(\"The individual cross-validation scores are: \\n\",CV_scores)\nprint(\"The minimum cross-validation score is %.3f\" % min(CV_scores))\nprint(\"The maximum cross-validation score is %.3f\" % max(CV_scores))\nprint(colored(\"The mean  cross-validation   score is %.5f \u00b1 %0.2f\" % (CV_scores.mean(), CV_scores.std() * 2), color='yellow'))\nprint(colored(\"The test (i.e. leaderboard)  score is %.5f\" % accuracy_score(solution['Survived'],predictions), color='green'))\ncv = StratifiedKFold(n_splits=K_splits)\nvisualizer = CVScores(classifier, cv=cv, scoring='f1_weighted',size=(1200, 400))\nvisualizer.fit(X_train, y_train)\nvisualizer.show();","b0793fd7":"We can see that the straight line (a *low [variance](https:\/\/en.wikipedia.org\/wiki\/Variance)* model) fails to capture the underlying parabolic curve in the data, this is underfitting. \nAt the other extreme the high degree polynomial (a *low [bias](https:\/\/en.wikipedia.org\/wiki\/Bias_of_an_estimator)* model) captures too much of the noise at the same time as the underlying parabola, and is overfitting. Although it is following the data points provided (i.e. the *training* data), this curve is not transferable to new data (i.e. the *test* data).\n\nWe can produce an analogous example for the classification of, say, orange dots and blue dots. To do this we shall use the [decision tree classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) (which incidentally we shall also be using below for the Titanic data) and plot the resulting [decision boundary](https:\/\/en.wikipedia.org\/wiki\/Decision_boundary):","95045ae4":"We now find that the $CV( \\mathrm{score}) < LB(\\mathrm{score})$.\n\nThat said, it is more usual to find that $CV( \\mathrm{score}) \\approx LB(\\mathrm{score})$ up to the point of overfitting. Also note that the leaderboard score is lower when underfitting (`0.66`) than it was in the overfitting case above (`0.76`). The ideal situation is to adjust the parameters of the estimator so as to find the maximum leaderboard (test) score right up to the point where the two values start to diverge. In this small Titanic example it is difficult to continuously adjust the decision tree estimator to provide a demonstration, so here is a [validation curve](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html) from a different example, however the concept is the same:\n\n![image.png](attachment:image.png)\n\n# Conclusions:\n* For the small Titanic dataset even the humble decision tree classifier with the scikit-learn default settings slightly overfits the data.\n* Using any of the more advanced estimators, such as random forest, XGBoost *etc.*, is almost certainly overkill for the Titanic data.\n\n### Final remarks\nIf basically all estimators overfit the Titanic data, doesn't that imply that almost everybody is overfitting?\n* *Yes.* \n\n...and isn't that a bad thing?\n* *In general Yes*. However in the case of the Titanic nobody really cares because **there is no more data available to use**. Our Titanic models are never used in production so it is of no consequence it they overfit. The only thing of interest is the leaderboard score. Indeed, one of the [best scoring Titanic Notebooks](https:\/\/www.kaggle.com\/cdeotte\/titanic-wcg-xgboost-0-84688) is that of Chris Deotte with, up until July 2020,  a score of `0.85167` on 50% of the test data. However, when the competition was modified to being evaluated with [100% of the test data](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/163366) the aforementioned Notebook score dropped to `0.82535`, so even the #1 kaggle Grandmaster (knowingly, an important caveat) overfitted the Titanic.\n\n...also:\n\n* K-fold cross-validation **does not prevent overfitting**: all it is doing is trying it's very best to provide a more robust accuracy score, but it does not stop an overly flexible model from being an overly flexible model.\n\n* An additional mechanism for overfitting is what is known as [target leakage](https:\/\/en.wikipedia.org\/wiki\/Leakage_(machine_learning%29); on kaggle the dataset used in the public leaderboard calculation is *always* the same one. If one becomes guided by the public leaderboard score each time one makes a submission, and iteratively goes back to adjust the model parameters\/hyperparameters *etc*. to improve ones leaderboard score, then in some indirect way information about the leaderboard data actually becomes incorporated back into ones model. Doing this is tantamount to using the public leaderboard data as our test dataset.\n\n* We have seen above that there is an *irreducible* error term for any realistic dataset; for the Titanic it is not possible to obtain a score of 100% with a model, and indeed I strongly suspect that it is not even possible to ever obtain a score of more than 85% with the Titanic dataset. (Note that any 100% score on the Titanic leaderboard was obtained by submitting the publicly available solution file, and was **not** the result of submitting a model). \n\n### *'Trust your CV'* and the shakeup\nA mantra one sees over and over on kaggle competitions regarding overfitting is ***'Trust your CV'***. We have seen that overfitting the Titanic has little consequence. However overfitting the Public leaderboard score in a kaggle competition can have disastrous consequences when when at the end of the competition your work is compared to the, up till then, secret *Private leaderboard*. An underfitted model will under-perform and is unlikely to end up in the medals, but also a model that is overfitted so as to achieve as high a score as possible on the Public leaderboard will plummet down the rankings when it is finally compared to the Private leaderboard data. This is known as the *shakeup*. The solution to this is to tune your model, features, and parameters so that your resulting CV score closely matches the public leaderboard score, and if the data used to calculate the Public leaderboard is representative of the data used to calculate the Private leaderboard (which is *not* always the case!), then there should be no unwelcome surprises at the end of the competition. One can check whether the training data is indeed a good representative of the test data using a technique known as \"adversarial validation\". For more details see my notebook [\"*What is Adversarial Validation?*\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/what-is-adversarial-validation).\n\nIf you are curious you can take a look at [\"Shakeup scatterplots: Boxes, strings and things...\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/shakeup-scatterplots-boxes-strings-and-things) to see some of the things that can, and do, happen during shakeups, and one can also use the [shakeup interactive scatterplot maker](https:\/\/www.kaggle.com\/carlmcbrideellis\/shakeup-interactive-scatterplot-maker) to examine the shakeup of any particular past kaggle competition of interest.\n\n# Related material\n* [\"Underfitting and Overfitting\"](https:\/\/www.kaggle.com\/dansbecker\/underfitting-and-overfitting) by [Dan Becker](https:\/\/www.kaggle.com\/dansbecker) in the kaggle course [\"Intro to Machine Learning\"](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)\n* [\"Overfitting and Underfitting\"](https:\/\/www.kaggle.com\/ryanholbrook\/overfitting-and-underfitting) by [Ryan Holbrook](https:\/\/www.kaggle.com\/ryanholbrook) in the kaggle course [\"Intro to Deep Learning\"](https:\/\/www.kaggle.com\/learn\/intro-to-deep-learning)\n* [\"Overfitting\"](https:\/\/en.wikipedia.org\/wiki\/Overfitting) on Wikipedia\n* [\"Cross-validation: evaluating estimator performance\"](https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html) on Scikit-learn\n* [Trevor Hastie, Robert Tibshirani  and Jerome Friedman \"The Elements of Statistical Learning\" Springer (2nd edition)](https:\/\/web.stanford.edu\/~hastie\/ElemStatLearn\/) Chapter 7: \"*Model Assessment and Selection*\"\n* [Trevor Hastie and Robert Tibshirani \"Cross-validation\", (YouTube video)](https:\/\/youtu.be\/ngrOYWgJjb4)\n* [Trevor Hastie and Robert Tibshirani \"K-fold Cross-validation\", (YouTube video)](https:\/\/youtu.be\/rSGzUy13F_0)\n* [Gavin C. Cawley and Nicola L. C. Talbot \"*On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation*\", Journal of Machine Learning Research, volume **11** pp. 2079-2107 (2010)](https:\/\/jmlr.csail.mit.edu\/papers\/volume11\/cawley10a\/cawley10a.pdf)\n* [Sebastian Raschka \"*Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning*\", arXiv:1811.12808](https:\/\/arxiv.org\/pdf\/1811.12808.pdf)\n* [Belkin *et al.* \"*Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off*\", PNAS vol. 116 15849-15854 (2019)](https:\/\/www.pnas.org\/content\/116\/32\/15849)","47ba452d":"let us see the results:","c665d030":"We can also display the cross-validated scores as a bar chart (one bar for each fold) using the [Yellowbrick `CVScores` visualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/model_selection\/cross_validation.html)","f86e43d4":"Indeed we can see that the 'overfitting' decision surface on the right is almost picking out individual data points, and is is easy to imagine how such a surface does not adapt well to the incorporation and the subsequent correct classification of new data.\n\nAs the great mathematician [John von Neumann](https:\/\/en.wikipedia.org\/wiki\/John_von_Neumann) famously [once said](https:\/\/www.nature.com\/articles\/427297a):\n\n> \"*...with four parameters I can fit an elephant, and with five I can make him wiggle his trunk.*\"\n\nAn example of underfitting the Titanic would be the [very simple model](https:\/\/www.kaggle.com\/carlmcbrideellis\/kiss-small-and-simple-titanic-models) that nobody survived the tragedy. Such a model actually has an accuracy of `0.62201`. It is resilient to new data, however it fails to capture much of what is going on, and thus under-performs. At the other extreme would be, say,  a decision tree with so many splits that it has as many leaves as there are passengers.\n\nThe total error in our model can be represented by the famous equation (see: [\"The Elements of Statistical Learning\"](https:\/\/web.stanford.edu\/~hastie\/ElemStatLearn\/) by Trevor Hastie, Robert Tibshirani  and Jerome Friedman (2nd Ed.) , \u00a7 7.3 Eq. 7.9) for the expected prediction error term ($\\epsilon$):\n\n> **prediction error = bias$^2$ + variance + irreducible error**\n\nleading to the all important concept of the [bias\u2013variance tradeoff](https:\/\/en.wikipedia.org\/wiki\/Bias%E2%80%93variance_tradeoff). The *irreducible error* term, made up of things like noise, as the name suggests cannot be accounted for or eliminated **no matter what model we are using**. \n## Comparison of Titanic scores: The cross-validation score *Vs.* the leaderboard score (*i.e.* the test score)\n\nNow to address a question that appears every so often on the Titanic discussion forum: Why is my $LB(\\mathrm{score}) < CV( \\mathrm{score})$? \n\nLet us make a model using a simple [decision tree classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html), having the default parameters used by scikit-learn:","fb129901":"# Overfitting and underfitting the Titanic\n## Introduction\nBefore we start let us quickly look at a very graphic example of underfitting, a good fit, and overfitting.\nWe shall create an artificial dataset that consists of a noisy parabola, and to that also add some random outliers.\nWe then shall then fit the data to three models that have increasing degrees of freedom: a straight line, a parabola, and finally a high degree polynomial, each to the very same dataset:","f4f4e419":"I strongly suspect that what we are seeing is a case of overfitting the Titanic data, even with the [decision tree classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html).\nWe can also see what happens if we deliberately try to underfit by using a very small model:"}}