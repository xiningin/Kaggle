{"cell_type":{"b052919a":"code","eda5a176":"code","cc0a4d7a":"code","c6bcfc88":"code","ec42f973":"code","986940aa":"code","72f0fa58":"code","f858b091":"code","c18395f0":"code","5b8671e8":"code","c07e2bf4":"code","8bb10ad8":"code","30cc5e83":"code","d4677a22":"code","8367c73e":"code","a687bc6e":"code","65170b81":"code","a0420eb0":"code","e2d17649":"code","1aa2e0fe":"code","29a32fdd":"code","cd1d9cba":"code","d0981c60":"code","4b8a0e97":"code","8d1bc00b":"code","00cad1b5":"markdown"},"source":{"b052919a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","eda5a176":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")","cc0a4d7a":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null","c6bcfc88":"import sys\nsys.path.insert(0, \"..\/input\/transformers\/transformers-master\/\")","ec42f973":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","986940aa":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","72f0fa58":"import tensorflow as tf\nimport torch\nfrom transformers import *\nimport tqdm\ntokenizer = DistilBertTokenizer.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")\nmodel = DistilBertModel.from_pretrained(\"..\/input\/distilbertbaseuncased\/\")","f858b091":"model.cuda()","c18395f0":"question_body_ids = train[\"question_body\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nquestion_body_ids_test = test[\"question_body\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nquestion_body_vectors = []\nquestion_body_vectors_test = []\nfor question_body in tqdm.tqdm(question_body_ids):\n    input_ids = torch.Tensor(question_body).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        question_body_vectors.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        question_body_vectors.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)\n    \nfor question_body in tqdm.tqdm(question_body_ids_test):\n    input_ids = torch.Tensor(question_body).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        question_body_vectors_test.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        question_body_vectors_test.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)","5b8671e8":"question_title_ids = train[\"question_title\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nquestion_title_ids_test = test[\"question_title\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nquestion_title_vectors = []\nquestion_title_vectors_test = []\nfor question_title in tqdm.tqdm(question_title_ids):\n    input_ids = torch.Tensor(question_title).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        question_title_vectors.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        question_title_vectors.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)\n    \nfor question_title in tqdm.tqdm(question_title_ids_test):\n    input_ids = torch.Tensor(question_title).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        question_title_vectors_test.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        question_title_vectors_test.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)","c07e2bf4":"answer_ids = train[\"answer\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nanswer_ids_test = test[\"answer\"].str.slice(0, 500).str.lower().apply(tokenizer.encode)\nanswer_vectors = []\nanswer_vectors_test = []\nfor answer in tqdm.tqdm(answer_ids):\n    input_ids = torch.Tensor(answer).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        answer_vectors.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        answer_vectors.append(np.zeros(outputs[0].cpu().detach().numpy().max(axis = 1)).shape)\n    \nfor answer in tqdm.tqdm(answer_ids_test):\n    input_ids = torch.Tensor(answer).to(torch.int64).unsqueeze(0)\n    try:\n        outputs = model(input_ids.cuda())\n        answer_vectors_test.append(outputs[0].detach().cpu().numpy().max(axis = 1))\n    except:\n        answer_vectors_test.append(np.zeros(outputs[0].detach().cpu().numpy().max(axis = 1)).shape)","8bb10ad8":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 50)\nquestion_title = tfidf.fit_transform(train[\"question_title\"].values)\nquestion_title_test = tfidf.transform(test[\"question_title\"].values)\nquestion_title = tsvd.fit_transform(question_title)\nquestion_title_test = tsvd.transform(question_title_test)\n\nquestion_body = tfidf.fit_transform(train[\"question_body\"].values)\nquestion_body_test = tfidf.transform(test[\"question_body\"].values)\nquestion_body = tsvd.fit_transform(question_body)\nquestion_body_test = tsvd.transform(question_body_test)\n\nanswer = tfidf.fit_transform(train[\"answer\"].values)\nanswer_test = tfidf.transform(test[\"answer\"].values)\nanswer = tsvd.fit_transform(answer)\nanswer_test = tsvd.transform(answer_test)","30cc5e83":"# from gensim import utils\nfrom tqdm import tqdm\n# from gensim.models.keyedvectors import KeyedVectors","d4677a22":"# import gc\n# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n# def load_news(embed_dir = '..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'):\n#     embeddings_index = KeyedVectors.load_word2vec_format(embed_dir, binary=True)\n#     emb_ind = {}\n#     for i, vec in tqdm(enumerate(embeddings_index.wv.vectors)):\n#         emb_ind[embeddings_index.wv.index2word[i]] = vec\n#     del embeddings_index\n#     gc.collect()\n#     return emb_ind","8367c73e":"# vector_lookup = load_news()","a687bc6e":"# null_vector = vector_lookup[\"the\"]\n# def make_bov(sentence):\n#     sent_vec = np.zeros((300))\n#     sentence = sentence.split()\n#     sent_length = len(sentence)\n#     if sent_length == 0:\n#         sent_length = 1\n#     for word in sentence:\n#         try:\n#             sent_vec += vector_lookup[word.lower()]\n#         except:\n#             sent_vec += null_vector\n#     sent_vec \/= sent_length\n#     return sent_vec","65170b81":"# question_title_bov = np.array([make_bov(sent) for sent in train[\"question_title\"].values])\n# question_title_bov_test = np.array([make_bov(sent) for sent in test[\"question_title\"].values])\n\n# question_bov = np.array([make_bov(sent) for sent in train[\"question_body\"].values])\n# question_bov_test = np.array([make_bov(sent) for sent in test[\"question_body\"].values])\n\n# answer_bov = np.array([make_bov(sent) for sent in train[\"answer\"].values])\n# answer_bov_test = np.array([make_bov(sent) for sent in test[\"answer\"].values])","a0420eb0":"# question_title_len = np.array([len(sent.split()) + 1 for sent in train[\"question_title\"].values])[:, None]\n# question_title_len_test = np.array([len(sent.split()) + 1 for sent in test[\"question_title\"].values])[:, None]\n\n# question_len = np.array([len(sent.split()) + 1 for sent in train[\"question_body\"].values])[:, None]\n# question_len_test = np.array([len(sent.split()) + 1 for sent in test[\"question_body\"].values])[:, None]\n\n# answer_len = np.array([len(sent.split()) + 1 for sent in train[\"answer\"].values])[:, None]\n# answer_len_test = np.array([len(sent.split()) + 1 for sent in test[\"answer\"].values])[:, None]","e2d17649":"# category_means_map = train.groupby(\"category\")[target_cols].mean().T.to_dict()\n# category_te = train[\"category\"].map(category_means_map).apply(pd.Series)\n# category_te_test = test[\"category\"].map(category_means_map).apply(pd.Series)","1aa2e0fe":"# train_features = np.concatenate([question_title, question_body, answer#, category_te.values\n#                                 ], axis = 1)\n# test_features = np.concatenate([question_title_test, question_body_test, answer_test#, category_te_test.values\n#                                ], axis = 1)\n\n# train_features = np.concatenate([question_title, question_body, answer,\n#                                 question_title_bov, question_bov, answer_bov,\n#                                  question_title_len, question_len, answer_len,\n#                                  #, category_te.values\n#                                 ], axis = 1)\n# test_features = np.concatenate([question_title_test, question_body_test, answer_test,\n#                                 question_title_bov_test, question_bov_test, answer_bov_test,\n#                                 question_title_len_test, question_len_test, answer_len_test,\n#                                 #, category_te_test.values\n#                                ], axis = 1)\n\n# train_features = np.array(question_body_vectors)[:, 0, :]\n# test_features = np.array(question_body_vectors_test)[:, 0, :]\n\n\ntrain_features = np.concatenate([question_title, question_body, answer,\n                                 np.array(question_body_vectors)[:, 0, :],\n                                np.array(question_title_vectors)[:, 0, :],\n                                np.array(answer_vectors)[:, 0, :]\n                               ], axis = 1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test,\n                                np.array(question_body_vectors_test)[:, 0, :],\n                                np.array(question_title_vectors_test)[:, 0, :],\n                                np.array(answer_vectors_test)[:, 0, :]\n                               ], axis = 1)","29a32fdd":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom sklearn.model_selection import KFold\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\n\nnum_folds = 5\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nfor train_index, val_index in kf.split(train_features):\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(2048, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(1024),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 50, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation\/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n\n    test_preds += model.predict(test_features)\/num_folds\n    \nprint(fold_scores)","cd1d9cba":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","d0981c60":"sub.shape","4b8a0e97":"for col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]","8d1bc00b":"sub.to_csv(\"submission.csv\", index = False)","00cad1b5":"Thanks to @abhishek for figuring out how to use huggingface with internet off. "}}