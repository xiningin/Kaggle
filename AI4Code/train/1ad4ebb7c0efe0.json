{"cell_type":{"9e0aba66":"code","3de495e7":"code","ebb2a26e":"code","cd3bebec":"code","98982a94":"code","85897ca9":"code","e6b0f290":"code","a9f81324":"code","eab8ba54":"code","4199c693":"code","7de7415e":"code","5797572f":"code","2cbe8247":"code","0f431391":"code","5d825a57":"code","7724e0a6":"code","157b71b9":"code","b3686eae":"markdown","1a0635e3":"markdown","53eb4edf":"markdown","5818fd99":"markdown","7334a2b3":"markdown","a22d3af0":"markdown","a6ad9513":"markdown","f6409d4a":"markdown","841a327b":"markdown","afa587be":"markdown","ad283eb7":"markdown","b360f14b":"markdown","380c45b9":"markdown","5e1c3874":"markdown","2c28eda5":"markdown","fe233f94":"markdown","5a1fd4e0":"markdown"},"source":{"9e0aba66":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nimport transformers","3de495e7":"n_epochs = 100\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=100)\nlambda1 = lambda epoch: 0.95 ** epoch\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda1)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","ebb2a26e":"n_epochs = 20\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=100)\nlambda1 = lambda epoch: 0.7\nscheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lambda1)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","cd3bebec":"n_epochs = 20\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=20)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","98982a94":"n_epochs = 50\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=20)\nscheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 15, 30], gamma=0.4)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","85897ca9":"n_epochs = 10\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=10)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.4)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","e6b0f290":"n_epochs = 100\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=50)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","a9f81324":"# Pseudo implementation of the scheduler.\n\nn_epochs = 10\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=1.0)\n# if there are no improvements for the consecutive `patience` steps according to criterion `mode`,\n# reduce the learning rate by the `factor`.\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.3, patience=7)\n\nlrs = []\nfor i in range(n_epochs):\n    # ...Optimize here...\n    # optimizer.step()\n    # lrs.append(optimizer.param_groups[0][\"lr\"])\n    # ...Step here..\n    # scheduler.step()\n    pass\n\n# plt.plot(lrs)\n# plt.show()","eab8ba54":"n_epochs = 100\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=1.0)\n# From 1e-5 to 1e-3 for 5 epoch\n# From 1e-3 to 1e-5 for 5 epoch\n# Repeat\nscheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=5, mode=\"triangular\")\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","4199c693":"n_epochs = 100\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, epochs=n_epochs, steps_per_epoch=10)\n\nlrs = []\nfor i in range(1000):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","7de7415e":"n_epochs = 150\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","5797572f":"total_samples = 968\nbs = 32\nn_epochs = 50\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_constant_schedule(optimizer)\nlrs = []\nfor i in range(n_epochs):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","2cbe8247":"total_samples = 968\nbs = 32\nn_epochs = 10\n\nnum_warmup_steps = (total_samples \/\/ bs) * 2\nnum_total_steps = (total_samples \/\/ bs) * n_epochs\n\nmodel = nn.Linear(2, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_constant_schedule_with_warmup(optimizer, \n                                                         num_warmup_steps=num_warmup_steps)\nlrs = []\nfor i in range(num_total_steps):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","0f431391":"total_samples = 968\nbs = 32\nn_epochs = 10\n\nnum_warmup_steps = (total_samples \/\/ bs) * 2\nnum_total_steps = (total_samples \/\/ bs) * n_epochs\n\nmodel = nn.Linear(2, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_cosine_schedule_with_warmup(optimizer, \n                                                         num_warmup_steps=num_warmup_steps, \n                                                         num_training_steps=num_total_steps)\nlrs = []\nfor i in range(num_total_steps):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","5d825a57":"total_samples = 968\nbs = 32\nn_epochs = 100\nnum_cycles = 3\n\nnum_warmup_steps = (total_samples \/\/ bs) * 10\nnum_training_steps = (total_samples \/\/ bs) * n_epochs\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, \n                                                                            num_warmup_steps=num_warmup_steps, \n                                                                            num_training_steps=num_training_steps,\n                                                                            num_cycles=num_cycles)\nprint(f\"Num_warmup_steps: {num_warmup_steps:>5}\")\nprint(f\"num_training_steps: {num_training_steps}\")\nprint(f\"Recycles every: {int((num_training_steps - num_warmup_steps) \/ num_cycles):>7} step.\")\nlrs = []\nfor i in range(num_training_steps):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","7724e0a6":"total_samples = 968\nbs = 32\nn_epochs = 100\n\nnum_warmup_steps = (total_samples \/\/ bs) * 20\nnum_training_steps = (total_samples \/\/ bs) * n_epochs\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n                                                         num_warmup_steps=num_warmup_steps, \n                                                         num_training_steps=num_training_steps)\nprint(f\"Num_warmup_steps: {num_warmup_steps:>5}\")\nprint(f\"num_training_steps: {num_training_steps}\")\nlrs = []\nfor i in range(num_training_steps):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","157b71b9":"total_samples = 968\nbs = 32\nn_epochs = 500\npower = 3.0\n\nnum_warmup_steps = (total_samples \/\/ bs) * 50\nnum_training_steps = (total_samples \/\/ bs) * n_epochs\n\nmodel = nn.Linear(10, 5)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\nscheduler = transformers.get_polynomial_decay_schedule_with_warmup(optimizer, \n                                                                   num_warmup_steps=num_warmup_steps, \n                                                                   num_training_steps=num_training_steps,\n                                                                   power=power)\nprint(f\"Num_warmup_steps: {num_warmup_steps:>6}\")\nprint(f\"num_training_steps: {num_training_steps}\")\nlrs = []\nfor i in range(num_training_steps):\n    optimizer.step()\n    lrs.append(optimizer.param_groups[0][\"lr\"])\n    scheduler.step()\n    \nplt.plot(lrs)\nplt.show()","b3686eae":"# get_cosine_schedule_with_warmup\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_cosine_schedule_with_warmup)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup)","1a0635e3":"# CosineAnnealingWarmRestarts\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.CosineAnnealingWarmRestarts)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#CosineAnnealingWarmRestarts)","53eb4edf":"# CosineAnnealing LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.CosineAnnealingLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#CosineAnnealingLR)\n","5818fd99":"# OneCycle LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.OneCycleLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#OneCycleLR)\n","7334a2b3":"# get_constant_schedule\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_constant_schedule)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_constant_schedule_with_warmup)","a22d3af0":"# Step LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.StepLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#StepLR)\n\nFor every `step_size` decrease the learning rate according to `gamma`\n\nlr = lr * gamma","a6ad9513":"# Cyclic LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.CyclicLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#CyclicLR)\n","f6409d4a":"# MultiStep LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.MultiStepLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#MultiStepLR)\n","841a327b":"# get_polynomial_decay_schedule_with_warmup\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_polynomial_decay_schedule_with_warmup)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_polynomial_decay_schedule_with_warmup)","afa587be":"# Exponential LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.ExponentialLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#ExponentialLR)\n","ad283eb7":"# Lambda LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.LambdaLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#LambdaLR)","b360f14b":"# ReduceLROnPlateau\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.ReduceLROnPlateau)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#ReduceLROnPlateau)\n","380c45b9":"# get_cosine_with_hard_restarts_schedule_with_warmup\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_cosine_with_hard_restarts_schedule_with_warmup)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_with_hard_restarts_schedule_with_warmup)\n","5e1c3874":"# get_linear_schedule_with_warmup\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_linear_schedule_with_warmup)\n","2c28eda5":"# Pytorch\n\n#### [Under the pytorch.optim](https:\/\/pytorch.org\/docs\/stable\/optim.html)\n* [LambdaLR](#Lambda-LR)\n* [MultiplicativeLR](#Multiplicative-LR)\n* [StepLR](#Step-LR)\n* [MultiStepLR](#MultiStep-LR)\n* [ExponentialLR](#Exponential-LR)\n* [CosineAnnealingLR](#CosineAnnealing-LR)\n* [ReduceLROnPlateau](#ReduceLROnPlateau)\n* [CyclicLR](#Cyclic-LR)\n* [OneCycleLR](#OneCycle-LR)\n* [CosineAnnealingWarmRestarts](#CosineAnnealingWarmRestarts)\n\n# Transformers\n\n#### [Under the transformers optimizer schedules](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html)\n\n* [get_constant_schedule](#get_constant_schedule)\n* [get_constant_schedule_with_warmup](#get_constant_schedule_with_warmup)\n* [get_cosine_schedule_with_warmup](#get_cosine_schedule_with_warmup)\n* [get_cosine_with_hard_restarts_schedule_with_warmup](#get_cosine_with_hard_restarts_schedule_with_warmup)\n* [get_linear_schedule_with_warmup](#get_linear_schedule_with_warmup)\n* [get_polynomial_decay_schedule_with_warmup](#get_polynomial_decay_schedule_with_warmup)","fe233f94":"# get_constant_schedule_with_warmup\n\n* [Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#transformers.get_constant_schedule_with_warmup)\n* [Source](https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_constant_schedule_with_warmup)","5a1fd4e0":"# Multiplicative LR\n\n* [Documentation](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.lr_scheduler.MultiplicativeLR)\n* [Source](https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/lr_scheduler.html#MultiplicativeLR)"}}