{"cell_type":{"36fbc9dc":"code","e05fc8b9":"code","74d29edf":"code","d7aa7ff9":"code","5e658404":"code","ab513943":"code","24a7576d":"code","a432c9af":"code","745f2606":"code","bd73e9b2":"code","3040c1b2":"code","c50c105b":"code","23d13444":"code","9b9c2eec":"code","16a43492":"code","61803eb1":"code","9deb0945":"code","662c00b9":"code","3247ed08":"code","ecdbc64c":"code","702daa62":"code","e7c177a5":"code","2e5f5687":"code","cfb30566":"code","0fe3e001":"code","803ad326":"code","748b2c8c":"code","9994975e":"code","e2ac55e1":"code","7b7b1e06":"code","145f9a39":"code","c7782f73":"code","2dfe4a53":"code","f9499120":"code","5d5c7014":"code","51fb3949":"code","734b16c5":"code","32278b33":"code","c058e75d":"code","e9529558":"code","0f407518":"code","345ecd24":"code","675fe25d":"code","000dd503":"code","8272a6f2":"code","de543887":"code","49212a7d":"code","a2e661dc":"code","0bb1596c":"code","d9fe2940":"code","956459b9":"code","0aca6a0b":"code","a6e89880":"markdown","0231cb5d":"markdown","d39f5e80":"markdown","ecd922df":"markdown","40942a82":"markdown","dc27f1d6":"markdown","b53779ce":"markdown","f1a01135":"markdown","af75e736":"markdown","d6274392":"markdown","9ac565a5":"markdown","9d47d2e4":"markdown","78ebf8a1":"markdown","9570f406":"markdown","c1114d3d":"markdown","01df43f4":"markdown","b31d828a":"markdown","f085544c":"markdown","78652347":"markdown","66f5395b":"markdown","361b2a11":"markdown","a2b5e337":"markdown","924c3dd4":"markdown","01b42a67":"markdown","b7a5f362":"markdown","b60083ff":"markdown","3cd145d7":"markdown","ee43d887":"markdown","42ee3599":"markdown","af45336a":"markdown","2e5647a0":"markdown","52e74f78":"markdown","33f2423a":"markdown","a4c79042":"markdown","ef1c5eeb":"markdown","ea9f59e5":"markdown"},"source":{"36fbc9dc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport statsmodels.api as sm\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 2000)\npd.set_option('display.width', 1000)","e05fc8b9":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","74d29edf":"train.corr()['SalePrice'].sort_values(ascending=False).head(10)","d7aa7ff9":"sns.scatterplot(train['GrLivArea'],train['SalePrice'])","5e658404":"def find_outliers_tukey(x):\n    q1 = np.percentile(x, 25)\n    q3 = np.percentile(x, 75)\n    iqr = q3-q1   \n    floor = q1 - 1.5*iqr\n    ceiling = q3 + 1.5*iqr\n    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n    outlier_values = list(x[outlier_indices])\n\n    return outlier_indices\nout=find_outliers_tukey(train['TotalBsmtSF'])","ab513943":"train=train.drop(out)","24a7576d":"#take another look:\nsns.scatterplot(train['GrLivArea'],train['SalePrice'])","a432c9af":"sns.distplot((train['SalePrice']))","745f2606":"train['SalePrice']=np.log(train['SalePrice'])\nsns.distplot(train['SalePrice'])","bd73e9b2":"dataset = pd.concat(objs=[train, test], axis=0,sort=False,ignore_index=True)\n\n","3040c1b2":"dataset.isnull().sum().sort_values(ascending=False)","c50c105b":"from sklearn.impute import SimpleImputer\nimp_num=SimpleImputer(missing_values=np.nan,strategy='mean') #mean for numericals and mode for categoricals\ndataset[['MasVnrArea','LotFrontage','GarageArea']]=pd.DataFrame(imp_num.fit_transform(dataset[['MasVnrArea','LotFrontage'\n                                                                                               ,'GarageArea']]))\n","23d13444":"imp_cat=SimpleImputer(missing_values=np.nan,strategy='most_frequent')\n\ndataset[['Electrical','MasVnrType','SaleType','MSZoning','Utilities','Exterior1st','Exterior2nd','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType']]=pd.DataFrame(imp_cat.fit_transform(dataset[['Electrical','MasVnrType','SaleType','MSZoning','Utilities','Exterior1st','Exterior2nd','BsmtFullBath','BsmtHalfBath','KitchenQual','Functional','SaleType']]))","9b9c2eec":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):  \n    dataset[col] = dataset[col].fillna(0)\n    \nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    dataset[col] = dataset[col].fillna('Nothing')    ","16a43492":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    dataset[col] = dataset[col].fillna(0)\n    \nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    dataset[col] = dataset[col].fillna('Nothing')    ","61803eb1":"dataset['Alley'] = dataset['Alley'].fillna('Nothing')\ndataset['FireplaceQu'] = dataset['FireplaceQu'].fillna('Nothing')\ndataset['Fence'] = dataset['Fence'].fillna('Nothing')\ndataset['PoolQC']=dataset['PoolQC'].fillna('Nothing')\ndataset['MiscFeature']=dataset['MiscFeature'].fillna('Nothing')","9deb0945":"import pandas as pd\ndataset.isnull().sum().sort_values(ascending=False).head()","662c00b9":"dataset.dtypes","3247ed08":"dataset['MSSubClass']=dataset['MSSubClass'].astype('str')\ndataset['MoSold']=dataset['MoSold'].astype('str')\n","ecdbc64c":"#instead of 2006,2007... label them as 0,1 ... for ease in use\nfrom sklearn.preprocessing import LabelEncoder\ncat_encoder=LabelEncoder()\nprint(cat_encoder.fit_transform(dataset['YrSold'].values))\ndataset['YrSold']=cat_encoder.fit_transform(dataset['YrSold'].values)\n","702daa62":"\nexternal=['ExterQual','ExterCond','HeatingQC']\nfor e in external:\n    dataset[e]=dataset[e].map({'Ex':4,'Gd':3,'TA':2,'Fa':1,'Po':0})\n    \n    \nbasement=['BsmtQual','BsmtCond','GarageQual','GarageCond','FireplaceQu','KitchenQual']\nfor b in basement:\n    dataset[b]=dataset[b].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'Nothing':0})\n    \n\ndataset['BsmtFinType2']=dataset['BsmtFinType2'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'Nothing':0})\ndataset['BsmtFinType1']=dataset['BsmtFinType1'].map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'Nothing':0})\ndataset['BsmtExposure']=dataset['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'Nothing':0})\ndataset['LandSlope']=dataset['LandSlope'].map({'Gtl':2,'Mod':1,'Sev':0})\ndataset['Fence']=dataset['Fence'].map({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'Nothing':0})\n    \n","e7c177a5":"def add_UltimateYear_ix (X):\n    UltimateYear_ix = X[:,YearBuilt]+X[:,YearRemodAdd]+X[:,YrSold]\nUltimateYear=pd.DataFrame(data={'UltimateYear':dataset['YearBuilt']+dataset['YearRemodAdd']+dataset['YrSold']})\ndataset.insert(loc=60,column='UltimateYear',value=UltimateYear)\ndataset=dataset.drop(['YearBuilt','YearRemodAdd','YrSold'],axis=1)","2e5f5687":"#Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \ndataset=dataset.drop(['Utilities'],axis=1)\ndataset=dataset.drop(['Id'],axis=1) #and drop id column cuase there in no use for it in our model\n","cfb30566":"for col_name in dataset.columns:\n    if dataset[col_name].dtypes=='object':\n        unique_cat=len(dataset[col_name].unique())\n        print(\"feature {col_name} has {unique_cat} unique categories\".format(col_name=col_name,unique_cat=unique_cat)) #intresting syntax!\n        ","0fe3e001":"pd.value_counts(dataset['Exterior1st'],normalize=True).sort_values(ascending=False)*100\n","803ad326":"pd.value_counts(dataset['Exterior2nd'],normalize=True).sort_values(ascending=False)*100","748b2c8c":"pd.value_counts(dataset['Neighborhood']).sort_values(ascending=False)","9994975e":"pd.value_counts(dataset['MSSubClass']).sort_values(ascending=False)","e2ac55e1":"pd.value_counts(dataset['Condition1']).sort_values(ascending=False)","7b7b1e06":"# In this case, bucket low frequecy categories as \"Other\"\ndataset['Exterior1st']=dataset['Exterior1st'].replace(['ImStucc','CBlock','Stone','AsphShn','BrkComm','Stucco','AsbShng','WdShing','BrkFace'],'other')\ndataset['Exterior2nd']=dataset['Exterior2nd'].replace(['ImStucc','CBlock','Stone','AsphShn','Brk Cmn','Stucco','AsbShng','WdShing','Other','BrkFace'],'other')\ndataset['Neighborhood']=dataset['Neighborhood'].replace(['Blueste','NPkVill','Veenker','Blmngtn','BrDale','MeadowV','ClearCr'],'other')\ndataset['MSSubClass']=dataset['MSSubClass'].replace(['150','40','180','45','75'],'other')\ndataset['Condition1']=dataset['Condition1'].replace(['RRNe','RRNn','PosA','RRAe','PosN','RRAn'],'other')\n","145f9a39":"from scipy.stats import norm, skew\nnumeric_feats = dataset.dtypes[dataset.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = dataset[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","c7782f73":"skewness = skewness[abs(skewness) > 0.75].dropna()\nskewness.shape[0]","2dfe4a53":"skewness = skewness[abs(skewness) > 0.75].dropna()\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p,inv_boxcox\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    dataset[feat] = boxcox1p(dataset[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","f9499120":"train_objs_num = len(train)\ndataset_preprocessed = pd.get_dummies(dataset,drop_first=True)\ntrain_preprocessed = dataset_preprocessed[:train_objs_num]\ntest_preprocessed = dataset_preprocessed[train_objs_num:]","5d5c7014":"#from sklearn.utils import shuffle\n#df = shuffle(df)","51fb3949":"X_train=train_preprocessed.drop(['SalePrice'],axis=1)\ny_train=train_preprocessed['SalePrice']\nTest=test_preprocessed.drop(['SalePrice'],axis=1)","734b16c5":"X_train.shape","32278b33":"# Such a large set of features can cause overfitting and also slow computing\n# Use feature selection to select the most important features\nimport sklearn.feature_selection\n\nselect = sklearn.feature_selection.SelectKBest(k=180)\nselected_features = select.fit(X_train, y_train)\nindices_selected = selected_features.get_support(indices=True)\ncolnames_selected = [X_train.columns[i] for i in indices_selected]\n\nX_train_selected = X_train[colnames_selected]\nX_test_selected = Test[colnames_selected]","c058e75d":"(colnames_selected)","e9529558":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV","0f407518":"def rmsle(model):   #cross validation for 5 fold\n    rmse= np.sqrt(-cross_val_score(model, X_train_selected, y_train, scoring=\"neg_mean_squared_error\",cv =5))\n    return(rmse.mean())\n","345ecd24":"gb_reg=GradientBoostingRegressor(n_estimators=2000, learning_rate=0.02,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=30, min_samples_split=30, \n                                   loss='huber')\n#gb_reg.fit(X_train,y_train);\nrmsle(gb_reg)","675fe25d":"forest_reg=RandomForestRegressor(n_estimators=200,max_features=14)\nforest_reg.fit(X_train,y_train);\nrmsle(forest_reg)","000dd503":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nrmsle(lasso)","8272a6f2":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))\nrmsle(ENet)","de543887":"KRR = KernelRidge(alpha=0.6, kernel='linear', degree=2, coef0=2.5)\nrmsle(KRR)","49212a7d":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nrmsle(model_xgb)","a2e661dc":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=500,\n                              max_bin = 90, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nrmsle(model_lgb)","0bb1596c":"br=BayesianRidge()\nrmsle(br)","d9fe2940":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1) ","956459b9":"averaged_models = AveragingModels(models = (ENet, gb_reg, KRR, lasso,model_lgb))\n\nrmsle(averaged_models)","0aca6a0b":"#averaged_models.fit(X_train,y_train)\n#predictions=(averaged_models.predict(Test))\n#predictionsdf = pd.DataFrame({'Predictions':np.exp(predictions)})\n#predictionsdf.to_csv(r'C:\\Users\\Talion\\Desktop\\predictions.csv')","a6e89880":"finally for saving your final predictions:","0231cb5d":"## Elastic Net","d39f5e80":"seperating our dataset into train and test and prepare for modeling","ecd922df":"## non meaningfull numericals :\nwe use sklearn SimpleImputer from sklearn  ","40942a82":"#### not all 'NANs are missing values\n#### According to the \"data descriptions\" only NANs for LotFrontage,Electrical,GarageCars,GarageArea,MasVnrType , MasVnrArea & SaleType are Actually missing and nothing has been maped to nan\n#### we'll deal with them appropiatly \n\nfirst we deal with those 4:","dc27f1d6":"## Creating new functions:\ni'm not comfortable with many features for Year\nlets create a better feature!","b53779ce":"## BayesianRidge:","f1a01135":"#### Box Cox Transformation of (highly) skewed features","af75e736":"## XGboost","d6274392":"## One Hot Encoding:","9ac565a5":"## Modeling:","9d47d2e4":"## LightGBM :","78ebf8a1":"## Skwed features:","9570f406":"lets quickly take a peek at some most important attributes:","c1114d3d":"just for shuffling the training set before modeling.that is optional i'll leave it to you(just train,we have to maintain order of test)\n\n\n","01df43f4":"## Non meaningfull Categorical :","b31d828a":"## stacking:","f085544c":"###### ooookay it's done","78652347":"## lasso Regression:","66f5395b":"## lets take a closer look at remainng categorical variables:\nsince we are smart data scientists we dont want to over complicate our model and since this is relativly large feature dataset \nbefore to go into dummy variables we are about to see can we get rid of some extra column?so many features can cuase overfitting","361b2a11":"## Meaningfull NANS:","a2b5e337":"We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n","924c3dd4":"* first of all i have to say i used some parts of [https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard](http:\/\/) for this kernal (including boxcox transformation and tuning hyper parameters in modeling sections.\nso thank u @serigne!\nthe perpouse of this kernal is not having the most accurate model.i just want to give you a blue print about what you should do for regression problems ","01b42a67":"# define a cross validation function :","b7a5f362":"# outliers:\n\nIdentifies extreme values in data\n\nOutliers are defined as:\nValues below Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1)","b60083ff":"## reduce dimensionality:\nas smart data scientists we want to achieve best results with minimum complexity.\nSuch a large set of features can cause overfitting and also slow computing\nonce again we Use feature selection to select the most important features :","3cd145d7":"## Dealing with NaNs and Zeroes:\nfirst lets count number of Nans for each column","ee43d887":"# RandomForesst:","42ee3599":"# gradient boosting","af45336a":"## Ordinal Variables:","2e5647a0":"log transormation:","52e74f78":"before jumping into dummy variables: the thing about  pd.get_dummies function is it only recognise categorical datas\nfor example:MSZoning variables seems to be numerical but each one is actually represnting a categorical variable.but since it \nis numerical in our dataset,get_dummies function won't recognize it and will treat them as numerical hence no dummy column will be created\nso what we are going to do is make sure datas are in right type\notherwise there is a lot of ordinal variables that we can use onehot encoding on them","33f2423a":"'MSSubClass' is categorical variable but it is actually an integer\n","a4c79042":"### removing some useless variables:","ef1c5eeb":"once again much better","ea9f59e5":"thats much better!"}}