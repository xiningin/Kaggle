{"cell_type":{"be370a7a":"code","33197dd8":"code","3315c224":"code","4b683f97":"code","e7dcd04a":"code","02005a6d":"code","f5f147eb":"code","f03a3db3":"code","1c90c851":"code","aa754884":"code","8ac42099":"code","a7bcf149":"code","06389c62":"code","5fe81420":"code","c09642f1":"code","53ee35f6":"code","7dc95041":"code","c659cfde":"code","421ef6e5":"code","1596ed48":"markdown","6b895ba4":"markdown","0fd5aa84":"markdown","8b2143c9":"markdown","5530ef13":"markdown","0c604dd6":"markdown","4639dec6":"markdown","4a7538d3":"markdown","ca8624e2":"markdown","ea42b65c":"markdown","61ca2397":"markdown","4eede1fd":"markdown"},"source":{"be370a7a":"import matplotlib.pyplot as plt\nimport cv2\n\nfilepath = \"..\/input\/artificial-lunar-rocky-landscape-dataset\/images\/\"\n\ndef showImages(dim, file, imgid,file2, imgid2, title):\n    plt.figure(figsize=(dim,dim))\n    plt.subplot(6,6,1)\n    plt.title(title)\n    img = cv2.imread(filepath + file + '\/' + imgid)  \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.subplot(6,6,1+1)\n    img2 = cv2.imread(filepath + file2 + '\/' + imgid2)  \n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n    plt.imshow(img2)\n    \n    \nshowImages(30, 'clean', 'clean0001.png', 'render', 'render0001.png', 'Example 1')\nshowImages(30, 'clean', 'clean0002.png', 'render', 'render0002.png', 'Example 2')\nshowImages(30, 'clean', 'clean0003.png', 'render', 'render0003.png', 'Example 3')","33197dd8":"import cv2 \nimport os\nimport numpy as np\n\n\nInputPath = \"..\/input\/artificial-lunar-rocky-landscape-dataset\/images\/\"\n\nimage_path = InputPath + 'render'\nmask_path = InputPath + 'ground'\n\nimg_height = 128\nimg_width = 128\nimg_size = 128\n\nfiles = sorted(next(os.walk(image_path))[2])\nfiles2 = sorted(next(os.walk(mask_path))[2])\n\nX = np.zeros((len(files), img_height, img_width, 3), dtype=np.uint8)\ny = np.zeros((len(files2), img_height, img_width, 3), dtype=np.uint8)\n\nX = X.astype(np.float32)\ny = y.astype(np.float32)\n\nfor i, id_ in enumerate(files):\n    images = os.path.join(image_path + \"\/\" + id_)\n    img = cv2.imread(images, 1)\n    img = cv2.resize(img, (img_size, img_size))\n    img = np.expand_dims(img, axis=0)\n    X[i] = img\n\nfor i, id_ in enumerate(files2):\n    masks = os.path.join(mask_path + \"\/\" + id_)\n    mask = cv2.imread(masks, 1)\n    mask = cv2.resize(mask, (img_size, img_size))\n    mask = np.expand_dims(mask, axis=0)\n    y[i] = mask","3315c224":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","4b683f97":"X_train.shape","e7dcd04a":"train_generator = zip(X_train, y_train)\ntest_generator = zip(X_test, y_test)\n\ndef modelInputs(train_generator):\n    for (img,mask) in train_generator:\n        images = img.reshape(1, 128, 128, 3)\n        masks = mask.reshape(1, 128, 128, 3)\n        yield (images,masks)","02005a6d":"import keras.backend as K\n\ndef tversky(y_true, y_pred, smooth=1, alpha=0.7):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1 - y_pred_pos))\n    false_pos = K.sum((1 - y_true_pos) * y_pred_pos)\n    return (true_pos + smooth) \/ (true_pos + alpha * false_neg + (1 - alpha) * false_pos + smooth)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true, y_pred)","f5f147eb":"from tensorflow.keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dropout\nfrom tensorflow.keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.metrics import MeanIoU\n\n\ndef unet(input_size=(128, 128, 3)):\n    input = Input(input_size)\n\n    conv1 = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(input)\n    drop1 = Dropout(0.1)(conv1)\n    conv1 = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(drop1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(pool1)\n    drop2 = Dropout(0.1)(conv2)\n    conv2 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(drop2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(pool2)\n    drop3 = Dropout(0.2)(conv3)\n    conv3 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(drop3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(pool3)\n    drop4 = Dropout(0.2)(conv4)\n    conv4 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\")(drop4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    # Bottleneck\n    conv5 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(pool4)\n    drop5 = Dropout(0.3)(conv5)\n    conv5 = Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(drop5)\n\n    # Expanding\/Upsampling path\n    conv6 = Conv2DTranspose(filters=128, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(conv5)\n    concat6 = concatenate([conv6, conv4])\n    conv6 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(concat6)\n    drop6 = Dropout(0.2)(conv6)\n    conv6 = Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(drop6)\n\n    conv7 = Conv2DTranspose(filters=64, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(conv6)\n    concat7 = concatenate([conv7, conv3])\n    conv7 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(concat7)\n    drop7 = Dropout(0.2)(conv7)\n    conv7 = Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"he_normal\", activation=\"relu\")(drop7)\n\n    conv8 = Conv2DTranspose(filters=32, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(conv7)\n    concat8 = concatenate([conv8, conv2])\n    conv8 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"normal\", activation=\"relu\")(concat8)\n    drop8 = Dropout(0.1)(conv8)\n    conv8 = Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"normal\", activation=\"relu\")(drop8)\n\n    conv9 = Conv2DTranspose(filters=16, kernel_size=(2, 2), strides=(2, 2), padding=\"same\")(conv8)\n    concat9 = concatenate([conv9, conv1], axis=3)\n    conv9 = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"normal\", activation=\"relu\")(concat9)\n    drop9 = Dropout(0.1)(conv8)\n    conv9 = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", kernel_initializer=\"normal\", activation=\"relu\")(conv9)\n\n    output = Conv2D(3, (1, 1), activation=\"softmax\")(conv9)\n\n    model = Model(inputs=[input], outputs=[output])\n\n    model.compile(optimizer = Adam(lr = 1e-4), loss =tversky_loss,metrics=[\"accuracy\"])\n\n    return model","f03a3db3":"from tensorflow.keras.callbacks import ModelCheckpoint\n\nmodel = unet()\n        \nmodel_checkpoint = ModelCheckpoint('Lunar.h5', monitor='loss',verbose=1, save_best_only=True)\nmodel.fit(modelInputs(train_generator),steps_per_epoch=187,epochs=47,callbacks=[model_checkpoint])","1c90c851":"from keras.models import load_model\n\nAstroModel = load_model('Lunar.h5', custom_objects={'tversky_loss': tversky_loss})     ","aa754884":"import cv2\n\ndef Predictions(imgid1):\n\n    plt.figure(figsize=(30,30))\n    plt.subplot(5,5,1)\n\n    img = cv2.imread(InputPath + \"render\/\" + imgid1)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(128,128))\n    img = img.reshape(1,128,128,3)\n\n    prediction = AstroModel.predict(img)\n    \n    prediction = prediction.reshape(128, 128, 3)\n    prediction = cv2.resize(prediction, (710, 470))\n    plt.title(\"Prediction\")\n    plt.imshow(prediction)","8ac42099":"Predictions(\"render0001.png\")","a7bcf149":"def GroundTruth(imgid):\n\n    img2 = cv2.imread(InputPath + \"clean\/\" + imgid)\n    img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n    plt.title(\"Ground truth\")\n    plt.imshow(img2)","06389c62":"GroundTruth(\"clean0001.png\")","5fe81420":"Predictions(\"render0940.png\")","c09642f1":"GroundTruth(\"clean0940.png\")","53ee35f6":"Predictions(\"render9700.png\")","7dc95041":"GroundTruth(\"clean9700.png\")","c659cfde":"Predictions(\"render0134.png\")","421ef6e5":"GroundTruth(\"clean0134.png\")","1596ed48":"## Data Visualization\n\nLet's start by visualizing some of the images using opencv and exploring the data some more. ","6b895ba4":"## Predictions + Model Performance","0fd5aa84":"## Compiling the Model","8b2143c9":"# Artificial Lunar Rocky Landscape Semantic Segmentation with U-Net Architecture \n\n1. About the dataset\n2. Data Visualization \n3. Data Preprocessing \n4. U-Net Architecture\n5. Compiling the model \n6. Predictions + Model Performance\n7. Notes on Model Performance\n9. Final thoughts","5530ef13":"## The Tversky Loss\n\nThe Tversky loss is an asymmetric similarity measure that compares the ground truths and generated masks. It is somewhat of a generalization of dice coefficent and tanimoto coefficient. The following loss code was written by:\n\n@url -> https:\/\/gist.github.com\/robinvvinod\/60d61a3ca642ddd826cf1e6c207cb421\n\n@author -> robinvvinod","0c604dd6":"## About the Dataset\n\nThis dataset contains 9766 realistic renders of lunar landscapes and their masks (segmented into three classes: sky, small rocks, bigger rocks). Additionally, a csv file of bounding boxes and cleaned masks of ground truths are provided. \n\nAn interesting feature of this dataset is that the images are synthetic; they were created using Planetside Software's Terragen. This isn't too obvious immediately as the renderings are highly realistic but it does make more sense after taking into account the scarcity of space imagery data. \n\nAcknowledgment: Romain Pessia and Genya Ishigami of the Space Robotics Group, Keio University, Japan. You can find the dataset [here](http:\/\/www.kaggle.com\/romainpessia\/artificial-lunar-rocky-landscape-dataset). ","4639dec6":"## Final Thoughts\n\nIf I were to improve this model, I would play around a bit more with data augmentation. Maybe changing the contrasts of the renders would be beneficial in helping the model differentiate between the rocks and the surface and not just the lunar landscape and the sky. \n\n\nI would also look into experimenting more with different backbones\/encoders for the U-net architecture and observing the difference in performances. Might be useful in improving the accuracy. \n\nAlso, I would probably look more into the use of other losses (meanIOU, etc).\n\nSpace data is hard to come by; synthetic or not, this was an interesting one to work with!\n\nAgain, if anyone has any ideas as to how to improve the model, I'd be interested in hearing them in the comments!\n\nCheck out some of my other notebooks: \n\n[Designing Convolutional Neural Networks with MNIST dataset](http:\/\/www.kaggle.com\/tenzinmigmar\/fashion-mnist)\n\n[Galaxy Multi-Image Classification with LeNet-5](http:\/\/www.kaggle.com\/tenzinmigmar\/galaxy-multi-image-classification-with-lenet-5)","4a7538d3":"## The U-Net Architecture\n\n![image.png](attachment:image.png)\n\nThe U-Net architecture is well known for its use in biomedical imaging segmentation but has since then been a common architecture used in all kinds of image segmentation applications. Called U-Net for its distinctly U-shaped symmetrical shape, the architecture follows an encoder-decoder approach. The encoder is used to downsample inputs into feature representations at different levels and the decoder is used to upsample or generate the segmented mask using outputs from the encoder. This architecture will be used for this task of multiclass semantic segmentation. ","ca8624e2":"## Data Preprocessing\n\nNow, we will preprocess the data. Essentially, we're reading in the images and masks and replacing that array with an empty array of 0s. Then performing train test split on the arrays to split the data 20-80% into test and train samples.","ea42b65c":"## Notes on Model Performance\n\nI wasn't the happiest with the predictions the model generated. It did quite well segmenting the sky and the surface, but we want the model to segment the rocks too. I did a bit more research into different losses and tried tweaking the learning rates plus added ReduceLROnPlateau checkpoint but it still wasn't doing too well. If anyone has any ideas as to how I can improve the model's performance for multi-class segmentation or can see any issues in the model causing this problem, please let me know!","61ca2397":"![image.png](attachment:image.png)","4eede1fd":"![image.png](attachment:image.png)"}}