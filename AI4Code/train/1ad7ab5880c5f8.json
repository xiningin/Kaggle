{"cell_type":{"9d9d10e2":"code","983f3c5b":"code","f7d070f0":"code","207feab1":"code","0b839a5c":"code","edbce069":"code","edffffc0":"code","029617a9":"code","617a1946":"code","4ad00840":"code","98acb22c":"code","25bcf682":"code","c3d8a0a2":"code","8f53ce91":"code","4ae00fc4":"code","198c4506":"code","4cb225aa":"code","5c920382":"code","e10aee1c":"code","c7afdc89":"code","cccc8514":"code","b8b5c68d":"code","cb996daa":"code","51e99584":"code","77e4d092":"code","1393e232":"code","aabb9b5a":"code","be4a5b89":"code","afbc2cde":"code","6fa82652":"code","c1519b23":"code","72819d04":"code","8ab1e6dd":"code","68da4bd0":"code","72339577":"markdown","8b2434fb":"markdown","79f0ce0f":"markdown"},"source":{"9d9d10e2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nfrom PIL import Image\n\nimport re\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.pipeline import Pipeline\nimport warnings\n\nnlp = spacy.load('en_core_web_sm')\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","983f3c5b":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding='latin-1')\ndf.head(-5)","f7d070f0":"df.isnull().sum()","207feab1":"df.describe()","0b839a5c":"df = df.iloc[:, [0,1]].rename(columns={\"v1\": \"Label\", \"v2\": \"Message\"})\ndf.head(-5)","edbce069":"df.groupby('Label').describe()","edffffc0":"df.groupby('Label').describe()","029617a9":"X = df['Message']\ny = df['Label']\n\nprint(\"X Shape:\", X.shape)\nprint(\"y Shape:\", y.shape)","617a1946":"df[\"Label\"].value_counts().plot(kind=\"bar\")\nplt.show()","4ad00840":"twitter_mask = np.array(Image.open('..\/input\/masksforwordclouds\/twitter_mask3.jpg'))\n\nwc_ham = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='Set2', \n               collocations=False, stopwords = STOP_WORDS, mask=twitter_mask).generate(' '.join(text for text in df.loc[df['Label'] == 'ham', 'Message']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for HAM messages')\nplt.imshow(wc_ham, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","98acb22c":"wc_spam = WordCloud(width = 3000, height = 2000, random_state=1, background_color='white', colormap='Set1', \n               collocations=False, stopwords = STOP_WORDS, mask=twitter_mask).generate(' '.join(text for text in df.loc[df['Label'] == 'spam', 'Message']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for Spam messages')\nplt.imshow(wc_spam, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","25bcf682":"def TextCleaning(X):\n    documents = []\n    \n    for sent in X:\n        # Remove all single characters\n        sent = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sent)\n        \n        # Substituting multiple spaces with single space\n        sent = re.sub(r'\\s+', ' ', sent, flags=re.I)\n        \n        doc = nlp(sent)\n        \n        document = [token.lemma_ for token in doc]\n        \n        document = ' '.join(document)\n        \n        documents.append(document)\n    return documents","c3d8a0a2":"X[:5]","8f53ce91":"X = TextCleaning(X)\nX[:5]","4ae00fc4":"Vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)","198c4506":"X = Vectorizer.fit_transform(X, y)\nX.shape","4cb225aa":"X = X.toarray()","5c920382":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=18)\nprint(\"X_train shape:\", X_train.shape)\nprint(\"y_train shape:\", y_train.shape)\nprint(\"X_test shape:\", X_test.shape)\nprint(\"y_test shape:\", y_test.shape)","e10aee1c":"NB = MultinomialNB()\nNB.fit(X_train, y_train)","c7afdc89":"print(NB.score(X_train, y_train))\nprint(NB.score(X_test, y_test))","cccc8514":"y_pred = NB.predict(X_test)\ny_pred","b8b5c68d":"# Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nCM = pd.DataFrame(CM, index=df['Label'].unique(), columns=df['Label'].unique())\n\nsns.heatmap(CM, annot=True, cbar=None, cmap=\"Blues\", fmt = 'g')\nplt.title(\"Confusion Matrix\"), plt.tight_layout()\nplt.ylabel(\"True Class\"), plt.xlabel(\"Predicted Class\")\nplt.show()","cb996daa":"print(classification_report(y_test, y_pred))","51e99584":"NB.predict(Vectorizer.transform(['Hi Abdelrahman Abozied, I have a job offer for you as a Machine Learning Engineer']))","77e4d092":"NB.predict(Vectorizer.transform(['Congratulations, you won @ free rolex']))","1393e232":"class TextCleaning():\n    def __init__(self):\n        print(\"call init\")\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        documents = []\n        for sent in X:\n            # Remove all single characters\n            sent = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sent)\n\n            # Substituting multiple spaces with single space\n            sent = re.sub(r'\\s+', ' ', sent, flags=re.I)\n\n            doc = nlp(sent)\n\n            document = [token.lemma_ for token in doc]\n\n            document = ' '.join(document)\n\n            documents.append(document)\n        return documents","aabb9b5a":"EmailClassification = Pipeline([('TextCleaning', TextCleaning()),\n                                ('Vectorizer', TfidfVectorizer(stop_words=STOP_WORDS)),\n                                ('NB', MultinomialNB())])","be4a5b89":"EmailClassification.fit(df['Message'], y)\nprint(\"Model score:\", EmailClassification.score(df['Message'], y))","afbc2cde":"EmailClassification.predict(['Hi Abdelrahman Abozied, I have a job offer for you as a Machine Learning Engineer'])","6fa82652":"EmailClassification.predict(\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\")[0]","c1519b23":"from joblib import dump, load\ndump(EmailClassification, 'EmailClasification.joblib')","72819d04":"model = load('EmailClasification.joblib')","8ab1e6dd":"model.predict(['Hi Abdelrahman Abozied, I have a job offer for you as a Machine Learning Engineer'])","68da4bd0":"model.predict(['congratulations , you won @ rolex'])","72339577":"# 2- Data","8b2434fb":"Hello Future Engineers,\n\nI'm Abdelrahman Abozied, Machine Learning Engineer.\n\nNice to meet you!\n\n# 1- Import Libraries","79f0ce0f":"# 3- Model"}}