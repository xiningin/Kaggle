{"cell_type":{"d969d6c7":"code","d67feddb":"code","842bd4c0":"code","6dabc925":"code","ab8d240c":"code","6f731c12":"code","41fef2f9":"code","cc2eec06":"code","2c099ce5":"code","ae648e52":"code","c3c90f2d":"code","1e47a958":"code","ba9f0838":"code","0b807089":"code","e9821273":"code","65033b34":"markdown","8f8f2325":"markdown","afdf6f7a":"markdown","17d2b391":"markdown","1e9edb2f":"markdown","6f219e64":"markdown","3b8de29b":"markdown","059aad2b":"markdown","06a58735":"markdown","32fafa37":"markdown","a3842b82":"markdown","02db9ad9":"markdown","1a8be5da":"markdown","d9a005b3":"markdown","1fb9acc8":"markdown","86db7e7d":"markdown","41d37c23":"markdown","ea1b58c7":"markdown","ec7e104d":"markdown","e60a0631":"markdown"},"source":{"d969d6c7":"import pandas as pd # for creating dataframe from numerical array \nimport numpy as np # for handling all the mathematics \nimport seaborn as sns # for the better visualization which will help you learn how dimensionality reduction happens\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets # for the import of mniset dataset by another way\nfrom sklearn import manifold# to perform the t-SNE\n\n%matplotlib inline","d67feddb":"df = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\ndf","842bd4c0":"# removing the label from the dataset as we are not gonna ever doing dimension reduction on our label\ndf1 = df.iloc[:,1:]\ndf1","6dabc925":"from sklearn.preprocessing import StandardScaler\ndf_std = StandardScaler().fit_transform(df1)\ndf_std","ab8d240c":"df_cov_matrix = np.cov(df_std.T)\ndf_cov_matrix","6f731c12":"eig_vals, eig_vecs = np.linalg.eig(df_cov_matrix)\nprint(eig_vecs)\nprint(eig_vals)","41fef2f9":"eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\nprint(\"Eigenvalues in descending order:\")\nfor i in eig_pairs:\n print(i[0])","cc2eec06":"total = sum(eig_vals)\nvar_exp = [(i \/ total)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Variance captured by each component is\",var_exp)\nprint(\"Cumulative variance captured as we travel with each component\",cum_var_exp)","2c099ce5":"from sklearn.decomposition import PCA\npca = PCA().fit(df_std)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"No of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.show()","ae648e52":"from sklearn.decomposition import PCA\npca = PCA(n_components = 10)\npcs = pca.fit_transform(df_std)\ndf_new = pd.DataFrame(data=pcs, columns={\"PC1\",\"PC2\",\"PC3\",\"PC4\",\"PC5\",\"PC6\",\"PC7\",\"PC8\",\"PC9\",\"PC10\"})\ndf_new[\"target\"] = df[\"label\"]","c3c90f2d":"sns.lmplot(x='PC1',\n           y='PC2',\n           data=df_new,\n           fit_reg=False,\n           legend=True,\n           size=9,\n           hue='target',\n           scatter_kws={\"s\":80})","1e47a958":"data = datasets.fetch_openml('mnist_784',version=1,return_X_y = True)\npixel_value, target = data\ntargets = target.astype(int)","ba9f0838":"tnse = manifold.TSNE(n_components=2,random_state = 42)\nnew_data = tnse.fit_transform(pixel_value[:3000,:])","0b807089":"tnse_df = pd.DataFrame(np.column_stack((new_data,targets[:3000])),\n                      columns = [\"x\",\"y\",\"targets\"])\ntnse_df.loc[:,\"targets\"] = tnse_df.targets.astype(int)","e9821273":"grid = sns.FacetGrid(tnse_df,hue=\"targets\",size=8)\ngrid.map(plt.scatter,\"x\",\"y\").add_legend()","65033b34":"# T-SNE(t- Distributed Stochastic Neighbour Embedding)\n\nThis is actually going to different world than the previous guys and here it is considered as most efficient way to perform the dimensionality reduction rather than any other. It performs such a great clustring of data which helps the person to visualize things very easily. :)","8f8f2325":"# Steps of PCA\n![image.png](attachment:image.png)","afdf6f7a":"![image.png](attachment:image.png)","17d2b391":"## Why fit_transform() on Train and transform() on Test ??\nWe use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data. We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.","1e9edb2f":"Dimensionality reduction is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data. \n\n## Why so much care about dimension ?\n\nWell, We care because the curse of dimensionality demands that we do. The curse of dimensionality refers to all the problems that arise when working with data in the higher dimensions, that did not exist in the lower dimensions.\nAs the number of features increase, the number of samples also increases proportionally. The more features we have, the more number of samples we will need to have all combinations of feature values well represented in our sample. This graph is speaking actually more than me :)\n\n![image.png](attachment:image.png)","6f219e64":"Till Now, we have completed the first 2 steps and we are on the third step to find the covariance matrix which is very easy by our libary numpy and that is just done below by taking the transpose of scaled df.","3b8de29b":"Here, n_components defines the components and as we can visualize them well in a two- dimensional settings, we kept n_components =2","059aad2b":"# DIMENSIONALITY REDUCTION\n\n![image.png](attachment:image.png)","06a58735":"We are going to rotate our data to fit these new axes. But what will the coordinates of the rotated data be?\nTo convert the data into the new axes, we will multiply the original X, Y data by Eigenvectors, which indicate the direction of the new axes (principal components).\nBut first, we need to deduce the Eigenvectors (there are two \u2014 one per axis). Each Eigenvector will correspond to an Eigenvalue, whose magnitude indicates how much of the data\u2019s variability is explained by its Eigenvector.\nFrom the definition of Eigenvalue and Eigenvector:\n\n[Covariance matrix].[Eigenvector] = [Eigenvalue].[Eigenvector]\n\n![image.png](attachment:image.png)\nThis figure shows New axes of the dataset when re-plotted with the PCA components.\n\nHope it helps you!!","32fafa37":"As you can see above it shows the data in 2D but the probelm occurs that its is so clustred that we be able to segregate that which one is where if it is black and white. Is't it ???","a3842b82":"## What is MNIST dataset ??\n\nWell actually its a dataset having numbers from 0 to 9 distributed all our main usually in thi dataset is to predict the number from 0 to 9. But lets first understand our data, directly working on data and applying models to that is not a very big cup of tea. Besides, Understanding data and making something useful out of that is what all Data Science.\n\nSo lets get started !!!!!!!!!","02db9ad9":"Well there are many dataset in which you might have worked or will be working like **IRIS Dataset**, **Titanic dataset**, **MNIST**, and many more. I have choosed here to go with MNIST dataset which will really make understanding more easy. ;)","1a8be5da":"Now comes the 4th step where in we are calculating the eign values and eign vector for the principal component analysis, this is the most important step of all, which includes the heart of PCA.\n\n## What are Eigenvectors?\n\nWe know that vectors have both magnitude and direction when plotted on an XY (2-dim) plane. As required for this article, linear transformation of a vector, is the multiplication of a vector with a matrix that changes the basis of the vector and also its direction.\nWhen a vector is plotted, it\u2019s direction is along its span. Now, there are some special vectors, which when transformed linearly, their directions don\u2019t change, that is, they don\u2019t get knocked off their span (the line passing through it\u2019s origin and tip). Instead they\u2019re either squished or stretched.\nThis leads us to Eigenvalues.\n\n## What are Eigenvalues?\n\nThey\u2019re simply the constants that increase or decrease the Eigenvectors along their span when transformed linearly.","d9a005b3":"In the below code, we are actually seeing the variance captured by each component, which is important because for PCA analysis we need to define the no of components we need, which comes from here if the variance values reaches above 95% in first 5 values, this means that first 5 components are enough for data to be plot on graph and thus the no of PCA components = 5\n\nLet's find out this is in our case!! :)","1fb9acc8":"# Powerful t-SNE\nAnd now You see what a beautiful structure has came out to us. WOW!!!\n## Conclusions:\n\nWell from the graph its clearly visible that \n\n* 0(coloured blue are stacked at one place with no confusion among other numbers)\n\n* 1(coloured orange is also very far from zero and a bit of purple which stand for 7, which is quite obvious)\n\n* 2(coloured green is also having cluster aside with no confusion at all.)\n\n* 3(coloured red is also differently placed with the interruption of brown which is of 5)\n\n* 4 and 9 have been very mixed as they both looks same to computers, well sometime even humans make mistakes, its only a machine ;)\n\n* 3 and 8 are also closely related with each others without any doubt this type of confusion is common.\n\n* 6 has been stacked totally different without any confusion making our looks pretty and completes it","86db7e7d":"# A detail on Eigen vector and Values.. :)\n\nSuppose we have plotted a scatter plot of random variables, and a line of best fit is drawn between these points. This line of best fit, shows the direction of maximum variance in the dataset. The Eigenvector is the direction of that line, while the eigenvalue is a number that tells us how the data set is spread out on the line which is an Eigenvector.\n![image.png](attachment:image.png)\n\n","41d37c23":"The main principal component, depicted by the black line, is the first Eigenvector. The second Eigenvector will be perpendicular or orthogonal to the first one. The reason the two Eigenvectors are orthogonal to each other is because the Eigenvectors should be able to span the whole x-y area. Naturally, a line perpendicular to the black line will be our new Y axis, the other principal component.\n\n![image.png](attachment:image.png)\n\n","ea1b58c7":"For you people to understand, I have made a plot which shows the relation between no of components and the variance and we will easily find out that the components corresponding to the percentage.","ec7e104d":"Well, I may use the same data as above, but i have shown you a different way of downloading data even if you know from where to start sklearn has libaray dataset which has many basic dataset which can be import just by one command.\n\nThe return_X_Y means to return the independent and dependent variable both and stored in data which will be like \n\ndata=[independent variable, dependent variable] \n\nand thats why we give them name as pixel_value and target respectively.\n","e60a0631":"Hope that this kernel will add more knowledge to you and you will like it just like my other notebooks.\n\nThanks for your time, I hope I doesn't waste your time here :)\n\nHappy learning ;)"}}