{"cell_type":{"a9294640":"code","86679051":"code","8b8a031a":"code","24dcd0cd":"code","9ab892b5":"code","d8d9628d":"code","11f08a06":"code","7a8c1a84":"code","e34c42ec":"code","3d8524e7":"code","c7caf27e":"code","d0d89137":"code","0af50c0f":"code","88fb63c5":"code","3d7da0f8":"code","fe7729c7":"code","c74eb2c1":"code","3995f448":"code","0db9fff3":"code","3184ba48":"code","64ce10fc":"code","bb7191ad":"code","171e9fa7":"markdown","1fba51ed":"markdown","419874c3":"markdown","282736e3":"markdown","9aefc9a8":"markdown","9fe6876f":"markdown","34764865":"markdown","e8931c00":"markdown","6a977bb4":"markdown","fd60ca00":"markdown","f41f6c65":"markdown","2b006cfb":"markdown","3f63595b":"markdown"},"source":{"a9294640":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86679051":"data=pd.read_csv('\/kaggle\/input\/bangalore-house-prediction-processed-dataset\/bangalore house price prediction cleaned.csv')\ndata.head(10)","8b8a031a":"data.isnull().sum() ","24dcd0cd":"X=data.drop('price', axis=1)\ny=data['price']\nprint(X.shape)\nprint(y.shape)","9ab892b5":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.7,random_state=42)","d8d9628d":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()","11f08a06":"sc.fit(X_train)\nX_train=sc.fit_transform(X_train)\nX_test=sc.fit_transform(X_test)","7a8c1a84":"from sklearn.svm import SVR\nsvr=SVR(kernel='linear', C=100)\nmodel1=svr.fit(X_train,y_train)\ns=round((model1.score(X_test,y_test) * 100),2)\nprint('Accuracy of {} model is {}%'.format(svr,s))","e34c42ec":"cap=[]\nfrom sklearn.svm import SVR\nfor i in range(100,1000,100):\n    svr=SVR(kernel='rbf', C=i)\n    model2=svr.fit(X_train,y_train)\n    s=round((model2.score(X_test,y_test) * 100),2)\n    cap.append(s)\n    print('Accuracy of {} model is {}%'.format(svr,s))","3d8524e7":"import plotly.graph_objects as go\nimport numpy as np\n\nx = np.arange(100,1000,100)\n\nfig = go.Figure(data=go.Scatter(x=x, y=cap, mode='lines+markers',))\nfig.update_layout(title='Accuracy vs C(Regularization) value - RBF Kernel',\n                   xaxis_title='C Value',\n                   yaxis_title='Accuracy (in %)')\nfig.show()","c7caf27e":"cap1=[]\nfrom sklearn.svm import SVR\nfor i in range(100,1000,100):\n    svr=SVR(kernel='poly', C=i, degree=2)\n    model3=svr.fit(X_train,y_train)\n    m=round((model3.score(X_test,y_test) * 100),2)\n    cap1.append(m)\n    print('Accuracy of {} model is {}%'.format(svr,m))","d0d89137":"import plotly.graph_objects as go\nimport numpy as np\n\nx = np.arange(100,1000,100)\n\nfig = go.Figure(data=go.Scatter(x=x, y=cap1, mode='lines+markers'))\nfig.update_layout(title='Accuracy vs C(Regularization) value - Polynomial Kernal',\n                   xaxis_title='C Value',\n                   yaxis_title='Accuracy (in %)')\nfig.show()","0af50c0f":"svr=SVR(kernel='linear', C=100)\nmodel1=svr.fit(X_train,y_train)\nmodel1.predict([X_test[0]])","88fb63c5":"svr=SVR(kernel='poly', C=200, degree=2)\nmodel3=svr.fit(X_train,y_train)\nmodel3.predict([X_test[0]])","3d7da0f8":"svr=SVR(kernel='rbf', C=900)\nmodel2=svr.fit(X_train,y_train)\nmodel2.predict([X_test[0]])","fe7729c7":"y_test[960]","c74eb2c1":"print('All predictions below are in Lakhs,\\n')\nprint('Prediction through Linear Kernel:',model1.predict([X_test[0]]))\nprint('Prediction through Radial Kernel:',model2.predict([X_test[0]]))\nprint('Prediction through Polynomial Kernel:',model3.predict([X_test[0]]))\nprint('Actual Amount of House:',y_test[960])","3995f448":"svr=SVR(kernel='rbf', C=900)\nmodel2=svr.fit(X_train,y_train)\ny_pred=model2.predict(X_test)","0db9fff3":"pp=pd.DataFrame(y_pred, columns=['Predicted_Price'])\npp","3184ba48":"op=pd.DataFrame(y_test)\nop.reset_index(drop=True, inplace=True)\nop","64ce10fc":"final_df=pd.concat((op,pp),axis=1)\nfinal_df","bb7191ad":"fig,axes=plt.subplots(figsize=[20,18])\nplt.scatter(x='price',y='Predicted_Price',data=final_df, color='crimson')\n\np1 = max(max(final_df['Predicted_Price']), max(final_df['price']))\np2 = min(min(final_df['Predicted_Price']), min(final_df['price']))\nplt.plot([p1, p2], [p1, p2], 'o-')\n\nplt.xlabel('Orginal Price', fontsize=15)\nplt.ylabel('Predicted Price', fontsize=15)\nplt.axis('equal')\nplt.show()","171e9fa7":"## From above predictions we could observe that in our case Radial Kernel is performing best while other 2 models are not performing well due to Bias-variance tradeoff.","1fba51ed":"### From all the above observations we can either go with RBF Kernel or Linear Kernel for max accuracy but at the same time we need to be careful about the overfitting situation of model. It's always recommemded that if we have a data that is linear in nature then we should go with Linear kernels or else we should go with RBF\/Polynomial Kernel","419874c3":"# Using RBF Kernel in SVR","282736e3":"### Checking for Null values","9aefc9a8":"### From above graph we can say that max accuracy that we are able to get is till C=200 in Polynomial Kernel and after that our accuracy starts decreasing.","9fe6876f":"### Scaling the data and fitting the data to our model","34764865":"## From above observations we can see that accuracy of our model increases with increase in value of C (Regularization)","e8931c00":"### Splitting train and test data","6a977bb4":"### Let's predict the house prices using different models:","fd60ca00":"### SVR or SVC works best with the transformed\/scaled data to avoid any bias while fitting the data into the model. So, let's perform the feature scaling","f41f6c65":"# Using Linear Kernel in SVR","2b006cfb":"## Linear Kernel:","3f63595b":"### No Null data present as such, so we can proceed with our analysis further"}}