{"cell_type":{"cb00eaf2":"code","610c7743":"code","59aef0dd":"code","b04f8fc7":"code","616d2196":"code","375e8dc9":"code","e60fe710":"code","f65f8a06":"code","d4fb9e9f":"code","48d9be3c":"code","72f1670b":"code","843d5764":"code","184aa2d6":"code","7ccf0e8b":"code","8216724b":"code","3d7d6173":"code","ecc82daa":"code","34b730fc":"code","69695f29":"code","ee7105d5":"code","d3ef57cb":"code","eef240bf":"code","228d76e4":"code","c9fac0b0":"code","c7b534c3":"code","832cd289":"code","1df30f30":"code","d7474811":"code","492a7cdc":"code","307fa626":"code","8cbe072c":"code","6ebc0bec":"code","8d02e756":"code","49d02400":"code","a3334857":"code","8decfda9":"code","60c37a19":"code","68695eec":"code","55195de8":"code","71340a30":"code","b6d4180b":"code","a6e0b871":"code","cb29c091":"code","41b4e531":"code","f8ca01ba":"markdown","11a8de87":"markdown","43b21c57":"markdown","76fbf74e":"markdown","24f8d1f4":"markdown","0e36976d":"markdown","9d83d62b":"markdown","e9c1a6eb":"markdown","e91a7039":"markdown","5fbbce97":"markdown","c0323d84":"markdown","2a4cd49b":"markdown"},"source":{"cb00eaf2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","610c7743":"!pwd","59aef0dd":"cd ..\/input\/mnist-dataset","b04f8fc7":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport struct\nimport os\nimport math","616d2196":"# \u8bfb\u53d6\u6570\u636e\n\n# \u53c2\u8003\u535a\u5ba2\uff1ahttps:\/\/blog.csdn.net\/u013597931\/article\/details\/80099243\ndef load_mnist_train(path, kind='train'):    \n    labels_path = os.path.join(path+'%s-labels-idx1-ubyte\/'% kind,'%s-labels-idx1-ubyte'% kind)\n    images_path = os.path.join(path+'%s-images-idx3-ubyte\/'% kind,'%s-images-idx3-ubyte'% kind)    # \u8bad\u7ec3\u96c6\u7684\u8def\u5f84\u8bfb\u53d6\n    with open(labels_path, 'rb') as lbpath:          \n        magic, n = struct.unpack('>II',lbpath.read(8))\n        labels = np.fromfile(lbpath,dtype=np.uint8)\n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n    return images, labels\ndef load_mnist_test(path, kind='t10k'):\n    labels_path = os.path.join(path+'%s-labels-idx1-ubyte\/'% kind,'%s-labels-idx1-ubyte'% kind)\n    images_path = os.path.join(path+'%s-images-idx3-ubyte\/'% kind,'%s-images-idx3-ubyte'% kind)\n    with open(labels_path, 'rb') as lbpath:\n        magic, n = struct.unpack('>II',lbpath.read(8))\n        labels = np.fromfile(lbpath,dtype=np.uint8)\n    with open(images_path, 'rb') as imgpath:\n        magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n        images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n    return images, labels   \n\npath='\/kaggle\/input\/mnist-dataset\/'\ntrain_images,train_labels=load_mnist_train(path)    # \u8bad\u7ec3\u6570\u636e\ntest_images,test_labels=load_mnist_test(path)      # \u6d4b\u8bd5\u6570\u636e\n\nfig=plt.figure(figsize=(8,8))\nfig.subplots_adjust(left=0,right=1,bottom=0,top=1,hspace=0.05,wspace=0.05)\nfor i in range(30):\n    images = np.reshape(train_images[i], [28,28])\n    ax=fig.add_subplot(6,5,i+1,xticks=[],yticks=[])\n    ax.imshow(images,cmap=plt.cm.binary,interpolation='nearest')\n    ax.text(0,7,str(train_labels[i]))\nplt.show()","375e8dc9":"# \u6570\u636e\u89c4\u6a21\nprint(train_images.shape)\nprint(test_images.shape)\n\nprint(train_labels.shape)\nprint(test_labels.shape)","e60fe710":"# \u6570\u636e\u5f52\u4e00\u5316\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0","f65f8a06":"# \u62bd\u53d6\u4e00\u4e2a\u5c0f\u6279\u91cf\nBatch_size = 10000\nbatch_train = np.random.choice(np.arange(train_images.shape[0]), Batch_size)\nbatch_test = np.random.choice(np.arange(test_images.shape[0]), 2000)\ntrain_images0 = train_images[batch_train]\ntrain_labels0 = train_labels[batch_train]\ntest_images0 = test_images[batch_test]\ntest_labels0 = test_labels[batch_test]\nprint(train_images0.shape)","d4fb9e9f":" np.seterr(divide='ignore', invalid='ignore')  # \u5ffd\u7565\u6389\u9664\u4e8e0\u7684\u5143\u7d20","48d9be3c":"# \u5168\u8fde\u63a5\u5c42\u7684\u521d\u59cb\u5316\u7c7b\nclass FullyConnected():\n    def __init__(self, W, b):\n        '''\n        Parameter:\n        W: \u6743\u91cd\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(N, M), N\u4e3a\u8f93\u5165\u795e\u7ecf\u5143\u7684\u4e2a\u6570\uff0cM\u4e3a\u8f93\u51fa\u795e\u7ecf\u5143\u7684\u4e2a\u6570\n        b: \u504f\u79fb\u91cf\u77e9\u9635 (M\uff0c)\n        '''\n        self.W = W    # \u8d4b\u503c\n        self.b = b    # \u8d4b\u503c\n\n        self.x = None      # \u7528\u6765\u5b58\u50a8\u8f93\u5165\u795e\u7ecf\u5143\u7684\u77e9\u9635\uff0c\u4e3a\u53cd\u5411\u4f20\u64ad\u63d0\u4f9b\u4fbf\u5229\n\n        self.dW = None   # \u7528\u6765\u5b58\u50a8\u68af\u5ea6\uff0c\u68af\u5ea6\u4e0b\u964d\u65f6\u9700\u8981\u7528\u6765\u66f4\u65b0\u6743\u91cd\n        self.db = None\n    \n    # \u5168\u8fde\u63a5\u5c42\u7684\u524d\u5411\u4f20\u64ad\n    def forward(self, x):\n        '''\n        input:\n        x: \u8f93\u5165\u795e\u7ecf\u5143\u7684\u77e9\u9635, \u5f62\u72b6(B, N)\uff0cB\u4e3a\u6279\u91cf\u5927\u5c0f\uff0cN\u4e3a\u8f93\u5165\u795e\u7ecf\u5143\u7684\u4e2a\u6570\n        output:\n        y: \u8f93\u51fa\u795e\u7ecf\u5143\u7684\u77e9\u9635, \u5f62\u72b6(B, M)\uff0c M\u4e3a\u8f93\u51fa\u795e\u7ecf\u5143\u7684\u4e2a\u6570\n        '''\n        self.x = x            # \u5b58\u50a8\u8f93\u5165\u795e\u7ecf\u5143\u7684\u77e9\u9635\uff0c\u4fbf\u4e8e\u53cd\u5411\u4f20\u64ad\u8ba1\u7b97\u66f4\u65b0\u6743\u91cd\n        out = np.dot(self.x, self.W) + self.b   # \u5b8c\u6210\u4e00\u6b21\u524d\u5411\u4f20\u64ad\n        return out    # \u8fd4\u56de\u524d\u5411\u4f20\u64ad\u7ed3\u679c\n    \n    \n    # \u5168\u8fde\u63a5\u5c42\u7684\u53cd\u5411\u4f20\u64ad\n    def backward(self, dout):\n        '''\n        input:\n        dout: \u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u5168\u8fde\u63a5\u5c42\u8f93\u51fa\u7684\u68af\u5ea6\uff0c\u5f62\u72b6\u4e3a(B,M)\uff0cM\u662f\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u51fa\u795e\u7ecf\u5143\u4e2a\u6570\u3002\n        \u5728\u524d\u5411\u4f20\u64ad\u65f6\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165\u8bb0\u5f55\u5728\u4e86self.x\u4e2d\uff0c\u6545\u7531\u6b64\u6211\u4eec\u53ef\u4ee5\u5229\u7528dout\u548cself.x\u5f97\u5230W\u7684\u68af\u5ea6\n        output:\n        dx:  (B, N) \u5173\u4e8e\u8f93\u5165\u5c42\u7684\u68af\u5ea6\uff0c\u4fbf\u4e8e\u8fdb\u4e00\u6b65\u53cd\u5411\u4f20\u64ad\n        self.W\u548cself.b\u7684\u68af\u5ea6\u5206\u522b\u5b58\u50a8\u5728self.dW\u548cself.db\u4e2d\n        self.dW: (N, M) \u4e0eself.W\u5f62\u72b6\u76f8\u540c\uff0cself.W\u7684\u68af\u5ea6\n        self.db: (M,)\uff0c self.b\u7684\u68af\u5ea6\n        \u5c06x\u7684\u68af\u5ea6\u8fd4\u56de\u3002\n        '''\n        # \u4ee5\u4e0b\u6240\u6709\u5373\u4e3a\u77e9\u9635\u7684\u6c42\u5bfc\u65b9\u6cd5\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u6839\u636e\u5f62\u72b6\u8f93\u5165\u8f93\u51fa\u6c42\u89e3\n        # \u5747\u4f9d\u8d56\u4e8e\u516c\u5f0f Y = X^T + W\n        self.db = np.sum(dout, axis=0)    # \u9700\u8981\u5c06\u5f97\u5230\u7684\u6240\u6709dout\u5ef6y\u8f74\u76f8\u52a0\uff0c\u56e0\u4e3a\u53d6loss\u662f\u5c31\u9664\u4ee5\u4e86batch_size\n        self.dW = np.dot(self.x.T, dout)  # \u5728\u524d\u5411\u4f20\u64ad\u65f6\u5168\u8fde\u63a5\u5c42\u7684\u8f93\u5165\u8bb0\u5f55\u5728\u4e86self.x\u4e2d\uff0c\u8fd9\u4e00\u9879\u6839\u636e\u77e9\u9635\u6c42\u5bfc\u5f97\u5230\u6211\u4eec\u7ed3\u679c\n        dx = np.dot(dout, self.W.T)       # \u7531\u77e9\u9635\u6c42\u5bfc\u5f97\u51fa\u7ed3\u679c\n        return dx                       # \u8fd4\u56de\u5bf9\u8f93\u5165\u5c42\u6c42\u5bfc\u7684\u7ed3\u679c\uff0c\u4fbf\u4e8e\u8bb0\u5f55\u8fdb\u4e00\u6b65\u53cd\u5411\u4f20\u64ad","72f1670b":"# \u5377\u79ef\u5c42\u521d\u59cb\u5316\u7c7b\nclass Conv2d():\n    '''\n    Parameter:\n        in_channels: C_in from expected input shape (B, C_in, H_in, W_in).\n        channels: C_out from output shape (B, C_out, H_out, W_out).\n        kernel_size: default 3.\n        stride: default 1.\n        padding: default 0.\n    '''\n    \n    def __init__(self, in_channels: int, channels: int, kernel_size: int=3,\n                 stride: int=1, padding: int=0, bias: bool=False):\n        \"\"\"\n        \u4e8c\u7ef4\u5377\u79ef\u5c42\n        input:\n        - W: numpy.array, (C_out, C_in, K_h, K_w)\n        - b: numpy.array, (C_out)\n        - stride: int\n        - pad: int\n\n        \"\"\"\n        self.W = tensor(np.random.randn(channels, in_channels, kernel_size, kernel_size))\n        # self.b = b\n        self.stride = stride\n        self.pad = padding\n        self.kernel_size = kernel_size\n        self.x = None\n        self.col = None\n        self.col_W = None\n        # self.dW = None   self.W.grad\n        # self.db = None\n\n\n    def forward(self, x):\n        \"\"\"\n        input:\n            x: input of shape (B, C_in, H_in, W_in).\n        output:\n            out: output of shape (B, C_out, H_out, W_out).\n        \"\"\"\n        FN, C, FH, FW = self.W.shape\n        N, C, H, W = x.shape\n        out_h = 1 + int((H + 2*self.pad - FH) \/ self.stride)\n        out_w = 1 + int((W + 2*self.pad - FW) \/ self.stride)\n\n        col = Conv2d_im2col(x)\n        col_W = self.W.reshape(FN, -1).T\n\n        out = np.dot(col, col_W)\n        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n        \n        self.x = x\n        self.col = col\n        self.col_W = col_W\n        \n        return out\n        \n\n\n    def backward(self, dy):\n        \"\"\"\n        input:\n            dy: output delta of shape (B, C_out, H_out, W_out).\n        output:\n            dx: input delta of shape (B, C_in, H_in, W_in).\n        \"\"\"\n        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_shape\n            out_h = (H + 2 * pad - filter_h) \/\/ stride + 1\n            out_w = (W + 2 * pad - filter_w) \/\/ stride + 1\n            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n            return img[:, :, pad:H + pad, pad:W + pad]\n        \n        FN, C, FH, FW = self.W.shape\n        dout = dy\n        dout = dout.transpose(0, 2, 3, 1).reshape(-1, FN)\n        # self.b.grad = np.sum(dout, axis=0)\n        self.W.grad = np.dot(self.col.T, dout)\n        self.W.grad = self.W.grad.transpose(1, 0).reshape(FN, C, FH, FW)\n        dcol = np.dot(dout, self.col_W.T)\n        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n        return dx\n\n\n\nclass Conv2d_im2col(Conv2d):\n    '''\n    \u8d3e\u6768\u6e05\u6240\u7ed9\u601d\u60f3\uff0c\u5229\u7528\u5c06\u5377\u79ef\u5c42\u8f6c\u5316\u4e3a\u4e8c\u7ef4\u5927\u77e9\u9635\uff0c\n    \u5f53\u7136\u8fd8\u6709\u4e00\u4e2acol2im\u662f\u53e6\u5916\u4e00\u4e2a\u5173\u4e8e\u6b64\u5c06\u4e8c\u4f4d\u5927\u77e9\u9635\u8f6c\u5316\u4e3a\u56fe\u7247\u7684\u5377\u79ef\u5c42\uff0c\u53cd\u5411\u4f20\u64ad\u7528\u3002\n    '''\n\n    def forward(self, x):\n\n        # TODO Implement forward propogation of\n        # 2d convolution module using im2col method.\n        input_data = x\n        filter_h, filter_w = self.kernel_size, self.kernel_size\n        stride = self.stride\n        pad = self.pad\n        N, C, H, W = input_data.shape\n        out_h = (H + 2 * pad - filter_h) \/\/ stride + 1\n        out_w = (W + 2 * pad - filter_w) \/\/ stride + 1\n\n        img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n        for y in range(filter_h):\n            y_max = y + stride * out_h\n            for x in range(filter_w):\n                x_max = x + stride * out_w\n                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n        return col","843d5764":"# \u6c60\u5316\u5c42\u7684\u521d\u59cb\u5316\u7c7b\nclass MaxPool:\n    \"\"\"MaxPooling\u5c42\n\n       Parameters:\n            kernel_size: default 2.\n            stride: default 2.\n            padding: default 0.\n        \"\"\"\n    def __init__(self, kernel_size: int=2,\n                 stride: int=2, padding: int=0):\n        '''\n        input:\n            kernel_size: default 2.\n            stride: default 2.\n            padding: default 0.\n        '''\n        self.pool_h = kernel_size\n        self.pool_w = kernel_size\n        self.stride = stride\n        self.pad = padding\n\n        \n    def forward(self, x):\n        \"\"\"\n        input:\n            x: input of shape (B, C, H_in, W_in).\n        output:\n            out: output of shape (B, C, H_out, W_out).\n        \"\"\"\n        \n        def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_data.shape\n            out_h = (H + 2 * pad - filter_h) \/\/ stride + 1\n            out_w = (W + 2 * pad - filter_w) \/\/ stride + 1\n\n            img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n            col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n\n            col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n            return col\n\n        \n        N, C, H, W = x.shape\n        FN, C, FH, FW = 1, C, self.pool_h, self.pool_w\n        out_h = (H + 2 * self.pad - FH) \/\/ self.stride + 1\n        out_w = (W + 2 * self.pad - FW) \/\/ self.stride + 1\n        col = im2col(x, FH, FW, self.stride, self.pad)\n        col = col.reshape((N*out_h*out_w*C, -1))\n        col = np.max(col, axis=-1)\n        col = col.reshape(N, out_h, out_w, C)\n        col = col.transpose(0, 3, 1, 2)\n        return col\n\n\n    def backward(self, dy):\n        \"\"\"\n        input:\n            dy: output delta of shape (B, C, H_out, W_out).\n        output:\n            out: input delta of shape (B, C, H_in, W_in).\n        \"\"\"\n        def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n            N, C, H, W = input_shape\n            out_h = (H + 2 * pad - filter_h) \/\/ stride + 1\n            out_w = (W + 2 * pad - filter_w) \/\/ stride + 1\n            col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n\n            img = np.zeros((N, C, H + 2 * pad + stride - 1, W + 2 * pad + stride - 1))\n            for y in range(filter_h):\n                y_max = y + stride * out_h\n                for x in range(filter_w):\n                    x_max = x + stride * out_w\n                    img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n\n            return img[:, :, pad:H + pad, pad:W + pad]\n        \n        \n        dout = dy\n        dout = dout.transpose(0, 2, 3, 1)\n        pool_size = self.pool_h * self.pool_w\n        dmax = np.zeros((dout.size, pool_size))\n        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n        dmax = dmax.reshape(dout.shape + (pool_size,))\n        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n        return dx\n\n\n","184aa2d6":"class BatchNorm1d:\n    '''\n    BN\u5c42\u5f53\u65f6\u53c2\u8003\u4e86\u4e00\u4e2a\u5916\u56fd\u4eba\u7684\u535a\u5ba2\uff0c\u5730\u5740\uff1ahttps:\/\/kratzert.github.io\/2016\/02\/12\/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\n    \u540e\u6765\u7684\u4ee3\u7801\u4e5f\u662f\u6839\u636e\u4ed6\u7684\u4f2a\u4ee3\u7801\u4e00\u6b65\u4e00\u6b65\u5b9e\u73b0\u7684\uff0c\u6574\u4f53\u5b9e\u73b0\u6bd4\u8f83\u4e71\uff0c\u56e0\u4e3a\u53ea\u6709\u8fd9\u4e2a\u4ee3\u7801\u4e0d\u662f\u6211\u81ea\u5df1\u5b8c\u5168\u5199\u7684\uff0c\u5982\u4e0b\u6240\u793a\uff1a\n    '''\n    def __init__(self, length: int, momentum: float=0.9):\n        \"\"\"Module which applies batch normalization to input.\n\n        Args:\n            length: L from expected input shape (N, L).\n            momentum: default 0.9.\n        \"\"\"\n        super(BatchNorm1d, self).__init__()\n\n        L = length\n        self.gamma = tensor.ones(L) # initialize the parameters\n        self.beta = tensor.zeros(L)  # same as up\n        self.momentum = momentum\n        self.eps = 1e-5\n        self.bn_param = {}\n        self.bn_param['running_mean'] = tensor.zeros(L)\n        self.bn_param['running_var'] = tensor.ones(L)\n        self.x_hat = None\n        self.var = None\n        self.avg = None\n        self.vareps = None\n        ...\n\n        # End of todo\n\n    def forward(self, x):\n        \"\"\"Forward propagation of batch norm module.\n\n        Args:\n            x: input of shape (N, L).\n        Returns:\n            out: output of shape (N, L).\n        \"\"\"\n        # https:\/\/blog.csdn.net\/weixin_39228381\/article\/details\/107896863\n        # https:\/\/zhuanlan.zhihu.com\/p\/196277511\n        \n        running_mean = self.bn_param['running_mean']\n        running_var = self.bn_param['running_var']\n        \n        N, L = x.shape  # get the batch and the length of a sample\n        self.avg = np.sum(x, axis=0) \/ N # get the every sample's average\n        self.var = np.sum((x - np.tile(self.avg, (N, 1))) ** 2, axis=0) \/ N  # get the every sample's variance\n        self.xmu = x - self.avg\n        self.vareps = (self.var + self.eps) ** 0.5   # get the Denominators\n        self.x_hat = (x - np.tile(self.avg, (N, 1))) \/ np.tile(self.vareps, (N, 1))  # get the normalized sequence \n        \n        out = self.gamma * self.x_hat + self.beta\n        \n        running_mean = self.momentum * running_mean + (1 - self.momentum) * self.avg\n        running_var = self.momentum * running_var + (1 - self.momentum) * self.var\n        self.bn_param['running_mean'] = running_mean\n        self.bn_param['running_var'] = running_var\n        \n        return out\n        ...\n\n        # End of todo\n#     mu = self.avg\n#     xmu = x - mu\n#     sq = xmu ** 2 \n#     var = 1. \/ N * np.sum(sq, axis=0)\n#     sqrtvar = np.sqrt(var + eps)\n#     ivar = 1 \/ sqrtvar\n#     x_hat = xmu * ivar\n#     gammax = gamma * x_hat\n        \n    def backward(self, dy):\n        \"\"\"Backward propagation of batch norm module.\n\n        Args:\n            dy: output delta of shape (N, L).\n        Returns:\n            dx: input delta of shape (N, L).\n        \"\"\"\n\n        # TODO Implement backward propogation\n        # of 1d batchnorm module.\n#         N, L = dy.shape\n#         var_plus_eps = self.vareps\n#         self.gamma.grad = np.sum(self.x_hat * dy, axis=0)\n#         self.beta.grad = np.sum(dy, axis=0)\n        \n#         dx_hat = dy * self.gamma   # x_hat's grad\n#         x_hat = self.x_hat\n\n# #         dx = N * dx_hat - np.sum(dx_hat, axis=0) + (1.0\/N) * np.sum(dx_hat, axis=0) * np.sum(dx_hat * x_hat, axis=0) - x_hat * np.sum(dx_hat * x_hat, axis=0) \n# #         dx *= (1 - 1.0\/N) \/ var_plus_eps\n        \n#         dx = dx_hat * (1 - 1. \/ N) * (1. \/ var_plus_eps) * (1 - 1. \/ (N * self.var) * self.xmu ** 2)\n#         return dx\n\n        xhat,gamma,xmu,ivar,sqrtvar,var,eps = self.x_hat, self.gamma, self.xmu, self.vareps, 1 \/ self.vareps, self.var, self.eps\n\n        #get the dimensions of the input\/output\n        N,D = dout.shape\n\n        #step9\n        dbeta = np.sum(dout, axis=0)\n        dgammax = dout #not necessary, but more understandable\n\n        #step8\n        dgamma = np.sum(dgammax*xhat, axis=0)\n        dxhat = dgammax * gamma\n\n        #step7\n        divar = np.sum(dxhat*xmu, axis=0)\n        dxmu1 = dxhat * ivar\n\n        #step6\n        dsqrtvar = -1. \/(sqrtvar**2) * divar\n\n        #step5\n        dvar = 0.5 * 1. \/np.sqrt(var+eps) * dsqrtvar\n\n        #step4\n        dsq = 1. \/N * np.ones((N,D)) * dvar\n\n        #step3\n        dxmu2 = 2 * xmu * dsq\n\n        #step2\n        dx1 = (dxmu1 + dxmu2)\n        dmu = -1 * np.sum(dxmu1+dxmu2, axis=0)\n\n\n        #step1\n        dx2 = 1. \/N * np.ones((N,D)) * dmu\n        \n        #step0\n        dx = dx1 + dx2\n\n        return dx","7ccf0e8b":"# \u6fc0\u6d3b\u51fd\u6570\u7684\u521d\u59cb\u5316\u7c7b\nclass Sigmoid():\n    '''\n    Parameter:\n    z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u7684\u77e9\u9635\n    '''\n    def __init__(self):\n        self.z = None\n    \n    \n    # Sigmoid\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u5411\u4f20\u64ad\u3002\n    def forward(self, y):\n        '''\n        input: \n        y:\u5168\u8fde\u63a5\u5c42\u524d\u5411\u4f20\u64ad\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        output:\n        z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        '''\n        z = np.exp(y) \/ (1 + np.exp(y)) # \u5229\u7528np.exp\u76f4\u63a5\u5bf9\u77e9\u9635\u8fd0\u7b97\n        self.z = z   # \u8d4b\u503c\n        return z  # \u8fd4\u56de\u77e9\u9635\n    \n    # sigmoid\u7684\u53cd\u5411\u4f20\u64ad\n    def backward(self, dout):\n        '''\n        input: \n        dout\uff1a\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8esigmoid\u8f93\u51fa\u7684\u68af\u5ea6\n        output:\n        dz:\u76f8\u5bf9\u4e8e\u77e9\u9635y\u5f97\u5230\u7684\u68af\u5ea6\n        '''\n        dz = dout * self.z * (np.ones(self.z.shape) - self.z)\n        return dz\n\nclass Relu:\n    def __init__(self):\n        '''\n        Parameter:\n        z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u7684\u77e9\u9635\n        '''\n        self.mask = None\n    \n    # Relu\u6fc0\u6d3b\u51fd\u6570\u7684\u524d\u5411\u4f20\u64ad\u3002\n    def forward(self, y):\n        '''\n        input: \n        y:\u5168\u8fde\u63a5\u5c42\u524d\u5411\u4f20\u64ad\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        output:\n        z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        '''\n        self.mask = (y <= 0)   # \u5f97\u5230\u5173\u4e8ey\u5927\u4e8e\u5c0f\u4e8e0\u7684\u771f\u503c\u7684\u77e9\u9635\n        z = y.copy()       # \u6df1\u5ea6\u62f7\u8d1d\u4e00\u4e2ay\u77e9\u9635\n        z[self.mask] = 0   # \u5c06\u5c0f\u4e8e\u96f6\u7684\u503c\u8d4b\u4e3a0\n        return z   # \u8fd4\u56de\u77e9\u9635\n\n    def backward(self, dout):\n        '''\n        input: \n        dout\uff1a\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8erelu\u8f93\u51fa\u7684\u68af\u5ea6\n        output:\n        dz:\u76f8\u5bf9\u4e8e\u77e9\u9635y\u5f97\u5230\u7684\u68af\u5ea6\n        '''\n        dout[self.mask] = 0\n        dz = dout\n        return dz\n    \n\nclass Cos:\n    def __init__(self):\n        '''\n        Parameter:\n        z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u7684\u77e9\u9635\n        '''\n        self.z = None\n        \n    def forward(self, y):\n        '''\n        input: \n        y:\u5168\u8fde\u63a5\u5c42\u524d\u5411\u4f20\u64ad\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        output:\n        z:\u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        '''\n        self.z = y\n        return np.cos(y)\n    \n    def backward(self, dout):\n        '''\n        input: \n        dout\uff1a\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8erelu\u8f93\u51fa\u7684\u68af\u5ea6\n        output:\n        dz:\u76f8\u5bf9\u4e8e\u77e9\u9635y\u5f97\u5230\u7684\u68af\u5ea6\n        '''        \n        return dout * np.sin(-self.z)\n    \nclass Tanh:\n    \n    def __init__(self):\n        '''\n        Parameter:\n         x: \u8f93\u5165\u7684\u77e9\u9635\n        '''\n        self.x = None\n    \n    def forward(self, x):\n        '''\n        input: \n        x:\u5168\u8fde\u63a5\u5c42\u524d\u5411\u4f20\u64ad\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        output:\n        \u6fc0\u6d3b\u51fd\u6570\u4f5c\u7528\u540e\u5f97\u5230\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a(B, N)\n        '''\n        self.x = x\n        return np.tanh(x)\n\n    def backward(self, dout):\n        '''\n        input: \n        dout\uff1a\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8erelu\u8f93\u51fa\u7684\u68af\u5ea6\n        output:\n        dz:\u76f8\u5bf9\u4e8e\u77e9\u9635y\u5f97\u5230\u7684\u68af\u5ea6\n        '''\n        return dout * (1 - np.tanh(self.x) ** 2)","8216724b":"# \u8bef\u5dee\u635f\u5931\u51fd\u6570\u7684\u7c7b\u5b9a\u4e49\uff08Cross Entropy+softmax)\n# \u6fc0\u6d3b\u51fd\u6570softmax\ndef softmax(y):\n    '''\n    input:\n    y:\u6700\u7ec8\u5f97\u5230\u7684\u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u77e9\u9635\n    output:\n    \u5c06\u5176\u4f7f\u7528softmax\u5f52\u4e00\u5316\u8fd4\u56de\u5904\u7406\u540e\u7684\u77e9\u9635\uff08\u5229\u4e8e\u8ba1\u7b97\u635f\u5931\u51fd\u6570\uff09\n    '''\n    y = y - np.max(y, axis=1, keepdims=True)     # \u9632\u6b62\u4ea7\u751fexp\u6ea2\u51fa\u7684\u5371\u9669\uff0c\u6240\u4ee5\u6bcf\u4e00\u884c\u90fd\u51cf\u53bb\u6700\u5927\u503c\uff0c\u4e14\u7531\u52a0\u51cf\u503c\u6027\u8d28\u6613\u5f97\u4e0d\u4f1a\u5bf9\u503c\u4ea7\u751f\u5f71\u54cd\n    return np.exp(y) \/ np.sum(np.exp(y), axis=1, keepdims=True)   # \u8fd4\u56desoftmax\u5904\u7406\u540e\u77e9\u9635\uff0c\u5229\u4e8e\u8fdb\u4e00\u6b65\u8ba1\u7b97\u635f\u5931\u51fd\u6570\n\n# \u7c7b\u5b9a\u4e49\nclass SoftmaxWithLoss():\n    '''\n    Parameter:\n    y : \u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u77e9\u9635\uff0c\u9700\u8981\u8fdb\u4e00\u6b65softmax\u5904\u7406\u5e76\u5229\u7528\u5176\u6c42\u51fa\u8bef\u5dee\u635f\u5931\uff0c\u5f62\u72b6\u4e3a(B, 10)\n    label: \u771f\u5b9e\u6807\u7b7e\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a (B\uff0c 1)\n    '''\n    \n    def __init__(self):\n        self.loss = None\n        self.z = None\n        self.label = None\n    \n    # SoftMax + Cross Entropy\u7684\u524d\u5411\u4f20\u64ad\n    def forward(self, y, label):\n        '''\n        input:\n        y : \u9884\u6d4b\u8f93\u51fa\u7ed3\u679c\u77e9\u9635\uff0c\u9700\u8981\u8fdb\u4e00\u6b65softmax\u5904\u7406\u5e76\u5229\u7528\u5176\u6c42\u51fa\u8bef\u5dee\u635f\u5931\uff0c\u5f62\u72b6\u4e3a(B, 10)\n        label: \u771f\u5b9e\u6807\u7b7e\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a (B\uff0c 1)\n        output:\n        loss: \u4ea4\u53c9\u71b5\u635f\u5931\n        '''\n        z = softmax(y)      # \u4f7f\u7528\u6fc0\u6d3b\u51fd\u6570\u5c06\u8f93\u51fa\u77e9\u9635\u5f52\u4e00\u5316\n        batch_size = z.shape[0]   # \u5f97\u5230batch_size\n        loss = -np.sum(np.log(z[np.arange(batch_size), label])) \/ batch_size    # \u6c42\u51fa\u5e73\u5747\u635f\u5931\u8bef\u5dee\u503c\uff0c\u4f7f\u7528\u4ea4\u53c9\u71b5\uff0c\u5229\u7528one-hot\u7279\u6027\u5f97\u5230\u6bcf\u7ec4\u8f93\u5165\u7684log\u503c\n#                 loss = -np.sum(np.log(z[np.arange(batch_size), t] + 1e-7)) \/ batch_size\n        self.loss = loss  # \u8bb0\u5f55\u635f\u5931\u503c\n        self.z = z     \n        self.label = label    # \u5b58\u50a8\u8bb0\u5f55\n        return loss    # \u8fd4\u56de\u8bef\u5dee\u635f\u5931\n    \n    # SoftMax + Cross Entropy\u7684\u53cd\u5411\u4f20\u64ad\n    def backward(self):\n        '''\n        output:\n        \u4ea4\u53c9\u71b5+softmax\u68af\u5ea6\n        '''\n        batch_size = self.z.shape[0]  # \u5f97\u5230batch_size\n        dz = np.copy(self.z)       # \u6df1\u62f7\u8d1d\n        for label_, z_ in zip(self.label, dz):   # \u7531\u6c42\u68af\u5ea6+onehot\u7f16\u7801\u63a8\u51fa\u4ec5\u9700\u5728\u771f\u5b9e\u503c\u6240\u5728\u4f4d\u7f6e\u51cf1\u5373\u5f97\u68af\u5ea6\n            z_[label_] -= 1\n        dz \/= batch_size   # \u53d6\u5e73\u5747\n        return dz   # \u8fd4\u56de\u68af\u5ea6","3d7d6173":"# \u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\nclass Network:\n    # \u521d\u59cb\u5316\n    def __init__(self, input_size, hidden_size1, hidden_size2, output_size=10, lr=0.1):\n        '''\n        Parameters:\n        input_size, hidden_size1, hidden_size2, output_size:\n        \u5206\u522b\u4e3a\u8f93\u5165\u5c42\u795e\u7ecf\u5143\u4e2a\u6570\u3001\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u6570\u3001\u9690\u85cf\u5c42\u795e\u7ecf\u5143\u4e2a\u6570\u3001\u8f93\u51fa\u5c42\u795e\u7ecf\u5143\u4e2a\u6570\u6570(\u624b\u5199\u6570\u5b57\u8bc6\u522b\u9ed8\u8ba4\u4e3a10), \u5b66\u4e60\u7387(\u9ed8\u8ba4\u4e3a0.1)\n        output:None\n        '''\n        W1 = np.random.randn(input_size, hidden_size1)  # \u968f\u673a\u521d\u59cb\u5316\u6743\u91cd\n        W2 = np.random.randn(hidden_size1, hidden_size2)\n        W3 = np.random.randn(hidden_size2, output_size)\n        b1 = np.random.randn(hidden_size1)\n        b2 = np.random.randn(hidden_size2)\n        b3 = np.random.randn(output_size)\n        \n        \n        self.lr = lr     # \u5b66\u4e60\u7387\n        self.layer_1 = FullyConnected(W1, b1)\n        self.sigmoid_1 = Sigmoid()\n        self.layer_2 = FullyConnected(W2, b2)\n        self.sigmoid_2 = Sigmoid()\n        self.layer_last = FullyConnected(W3, b3)\n        self.loss = SoftmaxWithLoss()\n    \n    # \u795e\u7ecf\u7f51\u7edc\u524d\u5411\u4f20\u64ad\n    def forward(self, x, label):\n        '''\n        input:\n        x: \u5f62\u72b6\u4e3a(B,N)\uff0c\u8f93\u5165\u7684\u539f\u59cb\u6570\u636e\uff0c B\u4e3a\u6279\u91cfBatch_size\n        label:\u8f93\u5165B\u4e2a\u6570\u636e\u7684\u5206\u7c7b\u7c7b\u522b\uff0c\u5f62\u72b6\u4e3a(B, 1)\n        output:\n        \u6700\u540e\u8f93\u51fa\u7684\u9884\u6d4b\u5411\u91cf\u4ee5\u53ca\u6211\u4eec\u5f97\u5230\u7684\u8bef\u5dee\n        '''\n        y1 = self.layer_1.forward(x)  # \u524d\u5411\u4f20\u64ad\uff0c\u4e00\u6b65\u6b65\u5f80\u540e\u8d70\n        z1 = self.sigmoid_1.forward(y1)\n        y2 = self.layer_2.forward(z1)\n        z2 = self.sigmoid_2.forward(y2)\n        y3 = self.layer_last.forward(z2)\n        loss = self.loss.forward(y3, label)\n        \n        return y3, loss\n    \n    \n    # \u795e\u7ecf\u7f51\u7edc\u53cd\u5411\u4f20\u64ad\n    def backward(self):\n        '''\n        input:None\n        output:\n        \u5404\u9879\u53c2\u6570\u7684\u68af\u5ea6\n        '''\n        d = self.loss.backward()     # \u53cd\u5411\u4f20\u64ad\uff0c\u4e00\u6b65\u6b65\u5f80\u524d\u8d70\uff0c\u548c\u524d\u5411\u5b8c\u5168\u76f8\u53cd\n        d = self.layer_last.backward(d)\n        d = self.sigmoid_2.backward(d)\n        d = self.layer_2.backward(d)\n        d = self.sigmoid_1.backward(d)\n        d = self.layer_1.backward(d)    # \u81f3\u6b64\uff0c\u6211\u4eec\u5355\u6b21\u53cd\u5411\u4f20\u64ad\u5b8c\u6210\u3002\n        \n        return self.layer_1.dW, self.layer_1.db, self.layer_2.dW, self.layer_2.db, self.layer_last.dW, self.layer_last.db  # \u5c06\u6bcf\u5c42\u95f4\u6743\u91cd\u7684W \u3001\u504f\u79fb\u91cfb\u68af\u5ea6\u8fd4\u56de\n        # \u8fd9\u4e00\u6b65\u53ea\u662f\u4e3a\u4e86\u65b9\u4fbf\u68c0\u67e5\u5c55\u793a\uff0c\u5e76\u6ca1\u6709\u5f88\u5927\u7684\u7528\u9014\n\n\n    # \u795e\u7ecf\u7f51\u7edc\u66f4\u65b0\u6743\u91cd\n    def refresh(self):\n        lr = self.lr\n        self.layer_1.W -= lr * self.layer_1.dW\n        self.layer_1.b -= lr * self.layer_1.db\n        self.layer_2.W -= lr * self.layer_2.dW\n        self.layer_2.b -= lr * self.layer_2.db\n        self.layer_last.W -= lr * self.layer_last.dW\n        self.layer_last.b -= lr * self.layer_last.db\n    \n    # \u8bad\u7ec3\u6a21\u578b\u5e76\u5224\u65ad\u6b63\u786e\u7387\n    def fit_pred(self, train_images, train_labels, test_images, test_labels, Epochs=5, batch_size=100, losses=None, accuracy=None):\n        '''\n        input\uff1a\n        train_images:\u8bad\u7ec3\u96c6\n        test_labels:\u6d4b\u8bd5\u96c6\n        output:None\n        '''\n        samples_num = train_images.shape[0]   # \u5f97\u5230\u8bad\u7ec3\u96c6\u6570\u91cf\n        pred, pred_loss, right_rate = self.predict(test_images, test_labels)    # \u8ba1\u7b97\u6d4b\u8bd5\u96c6\u7cbe\u5ea6\n        print(\"Initial Test -- Average loss:{:.4f}, Accuracy:{:.3f}\\n\".format(pred_loss, right_rate))  # \u7b2c\u4e00\u6b21\u8bad\u7ec3\u73b0\u5728\u8bad\u7ec3\u524d\u770b\u770b\u51c6\u786e\u7387\n        if losses is not None and accuracy is not None:\n            losses.append(pred_loss)\n            accuracy.append(right_rate)\n        for epoch in range(1, Epochs + 1):   # \u5728\u8bad\u7ec3\u96c6\u91cc\u9762\u8dd15\u6b21\n            i = 0\n            while i < samples_num:\n                self.forward(train_images[i:i+batch_size], train_labels[i:i+batch_size])  # \u6bcf\u6b21\u8bad\u7ec3batch_size\u4e2a\u6837\u672c\n                self.backward()         # \u53cd\u5411\u4f20\u64ad\n                self.refresh()          # \u66f4\u65b0\u53c2\u6570\n#                 print(\"Train Epoch: {}\\t batch_size_index:{} Loss:{:.6f}\".format(epoch, i+1, self.loss.loss))\n                i += batch_size\n                \n#             self.lr = (0.95 ** epoch) * self.lr     # \u66f4\u65b0\u5b66\u4e60\u7387\uff0c\u9632\u6b62\u5176\u56e0\u4e3a\u5b66\u4e60\u7387\u8fc7\u5927\u800c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u4e0b\u964d\u3002\n            print(\"Train Epoch: {}\\t Loss:{:.6f}\".format(epoch, self.loss.loss))\n            pred, pred_loss, right_rate = self.predict(test_images, test_labels)    # \u8ba1\u7b97\u6d4b\u8bd5\u96c6\u7cbe\u5ea6\n            print(\"Test -- Average loss:{:.4f}, Accuracy:{:.3f}\\n\".format(pred_loss, right_rate))\n            if losses is not None and accuracy is not None:\n                losses.append(pred_loss)\n                accuracy.append(right_rate)\n        return losses, accuracy\n    \n    # \u9884\u6d4b\u6a21\u578b\n    def predict(self, test_images, test_labels):\n        '''\n        '''\n        pred, loss = self.forward(test_images, test_labels)  # \u9884\u6d4b\u503c\u548c\u635f\u5931\n        pred = np.argmax(pred, axis=1)   # \u6c42\u51fa\u9884\u6d4b\u6807\u7b7e\n        return pred, loss, right_rate(pred, test_labels)   # \u8fd4\u56de\u9884\u6d4b\u503c\u5411\u91cf\u548c\u635f\u5931\u8bef\u5dee\u4ee5\u53ca\u6b63\u786e\u7387\u3002\n    def inference_(self, inference_images):\n        x = inference_images\n        y1 = self.layer_1.forward(x)  # \u524d\u5411\u4f20\u64ad\uff0c\u4e00\u6b65\u6b65\u5f80\u540e\u8d70\n        z1 = self.sigmoid_1.forward(y1)\n        y2 = self.layer_2.forward(z1)\n        z2 = self.sigmoid_2.forward(y2)\n        y3 = self.layer_last.forward(z2)\n        return y3","ecc82daa":"# \u8ba1\u7b97\u9884\u6d4b\u6570\u636e\u6b63\u786e\u7387\ndef right_rate(pred_label, label):\n    '''\n    input:\n    pred_label:\u9884\u6d4b\u7684\u7ed3\u679c\u6570\u7ec4\n    label:\u5b9e\u9645\u7684\u7c7b\u522b\u6570\u7ec4\n    output:\n    rate:\u6b63\u786e\u7387\n    '''\n    diff = pred_label - label\n    return diff.tolist().count(0) \/ len(diff)","34b730fc":"model = Network(784, 200, 60, lr=0.3)","69695f29":"# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)\n# model.lr = 0.03\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=10)\n# model.lr = 0.01\n# model.fit_pred(test_images,test_labels, test_images, test_labels, Epochs=20)","ee7105d5":"# df_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\n# train_img = df_train.values[:, 1:] \/ 255.0\n# train_lab = df_train.values[:, 0]","d3ef57cb":"# train_lab.shape, train_img.shape","eef240bf":"# model.lr = 0.05\n# model.fit_pred(train_img,train_lab, test_images, test_labels, Epochs=30)","228d76e4":"# df_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","c9fac0b0":"# inference_images = df_test.values \/ 255.0","c7b534c3":"# inference_images.shape","832cd289":"# pred = model.inference_(inference_images)\n# pred = np.argmax(pred, axis=1)   # \u6c42\u51fa\u9884\u6d4b\u6807\u7b7e","1df30f30":"# from pandas import Series,DataFrame\n# data = {'ImageId':Series(list(range(1, 28001))),\n#        'Label':Series(pred)}\n# submit = DataFrame(data)\n# print(submit)","d7474811":"# \u8f6c\u5316\u4e3acsv\u6587\u4ef6\n# submission = pd.concat([submit['ImageId'], submit['Label']], axis=1)\n# submission.to_csv('\/kaggle\/working\/submission_2.csv', index=False)","492a7cdc":"sig_loss = []\nsig_acc = []\nrelu_loss = []\nrelu_acc = []\ncos_loss = []\ncos_acc = []\ntanh_loss = []\ntanh_acc = []","307fa626":"all_loss = {}\naccuracy = {}","8cbe072c":"# \u5f53\u6fc0\u6d3b\u51fd\u6570\u662fsigmoid\u65f6\nmodel = Network(784, 200, 60, lr=0.3)\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=sig_loss, accuracy=sig_acc)","6ebc0bec":"# \u753b\u56fe\u90e8\u5206\nall_loss['sigmoid'] = sig_loss\naccuracy['sigmoid'] = sig_acc","8d02e756":"# \u5f53\u6fc0\u6d3b\u51fd\u6570\u65f6relu\u65f6\nmodel = Network(784, 200, 60, lr=0.1)  # \u8fd9\u91cc\u5c1d\u8bd5\u8fc7\uff0c\u53d1\u73b0\u5982\u679c\u5b66\u4e60\u7387\u8c03\u52300.3Relu\u6a21\u578b\u4e0d\u6536\u655b,\u5373\u4f7f\u5b66\u4e60\u7387\u8c03\u7684\u6bd4\u8f83\u4f4e\u6709\u65f6\u4e5f\u4e0d\u6536\u655b\uff0c\u5b9e\u9645\u4e0a\u662f\u7531\u4e8eRelu\u7684\u68af\u5ea6\u592a\u9ad8\u6240\u6307\nmodel.sigmoid_1 = Relu()\nmodel.sigmoid_2 = Relu()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=relu_loss, accuracy=relu_acc)","49d02400":"# \u753b\u56fe\u90e8\u5206\nall_loss['relu'] = relu_loss\naccuracy['relu'] = relu_acc","a3334857":"# \u5f53\u6fc0\u6d3b\u51fd\u6570\u65f6Cos\u65f6\nmodel = Network(784, 200, 60, lr=0.5)  # \u5c1d\u8bd5\u8fc7\u5404\u79cd\u5b66\u4e60\u7387\uff0c\u90fd\u4e0d\u884c\uff0c\u53ef\u80fd\u662f\u51fd\u6570\u4ee3\u7801\u6709\u95ee\u9898\nmodel.sigmoid_1 = Cos()\nmodel.sigmoid_2 = Cos()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=cos_loss, accuracy=cos_acc)\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)","8decfda9":"# \u753b\u56fe\u90e8\u5206\nall_loss['cos'] = cos_loss\naccuracy['cos'] = cos_acc","60c37a19":"# \u5f53\u6fc0\u6d3b\u51fd\u6570\u65f6Tanh\u65f6\nmodel = Network(784, 200, 60, lr=0.2) \nmodel.sigmoid_1 = Tanh()\nmodel.sigmoid_2 = Tanh()\nmodel.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30, losses=tanh_loss, accuracy=tanh_acc)\n# model.fit_pred(train_images,train_labels, test_images, test_labels, Epochs=30)","68695eec":"# \u753b\u56fe\u90e8\u5206\nall_loss['tanh'] = tanh_loss\naccuracy['tanh'] = tanh_acc","55195de8":"all_loss, accuracy","71340a30":"# \u8bbe\u7f6e\u989c\u8272\ncolors = ['blue', 'yellow', 'green', 'red']\ncolors = dict(zip(all_loss.keys(), colors))\nprint(colors)","b6d4180b":"epoch = np.arange(1, 32)\nplt.figure(figsize=(10, 7))\nplt.title('The loss of Every model with epoch')\nfor name, loss in all_loss.items():\n    # print(name, colors[name])\n    plt.plot(epoch, loss, color=colors[name], label=name, linewidth=1.5)\n\n    for x, y in zip(epoch, loss):\n        plt.text(x, y, '%3s'%round(y, 1), ha='center', va='bottom')\n\n\nplt.legend(loc='best')\nplt.savefig('\/kaggle\/working\/all_loss.png')\nplt.show()","a6e0b871":"epoch = np.arange(1, 32)\nplt.figure(figsize=(29, 20))\nfor index, data in enumerate(all_loss.items(), 1):\n    name, loss = data\n    plt.subplot(2, 2, index)\n    plt.title(f'The loss of model {name} with epoch')\n    plt.plot(epoch, loss, color=colors[name], label=name, linewidth=1.5)\n    plt.legend(loc='best')\n    \nplt.savefig('\/kaggle\/working\/The loss of every model with epoch')\nplt.show()","cb29c091":"epoch = np.arange(1, 32)\nplt.figure(figsize=(10, 7))\nplt.title('The Accuracy of Every model with epoch')\nfor name, rate in accuracy.items():\n    print(name, colors[name])\n    plt.plot(epoch, rate, color=colors[name], label=name, linewidth=1.5)\n\n    for x, y in zip(epoch, rate):\n        plt.text(x, y, '%3s'%round(y, 2), ha='center', va='bottom')\n\n\nplt.legend(loc='best', borderpad=3)\nplt.savefig('\/kaggle\/working\/all_accuracy.png')\nplt.show()","41b4e531":"epoch = np.arange(1, 32)\nfor name, rate in accuracy.items():\n    plt.figure()\n    plt.title(f'The Accuracy of model {name} with epoch')\n    plt.plot(epoch, rate, color=colors[name], label=name)\n    plt.legend(loc='best')\n    plt.savefig('\/kaggle\/working\/%s_accuracy.png'%name)\n    plt.show()","f8ca01ba":"## loss\u56fe","11a8de87":"### Cos function","43b21c57":"**I am a student from HUST, [Huazhong University of Science and Technology](https:\/\/en.wikipedia.org\/wiki\/Huazhong_University_of_Science_and_Technology). This work will be my homework for the lesson\uff0ccalled AI introduction by Associate Professor Yan Jin.**","76fbf74e":"### tanh","24f8d1f4":"# Visualization","0e36976d":"### sigmoid","9d83d62b":"# The model of every activation\n- \u6bcf\u4e2a\u6a21\u578b\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684loss\u66f2\u7ebf\n    * sigmoid\n    * relu\n    * Cos\n    * tanh\n- \u6bcf\u4e2a\u6a21\u578b\u4e0d\u540c\u7684\u5b66\u4e60\u7387\u7684\u66f2\u7ebf\uff08\u9650\u5b9a20\u4e2aepoch\uff09\n- SGD\u548cAdam\u7b97\u6cd5","e9c1a6eb":"# source codes","e91a7039":"### relu","5fbbce97":"### \u6bcf\u4e2a\u6a21\u578b\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570","c0323d84":"## accuracy","2a4cd49b":"**That' s all! Wish you good luck!**"}}