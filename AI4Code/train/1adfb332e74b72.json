{"cell_type":{"b79177db":"code","7929e66f":"code","e3f4b485":"code","cd91dd0c":"code","5559a436":"code","d5ce54dc":"code","f85e0731":"code","565e5a5b":"code","755d46b6":"code","89f02dd5":"code","8801a381":"code","dda97a9f":"code","cb0da8dd":"code","9accc99b":"code","7f7bbb03":"code","aeb9003b":"code","2762b29e":"code","47aa9f4f":"code","3bdfa846":"code","1c2d7abc":"code","abeab186":"code","e5419730":"code","46cd8252":"code","5c9fe292":"code","b64510df":"code","eec8ea16":"code","3eff704c":"code","1746ec76":"code","b8763802":"code","c62b47c4":"code","fd04cf97":"code","d6920752":"code","cc4fef80":"code","0e55924a":"code","d6d1bcbb":"code","f0c36019":"code","bcbd725d":"code","faa81ff4":"markdown","eac448b9":"markdown","f7367465":"markdown","f7bc5d17":"markdown","78b53188":"markdown","8e25f19d":"markdown"},"source":{"b79177db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7929e66f":"import pandas as pd\nfrom sklearn.datasets import load_iris\niris= load_iris()","e3f4b485":"df=pd.DataFrame(iris.data, columns=iris.feature_names)\ndf.head()","cd91dd0c":"df['target']=iris.target\ndf.head()","5559a436":"df[df.target==1]","d5ce54dc":"df[df.target==2]","f85e0731":"df['flowername']=df.target.apply(lambda x: iris.target_names[x])\ndf.head()","565e5a5b":"df0 = df[:50]\ndf1 = df[50:100]\ndf2 = df[100:]","755d46b6":"import matplotlib.pyplot as plt\n%matplotlib inline","89f02dd5":"#Sepal length vs Sepal Width\nplt.figure(figsize=(15,7))\nplt.xlabel('Sepal Length')\nplt.ylabel('Sepal Width')\nplt.scatter(df0['sepal length (cm)'], df0['sepal width (cm)'],color=\"green\",marker='+',label='setosa')\nplt.scatter(df1['sepal length (cm)'], df1['sepal width (cm)'],color=\"blue\",marker='.',label='versicolor')\nplt.scatter(df2['sepal length (cm)'],df1['sepal width (cm)'], color='red',marker='*',label='virginica')\nplt.legend()\nplt.show()","8801a381":"#Petal length vs Pepal Width \nplt.figure(figsize=(15,7))\nplt.xlabel('Petal Length')\nplt.ylabel('Petal Width')\nplt.scatter(df0['petal length (cm)'], df0['petal width (cm)'],color=\"green\",marker='+', label='setosa')\nplt.scatter(df1['petal length (cm)'], df1['petal width (cm)'],color=\"blue\",marker='.', label='versicolor')\nplt.scatter(df2['petal length (cm)'], df2['petal width (cm)'], color= 'red',marker='*',label='virginica')\nplt.legend()\nplt.show()","dda97a9f":"# Test, train split\nfrom sklearn.model_selection import train_test_split\nX= df.drop(['target','flowername'],axis='columns')\ny= df.target","cb0da8dd":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2, random_state=3)","9accc99b":"#Create KNN classification\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)","7f7bbb03":"knn.score(X_test,y_test)","aeb9003b":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=knn.predict(X_test)\nmatrix= confusion_matrix(y_test, y_pred)#confusion_matrix(truth, prediction)\nmatrix","2762b29e":"#Visualize the confusion matrix\nimport seaborn as sns\nplt.figure(figsize=(10,8))\nsns.heatmap(matrix, annot= True)\nplt.xlabel('predict')\nplt.ylabel('truth')","47aa9f4f":"#Classificaiton report\nfrom sklearn.metrics import classification_report\nprint( classification_report(y_test,y_pred))","3bdfa846":"# load digits data set\nfrom sklearn.datasets import load_digits\ndigits=load_digits()","1c2d7abc":"#print yhe keys and DECR of the dataset:\nprint(digits.keys())","abeab186":"print(digits.DESCR)","e5419730":"# print the shape of the images and data keys:\nprint(digits.images.shape)","46cd8252":"print(digits.data.shape)","5c9fe292":"#display digit 1010\nplt.figure(figsize=(10,7))\nplt.imshow(digits.images[1010], cmap=plt.cm.gray_r,interpolation='nearest')\nplt.show()","b64510df":"df=pd.DataFrame(digits.data, columns=digits.feature_names)\ndf.head()","eec8ea16":"df['target']=digits.target\ndf.head()","3eff704c":"#Train, Test split\nfrom sklearn.model_selection import train_test_split","1746ec76":"X=df.drop('target',axis='columns')\ny= df['target']","b8763802":"X_train, X_test,y_train, y_test =train_test_split(X,y, test_size= 0.3, random_state= 3)","c62b47c4":"#Create KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn= KNeighborsClassifier(n_neighbors=4)","fd04cf97":"knn.fit(X_train, y_train)","d6920752":"knn.score(X_test,y_test)","cc4fef80":"# Overfitting and Underfitting Curve\nimport numpy as np\nneighbors=np.arange(1,9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))","0e55924a":"#loop over different values of k\nfor i, k in enumerate(neighbors):\n    # set up a KNN Classifier with k neighbors: knn\n    knn= KNeighborsClassifier(n_neighbors=k)\n    # fit the classifier to the training data:\n    knn.fit(X_train, y_train)\n    # compute accuracy on the training set:\n    train_accuracy[i]= knn.score(X_train, y_train)\n    # compute accuracy on the testing set:\n    test_accuracy[i] = knn.score(X_test, y_test)\n# Generate plot:\nplt.figure(figsize=(10,7))\nplt.title('KNN: Varying Number of Neighbor')\nplt.plot(neighbors, test_accuracy, label='Test Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accurary')\nplt. show()\n    ","d6d1bcbb":"#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred= knn.predict(X_test)\nmatrix=confusion_matrix(y_test,y_pred)\nmatrix","f0c36019":"#Plot confusion matrix\nplt.figure(figsize=(15,7))\nsns.heatmap(matrix,annot=True)\nplt.xlabel('Predicted')\nplt.ylabel('Truth')\nplt.show()","bcbd725d":"# Classificaiton Report\nfrom sklearn.metrics import classification_report\nprint( classification_report(y_test,y_pred))","faa81ff4":"# <span style = 'color: green'>KNN ( K Nearest Neighbors) Classification: Machine Tutorial Using Python Sklearn\n## <h1><center>I. Case Study 1: Iris Data<\/center><\/h1>\n","eac448b9":"<h1><center>THE END<\/center><\/h1>","f7367465":"## Credit: DataCamp & Codebasics","f7bc5d17":"## <h1><center>II. Case Study 2: Digits Data<\/center><\/h1>\n\n","78b53188":"Interpretation:\n\nThere are 10 times the machine predicts the species is setora ( 0) and it's correct.\n\nThere are 9 times the machine predicts the species is versicolor( 1) and it's correct.\n\nThere are 10 times the machine predicts the species is virginica (2) and it's correct.\n\nThere are 1 time tha machine predict the species is virginica(2) and the true species is versiscolor(1)","8e25f19d":"The curve depicts the higher number of neighbor the less accuarcy of test and trainning data"}}