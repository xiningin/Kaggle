{"cell_type":{"82ae46f7":"code","58b9c006":"code","24b2d8bf":"code","de715f54":"code","a475a5a4":"code","af4bcd12":"code","3f3945ee":"code","5d42f578":"code","cec37b4d":"code","5b82aed2":"code","8d9f8f04":"code","740da244":"code","faf84877":"code","32f8eae7":"code","a8c60ade":"code","5ab012d4":"code","d7d4546f":"code","5693d63a":"code","e64fcebd":"code","41a3220e":"code","06301064":"code","a1eec85e":"code","cdbb5389":"code","6264d2f8":"code","bbe8abe4":"code","e81b3979":"code","19834867":"code","b8baa7b0":"code","1f4cbfab":"code","743113be":"code","f8df671a":"code","b4566bfa":"code","216b067a":"code","007b4879":"code","cca245d3":"code","1d3e1e2d":"code","b2847920":"code","948c614f":"code","8b8570ad":"code","b07bd34c":"code","6ff90ccf":"markdown","05aaa421":"markdown","a5bf1ef9":"markdown","0a11b56a":"markdown","dcffeba0":"markdown","1ca23a92":"markdown","cfb892bb":"markdown","cda46d1e":"markdown"},"source":{"82ae46f7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split,cross_val_score\nimport seaborn as sns\n\nfrom sklearn.cluster import KMeans\nfrom mpl_toolkits.mplot3d import Axes3D\nimport tensorflow as tf \nimport tensorflow_datasets as tfds\n\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom xgboost import XGBRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestClassifier","58b9c006":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","24b2d8bf":"data.describe(include='all')","de715f54":"#Classifying survivors based on Class\n\ndata.groupby('Pclass')['Survived'].mean()","a475a5a4":"#Classifying survivors based on Sex\ndata.groupby('Sex')['Survived'].mean()","af4bcd12":"#Classifying survivors based on Parents\/Children\ndata.groupby('Parch')['Survived'].mean()","3f3945ee":"#Classifying survivors based on Siblings\ndata.groupby('SibSp')['Survived'].mean()","5d42f578":"#Classifying survivors based on Embarking location\ndata.groupby('Embarked')['Survived'].mean()","cec37b4d":"\ndata[\"Age\"] = data[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndata['AgeGroup'] = pd.cut(data[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\ndata['AgeGroup']","5b82aed2":"#create a combined group of both datasets\ncombine = [data, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(data['Title'], data['Sex'])","8d9f8f04":"# Changing less used titles with more common ones\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ndata[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","740da244":"# Associating the various titles with numbers\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ndata.head()","faf84877":"# fill missing age with mode age group for each title\nmr_age = data[data[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = data[data[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = data[data[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = data[data[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = data[data[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = data[data[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n\n\nfor x in range(len(data[\"AgeGroup\"])):\n    if data[\"AgeGroup\"][x] == \"Unknown\":\n        data[\"AgeGroup\"][x] = age_title_mapping[data[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]","32f8eae7":"age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ndata['AgeGroup'] = data['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ndata.head()\n\n#dropping the Age feature for now, might change\ndata = data.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","a8c60ade":"# Defining the fare ranges\n\ndef new_fare(fare):\n    \"\"\"\n    This function maps Fare.\n    Categories are -> 0-8, 8-15, 15-31, 31+\n    \"\"\"\n    if fare <= 8:\n        return 0\n    if fare > 8 and fare <= 15:\n        return 1\n    if fare > 15 and fare <=31:\n        return 2\n    return 3","5ab012d4":"data['Embarked'].fillna('S',inplace=True) # Filling NA values in Embarked column","d7d4546f":"\ndef preprocess(train,test):\n    result = []\n    for df in [train,test]:\n        df = df.loc[df['Embarked'].dropna().index]\n        df['isAlone'] = (df['Parch'] + df['SibSp']).apply(lambda x: 1 if x==0 else 0)\n        df['Sex1'] = df['Sex'].map({\"male\": 0, \"female\": 1})\n        df['Embarked1'] = df['Embarked'].map({'C':2,'Q':3,'S':1})\n        df['Cabin1'] = df['Cabin'].apply(lambda x: 0 if str(x)=='nan' else 1)\n        df['Fare1'] = df['Fare'].apply(new_fare)\n        \n        df.drop(['Ticket', 'Cabin', 'Name','Sex','Embarked','Fare','PassengerId'], axis=1, inplace=True)\n        result.append(df)\n    return result","5693d63a":"train,test1 = preprocess(data,test)","e64fcebd":"x = train.drop(['Survived'],axis=1)\ny = train.Survived","41a3220e":"x\n","06301064":"label_X_train,label_X_valid,y_train,y_valid =  train_test_split(x,y, test_size=0.22,\n                                                      random_state=0)","a1eec85e":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(label_X_train, y_train)\ny_pred = gaussian.predict(label_X_valid)\nacc_gaussian = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_gaussian)\n","cdbb5389":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(label_X_train, y_train)\ny_pred = logreg.predict(label_X_valid)\nacc_logreg = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_logreg)","6264d2f8":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(label_X_train, y_train)\ny_pred = svc.predict(label_X_valid)\nacc_svc = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_svc)","bbe8abe4":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(label_X_train, y_train)\ny_pred = linear_svc.predict(label_X_valid)\nacc_linear_svc = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_linear_svc)","e81b3979":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(label_X_train, y_train)\ny_pred = decisiontree.predict(label_X_valid)\nacc_decisiontree = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_decisiontree)","19834867":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(label_X_train, y_train)\ny_pred = randomforest.predict(label_X_valid)\nacc_randomforest = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_randomforest)","b8baa7b0":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(label_X_train, y_train)\ny_pred = perceptron.predict(label_X_valid)\nacc_perceptron = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_perceptron)","1f4cbfab":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(label_X_train, y_train)\ny_pred = knn.predict(label_X_valid)\nacc_knn = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_knn)","743113be":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(label_X_train, y_train)\ny_pred = sgd.predict(label_X_valid)\nacc_sgd = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_sgd)","f8df671a":" #Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(label_X_train, y_train)\ny_pred = gbk.predict(label_X_valid)\nacc_gbk = round(accuracy_score(y_pred, y_valid) * 100, 2)\nprint(acc_gbk)","b4566bfa":"import xgboost as xgb\nfrom xgboost import XGBClassifier\n\ntrainx = xgb.DMatrix(label_X_train, label = y_train)\ntestx = xgb.DMatrix(label_X_valid, label = y_valid)\n\n\n\nxg = XGBClassifier()\nxg.fit(label_X_train,y_train)\ny_pred= xg.predict(label_X_valid)\nacc_xg = round(accuracy_score(y_pred,y_valid)*100,2)\nprint(acc_xg)\n","216b067a":"from sklearn.metrics import accuracy_score\n\ndef get_accuracy(X_train,X_valid,y_train,y_valid, max_depth=5, n_estimators=100, max_leaf_nodes=None):\n    model1 =  GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=1, \n                                   max_leaf_nodes=max_leaf_nodes)\n    model1.fit(X_train,y_train)\n    pred = model1.predict(X_valid)\n    return mean_absolute_error(y_valid, pred)","007b4879":"depths = [5, 10, 50, 100, 500]\nbest_accuracy, best_depth = -float('inf'), 0\nl=[]\nfor d in depths:\n    acc = get_accuracy(label_X_train,label_X_valid,y_train,y_valid, d)\n    print(f\"Depth:{d} \\t Accuracy:{acc}\")\n    l.append(acc)\n    if acc ==min(l):\n        best_accuracy, best_depth =acc,d\n        \n        \nprint(f\"Best accuracy is {best_accuracy} at depth {best_depth}\")","cca245d3":"num_leaf_nodes = [5, 10, 50, 100, 500]\nbest_accuracy, best_max_leaf_nodes = -float('inf'), None\nl1=[]\nfor leaf in num_leaf_nodes:\n    acc = get_accuracy(label_X_train,label_X_valid,y_train,y_valid, best_depth,leaf)\n    print(f\"LeafNodes:{leaf} \\t Accuracy:{acc}\")\n    l1.append(acc)\n    if acc==min(l1):\n        best_accuracy,best_max_leaf_nodes = acc,leaf\n        \nprint(f\"Best accuracy is {best_accuracy} at leaf node {best_max_leaf_nodes}\")","1d3e1e2d":"n_estimator_trees = [100, 200, 300, 400, 500]\nbest_accuracy, best_n_estimator = -float('inf'), 0\nl2=[]\nfor n_trees in n_estimator_trees:\n    acc = get_accuracy(label_X_train, label_X_valid, y_train, y_valid, max_depth=best_depth, max_leaf_nodes=best_max_leaf_nodes,\n                      n_estimators=n_trees)\n    print(f\"No. of trees: {n_trees} \\t Accuracy: {acc}\")\n    l2.append(acc)\n    if acc == min(l2):\n        best_accuracy, best_n_estimator = acc, n_trees\n        \nprint(f\"Best accuracy is {best_accuracy} at {best_n_estimator} no. of trees\")","b2847920":"final_model =  GradientBoostingClassifier(n_estimators=best_n_estimator, max_leaf_nodes=best_max_leaf_nodes,\n                                    max_depth=best_depth, random_state=1)\n\nfinal_model.fit(label_X_train,y_train)","948c614f":"\npreds = gbk.predict(test1)","8b8570ad":"pred = (np.round(preds).astype(int))","b07bd34c":"pred","6ff90ccf":"# Handling some NA values\n","05aaa421":"# Splitting the Dataset","a5bf1ef9":"# Classifying different types of survivors","0a11b56a":"# Results","dcffeba0":"# Preprocessing the dataset using the various functions used above","1ca23a92":"# Loading the Data","cfb892bb":"# Predicting the Score using various classifiers","cda46d1e":"**Combining the Train and Test Dataset**"}}