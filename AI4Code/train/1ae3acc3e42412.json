{"cell_type":{"6ec516f7":"code","a11cb10b":"code","1c5e6d21":"code","81f10bc0":"code","2c035c0c":"code","7025f1a9":"code","4350717f":"code","aa4c4527":"code","a27b27cc":"code","0ec0aba4":"code","2ac55ec0":"code","41396070":"code","e6685c92":"code","aee0a010":"code","163b54ca":"code","35f0e58c":"code","4b94b441":"code","fb495043":"code","3bd72d89":"code","cd493514":"code","ed6fd047":"code","ed695c27":"code","e73c3bb3":"code","7caad973":"code","d883f186":"code","ca789f85":"code","e7367012":"code","f4f2c712":"code","aa4efa1d":"code","b63dc6ec":"markdown","5c323cb4":"markdown","acdfd9d8":"markdown"},"source":{"6ec516f7":"from __future__ import division\nimport numpy as np\nfrom scipy import *\nfrom matplotlib import pyplot as gplt\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,5)\nfrom scipy import fftpack\nfrom matplotlib.colors import LightSource\nimport pandas as pd\n\ndef f(Y,x, N):\n    total = 0\n    for ctr in range(len(Y)):\n        total += Y[ctr] * (np.cos(x*ctr*2*np.pi\/N) + 1j*np.sin(x*ctr*2*np.pi\/N))\n    return real(total)\n\nidle_data1 = np.load(\"..\/input\/eeg8chanel\/data8\/idle\/1608706768.npy\")\nkanan_data2 = np.load(\"..\/input\/eeg8chanel\/data8\/kanan\/1608707012.npy\")\nkiri_data3 = np.load(\"..\/input\/eeg8chanel\/data8\/kiri\/1608707050.npy\")\nmaju_data4 = np.load(\"..\/input\/eeg8chanel\/data8\/maju\/1608706976.npy\")\n\n#year=tempdata[:,0]\nidle=idle_data1 [:,1]\n\n\n\n\nY=fft(idle)\nN=len(Y)\nprint(N)\n\nxs = range(N)\ngplt.plot(xs, [f(Y, x, N) for x in xs])\ngplt.show()\n\n","a11cb10b":"idle_data1 = np.load(\"..\/input\/eeg8chanel\/data8\/idle\/1608706768.npy\")\nkanan_data2 = np.load(\"..\/input\/eeg8chanel\/data8\/kanan\/1608707012.npy\")\nkiri_data3 = np.load(\"..\/input\/eeg8chanel\/data8\/kiri\/1608707050.npy\")\nmaju_data4 = np.load(\"..\/input\/eeg8chanel\/data8\/maju\/1608706976.npy\")\n\nD1=idle_data1[:,1]\nD2=kanan_data2[:,1]\nD3=kiri_data3[:,1]\nD4=maju_data4[:,1]\n\n\n\nD1=pd.DataFrame((D1))\nD1.to_csv('idle.csv')\nD1 = pd.read_csv('idle.csv')\nD1=pd.DataFrame(D1.values, columns = [\"label\", \"ch1\", \"ch2\", \"ch3\", \n                                          \"ch4\",\"ch5\",\"ch6\",\"ch7\",\"ch8\"])\nD1.loc[D1['label'] >-1, 'label'] = '1'\nD1.to_csv('idle.csv')\n\nD1\n","1c5e6d21":"D2=pd.DataFrame((D2))\nD2.to_csv('kanan.csv')\nD2 = pd.read_csv('kanan.csv')\nD2=pd.DataFrame(D2.values, columns = [\"label\", \"ch1\", \"ch2\", \"ch3\", \n                                          \"ch4\",\"ch5\",\"ch6\",\"ch7\",\"ch8\"])\nD2.loc[D2['label'] >-1, 'label'] = '2'\nD2.to_csv('kanan.csv')\nD2","81f10bc0":"D3= pd.DataFrame((D3))\nD3.to_csv('kiri.csv')\nD3 = pd.read_csv('kiri.csv')\nD3=pd.DataFrame(D3.values, columns = [\"label\", \"ch1\", \"ch2\", \"ch3\", \n                                          \"ch4\",\"ch5\",\"ch6\",\"ch7\",\"ch8\"])\nD3.loc[D3['label'] >-1, 'label'] = '3'\nD3.to_csv('kiri.csv')\nD3","2c035c0c":"D4 = pd.DataFrame((D4)) \nD4.to_csv('maju.csv')\nD4 = pd.read_csv('maju.csv')\nD4=pd.DataFrame(D4.values, columns = [\"label\", \"ch1\", \"ch2\", \"ch3\", \n                                          \"ch4\",\"ch5\",\"ch6\",\"ch7\",\"ch8\"])\nD4.loc[D4['label'] >-1, 'label'] = '4'\nD4.to_csv('maju.csv')\nD4","7025f1a9":"a = pd.read_csv(\".\/idle.csv\")\nb = pd.read_csv(\".\/kiri.csv\")\nc = pd.read_csv(\".\/maju.csv\")\nd = pd.read_csv(\".\/kanan.csv\")\ntot= pd.concat([a,b,c,d])\ntot.to_csv('total.csv')\n","4350717f":"df = pd.read_csv('total.csv')\ndf.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1, inplace=True)\n\ndf","aa4c4527":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nfrom keras.metrics import top_k_categorical_accuracy\ndef top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom glob import glob\nimport gc\ngc.enable()","a27b27cc":"from keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn; sn.set(font_scale=1.4)\nfrom sklearn.utils import shuffle  \nimport seaborn as sns","0ec0aba4":"df= pd.read_csv('total.csv')\n\nX = df.iloc[:,1:]\nY = df.iloc[:,2]\nl = ['complement'] * (250- X.shape[1]) \nfor index,col in enumerate(l):\n    X[col+str(index)] = 0\n\nX = X.values\nY = Y.values\nX.shape,Y.shape","2ac55ec0":"Y","41396070":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)\ndummy_y.shape","e6685c92":"def get_model():\n    model = Sequential()\n    model.add(LSTM(32,input_shape=(25,10), return_sequences=True))    \n    model.add(LSTM(32))  \n    model.add(Dense(4, activation = 'softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","aee0a010":"get_model().summary()","163b54ca":"from sklearn.model_selection import KFold\nimport numpy\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n\n# define 10-fold cross validation test harness\nkfold = KFold(n_splits=3, shuffle=True, random_state=seed)\ncvscores = []\nbest = -1\nfor train, test in kfold.split(X, dummy_y):\n    model = get_model()\n    standard = StandardScaler().fit(X[train])\n    \n    x_train_standard = standard.transform(X[train]).reshape(-1,25,10)\n    x_test_standard = standard.transform(X[test]).reshape(-1,25,10)\n    \n    model_history = model.fit(x_train_standard, dummy_y[train], epochs=25, batch_size=32, verbose=1)\n    scores = model.evaluate(x_test_standard, dummy_y[test], verbose=1)\n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n    if scores[1] > best:\n        best = scores[1]\n        history = model_history\nprint(\"%.2f%% (+\/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))","35f0e58c":"from keras.utils import Sequence\nclass SeqGen(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        return batch_x, batch_y","4b94b441":"# Fit the model\nmodel = get_model()\nx_train, x_test, y_train, y_test = train_test_split(\n            X, dummy_y, test_size=0.3, random_state=42, shuffle=True)\nstandard = StandardScaler().fit(x_train)\nx_train_standard = standard.transform(x_train).reshape(-1,25,10)\nx_test_standard = standard.transform(x_test).reshape(-1,25,10)\nhistory = model.fit_generator(SeqGen(x_train_standard,y_train,batch_size=32), validation_data=(x_test_standard,y_test), epochs=25, verbose=1)","fb495043":"model.evaluate(x_test_standard, y_test)","3bd72d89":"labels=np.argmax(y_test, axis=1)\npredictions = model.predict_classes(x_test_standard, batch_size=32, verbose=1)\nprint(classification_report(labels, predictions ))","cd493514":"y_tn=np.argmax(y_train, axis=1)\n\n\n\nsign = ['kiri', 'maju','idle','kanan']\nencoder = LabelEncoder()\nencoder_y = encoder.fit_transform(labels)\ntrain_labels = to_categorical(encoder_y,num_classes=None)\npredictions = model.predict_classes(x_train_standard, batch_size=32, verbose=1)\ncm = confusion_matrix(y_tn,predictions)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"Blues\",xticklabels=sign, yticklabels=sign, linecolor = 'black' ,\n                linewidth = 1 , annot = True, fmt='')\n","ed6fd047":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\nimport os\nimport random\nimport time","ed695c27":"df= pd.read_csv('total.csv')\n\nX = df.iloc[:,1:]\nY = df.iloc[:,2]\nl = ['complement'] * (250- X.shape[1]) \nfor index,col in enumerate(l):\n    X[col+str(index)] = 0\n\nX = X.values\nY = Y.values\nX.shape,Y.shape","e73c3bb3":"# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)\ndummy_y.shape","7caad973":"def model_cnn():\n    model = Sequential()\n\n    model.add(Conv1D(64, (3), input_shape=(25,10)))\n    model.add(Activation('relu'))\n\n    model.add(Conv1D(64, (2)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n\n    model.add(Conv1D(64, (2)))\n    model.add(Activation('relu'))\n    model.add(MaxPooling1D(pool_size=(2)))\n\n    model.add(Flatten())\n\n    model.add(Dense(512))\n\n    model.add(Dense(4))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n    return model\n\n","d883f186":"model_cnn().summary()","ca789f85":"from sklearn.model_selection import KFold\nimport numpy\n# fix random seed for reproducibility\nseed = 7\nnumpy.random.seed(seed)\n\n# define 10-fold cross validation test harness\nkfold = KFold(n_splits=3, shuffle=True, random_state=seed)\ncvscores = []\nbest = -1\nfor train, test in kfold.split(X, dummy_y):\n    \n    model = model_cnn()\n    standard = StandardScaler().fit(X[train])\n    \n    x_train_standard = standard.transform(X[train]).reshape(-1,25,10)\n    x_test_standard = standard.transform(X[test]).reshape(-1,25,10)\n    \n    model_history = model.fit(x_train_standard, dummy_y[train], epochs=25, batch_size=32, verbose=1)\n    scores = model.evaluate(x_test_standard, dummy_y[test], verbose=1)\n    \n    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    cvscores.append(scores[1] * 100)\n    if scores[1] > best:\n        best = scores[1]\n        history = model_history\nprint(\"%.2f%% (+\/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))","e7367012":"model.evaluate(x_test_standard, dummy_y[test])","f4f2c712":"labels=np.argmax( dummy_y[test], axis=1)\npredictions = model.predict_classes(x_test_standard, batch_size=32, verbose=1)\nprint(classification_report(labels, predictions ))","aa4efa1d":"y_tn=np.argmax(dummy_y[train], axis=1)\npredictions[1]\n\n\nsign = ['kiri', 'maju','idle','kanan']\nencoder = LabelEncoder()\nencoder_y = encoder.fit_transform(labels)\ntrain_labels = to_categorical(encoder_y,num_classes=None)\npredictions = model.predict_classes(x_train_standard, batch_size=32, verbose=1)\ncm = confusion_matrix(y_tn,predictions)\nplt.figure(figsize = (5,5))\nsns.heatmap(cm,cmap= \"Blues\",xticklabels=sign, yticklabels=sign, linecolor = 'black' ,\n                linewidth = 1 , annot = True, fmt='')","b63dc6ec":"#CNN 1D","5c323cb4":"#LSTM","acdfd9d8":"# DATA"}}