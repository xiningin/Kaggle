{"cell_type":{"e36b86e2":"code","c1b97bdb":"code","406d7550":"code","74f0e6d1":"code","92e0002e":"code","c05245e7":"code","46d5f07c":"code","74c06307":"code","b167403d":"code","6a1f0788":"code","bbc0c487":"code","ed1ac1d9":"code","3f7b036c":"code","fbcbf51a":"code","c54933f4":"code","900f5171":"code","ef5eae2b":"code","38ad6ce4":"code","0f86f932":"code","dee9e43f":"code","a1eb531a":"code","184df7fd":"code","f545c881":"code","40656a25":"code","1ced53db":"code","7407a944":"code","520c41b1":"code","7b7612e8":"code","97465fb5":"code","caabd990":"code","dd3f2fad":"code","895ad50b":"code","e662bcad":"code","456b7bc2":"code","2d4eb0f6":"code","fd419a32":"code","6c945ab8":"code","b772c772":"code","22cd7922":"code","70404671":"code","483de976":"code","1c4b1e46":"code","7316cf05":"code","8ae301ea":"code","c98ae876":"code","76aaf081":"code","f9fe98f8":"code","9681555d":"code","be1aac19":"code","099f6892":"code","335a46ce":"code","597f315c":"code","1652fadf":"code","22adc14d":"code","7a8dcf34":"code","5f18984b":"code","48a5dd5e":"code","efa6c02e":"code","26f638e9":"code","9103afb2":"code","24e68429":"code","3cddd805":"code","9524137b":"code","02a0d5a3":"code","3ee0c967":"code","9cfe57c0":"code","4528c69c":"code","a3ef5e15":"code","7241d711":"code","394d1992":"code","b9c20b80":"code","cac2c80c":"code","dfaefae2":"code","05c18d83":"code","eb4911bf":"code","990852ba":"code","21d2701f":"code","67eeccba":"code","f32656b3":"code","34a62d61":"code","b8fb79b3":"code","c76dda12":"code","20038a7f":"code","7363bbd8":"code","ffe2f369":"code","142e2891":"code","15d3a108":"code","fa3896b5":"code","9ddb2e1b":"code","3bef210e":"code","9be2de17":"code","d9e6e573":"code","21e89ea7":"code","0a15acc3":"code","61ec78d7":"code","0f6fb643":"code","d7b12868":"code","9b877dab":"code","5a3a9017":"code","47c20696":"code","fe7eb731":"code","8174e313":"code","8c27a816":"code","f14ca670":"code","162dbb51":"code","629b0db0":"code","4b759ece":"code","be8e487c":"code","c1ff5a39":"code","b0ac4e76":"code","6481d5f6":"code","c81e8439":"code","f27d6f0b":"code","d09c19f2":"markdown","dc89e2f7":"markdown","ca6f6217":"markdown","a5021e6c":"markdown","83904032":"markdown","fcbb7d02":"markdown","a9dfde1b":"markdown","fea4fabd":"markdown","dcb0de1a":"markdown","c6f77c38":"markdown","04f2b214":"markdown","657dafaa":"markdown","426e2f5b":"markdown","2905c856":"markdown","d4ba78d8":"markdown","7f61271d":"markdown","24ca7590":"markdown","108c28e2":"markdown","8513caa2":"markdown","1f73a509":"markdown","ae275613":"markdown","1dff40cc":"markdown","9ab756f7":"markdown","8a2c3083":"markdown","f25b4536":"markdown","f96e75fb":"markdown","26b4113a":"markdown","0d4b354d":"markdown","7e2102cc":"markdown","089ef2d2":"markdown","0901e5f5":"markdown","36a38c90":"markdown","15f5f7d0":"markdown","6669e910":"markdown","c1642952":"markdown","cb9aed04":"markdown","9bd13e6b":"markdown","f6c50f18":"markdown","aa8bce1a":"markdown","7950e6d3":"markdown","53d18fb1":"markdown","0b11aed4":"markdown","5da61a05":"markdown","0d92818d":"markdown","2948a0e7":"markdown","1af24aa0":"markdown","9389d164":"markdown","720cd6e4":"markdown","09e54c92":"markdown","78a3d5bb":"markdown","5ac5f169":"markdown","c841b262":"markdown","2b8d7623":"markdown","d41186c0":"markdown","20478c41":"markdown","b6a91a6b":"markdown","652b9177":"markdown","df4ca37e":"markdown","d7207bc0":"markdown","ca3ec17a":"markdown","c6f3dd5b":"markdown","58f6d82b":"markdown","11d7dae1":"markdown","65765fe6":"markdown","7757fc47":"markdown","7903ec8c":"markdown","6ac3d3b4":"markdown","0e68dd3c":"markdown","c0815c38":"markdown","c3e6215a":"markdown","30c0d270":"markdown","1b681254":"markdown","aa6fa907":"markdown","241ba36e":"markdown","9b34f9d6":"markdown","2d715eb0":"markdown","d74b8474":"markdown","0ef04b2c":"markdown","03f23d4c":"markdown","32299273":"markdown","314d8977":"markdown","42fcdf48":"markdown","6440191b":"markdown","bb58781a":"markdown","68b64fa0":"markdown","b15c3f69":"markdown","e2635ed0":"markdown","e3c7bd09":"markdown","0f11edff":"markdown","d196d78f":"markdown","2d0376f8":"markdown","111f4153":"markdown","14767e1e":"markdown","5d0976ea":"markdown","87e61e94":"markdown","27934bdd":"markdown","36e6038b":"markdown","29c2baae":"markdown","515b6a71":"markdown","b7b587ae":"markdown","dfbd4b3a":"markdown","fbffe969":"markdown","0a5ebfd5":"markdown","eb70875d":"markdown","03386e36":"markdown","2902fdba":"markdown","c8bec17d":"markdown","dca58819":"markdown","ec4c1e41":"markdown","ddc368e0":"markdown","7f13382d":"markdown","6ca3f394":"markdown","bac53b5f":"markdown","1c063a19":"markdown","573fd4f5":"markdown","859b4382":"markdown","0dd278c2":"markdown","5a88ffdc":"markdown","06da789a":"markdown","9f06f480":"markdown","71bf812d":"markdown","a104c3f0":"markdown","bc7e912a":"markdown","d64a7f04":"markdown","dcc4709e":"markdown","59292607":"markdown","c52bef3d":"markdown","5888b40b":"markdown","1e43bf53":"markdown","c7817741":"markdown","882c3484":"markdown","5b8c061b":"markdown","55c948ed":"markdown","d88663af":"markdown","363e16d9":"markdown","5de407ef":"markdown","9599766b":"markdown","1979434c":"markdown","4e7ba518":"markdown","96fb7076":"markdown","179b5c75":"markdown","7cf6bd2d":"markdown","a70bf5ab":"markdown","b2198d49":"markdown","5c220bbf":"markdown","48907613":"markdown","3b573e0e":"markdown","54bcc9dc":"markdown","a5991e74":"markdown","1b0f325b":"markdown","11cabd3f":"markdown","21f19ac0":"markdown","9d9db987":"markdown","4bc9bbca":"markdown","68e91b19":"markdown","3c2e87be":"markdown","a4737f51":"markdown","9fb035df":"markdown","0358b2b0":"markdown","78aee604":"markdown","1c6e4fd3":"markdown","c1ed6e77":"markdown","150b6777":"markdown","839ebba7":"markdown","6f90c791":"markdown","cbca72a0":"markdown","e2820c68":"markdown","a428054e":"markdown","962c6250":"markdown","dc3f5b48":"markdown"},"source":{"e36b86e2":"# To help with reading and manipulating data\nimport pandas as pd\nimport numpy as np\n\n# To help with data visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To be used for missing value imputation\nfrom sklearn.impute import SimpleImputer\n\n# To help with model building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    BaggingClassifier,\n)\nfrom xgboost import XGBClassifier\n\n!pip install lightgbm\nimport lightgbm as lgb\n\nfrom sklearn.dummy import DummyClassifier\n\n# To get different metric scores, and split data\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.metrics import (\n    f1_score,\n    accuracy_score,\n    recall_score,\n    precision_score,\n    confusion_matrix,\n    roc_auc_score,\n    plot_confusion_matrix,\n    plot_roc_curve,\n)\n\n# To be used for data scaling and encoding\nfrom sklearn.preprocessing import (\n    StandardScaler,\n    MinMaxScaler,\n    OneHotEncoder,\n    RobustScaler,\n)\nfrom sklearn.impute import SimpleImputer\n\n# To be used for tuning the model\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# To be used for creating pipelines and personalizing them\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import TransformerMixin\n\n\n# To oversample and undersample data\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# To define maximum number of columns to be displayed in a dataframe\npd.set_option(\"display.max_columns\", None)\n\n# To supress scientific notations for a dataframe\npd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n\n# set the background for the graphs\nplt.style.use(\"ggplot\")\n\n# For pandas profiling\nfrom pandas_profiling import ProfileReport\n\n# Printing style\n!pip install tabulate\nfrom tabulate import tabulate\n\n# To supress warnings\nimport warnings\n\n# date time\nfrom datetime import datetime\n\nwarnings.filterwarnings(\"ignore\")\n","c1b97bdb":"# Loading the dataset\nchurner = pd.read_csv(\"..\/input\/credit-card-customers\/BankChurners.csv\")","406d7550":"# Checking the number of rows and columns in the data\nchurner.shape","74f0e6d1":"additional_droppable_columns = [\n    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'\n]\n\nfor col in additional_droppable_columns:\n    if col in churner.columns.unique().tolist():\n        churner.drop(columns=[col], inplace=True)","92e0002e":"# Creating a copy dataset for analysis\n\ndata = churner.copy()","c05245e7":"# let's view the first 5 rows of the data\ndata.head()","46d5f07c":"# let's view the last 5 rows of the data\ndata.tail()","74c06307":"# let's check the data types of the columns in the dataset\ndata.info()","b167403d":"# let's check for duplicate values in the data\ndata.duplicated().sum()","6a1f0788":"# let's check for missing values in the data\ndf_null_summary = pd.concat(\n    [data.isnull().sum(), data.isnull().sum() * 100 \/ data.isnull().count()], axis=1\n)\ndf_null_summary.columns = [\"Null Record Count\", \"Percentage of Null Records\"]\ndf_null_summary[df_null_summary[\"Null Record Count\"] > 0].sort_values(\n    by=\"Percentage of Null Records\", ascending=False\n).style.background_gradient(cmap=\"YlOrRd\")","bbc0c487":"data.select_dtypes(include=\"object\").nunique()","ed1ac1d9":"data.select_dtypes(exclude=\"object\").nunique()","3f7b036c":"# let's view the statistical summary of the numerical columns in the data\ndata.describe().T","fbcbf51a":"# let's view the statistical summary of the categorical columns in the data\ndata.describe(include=\"object\").T","c54933f4":"# Below function prints unique value counts and percentages for the category\/object type variables\n\n\ndef category_unique_value():\n    for cat_cols in (\n        data.select_dtypes(exclude=[np.int64, np.float64]).columns.unique().to_list()\n    ):\n        print(\"Unique values and corresponding data counts for feature: \" + cat_cols)\n        print(\"-\" * 90)\n        df_temp = pd.concat(\n            [\n                data[cat_cols].value_counts(),\n                data[cat_cols].value_counts(normalize=True) * 100,\n            ],\n            axis=1,\n        )\n        df_temp.columns = [\"Count\", \"Percentage\"]\n        print(df_temp)\n        print(\"-\" * 90)","900f5171":"category_unique_value()","ef5eae2b":"data.drop(columns=[\"CLIENTNUM\"], inplace=True)","38ad6ce4":"data[\"Education_Level\"] = data[\"Education_Level\"].fillna(\"Unknown\")\ndata[\"Marital_Status\"] = data[\"Marital_Status\"].fillna(\"Unknown\")","0f86f932":"data.loc[data[data[\"Income_Category\"] == \"abc\"].index, \"Income_Category\"] = \"Unknown\"","dee9e43f":"category_unique_value()","a1eb531a":"# let's check for missing values in the data\ndf_null_summary = pd.concat(\n    [data.isnull().sum(), data.isnull().sum() * 100 \/ data.isnull().count()], axis=1\n)\ndf_null_summary.columns = [\"Null Record Count\", \"Percentage of Null Records\"]\ndf_null_summary[df_null_summary[\"Null Record Count\"] > 0].sort_values(\n    by=\"Percentage of Null Records\", ascending=False\n).style.background_gradient(cmap=\"YlOrRd\")","184df7fd":"category_columns = data.select_dtypes(include=\"object\").columns.tolist()","f545c881":"data[category_columns] = data[category_columns].astype(\"category\")","40656a25":"data.columns = [i.replace(\" \", \"_\").lower() for i in data.columns]","1ced53db":"data.info()","7407a944":"def summary(data: pd.DataFrame, x: str):\n    \"\"\"\n    The function prints the 5 point summary and histogram, box plot,\n    violin plot, and cumulative density distribution plots for each\n    feature name passed as the argument.\n\n    Parameters:\n    ----------\n\n    x: str, feature name\n\n    Usage:\n    ------------\n\n    summary('age')\n    \"\"\"\n    x_min = data[x].min()\n    x_max = data[x].max()\n    Q1 = data[x].quantile(0.25)\n    Q2 = data[x].quantile(0.50)\n    Q3 = data[x].quantile(0.75)\n\n    dict = {\"Min\": x_min, \"Q1\": Q1, \"Q2\": Q2, \"Q3\": Q3, \"Max\": x_max}\n    df = pd.DataFrame(data=dict, index=[\"Value\"])\n    print(f\"5 Point Summary of {x.capitalize()} Attribute:\\n\")\n    print(tabulate(df, headers=\"keys\", tablefmt=\"psql\"))\n\n    fig = plt.figure(figsize=(16, 8))\n    plt.subplots_adjust(hspace=0.6)\n    sns.set_palette(\"Pastel1\")\n\n    plt.subplot(221, frameon=True)\n    ax1 = sns.distplot(data[x], color=\"purple\")\n    ax1.axvline(\n        np.mean(data[x]), color=\"purple\", linestyle=\"--\"\n    )  # Add mean to the histogram\n    ax1.axvline(\n        np.median(data[x]), color=\"black\", linestyle=\"-\"\n    )  # Add median to the histogram\n    plt.title(f\"{x.capitalize()} Density Distribution\")\n\n    plt.subplot(222, frameon=True)\n    ax2 = sns.violinplot(x=data[x], palette=\"Accent\", split=True)\n    plt.title(f\"{x.capitalize()} Violinplot\")\n\n    plt.subplot(223, frameon=True, sharex=ax1)\n    ax3 = sns.boxplot(\n        x=data[x], palette=\"cool\", width=0.7, linewidth=0.6, showmeans=True\n    )\n    plt.title(f\"{x.capitalize()} Boxplot\")\n\n    plt.subplot(224, frameon=True, sharex=ax2)\n    ax4 = sns.kdeplot(data[x], cumulative=True)\n    plt.title(f\"{x.capitalize()} Cumulative Density Distribution\")\n\n    plt.show()","520c41b1":"summary(data, \"customer_age\")","7b7612e8":"summary(data, \"dependent_count\")","97465fb5":"summary(data, \"months_on_book\")","caabd990":"summary(data, \"total_relationship_count\")","dd3f2fad":"summary(data, \"months_inactive_12_mon\")","895ad50b":"summary(data, \"contacts_count_12_mon\")","e662bcad":"summary(data, \"credit_limit\")","456b7bc2":"data[data[\"credit_limit\"] > 23000][\"income_category\"].value_counts(normalize=True)","2d4eb0f6":"data[data[\"credit_limit\"] > 23000][\"card_category\"].value_counts(normalize=True)","fd419a32":"summary(data, \"total_revolving_bal\")","6c945ab8":"summary(data, \"avg_open_to_buy\")","b772c772":"summary(data, \"total_amt_chng_q4_q1\")","22cd7922":"summary(data, \"total_trans_amt\")","70404671":"summary(data, \"total_trans_ct\")","483de976":"summary(data, \"total_ct_chng_q4_q1\")","1c4b1e46":"summary(data, \"avg_utilization_ratio\")","7316cf05":"# Below code plots grouped bar for each categorical feature\n\ndef perc_on_bar(data: pd.DataFrame, cat_columns, target, hue=None, perc=True):\n    '''\n    The function takes a category column as input and plots bar chart with percentages on top of each bar\n    \n    Usage:\n    ------\n    \n    perc_on_bar(df, ['age'], 'prodtaken')\n    '''\n    \n    subplot_cols = 2\n    subplot_rows = int(len(cat_columns)\/2 + 1)\n    plt.figure(figsize=(16,3*subplot_rows))\n    for i, col in enumerate(cat_columns):\n        plt.subplot(subplot_rows,subplot_cols,i+1)\n        order = data[col].value_counts(ascending=False).index  # Data order  \n        ax=sns.countplot(data=data, x=col, palette = 'crest', order=order, hue=hue);\n        for p in ax.patches:\n            percentage = '{:.1f}%\\n({})'.format(100 * p.get_height()\/len(data[target]), p.get_height())\n            # Added percentage and actual value\n            x = p.get_x() + p.get_width() \/ 2\n            y = p.get_y() + p.get_height() + 40\n            if perc:\n                plt.annotate(percentage, (x, y), ha='center', color='black', fontsize='medium'); # Annotation on top of bars\n            plt.xticks(color='black', fontsize='medium', rotation= (-90 if col=='region' else 0));\n            plt.tight_layout()\n            plt.title(col.capitalize() + ' Percentage Bar Charts\\n\\n')\n","8ae301ea":"category_columns = data.select_dtypes(include=\"category\").columns.tolist()\ntarget_variable = \"attrition_flag\"\nperc_on_bar(data, category_columns, target_variable)","c98ae876":"# Below code plots box charts for each numerical feature by each type of Personal Loan (0: Not Borrowed, 1: Borroed)\ndef box_by_target(data: pd.DataFrame, numeric_columns, target, include_outliers):\n    \"\"\"\n    The function takes a category column, target column, and whether to include outliers or not as input\n    and plots bar chart with percentages on top of each bar\n\n    Usage:\n    ------\n\n    perc_on_bar(['age'], 'prodtaken', True)\n    \"\"\"\n    subplot_cols = 2\n    subplot_rows = int(len(numeric_columns) \/ 2 + 1)\n    plt.figure(figsize=(16, 3 * subplot_rows))\n    for i, col in enumerate(numeric_columns):\n        plt.subplot(8, 2, i + 1)\n        sns.boxplot(\n            data=data,\n            x=target,\n            y=col,\n            orient=\"vertical\",\n            palette=\"Blues\",\n            showfliers=include_outliers,\n        )\n        plt.tight_layout()\n        plt.title(str(i + 1) + \": \" + target + \" vs. \" + col, color=\"black\")","76aaf081":"numeric_columns = data.select_dtypes(exclude=\"category\").columns.tolist()\ntarget_variable = \"attrition_flag\"\nbox_by_target(data, numeric_columns, target_variable, True)","f9fe98f8":"box_by_target(data, numeric_columns, target_variable, False)","9681555d":"# Create a function that returns a Pie chart and a Bar Graph for the categorical variables:\ndef cat_view(df: pd.DataFrame, x, target):\n    \"\"\"\n    Function to create a Bar chart and a Pie chart for categorical variables.\n    \"\"\"\n    from matplotlib import cm\n\n    color1 = cm.inferno(np.linspace(0.4, 0.8, 30))\n    color2 = cm.viridis(np.linspace(0.4, 0.8, 30))\n    sns.set_palette(\"cubehelix\")\n    fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n\n    \"\"\"\n    Draw a Pie Chart on first subplot.\n    \"\"\"\n    s = data.groupby(x).size()\n\n    mydata_values = s.values.tolist()\n    mydata_index = s.index.tolist()\n\n    def func(pct, allvals):\n        absolute = int(pct \/ 100.0 * np.sum(allvals))\n        return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\n    wedges, texts, autotexts = ax[0].pie(\n        mydata_values,\n        autopct=lambda pct: func(pct, mydata_values),\n        textprops=dict(color=\"w\"),\n    )\n\n    ax[0].legend(\n        wedges,\n        mydata_index,\n        title=x.capitalize(),\n        loc=\"center left\",\n        bbox_to_anchor=(1, 0, 0.5, 1),\n    )\n\n    plt.setp(autotexts, size=12)\n\n    ax[0].set_title(f\"{x.capitalize()} Pie Chart\")\n\n    \"\"\"\n    Draw a Bar Graph on second subplot.\n    \"\"\"\n\n    df = pd.pivot_table(\n        data, index=[x], columns=[target], values=[\"credit_limit\"], aggfunc=len\n    )\n\n    labels = df.index.tolist()\n    no = df.values[:, 1].tolist()\n    yes = df.values[:, 0].tolist()\n\n    l = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    rects1 = ax[1].bar(\n        l - width \/ 2, no, width, label=\"Existing Customer\", color=color1\n    )\n    rects2 = ax[1].bar(\n        l + width \/ 2, yes, width, label=\"Attrited Customer\", color=color2\n    )\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax[1].set_ylabel(\"Scores\")\n    ax[1].set_title(f\"{x.capitalize()} Bar Graph\")\n    ax[1].set_xticks(l)\n    ax[1].set_xticklabels(labels)\n    ax[1].legend()\n\n    def autolabel(rects):\n\n        \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n\n        for rect in rects:\n            height = rect.get_height()\n            ax[1].annotate(\n                \"{}\".format(height),\n                xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                xytext=(0, 3),  # 3 points vertical offset\n                textcoords=\"offset points\",\n                fontsize=\"medium\",\n                ha=\"center\",\n                va=\"bottom\",\n            )\n\n    autolabel(rects1)\n    autolabel(rects2)\n\n    fig.tight_layout()\n    plt.show()\n\n    \"\"\"\n    Draw a Stacked Bar Graph on bottom.\n    \"\"\"\n\n    sns.set(palette=\"tab10\")\n    tab = pd.crosstab(data[x], data[target], normalize=\"index\")\n\n    tab.plot.bar(stacked=True, figsize=(16, 3))\n    plt.title(x.capitalize() + \" Stacked Bar Plot\")\n    plt.legend(loc=\"upper right\", bbox_to_anchor=(0, 1))\n    plt.show()","be1aac19":"cat_view(data, \"gender\", \"attrition_flag\")","099f6892":"cat_view(data, \"education_level\", \"attrition_flag\")","335a46ce":"cat_view(data, \"marital_status\", \"attrition_flag\")","597f315c":"cat_view(data, \"income_category\", \"attrition_flag\")","1652fadf":"cat_view(data, \"card_category\", \"attrition_flag\")","22adc14d":"# Below plot shows correlations between the numerical features in the dataset\n\nplt.figure(figsize=(20, 20))\nsns.set(palette=\"nipy_spectral\")\nsns.pairplot(data=data, hue=\"attrition_flag\", corner=True)","7a8dcf34":"# Plotting correlation heatmap of the features\ncodes = {'Existing Customer':0, 'Attrited Customer':1}\ndata_clean = data.copy()\ndata_clean['attrition_flag'] = data_clean['attrition_flag'].map(codes).astype(int)\n\n\nsns.set(rc={\"figure.figsize\": (15, 15)})\nsns.heatmap(\n    data_clean.corr(),\n    annot=True,\n    linewidths=0.5,\n    center=0,\n    cbar=False,\n    cmap=\"YlGnBu\",\n    fmt=\"0.2f\",\n)\nplt.show()\n","5f18984b":"# Building a function to standardize columns\n\ndef feature_name_standardize(df: pd.DataFrame):\n    df_ = df.copy()\n    df_.columns = [i.replace(\" \", \"_\").lower() for i in df_.columns]\n    return df_\n\n# Building a function to drop features\n\ndef drop_feature(df: pd.DataFrame, features: list = []):\n    df_ = df.copy()\n    if len(features) != 0:\n        df_ = df_.drop(columns=features)\n        \n    return df_\n\n# Building a function to treat incorrect value\n\ndef mask_value(df: pd.DataFrame, feature: str = None, value_to_mask: str = None, masked_value: str = None):\n    df_ = df.copy()\n    if feature != None and value_to_mask != None:\n        if feature in df_.columns:\n            df_[feature] = df_[feature].astype('object')\n            df_.loc[df_[df_[feature] == value_to_mask].index, feature] = masked_value\n            df_[feature] = df_[feature].astype('category')\n            \n    return df_\n\n# Building a custom imputer\n\ndef impute_category_unknown(df: pd.DataFrame, fill_value: str):\n    df_ = df.copy()\n    for col in df_.select_dtypes(include='category').columns.tolist():\n        df_[col] = df_[col].astype('object')\n        df_[col] = df_[col].fillna('Unknown')\n        df_[col] = df_[col].astype('category')\n    return df_\n\n# Building a custom data preprocessing class with fit and transform methods for standardizing column names\n\nclass FeatureNamesStandardizer(TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        \"\"\"All SciKit-Learn compatible transformers and classifiers have the\n        same interface. `fit` always returns the same object.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Returns dataframe with column names in lower case with underscores in place of spaces.\"\"\"\n        X_ = feature_name_standardize(X)\n        return X_\n    \n    \n# Building a custom data preprocessing class with fit and transform methods for dropping columns\n\nclass ColumnDropper(TransformerMixin):\n    def __init__(self, features: list):\n        self.features = features\n\n    def fit(self, X, y=None):\n        \"\"\"All SciKit-Learn compatible transformers and classifiers have the\n        same interface. `fit` always returns the same object.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Given a list of columns, returns a dataframe without those columns.\"\"\"\n        X_ = drop_feature(X, features=self.features)\n        return X_\n        \n    \n\n# Building a custom data preprocessing class with fit and transform methods for custom value masking\n\nclass CustomValueMasker(TransformerMixin):\n    def __init__(self, feature: str, value_to_mask: str, masked_value: str):\n        self.feature = feature\n        self.value_to_mask = value_to_mask\n        self.masked_value = masked_value\n\n    def fit(self, X, y=None):\n        \"\"\"All SciKit-Learn compatible transformers and classifiers have the\n        same interface. `fit` always returns the same object.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Return a dataframe with the required feature value masked as required.\"\"\"\n        X_ = mask_value(X, self.feature, self.value_to_mask, self.masked_value)\n        return X_\n    \n    \n# Building a custom class to one-hot encode using pandas\nclass PandasOneHot(TransformerMixin):\n    def __init__(self, columns: list = None):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        \"\"\"All SciKit-Learn compatible transformers and classifiers have the\n        same interface. `fit` always returns the same object.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Return a dataframe with the required feature value masked as required.\"\"\"\n        X_ = pd.get_dummies(X, columns = self.columns, drop_first=True)\n        return X_\n    \n# Building a custom class to fill nulls with Unknown\nclass FillUnknown(TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        \"\"\"All SciKit-Learn compatible transformers and classifiers have the\n        same interface. `fit` always returns the same object.\"\"\"\n        return self\n\n    def transform(self, X):\n        \"\"\"Return a dataframe with the required feature value masked as required.\"\"\"\n        X_ = impute_category_unknown(X, fill_value='Unknown')\n        return X_\n","48a5dd5e":"df = churner.copy()\ndf.describe(include=\"all\").T","efa6c02e":"# The static variables\n\n# For dropping columns\ncolumns_to_drop = [\n    \"clientnum\",\n    \"credit_limit\",\n    \"dependent_count\",\n    \"months_on_book\",\n    \"avg_open_to_buy\",\n    \"customer_age\",\n]\n\n# For masking a particular value in a feature\ncolumn_to_mask_value = \"income_category\"\nvalue_to_mask = \"abc\"\nmasked_value = \"Unknown\"\n\n# Random state and loss\nseed = 1\nloss_func = \"logloss\"\n\n# Test and Validation sizes\ntest_size = 0.2\nval_size = 0.25\n\n# Dependent Varibale Value map\ntarget_mapper = {\"Attrited Customer\": 1, \"Existing Customer\": 0}\n","26f638e9":"cat_columns = df.select_dtypes(include=\"object\").columns.tolist()\ndf[cat_columns] = df[cat_columns].astype(\"category\")","9103afb2":"X = df.drop(columns=[\"Attrition_Flag\"])\ny = df[\"Attrition_Flag\"].map(target_mapper)","24e68429":"# Splitting data into training, validation and test set:\n# first we split data into 2 parts, say temporary and test\n\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=seed, stratify=y\n)\n\n# then we split the temporary set into train and validation\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=val_size, random_state=seed, stratify=y_temp\n)\nprint(\n    \"Training data shape: \\n\\n\",\n    X_train.shape,\n    \"\\n\\nValidation Data Shape: \\n\\n\",\n    X_val.shape,\n    \"\\n\\nTesting Data Shape: \\n\\n\",\n    X_test.shape,\n)","3cddd805":"print(\"Training: \\n\", y_train.value_counts(normalize=True))\nprint(\"\\n\\nValidation: \\n\", y_val.value_counts(normalize=True))\nprint(\"\\n\\nTest: \\n\", y_test.value_counts(normalize=True))","9524137b":"# To Standardize feature names\nfeature_name_standardizer = FeatureNamesStandardizer()\n\nX_train = feature_name_standardizer.fit_transform(X_train)\nX_val = feature_name_standardizer.transform(X_val)\nX_test = feature_name_standardizer.transform(X_test)\n\n# To Drop unnecessary columns\ncolumn_dropper = ColumnDropper(features=columns_to_drop)\n\nX_train = column_dropper.fit_transform(X_train)\nX_val = column_dropper.transform(X_val)\nX_test = column_dropper.transform(X_test)\n\n# To Mask incorrect\/meaningless value of a feature\nvalue_masker = CustomValueMasker(\n    feature=column_to_mask_value, value_to_mask=value_to_mask, masked_value=masked_value\n)\n\nX_train = value_masker.fit_transform(X_train)\nX_val = value_masker.transform(X_val)\nX_test = value_masker.transform(X_test)\n\n# To impute categorical Nulls to Unknown\ncat_columns = X_train.select_dtypes(include=\"category\").columns.tolist()\nimputer = FillUnknown()\n\nX_train[cat_columns] = imputer.fit_transform(X_train[cat_columns])\nX_val[cat_columns] = imputer.transform(X_val[cat_columns])\nX_test[cat_columns] = imputer.transform(X_test[cat_columns])\n\n# To encode the data\none_hot = PandasOneHot()\n\nX_train = one_hot.fit_transform(X_train)\nX_val = one_hot.transform(X_val)\nX_test = one_hot.transform(X_test)\n\n\n# Scale the numerical columns\nrobust_scaler = RobustScaler(with_centering=False, with_scaling=True)\nnum_columns = [\n    \"total_relationship_count\",\n    \"months_inactive_12_mon\",\n    \"contacts_count_12_mon\",\n    \"total_revolving_bal\",\n    \"total_amt_chng_q4_q1\",\n    \"total_trans_amt\",\n    \"total_trans_ct\",\n    \"total_ct_chng_q4_q1\",\n    \"avg_utilization_ratio\",\n]\n\nX_train[num_columns] = pd.DataFrame(\n    robust_scaler.fit_transform(X_train[num_columns]),\n    columns=num_columns,\n    index=X_train.index,\n)\nX_val[num_columns] = pd.DataFrame(\n    robust_scaler.transform(X_val[num_columns]), columns=num_columns, index=X_val.index\n)\nX_test[num_columns] = pd.DataFrame(\n    robust_scaler.transform(X_test[num_columns]),\n    columns=num_columns,\n    index=X_test.index,\n)","02a0d5a3":"X_train.head(3)","3ee0c967":"X_val.head(3)","9cfe57c0":"X_test.head(3)","4528c69c":"print(\n    \"Training data shape: \\n\\n\",\n    X_train.shape,\n    \"\\n\\nValidation Data Shape: \\n\\n\",\n    X_val.shape,\n    \"\\n\\nTesting Data Shape: \\n\\n\",\n    X_test.shape,\n)","a3ef5e15":"def get_metrics_score(\n    model, train, test, train_y, test_y, threshold=0.5, flag=False, roc=True\n):\n    \"\"\"\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable\n    threshold: thresold for classifiying the observation as 1\n    flag: If the flag is set to True then only the print statements showing different will be displayed. The default value is set to True.\n    roc: If the roc is set to True then only roc score will be displayed. The default value is set to False.\n    \"\"\"\n    # defining an empty list to store train and test results\n\n    score_list = []\n\n    pred_train = model.predict_proba(train)[:, 1] > threshold\n    pred_test = model.predict_proba(test)[:, 1] > threshold\n\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n\n    train_acc = accuracy_score(pred_train, train_y)\n    test_acc = accuracy_score(pred_test, test_y)\n\n    train_recall = recall_score(train_y, pred_train)\n    test_recall = recall_score(test_y, pred_test)\n\n    train_precision = precision_score(train_y, pred_train)\n    test_precision = precision_score(test_y, pred_test)\n\n    train_f1 = f1_score(train_y, pred_train)\n    test_f1 = f1_score(test_y, pred_test)\n\n    pred_train_proba = model.predict_proba(train)[:, 1]\n    pred_test_proba = model.predict_proba(test)[:, 1]\n\n    train_roc_auc = roc_auc_score(train_y, pred_train_proba)\n    test_roc_auc = roc_auc_score(test_y, pred_test_proba)\n\n    score_list.extend(\n        (\n            train_acc,\n            test_acc,\n            train_recall,\n            test_recall,\n            train_precision,\n            test_precision,\n            train_f1,\n            test_f1,\n            train_roc_auc,\n            test_roc_auc,\n        )\n    )\n\n    if flag == True:\n\n        print(\"Accuracy on training set : \", accuracy_score(pred_train, train_y))\n        print(\"Accuracy on test set : \", accuracy_score(pred_test, test_y))\n        print(\"Recall on training set : \", recall_score(train_y, pred_train))\n        print(\"Recall on test set : \", recall_score(test_y, pred_test))\n        print(\"Precision on training set : \", precision_score(train_y, pred_train))\n        print(\"Precision on test set : \", precision_score(test_y, pred_test))\n        print(\"F1 on training set : \", f1_score(train_y, pred_train))\n        print(\"F1 on test set : \", f1_score(test_y, pred_test))\n\n    if roc == True:\n        if flag == True:\n            print(\n                \"ROC-AUC Score on training set : \",\n                roc_auc_score(train_y, pred_train_proba),\n            )\n            print(\n                \"ROC-AUC Score on test set : \", roc_auc_score(test_y, pred_test_proba)\n            )\n\n    return score_list  # returning the list with train and test scores","7241d711":"def make_confusion_matrix(model, test_X, y_actual, labels=[1, 0]):\n    \"\"\"\n    model : classifier to predict values of X\n    test_X: test set\n    y_actual : ground truth\n\n    \"\"\"\n    y_predict = model.predict(test_X)\n    cm = metrics.confusion_matrix(y_actual, y_predict, labels=[1, 0])\n    df_cm = pd.DataFrame(\n        cm,\n        index=[i for i in [\"Actual - Attrited\", \"Actual - Existing\"]],\n        columns=[i for i in [\"Predicted - Attrited\", \"Predicted - Existing\"]],\n    )\n    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() \/ np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n    labels = np.asarray(labels).reshape(2, 2)\n    plt.figure(figsize=(5, 3))\n    sns.heatmap(df_cm, annot=labels, fmt=\"\", cmap=\"Blues\").set(title=\"Confusion Matrix\")","394d1992":"# # defining empty lists to add train and test results\n\nmodel_names = []\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\nroc_auc_train = []\nroc_auc_test = []\ncross_val_train = []\n\n\ndef add_score_model(model_name, score, cv_res):\n    \"\"\"Add scores to list so that we can compare all models score together\"\"\"\n    model_names.append(model_name)\n    acc_train.append(score[0])\n    acc_test.append(score[1])\n    recall_train.append(score[2])\n    recall_test.append(score[3])\n    precision_train.append(score[4])\n    precision_test.append(score[5])\n    f1_train.append(score[6])\n    f1_test.append(score[7])\n    roc_auc_train.append(score[8])\n    roc_auc_test.append(score[9])\n    cross_val_train.append(cv_res)","b9c20b80":"models = []  # Empty list to store all the models\ncv_results = []\n\n# Appending models into the list\nmodels.append((\"Bagging\", BaggingClassifier(random_state=seed)))\nmodels.append((\"Random forest\", RandomForestClassifier(random_state=seed)))\nmodels.append((\"GBM\", GradientBoostingClassifier(random_state=seed)))\nmodels.append((\"Adaboost\", AdaBoostClassifier(random_state=seed)))\nmodels.append((\"Xgboost\", XGBClassifier(random_state=seed, eval_metric=loss_func)))\nmodels.append((\"dtree\", DecisionTreeClassifier(random_state=seed)))\nmodels.append((\"Light GBM\", lgb.LGBMClassifier(random_state=seed)))\n\n# For each model, run cross validation on 9 folds (+ 1 validation fold) with scoring for recall\nfor name, model in models:\n    scoring = \"recall\"\n    kfold = StratifiedKFold(\n        n_splits=10, shuffle=True, random_state=1\n    )  # Setting number of splits equal to 10\n\n    cv_result = cross_val_score(\n        estimator=model, X=X_train, y=y_train, scoring=scoring, cv=kfold\n    )\n    cv_results.append(cv_result)\n\n    model.fit(X_train, y_train)\n    model_score = get_metrics_score(model, X_train, X_val, y_train, y_val)\n    add_score_model(name, model_score, cv_result.mean())\n\nprint(\"Operation Completed!\")","cac2c80c":"comparison_frame = pd.DataFrame(\n    {\n        \"Model\": model_names,\n        \"Cross_Val_Score_Train\": cross_val_train,\n        \"Train_Accuracy\": acc_train,\n        \"Test_Accuracy\": acc_test,\n        \"Train_Recall\": recall_train,\n        \"Test_Recall\": recall_test,\n        \"Train_Precision\": precision_train,\n        \"Test_Precision\": precision_test,\n        \"Train_F1\": f1_train,\n        \"Test_F1\": f1_test,\n        \"Train_ROC_AUC\": roc_auc_train,\n        \"Test_ROC_AUC\": roc_auc_test,\n    }\n)\n\n# Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(\n    by=[\"Cross_Val_Score_Train\", \"Test_Recall\"], ascending=False\n).style.highlight_max(color=\"lightgreen\", axis=0).highlight_min(color=\"pink\", axis=0)","dfaefae2":"# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(cv_results)\nax.set_xticklabels(model_names)\n\nplt.show()","05c18d83":"print(\"Before UpSampling, counts of label 'Yes': {}\".format(sum(y_train == 1)))\nprint(\"Before UpSampling, counts of label 'No': {} \\n\".format(sum(y_train == 0)))\n\nsm = SMOTE(\n    sampling_strategy=\"minority\", k_neighbors=10, random_state=seed\n)  # Synthetic Minority Over Sampling Technique\n\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n\n\nprint(\"After UpSampling, counts of label 'Yes': {}\".format(sum(y_train_over == 1)))\nprint(\"After UpSampling, counts of label 'No': {} \\n\".format(sum(y_train_over == 0)))\n\n\nprint(\"After UpSampling, the shape of train_X: {}\".format(X_train_over.shape))\nprint(\"After UpSampling, the shape of train_y: {} \\n\".format(y_train_over.shape))","eb4911bf":"models_over = []\n\n# Appending models into the list\n\nmodels_over.append((\"Bagging UpSampling\", BaggingClassifier(random_state=seed)))\nmodels_over.append(\n    (\"Random forest UpSampling\", RandomForestClassifier(random_state=seed))\n)\nmodels_over.append((\"GBM UpSampling\", GradientBoostingClassifier(random_state=seed)))\nmodels_over.append((\"Adaboost UpSampling\", AdaBoostClassifier(random_state=seed)))\nmodels_over.append(\n    (\"Xgboost UpSampling\", XGBClassifier(random_state=seed, eval_metric=loss_func))\n)\nmodels_over.append((\"dtree UpSampling\", DecisionTreeClassifier(random_state=seed)))\nmodels_over.append((\"Light GBM UpSampling\", lgb.LGBMClassifier(random_state=seed)))\n\nfor name, model in models_over:\n    scoring = \"recall\"\n    kfold = StratifiedKFold(\n        n_splits=10, shuffle=True, random_state=1\n    )  # Setting number of splits equal to 10\n\n    cv_result_over = cross_val_score(\n        estimator=model, X=X_train_over, y=y_train_over, scoring=scoring, cv=kfold\n    )\n    cv_results.append(cv_result_over)\n\n    model.fit(X_train_over, y_train_over)\n    model_score_over = get_metrics_score(\n        model, X_train_over, X_val, y_train_over, y_val\n    )\n    add_score_model(name, model_score_over, cv_result_over.mean())\n\nprint(\"Operation Completed!\")","990852ba":"comparison_frame = pd.DataFrame(\n    {\n        \"Model\": model_names,\n        \"Cross_Val_Score_Train\": cross_val_train,\n        \"Train_Accuracy\": acc_train,\n        \"Test_Accuracy\": acc_test,\n        \"Train_Recall\": recall_train,\n        \"Test_Recall\": recall_test,\n        \"Train_Precision\": precision_train,\n        \"Test_Precision\": precision_test,\n        \"Train_F1\": f1_train,\n        \"Test_F1\": f1_test,\n        \"Train_ROC_AUC\": roc_auc_train,\n        \"Test_ROC_AUC\": roc_auc_test,\n    }\n)\n\n# Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(\n    by=[\"Test_Recall\", \"Cross_Val_Score_Train\"], ascending=False\n).style.highlight_max(color=\"lightgreen\", axis=0).highlight_min(color=\"pink\", axis=0)","21d2701f":"rus = RandomUnderSampler(random_state=1)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)","67eeccba":"print(\"Before Under Sampling, counts of label 'Yes': {}\".format(sum(y_train == 1)))\nprint(\"Before Under Sampling, counts of label 'No': {} \\n\".format(sum(y_train == 0)))\n\nprint(\"After Under Sampling, counts of label 'Yes': {}\".format(sum(y_train_un == 1)))\nprint(\"After Under Sampling, counts of label 'No': {} \\n\".format(sum(y_train_un == 0)))\n\nprint(\"After Under Sampling, the shape of train_X: {}\".format(X_train_un.shape))\nprint(\"After Under Sampling, the shape of train_y: {} \\n\".format(y_train_un.shape))","f32656b3":"models_under = []\n\n# Appending models into the list\n\nmodels_under.append((\"Bagging DownSampling\", BaggingClassifier(random_state=seed)))\nmodels_under.append(\n    (\"Random forest DownSampling\", RandomForestClassifier(random_state=seed))\n)\nmodels_under.append((\"GBM DownSampling\", GradientBoostingClassifier(random_state=seed)))\nmodels_under.append((\"Adaboost DownSampling\", AdaBoostClassifier(random_state=seed)))\nmodels_under.append(\n    (\"Xgboost DownSampling\", XGBClassifier(random_state=seed, eval_metric=loss_func))\n)\nmodels_under.append((\"dtree DownSampling\", DecisionTreeClassifier(random_state=seed)))\nmodels_under.append((\"Light GBM DownSampling\", lgb.LGBMClassifier(random_state=seed)))\n\nfor name, model in models_under:\n    scoring = \"recall\"\n    kfold = StratifiedKFold(\n        n_splits=10, shuffle=True, random_state=1\n    )  # Setting number of splits equal to 10\n\n    cv_result_under = cross_val_score(\n        estimator=model, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold\n    )\n    cv_results.append(cv_result_under)\n\n    model.fit(X_train_un, y_train_un)\n    model_score_under = get_metrics_score(model, X_train_un, X_val, y_train_un, y_val)\n    add_score_model(name, model_score_under, cv_result_under.mean())\n\nprint(\"Operation Completed!\")","34a62d61":"comparison_frame = pd.DataFrame(\n    {\n        \"Model\": model_names,\n        \"Cross_Val_Score_Train\": cross_val_train,\n        \"Train_Accuracy\": acc_train,\n        \"Test_Accuracy\": acc_test,\n        \"Train_Recall\": recall_train,\n        \"Test_Recall\": recall_test,\n        \"Train_Precision\": precision_train,\n        \"Test_Precision\": precision_test,\n        \"Train_F1\": f1_train,\n        \"Test_F1\": f1_test,\n        \"Train_ROC_AUC\": roc_auc_train,\n        \"Test_ROC_AUC\": roc_auc_test,\n    }\n)\n\n# Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(\n    by=[\"Test_Recall\", \"Cross_Val_Score_Train\"], ascending=False\n).style.highlight_max(color=\"lightgreen\", axis=0).highlight_min(color=\"pink\", axis=0)","b8fb79b3":"%%time\n\n# defining model\nmodel = XGBClassifier(random_state=seed, eval_metric=loss_func)\n\n\n# Parameter grid to pass in RandomizedSearchCV\nparam_grid={'n_estimators':np.arange(50,500,50),\n            'scale_pos_weight':[2,5,10],\n            'learning_rate':[0.01,0.1,0.2,0.05],\n            'gamma':[0,1,3,5],\n            'subsample':[0.8,0.9,1],\n            'max_depth':np.arange(4,20,1),\n            'reg_lambda':[5,10, 15, 20]} \n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nxgb_tuned = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=10, random_state=seed, n_jobs = -1)\n\n#Fitting parameters in RandomizedSearchCV\nxgb_tuned.fit(X_train_un,y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(xgb_tuned.best_params_,xgb_tuned.best_score_))","c76dda12":"# building model with best parameters\nxgb_tuned_model = XGBClassifier(\n    n_estimators=150,\n    scale_pos_weight=10,\n    subsample=1,\n    reg_lambda=20,\n    max_depth=5,\n    learning_rate=0.01,\n    gamma=0,\n    eval_metric=loss_func,\n    random_state=seed,\n)\n# Fit the model on training data\nxgb_tuned_model.fit(X_train_un, y_train_un)","20038a7f":"xgb_tuned_model_score = get_metrics_score(\n    xgb_tuned_model, X_train, X_val, y_train, y_val\n)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nscoring = \"recall\"\nxgb_down_cv = cross_val_score(\n    estimator=xgb_tuned_model, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold\n)\n\n\nadd_score_model(\n    \"XGB Tuned with Down Sampling\", xgb_tuned_model_score, xgb_down_cv.mean()\n)","7363bbd8":"make_confusion_matrix(xgb_tuned_model, X_val, y_val)","ffe2f369":"%%time\n\n# defining model\nmodel = AdaBoostClassifier(random_state=seed)\n\n\n\n# Parameter grid to pass in RandomizedSearchCV\nparam_grid={'n_estimators':np.arange(50,2000,50),\n            'learning_rate':[0.01,0.1,0.2,0.05]} \n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nada_tuned = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=10, random_state=seed, n_jobs = -1)\n\n#Fitting parameters in RandomizedSearchCV\nada_tuned.fit(X_train_un,y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(ada_tuned.best_params_,ada_tuned.best_score_))","142e2891":"# building model with best parameters\nada_tuned_model = AdaBoostClassifier(\n    n_estimators=1050, learning_rate=0.1, random_state=seed\n)\n# Fit the model on training data\nada_tuned_model.fit(X_train_un, y_train_un)","15d3a108":"ada_tuned_model_score = get_metrics_score(\n    ada_tuned_model, X_train, X_val, y_train, y_val\n)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nscoring = \"recall\"\nada_down_cv = cross_val_score(\n    estimator=ada_tuned_model, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold\n)\n\n\nadd_score_model(\n    \"AdaBoost Tuned with Down Sampling\", ada_tuned_model_score, ada_down_cv.mean()\n)","fa3896b5":"make_confusion_matrix(ada_tuned_model, X_val, y_val)","9ddb2e1b":"%%time\n\n# defining model\nmodel = lgb.LGBMClassifier(random_state=seed)\n\n# Hyper parameters\nmin_gain_to_split = [0.01, 0.1, 0.2, 0.3]\nmin_data_in_leaf = [10, 20, 30, 40, 50]\nfeature_fraction = [0.8, 0.9, 1.0]\nmax_depth = [5, 8, 15, 25, 30]\nextra_trees = [True, False]\nlearning_rate = [0.01,0.1,0.2,0.05]\n\n# Parameter grid to pass in RandomizedSearchCV\nparam_grid={'min_gain_to_split': min_gain_to_split,\n               'min_data_in_leaf': min_data_in_leaf,\n               'feature_fraction': feature_fraction,\n               'max_depth': max_depth,\n               'extra_trees': extra_trees,\n               'learning_rate': learning_rate,\n               'boosting_type': ['gbdt'],\n               'objective': ['binary'],\n               'is_unbalance': [True],\n               'metric': ['binary_logloss'],} \n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nlgbm_tuned = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=10, random_state=seed, n_jobs = -1)\n\n#Fitting parameters in RandomizedSearchCV\nlgbm_tuned.fit(X_train_un,y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(lgbm_tuned.best_params_,lgbm_tuned.best_score_))","3bef210e":"# building model with best parameters\nlgbm_tuned_model = lgb.LGBMClassifier(\n               min_gain_to_split = 0.01,\n               min_data_in_leaf = 50,\n               feature_fraction = 0.8,\n               max_depth = 8,\n               extra_trees = False,\n               learning_rate = 0.2,\n               objective = 'binary',\n               metric = 'binary_logloss',\n               is_unbalance = True,\n               boosting_type = 'gbdt',\n               random_state = seed\n)\n# Fit the model on training data\nlgbm_tuned_model.fit(X_train_un, y_train_un)\n","9be2de17":"lgbm_tuned_model_score = get_metrics_score(\n    lgbm_tuned_model, X_train, X_val, y_train, y_val\n)\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nscoring = \"recall\"\nlgb_down_cv = cross_val_score(\n    estimator=lgbm_tuned_model, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold\n)\n\n\nadd_score_model(\n    \"Light GBM Tuned with Down Sampling\", lgbm_tuned_model_score, lgb_down_cv.mean()\n)","d9e6e573":"make_confusion_matrix(lgbm_tuned_model, X_val, y_val)","21e89ea7":"%%time\n\n# defining model\nmodel = GradientBoostingClassifier(random_state=seed)\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10, 15] \n\n\n# Parameter grid to pass in RandomizedSearchCV\nparam_grid={'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf} \n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\ngbm_tuned = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=10, random_state=seed, n_jobs = -1)\n\n#Fitting parameters in RandomizedSearchCV\ngbm_tuned.fit(X_train_un,y_train_un)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(gbm_tuned.best_params_,gbm_tuned.best_score_))","0a15acc3":"# building model with best parameters\ngbm_tuned_model = GradientBoostingClassifier(\n    n_estimators=700,\n    max_features=\"auto\",\n    max_depth=25,\n    min_samples_split=2,\n    min_samples_leaf=15,\n    random_state=seed,\n)\n# Fit the model on training data\ngbm_tuned_model.fit(X_train_un, y_train_un)","61ec78d7":"gbm_tuned_model_score = get_metrics_score(\n    gbm_tuned_model, X_train, X_val, y_train, y_val\n)\n\n\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n\nscoring = \"recall\"\ngbm_down_cv = cross_val_score(\n    estimator=gbm_tuned_model, X=X_train_un, y=y_train_un, scoring=scoring, cv=kfold\n)\n\n\nadd_score_model(\n    \"GBM Tuned with Down Sampling\", gbm_tuned_model_score, gbm_down_cv.mean()\n)","0f6fb643":"make_confusion_matrix(gbm_tuned_model, X_val, y_val)","d7b12868":"comparison_frame = pd.DataFrame(\n    {\n        \"Model\": model_names,\n        \"Cross_Val_Score_Train\": cross_val_train,\n        \"Train_Accuracy\": acc_train,\n        \"Test_Accuracy\": acc_test,\n        \"Train_Recall\": recall_train,\n        \"Test_Recall\": recall_test,\n        \"Train_Precision\": precision_train,\n        \"Test_Precision\": precision_test,\n        \"Train_F1\": f1_train,\n        \"Test_F1\": f1_test,\n        \"Train_ROC_AUC\": roc_auc_train,\n        \"Test_ROC_AUC\": roc_auc_test,\n    }\n)\n\n\nfor col in comparison_frame.select_dtypes(include=\"float64\").columns.tolist():\n    comparison_frame[col] = round(comparison_frame[col] * 100, 0).astype(int)\n\n\ncomparison_frame.tail(4).sort_values(\n    by=[\"Cross_Val_Score_Train\", \"Test_Recall\"], ascending=False\n)","9b877dab":"feature_names = X_train.columns\nimportances = gbm_tuned_model.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","5a3a9017":"gbm_tuned_model_test_score = get_metrics_score(\n    gbm_tuned_model, X_train, X_test, y_train, y_test\n)\n\nfinal_model_names = [\"gbm Tuned Down-sampled Trained\"]\nfinal_acc_train = [gbm_tuned_model_test_score[0]]\nfinal_acc_test = [gbm_tuned_model_test_score[1]]\nfinal_recall_train = [gbm_tuned_model_test_score[2]]\nfinal_recall_test = [gbm_tuned_model_test_score[3]]\nfinal_precision_train = [gbm_tuned_model_test_score[4]]\nfinal_precision_test = [gbm_tuned_model_test_score[5]]\nfinal_f1_train = [gbm_tuned_model_test_score[6]]\nfinal_f1_test = [gbm_tuned_model_test_score[7]]\nfinal_roc_auc_train = [gbm_tuned_model_test_score[8]]\nfinal_roc_auc_test = [gbm_tuned_model_test_score[9]]\n\nfinal_result_score = pd.DataFrame(\n    {\n        \"Model\": final_model_names,\n        \"Train_Accuracy\": final_acc_train,\n        \"Test_Accuracy\": final_acc_test,\n        \"Train_Recall\": final_recall_train,\n        \"Test_Recall\": final_recall_test,\n        \"Train_Precision\": final_precision_train,\n        \"Test_Precision\": final_precision_test,\n        \"Train_F1\": final_f1_train,\n        \"Test_F1\": final_f1_test,\n        \"Train_ROC_AUC\": final_roc_auc_train,\n        \"Test_ROC_AUC\": final_roc_auc_test,\n    }\n)\n\n\nfor col in final_result_score.select_dtypes(include=\"float64\").columns.tolist():\n    final_result_score[col] = final_result_score[col] * 100\n\n\nfinal_result_score","47c20696":"make_confusion_matrix(gbm_tuned_model, X_test, y_test)","fe7eb731":"import scikitplot as skplt\n\ny_pred_prob = gbm_tuned_model.predict_proba(X_test)\n\nskplt.metrics.plot_cumulative_gain(y_test, y_pred_prob, figsize=(15, 5))\nplt.show()","8174e313":"plot_roc_curve(gbm_tuned_model, X_test, y_test)\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1], [0, 1], \"b--\")\nplt.xlim([-0.05, 1])\nplt.ylim([0, 1.05])\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\nplt.show()","8c27a816":"# The static variables\n# Random state and loss\nseed = 1\nloss_func = \"logloss\"\n\n# Test and Validation sizes\ntest_size = 0.2\nval_size = 0.25\n\n# Dependent Varibale Value map\ntarget_mapper = {\"Attrited Customer\": 1, \"Existing Customer\": 0}\n\ndf_pipe = churner.copy()\ncat_columns = df_pipe.select_dtypes(include=\"object\").columns.tolist()\ndf_pipe[cat_columns] = df_pipe[cat_columns].astype(\"category\")","f14ca670":"X = df_pipe.drop(columns=[\"Attrition_Flag\"])\ny = df_pipe[\"Attrition_Flag\"].map(target_mapper)","162dbb51":"# Splitting data into training, validation and test set:\n# first we split data into 2 parts, say temporary and test\n\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=test_size, random_state=seed, stratify=y\n)\n\n# then we split the temporary set into train and validation\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=val_size, random_state=seed, stratify=y_temp\n)\nprint(X_train.shape, X_val.shape, X_test.shape)","629b0db0":"print(y_train.value_counts(normalize=True))\nprint(y_val.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","4b759ece":"under_sample = RandomUnderSampler(random_state=seed)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)","be8e487c":"# For dropping columns\ncolumns_to_drop = [\n    \"clientnum\",\n    \"credit_limit\",\n    \"dependent_count\",\n    \"months_on_book\",\n    \"avg_open_to_buy\",\n    \"customer_age\",\n]\n\n# For masking a particular value in a feature\ncolumn_to_mask_value = \"income_category\"\nvalue_to_mask = \"abc\"\nmasked_value = \"Unknown\"\n\n# One-hot encoding columns\ncolumns_to_encode = [\n    \"gender\",\n    \"education_level\",\n    \"marital_status\",\n    \"income_category\",\n    \"card_category\",\n]\n\n# Numerical Columns\nnum_columns = [\n    \"total_relationship_count\",\n    \"months_inactive_12_mon\",\n    \"contacts_count_12_mon\",\n    \"total_revolving_bal\",\n    \"total_amt_chng_q4_q1\",\n    \"total_trans_amt\",\n    \"total_trans_ct\",\n    \"total_ct_chng_q4_q1\",\n    \"avg_utilization_ratio\",\n]\n\n# Columns for null imputation with Unknown\ncolumns_to_null_imp_unknown = [\"education_level\", \"marital_status\"]","c1ff5a39":"# To Standardize feature names\nfeature_name_standardizer = FeatureNamesStandardizer()\n\n# To Drop unnecessary columns\ncolumn_dropper = ColumnDropper(features=columns_to_drop)\n\n# To Mask incorrect\/meaningless value of a feature\nvalue_masker = CustomValueMasker(\n    feature=column_to_mask_value, value_to_mask=value_to_mask, masked_value=masked_value\n)\n\n# Missing value imputation\nimputer = FillUnknown()\n\n# To encode the categorical data\none_hot = OneHotEncoder(handle_unknown=\"ignore\")\n\n# To scale numerical columns\nscaler = RobustScaler()\n\n\n# creating a transformer for feature name standardization and dropping columns\ncleanser = Pipeline(\n    steps=[\n        (\"feature_name_standardizer\", feature_name_standardizer),\n        (\"column_dropper\", column_dropper),\n        (\"value_mask\", value_masker),\n        (\"imputation\", imputer),\n    ]\n)\n\n# creating a transformer for data encoding\n\nencode_transformer = Pipeline(steps=[(\"onehot\", one_hot)])\nnum_scaler = Pipeline(steps=[(\"scale\", scaler)])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"encoding\", encode_transformer, columns_to_encode),\n        (\"scaling\", num_scaler, num_columns),\n    ],\n    remainder=\"passthrough\",\n)\n\n# Model\n\ngbm_tuned_model = GradientBoostingClassifier(\n    n_estimators=700,\n    max_features=\"auto\",\n    max_depth=25,\n    min_samples_split=2,\n    min_samples_leaf=15,\n    random_state=seed,\n)\n\n# Creating new pipeline with best parameters\nmodel_pipe = Pipeline(\n    steps=[\n        (\"cleanse\", cleanser),\n        (\"preprocess\", preprocessor),\n        (\"model\", gbm_tuned_model),\n    ]\n)\n# Fit the model on training data\nmodel_pipe.fit(X_train_un, y_train_un)","b0ac4e76":"print(\n    \"Accuracy on Test is: {}%\".format(round(model_pipe.score(X_test, y_test) * 100, 0))\n)","6481d5f6":"pred_train_p = model_pipe.predict_proba(X_train_un)[:, 1] > 0.5\npred_test_p = model_pipe.predict_proba(X_test)[:, 1] > 0.5\n\npred_train_p = np.round(pred_train_p)\npred_test_p = np.round(pred_test_p)\n\ntrain_acc_p = accuracy_score(pred_train_p, y_train_un)\ntest_acc_p = accuracy_score(pred_test_p, y_test)\n\ntrain_recall_p = recall_score(y_train_un, pred_train_p)\ntest_recall_p = recall_score(y_test, pred_test_p)","c81e8439":"print(\"Recall on Test is: {}%\".format(round(test_recall_p * 100, 0)))","f27d6f0b":"mask = np.zeros_like(data_clean.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.set(rc={\"figure.figsize\": (15, 15)})\n\nsns.heatmap(\n    data_clean.corr(),\n    cmap=sns.diverging_palette(20, 220, n=200),\n    annot=True,\n    mask=mask,\n    center=0,\n)\nplt.show()","d09c19f2":"## Test scores","dc89e2f7":"### Building the model with the resulted best parameters","ca6f6217":"The first step of univariate analysis is to check the distribution\/spread of the data. This is done using primarily `histograms` and `box plots`. Additionally we'll plot each numerical feature on `violin plot` and ` cumulative density distribution plot`. For these 4 kind of plots, we are building below `summary()` function to plot each of the numerical attributes. Also, we'll display feature-wise `5 point summary`.","a5021e6c":"# Building Models","83904032":"# Build Models with Undersampled Data","fcbb7d02":"We are now all set to build, train and validate the model","a9dfde1b":">- There are a total of 21 columns and 10,127 observations in the dataset\n>- We can see that `Education_Level` and `Marital_Status` have less than 10,127 non-null values i.e. columns have missing values.","fea4fabd":"### Heatmap to understand correlations between independent and dependent variables","dcb0de1a":"# Exploratory Data Analysis","c6f77c38":"## Defining the static variables","04f2b214":"### Numerical Feature Summary","657dafaa":"## Dependent and independent variables","426e2f5b":"## Unique values for Numerical Columns","2905c856":"### Confusion matrix on validation","d4ba78d8":"## Numerical column statistics","7f61271d":"> It appears Light GBM, XGBoost, GBM are the models with good potential. Ada Boost also looks good with the higher end outlier performance score","24ca7590":"## Build the pipeline","108c28e2":"Pre-processing steps:\n\n1. Data Split into Dependent and Target sets\n1. Data Split to Train, Test and Validation sets\n1. Standardize feature names\n1. Drop unnecessary columns (`Client Number`, `Customer Age`, `Dependent Count`, `Months on Book`, `Open to Buy`, `Credit Limit`)\n1. Missing Value\/Incorrect Value treatment\n1. Encoding\n1. Scaling\/Outlier treatment","8513caa2":">- The 4 best models are:\n>1. XGBoost trained with undersampled data\n>2. AdaBoost trained with undersampled data\n>3. Light GBM trained with undersampled data\n>4. GBM trained with undersampled data\n&nbsp;\n\n>We will now try to tune these 4 models using Random Search CV","1f73a509":"**Here we are converting Object data type to Category**","ae275613":"## Cumulative Lift\/Gain Chart","1dff40cc":"## Unique values for Category columns","9ab756f7":"Typically a hyperparameter has a known effect on a model in the general sense, but it is not clear how to best set a hyperparameter for a given dataset. Further, many machine learning models have a range of hyperparameters and they may interact in nonlinear ways.\n\nAs such, it is often required to search for a set of hyperparameters that result in the best performance of a model on a dataset. This is called hyperparameter optimization, hyperparameter tuning, or hyperparameter search.\n\nAn optimization procedure involves defining a search space. This can be thought of geometrically as an n-dimensional volume, where each hyperparameter represents a different dimension and the scale of the dimension are the values that the hyperparameter may take on, such as real-valued, integer-valued, or categorical.\n\n**Search Space:** Volume to be searched where each dimension represents a hyperparameter and each point represents one model configuration.\nA point in the search space is a vector with a specific value for each hyperparameter value. The goal of the optimization procedure is to find a vector that results in the best performance of the model after learning, such as maximum accuracy or minimum error.\n\nA range of different optimization algorithms may be used, although two of the simplest and most common methods are random search and grid search.\n\n**Random Search:** Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\n&nbsp;\n\n**Grid Search:** Define a search space as a grid of hyperparameter values and evaluate every position in the grid.\n\n","8a2c3083":"# Actionable Insights and Recommendations","f25b4536":"We'll move on to data analysis now.","f96e75fb":"### Building the model with the resulted best parameters","26b4113a":"## Build and Train Models","0d4b354d":">- `Average Open to Buy` has lots of higher end outliers, which means there are customers who uses only very small amount of their credit limit\n>- Data is right skewed","7e2102cc":"We are building and training the same 7 models as before. We are however going to use the over-sampled training data for training the models.","089ef2d2":"### Target vs. All Categorical Columns","0901e5f5":"### Get scores","36a38c90":">- Attrition does not seem to be related with Education","15f5f7d0":"We are building below 7 models:  \n&nbsp;\n\n\n1. Bagging\n1. Random Forest Classification\n1. Gradient Boosting Machine\n1. Adaptive Boosting\n1. eXtreme Gradient Boosting\n1. Decision Tree Classification (Classification and Regression Trees - CART)\n1. Light Gradient Boosting Machine\n\n&nbsp;\n\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.  \n&nbsp;\n\nSince it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms.\nBefore is a diagrammatic representation by the makers of the Light GBM to explain the difference clearly.\n\nSource: towards data science","6669e910":"### Target vs. All numerical columns","c1642952":"# Undersampling train data using Random Under Sampler","cb9aed04":"## Score the pipeline using test data","9bd13e6b":"## View top and bottom 5 records","f6c50f18":"### Get scores","aa8bce1a":"## Creating data copy","7950e6d3":"## Choice of models for tuning","53d18fb1":"# Model Building Considerations","0b11aed4":"### Function to Get Scores","5da61a05":"### Confusion matrix on validation","0d92818d":"### Building the model with the resulted best parameters","2948a0e7":">- The target variable `Attrition Flag` has `Existing to Attrited ratio` of `83.9 : 16.1`. There is imbalance in the dataset\n>- `~93%` customers are having `Blue Card`\n>- `Income Category` has a value `abc` for `10%` records, which we'll change to `Unknown`","1af24aa0":"# Productionizing the model","9389d164":"![Screenshot%202021-09-10%20at%2001.40.10.png](attachment:Screenshot%202021-09-10%20at%2001.40.10.png)","720cd6e4":"> The above chart shows, if we sort the customers in the descending order of the probability of attrition (class 1), and target the top ~30% of the population, we are most likely to find 100% of the people who would actually attrite.","09e54c92":"Data pre-processing is one of the the most important parts of the job before starting to train the model with the dataset. We need to impute missing values, fix any illogical data value in columns, convert category columns to numeric (either ordinal, or binary using one-hot encoding), scale the data to deal with the distribution skewness and outliers, before feeding the data to a model. \n&nbsp;\n\nWe are using the pre-available transformation classes and the custom classes that we created to first fit the training data and then transform the train, validation and test dataset. This is the standard logical practice to keep the influence of test and validation data in the train dataset to prevent\/avoid data leakage while training or validating the model.","78a3d5bb":"## Univariate Analysis","5ac5f169":"# Import the libraries","c841b262":"## Missing data","2b8d7623":"## Tuning AdaBoost with Down Sampled data","d41186c0":"We are plotting the cross validation results for the 7 models in a Box plot, to check which models are potentially good.","20478c41":"**Splitting the dataset into dependent and independent variable sets**","b6a91a6b":"## Shape of the data","652b9177":"> `Average utilization` is right skewed","df4ca37e":"## Dependent and independent variables","d7207bc0":"# Oversampling train data using SMOTE","ca3ec17a":"## Feature Importance","c6f3dd5b":"### Percentage on bar chart for Categorical Features","58f6d82b":"> The data is normally distributed, with only 2 outliers on the right side (higher end)","11d7dae1":"## Comparing Models","65765fe6":"## Tuning Light GBM with Down-Sampled data","7757fc47":"# Loading Data","7903ec8c":"# Check Test Data on GBM Tuned and Trained with Downsampled Data","6ac3d3b4":"## Tuning XGBOOST with Down Sampled data","0e68dd3c":"> Outliers are on both higher and lower end","c0815c38":"## Duplicates","c3e6215a":">- The best model with respect to cross validation score and test recall is `Light GBM`\n>- The next best models are `XGBoost`, `GBM` and `AdaBoost` respectively","30c0d270":"### Finding best parameter for high recall using Random Search with cross validation","1b681254":"## Data processing Steps","aa6fa907":">- There are clusters formed with respect to attrition for the variables `total revolving amount`, `total amount change Q4 to Q1`, `total transaction amount`, `total transaction count`, `total transaction count change Q4 to Q1`\n>- There are strong correlation between a few columns as well, which we'll check in below correlation heatmap. ","241ba36e":"Now that we have finalized our model, we'll build a model pipeline to streamline all the steps of model building.\nWe'll start will the initial dataset and proceed with the pipeline building steps.\n\nMachine Learning (ML) pipeline, theoretically, represents different steps including data transformation and prediction through which data passes. The outcome of the pipeline is the trained model which can be used for making the predictions. Sklearn.pipeline is a Python implementation of ML pipeline. Instead of going through the model fitting and data transformation steps for the training and test datasets separately, we can use Sklearn.pipeline to automate these steps. Here is the diagram representing the pipeline for training our machine learning model based on supervised learning, and then using test data to predict the labels.","9b34f9d6":">- Attrition does not seem to be related with Income Category","2d715eb0":"# Description\n\n## Background & Context\n\nThe Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances.\n\nCustomers\u2019 leaving credit cards services would lead bank to loss, so the bank wants to analyze the data of customers and identify the customers who will leave their credit card services and reason for same \u2013 so that bank could improve upon those areas\n\nAs a Data scientist at Thera bank, we need to come up with a classification model that will help the bank improve its services so that customers do not renounce their credit cards\n\nWe need to identify the best possible model that will give the required performance\n\n## Objective\n\n- Explore and visualize the dataset.\n- Build a classification model to predict if the customer is going to churn or not\n- Optimize the model using appropriate techniques\n- Generate a set of insights and recommendations that will help the bank\n\n## Data Dictionary\n\n- `CLIENTNUM`: Client number. Unique identifier for the customer holding the account  \n&nbsp;\n- `Attrition_Flag`: Internal event (customer activity) variable - if the account is closed then \"Attrited Customer\" else \"Existing Customer\"  \n&nbsp;\n- `Customer_Age`: Age in Years  \n&nbsp;\n- `Gender`: Gender of the account holder  \n&nbsp;\n- `Dependent_count`: Number of dependents  \n&nbsp;\n- `Education_Level`:  Educational Qualification of the account holder - Graduate, High School, Unknown, Uneducated, College(refers to a college student), Post-Graduate, Doctorate.  \n&nbsp;\n- `Marital_Status`: Marital Status of the account holder  \n&nbsp;\n- `Income_Category`: Annual Income Category of the account holder  \n&nbsp;\n- `Card_Category`: Type of Card  \n&nbsp;\n- `Months_on_book`: Period of relationship with the bank  \n&nbsp;\n- `Total_Relationship_Count`: Total no. of products held by the customer  \n&nbsp;\n- `Months_Inactive_12_mon`: No. of months inactive in the last 12 months  \n&nbsp;\n- `Contacts_Count_12_mon`: No. of Contacts between the customer and bank in the last 12 months  \n&nbsp;\n- `Credit_Limit`: Credit Limit on the Credit Card  \n&nbsp;\n- `Total_Revolving_Bal`: The balance that carries over from one month to the next is the revolving balance  \n&nbsp;\n- `Avg_Open_To_Buy`: Open to Buy refers to the amount left on the credit card to use (Average of last 12 months)  \n&nbsp;\n- `Total_Trans_Amt`: Total Transaction Amount (Last 12 months)  \n&nbsp;\n- `Total_Trans_Ct`: Total Transaction Count (Last 12 months)  \n&nbsp;\n- `Total_Ct_Chng_Q4_Q1`: Ratio of the total transaction count in 4th quarter and the total transaction count in 1st quarter  \n&nbsp;\n- `Total_Amt_Chng_Q4_Q1`: Ratio of the total transaction amount in 4th quarter and the total transaction amount in 1st quarter  \n&nbsp;\n- `Avg_Utilization_Ratio`: Represents how much of the available credit the customer spent  \n&nbsp;","d74b8474":">- `High Imbalance` in data since the existing vs. attrited customers ratio is 84:16\n>- Data is almost equally distributed between `Males and Females`\n>- `31%` customers are `Graduate`\n>- `~85%` customers are `either Single or Married`, where `46.7%` of the customers are `Married`\n>- `35%` customers earn `less than $40k` and `36%` earns `$60k or more`\n>- `~93%` customers have `Blue card`","0ef04b2c":">1. `XGBoost` with down-sampling has the best validation recall of 96.3%, along-with 95% cross validation score on train, and 0.99 AUC, which means is it has high possibility of performing very well in unseen dataset. There is a bit of over-fitting, which I expect to resolve by tuning.\n&nbsp;\n\n>2. `AdaBoost` is generalizing the model very well, it is neither over-fitting, nor has any bias, AUC is 0.985 and cross validation score on train is 93%, recall on validation set is same as XGBoost (96.3%). I expect to improve the model (~94% on validation set) via tuning.\n&nbsp;\n\n>3. `Light GBM` works really well in all aspects, but there is slight over-fitting problem, which I expect to resolve by tuning. Accuracy on validation is 94%, with cross validation score on train 95%, recall on validation ~96%, AUC is 0.99. This looks like a very promising model.\n&nbsp;\n\n>4. `GBM` is not overfitting, and neither it is suffering from bias or variance. Recall on validation is ~96%, accuracy on validation ~94%, AUC is ~0.99, cross validation score on train is ~95%. This would be my top choice because none of the training scores are 100%, meaning it is not trying to explain every single aspect of training data by overfitting it.","03f23d4c":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Description\" data-toc-modified-id=\"Description-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Description<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Background-&amp;-Context\" data-toc-modified-id=\"Background-&amp;-Context-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Background &amp; Context<\/a><\/span><\/li><li><span><a href=\"#Objective\" data-toc-modified-id=\"Objective-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Objective<\/a><\/span><\/li><li><span><a href=\"#Data-Dictionary\" data-toc-modified-id=\"Data-Dictionary-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Data Dictionary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Import-the-libraries\" data-toc-modified-id=\"Import-the-libraries-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Import the libraries<\/a><\/span><\/li><li><span><a href=\"#Loading-Data\" data-toc-modified-id=\"Loading-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Loading Data<\/a><\/span><\/li><li><span><a href=\"#Data-summary\" data-toc-modified-id=\"Data-summary-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Data summary<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Shape-of-the-data\" data-toc-modified-id=\"Shape-of-the-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Shape of the data<\/a><\/span><\/li><li><span><a href=\"#View-top-and-bottom-5-records\" data-toc-modified-id=\"View-top-and-bottom-5-records-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>View top and bottom 5 records<\/a><\/span><\/li><li><span><a href=\"#Data-Types\" data-toc-modified-id=\"Data-Types-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Data Types<\/a><\/span><\/li><li><span><a href=\"#Duplicates\" data-toc-modified-id=\"Duplicates-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Duplicates<\/a><\/span><\/li><li><span><a href=\"#Missing-data\" data-toc-modified-id=\"Missing-data-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Missing data<\/a><\/span><\/li><li><span><a href=\"#Unique-values-for-Category-columns\" data-toc-modified-id=\"Unique-values-for-Category-columns-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;<\/span>Unique values for Category columns<\/a><\/span><\/li><li><span><a href=\"#Unique-values-for-Numerical-Columns\" data-toc-modified-id=\"Unique-values-for-Numerical-Columns-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;<\/span>Unique values for Numerical Columns<\/a><\/span><\/li><li><span><a href=\"#Numerical-column-statistics\" data-toc-modified-id=\"Numerical-column-statistics-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;<\/span>Numerical column statistics<\/a><\/span><\/li><li><span><a href=\"#Categorical-column-statistics\" data-toc-modified-id=\"Categorical-column-statistics-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;<\/span>Categorical column statistics<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pre-EDA-data-processing\" data-toc-modified-id=\"Pre-EDA-data-processing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Pre-EDA data processing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dropping-Id-column\" data-toc-modified-id=\"Dropping-Id-column-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Dropping Id column<\/a><\/span><\/li><li><span><a href=\"#Treating-missing-values-in-Education-Level-and-Marital-Status\" data-toc-modified-id=\"Treating-missing-values-in-Education-Level-and-Marital-Status-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Treating missing values in <code>Education Level<\/code> and <code>Marital Status<\/code><\/a><\/span><\/li><li><span><a href=\"#Treating-Income-Category-=-abc\" data-toc-modified-id=\"Treating-Income-Category-=-abc-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Treating <code>Income Category<\/code> = <code>abc<\/code><\/a><\/span><\/li><li><span><a href=\"#Checking-operation-outcome\" data-toc-modified-id=\"Checking-operation-outcome-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Checking operation outcome<\/a><\/span><\/li><li><span><a href=\"#Data-type-conversions\" data-toc-modified-id=\"Data-type-conversions-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>Data type conversions<\/a><\/span><\/li><li><span><a href=\"#Standardizing-column-names\" data-toc-modified-id=\"Standardizing-column-names-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;<\/span>Standardizing column names<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Univariate-Analysis\" data-toc-modified-id=\"Univariate-Analysis-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Univariate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Numerical-Feature-Summary\" data-toc-modified-id=\"Numerical-Feature-Summary-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;<\/span>Numerical Feature Summary<\/a><\/span><\/li><li><span><a href=\"#Percentage-on-bar-chart-for-Categorical-Features\" data-toc-modified-id=\"Percentage-on-bar-chart-for-Categorical-Features-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;<\/span>Percentage on bar chart for Categorical Features<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bi-variate-Analysis\" data-toc-modified-id=\"Bi-variate-Analysis-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Bi-variate Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Target-vs.-All-numerical-columns\" data-toc-modified-id=\"Target-vs.-All-numerical-columns-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;<\/span>Target vs. All numerical columns<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#With-outliers\" data-toc-modified-id=\"With-outliers-6.2.1.1\"><span class=\"toc-item-num\">6.2.1.1&nbsp;&nbsp;<\/span>With outliers<\/a><\/span><\/li><li><span><a href=\"#Without-outliers\" data-toc-modified-id=\"Without-outliers-6.2.1.2\"><span class=\"toc-item-num\">6.2.1.2&nbsp;&nbsp;<\/span>Without outliers<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Target-vs.-All-Categorical-Columns\" data-toc-modified-id=\"Target-vs.-All-Categorical-Columns-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;<\/span>Target vs. All Categorical Columns<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Multi-variate-Plots\" data-toc-modified-id=\"Multi-variate-Plots-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Multi-variate Plots<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan\" data-toc-modified-id=\"Pairplot-of-all-available-numeric-columns,-hued-by-Personal-Loan-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;<\/span>Pairplot of all available numeric columns, hued by Personal Loan<\/a><\/span><\/li><li><span><a href=\"#Heatmap-to-understand-correlations-between-independent-and-dependent-variables\" data-toc-modified-id=\"Heatmap-to-understand-correlations-between-independent-and-dependent-variables-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;<\/span>Heatmap to understand correlations between independent and dependent variables<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Data Preprocessing<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Building-data-transformer-functions-and-classes\" data-toc-modified-id=\"Building-data-transformer-functions-and-classes-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Building data transformer functions and classes<\/a><\/span><\/li><li><span><a href=\"#Creating-data-copy\" data-toc-modified-id=\"Creating-data-copy-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Creating data copy<\/a><\/span><\/li><li><span><a href=\"#Defining-the-static-variables\" data-toc-modified-id=\"Defining-the-static-variables-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Defining the static variables<\/a><\/span><\/li><li><span><a href=\"#Data-Type-Conversions\" data-toc-modified-id=\"Data-Type-Conversions-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Data Type Conversions<\/a><\/span><\/li><li><span><a href=\"#Dependent-and-independent-variables\" data-toc-modified-id=\"Dependent-and-independent-variables-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;<\/span>Dependent and independent variables<\/a><\/span><\/li><li><span><a href=\"#Split-data-in-Train,-Validation-and-Test-sets\" data-toc-modified-id=\"Split-data-in-Train,-Validation-and-Test-sets-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;<\/span>Split data in Train, Validation and Test sets<\/a><\/span><\/li><li><span><a href=\"#Data-processing\" data-toc-modified-id=\"Data-processing-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;<\/span>Data processing<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Building-Considerations\" data-toc-modified-id=\"Model-Building-Considerations-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Model Building Considerations<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-evaluation-criterion:\" data-toc-modified-id=\"Model-evaluation-criterion:-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Model evaluation criterion:<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-can-make-wrong-predictions-as:\" data-toc-modified-id=\"Model-can-make-wrong-predictions-as:-8.1.1\"><span class=\"toc-item-num\">8.1.1&nbsp;&nbsp;<\/span>Model can make wrong predictions as:<\/a><\/span><\/li><li><span><a href=\"#Which-case-is-more-important?\" data-toc-modified-id=\"Which-case-is-more-important?-8.1.2\"><span class=\"toc-item-num\">8.1.2&nbsp;&nbsp;<\/span>Which case is more important?<\/a><\/span><\/li><li><span><a href=\"#How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?\" data-toc-modified-id=\"How-to-reduce-this-loss-i.e-need-to-reduce-False-Negatives?-8.1.3\"><span class=\"toc-item-num\">8.1.3&nbsp;&nbsp;<\/span>How to reduce this loss i.e need to reduce False Negatives?<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix\" data-toc-modified-id=\"Model-Evaluation-Functions---Scoring-&amp;-Confusion-Matrix-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Model Evaluation Functions - Scoring &amp; Confusion Matrix<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Function-to-Get-Scores\" data-toc-modified-id=\"Function-to-Get-Scores-8.2.1\"><span class=\"toc-item-num\">8.2.1&nbsp;&nbsp;<\/span>Function to Get Scores<\/a><\/span><\/li><li><span><a href=\"#Function-to-Draw-Confusion-Matrix\" data-toc-modified-id=\"Function-to-Draw-Confusion-Matrix-8.2.2\"><span class=\"toc-item-num\">8.2.2&nbsp;&nbsp;<\/span>Function to Draw Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Function-to-Add-Scores-to-Scoring-Lists\" data-toc-modified-id=\"Function-to-Add-Scores-to-Scoring-Lists-8.2.3\"><span class=\"toc-item-num\">8.2.3&nbsp;&nbsp;<\/span>Function to Add Scores to Scoring Lists<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Building-Models\" data-toc-modified-id=\"Building-Models-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Building Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-and-Train-Models\" data-toc-modified-id=\"Build-and-Train-Models-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Build and Train Models<\/a><\/span><\/li><li><span><a href=\"#Comparing-Models\" data-toc-modified-id=\"Comparing-Models-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Comparing Models<\/a><\/span><\/li><li><span><a href=\"#Plotting-the-cross-validation-result-comparison\" data-toc-modified-id=\"Plotting-the-cross-validation-result-comparison-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Plotting the cross-validation result comparison<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Oversampling-train-data-using-SMOTE\" data-toc-modified-id=\"Oversampling-train-data-using-SMOTE-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Oversampling train data using SMOTE<\/a><\/span><\/li><li><span><a href=\"#Build-Models-with-Oversampled-Data\" data-toc-modified-id=\"Build-Models-with-Oversampled-Data-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Build Models with Oversampled Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-and-Train-Models\" data-toc-modified-id=\"Build-and-Train-Models-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Build and Train Models<\/a><\/span><\/li><li><span><a href=\"#Comparing-Models\" data-toc-modified-id=\"Comparing-Models-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Comparing Models<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Undersampling-train-data-using-Random-Under-Sampler\" data-toc-modified-id=\"Undersampling-train-data-using-Random-Under-Sampler-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;<\/span>Undersampling train data using Random Under Sampler<\/a><\/span><\/li><li><span><a href=\"#Build-Models-with-Undersampled-Data\" data-toc-modified-id=\"Build-Models-with-Undersampled-Data-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;<\/span>Build Models with Undersampled Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Build-and-Train-Models\" data-toc-modified-id=\"Build-and-Train-Models-13.1\"><span class=\"toc-item-num\">13.1&nbsp;&nbsp;<\/span>Build and Train Models<\/a><\/span><\/li><li><span><a href=\"#Comparing-Models\" data-toc-modified-id=\"Comparing-Models-13.2\"><span class=\"toc-item-num\">13.2&nbsp;&nbsp;<\/span>Comparing Models<\/a><\/span><\/li><li><span><a href=\"#Choice-of-models-for-tuning\" data-toc-modified-id=\"Choice-of-models-for-tuning-13.3\"><span class=\"toc-item-num\">13.3&nbsp;&nbsp;<\/span>Choice of models for tuning<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Model-Tuning-using-RandomizedSearchCV\" data-toc-modified-id=\"Model-Tuning-using-RandomizedSearchCV-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;<\/span>Model Tuning using RandomizedSearchCV<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Tuning-XGBOOST-with-Down-Sampled-data\" data-toc-modified-id=\"Tuning-XGBOOST-with-Down-Sampled-data-14.1\"><span class=\"toc-item-num\">14.1&nbsp;&nbsp;<\/span>Tuning XGBOOST with Down Sampled data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation\" data-toc-modified-id=\"Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation-14.1.1\"><span class=\"toc-item-num\">14.1.1&nbsp;&nbsp;<\/span>Finding best parameter for high recall using Random Search with cross validation<\/a><\/span><\/li><li><span><a href=\"#Building-the-model-with-the-resulted-best-parameters\" data-toc-modified-id=\"Building-the-model-with-the-resulted-best-parameters-14.1.2\"><span class=\"toc-item-num\">14.1.2&nbsp;&nbsp;<\/span>Building the model with the resulted best parameters<\/a><\/span><\/li><li><span><a href=\"#Get-scores\" data-toc-modified-id=\"Get-scores-14.1.3\"><span class=\"toc-item-num\">14.1.3&nbsp;&nbsp;<\/span>Get scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-matrix-on-validation\" data-toc-modified-id=\"Confusion-matrix-on-validation-14.1.4\"><span class=\"toc-item-num\">14.1.4&nbsp;&nbsp;<\/span>Confusion matrix on validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Tuning-AdaBoost-with-Down-Sampled-data\" data-toc-modified-id=\"Tuning-AdaBoost-with-Down-Sampled-data-14.2\"><span class=\"toc-item-num\">14.2&nbsp;&nbsp;<\/span>Tuning AdaBoost with Down Sampled data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation\" data-toc-modified-id=\"Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation-14.2.1\"><span class=\"toc-item-num\">14.2.1&nbsp;&nbsp;<\/span>Finding best parameter for high recall using Random Search with cross validation<\/a><\/span><\/li><li><span><a href=\"#Building-the-model-with-the-resulted-best-parameters\" data-toc-modified-id=\"Building-the-model-with-the-resulted-best-parameters-14.2.2\"><span class=\"toc-item-num\">14.2.2&nbsp;&nbsp;<\/span>Building the model with the resulted best parameters<\/a><\/span><\/li><li><span><a href=\"#Get-scores\" data-toc-modified-id=\"Get-scores-14.2.3\"><span class=\"toc-item-num\">14.2.3&nbsp;&nbsp;<\/span>Get scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-matrix-on-validation\" data-toc-modified-id=\"Confusion-matrix-on-validation-14.2.4\"><span class=\"toc-item-num\">14.2.4&nbsp;&nbsp;<\/span>Confusion matrix on validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Tuning-Light-GBM-with-Down-Sampled-data\" data-toc-modified-id=\"Tuning-Light-GBM-with-Down-Sampled-data-14.3\"><span class=\"toc-item-num\">14.3&nbsp;&nbsp;<\/span>Tuning Light GBM with Down-Sampled data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation\" data-toc-modified-id=\"Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation-14.3.1\"><span class=\"toc-item-num\">14.3.1&nbsp;&nbsp;<\/span>Finding best parameter for high recall using Random Search with cross validation<\/a><\/span><\/li><li><span><a href=\"#Building-the-model-with-the-resulted-best-parameters\" data-toc-modified-id=\"Building-the-model-with-the-resulted-best-parameters-14.3.2\"><span class=\"toc-item-num\">14.3.2&nbsp;&nbsp;<\/span>Building the model with the resulted best parameters<\/a><\/span><\/li><li><span><a href=\"#Get-scores\" data-toc-modified-id=\"Get-scores-14.3.3\"><span class=\"toc-item-num\">14.3.3&nbsp;&nbsp;<\/span>Get scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-matrix-on-validation\" data-toc-modified-id=\"Confusion-matrix-on-validation-14.3.4\"><span class=\"toc-item-num\">14.3.4&nbsp;&nbsp;<\/span>Confusion matrix on validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Tuning-GBM-with-Down-Sampled-data\" data-toc-modified-id=\"Tuning-GBM-with-Down-Sampled-data-14.4\"><span class=\"toc-item-num\">14.4&nbsp;&nbsp;<\/span>Tuning GBM with Down Sampled data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation\" data-toc-modified-id=\"Finding-best-parameter-for-high-recall-using-Random-Search-with-cross-validation-14.4.1\"><span class=\"toc-item-num\">14.4.1&nbsp;&nbsp;<\/span>Finding best parameter for high recall using Random Search with cross validation<\/a><\/span><\/li><li><span><a href=\"#Building-the-model-with-the-resulted-best-parameters\" data-toc-modified-id=\"Building-the-model-with-the-resulted-best-parameters-14.4.2\"><span class=\"toc-item-num\">14.4.2&nbsp;&nbsp;<\/span>Building the model with the resulted best parameters<\/a><\/span><\/li><li><span><a href=\"#Get-scores\" data-toc-modified-id=\"Get-scores-14.4.3\"><span class=\"toc-item-num\">14.4.3&nbsp;&nbsp;<\/span>Get scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-matrix-on-validation\" data-toc-modified-id=\"Confusion-matrix-on-validation-14.4.4\"><span class=\"toc-item-num\">14.4.4&nbsp;&nbsp;<\/span>Confusion matrix on validation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Comparing-Models\" data-toc-modified-id=\"Comparing-Models-14.5\"><span class=\"toc-item-num\">14.5&nbsp;&nbsp;<\/span>Comparing Models<\/a><\/span><\/li><li><span><a href=\"#Final-Model-Selection\" data-toc-modified-id=\"Final-Model-Selection-14.6\"><span class=\"toc-item-num\">14.6&nbsp;&nbsp;<\/span>Final Model Selection<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Check-Test-Data-on-GBM-Tuned-and-Trained-with-Downsampled-Data\" data-toc-modified-id=\"Check-Test-Data-on-GBM-Tuned-and-Trained-with-Downsampled-Data-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;<\/span>Check Test Data on GBM Tuned and Trained with Downsampled Data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><li><span><a href=\"#Test-scores\" data-toc-modified-id=\"Test-scores-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;<\/span>Test scores<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-15.3\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Cumulative-Lift\/Gain-Chart\" data-toc-modified-id=\"Cumulative-Lift\/Gain-Chart-15.4\"><span class=\"toc-item-num\">15.4&nbsp;&nbsp;<\/span>Cumulative Lift\/Gain Chart<\/a><\/span><\/li><li><span><a href=\"#ROC-AUC-Curve\" data-toc-modified-id=\"ROC-AUC-Curve-15.5\"><span class=\"toc-item-num\">15.5&nbsp;&nbsp;<\/span>ROC-AUC Curve<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Productionizing-the-model\" data-toc-modified-id=\"Productionizing-the-model-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;<\/span>Productionizing the model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Set-Static-variables\" data-toc-modified-id=\"Set-Static-variables-16.1\"><span class=\"toc-item-num\">16.1&nbsp;&nbsp;<\/span>Set Static variables<\/a><\/span><\/li><li><span><a href=\"#Dependent-and-independent-variables\" data-toc-modified-id=\"Dependent-and-independent-variables-16.2\"><span class=\"toc-item-num\">16.2&nbsp;&nbsp;<\/span>Dependent and independent variables<\/a><\/span><\/li><li><span><a href=\"#Split-data-in-Train,-Validation-and-Test-sets\" data-toc-modified-id=\"Split-data-in-Train,-Validation-and-Test-sets-16.3\"><span class=\"toc-item-num\">16.3&nbsp;&nbsp;<\/span>Split data in Train, Validation and Test sets<\/a><\/span><\/li><li><span><a href=\"#Undersampling-the-training-data-since-that-generalized-this-model-really-well\" data-toc-modified-id=\"Undersampling-the-training-data-since-that-generalized-this-model-really-well-16.4\"><span class=\"toc-item-num\">16.4&nbsp;&nbsp;<\/span>Undersampling the training data since that generalized this model really well<\/a><\/span><\/li><li><span><a href=\"#Data-processing-Steps\" data-toc-modified-id=\"Data-processing-Steps-16.5\"><span class=\"toc-item-num\">16.5&nbsp;&nbsp;<\/span>Data processing Steps<\/a><\/span><\/li><li><span><a href=\"#Build-the-pipeline\" data-toc-modified-id=\"Build-the-pipeline-16.6\"><span class=\"toc-item-num\">16.6&nbsp;&nbsp;<\/span>Build the pipeline<\/a><\/span><\/li><li><span><a href=\"#Score-the-pipeline-using-test-data\" data-toc-modified-id=\"Score-the-pipeline-using-test-data-16.7\"><span class=\"toc-item-num\">16.7&nbsp;&nbsp;<\/span>Score the pipeline using test data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Accuracy\" data-toc-modified-id=\"Accuracy-16.7.1\"><span class=\"toc-item-num\">16.7.1&nbsp;&nbsp;<\/span>Accuracy<\/a><\/span><\/li><li><span><a href=\"#Recall\" data-toc-modified-id=\"Recall-16.7.2\"><span class=\"toc-item-num\">16.7.2&nbsp;&nbsp;<\/span>Recall<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Actionable-Insights-and-Recommendations\" data-toc-modified-id=\"Actionable-Insights-and-Recommendations-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;<\/span>Actionable Insights and Recommendations<\/a><\/span><\/li><\/ul><\/div>","32299273":"Goal of `Bi-variate` analysis is to find inter-dependencies between features.","314d8977":"> `Dependent Count` is mostly `2 or 3`","42fcdf48":"Converting the data type of the category variables from object\/float to category","6440191b":"# Data summary","bb58781a":"## Comparing Models","68b64fa0":"**Checking the ratio of labels in the target column for each of the data segments**","b15c3f69":"## Plotting the cross-validation result comparison","e2635ed0":"## Undersampling the training data since that generalized this model really well","e3c7bd09":"***Note***:\n&nbsp;\nThe missing value treatment should be done after splitting the data into Train, Validation and Test sets. However, in this case, the treatment is generic, since we are filling in the data with `Unknown`. Thus, the treatment can be done on the overall dataset. Similar strategy is applicable for treating the `Income Category` column value `abc`","0f11edff":"## Tuning GBM with Down Sampled data","d196d78f":"Our dataset has a huge imbalance in target variable labels. To deal with such datasets, we have a few tricks up our sleeves, which we call `Imbalanced Classification`.\n\nImbalanced classification involves developing predictive models on classification datasets that have a severe class imbalance.\n\nThe challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on the minority class, although typically it is performance on the minority class that is most important, which is the case in our study here.\n\nOne approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the `Synthetic Minority Oversampling Technique, or SMOTE` for short.","2d0376f8":"## Treating missing values in `Education Level` and `Marital Status`","111f4153":">All the null data values have been treated along with the incorrect\/junk data in Income Category column","14767e1e":"## Split data in Train, Validation and Test sets","5d0976ea":">- Age has only `45` unique values i.e. most of the customers are of similar age\n","87e61e94":"## Comparing Models","27934bdd":">- The `XGBoost model with hyper parameter tuning and trained with undersampled dataset`, has best recall on validation set of ~99%, but accuracy is lower than the human level accuracy (i,e, classifying everyone as non-attriting customers). Thus, we are not selecting this model as the final model  \n&nbsp;\n>- The validation recall of ~97% is provided by the `GBM with hyper parameter tuning trained with undersampled dataset`, has validation accuracy of ~94%, and precision of ~74%, Validation AUC ~99%, Cross Validation Mean of 96%. Also, the model is neither suffering from bias, nor variance.  We are selecting `GBM Tuned with Down Sampling` model as our final model","36e6038b":"> There are higher end outliers in `Credit Limit`. This might be because the customers are high end.","29c2baae":"## Building data transformer functions and classes","515b6a71":"## Set Static variables","b7b587ae":"Firstly we'll work on building models individually after data pre-processing, and later we'll build an ML pipeline to run end to end process of pre-processing and model building. We are creating a data copy for the first part.","dfbd4b3a":">- The best 4 models with respect to validation recall and cross validation score, are as follows:\n>1. Light GBM trained with over\/up-sampled data\n>1. GBM trained with over\/up-sampled data\n>1. AdaBoost trained with over\/up-sampled data\n>1. XGBoost trained with over\/up-sampled data","fbffe969":">- Mean value for the `Customer Age` column is approx 46 and the median is also 46. This shows that majority of the customers are under 46 years of age.\n>- `Dependent Count` column has mean and median of `~2`\n>- `Months on Book` column has mean and median of `36` months. `Minimum` value is 13 months, showing that the dataset captures data for customers with the bank at least 1 whole years\n>- `Total Relationship Count` has mean and median of `~4`\n>- `Credit Limit` has a wide range of `1.4K to 34.5K`, the median being `4.5K`, way less than the mean `8.6K`\n>- `Total Transaction Count` has mean of `~65` and median of `67`","0a5ebfd5":">- Attrition does not seem to be related with Marital Status","eb70875d":"## ROC-AUC Curve","03386e36":"## Checking operation outcome","2902fdba":"#### With outliers","c8bec17d":"## Split data in Train, Validation and Test sets","dca58819":"Cumulative lift\/gain chart is important to understand how a model would perform in production system with unseen data","ec4c1e41":"We are creating a few functions to score the models, show the confusion matrix","ddc368e0":"## Model evaluation criterion:\n\n### Model can make wrong predictions as:\n1. Predicting a customer will attrite and the customer does not attrite - Loss of resources\n2. Predicting a customer will not attrite and the customer attrites - Loss of opportunity for churning the customer\n\n### Which case is more important? \n* Predicting that customer will not attrite, but actually attrites, would result in loss for the bank since if predicted correctly, marketing\/sales team could have contacted the customer to retain them. This would result in losses. So, the false negatives should be minimized.\n\n### How to reduce this loss i.e need to reduce False Negatives?\n* Company wants Recall to be maximized, greater the Recall lesser the chances of false negatives.","7f13382d":">- The most important features to understand customer credit card churn, are\n> 1. Total Transaction Count\n> 2. Total Transaction Amount\n> 3. Total Revolving Balance\n> 4. Total Amount Change Q4 to Q1\n> 5. Total Count Change Q4 to Q1\n> 6. Total Relationship Count  \n>&nbsp;\n>\n>- All of these features are negatively correlated with the Attrition Flag, meaning, the lower the values of these features, the higher the chances of a customer to attrite  \n>&nbsp;\n>\n>- Bank should connect with the customer more often to increase the connect, and provide the customer with various offers and schemes to increase relationships of the customer with the bank  \n>&nbsp;\n>\n>- Bank should offer cashback schemes on credit cards, which might encourage the customers on using the credit card more often  \n>&nbsp;\n>\n>- Bank should also offer credit limit increase for the customers who are regularly using the credit card. This should increase the credit card spends\/transaction amounts.  \n>&nbsp;\n>\n>- 0% interest EMI on credit card is also another offer that can be provided to customers to encourage the customers buy products of higher cost with credit card, and convert the expenditure to EMI, so that total transaction amount as well as transaction counts would increase. The balance would also revolve nicely.  \n>&nbsp;\n>\n>- Along with the available types of cards, bank can introduce credit cards specific to online shopping (with % cashback offers) or online food ordering. This way the card will be used more frequently.  \n>&nbsp;\n>\n>- With our model, we can predict which customers are likely to attrite, and according to the predicted probability, at least top 20-30% customers can be reached out to discuss credit card offers, credit limit increase etc, to try retain those customers.","6ca3f394":">- `Platinum` card holder are appearing to be having attrition tendency, however, since there are only 20 data points for platinum card holders, this observation would be biased","bac53b5f":"## Comparing Models","1c063a19":"#### Without outliers","573fd4f5":">- Again lower and higher end outliers are noticed.\n>- Here less number of contacts between the bank and the customer should be interesting to be checked","859b4382":"> Our model appears to be really good, since the AUC is almost 1.","0dd278c2":"### Building the model with the resulted best parameters","5a88ffdc":"### Finding best parameter for high recall using Random Search with cross validation","06da789a":"## Model Evaluation Functions - Scoring & Confusion Matrix","9f06f480":"> Attrited customers have\n&nbsp;\n>- Lower `total transaction amount`\n>- Lower `total transaction count`\n>- Lower `utilization ratio`\n>- Lower `transaction count change Q4 to Q1`\n>- Higher `number of times contacted with or by the bank`","71bf812d":">- `Credit Limit` and `Average Open to Buy` have 100% collinearity\n>- `Months on book` and `Customer Age` have quite strong correlation\n>- `Average Utilization Ration` and `Total Revolving Balance` are also a bit correlated it appears\n>- `Attrition Flag` does not have highly strong correlation with any of the numeric variables\n>- Customer Churn appears to be uncorrelated with `Customer Age`, `Dependent Count`, `Months on Book`, `Open to Buy`, `Credit Limit`, we'll remove these from dataset","a104c3f0":"## Categorical column statistics","bc7e912a":">- Attrition does not seem to be related with Gender","d64a7f04":"## Build and Train Models","dcc4709e":"## Dropping Id column","59292607":"## Final Model Selection","c52bef3d":"The performance of the model with the test data is almost similar to the performance on the validation dataset.","5888b40b":"## Data processing","1e43bf53":"## Standardizing column names","c7817741":"If the model can not distinguish the classes well, the Area Under Curve is really low, close to 0.5.","882c3484":"## Treating `Income Category` = `abc`","5b8c061b":"### Function to Add Scores to Scoring Lists","55c948ed":"Let's check the performance of the model on Test (unseen) dataset.","d88663af":"> Most of the customers have `4 or more` relations with the bank","363e16d9":"### Pairplot of all available numeric columns, hued by Personal Loan","5de407ef":"> The customers with `credit limit` more than `23K` have `~87%` people earning `$60K or more`, and `90%` have `Blue or Silver` card","9599766b":"## Data Types","1979434c":"We are building 8 models here, Logistic Regression, Bagging, Random Forest, Gradient Boosting, Ada Boosting, Extreme Gradient Boosting, Decision Tree, and Light Gradient Boosting.","4e7ba518":"> `Total revolving balance` of 0 would mean the customer never uses the credit card","96fb7076":"Removing the spaces from column names, and standardizing the column names to lower case","179b5c75":"# Pre-EDA data processing","7cf6bd2d":">- Most customers are on the books for `3 years`\n>- There are outliers on both lower and higher end","a70bf5ab":"ROC AUC characteristic is important to understand how good the model is.\n\nIf the model is really good in identifying the classes, the Area Under Curve is really high, close to 1.","b2198d49":"> `Total Transaction Amount` has lots of higher end outliers","5c220bbf":"### Get scores","48907613":"# Model Tuning using RandomizedSearchCV","3b573e0e":"### Function to Draw Confusion Matrix","54bcc9dc":"## Multi-variate Plots","a5991e74":"### Accuracy","1b0f325b":"## Build and Train Models","11cabd3f":"We are again building the same 7 models as before and training with the undersampled dataset, and use the validation dataset to score the models.","21f19ac0":"**Let's start by building different models using KFold and cross_val_score and tune the best model using RandomizedSearchCV**\n\n- `Stratified K-Folds cross-validation` provides dataset indices to split data into train\/validation sets. Split dataset into k consecutive folds (without shuffling by default) keeping the distribution of both classes in each fold the same as the target variable. Each fold is then used once as validation while the k - 1 remaining folds form the training set.","9d9db987":"### Confusion matrix on validation","4bc9bbca":"### Finding best parameter for high recall using Random Search with cross validation","68e91b19":"# Build Models with Oversampled Data","3c2e87be":"For the categorical variables, it is best to analyze them at percentage of total on bar charts\nBelow function takes a category column as input and plots bar chart with percentages on top of each bar","a4737f51":"## Data type conversions","9fb035df":"> Outliers are on both higher and lower end","0358b2b0":"**Let's check the number of unique values in each column**","78aee604":"## Bi-variate Analysis","1c6e4fd3":"# Data Preprocessing","c1ed6e77":"## Data Type Conversions","150b6777":"### Get scores","839ebba7":"## Confusion Matrix","6f90c791":"### Recall","cbca72a0":">- There are lower and higher end outliers for `Months inactive in last 12 months`\n>- Lower end outliers are not concerning since 0 value means the customer is always active. The customers who are inactive for 5 or more months are to be concerned about.","e2820c68":"Undersampling is another way of dealing with imbalance in the dataset.\n\nRandom undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset until a balanced dataset is created.","a428054e":">- No missing values","962c6250":"### Confusion matrix on validation","dc3f5b48":"### Finding best parameter for high recall using Random Search with cross validation"}}