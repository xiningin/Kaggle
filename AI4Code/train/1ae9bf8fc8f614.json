{"cell_type":{"190249c1":"code","d68b3b34":"code","6f1c91b1":"code","90456548":"code","b3bfc271":"code","8ec38971":"code","b83186b2":"code","0d17b6e0":"code","023c8c84":"code","40f8c73c":"code","0b5279c0":"code","1edcadf1":"code","af05576c":"code","32562f8d":"code","a4a45e63":"code","e1212de0":"code","c531a476":"code","76162111":"code","64aabc03":"code","0a1c1314":"code","a03bcc10":"code","d643a1c6":"code","b8df90d9":"code","827987c0":"code","7df745f3":"code","ab5b2c42":"code","1ec28085":"code","5a2e602b":"code","0ed8294c":"code","0655f257":"code","65424816":"code","2daa3a07":"code","84b2e352":"code","86ad8fa7":"code","5f028652":"code","55f72886":"code","7299b3ea":"code","c128ef76":"code","adc4ae9c":"code","fc997870":"code","7f9bbe40":"code","51fd8fc6":"code","299927e2":"code","94eac6f7":"code","d1f8b20b":"code","bf49db08":"code","257a19d6":"code","2a075ffb":"code","7e98fd82":"code","0d0b339e":"code","0ddff499":"code","0576d7dd":"code","3a1bf9c5":"code","ad98a5be":"code","7dd31191":"code","488d8de2":"code","dae875ce":"code","a2a940a6":"code","70594642":"code","e70cbc98":"code","ba33e40b":"code","b45c8e9d":"code","16fb9181":"code","71d5dacb":"code","03a755fb":"code","a50cc261":"code","7e1267fb":"code","a7aca001":"code","8fd0c7b6":"code","257a0798":"code","7e8a8f0a":"code","4b6d6e25":"code","724c6384":"code","5af4ec02":"code","ebbed42a":"code","0eef34d9":"code","067d39e9":"code","d2440907":"code","9f943941":"code","6a69baa9":"code","cf922100":"code","b07e8427":"code","4af15af0":"code","1064f6a5":"code","73dc68e1":"code","58220023":"code","c394e80f":"code","703b3012":"code","6095ac8e":"code","319dc6a7":"code","1737da4c":"code","6c6a0211":"code","eac9d2d1":"code","e7a10dfa":"code","88a071ba":"code","4f2552aa":"code","220d837f":"code","56608ce4":"code","15e8801a":"code","7bad09eb":"code","293322e2":"code","d74f7ce7":"code","2c44a29f":"code","4de8c2c3":"code","4a76b830":"code","4fcb6141":"code","b47951f3":"code","ac95c20b":"code","30d31e64":"code","5f71589e":"code","e5ac53b0":"code","ea25f1e1":"code","da4190f5":"code","e842def5":"code","5b352380":"markdown","ec67e095":"markdown","1bb0c7ec":"markdown","b271482e":"markdown","d1eca6e9":"markdown","f4dc5fff":"markdown","a08e89f6":"markdown","81bed7de":"markdown","b4123503":"markdown","4ff655d7":"markdown","e2c68d4a":"markdown","488e02da":"markdown"},"source":{"190249c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.tabular import *\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim\n\nimport gc\ngc.collect()\n\nimport re\nimport os\nimport re\nimport gc\nimport pickle  \nimport random\nimport keras\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\n\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Lambda\nfrom keras.optimizers import Adam\nfrom keras.callbacks import Callback\nfrom scipy.stats import spearmanr, rankdata\nfrom os.path import join as path_join\nfrom numpy.random import seed\nfrom urllib.parse import urlparse\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom bayes_opt import BayesianOptimization\nfrom lightgbm import LGBMRegressor\nfrom nltk.tokenize import wordpunct_tokenize\nfrom nltk.stem.snowball import EnglishStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom functools import lru_cache\nfrom tqdm import tqdm as tqdm\nfrom fastai.text import *\nfrom fastai.metrics import *","d68b3b34":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 42\nseed_everything(SEED)","6f1c91b1":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\nsub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","90456548":"train.shape, test.shape, sub.shape","b3bfc271":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\n', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_numbers(x))\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","8ec38971":"import pandas as pd\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nimport re\nimport string","b83186b2":"uri_re = r'(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))'\n\ndef stripTagsAndUris(x):\n    if x:\n        # BeautifulSoup on content\n        soup = BeautifulSoup(x, \"html.parser\")\n        # Stripping all <code> tags with their content if any\n        if soup.code:\n            soup.code.decompose()\n        # Get all the text out of the html\n        text =  soup.get_text()\n        # Returning text stripping out all uris\n        return re.sub(uri_re, \"\", text)\n    else:\n        return \"\"","0d17b6e0":"train[\"question_title\"] = train[\"question_title\"].map(stripTagsAndUris)\ntrain[\"question_body\"] = train[\"question_body\"].map(stripTagsAndUris)\ntrain[\"answer\"] = train[\"answer\"].map(stripTagsAndUris)\n\ntest[\"question_title\"] = test[\"question_title\"].map(stripTagsAndUris)\ntest[\"question_body\"] = test[\"question_body\"].map(stripTagsAndUris)\ntest[\"answer\"] = test[\"answer\"].map(stripTagsAndUris)","023c8c84":"def removePunctuation(x):\n    # Lowercasing all words\n    x = x.lower()\n    # Removing non ASCII chars\n    x = re.sub(r'[^\\x00-\\x7f]',r' ',x)\n    # Removing (replacing with empty spaces actually) all the punctuations\n    return re.sub(\"[\"+string.punctuation+\"]\", \" \", x)","40f8c73c":"train[\"question_title\"] = train[\"question_title\"].map(removePunctuation)\ntrain[\"question_body\"] = train[\"question_body\"].map(removePunctuation)\ntrain[\"answer\"] = train[\"answer\"].map(removePunctuation)\n\ntest[\"question_title\"] = test[\"question_title\"].map(removePunctuation)\ntest[\"question_body\"] = test[\"question_body\"].map(removePunctuation)\ntest[\"answer\"] = test[\"answer\"].map(removePunctuation)","0b5279c0":"stops = set(stopwords.words(\"english\"))\ndef removeStopwords(x):\n    # Removing all the stopwords\n    filtered_words = [word for word in x.split() if word not in stops]\n    return \" \".join(filtered_words)","1edcadf1":"train[\"question_title\"] = train[\"question_title\"].map(removeStopwords)\ntrain[\"question_body\"] = train[\"question_body\"].map(removeStopwords)\ntrain[\"answer\"] = train[\"answer\"].map(removeStopwords)\n\ntest[\"question_title\"] = test[\"question_title\"].map(removeStopwords)\ntest[\"question_body\"] = test[\"question_body\"].map(removeStopwords)\ntest[\"answer\"] = test[\"answer\"].map(removeStopwords)","af05576c":"target_cols_questions = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written']\n\ntarget_cols_answers = ['answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']\n\ntargets = target_cols_questions + target_cols_answers\n\ninput_columns = ['question_title', 'question_body', 'answer']","32562f8d":"train = clean_data(train, ['answer', 'question_body', 'question_title'])\ntest = clean_data(test, ['answer', 'question_body', 'question_title'])","a4a45e63":"train.head()","e1212de0":"find = re.compile(r\"^[^.]*\")\n\ntrain['netloc_1'] = train['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_1'] = test['url'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_2'] = train['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_2'] = test['question_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\n\ntrain['netloc_3'] = train['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])\ntest['netloc_3'] = test['answer_user_page'].apply(lambda x: re.findall(find, urlparse(x).netloc)[0])","c531a476":"train = train[input_columns + targets]\ntest = test[input_columns]","76162111":"train, val = train_test_split(train, test_size=0.2, shuffle=True, random_state=42)","64aabc03":"train.shape, val.shape","0a1c1314":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","a03bcc10":"!ls ..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased","d643a1c6":"!ls ..\/input\/distilbertbaseuncased\/","b8df90d9":"from collections import defaultdict\nfrom dataclasses import dataclass\nimport functools\nimport gc\nimport itertools\nimport json\nfrom multiprocessing import Pool\nimport os\nfrom pathlib import Path\nimport random\nimport re\nimport shutil\nimport subprocess\nimport time\nfrom typing import Callable, Dict, List, Generator, Tuple\nfrom os.path import join as path_join\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json._json import JsonReader\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, Subset, DataLoader\n\nfrom transformers import BertTokenizer, AdamW, BertModel, BertPreTrainedModel, BertConfig, DistilBertConfig, DistilBertModel, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers.optimization import get_linear_schedule_with_warmup","827987c0":"# # From the Ref Kernel's\n# from math import floor, ceil\n\n# def _get_masks(tokens, max_seq_length):\n#     \"\"\"Mask for padding\"\"\"\n#     if len(tokens)>max_seq_length:\n#         raise IndexError(\"Token length more than max seq length!\")\n#     return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\n# def _get_segments(tokens, max_seq_length):\n#     \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    \n#     if len(tokens) > max_seq_length:\n#         raise IndexError(\"Token length more than max seq length!\")\n        \n#     segments = []\n#     first_sep = True\n#     current_segment_id = 0\n    \n#     for token in tokens:\n#         segments.append(current_segment_id)\n#         if token == \"[SEP]\":\n#             if first_sep:\n#                 first_sep = False \n#             else:\n#                 current_segment_id = 1\n#     return segments + [0] * (max_seq_length - len(tokens))\n\n# def _get_ids(tokens, tokenizer, max_seq_length):\n#     \"\"\"Token ids from Tokenizer vocab\"\"\"\n    \n#     token_ids = tokenizer.convert_tokens_to_ids(tokens)\n#     input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n#     return input_ids\n\n# def _trim_input(title, question, answer, max_sequence_length=512, t_max_len=30, q_max_len=239, a_max_len=239):\n    \n#     #293+239+30 = 508 + 4 = 512\n#     t = tokenizer.tokenize(title)\n#     q = tokenizer.tokenize(question)\n#     a = tokenizer.tokenize(answer)\n    \n#     t_len = len(t)\n#     q_len = len(q)\n#     a_len = len(a)\n\n#     if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n#         if t_max_len > t_len:\n#             t_new_len = t_len\n#             a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n#             q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n#         else:\n#             t_new_len = t_max_len\n      \n#         if a_max_len > a_len:\n#             a_new_len = a_len \n#             q_new_len = q_max_len + (a_max_len - a_len)\n#         elif q_max_len > q_len:\n#             a_new_len = a_max_len + (q_max_len - q_len)\n#             q_new_len = q_len\n#         else:\n#             a_new_len = a_max_len\n#             q_new_len = q_max_len\n            \n            \n#         if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n#             raise ValueError(\"New sequence length should be %d, but is %d\"%(max_sequence_length, (t_new_len + a_new_len + q_new_len + 4)))\n        \n#         t = t[:t_new_len]\n#         q = q[:q_new_len]\n#         a = a[:a_new_len]\n    \n#     return t, q, a\n\n# def _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n#     \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n#     stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n#     input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n#     input_masks = _get_masks(stoken, max_sequence_length)\n#     input_segments = _get_segments(stoken, max_sequence_length)\n\n#     return [input_ids, input_masks, input_segments]\n\n# # def compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n# #     input_ids, input_masks, input_segments = [], [], []\n# #     for _, instance in tqdm(df[columns].iterrows()):\n# #         t, q, a = instance.question_title, instance.question_body, instance.answer\n# #         t, q, a = _trim_input(t, q, a, max_sequence_length)\n# #         ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n# #         input_ids.append(ids)\n# #         input_masks.append(masks)\n# #         input_segments.append(segments)\n# #     return [\n# #         torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n# #         torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n# #         torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n# #     ]\n\n# # def compute_output_arrays(df, columns):\n# #     return np.asarray(df[columns])","7df745f3":"# tokenizer = BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt\")\n# input_categories_train = list(train.columns[[0,1,2]])\n# input_categories_val = list(val.columns[[0,1,2]])\n# input_categories_test = list(test.columns[[0,1,2]])","ab5b2c42":"# input_categories_test","1ec28085":"# max_sequence_length = 512\n# input_ids_train, input_masks_train, input_segments_train = _convert_to_bert_inputs(train['question_title'], train['question_body'], train['answer'], tokenizer, max_sequence_length)","5a2e602b":"# %%time\n# outputs_train = compute_output_arrays(train, columns = targets)\n# outputs_val = compute_output_arrays(val, columns = targets)\n\n# inputs_train = compute_input_arays(train, input_categories_train, tokenizer, max_sequence_length=512)\n# inputs_val = compute_input_arays(val, input_categories_val, tokenizer, max_sequence_length=512)\n\n# test_inputs = compute_input_arays(test, input_categories_test, tokenizer, max_sequence_length=512)","0ed8294c":"# %%time\n# lengths_train = np.argmax(inputs_train[0] == 0, axis=1)\n# lengths_train[lengths_train == 0] = inputs_train[0].shape[1]\n# y_train_torch = torch.tensor(train[targets].values, dtype=torch.float32)\n\n# lengths_val = np.argmax(inputs_val[0] == 0, axis=1)\n# lengths_val[lengths_val == 0] = inputs_val[0].shape[1]\n# y_val_torch = torch.tensor(val[targets].values, dtype=torch.float32)\n\n# sequences = np.array(test_inputs[0])\n# lengths_test = np.argmax(sequences == 0, axis=1)\n# lengths_test[lengths_test == 0] = sequences.shape[1]","0655f257":"# y_train_torch","65424816":"# from torch.utils import data\n# dataset_train = data.TensorDataset(inputs_train[0], #input_ids\n#                              inputs_train[1], #input_masks\n#                              inputs_train[2], #input_segments\n#                              y_train_torch, #targets,\n#                              lengths_train, #lengths of each seq\n#                             )\n\n\n# dataset_val = data.TensorDataset(inputs_val[0], #input_ids\n#                              inputs_val[1], #input_masks\n#                              inputs_val[2], #input_segments\n#                              y_val_torch, #targets,\n#                              lengths_val, #lengths of each seq\n#                             )\n\n\n# dataset_test = data.TensorDataset(test_inputs[0], #input_ids\n#                              test_inputs[1], #input_masks\n#                              test_inputs[2], #input_segments\n#                              torch.from_numpy(lengths_test), #lengths of each seq\n#                             )","2daa3a07":"# next(iter(dataset_train))","84b2e352":"# BATCH_SIZE = 32\n# train_loader = data.DataLoader(dataset_train,\n#                                batch_size=BATCH_SIZE,\n#                                shuffle=True,\n#                                drop_last=True,\n#                               )\n\n# val_loader = data.DataLoader(dataset_val,\n#                                batch_size=BATCH_SIZE,\n#                                shuffle=True,\n#                                drop_last=True,\n#                               )\n\n# test_loader = data.DataLoader(dataset_test,\n#                                batch_size=BATCH_SIZE,\n#                                shuffle=False,\n#                                drop_last=False,\n#                               )\n","86ad8fa7":"# next(iter(train_loader)) #input_ids, input_masks, input_segments, targets, lengths","5f028652":"# y_train_torch[0], len(y_train_torch[0])","55f72886":"# Creating a config object to store task specific information\nclass Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n        \nconfig = Config(\n    testing=False,\n    seed = 42,\n    roberta_model_name='bert-base-uncased', \n    use_fp16=False,\n    bs=32, \n#     max_seq_len=512, \n    hidden_dropout_prob=.25,\n    hidden_size=768, \n    start_tok = \"[CLS]\",\n    end_tok = \"[SEP]\",\n)","7299b3ea":"# forward tokenizer\n\nclass FastAiRobertaTokenizer_t(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=30, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","c128ef76":"# forward tokenizer\n\nclass FastAiRobertaTokenizer_q(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=120, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","adc4ae9c":"# forward tokenizer\n\nclass FastAiRobertaTokenizer_a(BaseTokenizer):\n    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=400, **kwargs): \n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len \n    def __call__(self, *args, **kwargs): \n        return self \n    def tokenizer(self, t:str) -> List[str]: \n        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]","fc997870":"# create fastai tokenizer \nbert_tok = BertTokenizer.from_pretrained('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased-vocab.txt')\n\nfastai_tokenizer_t = Tokenizer(tok_func=FastAiRobertaTokenizer_t(bert_tok, max_seq_len=30), \n                             pre_rules=[], post_rules=[])\nfastai_tokenizer_q = Tokenizer(tok_func=FastAiRobertaTokenizer_q(bert_tok, max_seq_len=120), \n                             pre_rules=[], post_rules=[])\nfastai_tokenizer_a = Tokenizer(tok_func=FastAiRobertaTokenizer_a(bert_tok, max_seq_len=400), \n                             pre_rules=[], post_rules=[])\n","7f9bbe40":"# create fastai vocabulary \npath = Path()\nbert_tok.save_vocabulary(path)\n   \nfastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","51fd8fc6":"train.head()","299927e2":"databunch_1 = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer_t,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=input_columns[0],\n                  label_cols=targets,\n                  bs=32,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndatabunch_2 = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer_q,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=input_columns[1],\n                  label_cols=targets,\n                  bs=8,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndatabunch_3 = TextDataBunch.from_df(\".\", train, val, test,\n                  tokenizer=fastai_tokenizer_a,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=input_columns[2],\n                  label_cols=targets,\n                  bs=8,\n                  mark_fields=True,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )\n\ndatabunch_1.save('databunch_1.pkl')\ndatabunch_2.save('databunch_2.pkl')\ndatabunch_3.save('databunch_3.pkl')","94eac6f7":"databunch_1.show_batch()","d1f8b20b":"databunch_2.show_batch()","bf49db08":"databunch_3.show_batch()","257a19d6":"start_time = time.time()\n\nseed = 42\n\nnum_labels = len(targets)\nn_epochs = 3\nlr = 2e-5\nwarmup = 0.05\nbatch_size = 32\naccumulation_steps = 4\n\nbert_model_config = '..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/bert_config.json'\n\nbert_model = 'bert-base-uncased'\ndo_lower_case = 'uncased' in bert_model\ndevice = torch.device('cuda')\n\noutput_model_file = 'bert_pytorch.bin'\noutput_optimizer_file = 'bert_pytorch_optimizer.bin'\noutput_amp_file = 'bert_pytorch_amp.bin'\n\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","2a075ffb":"class BertForSequenceClassification(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    \"\"\"\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        return logits","7e98fd82":"loss_func = nn.BCEWithLogitsLoss()","0d0b339e":"def reduce_loss(loss, reduction='sum'):\n    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    def __init__(self, \u03b5:float=0.1, reduction='sum'):\n        super().__init__()\n        self.\u03b5,self.reduction = \u03b5,reduction\n    \n    def forward(self, output, target):\n        c = output.size()[-1]\n        log_preds = F.log_softmax(output, dim=-1)\n        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n        nll = F.nll_loss(log_preds, target.long(), reduction=self.reduction)\n        return lin_comb(loss\/c, nll, self.\u03b5)","0ddff499":"# class LabelSmoothingLoss(nn.Module):\n#     def __init__(self, classes, smoothing=0.0, dim=-1):\n#         super(LabelSmoothingLoss, self).__init__()\n#         self.confidence = 1.0 - smoothing\n#         self.smoothing = smoothing\n#         self.cls = classes\n#         self.dim = dim\n\n#     def forward(self, pred, target):\n#         pred = pred.log_softmax(dim=self.dim)\n#         with torch.no_grad():\n#             # true_dist = pred.data.clone()\n#             true_dist = torch.zeros_like(pred)\n#             true_dist.fill_(self.smoothing \/ (self.cls - 1))\n#             true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n#         return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))","0576d7dd":"loss_func = nn.BCEWithLogitsLoss()","3a1bf9c5":"bert_config = BertConfig.from_json_file(bert_model_config)\nbert_config.num_labels = len(targets)\n\nmodel_path = os.path.join('..\/input\/pretrained-bert-models-for-pytorch\/bert-base-uncased\/')\n\nmodel = BertForSequenceClassification.from_pretrained(model_path, config=bert_config)\n\nlearn_bert_1 = Learner(databunch_1, model, loss_func=loss_func, model_dir='\/temp\/model')\nlearn_bert_2 = Learner(databunch_2, model, loss_func=loss_func, model_dir='\/temp\/model')\nlearn_bert_3 = Learner(databunch_3, model, loss_func=loss_func, model_dir='\/temp\/model')","ad98a5be":"model.bert.embeddings","7dd31191":"def bert_clas_split(self) -> List[nn.Module]:\n    \n    bert = model.bert\n    embedder = bert.embeddings\n    pooler = bert.pooler\n    encoder = bert.encoder\n    classifier = [model.dropout, model.classifier]\n    n = len(encoder.layer)\/\/3\n    print(n)\n    groups = [[embedder], list(encoder.layer[:n]), list(encoder.layer[n+1:2*n]), list(encoder.layer[(2*n)+1:]), [pooler], classifier]\n    return groups","488d8de2":"x = bert_clas_split(model)","dae875ce":"learn_bert_1.layer_groups","a2a940a6":"learn_bert_1.summary()","70594642":"learn_bert_1.split([x[1],  x[3],  x[5]])","e70cbc98":"learn_bert_1.layer_groups","ba33e40b":"learn_bert_1.freeze_to(-1)","b45c8e9d":"learn_bert_1.summary()","16fb9181":"learn_bert_1.lr_find()","71d5dacb":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-poster')\nstyle.use('ggplot')","03a755fb":"learn_bert_1.recorder.plot(suggestion=True)","a50cc261":"learn_bert_1.fit_one_cycle(3, max_lr=slice(1e-3, 5e-3), moms=(0.8,0.7), pct_start=0.2, wd =1.)","7e1267fb":"learn_bert_1.freeze_to(-2)\nlearn_bert_1.summary()","a7aca001":"learn_bert_1.lr_find()\nlearn_bert_1.recorder.plot(suggestion=True)","8fd0c7b6":"learn_bert_1.fit_one_cycle(3, max_lr=slice(1e-5, 1e-3), moms=(0.8,0.7), pct_start=0.4, wd =1.)","257a0798":"learn_bert_1.freeze_to(-3)\nlearn_bert_1.summary()","7e8a8f0a":"learn_bert_1.lr_find()\nlearn_bert_1.recorder.plot(suggestion=True)","4b6d6e25":"learn_bert_1.fit_one_cycle(3, max_lr=slice(1e-6, 1e-4), moms=(0.8,0.7), pct_start=0.4, wd =1.)","724c6384":"learn_bert_1.unfreeze()\nlearn_bert_1.summary()","5af4ec02":"learn_bert_1.lr_find()\nlearn_bert_1.recorder.plot(suggestion=True)","ebbed42a":"learn_bert_1.fit_one_cycle(3, slice(1e-7, 1e-5), moms=(0.8,0.7), pct_start=0.4, wd =1.)","0eef34d9":"def get_ordered_preds(learn_bert_1, ds_type, preds):\n    np.random.seed(42)\n    sampler = [i for i in learn_bert_1.data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    preds = [p[reverse_sampler] for p in preds]\n    return preds","067d39e9":"test_raw_preds = learn_bert_1.get_preds(ds_type=DatasetType.Test)\ntest_preds_bert_1 = get_ordered_preds(learn_bert_1, DatasetType.Test, test_raw_preds)\ntest_preds_bert_1 = torch.FloatTensor(test_preds_bert_1[0])","d2440907":"test_preds_bert_1","9f943941":"learn_bert_2.split([x[1],  x[3],  x[5]])","6a69baa9":"learn_bert_2.freeze_to(-1)\nlearn_bert_2.summary()","cf922100":"learn_bert_2.lr_find()\nlearn_bert_2.recorder.plot(suggestion=True)","b07e8427":"learn_bert_2.fit_one_cycle(10, max_lr=slice(1e-3, 5e-3), moms=(0.8,0.7), pct_start=0.2, wd =1.)","4af15af0":"learn_bert_2.freeze_to(-2)\nlearn_bert_2.summary()","1064f6a5":"learn_bert_2.lr_find()\nlearn_bert_2.recorder.plot(suggestion=True)","73dc68e1":"learn_bert_2.fit_one_cycle(10, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =1.)","58220023":"learn_bert_2.freeze_to(-3)\nlearn_bert_2.summary()","c394e80f":"learn_bert_2.lr_find()\nlearn_bert_2.recorder.plot(suggestion=True)","703b3012":"learn_bert_2.fit_one_cycle(3, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =1.)","6095ac8e":"learn_bert_2.unfreeze()\nlearn_bert_2.summary()","319dc6a7":"learn_bert_2.lr_find()\nlearn_bert_2.recorder.plot(suggestion=True)","1737da4c":"learn_bert_2.fit_one_cycle(2, max_lr=slice(1e-6, 1e-5), moms=(0.8,0.7), pct_start=0.2, wd =1.5)","6c6a0211":"def get_ordered_preds(learn_bert_2, ds_type, preds):\n    np.random.seed(42)\n    sampler = [i for i in learn_bert_2.data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    preds = [p[reverse_sampler] for p in preds]\n    return preds","eac9d2d1":"test_raw_preds = learn_bert_2.get_preds(ds_type=DatasetType.Test)\ntest_preds_bert_2 = get_ordered_preds(learn_bert_2, DatasetType.Test, test_raw_preds)\ntest_preds_bert_2 = torch.FloatTensor(test_preds_bert_2[0])\ntest_preds_bert_2","e7a10dfa":"learn_bert_3.split([x[1],  x[3],  x[5]])","88a071ba":"learn_bert_3.freeze_to(-1)\nlearn_bert_3.summary()","4f2552aa":"learn_bert_3.lr_find()\nlearn_bert_3.recorder.plot(suggestion=True)","220d837f":"learn_bert_3.fit_one_cycle(10, max_lr=slice(1e-3, 5e-3), moms=(0.8,0.7), pct_start=0.2, wd =1.)","56608ce4":"learn_bert_3.freeze_to(-2)\nlearn_bert_3.summary()","15e8801a":"learn_bert_3.lr_find()\nlearn_bert_3.recorder.plot(suggestion=True)","7bad09eb":"learn_bert_3.fit_one_cycle(10, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =1.)","293322e2":"learn_bert_3.freeze_to(-3)\nlearn_bert_3.summary()","d74f7ce7":"learn_bert_3.lr_find()\nlearn_bert_3.recorder.plot(suggestion=True)","2c44a29f":"learn_bert_3.fit_one_cycle(3, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =1.5)","4de8c2c3":"learn_bert_3.unfreeze()\nlearn_bert_3.summary()","4a76b830":"learn_bert_3.lr_find()\nlearn_bert_3.recorder.plot(suggestion=True)","4fcb6141":"learn_bert_3.fit_one_cycle(2, max_lr=slice(1e-5, 1e-4), moms=(0.8,0.7), pct_start=0.2, wd =1.5)","b47951f3":"def get_ordered_preds(learn_bert_3, ds_type, preds):\n    np.random.seed(42)\n    sampler = [i for i in learn_bert_3.data.dl(ds_type).sampler]\n    reverse_sampler = np.argsort(sampler)\n    preds = [p[reverse_sampler] for p in preds]\n    return preds","ac95c20b":"test_raw_preds = learn_bert_3.get_preds(ds_type=DatasetType.Test)\ntest_preds_bert_3 = get_ordered_preds(learn_bert_3, DatasetType.Test, test_raw_preds)\ntest_preds_bert_3 = torch.FloatTensor(test_preds_bert_3[0])\ntest_preds_ber_3","30d31e64":"final_preds_test = (test_preds_bert_1 + test_preds_bert_2  + test_preds_bert_3 )\/3","5f71589e":"sub.iloc[:, 1:] = final_preds_test.numpy()\nsub.to_csv('submission.csv', index=False)\nsub.head()","e5ac53b0":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n    sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\nplt.tight_layout()\nplt.show()\nplt.close()","ea25f1e1":"# y_train = train[targets].values\n\n# for column_ind in range(30):\n#     curr_column = y_train[:, column_ind]\n#     values = np.unique(curr_column)\n#     map_quantiles = []\n#     for val in values:\n#         occurrence = np.mean(curr_column == val)\n#         cummulative = sum(el['occurrence'] for el in map_quantiles)\n#         map_quantiles.append({'value': val, 'occurrence': occurrence, 'cummulative': cummulative})\n            \n#     for quant in map_quantiles:\n#         pred_col = final_preds_test[:, column_ind]\n#         q1, q2 = np.quantile(pred_col, quant['cummulative']), np.quantile(pred_col, min(quant['cummulative'] + quant['occurrence'], 1))\n#         pred_col[(pred_col >= q1) & (pred_col <= q2)] = quant['value']\n#         final_preds_test[:, column_ind] = pred_col","da4190f5":"# sub.iloc[:, 1:] = final_preds_test.numpy()\n# sub.to_csv('submission.csv', index=False)\n# sub.head()","e842def5":"# fig, axes = plt.subplots(6, 5, figsize=(18, 15))\n# axes = axes.ravel()\n# bins = np.linspace(0, 1, 20)\n\n# for i, col in enumerate(targets):\n#     ax = axes[i]\n#     sns.distplot(train[col], label=col, bins=bins, ax=ax, color='blue')\n#     sns.distplot(sub[col], label=col, bins=bins, ax=ax, color='orange')\n#     # ax.set_title(col)\n#     ax.set_xlim([0, 1])\n# plt.tight_layout()\n# plt.show()\n# plt.close()","5b352380":"## Databunch 2","ec67e095":"# BERT","1bb0c7ec":"# Databunch 3","b271482e":"## Databunch 1","d1eca6e9":"# Splitting the model","f4dc5fff":"# Transformers","a08e89f6":"# Prediction","81bed7de":"# train-val-test split","b4123503":"# Import data and Libraries","4ff655d7":"## Build BERT Model","e2c68d4a":"## Setup model","488e02da":"## Cleaning the data"}}