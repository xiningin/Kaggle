{"cell_type":{"a9209fd9":"code","18c2d94d":"code","37e868e4":"code","f6ee8833":"code","5ccce9ef":"code","2f62d4b1":"code","b467cc75":"code","7e4d09e4":"code","0873e0c6":"code","2d180c02":"code","4fd8e926":"code","a7d6602a":"code","c5152a68":"code","00dc4eab":"code","c35c60b2":"code","28b36694":"code","c55c1239":"code","9a5ce0ce":"code","720e3f34":"code","fa10e671":"code","6d51e718":"code","3dc0fcbd":"code","a95c2fa6":"code","2a687742":"code","673f611e":"code","5af31e69":"code","e741a83b":"markdown","0a74453b":"markdown","59abbbfa":"markdown","02fe32a6":"markdown","eb8e76dc":"markdown","1f6bd92f":"markdown","005156e6":"markdown","f1455bfb":"markdown","36fe7aab":"markdown","5e209a6f":"markdown","1b2bfd92":"markdown"},"source":{"a9209fd9":"import pandas as pd","18c2d94d":"df = pd.read_csv(\"..\/input\/train.csv\")","37e868e4":"target_per_household = df.groupby(['idhogar'])['Target'].nunique()\n\nno_target = len(target_per_household.loc[target_per_household == 0])\nunique_target = len(target_per_household.loc[target_per_household == 1])\nmore_targets = len(target_per_household.loc[target_per_household > 1])\nmore_targets_perc = more_targets \/ (no_target + unique_target + more_targets)\n\nprint(\"No per household: {}\".format(no_target))\nprint(\"1 target per household: {}\".format(unique_target))\nprint(\"More targets per household: {} or {:.1f}%\" .format(more_targets, more_targets_perc * 100))","f6ee8833":"df.loc[(df.idhogar == '1b31fd159'), 'meaneduc'] = 10\ndf.loc[(df.idhogar == 'a874b7ce7'), 'meaneduc'] = 5\ndf.loc[(df.idhogar == 'faaebf71a'), 'meaneduc'] = 12\ndf.edjefe.replace({'no': 0}, inplace=True)\ndf.edjefa.replace({'no': 0}, inplace=True)\ndf.edjefe.replace({'yes': 1}, inplace=True)\ndf.edjefa.replace({'yes': 1}, inplace=True)","5ccce9ef":"categorical_features = df.columns.tolist()\nfor feature in df.describe().columns:\n    categorical_features.remove(feature)\n\n# Just for saving them\nnumerical_features = df.columns.tolist()\nfor categorical_feature in categorical_features:\n    numerical_features.remove(categorical_feature)\n    \ncategorical_features","2f62d4b1":"df[categorical_features].head()","b467cc75":"df[['edjefe', 'SQBedjefe']].head()","7e4d09e4":"df[['edjefe', 'SQBedjefe']].head()","0873e0c6":"print(\"Number of observations {}\".format(len(df)))","2d180c02":"features_with_null = df.isna().sum().sort_values(ascending=False)\nfeatures_with_null = features_with_null.loc[features_with_null > 0]\nfeature_names_with_null = features_with_null.index.tolist()\n\nfeatures_with_null","4fd8e926":"selectable_features = numerical_features.copy()\nselectable_features.remove('Target')\nfor feature in feature_names_with_null:\n    selectable_features.remove(feature)\n\nX = df[selectable_features]\ny = df.Target","a7d6602a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=112, test_size=0.2)","c5152a68":"# from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nclf= RandomForestClassifier()\nclf.fit(X_train, y_train)\n\nsorted(zip(X.columns, clf.feature_importances_ * 100), key=lambda x: -x[1])","00dc4eab":"id_with_null = df[df.meaneduc.isna()].idhogar\ndf[df.idhogar.isin(id_with_null)][['idhogar', 'meaneduc', 'escolari', 'age']]","c35c60b2":"df.loc[(df.idhogar == '1b31fd159'), 'meaneduc'] = 10\ndf.loc[(df.idhogar == 'a874b7ce7'), 'meaneduc'] = 5\ndf.loc[(df.idhogar == 'faaebf71a'), 'meaneduc'] = 12","28b36694":"df.edjefe.replace({'no': 0}, inplace=True)\ndf.edjefa.replace({'no': 0}, inplace=True)\ndf.edjefe.replace({'yes': 1}, inplace=True)\ndf.edjefa.replace({'yes': 1}, inplace=True)","c55c1239":"selectable_features = numerical_features.copy()\nselectable_features.append('edjefe')\nselectable_features.append('edjefa')\nselectable_features.remove('Target')\nfor feature in feature_names_with_null:\n    selectable_features.remove(feature)\n\nX = df[selectable_features]\ny = df.Target","9a5ce0ce":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=112, test_size=0.2)","720e3f34":"selected_features = ['SQBedjefe', 'SQBdependency', 'overcrowding', 'qmobilephone', 'SQBage', 'rooms', 'SQBhogar_nin', 'edjefe', 'edjefa' ]\n\nX_train_4predict = X_train[selected_features]\npredictor = RandomForestClassifier()\npredictor.fit(X_train_4predict, y_train)","fa10e671":"X_test_4predict = X_test[selected_features]\ny_predict = predictor.predict(X_test_4predict)","6d51e718":"from sklearn.metrics import precision_recall_fscore_support as score\n\nprecision, recall, fscore, support = score(y_test, y_predict)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","3dc0fcbd":"from sklearn.metrics import f1_score\nf1_score(y_test, y_predict, average='macro')","a95c2fa6":"df_eval = pd.read_csv(\"..\/input\/test.csv\")","2a687742":"df_eval.edjefe.replace({'no': 0}, inplace=True)\ndf_eval.edjefa.replace({'no': 0}, inplace=True)\ndf_eval.edjefe.replace({'yes': 1}, inplace=True)\ndf_eval.edjefa.replace({'yes': 1}, inplace=True)","673f611e":"X_eval = df_eval[selected_features]\ndf_eval['Target'] = predictor.predict(X_eval)","5af31e69":"df_eval[['Id', 'Target']].to_csv(\"sample_submission.csv\", index=False)","e741a83b":"# 5. Let's take a step backward\nOut of the feature selection education is an important feature but we ignored **edjefa** and **meaneduc**","0a74453b":"# 5. Predicting and sending","59abbbfa":"We will fullfill meaneduc with the average value of the household","02fe32a6":"Kaggle gave me a result of 0.349  on this first try, which show a huge overfitt, which is also normal with a model like random forest.\n\nI also ranked 82 \/ 106.","eb8e76dc":"# 1. Target are per household","1f6bd92f":"Features ID, those will obviously not beeing predictive (or will overfit), so we can ignore:\n- Id\n- idhogar\n\nOther categorical features:\n- **dependency**': Dependency rate. We can use its squraed feature **SQBdependency**.\n- **edjefe**, years of education of male head of household. We can use its squared feature **SQBedjefe**\n- **edjefa**, years of education of female head of household.","005156e6":"As in the competition title, \"Household Poverty Level Prediction\", we will consider the Target per household, and define the other as **outliers** that we will in a first time **delete**.\n\nKaggle discussion, mention to clean the data using the household value in caseof discrepency: https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/discussion\/61403","f1455bfb":"# 6. Second evaluation\nHow to resist?","36fe7aab":"* rez_esc      7928 null values for Years behind in school. Too much null values: unusable\n* v2a1         6860 null values for Monthly rent payment. Unusable.\n\n* v18q1        7342 null values for number of tablets household owns. Unusable but summing **v18q** by household may help.\n\n* meaneduc        5 null values for average years of education for adults. We may fullfill those values.\n* SQBmeaned       5 null values for square of the mean years of education of adults. We may fullfill those values.\n\n# 4. Feature selection with RandomForest","5e209a6f":"# 3. Empty values in numerical features","1b2bfd92":"# 2. Categorical features"}}