{"cell_type":{"4c3603d2":"code","8b4f9476":"code","f7002b05":"code","677a68f0":"code","9b53afbf":"code","43204bc9":"code","160dde77":"code","09779fe7":"code","69ceb36f":"code","6e54c666":"code","566514de":"code","209c718e":"code","fb4405b7":"code","c147141d":"code","fc5ae0e9":"code","fcdbf034":"code","3ab32945":"code","de714999":"markdown","f009b760":"markdown","f5e1020c":"markdown","6c8f64b3":"markdown","90217a68":"markdown","bc3542dd":"markdown","f41572fc":"markdown","23e261a8":"markdown","c3ad3216":"markdown","4df688c6":"markdown","3d43b6d3":"markdown","07b8ce37":"markdown","229c5b66":"markdown"},"source":{"4c3603d2":"import numpy as np\r\nimport pandas as pd\r\nimport plotly.express as px\r\nimport plotly.graph_objects as go\r\nimport plotly.figure_factory as ff\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.model_selection import train_test_split","8b4f9476":"from plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)","f7002b05":"mean_01=np.array([1,1])\r\ncov_01=np.array([[1,-0.2],[-0.2,1]])\r\nmean_02=np.array([3,4])\r\ncov_02=np.array([[1,0.1],[0.1,1]])\r\nnp.random.seed(42)\r\ndata_01=np.random.multivariate_normal(mean_01,cov_01,500, check_valid= \"warn\")\r\ndata_02=np.random.multivariate_normal(mean_02,cov_02,500, check_valid= \"warn\")\r\ndata = np.vstack((data_01,data_02))\r\ndf_train = pd.DataFrame(data, columns = [\"Feature_1\", \"Feature_2\"])\r\ndf_train[\"class\"] = [0]*500 + [1]*500 ","677a68f0":"df_train.info()","9b53afbf":"## For Contributing, refer to expectedoutput1.html in the expected outputs folder.\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][:500],\n    y=df_train[\"Feature_2\"][:500],\n    name=\"Class A\",\n    mode=\"markers\",\n    marker_color='rgba(84, 138, 252, 0.65)',\n    marker_line_color='rgba(26, 9, 243, 0.8)',\n    marker_size = 10,\n    marker_line_width = 1,\n    marker_symbol='circle',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class A<extra><\/extra>',\n))\n\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][500:],\n    y=df_train[\"Feature_2\"][500:],\n    name=\"Class B\",\n    mode=\"markers\",\n    marker_color='rgba(255, 117, 0, 0.65)',\n    marker_line_color='rgba(255, 73, 0, 1)',\n    marker_size = 10,\n    marker_line_width = 1,\n    marker_symbol='star-triangle-up',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class B<extra><\/extra>'\n))\n\nfig.update_layout(\n    title=\"Visualization of the dataset\",\n    xaxis_title=\"Feature_2\",\n    yaxis_title=\"Feature_1\",\n    legend_title=\"\",\n    width=800,\n    height=800,\n)\n\nfig.show()","43204bc9":"X = df_train[[\"Feature_1\",\"Feature_2\"]]\r\nY = df_train[[\"class\"]]\r\nX = np.hstack((np.ones((1000,1)),X.to_numpy()))\r\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y.to_numpy(), test_size=0.2, random_state=42)\r\nprint(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)","160dde77":"def hypothesis(x,theta):\r\n    sigmoid=(1.0\/(1.0 + np.exp(-1.0*np.dot(x,theta))))\r\n    return(sigmoid)\r\n\r\ndef error(X,Y,theta):\r\n    m=X.shape[0]\r\n    err=0\r\n    for i in range(m):\r\n        hx=hypothesis(X[i],theta)\r\n        err+=Y[i]*np.log2(hx) + (1-Y[i])*np.log2(1-hx)\r\n    err \/=m\r\n    return(-err)\r\n\r\ndef gradient(X,Y,theta):\r\n    grad=np.zeros((X.shape[1]))\r\n    m=X.shape[0]\r\n    fea=X.shape[1]\r\n    for i in range(m):\r\n        hx=hypothesis(X[i],theta)\r\n        for j in range(fea):\r\n            grad[j]+=(hx-Y[i])*X[i,j]\r\n    grad=grad\/m\r\n    return(grad)\r\n    \r\ndef gradient_ascent(X,Y,learning_rate=0.5):\r\n    theta=2*np.random.random(X.shape[1])\r\n    theta[0]=0\r\n    error_list=[]\r\n    acc_list=[]\r\n    theta_list = []\r\n    for i in range(100):\r\n        grad=gradient(X,Y,theta)\r\n        err=error(X,Y,theta)\r\n        error_list.append(err)\r\n        theta_list.append(theta)\r\n        acc_list.append(accuracy(X,Y,theta))\r\n\r\n        for j in range(X.shape[1]):\r\n            theta[j]-=learning_rate*grad[j]\r\n    probabilty_list = predic_proba(X, theta)\r\n    return(theta, theta_list, error_list,acc_list, probabilty_list)\r\n\r\ndef predict(x,theta):\r\n    p=hypothesis(x,theta)\r\n    if p<0.5:\r\n        return 0\r\n    else:\r\n        return 1\r\n\r\ndef predic_proba(x,theta):\r\n    probabilty_list = []\r\n    for i in range(X.shape[0]):\r\n        probability = hypothesis(X[i],theta)\r\n        probabilty_list.append(probability)\r\n    return probabilty_list\r\n\r\ndef accuracy(X,Y,theta):\r\n    y_pred=[]\r\n    for i in range(X.shape[0]):\r\n        p=predict(X[i],theta)\r\n        y_pred.append(p)\r\n    y_pred=np.array(y_pred)\r\n    y_pred=y_pred.reshape((-1,1))\r\n    return(Y==y_pred).sum()\/X.shape[0]","09779fe7":"theta, theta_list, error_list, acc_list, probabilty_list=gradient_ascent(X_train,Y_train)","69ceb36f":"## For Contributing, refer to expectedoutput2.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.plot(error_list)\r\nplt.title(\"Error ,i.e Negative of maximum likelihood\")\r\nplt.show()","6e54c666":"## For Contributing, refer to expectedoutput2.html in the expected outputs folder.\n\n\n# Converting list of arrays to 1D list\nfor i in range(len(error_list)):\n    error_list[i] = error_list[i].tolist()\nfrom itertools import chain\nerror_list = list(chain.from_iterable(error_list))\n\n# Visualising the error\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    y = error_list, \n    hovertemplate = 'Iteration: %{x} <br \/>Error: %{y}<extra><\/extra>',\n    mode='lines+markers',\n    marker_symbol='circle',\n    marker_color='rgba(84, 138, 252, 0.65)',\n    marker_line_color='rgba(26, 9, 243, 0.8)',\n    marker_size = 6,\n    marker_line_width = 1,\n))\n\n\nfig.update_layout(\n    title=\"Visualising the error\",\n    xaxis_range=[-5,105],\n    xaxis_title = \"Iteration\",\n    yaxis_title = \"Error (Negative of maximum likelihood)\",\n    legend_title=\"\",\n    width=800,\n    height=800,\n)\nfig.show()","566514de":"## For Contributing, refer to expectedoutput3.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.plot(acc_list)\r\nplt.title(\"Accuracy Graph\")\r\nplt.show()\r\nacc=accuracy(X_test,Y_test,theta)","209c718e":"print(\"The accuracy for the algorithm is:\",acc)\r\nprint(\"The final theta parameters calculated are:\",theta)","fb4405b7":"theta_list","c147141d":"## For Contributing, refer to expectedoutput4.html in the expected outputs folder.\n","fc5ae0e9":"# Animation ki koshish\n\niterations = np.arange(1,101) \nxx = np.linspace(-4,8,15)\n\nsliders_dict = {\n    \"active\": 0,\n    \"yanchor\": \"top\",\n    \"xanchor\": \"left\",\n    \"currentvalue\": {\n        \"font\": {\"size\": 20},\n        \"prefix\": \"No. of Iterations:\",\n        \"visible\": True,\n        \"xanchor\": \"right\"\n    },\n    \"transition\": {\"duration\": 300, \"easing\": \"cubic-in-out\"},\n    \"pad\": {\"b\": 10, \"t\": 50},\n    \"len\": 0.9,\n    \"x\": 0.1,\n    \"y\": 0,\n    \"steps\": []\n}\n\nfor i in range(len(iterations)):\n    slider_step = {'args': [\n        [iterations[i]],{\n            'frame': {'duration': 300, 'redraw': True},\n            'mode': 'immediate',\n            'transition': {'duration': 300}\n        }],\n    'label': str(iterations[i]),\n    'method': 'animate'}\n    sliders_dict['steps'].append(slider_step)\n\n    \n# make figure\nfig_dict = {\n#     make data\n    \"data\": [go.Scatter(x = df_train[\"Feature_1\"][:500], \n                        y = df_train[\"Feature_2\"][:500], \n                        name=\"Class A\",\n                        mode=\"markers\", \n                        marker_color='rgba(84, 138, 252, 0.65)', \n                        marker_line_color='rgba(26, 9, 243, 0.8)', \n                        marker_size = 10, marker_line_width = 1, \n                        marker_symbol='circle', \n                        hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class A<extra><\/extra>'\n                       ),\n             go.Scatter(x = df_train[\"Feature_1\"][500:], \n                        y=df_train[\"Feature_2\"][500:], \n                        name=\"Class B\", \n                        mode=\"markers\", \n                        marker_color='rgba(255, 117, 0, 0.65)',\n                        marker_line_color='rgba(255, 73, 0, 1)', \n                        marker_size = 10, \n                        marker_line_width = 1,\n                        marker_symbol='star-triangle-up', \n                        hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class B<extra><\/extra>'\n                       ), \n             go.Scatter(x = xx, \n                        y = -1*(theta_list[0][0]+xx*theta_list[0][1])\/theta_list[0][2], \n                        mode=\"lines\", \n                        name=\"Decision Boundary\")\n            ],\n    \n    \"layout\": {},\n    \"frames\": [go.Frame(data = [go.Scatter(x = df_train[\"Feature_1\"][:500], \n                        y = df_train[\"Feature_2\"][:500], \n                        name=\"Class A\",\n                        mode=\"markers\", \n                        marker_color='rgba(84, 138, 252, 0.65)', \n                        marker_line_color='rgba(26, 9, 243, 0.8)', \n                        marker_size = 10, marker_line_width = 1, \n                        marker_symbol='circle', \n                        hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class A<extra><\/extra>'\n                       ),\n                       go.Scatter(x = df_train[\"Feature_1\"][500:], \n                        y=df_train[\"Feature_2\"][500:], \n                        name=\"Class B\", \n                        mode=\"markers\", \n                        marker_color='rgba(255, 117, 0, 0.65)',\n                        marker_line_color='rgba(255, 73, 0, 1)', \n                        marker_size = 10, \n                        marker_line_width = 1,\n                        marker_symbol='star-triangle-up', \n                        hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class B<extra><\/extra>'\n                       ), \n                       go.Scatter(x = xx, \n                        y = -1*(theta_list[i][0]+xx*theta_list[i][1])\/theta_list[i][2], \n                        mode=\"lines\", \n                        name=\"Decision Boundary\")], name = str(iterations[i])) for i in range(1,len(iterations))    \n              ]\n}\n\n# make layout\nfig_dict[\"layout\"][\"updatemenus\"] = [\n    {\n        \"buttons\": [\n            {\n                \"args\": [None, {\"frame\": {\"duration\": 500, \"redraw\": False},\n                                \"fromcurrent\": True, \"transition\": {\"duration\": 300,\n                                                                    \"easing\": \"quadratic-in-out\"}}],\n                \"label\": \"Play\",\n                \"method\": \"animate\"\n            },\n            {\n                \"args\": [[None], {\"frame\": {\"duration\": 0, \"redraw\": False},\n                                  \"mode\": \"immediate\",\n                                  \"transition\": {\"duration\": 0}}],\n                \"label\": \"Pause\",\n                \"method\": \"animate\"\n            }\n        ],\n        \"direction\": \"left\",\n        \"pad\": {\"r\": 10, \"t\": 87},\n        \"showactive\": False,\n        \"type\": \"buttons\",\n        \"x\": 0.1,\n        \"xanchor\": \"right\",\n        \"y\": 0,\n        \"yanchor\": \"top\"\n    }\n]\n\n\nfig_dict[\"layout\"][\"sliders\"] = [sliders_dict]\nfig = go.Figure(fig_dict)\n\nfig.update_layout( width=800, height=800, \n                 xaxis_title=\"Feature_1\", yaxis_title=\"Feature_2\", \n                 title = \"Visualising the convergence of decision boundary\")\nfig.show()","fcdbf034":"# Plotting decision boundary without animation\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][:500],\n    y=df_train[\"Feature_2\"][:500],\n    name=\"Class A\",\n    mode=\"markers\",\n    marker_color='rgba(84, 138, 252, 0.65)',\n    marker_line_color='rgba(26, 9, 243, 0.8)',\n    marker_size = 10,\n    marker_line_width = 1,\n    marker_symbol='circle',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class A<extra><\/extra>',\n))\n\nfig.add_trace(go.Scatter(\n    x=df_train[\"Feature_1\"][500:],\n    y=df_train[\"Feature_2\"][500:],\n    name=\"Class B\",\n    mode=\"markers\",\n    marker_color='rgba(255, 117, 0, 0.65)',\n    marker_line_color='rgba(255, 73, 0, 1)',\n    marker_size = 10,\n    marker_line_width = 1,\n    marker_symbol='star-triangle-up',\n    hovertemplate = 'Feature_1: %{x:.5f} <br \/>Feature_2: %{y:.5f}<extra><\/extra>, Class B<extra><\/extra>'\n))\n\nxx = np.linspace(-4,8,15)\nfig.add_trace(go.Scatter(\n    x = xx,\n    y = -1*(theta[0]+xx*theta[1])\/theta[2],\n    mode=\"lines\",\n    name = \"Decision Boundary\"\n))\n\nfig.update_layout(\n    title=\"Visualization of the dataset\",\n    xaxis_title=\"Feature_2\",\n    yaxis_title=\"Feature_1\",\n    legend_title=\"\",\n    width=800,\n    height=800,\n)\n\nfig.show()","3ab32945":"## For Contributing, refer to expectedoutput5.html in the expected outputs folder.\r\n\r\nplt.figure(figsize=(8,8))\r\nplt.xlabel(\"Feature_1\")\r\nplt.ylabel(\"Feature_2\")\r\nplt.scatter(df_train[\"Feature_1\"][:500],df_train[\"Feature_2\"][:500], label=\"Class A\", edgecolor = \"b\", s = 60, alpha = 0.8)\r\nplt.scatter(df_train[\"Feature_1\"][500:],df_train[\"Feature_2\"][500:], label=\"Class B\", marker=\"^\", edgecolor = \"r\", s =60, alpha = 0.8)\r\nx=np.linspace(-4,8,15)\r\ny=-1*(theta[0]+x*theta[1])\/theta[2]\r\nplt.plot(x,y,color='black')\r\nplt.legend()\r\nplt.show()","de714999":"## Visualising Error over training set","f009b760":"## Visualising the decision boundry over iterations","f5e1020c":"# Loading the training dataset","6c8f64b3":"For demonstration purposes, let us take a 2 dimensional dataset with tow features (Feature_1 and Feature_2) and consisting of two classes (Class A and Class B) having a distribution specifications as follows:\r\n\r\n**Class A:** The Class A is centred around the mean of (1,1) and has the covariance matrix [[1,-0.2],[-0.2,1]]\r\n\r\n**Class B:** The Class B is centred around the mean of (3,4) and has the covariance matrix [[1,0.1],[0.1,1]]\r\n\r\nDefintions: \r\n\r\n**Mean:** A Class with centre (x1, x2) as mean denotes that the average value along \"Feature_1\" is x1 and the average value along \"Feature_2\" is x2\r\n\r\nP.S: Since we would like to ensure that the outputs corrosponds to the desired output, we will also add the seed value of 42 while generating these distributions.\r\n\r\n","90217a68":"## Training the model","bc3542dd":"# Importing important libraries","f41572fc":"## Preparing training and test sets","23e261a8":"## Defining the model","c3ad3216":"# Visualising the dataset","4df688c6":"## Visalising accuracy over test set","3d43b6d3":"This notebook is a part of the OneML_ContriHub the link to which can be found [here](https:\/\/github.com\/ContriHUB\/OneML_ContriHub)","07b8ce37":"# Machine Learning Model","229c5b66":"# Plotting the decision boundry"}}