{"cell_type":{"ae18bf81":"code","3cd32472":"code","4d2c06bf":"code","5c5042b9":"code","0f052713":"code","843223eb":"code","a52a803c":"code","be3886b2":"code","7ff915bf":"code","bf62843e":"code","6201228b":"code","bf419c27":"code","a2391b6c":"code","285b35ca":"code","8a0d88b2":"code","ab94994e":"code","4cff378d":"code","244a2d39":"code","b0bf0332":"code","72916b92":"code","256ee160":"code","9e9538f6":"code","c7e9488c":"code","29629ec3":"code","b1a7dbc5":"code","0cd67acd":"code","adb7a72a":"code","218ad73b":"code","543c5320":"code","2ac75959":"code","b2ebd222":"code","57135562":"code","c215385b":"code","3923b5d5":"code","bee5e518":"code","ce2e9586":"code","476ffeea":"code","5681431c":"code","033ee875":"code","a51c26a4":"code","b73399a6":"code","71b83c7e":"code","e503ea9b":"code","c13511df":"code","d2379fe8":"code","5374de4d":"code","81068a07":"code","2d831819":"code","94271125":"code","0f88eb2b":"code","3ef33dbf":"code","bb691fdd":"code","d4b28465":"code","9210a47c":"code","e37f6a93":"code","efae1f5e":"code","075965f8":"code","fb1dc734":"code","82bc85f0":"code","a75847e7":"code","f3702771":"code","b18b8c45":"code","b0ffa96a":"code","f6687d73":"code","6d783ada":"code","3e6ec285":"markdown","44f0ef8c":"markdown","61efc65d":"markdown","7af00f79":"markdown","7e8938ee":"markdown","c35c7e5e":"markdown","f471b570":"markdown","5c9da02a":"markdown","6f78642a":"markdown","d126b86b":"markdown","a964f00a":"markdown","a8d8dad7":"markdown","26827ea2":"markdown","15d29bcc":"markdown","f7c7a28a":"markdown","6d33bdee":"markdown"},"source":{"ae18bf81":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3cd32472":"import pandas as pd\nimport numpy as np\nimport bokeh\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom matplotlib import style\nimport re\nimport time\nimport string\nimport warnings\n\n# for all NLP related operations on text\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import *\nfrom nltk.classify import NaiveBayesClassifier\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\n\n# To identify the sentiment of text\nfrom textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nfrom textblob.np_extractors import ConllExtractor\n\n# ignoring all the warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# downloading stopwords corpus\nnltk.download('stopwords')\n#nltk.download('wordnet')\n#nltk.download('vader_lexicon')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('movie_reviews')\n#nltk.download('punkt')\n#nltk.download('conll2000')\n#nltk.download('brown')\nstopwords = set(stopwords.words(\"english\"))\n\n# for showing all the plots inline\n%matplotlib inline","4d2c06bf":"df_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","5c5042b9":"sub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsub","0f052713":"print(\"Train File\")\ndisplay(df_train.head())\nprint(\"Test File\")\ndisplay(df_test.head())","843223eb":"print(\"Train File Shape\",df_train.shape)\nprint(\"Test File Shape\",df_test.shape)","a52a803c":"print(\"Train File\")\ndisplay(df_train.isnull().sum())","be3886b2":"cols = ['keyword','location']\n\nsns.barplot(x=df_train[cols].isnull().sum(),y=df_train[cols].isnull().sum())\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)","7ff915bf":"print(\"Train File\")\ndisplay(df_train.columns)","bf62843e":"print(\"Train File\")\ndisplay(df_train['target'].value_counts())\ndisplay(df_train['location'].value_counts())\ndisplay(len(df_train['keyword'].value_counts()))","6201228b":"df_train['Text_Length']=df_train['text'].apply(lambda x:len(x) - x.count(\" \"))\ndf_train.head()","bf419c27":"df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df_train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf_train.drop(columns=['target_mean'], inplace=True)","a2391b6c":"sns.countplot(df_train['target'])\nplt.tick_params(axis='x', labelsize=15)","285b35ca":"def hashtag_extract(text_list):\n    hashtags = []\n    # Loop over the words in the tweet\n    for text in text_list:\n        ht = re.findall(r\"#(\\w+)\", text)\n        hashtags.append(ht)\n\n    return hashtags\n\ndef generate_hashtag_freqdist(hashtags):\n    a = nltk.FreqDist(hashtags)\n    d = pd.DataFrame({'Hashtag': list(a.keys()),\n                      'Count': list(a.values())})   \n    d = d.nlargest(columns=\"Count\", n = 50)\n    plt.figure(figsize=(16,7))\n    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n    plt.xticks(rotation=80)\n    ax.set(ylabel = 'Count')\n    plt.show()","8a0d88b2":"hashtags = hashtag_extract(df_train['text'])\ndf_train['hashtags'] = hashtags\nhashtags = sum(hashtags, [])","ab94994e":"hashtags","4cff378d":"generate_hashtag_freqdist(hashtags)","244a2d39":"df_train.head()","b0bf0332":"df_train['hashtags'] = df_train['hashtags'].astype(str)","72916b92":"df_train['hashtags'] = df_train['hashtags'].str.strip('[')\ndf_train['hashtags'] = df_train['hashtags'].str.strip(']').astype(str)\ndf_train","256ee160":"df_train['hashtags'] = df_train['hashtags'].replace({'':np.nan})\ndf_train","9e9538f6":"# 1 way\ndef fetch_sentiment_using_SIA(text):\n    sid = SentimentIntensityAnalyzer()\n    polarity_scores = sid.polarity_scores(text)\n    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'\n\n# 2 way\ndef fetch_sentiment_using_textblob(text):\n    analysis = TextBlob(text)\n    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'","c7e9488c":"sentiments_using_SIA = df_train.text.apply(lambda tweet: fetch_sentiment_using_SIA(tweet))\ndisplay(pd.DataFrame(sentiments_using_SIA.value_counts()))\n\nsentiments_using_textblob = df_train.text.apply(lambda tweet: fetch_sentiment_using_textblob(tweet))\ndisplay(pd.DataFrame(sentiments_using_textblob.value_counts()))","29629ec3":"df_train['sentiment'] = sentiments_using_SIA\ndf_train.head()","b1a7dbc5":"sns.countplot(x= df_train['sentiment'])","0cd67acd":"train = df_train.drop(['Text_Length','hashtags','sentiment'],axis=1)\ntrain","adb7a72a":"def remove_pattern(text, pattern_regex):\n    r = re.findall(pattern_regex, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    \n    return text ","218ad73b":"# We are keeping cleaned tweets in a new column called 'tidy_tweets'\ntrain['tidy_tweets'] = np.vectorize(remove_pattern)(train['text'], \"@[\\w]*: | *RT*\")\ntrain.head(10)\n\ntest['tidy_tweets'] = np.vectorize(remove_pattern)(test['text'], \"@[\\w]*: | *RT*\")","543c5320":"cleaned_tweets = []\ncleaned_tweets_test = []\n\nfor index, row in train.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets.append(' '.join(words_without_links))\n    \nfor index, row in test.iterrows():\n    # Here we are filtering out all the words that contains link\n    words_without_links = [word for word in row.tidy_tweets.split() if 'http' not in word]\n    cleaned_tweets_test.append(' '.join(words_without_links))\n\ntrain['tidy_tweets'] = cleaned_tweets\ntest['tidy_tweets'] = cleaned_tweets_test\ntrain.head(10)","2ac75959":"train = train[train['tidy_tweets']!='']\ntest = test[test['tidy_tweets']!='']\ntrain.head()","b2ebd222":"train.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntest.drop_duplicates(subset=['tidy_tweets'], keep=False)\ntrain.head()","57135562":"train = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\ntrain.head()","c215385b":"train['absolute_tidy_tweets'] = train['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")\ntest['absolute_tidy_tweets'] = test['tidy_tweets'].str.replace(\"[^a-zA-Z# ]\", \"\")","3923b5d5":"stopwords_set = set(stopwords)\ncleaned_tweets = []\ncleaned_tweets_test = []\n\nfor index, row in train.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets.append(' '.join(words_without_stopwords))\n    \n    \nfor index, row in test.iterrows():\n    \n    # filerting out all the stopwords \n    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set and '#' not in word.lower()]\n    \n    # finally creating tweets list of tuples containing stopwords(list) and sentimentType \n    cleaned_tweets_test.append(' '.join(words_without_stopwords))\n    \n    \ntrain['absolute_tidy_tweets'] = cleaned_tweets\ntest['absolute_tidy_tweets'] = cleaned_tweets_test\ntrain.head(10)","bee5e518":"tokenized_tweet = train['absolute_tidy_tweets'].apply(lambda x: x.split())\n\ntokenized_tweet_test = test['absolute_tidy_tweets'].apply(lambda x: x.split())\n\ntokenized_tweet.head()","ce2e9586":"word_lemmatizer = WordNetLemmatizer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\n\ntokenized_tweet_test = tokenized_tweet_test.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])\ntokenized_tweet.head()","476ffeea":"for i, tokens in enumerate(tokenized_tweet):\n    tokenized_tweet[i] = ' '.join(tokens)\n\n\nfor i, tokens in enumerate(tokenized_tweet_test):\n    tokenized_tweet_test[i] = ' '.join(tokens)\n\n    \ntrain['absolute_tidy_tweets'] = tokenized_tweet\ntest['absolute_tidy_tweets'] = tokenized_tweet_test\ntrain.head(10)","5681431c":"class PhraseExtractHelper(object):\n    def __init__(self):\n        self.lemmatizer = nltk.WordNetLemmatizer()\n        self.stemmer = nltk.stem.porter.PorterStemmer()\n    \n    def leaves(self, tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n\n    def normalise(self, word):\n        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n        word = word.lower()\n        # word = self.stemmer.stem_word(word) # We will loose the exact meaning of the word \n        word = self.lemmatizer.lemmatize(word)\n        return word\n\n    def acceptable_word(self, word):\n        \"\"\"Checks conditions for acceptable word: length, stopword. We can increase the length if we want to consider large phrase\"\"\"\n        accepted = bool(3 <= len(word) <= 40\n            and word.lower() not in stopwords\n            and 'https' not in word.lower()\n            and 'http' not in word.lower()\n            and '#' not in word.lower()\n            )\n        return accepted\n\n    def get_terms(self, tree):\n        for leaf in self.leaves(tree):\n            term = [ self.normalise(w) for w,t in leaf if self.acceptable_word(w) ]\n            yield term","033ee875":"sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\ngrammar = r\"\"\"\n    NBAR:\n        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n        \n    NP:\n        {<NBAR>}\n        {<NBAR><IN><NBAR>}  # Above, connected with in\/of\/etc...\n\"\"\"\nchunker = nltk.RegexpParser(grammar)","a51c26a4":"key_phrases = []\nkey_phrases_test = []\nphrase_extract_helper = PhraseExtractHelper()\n\nfor index, row in train.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases.append(' '.join(term))\n    \n    key_phrases.append(tweet_phrases)\n    \nkey_phrases[:10]\n\nfor index, row in test.iterrows(): \n    toks = nltk.regexp_tokenize(row.tidy_tweets, sentence_re)\n    postoks = nltk.tag.pos_tag(toks)\n    tree = chunker.parse(postoks)\n\n    terms = phrase_extract_helper.get_terms(tree)\n    tweet_phrases_test = []\n\n    for term in terms:\n        if len(term):\n            tweet_phrases_test.append(' '.join(term))\n    \n    key_phrases_test.append(tweet_phrases_test)\n    \nkey_phrases_test[:10]","b73399a6":"textblob_key_phrases = []\ntextblob_key_phrases_test = []\nextractor = ConllExtractor()\n\nfor index, row in train.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases.append(list(blob.noun_phrases))\n\ntextblob_key_phrases[:10]\n\nfor index, row in test.iterrows():\n    # filerting out all the hashtags\n    words_without_hash = [word for word in row.tidy_tweets.split() if '#' not in word.lower()]\n    \n    hash_removed_sentence = ' '.join(words_without_hash)\n    \n    blob = TextBlob(hash_removed_sentence, np_extractor=extractor)\n    textblob_key_phrases_test.append(list(blob.noun_phrases))\n\ntextblob_key_phrases_test[:10]","71b83c7e":"train['key_phrases'] = textblob_key_phrases\n\ntest['key_phrases'] = textblob_key_phrases_test\ntrain.head(10)","e503ea9b":"def generate_wordcloud(all_words):\n    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate(all_words)\n\n    plt.figure(figsize=(14, 10))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()","c13511df":"all_words = ' '.join([text for text in train['absolute_tidy_tweets'][train.target == 1]])\ngenerate_wordcloud(all_words)","d2379fe8":"all_words = ' '.join([text for text in train['absolute_tidy_tweets'][train.target == 0]])\ngenerate_wordcloud(all_words)","5374de4d":"tweets_df = train[train['key_phrases'].str.len()>0]","81068a07":"tweets_df","2d831819":"phrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\ntweets_df['Concatenated'] = tweets_df['absolute_tidy_tweets'] + phrase_sents","94271125":"bow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english')\nbow_word_feature = bow_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])\n\nphrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\n\nbow_phrase_vectorizer = CountVectorizer(max_df=0.90, min_df=2)\nbow_phrase_feature = bow_phrase_vectorizer.fit_transform(phrase_sents)","0f88eb2b":"# TF-IDF features\ntfidf_word_vectorizer = TfidfVectorizer(max_features=36000,max_df=0.99, min_df=1, stop_words='english',ngram_range=(1,2))\n# TF-IDF feature matrix\ntfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df['Concatenated'])","3ef33dbf":"phrase_sents = tweets_df['key_phrases'].apply(lambda x: ' '.join(x))\n# TF-IDF phrase feature\ntfidf_phrase_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, ngram_range=(1,2))\ntfidf_phrase_feature = tfidf_phrase_vectorizer.fit_transform(phrase_sents)","bb691fdd":"target_variable = tweets_df['target']","d4b28465":"def plot_confusion_matrix(matrix):\n    plt.clf()\n    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Set2_r)\n    classNames = ['Positive', 'Negative']\n    plt.title('Confusion Matrix')\n    plt.ylabel('Predicted')\n    plt.xlabel('Actual')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TP','FP'], ['FN', 'TN']]\n\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(matrix[i][j]))\n    plt.show()","9210a47c":"def naive_model(X_train, X_test, y_train, y_test):\n    naive_classifier = MultinomialNB()\n    naive_classifier.fit(X_train.toarray(), y_train)\n\n    # predictions over test set\n    predictions = naive_classifier.predict(X_test.toarray())\n\n    # calculating Accuracy Score\n    print(f'Accuracy Score :: {accuracy_score(y_test, predictions)}')\n    conf_matrix = confusion_matrix(y_test, predictions, labels=[True, False])\n    plot_confusion_matrix(conf_matrix)\n    \n    return naive_classifier","e37f6a93":"X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=272)\nTF_word = naive_model(X_train, X_test, y_train, y_test)\nTF_word","efae1f5e":"X_train, X_test, y_train, y_test = train_test_split(tfidf_phrase_feature, target_variable, test_size=0.3, random_state=272)\nTF_phrase = naive_model(X_train, X_test, y_train, y_test)\nTF_phrase","075965f8":"X_train, X_test, y_train, y_test = train_test_split(bow_phrase_feature, target_variable, test_size=0.3, random_state=272)\nBOW_phrase = naive_model(X_train, X_test, y_train, y_test)","fb1dc734":"X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=272)\nBOW_words = naive_model(X_train, X_test, y_train, y_test)","82bc85f0":"phrase_sents_test = test['key_phrases'].apply(lambda x: ' '.join(x))\ntest['Concatenated'] = test['absolute_tidy_tweets'] + phrase_sents_test","a75847e7":"# TF-IDF feature matrix\ntfidf_word_feature_test = tfidf_word_vectorizer.fit_transform(test['Concatenated'])","f3702771":"test['target'] = TF_word.predict(tfidf_word_feature_test.toarray())","b18b8c45":"test","b0ffa96a":"submission = test[['id','target']]","f6687d73":"submission.set_index('id',inplace=True)\nsubmission","6d783ada":"submission.to_csv('submission.csv')","3e6ec285":"# Visualization","44f0ef8c":"# Predictions","61efc65d":"### 3.","7af00f79":"# PreProcessing","7e8938ee":"### 2.","c35c7e5e":"### TF-idf Model","f471b570":"# Preprocessing","5c9da02a":"# EDA","6f78642a":"# **Getting Data**","d126b86b":"# Feature Extraction","a964f00a":"### Bag of Words","a8d8dad7":"### BAG of WORDS model","26827ea2":"### Insight of Submission file","15d29bcc":"## Text","f7c7a28a":"### 1. ","6d33bdee":"## Key Phrases"}}