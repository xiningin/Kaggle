{"cell_type":{"8049bdf1":"code","4f870e75":"code","498a789e":"code","64e9fadb":"code","dea16fd4":"code","af788c9b":"code","22c31469":"code","4488ace0":"code","39667f43":"code","21201eb9":"code","612edd67":"code","67a3f0b1":"code","b7bd5ee4":"code","e0e559d0":"code","d99ed250":"code","f47cbf15":"code","4b26cf56":"code","6ea1adc9":"code","71886f75":"code","782577d2":"code","ac6c51b2":"code","fb57c5ad":"code","2d01e787":"code","cb00b303":"code","6e440415":"code","fda90417":"code","831d48f4":"code","aa7f667f":"code","14e279a1":"markdown","65e488b5":"markdown","d6f235f1":"markdown","8f795712":"markdown","d5cccc07":"markdown","aec0b52e":"markdown","9cbc5036":"markdown","c685dd84":"markdown","ad3a6d3c":"markdown","275875af":"markdown","a505109c":"markdown"},"source":{"8049bdf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f870e75":"import nltk\nimport pandas as pd\nimport string\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport regex as re\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, recall_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom xgboost import XGBClassifier","498a789e":"data = pd.read_csv('\/kaggle\/input\/spam-filter\/emails.csv')      # reading data","64e9fadb":"data.head()        # checking data","dea16fd4":"data.info()","af788c9b":"# Checking null\/missing values\ndata.isnull().sum()","22c31469":"# Checking counts of spams and non-spams\ndata['spam'].value_counts()","4488ace0":"# Removing Punctutaion\ndef remove_punctuation(text):\n    no_punct=\"\".join([words for words in text if words not in string.punctuation])\n    return no_punct\ndata[\"text\"] = data['text'].apply(lambda x: remove_punctuation(x))\ndata.head()","39667f43":"# Removing Stopwords\nstopword = set(stopwords.words('english'))\nstopword.add('Subject')\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in stopword])\ndata['text'] = data['text'].apply(lambda x: remove_stopwords(x))\ndata.head()","21201eb9":"# Tokenizing\n'''Tokenization is the process of breaking text into smaller pieces called tokens. \nThese smaller pieces can be sentences, words, or sub-words.'''\ndef tokenize(text):\n    split=re.split(\"\\W+\",text) \n    return split\ndata['text']=data['text'].apply(lambda x: tokenize(x.lower()))\ndata.head()","612edd67":"# Lemmatizing\n'''Lemmatizing is the process of reducing a word to its root form.'''\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text])\n\ndata['text'] = data[\"text\"].apply(lambda text: lemmatize_words(text))\ndata.head()","67a3f0b1":"# Splitting the data according to spam and non-spam \nspam = \" \".join(data[data['spam'] == 1]['text'].tolist())\nnon_spam = \" \".join(data[data['spam'] == 0]['text'].tolist())","b7bd5ee4":"# Finding most repeated words in the data\ndef return_top_words(text,words = 10):\n    allWords = nltk.tokenize.word_tokenize(text)\n    stopwords = nltk.corpus.stopwords.words('english')\n    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in stopwords)    \n    mostCommontuples= allWordExceptStopDist.most_common(words)\n    mostCommon = [tupl[0] for tupl in mostCommontuples]\n    return mostCommon","e0e559d0":"top_10_spam = return_top_words(spam,10)\ntop_10_non_spam = return_top_words(non_spam,10)","d99ed250":"print(top_10_spam)\nprint(top_10_non_spam)","f47cbf15":"stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in data.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(spam) ","4b26cf56":"plt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","6ea1adc9":"stopwords = set(STOPWORDS) \n  \n# iterate through the csv file \nfor val in data.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(non_spam) ","71886f75":"plt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","782577d2":"X = data['text']\ny = data['spam']","ac6c51b2":"# TF-IDF (Term Frequency - Inverse Document Frequency)\n'''This is a technique to quantify a word in documents, we generally compute a weight to each word\nwhich signifies the importance of the word in the document and corpus. \nThis method is a widely used technique in Information Retrieval and Text Mining.'''\nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nX_ct  = vectorizer.transform(X)","fb57c5ad":"# Splitting the data\nX_train,X_test,y_train,y_test = train_test_split(X_ct,y,test_size=0.2,random_state=42)","2d01e787":"print(X_train.shape)\nprint(y_train.shape)","cb00b303":"print(X_test.shape)\nprint(y_test.shape)","6e440415":"knn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train,y_train)\ny_pred1 = knn_classifier.predict(X_test)\nprint(\"accuracy score is :\",accuracy_score(y_test,y_pred1))\nprint(classification_report(y_test,y_pred1))","fda90417":"nb= MultinomialNB()\nnb.fit(X_train,y_train)\ny_pred2 = nb.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))","831d48f4":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred3 = rf.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred3))\nprint(classification_report(y_test,y_pred3))","aa7f667f":"xg = XGBClassifier()\nxg.fit(X_train, y_train)\ny_pred4 = xg.predict(X_test)\nprint(\"accuracy score is: \",accuracy_score(y_test,y_pred4))\nprint(classification_report(y_test,y_pred4))","14e279a1":"#### Accuracy Score for KNN Classifier is 97%","65e488b5":"#### Accuracy Score for Random Forest is 97%","d6f235f1":"### Random Forest","8f795712":"### **Naive Bayes**","d5cccc07":"### WordCloud","aec0b52e":"### XGBoost","9cbc5036":"#### Accuracy score for Naive Bayes Classifier is 89%","c685dd84":"### Observing Data","ad3a6d3c":"### **KNN Classifier**","275875af":"#### Accuracy Score for XGBoost is 98%","a505109c":"### Preprocessing"}}