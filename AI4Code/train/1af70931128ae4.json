{"cell_type":{"302e8ed7":"code","f4a6245e":"code","d17e90d6":"code","862a3c4c":"code","6c1cfecb":"code","c64d9cca":"code","59b69310":"code","6c22d1c2":"code","b727660b":"code","3760ff88":"code","2f75eaae":"code","6e4fbb6d":"code","1740ec4f":"code","f7c6f626":"code","3c2bceb5":"code","6fe9e580":"code","f2f4bd76":"code","52cc51d5":"code","778525d7":"code","cb62092e":"code","e4303c03":"code","1f3adc62":"code","d3bff7dc":"code","6439f329":"code","5e768dca":"code","0a8f61f9":"code","e225b6e9":"code","6fbd6624":"code","cc7feeea":"code","6269b57d":"code","b364ecf4":"code","992a5ff1":"markdown","ccf876df":"markdown","9ef0a79b":"markdown","bae715a9":"markdown"},"source":{"302e8ed7":"from __future__ import print_function\nfrom __future__ import division\nfrom __future__ import absolute_import","f4a6245e":"# importing libraries\n\nimport os\nimport datetime # handling timestamps\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\n\n# cell outputs\nimport IPython\nimport IPython.display\n\n# classical Time Series Tools\nimport statsmodels.api as api\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf\nfrom statsmodels.tsa.arima_model import ARMA, ARIMA\n\n# metrics\nfrom sklearn.metrics import explained_variance_score, mean_absolute_error\n\n# plotting\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nmpl.rcParams['figure.figsize'] = (8,6)\nmpl.rcParams['axes.grid'] = False\n\n","d17e90d6":"print(tf.__version__)","862a3c4c":"data_path = '..\/input\/cryptocurrencypricehistory\/coin_Bitcoin.csv'","6c1cfecb":"bitcoin = pd.read_csv(data_path)\n","c64d9cca":"bitcoin.head()","59b69310":"date_time = pd.to_datetime(bitcoin.pop('Date'),format='%Y-%m-%d %H:%M:%S')","6c22d1c2":"# plotting the Closing price and Marketcap\nplot_cols = ['Close','Marketcap']\nplot_features = bitcoin[plot_cols]\nplot_features.index = date_time\n_ = plot_features.plot(subplots=True)\n\n# plotting an excerpt\nplot_features = bitcoin[plot_cols][:400]\nplot_features.index = date_time[:400]\n_ = plot_features.plot(subplots=True)\n","b727660b":"# summary statistics\nbitcoin.describe().transpose()","3760ff88":"# splitting the data in train, test and val\n\nn = len(bitcoin)\n\ntrain_df = bitcoin[0:int(n*0.7)]\nval_df   = bitcoin[int(0.7*n):int(0.9*n)]\ntest_df  = bitcoin[int(0.9*n):]\n","2f75eaae":"# normalizing the features\n\ntrain_mean = train_df.mean()\ntrain_std  = train_df.std()\n\ntrain_df = (train_df-train_mean)\/train_std\nval_df   = (val_df-train_mean)\/train_std\ntest_df  = (test_df-train_mean)\/train_std\n\n","6e4fbb6d":"# plotting the standard deviation for the features\ndf_std = (bitcoin - train_mean)\/train_std\ndf_std = df_std[plot_cols]\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nax = sns.violinplot(x='Column',y='Normalized',data=df_std)\n_ = ax.set_xticklabels(df_std.keys(),rotation=90)\n","1740ec4f":"# evaluate stationarity\ndef evaluate_stationarity(timeseries, timeframe=7):\n    roll_mean = timeseries.rolling(window=timeframe).mean()\n    roll_std  = timeseries.rolling(window=timeframe).std()\n    \n    # plot the rolling statistics\n    orig = plt.plot(timeseries,color='blue',label='Original')\n    mean = plt.plot(roll_mean,color='red',label='Rolling Mean')\n    std  = plt.plot(roll_std,color='black',label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean and Standard Deviation')\n    plt.show(block=False)\n    \n    # perform ADF test\n    df_test = adfuller(timeseries,autolag='AIC')\n    df_output = pd.Series(df_test[0:4],index=['Test Statistics','p-value','#lags used','Number of Observations Used'])\n    for k,v in df_test[4].items():\n        df_output['Critical Value ({})'.format(k)]=v\n    print(df_output)    \n    \n","f7c6f626":"train_close = train_df['Close']\ntrain_close.index = date_time[0:int(n*0.7)]\ntest_close  = test_df['Close']\ntest_close.index = date_time[int(n*0.9):]\nval_close  = val_df['Close']\nval_close.index = date_time[int(n*0.7):int(n*0.9)]\n\n\nevaluate_stationarity(train_close, timeframe=356)\n","3c2bceb5":"# apply some transformations to make the data stationary\nlogged_data = np.log(1 + train_close)\nevaluate_stationarity(logged_data,timeframe=30)","6fe9e580":"# decomposing the time series\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(list(logged_data),freq=30)","f2f4bd76":"def plot_components(original, decomposition):\n    trend = decomposition.trend\n    seasonal = decomposition.seasonal\n    residual = decomposition.resid \n\n    plt.subplot(411)\n    plt.plot(original,label='Original')\n    plt.legend(loc='best')\n    plt.subplot(412)\n    plt.plot(trend,label='Trend')\n    plt.legend(loc='best')\n    plt.subplot(413)\n    plt.plot(seasonal,label='Seasonality')\n    plt.legend(loc='best')\n    plt.subplot(414)\n    plt.plot(residual,label='Residual')\n    plt.legend(loc='best')\n    \n    \nplot_components(logged_data, decomposition)    ","52cc51d5":"logged_residual = pd.Series(decomposition.resid)\nlogged_residual.index = date_time[:int(0.7*n)]\nlogged_residual.dropna(inplace=True)\nevaluate_stationarity(logged_residual,30)\n","778525d7":"# to determine the best ARIMA Model, we will perform grid-search on (p,d,q)\np_range = range(1,5)\nd_range = range(1,3)\nq_range = range(1,5)\n\n\n\ndef search_best_arima(train,p_range,d_range,q_range):\n    best_aic = np.inf\n    best_model = None\n    best_order = None\n    \n    for (p,d,q) in list(zip(p_range,d_range,q_range)):\n        arima = ARIMA(train.values, order=(p,d,q)).fit(method='mle',trend='nc',disp=0)\n        aic = arima.aic\n        if aic < best_aic:\n            best_model = arima\n            best_order = (p,d,q)\n            best_aic = aic\n    return best_model,best_aic,best_order\n\n\narima,aic,order = search_best_arima(logged_data,p_range,d_range,q_range)","cb62092e":"print(\"Best AIC : \",aic)\nprint(\"Best Order : \",order)","e4303c03":"arima.plot_predict(1,3006,alpha=0.05)","1f3adc62":"required_cols = ['High','Low','Open','Close','Marketcap']\n\ntrain_df = train_df[required_cols]\ntest_df  = test_df[required_cols]\nval_df   = val_df[required_cols]","d3bff7dc":"class WindowGenerator():\n    def __init__(self,input_width,shift,label_width,train_df=train_df,test_df=test_df,val_df=val_df,\n                label_columns=None):\n        # store the raw data\n        self.train_df = train_df\n        self.val_df   = val_df \n        self.test_df = test_df\n\n        # work out the label column indices\n        self.label_columns = label_columns \n        if label_columns is not None:\n              self.label_column_indices = {name : i for i, name in enumerate(label_columns)}\n\n        self.column_indices = {name : i for i, name in enumerate(train_df.columns)}\n\n        # work out the window parameters\n        self.input_width = input_width \n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slices = slice(0, input_width)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slices]\n\n        self.label_start = self.total_window_size - self.label_width\n        self.label_slice = slice(self.label_start,None)\n        self.label_indices = np.arange(self.total_window_size)[self.label_slice]\n        \n        self._create_example_window()\n        \n    def split_window(self, features):\n        inputs = features[:,self.input_slices,:]\n        labels = features[:,self.label_slice,:]\n        if self.label_columns is not None:\n            labels = tf.stack(\n                [labels[:,:,self.column_indices[name]] for name in self.label_columns ],\n                axis=-1\n            )\n        inputs.set_shape([None,self.input_width,None])\n        labels.set_shape([None,self.label_width,None])\n        \n        return inputs,labels\n    \n    def _create_example_window(self):\n        example_window = tf.stack([\n            np.array(self.train_df[:self.total_window_size]),\n            np.array(self.train_df[100:100+self.total_window_size]),\n            np.array(self.train_df[200:200+self.total_window_size]),\n        ])\n        \n        self.example = self.split_window(example_window)\n        \n    def plot(self,model=None,plot_col='Close', max_subplots=3):\n        inputs, labels = self.example\n        plt.figure(figsize=(40,30))\n        plot_col_index = self.column_indices[plot_col]\n        max_n = min(max_subplots,len(inputs))\n        for n in range(max_n):\n            plt.subplot(max_n,1,n+1)\n            plt.ylabel(f'{plot_col} [normed]')\n            plt.plot(self.input_indices, inputs[n,:,plot_col_index],\n                    label='Inputs',marker='.',zorder=-10)\n            \n            if self.label_columns:\n                label_col_index = self.label_column_indices.get(plot_col,None)\n            else:\n                label_col_index = plot_col_index\n            \n            if label_col_index is None:\n                continue\n            \n            plt.scatter(self.label_indices, labels[n,:,label_col_index],\n                       edgecolors='k',label='Labels',c='#2ca02c',s=64)\n            if model is not None:\n                predictions = model(inputs)\n                plt.scatter(self.label_indices,predictions[n,:,label_col_index],\n                           marker='X',edgecolors='k',label='Predictions',\n                           c='#ff7f0e',s=64)\n            if n == 0:\n                plt.legend()\n            \n            plt.xlabel('Time [d]')\n    \n    def make_dataset(self,data):\n        data = np.array(data,dtype=np.float32)\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n             data=data,\n             targets=None,\n             sequence_length=self.total_window_size,\n             sequence_stride=1,\n             shuffle=True,\n             batch_size=32\n          \n        )\n        \n        ds = ds.map(self.split_window)\n        return ds\n    \n    @property\n    def train(self):\n        return self.make_dataset(self.train_df)\n    \n    @property\n    def test(self):\n        return self.make_dataset(self.test_df)\n    \n    @property\n    def val(self):\n        return self.make_dataset(self.val_df)\n    \n  \n    def __repr__(self):\n            return '\\n'.join([\n                f'Total window size: {self.total_window_size}',\n                f'Input slices: {self.input_slices}',\n                f'Input indices: {self.input_indices}',\n                f'Label slices: {self.label_slice}',\n                f'Label indices: {self.label_indices}',\n                f'Label column name(s): {self.label_columns}'])\n","6439f329":"window = WindowGenerator(\n                   input_width=52*7,label_width=7,shift=7,label_columns=['Close'])\n\nwindow.plot()","5e768dca":"window.train","0a8f61f9":"# LSTM Model\n\nnum_features = 1\nOUT_STEPS = 7\n\nlstm_model = tf.keras.Sequential([\n    tf.keras.layers.LSTM(32,return_sequences=False),\n    tf.keras.layers.Dense(OUT_STEPS*num_features),\n    tf.keras.layers.Reshape([OUT_STEPS,num_features])\n])\n\n\nlstm_model.compile(loss=tf.losses.MeanSquaredError(),\n              optimizer=tf.optimizers.Adam(),\n              metrics=[tf.metrics.MeanAbsoluteError()])\n\nhistory = lstm_model.fit(window.train,epochs=25,validation_data=window.val)","e225b6e9":"window.plot(model=model)","6fbd6624":"lstm_model.evaluate(window.val)","cc7feeea":"# Conv Model\nCONV_WIDTH = 90\nconv_model = tf.keras.Sequential([\n    # Shape [batch, time, features] -> [batch, CONV_WIDTH, features]\n    tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:,:]),\n    # Shape => [batch, 1, conv_units]\n    tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\n    tf.keras.layers.Dense(OUT_STEPS*num_features,\n                          kernel_initializer=tf.initializers.zeros()),\n    tf.keras.layers.Reshape([OUT_STEPS, num_features])                      \n])\n\nconv_model.compile(loss=tf.losses.MeanSquaredError(),\n              optimizer=tf.optimizers.Adam(),\n              metrics=[tf.metrics.MeanAbsoluteError()])\n\nhistory = conv_model.fit(window.train,epochs=25,validation_data=window.val)","6269b57d":"window.plot(conv_model)","b364ecf4":"conv_model.evaluate(window.val)","992a5ff1":"### Neural Network Models\n","ccf876df":"### Loading Data","9ef0a79b":"#### In this notebook, we will look at the historical prices of Bitcoin and forecast the closing price for next 7 days","bae715a9":"## Bitcoin Weekly Price Prediction with Machine Learning"}}