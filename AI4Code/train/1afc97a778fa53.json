{"cell_type":{"a5880be2":"code","0c4885ff":"code","3dd27d50":"code","0a7d8fa9":"code","0ffea4fd":"code","947d6553":"code","6b4a1276":"code","adf2cb88":"code","0357321e":"code","723781ce":"code","9bc5378b":"code","0d6fe30a":"code","5d5ee5fd":"code","25e34396":"code","cff9dc5b":"code","0887a6e0":"code","06cfcd27":"code","8f1841e7":"code","05d61245":"code","0c382852":"markdown","838b8ce6":"markdown","cb98df49":"markdown","94b71dd9":"markdown","90c1bc5d":"markdown","f8196745":"markdown","800f4821":"markdown","8aed0ad9":"markdown","19de0e07":"markdown","61d45981":"markdown","0a22fdaf":"markdown","4740bd96":"markdown","d5355b59":"markdown","5a74f6c1":"markdown","37837aaf":"markdown","0363e1e8":"markdown","d001dae3":"markdown","498362eb":"markdown"},"source":{"a5880be2":"import numpy as np\nimport pandas as pd\nimport os\n\nos.listdir(\"..\/input\")\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","0c4885ff":"def merge_train_data():    \n    print( \"Loading Train Data...\")\n    train_transaction = pd.read_csv('..\/input\/train_transaction.csv')\n    train_identity = pd.read_csv('..\/input\/train_identity.csv')\n    \n    print( \"Shape of train_transaction\" , train_transaction.shape )\n    train_transaction.head()\n    \n    print(\"Shape of train_identity\" , train_identity.shape )\n    train_identity.head()\n    \n    print( \"Merging...\" )\n    train_merged = pd.merge( train_transaction , train_identity , on='TransactionID' , how='left' )\n    \n    del train_transaction \n    del train_identity\n    \n    return train_merged","3dd27d50":"train_merged = merge_train_data()\ntrain_merged.shape\ntrain_merged.head()","0a7d8fa9":"chk_NaN = train_merged.isnull().sum()\nchk_NaN","0ffea4fd":"def merge_test_data():\n    print( \"Loading Test Data...\")\n    test_transaction = pd.read_csv('..\/input\/test_transaction.csv')\n    test_identity = pd.read_csv('..\/input\/test_identity.csv')\n    \n    print( \"Shape of test_transaction\" , test_transaction.shape )\n    test_transaction.head()\n    \n    print(\"Shape of test_identity\" , test_identity.shape )\n    test_identity.head()\n    \n    print( \"Merging...\" )\n    test_merged = pd.merge( test_transaction , test_identity , on='TransactionID' , how='left' )\n\n    del test_transaction\n    del test_identity\n    return test_merged","947d6553":"test_merged = merge_test_data()\ntest_merged.shape\ntest_merged.head()","6b4a1276":"chk_NaN = test_merged.isnull().sum()\nchk_NaN","adf2cb88":"%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot( x = 'isFraud', data = train_merged )\nplt.xticks(rotation=45)\nplt.title( 'Target' )\nplt.show()","0357321e":"target = train_merged['isFraud']\ntrain_merged = train_merged.drop('isFraud' , axis='columns')","723781ce":"train_merged.shape","9bc5378b":"rows_count = train_merged.shape[0]\n\nmissing_value = pd.DataFrame(columns=['Missing Rate'])\nmissing_value['Missing Rate'] = train_merged.isnull().sum() \/ rows_count\n\nhigh_miss_rate = missing_value[ missing_value['Missing Rate'] >= 0.85 ].index.values.tolist()\n\nprint(\"High Missing Rate(85%) Feature Count : \",len(high_miss_rate))\n\ntrain_merged = train_merged.drop(high_miss_rate , axis='columns')\ntrain_merged.shape\ndel high_miss_rate","0d6fe30a":"cols_int = []\ncols_float = []\ncols_object = []\n\nall_cols_list = train_merged.columns.values.tolist()\n\nfor col in all_cols_list:\n    if train_merged[col].dtypes == 'int64':\n        cols_int.append(col)\n    elif train_merged[col].dtypes == 'float64':\n        cols_float.append(col)\n    elif train_merged[col].dtypes == 'object':\n        cols_object.append(col)\n    else:\n        print('Exception')\n\nprint( 'Total train_merged Feature Numbers : ', len(all_cols_list))\nprint( 'int64 Feature Numbers : ', len(cols_int))\nprint( 'float64 Feature Numbers : ', len(cols_float))\nprint( 'object Feature Numbers : ', len(cols_object))","5d5ee5fd":"import xgboost as xgb\nfrom xgboost import XGBClassifier","25e34396":"# fit model no training data\nmodel = XGBClassifier( nthread = 4 )\nmodel.fit( train_merged[cols_int + cols_float] , target)","cff9dc5b":"xgb_fea_imp = pd.DataFrame(list(model.get_booster().get_fscore().items()),columns=['feature','importance']).sort_values('importance', ascending=False)\nxgb_fea_imp.shape\nxgb_fea_imp.head(10)","0887a6e0":"import lightgbm as lgb\n\ntrain_ds = lgb.Dataset( train_merged[ xgb_fea_imp['feature'].tolist() ] , label = target )\n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_threads' : 4,\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 1\n}\n\nmodel = lgb.train(parameters, train_ds ,num_boost_round=5000)\n","06cfcd27":"prob = model.predict( test_merged[ xgb_fea_imp['feature'].tolist() ] )\nprob","8f1841e7":"submission = pd.DataFrame()\nsubmission['TransactionID'] = test_merged['TransactionID']\nsubmission['isFraud'] = prob","05d61245":"submission.head()","0c382852":"#### XGBoost has selected 132 features with Importance.\n#### Now let's train using these features.","838b8ce6":"#### Transaction data has about 590,000 data, 394 features, while identity data has about 140,000 data, and 41 features.\n\n#### As we compare the shape of these two data sets, when we merge identity Data based on transaction data, merged data will include many NaN data.","cb98df49":"#### Total 74 features are consisted of NaN over 85% and will be removed from train data set.\n#### However, since we can do feature engineering later using the ratio of NaN or the meaning of NaN, let's try to decide whether to use it","94b71dd9":"#### In the train dats set, the target column is removed from the features.","90c1bc5d":"### In addition, the following applies to the following kernel.\n- Using categorical features, additional Feature Engineering\n- Application of Cross Validation\n- Stacking","f8196745":"#### The features I used are all features without special feature engineering\n#### We only used numerical features, and we did not use any other categorical features at all.","800f4821":"#### Similar to train data, let's load test data and merge as same ways of train data","8aed0ad9":"#### The distribution of target is very unbalanced.","19de0e07":"#### Let's take a look at each of the remaining features by their data type.\n#### Let's break it down into Int, Float, and Object","61d45981":"### OK. Let's start the kernel by loading the basic packages.","0a22fdaf":"#### Let's measure the feature importance of the remaining features with XGBoost.","4740bd96":"#### Currently, the total number of features is 433 and I think it seems there are too many features.\n#### So, I decided to remove some features that has little information","d5355b59":"#### Not a high score, but it seems appropriate for use as a Baseline Model.","5a74f6c1":"#### I will check if the necessary data set files are located correctly.\n#### 2 train data sets, 2 test data sets and lastly data for submission.\n#### All data sets are OK.","37837aaf":"* As mentioned earlier, this competition is divided into train and test data set into transaction and identity data sets.\n* Therefore, we need to combine these two data sets into one.\n* The 'merge_train_data' is a function that loads two files of Train Data Set and merges identity based on transaction data set.","0363e1e8":"#### I will share with you the next kernel on the methods I will use.\n#### Please let me know if I made a mistake or have a better suggestion.\n\n### Thank you for reading.","d001dae3":"#### First, let's remove the feature with a large number of NaN.","498362eb":"Hello, This Kernel is the first kernel I write in Kaggle.\n\nThis competetion is the unbalance binary classification competition, which is common in Kaggle.\n\nWhat is unique is that Train \/ Test Data is provided in two separated files, transaction and identify, both are linked by 'TransactionID' value."}}