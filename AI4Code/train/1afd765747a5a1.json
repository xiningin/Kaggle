{"cell_type":{"c05c1f84":"code","3292d216":"code","d4b040ed":"code","8beb4779":"code","e79f702e":"code","f745d555":"code","dfb73edb":"code","715f011f":"code","1665496a":"code","895172ca":"code","e635b308":"code","d70e3bc6":"code","5c6cf5d1":"code","d75deffb":"code","232f5593":"code","2b6cde89":"code","525acca8":"code","b18db548":"code","7dcc80bc":"code","54bdc034":"code","d4f8914d":"code","46efcac9":"code","cf21d8db":"code","38da020d":"code","c5b4b746":"code","80c4e4be":"code","aba204a7":"code","c73cb58d":"code","daef6154":"code","e4c30341":"code","141c454c":"code","d97f376f":"code","4e2ba385":"code","00e45554":"code","07bacb7f":"markdown","6f14c460":"markdown","b0e89cdb":"markdown","595923d4":"markdown","d25df88f":"markdown","602f3531":"markdown","b2c7a5ff":"markdown","91628c3c":"markdown","02aed41b":"markdown","cf1eee74":"markdown","f16b5bd8":"markdown","3252bd6b":"markdown","89538eed":"markdown","454c50be":"markdown","90638d76":"markdown","3c4d0359":"markdown","8722249d":"markdown","7485274d":"markdown","885f780f":"markdown","8e691455":"markdown","53f00ede":"markdown","1392c8cb":"markdown","e599db90":"markdown","23810606":"markdown","0ee4a3f5":"markdown","74f6a4fd":"markdown","1d8c8044":"markdown"},"source":{"c05c1f84":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.tokenize import word_tokenize\nfrom IPython.display import Image, display","3292d216":"plt.style.use('ggplot')\n%config InlineBackend.figure_format = 'retina'\n# sns.set(font_scale=1.1)","d4b040ed":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","8beb4779":"train.shape","e79f702e":"train.head()","f745d555":"def without_hue(plot, feature):\n    total = len(feature)\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.05\n        y = p.get_y() + p.get_height()\n        ax.annotate(percentage, (x, y+30), size = 12)\n    plt.show()","dfb73edb":"plt.figure(figsize=(8, 5))\nplt.title('Distribution of class labels')\nax = sns.countplot(x='target', data=train);\nwithout_hue(ax, train.target)","715f011f":"def plot_most_common(data, n, column, filter_by_class=True, **kwargs):\n    # Remove any NaN values in the column\n    d = data[~data[column].isna()]\n    if kwargs.get('make_copy', True):\n        data_f = d.copy()\n    else:\n        data_f = d\n    target_col = kwargs['target']\n    if filter_by_class:\n        # Plot for a particular class\n        target_val = kwargs['target_val']\n        data_f = data_f[data_f[target_col] == target_val]\n        c = Counter(data_f[column])\n        plot_data = pd.DataFrame(c.most_common(n))\n        plot_data.columns = [column, 'count']\n        ax = sns.barplot(x='count', y=column, data=plot_data, \n                         palette=kwargs.get('palette', 'Blues_r'))\n        plt.title(kwargs.get('title'))\n        return ax\n    else:\n        # Plot for both the classes combined\n        both_target = kwargs.get('both_target', [0, 1])\n        data_f_0 = data_f[data_f[target_col] == both_target[0]]\n        data_f_1 = data_f[data_f[target_col] == both_target[1]]\n        c1 = Counter(data_f_0[column])\n        c2 = Counter(data_f_1[column])\n        plot_data = pd.DataFrame([c1, c2]).T\n        if kwargs['handle_singular'] == 'drop':\n            plot_data = plot_data.dropna().reset_index()\n        elif kwargs['handle_singular'] == 'fill':\n            plot_data = plot_data.fillna(value=0).reset_index()\n        \n        plot_data.columns = [column, both_target[0], both_target[1]]\n        plot_data = plot_data.sort_values(by=[1, 0], ascending=[False, True]).head(n)\n        out = pd.melt(plot_data, id_vars=column, var_name='class', value_name='count')\n        out['count'] = out['count'].astype(int)\n        ax = sns.barplot(y='keyword', x='count', hue='class', \n                         data=out, palette=['tomato', 'cornflowerblue'])\n        plt.title(kwargs.get('title'))\n        return ax\n\nsns.set(font_scale=1.8)\nplt.figure(figsize=(28, 16))\nplt.subplot(121)\nplot_most_common(train, 10, 'location', target='target', target_val=0, \n                 title='Top locations for metaphores')\nplt.subplot(122)\nplot_most_common(train, 10, 'location', target='target', target_val=1, palette='Reds_r', \n                 title='Top locations for real disasters')\nplt.show()","1665496a":"plt.figure(figsize=(28, 16), dpi=100)\nplt.subplot(121)\nsns.set(font_scale=1.5)\nplot_most_common(train, 20, 'keyword', target='target', target_val=0, \n                 title='Top keywords for metaphores')\nplt.subplot(122)\nplot_most_common(train, 20, 'keyword', target='target', target_val=1, palette='Reds_r', \n                 title='Top keywords for real disasters')\nplt.show()","895172ca":"sns.set(font_scale=1)\nplt.figure(figsize=(8, 40), dpi=100)\ng = plot_most_common(train, 221, 'keyword', False, target='target', handle_singular='fill',\n                    title='Top 80 overlapping keywords');\ng.set_xticklabels(g.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","e635b308":"train['tweet_len'] = train.text.apply(lambda x: len(x))","d70e3bc6":"train.head()","5c6cf5d1":"plt.figure(figsize=(10, 6))\nsns.distplot(train[train['target'] == 1]['tweet_len'] , \n             label = 'Disaster', color = 'red')\nsns.distplot(train[train['target'] == 0]['tweet_len'], \n             label = 'Metaphore' , color = 'blue')\nplt.suptitle('How tweet length impact class label', fontsize=20)\nplt.legend()\nplt.show()","d75deffb":"# \" \".join(word_tokenize(train['text'][0].lower()))","232f5593":"def plot_wordcloud(data, col, background_color, \n                   colormap, remove_stopwords=True,\n                  split_class=False, **kwargs):\n    \n    text = data[col].apply(lambda x: \" \".join(word_tokenize(x.lower())))\n    text = \" \".join(text)\n    if remove_stopwords:\n        stopwords = STOPWORDS\n    else:\n        stopwords = []\n    wordcloud = WordCloud(width=3000, height=2000, random_state=42, \n                          background_color=background_color, \n                          colormap=colormap, collocations=False, \n                          stopwords=stopwords).generate(text)\n    \n    plt.figure(figsize=(20, 15))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\");\n    plt.show()\n    return text","2b6cde89":"# text = plot_wordcloud(train, 'text', 'black', 'Set2')\ndisplay(Image(filename='..\/input\/wordcloud2\/wordcloud.png'))","525acca8":"!pip install urlextract","b18db548":"from urlextract import URLExtract\nextractor = URLExtract()","7dcc80bc":"train['has_url'] = train.text.apply(lambda x: extractor.has_urls(x))\ntrain.head()","54bdc034":"sns.countplot(x='target', hue='has_url', data=train);","d4f8914d":"train_with_links = train[train.has_url == True]","46efcac9":"train_with_links_class_0 = train_with_links[train_with_links.target == 0].copy()\ntrain_with_links_class_1 = train_with_links[train_with_links.target == 1].copy()","cf21d8db":"def remove_urls(text):\n    urls = extractor.find_urls(text)\n    for url in urls:\n        text = text.replace(url, \"\")\n    return text","38da020d":"train_with_links_class_0['text_no_url'] = train_with_links_class_0.text.apply(remove_urls)\ntrain_with_links_class_1['text_no_url'] = train_with_links_class_1.text.apply(remove_urls)","c5b4b746":"display?","80c4e4be":"display(Image('..\/input\/wordcloud2\/wordcloud0.png', retina=True))","aba204a7":"display(Image('..\/input\/wordcloud2\/wordcloud1.png', retina=True))","c73cb58d":"!pip install Pandarallel","daef6154":"import requests\nfrom pandarallel import pandarallel","e4c30341":"pandarallel.initialize(progress_bar=True)","141c454c":"def extract_urls(text):\n    urls = extractor.find_urls(text)\n    u = []\n    \n    for url in urls:\n        try:\n            r = requests.get(url, timeout=10)\n            for h in r.history:\n                pass\n            u.append(r.url)\n        except:\n            u.append(url)\n    return u","d97f376f":"train_with_links_class_0['urls'] = train_with_links_class_0.text.parallel_apply(extract_urls)","4e2ba385":"train_with_links_class_1['urls'] = train_with_links_class_1.text.parallel_apply(extract_urls)","00e45554":"newTrain = pd.concat([train_with_links_class_0, train_with_links_class_1])\nnewTrain.to_csv('newTrain.csv', index=False)","07bacb7f":"#### Conclusions\n1. It seems almost 14% more people love metaphores\n2. The difference is still not a lot between these which indicates a lot of people also communicate their status with other using twitter.\n3. If there arises any class imbalance issue, we can use libraries like [nlpAug](https:\/\/github.com\/makcedward\/nlpaug) for upscaling the lower classes.","6f14c460":"It can be a common human nature to talk about the metaphorical statements more than the disasters. Ususlly when people tweet about disasters, they should not be in the mood to be poetic but rather they should be focusing on the real incident and tweet about it. Sometime they may share some news clips and other things as compared to writting more about it. Let's verify these claims. One nice way to do that is to check the distribution of the length of the tweets for both the class labels and see if they reveal something or not.","b0e89cdb":"### When people talk about buildings?","595923d4":"#### Conclusions\n\n1. Location should have played a very important role but we can see that it very random and unstructured. Hence I think location would not be a very efficent predictor.\n2. Some location and their variations are repeated. For example **USA**, **US** and **United States**. Combining these and reproducting the plot might reveal some more information.\n3. However if other features are not well performing, we can use this to boost up the validation score during modeling.","d25df88f":"### Conclusion\n\n1. **Alas!** there seems to almost a complete overlapping on the tweet length.\n2. We might have to explore some other features to see if we can find some trend or not.","602f3531":"We can see a lot of keywords like \n\n - Crush\n - Wrecked\n - Explode\n\nare used as a methaphore which may have good association with actual disasters. Thinking carefully this makes real sense because these keywords in real life can actually be used for a lot of other purposes where they are used metaphorically.\n\nIt also seems a lot of keywords are overlapping. We may want to see that if a keyword is being used for both the tags, where it is more prevalent. For example **damage** is a keyword that can be used for some person or situation metaphorically or in literal sense too. Let's explore that. If a lot of such keywords exists, then **keyword** alone won't be a good feature to predict the labels.","b2c7a5ff":"> I could have used my own code to display the wordcloud but [this](https:\/\/monkeylearn.com\/word-cloud\/) looked much more polished and they performed some preprocessing too. Hence I am using this. ","91628c3c":"### Metaphorical Tweets","02aed41b":"#### Conclusion\n\n1. We can see some startling differences between the wordclouds. This indicates that words in raw text of the tweet can be useful.\n2. A lot of people while taling about non disaster, refer hollywood movies and youtube videos.\n3. When talking about actual disaster, people generally tend to talk about hiroshima, disaster etc.\n4. I wonder what are the primary domain names of these links for both the categories.","cf1eee74":"Now that we have this data, lets see the difference domain for both these group into a word cluster.","f16b5bd8":"This can be important because some location are more prone to natural disasters as compared to others like Japan suffers from earthquakes a lot. So lets see if it is important or not.","3252bd6b":"## Does location play any role for disasters?","89538eed":"Most of the data science and machine learning stuffs should start from gather and cleaning the data and analyzing it and not directly moving to building models specially when it is about learning and knowledge. Nevertheless, the most difficult part was already handled by Kaggle. Now we should start with proper EDA and exploration of the dataset and all the EDA should always be based on some questions about the data and answering them keeping the actual objective in mind. Here our objective is to see if a tweet is really about the disaster or it is metaphorically typed. So lets start with our questions.","454c50be":"### Conclusions\n1. We can see a lot of overlapping of with just top 80 keywords.\n2. This indicates that building a rule based systems is not easy.\n3. This also indicates that feature engineering on this column may not useful a lot.\n4. This feature can still be useful to some extent.\n5. Apart from only 2 keywords, almost all the keywords seems to be associated with both the class labels. Clearly no linear seperation. This may indicate that the class labels can not be well predicted from a linear model like simply with the **keyword** feature.","90638d76":"Keywords are not easy unlike the location. Keyword don't associate with any type with assurance. For example, the keyword **blast** can be equally good or bad and can be related to disaster or not.","3c4d0359":"## How does keyword play a role into this?","8722249d":"One idea is to check most frequent occuing words in the actual tweet itself and see if we can find any particualar pattern.","7485274d":"## Does people talk about disaster metaphorically more?","885f780f":"## What are the domains of these two category of people?","8e691455":"## Who loves to use web links in their tweets?","53f00ede":"# Stay tuned for more","1392c8cb":"### Actual Disaster","e599db90":"### Difference in links in these two groups","23810606":"### Conclusions\n\n1. We can see very clearly that two keywords poped very heavily. One is **http** and another is **building**.\n2. This raise to two facts, first is that building related disasters can be more prone than others and secondly a lot of people share some link in their tweets.\n3. It can be useful to see if more people are sharing link with class 0 or class 1.\n4. People did talk about some random **Youtube** videos also in tweets.\n5. Out of all the countries in the world, we did see **Japan** being more prominent and locgically they should be because of the sufferings they had to go through in the past.","0ee4a3f5":"## Do people elaborate the real disasters more ?","74f6a4fd":"### Does people only talks about disaster when it comes to Japan?","1d8c8044":"### Conclusion\n\n1. Interesting to see that for class label 1 more people has url in their tweets as compared to class labels 0. This ensures that when people use url, they tend talk more about disaster as compared to metaphor. \n2. This raises the question that what is the difference between the links shared by people who talk about disasters vs people who use it metaphorically."}}