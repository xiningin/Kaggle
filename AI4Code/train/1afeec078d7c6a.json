{"cell_type":{"06e2bd4e":"code","6d63d01e":"code","97793434":"code","ba7df0fc":"code","89614bab":"code","b4b6d26b":"code","fcdcd776":"code","ac750f80":"code","e2f963f0":"code","c7c50e40":"code","2d281d5f":"code","e3d3aae9":"code","16726edf":"code","6700c0c0":"code","9e7a0771":"code","a788d8c1":"code","58b5b0ea":"code","5842169a":"code","3d485220":"code","095eafdb":"code","f693b122":"code","30132f3a":"code","89878960":"code","f308e6bc":"markdown","e85883e2":"markdown","4006b7ca":"markdown","f72ac891":"markdown","0d49849e":"markdown","56e4a3ed":"markdown","abf9a655":"markdown","3c0f0d34":"markdown","b7af3d0b":"markdown","6b6c859b":"markdown","a30e2f19":"markdown","3f559035":"markdown","85e8a3a1":"markdown","2723017b":"markdown"},"source":{"06e2bd4e":"import re, string\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom torch import nn\nfrom sklearn.manifold import TSNE\nfrom collections import Counter\nfrom numpy import savez_compressed\nfrom random import randint","6d63d01e":"# taking a look at one file...\nwith open ('..\/input\/spanish_corpus\/spanishText_10000_15000', 'r', encoding='latin-1') as f:\n    spanishText_1 = f.read()\n\nspanishText_1[:1000]","97793434":"# getting rid of the XML markup\nspanishText_1 = re.sub('<.*>', '', spanishText_1)\n#getting rid of the 'ENDOFARTICLE' text\nspanishText_1 = re.sub('ENDOFARTICLE.', '', spanishText_1)\n# getting rid of most of the punctuation characters\npunctuation2remove = \"[\" + re.sub('[,.;:?!()+\/-]', '', string.punctuation) + \"]\"\nspanishText_1 = re.sub(punctuation2remove, '', spanishText_1)\n# getting rid of multiple consecutive '\\n' characters\nspanishText_1 = re.sub('\\n\\n+', '\\n', spanishText_1)\n# getting rid of all those strange semicolons that are preceding any '\\n\\ character\nspanishText_1 = re.sub(';+\\n', '\\n', spanishText_1)\n# getting rid of all the dashes (that could or could not be present) that are surrounded by any whitespace characters (i.e. [ \\t\\n\\r\\f\\v])\n# the reson of explicitly getting rid of dashes surrounded by whitespaces characters is to avoid removing dashes that are used by composed words\n# like some last names (i.e. Garica-Rojas, Montero-Calvo) or some relational adjectives (i.e. f\u00edsco-qu\u00edmico, \u00e9pico-l\u00edrico), etc.\nspanishText_1 = re.sub('\\s*-\\s', ' ', spanishText_1)\n# getting rid of any dots (that could or could not be present) that are precided by any whitespace characters\nspanishText_1 = re.sub('\\s+\\.', ' ', spanishText_1)\n# Also remove any '\\n' that might be found at the begining of a line.\nspanishText_1 = re.sub('^\\n', '', spanishText_1, flags=re.MULTILINE)\n# remove any word that is standing all alone (i.e. single words used as names of chapters\/subsections, etc.). These words are not helping us to\n# construct a meaningful embedding (the relationship between these words and other preceding\/following words will be learned by the algorithm,\n# but in these special cases, such relationships are just noise).\nspanishText_1 = re.sub('^\\s*\\w+\\s*\\n', '', spanishText_1, flags=re.MULTILINE)\n# remove all numbers and\/or dates enclosed in parenthesis\n# i.e. (672-680), (+737) (1980-)\nspanishText_1 = re.sub('\\((\\s*|\\+*|\\w\\.\\s*)\\d+(\\-*|\\s*|,\\s*)\\d*\\-*\\)', ' ', spanishText_1)\n# remove (if any) set of empty parenthesis\nspanishText_1 = re.sub('\\(\\s*\\)', ' ', spanishText_1)\n\n# Maybe the could be other patterns we could cut off the corpus, but I decided to stop with that for now.\n# checkint out how the text looks like\nspanishText_1[:1000]","ba7df0fc":"spanishText_1 = re.sub(',', ' <COMMA> ', spanishText_1)\nspanishText_1 = re.sub('\\.', ' <PERIOD> ', spanishText_1)\nspanishText_1 = re.sub(';', ' <SEMICOLON> ', spanishText_1)\nspanishText_1 = re.sub(':', ' <CCOLONN> ', spanishText_1)\nspanishText_1 = re.sub('\\?', ' <QUESTIONMARK> ', spanishText_1)\nspanishText_1 = re.sub('!', ' <EXCLAMATIONMARK> ', spanishText_1)\nspanishText_1 = re.sub('\\(', ' <LEFT_PARENTHESIS> ', spanishText_1)\nspanishText_1 = re.sub('\\)', ' <RIGHT_PARENTHESIS> ', spanishText_1)\nspanishText_1 = re.sub('\/', ' <SLASH> ', spanishText_1)\nspanishText_1 = re.sub('\\+', ' <PLUS_SIGN> ', spanishText_1)\nspanishText_1 = re.sub('\\-', ' <DASH> ', spanishText_1)\nspanishText_1 = re.sub('\\s\\d+\\s', ' <NUMBER> ', spanishText_1)\nspanishText_1[:1000]","89614bab":"def getListWordsPreprocessed(corpus):\n    ''' return a corpus after removing all the patterns and the xml markup (if any) '''\n    text = corpus.lower()\n    text = re.sub('<.*>', '', text)\n    text = re.sub('ENDOFARTICLE.', '', text)\n    punctuation2remove = \"[\" + re.sub('[,.;:?!()+\/-]', '', string.punctuation) + \"]\"\n    text = re.sub(punctuation2remove, '', text)\n    text = re.sub('\\n\\n+', '\\n', text)\n    text = re.sub(';+\\n', '\\n', text)\n    text = re.sub('\\s*-\\s', ' ', text)\n    text = re.sub('\\s+\\.', ' ', text)\n    text = re.sub('^\\n', '', text, flags=re.MULTILINE)\n    text = re.sub('^\\s*\\w+\\s*\\n', '', text, flags=re.MULTILINE)\n    text = re.sub('\\((\\s*|\\+*|\\w\\.\\s*)\\d+(\\-*|\\s*|,\\s*)\\d*\\-*\\)', ' ', text)\n    text = re.sub('\\(\\s*\\)', ' ', text)\n    text = text.replace(',', ' <COMMA> ')\n    text = text.replace('.', ' <PERIOD> ')\n    text = text.replace(';', ' <SEMICOLON> ')\n    text = text.replace(':', ' <CCOLONN> ')\n    text = text.replace('?', ' <QUESTIONMARK> ')\n    text = text.replace('!', ' <EXCLAMATIONMARK> ')\n    text = text.replace('(', ' <LEFT_PARENTHESIS> ')\n    text = text.replace(')', ' <RIGHT_PARENTHESIS> ')\n    text = text.replace('\/', ' <SLASH> ')\n    text = text.replace('+', ' <PLUS_SIGN> ')\n    text = text.replace('-', ' <DASH> ')\n    text = re.sub('\\s\\d+\\s', ' <NUMBER> ', text)\n    words = text.split()\n    #remove all words with 5 or fewer occurences\n    word_cnts = Counter(words)\n    trimmed_words = [word for word in words if word_cnts[word] > 5]\n    return trimmed_words","b4b6d26b":"for dirname, _, filenames in os.walk('..\/input\/spanish_corpus\/'):\n    for filename in filenames:\n        with open('corpus.txt', 'a', encoding='latin-1') as ffile:\n            with open(os.path.join(dirname, filename), 'r', encoding='latin-1') as rfile:\n                ffile.write(rfile.read())","fcdcd776":"# extract the text of corpus.txt into a variable\nwith open('.\/corpus.txt', 'r', encoding='latin-1') as f:\n    text = f.read()\n# get a list of words (preprocessed text)\nwords = getListWordsPreprocessed(text)\nprint(words[:50])","ac750f80":"print(\"Total amount of words: {}\".format(len(words)))\nprint(\"Amount of unique words: {}\".format(len(set(words))))","e2f963f0":"# creating a counter of words ...\nvocabulary_counts = Counter(words)\n# let's see the 10 most common words\nprint(\"10 most commmon words:\")\nprint(vocabulary_counts.most_common(10))\n\n# sorting the words in order of frequency (from most to least frequent)\nvocabulary_sorted = sorted(vocabulary_counts, key=vocabulary_counts.get, reverse=True)\n\n# creating the lookup tables\nint_to_vocab = {ii: word for ii, word in enumerate(vocabulary_sorted)}\nvocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n\n# create a vocabulary of ints (i..e map the complete vocabulary to its int values)\nint_vocabulary = [vocab_to_int[word] for word in words]\nprint(\"First 50 int-words of the int vocabulary:\")\nprint(int_vocabulary[:50])","c7c50e40":"# threshold\nt = 1e-5\nint_vocab_counter = Counter(int_vocabulary)\ntotal_amount_words = len(int_vocabulary)\nfreqs = {word: count\/total_amount_words for word, count in int_vocab_counter.items()}\nprob_to_drop = {word: 1 - np.sqrt(t\/freqs[word]) for word in int_vocab_counter}\n# new list of words for training\n# some words will be dropped according to the subsampling equeation\ntrain_words = [word for word in int_vocabulary if random.random() < (1 - prob_to_drop[word])]","2d281d5f":"def get_context(list_w, idx, window_size=5):\n    ''' create and return a window of 'window_size' words \n        that surrounds the word in index 'idx' \n    '''\n    r = random.randint(1, window_size)\n    try:\n        context_list = [list_w[index] for index in range(idx-r, idx+r+1) if index is not idx and index >= 0]\n    except IndexError:\n        context_list = [list_w[index] for index in range(idx-r, len(list_w)) if index is not idx and index >=0]\n    return context_list","e3d3aae9":"def get_batches(words, batch_size, window_size=5):\n    ''' Create batches of words contexts\n        The batches are tupples (inputs, targets) '''\n    num_batches = len(words) \/\/ batch_size\n    # we will work with a size of vocabulary that allows for\n    # full batches only\n    words = words[:batch_size*num_batches]\n    for index in range(0, len(words), batch_size):\n        x, y = [], []\n        batch = words[index:index+batch_size]\n        for indx in range(len(batch)):\n            targets = get_context(batch, indx, window_size)\n            x.extend([batch[indx]]*len(targets))\n            y.extend(targets)\n        yield x,y","16726edf":"class SkipGramNeg(nn.Module):\n    def __init__(self, vocab_size, embedded_dim, noise_dist=None):\n        super().__init__()\n        self.n_vocab = vocab_size\n        self.n_embed = embedded_dim\n        self.noise_dist = noise_dist\n        # define embeddings for input and output vocabulary\n        self.embedded_input = nn.Embedding(vocab_size, embedded_dim)\n        self.embedded_output = nn.Embedding(vocab_size, embedded_dim)\n        # Initialize both embedding tables with uniform distribution\n        self.embedded_input.weight.data.uniform_(-1,1)\n        self.embedded_output.weight.data.uniform_(-1,1)\n        \n    def forward_input(self, input_words):\n        # return input vector embeddings\n        return self.embedded_input(input_words)\n    \n    def forward_output(self, output_words):\n        # return output vector embeddings\n        return self.embedded_output(output_words)\n    \n    def forward_noise(self, batch_size, n_samples):\n        \"\"\" Generate noise vectors with shape (batch_size, n_samples, n_embed)\"\"\"\n        if self.noise_dist is None:\n            # Sample words uniformly\n            noise_dist = torch.ones(self.n_vocab)\n        else:\n            noise_dist = self.noise_dist\n            \n        # Sample words from our noise distribution\n        noise_words = torch.multinomial(noise_dist,\n                                        batch_size * n_samples,\n                                        replacement=True)\n        \n        device = \"cuda\" if model.embedded_output.weight.is_cuda else \"cpu\"\n        noise_words = noise_words.to(device)\n        # reshape the embeddings so that they have dims (batch_size, n_samples, n_embed)\n        noise_embeddings = self.forward_output(noise_words)\n\n        \n        return noise_embeddings.view(batch_size, n_samples, self.n_embed)","6700c0c0":"class NegativeSamplingLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, input_vectors, output_vectors, noise_vectors):\n        \n        batch_size, embed_size = input_vectors.shape\n        # Input vectors should be a batch of column vectors\n        input_vectors = input_vectors.view(batch_size, embed_size, 1)\n        # Output vectors should be a batch of row vectors\n        output_vectors = output_vectors.view(batch_size, 1, embed_size)\n        \n        # bmm = batch matrix multiplication\n        # correct log-sigmoid loss\n        out_loss = torch.bmm(output_vectors, input_vectors).sigmoid().log()\n        out_loss = out_loss.squeeze()\n        \n        # incorrect log-sigmoid loss\n        noise_loss = torch.bmm(noise_vectors.neg(), input_vectors).sigmoid().log()\n        noise_loss = noise_loss.squeeze().sum(1)  # sum the losses over the sample of noise vectors\n\n        # negate and sum correct and noisy log-sigmoid losses\n        # return average batch loss\n        return -(out_loss + noise_loss).mean()","9e7a0771":"def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n    ''' embedding should be a pytorch embedding module '''\n    embed_vectors = embedding.weight\n    # magnitude of embedding vectors, |b|\n    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n    # pick N words from ranges (0, window) and (1000, 1000+window).\n    # lower id implies more frequent\n    valid_exmpl = np.array(random.sample(range(valid_window), valid_size\/\/2))\n    valid_exmpl = np.append(valid_exmpl, random.sample(range(1000, 1000+valid_window), valid_size\/\/2))\n    valid_exmpl = torch.LongTensor(valid_exmpl).to(device)\n\n    valid_vectors = embedding(valid_exmpl)\n    similarities = torch.mm(valid_vectors,embed_vectors.t())\/magnitudes\n    return valid_exmpl, similarities","a788d8c1":"# before continuing with the training, all those not used and\/or auxiliary variables must be removed from RAM\ndel int_vocab_counter\ndel total_amount_words\ndel prob_to_drop\ndel spanishText_1\ndel text\ndel words\ndel vocabulary_counts\ndel vocabulary_sorted\ndel int_vocabulary","58b5b0ea":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# getting a noise distribution using the frequency of words\n# in the vocabulary. Frequencies were calculated earlier.\nword_freqs = np.array(sorted(freqs.values(), reverse=True))\nunigram_distribution = word_freqs\/word_freqs.sum()\nnoise_distribution = torch.from_numpy(unigram_distribution**(0.75)\/np.sum(unigram_distribution**(0.75)))\n\nembedding_dim = 300\nmodel = SkipGramNeg(len(vocab_to_int), embedding_dim, noise_dist=noise_distribution).to(device)\n\ncriterion = NegativeSamplingLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\nepochs = 8\nsteps = 0\nprint_every = 20000\n\nfor e in range(epochs):\n    # if epoch > 5 and learning rate of optimizer is bigger than 0.002,\n    # then set the learning rate to 0.001\n    if e > 5 and optimizer.defaults.get('lr') > 0.002:\n        optimizer.defaults['lr'] = 0.001\n        print(\"Learning rate of optimizer adjusted to {}\".format(optimizer.defaults.get('lr')))\n    # get batches of inputs and targets words\n    for input_words, target_words in get_batches(train_words, batch_size=512, window_size=5):\n        steps += 1\n        inputs, targets = torch.LongTensor(input_words), torch.LongTensor(target_words)\n        inputs, targets = inputs.to(device), targets.to(device)\n        # input vector, output vector and noise vector\n        in_vector  = model.forward_input(inputs)\n        out_vector = model.forward_output(targets)\n        noise_vector = model.forward_noise(inputs.shape[0], 7)\n        # negative sampling loss\n        loss = criterion(in_vector, out_vector, noise_vector)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # loss stats\n        if steps % print_every == 0:\n            print(\"Epoch: {}\/{}\".format(e+1, epochs))\n            print(\"Loss: \", loss.item())\n            valid_examples, valid_similarities = cosine_similarity(model.embedded_input, valid_size=24, valid_window=1000, device=device)\n            _, closest_idx = valid_similarities.topk(6)\n\n            valid_examples, closest_idx = valid_examples.to('cpu'), closest_idx.to('cpu')\n            for i, valid_idx in enumerate(valid_examples):\n                closest_words = [int_to_vocab[idx.item()] for idx in closest_idx[i]][1:]\n                print(int_to_vocab[valid_idx.item()] + \"|\" + ','.join(closest_words))\n            print(\"...\\n\")","5842169a":"# save the embeddings to a compressed binary format\nembeddings = model.embedded_input.weight.to('cpu').data.numpy()\nsavez_compressed('spanish_embeddings.npz', embeddings)\nwith open('vocab_to_int.pickle', 'wb') as f_handle:\n    pickle.dump(vocab_to_int, f_handle, protocol=pickle.HIGHEST_PROTOCOL)\nwith open('int_to_vocab.pickle', 'wb') as f_handle:\n    pickle.dump(int_to_vocab, f_handle, protocol=pickle.HIGHEST_PROTOCOL)","3d485220":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# getting embeddings from the embedding layer of our model, by name\nembeddings = model.embedded_input.weight.to('cpu').data.numpy()\nviz_words = 400\noffset = randint(0, viz_words)\ntsne = TSNE()\nembed_tsne = tsne.fit_transform(embeddings[offset:viz_words+offset, :])\nfig, ax = plt.subplots(figsize=(20, 20))\nfor idx in range(viz_words):\n    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)","095eafdb":"ax = sns.heatmap(embeddings.T, cmap=\"Blues\")","f693b122":"ax = sns.heatmap(embeddings[:100,:].T, cmap=\"Blues\")","30132f3a":"ax = sns.heatmap(embeddings[50:100,:].T, cmap=\"Blues\")","89878960":"ax = sns.heatmap(embeddings[75:100,:].T, cmap=\"Blues\")","f308e6bc":"Start the prepocessing now .....","e85883e2":"### Make lookup tables\nNow, lookup tables need to be created. These lookup tables are meant to map words to integers and viceversa.\nEach word is assigned an integer. The integer numbers are assigned in descending frequency (i.e. most frequent workd = 0, next most frequent word = 1, and so on).\nWe need this look up tables because we need a list of integers, instead of a list of words.","4006b7ca":"## Validation\n\nTo validate that the embeddings development is learning the semantics of the words, we can use the _similarity_ function.<br>\nThe _similarity_ function is defined as:\n\n$$ \\mathrm{similarity} = cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|} $$\n\nWhen the angle between two vectors is small, the cosine of such angle tends to equals $1$. Under these conditions, the vectors are pointing basically to the same direction. In the case of embeddings, the direction its vectors point to, is defined by the semantics learned (and represented) by each vector (remember that a vector will be a feature representation of a specific word).<br>\nSo, by measuring the cosine between two vectors of our embeddings, we can assess whether the semantics the two vectors are close from each other. So for instance, if we would measure the cosine between two vectors which represent synonim words, we'd get values really really close to $1$ (if we do not get $1$).\nOf course, you need to have a descent knowledge of spanish vocabulary to really see the embeddings are learning some semantics ;)","f72ac891":"Now, all the data files must be concatenated to process all of them and work with the data all at once.","0d49849e":"### Subsampling\nHigh freqency words don't provide too much context to the nearby words - in fact they might just increase noise that will undermine the algorithm's training process. By removing those high frequency words, the training process will be more efficient and the training time will decrease. In order to remove such high frequency words, Kikolov's subsampling process will be implemented. This subsampling process removes a word based on the following probability equacion:\n$$ P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}} $$\n\nwhere $t$ is a threshold parameter and $f(w_i)$ is the frequency of word $w_i$ in the total dataset.","56e4a3ed":"### Saving embeddings & lookup tables","abf9a655":"## Visualizing the embedding vectors\n","3c0f0d34":"## Pre-processing\n### Clean up the corpus\nOn each one of the files, there are xml markups and maybe punctuation that needs to be gotten rid of.\nIn the following cells I will remove some of those unwanted patterns in the texts.","b7af3d0b":"The patter search implemented in the previous cells seems to work well.\nSo a function that seraches for those patterns and removes them is going to be needed to preprocess other files","6b6c859b":"## Acknoledgments\n\n* Udacity deep learning course. This notebook was particularly inspired by [this notebook](https:\/\/github.com\/armhzjz\/deep-learning-v2-pytorch\/blob\/master\/word2vec-embeddings\/Negative_Sampling_Exercise.ipynb).\n* [Notebook and dataset](https:\/\/www.kaggle.com\/rtatman\/120-million-word-spanish-corpus) from Rachael Tatman","a30e2f19":"So far it looks good.\nI still need to replace the remaining punctuations with unique tockens as follows:\n\n + ,    <COMMA\\><br>\n + .    <PERIOD\\><br>\n + ;    <SEMICOLON\\><br>\n + :    <CCOLONN\\><br>\n + ?    <QUESTIONMARK\\><br>\n + !    <EXCLAMATIONMARK\\><br>\n + (    <LEFT_PARENTHESIS\\><br>\n + )    <RIGHT_PARENTHESIS\\><br>\n + \/    <SLASH\\><br>\n + \\+    <PLUS_SIGN\\><br>\n + \\-    <DASH\\><br>\n + any number   <NUMBER\\><br>","3f559035":"## Training","85e8a3a1":"### Preparing batches\nNow we need to prepare some batches to handle the data to the algorithm so that it can train on it.\nTo crate the batches, I will use the skip-gram architecture, which consist on define a context for each word. The context of a word is defined by its $N$ surrounding words. So we take the context words and put them on a _window_. Note that the sorrounded word (or target word) will not be part of the _window_.\n\nFrom [Mikolov et al.](https:\/\/arxiv.org\/pdf\/1301.3781.pdf): \n\n\"Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples... If we choose $C = 5$, for each training word we will select randomly a number $R$ in range $[ 1: C ]$, and then use $R$ words from history and $R$ words from the future of the current word as correct labels.\"","2723017b":"## SkipGram model & Negative Sampling\n### SkipGram model\n\nThe SkipGram model is basically a network consisting of an embedding layer followed by a linear layer and a softmax layer. The targets this model is trained against, is the word missing in the context batches. In other words, we want to predict the target word based on the provided context.<br>\n<img src=\"https:\/\/raw.githubusercontent.com\/armhzjz\/deep-learning-v2-pytorch\/caee035b28355119b365235156c9e6c9658565c1\/word2vec-embeddings\/assets\/word2vec_architectures.png\" width=50%>\n<br>We pass in a word and try to predict the words surrounding it in the text. In this way, we can train the network to learn representations for words that show up in similar contexts.\n\nDuring the training of this network, the embedding layer will begin to construct semantic representations of each word on the vocabulary by \"learning\" or \"understand\" the relationship between them (between the words). This learning occurs when adjusting the network's weights after each back propagation of the error and its corresponding optimizer step.\n\nUnfortunately, this proces is inefficient - or fortunately, there is a more effective way\n\n### Negative Sampling\n\nAt every step of the training process, the network uses the output of the softmax layer to update the network's weights; at every step, there are very small changes to millions of weights even though there is only one true example. This is what makes the network inefficient. In order to overcome this, the loss of the softmax can be approximated by updating a small subsample of all the network's weights instead of updating all its weights. This is, the update will be done for the weights that correspond to the corrent target plus a small number of weights of incorrect tagets (i.e. noise). This is called [negative sampling](http:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf).\n\nFrom one of the notebooks of Deep Learning Udacity Nanodegrees:\n\n>>\nSince we're not taking the softmax output over all the words, we're really only concerned with one output word at a time. Similar to how we use an embedding table to map the input word to the hidden layer, we can now use another embedding table to map the hidden layer to the output word. Now we have two embedding layers, one for input words and one for output words. Secondly, we use a modified loss function where we only care about the true example and a small subset of noise examples.\n\n$$\n- \\large \\log{\\sigma\\left(u_{w_O}\\hspace{0.001em}^\\top v_{w_I}\\right)} -\n\\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}\\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)}\n$$\n\n>>\nThis is a little complicated so I'll go through it bit by bit. $u_{w_O}\\hspace{0.001em}^\\top$ is the embedding vector for our \"output\" target word (transposed, that's the $^\\top$ symbol) and $v_{w_I}$ is the embedding vector for the \"input\" word. Then the first term \n\n$$\\large \\log{\\sigma\\left(u_{w_O}\\hspace{0.001em}^\\top v_{w_I}\\right)}$$\n\n>>\nsays we take the log-sigmoid of the inner product of the output word vector and the input word vector. Now the second term, let's first look at \n\n$$\\large \\sum_i^N \\mathbb{E}_{w_i \\sim P_n(w)}$$ \n\n>>\nThis means we're going to take a sum over words $w_i$ drawn from a noise distribution $w_i \\sim P_n(w)$. The noise distribution is basically our vocabulary of words that aren't in the context of our input word. In effect, we can randomly sample words from our vocabulary to get these words. $P_n(w)$ is an arbitrary probability distribution though, which means we get to decide how to weight the words that we're sampling. This could be a uniform distribution, where we sample all words with equal probability. Or it could be according to the frequency that each word shows up in our text corpus, the unigram distribution $U(w)$. The authors found the best distribution to be $U(w)^{3\/4}$, empirically. \n\n>>\nFinally, in \n\n$$\\large \\log{\\sigma\\left(-u_{w_i}\\hspace{0.001em}^\\top v_{w_I}\\right)},$$ \n\n>>\nwe take the log-sigmoid of the negated inner product of a noise vector with the input vector.\n\n>>\n<img src=\"https:\/\/raw.githubusercontent.com\/armhzjz\/deep-learning-v2-pytorch\/caee035b28355119b365235156c9e6c9658565c1\/word2vec-embeddings\/assets\/neg_sampling_loss.png\" width=50%>\n\n>>\nTo give you an intuition for what we're doing here, remember that the sigmoid function returns a probability between 0 and 1. The first term in the loss pushes the probability that our network will predict the correct word $w_O$ towards 1. In the second term, since we are negating the sigmoid input, we're pushing the probabilities of the noise words towards 0."}}