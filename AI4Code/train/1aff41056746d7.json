{"cell_type":{"5ec16b65":"code","043ab78b":"code","c4042311":"code","de14a209":"code","3ddb2819":"code","7f442179":"code","d0a3c591":"code","7f7761d7":"code","44a3c10e":"code","9028332b":"code","ae32b40f":"code","7f0a240f":"code","f6c5de28":"code","2db5a26d":"code","e00aaa04":"code","7343a91b":"code","67bf5170":"code","bbf45b9f":"code","ba0acf7c":"code","ceffc012":"code","e3056784":"markdown","974903d9":"markdown","f53b85fd":"markdown","eff6ea2d":"markdown","db44e439":"markdown"},"source":{"5ec16b65":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier, plot_importance\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn import model_selection\nfrom sklearn.metrics import  classification_report, accuracy_score, log_loss, roc_auc_score\nfrom mlxtend.classifier import StackingCVClassifier\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold, mutual_info_classif, SelectKBest\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier","043ab78b":"df_train = pd.read_csv(\"..\/input\/mlchallenge-A21\/train_final.csv\")\ndf_test = pd.read_csv(\"..\/input\/mlchallenge-A21\/test_final.csv\")","c4042311":"df = pd.concat([df_train,df_test],axis=0,sort=False)\ndf.drop(['target'], axis=1, inplace=True)","de14a209":"df['nan_count'] = df.isnull().sum(axis=1)","3ddb2819":"useful_features = [c for c in df.columns if c not in (\"index\")]\nobject_cols = [col for col in useful_features if 'c' in col]\nnum_cols = [col for col in useful_features if 'n' in col]","7f442179":"for cols in object_cols:\n    df[cols] = df[cols].fillna('NONE')\nfor cols in num_cols:\n    df[cols] = df[cols].fillna(df[cols].mode().iloc[0])","d0a3c591":"# We assume categorical columns follows order, hence ordinal encoding\nfor cols in object_cols:\n    Oe = OrdinalEncoder()\n    Oe.fit(df[[cols]])\n    df[[cols]] = Oe.transform(df[[cols]])","7f7761d7":"#Dropping constant column\ndf.drop(['n7'], axis=1, inplace=True)","44a3c10e":"#New Features\ndf['std'] = df.std(axis=1)\ndf['var'] = df.var(axis=1)\ndf[\"mean\"] = df.mean(axis=1)\ndf[\"min\"] = df.min(axis=1)\ndf[\"max\"] = df.max(axis=1)","9028332b":"columns = list(df.columns)\nfor item in df.columns:\n    if item == 'index':\n        pass\n    else:\n        df_mean = df[item].mean()\n        df_std = df[item].std()\n        df[item] = (df[item] - df_mean) \/ df_std","ae32b40f":"train = df[:80000]\ntest = df[80000:]","7f0a240f":"train['target'] = df_train['target']\ntrain.head()","f6c5de28":"target = ['target']\nnot_features = ['index','kfold', 'target']\ncols = list(train.columns)\nfeatures = [feat for feat in cols if feat not in not_features]","2db5a26d":"train[\"kfold\"] = -1","e00aaa04":"train_targets = train[target]","7343a91b":"#Using Stratified kfolds as our targets class were unbalanced\n# initialize stratified k-fold\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=train, y=train_targets)):\n    train.loc[valid_indicies, \"kfold\"] = fold","67bf5170":"useful_features = [c for c in df.columns if c not in (\"index\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'c' in col]\ndf_test = test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  train[train.kfold != fold].reset_index(drop=True)\n    xvalid = train[train.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid['index'].values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    params = {   'learning_rate': 0.07483794809004472,\n 'lambda_l1': 8.593693997380945e-05,\n 'lambda_l2': 3.248981989796244e-08,\n 'num_leaves': 65,\n 'feature_fraction': 0.42420534006521865,\n 'bagging_fraction': 0.829301270168472,\n 'bagging_freq': 2,\n 'min_child_samples': 88\n             }\n    \n    model = LGBMClassifier(**params, early_stopping_rounds=2000,random_state=42, nestimators=7000)\n    model.fit(xtrain, ytrain, eval_set = [(xvalid, yvalid)],verbose=False)\n    preds_valid = model.predict_proba(xvalid)[:, 1]\n    test_preds = model.predict_proba(xtest)[:,1]\n    final_test_predictions.append(test_preds)\n    roc = roc_auc_score(yvalid, preds_valid)\n    print(fold, roc)\n    scores.append(roc)\n\nprint(np.mean(scores), np.std(scores))","bbf45b9f":"index_column = test.iloc[:, 0]","ba0acf7c":"submission = pd.DataFrame()\nsubmission['index'] = index_column\nsubmission['target'] = model.predict_proba(df_test)[:,1]\nsubmission.to_csv('submission_LGBM.csv', index=False)","ceffc012":"submission.head()","e3056784":"# *Pre-processing and Feature Engineering*","974903d9":"# *Modeling*","f53b85fd":"# *LGBM CLASSIFIER* ","eff6ea2d":"Used optuna for hyperparameter tuning and thus got the following parameters, which i am going to use","db44e439":"# Submission"}}