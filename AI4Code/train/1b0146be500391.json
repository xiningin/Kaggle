{"cell_type":{"ba636c8a":"code","ebadf431":"code","d1807ac1":"code","30610591":"code","6d763703":"code","62888820":"code","79161770":"code","274d3156":"code","de583680":"code","d5e34987":"code","f1ddd37c":"code","e6e1c3e6":"code","d2f24a27":"code","b115afd8":"code","436dea8c":"code","f43ac5c3":"code","3a9bb20a":"code","f8a2d17d":"code","d9f4c839":"code","e0cf0a51":"code","f2759959":"code","3319f1f1":"code","3c74a89f":"code","4bfb56e5":"code","505a9824":"code","05fbd81b":"code","f712b120":"code","b14bfc62":"code","d72f1474":"code","7dda5b4c":"code","18653cd7":"code","a393aa42":"code","c30dc99d":"code","09722a5a":"code","0e6e696e":"code","ed80676c":"code","9e4ef726":"code","f94f122c":"code","48ebea2f":"code","9527750a":"code","36d6c689":"code","2a636f24":"code","2aaf4600":"markdown","1a31a964":"markdown","b0cf1a17":"markdown","cd11e649":"markdown","51af9cf9":"markdown","98f3cd36":"markdown","5dde4684":"markdown","0ef0f0cd":"markdown","46467314":"markdown","4e40081f":"markdown","e7fab827":"markdown","fe22f02c":"markdown"},"source":{"ba636c8a":"# Regular imports\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport lightgbm as lhgbm\n\n#For Visualizations\nimport pandas_profiling as pp\nimport matplotlib as mpl\nimport seaborn as sns\n\n#Ensemble Models\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\nplt.style.use('classic')\npd.set_option('max_columns', None)\n\nfrom sklearn.linear_model import *\n\n\n#To Split Data \nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, cross_val_score, KFold\n\n#To Preprocess Data \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OrdinalEncoder, FunctionTransformer, OneHotEncoder, Normalizer\nfrom sklearn.impute import SimpleImputer\nfrom imblearn import FunctionSampler\n\n\n#To Pipeline the process \nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n#from lineartree import  LinearTreeRegressor, LinearBoostRegressor\nfrom sklearn.pipeline import Pipeline\n\n# used in Utilities\/Functions section\nfrom scipy import stats\n\n## Import Models\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n\n# Import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, r2_score\nfrom xgboost import plot_importance\n\n\n# variables\ntarget_feature = 'target' \nrandom_seed    = 42\nmax_rows_per_class  = 2000 ## use high number (300000) to get all data  or full_run ='Y'\nfull_run ='Y'  ","ebadf431":"#Python libraries and their versions used for this problem\nprint('SciKit Learn:',sk.__version__)\nprint('Pandas:',pd.__version__)\nprint('Numpy:',np.__version__)\nprint('Seaborn:',sns.__version__)\nprint('MatPlot Library:', mpl.__version__)\nprint('XG Boost:',xgb.__version__)\nprint('Pandas Profiling:', pp.__version__)\nprint('LightGBM:', lhgbm.__version__)\n","d1807ac1":"#Define Utilities\/Functions\n\n# Function to show essential info about Dataset\ndef ShowEssentialInfo(df):\n    # Check No of rows & columns\n    print(\"\\n\",'*** Shape:',df.shape)\n\n    # Check Info about Object types of data\n    print(\"\\n\",'*** Info:')\n    df.info()\n\n    #Count missing values \n    print(\"\\n\",'*** Missing values:')\n    print(df.isnull().sum())\n\n    #Data Statistics\n    print(\"\\n\",'*** Data Statistics:')\n    print(df.describe(include='all'))\n    \n    #Data Skew\n    #print(\"\\n\",'*** Skewness:')\n    #print('#** skewness is a degree of asymmetry observed in a probability distribution that deviates from the symmetrical normal distribution (bell curve) in a given set of data')\n    #print('#** If the skewness is between -0.5 & 0.5, the data are nearly symmetrical.')\n    #print('#** If the skewness is between -1 & -0.5 (negative skewed) or between 0.5 & 1(positive skewed), the data are slightly skewed.')\n    #print('#** If the skewness is lower than -1 (negative skewed) or greater than 1 (positive skewed), the data are extremely skewed.')\n    #print(df.skew(axis=1))\n \n    #Data Kurtosis\n    #print(\"\\n\",'*** Kurtosis:')\n    #print('#** Kurtosis quantifies shape of the distribution and the degree of presence of outliers in the distribution.')\n    #print('# High kurtosis in a data set is an indicator that data has heavy outliers.')\n    #print('# Low kurtosis in a data set is an indicator that data has lack of outliers.')\n    #print('# If kurtosis value + means pointy and \u2014 means flat.')    \n    #print(df.kurt(axis=1))\n \n    \n\ndef treatoutliers(df=None, columns=None, factor=1.5, method='IQR', treatment='cap'):\n\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n#         print(floor, ceil)\n        if treatment == 'remove':\n            print(treatment, column)\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            print(treatment, column)\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n    \ndef get_sample_dataset(df, categorical_features, max_rows_per_class=1000):\n    rows = []\n    df_sub = pd.DataFrame()\n    for x in categorical_features:\n        for idx,name in enumerate(df[x].value_counts().index.tolist()):\n            nrows = df[x].value_counts()[idx]\n            nsample = min(nrows, max_rows_per_class)\n            data = df.loc[df[x] == name].sample(n=nsample, random_state=random_seed)\n            #print(data.info())\n            df_sub = df_sub.append(data)\n    return df_sub\n\n\ndef log_transform(x):\n    return np.log(x + 1)\n\ndef exp_transform(x):\n    return np.exp(x)\n","30610591":"# Load the training data\nX_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\nX_all = pd.concat([X_full,X_test_full]) \n","6d763703":"## Check for Data types & Missing data\nShowEssentialInfo(X_full)","62888820":"ShowEssentialInfo(X_test_full)","79161770":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].nunique() <= 15 and \n                    X_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\n#\nprint('categorical_features:', categorical_features)\nprint('numeric_features:', numeric_features)\nprint('my_features:', my_features)\n\n#remove target column from Numeric features\nnumeric_features.remove(target_feature)\nprint('numeric_features minus target column:', numeric_features)\n","274d3156":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#print(X_random_subset['cat9'].value_counts(()).index.tolist())\n\n## Feature Selection using variance_inflation_factor\ndef cal_vif(X, thresh=5):\n    output = pd.DataFrame()\n    k =X_vif.shape[1]\n    vif = [variance_inflation_factor(X_vif.values, i) for i in range(k)]\n    for i in range(1,k):\n        print('Iteration No ', i)\n        print(vif)\n        a = int(np.argmax(vif))\n        if(vif[a]<=thresh):\n            print('break')\n            break\n        if(i==1):\n            print('i=1', i)\n            output=X_vif.drop(X_vif.columns[a], axis=1)  \n        elif(i>1):\n            print('i>1', i)\n            output=output.drop(output.columns[a], axis=1)\n        l = len(output.columns)    \n        vif=[variance_inflation_factor(output.values, j) for j in range(l)]\n    return(output)     \n\n\n# creating dummies for categorical_features\n\n# the independent variables set\nX_vif=X_all.copy()\ny_vif=X_vif[target_feature]\nX_vif=X_vif.drop(target_feature,1)\n\n#{'A':0, 'B':1, 'C':2, 'D':3, 'E':5, 'F':6,'G':7,'H':8,'I':9,'J':10,'K':11,'L':12,'M':13,'N':14,'O':15}\nX_vif[categorical_features] = X_vif[categorical_features] = X_vif[categorical_features].applymap(lambda x: ord(x)-65)\n\nselected_features = cal_vif(X_vif, 7)\nselected_features.head()","de583680":"selected_features['target'] = y_vif","d5e34987":"exclude_columns = set(list(X_full.columns)) - set(list(selected_features.columns))\nprint(exclude_columns)","f1ddd37c":"ShowEssentialInfo(selected_features)","e6e1c3e6":"## Correlations\ncorrelations = X_full.corr()\nf, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(correlations, square=True, cbar=True, annot=True, vmax=.9);","d2f24a27":"#Correlation with output variable\ncor_target = abs(correlations[target_feature])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","b115afd8":"## Data Distribution of numeric features \nX_full[numeric_features].hist(figsize=(24,12))","436dea8c":"## Verify distribution with log transform \nX_full[numeric_features].hist(figsize=(24,12), log = True)","f43ac5c3":"# A scatter plot matrix is a grid (or matrix) of scatter plots used to visualize bivariate relationships between combinations of variables. \n# Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart.\n\n#pd.plotting.scatter_matrix(X_full[:1000], figsize=(30, 20), alpha=0.2)\n","3a9bb20a":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns', fontsize=16);","f8a2d17d":"sns.boxplot(data=X_full[['target']], orient=\"h\", palette=\"Set2\" );\nplt.xticks(fontsize= 14)\nplt.title('Box plot of target column', fontsize=16);","d9f4c839":"# Deal with duplicate rows\nX_full[my_features].drop_duplicates()","e0cf0a51":"# Deal with Outliers\n    \n#Quantile-based Flooring and Capping \nfor colName in [['target','cont0','cont6','cont8']]:\n    X_full = treatoutliers(df=X_full,columns=colName, treatment='cap')      \n    \nShowEssentialInfo(X_full)","f2759959":"sns.boxplot(data=X_full[['target']], orient=\"h\", palette=\"Set2\" );\nplt.xticks(fontsize= 14)\nplt.title('Box plot of target column after handling Outliers', fontsize=16);","3319f1f1":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full[numeric_features], orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns after handling Outliers', fontsize=16);","3c74a89f":"# Deal with missing data\n## No Missing data in this dataset :)","4bfb56e5":"\nif full_run == 'N' :\n    X_random_subset = get_sample_dataset(X_full, categorical_features, max_rows_per_class)\nelse:\n    X_random_subset = X_full","505a9824":"ShowEssentialInfo(X_random_subset)","05fbd81b":"# Remove rows with missing target, separate target from predictors\nX_random_subset.dropna(axis=0, subset=[target_feature], inplace=True)\ny = X_random_subset.pop(target_feature)\n\n\n#Prieview features\nShowEssentialInfo(X_random_subset)","f712b120":"# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_random_subset, y, \n                                                                train_size=0.9, test_size=0.1,\n                                                                random_state=0)","b14bfc62":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].dtype in [\"object\"] and\n                     cname not in exclude_columns\n                        ]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64'] and\n                     cname not in exclude_columns\n                 ]\n\n#remove target column from Numeric \/ categorical features\nif numeric_features.count(target_feature) == 1:\n    numeric_features.remove(target_feature)\nelif  categorical_features.count(target_feature) == 1:\n    categorical_features.remove(target_feature)\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)","d72f1474":"X_train = X_train_full[my_features]\nX_valid = X_valid_full[my_features]\nX_test = X_test_full[my_features]","7dda5b4c":"ShowEssentialInfo(X_train)","18653cd7":"ShowEssentialInfo(X_valid)","a393aa42":"ShowEssentialInfo(X_test)","c30dc99d":"# Define the model Parameters, can be optimized using either Optuna or Grid Search CV\n\n#lgbm_params = {'n_estimators' : 5000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'max_bin' : 512, 'random_state' : random_seed}\n#xgb_params = {'n_estimators': 5000, 'max_depth': 3, 'learning_rate': 0.03628302216953097, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.7875490025178415, 'colsample_bytree': 0.11807135201147481, 'reg_alpha': 23.13181079976304, 'reg_lambda': 0.0008746338866473539, 'random_state':random_seed}\n\n\nlgbm_params = {'n_estimators' : 10000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'random_state' : random_seed, 'device':'gpu'}\nxgb_params = {'n_estimators': 10000, 'max_depth': 3, 'learning_rate': 0.036, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.79, 'colsample_bytree': 0.112, 'reg_alpha': 23.132, 'reg_lambda': 0.0009, 'random_state':random_seed, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor'}\n#xgb_params = {'n_estimators': 5000, 'max_depth': 3, 'learning_rate': 0.036, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.79, 'colsample_bytree': 0.112, 'reg_alpha': 23.132, 'reg_lambda': 0.0009, 'random_state':random_seed, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor'}\n\n\nlgbm = LGBMRegressor(**lgbm_params)\nxgb = XGBRegressor(**xgb_params) \n\nrf  = RandomForestRegressor() \n\nnSplits = 10\nkf = KFold(n_splits=nSplits, shuffle=True, random_state=random_seed)\n\nstack_model = StackingCVRegressor(regressors=(xgb,lgbm),\n                            meta_regressor=rf, cv=kf,\n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,\n                            random_state=random_seed)","09722a5a":"%%time\n\ntransformer = FunctionTransformer(exp_transform)\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean'))\n      #,('transformer', transformer)\n       ,('scaler', StandardScaler())\n       ,('RobustScaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True))  \n       #,('scaler', StandardScaler())\n       # ,('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False))\n    #,('scaler', MinMaxScaler())\n      #,('normalizer',  Normalizer())\n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    #('imputer', SimpleImputer(strategy='constant'))\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    #,('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ,('scaler', OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder=\"passthrough\"\n  )\n","0e6e696e":"#pca = PCA(n_components=50)\n#Bundle preprocessing and modeling code in a pipeline\nstack_bundle = Pipeline(steps=[('preprocessor', preprocessor),\n                      #('pca',pca),\n                      ('model', stack_model)\n                     ])\n\nstack_clf = TransformedTargetRegressor(regressor=stack_bundle, transformer=RobustScaler())\n\n#stack_clf = stack_bundle\n","ed80676c":"from sklearn import set_config\nset_config(display='diagram')\nstack_clf\n","9e4ef726":"%%time\nfinal_model = stack_clf.fit(X_train,y_train)\n\npreds_valid = final_model.predict(X_valid)\nrmse = mean_squared_error(y_valid, preds_valid,squared = False)\nprint('RMSE:', rmse)\n","f94f122c":"result_df=pd.DataFrame({'Actual':y_valid, 'Predicted':preds_valid, 'Diff':preds_valid-y_valid})  \nresult_df['Diff'].round().value_counts()","48ebea2f":"## setting plot style\nplt.style.use('fivethirtyeight')\n  \n## plotting residual errors in training data\nplt.scatter(final_model.predict(X_train), final_model.predict(X_train) - y_train,\n            color = \"green\", s = 10, label = 'Train data')\n  \n## plotting residual errors in Validation data\nplt.scatter(preds_valid, preds_valid-y_valid,\n            color = \"blue\", s = 10, label = 'Validation data')\n  \n## plotting line for zero residual error\nplt.hlines(y = 0, xmin = 0, xmax = 50, linewidth = 2)\n  \n## plotting legend\nplt.legend(loc = 'upper right')\n  \n## plot title\nplt.title(\"Residual errors\")\n  \n## method call for showing the plot\nplt.show()","9527750a":"final_model.get_params","36d6c689":"#Compare results\nplt.plot(y_valid.values, label='Actual')\nplt.plot(preds_valid, label='Predicted')\nplt.ylabel('Target')\n\nplt.legend()\nplt.show()","2a636f24":"# Use the model to generate predictions\npredictions = final_model.predict(X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","2aaf4600":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nLets fit an appropriate model to the data.","1a31a964":"The above distribution looks good after log transformation","b0cf1a17":"Looks like few outliers in Cont0, Cont6, Cont8, target columns.\nLets check the ouliers in  target column now.","cd11e649":"\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","51af9cf9":"No Duplicate Rows in the data set...","98f3cd36":"# Step 1: Import helpful libraries","5dde4684":"All features are weakly correlated to target feature","0ef0f0cd":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.","46467314":"# Step 3: Prepare the data\n\n","4e40081f":"%%time\n#evaluate model using cross val score\nrmse = -1 * cross_val_score(stack_clf, X_train, y_train,\n                              cv=2,\n                              scoring='neg_root_mean_squared_error')\nprint(rmse.mean())\nprint(rmse)","e7fab827":"No Null values in Data","fe22f02c":"The next code cell separates the target (which we assign to `y`) from the training features."}}