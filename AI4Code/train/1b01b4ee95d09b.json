{"cell_type":{"48aa291e":"code","5f7c4dbe":"code","b022b6fe":"code","6906915b":"code","c2c85bda":"code","af60e447":"code","55af7ab3":"code","8cb74706":"code","c14c93c1":"code","86f72fe1":"code","a4f3f513":"code","5c68cf2a":"code","ec56f4ed":"code","1eb5bfea":"code","48813d39":"markdown","646e2dbc":"markdown","6e935e4b":"markdown","47b143ea":"markdown","2587f692":"markdown","9f019672":"markdown","47f37249":"markdown","6bfb3517":"markdown","4bb87cca":"markdown","4cdf9731":"markdown","d0d8b33b":"markdown","f5247a2d":"markdown","63f8578c":"markdown","c746a939":"markdown","1f8e228c":"markdown","20418c81":"markdown","a9b3710d":"markdown"},"source":{"48aa291e":"#scipy\/plotnine compatibility fix\n!pip install scipy==1.0.0\nimport scipy\nscipy.__version__","5f7c4dbe":"import numpy as np\nimport pandas as pd\n\n#widgets\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n#plots\nimport matplotlib.pyplot as plt\nfrom plotnine import *\n\n#stats\nimport scipy as sp\nimport statsmodels as sm","b022b6fe":"class Arm:\n    def __init__(self, true_p):\n        self.true_p = true_p\n        self.reset()\n    def reset(self):\n        self.impressions = 0\n        self.actions = 0\n    def get_state(self):\n        return self.impressions,self.actions\n    def get_rate(self):\n        return self.actions \/ self.impressions if self.impressions >0 else 0.\n    def pull(self):\n        self.impressions+=1\n        res = 1 if np.random.random() < self.true_p else 0\n        self.actions+=res\n        return res","6906915b":"a = Arm(0.1)\nfor i in range(100): a.pull()\na.get_state()","c2c85bda":"class MusketeerEnv:\n    def __init__(self, true_ps, avg_impressions):\n        self.true_ps = true_ps\n        self.avg_impressions = avg_impressions\n        self.nb_arms = len(true_ps)\n        self.vr_agent = BanditAgent()\n        self.reset()\n    def reset(self):\n        self.t = -1\n        self.ds=[]\n        self.arms = [Arm(p) for p in self.true_ps]\n        return self.get_state()\n    def get_state(self):\n        return [self.arms[i].get_state() for i in range(self.nb_arms)]\n    def get_rates(self):\n        return [self.arms[i].get_rate() for i in range(self.nb_arms)]\n    def get_impressions(self):\n        return int(np.random.triangular(self.avg_impressions\/2,\n                                    self.avg_impressions,\n                                    self.avg_impressions*1.5))\n    def step(self, ps):\n        self.t+=1\n        impressions = self.get_impressions()\n        for i in np.random.choice(a=self.nb_arms,size=impressions,p=ps):\n            self.arms[i].pull()\n        self.record()\n        return self.get_state()\n    #use agent to calculate value remaining\n    def value_remaining(self,n=1000,q=95):\n        state = self.get_state()\n        best_idx = np.argmax(self.vr_agent.thompson_stochastic(state))\n        l=[]\n        for i in range(n): l.append(self.vr_agent.thompson_one(state)[None,:])\n        l = np.concatenate(l,0)\n        l_max = l.max(1)\n        l_best = l[:,best_idx]\n        vs = (l_max - l_best)\/l_best\n        return np.percentile(vs,q)\n    def record(self):\n        d = {'t':self.t,'max_rate':0,'opt_impressions':0}\n        for i in range(self.nb_arms):\n            d[f'impressions_{i}'],d[f'actions_{i}'] = self.arms[i].get_state()\n            d[f'rate_{i}'] = self.arms[i].get_rate()\n            if d[f'rate_{i}'] > d['max_rate']: \n                d['max_rate'] = d[f'rate_{i}']\n                d['opt_impressions'] = d[f'impressions_{i}']\n        d['total_impressions'] = sum([self.arms[i].impressions for i in range(self.nb_arms)])\n        d['opt_impressions_rate'] = d['opt_impressions'] \/ d['total_impressions']\n        d['total_actions'] = sum([self.arms[i].actions for i in range(self.nb_arms)])\n        d['total_rate'] = d['total_actions'] \/ d['total_impressions']\n        d['regret_rate'] = d['max_rate'] - d['total_rate']\n        d['regret'] = d['regret_rate'] * d['total_impressions']\n        d['value_remaining'] = self.value_remaining()\n        self.ds.append(d)\n    def show_df(self):\n        df = pd.DataFrame(self.ds)\n        cols = ['t'] + [f'rate_{i}' for i in range(self.nb_arms)]+ \\\n               [f'impressions_{i}' for i in range(self.nb_arms)]+ \\\n               [f'actions_{i}' for i in range(self.nb_arms)]+ \\\n               ['total_impressions','total_actions','total_rate']+ \\\n               ['opt_impressions','opt_impressions_rate']+ \\\n               ['regret_rate','regret','value_remaining']\n        df = df[cols]\n        return df","af60e447":"class BanditAgent:\n    def __init__(self):\n        pass\n    #thompsons\n    def thompson_one(self,state):\n        res = [np.random.beta(i[1]+1,i[0]-i[1]+1) for i in state]\n        res = np.array(res)\n        return res\n    def thompson_stochastic(self,state,n=1000):\n        l = []\n        for i in range(n): l.append(self.thompson_one(state)[None,:])\n        l = np.concatenate(l,0)\n        is_max = l.max(1)[:,None] == l\n        return is_max.mean(0)","55af7ab3":"env = MusketeerEnv(true_ps = [0.1,0.12,0.13], avg_impressions=400)\nfor i in range(1000):\n    env.step([0.6,0.2,0.2])\nenv.get_rates()","8cb74706":"env.show_df().head()","c14c93c1":"class BanditAgent:\n    def __init__(self):\n        pass\n    #baselines\n    def equal_weights(self,state):\n        res = np.array([1\/len(state) for i in range(len(state))])\n        return res\n    def randomize(self,state):\n        res = np.random.rand(len(state))\n        res \/= res.sum()\n        return res\n    \n    #stochastic policies\n    def eps_greedy(self, state, t, start_eps=0.3, end_eps=0.01, gamma=0.99):\n        eps = max(end_eps,start_eps * gamma**t)\n        res = np.array([eps\/len(state) for i in range(len(state))])\n        best_idx = np.argmax([i[1]\/i[0] for i in state]) if t > 0 else np.random.choice(range(len(state)))\n        res[best_idx] += 1-eps\n        return res\n    def softmax(self, state, t, start_tau=1e-1, end_tau=1e-4, gamma=0.9):\n        tau = max(end_tau,start_tau*gamma**t)\n        sum_exp = sum([np.exp(i[1]\/(i[0]+1e6)\/tau) for i in state])\n        res = np.array([np.exp(i[1]\/(i[0]+1e6)\/tau) \/ sum_exp for i in state])\n        return res\n    \n    #deterministic policies\n    def ucb(self, state, t):\n        for i in state:\n            if i[0]==0:\n                return self.equal_weights(state)\n        res = [(i[1]\/i[0] + np.sqrt(2*np.log(t+1)\/i[0])) for i in state]\n        res = np.array(res)\n        res_d = np.zeros(len(state))\n        res_d[np.argmax(res)] = 1\n        return res_d\n    def thompson_deterministic(self, state):\n        res = [np.random.beta(i[1]+1,i[0]-i[1]+1) for i in state]\n        res = np.array(res)\n        res_d = np.zeros(len(state))\n        res_d[np.argmax(res)] = 1\n        return res_d\n    \n    #thompsons\n    def thompson_one(self,state):\n        res = [np.random.beta(i[1]+1,i[0]-i[1]+1) for i in state]\n        res = np.array(res)\n        return res\n    def thompson_stochastic(self,state,n=1000):\n        l = []\n        for i in range(n): l.append(self.thompson_one(state)[None,:])\n        l = np.concatenate(l,0)\n        is_max = l.max(1)[:,None] == l\n        return is_max.mean(0)","86f72fe1":"env = MusketeerEnv(true_ps = [0.12,0.13,0.14], avg_impressions=400)\na = BanditAgent()\nfor i in range(20):\n    p = a.equal_weights(env.get_state())\n    env.step(p)\n    t=i\na.equal_weights(env.get_state()), a.randomize(env.get_state()), a.eps_greedy(env.get_state(),t),\\\na.softmax(env.get_state(),t), a.thompson_stochastic(env.get_state()), \\\na.ucb(env.get_state(),t), a.thompson_deterministic(env.get_state())","a4f3f513":"envs = [MusketeerEnv(true_ps = [0.12,0.13,0.14], avg_impressions=400) for i in range(7)]\na = BanditAgent()\nfor t in range(200):\n    states = [env.get_state() for env in envs]\n    actions = [a.equal_weights(states[0]), a.randomize(states[1]),\n               a.eps_greedy(states[2],t), a.softmax(states[3],t),\n               a.thompson_stochastic(states[4]),\n               a.ucb(states[5],t), a.thompson_deterministic(states[6])]\n    for i in range(7): envs[i].step(actions[i])\ndfs = [env.show_df() for env in envs]\npolicies = ['equal_weights','randomize','eps_greedy','softmax','thompson_stochastic',\n            'ucb','thompson_deterministic']\nfor i in range(7): dfs[i]['policy'] = policies[i]\ndf = pd.concat(dfs)[['policy','t','opt_impressions_rate','regret_rate','regret','value_remaining']]","5c68cf2a":"df.tail()","ec56f4ed":"df_m = df.melt(id_vars=['policy','t'])\ndf_m.tail()","1eb5bfea":"g = (ggplot(df_m, aes(x='t',y='value',color='policy',group='policy')) +\n    geom_line() + theme_minimal() + facet_wrap('~variable',scales='free_y'))\ng","48813d39":"For instance, in a traditional A\/B test with a default variation, new variation `A` and new variation `B`. We may divide 60% traffic to the default variation and 20% each to `A` and `B`. After 1,000 time steps, we will get the following results.","646e2dbc":"# A\/B Testing from Scratch: Multi-armed Bandits","6e935e4b":"## Stopping Criteria: Value Remaining\nStopping criteria for MAB experiments such as ROPE and expected loss as described in [Bayesian A\/B Testing: a step-by-step guide](http:\/\/www.claudiobellei.com\/2017\/11\/02\/bayesian-AB-testing\/); here we are using **value remaining** as introduced by [Google](https:\/\/support.google.com\/analytics\/answer\/2846882?hl=en). Value remaining per round of experiment is defined as:\n\n$$V_t = \\frac{rate_{max}-rate_{opt}}{rate_{opt}}$$\n\nAs experiment goes on, we plot the distribution of $V_t$ and stops when the $1-\\alpha$ percentile is lower than our threshold. Intuitively, this is to say that we are $1-\\alpha$% confident that our \"best\" arm might be beaten by the margin equals to the threshold. For practical purpose, we try 95th percentile and threshold of 0.01.","47b143ea":"## Simulation Results","2587f692":"With our initial agent parameters, we obtain the following results. We can see that stochastic Thompson sampling learns the fastest and is the most stable policy. ","9f019672":"An MAB agent solves the explore-vs-exploit dilemma. Exploitation means we choose what we know as the best choice at the current timestep, sometimes called being *greedy*; on the other hand, exploration means we try pulling other arms in order to know more about the environment. \n\nExploiting 100% of the time is a bad idea. For instance; let us assume there are two arms `A` and `B` with true probabilities 0.1 and 0.9 and it happens that when we pull `A` it returns a conversion whereas when we pull `B` it does not. If our policy is to always exploit, we would end up pulling only `A` which has much lower return rate than `B`. This is when you do not have any experiment set up for your content at all.\n\nIn contrast, if we always explore, we would end up pulling both arms randomly with expected return rates of $0.9 * 0.5 + 0.1 * 0.5 = 0.5$ instead of much higher if we could find out `B` is the better arm. This is close to what happens in a traditional A\/B test during the test period.\n\nSome common policies for distributing impressions to each arm are:\n1. **Equal weights**: all arms have the same amount of traffic or a fixed amount.\n2. **Randomize**: randomly assign traffic to all arms.\n3. **Epsilon-greedy**: Assign a majority of traffic to the \"best\" arm at that time step, and the rest randomized among all arms; the degree of random traffic can be decayed by a parameter `gamma` as time goes on.\n4. **Softmax or Boltzmann exploration**: Assigns traffic equal to the softmax activation of their current return rates; regulated by temperature parameter `tau` (lower `tau` means less exploration) that can also be decayed by `gamma` over time.\n$$P(A_i) = \\frac{e^{rate_i\/\\tau}}{\\sum{e^{rate_i\/\\tau}}}$$\n\n5. **Upper Confidence Bound**: by utilizing Hoeffding\u2019s Inequality, we can have a deterministic policy based on number of times the arms are pulled so far and impressions of each arm:\n\n$$A = argmax(rate_i + \\sqrt{\\frac{2\\log{t}}{impressions_i}})$$\n\n6. **Deterministic Thompson Sampling**: based on a posterior distribution (in our case a Beta distribution) for each arm, sample that number of rates. Choose the arm with the highest sampled rate.\n\n7. **Stochastic Thompson Sampling**: Instead of sampling only once, perform a Monte Carlo simulation for an arbitrary number of times, the traffic to each arm is divided by the percentage of times that arm is the best arm in the simulation.","47f37249":"We treat serving a variation of content, be it product listings, recommended products, search results, online ads, or whatever we want to experiment on as *pulling an arm*. The arm will record an impression and, at an arbitrary amount of delay time, an action such as a click or add-to-cart based on that impression. In our example, we define our arm as a Bernoulli trial with the true probability of conversion ($\\frac{actions}{impressions}$) `true_p`.","6bfb3517":"## Agent","4bb87cca":"In order to evaluate an MAB agent, we use 3 main metrics:\n1. `opt_impressions_rate`: cumulative percentage of impressions we have given to the optimal arm at that timestep; this shows us how often we have picked the \"best\" arm\n2. `regret_rate`: cumulative conversion rate of the best arm at that timestep minus cumulative conversion rate of all impressions; this shows us the difference in conversion rate we have lost by not picking the \"best\" arm\n3. `regret`: cumulative actions if we had chosen the \"best\" arm minus actual cumulative conversions; this shows us how much actions we have lost by not picking the \"best\" arm","4cdf9731":"We simulate 3 campaigns with true probabilities of 12%, 13% and 14% respectively. Our number of overall impressions is 400 on average.","d0d8b33b":"We simulate an environment is an arbitrary number of arms with a set of predefined true probability `true_p` and average number of impressions `avg_impression` per time period `t`. This environment mimics most content serving APIs which display each variation at the ratio `ps` as defined by experimenters.","f5247a2d":"## Arms, Variations, Ads, or Anything","63f8578c":"## References","c746a939":"Frequentist and Bayesian A\/B tests require you to divide your traffic into arbitrary groups for a period of time, then perform statistical tests based on the results. By definition, this forces us to divert out traffic to suboptimal variations during the test period, resulting in lower overall conversion rates. On the other hand, multi-barmed bandit appraoch (MAB) dynamically adjusts the percentage of traffic shown to each variation according to how they have performed so far during the test, resulting in smaller loss in conversion rates.\n\n![Traditional A\/B Test vs Multi-armed Bandits](https:\/\/automizy.com\/wp-content\/uploads\/2017\/03\/Multi-armed-bandit.png)\n\nSource: [Automizy](https:\/\/automizy.com\/blog\/increase-email-course-open-rates-with-machine-learning\/) via [Multi-Arm Bandits: a potential alternative to A\/B tests](https:\/\/medium.com\/brillio-data-science\/multi-arm-bandits-a-potential-alternative-to-a-b-tests-a647d9bf2a7e)","1f8e228c":"## Environment","20418c81":"Here are some useful resources reviewed for this notebook.","a9b3710d":"- [tl;dr Bayesian A\/B test](https:\/\/medium.com\/hockey-stick\/tl-dr-bayesian-a-b-testing-with-python-c495d375db4d)\n- [Bayesian A\/B Testing: a step-by-step guide](http:\/\/www.claudiobellei.com\/2017\/11\/02\/bayesian-AB-testing\/)\n- [Bayesian Coin Flips](https:\/\/www.thomasjpfan.com\/2015\/09\/bayesian-coin-flips\/)\n- [Multi-Arm Bandits: a potential alternative to A\/B tests](https:\/\/medium.com\/brillio-data-science\/multi-arm-bandits-a-potential-alternative-to-a-b-tests-a647d9bf2a7e)\n- [Multi Armed Bandits and Exploration Strategies](https:\/\/sudeepraja.github.io\/Bandits\/)\n- [MAB Google](https:\/\/support.google.com\/analytics\/answer\/2846882?hl=en)"}}