{"cell_type":{"c3f82bdb":"code","b0886f80":"code","8a57b3e4":"code","89aaa2b0":"code","d86835e4":"code","f179498f":"code","e6a4f97d":"code","cf635120":"code","9d6ef01c":"code","36b5f8f7":"code","63a1723f":"code","149df924":"code","45345cf3":"code","4267372c":"code","eb01a2f8":"code","95671af0":"code","8e24b4cd":"code","9c6d97de":"code","9f66cf9d":"code","5e0cdf0b":"code","5773b1de":"code","908c6458":"code","220f1b7a":"code","34e4830e":"code","2c76780c":"code","fa5d07a1":"code","610da22a":"code","8ff92920":"code","c9461d15":"code","65427cd1":"markdown","c2f4c90b":"markdown","dc88df72":"markdown","a0cac071":"markdown","0aecd52d":"markdown","918ee846":"markdown","00526559":"markdown","1ff17403":"markdown","c3091822":"markdown","18b9d475":"markdown","df2d427f":"markdown","afbc857f":"markdown","3129fecc":"markdown","b055be6a":"markdown","e7829b1a":"markdown","71fbe96b":"markdown","ce42ab5d":"markdown"},"source":{"c3f82bdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b0886f80":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","8a57b3e4":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\ndata.head()","89aaa2b0":"data.describe()","d86835e4":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfig, axs = plt.subplots(nrows=2, ncols=7, figsize=(20,10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.boxplot(y=k, data=data, ax=axs[index])\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","f179498f":"data.shape","e6a4f97d":"data = data[~(data['MEDV'] >= 50.0)]","cf635120":"data.shape","9d6ef01c":"plt.figure(figsize=(20, 10))\nsns.heatmap(data.corr().abs(),  annot=True)","36b5f8f7":"corr_matrix = data.corr()\ncorr_matrix[\"MEDV\"].sort_values(ascending=False)","63a1723f":"column_sets = ['LSTAT', 'INDUS', 'TAX', 'NOX', 'PTRATIO', 'AGE', 'RAD', 'RM' ]\nx = data.loc[:,column_sets]\ny = data['MEDV']","149df924":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler()\nx = pd.DataFrame(min_max_scaler.fit_transform(x), columns=column_sets)\nx.head()","45345cf3":"fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k in column_sets:\n        sns.distplot(x[k], ax=axs[index])\n        index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","4267372c":"for k in column_sets:\n    if (x[k].skew()) > 0.5 or (x[k].skew()) < -0.5:\n        print(k,x[k].skew(),'------------> highly skewed')\n    else:\n        print(k,x[k].skew())","eb01a2f8":"for k in column_sets:\n    if (x[k].skew()) > 0.5 or (x[k].skew()) < -0.5:\n        x[k] = np.log1p(x[k])","95671af0":"fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k in column_sets:\n        sns.distplot(x[k], ax=axs[index])\n        index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","8e24b4cd":"fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor i, k in enumerate(column_sets):\n    sns.scatterplot(y=y, x=x[k], ax=axs[i])\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","9c6d97de":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","9f66cf9d":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, cross_val_predict\nfrom sklearn.preprocessing import PolynomialFeatures\nkf = KFold(n_splits=10)\nscores_map = {}","5e0cdf0b":"model1 = LinearRegression()\nscores = cross_val_score(model1, x, y ,cv=kf, scoring = \"neg_mean_squared_error\")\nscores_map['LinearRegression'] = scores\nlin_rmse_scores = np.sqrt(-scores)\ndisplay_scores(lin_rmse_scores)","5773b1de":"l_ridge = Ridge()\nscores = cross_val_score(l_ridge, x, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['Ridge'] = scores\nrid_rmse_scores = np.sqrt(-scores)\ndisplay_scores(rid_rmse_scores)","908c6458":"l_ridge = Lasso()\nscores = cross_val_score(l_ridge, x, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['Lasso'] = scores\nlas_rmse_scores = np.sqrt(-scores)\ndisplay_scores(las_rmse_scores)","220f1b7a":"poly_regs= PolynomialFeatures(degree= 2)  \nx_poly= poly_regs.fit_transform(x)  \npoly =LinearRegression() \nscores = cross_val_score(poly, x_poly, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['poly'] = scores\npoly_rmse_scores = np.sqrt(-scores)\ndisplay_scores(poly_rmse_scores)","34e4830e":"model4 = KNeighborsRegressor()\ngrid = {'n_neighbors' : [3,5,7,9,10]}\ngrid_knn = GridSearchCV(model4, grid, cv = kf, scoring='neg_mean_squared_error')\nresults = grid_knn.fit(x,y)\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","2c76780c":"knn = KNeighborsRegressor(n_neighbors=9)\nscores = cross_val_score(knn, x, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['knn'] = scores\nknn_rmse_scores = np.sqrt(-scores)\ndisplay_scores(knn_rmse_scores)","fa5d07a1":"svr = SVR()\ngrid_sv = GridSearchCV(svr, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\nresults = grid_sv.fit(x,y)\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)\n","610da22a":"svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\nscores = cross_val_score(svr_rbf, x, y, cv=kf, scoring='neg_mean_squared_error')\nscores_map['SVR'] = scores\nsvr_rmse_scores = np.sqrt(-scores)\ndisplay_scores(svr_rmse_scores)","8ff92920":"models = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('KNN', KNeighborsRegressor(n_neighbors=9)))\nmodels.append(('TREE', DecisionTreeRegressor(max_depth=5)))\nmodels.append(('SVM', SVR(kernel='rbf', C=1e3, gamma=0.1)))\nmodels.append(('GRAD', GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'neg_mean_squared_error'\nfor name, model in models:\n    \n        kfold = KFold(n_splits=10)\n        cv_results = cross_val_score(model, x, y, cv=kfold, scoring=scoring)\n        rmse_scores = np.sqrt(-cv_results)\n        results.append(rmse_scores)\n        names.append(name)\n        msg = \"%s: %f +- (%f)\" % (name, rmse_scores.mean(), rmse_scores.std())\n        print(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c9461d15":"models = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('Ridge', Ridge()))\nmodels.append(('Lasso', Lasso()))\nmodels.append(('KNN', KNeighborsRegressor(n_neighbors=9)))\nmodels.append(('TREE', DecisionTreeRegressor(max_depth=5)))\nmodels.append(('SVM', SVR(kernel='rbf', C=1e3, gamma=0.1)))\nmodels.append(('GRAD', GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)))\nmodels.append(('LR1', LinearRegression()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'neg_mean_squared_error'\nfor name, model in models:\n        \n        kfold = KFold(n_splits=10)\n        predicted = cross_val_predict(model, x, y, cv=kfold)\n        fig, ax = plt.subplots()\n        ax.scatter(y, predicted, edgecolors=(0, 0, 0))\n        ax.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=4)\n        ax.set_title(name)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        plt.show()\n","65427cd1":"Comparing the scores obtained by different models","c2f4c90b":"Applying the machine learning algorithms","dc88df72":"Plotting the data","a0cac071":"Handling the skewed data","0aecd52d":"Picking up the most relevant features","918ee846":"Comparing the model prediction by different models.","00526559":"KNeighbors Regressor","1ff17403":"Finding correlations among all the features","c3091822":"Data Preprocessing\nScaling the data","18b9d475":"Finding correlations with the MEDV","df2d427f":"Ridge Regression","afbc857f":"Lasso Regression","3129fecc":"Removing data that has MEDV values greater than or equal to 50","b055be6a":"Polynomial Regression","e7829b1a":"Support vector Regressor","71fbe96b":"Preparing the data","ce42ab5d":" LinearRegression"}}