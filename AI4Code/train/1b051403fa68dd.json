{"cell_type":{"891a5e27":"code","c4bbaee4":"code","2c01ea4c":"code","2ba338d8":"code","ed7a3352":"code","d9d81829":"code","038f2c3f":"code","da4104be":"code","74d8296d":"code","04f25932":"code","80503a60":"code","393d9053":"code","b1eea2ab":"code","15cf76ea":"code","4bb9b38c":"code","7ebacfae":"code","a69622c5":"code","a5a3e814":"code","413ee7d9":"code","1014663e":"code","81a18d01":"code","fc8d57d7":"code","3e722ea5":"code","3dc1a39f":"code","0f138f30":"code","71e61121":"code","c6e82cb1":"code","c66b9e0b":"code","930f7d1f":"code","e6a9d17f":"code","fae805cf":"code","bbe62a57":"code","55021cc7":"code","3679b63a":"code","251ef7bb":"markdown","6b00d53b":"markdown","76cc9c94":"markdown","c9161b07":"markdown","9a5289e1":"markdown","05dc32e9":"markdown","f5d3d821":"markdown","55f40a36":"markdown","33762824":"markdown","4e52c88a":"markdown","9fddf193":"markdown","55061def":"markdown","13d0db19":"markdown","1bfcc78e":"markdown","31a5ec5e":"markdown","8f118f86":"markdown","c31b44ac":"markdown","41cb79d2":"markdown","3b93b85b":"markdown","e617fcab":"markdown","130944f3":"markdown","1e5e4b00":"markdown","83a42826":"markdown","cc0dcd4b":"markdown","5881436b":"markdown","f53d8ecc":"markdown","7300ab2d":"markdown","7aa4c002":"markdown","57c65862":"markdown","bcc732ee":"markdown","3459becc":"markdown","93a2f7bd":"markdown","1e062914":"markdown","a1f44c19":"markdown","c64b180c":"markdown","c9519525":"markdown","b069c8e8":"markdown","c075638c":"markdown","3996f847":"markdown","fba27a2d":"markdown","46b01246":"markdown"},"source":{"891a5e27":"!pip install segmentation-models-pytorch\n!pip install --pre torch==1.7.0.dev20200701+cu101 torchvision==0.8.0.dev20200701+cu101 -f https:\/\/download.pytorch.org\/whl\/nightly\/cu101\/torch_nightly.html","c4bbaee4":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"WANDB_SILENT\"] = 'True'\nimport warnings\nwarnings.filterwarnings('ignore')\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom catalyst import utils\nimport cv2\nimport glob\nfrom PIL import Image\nimport os.path as osp\nfrom tqdm import tqdm\nfrom typing import Callable, List, Tuple\nimport torch\nimport catalyst\nimport wandb","2c01ea4c":"def example(image_path = '..\/input\/uavid-semantic-segmentation-dataset\/train\/train\/seq33\/Images\/000200.png', \n             mask_path ='..\/input\/uavid-semantic-segmentation-dataset\/train\/train\/seq33\/Labels\/000200.png'):\n    image = Image.open(image_path)\n    mask = Image.open(mask_path)    \n    plt.figure(figsize=(18, 24))\n    plt.subplot(1, 2, 1)\n    plt.imshow(image)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(mask)\nexample()","2ba338d8":"import pandas as pd\nimport seaborn as sns\ncls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human']\npixels = [30.436, 25.977, 17.120, 14.322, 9.464, 1.405, 1.115, 0.162]\npix = pd.DataFrame({'Classes': cls, 'Pixel Number': pixels})\n\ndef plot_pixel():\n    plt.figure(figsize=(14,12))\n    sns.set_palette(['#800000','#008000','#000000', '#804080', '#808000', '#C000C0', '#400080', '#404000'])\n    sns.barplot(x='Classes', y='Pixel Number', data=pix)\n    plt.title('Pixel Number Histogram')\n    plt.ylabel('%')\n    plt.xticks(rotation=45)","ed7a3352":"plot_pixel()","d9d81829":"class UAVidColorTransformer:\n    def __init__(self):\n    # color table.\n        self.clr_tab = self.createColorTable()\n    # id table.\n        id_tab = {}\n        for k, v in self.clr_tab.items():\n            id_tab[k] = self.clr2id(v)\n        self.id_tab = id_tab\n\n    def createColorTable(self):\n        clr_tab = {}\n        clr_tab['Clutter'] = [0, 0, 0]\n        clr_tab['Building'] = [128, 0, 0]\n        clr_tab['Road'] = [128, 64, 128]\n        clr_tab['Static_Car'] = [192, 0, 192]\n        clr_tab['Tree'] = [0, 128, 0]\n        clr_tab['Vegetation'] = [128, 128, 0]\n        clr_tab['Human'] = [64, 64, 0]\n        clr_tab['Moving_Car'] = [64, 0, 128]\n        return clr_tab\n\n    def colorTable(self):\n        return self.clr_tab\n   \n    def clr2id(self, clr):\n        return clr[0]+clr[1]*255+clr[2]*255*255\n\n  #transform to uint8 integer label\n    def transform(self,label, dtype=np.int32):\n        height,width = label.shape[:2]\n    # default value is index of clutter.\n        newLabel = np.zeros((height, width), dtype=dtype)\n        id_label = label.astype(np.int64)\n        id_label = id_label[:,:,0]+id_label[:,:,1]*255+id_label[:,:,2]*255*255\n        for tid,val in enumerate(self.id_tab.values()):\n            mask = (id_label == val)\n            newLabel[mask] = tid\n        return newLabel\n\n  #transform back to 3 channels uint8 label\n    def inverse_transform(self, label):\n        label_img = np.zeros(shape=(label.shape[0], label.shape[1],3),dtype=np.uint8)\n        values = list(self.clr_tab.values())\n        for tid,val in enumerate(values):\n            mask = (label==tid)\n            label_img[mask] = val\n        return label_img","038f2c3f":"clrEnc = UAVidColorTransformer()\ndef prepareTrainIDForDir(gtDirPath, saveDirPath):\n    gt_paths = [p for p in os.listdir(gtDirPath) if p.startswith('seq')]\n    for pd in tqdm(gt_paths):\n        lbl_dir = osp.join(gtDirPath, pd, 'Labels')\n        lbl_paths = os.listdir(lbl_dir)\n        if not osp.isdir(osp.join(saveDirPath, pd, 'TrainId')):\n            os.makedirs(osp.join(saveDirPath, pd, 'TrainId'))\n            assert osp.isdir(osp.join(saveDirPath, pd, 'TrainId')), 'Fail to create directory:%s'%(osp.join(saveDirPath, pd, 'TrainId'))\n        for lbl_p in lbl_paths:\n            lbl_path = osp.abspath(osp.join(lbl_dir, lbl_p))\n            trainId_path = osp.join(saveDirPath, pd, 'TrainId', lbl_p)\n            gt = np.array(Image.open(lbl_path))\n            trainId = clrEnc.transform(gt, dtype=np.uint8)\n            Image.fromarray(trainId).save(trainId_path)","da4104be":"prepareTrainIDForDir('..\/input\/uavid-semantic-segmentation-dataset\/train\/train', '.\/trainlabels\/')\nprepareTrainIDForDir('..\/input\/uavid-semantic-segmentation-dataset\/valid\/valid', '.\/validlabels\/')","74d8296d":"train_image_list = sorted(glob.glob(pathname='..\/input\/uavid-semantic-segmentation-dataset\/train\/train\/*\/Images\/*.png', recursive=True))\ntrain_mask_list =  sorted(glob.glob(pathname='.\/trainlabels\/*\/TrainId\/*.png', recursive=True))\nvalid_image_list = sorted(glob.glob(pathname='..\/input\/uavid-semantic-segmentation-dataset\/valid\/valid\/*\/Images\/*.png', recursive=True))\nvalid_mask_list =  sorted(glob.glob(pathname='.\/validlabels\/*\/TrainId\/*.png', recursive=True))\nprint(train_image_list[42])\nprint(train_mask_list[42])","04f25932":"SEED = 42\nutils.set_global_seed(SEED)\nutils.prepare_cudnn(deterministic=True)\nis_fp16_used = True","80503a60":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset\n\nclass Dataset(BaseDataset):\n\n    CLASSES = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\n    \n    def __init__(\n            self, \n            images_list, \n            masks_list, \n            classes=None, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.images_list = images_list\n        self.masks_list = masks_list\n        self.classes = classes\n        \n        # convert str names to class values on masks\n        if self.classes is not None:\n            self.class_values = np.array([self.CLASSES.index(cls.lower()) for cls in classes]) \/ 255\n\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_list[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_list[i], 0)\n        mask = mask.astype('float') \/ 255\n        \n        # extract certain classes from mask (e.g. cars)\n        if self.classes is not None:\n            masks = [(mask == v) for v in self.class_values]\n            mask = np.stack(masks, axis=-1).astype('float')\n        else:\n            mask = np.expand_dims(mask, 2)\n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n\n    def __len__(self):\n        return len(self.images_list)","393d9053":"def visualize(image, mask, label=None, truth=None,  augment=False):\n    if truth is None:\n        plt.figure(figsize=(14, 20))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        if augment == False:\n            plt.title(f\"{'Original Image'}\")\n        else:\n            plt.title(f\"{'Augmented Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        if label is not None:\n            plt.title(f\"{label.capitalize()}\")\n        \n    else:\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 3, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(truth)\n        plt.title(f\"{'Ground Truth'}\")\n        \ndef visualize_overlay(image, mask, truth_path=None):\n    if truth_path is None:\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n    else:\n        truth = Image.open(truth_path)\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 3, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 3, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")\n        \n        plt.subplot(1, 3, 3)\n        plt.imshow(truth)\n        plt.title(f\"{'Ground Truth'}\")\n        \ndef visualize_prediction(image, mask):\n        plt.figure(figsize=(26, 36))\n        plt.subplot(1, 2, 1)\n        plt.imshow(image)\n        plt.title(f\"{'Original Image'}\")\n\n        plt.subplot(1, 2, 2)\n        plt.imshow(mask)\n        plt.title(f\"{'Prediction'}\")","b1eea2ab":"labels = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\nfor label in labels:\n    dataset = Dataset(train_image_list, train_mask_list, classes=[label])\n\n    image, mask = dataset[4]\n    visualize(\n        image=image, mask=mask.squeeze(),\n        label = label)","15cf76ea":"import albumentations as albu\n\ndef get_training_augmentation():\n    train_transform = [\n\n        albu.Resize(576, 1024, p=1),\n        albu.HorizontalFlip(p=0.5),\n\n        albu.OneOf([\n            albu.RandomBrightnessContrast(\n                  brightness_limit=0.4, contrast_limit=0.4, p=1),\n            albu.CLAHE(p=1),\n            albu.HueSaturationValue(p=1)\n            ],\n            p=0.9,\n        ),\n\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    test_transform = [albu.Resize(576, 1024, p=1),\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","4bb9b38c":"labels = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\nfor label in labels:\n    augmented_dataset = Dataset(\n        train_image_list, \n        train_mask_list, \n        augmentation=get_training_augmentation(), \n        classes=[label],\n    )\n\n# same image with different random transforms\n    image, mask = augmented_dataset[8]\n    visualize(\n        image=image, mask=mask.squeeze(),\n        label = label, augment=True)","7ebacfae":"import segmentation_models_pytorch as smp\n\nENCODER = 'efficientnet-b3'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid'\nCLASSES = ['clutter', 'building', 'road', 'static_car', 'tree', 'vegetation', 'human', 'moving_car']\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES), \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","a69622c5":"batch_size = 6\n\ntrain_dataset = Dataset(\n    train_image_list, \n    train_mask_list, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\nvalid_dataset = Dataset(\n    valid_image_list, \n    valid_mask_list, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    classes=CLASSES,\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True, drop_last=False)\n\nloaders = {\n    \"train\": train_loader,\n    \"valid\": valid_loader\n}","a5a3e814":"if is_fp16_used:\n    fp16_params = dict(opt_level=\"O1\")\nelse:\n    fp16_params = None\n\nprint(f\"FP16 params: {fp16_params}\")","413ee7d9":"from catalyst.contrib.nn import BCEDiceLoss, RAdam, Lookahead, OneCycleLRWithWarmup\nfrom catalyst.dl import SupervisedRunner\n\nlogdir = \".\/logs\"\nnum_epochs = 30\nlearning_rate = 1e-3\nbase_optimizer = RAdam([\n    {'params': model.decoder.parameters(), 'lr': learning_rate}, \n    {'params': model.encoder.parameters(), 'lr': 1e-4},\n    {'params': model.segmentation_head.parameters(), 'lr': learning_rate},\n])\noptimizer = Lookahead(base_optimizer)\ncriterion = BCEDiceLoss(activation=None)\nrunner = SupervisedRunner()\nscheduler = OneCycleLRWithWarmup(\n    optimizer, \n    num_steps=num_epochs, \n    lr_range=(0.0016, 0.0000001),\n    init_lr = learning_rate,\n    warmup_steps=2\n)","1014663e":"from catalyst.dl.callbacks import IouCallback, WandbLogger, EarlyStoppingCallback, ClasswiseIouCallback\n\ncallbacks = [\n    IouCallback(activation = 'none'),\n    ClasswiseIouCallback(classes=CLASSES, activation = 'none'),\n    EarlyStoppingCallback(patience=7, metric='iou', minimize=False),\n    WandbLogger(project='Project_Name', name='Run_Name'),\n    \n]","81a18d01":"runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=callbacks,\n    logdir=logdir,\n    num_epochs=num_epochs,\n    # save our best checkpoint by IoU metric\n    main_metric=\"iou\",\n    # IoU needs to be maximized.\n    minimize_metric=False,\n    # for FP16. It uses the variable from the very first cell\n    fp16=fp16_params,\n    # prints train logs\n    verbose=True,\n)","fc8d57d7":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human']\ns_iou = [85.64, 72.85, 55.86, 69.78, 54.93, 0, 0, 0]\nsing_iou = pd.DataFrame({'Classes': cls, 'IoU': s_iou})\nsingle_iou = sing_iou.pivot_table(values='IoU', columns='Classes')\nsingle_iou","3e722ea5":"batch = next(iter(loaders['valid']))\ndataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation())\nimage, _ = dataset[0]\ntruth_path='..\/input\/uavid-semantic-segmentation-dataset\/valid\/valid\/seq16\/Labels\/000000.png'","3dc1a39f":"from catalyst.utils import mask_to_overlay_image\n\npred = mask_to_overlay_image(image=image, masks=single[0], threshold=0.4)\nvisualize_overlay(image, pred, truth_path=truth_path)","0f138f30":"import segmentation_models_pytorch as smp\n\nENCODER = 'efficientnet-b3'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid'\nCLASSES = ['clutter', 'building', 'road', 'tree', 'vegetation']\n\n# create segmentation model with pretrained encoder\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=len(CLASSES),\n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","71e61121":"from torch.optim.lr_scheduler import ReduceLROnPlateau\n\nlogdir = \".\/logs\"\nnum_epochs = 40\nlearning_rate = 1.5e-3\nbase_optimizer = RAdam([\n    {'params': model.decoder.parameters(), 'lr': learning_rate, 'weight_decay': 1e-3}, \n    {'params': model.encoder.parameters(), 'lr': 1e-4, 'weight_decay': 1e-4},\n    {'params': model.segmentation_head.parameters(), 'lr': learning_rate},\n])\noptimizer = Lookahead(base_optimizer)\nscheduler = ReduceLROnPlateau(optimizer, factor=0.3, patience=3, mode='max')\ncriterion = BCEDiceLoss(activation=None)\nrunner = SupervisedRunner()","c6e82cb1":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation']\ns_iou = [85.64, 72.85, 55.86, 69.78, 54.93]\ncls5_iou = [86.79, 74.16, 58.01, 71.08, 53.59]\nsing_iou = pd.DataFrame([s_iou, cls5_iou], columns=cls, index=['single', 'top5'])\nsing_iou","c66b9e0b":"pred5 = mask_to_overlay_image(image=image, masks=cls5[0], threshold=0.4)\nvisualize_overlay(image, pred5, truth_path=truth_path)","930f7d1f":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['moving_car'])\nimage, ground_truth = dataset[0]\n\ngrs2 = mask_to_overlay_image(image=image, masks=mc[0], threshold=0.4)\nvisualize(image, grs2, truth=ground_truth.squeeze())","e6a9d17f":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['static_car'])\nimage, ground_truth = dataset[0]\n\ngrs3 = mask_to_overlay_image(image=image, masks=sc[0], threshold=0.4)\nvisualize(image, grs3, truth=ground_truth.squeeze())","fae805cf":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation(), classes=['human'])\nimage, ground_truth = dataset[0]\n\ngrs4 = mask_to_overlay_image(image=image, masks=hum[0], threshold=0.4)\nvisualize(image, grs4, truth=ground_truth.squeeze())","bbe62a57":"cls = ['Building', 'Tree', 'Clutter', 'Road', 'Vegetation', 'Static Car', 'Moving Car', 'Human', 'mIoU']\nensemble = [86.79, 74.16, 58.01, 71.08, 53.59, 51.34, 39.27, 22.21, 57.06]\nfinal = pd.DataFrame([ensemble], columns=cls, index=['ensemble'])\nfinal","55021cc7":"dataset = Dataset(valid_image_list, valid_mask_list, augmentation=get_validation_augmentation())\nimage, _ = dataset[0]\nimage1, _ = dataset[1]\nimage2, _ = dataset[2]\nimage3, _ = dataset[3]\nimage4, _ = dataset[4]\nimages = [image, image1, image2, image3, image4]\nfull = [[cls5[i][0], cls5[i][1], cls5[i][2], sc[i][0], cls5[i][3], cls5[i][4], hum[i][0], mc[i][0]]  for i in range(5)]\nfull_truth1 = [mask_to_overlay_image(image=images[i], masks=full[i], threshold=0.4) for i in range(5)]","3679b63a":"for i in range(5):\n    visualize_prediction(images[i], full_truth1[i])","251ef7bb":"In our experiments, we'll use FPN model with EfficientnetB3 encoder. The motivation was to select best model in Memory Consumption, Accuracy trade-off.","6b00d53b":"# Dataset","76cc9c94":"As we can see, we\u2019ve shown strong results in most pixels classes. But we\u2019re not able to identify cars and people. \n\nTo address this problem, we create 3 additional models for static car, moving car and human classes.","c9161b07":"New experiment settings:\n* Loss: The same\n* Optimizer: The same\n* Scheduler: ReduceLROnPlateau with patience 3 and factor 0.3\n* Number of epochs changed to 40. Added weight decay. Learning rate is set to 1.5e-3. Batch size reduced to 5.","9a5289e1":"Visualizing augmented images and masks.","05dc32e9":"### Static Car","f5d3d821":"In our Dataset, we can read images, extract values of classes from segmentation mask, apply augmentation and pre-processing transformations.","55f40a36":"Selecting optimization level: **01** - Mixed Precision (recommended for typical use)","33762824":"In this notebook we\u2019ll use the following notable libraries: \n\n* [segmentation_models.pytorch](https:\/\/github.com\/qubvel\/segmentation_models.pytorch) has a lot of encoders for each model architecture.\n* [albumentations](https:\/\/github.com\/albumentations-team\/albumentations) has spatial-level transforms that change both an input image and mask simultaneously.\n* [catalyst](https:\/\/github.com\/catalyst-team\/catalyst) PyTorch framework that helps with reproducibility, fast experimentation and has a lot of useful utils.\n* [wandb](https:\/\/www.wandb.com\/) Logger to track metrics, save hyper-parameters, gradients and model checkpoints.","4e52c88a":"### Human","9fddf193":"As we can see from the table, we\u2019ve improved previous results in 4 of 5 classes.","55061def":"Set seed and mix precision training.","13d0db19":"Visualizing segmentation masks for all classes.","1bfcc78e":"Here we try several learning rates: 3e-4, 7e-4, 1e-3, 1.5e-3, 3e-3","31a5ec5e":"## Moving Car","8f118f86":"Dataset comprises 30 video sequences capturing 4K high-resolution images in slanted views. In total, 300 images have been densely labeled with 8 classes.\nThe image resolution is mostly 3840\u00d72160 or 4096\u00d72160.\n\n![](https:\/\/i.ibb.co\/r73CD9M\/colors.png)\n\n1. building: living houses, garages, skyscrapers, security booths, and buildings under construction.\n2. road: road or bridge surface that cars can run on legally. Parking lots are not included.\n3. tree: tall trees that have canopies and main trunks.\n4. low vegetation: grass, bushes and shrubs.\n5. static car: cars that are not moving, including static buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n6. moving car: cars that are moving, including moving buses, trucks, automobiles, and tractors. Bicycles and motorcycles are not included.\n7. human: pedestrians, bikers, and all other humans occupied by different activities.\n8. clutter: all objects not belonging to any of the classes above.","c31b44ac":"If we use joblib.Parallel, we can speedup ~3x","41cb79d2":"Here we will work with [UAVid](https:\/\/uavid.nl\/#home) dataset, which focusing on urban scenes. Our goal is to predict per-pixel semantic labeling.\n\nAnd the evaluation metric is IoU. \n![](https:\/\/www.oreilly.com\/library\/view\/deep-learning-for\/9781788295628\/assets\/63fb2c41-8e83-49c5-ad3a-fee59e8a178b.png)","3b93b85b":"Most of the pixels are from classes like building, tree, clutter, road, and low vegetation. Fewer pixels are from moving\ncar and static car classes, which are both fewer than 1.5% of\nthe total pixels. For human class, it is almost zero, fewer\nthan 0.2% of the total pixels.","e617fcab":"![](https:\/\/i.ibb.co\/cXKMJRS\/sc.png)","130944f3":"![](https:\/\/i.ibb.co\/hRhzVBf\/cls5.png)","1e5e4b00":"# Data overview","83a42826":"# Data Preparation","cc0dcd4b":"We\u2019ll need helper functions for label image conversion from 3 channel RGB color image to 1 channel label index image","5881436b":"Importing callbacks for metrics and logging","f53d8ecc":"After creating label images, we\u2019ll define lists of images and labels for our Dataset.","7300ab2d":"## Clutter, Building, Road, Tree, Vegetation","7aa4c002":"![](https:\/\/i.ibb.co\/XphB6s2\/mc.png)","57c65862":"![](https:\/\/i.ibb.co\/qjGFnbw\/single.png)","bcc732ee":"## Single Model","3459becc":"Hello everyone!\n\n\n\n![](https:\/\/uavid.nl\/UAVid_files\/imgs\/UAVid_example.png)","93a2f7bd":"Experiment settings:\n* Loss: BCEDiceLoss with 0.5 contibution of BCE and Dice\n* Optimizer: Lookahead(improves the learning stability and lowers the variance of its inner optimizer)\n* Scheduler: OneCycleLRWithWarmup with 2 warmup steps\n* Initial learning rate is set to 1e-3, and 1e-4 on encoder. Number of epochs to 30.","1e062914":"Model training, set main_metric to **'iou'**","a1f44c19":"# Model Creation","c64b180c":"# Semantic Segmentation of High-Resolution Aerial Images","c9519525":"![](https:\/\/i.ibb.co\/VVDpYxt\/hum.png)","b069c8e8":"We\u2019ll define Dataloaders and set batch size to 6, because of memory limitation.","c075638c":"We'll resize images to `576*1024` to keep 9:16 ratio. \n\nAugmentation list:\n* *HorizontalFlip*\n* *OneOf(RandomBrightnessContrast, CLAHE, HueSaturationValue)*\n* *IAAAdditiveGaussianNoise* with 0.2 probability\n\n*Note:* For better result we could crop each image into 16 evenly distributed smaller(1280*720) overlapped images that cover the whole image for training.","3996f847":"Helper functions for visualization.","fba27a2d":"## Ensemble","46b01246":"First, we\u2019ll install `segmentation-models-pytorch` and `torch nightly` for native amp support. And import libraries."}}