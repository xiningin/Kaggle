{"cell_type":{"101e4af2":"code","1894d2ce":"code","3aee2259":"code","63b16757":"code","1e683a1a":"code","6fbb2393":"code","c33f7273":"code","1f0aec8e":"code","a66be453":"code","c1255fe6":"code","35ef7b6a":"code","c46b2782":"code","aacb7b16":"code","1a251975":"code","2fe48cb6":"code","3e0386fa":"code","96c306db":"code","3463161f":"code","8d2e11bd":"code","a406f478":"code","2b3078f2":"code","20f646dd":"code","882f1eaa":"code","e2215180":"code","551808f2":"code","2f6be4d4":"code","464266db":"code","517a5979":"code","983907fa":"code","99521fb4":"code","53d94ba9":"code","7780650c":"code","c62193ad":"markdown","886785de":"markdown","5fa83871":"markdown","f0163b6f":"markdown","cb2304a1":"markdown","12f99bbe":"markdown","d64926ee":"markdown","56792c8c":"markdown","b0c4ef34":"markdown","9c15eb14":"markdown","4fc7b08c":"markdown","ac0dc657":"markdown","6b92bc4e":"markdown"},"source":{"101e4af2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\n","1894d2ce":"column_names = [\"Id\", \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Income\"]","3aee2259":"test_data = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)\ntrain_data = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", names = column_names, na_values='?').drop(0, axis = 0).reset_index(drop = True)","63b16757":"train_data = train_data.drop(\"Id\", axis = 1)\ntrain_data = train_data.drop(\"Education\", axis = 1)\ntest_data = test_data.drop(\"Id\", axis = 1)\ntest_data = test_data.drop(\"Education\", axis = 1)\ntest_data = test_data.drop(\"Income\", axis = 1)","1e683a1a":"test_data.head()","6fbb2393":"train_data.columns","c33f7273":"train_data.shape","1f0aec8e":"train_data.head()","a66be453":"train_data.info()","c1255fe6":"train_data['Age'] = train_data['Age'].astype('int64')\ntrain_data['fnlwgt'] = train_data['fnlwgt'].astype('int64')\ntrain_data['Education-Num'] = train_data['Education-Num'].astype('int64')\ntrain_data['Capital Gain'] = train_data['Capital Gain'].astype('int64')\ntrain_data['Capital Loss'] = train_data['Capital Loss'].astype('int64')\ntrain_data['Hours per week'] = train_data['Hours per week'].astype('int64')\n","35ef7b6a":"train_data.info()","c46b2782":"train_data.describe()","aacb7b16":"train_data.describe(include=['object'])","1a251975":"train_data = train_data.fillna('missing')  # Preencher dados faltantes com a string 'missing'\ntest_data = test_data.fillna('missing')","2fe48cb6":"def count_null_values(data):  # Returns a DataFrame with count of null values\n    \n    \n    counts_null = []\n    for column in data.columns:\n        counts_null.append(data[column].isnull().sum())\n    counts_null = np.asarray(counts_null)\n\n    counts_null = pd.DataFrame({'feature': data.columns, 'count.': counts_null,\n                                'freq. [%]': 100*counts_null\/data.shape[0]}).set_index('feature', drop = True)\n    counts_null = counts_null.sort_values(by = 'count.', ascending = False)\n    \n    return counts_null","3e0386fa":"count_null_values(train_data)","96c306db":"train_data_analysis = train_data.apply(preprocessing.LabelEncoder().fit_transform)","3463161f":"corr_mat = train_data_analysis.corr()\nsns.set()\nplt.figure(figsize=(15,12))\nsns.heatmap(corr_mat, annot=True)","8d2e11bd":"train_data_analysis.corr().Income.sort_values()","a406f478":"abs(train_data_analysis.corr().Income).sort_values(ascending=False)","2b3078f2":"x_train = train_data[[\"Capital Gain\", \"Education-Num\", \"Relationship\", \"Age\", \"Hours per week\", \"Sex\", \"Marital Status\", \n                      \"Capital Loss\"]].apply(preprocessing.LabelEncoder().fit_transform)\ny_train = train_data.Income\n\nx_test = test_data[[\"Capital Gain\", \"Education-Num\", \"Relationship\", \"Age\", \"Hours per week\", \"Sex\", \"Marital Status\", \n                      \"Capital Loss\"]].apply(preprocessing.LabelEncoder().fit_transform)","20f646dd":"scaler = MinMaxScaler()  # Scaler para normalizar os dados contidos nos atributos\n\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","882f1eaa":"knn = KNeighborsClassifier(n_neighbors = 23, p = 1)\nstart = time.time()\nscores = cross_val_score(knn, x_train, y_train, cv = 10)\nprint('K-Nearest Neighbors CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(scores.mean(), scores.std()))\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","e2215180":"start = time.time()\nknn.fit(x_train, y_train)\ny_predict_knn = knn.predict(x_test)\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","551808f2":"log_clf = LogisticRegression(solver = 'lbfgs', C = 1.0, penalty = 'l2', warm_start =  True)\nstart = time.time()\nlog_scores = cross_val_score(log_clf, x_train, y_train, cv = 10)\nprint('Logistic Regression CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(log_scores.mean(), log_scores.std()))\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","2f6be4d4":"start = time.time()\nlog_clf.fit(x_train, y_train)\ny_predict_log = log_clf.predict(x_test)\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","464266db":"rf_clf = RandomForestClassifier(n_estimators = 700, max_depth = 12)\nstart = time.time()\nrf_scores = cross_val_score(rf_clf, x_train, y_train, cv = 10)\nprint('Random Forest CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(rf_scores.mean(), rf_scores.std()))\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","517a5979":"start = time.time()\nrf_clf.fit(x_train, y_train)\ny_predict_rf = rf_clf.predict(x_test)\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","983907fa":"gnb_clf = GaussianNB()\nstart = time.time()\ngnb_scores = cross_val_score(gnb_clf, x_train, y_train, cv = 10)\nprint('Gaussian Naive Bayes CV accuracy: {0:1.4f} +-{1:2.5f}\\n'.format(gnb_scores.mean(), gnb_scores.std()))\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","99521fb4":"start = time.time()\ngnb_clf.fit(x_train, y_train)\ny_predict_gnb = gnb_clf.predict(x_test)\nprint ('Time elapsed: {0:1.2f}\\n'.format(time.time()-start))","53d94ba9":"df_pred_knn = pd.DataFrame({'Income':y_predict_knn})\ndf_pred_log = pd.DataFrame({'Income':y_predict_log})\ndf_pred_rf = pd.DataFrame({'Income':y_predict_rf})\ndf_pred_gnb = pd.DataFrame({'Income':y_predict_gnb})","7780650c":"df_pred_knn.to_csv(\"knn_prediction.csv\", index = True, index_label = 'Id')\ndf_pred_log.to_csv(\"log_prediction.csv\", index = True, index_label = 'Id')\ndf_pred_rf.to_csv(\"rf_prediction.csv\", index = True, index_label = 'Id')\ndf_pred_gnb.to_csv(\"gnb_prediction.csv\", index = True, index_label = 'Id')","c62193ad":"Os classificadores utilizados neste <i>notebook<\/i> foram, como se pode verificar acima, <b>K-Nearest Neighbors<\/b>, <b>Logistic Regression<\/b>, <b>Random Forest<\/b> e <b>(Gaussian) Naive Bayes<\/b>. O primeiro foi inclu\u00eddo apenas para facilitar a compara\u00e7\u00e3o dos resultados obtidos no trabalho anterior. No entanto, nos limitamos \u00e0 aplica\u00e7\u00e3o das t\u00e9cnicas <i>label encoding<\/i> e <i>scaling<\/i> dos atributos, sem no entanto realizar <i>grid search<\/i> devido ao longo tempo de processamento. Quando necess\u00e1rio, foram selecionado hiperpar\u00e2metros que garantem resultados considerados satisfat\u00f3rios.\n\nA ideia por tr\u00e1s deste trabalho foi comparar um pequeno conjunto de classificadores relativamente simples, compar\u00e1veis ao KNN, e ao mesmo tempo importantes; ou que partem de pressupostos que poderiam colocar sua efic\u00e1cia em xeque num primeiro momento, como o <i>Naive Bayes<\/i> ou o <i>Random Forest<\/i>. Durante as aulas, foi enfatizado que, apesar da simplicidade, seus desempenhos s\u00e3o surpreendentes. Procurou-se verificar essa informa\u00e7\u00e3o de maneira pr\u00e1tica.\n\nN\u00e3o houve diferen\u00e7a significativa percebida na dificuldade de gerar os algoritmos de classifica\u00e7\u00e3o, mas o mais simples foi o <i>Naive Bayes<\/i>, que n\u00e3o exigiu configura\u00e7\u00e3o de hiperpar\u00e2metros.\n\nEsses dois aspectos, relativa simplicidade e baixo tempo de execu\u00e7\u00e3o combinados com altas acur\u00e1cias, parecem explicar, ao menos em parte, por que esses algoritmos s\u00e3o considerados bem-sucedidos.\n\nQuanto ao tempo de execu\u00e7\u00e3o da valida\u00e7\u00e3o cruzada, os mais demorados foram, respectivamente, o <i>Random Forest<\/i> (devido em grande parte aos hiperpar\u00e2metros selecionados) e o <i>K-Nearest Neighbors<\/i>. Por outro lado, os tempos para treino e realiza\u00e7\u00e3o da predi\u00e7\u00e3o s\u00e3o consideravelmente mais curtos, especialmente no caso do <i>Random Forest<\/i>.\n\nEstes dois \u00faltimos que tamb\u00e9m alcan\u00e7aram os melhores resultados para os testes por <i>cross validation<\/i>, enquanto o <i>Naive Bayes<\/i> apresentou o pior. No entanto, ao realizar a submiss\u00e3o no <i>leaderboard<\/i>, os melhores resultados de acur\u00e1cia seguiram a seguinte ordem (decrescente): NB, RF, KNN, LOG.","886785de":"## Sele\u00e7\u00e3o de features","5fa83871":" Podemos descartar as colunas 'Id' (sem utilidade para classifica\u00e7\u00e3o) e 'Education' (informa\u00e7\u00e3o j\u00e1 quantificada pela coluna Education-Num):","f0163b6f":"Agora n\u00e3o h\u00e1 mais dados faltantes.","cb2304a1":"## Considera\u00e7\u00f5es finais","12f99bbe":"Separar dados para submiss\u00e3o:","d64926ee":"## Classificador: Gaussian Naive Bayes","56792c8c":"## Classificador: Random Forest","b0c4ef34":"## Classificador: Logistic Regression","9c15eb14":"## Classificador: K-Nearest Neighbors","4fc7b08c":"# PMR3508 - Trabalho 2: Aplica\u00e7\u00e3o de diferentes classificadores na base Adult","ac0dc657":"Sele\u00e7\u00e3o dos atributos de maior correla\u00e7\u00e3o com a vari\u00e1vel de interesse, os quais ser\u00e3o utilizados pelos classificadores:","6b92bc4e":"## Explora\u00e7\u00e3o e tratamento dos dados"}}