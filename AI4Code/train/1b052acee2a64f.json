{"cell_type":{"16dbe424":"code","e27cc676":"code","99791d61":"code","de5d76e1":"code","3d759da4":"code","33f0da92":"code","2f6460d4":"code","53c47c7b":"code","104d24fd":"code","6cb5b634":"code","01396cc1":"code","3b359020":"code","4b736b6b":"code","1223423c":"markdown","12c82545":"markdown","bd945d92":"markdown","c2d00ad9":"markdown","6f6a17e0":"markdown","b66cba1d":"markdown","e5e9e306":"markdown"},"source":{"16dbe424":"%%capture\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten,MaxPool2D,Dropout\nfrom keras.optimizers import RMSprop, Adam, Nadam\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e27cc676":"\ndef get_data():\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from keras.utils.np_utils import to_categorical\n    \n    y_train = pd.read_csv('\/kaggle\/input\/ahdd1\/Arabic Handwritten Digits Dataset CSV\/csvTrainLabel 60k x 1.csv')\n    X_train = pd.read_csv('\/kaggle\/input\/ahdd1\/csvTrainImages 60k x 784.csv')\n    y_test = pd.read_csv('\/kaggle\/input\/ahdd1\/Arabic Handwritten Digits Dataset CSV\/csvTestLabel 10k x 1.csv')\n    X_test = pd.read_csv('\/kaggle\/input\/ahdd1\/csvTestImages 10k x 784.csv')\n    # Normalise and reshape\n    X_train=X_train.values.reshape((-1,28,28,1))\/255.0\n    X_test=X_test.values.reshape((-1,28,28,1))\/255.0\n    \n    # OnehotEncode y_train\n    y_train=to_categorical(y_train, num_classes=10)\n    \n    # Split into Train and validation\n    X_train,X_valid,y_train, y_valid = train_test_split(X_train, \n                                                        y_train,\n                                                        test_size=0.3,\n                                                        shuffle=True)\n    \n    \n    return X_train,y_train,X_valid, y_valid\n    \n","99791d61":"X_train , y_train, X_valid, y_valid = get_data()\n# Data Augmentation to reduce bias\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        #rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","de5d76e1":"# Early Stopping\nclass custom_callback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self,epoch,logs={}):\n        if(logs.get('accuracy')>0.92):\n            self.model.stop_training=True\n            \n# LR Scheduler\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n","3d759da4":"# MODEL\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same',\n                activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=32, kernel_size=(5,5),padding='Same',\n                activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),padding='Same',\n                activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3),padding='Same',\n                activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.4))\n\nmodel.add(Flatten())\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dropout((0.4)))\nmodel.add(Dense(10,activation='softmax'))\n","33f0da92":"# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","2f6460d4":"epochs=10\nbatch_size=64\n# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_valid,y_valid),\n                              verbose = 2, callbacks=[learning_rate_reduction])","53c47c7b":"pd.DataFrame(history.history).plot(figsize=(10,6))","104d24fd":"y_pred = model.predict(X_test)\ny_test = to_categorical(y_test,num_classes=10)\nmodel.evaluate(X_test,y_test)","6cb5b634":"from hyperopt import Trials, STATUS_OK, tpe\nfrom hyperas import optim\nfrom hyperas.distributions import choice, uniform\nfrom hyperas import optim ","01396cc1":"def build_model():\n    \n    import os\n    import gc\n    import secrets\n\n    import tensorflow as tf\n    from tensorflow.keras import backend as K\n    from tensorflow.keras.models import Sequential, load_model\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    from tensorflow.keras import layers, regularizers, callbacks\n    from tensorflow.keras.optimizers import SGD, Adam\n    \n    model = Sequential()\n    model.add(layers.Conv2D(filters={{choice([16,32,64,128])}},\n                     kernel_size={{choice([4,5,7])}},\n                     padding='Same',\n                     activation='relu', input_shape=(28,28,1)))\n    \n    model.add(tf.keras.layers.Conv2D(filters={{choice([16,32,64,128])}},\n                     kernel_size={{choice([4,5,7])}},\n                     padding='Same',\n                     activation='relu', input_shape=(28,28,1)))\n    \n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Dropout({{uniform(0,1)}}))\n\n    model.add(Conv2D(filters={{choice([16,32,64,128])}},\n                     kernel_size={{choice([4,5,7])}},\n                     padding='Same',\n                     activation='relu', input_shape=(28,28,1)))\n    \n    model.add(Conv2D(filters={{choice([16,32,64,128])}},\n                     kernel_size={{choice([4,5,7])}},\n                     padding='Same',\n                     activation='relu', input_shape=(28,28,1)))\n    \n    model.add(MaxPool2D(pool_size=(2,2)))\n    \n    model.add(Dropout({{uniform(0,1)}}))\n\n    model.add(Flatten())\n    model.add(Dense({{choice([128,256,512])}},activation='relu'))\n    model.add(Dropout({{uniform(0,1)}}))\n    model.add(Dense(10,activation='softmax'))\n    \n    # Model compile\n    model.compile(optimizer = {{choice(['adam','nadam','rmsprop','adagrad','sgd'])}} ,\n                  loss = \"categorical_crossentropy\",\n                  metrics=[\"accuracy\"])\n    \n    \n    \n    X_train , y_train, X_valid, y_valid = get_data()\n    \n    # Data Augmentation to reduce bias\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        #rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n    datagen.fit(X_train)\n    \n    model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_valid,y_valid),\n                              verbose = 2)\n    \n    acc,score = model.evaluate(x_test, y_test, verbose=0)\n    \n    return {'loss': -acc, 'status': STATUS_OK, 'model': model}","3b359020":"best_run, best_model = optim.minimize(\n    model=build_model,\n    data=get_data,\n    algo=tpe.suggest,\n    max_evals=20,\n    trials=Trials(),\n    notebook_name = '__notebook_source__')","4b736b6b":"X_train, X_val, X_test, y_train, y_val, y_test = data()\n    print(\"Evalutation of best performing model:\")\n    print(best_model.evaluate(X_test, y_test))\n    print(\"Best performing model chosen hyper-parameters:\")\n    print(best_run)","1223423c":"## Data Pre-processing","12c82545":"## Model Building","bd945d92":"## Importing important modules","c2d00ad9":"# STEPS:\n\n1. Normalization and reshaping image data\n2. One Hot Encoding Labels\n3. Splitting into train\/test sets\n4. Image Augmentation using ImageDataGenerator *\n5. Custom Callback\/LR Scheduling and choosing Optimizer\n6. Making Model Architecture and compile model\n7. Fit data into DataGenerator object\n8. Plot Accuracy, Losses \n9. Download Hyperas Module and implement Hyperparameter Tuning\n10. Train using best Parameters","6f6a17e0":"## Hyper Parameter Tuning using Hyperas Module\n\n!pip install hyperas\n\n!pip install hyperopt","b66cba1d":"## Plotting Val and Train Loss ","e5e9e306":"## Compiling and Training"}}