{"cell_type":{"72e45167":"code","1f62e416":"code","6ce68560":"code","703c8b7d":"code","dbd3bb71":"code","e5e72afb":"code","d6fba70d":"code","4751fdb1":"code","c28b9819":"code","549d34b1":"code","42202528":"code","4473b2c3":"code","e93d2f0b":"code","e88999c7":"code","8322ebb7":"code","59d06108":"code","1e4928d0":"code","3c10da64":"code","5c90c2d3":"code","b344440e":"markdown","590db3ba":"markdown","00e99383":"markdown","5e343c25":"markdown","ec5582a1":"markdown"},"source":{"72e45167":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom lightgbm import LGBMClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","1f62e416":"df_train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv', index_col = 'id')\nY_train = df_train['target'].copy()\nX_train = df_train.copy().drop('target', axis = 1)\n\nX_test = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv', index_col = 'id')","6ce68560":"df_train","703c8b7d":"df_train.info()","dbd3bb71":"X_test.info()","e5e72afb":"df_train.describe().T","d6fba70d":"df_train.nunique().sort_values()","4751fdb1":"plt.figure(figsize=(12,5))\n\nclass_order = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9']\nax = sns.countplot(x=\"target\", data=df_train, palette=\"BuPu\", order = class_order)\n\nplt.xlabel(\"Class\", fontsize= 12)\nplt.ylabel(\"N_Samples\", fontsize= 12)\nplt.title(\"Number of Samples per Class\", fontsize= 13)\nplt.ylim(0,100000)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.18, p.get_height()+3000))\n\nplt.show()","c28b9819":"features = X_train.columns\n\nplt.figure(figsize=(15,70))\n\nfor i,col in enumerate(features):    \n    plt.subplot(25,3,i + 1)\n    sns.distplot(df_train.loc[:,col])\n    plt.ylabel('')\n    plt.tight_layout()\n\nplt.show()","549d34b1":"def cv_function (X_train, Y_train, model, splits = 10):\n    \n    kfold = StratifiedKFold(n_splits = splits)\n    logloss = []\n   \n    cv_pred = np.zeros((200000,9))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, test_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xtest = X_train.iloc[test_idx]\n        ytest = Y_train.iloc[test_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 100, eval_set = [(xtest,ytest)], verbose = False)\n\n        #create predictions\n        preds = model.predict_proba(xtest)\n        cv_pred[test_idx] = preds\n                              \n        # calculate and append accuracy\n        fold_logloss = metrics.log_loss(ytest,preds)\n        print(\"LogLoss: {0:0.5f}\". format(fold_logloss))\n        logloss.append(fold_logloss)\n        \n    print (np.mean(logloss))\n    #return np.mean(accuracies)\n    return cv_pred","42202528":"lgbm_model = LGBMClassifier(n_estimators = 2000, learning_rate = 0.02, random_state = 42, num_class = 9, metric = 'multi_logloss',\n                           subsample = 0.8, colsample_bytree = 0.8, reg_alpha = 0.5, reg_lambda = 0.5, max_depth = 20)","4473b2c3":"lgbm_cvpred = cv_function(X_train, Y_train, lgbm_model)\n#1.7502096540231844\n#1.7495114276370374 after adding subsample and colsample_bytree\n#1.7489962868771634 after adding reg_alpha and reg_lambda","e93d2f0b":"def prediction (X_train, Y_train, model, X_test):\n    \n    kfold = StratifiedKFold(n_splits = 10)\n\n    y_pred = np.zeros((100000,9))\n    train_oof = np.zeros((200000,9))\n    \n    for idx in kfold.split(X=X_train, y=Y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train.iloc[train_idx]\n        ytrain = Y_train.iloc[train_idx]\n        xval = X_train.iloc[val_idx]\n        yval = Y_train.iloc[val_idx]\n        \n        # fit model for current fold\n        model.fit(xtrain, ytrain, \n            early_stopping_rounds = 100, eval_set = [(xval,yval)], verbose = False)\n\n        #create predictions\n        y_pred += model.predict_proba(X_test)\/kfold.n_splits\n        print(y_pred)\n               \n        val_pred = model.predict_proba(xval)\n        # getting out-of-fold predictions on training set\n        train_oof[val_idx] = val_pred\n        \n        # calculate and append logloss\n        fold_logloss = metrics.log_loss(yval,val_pred)\n        print(\"Logloss: {0:0.5f}\". format(fold_logloss))\n  \n    return y_pred, train_oof","e88999c7":"lgbm_pred, train_oof  = prediction (X_train, Y_train, lgbm_model, X_test)","8322ebb7":"print(\"Logloss: {0:0.6f}\".format(metrics.log_loss(Y_train,train_oof)))","59d06108":"train_oof = pd.DataFrame(train_oof, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\ntrain_oof","1e4928d0":"pred_test = pd.DataFrame(lgbm_pred, columns = ['Class_1', 'Class_2', 'Class_3', 'Class_4', 'Class_5', 'Class_6', 'Class_7', 'Class_8', 'Class_9'])\npred_test","3c10da64":"train_oof.to_csv('lgbm_train_oof.csv', index=False)\ntrain_oof","5c90c2d3":"output = pred_test\noutput['id'] = X_test.index\noutput.to_csv('submission.csv', index=False)\n\noutput","b344440e":"## Importing Libraries and Datasets","590db3ba":"## CV with Base LightGBM","00e99383":"## Exploring the Data","5e343c25":"## Making Predictions","ec5582a1":"# <center>Tabular Playground Series - June\/2021<center>\n## <center>Starter - EDA + Base LightGBM<center>\n---\nAt first, it looks like this competition will be pretty much similar to the last one. More classes and more features, but with a similar distribution, as shown on the distribution plot. This notebook provides a baseline score for further approaches.\n    \nMy other notebooks in this competition:\n- [Tabular Playground Series - June\/2021: Simple Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-simple-nn-with-keras)\n- [Tabular Playground Series - June\/2021: Keras Neural Network with Embedding Layer](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-keras-nn-with-embedding)\n- [Tabular Playground Series - June\/2021: Wide and Deep Neural Network with Keras](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-wide-and-deep-nn-w-keras)\n- [Tabular Playground Series - June\/2021: LightAutoML with KNN Features](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-lightautoml-w-knn-feats)\n- [Tabular Playground Series - June\/2021: Keras Neural Network with Skip Connections](https:\/\/www.kaggle.com\/jonaspalucibarbosa\/tps06-21-keras-nn-with-skip-connections)"}}