{"cell_type":{"f3ade7bc":"code","2e2af40e":"code","2206e492":"code","0e606095":"code","75a48af4":"code","c93b617e":"code","6f032d87":"code","008ffd43":"code","20eb1bfb":"code","a7ccedae":"code","74ab2026":"code","7db650b0":"code","d04623ec":"code","c9eb84f5":"code","47bf5919":"code","d1b6729e":"code","ca69b340":"code","1a45980f":"code","a3e71a7c":"markdown","fe49c6fb":"markdown","c79e15e5":"markdown","410be7da":"markdown","6ca8225b":"markdown","e4c55366":"markdown","a5029f19":"markdown","ca8a3bf4":"markdown","2a17ca87":"markdown","cb046bdc":"markdown"},"source":{"f3ade7bc":"import os\nimport re\n\nimport numpy as np\nimport tensorflow as tf\n\nnp.random.seed(1)\ntf.set_random_seed(2)\n\nimport pandas as pd\nimport keras\n# from tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import f1_score, classification_report, log_loss\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Flatten\nfrom keras.layers import Dropout, Conv1D, GlobalMaxPool1D, GRU, GlobalAvgPool1D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\n# https:\/\/www.kaggle.com\/inspector\/keras-hyperopt-example-sketch\n\n#training constants\nMAX_SEQ_LEN = 25 #this is based on a quick analysis of the len of sequences train['text'].apply(lambda x : len(x.split(' '))).quantile(0.95)\nDEFAULT_BATCH_SIZE = 128\nprint(os.listdir('..\/input'))","2e2af40e":"data = pd.read_csv('..\/input\/Sentiment.csv')\ntrain, test = train_test_split(data, random_state = 42, test_size=0.1)\nprint(train.shape)\nprint(test.shape)","2206e492":"# Mapping of common contractions, could probbaly be done better\nCONTRACTION_MAPPING = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n                       \"It's\": 'It is', \"Can't\": 'Can not',\n                      }\n\ndef clean_text(text, mapping):\n    replace_white_space = [\"\\n\"]\n    for s in replace_white_space:\n        text = text.replace(s, \" \")\n    replace_punctuation = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\", \"\\'\", r\"\\'\"]\n    for s in replace_punctuation:\n        text = text.replace(s, \"'\")\n    \n    # Random note: removing the URL's slightly degraded performance, it's possible the model learned that certain URLs were positive\/negative\n    # And was able to extrapolate that to retweets. Could also explain why re-training the Embeddings improves performance.\n    # remove twitter url's\n#     text = re.sub(r\"http[s]?:\/\/t.co\/[A-Za-z0-9]*\",\"TWITTERURL\",text)\n    mapped_string = []\n    for t in text.split(\" \"):\n        if t in mapping:\n            mapped_string.append(mapping[t])\n        elif t.lower() in mapping:\n            mapped_string.append(mapping[t.lower()])\n        else:\n            mapped_string.append(t)\n    return ' '.join(mapped_string)","0e606095":"# Get tweets from Data frame and convert to list of \"texts\" scrubbing based on clean_text function\n# CONTRACTION_MAPPING is a map of common contractions(e.g don't => do not)\ntrain_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in train['text'].values]\ntest_text_vec = [clean_text(text, CONTRACTION_MAPPING) for text in test['text'].values]\n\n\n# tokenize the sentences\ntokenizer = Tokenizer(lower=False)\ntokenizer.fit_on_texts(train_text_vec)\ntrain_text_vec = tokenizer.texts_to_sequences(train_text_vec)\ntest_text_vec = tokenizer.texts_to_sequences(test_text_vec)\n\n# pad the sequences\ntrain_text_vec = pad_sequences(train_text_vec, maxlen=MAX_SEQ_LEN)\ntest_text_vec = pad_sequences(test_text_vec, maxlen=MAX_SEQ_LEN)\n\nprint('Number of Tokens:', len(tokenizer.word_index))\nprint(\"Max Token Index:\", train_text_vec.max(), \"\\n\")\n\nprint('Sample Tweet Before Processing:', train[\"text\"].values[0])\nprint('Sample Tweet After Processing:', tokenizer.sequences_to_texts([train_text_vec[0]]), '\\n')\n\nprint('What the model will interpret:', train_text_vec[0].tolist())","75a48af4":"# One Hot Encode Y values:\nencoder = LabelEncoder()\n\ny_train = encoder.fit_transform(train['sentiment'].values)\ny_train = to_categorical(y_train) \n\ny_test = encoder.fit_transform(test['sentiment'].values)\ny_test = to_categorical(y_test) ","c93b617e":"# get an idea of the distribution of the text values\nfrom collections import Counter\nctr = Counter(train['sentiment'].values)\nprint('Distribution of Classes:', ctr)\n\n# get class weights for the training data, this will be used data\ny_train_int = np.argmax(y_train,axis=1)\ncws = class_weight.compute_class_weight('balanced', np.unique(y_train_int), y_train_int)\nprint(cws)","6f032d87":"from hyperopt import hp, fmin, tpe, hp, STATUS_OK, Trials, space_eval\n\nDROPOUT_CHOICES = np.arange(0.0, 1.0, 0.1)\nUNIT_CHOICES = np.arange(8, 129, 8, dtype=int)\nFILTER_CHOICES = list(range(1, 9, 1))\nEMBED_UNITS = np.arange(32, 513, 32, dtype=int)\nspace = {\n    \n    'spatial_dropout': hp.choice('spatial_dropout', DROPOUT_CHOICES),\n    'embed_units': hp.choice('embed_units', EMBED_UNITS),\n    'conv1_units':  hp.choice('conv1_units', UNIT_CHOICES),\n    'conv1_filters': hp.choice('conv1_filters', FILTER_CHOICES),\n    #nesting the layers ensures they're only un-rolled sequentially\n    'conv2': hp.choice('conv2', [False, {\n        'conv2_units':  hp.choice('conv2_units', UNIT_CHOICES),\n        'conv2_filters': hp.choice('conv2_filters', FILTER_CHOICES),\n        #only make the 3rd layer availabile if the 2nd one is\n        'conv3': hp.choice('conv3', [False, {\n            'conv3_units':  hp.choice('conv3_units', UNIT_CHOICES),\n            'conv3_filters': hp.choice('conv3_filters', FILTER_CHOICES),\n        }]),\n    }]),\n    'dense_units':  hp.choice('dense_units', UNIT_CHOICES),\n    'batch_size':  hp.choice('batch_size', UNIT_CHOICES),\n    'dropout1':  hp.choice('dropout1', DROPOUT_CHOICES),\n    'dropout2':  hp.choice('dropout2', DROPOUT_CHOICES)\n}","008ffd43":"X_train = train_text_vec\nX_test = test_text_vec\n\ndef objective(params, verbose=0, checkpoint_path = 'model.hdf5'):\n    \n    if verbose > 0:\n        print ('Params testing: ', params)\n        print ('\\n ')\n    \n    model = Sequential()\n    model.add(Embedding(input_dim = (len(tokenizer.word_counts) + 1), output_dim = params['embed_units'], input_length = MAX_SEQ_LEN))\n    model.add(SpatialDropout1D(params['spatial_dropout']))\n    model.add(Conv1D(params['conv1_units'], params['conv1_filters']))\n    \n    #layers are hyperparameters and can be excluded\/included dynamically(which is fun)\n    if params['conv2']:\n        model.add(Conv1D(params['conv2']['conv2_units'], params['conv2']['conv2_filters']))\n        \n    if params['conv2'] and params['conv2']['conv3']:\n        model.add(Conv1D(params['conv2']['conv3']['conv3_units'], params['conv2']['conv3']['conv3_filters']))\n    \n    model.add(GlobalMaxPool1D())\n    model.add(Dropout(params['dropout1']))\n    model.add(Dense(params['dense_units'], activation='relu'))\n    model.add(Dropout(params['dropout2']))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    model.fit(\n        X_train, \n        y_train, \n        validation_data=(X_test, y_test),  \n        epochs=8,  #usually train the model for best accuracy, but when dropout is really low, the time to convergence can be excessive\n        batch_size=params['batch_size'],\n        class_weight=cws,\n         #saves the most accurate model, usually you would save the one with the lowest loss\n        callbacks= [\n            ModelCheckpoint(checkpoint_path, monitor='val_acc', verbose=verbose, save_best_only=True),\n            EarlyStopping(patience = 2, verbose=verbose,  monitor='val_acc')\n        ],\n        verbose=verbose\n    ) \n    \n    model.load_weights(checkpoint_path)\n    predictions = model.predict(X_test, verbose=verbose)\n    acc = (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean()\n    return {'loss': -acc, 'status': STATUS_OK}    ","20eb1bfb":"#hidden to clear out the tensorflow warnings\n\nobjective({\n    'spatial_dropout': 0.25,\n    'embed_units': 128,\n    'conv1_units':  64,\n    'conv1_filters': 4,\n    'conv2': {\n        'conv2_units': 32,\n        'conv2_filters': 4,\n        'conv3': {\n            'conv3_units': 16,\n            'conv3_filters': 4        \n        },\n    },\n    'dense_units':  64,\n    'batch_size':  64,\n    'dropout1': 0.0,\n    'dropout2': 0.0,\n}, verbose=2)","a7ccedae":"objective({\n    'spatial_dropout': 0.25,\n    'embed_units': 128,\n    'conv1_units':  64,\n    'conv1_filters': 4,\n    'conv2': {\n        'conv2_units': 32,\n        'conv2_filters': 4,\n        'conv3': {\n            'conv3_units': 16,\n            'conv3_filters': 4        \n        },\n    },\n    'dense_units':  64,\n    'batch_size':  64,\n    'dropout1': 0.0,\n    'dropout2': 0.0,\n}, verbose=2)","74ab2026":"trials = Trials()\nbest = fmin(objective, space, algo=tpe.suggest, trials=trials, max_evals=100, rstate=np.random.RandomState(99))","7db650b0":"space_eval(space, best)","d04623ec":"from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nimport matplotlib.pyplot as plt\n\ny = np.array([ -t['loss'] for t in trials.results])\nx = np.arange(1, len(y) + 1, 1)\n\ny_max = np.max(y)\nbest_y = y[y == y_max]\nbest_y_xs = x[y == y_max]\n\n#just to calculate a locally weighted average\nreg = KNeighborsRegressor()\nreg.fit(x.reshape(-1, 1), y)\npreds = reg.predict(x.reshape(-1, 1))\n\nplt.plot(x, y, 'go', alpha=0.5)\nplt.plot(best_y_xs, best_y, 'ro')\nplt.plot(x, preds, '--')\n\nplt.ylabel('Accuracy')\nplt.xlabel('Iteration')\nplt.title('Accuracy Per Step')\nplt.show()","c9eb84f5":"def extract_params(trials):\n    n_trials = len(trials.trials)\n    keys = trials.vals.keys()\n    data = []\n    conv2_idx = -1\n    conv3_idx = -1\n    for i in range(n_trials):\n        vals = {} \n        conv2_layer_active = (trials.vals['conv2'][i] == 1)\n        if conv2_layer_active: conv2_idx += 1\n\n        conv3_layer_active = (trials.vals['conv3'][conv2_idx] == 1)    \n        if conv2_layer_active and conv3_layer_active: conv3_idx += 1\n\n        for k in keys:\n            if k in ('conv2_units', 'conv2_filters', 'conv3'):\n                if conv2_layer_active:\n                    vals[k] =  trials.vals[k][conv2_idx]  \n            elif k in ('conv3_units', 'conv3_filters'):\n                if conv3_layer_active:\n                    vals[k] =  trials.vals[k][conv3_idx]  \n            else:\n                vals[k] = trials.vals[k][i]\n\n        data.append(space_eval(space, vals))\n\n    for idx, data_dict in enumerate(data):\n        data_dict['accuracy'] = -trials.trials[idx]['result']['loss']\n    \n    return data\n\ndef _flatten(data):\n    new_data = {}\n    for k in data:\n        #there's a more elegant(recursive) way to code this, but not the focus of this project...\n        if k == 'conv2':\n            new_data['conv2'] = int(bool(data['conv2']))\n            conv2_dict = data['conv2'] if data['conv2'] else {}\n            for k_c2 in conv2_dict:\n                if k_c2.startswith('conv2'):\n                     new_data[k_c2] = conv2_dict[k_c2]\n                elif k_c2 == 'conv3':\n                    new_data['conv3'] = int(bool(conv2_dict['conv3']))\n                    conv3_dict = conv2_dict['conv3'] if conv2_dict['conv3'] else {}\n                    for k_c3 in conv3_dict:\n                        if k_c3.startswith('conv3'):\n                             new_data[k_c3] = conv2_dict['conv3'][k_c3]\n        else:\n            new_data[k] = data[k]\n    return new_data\n    \n","47bf5919":"import pandas as pd\ndata = list(map(_flatten, extract_params(trials)))\ndf = pd.DataFrame(list(data))\ndf = df.fillna(0) #missing values occur when the object is not populated","d1b6729e":"corr = df.corr()\ncorr","ca69b340":"matfig = plt.figure(figsize=(8,8))\nplt.matshow(corr, fignum=matfig.number)\nplt.xticks(range(len(corr.columns)), corr.columns, rotation='vertical')\nplt.yticks(range(len(corr.columns)), corr.columns)\nplt.colorbar()\nplt.show()","1a45980f":"def get_mean_by_bin(x, y):\n    return [y[x == x_bin].mean() for x_bin in x]\n\nfor col in df.columns:\n#     reg = KNeighborsRegressor()\n#     reg.fit(df[col].values.reshape(-1, 1), df.accuracy)\n#     preds = reg.predict(np.sort(df[col].values).reshape(-1, 1))\n    x_argsort = np.argsort(df[col].values)\n    x_sorted = df[col].values[x_argsort]\n    y_sorted = df.accuracy.values[x_argsort]\n    \n    y_max = np.max(y_sorted)\n    best_y = y_sorted[y_sorted == y_max]\n    best_y_xs = x_sorted[y_sorted == y_max]\n    \n    plt.plot(df[col], df.accuracy, 'go', alpha=0.5)\n    plt.plot(x_sorted, get_mean_by_bin(x_sorted, y_sorted) , '--')\n    plt.plot(best_y_xs, best_y, 'ro')\n    plt.xlabel(col)\n    plt.ylabel('Accuracy')\n    plt.title('%s vs Accuracy' % (col, ))\n    plt.show()","a3e71a7c":"# Baseline Using \"Common\" Default Parameters\n---\n\n### Based on normal settings for this type of model","fe49c6fb":"# Define Search Space and Search Structure\n---\n## In this case we're going to search over a large possible set of configurations for each value. Additionally, the model will test if a 2nd and 3rd convolutional layer improve or hinder the model and search those layers for optimal settings as well","c79e15e5":"### Trainer that defines model architecture and training parameters.","410be7da":"## Best Parameters Found\n### Would you have picked these???","6ca8225b":"# Conclusion\n\n## Hyperparameter tuning is an effective(and necessary) approach for fine-tuning your model to maximize it's performance. It's imporant to not overfit the validation set when doing so and it's recommend to use K Fold Cross Validation with in the fit\/predict loop (e.g. for each set of HyperParameters run K-Fold CV and then take the mean\/median result as the loss for that set of parameters).\n\n## Additionally, tuning can also be used to help select layers and drive architectural decisions(not just tuning existing layers). \n\n## Also, consider defining custom Loss functions to factor in the complexity of the model(# of layers, # of units, etc) relative to the accuracy adding a form of regularization to the model(similar to Ridge Regression)\n","e4c55366":"# Code to Set-Up the Training Data\n---\n### Note: Some helper functions are hidden for brevity","a5029f19":"# Running HyperOpt with 100 Trials","ca8a3bf4":"# Intro\n---\n## The purpose of this notebook is to implement hyperparameter optimization on a moderately-complex decision space.\n## The model we'll be testing is a Stacked Convolutional Network Applied to a NLP Task(this was chose mostly because of speed of training). The work extends a [previous post](https:\/\/www.kaggle.com\/mkowoods\/deep-learning-lstm-for-tweet-classification), in which the best model was a stacked BiDirectional LSTM(returning sequences) into a Conv1D Layer with a pre-trained Embedding Layer (Glove 300D and Glove Twitter Embeddings Stacked Horizontally).\n## The model trained here performs comparably to that model (slightly lower accuracy 0.707 vs 0.71), with significantly lower complexity.\n\n## More specifically, the model improves ~5% from a Baseline architecture using \"sane default\" parameters(acc: ~0.67).\n\n## TLDR; Use Hyperparameter optimization to validate architectural choices and fine-tune \"good\" models. You get \"free\" accuracy with just a little additional boiler plate code(and server time). Also, take sometime to analyze charts of the search results, it can be interesting to see what parameters are and aren't correlated with your target metric and can help improve decision making on next steps.","2a17ca87":"# Improvement per Iteration\n## Since, we're using hyperopts optimization model(TPE), overtime the algorithm should coalesce on a set of \"optimal\" features. This is opposed to Random Search, which will just explore the space with no particular direction. (In the case when the optimal result is learned early in the training performance can degrade overtime, adding earlystopping would help).","cb046bdc":"# Visualize how different hyper-parameters are correlated to performance?\n\n## Let's take a look to see if there are any clear patterns we can exploit to further improve training"}}