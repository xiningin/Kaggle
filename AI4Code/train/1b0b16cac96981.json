{"cell_type":{"d2dbb148":"code","943e1b39":"code","6b1624bd":"code","cae55554":"code","2238c431":"code","83e1a34e":"code","9bf7b93f":"code","6b97c8ae":"code","b106ffff":"code","b7309958":"code","f858c3dd":"code","f7679241":"code","bcb51666":"code","d272422c":"code","8160a13c":"code","d1587611":"code","87069924":"code","772b6f9f":"code","d60275e1":"code","446a3db1":"code","67dec6ba":"code","db89c8f3":"code","153ff323":"code","7fb3c61e":"code","79972700":"code","c7f9b2d4":"code","f9d96239":"code","303101d7":"code","7823e0da":"code","e0117b71":"code","75d8a55e":"code","c6469c86":"code","ad2a2a9a":"code","b1b3f9e8":"code","58041d07":"code","da5ee2e3":"code","3ac724ce":"code","863034a3":"code","3b60e24c":"code","462dd1c6":"code","25925255":"code","2992f77b":"code","8dcbe246":"code","c2edb849":"code","ef8e82ac":"code","56c641d9":"code","d1dbaba7":"code","daeaa449":"code","4a10264e":"code","b320c9ec":"code","bed6bbb5":"code","753ca0ee":"code","5f056024":"code","ae1a7c10":"code","66af3288":"code","19fc8bba":"code","fec33e9d":"code","50c7f4be":"code","fda0c28b":"code","69c93e81":"code","12a30f2b":"code","04052a6c":"code","c03b957e":"code","0079ec3a":"code","b1e96233":"code","487d6256":"code","8a549359":"code","fea1b54d":"code","b3598443":"code","bcf3b0ab":"code","d322baf2":"code","4659d424":"code","add12219":"code","7833fe4f":"code","206746b0":"markdown","69bfa8d7":"markdown","b40de09a":"markdown","a5854d71":"markdown","0fb23ad8":"markdown","04e34f3d":"markdown","a2c8ea6e":"markdown","20ba4136":"markdown","1e8119f6":"markdown","2e5a73a0":"markdown","29f6317c":"markdown","b35ec2e2":"markdown","e4566fbc":"markdown","946430c0":"markdown","c3b76424":"markdown","7d5e4bd5":"markdown","52306cfc":"markdown","741edc32":"markdown","99db7702":"markdown","fcc6aa6d":"markdown","d9f7449b":"markdown","019a342f":"markdown","613d68d3":"markdown","67960ba9":"markdown","2d4649b4":"markdown","57d5e437":"markdown","a07ef805":"markdown","e66660e1":"markdown","38031d97":"markdown","b1966836":"markdown","64e1a550":"markdown","a7f6c509":"markdown","c9ed9f68":"markdown","57b2c2aa":"markdown","09bdbb33":"markdown","e8ecf7df":"markdown","b05297fe":"markdown","d102e3ca":"markdown","3469f668":"markdown","a7d5667c":"markdown","5ba8af70":"markdown","723a82ca":"markdown","99244d9c":"markdown","4addf05f":"markdown","26525108":"markdown","0fc6552b":"markdown","f1cc465b":"markdown"},"source":{"d2dbb148":"#                                      CONSTANTES\n# =========================================================================================\n\n#Semilla:\nSEED = 333\n\n# Validacion Cruzada Stratificada(n_splits=5):\nfrom sklearn.model_selection import StratifiedKFold\nCV = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)","943e1b39":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndf_variants = pd.read_csv(\"..\/input\/data-c\/training_variants\",index_col='ID', engine='python')\ndf_variants.sample(10, random_state=SEED)","6b1624bd":"# Breve descripcion de las variables \nprint(\"Hay un total de \",df_variants.shape[0], \"casos,\")\nprint(len(df_variants.Gene.unique()), \"valores posbiles de genes,\")\nprint(len(df_variants.Variation.unique()), \"valores posibles de variaciones y \")\nprint(len(df_variants.Class.unique()),  \"clases\")","cae55554":"# Eliminamos Variation pues no aporta mucha informacion (hay casi un valor para cada caso)\ndf_variants = df_variants.drop([\"Variation\"], axis=1)","2238c431":"# Distribucion de cada clase\nimport plotly.express as px\nfig = px.pie(df_variants,names='Class',title='Distribucion de cada Clase')\nfig.update_traces(textposition='inside', textinfo='value+percent+text')\nfig.show()","83e1a34e":"# Habilita que se pueda graficar directamente desde el dataframe\nimport cufflinks as cf\n\n# Agrupa por clase y tipo de gen (descartando el resto de atributos)\ngrouped_df = df_variants.groupby([\"Class\", \"Gene\"]).Class.agg(\"count\")\n\n# Normaliza para obtener la frecuencia relativa\ngrouped_df \/= grouped_df.groupby(level=0).sum(axis=0)\n\n# Desapila para obtener un formato que sea f\u00e1cil de plotear\ngrouped_df = grouped_df.unstack(level=1).fillna(0)\n\ngrouped_df.iplot(kind=\"bar\", barmode=\"stack\", asFigure=True,title=\"Frecuencia Relativa de Genes agrupados por Clase\")","9bf7b93f":"# Separacion variables predictivas 'X' y la clase 'y'\nX = df_variants.drop(columns=['Class'])\ny = df_variants['Class']\n\n# Numerizamos \nfrom sklearn.preprocessing import LabelEncoder\nenc = LabelEncoder()\nX = pd.DataFrame(enc.fit_transform(X))\nX.sample(5,random_state = SEED)\n\n# Separacion training\/test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\n# Breve descripcion de los conjuntos training\/test\nprint(\"X_train \",len(X_train))\nprint(\"X_test \",len(X_test))\nprint(\"y_train \",len(y_train))\nprint(\"y_test \",len(y_test))","6b97c8ae":"# Para crear pipelines de manera sencilla\nfrom imblearn.pipeline import make_pipeline as make_pipeline\n\n# Seleccion de Variables y Estimador\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Binarizado\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Aumento\nfrom imblearn.combine import SMOTETomek\n\n# Transformadores\nfrom sklearn.compose import make_column_transformer,make_column_selector\n\n# Estimador para la seleccion de variables\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Pipeline\ndef create_pipeline(estimator,binarizado = False,seleccion = False,aumento = False,cv=CV):\n    if binarizado:\n        # Binarizado-numerizado. Ignoramos las categorias que quedaron en el conjunto de validacion y no se pudieron reconocer \n        one_hot = OneHotEncoder(sparse=False, handle_unknown='ignore')\n        if seleccion:\n            # Selector de variables con arbol de decision por ser de los mas rapidos y basados en ganancias de informacion\n            feature_selection = RFECV(estimator=DecisionTreeClassifier(random_state=SEED),cv=cv)\n            if aumento:\n                pipeline = make_pipeline(one_hot,feature_selection,SMOTETomek(random_state=SEED),estimator)\n            else:\n                pipeline = make_pipeline(one_hot,feature_selection,estimator)\n        else:\n            if aumento:\n                pipeline = make_pipeline(one_hot,SMOTETomek(random_state=SEED),estimator)\n            else:\n                pipeline = make_pipeline(one_hot,estimator)\n    else:\n        if aumento:\n             pipeline = make_pipeline(SMOTETomek(random_state=SEED),estimator)\n        else:\n             pipeline = make_pipeline(estimator)\n    return pipeline","b106ffff":"# Importamos la metrica principal de evaluacion\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn import metrics\n\n# Dataframe de guardado del test\ndf_results = pd.DataFrame(columns = [\"clf\",\"log_loss\",\"accuracy\",\"f1-macro\",\"ROC\"])\n\n# Funcion de guardado del resultado del test\ndef add_res(clf,name,X_test = X_test):   \n    # Guardamos las predicciones\n    y_predict_proba = clf.predict_proba(X_test)\n    y_pred = clf.predict(X_test)\n    \n    # Guardamos los resultados de las distintas metricas\n    log_loss = metrics.log_loss(y_test,y_predict_proba)\n    acc = metrics.accuracy_score(y_test,y_pred)\n    f1 = metrics.f1_score(y_test, y_pred, average='macro')\n    roc = metrics.roc_auc_score(y_test,y_predict_proba,multi_class='ovr')\n    # Actualizamos el dataframe\n    df_results.loc[len(df_results)]=[name,log_loss,acc,f1,roc] \n    #print(y_pred2)","b7309958":"from sklearn.metrics import log_loss\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\nadd_res(dummy_clf,'zero_R')","f858c3dd":"from sklearn.model_selection import GridSearchCV\nimport plotly.graph_objects as go\nimport plotly.io as pio\nfrom IPython.display import HTML\nimport base64\n\nimport plotly.graph_objects as go\nimport plotly.io as pio\n\ndef plot_scores(param_to_evaluate,scores,df_results,postive=-1):\n    scores_title = \"\"\n    fig = go.Figure()\n    for score in scores:\n        fig.add_trace(go.Scatter(x = df_results[param_to_evaluate], y = postive*df_results[score], name=score,\n                        line_shape='linear'))\n        scores_title = scores_title + score + ';'\n    # Edit the layout\n    fig.update_layout(title=' Scores: '+ scores_title, xaxis_title = param_to_evaluate)\n    fig.show()\n\n# Generacion del GridSearch\ndef generate_gs(pipe,grid_params,cv_g = CV,scoring =[\"neg_log_loss\",\"accuracy\",\"f1_macro\",\"roc_auc_ovr\"]):\n    gs = GridSearchCV(\n        pipe,\n        grid_params,\n        verbose=1,\n        cv =cv_g,\n        n_jobs = -1,\n        scoring = scoring,\n        refit = \"neg_log_loss\"\n    )\n    return gs\n\n## Columnas de guardado para los algortimos \nCOLUMNS = ['mean_fit_time','std_fit_time',\n           'mean_test_neg_log_loss','std_test_neg_log_loss','rank_test_neg_log_loss',\n           'mean_test_accuracy','std_test_accuracy','rank_test_accuracy',\n           'mean_test_f1_macro','std_test_f1_macro','rank_test_f1_macro',\n           'mean_test_roc_auc_ovr','std_test_roc_auc_ovr','rank_test_roc_auc_ovr']\n\n\n# Funcion de guardado de resultados que es un subconjunto de cv_results. \n# Guarda los resultados de los parametros del algoritmo y las metricas que le pasamos como parametro.\ndef save_results(gs,params_to_evaluate,columns=COLUMNS):\n    aux = pd.DataFrame(gs.cv_results_)\n    gs_res = pd.DataFrame()\n    for col in params_to_evaluate:\n        gs_res[col] = aux[col]\n    for col in columns:\n        gs_res[col] = aux[col]\n    return gs_res","f7679241":"# Creamos nuestro clasificador en su forma primitiva\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\n#Creamos un diccionario con los parametros de interes de nuestro estimador y los valores posibles de estos\nknn_params = {\n    # Consideramos los vecinos mas cercanos hasta sqrt(3321)= 57.628 (redondeamos a 60)\n    'kneighborsclassifier__n_neighbors': tuple(range(1,60,2))\n}\n\n# Creamos el pipeline con las opciones de preprocesado a evaluar\npipe_knn = create_pipeline(knn,binarizado = True)\n\n# Creamos el GSCV\ngs_knn = generate_gs(pipe_knn,knn_params)","bcb51666":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 143.5 min - 300 ; 207.3 - 442; \ngs_knn.fit(X_train,y_train)","d272422c":"# Guardamos los parametros del clasificador que nos interesa evaluar\nparams_to_evaluate_knn = ['param_kneighborsclassifier__n_neighbors']\n\n# Extraemos de cv_results los resultados a mostrar\nknn_gs_res = save_results(gs_knn,params_to_evaluate_knn)\n\n# Evaluamos el parametro de los vecinos mas cercanos\nplot_scores('param_kneighborsclassifier__n_neighbors',['mean_test_neg_log_loss'],knn_gs_res)\n\n# Mostramos los 10 mejores resultados\nknn_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","8160a13c":"# Exportamos los resultados\nknn_gs_res.to_csv('KNN.csv')","d1587611":"add_res(gs_knn,'KNN')","87069924":"# Creamos el pipeline con las opciones de preprocesado a evaluar\npipe_knn = create_pipeline(knn,binarizado = True,seleccion = True)\n\n# Creamos el GSCV\ngs_knn = generate_gs(pipe_knn,knn_params)","772b6f9f":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 143.5 min - 300 ; 207.3 - 442; \ngs_knn.fit(X_train,y_train)","d60275e1":"knn_gs_res = save_results(gs_knn,params_to_evaluate_knn)\n\n# Evaluamos el parametro de los vecinos mas cercanos\nplot_scores('param_kneighborsclassifier__n_neighbors',['mean_test_neg_log_loss'],knn_gs_res)\n\nknn_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","446a3db1":"# Exportamos los resultados\nknn_gs_res.to_csv('KNN-sel.csv')","67dec6ba":"add_res(gs_knn,'KNN-sel')","db89c8f3":"pipe_knn = create_pipeline(knn,binarizado = True,aumento = True)\ngs_knn = generate_gs(pipe_knn,knn_params)","153ff323":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 143.5 min - 300 ; 207.3 - 442; \ngs_knn.fit(X_train,y_train)","7fb3c61e":"knn_gs_res = save_results(gs_knn,params_to_evaluate_knn)\n\n# Evaluamos el parametro de los vecinos mas cercanos\nplot_scores('param_kneighborsclassifier__n_neighbors',['mean_test_neg_log_loss'],knn_gs_res)\n\nknn_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","79972700":"# Exportamos los resultados\nknn_gs_res.to_csv('KNN-aum.csv')","c7f9b2d4":"add_res(gs_knn,'KNN-aum')","f9d96239":"pipe_knn = create_pipeline(knn,binarizado = True,seleccion = True,aumento = True)\ngs_knn = generate_gs(pipe_knn,knn_params)","303101d7":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 143.5 min - 300 ; 207.3 - 442; \ngs_knn.fit(X_train,y_train)","7823e0da":"knn_gs_res = save_results(gs_knn,params_to_evaluate_knn)\n\n# Evaluamos el parametro de los vecinos mas cercanos\nplot_scores('param_kneighborsclassifier__n_neighbors',['mean_test_neg_log_loss'],knn_gs_res)\n\nknn_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","e0117b71":"# Exportamos los resultados\nknn_gs_res.to_csv('KNN-aum+sel.csv')","75d8a55e":"add_res(gs_knn,'KNN-sel+aum')","c6469c86":"# Debemos usar el naive bayes categorico que aprenda las probabilidades a priori de las clases\nfrom sklearn.naive_bayes import CategoricalNB\ncat_NB = CategoricalNB(fit_prior=True)\n\n#Creamos un diccionario con los parametros de nuestro estimador y los valores posibles de estos\nNB_grid_params = {\n    # Probamos con y sin suavizado de Laplace\n    'categoricalnb__alpha': (0.0,1.0)\n}\n# Creamos el pipeline y el gridsearch con las opciones de preprocesado y los parametros del clasificador a evaluar\npipe_NB = create_pipeline(cat_NB)\ngs_NB = generate_gs(pipe_NB,NB_grid_params)","ad2a2a9a":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 207.3 - 442; \ngs_NB.fit(X_train,y_train)","b1b3f9e8":"# Mostramos los resultados del entrenamiento\nparams_to_evaluate_NB = ['param_categoricalnb__alpha']\nNB_gs_res = save_results(gs_NB,params_to_evaluate_NB)\nNB_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(5)","58041d07":"# Exportamos resultados \nNB_gs_res.to_csv('NB.csv')","da5ee2e3":"add_res(gs_NB,'NB')","3ac724ce":"# Ahora no tiene sentido aprendes las probabilidades a priori pues todas las clases tienen la misma ocurrencia\ncat_NB = CategoricalNB(fit_prior=False)\n\npipe_NB = create_pipeline(cat_NB,aumento = True)\ngs_NB = generate_gs(pipe_NB,NB_grid_params)","863034a3":"# 19.9 - 42 ; 89.3 - 192 ; 115 min - 245 task; 207.3 - 442; \ngs_NB.fit(X_train,y_train)","3b60e24c":"# Mostramos los resultados del entrenamiento\nNB_gs_res = save_results(gs_NB,params_to_evaluate_NB)\nNB_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","462dd1c6":"# Exportamos resultados \nNB_gs_res.to_csv('NB-aum.csv')","25925255":"add_res(gs_NB,'NB-aum')","2992f77b":"# Creamos el clasificador en su forma primitiva\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(random_state=SEED,ccp_alpha = 0.01)\n\n#Creamos un diccionario con los parametros de nuestro estimador y los valores posibles de estos\ntree_grid_params = {\n    'decisiontreeclassifier__criterion': ['gini','entropy']\n}\n\n# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree,binarizado = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","8dcbe246":"gs_tree.fit(X_train,y_train)","c2edb849":"params_to_evaluate_tree = ['param_decisiontreeclassifier__criterion']\nTree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","ef8e82ac":"# Exportamos resultados \nTree_gs_res.to_csv('Tree.csv')","56c641d9":"add_res(gs_tree,'tree')","d1dbaba7":"# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree,binarizado = True,aumento = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","daeaa449":"gs_tree.fit(X_train,y_train)","4a10264e":"Tree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","b320c9ec":"# Exportamos resultados \nTree_gs_res.to_csv('tree-aumento.csv')","bed6bbb5":"add_res(gs_tree,'tree-aum')","753ca0ee":"# Creamos el arbol modificado para que haga un balanceo\ntree_balance = DecisionTreeClassifier(random_state = SEED,class_weight = 'balanced',ccp_alpha = 0.01)\n\n# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree_balance,binarizado = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","5f056024":"gs_tree.fit(X_train,y_train)","ae1a7c10":"Tree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","66af3288":"# Exportamos resultados \nTree_gs_res.to_csv('tree-balanced.csv')","19fc8bba":"add_res(gs_tree,'tree-balanced')","fec33e9d":"# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree,binarizado = True,seleccion = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","50c7f4be":"gs_tree.fit(X_train,y_train)","fda0c28b":"Tree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","69c93e81":"# Exportamos resultados \nTree_gs_res.to_csv('tree-sel.csv')","12a30f2b":"add_res(gs_tree,'tree-sel')","04052a6c":"# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree,binarizado = True,seleccion = True,aumento = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","c03b957e":"gs_tree.fit(X_train,y_train)","0079ec3a":"Tree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","b1e96233":"# Exportamos resultados \nTree_gs_res.to_csv('tree-sel+aum.csv')","487d6256":"add_res(gs_tree,'tree-sel+aum')","8a549359":"# Creamos el pipeline y la busqueda exhaustiva\npipe_tree = create_pipeline(tree_balance,binarizado = True,seleccion = True)\ngs_tree = generate_gs(pipe_tree,tree_grid_params)","fea1b54d":"gs_tree.fit(X_train,y_train)","b3598443":"Tree_gs_res = save_results(gs_tree,params_to_evaluate_tree)\nTree_gs_res.sort_values(by='mean_test_neg_log_loss',ascending=False).head(10)","bcf3b0ab":"# Exportamos resultados \nTree_gs_res.to_csv('tree-sel+balance.csv')","d322baf2":"add_res(gs_tree,'tree-sel+balance')","4659d424":"# Exportamos resultados\ndf_results = df_results.sort_values(by='log_loss').reset_index(drop=True)\ndf_results.to_csv('results.csv')\ndf_results","add12219":"# Eliminamos el zero R para que este bien escalado la grafica\nfig = px.bar(df_results.drop([len(df_results)-1]), x='clf', y='log_loss')\nfig.update_traces(marker_color='red')\nfig.update_layout(title_text='log_loss scores: ')\nfig.show()","7833fe4f":"# Seleccionamos la instacia con el mejor log_loss\nnb = df_results.iloc[0]\n\n# Seleccionamos los valores a mostrar\nx = ['accuracy','f1-macro','ROC']\ny = [nb['accuracy'],nb['f1-macro'],nb['ROC']]\n\n# Pintamos\nfig = go.Figure([go.Bar(x=x, y=y)])\nfig.update_layout(title_text='Scores complementarios: '+ nb['clf'])\nfig.show()","206746b0":"#### 4.4.1 \u00c1rbol de Decisi\u00f3n","69bfa8d7":"#### 5.2.3 KNN Aumento","b40de09a":"TESTING","a5854d71":"TESTING","0fb23ad8":"### Preparaci\u00f3n del Pipeline\n\nSe he construido lo m\u00e1s flexible posible, siendo su versi\u00f3n por defecto la clasificaci\u00f3n directa. Luego ir\u00edamos a\u00f1adiendo las distintas t\u00e9cnicas: binarizado,selecci\u00f3n y aumento sint\u00e9tico.","04e34f3d":"**Preparaci\u00f3n del Dataframe**","a2c8ea6e":"TESTING","20ba4136":"## 1. Constantes Globales\n\nAqu\u00ed definimos algunas constantes necesarias para la ejecuci\u00f3n de nuestros m\u00e9todos. Por una lado, tenemos la semilla, que es indispensable para la reproducibilidad de los resultados, un generador de validaci\u00f3n cruzada, para evitar el componente aleatorio de la divisi\u00f3n del conjunto de datos en la validaci\u00f3n.","1e8119f6":"## 4. Preprocesado\n\nPara el preprocesamiento, hemos usado las t\u00e9cnicas de Numerizado, Binarizado, Selecci\u00f3n de Variables y Aumento de instancias para el balanceo. Para ello, hemos usado pipelines ya que evitan la fuga de datos.","2e5a73a0":"#### 4.4.2 \u00c1rbol de Decisi\u00f3n Aumento","29f6317c":"TESTING","b35ec2e2":"**Frecuencia Relativa de cada gen en cada clase**\n\nEn lugar de tener 9 histogramas, tenemos uno solo donde cada barra nos muestra la frecuencia de cada gen en esa clase.","e4566fbc":"TESTING","946430c0":"TESTING","c3b76424":"## 2. Carga de Datos\nEn esta libreta solo vamos a utilizar el conjunto de datos 'Variants', es decir, solo con las variables predictoras 'Gene' y 'Variation'; y la variable objetivo.","7d5e4bd5":"#### 4.4.3 \u00c1rbol de Decisi\u00f3n Balanceo mediante pesado de las Clases","52306cfc":"TESTING","741edc32":"## 5. Conclusiones\n\nhttps:\/\/towardsdatascience.com\/machine-learning-classifiers-comparison-with-python-33149aecdbca","99db7702":"TESTING","fcc6aa6d":"### 5.2 KNN\n\nPara el correcto funcionamiento del KNN con atributos categ\u00f3ricos, es necesario realizar una codificaci\u00f3n one-hot ya que utiliza medidas de distancia (Euclidea por defecto) para atributos num\u00e9ricos. Entonces el contraste ser\u00eda con la selecci\u00f3n de variables y el aumento sint\u00e9tico. Respecto a los par\u00e1metros a evaluar, consideramos que el n\u00famero de vecinos es el m\u00e1s importante a evaluar. Probamos con n_vecinos hasta sqrt(3321).","d9f7449b":"**NUMERIZADO**\n\nAlgunos algoritmos no funcionan bien sin el numerizado, por tanto, en casos donde no queramos aplicar el binarizado, debemos dejar esta opcion por defecto. No hay ning\u00fan problema pues solo transformamos string a entero, lo cu\u00e1l no produce fuga de datos. En caso de ponerlo dentro del pipeline, al no poder poner handle_unknown = 'ignore' que pon\u00edamos en el OneHotEncoder, en el testing, no reconoce algunas categorias y salta error. ","019a342f":"RESULTADOS DE LOS MODELOS CON LA M\u00c9TRICA PRINCIPAL: LOG_LOSS","613d68d3":"M\u00c9TRICAS COMPLEMENTARIAS DEL MEJOR MODELO","67960ba9":"TESTING","2d4649b4":"#### 4.4.6 \u00c1rbol de Decisi\u00f3n Binarizado + Selecci\u00f3n + Balanceo mediante pesado de Clases","57d5e437":"### 5.1 Preparando el GridSearchCV","a07ef805":"#### 5.2.4 KNN Selecci\u00f3n + Aumento","e66660e1":"#### 5.2.1 KNN ","38031d97":"#### 5.2.2 KNN Selecci\u00f3n","b1966836":"## 3. Exploraci\u00f3n","64e1a550":"# \u00cdndice\n1. Constantes Globales\n2. Carga de Datos\n3. Exploraci\u00f3n\n4. Preprocesado\n5. Clasificaci\u00f3n sin Texto\n              5.1 KNN\n              5.2 NB\n              5.3 Arbol de Desicion","a7f6c509":"https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel","c9ed9f68":"TESTING","57b2c2aa":"### 5.3 NB\n\nEste clasificador tiene poco juego de par\u00e1metros, por una lado tenemos si hacer o no suavizado de Laplace (alpha), y por otro,solo en el caso de no aumentar, si aprender o no de la probabilidad 'a priori' de las clases basado en su aparici\u00f3n en el conjunto de datos (fit_prior).","09bdbb33":"#### 5.3.2 NB Aumento","e8ecf7df":"TESTING","b05297fe":"Observamos que en algunas clases existen genes claramente predomintantes:\n\n1.     La clase 5 tiene el gen BRCA1 muy frecuente con diferencia(casi el 40% de los casos). Le sigue el BRCA2 con una frecuencia de casi el 10%\n2.     En la clase 6 el BRCA2 aparece el 30% de los casos y el BRCA1 el 20% aproximadamente. Lo que significa que entre los dos ocupan la mitad de los casos.\n3.     En la clase 9 tenemos el gen SF3B1 con una frecuencia del 40% de los casos.","d102e3ca":"#### 5.3.1 NB ","3469f668":"Vemos que se trata de un problema no balanceado, pues hay clases predominantes como son la 7 (casi el 30%),seguida por la 4 (21%), la 1 (17%) y la 2 (14%). Mientras que hay otras bastante infrarepresentadas como son la 8 y la 9 (un 1% las dos).","a7d5667c":"## 5. Clasificaci\u00f3n sin Texto\n\nPasamos a clasificar. Para saber cuales par\u00e1metros de nuestros algoritmos son mejores hemos realizado una b\u00fasqueda exhaustiva usando la librer\u00eda 'gridsearchcv' de python, donde entrena y eval\u00faa cada combinaci\u00f3n posible de estos. Al no ser posible evaluar todos los casos de cada algoritmo, hemos elegido aquellos par\u00e1metros que creemos que m\u00e1s van a influir en la clasificaci\u00f3n. Despu\u00e9s del entrenamiento-validaci\u00f3n, pasamos a guardar los resultados de cada algoritmo en un dataframe, para representarlos en una funci\u00f3n para su posterior an\u00e1lisis en las conclusiones.\n\nPor otro lado, tenemos las distintas t\u00e9cnicas de preprocesado, las cuales iremos contrastando para analizar cuales de \u00e9stas dan los mejores resultados.\n\nFinalmente, acabar\u00edamos con las conclusiones, donde expondremos cuales son los mejores algoritmos y t\u00e9cnicas de preprocesado.\n\nClasificadores a Utilizar: \n\n0. Zero R\n1. KNN\n2. NB\n3. Arbol de Desicion","5ba8af70":"TESTING","723a82ca":"Distribuci\u00f3n de las Clases","99244d9c":"#### 4.4.4 \u00c1rbol de Decisi\u00f3n Selecci\u00f3n","4addf05f":"### 5.0 Zero R","26525108":"TESTING","0fc6552b":"#### 4.4.5 \u00c1rbol de Decisi\u00f3n Selecci\u00f3n + Aumento","f1cc465b":"### 4.4 \u00c1rbol de Decisi\u00f3n\n\nPara este clasificador, hemos hecho una peque\u00f1a poda (ccp_alpha = 0.01) para evitar el sobreajuste. Tambi\u00e9n, para contrastar con el aumento sint\u00e9tico, hemos modificado el algortimo en algunos casos para que realice un aumento impl\u00edcito (class_weight = 'balanced')."}}