{"cell_type":{"21260be8":"code","e6b33e64":"code","1069bd86":"code","0f01dfab":"code","f39fcdb3":"code","7ae4b080":"code","6e203d5c":"code","ed3468b4":"code","2a7b9103":"code","0a121754":"code","882356fa":"code","c65d035a":"code","3ca63ee9":"code","a0f42b63":"code","a0d553cc":"code","89f4f4e6":"code","6688a9fa":"code","ac8e4014":"code","8a2a3140":"code","84e85671":"code","f9bcba5c":"code","a5bbe1c1":"code","0493b55a":"code","a81814cc":"code","4b203925":"code","689cf13d":"code","4db960b1":"code","0630b39c":"code","1a77e268":"code","6d412a07":"code","5d0781d5":"code","ba04b99a":"code","97680c58":"code","3f4daccf":"code","f17645ad":"code","7ad7b8e4":"code","c919a2bd":"code","8d2076a9":"code","8e2a8cde":"code","769b77c8":"code","dc1a6fb6":"code","a12962cf":"markdown","be5be272":"markdown","7ad748a4":"markdown","2889571d":"markdown","e0523438":"markdown","0de901af":"markdown","34487af8":"markdown","43cf2eda":"markdown","75351e64":"markdown","bcf3a033":"markdown","3b651368":"markdown","b78f4a7e":"markdown","916e0e14":"markdown"},"source":{"21260be8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6b33e64":"import matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, LSTM, Embedding, Input, Conv1D, MaxPooling1D\nfrom keras.layers.merge import concatenate\nfrom keras.layers import Dropout\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import EarlyStopping\n\nfrom gensim.models import KeyedVectors\nfrom gensim.models import Word2Vec\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport time\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","1069bd86":"# load google's pre-trained word2vec embeddings\nfilename = \"\/kaggle\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin\"\n\nstart = time.time()\ngoogle_embeddings = KeyedVectors.load_word2vec_format(filename, binary=True)\n\nprint(\"Load time (seconds): \", (time.time() - start))","0f01dfab":"# load Stanford's pre-trained GloVe embeddings\nglove_file = \"\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.300d.txt\"\nglove_word2vec_file = \"glove.6B.300d.txt.word2vec\"\n\nglove2word2vec(glove_file, glove_word2vec_file)","f39fcdb3":"# glove embeddings\nstart = time.time()\n\nglove_embeddings = KeyedVectors.load_word2vec_format(glove_word2vec_file, binary=False)\n\nprint(\"Load time (seconds): \", (time.time() - start))","7ae4b080":"# load data\ndf = pd.read_csv(\"\/kaggle\/input\/enron-email-classification-using-machine-learning\/preprocessed.csv\")\n\n# view first 5 rows of the dataframe 'df'\ndf.head()","6e203d5c":"no_words_arr = []\nfor text in df['text']:\n    no_words = len(text.split())\n    no_words_arr.append(no_words)\n    \ndf['no_words'] = no_words_arr","ed3468b4":"df[df['no_words'] < 100].shape","2a7b9103":"# shape of the data\ndf.shape","0a121754":"def label_encoder(df):\n    class_le = LabelEncoder()\n    # apply label encoder on the 'X-Folder' column\n    y = class_le.fit_transform(df['X-Folder'])\n    return y","882356fa":"y = label_encoder(df)\ncorpus = df['text']","c65d035a":"# split the data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.1, random_state=0)","3ca63ee9":"all_words = []\n\nfor sent in corpus:\n    tokenize_word = word_tokenize(sent)\n    for word in tokenize_word:\n        all_words.append(word)","a0f42b63":"# retrieve all unique words from all_words\nunique_words = set(all_words)\nprint(\"Unique words: \",len(unique_words))","a0d553cc":"# prepare tokenizer\nt = Tokenizer()\n\n# fit the tokenizer on the docs\nt.fit_on_texts(corpus)\n\n# integer encode the documents\ntrain_encoded_docs = t.texts_to_sequences(X_train)\ntest_encoded_docs = t.texts_to_sequences(X_test)","89f4f4e6":"# find the largest doc to make all the docs of uniform size i.e size of largest doc\nword_count = lambda doc: len(word_tokenize(doc))\nlongest_doc = max(corpus, key=word_count)\nlength_longest_doc = len(word_tokenize(longest_doc))\nlength_longest_doc = 100","6688a9fa":"# to make all the docs of equal size, we will add zeros to empty indexes\ntrain_padded_docs = pad_sequences(train_encoded_docs, length_longest_doc, padding='post')\ntest_padded_docs = pad_sequences(test_encoded_docs, length_longest_doc, padding='post')","ac8e4014":"# one-hot encode the output labels\nY_train = to_categorical(y_train, 20)\nY_test = to_categorical(y_test, 20)","8a2a3140":"docs = []\n\nfor doc in corpus:\n    li = list(doc.split())\n    docs.append(li)","84e85671":"start = time.time()\n# train the model\nmodel = Word2Vec(docs, size=300, window=5, min_count=1, workers=4, sg=0)\n# summarize the loaded model\nprint(model)\n# save the model\nmodel.save(\"email_embeddings.bin\")\n\nprint(\"Training time (seconds): \", (time.time() - start))","f9bcba5c":"# load own word embeddings\nstart = time.time()\n\nfilename = \"email_embeddings.bin\"\n\nemail_embeddings = Word2Vec.load(filename)\n\nprint(\"Load time (seconds): \", (time.time() - start))","a5bbe1c1":"vocab_size = len(email_embeddings.wv.vocab)","0493b55a":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 300))\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    else:\n        count += 1","a81814cc":"embedding_matrix.shape","4b203925":"print(\"Number of words not present in email_embeddings: \", count)","689cf13d":"# define the model\nemail_model = Sequential()\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=length_longest_doc, trainable=False)\nemail_model.add(e)\nemail_model.add(LSTM(100, dropout=0.4))\nemail_model.add(Flatten())\nemail_model.add(Dense(20, activation='softmax'))\n\n\n# compile the model\nemail_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# print model summary\nemail_model.summary()","4db960b1":"start = time.time()\n\n# train the model\nemail_hist = email_model.fit(train_padded_docs, Y_train, epochs=100, verbose=1, validation_split=0.1)\n\nprint(\"Training time (minutes): \", (round((time.time() - start)\/60, 2)))","0630b39c":"# evaluate the model\nemail_train_eval = email_model.evaluate(train_padded_docs, Y_train, verbose=0)\nemail_test_eval = email_model.evaluate(test_padded_docs, Y_test, verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(email_train_eval[1], email_train_eval[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(email_test_eval[1], email_test_eval[0]))","1a77e268":"fig, axis = plt.subplots(1,2, figsize=(12, 6))\n# plot the loss\naxis[0].set_title(\"Loss\")\naxis[0].plot(email_hist.history['loss'], label='train')\naxis[0].plot(email_hist.history['val_loss'], label='validation')\naxis[0].legend()\naxis[0].set_xlabel('Epochs')\naxis[0].set_ylabel(\"Loss\")\n\n# plot the accuracy\naxis[1].set_title(\"Accuracy\")\naxis[1].plot(email_hist.history['accuracy'], label='train')\naxis[1].plot(email_hist.history['val_accuracy'], label='validation')\naxis[1].legend()\naxis[1].set_xlabel('Epochs')\naxis[1].set_ylabel(\"Accuracy\")\n\nplt.show()","6d412a07":"# create embedding matrix\nembedding_matrix = np.zeros((vocab_size, 300))\n\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in google_embeddings.wv.vocab.keys():\n        embedding_vector = google_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    elif word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    else: \n        count += 1","5d0781d5":"embedding_matrix.shape","ba04b99a":"# define the model\nw2v_model = Sequential()\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=length_longest_doc, trainable=False)\nw2v_model.add(e)\nw2v_model.add(LSTM(100, dropout=0.4))\nw2v_model.add(Flatten())\nw2v_model.add(Dense(20, activation='softmax'))\n\n# compile the model\nw2v_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# print model summary\nw2v_model.summary()","97680c58":"start = time.time()\n\n# train the model\nw2v_hist = w2v_model.fit(train_padded_docs, Y_train, epochs=100, verbose=1, validation_split=0.1)\n\nprint(\"Training time (minutes): \", (round((time.time() - start)\/60, 2)))","3f4daccf":"# evaluate the model\nw2v_train_eval = w2v_model.evaluate(train_padded_docs, Y_train, verbose=0)\nw2v_test_eval = w2v_model.evaluate(test_padded_docs, Y_test, verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(w2v_train_eval[1], w2v_train_eval[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(w2v_test_eval[1], w2v_test_eval[0]))","f17645ad":"fig, axis = plt.subplots(1,2, figsize=(12, 6))\n# plot the loss\naxis[0].set_title(\"Loss\")\naxis[0].plot(w2v_hist.history['loss'], label='train')\naxis[0].plot(w2v_hist.history['val_loss'], label='validation')\naxis[0].legend()\naxis[0].set_xlabel('Epochs')\naxis[0].set_ylabel(\"Loss\")\n\n# plot the accuracy\naxis[1].set_title(\"Accuracy\")\naxis[1].plot(w2v_hist.history['accuracy'], label='train')\naxis[1].plot(w2v_hist.history['val_accuracy'], label='validation')\naxis[1].legend()\naxis[1].set_xlabel('Epochs')\naxis[1].set_ylabel(\"Accuracy\")\n\nplt.show()","7ad7b8e4":"# create embedding matrix\nembedding_matrix = np.zeros((vocab_size, 300))\n\ncount = 0\n\nfor word, i in t.word_index.items():\n    if word in glove_embeddings.wv.vocab.keys():\n        embedding_vector = glove_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    elif word in email_embeddings.wv.vocab.keys():\n        embedding_vector = email_embeddings[word]\n        \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            \n    else: \n        count += 1","c919a2bd":"\n# define the model\nglove_model = Sequential()\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=100, trainable=False)\nglove_model.add(e)\nglove_model.add(LSTM(100, dropout=0.4))\nglove_model.add(Flatten())\nglove_model.add(Dense(20, activation='softmax'))\n\n# compile the model\nglove_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# print model summary\nglove_model.summary()","8d2076a9":"start = time.time()\n# train the model\nhist = glove_model.fit(train_padded_docs, Y_train, epochs=100, validation_split=0.1)\n\nprint(\"Training time (minutes): \",(round((time.time()-start)\/60, 2)))","8e2a8cde":"# evaluate the model\ntrain_accr = glove_model.evaluate(train_padded_docs, Y_train, verbose=0)\ntest_accr = glove_model.evaluate(test_padded_docs, Y_test, verbose=0)\n\nprint(\"Train Accuracy: {:0.3f}    Loss: {:0.3f}\".format(train_accr[1], train_accr[0]))\nprint(\"Test Accuracy:  {:0.3f}    Loss: {:0.3f}\".format(test_accr[1], test_accr[0]))","769b77c8":"# plot the loss\nplt.title(\"Loss\")\nplt.plot(hist.history['loss'], label='train')\nplt.plot(hist.history['val_loss'], label='validation')\nplt.legend()\nplt.show()","dc1a6fb6":"# plot the accuracy\nplt.title(\"Accuracy\")\nplt.plot(hist.history['accuracy'], label='train')\nplt.plot(hist.history['val_accuracy'], label='validation')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel(\"Accuracy\")\nplt.show()","a12962cf":"## 1. Import necessary libraries","be5be272":"## 2. Load Pre-trained word embeddings","7ad748a4":"## 5. Learn Own Word Embeddings","2889571d":"#### Encode class labels","e0523438":"## 7. Using Word2Vec","0de901af":"#### 2.2 Stanford's Word Embedding","34487af8":"## 3. Load Data","43cf2eda":"## 4. Prepare Data","75351e64":"**Find total number of words in our corpus**","bcf3a033":"## 6. Email Classification using Email Word Embeddings","3b651368":"## Enron Email Classification using Word Embeddings and LSTM","b78f4a7e":"## 8. Using GloVe ","916e0e14":"#### 2.1 Google's Word Embedding"}}