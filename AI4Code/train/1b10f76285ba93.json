{"cell_type":{"f973ff72":"code","f89603ab":"code","52e91d19":"code","65621f07":"code","3e75eecd":"code","393d8f2f":"code","c474c117":"code","7ce0d1b4":"code","930e2ea9":"code","dfd139c5":"code","83c11f70":"code","e83daad3":"code","38902228":"code","0d198fe0":"code","cda1f22d":"code","44bb4787":"code","40d36baa":"code","81062b5c":"code","ed560ebe":"code","7c2f6165":"code","db37c874":"code","287cec2c":"code","0135cad9":"code","5a276b7c":"code","7d1a3510":"code","64e90ebd":"code","08815320":"code","22c3cb96":"code","e4daa642":"code","323c7104":"code","3f163d99":"code","c2c6c870":"code","f7d160e5":"code","fa81a435":"code","b6f4c12a":"code","263dba3a":"code","39e03cc9":"code","0e873876":"code","da2f9f2c":"code","c7a7fac7":"code","fabe4bdd":"code","dda815cd":"code","87beadce":"markdown","aa59c8db":"markdown","025e3840":"markdown","91301c19":"markdown","1da7355e":"markdown","fb2b20bd":"markdown","693f8991":"markdown","28976872":"markdown","d9fbecdd":"markdown","319199e5":"markdown","3fad4c33":"markdown","f6f5f3fd":"markdown","13b4411a":"markdown","71a77bdf":"markdown","72624997":"markdown"},"source":{"f973ff72":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","f89603ab":"import numpy as np \nimport pandas as pd\nimport spacy\nimport seaborn as sns\nimport string","52e91d19":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","65621f07":"nlp = spacy.load(\"en_core_web_sm\")\n","3e75eecd":"#READING INPUT\ndata = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/train.csv\")\ndata.head()","393d8f2f":"test = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/test.csv\")","c474c117":"doc = nlp(data[\"text\"][0])\nfor token in doc[0:5]:\n    print(token.text, token.pos , token.pos_, token.dep_) # part of speach and syntax dependency","7ce0d1b4":"for token in doc[0:5]:\n    print(token.text,  token.pos_, token.lemma_) # part of speach and syntax dependency","930e2ea9":"sns.barplot(x=['Edgar Allen Poe', 'Mary Wollstonecraft Shelley', 'H.P. Lovecraft'], y=data['author'].value_counts())","dfd139c5":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})\ndata.head()","83c11f70":"from nltk.corpus import stopwords\nstopwords = stopwords.words('english')\nprint (stopwords)","e83daad3":"## Number of words in the text ##\ndata[\"num_words\"] = data[\"text\"].apply(lambda x: len(str(x).split()))\ntest[\"num_words\"] = test[\"text\"].apply(lambda x: len(str(x).split()))\n## Number of unique words in the text ##\ndata[\"num_unique_words\"] = data[\"text\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n## Number of characters in the text ##\ndata[\"num_chars\"] = data[\"text\"].apply(lambda x: len(str(x)))\ntest[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n## Number of stopwords in the text ##\ndata[\"num_stopwords\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\ntest[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords]))\n## Number of punctuations in the text ##\ndata[\"num_punctuations\"] =data['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n## Number of title case words in the text ##\ndata[\"num_words_upper\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n## Number of title case words in the text ##\ndata[\"num_words_title\"] = data[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n## Max length of the words in the text ##\ndata[\"max_word_len\"] = data[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))\ntest[\"max_word_len\"] = test[\"text\"].apply(lambda x: np.max([len(w) for w in str(x).split()]))","38902228":"\n\n# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\ndef cleanup_text(docs, logging=False):\n    texts = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n        tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n        #tokens = [tok for tok in tokens if tok not in punctuations]\n        tokens = ' '.join(tokens)\n        texts.append(tokens)\n    return pd.Series(texts)","0d198fe0":"print('Original training data shape: ', data['text'].shape)\ndata[\"text_cleaned\"]= cleanup_text(data['text'], logging=True)\nprint('Cleaned up training data shape: ', data[\"text_cleaned\"].shape)","cda1f22d":"print('Original training data shape: ', test['text'].shape)\ntest[\"text_cleaned\"] = cleanup_text(test['text'], logging=True)\nprint('Cleaned up training data shape: ', test[\"text_cleaned\"].shape)","44bb4787":"\ndata[\"num_unique_words_clenaed\"] = data[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))\ntest[\"num_unique_words_cleaned\"] = test[\"text_cleaned\"].apply(lambda x: len(set(str(x).split())))","40d36baa":"def numberOfADV(docs, logging=False):\n    numberOfADV = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADP']\n        #tokens = [tok for tok in tokens if tok not in stopwords and tok not in string.punctuation]\n        #tokens = [tok for tok in tokens if tok not in punctuations]\n        #tokens = ' '.join(tokens)\n        \n        numberOfADV.append(len(tokens))\n    return pd.Series(numberOfADV)","81062b5c":"data[\"num_of_ADV\"] = numberOfADV(data['text_cleaned'], logging=True)","ed560ebe":"test[\"num_of_ADV\"] = numberOfADV(test['text_cleaned'], logging=True)","7c2f6165":"def numberOfADJ(docs, logging=False):\n    numberOfADV = []\n    counter = 1\n    for doc in docs:\n        if counter % 1000 == 0 and logging:\n            print(\"Processed %d out of %d documents.\" % (counter, len(docs)))\n        counter += 1\n        doc = nlp(doc, disable=['parser', 'ner'])\n        tokens = [tok.lemma_.lower().strip() for tok in doc if tok.pos_ == 'ADJ']\n        \n        numberOfADV.append(len(tokens))\n    return pd.Series(numberOfADV)","db37c874":"data[\"num_of_ADJ\"] = numberOfADJ(data['text_cleaned'], logging=True)","287cec2c":"test[\"num_of_ADJ\"] = numberOfADJ(test['text_cleaned'], logging=True)","0135cad9":"data.head()","5a276b7c":"test.head()","7d1a3510":"sns.barplot(x= data[\"author\"], y = data[\"num_of_ADJ\"])","64e90ebd":"sns.barplot(x= data[\"author\"], y = data[\"num_of_ADV\"])","08815320":"from sklearn.feature_extraction.text import CountVectorizer","22c3cb96":"vect = CountVectorizer()","e4daa642":"X_train_matrix = vect.fit_transform(data[\"text_cleaned\"]) \nX_test_matrix = vect.transform(test[\"text_cleaned\"]) ","323c7104":"features = vect.get_feature_names()\ndf_X_train_matrix = pd.DataFrame(X_train_matrix.toarray(), columns=features)\ndf_X_train_matrix.head()\n","3f163d99":"df_X_test_matrix = pd.DataFrame(X_test_matrix.toarray(), columns=features)\ndf_X_test_matrix.head()","c2c6c870":"data_df = data.drop([\"id\",\"text\", \"text_cleaned\", \"author\"], axis = 1)\n\ndf_train = pd.concat([data_df, df_X_train_matrix], axis=1)\n\ntest_df = test.drop([\"id\",\"text\", \"text_cleaned\"], axis = 1)\n\ndf_test = pd.concat([test_df, df_X_test_matrix], axis=1)","f7d160e5":"df_train.head()","fa81a435":"X = df_train.drop(\"author_num\", axis = 1)\ny = data['author_num']","b6f4c12a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify = y)","263dba3a":"from sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\n\nprint (clf.score(X_test, y_test))","39e03cc9":"predicted_result=clf.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","0e873876":"predictions = clf.predict_proba(X_test)\n\nprint (\"logloss: %0.3f \" % multiclass_logloss(y_test, predictions))","da2f9f2c":"sample = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/sample_submission.csv\")\nsample.head()","c7a7fac7":"\npredicted_result = clf.predict_proba(df_test)","fabe4bdd":"result=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()","dda815cd":"result.to_csv(\"submission_v3.csv\", index=False)","87beadce":"# Meta features\n\n* features that are extracted from the text like number of words, number of stop words, number of punctuations etc Number of words in the text\n* Number of unique words in the text\n* Number of characters in the text\n* Number of stopwords\n* Number of punctuations\n* Number of upper case words\n* Number of title case words\n","aa59c8db":"# target","025e3840":"* https:\/\/spacy.io\/\nspaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, scikit-learn, Gensim and the rest of Python's awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems.","91301c19":"### Logloss for this competition\n","1da7355e":"### Concatenate","fb2b20bd":"# Spacy\n\n","693f8991":"## Vectorisation","28976872":"# Submission","d9fbecdd":"# Lemmatisation\n\n* in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.","319199e5":"# Tokenisation\n\n* the process of breaking up the original text into components (tokens)","3fad4c33":"## Split training and test data","f6f5f3fd":"we map \"EAP\" to 0 \"HPL\" to 1 and \"MWS\" to 2 as it will be more convenient for our classifier. \nIn other words we are just telling our computer that if classifier predicts 0 for the text then it means that it is preicting \"EAP\", if 1 then it means that it is predicting \"HPL\", if 2 then it means that it is predicting \"MWS\".","13b4411a":"## feature extraction on Spacy","71a77bdf":"# X and y","72624997":"## Model"}}