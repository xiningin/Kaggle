{"cell_type":{"1354c1f7":"code","f187cb78":"code","b0c58e9b":"code","7b673a46":"code","8f8c0aaa":"code","0a8ce2ae":"code","886cd341":"code","62b577ca":"code","d501c90d":"code","d17b3888":"code","d8fdf72a":"code","6cab4a29":"code","7fc8de13":"code","56190914":"markdown","9a549f0e":"markdown"},"source":{"1354c1f7":"# Usual imports\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/embeddings\"))\n\n# Plotly based imports for visualization\nfrom plotly import tools\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# Keras based imports\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import CuDNNLSTM, CuDNNGRU, Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.merge import concatenate","f187cb78":"quora_train = pd.read_csv(\"..\/input\/train.csv\")\nquora_test = pd.read_csv(\"..\/input\/test.csv\")\nquora_train.head()","b0c58e9b":"punctuations = string.punctuation\n\ndef punct_remover(my_str):\n    my_str = my_str.lower()\n    no_punct = \"\"\n    for char in my_str:\n       if char not in punctuations:\n           no_punct = no_punct + char\n    return no_punct\n\npunctuations","7b673a46":"tqdm.pandas()\nquestions = quora_train[\"question_text\"].progress_apply(punct_remover)","8f8c0aaa":"test = quora_test[\"question_text\"].progress_apply(punct_remover)","0a8ce2ae":"# fit a tokenizer\ndef create_tokenizer(lines):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the maximum document length\ndef max_length(lines):\n    return max([len(s.split()) for s in lines])\n \n# encode a list of lines\ndef encode_text(tokenizer, lines, length):\n    # integer encode\n    encoded = tokenizer.texts_to_sequences(lines)\n    # pad encoded sequences\n    padded = pad_sequences(encoded, maxlen=length, padding='post')\n    return padded","886cd341":"embeddings_index = dict()\nf = open('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt',encoding='utf8')\nfor line in f:\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nembed_token = create_tokenizer(questions)\nvocabulary_size = 90000\n\nembedding_matrix = np.zeros((vocabulary_size, 300))\nfor word, index in embed_token.word_index.items():\n    if index > vocabulary_size - 1:\n        break\n    else:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[index] = embedding_vector","62b577ca":"# define the model\ndef define_model(length, vocab_size):\n    # channel 1\n    inputs1 = Input(shape=(length,))\n    embedding1 = Embedding(vocabulary_size, 300, weights=[embedding_matrix])(inputs1)\n    conv1 = Conv1D(filters=16, kernel_size=4, activation='relu')(embedding1)\n    drop1 = Dropout(0.5)(conv1)\n    lstm1 = Bidirectional(CuDNNLSTM(10, return_sequences = True))(drop1)\n    gru1 = Bidirectional(CuDNNGRU(10, return_sequences = True))(lstm1)\n    pool1 = MaxPooling1D(pool_size=2)(gru1)\n    flat1 = Flatten()(pool1)\n    # channel 2\n    inputs2 = Input(shape=(length,))\n    embedding2 = Embedding(vocabulary_size, 300, weights=[embedding_matrix])(inputs2)\n    conv2 = Conv1D(filters=16, kernel_size=6, activation='relu')(embedding2)\n    drop2 = Dropout(0.5)(conv2)\n    lstm2 = Bidirectional(CuDNNLSTM(10, return_sequences = True))(drop2)\n    gru2 = Bidirectional(CuDNNLSTM(10, return_sequences = True))(lstm2)\n    pool2 = MaxPooling1D(pool_size=2)(gru2)\n    flat2 = Flatten()(pool2)\n    # channel 3\n    inputs3 = Input(shape=(length,))\n    embedding3 = Embedding(vocabulary_size, 300, weights=[embedding_matrix])(inputs3)\n    conv3 = Conv1D(filters=16, kernel_size=8, activation='relu')(embedding3)\n    drop3 = Dropout(0.5)(conv3)\n    lstm3 = Bidirectional(CuDNNLSTM(10, return_sequences = True))(drop3)\n    gru3 = Bidirectional(CuDNNGRU(10, return_sequences = True))(lstm3)\n    pool3 = MaxPooling1D(pool_size=2)(gru3)\n    flat3 = Flatten()(pool3)\n    # merge\n    merged = concatenate([flat1, flat2, flat3])\n    # interpretation\n    dense1 = Dense(10, activation='relu')(merged)\n    outputs = Dense(1, activation='sigmoid')(dense1)\n    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n    # compile\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # summarize\n    print(model.summary())\n    plot_model(model, show_shapes=True, to_file='multichannel.png')\n    return model","d501c90d":"%%time\n# Preprocess data\n\n# create tokenizer\ntokenizer = create_tokenizer(questions)\n# calculate max document length\nlength = max_length(questions)\n# calculate vocabulary size\nvocab_size = len(tokenizer.word_index) + 1\nprint('Max document length: %d' % length)\nprint('Vocabulary size: %d' % vocab_size)\n# encode data\ntrainX = encode_text(tokenizer, questions, length)\ntestX = encode_text(tokenizer, test, length)\nprint(trainX.shape, testX.shape)","d17b3888":"# define model\nmodel = define_model(length, vocab_size)","d8fdf72a":"# fit model\nmodel.fit([trainX,trainX,trainX], quora_train[\"target\"].values, epochs=5, batch_size=4096)\n\n# save the model\nmodel.save('model.h5')","6cab4a29":"preds = model.predict([testX,testX,testX])\npreds = (preds[:,0] > 0.5).astype(np.int)","7fc8de13":"submission = pd.DataFrame.from_dict({'qid': quora_test['qid']})\nsubmission['prediction'] = preds\nsubmission.to_csv('submission.csv', index=False)","56190914":"This kernel is highly inspired from the following article: [https:\/\/machinelearningmastery.com\/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis\/](https:\/\/machinelearningmastery.com\/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis\/)","9a549f0e":"# About the dataset\nAn existential problem for any major website today is how to handle toxic and divisive content. Quora wants to tackle this problem head-on to keep their platform a place where users can feel safe sharing their knowledge with the world.\n\nQuora is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n\nIn this competition, Kagglers will develop models that identify and flag insincere questions. To date, Quora has employed both machine learning and manual review to address this problem. With your help, they can develop more scalable methods to detect toxic and misleading content.\n\nHere's your chance to combat online trolls at scale. Help Quora uphold their policy of \u201cBe Nice, Be Respectful\u201d and continue to be a place for sharing and growing the world\u2019s knowledge."}}