{"cell_type":{"f3519a5e":"code","99b5ebb5":"code","2baed8e7":"code","f98ce065":"code","172e977a":"code","0baf0514":"code","a912c164":"code","1545364a":"code","266f7437":"code","cf9f69cd":"code","4a8f369b":"code","851c6961":"code","8ccda50c":"code","03ee161c":"code","ecedf844":"code","1189f927":"code","d5b7fe13":"code","5734909b":"code","0d7ae131":"code","9f8d6dff":"code","cefc99af":"code","5eb570ec":"code","80b1f717":"code","0011b246":"code","2f32d78c":"code","820b5463":"code","469fbcd4":"code","748b91f0":"code","308f2c55":"code","8aa4643c":"code","aa0a3166":"code","c9b9b0b3":"code","7e90f79c":"code","f6df936a":"code","1cdb59ba":"code","5c63ff5a":"code","8761af79":"code","f6e4c801":"code","3251f113":"code","60ae5bba":"code","36af71a1":"code","02b3d87e":"code","b242fc9b":"code","be46da71":"code","4a6fc234":"code","561e319c":"code","3ce73663":"code","2bfefaba":"markdown","e17926ac":"markdown","4e1027e8":"markdown","65f7c0f6":"markdown","0316c73a":"markdown","adac8d55":"markdown","18c33588":"markdown","1b8976aa":"markdown","4587f309":"markdown","4c1ddedd":"markdown","a844e710":"markdown","d8ca459f":"markdown","4929c8cf":"markdown","52afcd00":"markdown","1af2193b":"markdown","94a7f579":"markdown","5d3dcdfe":"markdown","2c92a898":"markdown","33b5f6a5":"markdown","ef07e1f9":"markdown","74226585":"markdown","cf8941dd":"markdown","02c25aa4":"markdown","5bd0b837":"markdown","c4e85aee":"markdown","b48eeafe":"markdown","c3fc57fc":"markdown","197d2c22":"markdown","c2113bbf":"markdown","8d9349b4":"markdown","4feadbc8":"markdown","95880361":"markdown","8e3aa5ef":"markdown","11fa63db":"markdown","97853f65":"markdown"},"source":{"f3519a5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99b5ebb5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2baed8e7":"# Reading the csv file\ndata = pd.read_csv('\/kaggle\/input\/pakistans-largest-ecommerce-dataset\/Pakistan Largest Ecommerce Dataset.csv')\ndf = data.copy()","f98ce065":"df.head()","172e977a":"df.tail()","0baf0514":"df.columns","a912c164":"df.dtypes","1545364a":"# Checking for missing \/ NaN values\ndf.isnull().sum()","266f7437":"# Doing a visual inspection of all columns\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","cf9f69cd":"df.drop([\"Unnamed: 21\", \"Unnamed: 22\", \"Unnamed: 23\", \"Unnamed: 24\", \"Unnamed: 25\"], axis = 1, inplace=True)\ndf.dropna(subset=[\"item_id\"], axis=0, inplace=True)\ndf.rename(columns={\" MV \": \"MV\", \"category_name_1\": \"category_name\"}, inplace = True)","4a8f369b":"df=df.drop_duplicates()","851c6961":"print(\"The number of rows with negative or zero Quantity:\",sum(n <= 0 for n in df.qty_ordered))\nprint(\"The number of rows with negative Price:\",sum(n < 0 for n in df.price))","8ccda50c":"df['sku']=df['sku'].str.upper()","03ee161c":"df['status'].value_counts()","ecedf844":"df.groupby('BI Status')['status'].value_counts()","1189f927":"df['status'] = df['status'].replace('complete', 'Completed')\ndf['status'] = df['status'].replace('closed', 'Completed')\ndf['status'] = df['status'].replace('received', 'Completed')\ndf['status'] = df['status'].replace('paid', 'Completed')\ndf['status'] = df['status'].replace('cod', 'Completed')\ndf['status'] = df['status'].replace('order_refunded', 'Refund')\ndf['status'] = df['status'].replace('refund', 'Refund')\ndf['status'] = df['status'].replace('exchange', 'Refund')\ndf['status'] = df['status'].replace('pending', 'Pending')\ndf['status'] = df['status'].replace('payment_review', 'Pending')\ndf['status'] = df['status'].replace('processing', 'Pending')\ndf['status'] = df['status'].replace('holded', 'Pending')\ndf['status'] = df['status'].replace('pending_paypal', 'Pending')\ndf['status'] = df['status'].replace(r'\\\\N', 'Pending', regex=True)\ndf['status'] = df['status'].replace('fraud', 'Fraud')\ndf['status'] = df['status'].replace('canceled', 'Cancelled')","d5b7fe13":"df['status'].value_counts()","5734909b":"df['BI Status'] = df['BI Status'].replace('#REF!', 'Net')","0d7ae131":"df['BI Status'].value_counts()","9f8d6dff":"df[df['status'].isnull()]","cefc99af":"df['status'].fillna(\"Cancelled\",inplace=True)","5eb570ec":"df['category_name'].value_counts()","80b1f717":"df['category_name'] = df['category_name'].replace(r'\\\\N', 'Unknown', regex=True)\ndf['category_name'].fillna(\"Unknown\",inplace=True)","0011b246":"df[df['sku'].isnull()]","2f32d78c":"df['sku'].fillna(\"Missing\",inplace=True)","820b5463":"df['sales_commission_code'].value_counts()","469fbcd4":"df[df['sales_commission_code'].isnull()]","748b91f0":"df['sales_commission_code'].fillna(\"Missing\",inplace=True)\ndf['sales_commission_code'] = df['sales_commission_code'].replace(r'\\\\N', 'Missing', regex=True)","308f2c55":"df[df['Customer ID'].isnull()]","8aa4643c":"df['Customer ID'].fillna(\"0\",inplace=True)\ndf['Customer Since'].fillna(\"1-2018\",inplace=True)","aa0a3166":"df.isnull().sum()","c9b9b0b3":"df[[\"item_id\"]] = df[[\"item_id\"]].astype(\"str\")\ndf[[\"Month\"]] = df[[\"Month\"]].astype(\"int\")\ndf[[\"Year\"]] = df[[\"Year\"]].astype(\"int\")\ndf['created_at'] = pd.to_datetime(df['created_at'])\ndf[[\"qty_ordered\"]] = df[[\"qty_ordered\"]].astype(\"int\")\ndf[[\"Customer ID\"]] = df[[\"Customer ID\"]].astype(\"str\")\ndf[[\"increment_id\"]] = df[[\"increment_id\"]].astype(\"str\")\n\n## creating new columns to drill down the time dimension\ndf['day_of_week'] = df['created_at'].dt.dayofweek.astype(str) # 0 = monday.\n#df['weekday_flag'] = (df['day_of_week'] \/\/ 5 != 1).astype(str)\ndf['date_of_month'] = df['created_at'].dt.day\ndf['Week'] = df['created_at'].dt.week","7e90f79c":"df.info()","f6df936a":"df = df.reset_index()","1cdb59ba":"df['category_name'].value_counts()","5c63ff5a":"import plotly.express as px\n\ntemp = df.loc[(df['status']=='Completed') & (df['FY']!='FY19'),['Month','Week','date_of_month','day_of_week','category_name']]","8761af79":"df1 = temp.groupby(['Month','category_name']).size().reset_index(name='count')\ndf1['Percentage'] = 100 * df1['count'] \/ df1.groupby('Month')['count'].transform('sum')\nfig = px.bar(df1, x=\"Month\", y=\"count\", color=\"category_name\", text=df1['Percentage'].apply(lambda x: '{0:1.2f}%'.format(x)), title=\"Monthly Transactions by Item Category\")\nfig.add_annotation(x=11, y=85000,\n            text=\"Annual Black Friday Sales\",\n            showarrow=True,\n            arrowhead=1)\nfig.add_annotation(x=3, y=35000,\n            text=\"23rd March deals\",\n            showarrow=True,\n            arrowhead=1)\nfig.add_annotation(x=5, y=35000,\n            text=\"Eid-ul-Fitr\",\n            showarrow=True,\n            arrowhead=1)\nfig.show()","f6e4c801":"plt.figure(figsize=(15,6))\ncrosstab = pd.crosstab(temp['Month'], temp['category_name'])\nsns.heatmap(crosstab, cmap=\"YlGnBu\")","3251f113":"df1 = temp.groupby(['Week','category_name']).size().reset_index(name='count')\ndf1['Percentage'] = 100 * df1['count'] \/ df1.groupby('Week')['count'].transform('sum')\nfig = px.bar(df1, x=\"Week\", y=\"count\", color=\"category_name\", text=df1['Percentage'].apply(lambda x: '{0:1.2f}%'.format(x)), title=\"Weekly Transactions by Item Category\")\nfig.add_annotation(x=47, y=52000,\n            text=\"Annual Black Friday Sales\",\n            showarrow=True,\n            arrowhead=1)\nfig.add_annotation(x=20, y=13000,\n            text=\"Eid-ul-Fitr\",\n            showarrow=True,\n            arrowhead=1)\nfig.show()","60ae5bba":"plt.figure(figsize=(15,6))\ncrosstab = pd.crosstab(temp['Week'], temp['category_name'])\nsns.heatmap(crosstab, cmap=\"YlGnBu\")","36af71a1":"df1 = temp.groupby(['date_of_month','category_name']).size().reset_index(name='count')\ndf1['Percentage'] = 100 * df1['count'] \/ df1.groupby('date_of_month')['count'].transform('sum')\nfig = px.bar(df1, x=\"date_of_month\", y=\"count\", color=\"category_name\", text=df1['Percentage'].apply(lambda x: '{0:1.2f}%'.format(x)), title=\"Daily Transactions by Item Category\")\nfig.show()","02b3d87e":"plt.figure(figsize=(15,6))\ncrosstab = pd.crosstab(temp['date_of_month'], temp['category_name'])\nsns.heatmap(crosstab, cmap=\"YlGnBu\")","b242fc9b":"df1 = temp.groupby(['day_of_week','category_name']).size().reset_index(name='count')\ndf1['Percentage'] = 100 * df1['count'] \/ df1.groupby('day_of_week')['count'].transform('sum')\nfig = px.bar(df1, x=\"day_of_week\", y=\"count\", color=\"category_name\", text=df1['Percentage'].apply(lambda x: '{0:1.2f}%'.format(x)), title=\"Daily Transactions by Item Category\")\nfig.show()","be46da71":"plt.figure(figsize=(15,6))\ncrosstab = pd.crosstab(temp['day_of_week'], temp['category_name'])\nsns.heatmap(crosstab, cmap=\"YlGnBu\")","4a6fc234":"import scipy.stats as stats\n\ndf1 = pd.crosstab(df['date_of_month'], df['category_name'])\nobserved = df1.values\nval=stats.chi2_contingency(df1)\nexpected = val[3]","561e319c":"from scipy.stats import chi2\nchi_square=sum([(o-e)**2.\/e for o,e in zip(observed,expected)])\nchi_square_statistic=chi_square[0]+chi_square[1]\n\n# Specifying alpha as 0.05 or p-value criteria as 95%\nalpha = 0.05\nno_of_rows=df1.shape[0]\nno_of_columns=df1.shape[1]\nddof=(no_of_rows-1)*(no_of_columns-1)\n\ncritical_value=chi2.ppf(q=1-alpha,df=ddof)\np_value=1-chi2.cdf(x=chi_square_statistic,df=ddof)","3ce73663":"if chi_square_statistic>=critical_value:\n    print(\"There is a relationship between Order Date and Item Category\")\nelse:\n    print(\"There is no relationship between Order Date and Item Category\")\n    \nif p_value<=alpha:\n    print(\"There is a relationship between Order Date and Item Category\")\nelse:\n    print(\"There is no relationship between Order Date and Item Category\")","2bfefaba":"##### Obsevations\n- 20 NaN values for **'sku'** exist in the dataset and these values can be replaced.\n\n##### Action\n- Replace NaN values with a new sku code **'Missing'**","e17926ac":"#### Handling missing values in 'Sales_commission_code' column","4e1027e8":"#### Transactions by Week Number","65f7c0f6":"##### Observations\n- **Day 4 (Friday)** has significantly more transactions than other weekdays, which is partly because of the **Black Friday sales** impact in the dataset, but the start of the weekend factor may also have a role. \n- Rest of the days have no significant pattern, other than **Sunday**, where transactions have been **lowest**\n- **Men's Fashion** as well as **Women's Fashion** products have **more %age of orders** over the weekend than on week days","0316c73a":"## Step 1: Data Pre-processing","adac8d55":"### Conclusion\n\n- The chi-squared result shows that there is some statistical relationship between **Order Date and Item Category**, however, any correlation between the two is either weak or non-existent.\n- On a yearly level, the **last Friday of November** has increased transactions across all item categories. Similar trends, with a smaller peak, are seen on **Eid-ul-Fitr and 23rd March**.\n- On a monthly level, **Week 3 and Week 4 have higher transactions** than **Week 1 and 2** across all item categories especially **Mobile & Tablets, Entertainment, Appliances, Home and Living, Superstore, Beauty and Grooming and Fashion Products for Men and Women**\n- On a weekly level, **Friday** has higher transactions than other days with **Sunday having the least** transactions across all categories\n- **Men's and Women's Fashion Products** have higher transactions over the **weekends** than weekdays\n- E-commerce retailers can use this knowledge to do **marketing campaigns** and **plan product launches** at a weekly and monthly level.","18c33588":"##### Observations\n- There are 164 NaN values in the **'category_name'** column that can be filled using some information from **'sku'** column. Not doing it right now\n- 7850 transactions have a unicode label associated with them.\n- 164 transactions have NaN values.\n\n##### Actions\n- Replacing the unicode label and NaN values with label 'Unknown'","1b8976aa":"##### Observations\n- Out of 26 columns, last 5 columns in the dataset contain NaN values for all records\n- Records at 464051 indices (from the bottom) contain NaN values for all columns\n- ' MV ' is an ambiguous column name with extra spaces\n- Some of the columns have incorrect data types\n\n##### Actions\n- Last 5 columns need to be dropped from the dataset\n- 464051 rows, containing NaN values need to be dropped from the dataset\n- Renamed the columns ' MV ' and 'category_name_1' to 'MV' and 'category_name'","4587f309":"##### Dropping duplicate entries, if any, from the dataset","4c1ddedd":"#### Handling NaN values in 'sku' column","a844e710":"##### Observations\n- All transactions marked as either **'complete' or 'closed'**, fall in the **'Net' category** for 'BI Status'\n- All transactions marked as **'received','paid','cod','exchanged' or something related to refund** are marked in **'Valid' category**\n- All transactions marked as **either 'canceled' or something to do with incomplete transation** are marked in **'Gross' category**\n- '#REF!' looks an erroneus label.\n\n##### Actions\n**Replace values inside the 'status' column by creating new labels**\n\n- **'complete','closed','received','paid','cod'** will belong to category **'Completed'**\n- **'order_refunded','refund', 'exchange'** will belong to category **'Refund'**\n- **'pending','payment_review','processing','holded','pending_paypal','\\N'** will beling to **'Pending'**\n- **'canceled'** will belong to **'Cancelled'**\n- **'fraud'** will belong to **'Fraud'**\n**Also replace the '#REF!'' entry to 'Net' in 'BI status'**","d8ca459f":"#### Exploring the Item Category","4929c8cf":"### Is there a correlation between Order Date and Item Category?\n\n- In order to explore this relationship, it is important to see the dimension of time with multiple levels of granularity like Month number, Week number, Day of week and Date of month.\n- Also the item category labels need to be explored further","52afcd00":"##### Observations\n- Both parameters for chi-squared test result validation indicate that there is a statistical relationship between Order Date and Item Category\n","1af2193b":"##### Observation\n- 15 NaN values in 'status' column have 'Gross' in the BI column meaning all these transactions are not valid\n\n##### Actions\n- Replacing NaN values with label **'Cancelled'** in line with our understanding of the data","94a7f579":"##### Observations\n- The Item Categories have sufficient labels which are distinctive and cannot be reduced further\n- It has already been established from other notebooks that cancelled transactions are more than completed transactions and do not contrbiute towards revenue.\n- Cancelled transactions occur at the same time as Completed transactions and mostly driven by payment methods\n- It is better to perform the rest of the analysis for **'Completed transactions'** and for **FY17 and FY18** as one month's data for FY19 can bias the results\n\n##### Actions\n- Explore the number of transactions by different time dimensions","5d3dcdfe":"#### Transactions by Date of Month","2c92a898":"##### Observations\n- A substantial increase in transactions seen in **month of November** across all categories, most probably driven by **Annual Black Friday sales**\n- Smaller peaks seen in **March and May** which are driven by **23rd March and Eid-ul-Fitr Sales**\n- **Appliances, Fashion products for both men and women and Mobiles & Tablets** see a significant increase for the **Black Friday period** while most other categories also see an increase for number of transactions\n- **'Others'** category is most active during the **23rd March peak**, which points to some sort of limited time offer or deal. \n- **Entertainment** and **Appliances** almost have higher numbers around the March and May peaks.","33b5f6a5":"##### Convert all values in 'sku' column to upper case for uniformity","ef07e1f9":"#### Handling NaN values in 'category_name' column","74226585":"## Step 2: Exploratory Data Analysis","cf8941dd":"##### Observations\n- The column has a large number of NaN values and there are more than 7000 types of values in this column\n- The column does not seem to add any value for further analysis and can be dropped at a later stage\n- At this stage, NaN values as well as unicode labels can be replaced with 'Missing'\n\n##### Actions\n- Replacing NaN and unicode values with **'Missing'**","02c25aa4":"##### Observations\n- There are a lot of labels for 'status' column.\n- Need to check if any relationship exists between 'status' and 'BI Status' columns","5bd0b837":"#### Transactions by Month","c4e85aee":"#### Convert the datatypes of columns","b48eeafe":"#### Handling missing values in 'Customer ID' and 'Customer Since' columns","c3fc57fc":"##### Observations\n- Week 3 and Week 4 of have considerably more transactions than Week 1 and Week 2. Although, this has a bias because of the **Black Friday** and **23rd March deal dates**, which fall in Week 4 every year. Also the **Eid-ul-Fitr dates** for **FY17 and FY18 happened in Week 3 and Week 4**, but still there is a significant pattern seen in the plot above.\n- The increase is most significant for **Appliances, Fashion Products, Entertainment, Home & Living and Mobiles & Tablets**, which means consumers purchase these products more in Week 3 and Week 4 than the rest of the month.\n- **Books, Kids & Baby, School & Education products** do not follow this pattern","197d2c22":"##### Observations\n- There are a total of 11 rows where the 'Customer ID' column is NaN and exactly the same rows in 'Customer since' are also NaN, which makes sense and shows that these columns have a relationship.\n- All 11 records are from FY18, with the first record from 01-2018.\n- For keeping the records in dataset for analysis, a fake 'Customer ID' value of '0' can be assigned with '01-2018' assigned to all records in 'Customer Since' column\n\n##### Actions\n- Replaced 'Customer ID' with value **'0'** and 'Customer Since' with value **'01-2018'** for all NaN values","c2113bbf":"##### Handling Null values in 'status' column","8d9349b4":"### Statistical relationship betwen Order Date and Item Category","4feadbc8":"#### Checking for Null values again and setting appropriate datatypes","95880361":"##### Observations\n- Significant peak seen for **Week 46 and 47**, which represent the **Black Friday** for FY17 and FY18 respectively\n- Another peak seen in **Week 20**, which represents the Eid-ul-Fitr period.","8e3aa5ef":"#### Exploring all columns, finding and Imputing Null Values\n#### Categorical Variables","11fa63db":"#### Transactions by Day of Week","97853f65":"##### Basic data quality and integrity checks"}}