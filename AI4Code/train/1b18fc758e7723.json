{"cell_type":{"11ef75d8":"code","0f37f049":"code","a05833ac":"code","6c9dd3a3":"code","b18b73e1":"code","2018a4f9":"code","fb044f41":"code","3120b4c3":"code","29633fb2":"code","1f4ca896":"code","947a622a":"code","4b74a7f6":"code","0ec0b08b":"code","e348d853":"code","7ab9c0f9":"code","78d07c75":"code","8781f5a5":"code","e371d62e":"code","de74f67c":"code","e034b186":"code","b15fa0b2":"code","0ce27d15":"code","8d6ac38a":"code","38fc0369":"code","b01e4213":"code","f4ac1d32":"code","7a9ec277":"code","53b179d0":"code","5107831b":"code","c3997384":"code","065e2d43":"code","a26807d3":"code","b9fee25e":"code","5a3e53c6":"code","11ae1e28":"code","03d3e3cf":"code","05963df2":"code","a3e7151b":"code","7f8c61ef":"code","50971317":"code","45f6d976":"code","d4f8ff7e":"code","22725030":"code","c629712b":"code","f7dcf884":"code","e1f6d9e7":"code","6c1867e4":"code","4c728b19":"code","87257504":"code","d1491e2d":"code","87373e91":"code","b9b3326c":"code","724c2335":"code","8761d85e":"code","2bec6204":"code","ab4f0e50":"code","429c5a64":"code","d42dd3e6":"code","40db0020":"code","fc3428d5":"code","eff5ed5e":"code","8a89d045":"code","009a8928":"code","f4706897":"code","4369f6f7":"code","5f461f33":"code","e5dd9bf5":"code","af3ae715":"code","bb9a70ea":"code","28592b33":"markdown","b0137d4d":"markdown","dbb66903":"markdown","b38d6bcd":"markdown","8cb36a07":"markdown","734dcd41":"markdown","3996a38a":"markdown","db0c2d53":"markdown","b4e576ba":"markdown","fb058a4b":"markdown","d464f360":"markdown","93e57ac7":"markdown","1ae5505f":"markdown","49456269":"markdown","3f78b5f2":"markdown","b6d683f6":"markdown","5a1b60ff":"markdown","7e9af05c":"markdown","39a929d3":"markdown","4b6ffdbb":"markdown","fe8d1f83":"markdown","1adce1f0":"markdown","e1d5e164":"markdown","ab444a25":"markdown","a313573a":"markdown","c413a193":"markdown","1a527a5c":"markdown","953a61a6":"markdown","174a63b9":"markdown","5a083b79":"markdown","63d0ae0a":"markdown","808f37c9":"markdown","019eae9b":"markdown","1200ae40":"markdown","4d271095":"markdown","aeeecd66":"markdown","05e0ebe4":"markdown","9bae5a71":"markdown","c0f083a8":"markdown","f9dd684f":"markdown","a7854214":"markdown","665f3160":"markdown","1e67977f":"markdown","68767558":"markdown","82de4881":"markdown","f57c761c":"markdown","a9d4325e":"markdown","9d6735a5":"markdown","641f43ce":"markdown","20d9dc8e":"markdown","2f1cc89d":"markdown","7b6946e7":"markdown","de147f28":"markdown","6dbcb5e5":"markdown","8fd50f84":"markdown","c6bcae94":"markdown","8ed6268a":"markdown","8eca7cf9":"markdown","7a0cf894":"markdown","a9250857":"markdown","90ff0947":"markdown","59f78891":"markdown"},"source":{"11ef75d8":"# pandas and numpy for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualizations!\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot formatting and default style\nplt.rcParams['font.size'] = 18\nplt.style.use('fivethirtyeight')\n%matplotlib inline","0f37f049":"# Read in data and sort\nrandom = pd.read_csv('..\/input\/home-credit-model-tuning\/random_search_simple.csv').sort_values('score', ascending = False).reset_index()\nopt = pd.read_csv('..\/input\/home-credit-model-tuning\/bayesian_trials_simple.csv').sort_values('score', ascending = False).reset_index()\n\nprint('Best score from random search:         {:.5f} found on iteration: {}.'.format(random.loc[0, 'score'], random.loc[0, 'iteration']))\nprint('Best score from bayesian optimization: {:.5f} found on iteration: {}.'.format(opt.loc[0, 'score'], opt.loc[0, 'iteration']))","a05833ac":"import pprint\nimport ast\n\nkeys = []\nfor key, value in ast.literal_eval(random.loc[0, 'hyperparameters']).items():\n    print(f'{key}: {value}')\n    keys.append(key)","6c9dd3a3":"for key in keys:\n    print('{}: {}'.format(key, ast.literal_eval(opt.loc[0, 'hyperparameters'])[key]))","b18b73e1":"# Kdeplot of model scores\nplt.figure(figsize = (10, 6))\nsns.kdeplot(opt['score'], label = 'Bayesian Opt')\nsns.kdeplot(random['score'], label = 'Random Search')\nplt.xlabel('Score (5 Fold Validation ROC AUC)'); plt.ylabel('Density');\nplt.title('Random Search and Bayesian Optimization Results');","2018a4f9":"random['set'] = 'random'\nscores = random[['score', 'iteration', 'set']]\n\nopt['set'] = 'opt'\nscores = scores.append(opt[['set', 'iteration', 'score']], sort = True)\nscores.head()","fb044f41":"plt.figure(figsize = (12, 6))\n\nplt.subplot(121)\nplt.hist(random['score'], bins = 20, color = 'blue', edgecolor = 'k')\nplt.xlim((0.72, 0.80))\nplt.xlabel('Score'); plt.ylabel(\"Count\"); plt.title('Random Search Distribution of Scores');\n\nplt.subplot(122)\nplt.hist(opt['score'], bins = 20, color = 'blue', edgecolor = 'k')\nplt.xlim((0.72, 0.80))\nplt.xlabel('Score'); plt.ylabel(\"Count\"); plt.title('Bayes Opt Search Distribution of Scores');","3120b4c3":"scores.groupby('set')['score'].agg(['mean', 'max', 'min', 'std', 'count'])","29633fb2":"plt.rcParams['font.size'] = 16\n\nbest_random_score = random.loc[0, 'score']\nbest_random_iteration = random.loc[0, 'iteration']\n\nbest_opt_score = opt.loc[0, 'score']\nbest_opt_iteration = opt.loc[0, 'iteration']\n\nsns.lmplot('iteration', 'score', hue = 'set', data = scores, size = 8)\nplt.scatter(best_random_iteration, best_random_score, marker = '*', s = 400, c = 'blue', edgecolor = 'k')\nplt.scatter(best_opt_iteration, best_opt_score, marker = '*', s = 400, c = 'red', edgecolor = 'k')\nplt.xlabel('Iteration'); plt.ylabel('ROC AUC'); plt.title(\"Validation ROC AUC versus Iteration\");","1f4ca896":"random_fit = np.polyfit(random['iteration'], random['score'], 1)\nprint('Random search slope: {:.8f}'.format(random_fit[0]))","947a622a":"opt_fit = np.polyfit(opt['iteration'], opt['score'], 1)\nprint('opt search slope: {:.8f}'.format(opt_fit[0]))","4b74a7f6":"opt_fit[0] \/ random_fit[0]","0ec0b08b":"print('After 10,000 iterations, the random score is: {:.5f}.'.format(\nrandom_fit[0] * 1e5 + random_fit[1]))","e348d853":"print('After 10,000 iterations, the bayesian score is: {:.5f}.'.format(\nopt_fit[0] * 1e5 + opt_fit[1]))","7ab9c0f9":"import ast\ndef process(results):\n    \"\"\"Process results into a dataframe with one column per hyperparameter\"\"\"\n    \n    results = results.copy()\n    results['hyperparameters'] = results['hyperparameters'].map(ast.literal_eval)\n    \n    # Sort with best values on top\n    results = results.sort_values('score', ascending = False).reset_index(drop = True)\n    \n     # Create dataframe of hyperparameters\n    hyp_df = pd.DataFrame(columns = list(results.loc[0, 'hyperparameters'].keys()))\n\n    # Iterate through each set of hyperparameters that were evaluated\n    for i, hyp in enumerate(results['hyperparameters']):\n        hyp_df = hyp_df.append(pd.DataFrame(hyp, index = [0]), \n                               ignore_index = True, sort= True)\n        \n    # Put the iteration and score in the hyperparameter dataframe\n    hyp_df['iteration'] = results['iteration']\n    hyp_df['score'] = results['score']\n    \n    return hyp_df","78d07c75":"random_hyp = process(random)\nopt_hyp = process(opt)\n\nrandom_hyp.head()","8781f5a5":"# Hyperparameter grid\nparam_grid = {\n    'is_unbalance': [True, False],\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100))\n}","e371d62e":"best_random_hyp = random_hyp.loc[0, :]\nbest_opt_hyp = opt_hyp.loc[0, :]","de74f67c":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# Density plots of the learning rate distributions \nsns.kdeplot(param_grid['learning_rate'], label = 'Sampling Distribution', linewidth = 4, color = 'k')\nsns.kdeplot(random_hyp['learning_rate'], label = 'Random Search', linewidth = 4, color = 'blue')\nsns.kdeplot(opt_hyp['learning_rate'], label = 'Bayesian', linewidth = 4, color = 'green')\nplt.vlines([best_random_hyp['learning_rate']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['blue'])\nplt.vlines([best_opt_hyp['learning_rate']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['green'])\nplt.legend()\nplt.xlabel('Learning Rate'); plt.ylabel('Density'); plt.title('Learning Rate Distribution');\n\nprint('Best value from random search: {:.5f}.'.format(best_random_hyp['learning_rate']))\nprint('Best value from Bayesian: {:.5f}.'.format(best_opt_hyp['learning_rate']))","e034b186":"def plot_hyp_dist(hyp):\n    \"\"\"Plots distribution of hyp along with best values of hyp as vertical line\"\"\"\n    plt.figure(figsize = (16, 6))\n    plt.rcParams['font.size'] = 18\n\n    # Density plots of the learning rate distributions \n    sns.kdeplot(param_grid[hyp], label = 'Sampling Distribution', linewidth = 4, color = 'k')\n    sns.kdeplot(random_hyp[hyp], label = 'Random Search', linewidth = 4, color = 'blue')\n    sns.kdeplot(opt_hyp[hyp], label = 'Bayesian', linewidth = 4, color = 'green')\n    plt.vlines([best_random_hyp[hyp]],\n               ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['blue'])\n    plt.vlines([best_opt_hyp[hyp]],\n               ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['green'])\n    plt.legend()\n    plt.xlabel(hyp); plt.ylabel('Density'); plt.title('{} Distribution'.format(hyp));\n\n    print('Best value from random search: {:.5f}.'.format(best_random_hyp[hyp]))\n    print('Best value from Bayesian: {:.5f}.'.format(best_opt_hyp[hyp]))\n    plt.show()","b15fa0b2":"plot_hyp_dist('min_child_samples')","0ce27d15":"plot_hyp_dist('num_leaves')","8d6ac38a":"plot_hyp_dist('reg_alpha')","38fc0369":"plot_hyp_dist('reg_lambda')","b01e4213":"plot_hyp_dist('subsample_for_bin')","f4ac1d32":"plot_hyp_dist('colsample_bytree')","7a9ec277":"random_hyp.groupby('boosting_type')['score'].agg(['mean', 'max', 'min', 'std', 'count'])","53b179d0":"opt_hyp.groupby('boosting_type')['score'].agg(['mean', 'max', 'min', 'std', 'count'])","5107831b":"plt.figure(figsize = (16, 6))\n\nplt.subplot(121)\nrandom_hyp.groupby('boosting_type')['score'].agg(['mean', 'max', 'min', 'std'])['mean'].plot.bar(color = 'b')\nplt.ylabel('Score'); plt.title('Random Search Boosting Type Scores', size = 14);\n\nplt.subplot(122)\nopt_hyp.groupby('boosting_type')['score'].agg(['mean', 'max', 'min', 'std'])['mean'].plot.bar(color = 'b')\nplt.ylabel('Score'); plt.title('Bayesian Boosting Type Scores', size = 14);","c3997384":"plt.figure(figsize = (20, 8))\nplt.rcParams['font.size'] = 18\n\n# Density plots of the learning rate distributions \nsns.kdeplot(param_grid['subsample'], label = 'Sampling Distribution', linewidth = 4, color = 'k')\nsns.kdeplot(random_hyp[random_hyp['boosting_type'] == 'gbdt']['subsample'], label = 'Random Search', linewidth = 4, color = 'blue')\nsns.kdeplot(opt_hyp[opt_hyp['boosting_type'] == 'gbdt']['subsample'], label = 'Bayesian', linewidth = 4, color = 'green')\nplt.vlines([best_random_hyp['subsample']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['blue'])\nplt.vlines([best_opt_hyp['subsample']],\n           ymin = 0.0, ymax = 50.0, linestyles = '--', linewidth = 4, colors = ['green'])\nplt.legend()\nplt.xlabel('Subsample'); plt.ylabel('Density'); plt.title('Subsample Distribution');\n\nprint('Best value from random search: {:.5f}.'.format(best_random_hyp['subsample']))\nprint('Best value from Bayesian: {:.5f}.'.format(best_opt_hyp['subsample']))","065e2d43":"random_hyp.groupby('is_unbalance')['score'].agg(['mean', 'max', 'min', 'std', 'count'])","a26807d3":"opt_hyp.groupby('is_unbalance')['score'].agg(['mean', 'max', 'min', 'std', 'count'])","b9fee25e":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        opt_hyp[hyper] = opt_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot('iteration', hyper, data = opt_hyp, ax = axs[i])\n        axs[i].scatter(best_opt_hyp['iteration'], best_opt_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","5a3e53c6":"fig, axs = plt.subplots(1, 3, figsize = (18, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin']):\n        opt_hyp[hyper] = opt_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot('iteration', hyper, data = opt_hyp, ax = axs[i])\n        axs[i].scatter(best_opt_hyp['iteration'], best_opt_hyp[hyper], marker = '*', s = 200, c = 'k')\n        axs[i].set(xlabel = 'Iteration', ylabel = '{}'.format(hyper), title = '{} over Search'.format(hyper));\n\nplt.tight_layout()","11ae1e28":"random_hyp['set'] = 'Random Search'\nopt_hyp['set'] = 'Bayesian'\n\n# Append the two dataframes together\nhyp = random_hyp.append(opt_hyp, ignore_index = True, sort = True)\nhyp.head()","03d3e3cf":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['colsample_bytree', 'learning_rate', 'min_child_samples', 'num_leaves']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i], color = 'b', scatter_kws={'alpha':0.6})\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'b', edgecolor = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));\n        \n        opt_hyp[hyper] = opt_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = opt_hyp, ax = axs[i], color = 'g', scatter_kws={'alpha':0.6})\n        axs[i].scatter(best_opt_hyp[hyper], best_opt_hyp['score'], marker = '*', s = 200, c = 'g', edgecolor = 'k')\n\nplt.legend()\nplt.tight_layout()\n\n","05963df2":"# hyper = 'learning_rate'\n\n# fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n\n# random_hyp[hyper] = random_hyp[hyper].astype(float)\n# # Scatterplot\n# sns.regplot(hyper, 'score', data = random_hyp, ax = ax, color = 'b', scatter_kws={'alpha':0.6})\n# ax.scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'b', edgecolor = 'k')\n\n# opt_hyp[hyper] = opt_hyp[hyper].astype(float)\n# # Scatterplot\n# sns.regplot(hyper, 'score', data = opt_hyp, ax = ax, color = 'g', scatter_kws={'alpha':0.6})\n# ax.scatter(best_opt_hyp[hyper], best_opt_hyp['score'], marker = '*', s = 200, c = 'g', edgecolor = 'k')\n\n# ax.set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper))\n# ax.set(xscale = 'log');","a3e7151b":"fig, axs = plt.subplots(1, 4, figsize = (24, 6))\ni = 0\n\n# Plot of four hyperparameters\nfor i, hyper in enumerate(['reg_alpha', 'reg_lambda', 'subsample_for_bin', 'subsample']):\n        random_hyp[hyper] = random_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = random_hyp, ax = axs[i], color = 'b', scatter_kws={'alpha':0.6})\n        axs[i].scatter(best_random_hyp[hyper], best_random_hyp['score'], marker = '*', s = 200, c = 'b', edgecolor = 'k')\n        axs[i].set(xlabel = '{}'.format(hyper), ylabel = 'Score', title = 'Score vs {}'.format(hyper));\n        \n        opt_hyp[hyper] = opt_hyp[hyper].astype(float)\n        # Scatterplot\n        sns.regplot(hyper, 'score', data = opt_hyp, ax = axs[i], color = 'g', scatter_kws={'alpha':0.6})\n        axs[i].scatter(best_opt_hyp[hyper], best_opt_hyp['score'], marker = '*', s = 200, c = 'g', edgecolor = 'k')\n\nplt.legend()\nplt.tight_layout()\n\n","7f8c61ef":"from mpl_toolkits.mplot3d import Axes3D\nplt.rcParams['axes.labelpad'] = 12","50971317":"fig = plt.figure(figsize = (10, 10))\nax = fig.add_subplot(111, projection='3d')\n\nax.scatter(random_hyp['reg_alpha'], random_hyp['reg_lambda'],\n           random_hyp['score'], c = random_hyp['score'], \n           cmap = plt.cm.seismic_r, s = 40)\n\nax.set_xlabel('Reg Alpha')\nax.set_ylabel('Reg Lambda')\nax.set_zlabel('Score')\n\nplt.title('Score as Function of Reg Lambda and Alpha');","45f6d976":"best_random_hyp","d4f8ff7e":"fig = plt.figure(figsize = (10, 10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(random_hyp['learning_rate'], random_hyp['n_estimators'],\n           random_hyp['score'], c = random_hyp['score'], \n           cmap = plt.cm.seismic_r, s = 40)\n\nax.set_xlabel('Learning Rate')\nax.set_ylabel('Number of Estimators')\nax.set_zlabel('Score')\n\nplt.title('Score as Function of Learning Rate and Estimators', size = 16);","22725030":"plt.figure(figsize = (8, 7))\nplt.plot(random_hyp['learning_rate'], random_hyp['n_estimators'], 'ro')\nplt.xlabel('Learning Rate'); plt.ylabel('N Estimators'); plt.title('Number of Estimators vs Learning Rate');","c629712b":"from mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\ndef plot_3d(x, y, z, df, cmap = plt.cm.seismic_r):\n    \"\"\"3D scatterplot of data in df\"\"\"\n\n    fig = plt.figure(figsize = (10, 10))\n    \n    ax = fig.add_subplot(111, projection='3d')\n    \n    # 3d scatterplot\n    ax.scatter(df[x], df[y],\n               df[z], c = df[z], \n               cmap = cmap, s = 40)\n\n    # Plot labeling\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    ax.set_zlabel(z)\n\n    plt.title('{} as function of {} and {}'.format(\n               z, x, y), size = 18);\n    \nplot_3d('learning_rate', 'n_estimators', 'score', opt_hyp)","f7dcf884":"plt.figure(figsize = (8, 7))\nplt.plot(opt_hyp['learning_rate'], opt_hyp['n_estimators'], 'ro')\nplt.xlabel('Learning Rate'); plt.ylabel('N Estimators'); plt.title('Number of Estimators vs Learning Rate');","e1f6d9e7":"plot_3d('reg_alpha', 'reg_lambda', 'score', opt_hyp)","6c1867e4":"best_opt_hyp","4c728b19":"random_hyp['n_estimators'] = random_hyp['n_estimators'].astype(np.int32)\nrandom_hyp.corr()['score']","87257504":"random_hyp[random_hyp['boosting_type'] == 'gbdt'].corr()['score']['subsample']","d1491e2d":"opt_hyp['n_estimators'] = opt_hyp['n_estimators'].astype(np.int32)\nopt_hyp.corr()['score']","87373e91":"opt_hyp[opt_hyp['boosting_type'] == 'gbdt'].corr()['score']['subsample']","b9b3326c":"plt.figure(figsize = (12, 12))\n\n# Heatmap of correlations\nsns.heatmap(random_hyp.corr().round(2), cmap = plt.cm.gist_heat_r, vmin = -1.0, annot = True, vmax = 1.0)\nplt.title('Correlation Heatmap');","724c2335":"plt.figure(figsize = (12, 12))\n\n# Heatmap of correlations\nsns.heatmap(opt_hyp.corr().round(2), cmap = plt.cm.gist_heat_r, vmin = -1.0, annot = True, vmax = 1.0)\nplt.title('Correlation Heatmap');","8761d85e":"# Create training data and labels\nhyp = hyp.drop(columns = ['metric', 'set', 'verbose'])\nhyp['n_estimators'] = hyp['n_estimators'].astype(np.int32)\nhyp['min_child_samples'] = hyp['min_child_samples'].astype(np.int32)\nhyp['num_leaves'] = hyp['num_leaves'].astype(np.int32)\nhyp['subsample_for_bin'] = hyp['subsample_for_bin'].astype(np.int32)\nhyp = pd.get_dummies(hyp)\n\ntrain_labels = hyp.pop('score')\ntrain = np.array(hyp.copy())","2bec6204":"from sklearn.linear_model import LinearRegression\n\n# Create the lasso regression with cv\nlr = LinearRegression()\n\n# Train on the data\nlr.fit(train, train_labels)","ab4f0e50":"x = list(hyp.columns)\nx_values = lr.coef_\n\ncoefs = {variable: coef for variable, coef in zip(x, x_values)}\ncoefs","429c5a64":"import lightgbm as lgb","d42dd3e6":"train = pd.read_csv('..\/input\/home-credit-simple-featuers\/simple_features_train.csv')\nprint('Full Training Features Shape: ', train.shape)\ntest = pd.read_csv('..\/input\/home-credit-simple-featuers\/simple_features_test.csv')\nprint('Full Testing Features Shape: ', test.shape)","40db0020":"train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n\ntest_ids = list(test['SK_ID_CURR'])\ntest = test.drop(columns = ['SK_ID_CURR'])","fc3428d5":"features = list(train.columns)","eff5ed5e":"random_best = ast.literal_eval(random.loc[0, 'hyperparameters'])\n\nrmodel = lgb.LGBMClassifier(**random_best)\nrmodel.fit(train, train_labels)","8a89d045":"rpreds = rmodel.predict_proba(test)[:, 1]\nrsub = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': rpreds})\nrsub.to_csv('submission_random_search.csv', index = False)","009a8928":"bayes_best = ast.literal_eval(opt.loc[0, 'hyperparameters'])\n\nbmodel = lgb.LGBMClassifier(**bayes_best)\nbmodel.fit(train, train_labels)","f4706897":"bpreds = bmodel.predict_proba(test)[:, 1]\nbsub = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': bpreds})\nbsub.to_csv('submission_bayesian_optimization.csv', index = False)","4369f6f7":"random_fi = pd.DataFrame({'feature': features, 'importance': rmodel.feature_importances_})\nbayes_fi = pd.DataFrame({'feature': features, 'importance': bmodel.feature_importances_})","5f461f33":"def plot_feature_importances(df):\n    \"\"\"\n    Plots 15 most important features and returns a sorted feature importance dataframe.\n    \n    Parameters\n    --------\n    df : dataframe\n        Dataframe of feature importances. Columns must be feature and importance\n\n        \n    Return\n    --------\n    df : dataframe\n        Dataframe ordered by feature importances with a normalized column (sums to 1)\n        and a cumulative importance column\n    \n    \"\"\"\n    \n    plt.rcParams['font.size'] = 18\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n\n    \n    return df","e5dd9bf5":"norm_randomfi = plot_feature_importances(random_fi)\nnorm_randomfi.head(10)","af3ae715":"norm_bayesfi = plot_feature_importances(bayes_fi)\nnorm_bayesfi.head(10)","bb9a70ea":"random.loc[0, 'hyperparameters']","28592b33":"In both search methods, the `gbdt` (gradient boosted decision tree) and `dart` (dropout meets additive regression tree) do much better than `goss` (gradient based one-sided sampling). `gbdt` does the best on average (and for the max), so it might make sense to use that method in the future! Let's view the results as a barchart:","b0137d4d":"The feature importances look to be relatively stable across hyperparameter values. This is what I expected, but at the same time, we can see that the _absolute magnitude_ of the importances differs significantly but not the _relative ordering_.\n\n\u5728\u8d85\u53c2\u6570\u503c\u4e2d\uff0c\u7279\u6027\u7684\u91cd\u8981\u6027\u770b\u8d77\u6765\u662f\u76f8\u5bf9\u7a33\u5b9a\u7684\u3002\u8fd9\u662f\u6211\u6240\u671f\u671b\u7684\uff0c\u4f46\u540c\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u91cd\u8981\u6027\u7684\u7edd\u5bf9\u5927\u5c0f\u5dee\u522b\u5f88\u5927\uff0c\u4f46\u76f8\u5bf9\u987a\u5e8f\u5374\u6ca1\u6709\u5dee\u522b\u3002","dbb66903":"First up is `reg_alpha` and `reg_lambda`. These control the amount of regularization on each decision tree and help to prevent overfitting to the training data.","b38d6bcd":"# Conclusions\n# \u603b\u7ed3\n\nRandom search narrowly beat out Bayesian optimization in terms of finding the hyperparameter values that resulted in the highest cross validation ROC AUC. That single number does not tell the whole story though as the Bayesian method average ROC AUC was much higher than that of random search. We expect this to be the case because Bayesian optimization should focus on higher scoring values based on the surrogate model of the objective function it constructs. Morevoer, this tells us Bayesian optimization is a valuable technique, but random search can still happen upon better values in fewer search iterations if we are lucky. \n\n\u5728\u5bfb\u627e\u8d85\u53c2\u6570\u503c\u65b9\u9762\uff0c\u968f\u673a\u641c\u7d22\u4ee5\u5fae\u5f31\u4f18\u52bf\u51fb\u8d25\u4e86\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6700\u9ad8\u7684\u4ea4\u53c9\u9a8c\u8bc1ROC AUC\u3002\u867d\u7136\u8d1d\u53f6\u65af\u65b9\u6cd5\u7684ROC AUC\u5e73\u5747\u503c\u6bd4\u968f\u673a\u641c\u7d22\u7684ROC AUC\u8981\u9ad8\u5f97\u591a\uff0c\u4f46\u8fd9\u4e2a\u6570\u5b57\u5e76\u4e0d\u80fd\u8bf4\u660e\u95ee\u9898\u7684\u5168\u90e8\u3002\u6211\u4eec\u5e0c\u671b\u51fa\u73b0\u8fd9\u79cd\u60c5\u51b5\uff0c\u56e0\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u5e94\u8be5\u57fa\u4e8e\u5b83\u6240\u6784\u5efa\u7684\u76ee\u6807\u51fd\u6570\u7684\u4ee3\u7406\u6a21\u578b\u5173\u6ce8\u66f4\u9ad8\u7684\u8bc4\u5206\u503c\u3002\u6b64\u5916\uff0c\u8fd9\u544a\u8bc9\u6211\u4eec\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u4e00\u79cd\u6709\u4ef7\u503c\u7684\u6280\u672f\uff0c\u4f46\u5982\u679c\u6211\u4eec\u5e78\u8fd0\u7684\u8bdd\uff0c\u968f\u673a\u641c\u7d22\u4ecd\u7136\u53ef\u4ee5\u5728\u66f4\u5c11\u7684\u641c\u7d22\u8fed\u4ee3\u4e2d\u83b7\u5f97\u66f4\u597d\u7684\u503c\u3002\n\n* Random search slightly outperformed Bayesian optimization in terms of cv ROC AUC \n\n\u968f\u673a\u641c\u7d22\u5728cv ROC AUC\u65b9\u9762\u7565\u4f18\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\n\n* Bayesian optimization average scores were much higher than random search indicating it spends more time evaluating \"better\" hyperparameters\n\n\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5e73\u5747\u5f97\u5206\u8fdc\u9ad8\u4e8e\u968f\u673a\u641c\u7d22\uff0c\u8868\u660e\u5b83\u82b1\u66f4\u591a\u65f6\u95f4\u8bc4\u4f30\u201c\u66f4\u597d\u201d\u7684\u8d85\u53c2\u6570\n\n* Bayesian scored 0.791 when submitted and random search scored 0.790 indicating that with enough iterations, the methods deliver similar results \n\n\u8d1d\u53f6\u65af\u63d0\u4ea4\u65f6\u5f97\u52060.791\uff0c\u968f\u673a\u641c\u7d22\u5f97\u52060.790\uff0c\u8bf4\u660e\u8fed\u4ee3\u6b21\u6570\u8db3\u591f\u591a\uff0c\u7ed3\u679c\u76f8\u4f3c\n\n* Boosting type \"gdbt\" did much better than \"goss\" with \"dart\" nearly as good\n\n\u52a9\u63a8\u7c7b\u578b\u201cgdbt\u201d\u6bd4\u201cgoss\u201d\u505a\u5f97\u66f4\u597d\uff0c\u201cdart\u201d\u51e0\u4e4e\u4e00\u6837\u597d\n\n* A lower learning rate resulted in higher model scores: lower than 0.02 looks to be optimal\n\n\u8f83\u4f4e\u7684\u5b66\u4e60\u7387\u5bfc\u81f4\u8f83\u9ad8\u7684\u6a21\u578b\u5206\u6570:\u4f4e\u4e8e0.02\u770b\u8d77\u6765\u662f\u6700\u4f73\u7684\n\n* `reg_alpha` and `reg_lambda` should complement one another: if one is high (above 0.5), than the other should be lower (below 0.5)\n\n`reg_alpha`\u548c`reg_lambda`\u5e94\u8be5\u4e92\u76f8\u8865\u5145:\u5982\u679c\u4e00\u4e2a\u9ad8(\u9ad8\u4e8e0.5)\uff0c\u90a3\u4e48\u53e6\u4e00\u4e2a\u5e94\u8be5\u4f4e(\u4f4e\u4e8e0.5)\n\n* Some subsampling appears to increase the model scores\n\n\u4e00\u4e9b\u5b50\u62bd\u6837\u4f3c\u4e4e\u589e\u52a0\u4e86\u6a21\u578b\u5206\u6570\n\n* The other hyperparameters either did not have a significant effect, or their effects are intertwined and hence could not be disentangled in this study\n\n\u5176\u4ed6\u8d85\u53c2\u6570\u8981\u4e48\u6ca1\u6709\u663e\u8457\u5f71\u54cd\uff0c\u8981\u4e48\u5b83\u4eec\u7684\u5f71\u54cd\u76f8\u4e92\u4ea4\u7ec7\uff0c\u56e0\u6b64\u4e0d\u80fd\u5728\u672c\u7814\u7a76\u4e2d\u89e3\u5f00\n\nFeel free to build upon these results! I'm curious if the best hyperparameters for this dataset will translate to other datasets, either for this problem, or for vastly different data science problems. The best way to find out is to try them! \n\n\u8bf7\u5728\u8fd9\u4e9b\u7ed3\u679c\u7684\u57fa\u7840\u4e0a\u968f\u610f\u6784\u5efa!\u6211\u5f88\u597d\u5947\u8fd9\u4e2a\u6570\u636e\u96c6\u7684\u6700\u4f73\u8d85\u53c2\u6570\u662f\u5426\u4f1a\u8f6c\u5316\u4e3a\u5176\u4ed6\u6570\u636e\u96c6\uff0c\u65e0\u8bba\u662f\u9488\u5bf9\u8fd9\u4e2a\u95ee\u9898\uff0c\u8fd8\u662f\u9488\u5bf9\u622a\u7136\u4e0d\u540c\u7684\u6570\u636e\u79d1\u5b66\u95ee\u9898\u3002\u6700\u597d\u7684\u65b9\u6cd5\u5c31\u662f\u53bb\u5c1d\u8bd5!\n\nIf you're looking for more work on this problem, I have a series of notebooks documenting my work:\n\n\u5982\u679c\u4f60\u5728\u5bfb\u627e\u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u95ee\u9898\u7684\u5de5\u4f5c\uff0c\u6211\u6709\u4e00\u7cfb\u5217\u7b14\u8bb0\u8bb0\u5f55\u6211\u7684\u5de5\u4f5c:\n\n\n__Additional Notebooks__ \n\n* [A Gentle Introduction](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction)\n* [Manual Feature Engineering Part One](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering)\n* [Manual Feature Engineering Part Two](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering-p2)\n* [Introduction to Automated Feature Engineering](https:\/\/www.kaggle.com\/willkoehrsen\/automated-feature-engineering-basics)\n* [Advanced Automated Feature Engineering](https:\/\/www.kaggle.com\/willkoehrsen\/tuning-automated-feature-engineering-exploratory)\n* [Feature Selection](https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-feature-selection)\n* [Intro to Model Tuning: Grid and Random Search](https:\/\/www.kaggle.com\/willkoehrsen\/intro-to-model-tuning-grid-and-random-search)\n* [Automated Model Tuning](https:\/\/www.kaggle.com\/willkoehrsen\/automated-model-tuning)\n\nThanks for reading and feel free to share any constructive criticism or feedback. \n\n\u611f\u8c22\u60a8\u7684\u9605\u8bfb\uff0c\u8bf7\u968f\u65f6\u5206\u4eab\u4efb\u4f55\u6709\u5efa\u8bbe\u6027\u7684\u6279\u8bc4\u6216\u53cd\u9988\u3002\n\nBest,\n\nWill","8cb36a07":"Here there appears to be a clear trend: a lower learning rate leads to higher values! What does the plot of just learning rate versus number of estimators look like?","734dcd41":"### Correlations for Random Search","3996a38a":"# Hyperparameter Values\n\nFor each hyperparameter, we will plot the values tried by both searches as well as the reference distribution (which was the same in both cases, just a grid for random and distributions for Bayesian). We would expect the random search to almost exactly match the reference - it will converge on the reference given enough iterations.\n\nFirst, we will process the results into a dataframe where each column is one hyperparameter. Saving the file converted the dictionary into a string, so we use `ast.literal_eval` to convert back to a dictionary before adding as a row in the dataframe.","db0c2d53":"There are not any strong trends here. Next we will try to look at two hyperparameters simultaneously versus the score in a 3-dimensional plot. This makes sense for hyperparameters that work in concert, such as the learning rate and the number of esimators or the two regularization values.","b4e576ba":"Even though the search domain extended from 0.005 to 0.2, both optimal values clustered around a lower value. Perhaps this tells us we should concentrate further searches in this area below 0.02?\n\nThat code was a little tedious, so let's write a function that makes the same code for any hyperparameter (feel free to pick your own colors!).","fb058a4b":"As expected, the `learning_rate` has one of the greatest correlations with the score. The `subsample` rate might be affected by the fact that 1\/3 of the time this was set to 1.0.","d464f360":"If we compare the individual values, we actually see that they are fairly close together when we consider the entire search grid! \n\n## Distribution of Scores    \u5206\u6570\u5206\u5e03\n\nLet's plot the distribution of scores for both models in a kernel density estimate plot.\n\u8ba9\u6211\u4eec\u753b\u51fa\u4e24\u79cd\u6a21\u578b\u5728\u6838\u5bc6\u5ea6\u4f30\u8ba1\u56fe\u4e2d\u7684\u5206\u6570\u5206\u5e03\u3002","93e57ac7":"If we are going by mean, then Bayesian optimization is the clear winner. If we go by high score, then random search just wins out. \n\u5982\u679c\u6211\u4eec\u662f\u5e73\u5747\u7684\uff0c\u90a3\u4e48\u8d1d\u53f6\u65af\u4f18\u5316\u662f\u660e\u786e\u7684\u8d62\u5bb6\u3002\u5982\u679c\u6211\u4eec\u53d6\u5f97\u9ad8\u5206\uff0c\u90a3\u4e48\u968f\u673a\u641c\u7d22\u5c31\u4f1a\u80dc\u51fa\u3002","1ae5505f":"__Take these with some skepticism because they were performed on a very small subset of the data!__ \n\nFor more rigorous results, we will turn to the evaluation metrics from running __500 iterations (with random search)__ and __400+ iterations (with Bayesian Optimization)__ on a full training dataset with about 700 features (the features are from [this notebook](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https:\/\/www.kaggle.com\/jsaguiar)). These iterations took around 6 days on a machine with 128 GB of RAM so they will not run in a kernel! The Bayesian Optimization method is still running and I will update the results as they finish.\n\n__In this notebook  we will focus only on the results and building the best model, so for the explanations of the methods, refer to the previous notebooks! __\n\n# Overall Results\n\nFirst, let's start with the most basic question: which model produced the highest cross validation ROC AUC score (using 5 folds) on the training dataset?","49456269":"This plot is very easy to interpret: the lower the learning rate, the more estimators that will be trained. From our knowledge of the model, this makes sense: each individual decision trees contribution is lessened as the learning rate is decreased leading to a need for more decision trees in the ensemble. Moreover, from the previous graphs, it appears that decreasing the learning rate increases the model score.","3f78b5f2":"#### Bayesian Optimization best Hyperparameters","b6d683f6":"The bayesian optimization results are close in trend to those from random search: lower learning rate leads to higher cross validation scores.","5a1b60ff":"Well, there you go! __Random search slightly outperformed  Bayesian optimization and found a higher cross validation model in far fewer iterations.__ However, as we will shortly see, this does not mean random search is the better hyperparameter optimization method. \n\nWhen submitted to the competition (at the end of this notebook):\n\n* __Random search results scored 0.790__\n* __Bayesian optimization results scored 0.791__\n\nWhat were the best model hyperparameters from both methods?\n\n####  Random Search best Hyperparameters","7e9af05c":"# Distributions of Search Values\n\nBelow are the kernel density estimate plots for each hyperparameter. The dashed vertical lines indicate the \"optimal\" value found in the respective searches. \n\nWe start with the learning rate:","39a929d3":"First we need to format the data and extract the labels.\n\n\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u683c\u5f0f\u5316\u6570\u636e\u5e76\u63d0\u53d6\u6807\u7b7e\u3002","4b6ffdbb":"Again keeping in mind that Bayesian optimization has not yet finished, we can see a clear upward trend for this method and no trend whatsoever for random search. \n\n### Linear Regression of Scores versus Iteration\n\nTo show that Bayesian optimization improves over time, we can regress the score by the iteration. Then, we can use this to extrapolate into the future, __a wildly inappropriate technique in this case, but fun nonetheless!__\n\nHere we use `np.polyfit` with a degree of 1 for the linear regression (you can compare the results with `LinearRegression`  from `sklearn.linear_model`.","fe8d1f83":"### Bayesian Optimization","1adce1f0":"## 3D Plots \n\nTo try and examine the simultaneous effects of hyperparameters, we can make 3D plots with 2 hyperparameters and the score. A truly accurate plot would be 10-D (one for each hyperparameter) but in this case we will stick to 3 dimensions. 3D plots can be made in matplotlib by import `Axes3D` and specifying the `3d` projection in a call to `.add_subplot`","e1d5e164":"# Hyperparameters versus Iteration\n\nNext we will take a look at the __evolution__ of the Bayesian search (random search shows no pattern as expected) by graphing the values versus the iteration. This can inform us the direction in which the search was heading in terms of where the values tended to cluster. Given these graphs, we might then be able to extrapolate values that lead to even higher scores (or maybe not, _extrapolation is dangerous_!)\n\nThe black star in the plots below signifies the best scoring value.","ab444a25":"Finally, we can look at the instance of `is_unbalance`, a hyperparameter that tells LightGBM whether or not to treat the problem as unbalance classification.","a313573a":"__The only clear distinction is that the score decreases as the learning rate increases.__ Of course, we cannot say whether that is due to the learning rate itself, or some other factor (we will look at the interplay between the learning rate and the number of esimators shortly). The learning rate domain was on a logarithmic scale, so it's most accurate for the plot to be as well (unfortunately I cannot get this to work yet).","c413a193":"__`gbdt` (or `dart`) it should be! Notice that random search tried `gbdt` about the same number of times as the other two (since it selected with no reasoning) while Bayesian optimization tried `gbdt` much more often. __\n\nSince `gbdt` supports `subsample` (using on a sample of the observations to train on in every tree) we can plot the distribution of `subsample` where `boosting_type=='gbdt'`. We also show the reference distribution.","1a527a5c":"# Plots of Hyperparameters vs Score\n\n![](http:\/\/)These next plots show the value of a single hyperparameter versus the score. We want to avoid placing too much emphasis on these graphs, because we are not changing one hyperparameter at a time. Therefore, if there are trends, it might not be solely due to the single hyperparameter we show. A truly accurate grid would be 10-dimensional and show the values of __all__ hyperparameters and the resulting score. If we could understand a __10-dimensional__ graph, then we might be able to figure out the optimal combination of hyperparameters! ","953a61a6":"## Recap \n\nIn the respective notebooks, we examined we performed 1000 iterations of random search and Bayesian optimization on a reduced sample of the dataset (10000 rows). We compared the cross-validation ROC AUC on the training data, the score on a \"testing set\" (6000 observations) and the score on the real test set when submitted to the competition leaderboard. Results are below:\n\n| Method                               | Cross Validation Score | Test Score (on 6000 Rows) | Submission to Leaderboard | Iterations to best score |\n|--------------------------------------|------------------------|---------------------------|---------------------------|--------------------------|\n| Random Search                        | 0.73110                | 0.73274                   | 0.782                     | 996                      |\n| Bayesian Hyperparameter Optimization | 0.73448                | 0.73069                   | 0.792                     | 596                      ","174a63b9":"# Roadmap\n\nOur plan of action is as follows:\n\n1. High Level Overview\n    * Which method did best? \n2. Examine distribution of scores\n    * Are there trends over the course of the search?\n3. Explore hyperparameter values\n    * Look at values over the course of the search\n    * Identify correlations between hyperparameters and the score\n4. Perform \"meta\" machine learning using these results\n    * Fit a linear regression to results and look at coefficients\n5. Train a model on the full set of features using the best performing values\n    * Try best results from both random search and bayesian optimization\n6.  Lay out next steps\n    * How can we use these results for this _and other_ problems? \n    * Are there better methods for hyperparameter optimization","5a083b79":"We can do this for all of the hyperparameters. These results can be used to inform further searches. They can even be used to define a grid search over a concentrated region. The problem with grid search is the insane compuational and time costs involved, and a smaller hyperparameter grid will help immensely! ","63d0ae0a":"__According to the average score, it pretty much does not matter if this hyperparameter is `True` or `False`.__ To be honest, I'm not sure what difference this is supposed to make, so anyone who wants can fill me in!","808f37c9":"The `learning_rate` again appears to be moderately correlated with the score. This should tell us again that a lower learning rate tends to co-occur with a higher cross-validation score, but not that this is nexessarily the cause of the higher score. ","019eae9b":"There is a significant disagreement between the two methods on the optimal value for `subsample`. Perhaps we would want to leave this as a wide distribution in any further searches (although some subsampling does look to be beneficial).","1200ae40":"# Meta-Machine Learning\n# \u5143\u673a\u5668\u5b66\u4e60\n\nSo we have a labeled set of data: the hyperparameter values and the resulting score. Clearly, the next step is to use these for machine learning? Yes, here we will perform _meta-machine learning_ by fitting an estimator on top of the hyperparameter values and the scores. This is a supervised regression problem, and although we can use any method for learning the data, here we will stick to a linear regression. This will let us examine the coefficients on each hyperparameter and will help reduce overfitting. \n\n\u6240\u4ee5\u6211\u4eec\u6709\u4e00\u7ec4\u6807\u7b7e\u6570\u636e:\u8d85\u53c2\u6570\u503c\u548c\u7ed3\u679c\u5206\u6570\u3002\u5f88\u660e\u663e\uff0c\u4e0b\u4e00\u6b65\u662f\u5c06\u5b83\u4eec\u7528\u4e8e\u673a\u5668\u5b66\u4e60?\u662f\u7684\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u6267\u884c\u5143\u673a\u5668\u5b66\u4e60\u62df\u5408\u4e00\u4e2a\u4f30\u8ba1\u5728\u8d85\u53c2\u6570\u503c\u548c\u5206\u6570\u3002\u8fd9\u662f\u4e00\u4e2a\u76d1\u7763\u56de\u5f52\u95ee\u9898\uff0c\u5c3d\u7ba1\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4efb\u4f55\u65b9\u6cd5\u6765\u5b66\u4e60\u6570\u636e\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u575a\u6301\u7ebf\u6027\u56de\u5f52\u3002\u8fd9\u5c06\u8ba9\u6211\u4eec\u68c0\u67e5\u6bcf\u4e2a\u8d85\u53c2\u6570\u4e0a\u7684\u7cfb\u6570\uff0c\u5e76\u5c06\u6709\u52a9\u4e8e\u51cf\u5c11\u8fc7\u62df\u5408\u3002","4d271095":"### Random Search","aeeecd66":"Again, we probably want one of the regularization values to be high and the other to be low. This must help to \"balance\" the model between bias and variance. ","05e0ebe4":"#### Feature Importances\n#### \u7279\u5f81\u91cd\u8981\u6027\n\nAs a final step, we can compare the feature importances between the models from the best hyperparameters. It would be interesting to see if the hyperparameter values has an effect on the feature importances.\n\n\u4f5c\u4e3a\u6700\u540e\u4e00\u6b65\uff0c\u6211\u4eec\u53ef\u4ee5\u6bd4\u8f83\u6765\u81ea\u6700\u4f73\u8d85\u53c2\u6570\u7684\u6a21\u578b\u4e4b\u95f4\u7684\u7279\u5f81\u91cd\u8981\u6027\u3002\u770b\u770b\u8d85\u53c2\u6570\u503c\u662f\u5426\u5bf9\u7279\u6027\u7684\u91cd\u8981\u6027\u6709\u5f71\u54cd\u662f\u5f88\u6709\u8da3\u7684\u3002","9bae5a71":"Feel free to use this code for your own heatmaps! (Also send me color recommendations because I am not great at picking out a palette).","c0f083a8":"## Correlation Heatmap\n\nNow we can make a heatmap of the correlations. I enjoy heatmaps and thankfully, they are not very difficult to make in `seaborn`.","f9dd684f":"# Implementation\n# \u6267\u884c\n\nThe full set of features on which these results come are from [this notebook](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features) by [Aguiar](https:\/\/www.kaggle.com\/jsaguiar)). Here, we will load in the same features, train on the full training features and make predictions on the testing data. These can then be uploaded to the competition.\n\n\u8fd9\u4e9b\u7ed3\u679c\u6765\u81ea[\u672c\u7b14\u8bb0\u672c](https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features) [Aguiar](https:\/\/www.kaggle.com\/jsaguiar))\u7684\u5168\u5957\u529f\u80fd\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u52a0\u8f7d\u76f8\u540c\u7684\u529f\u80fd\uff0c\u8bad\u7ec3\u5b8c\u6574\u7684\u8bad\u7ec3\u529f\u80fd\uff0c\u5e76\u5bf9\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002\u8fd9\u4e9b\u89c6\u9891\u53ef\u4ee5\u4e0a\u4f20\u81f3\u6bd4\u8d5b\u4e2d\u3002","a7854214":"### Competition Results\n### \u7ade\u4e89\u7684\u7ed3\u679c\n\n* __Random search results scored 0.790__\n\n\u968f\u673a\u641c\u7d22\u7ed3\u679c\u5f97\u52060.790__\n\n* __Bayesian optimization results scored 0.791__\n\n\u8d1d\u53f6\u65af\u4f18\u5316\u7ed3\u679c\u5f97\u52060.791__\n\nIf we go by best score on the public leaderboard, Bayesian Optimization wins! However, the public leaderboard is based only on 10% of the test data, so it's possible this is a result of overfitting to this particular subset of the testing data. Overall, I would say the complete results suggest that both methods produce similar outcomes especially when run for enough iterations. Either method is better than hand-tuning! \n\n\u5982\u679c\u6211\u4eec\u5728\u516c\u5f00\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u6700\u597d\u7684\u5206\u6570\uff0c\u90a3\u4e48\u8d1d\u53f6\u65af\u4f18\u5316\u5c31\u83b7\u80dc\u4e86!\u7136\u800c\uff0c\u516c\u5171\u6392\u884c\u699c\u53ea\u57fa\u4e8e10%\u7684\u6d4b\u8bd5\u6570\u636e\uff0c\u6240\u4ee5\u8fd9\u53ef\u80fd\u662f\u6d4b\u8bd5\u6570\u636e\u7684\u8fc7\u5ea6\u62df\u5408\u7684\u7ed3\u679c\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u60f3\u8bf4\u7684\u662f\uff0c\u5b8c\u6574\u7684\u7ed3\u679c\u8868\u660e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u4ea7\u751f\u7c7b\u4f3c\u7684\u7ed3\u679c\uff0c\u7279\u522b\u662f\u5728\u8fd0\u884c\u8db3\u591f\u7684\u8fed\u4ee3\u65f6\u3002\u4efb\u4f55\u4e00\u79cd\u65b9\u6cd5\u90fd\u6bd4\u624b\u5de5\u8c03\u4f18\u66f4\u597d!","665f3160":"Bayesian optimization did not produce the highest individual score, but it did tend to spend more time evaluating \"better\" values of hyperparameters. __Random search got lucky and found the best values but Bayesian optimization tended to \"concentrate\" on better-scoring values__. That's pretty much what we expect: random search does a good job of exploring the search space which means it will probably happen upon a high-scoring set of values (if the space is not extremely high-dimensional) while Bayesian optimization will tend to focus on a set of values that yield higher scores. __If all you wanted was the conclusion, then you're probably good to go. If you really enjoy making plots and doing exploratory data analysis and want to gain a better understanding of how these methods work, then read on!__ In the next few sections, we will thoroughly explore these results.\n\n\u8d1d\u53f6\u65af\u4f18\u5316\u5e76\u6ca1\u6709\u4ea7\u751f\u6700\u9ad8\u7684\u4e2a\u4eba\u5f97\u5206\uff0c\u4f46\u5b83\u786e\u5b9e\u503e\u5411\u4e8e\u82b1\u8d39\u66f4\u591a\u7684\u7cbe\u529b\u6765\u8bc4\u4f30\u8d85\u53c2\u6570\u7684\u201c\u66f4\u597d\u201d\u503c\u3002--\u968f\u673a\u641c\u7d22\u627e\u5230\u4e86\u6700\u597d\u7684\u503c\uff0c\u4f46\u8d1d\u53f6\u65af\u4f18\u5316\u503e\u5411\u4e8e\u201c\u96c6\u4e2d\u201d\u4e8e\u66f4\u597d\u7684\u5206\u6570\u503c\u3002\u8fd9\u6b63\u662f\u6211\u4eec\u6240\u671f\u671b\u7684\uff1a\u968f\u673a\u641c\u7d22\u5bf9\u641c\u7d22\u7a7a\u95f4\u7684\u63a2\u7d22\u505a\u5f97\u5f88\u597d\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u5f88\u53ef\u80fd\u4f1a\u53d1\u751f\u5728\u4e00\u7ec4\u9ad8\u5206\u503c\u4e0a(\u5982\u679c\u7a7a\u95f4\u4e0d\u662f\u6781\u9ad8\u7ef4\u7684\u8bdd)\u3002\u800c\u8d1d\u53f6\u65af\u4f18\u5316\u5219\u503e\u5411\u4e8e\u5c06\u91cd\u70b9\u653e\u5728\u4e00\u7ec4\u80fd\u4ea7\u751f\u8f83\u9ad8\u5206\u6570\u7684\u503c\u4e0a\u3002--\u5982\u679c\u4f60\u60f3\u8981\u7684\u662f\u7ed3\u8bba\u7684\u8bdd\uff0c\u90a3\u4e48\u8fd9\u6837\u505a\u662f\u53ef\u4ee5\u7684\u3002\u5982\u679c\u60a8\u771f\u7684\u559c\u6b22\u5236\u4f5c\u56fe\u8868\uff0c\u8fdb\u884c\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\u5e76\u4e14\u60f3\u8981\u66f4\u597d\u5730\u4e86\u89e3\u8fd9\u4e9b\u65b9\u6cd5\u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff0c\u90a3\u4e48\u5728\u63a5\u4e0b\u6765\u7684\u51e0\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e9b\u7ed3\u679c\u3002\n\nOur plan for going through the results is as follows:\n\u6211\u4eec\u7684\u8ba1\u5212\u662f\uff1a\n* Distribution of scores\n\u5206\u6570\u5206\u5e03\n    * Overall distribution\n    \u603b\u4f53\u5206\u5e03\n    * Score versus the iteration (did scores improve as search progressed)\n    \u5f97\u5206\u76f8\u5bf9\u4e8e\u8fed\u4ee3(\u968f\u7740\u641c\u7d22\u7684\u8fdb\u5c55\uff0c\u5206\u6570\u662f\u5426\u6709\u6240\u63d0\u9ad8)\n* Distribution of hyperparameters\n\u8d85\u53c2\u6570\u7684\u5206\u5e03\n    * Overall distribution including the hyperparameter grid for a reference\n    \u5305\u62ec\u4f9b\u53c2\u8003\u7684\u8d85\u53c2\u6570\u7f51\u683c\u7684\u603b\u4f53\u5206\u5e03\n    * Hyperparameters versus iteration to look at _evolution_ of values\n    \u8d85\u53c2\u6570\u4e0e\u8fed\u4ee3\u4ee5\u67e5\u770b\u503c\u7684\u6f14\u5316\n* Hyperparameter values versus the score\n\u8d85\u53c2\u6570\u503c\u4e0e\u5206\u6570\n    * Do scores improve with certain values of hyperparameters (correlations)\n    \u968f\u7740\u67d0\u4e9b\u8d85\u53c2\u6570(\u76f8\u5173\u6027)\u503c\u7684\u589e\u52a0\uff0c\u6210\u7ee9\u4f1a\u63d0\u9ad8\u5417\uff1f\n    * 3D plots looking at effects of 2 hyperparameters at a time on the score\n    \u4e00\u6b21\u4e24\u4e2a\u8d85\u53c2\u6570\u5bf9\u5f97\u5206\u5f71\u54cd\u76843D\u56fe\n* Additional Plots\n\u5176\u4ed6\u7684\u7ed8\u56fe\n    * Time to run each evaluation for Bayesian optimization\n    \u8fd0\u884c\u6bcf\u4e2a\u8bc4\u4f30\u4ee5\u8fdb\u884c\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u65f6\u95f4\n    * Correlation heatmaps of hyperparameters with score\n    \u8d85\u53c2\u6570\u4e0e\u5f97\u5206\u7684\u76f8\u5173\u70ed\u56fe\n    \nThere will be all sorts of plots: heatmaps, 3D scatterplots, density plots, bar charts (hey even bar charts can be helpful!)\n\u4f1a\u6709\u5404\u79cd\u5404\u6837\u7684\u56fe\u89e3\uff1a\u70ed\u56fe\u3001\u4e09\u7ef4\u6563\u70b9\u56fe\u3001\u5bc6\u5ea6\u56fe\u3001\u6761\u5f62\u56fe(\u5373\u4f7f\u662f\u6761\u5f62\u56fe\u4e5f\u4f1a\u6709\u5e2e\u52a9\uff01)\nAfter going through the results, we will do a little meta-machine learning, and implement the best model on the full set of features.\n\u5728\u7814\u7a76\u4e86\u7ed3\u679c\u4e4b\u540e\uff0c\u6211\u4eec\u5c06\u8fdb\u884c\u4e00\u4e9b\u5143\u673a\u5668\u5b66\u4e60\uff0c\u5e76\u5728\u5b8c\u6574\u7684\u7279\u6027\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f73\u7684\u6a21\u578b\u3002","1e67977f":"The `reg_alpha` and `reg_lambda` best scores seem to complement one another for Bayesian optimization. In other words, if either `reg_lambda` or `reg_alpha` is high (say greater than 0.5), then the other should be low (below 0.5). These hyperparameters control a penalty placed on the weights of the trees and thus are meant to control overfitting. It might make sense if only one needs to be high then.","68767558":"### Correlations for Bayesian Optimization","82de4881":"The Bayesian slope is about 15 times greater than that of random search! What happens if we say run these methods for 10,000 iterations?","f57c761c":"Keep in mind that random search ran for more iterations (as of now). Even so, we can see that Bayesian Optimization tended to produce much more higher cross validation scores. Let's look at the statistical averages:\n\u8bf7\u8bb0\u4f4f\uff0c\u5c3d\u7ba1\u5982\u6b64\uff0c\u4e3a\u4e86\u83b7\u5f97\u66f4\u591a\u7684\u8fed\u4ee3(\u5230\u73b0\u5728\u4e3a\u6b62)\uff0c\u6211\u4eec\u4ecd\u7136\u53ef\u4ee5\u770b\u5230\uff0c\u8d1d\u53f6\u65af\u4f18\u5316\u503e\u5411\u4e8e\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u83b7\u5f97\u66f4\u591a\u7684\u5206\u6570\u3002\u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u7edf\u8ba1\u5e73\u5747\u6570\uff1a","a9d4325e":"### Function for 3D plotting\n\nAny time you write code more than twice, it should be encoded into a function! That's what the next code block is for: putting this code into a function that we can use many times! This function can be used for __any__ 3d plotting needs.","9d6735a5":"# Distribution of Scores   \u5206\u6570\u5206\u5e03\n\nWe already saw the kernel density estimate plot, so let's go on to a bar plot. First we'll get the data in a long format.\n\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u4e86\u5185\u6838\u5bc6\u5ea6\u4f30\u8ba1\u56fe\uff0c\u6240\u4ee5\u8ba9\u6211\u4eec\u7ee7\u7eed\u770b\u4e00\u4e2a\u6761\u5f62\u56fe\u3002\u6211\u4eec\u5148\u53bb\u62ff\u957f\u683c\u5f0f\u7684\u6570\u636e\u3002","641f43ce":"We can also save the features to later use for plotting feature importances.\n\n\u6211\u4eec\u8fd8\u53ef\u4ee5\u4fdd\u5b58\u8fd9\u4e9b\u7279\u6027\uff0c\u4ee5\u4fbf\u7a0d\u540e\u7ed8\u5236\u7279\u6027\u7684\u91cd\u8981\u6027\u3002","20d9dc8e":"It's a little difficult to tell much from this plot. If we look at the best values and then look at the plot, we can see that scores do tend to be higher around 0.9 for `reg_alpha`and 0.2 for `reg_lambda`.  Later, we'll make the same plot for the Bayesian Optimization for comparison.","2f1cc89d":"### Boosting Type\n\nThe boosting type deserves its own section because it is a categorical variable, and because as we will see, it has an outsized effect on model performance. First, let's calculate statistics grouped by boosting type for each search method.","7b6946e7":"The random search slope is basically zero. ","de147f28":"If we wanted, we could treat this as _another optimization problem_ and try to maximize the linear regression in terms of the score! However, for now I think we have done enough optimization. \n\n\u5982\u679c\u6211\u4eec\u613f\u610f\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u6b64\u89c6\u4e3a\u53e6\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5c1d\u8bd5\u5728\u5206\u6570\u65b9\u9762\u6700\u5927\u5316\u7ebf\u6027\u56de\u5f52!\u7136\u800c\uff0c\u76ee\u524d\u6211\u8ba4\u4e3a\u6211\u4eec\u5df2\u7ecf\u505a\u4e86\u8db3\u591f\u7684\u4f18\u5316\u3002\n\nIt's time to move on to implementing the best hyperparameter values from random and Bayesian optimization on the full dataset.\n\n\u73b0\u5728\u662f\u65f6\u5019\u5728\u5b8c\u6574\u7684\u6570\u636e\u96c6\u4e0a\u4ece\u968f\u673a\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\u5b9e\u73b0\u6700\u4f73\u8d85\u53c2\u6570\u503c\u4e86\u3002","6dbcb5e5":"# Correlations between Hyperparameters and Score\n\nTime for another dangerous act: finding correlations between the hyperparameters and the score. These are not going to be accurate because again, we are not varying one value at a time! Nonetheless, we may discover useful insight about the Gradient Boosting Machine model.","8fd50f84":"The next plot is learning rate and number of estimators versus the score. __Remember that the number of estimators was selected using early stopping for 100 rounds with 5-fold cross validation__. The number of estimators __was not__ a hyperparameter in the grid that we searched over. Early stopping is a more efficient method of finding the best number of estimators than including it in a search (based on my limited experience)!","c6bcae94":"Next we define the hyperparameter grid that was used (the same ranges applied in both searches).","8ed6268a":"Incredible! I told you this was wildly inappropriate. Nonetheless, the slope does indicate that Bayesian optimization \"learns\" the hyperparameter values that do better over time. It then concentrates on evaluating these rather than spending time exploring other values as does random search. This means it can get stuck in a local optimum and can tend to __exploit__ values rather than continue to __explore__.\n\nNow we will move on to the actual values of the hyperparameters.","8eca7cf9":"That's a lot of plot for not very much code! We can see that the number of estimators and the learning rate have the greatest magnitude correlation (ignoring subsample which is influenced by the boosting type).","7a0cf894":"We want to be careful about placing too much value in these results, because remember, the Bayesian optimization could have found a local minimum of the cross validation loss that it is exploting. Moreover, the trends here are generally pretty small. It is encouraging that the best value was found close to the end of the search indicating cross validation scores were continuing to improve. \n\nNext, we can look at the values of the score as a function of the hyperparameter values. This is again a dangerous area! ","a9250857":"# Introduction: Random vs Bayesian Optimization Model Tuning\n# \u7b80\u4ecb:\u968f\u673a\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u6a21\u578b\u8c03\u4f18\n\nIn this notebook, we will compare random search and Bayesian optimization hyperparameter tuning methods implemented in two previous notebooks.\n\n\u5728\u672c\u7b14\u8bb0\u672c\u4e2d\uff0c\u6211\u4eec\u5c06\u6bd4\u8f83\u5728\u524d\u4e24\u4e2a\u7b14\u8bb0\u672c\u4e2d\u5b9e\u73b0\u7684\u968f\u673a\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u8d85\u53c2\u6570\u8c03\u4f18\u65b9\u6cd5\u3002\n\n* [Intro to Model Tuning: Grid and Random Search](https:\/\/www.kaggle.com\/willkoehrsen\/intro-to-model-tuning-grid-and-random-search)\n\n\u6a21\u578b\u8c03\u6574\u7b80\u4ecb:\u7f51\u683c\u548c\u968f\u673a\u641c\u7d22\n* [Automated Model Tuning](https:\/\/www.kaggle.com\/willkoehrsen\/automated-model-tuning)\n\n\u81ea\u52a8\u5316\u6a21\u578b\u8c03\u4f18\n\nIn those notebooks we saw results of the methods applied to a limited dataset (10000 observations) but here we will explore results on a complete dataset with 700 + features.  The results in this notebook are from 500 iterations of random search and 400 iterations of Bayesian Optimization (these took about 5 days to run each). We will thoroughly explore the results both visually and statistically, and then implement the best hyperparameter values on a full set of features. After all the hard work in the random search and Bayesian optimization notebooks, now we get to have some fun! \n\n\u5728\u8fd9\u4e9b\u7b14\u8bb0\u4e2d\uff0c\u6211\u4eec\u770b\u5230\u4e86\u5e94\u7528\u4e8e\u6709\u9650\u6570\u636e\u96c6(10000\u6b21\u89c2\u6d4b)\u7684\u65b9\u6cd5\u7684\u7ed3\u679c\uff0c\u4f46\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u63a2\u7d22\u5305\u542b700\u591a\u4e2a\u7279\u5f81\u7684\u5b8c\u6574\u6570\u636e\u96c6\u7684\u7ed3\u679c\u3002\u8fd9\u4e2a\u7b14\u8bb0\u672c\u7684\u7ed3\u679c\u6765\u81ea500\u6b21\u968f\u673a\u641c\u7d22\u548c400\u6b21\u8d1d\u53f6\u65af\u4f18\u5316(\u6bcf\u4e00\u6b21\u8fd0\u884c\u5927\u7ea6\u9700\u89815\u5929)\u3002\u6211\u4eec\u5c06\u4ece\u89c6\u89c9\u4e0a\u548c\u7edf\u8ba1\u4e0a\u5f7b\u5e95\u63a2\u7d22\u7ed3\u679c\uff0c\u7136\u540e\u5728\u4e00\u7ec4\u5b8c\u6574\u7684\u7279\u5f81\u4e0a\u5b9e\u73b0\u6700\u4f73\u8d85\u53c2\u6570\u503c\u3002\u5728\u968f\u673a\u641c\u7d22\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u7b14\u8bb0\u672c\u4e2d\u8f9b\u82e6\u5de5\u4f5c\u4e4b\u540e\uff0c\u73b0\u5728\u6211\u4eec\u5f97\u5230\u4e86\u4e00\u4e9b\u4e50\u8da3!\n\nAt each step, we will use plenty of figures and statistics to explore the data. This will be a fun notebook (even though it may not land you at the top of the leaderboard)! \n\n\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u5927\u91cf\u7684\u6570\u5b57\u548c\u7edf\u8ba1\u6570\u636e\u6765\u63a2\u7d22\u6570\u636e\u3002\u8fd9\u5c06\u662f\u4e00\u4e2a\u6709\u8da3\u7684\u7b14\u8bb0\u672c(\u5373\u4f7f\u5b83\u53ef\u80fd\u4e0d\u4f1a\u8ba9\u4f60\u5728\u6392\u884c\u699c\u7684\u9876\u7aef)!","90ff0947":"## Score versus Iteration\n\nNow, to see if either method improves over the course of the search, we need to plot the score as a function of the iteration. ","59f78891":"Now for the next four hyperparameters versus the score."}}