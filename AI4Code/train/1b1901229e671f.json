{"cell_type":{"5e8cc90d":"code","054b0b5a":"code","0a88c96a":"code","79f5afc2":"code","e774f6cd":"code","846adacd":"code","92a35ca4":"code","9bb187c8":"code","8189debf":"code","682c28a0":"code","d44facb4":"markdown","4244eb5b":"markdown","8d734200":"markdown","f158e713":"markdown","014255f8":"markdown","751c3a7e":"markdown","69a49dca":"markdown","e59b887d":"markdown","6cab7708":"markdown","a5fe8af9":"markdown","093c4537":"markdown","0457cc20":"markdown","a2ce38e6":"markdown","3c2d37be":"markdown","28b11a11":"markdown","1e13a4ff":"markdown","b9e3179e":"markdown"},"source":{"5e8cc90d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.preprocessing import image\nnp.seterr(divide='ignore', invalid='ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n","054b0b5a":"from keras.models import load_model\nmodel = load_model('..\/input\/cats-dogs-wo-pretrain-nw\/catndogs_wo_pretrain_nw.h5')","0a88c96a":"model.summary()","79f5afc2":"img = image.load_img('..\/input\/dogpic\/dog.jpg', target_size=(128,128))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis=0)\nimg_tensor \/= 255.","e774f6cd":"import matplotlib.pyplot as plt\nplt.imshow(img_tensor[0])\nplt.xlabel('The test dog picture')\nplt.show()","846adacd":"from keras import models\nlayer_outputs = [layer.output for layer in model.layers[:8]] #Extracts the ouput of the top eight layers\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs) #creates a model that will return these outputs","92a35ca4":"activations = activation_model.predict(img_tensor) \nfirst_layer_activation = activations[0]#activation of the first convolution layer for the cat image input\nprint(first_layer_activation.shape) #It\u2019s a 126 x 126 feature map with 32 channels","9bb187c8":"import matplotlib.pyplot as plt\nplt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')","8189debf":"plt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')","682c28a0":"import math\n# defining number of layers\nlayer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name)\n    \nimages_per_row = 16    \n\n# defining number of features in the feature map\nfor layer_name, layer_activation in zip(layer_names, activations):\n    n_features = layer_activation.shape[-1]\n    size = layer_activation.shape[1]\n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,:, :,col * images_per_row + row]\n            channel_image = (channel_image-channel_image.mean())\/\/channel_image.std()  \n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n            row * size : (row + 1) * size] = channel_image\n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n    scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    ","d44facb4":"**Inference:****\n1. The first layer acts as a collection of various edge detectors. At that stage, the activations retain almost all of the information present in    the initial picture.\n2. As you go higher, the activations become increasingly abstract and less visually interpretable. They begin to encode higher-level concepts such as \u201cdog ear\u201d and \u201cdog eye.\u201d Higher presentations carry increasingly less information about the visual contents of the image, and increasingly more information related to the class of the image.\n3. The sparsity of the activations increases with the depth of the layer: in the first layer, all filters are activated by the input image; but in the following layers,more and more filters are blank. This means the pattern encoded by the filter isn\u2019t found in the input image.","4244eb5b":"For the visualization purpose, we will be using the pre-trained model from my kernel : [cats&dogs_wo_pretrain_nw](http:\/\/www.kaggle.com\/descrierx\/cats-dogs-wo-pretrain-nw).\n\nAccuracy achieved by this kernel for distinguishing between a cat and a dog is more than 86%(using a simple covnet designed from scratch).","8d734200":"This channel appears to encode edge detector.","f158e713":"This one looks like an eye detector,useful to encode dog eyes.","014255f8":"The dataset contains a pre trianed model and a test dog picture.","751c3a7e":"****Visualizing every channel in every intermediate activation...","69a49dca":"Visualizing the fourth channel...","e59b887d":"**Loading the model**\n\n","6cab7708":"Loading an input image\u2014a picture of a dog, not part of the images the network was trained on...","a5fe8af9":"Visualizing the seventh channel...","093c4537":"Any furthur insights and inputs for improvement are always appreciated.\nDo upvote ;)","0457cc20":"****Instantiating a model from an input tensor and a list of output tensors...","a2ce38e6":"****Running the model in predict mode...","3c2d37be":"Visualizing intermediate activations consists of displaying the feature maps that are\noutput by various convolution and pooling layers in a network, given a certain input\n(the output of a layer is often called its activation, the output of the activation function).\nThis gives a view into how an input is decomposed into the different filters\nlearned by the network.","28b11a11":"A model is instantiated using two arguments: an input tensor (or list of input tensors) and an output tensor(or list of output tensors). \nThe resulting class is a Keras model,just like the Sequential models, mapping the specified inputs to the specified outputs.","1e13a4ff":"*Therefore,we can conclude that a deep neural network effectively acts as an information distillation pipeline, with raw data going in (in this case, RGB pictures) and being repeatedly transformed so that irrelevant information is filtered out (for example, the specific visual appearance of the image), and useful information is magnified and refined (for example, the class of the image).*","b9e3179e":"Each channel encodes relatively independent features, so the proper way to visualize these feature maps is by independently plotting the contents of every channel as a 2D image.We will extract and plot every channel in each of the eight activation maps,and stack the results in one big image tensor, with channels stacked side by side."}}