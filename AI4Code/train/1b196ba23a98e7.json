{"cell_type":{"9f7ed1fc":"code","9731bb86":"code","8a7990e2":"code","cd82b2c7":"code","0a6dfdbc":"code","7b021b68":"code","2877612c":"code","e47301be":"code","9380fe51":"code","77d689ee":"code","a0fa42e7":"code","7fca8e71":"code","703b346d":"code","2f67a2d9":"code","143d1c3b":"code","207d94c0":"code","7d08be65":"code","1db98068":"code","6c98aa9f":"code","dcfbb255":"code","f476c3e3":"code","672a388a":"code","23b6cd69":"code","de7e4cd3":"code","627c4da7":"code","c3b32619":"code","50938891":"code","0258f4ef":"code","4fdddade":"code","4aac69aa":"code","e79b932c":"code","1386d221":"code","5510c09c":"code","bb0d27b5":"code","cdbfaa80":"code","0816d5d3":"code","0e449f14":"code","5db25018":"code","76b29721":"code","49c71d1e":"code","26295b4c":"code","4309a6e7":"code","60d77107":"code","996c07ab":"code","fbb08062":"code","7abfeab5":"code","8bdd718e":"code","c31d9a7b":"code","6f17826c":"code","4582523f":"code","5a3843cd":"code","0299f00e":"code","c794e4cf":"code","2396c5a1":"code","263cc1de":"code","f7f60774":"code","24cbe6c4":"code","c7e43b17":"code","fb5ce853":"code","50969d73":"code","b9cce50b":"code","e822ae78":"code","cca6bbc7":"code","fb5bbeae":"code","aec0c2f2":"code","75a89be7":"code","7d879dd6":"code","513514bf":"code","38145f16":"code","791374e8":"code","bebe8aaf":"code","ea67c2e0":"code","64794383":"code","f6df70f3":"code","b0d32328":"code","89a8a5d3":"code","9a6e3eaf":"code","8d75ba66":"code","fc8f7ab3":"markdown","8e411f80":"markdown","a85aa03e":"markdown","5971f764":"markdown","f1e4b119":"markdown","0e35a828":"markdown","72f11c6b":"markdown","0551df34":"markdown","98a60171":"markdown","7c682e04":"markdown","dfc36724":"markdown","53ca4e49":"markdown","c8c89183":"markdown","3f738b01":"markdown","c783a5c6":"markdown","9675f5ac":"markdown","f892b47f":"markdown","eb3e95aa":"markdown","f789ca5e":"markdown","5f7d079d":"markdown","394ab419":"markdown","738110a9":"markdown","79484dc8":"markdown","51be50b2":"markdown","dc228cec":"markdown","79fdddd1":"markdown","9af088db":"markdown","df33e4bf":"markdown","bb6c7693":"markdown"},"source":{"9f7ed1fc":"#Loading libraries \/ methods:\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.metrics import log_loss, classification_report, confusion_matrix\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, Input\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow_addons.optimizers import Lookahead\nfrom tqdm.keras import TqdmCallback\n\nsns.set()\n%matplotlib inline","9731bb86":"#Loading datasets (we'll ignore the non-scored dataset):\n\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","8a7990e2":"#Visualization of the train data:\ntrain_features.head()","cd82b2c7":"#Visualization of the test data:\ntest_features.head()","0a6dfdbc":"#Visualization of the target data:\ntrain_targets.head()","7b021b68":"#Dimension of the datasets:\n\nprint('Number of rows in training set: ', train_features.shape[0])\nprint('Number of columns in training set: ', train_features.shape[1])\nprint('Number of rows in test set: ', test_features.shape[0])\nprint('Number of columns in test set: ', test_features.shape[1])\nprint('Number of rows in target set: ',train_targets.shape[0])\nprint('Number of columns in target set: ',train_targets.shape[1])\n\n#The dataset is split into the following setting: 85% into the training \n#set (23814), 15% into the test set (3982).","2877612c":"#There are 772 gene expression features (g-0 to g-771):\ntrain_features.iloc[:, 4:776].head()","e47301be":"\n#There are 100 cell viability features (c-0 to c-99):\ntrain_features.iloc[:, 776:876].head()","9380fe51":"#Gene expression:\ntrain_gene = pd.DataFrame(train_features.iloc[:, 4:776])\n\n#Cell Viability:\ntrain_cell = pd.DataFrame(train_features.iloc[:, 776:876])\n\n#Gene expression and Cell Viability:\ntrain_gc = pd.DataFrame(train_features.iloc[:, 4:876])\ntrain_gc.head(5)","77d689ee":"#Data preprocessing (test_features)\n\n#Converting cp_type and cp_dose as binary:\ndef Preprocess(data):\n    data = data.copy()\n    data.loc[:, 'cp_type'] = data.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del data['sig_id']\n    return data\n\ntest = Preprocess(test_features)","a0fa42e7":"#Loading csv files\nx_train_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/x_train_orig.csv')\ny_train_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/y_train_orig.csv')\nx_test_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/x_test_orig.csv')\ny_test_orig = pd.read_csv('\/kaggle\/input\/orig-data\/orig_data\/y_test_orig.csv')\n\nx_train_orig = np.asmatrix(x_train_orig)\ny_train_orig = np.asmatrix(y_train_orig)\nx_test_orig = np.asmatrix(x_test_orig)\ny_tes_orig = np.asmatrix(y_test_orig)\n\nprint(x_train_orig.shape, y_train_orig.shape, x_test_orig.shape, y_test_orig.shape)","7fca8e71":"#Loading csv files\nx_train_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/x_train_wv.csv')\ny_train_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/y_train_wv.csv')\nx_test_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/x_test_wv.csv')\ny_test_wv = pd.read_csv('\/kaggle\/input\/reddata\/red_data\/y_test_wv.csv')\n\nx_train_wv = np.asmatrix(x_train_wv)\ny_train_wv = np.asmatrix(y_train_wv)\nx_test_wv = np.asmatrix(x_test_wv)\ny_test_wv= np.asmatrix(y_test_wv)\n\nprint(x_train_wv.shape, y_train_wv.shape, x_test_wv.shape, y_test_wv.shape)","703b346d":"#Loading csv files\nx_train_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/x_train_pca.csv')\ny_train_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/y_train_pca.csv')\nx_test_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/x_test_pca.csv')\ny_test_pca = pd.read_csv('\/kaggle\/input\/pca-data\/pca_data\/y_test_pca.csv')\n\nx_train_pca = np.asmatrix(x_train_pca)\ny_train_pca = np.asmatrix(y_train_pca)\nx_test_pca = np.asmatrix(x_test_pca)\ny_test_pca = np.asmatrix(y_test_pca)\n\nprint(x_train_pca.shape, y_train_pca.shape, x_test_pca.shape, y_test_pca.shape)","2f67a2d9":"#Loading csv files\nx_train_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/x_train_pca_wv.csv')\ny_train_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/y_train_pca_wv.csv')\nx_test_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/x_test_pca_wv.csv')\ny_test_pca_wv = pd.read_csv('\/kaggle\/input\/pca-wv-data\/pca_wv_data\/y_test_pca_wv.csv')\n\nx_train_pca_wv = np.asmatrix(x_train_pca_wv)\ny_train_pca_wv = np.asmatrix(y_train_pca_wv)\nx_test_pca_wv = np.asmatrix(x_test_pca_wv)\ny_tes_pca_wv = np.asmatrix(y_test_pca_wv)\n\nprint(x_train_pca_wv.shape, y_train_pca_wv.shape, x_test_pca_wv.shape, y_test_pca_wv.shape)","143d1c3b":"#Distribution of data in the training dataset, taking into consideration the\n#features: cp_type, cp_time and cp_dose.\n\nfig, axarr = plt.subplots(2, 2, figsize=(12, 12))\n\n#cp_time\nsns.countplot(train_features.select_dtypes('int64')['cp_time'], ax=axarr[0][0])\naxarr[0][0].set_title(\"Exposure time (in hours)\")\n\n#cp_type\nsns.countplot(train_features.select_dtypes('object')['cp_type'], ax=axarr[0][1])\naxarr[0][1].set_title(\"Type of perturbation (compound or control)\")\n\n#cp_dose\nsns.countplot(train_features.select_dtypes('object')['cp_dose'], ax=axarr[1][0])\naxarr[1][0].set_title(\"Dosage (high or low)\")\n\naxarr[1][1].set_axis_off()","207d94c0":"#Distribution of data in the test dataset, taking into consideration the\n#features: cp_type, cp_time and cp_dose.\n\nfig, axarr = plt.subplots(2, 2, figsize=(12, 12))\n\n#cp_time\nsns.countplot(test_features.select_dtypes('int64')['cp_time'], ax=axarr[0][0])\naxarr[0][0].set_title(\"Exposure time (in hours)\")\n\n#cp_type\nsns.countplot(test_features.select_dtypes('object')['cp_type'], ax=axarr[0][1])\naxarr[0][1].set_title(\"Type of perturbation (compound or vehicle)\")\n\n#cp_dose\nsns.countplot(test_features.select_dtypes('object')['cp_dose'], ax=axarr[1][0])\naxarr[1][0].set_title(\"Dosage (high or low)\")\n\naxarr[1][1].set_axis_off()","7d08be65":"#Distributions for the first five gene expression features\n#with respect to the cp_time variable \n\ngene_columns = [cols for cols in train_features.columns if cols.startswith('g-')]\n\nsns.pairplot(data=train_features[gene_columns[0:5]+['cp_time']],\n             hue='cp_time', dropna=True, palette='Set2')","1db98068":"#Distributions for the first five gene expression features\n#with respect to the cp_dose variable \n\nsns.pairplot(data=train_features[gene_columns[5:10]+['cp_dose']],\n             hue='cp_dose', dropna=True, palette='Set2')","6c98aa9f":"#Distributions for the first five gene expression features\n#with respect to the cp_type variable \n\nsns.pairplot(data=train_features[gene_columns[10:15]+['cp_type']],\n             hue='cp_type', dropna=True, palette='Set2')","dcfbb255":"#Distributions for the first five cell viability features\n#with respect to the cp_time variable \n\ncell_columns = [cols for cols in train_features.columns if cols.startswith('c-')]\n\nsns.pairplot(data=train_features[cell_columns[0:5]+['cp_time']],\n             hue='cp_time', dropna=True, palette='Set2')","f476c3e3":"#Distributions for the first five cell viability features\n#with respect to the cp_dose variable \n\nsns.pairplot(data=train_features[cell_columns[5:10]+['cp_dose']],\n             hue='cp_dose', dropna=True, palette='Set2')","672a388a":"#Distributions for the first five cell viability features\n#with respect to the cp_type variable \n\nsns.pairplot(data=train_features[cell_columns[10:15]+['cp_type']],\n             hue='cp_type', dropna=True, palette='Set2')","23b6cd69":"#Heatmap: Correlation matrices of 15 cell viability features and 15 gene expression features\n\nframes = [train_gene.iloc[:, 1:30], train_cell.iloc[:, 1:30]]\ndf = pd.concat(frames)\n\ncorr = df.corr(method='pearson')\n\n# plot\nax = plt.figure(figsize=(9,9))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.heatmap(corr, cmap=cmap, square=True, linewidths=.8, center = 0.8,\n            cbar_kws={\"label\": \"pearson correlation coefficient\", \"shrink\": .6})\n\nplt.title('Gene Expression and Cellular Viability Correlations', size=20, pad=10)","de7e4cd3":"#Canonical Correlation Analysis (CCA): Gene Exp. vs Cell Viability\n\ncca = CCA(n_components=1)\ncca.fit(train_gene, train_cell)\n\nU_c, V_c = cca.transform(train_gene, train_cell)\nplt.plot(U_c, V_c)\nresult = np.corrcoef(U_c.T, V_c.T)[0,1]\nresult","627c4da7":"#There is a linear relationship between the gene expression features and the cell viability features.","c3b32619":"#Distribution of MoA targets\n\nmoa_counts = train_targets.sum(axis=1)\nax = sns.countplot(moa_counts, palette='Set2')\nax.set_title(\"# MoAs by perturbation\")","50938891":"#Frequency of MoA targets\n\nfor freq in set(moa_counts):\n    frequency = len(moa_counts[moa_counts == freq])\/len(moa_counts)\n    \n    print(str(freq)+': '+str(round(frequency,6)*100)+'%')","0258f4ef":"#Most frequent MoA vs least frequent MoA target\n\nper_moa_counts = train_targets.transpose()[1:].sum(axis=1).sort_values(ascending=False)\n\nfig, axarr = plt.subplots(2, 2, figsize=(12, 12))\n\n#more_MoAs\nper_moa_counts[:5].sort_values(ascending=True).plot(kind='barh', x='index', legend=False, ax=axarr[0][0], color='orange')\naxarr[0][0].set_title(\"Most frequent MoAs\")\n\naxarr[0][1].set_axis_off()\n\naxarr[1][0].set_axis_off()\n\n#less_MoAs\nper_moa_counts[-5:].sort_values(ascending=True).plot(kind='barh', x='index', legend=False, ax=axarr[1][1], color='purple')\naxarr[1][1].set_title(\"Least frequent MoAs\")","4fdddade":"#PCA: gene expression features\n#n_components = 0.93: at least 93% of the data variance explained.\n\npca = PCA(n_components = 0.93)\ng_pca = pca.fit_transform(train_features.iloc[:, 4:776]) \ng_pca.shape","4aac69aa":"#The 442 Principal Components\n\npc_df = pd.DataFrame(data = g_pca, columns = range(1, 443))\npc_df = pc_df.add_prefix('PC')\n\npc_df['cp_type']= train_features['cp_type']\npc_df['cp_dose']= train_features['cp_dose']\npc_df['cp_time']= train_features['cp_time']\n\npc_df.head(3)","e79b932c":"#Scree plot: gene expression (explained variance ratio)\n\nexplained_variance_v = (pca.explained_variance_ratio_)*100 \n\nper_var = np.round(explained_variance_v[:10,], decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n \nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot (gene expression)')\nplt.show()","1386d221":"#Cumulative explained variance for gene expression features\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('PCA: Cumulative Explained Variance for Gene Expression Variables')","5510c09c":"#Distribution of the gene features w.r.t. the CP1 and CP2,\n#highlighting the cp_time, cp_dose, and cp_type variables:\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\n\nsns.scatterplot(x= \"PC1\", y= \"PC2\", data=pc_df, hue=\"cp_time\", s=100, ax = ax1)\nsns.scatterplot(x=\"PC1\", y=\"PC2\", data=pc_df, hue=\"cp_dose\", s=100, ax = ax2, palette = \"pastel\")\nsns.scatterplot(x=\"PC1\", y=\"PC2\", data=pc_df, hue=\"cp_type\", s=100, ax = ax3, palette = \"pastel\")","bb0d27b5":"#PCA: cell viability features\n#n_components = 0.93: at least 93% of the data variance explained.\n\npca2 = PCA(n_components = 0.93)\nv_pca = pca2.fit_transform(train_features.iloc[:, 777:876])\nv_pca.shape","cdbfaa80":"#The 30 Principal Components\n\nviab_df = pd.DataFrame(data = v_pca, columns = range(1, 31))\nviab_df = viab_df.add_prefix('PC')\n\nviab_df['cp_type']= train_features['cp_type']\nviab_df['cp_dose']= train_features['cp_dose']\nviab_df['cp_time']= train_features['cp_time']\n\nviab_df.head(3)","0816d5d3":"#Scree plot: Cell viability (explained variance ratio)\n\nexplained_variance_v = (pca2.explained_variance_ratio_)*100 \n\nper_var = np.round(explained_variance_v[:10,], decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n \nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot (cell viability)')\nplt.show()","0e449f14":"#Cumulative explained variance for cellular viability features\n\nplt.plot(np.cumsum(pca2.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('PCA: Cumulative Explained Variance for Cellular Viability Variables')","5db25018":"#Distribution of the cell viab. features w.r.t. the CP1 and CP2,\n#highlighting the cp_time, cp_dose, and cp_type variables:\n\nfig2, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,5))\n\nsns.scatterplot(x = \"PC1\", y = \"PC2\", data = viab_df, hue=\"cp_time\", s = 100, ax = ax1)\nsns.scatterplot(x = \"PC1\", y = \"PC2\", data = viab_df, hue=\"cp_dose\", s = 100, ax = ax2, palette = \"pastel\")\nsns.scatterplot(x = \"PC1\", y = \"PC2\", data = viab_df, hue=\"cp_type\", s = 100, ax = ax3, palette = \"pastel\")","76b29721":"#Standardization\nscaler=StandardScaler()\nscaler.fit(train_gc)\n\nscaled_data=scaler.transform(train_gc)\n\n\n#PCA: Gene expression & Cell vialbility\n#n_components=0.95: Keep features explaining at least 95% of the variability.\n\npca_gc = PCA(n_components=0.95) #at least 95% of the variability.\npca_gc.fit(scaled_data)\n\ngc_pca = pca_gc.transform(scaled_data)\nprint(scaled_data.shape, gc_pca.shape)","49c71d1e":"#Cumulative explained variance for Gene Exp. and Cell Viab. features\n\nplt.plot(np.cumsum(pca_gc.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('PCA: Cumulative Explained Variance for Gene Exp. and Cell Viab. Variables')","26295b4c":"print(np.cumsum(pca_gc.explained_variance_ratio_)[487])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[493])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[500])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[510])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[520])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[530])\nprint(np.cumsum(pca_gc.explained_variance_ratio_)[540])","4309a6e7":"#Scree plot: first 10 PCs\n\npca_gc_10 = PCA(n_components=10) #the first ten PC\npca_gc_10.fit(scaled_data)\n\ngc_pca_10 = pca_gc_10.transform(scaled_data)\n\nper_var = np.round(pca_gc_10.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n \nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot (gene exp. and cell viab.)')\nplt.show()","60d77107":"#Plotting the features w.r.t. CP1 and CP2:\n\nplt.figure(figsize=(8,6))\nplt.scatter(gc_pca[:,0],gc_pca[:,1])\nplt.xlabel('First principle component')\nplt.ylabel('Second principle component')","996c07ab":"#Building the Logistic Regression model (neural network without hidden layers):\n\ndef LR_Model(inputsize):\n    classifier = Sequential()\n    classifier.add(Dense(206, activation=\"sigmoid\", \n                              kernel_regularizer=regularizers.l1(0),\n                              input_dim=inputsize))\n    \n    classifier.compile(optimizer='adam', loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n    \n    return classifier","fbb08062":"#Fitting the LR model (original data):\n\nLR_Model_orig = LR_Model(875)\n\nhistory_lr_orig = LR_Model_orig.fit(x_train_orig, \n                              y_train_orig,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","7abfeab5":"#Training \/ Validation Loss (original data)\n\ntrg_loss_orig = history_lr_orig.history['loss']\nval_loss_orig = history_lr_orig.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_orig, 'b',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","8bdd718e":"#Loss for the test partition:\n\nLR_Model_orig.evaluate(x_test_orig, y_test_orig)\nprint(min(history_lr_orig.history[\"val_loss\"]))\nprint(history_lr_orig.history[\"val_loss\"][299])","c31d9a7b":"#Fitting the LR model (reducted data - without clt_vehicle):\n\nLR_Model_red = LR_Model(875)\n\nhistory_lr_red = LR_Model_red.fit(x_train_wv, \n                              y_train_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","6f17826c":"#Training \/ Validation Loss (reduced data)\n\ntrg_loss_red = history_lr_red.history['loss']\nval_loss_red = history_lr_red.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_red, 'k',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_red, 'y',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (reduced data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","4582523f":"#Loss for the test partition\n\nLR_Model_red.evaluate(x_test_wv, y_test_wv)\nprint(min(history_lr_red.history[\"val_loss\"]))\nprint(history_lr_red.history[\"val_loss\"][299])","5a3843cd":"#Fitting the LR model (PCA data):\n\nLR_Model_pca = LR_Model(490)\n\nhistory_lr_pca = LR_Model_pca.fit(x_train_pca, \n                              y_train_pca,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","0299f00e":"#Training \/ Validation Loss (PCA data)\n\ntrg_loss_pca = history_lr_pca.history['loss']\nval_loss_pca = history_lr_pca.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_pca, 'g',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_pca, 'm',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","c794e4cf":"#Loss for the test partition\n\nLR_Model_pca.evaluate(x_test_pca, y_test_pca)\nprint(min(history_lr_pca.history[\"val_loss\"]))\nprint(history_lr_pca.history[\"val_loss\"][299])","2396c5a1":"#Fitting the LR model (reduced PCA data):\n\nLR_Model_pca_wv = LR_Model(490)\n\nhistory_lr_pca_wv = LR_Model_pca.fit(x_train_pca_wv, \n                              y_train_pca_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","263cc1de":"#Training \/ Validation Loss (reduced PCA data)\n\ntrg_loss_pca_wv = history_lr_pca_wv.history['loss']\nval_loss_pca_wv = history_lr_pca_wv.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_pca_wv, 'c',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - LR model (reduced PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","f7f60774":"#Loss for the test partition\n\nLR_Model_pca_wv.evaluate(x_test_pca_wv, y_test_pca_wv)\nprint(min(history_lr_pca_wv.history[\"val_loss\"]))\nprint(history_lr_pca_wv.history[\"val_loss\"][299])","24cbe6c4":"#Validation Loss (original, reduced, PCA, and reduced PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss (original)')\nplt.plot(epochs, val_loss_red, 'y',  linewidth=3, label='Validation Loss (reduced)')\nplt.plot(epochs, val_loss_pca, 'm',  linewidth=3, label='Validation Loss (PCA)')\nplt.plot(epochs, val_loss_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss (reduced PCA)')\nplt.title(\"Validation Loss - LR model (original, reduced, PCA, and reduced PCA data)  \")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nax.patch.set_edgecolor('black')\nax.patch.set_linewidth('1')  \nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","c7e43b17":"#Hyperparameter optimization (tuning)\n#Neural network model: \n\n'''\ndef NeuralNet(optimizer, kernel_initializer, activation, neurons, drop_rate):\n    classifier = Sequential()\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer, input_dim=875))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(drop_rate))\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(drop_rate))\n    classifier.add(Dense(units = neurons, activation=activation,\n                        kernel_initializer=kernel_initializer))\n    classifier.add(BatchNormalization())\n    classifier.add(Dense(206, activation=\"sigmoid\"))\n    classifier.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = NeuralNet)\n'''\n\n#Grid Search method:\n\n'''\nparameters = {'batch_size': [1000, 2300],\n              'epochs': [50, 100, 300],\n              'optimizer': ['adam', 'adamw'],\n              'kernel_initializer': ['random_uniform', 'normal'], \n              'activation': ['relu', 'leaky-relu', 'elu'],\n              'neurons': [1048, 512, 256, 128],\n              'drop_rate': [0.2, 0.3, 0.4]}\n\ngrid_search = GridSearchCV(estimator = classifier, \n                           param_grid = parameters, \n                           scoring = 'accuracy', cv = 5)\n\ngrid_search = grid_search.fit(x_train_orig, y_train_orig)\n\nbest_param = grid_search.best_params_\nbest_param\nbest_acc = grid_search.best_score_\nbest_acc\n'''","fb5ce853":"#Hyperparameters were set by the Grid Search method (previous code):\n\nbatchsize_hyp = 2300\nepochs_hyp = 300\nhiddenlayers_hyp = 512\ndropout_hyp = 0.3\nopt_hyp = 'adam'","50969d73":"#Building the DNN model:\n\ndef DNN_Model(inputsize):\n    classifier = Sequential()\n    classifier.add(WeightNormalization(Dense(512, activation=\"relu\", input_dim=inputsize)))\n    classifier.add(BatchNormalization())\n    classifier.add(Dropout(0.3))\n    classifier.add(WeightNormalization(Dense(256, activation=\"relu\"))) \n    classifier.add(BatchNormalization())\n    classifier.add(WeightNormalization(Dense(128, activation=\"relu\"))) \n    classifier.add(BatchNormalization())\n    classifier.add(WeightNormalization(Dense(206, activation=\"sigmoid\"))) \n    \n    classifier.compile(optimizer='adam', loss = 'binary_crossentropy', \n                       metrics = ['accuracy'])\n    \n    return classifier","b9cce50b":"#Fitting the DNN model (original data):\n\nDNN_Model_orig = DNN_Model(875)\n\nhistory_dnn_orig = DNN_Model_orig.fit(x_train_orig, \n                              y_train_orig,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","e822ae78":"#Training \/ Validation Loss (original data)\n\ntrg_loss_dnn_orig = history_dnn_orig.history['loss']\nval_loss_dnn_orig = history_dnn_orig.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_orig, 'b',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_orig, 'r',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","cca6bbc7":"#Loss for the test partition\n\nDNN_Model_orig.evaluate(x_test_orig, y_test_orig)\nprint(min(history_dnn_orig.history[\"val_loss\"]))\nprint(history_dnn_orig.history[\"val_loss\"][299])","fb5bbeae":"#Fitting the DNN model (reducted data - without clt_vehicle):\n\nDNN_Model_red = DNN_Model(875)\n\nhistory_dnn_red = DNN_Model_red.fit(x_train_wv, \n                              y_train_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","aec0c2f2":"#Training \/ Validation Loss (reduced data)\n\ntrg_loss_dnn_red = history_dnn_red.history['loss']\nval_loss_dnn_red = history_dnn_red.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_red, 'k',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_red, 'y',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (reduced data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","75a89be7":"#Loss for the test partition\n\nDNN_Model_red.evaluate(x_test_wv, y_test_wv)\nprint(min(history_dnn_red.history[\"val_loss\"]))\nprint(history_dnn_red.history[\"val_loss\"][299])","7d879dd6":"#Fitting the DNN model (PCA data):\n\nDNN_Model_pca = DNN_Model(490)\n\nhistory_dnn_pca = DNN_Model_pca.fit(x_train_pca, \n                              y_train_pca,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","513514bf":"#Training \/ Validation Loss (PCA data)\n\ntrg_loss_dnn_pca = history_dnn_pca.history['loss']\nval_loss_dnn_pca = history_dnn_pca.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_pca, 'g',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_pca, 'm',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","38145f16":"#Loss for the test partition\n\nDNN_Model_pca.evaluate(x_test_pca, y_test_pca)\nprint(min(history_dnn_pca.history[\"val_loss\"]))\nprint(history_dnn_pca.history[\"val_loss\"][299])","791374e8":"#Fitting the DNN model (reduced PCA data):\n\nDNN_Model_pca_wv = DNN_Model(875)\n\nhistory_dnn_pca_wv = DNN_Model_pca_wv.fit(x_train_pca_wv, \n                              y_train_pca_wv,\n                              validation_split=0.3,\n                              epochs=300, \n                              batch_size=2300,\n                              verbose=0,\n                              callbacks=[TqdmCallback()])","bebe8aaf":"#Training \/ Validation Loss (original data)\n\ntrg_loss_dnn_pca_wv = history_dnn_pca_wv.history['loss']\nval_loss_dnn_pca_wv = history_dnn_pca_wv.history['val_loss']\nepochs = range(1, 301)\n\nfig = plt.figure(figsize=(20,8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, trg_loss_dnn_pca_wv, 'c',  linewidth=3, label='Training Loss')\nplt.plot(epochs, val_loss_dnn_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss')\nplt.title(\"Training \/ Validation Loss - DNN model (original data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","ea67c2e0":"#Loss for the test partition\n\nDNN_Model_pca_wv.evaluate(x_test_pca_wv, y_test_pca_wv)\nprint(min(history_dnn_pca_wv.history[\"val_loss\"]))\nprint(history_dnn_pca_wv.history[\"val_loss\"][299])","64794383":"#Validation Loss (original, reduced, PCA, and reduced PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_dnn_orig, 'r',  linewidth=3, label='Validation Loss (original)')\nplt.plot(epochs, val_loss_dnn_red, 'y',  linewidth=3, label='Validation Loss (reduced)')\nplt.plot(epochs, val_loss_dnn_pca, 'm',  linewidth=3, label='Validation Loss (PCA)')\nplt.plot(epochs, val_loss_dnn_pca_wv, 'darkgreen',  linewidth=3, label='Validation Loss (reduced PCA)')\nplt.title(\"Validation Loss - DNN model (original, reduced, PCA, and reduced PCA data)  \")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","f6df70f3":"#Validation Loss - DNN vs LR model (origina and PCA data)   \n\nfig = plt.figure(figsize=(20, 8))\nax = fig.add_subplot(1, 2, 1)\nplt.plot(epochs, val_loss_orig, 'r',  linewidth=3, label='Validation Loss (LR - original)')\nplt.plot(epochs, val_loss_pca, 'y',  linewidth=3, label='Validation Loss (LR - PCA)')\nplt.plot(epochs, val_loss_dnn_orig, 'm',  linewidth=3, label='Validation Loss (DNN - original)')\nplt.plot(epochs, val_loss_dnn_pca, 'darkgreen',  linewidth=3, label='Validation Loss (DNN - PCA)')\nplt.title(\"Validation Loss - DNN vs LR model (original and PCA data)\")\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Epochs\")\nax.set_facecolor(\"white\")\nplt.legend(loc='best')\n    \nplt.tight_layout()\nplt.show()","b0d32328":"sample_submit = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\nsample_submit.head()","89a8a5d3":"submit_pred = DNN_Model_orig.predict(test)","9a6e3eaf":"sample_submit.iloc[:,1:] = submit_pred\nsample_submit.head()","8d75ba66":"sample_submit.to_csv(\"submission.csv\", index=False, header=True) ","fc8f7ab3":"<div id=\"lr\"><\/div>\n\n## 5. A Logistic Regression Model (neural networks with no hidden layers)","8e411f80":"<div id=\"targets\"><\/div>\n\n### 3.5. MoA Targets","a85aa03e":"# Mechanisms of Action Prediction\n\n<br>\n\n### Contents\n\n* <a href=\"#intro\">1. Introduction<\/a>\n   * <a href=\"#datadec\">1.1. Data description<\/a>\n- <a href=\"#data\">2. Data overview and data preprocessing<\/a>\n* <a href=\"#eda\">3. Exploratory Data Analysis<\/a>\n   * <a href=\"#cp\">3.1. Treatment features (cp_type, cp_time, cp_dose)<\/a>\n   * <a href=\"#gene\">3.2. Gene expression features<\/a>\n   - <a href=\"#cell\">3.3. Cell viability features<\/a>\n   - <a href=\"#corr\">3.4. Correlations: cell viability and gene expression features<\/a>\n   - <a href=\"#targets\">3.5. MoA Targets<\/a>\n- <a href=\"#pca\">4. Dimensionality reduction via PCA<\/a>\n   - <a href=\"#pcag\">4.1. PCA: Gene expression<\/a>\n   - <a href=\"#pcac\">4.2. PCA: Cell viability<\/a>\n   - <a href=\"#pcac\">4.3. PCA: Gene expression and Cell viability<\/a>\n- <a href=\"#lr\">5. A Logistic Regression Model (neural networks without hidden layers)<\/a>\n   - <a href=\"#data-prep\">5.1. Data preprocessing<\/a>\n   - <a href=\"#hyper-lr\">5.2. Lambda optimization<\/a>\n   - <a href=\"#train-original-lr\">5.3. Predictions for the original data<\/a>\n   - <a href=\"#train-red-lr\">5.4. Predictions for the reduced data<\/a>\n   - <a href=\"#train-pca-lr\">5.5. Predictions for the PCA data<\/a>\n   - <a href=\"#train-pca_wv-lr\">5.6. Predictions for the reduced PCA data\n   - <a href=\"#comp-lr\">5.7. Comparing the results<\/a>\n- <a href=\"#dnn\">6. Multi-label classification via Deep Neural Networks<\/a>\n   - <a href=\"#hyper\">6.1. Hyperparameter optimization<\/a>\n   - <a href=\"#train-original\">6.2. Predictions for the original data<\/a>\n   - <a href=\"#train-red\">6.3. Predictions for the reduced data<\/a>\n   - <a href=\"#train-pca\">6.4. Predictions for the PCA data<\/a>\n   - <a href=\"#train-pca_wv\">6.5. Predictions for the reduced PCA data\n   - <a href=\"#comp\">6.6. Comparing the results<\/a>\n   - <a href=\"#comp_models\">6.7. Comparing the models<\/a>\n- <a href=\"#submit\">7. Submission<\/a>","5971f764":"<div id=\"cell\"><\/div>\n\n### 3.3. Cell viability features","f1e4b119":"### 6.7. Comparing the models","0e35a828":"### 5.4. Predictions for the PCA data","72f11c6b":"<div id=\"pca-gc\"><\/div>\n\n### 4.3. PCA: Gene expression and Cell viability","0551df34":"<div id=\"pcag\"><\/div>\n\n### 4.1. PCA: Gene expression","98a60171":"<div id=\"gene\"><\/div>\n\n### 3.2. Gene expression features","7c682e04":"<div id=\"corr\"><\/div>\n\n### 3.4. Correlations: cell viability and gene expression features","dfc36724":"# <div id=\"eda\"><\/div>\n\n## 3. Exploratory Data Analysis","53ca4e49":"### 6.2. Predictions for the original data","c8c89183":"<div id=\"data\"><\/div>\n\n## 2. Data overview and data preprocessing","3f738b01":"<div id=\"cp\"><\/div>\n\n### 3.1. Treatment features (cp_type, cp_time, cp_dose)","c783a5c6":"<div id=\"submit\"><\/div>\n\n## 7. Submission","9675f5ac":"<div id=\"#comp\"><\/div>\n\n### 6.6. Comparing the results","f892b47f":"<div id=\"#train-red\"><\/div>\n\n### 6.3. Predictions for the reduced data","eb3e95aa":"<div id=\"#hyper\"><\/div>\n\n### 6.1. Hyperparameter optimization (tuning)","f789ca5e":"### 5.6. Comparing the results","5f7d079d":"### 5.5. Predictions for the reduced PCA data","394ab419":"<div id=\"#train-pca_wv\"><\/div>\n\n### 6.5. Predictions for the reduced PCA data","738110a9":"<div id=\"pcac\"><\/div>\n\n### 4.2. PCA: Cell viability","79484dc8":"<div id=\"pca\"><\/div>\n\n## 4. Dimensionality reduction via PCA","51be50b2":"<div id=\"intro\"><\/div>\n\n## 1. Introduction\n\nThe [Connectivity Map](https:\/\/clue.io\/), a project within the Broad Institute of MIT and [Harvard, the Laboratory for Innovation Science at Harvard (LISH)](https:\/\/lish.harvard.edu\/), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.\n\n\n<b>What is the Mechanism of Action (MoA) of a drug? And why is it important?<\/b>\n\nIn the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanism of a disease. In this new framework, scientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\n<br>\n\n<div id=\"datadec\"><\/div>\n\n### 1.1. Data description\n\n<code>train_features.csv<\/code>: Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n\n<code>cp_type (categorical)<\/code>: Samples treated with a compound or with a control perturbation. Categories include \"trt_cp\" and \"ctl_vehicle\", respectively. There is no MoA for \"ctl_vehicle\".\n\n<code>cp_time (categorical)<\/code>: Treatment duration in hours. Categories include 24, 48, and 72 hours.\n\n<code>cp_dose (categorical)<\/code>: Drug dose. Categories include \"D1\", \"D2\" for low and high dose, respectively.\n\n<code>g-[0-771] (continous)<\/code>: Gene expression data - a measure of activation in a given gene after the drug is applied.\n\n<code>c-[0-99] (continous)<\/code>: Cell viability. Basically count of live cells after the drug is applied.\n    \n\n<code>train_targets_scored.csv<\/code>: The binary MoA targets that are scored. There are 206 MoA targets.\n\n<code>test_features.csv<\/code>: Features for the test data. We must predict the probability of each scored MoA for each row in the test data.\n    \n\n<hr>","dc228cec":"<div id=\"dnn\"><\/div>\n\n## 6. Multi-label classification via Deep Neural Networks","79fdddd1":"### 5.1. Lambda optimization\n\nhttps:\/\/www.kaggle.com\/heitorbaldo\/moa-prediction-pca-dnn-r","9af088db":"### 5.3. Predictions for the reduced data","df33e4bf":"### 5.2. Predictions for the original data","bb6c7693":"<div id=\"#train-pca\"><\/div>\n\n### 6.4. Predictions for the PCA data"}}