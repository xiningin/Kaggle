{"cell_type":{"fd78d053":"code","dd0a9dad":"code","ab57ac73":"code","efc19a08":"code","db2f0290":"code","edb62c28":"code","2426c656":"code","3ab5a80a":"code","091a1b75":"code","9ca51726":"code","29443007":"code","7c9f9ae3":"code","2d08af9f":"code","a01e2a9d":"code","28e0f815":"code","20b9b79e":"code","a68d837a":"code","bbf08c36":"code","21eb2bf9":"code","792f1b10":"code","b3150abf":"code","cf45e579":"code","4d574393":"code","827fa34d":"code","7206494e":"code","24400da4":"code","ffe290fd":"code","d0e47d65":"code","536e3a9d":"code","9ba9a32e":"code","1a440cd3":"code","81c86c82":"code","4b9fa935":"code","5b802f76":"code","4179c28b":"code","0212d146":"code","6ea58e4b":"code","f10a489c":"code","f87968a4":"code","b9b381dc":"code","5d845326":"code","408f5672":"code","3e2ec3e9":"code","6ae5078e":"code","a3dceda6":"code","2178b1a9":"code","57831e2c":"code","97635987":"code","a4c2efb5":"code","a6ea2d30":"code","64cafeec":"code","ed206f86":"code","3f58dcec":"code","084ef5aa":"code","8f2c9515":"code","71d31f57":"markdown","a07739f2":"markdown","46c90418":"markdown","0e41bbfb":"markdown","ef998ba7":"markdown","8c2e13c7":"markdown","cb606975":"markdown","74717489":"markdown","8ce7749b":"markdown","51f724fc":"markdown","314afc99":"markdown","1c3ccd8a":"markdown","55d2d0d7":"markdown","846eef14":"markdown","96befaa5":"markdown","3343b534":"markdown","5338d054":"markdown","c87e5de4":"markdown","76425fbe":"markdown","2e88ae4b":"markdown","031195e8":"markdown","6e1ab6aa":"markdown","f0a22b9a":"markdown","0b9e8f28":"markdown","b43379c4":"markdown","1f26e9a4":"markdown","65192e1c":"markdown","b3a05e4e":"markdown","ec9c4708":"markdown","f8fad27f":"markdown","3c2177da":"markdown","7e258a5a":"markdown","6d796b20":"markdown","aa2c4a9b":"markdown","3f8926a0":"markdown"},"source":{"fd78d053":"#importing the required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.model_selection import train_test_split\n\nfrom numpy import mean, std","dd0a9dad":"#reading the dataset and converting it to dataframe\ndf = pd.read_csv(\"..\/input\/adult-census-income\/adult.csv\")","ab57ac73":"#Viewing the top 5 rows of our dataset\ndf.head()","efc19a08":"sns.countplot(df.income)","db2f0290":"sns.distplot(df[df.income=='<=50K'].age, color='g')\nsns.distplot(df[df.income=='>50K'].age, color='r')","edb62c28":"plt.xticks(rotation=90)\nsns.countplot(df.workclass, hue=df.income, palette='tab10')","2426c656":"sns.distplot(df[df.income=='<=50K'].fnlwgt, color='r')\nsns.distplot(df[df.income=='>50K'].fnlwgt, color='g')","3ab5a80a":"plt.xticks(rotation=90)\nsns.countplot(df.education, hue=df.income, palette='muted')","091a1b75":"sns.countplot(df[\"education.num\"], hue=df.income)","9ca51726":"plt.xticks(rotation=90)\nsns.countplot(df['marital.status'], hue=df.income)","29443007":"plt.xticks(rotation=90)\nsns.countplot(df.occupation, hue=df.income, palette='rocket')","7c9f9ae3":"plt.xticks(rotation=90)\nsns.countplot(df.relationship, hue=df.income, palette='muted')","2d08af9f":"plt.xticks(rotation=90)\nsns.countplot(df.race, hue=df.income, palette='Set2')","a01e2a9d":"plt.xticks(rotation=90)\nsns.countplot(df.sex, hue=df.income)","28e0f815":"df['capital.gain'].value_counts()","20b9b79e":"df['capital.loss'].value_counts()","a68d837a":"sns.distplot(df[df.income=='<=50K']['hours.per.week'], color='b')\nsns.distplot(df[df.income=='>50K']['hours.per.week'], color='r')","bbf08c36":"df['native.country'].value_counts()","21eb2bf9":"df[df.select_dtypes(\"object\") ==\"?\"] = np.nan\nnans = df.isnull().sum()\nif len(nans[nans>0]):\n    print(\"Missing values detected.\\n\")\n    print(nans[nans>0])\nelse:\n    print(\"No missing values. You are good to go.\")","792f1b10":"#majority of the values are \"Private\". Lets fill the missing values as \"Private\".\ndf.workclass.fillna(\"Private\", inplace=True)\n\ndf.occupation.fillna(method='bfill', inplace=True)\n\n#majority of the values are \"United-States\". Lets fill the missing values as \"United-States\".\ndf['native.country'].fillna(\"United-States\", inplace=True)\n\nprint(\"Handled missing values successfully.\")","b3150abf":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils import column_or_1d\n\nclass MyLabelEncoder(LabelEncoder):\n\n    def fit(self, y, arr=[]):\n        y = column_or_1d(y, warn=True)\n        if arr == []:\n            arr=y\n        self.classes_ = pd.Series(arr).unique()\n        return self\n\nle = MyLabelEncoder()","cf45e579":"# age_enc = pd.cut(df.age, bins=(0,25,45,65,100), labels=(0,1,2,3))\ndf['age_enc'] = df.age.apply(lambda x: 1 if x > 30 else 0)\n\ndef prep_workclass(x):\n    if x == 'Never-worked' or x == 'Without-pay':\n        return 0\n    elif x == 'Private':\n        return 1\n    elif x == 'State-gov' or x == 'Local-gov' or x == 'Federal-gov':\n        return 2\n    elif x == 'Self-emp-not-inc':\n        return 3\n    else:\n        return 4\n\ndf['workclass_enc'] = df.workclass.apply(prep_workclass)\n\ndf['fnlwgt_enc'] = df.fnlwgt.apply(lambda x: 0 if x>200000 else 1)\n\nle.fit(df.education, arr=['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th','10th', '11th', '12th', \n                                             'HS-grad', 'Prof-school', 'Assoc-acdm', 'Assoc-voc', 'Some-college', 'Bachelors', 'Masters', 'Doctorate'])\ndf['education_enc'] = le.transform(df.education)\n\n\ndf['education.num_enc'] = df['education.num'].apply(lambda x: 1 if x>=9 else 0)\n\ndf['marital.status_enc'] = df['marital.status'].apply(lambda x: 1 if x=='Married-civ-spouse' or x == 'Married-AF-spouse' else 0)\n\ndef prep_occupation(x):\n    if x in ['Prof-specialty', 'Exec-managerial', 'Tech-support', 'Protective-serv']:\n        return 2\n    elif x in ['Sales', 'Craft-repair']:\n        return 1\n    else:\n        return 0\n\ndf['occupation_enc'] = df.occupation.apply(prep_occupation)\n\ndf['relationship_enc'] = df.relationship.apply(lambda x: 1 if x in ['Husband', 'Wife'] else 0)\n\ndf['race_enc'] = df.race.apply(lambda x: 1 if x=='White' else 0)\n\ndf['sex_enc'] = df.sex.apply(lambda x: 1 if x=='Male' else 0)\n\ndf['capital.gain_enc'] = pd.cut(df[\"capital.gain\"], \n                                bins=[-1,0,df[df[\"capital.gain\"]>0][\"capital.gain\"].median(), df[\"capital.gain\"].max()], labels=(0,1,2)).astype('int64')\n\ndf['capital.loss_enc'] = pd.cut(df[\"capital.loss\"], \n                                bins=[-1,0,df[df[\"capital.loss\"]>0][\"capital.loss\"].median(), df[\"capital.loss\"].max()], labels=(0,1,2)).astype('int64')\n\n# hpw_enc = pd.cut(df['hours.per.week'], bins= (0,30,40,53,168), labels=(0,1,2,3))\ndf['hours.per.week_enc'] = pd.qcut(df['hours.per.week'], q=5, labels=(0,1,2,3), duplicates='drop').astype('int64')\n\ndf['native.country_enc'] = df['native.country'].apply(lambda x: 1 if x=='United-States' else 0)\n\ndf['income_enc'] = df.income.apply(lambda x: 1 if x==\">50K\" else 0)\n\nprint(\"Encoding complete.\")","4d574393":"df.select_dtypes(\"object\").info()","827fa34d":"#dropping encoded columns - education, sex, income\ndf.drop(['education', 'sex', 'income'], 1, inplace=True)","7206494e":"for feature in df.select_dtypes(\"object\").columns:\n    df[feature]=le.fit_transform(df[feature])","24400da4":"df.info()","ffe290fd":"#Visualizing the pearson correlation with the target class\npcorr = df.drop('income_enc',1).corrwith(df.income_enc)\nplt.figure(figsize=(10,6))\nplt.title(\"Pearson Correlation of Features with Income\")\nplt.xlabel(\"Features\")\nplt.ylabel(\"Correlation Coeff\")\nplt.xticks(rotation=90)\nplt.bar(pcorr.index, list(map(abs,pcorr.values)))","d0e47d65":"df.drop(['workclass', 'fnlwgt','occupation', 'race', 'native.country', 'fnlwgt_enc', 'race_enc', 'native.country_enc'], 1, inplace=True)","536e3a9d":"sns.heatmap(df.corr().apply(abs))","9ba9a32e":"df.drop(['age', 'education.num_enc', 'education_enc', 'marital.status_enc', 'capital.gain', 'capital.loss', 'hours.per.week'], 1, inplace = True)","1a440cd3":"df.info()","81c86c82":"X = df.drop('income_enc', 1)\ny = df.income_enc","4b9fa935":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)","5b802f76":"print(\"No. of rows in training data:\",X_train.shape[0])\nprint(\"No. of rows in testing data:\",X_test.shape[0])","4179c28b":"oversample = RandomOverSampler(sampling_strategy=0.5) #50% oversampling\nX_over, y_over = oversample.fit_resample(X_train, y_train)","0212d146":"y_over.value_counts()","6ea58e4b":"#Model Imports\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","f10a489c":"seed= 42","f87968a4":"models = {\n    'LR':LogisticRegression(random_state=seed),\n    'SVC':SVC(random_state=seed),\n    'AB':AdaBoostClassifier(random_state=seed),\n    'ET':ExtraTreesClassifier(random_state=seed),\n    'GB':GradientBoostingClassifier(random_state=seed),\n    'RF':RandomForestClassifier(random_state=seed),\n    'XGB':XGBClassifier(random_state=seed),\n    'LGBM':LGBMClassifier(random_state=seed)\n    }","b9b381dc":"# evaluate a give model using cross-validation\ndef evaluate_models(model, xtrain, ytrain):\n    cv = StratifiedKFold(shuffle=True, random_state=seed)\n    scores = cross_val_score(model, xtrain, ytrain, scoring='accuracy', cv=cv, error_score='raise')\n    return scores\n\ndef plot_scores(xval,yval,show_value=False):\n    plt.ylim(ymax = max(yval)+0.5, ymin = min(yval)-0.5)\n    plt.xticks(rotation=45)\n    s = sns.barplot(xval,yval)\n    if show_value:\n        for x,y in zip(range(len(yval)),yval):\n            s.text(x,y+0.1,round(y,2),ha=\"center\")","5d845326":"# evaluate the models and store results for 100% oversampled minority class\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_models(model, X_train, y_train) \n    results.append(scores) \n    names.append(name) \n    print('*%s %.3f (%.3f)' % (name, mean(scores), std(scores)))","408f5672":"plt.boxplot(results, labels=names, showmeans=True)\nplt.show() ","3e2ec3e9":"param_grids = {\n    'LR':{'C':[0.001,0.01,0.1,1,10]},\n    'SVC':{'gamma':[0.01,0.02,0.05,0.08,0.1], 'C':range(1,8)},\n    \n    'AB':{'learning_rate': [0.05, 0.1, 0.2], 'n_estimators': [100, 200, 500]},\n    \n    'ET':{'max_depth':[5,8,10,12], 'min_samples_split': [5,9,12],\n          'n_estimators': [100,200,500,800]},\n    \n    'GB':{'learning_rate': [0.05, 0.1, 0.2], 'max_depth':[3,5,9],\n          'min_samples_split': [5,7,9], 'n_estimators': [100,200,500],\n          'subsample':[0.5,0.7,0.9]},\n    \n    'RF':{'max_depth':[3,5,9,15], 'n_estimators': [100, 200, 500, 1000],\n          'learning_rate': [0.05, 0.1, 0.2], 'min_samples_split': [5,9,12]},\n    \n    'XGB':{'max_depth':[3,5,7,9], 'n_estimators': [100, 200, 500],\n           'learning_rate': [0.05, 0.1, 0.2], 'subsample':[0.5,0.7,0.9]},\n    \n    'LGBM':{'n_estimators': [100,200,500],'learning_rate': [0.05, 0.1, 0.2],\n            'subsample':[0.5,0.7,0.9],'num_leaves': [25,31,50]}\n}","6ae5078e":"# !pip install sklearn-deap\n# from evolutionary_search import EvolutionaryAlgorithmSearchCV","a3dceda6":"# evaluate the models and store results\n# best_params = []\n# names= []\n# for name, param_grid, model in zip(param_grids.keys(), param_grids.values(), models.values()):\n#     eascv = EvolutionaryAlgorithmSearchCV(model, param_grid, verbose=3, cv=3)\n#     eascv.fit(X_train,y_train)\n#     names.append(name)\n#     best_params.append(eascv.best_params_)\n#     print(name)\n#     print(\"best score:\",eascv.best_score_)\n#     print(\"best params:\",eascv.best_params_)","2178b1a9":"best_params=[\n    {'C': 10},\n    {'gamma': 0.1, 'C': 2},\n    {'learning_rate': 0.1, 'n_estimators': 500},\n    {'max_depth': 12, 'min_samples_split': 9, 'n_estimators': 100},\n    {'learning_rate': 0.05, 'max_depth': 3, 'min_samples_split': 9, 'n_estimators': 200, 'subsample': 0.9},\n    {'max_depth': 9, 'n_estimators': 200, 'min_samples_split': 5},\n    {'max_depth': 3, 'n_estimators': 200, 'learning_rate': 0.1, 'subsample': 0.9},\n    {'n_estimators': 100, 'learning_rate': 0.05, 'subsample': 0.9, 'num_leaves': 25}\n            ]","57831e2c":"models = [\n    ('LR',LogisticRegression(random_state=seed)),\n    ('SVC',SVC(random_state=seed)),\n    ('AB',AdaBoostClassifier(random_state=seed)),\n    ('ET',ExtraTreesClassifier(random_state=seed)),\n    ('GB',GradientBoostingClassifier(random_state=seed)),\n    ('RF',RandomForestClassifier(random_state=seed)),\n    ('XGB',XGBClassifier(random_state=seed)),\n    ('LGBM',LGBMClassifier(random_state=seed))\n]","97635987":"for model, param in zip(models, best_params):\n    model[1].set_params(**param)","a4c2efb5":"models.append(('MLModel',StackingClassifier(estimators = models[:-1])))","a6ea2d30":"scores=[]\npreds=[]\nfor model in models:\n    model[1].fit(X_train,y_train)\n    print(model[0],\"trained.\")\n    scores.append(model[1].score(X_test,y_test))\n    preds.append(model[1].predict(X_test))\nprint(\"Results are ready.\")","64cafeec":"!pip install pyarc==1.0.23\n!pip install pyfim\nfrom pyarc import CBA, TransactionDB","ed206f86":"txns_train = TransactionDB.from_DataFrame(X_train.join(y_train))\ntxns_test = TransactionDB.from_DataFrame(X_test.join(y_test))\n\n\ncba = CBA(support=0.15, confidence=0.5, algorithm=\"m1\")\ncba.fit(txns_train)","3f58dcec":"cba_score = cba.rule_model_accuracy(txns_test) \nscores.append(cba_score)\nmodels.append([\"CBA\"])","084ef5aa":"model_names= [i[0] for i in models]\nscores = list(map(lambda x: x*100, scores))","8f2c9515":"plot_scores(model_names, scores, True)","71d31f57":"*This is a very **ambiguous** attribute. Will check during testing.*","a07739f2":"### Train Test Split (3:1)","46c90418":"*We observe that the majority of \">50K\" class is **Married-civ-spouse**. So we ll make it 1 and others 0*","0e41bbfb":"**Age**","ef998ba7":"## Exploratory Data Analysis","8c2e13c7":"*We can see the class imbalance in our target. This results in models that have poor predictive performance, specifically for the minority class. So, we need to random over sampling*","cb606975":"**capital.loss**","74717489":"### Label Encoding without Feature Engineering","8ce7749b":"**hours.per.week**","51f724fc":"### Random Over Sampling","314afc99":"**relationship**","1c3ccd8a":"# Adult Census Income Classification using Meta Learning","55d2d0d7":"*As we can see, there is a **class imbalance**. The \">50K\" class is comparatively very less. So, we will do **Random Over-Sampling** during preprocessing.*\n","846eef14":"**Education**","96befaa5":"**capital.gain**","3343b534":"**Workclass**","5338d054":"**Dropping redundant features**","c87e5de4":"**native.country**","76425fbe":"**race**","2e88ae4b":"### Finding and Handling Missing Data\n\n*Observing the dataset, I found that the null values are marked as \"?\", So, we will now convert them to numpy.nan(null values).*","031195e8":"**marital.status**","6e1ab6aa":"## Model Preparation","f0a22b9a":"## Preprocessing","0b9e8f28":"From the pearson correlation plot, we can see that correlation of few columns are very **low** with the target column, so, we ll drop them.","b43379c4":"**sex**","1f26e9a4":"*Majority of the data falls under **Private**. So, we will convert this into Private and not-Private.*","65192e1c":"**Income - Target column**","b3a05e4e":"*We can observe a rough margin **around 30**. We will divide age into 2 parts ie. under 30 and over 30. We need to check if its useful for our model during testing.*","ec9c4708":"We can see that **education_enc, education.num_enc and education.num** as well as **relationship_enc and marital.status_enc** have **high correlation**. So, we will only keep one of them based on their correlation with income_enc.\n\nWe also have some redundant feautres as we have engineered features from them(age, capital.gain, etc.).","f8fad27f":"### Feature Engineering and Encoding the columns","3c2177da":"## Using Classification Based on Assocation","7e258a5a":"**occupation**","6d796b20":"**fnlwgt**","aa2c4a9b":"**education.num**","3f8926a0":"### Feature Selection"}}