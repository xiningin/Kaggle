{"cell_type":{"e5040e83":"code","b71fdc25":"code","719ce620":"code","dacfc77f":"code","aa13688a":"code","8962cd3e":"code","82f911e8":"code","7ba770da":"code","390885d8":"code","18868dec":"code","8362b320":"code","203a10f9":"code","2ed8bb82":"code","7dfd4e48":"code","34411640":"code","14d24e1b":"code","6f812076":"code","cc926029":"code","da5ef049":"code","df2619f9":"code","12695200":"code","dbe58d6a":"code","40e2663b":"code","64ba069a":"code","9e96155f":"code","a8c3c3c0":"code","53c306e8":"code","4f308f0b":"code","a02dbbc1":"code","019d4b15":"code","f8aaeda3":"code","18fb299c":"code","686b7f46":"code","dad69d98":"code","5fc13bdd":"code","8799cc61":"code","7a8863fd":"code","b79e29c8":"code","eec11429":"code","5b90f82d":"code","b7c85446":"code","07246058":"code","d6bb58c0":"code","d5c479cc":"code","93f4d5d3":"code","c440eb40":"code","15770bc4":"code","836881ce":"code","58a5260c":"code","9bf36dcb":"code","95106500":"code","f2ef7b9e":"code","225fde58":"code","73b14f6f":"markdown","0a573277":"markdown","3e53edb3":"markdown","9559d64d":"markdown","00a4f5e8":"markdown","d361473e":"markdown"},"source":{"e5040e83":"# number\n\n#  characters\n#  words\n#  sentence\n#  paragraphs\n\n#  hastags\n#  mentions\n#  web address\n\n#  numbers\n#  punctuations\n#  emoji\n\n#  stopwords\n#  unique words\n#  full 140 characters used ?\n\n#  upper case words\n#  title case words\n \n#  avg. length\n \n# unigram\n# bigram\n# trigram\n\n# replace %20 by ' '\n\n# remove\n#  url\n#  emoticons\n#  html tags\n#  punctuations\n\n# correct spelling\n \n# wordcloud\n# visualize embeddings\n\n# count vectorizer\n# tf-idf\n\n# embedding\n#  lstm \n#  bidirectional\n# glove\n# bert\n# automl","b71fdc25":"# def clean_text(text):\n#     '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n#     and remove words containing numbers.'''\n#     text = text.lower()\n#     text = re.sub('\\[.*?\\]', '', text)\n#     text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n#     text = re.sub('<.*?>+', '', text)\n#     text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n#     text = re.sub('\\n', '', text)\n#     text = re.sub('\\w*\\d\\w*', '', text)\n#     return text","719ce620":"# basic\nimport numpy as np \nimport pandas as pd\n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\n\n# palette\ntw_pal = ['#55ACEE', '#292F33', '#66757F', '#CCD6DD', '#E1E8ED']\nsns.set_style(\"whitegrid\")\n\n# machine learning\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# nlp\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS \n\n# deep learning\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","dacfc77f":"# load data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","aa13688a":"# labeling targets based on target column\ntrain['label'] = train['target'].apply(lambda x: 'Real Disaster' if x==1 else 'Not a Real Disaster')","8962cd3e":"# random rows\ntrain.sample(5)","82f911e8":"# shape of the dataset\n# print(train.shape)\n# print(test.shape)\n\n# info\n# train.info()\n\n# describe \n# train.describe()\n\n# missing values\n# train.isna().sum()","7ba770da":"# %20 looks like it stands for character 'space' \nfor table in [train, test]:\n    for col in table.select_dtypes('object').columns:\n        table[col] = table[col].str.replace('%20', ' ')","390885d8":"# derived tables\n\ndisaster = train[train['target']==1]\nnot_a_disaster = train[train['target']==0]","18868dec":"# target label count\n\nsns.countplot(x='label', data=train, palette=tw_pal)\nplt.legend()\nplt.show()","8362b320":"# location count\n\nprint('Disaster location \\n'+'-'*30)\nprint(disaster['location'].value_counts()[:6])\nprint('\\nNot a disaster location \\n'+'-'*30)\nprint(not_a_disaster['location'].value_counts()[:6])","203a10f9":"# keyword count\n\nprint('Disaster keywords \\n'+'-'*30)\nprint(disaster['keyword'].value_counts()[:6])\nprint('\\nNot a disaster keywords \\n'+'-'*30)\nprint(not_a_disaster['keyword'].value_counts()[:6])","2ed8bb82":"# train['target_mean'] = train.groupby('keyword')['target'].transform('mean')\n\n# plt.figure(figsize=(6, 50))\n# sns.countplot(y = train.sort_values(by='target_mean', ascending=False)['keyword'],\n#               hue = train.sort_values(by='target_mean', ascending=False)['target'], \n#               palette = tw_pal)\n# plt.show()","7dfd4e48":"# tweet length\n\n# fig,ax = plt.subplots(1, 2, figsize=(12, 4))\n\n# ax[0].hist(train[train['target']==1]['text'].str.len(), color='#55ACEE', \n#            bins=15, range=(0, 160))\n# ax[0].set_title('Disaster')\n\n# ax[1].hist(train[train['target']==0]['text'].str.len(), color='#66757F', \n#            bins=15, range=(0, 160))\n# ax[1].set_title('No Disaster')\n\n# fig.suptitle('Characters in tweets')\n# plt.show()","34411640":"# tweet length\n\nplt.figure(figsize=(12, 5))\nplt.hist(train[train['target']==1]['text'].str.len(), \n         color='#55ACEE', alpha=0.7, bins=70, \n         range=(0, 160), label='Disaster')\nplt.hist(train[train['target']==0]['text'].str.len(), \n         color='#66757F', alpha=0.7, bins=70, \n         range=(0, 160), label='Not Disaster')\nplt.suptitle('Characters in tweets')\nplt.legend()\nplt.show()","14d24e1b":"# no. of characters, words, sentences, paragraphs\n\ntrain['no_chars'] = train['text'].apply(len)\ntrain['no_words'] = train['text'].str.split().apply(len)\ntrain['no_sent'] = train['text'].str.split('.').apply(len)\ntrain['no_para'] = train['text'].str.split('\\n').apply(len)\n\ntest['no_chars'] = test['text'].apply(len)\ntest['no_words'] = test['text'].str.split().apply(len)\ntest['no_sent'] = test['text'].str.split('.').apply(len)\ntest['no_para'] = test['text'].str.split('\\n').apply(len)\n\ncols = ['no_chars', 'no_words', 'no_sent', 'no_para']\ncol_names = ['Mean no. of Characters', \n             'Mean no. of Words', \n             'Mean no. of Sentences', \n             'Mean no. of Paragraphs']\n\nfig, ax = plt.subplots(1, 4, figsize=(24, 4))\nfor ind, val in enumerate(cols):\n    sns.barplot(x='label', y=val, palette=tw_pal, data=train, ax=ax[ind])\n    ax[ind].set_title(col_names[ind])","6f812076":"# Characters per word, Characters per sentences, Words per sentences\n\ntrain['chars_per_word'] = train['no_chars']\/train['no_words']\ntrain['chars_per_sent'] = train['no_chars']\/train['no_sent']\ntrain['words_per_sent'] = train['no_words']\/train['no_sent']\n\ntest['chars_per_word'] = test['no_chars']\/test['no_words']\ntest['chars_per_sent'] = test['no_chars']\/test['no_sent']\ntest['words_per_sent'] = test['no_words']\/test['no_sent']\n\ncols = ['chars_per_word', 'chars_per_sent', 'words_per_sent']\ncol_names = ['Characters per word', \n             'Characters per sentences', \n             'Words per sentences']\n\nfig, ax = plt.subplots(1, 3, figsize=(14, 4))\nfor ind, val in enumerate(cols):\n    sns.barplot(x='label', y=val, palette=tw_pal, data=train, ax=ax[ind])\n    ax[ind].set_title(col_names[ind])","cc926029":"# No. of hashtags, mentions, web addresses\n\ndef hash_count(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('#')])\n\ndef mention_count(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('@')])\n\ndef web_add(tweet):\n    w = tweet.split()\n    return len([word for word in w if word.startswith('http')])\n\ntrain['no_hashtags'] = train['text'].apply(hash_count)\ntrain['no_mentions'] = train['text'].apply(mention_count)\ntrain['no_web_add'] = train['text'].apply(web_add)\n\ntest['no_hashtags'] = test['text'].apply(hash_count)\ntest['no_mentions'] = test['text'].apply(mention_count)\ntest['no_web_add'] = test['text'].apply(web_add)\n\ncols = ['no_hashtags', 'no_mentions', 'no_web_add']\ncol_names = ['No. of hashtags', \n             'No. of mentions', \n             'No. of web addresses']\n\nfig, ax = plt.subplots(1, 3, figsize=(14, 4))\nfor ind, val in enumerate(cols):\n    sns.barplot(x='label', y=val, palette=tw_pal, data=train, ax=ax[ind])\n    ax[ind].set_title(col_names[ind])","da5ef049":"\n# def numbers(tweet):\n#     w = tweet.split()\n#     return sum([word.isnumeric() for word in w])\n","df2619f9":"# cols = ['no_chars', 'no_words', 'no_sent', 'no_para', 'avg_word_len', \n#         'no_hashtags', 'no_mentions', 'no_web_add', 'numbers']\n\n# titles = ['Average number of characters', 'Average number of words', \n#           'Average number of sentences', 'Average number of paragraphs', \n#           'Average word length', 'No. of hastags', 'No. of mentions', \n#           'No. of web addresses', 'No. of numbers']\n\n# for ind, val in enumerate(cols):\n#     plt.figure(figsize=(6,4))\n#     sns.barplot(x='target', y=val, palette=tw_pal, data=train)\n#     plt.suptitle(titles[ind])\n#     plt.show()","12695200":"# for ind, val in enumerate(cols):\n#     plt.figure(figsize=(12, 5))\n#     mn, mx = min(train[val]), max(train[val])\n#     plt.hist(train[train['target']==1][val], color='#55ACEE', \n#              alpha=0.7, label='Disaster', bins=10, range=[mn, mx])\n#     plt.hist(train[train['target']==0][val], color='#66757F', \n#              alpha=0.7, label='Not Disaster', bins=10, range=[mn, mx])\n#     plt.suptitle(titles[ind])\n#     plt.legend()\n#     plt.show()","dbe58d6a":"# for ind, val in enumerate(cols):\n#     plt.figure(figsize=(12, 5))\n#     sns.kdeplot(train[train['target']==1][val], alpha=0.4, shade=True, color=\"#55ACEE\")\n#     sns.kdeplot(train[train['target']==0][val], alpha=0.4, shade=True, color='#292F33')\n#     plt.suptitle(titles[ind])\n#     plt.legend()\n#     plt.show()","40e2663b":"# sns.countplot(x='keyword', data=train, ax=ax[ind])\n# plt.show()","64ba069a":"# train['tweet_target'].value_counts()","9e96155f":"# # cv = TfidfVectorizer(stop_words='english')\n# cv = CountVectorizer(stop_words = 'english')\n# cv.fit(train[\"text\"].append(test['text']))\n# train_vectors = cv.transform(train[\"text\"])\n# test_vectors = cv.transform(test[\"text\"])","a8c3c3c0":"# count_vect_df = pd.DataFrame(train_vectors.todense(), columns=cv.get_feature_names())\n# train = pd.concat([train, count_vect_df], axis=1)","53c306e8":"# count_vect_df = pd.DataFrame(test_vectors.todense(), columns=cv.get_feature_names())\n# test = pd.concat([test, count_vect_df], axis=1)","4f308f0b":"# train.head()","a02dbbc1":"# train.drop(['id', 'keyword', 'location', 'text'], axis=1, inplace=True)\n# test.drop(['id', 'keyword', 'location', 'text'], axis=1, inplace=True)","019d4b15":"# X = train.drop(['tweet_target'], axis=1)\n# # X = train[['no_chars', 'no_para', 'avg_word_len', 'no_hashtags', 'no_mentions', 'no_web_add']]\n# y = train['tweet_target']","f8aaeda3":"# scaler = MinMaxScaler()\n# X = scaler.fit_transform(X)\n# test = scaler.transform(test)","18fb299c":"# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","686b7f46":"# print(X.shape)\n# print(y.shape)","dad69d98":"# clf = KNeighborsClassifier()\n# clf.fit(X_train, y_train)\n\n# y_pred = clf.predict(X_test)\n# print(accuracy_score(y_test, y_pred))\n# print(confusion_matrix(y_test, y_pred))\n# print(cross_val_score(clf, X, y, cv=5).mean())","5fc13bdd":"# clf = BernoulliNB()\n# clf.fit(X_train, y_train)\n\n# y_pred = clf.predict(X_test)\n# print(accuracy_score(y_test, y_pred))\n# print(confusion_matrix(y_test, y_pred))\n# # print(cross_val_score(clf, X, y, cv=5).mean())","8799cc61":"# clf = BernoulliNBnoulliNB()\n# clf.fit(X_train, y_train)\n\n# y_pred = clf.predict(X_test)\n# print(accuracy_score(y_test, y_pred))\n# print(confusion_matrix(y_test, y_pred))\n# print(cross_val_score(clf, X, y, cv=5).mean())","7a8863fd":"# clf = LogisticRegression(solver='lbfgs')\n# clf.fit(X_train, y_train)\n\n# y_pred = clf.predict(X_test)\n# print(accuracy_score(y_test, y_pred))\n# print(confusion_matrix(y_test, y_pred))\n# print(cross_val_score(clf, X, y, cv=10).mean())","b79e29c8":"# tfidf_model = TfidfVectorizer(stop_words='english')\n# train_vectors = tfidf_model.fit_transform(train[\"text\"])\n\n# train_vectors = tfidf_model.fit_transform(train[\"text\"])\n# test_vectors = tfidf_model.transform(test[\"text\"])\n\n# clf = BernoulliNB()\n\n# cross_val_score(clf, train_vectors, train[\"target\"], cv=10, scoring=\"f1\").mean()","eec11429":"# clf.fit(train.drop(['tweet_target'], axis=1), train[\"tweet_target\"])\n# sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n# sample_submission[\"target\"] = clf.predict(test)\n# sample_submission.head()\n# sample_submission.to_csv(\"submission.csv\", index=False)","5b90f82d":"# train.head()","b7c85446":"# specific_wc = ['http', 'https']\n# sw = list(set(stopwords.words('english')))\n# sw = sw + specific_wc\n\n# print(sw[:5])\n# print(len(sw))","07246058":"# # containers for features and labels\n\n# sentences = []\n# labels = []\n\n# for ind, row in train.iterrows():\n#     labels.append(row['tweet_target'])\n#     sentence = row['text']\n#     for word in sw: # removing stop words\n#         token = \" \"+word+\" \"\n#         sentence = sentence.replace(token, \" \") # replacing stop words with space\n#         sentence = sentence.replace(\" \", \" \")\n#     sentences.append(sentence)","d6bb58c0":"# # label encoding labels \n\n# enc = LabelEncoder()\n# encoded_labels = enc.fit_transform(labels)\n\n# print(enc.classes_)\n# print(labels[:5])\n# print(encoded_labels[:5])","d5c479cc":"# # word cloud on entire reviews\n# wc = WordCloud(width = 600, height = 400, \n#                     background_color ='white', \n#                     stopwords = sw, \n#                     min_font_size = 10, colormap='Paired_r').generate(' '.join(sentences[:100]))\n# plt.imshow(wc)","93f4d5d3":"# # word cloud on positve reviews\n# pos_rev = ' '.join(train[train['tweet_target']==0]['text'].to_list()[:10000])\n# wc = WordCloud(width = 600, height = 400, \n#                     background_color ='white', \n#                     stopwords = sw, \n#                     min_font_size = 10, colormap='GnBu').generate(pos_rev)\n# plt.imshow(wc)","c440eb40":"# # word cloud on positve reviews\n# pos_rev = ' '.join(train[train['tweet_target']==1]['text'].to_list()[:10000])\n# wc = WordCloud(width = 600, height = 400, \n#                     background_color ='white', \n#                     stopwords = sw, \n#                     min_font_size = 10, colormap='RdGy').generate(pos_rev)\n# plt.imshow(wc)","15770bc4":"# # model parameters\n\n# vocab_size = 1000\n# embedding_dim = 16\n# max_length = 120\n# trunc_type='post'\n# padding_type='post'\n# oov_tok = \"<OOV>\"\n# training_portion = .7\n","836881ce":"# train test split\n# ---------------\n\n# proportion of training dataset\ntrain_size = int(len(sentences) * training_portion)\n\n# training dataset\ntrain_sentences = sentences[:train_size]\ntrain_labels = encoded_labels[:train_size]\n\n# validation dataset\nvalidation_sentences = sentences[train_size:]\nvalidation_labels = encoded_labels[train_size:]\n","58a5260c":"# # tokenizing, sequencing, padding features\n\n# tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n# tokenizer.fit_on_texts(train_sentences)\n# word_index = tokenizer.word_index\n\n# train_sequences = tokenizer.texts_to_sequences(train_sentences)\n# train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\n# validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n# validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","9bf36dcb":"# print(train_padded.shape)\n# print(validation_padded.shape)\n# print(train_labels.shape)\n# print(validation_labels.shape)","95106500":"# # model initialization\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.GlobalAveragePooling1D(),\n#     tf.keras.layers.Dense(24, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# # compile model\n# model.compile(loss='binary_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# # model summary\n# model.summary()\n\n# # train model\n# num_epochs = 20\n# history = model.fit(train_padded, train_labels, \n#                     epochs=num_epochs, verbose=1, \n#                     validation_data=(validation_padded, validation_labels))\n\n# # loss and accuracy\n\n# plt.figure(figsize=(10, 5))\n\n# plt.subplot(1, 2, 1)\n# plt.plot(history.history['accuracy'], label='Training Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Accuracy')\n# plt.legend()\n\n# plt.subplot(1, 2, 2)\n# plt.plot(history.history['loss'], label='Training Loss')\n# plt.plot(history.history['val_loss'], label='Validation Loss')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.show()","f2ef7b9e":"# # model initialization\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n#     tf.keras.layers.Dense(24, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# # compile model\n# model.compile(loss='binary_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# # model summary\n# model.summary()\n\n# # model fit\n# num_epochs = 20\n# history = model.fit(train_padded, train_labels, \n#                     epochs=num_epochs, verbose=1, \n#                     validation_data=(validation_padded, validation_labels))\n\n# # accuracy and loss\n\n# plt.figure(figsize=(10, 5))\n\n# plt.subplot(1, 2, 1)\n# plt.plot(history.history['accuracy'], label='Training Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Accuracy')\n# plt.legend()\n\n# plt.subplot(1, 2, 2)\n# plt.plot(history.history['loss'], label='Training Loss')\n# plt.plot(history.history['val_loss'], label='Validation Loss')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.show()","225fde58":"# # model initialization\n# model = tf.keras.Sequential([\n#     tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n#     tf.keras.layers.Conv1D(128, 5, activation='relu'),\n#     tf.keras.layers.GlobalMaxPooling1D(),\n#     tf.keras.layers.Dense(24, activation='relu'),\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# # compile model\n# model.compile(loss='binary_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# # model summary\n# model.summary()\n\n# # model fit\n# num_epochs = 20\n# history = model.fit(train_padded, train_labels, \n#                     epochs=num_epochs, verbose=1, \n#                     validation_data=(validation_padded, validation_labels))\n# # accuracy and loss\n\n# plt.figure(figsize=(10, 5))\n\n# plt.subplot(1, 2, 1)\n# plt.plot(history.history['accuracy'], label='Training Accuracy')\n# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Accuracy')\n# plt.legend()\n\n# plt.subplot(1, 2, 2)\n# plt.plot(history.history['loss'], label='Training Loss')\n# plt.plot(history.history['val_loss'], label='Validation Loss')\n# plt.xlabel(\"Epochs\")\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.show()","73b14f6f":"# Feature exploration","0a573277":"# Extracted Features","3e53edb3":"# Using TF-IDF","9559d64d":"# Using Word Embeding","00a4f5e8":"# Libraries","d361473e":"# Data"}}