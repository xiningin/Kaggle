{"cell_type":{"f14f89ad":"code","0bea0541":"code","cad67974":"code","f76a29da":"code","9a501bf4":"code","c9592445":"code","d2ea4d2c":"code","821f0988":"code","61513091":"code","fb599bd1":"code","244c9e5e":"code","12477922":"code","301a73c0":"code","3cff8173":"code","8db7de7d":"code","016e1a70":"code","1d05b3e3":"code","3d13b80a":"code","aa53bf8b":"code","6806dec6":"code","03ba05cb":"code","daf976ef":"markdown","bfeb74bf":"markdown","df64f542":"markdown","ff0dda67":"markdown","0ce97c86":"markdown","fd80ae40":"markdown","bd55352c":"markdown","bec4de0d":"markdown","483ad841":"markdown","868a3adb":"markdown","90150357":"markdown","99d460c8":"markdown","698c62f5":"markdown","cb7f7a91":"markdown","9687b376":"markdown","25de8a07":"markdown","89d46b90":"markdown","224a4196":"markdown","8b93dbbe":"markdown","d989fc15":"markdown","c9e215e8":"markdown","afd4325e":"markdown","b2acf934":"markdown","62800cf8":"markdown","dba1277e":"markdown","a92d8d9f":"markdown","33f70af9":"markdown","06b5cc65":"markdown","39d13428":"markdown"},"source":{"f14f89ad":"# pip install spacy\n\n# Install spacy if not already installed.","0bea0541":"import spacy\nimport pickle\nimport random\n\n# Import required modules","cad67974":"test = spacy.load('en')\nsent = '''My name is Amar Sharma, i stay in Mumbai.\nThe 2020 america presidential election is scheduled for Tuesday, November 3.'''\n\nts = test(sent)\nfor ent in ts.ents:\n  print(f'{ent.label_.upper():{10}} - {ent.text}')\n","f76a29da":"# one more example\n\nfor ent in test(\"Apple is looking at buying U.K. startup for $1 billion\").ents:\n  print(f'{ent.label_.upper():{10}} - {ent.text}')","9a501bf4":"print(f'PERSON - {spacy.explain(\"PERSON\")}')\nprint(f'GPE    - {spacy.explain(\"GPE\")}')\nprint(f'DATE   - {spacy.explain(\"DATE\")}')\nprint(f'MONEY  - {spacy.explain(\"MONEY\")}')","c9592445":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","d2ea4d2c":"# since our resume is in pdf format we will use PyMuPDF to extract data from it.\n# You can also use PyPDF2.","821f0988":"pip install PyMuPDF # https:\/\/pypi.org\/project\/PyMuPDF\/","61513091":"import sys,fitz\nfname = '\/kaggle\/input\/dataset-for-resume-information-retrieval\/Alice Clark CV.pdf'\ndoc= fitz.open(fname)\nalice_cv=\"\"\nfor page in doc:\n  alice_cv = alice_cv + str(page.getText())\n\nprint(alice_cv)\n\n# we have extracted the data from pdf file using PyMuPDF and stored in alice_cv variable.","fb599bd1":"test = spacy.load('en')\nts = test(\" \".join(alice_cv.split('\\n'))) # we have splitted our data with '\\n' and rejoined with space. ","244c9e5e":"# Here, we are only extracting all PERSON named entities.\n\nfor ent in ts.ents:\n  if ent.label_.upper() == 'PERSON':\n    print(f'{ent.label_.upper():{10}} - {ent.text}')","12477922":"# Here, we are only extracting all ORG named entities.\n\nfor ent in ts.ents:\n  if ent.label_.upper() == 'ORG':\n    print(f'{ent.label_.upper():{10}} - {ent.text}')","301a73c0":"train_data = pickle.load(open('\/kaggle\/input\/dataset-for-resume-information-retrieval\/train_data.pkl','rb'))\nprint(f\"Training data consist of {len(train_data)} manually labelled resume's.\")","3cff8173":"# Checking format of one resume data\n\ntrain_data[97]","8db7de7d":"# loading blank spacy model as we want to customize our model.\n# spacy.blank('en') will create a blank model of a given language class i.e., for here English.\n\nnlp = spacy.blank('en') ","016e1a70":"# Creating a function to train our model\n\ndef train_model(train_data):\n    \n  if 'ner' not in nlp.pipe_names:# Checking if NER is present in pipeline\n    ner = nlp.create_pipe('ner') # creating NER pipe if not present\n    nlp.add_pipe(ner, last=True) # adding NER pipe in the end\n\n  for _, annotation in train_data: # Getting 1 resume at a time from our training data of 200 resumes\n    for ent in annotation['entities']: # Getting each tuple at a time from 'entities' key in dictionary at index[1] i.e.,(0, 15, 'Name') and so on\n      ner.add_label(ent[2])  # here we are adding only labels of each tuple from entities key dict, eg:- 'Name' label of (0, 15, 'Name')\n    \n  # In above for loop we finally added all custom NER from training data.\n    \n\n  other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] # getting all other pipes except NER.\n  with nlp.disable_pipes(*other_pipes): # Disabling other pipe's as we want to train only NER.\n        optimizer = nlp.begin_training()\n        \n        for itn in range(10):         # trainig model for 10 iteraion\n            print('Starting iteration ' + str(itn))\n            random.shuffle(train_data) # shuffling data in every iteration \n            losses = {}\n            for text, annotations in train_data:\n              try:\n                nlp.update(\n                    [text],        #batch of texts\n                    [annotations], #batch of annotations\n                    drop=0.2,      #dropout rate -makes it harder to memorise\n                    sgd=optimizer, #callable to update weights\n                    losses=losses) #Dictionary to update with the loss, keyed by pipeline component.\n              except Exception as e:\n                pass","1d05b3e3":"# pass train data to function.\n\ntrain_model(train_data)","3d13b80a":"# Saving our trained model to re-use.\n\nnlp.to_disk('nlp_model')","aa53bf8b":"# Loading our trained model\n\nnlp_model = spacy.load('nlp_model')","6806dec6":"# Checking all the custom NER created\n\nnlp_model.entity.labels","03ba05cb":"doc = nlp_model(\" \".join(alice_cv.split('\\n')))\nfor ent in doc.ents:\n  print(f'{ent.label_.upper():{20}} - {ent.text}')","daf976ef":"As we have our training data ready, we will now train our spacy model and add custom NER.","bfeb74bf":"**ts** variable contains all POS tags,NER etc in it. We will extract only NER and manually verify output.","df64f542":"![image.png](attachment:image.png)","ff0dda67":"We call our above created funcion **'train_model'** to learn from training data.","0ce97c86":"# Solution \n\n\nThis is the reason why we have to first train our spacy model on some manually labelled data and create custom NER. \n\nSince, for testing we took resume data means we are here dealing with resume's, so we have to first train our spacy model on some manually labelled resume data. For training purpose we got data from online but you can create training data according to your requirement.","fd80ae40":"[More](https:\/\/spacy.io\/api\/annotation#named-entities) predefined named entites present in spacy.","bd55352c":"# References \n\n* https:\/\/spacy.io\/\n* https:\/\/github.com\/laxmimerit\/Resume-and-CV-Summarization-and-Parsing-with-Spacy-in-Python","bec4de0d":"![image.png](attachment:image.png)","483ad841":"# Conclusion\n\nAs per our requirement, we can manually create data for training, add custom NER and train our model.","868a3adb":"![image.png](attachment:image.png)","90150357":"## **Explanation of above code** :-\n\n**test = spacy.load('en')**, here **load()** is used to load model, '**en**' specifies name\/unicode of model to load i.e., English. [More](https:\/\/spacy.io\/api\/top-level#spacy.load)\n\n**sent** contains sentence passed for extracting named entities.\n\n**ts = test(sent)**, here we pass sent variable to test (object of spacy english model) which learns POS tags,NER etc and stores information in ts.\n\n**ts.ents** Here ents property contains all the entities as tuple identified by our model from sents.\n\n**ent.label_** contains label**(PERSON \/ GPE \/ DATE)** which is given by model to that entitiy.\n\n**ent.text** contains entity**(Amar Sharma \/ Mumbai \/ 2020 \/ america)** as string.\n\n\nHave you noticed that even as we specified america word initial character 'a' as smaller case still our model managed to identiy it as GPE.","99d460c8":"As you can see our pre trained model did not perform well on test data. Only name('Alice Clark') was labelled correctly and rest all are labelled incorrect.","698c62f5":"**Myself [Amar Sharma](https:\/\/www.kaggle.com\/amarsharma768)  and my team member [Parvez Shaikh](https:\/\/www.kaggle.com\/parvezahmedshaikh) created many such notebooks as part of the course work under \"Master in Data Science Programme\" at [Suven](https:\/\/datascience.suvenconsultants.com\/) , under mentor-ship of [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/) .**\n\n","cb7f7a91":"### Anatomy of our train data\n\nOur train data is stored as a tuple consisting of 200 resume data, each resume data consist of 2 parts\/indexes.\n\n* First index  [0] consist of all details(name, degree, designation, compaines worked at) in resume.\n* Second index [1] consist of a dictionary object having only one key i.e., 'entities' and look carefully at its value.\n\nValue of 'entities' key has a list of tuples and in each tuple we have some number and some labelling. \n\nFor Eg :- **(0, 15, 'Name')**, here 0 denotes start index and 15 denotes end index of label 'Name', which is 'Ramesh chokkala'.\nSimilarly, we can see that all the other tuple also has some start and end index alongwith their respective label. This is how you can manually create data for training.\n\n**Note** :- label of all training data should be same i.e., if you have specified label as 'Name' for one resume then for all the resume data wherever name is present for that label should be as 'Name' only and not something else.","9687b376":"Now, if spacy has so many named entities predefined then why are we creating custom NER.\nWell, in simple words we never trained our model it was pre trained, we directly created object of it and started extracting information on it which our model did very well. But, what happens when we try to get all named entities on a dataset that it was never trained  for. We are going to try one such example.","25de8a07":"# Introduction\nIn this notebook we will be learning about how to set custom named entity recognition (NER) in spacy.\n\n* **What is Named Entity**?\n\nA named entity is a \u201creal-world object\u201d that\u2019s assigned a name \u2013 for example, a person, a country, a product or a book title. \n\n\n* **About spaCy** :-\n\nspaCy is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion. [More](https:\/\/spacy.io\/)\n\n* **NLTK vs spaCy** :-\n\nWhile NLTK provides access to many algorithms to get something done, spaCy provides the best way to do it. It provides the fastest and most accurate syntactic analysis of any NLP library released to date. It also offers access to larger word vectors that are easier to customize. For an app builder mindset that prioritizes getting features done, spaCy would be the better choice. [More](https:\/\/www.activestate.com\/blog\/natural-language-processing-nltk-vs-spacy\/#:~:text=While%20NLTK%20provides%20access%20to,that%20are%20easier%20to%20customize.)\n\n* **Some [Features](https:\/\/spacy.io\/usage\/spacy-101#features) of spaCy** :- \n\n    * Tokenization\n    * Part-of-speech (POS) Tagging\n    * Lemmatization\n    * Named Entity Recognition (NER)\n    * Similarity\n    * Text Classification\n\nWe will be focusing on NER.\n\n* **What is NER ?**\n\nNamed-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [More](https:\/\/en.wikipedia.org\/wiki\/Named-entity_recognition)","89d46b90":"### Spacy pipeline\n\nBelow is the pipeline which is created when we pass our data to spacy model. \n\nRecommended [More](https:\/\/spacy.io\/usage\/processing-pipelines).","224a4196":"First, we will execute spacy model on a simple sentence just to get familiar with it.","8b93dbbe":"Now, we can pass our test data on our pre trained spacy model and evaluate how good it has performed.","d989fc15":"By the way, we can use explain method of spacy to know in what a named entites signifies. \n\nSee below code.","c9e215e8":"![image.png](attachment:image.png)","afd4325e":"I would like to humbly and sincerely thank my mentor [Rocky Jagtiani](https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/). He is more of a friend to me then mentor. The Python for Data Science taught by him and various assignments we did and are still doing is the best way to learn and skill in Data Science field.\n\nRecommended https:\/\/datascience.suvenconsultants.com\/","b2acf934":"From above output we can clearly see that our custom trained spacy model has worked very well and labelled our testing data correctly but not 100% as the skills are not mentioned in output. This could be due to less training data. To increase accuracy we should train our model on different formats of data.","62800cf8":"Above NER example shows all named entities which are present in given sentence.\n\n\n","dba1277e":"We will extract data from a resume and use this data as test data for our model to predict named entities from.","a92d8d9f":"We have our training data stored in pickle file.","33f70af9":"# **Coding part**","06b5cc65":"# **Why we are creating custom NER ?**","39d13428":"ORG named entities are also labelled incorrect except 'MICROSOFT'.\n\nNow, in these types of situation where labelling are almost incorrect what should we do?"}}