{"cell_type":{"e63acf26":"code","57b8a627":"code","cb26b3a1":"code","6cfe54e2":"code","14c76fa8":"code","dec06c7b":"code","43acc2d6":"code","55675d8d":"code","109e1fa7":"code","0f3b9d81":"code","d209aab9":"code","eead47e4":"markdown","6927fb2a":"markdown","7d30b982":"markdown","7e716ac7":"markdown","9765dc18":"markdown","5833b9d9":"markdown","1150aa26":"markdown"},"source":{"e63acf26":"#Make the necessary imports\nimport os\nimport sys\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.initializers import Constant","57b8a627":"GLOVE_DIR = \"..\/input\/glove6b\"\nTRAIN_DATA_DIR = \"..\/input\/aclimdb\/aclImdb\/train\"\nTEST_DATA_DIR = \"..\/input\/aclimdb\/aclImdb\/test\"","cb26b3a1":"#Within these, I only have a pos\/ and a neg\/ folder containing text files \nMAX_SEQUENCE_LENGTH = 1000\nMAX_NUM_WORDS = 20000 \nEMBEDDING_DIM = 100 \nVALIDATION_SPLIT = 0.2\n\n#started off from: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py\n#and from: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_lstm.py","6cfe54e2":"#Function to load the data from the dataset into the notebook. Will be called twice - for train and test.\ndef get_data(data_dir):\n    texts = []  # list of text samples\n    labels_index = {'pos':1, 'neg':0}  # dictionary mapping label name to numeric id\n    labels = []  # list of label ids\n    for name in sorted(os.listdir(data_dir)):\n        path = os.path.join(data_dir, name)\n        if os.path.isdir(path):\n            if name=='pos' or name=='neg':\n                label_id = labels_index[name]\n                for fname in sorted(os.listdir(path)):\n                        fpath = os.path.join(path, fname)\n                        text = open(fpath,encoding='utf8').read()\n                        texts.append(text)\n                        labels.append(label_id)\n    return texts, labels\n\ntrain_texts, train_labels = get_data(TRAIN_DATA_DIR)\ntest_texts, test_labels = get_data(TEST_DATA_DIR)\nlabels_index = {'pos':1, 'neg':0} \n\n#Just to see how the data looks like. \nprint(train_texts[0])\nprint(train_labels[0])\nprint(test_texts[24999])\nprint(test_labels[24999])","14c76fa8":"#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer \n#Tokenizer is fit on training data only, and that is used to tokenize both train and test data. \ntokenizer = Tokenizer(num_words=MAX_NUM_WORDS) \ntokenizer.fit_on_texts(train_texts) \ntrain_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes \ntest_sequences = tokenizer.texts_to_sequences(test_texts) \nword_index = tokenizer.word_index \nprint('Found %s unique tokens.' % len(word_index))","dec06c7b":"#Converting this to sequences to be fed into neural network. Max seq. len is 1000 as set earlier\n#initial padding of 0s, until vector is of size MAX_SEQUENCE_LENGTH\ntrainvalid_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntest_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\ntrainvalid_labels = to_categorical(np.asarray(train_labels))\ntest_labels = to_categorical(np.asarray(test_labels))\n\n# split the training data into a training set and a validation set\nindices = np.arange(trainvalid_data.shape[0])\nnp.random.shuffle(indices)\ntrainvalid_data = trainvalid_data[indices]\ntrainvalid_labels = trainvalid_labels[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\nx_train = trainvalid_data[:-num_validation_samples]\ny_train = trainvalid_labels[:-num_validation_samples]\nx_val = trainvalid_data[-num_validation_samples:]\ny_val = trainvalid_labels[-num_validation_samples:]\n#This is the data we will use for CNN and RNN training\nprint('Splitting the train data into train and valid is done')","43acc2d6":"print('Preparing embedding matrix.')\n\n# first, build index mapping words in the embeddings set\n# to their embedding vector\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),encoding='utf8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors in Glove embeddings.' % len(embeddings_index))\n#print(embeddings_index[\"google\"])\n\n# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\nnum_words = min(MAX_NUM_WORDS, len(word_index)) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    if i > MAX_NUM_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load these pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)\nprint(\"Preparing of embedding matrix is done\")","55675d8d":"print('Define a 1D CNN model.')\n\ncnnmodel = Sequential()\ncnnmodel.add(embedding_layer)\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(GlobalMaxPooling1D())\ncnnmodel.add(Dense(128, activation='relu'))\ncnnmodel.add(Dense(len(labels_index), activation='softmax'))\n\ncnnmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n#Train the model. Tune to validation set. \ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\n#Evaluate on test set:\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)","109e1fa7":"print(\"Defining and training a CNN model, training embedding layer on the fly instead of using pre-trained embeddings\")\ncnnmodel = Sequential()\ncnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(MaxPooling1D(5))\ncnnmodel.add(Conv1D(128, 5, activation='relu'))\ncnnmodel.add(GlobalMaxPooling1D())\ncnnmodel.add(Dense(128, activation='relu'))\ncnnmodel.add(Dense(len(labels_index), activation='softmax'))\n\ncnnmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['acc'])\n#Train the model. Tune to validation set. \ncnnmodel.fit(x_train, y_train,\n          batch_size=128,\n          epochs=1, validation_data=(x_val, y_val))\n#Evaluate on test set:\nscore, acc = cnnmodel.evaluate(test_data, test_labels)\nprint('Test accuracy with CNN:', acc)","0f3b9d81":"print(\"Defining and training an LSTM model, training embedding layer on the fly\")\n\n#model\nrnnmodel = Sequential()\nrnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\nrnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nrnnmodel.add(Dense(2, activation='sigmoid'))\nrnnmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nprint('Training the RNN')\n\nrnnmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=1,\n          validation_data=(x_val, y_val))\nscore, acc = rnnmodel.evaluate(test_data, test_labels,\n                            batch_size=32)\nprint('Test accuracy with RNN:', acc)","d209aab9":"print(\"Defining and training an LSTM model, using pre-trained embedding layer\")\n\nrnnmodel2 = Sequential()\nrnnmodel2.add(embedding_layer)\nrnnmodel2.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\nrnnmodel2.add(Dense(2, activation='sigmoid'))\nrnnmodel2.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nprint('Training the RNN')\n\nrnnmodel2.fit(x_train, y_train,\n          batch_size=32,\n          epochs=1,\n          validation_data=(x_val, y_val))\nscore, acc = rnnmodel2.evaluate(test_data, test_labels,\n                            batch_size=32)\nprint('Test accuracy with RNN:', acc)","eead47e4":"### 1D CNN Model with pre-trained embedding","6927fb2a":"Here we set all the paths of all the external datasets and models such as [glove](https:\/\/nlp.stanford.edu\/projects\/glove\/) and [IMDB reviews dataset](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/).","7d30b982":"### Loading and Preprocessing\n ","7e716ac7":"### LSTM Model with training your own embedding ","9765dc18":"### LSTM Model using pre-trained Embedding Layer","5833b9d9":"### 1D CNN model with training your own embedding","1150aa26":"In this notebook we will demonstrate different text classification models trained using the IMDB reviews dataset. "}}