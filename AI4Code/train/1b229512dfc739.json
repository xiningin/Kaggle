{"cell_type":{"a57c3591":"code","9dbd2f84":"code","f18f1812":"code","bdf27d78":"code","f7c807fd":"code","ed459dbc":"code","481a1d11":"code","20e0662d":"code","48d09b39":"code","b0267f33":"code","62a0d29a":"code","9d3453c4":"code","1d19d64e":"code","3eac1071":"code","57a13f17":"code","7e934f64":"code","6aaa742c":"code","1f04e54d":"code","61f82e45":"code","5d5f1dec":"code","c6d8fc44":"code","3c3c21fe":"code","24dd1dba":"code","44595027":"markdown","25e25d4d":"markdown","c777ea31":"markdown"},"source":{"a57c3591":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import log_loss\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9dbd2f84":"train=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')","f18f1812":"train.head()","bdf27d78":"test.dtypes","f7c807fd":"train['target'].value_counts()","ed459dbc":"test.head()","481a1d11":"train.shape,test.shape","20e0662d":"def create_folds(data,num_splits):\n    data['kfolds']=-1\n    \n    sk=StratifiedKFold(n_splits=num_splits,random_state=42,shuffle=True)\n    \n    for i,(t_,v_) in enumerate(sk.split(X=data.drop('target',axis=1),y=data.target.values)):\n        data.loc[v_,'kfolds']=i\n        \n    print(f\"Summary of the stratified folds\\n:{data['kfolds'].value_counts()}\")   \n    return data","48d09b39":"folds_data=create_folds(train,5)","b0267f33":"le=LabelEncoder()\nle.fit(folds_data['target'])\nfolds_data['target']=le.transform(folds_data['target'])","62a0d29a":"test.head()","9d3453c4":"N=4\nclf=RandomForestClassifier(criterion='gini',\n                           max_depth=7,\n                           n_estimators=2000,\n                           min_samples_split=5,\n                           max_features='auto',\n                           n_jobs=-1,\n                           oob_score=True,\n                           random_state=40\n                          )","1d19d64e":"def run(fold):\n    prob=pd.DataFrame((np.zeros(shape=(test.shape[0],4))),columns=['Class_1','Class_2','Class_3','Class_4'])\n    train_df=folds_data[folds_data['kfolds']==fold].reset_index(drop=True)\n    valid_df=folds_data[folds_data['kfolds']!=fold].reset_index(drop=True)\n    oof_preds=np.zeros(train_df.shape[0])\n    sub_preds=np.zeros(test.shape[0])\n    feats=[f for f in train_df.columns if f not in ['target','kfolds','id']]\n    feature_imp=pd.DataFrame((np.zeros(train_df[feats].shape[1],)),index=train_df[feats].columns)\n    clf.fit(train_df[feats],train_df['target'])\n    oof_preds=clf.predict_proba(valid_df[feats])\n    #print(oof_preds)\n    loss=log_loss(valid_df['target'],oof_preds)\n    print(f'Log loss:{np.mean(loss)}')\n    prob.loc[:,'Class_1']=clf.predict_proba(test.drop(['id'],axis=1))[:,0]\n    prob.loc[:,'Class_2']=clf.predict_proba(test.drop(['id'],axis=1))[:,1]\n    prob.loc[:,'Class_3']=clf.predict_proba(test.drop(['id'],axis=1))[:,2]\n    prob.loc[:,'Class_4']=clf.predict_proba(test.drop(['id'],axis=1))[:,3]\n    feature_imp.iloc[:,0]=clf.feature_importances_\n    #print(feature_imp)\n    feature_imp.rename(columns={feature_imp.columns[0]:f'fold_{fold}'},inplace=True)\n    print(f'OOB Score:{clf.oob_score_}')\n    return prob,feature_imp\n    ","3eac1071":"prob_1,feature_imp_1=run(0)","57a13f17":"prob_2,feature_imp_2=run(1)","7e934f64":"prob_3,feature_imp_3=run(2)","6aaa742c":"prob_4,feature_imp_4=run(3)","1f04e54d":"prob_5,feature_imp_5=run(4)","61f82e45":"prob=pd.concat([prob_1,prob_2,prob_3,prob_4,prob_5],axis=0).groupby(level=0).mean()\nfeature_importance=pd.concat([feature_imp_1,feature_imp_2,feature_imp_3,feature_imp_4,feature_imp_5],axis=1)","5d5f1dec":"feature_importance['fold_avg']=feature_importance.mean(axis=1)\nfeature_importance.sort_values(by='fold_avg',ascending=False,inplace=True)\nfeature_importance","c6d8fc44":"sample_submission=pd.DataFrame()\nsample_submission.loc[:,'id']=test['id']\nsample_submission.loc[:,['Class_1','Class_2','Class_3','Class_4']]=prob","3c3c21fe":"sample_submission.head()","24dd1dba":"sample_submission.to_csv('sample_submission.csv',index=False)","44595027":"# Baseline Model","25e25d4d":"## Creating Stratified K folds:","c777ea31":"### References:\n\n1.Abhishek Thakur's [Approaching Almost Any Machine Learning Problem Book](https:\/\/github.com\/abhi1thakur\/approachingalmost)\n\n2.Gunes Evitan - [Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial)"}}