{"cell_type":{"a96d074f":"code","e4f87881":"code","37600bba":"code","321ec08c":"code","8bfab549":"code","202035da":"code","a2711722":"code","f17d7c0c":"code","43c29f4f":"code","9af0afde":"code","01d11f8d":"code","32ce9b63":"code","e2f5fb62":"code","7e324ac7":"code","c9690d08":"code","add70492":"code","a17a8292":"code","36d5f3b2":"code","73659af1":"code","ac0571cd":"code","41be8183":"code","3f1c0a5a":"code","379abbc0":"code","590523fe":"code","7ca7f608":"code","bda11db3":"code","f029d883":"code","f0460811":"code","84b0530d":"code","01626823":"code","6ac16cee":"code","274c56aa":"code","100fc4ea":"code","af0fdbf6":"code","5803505f":"code","c388c8cf":"code","7176a304":"code","a1b4b0ea":"code","f6937bf3":"code","943900cc":"code","e1d0f8a1":"code","f1993b2d":"code","15ff61e9":"markdown","0866856c":"markdown","eabe97d4":"markdown","3088e241":"markdown","d2d9534f":"markdown","7dd7d5f7":"markdown","a7b3e8c0":"markdown","90212aa4":"markdown","07b63576":"markdown","0e5360e4":"markdown","244681f3":"markdown","8821c95c":"markdown","04d82b78":"markdown"},"source":{"a96d074f":"text = \"\"\"This book is about the economics of innovation and knowledge. One of the major conclusions drawn is that the perspectives standard economics imposes on society are biased, incomplete and inadequate. The focus on rational choice, allocation of scarce resources and equilibrium only captures some dimensions of the modern economy, notably short-term and static ones. Alternative perspectives, in which the focus is on learning as an interactive process and on processes of innovation, give visibility and direct attention to other, at least equally important and more dynamic, dimensions.\nSocial science is about human action and interaction, and it differs from natural science in several respects. It does not have access to laboratories where it is possible to organize controlled experiments. In spite of this, standard economics has gone far in adopting criteria and ideals from natural science, more precisely ideals that originate from Newtonian physics. This is reflected in standard economists\u2019 conception of equilibrium as an ideal reference state and their tendency to focus exclusively on quantitative relations, also paired with in its excessive use of mathematics.\nIn this book, I insist that economics should remain a social science while also taking into account the complexity of the strivings and hopes of human beings. People cannot be reduced to algorithms or automatons. The basic assumption about rational behaviour in economic models (in which individu- als and firms act as if they know everything about the future) is absurd and leads to equally absurd conclusions and to dubious policy recommendations.\nTaking a departure from more realistic assumptions about how and why people act as they do in society has implications for what constitutes a theory in social science. In social science, a theory should be regarded as a focusing device \u2013 no more and no less. This book presents two sets of theories or focus- ing devices \u2013 the innovation system and the learning economy that differ from those used in standard economics. These alternative focusing devices help us to see the core institutions in the economy (such as the market, the competition regime, the firm, the law, etc.) in a different light than that cast by mainstream economic theory.\nWhat is currently presented as the only and necessary pathway for the economy and for economic policy aiming at competitiveness and growth at the national level actually undermines both. The only certain outcome of cur- rent national strategies with focus on fiscal balance and cost competitiveness is that the rich get richer and the poor stay poor. Using an alternative analytical perspective, where the focus is on processes of innovation and learning, points in other possible directions for institutional design and economic policy, where the focus is on collective entrepreneurship, knowledge sharing and international collaboration.\"\"\"","e4f87881":"# We can split the text-chunk into something like sentences.\nsplit_text = text.split('.')\nprint(split_text)","37600bba":"# print out the first stentence\nsentence_3 = split_text[2]\nprint(sentence_3)","321ec08c":"# Let's create tokens\ntokens_sentence_3 = [word for word in sentence_3.split(' ')]\nprint(tokens_sentence_3)","8bfab549":"# Let's lowercase all these tokens and clean up the \\n (new line command)\n# Also we will replace \"()\" as well as make sure that only words lend in our list\ntokens_sentence_3_lower = [word.lower().strip() for word in sentence_3.split(' ')]\nprint('### OUTPUT1 ###')\nprint(tokens_sentence_3_lower)\nprint('\\n')\n    \ntokens_sentence_3_lower = [word.replace('(','').replace(')','') for word in tokens_sentence_3_lower if word.isalpha()]\n\nprint('### OUTPUT2 ###')\nprint(tokens_sentence_3_lower)\n","202035da":"# Removing stopwords\n\nstopwords_en = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]","a2711722":"tokens_sentence_3_clean = [word for word in tokens_sentence_3_lower if word not in stopwords_en]\nprint(tokens_sentence_3_clean)","f17d7c0c":"# Tokenizing sentences\nfrom nltk.tokenize import sent_tokenize\n\n# Tokenizing words\nfrom nltk.tokenize import word_tokenize\n\n# Tokenizing Tweets!\nfrom nltk.tokenize import TweetTokenizer","43c29f4f":"# Let's get our stences.\n# Note that the full-stops at the end of each sentence are still there\nsentences = sent_tokenize(text)\nprint(sentences)","9af0afde":"# Use word_tokenize to tokenize the third sentence: tokenized_sent\ntokenized_sent = word_tokenize(sentences[2])\n\n# Make a set of unique tokens in the entire scene: unique_tokens\nunique_tokens = set(word_tokenize(text))\nprint(unique_tokens)","01d11f8d":"tweets = [\"On behalf of @FLOTUS Melania & myself, THANK YOU for today's update & GREAT WORK! #SouthernBaptist @SendRelief,\u2026 https:\/\/t.co\/4yZCeXCt6n\",\n\"I will be going to Texas and Louisiana tomorrow with First Lady. Great progress being made! Spending weekend working at White House.\",\n\"Stock Market up 5 months in a row!\",\n\"'President Donald J. Trump Proclaims September 3, 2017, as a National Day of Prayer' #HurricaneHarvey #PrayForTexas\u2026 https:\/\/t.co\/tOMfFWwEsN\",\n\"Texas is healing fast thanks to all of the great men & women who have been working so hard. But still so much to do. Will be back tomorrow!\"]","32ce9b63":"# We can use the tweet tokenizer to parse these tweets:\n\ntknzr = TweetTokenizer()\ntweets_tokenized = [tknzr.tokenize(tweet) for tweet in tweets]\nprint(tweets_tokenized)","e2f5fb62":"# Get out all hashtags using loops\n\nhashtags = []\n\nfor tweet in tweets_tokenized:\n    hashtags.extend([word for word in tweet if word.startswith('#')])\n    \nprint(hashtags)","7e324ac7":"# We import the Counter module from python's standard collections\n\nfrom collections import Counter\n\nword_tokenized = word_tokenize(text)\nbow = Counter(word_tokenized)\nprint(bow.most_common())","c9690d08":"# Let's add some preprocessing\n\nfrom nltk.corpus import stopwords\n\nenglish_stopwords = stopwords.words('english')\n\nword_tokenized = word_tokenize(text)\n\n# lowercasing\ncleaned_word_tokenized = [word.lower().strip() for word in word_tokenized]\n# replacing some unwanted things\ncleaned_word_tokenized = [word.replace('(','').replace(')','') for word in cleaned_word_tokenized if word.isalpha()]\n# removing stopwords\ncleaned_word_tokenized = [word for word in cleaned_word_tokenized if word not in english_stopwords]\n\nbow = Counter(cleaned_word_tokenized)\nprint(bow.most_common())","add70492":"# Let's import a lemmatizer from NLTK and try how it works\nfrom nltk.stem import WordNetLemmatizer\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in cleaned_word_tokenized]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))","a17a8292":"# We start by importing the data, ~1900 Abstracts\/Titles from Scopus\nimport pandas as pd\n\nabstracts = pd.read_csv('..\/input\/abstracts.csv')","36d5f3b2":"# Let's inspect the data\nabstracts.head()","73659af1":"# Tokenize each abstract\nabstracts['tokenized'] = abstracts['Abstract'].map(lambda t: word_tokenize(t))","ac0571cd":"# lowecase, strip and ensure it's words\nabstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])","41be8183":"# lemmarize and remove stopwords\nabstracts['tokenized'] = abstracts['tokenized'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])","3f1c0a5a":"# We start by importing and initializing a Gensim Dictionary. \n# The dictionary will be used to map between words and IDs\n\nfrom gensim.corpora.dictionary import Dictionary\n\n# Create a Dictionary from the articles: dictionary\ndictionary = Dictionary(abstracts['tokenized'])","379abbc0":"# And this is how you can map back and forth\n# Select the id for \"firm\": firm_id\nfirm_id = dictionary.token2id.get(\"firm\")\n\n# Use computer_id with the dictionary to print the word\nprint(dictionary.get(firm_id))","590523fe":"# Create a Corpus: corpus\n# We use a list comprehension to transform our abstracts into BoWs\ncorpus = [dictionary.doc2bow(abstract) for abstract in abstracts['tokenized']]","7ca7f608":"# Print the first 10 word ids with their frequency counts from the fifth document\nprint(corpus[10][:10])\n\n# This is the same what we did before when we were counting words with the Counter (just in big)","bda11db3":"# Sort the doc for frequency: bow_doc\nbow_doc = sorted(corpus[10], key=lambda w: w[1], reverse=True)\n\n# Print the top 5 words of the document alongside the count\nfor word_id, word_count in bow_doc[:10]:\n    print(dictionary.get(word_id), word_count)","f029d883":"# Import the TfidfModel from Gensim\nfrom gensim.models.tfidfmodel import TfidfModel\n\n# Create and fit a new TfidfModel using the corpus: tfidf\ntfidf = TfidfModel(corpus)\n\n# Calculate the tfidf weights of doc: tfidf_weights\ntfidf_weights = tfidf[corpus[10]]\n\n# Print the first five weights\nprint(tfidf_weights[:5])","f0460811":"# Sort the weights from highest to lowest: sorted_tfidf_weights\nsorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in sorted_tfidf_weights[:10]:\n    print(dictionary.get(term_id), weight)","84b0530d":"# Now we can transform the whole corpus\ntfidf_corpus = tfidf[corpus]","01626823":"# Just like before, we import the model\nfrom gensim.models.lsimodel import LsiModel\n\n# And we fir it on the tfidf_corpus pointing to the dictionary as reference and the number of topics.\n# In more serious settings one would pick between 300-400\nlsi = LsiModel(tfidf_corpus, id2word=dictionary, num_topics=100)","6ac16cee":"# Once the model is ready, we can inspect the topics\nlsi.show_topics(num_topics=10)","274c56aa":"# And just as before, we can use the trained model to transform the corpus\nlsi_corpus = lsi[tfidf_corpus]","100fc4ea":"# Load the MatrixSimilarity\nfrom gensim.similarities import MatrixSimilarity\n\n# Create the document-topic-matrix\ndocument_topic_matrix = MatrixSimilarity(lsi_corpus)\ndocument_topic_matrix = document_topic_matrix.index","af0fdbf6":"# Let's identify some clusters in our corpus\n\n# We import KMeans form the Sklearn library\nfrom sklearn.cluster import KMeans\n\n# Instatiate a model with 4 clusters\nkmeans = KMeans(n_clusters=10)\n\n# And fit it on our matrix\nkmeans.fit(document_topic_matrix)","5803505f":"# Let's annotate our abstracts with the assigned cluster number\nabstracts['cluster'] = kmeans.labels_","c388c8cf":"# We can try to visualize our documents using TSNE - an approach for visualizing high-dimensional data\n\n# Import the module first\nfrom sklearn.manifold import TSNE\n\n# And instantiate\ntsne = TSNE()\n\n# Let's try to boil down the 100 dimensions into 2\nvisualization = tsne.fit_transform(document_topic_matrix)","7176a304":"# Import plotting library\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a1b4b0ea":"plt.figure(figsize=(15,15))\nsns.scatterplot(visualization[:,0],visualization[:,1], data = abstracts, palette='RdBu', hue=abstracts.cluster, legend='full')","f6937bf3":"# Preprocessing\nabstracts['title_tok'] = abstracts['Title'].map(lambda t: word_tokenize(t))\nabstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [word.lower().strip() for word in t if word.isalpha()])\nabstracts['title_tok'] = abstracts['title_tok'].map(lambda t: [wordnet_lemmatizer.lemmatize(word) for word in t if word not in stopwords_en])","943900cc":"# Collectiong\n\nCluster = 2\n\ncluster_titles = []\nfor x in abstracts[abstracts['cluster'] == Cluster]['title_tok']:\n    cluster_titles.extend(x)","e1d0f8a1":"# Transfortm into tf_idf format\ntitles_tfidf = tfidf[dictionary.doc2bow(cluster_titles)]","f1993b2d":"# Sort the weights from highest to lowest: sorted_tfidf_weights\ntitles_tfidf = sorted(titles_tfidf, key=lambda w: w[1], reverse=True)\n\n# Print the top 5 weighted words\nfor term_id, weight in titles_tfidf[:20]:\n    print(dictionary.get(term_id), weight)","15ff61e9":"#### TF-IDF - Term Frequency - Inverse Document Frequency\n\nA token is importan for a document if appears very often\nA token becomes less important for comparaison across a corpus if it appears all over the place in the corpus\n\n*Innovation* in a corpus of abstracts talking about innovation is not that important\n\n\\begin{equation*}\nw_{i,j} = tf_{i,j}*log(\\frac{N}{df_i})\n\\end{equation*}\n\n- $w_{i,j}$ = the TF-IDF score for a term i in a document j\n- $tf_{i,j}$ = number of occurence of term i in document j\n- $N$ = number of documents in the corpus\n- $df_i$ = number of documents with term i\n\n\nWe will use TF-IDF to transform our corpus. However, first we need to fir the TF-IDF model.","0866856c":"**Introducing Lambda Functions** Python allows you to write short functions in one line using the *lambda* keyword with a variable and a \":\". \nBelow we will transform the abstract column into a new one that we call tokenized compressing our preprocessing pipeline into 3 lines\n\nWe combine our lambda functions with the Pandas method \"map\" that apply this function to every row.","eabe97d4":"# Natural Language Processing (NLP) in Python\n## An introduction to key concepts and techniques\n\nIn this notebook we are going to explore central concepts of NLP and their implementation in modern high-level Python libraries.\nThis is aimed to be a very general introduction to make this field more approacheable and also provide some familiarity with the specific jargon. While NLP offers many opportunities as a technique (or actually an array of different techniques) for social science research the application is yet limited but growing.\n\nThe research field of NLP itself has been turn upside-down and developed a lot since the introduction of word embeddings around 2013 and the growth of deep learning (neural network models) in the past 3-4 years. Particularly recurrent neural networks and the LSTM (Long short-term memory) variation shifted the research field.\n\n![nlp problems](https:\/\/image.slidesharecdn.com\/lang-detect-161011092815\/95\/nlp-project-full-cycle-16-638.jpg)\n\n\nThis workshop aims at presenting established techniques that I think are most useful in a social science research setting.\n\nTo be more specific, below we will explore:\n\n- basic string manipulation\n- tokens and tokenization + some preprocessing\n- the Bag-of-Words model\n- topic modeling (and its close relation to dimensionality reduction \/ unsupervised machine learning)\n- entity extraction\n- text classification","3088e241":"At this point, our corpus is a document-topic matrix. in corpus-format. We can create a full matrix using the built in MatrixSimilarity function (which is actually used for similarity-queries)","d2d9534f":"Now let's explore the different clusters. For that we will look at the titles. We could do it \"manually\" but why not using NLP for that, too.\nWe will preprocess the titles, just as we did witht he abstracts and then use TF-IDF of the title-token-sum of each cluster to see which tokens are most important in which cluster.","7dd7d5f7":"### Basic string manipulation and tokenization\n\nIn the following we will just juse basic python string manipulation\n\nYou can do much much more if you learn using regular expressions (RegEx) but that would go too far.\n\nLet's start with this text from Benght-\u00c5ke Lundvall's *The Learning Economy and the Economics of Hope* (2016)\n[free download here](download here)\n\n>In this book, I insist that economics should remain a social science while also taking into account the complexity of the strivings and hopes of human beings. People cannot be reduced to algorithms or automatons. The basic assumption about rational behaviour in economic models (in which individu- als and firms act as if they know everything about the future) is absurd and leads to equally absurd conclusions and to dubious policy recommendations.\n\n![hope eco](https:\/\/www.business.aau.dk\/digitalAssets\/302\/302463_bengt_aake_bog_650x350.jpg)","a7b3e8c0":"The transformed corpus is much more interesting in terms of analysis than the pure bag of words representation. In fact, you could transform it now into a matrix and perform clustering and other unsupervised machine learning.\n\n![surprise](http:\/\/www.jaclynfriedman.com\/wp-content\/uploads\/2018\/06\/giphy-23.gif)\n\n**Surprise**: This is exactly what topic modelling is about! Algorithms like LSI are closely related to PCA, NMF and SVD.\n\n","90212aa4":"Let's see how this works with teweets using a well known example","07b63576":"Introducing NLTK, which will make your life much easier","0e5360e4":"### Bag of words model\n\nIn order for a computer to understand text we need to somehow find a useful representation.\nIf you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords\nIn that case you could represent each document with a (sparse) vector with 1 for \"keyword present\" and 0 for \"keyword absent\"\nWe can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.\nFor a corpus of documents that would give us a document-term matrix\n![example](https:\/\/i.stack.imgur.com\/C1UMs.png)\n\nLet's try creating a bag of words model from our initial example.","244681f3":"One important part of text preprocessing is normalization. Here we can use stemmers and lematizers to aggregate plural forms and similar. This can be extremely useful if working with languages that have a rich morphology such as Russian or Turkish.\n\n![example_stemm](https:\/\/image.slidesharecdn.com\/lightweightnaturallanguageprocessingnlp-120314154200-phpapp01\/95\/lightweight-natural-language-processing-nlp-34-728.jpg?cb=1331814243)","8821c95c":"So far you learned some basic unicode string manipulation and I also introduced NLTK. If you want to lean more about traditional NLP, check out the [free online book on NLTK](https:\/\/www.nltk.org\/book\/). You will learn old school NLP along with Python (and general programming foundations).\n\nWhen it comes to comparing documents (this is often what we want), simple \"keyword counts\" may be too simplistic and sure, we can do better \u2013 we can do topic modeling. One amazing library for working with state of the art topic models is Gensim.\n\n![gensim](https:\/\/rare-technologies.com\/wp-content\/uploads\/2017\/01\/atmodel_plot-855x645.png)\n\nLet's try to work with a bigger dataset.\n\nGensim allows you to work with a large number of high-performant NLP models including word embedding techniques.  We will be using something more traditional: TF-IDF and LSI","04d82b78":"Sure, one could do so much more to pre-process. We could try to identify bi-grams, remove prepositions, verbs etc. But already this brings us rather far.\n\nNow we will dive into Gensim further transform our abstracts using more advanced techniques."}}