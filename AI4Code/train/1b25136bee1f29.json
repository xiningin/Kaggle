{"cell_type":{"aa440b1f":"code","1a2da85b":"code","798f26a3":"code","4e7f40b5":"code","b32c1478":"code","39668b50":"code","e483b935":"code","e255cff9":"code","2b9093c1":"code","4a2d85d3":"code","e50b9820":"code","682d3e43":"code","4e1d7665":"code","3f220606":"code","901ed9e2":"code","21c98dbe":"code","a56f5090":"code","c6bcfc57":"markdown","12b4a997":"markdown","4c17c2a1":"markdown","94f80658":"markdown","d778d890":"markdown","e92f4219":"markdown"},"source":{"aa440b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gensim\nimport os\nimport re\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, GRU, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1a2da85b":"data = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndata.head()","798f26a3":"def clean_text(text):\n    text = text.lower()\n    \n    pattern = re.compile('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    text = pattern.sub('', text)\n    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n    emoji = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    \n    text = emoji.sub(r'', text)\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)        \n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text) \n    text = re.sub(r\"\\'ll\", \" will\", text)  \n    text = re.sub(r\"\\'ve\", \" have\", text)  \n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"did't\", \"did not\", text)\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"have't\", \"have not\", text)\n    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?\/;`~:<>+=-]\", \"\", text)\n    return text","4e7f40b5":"import string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\ndef CleanTokenize(df):\n    tweet_lines = list()\n    lines = df[\"text\"].values.tolist()\n\n    for line in lines:\n        line = clean_text(line)\n        # tokenize the text\n        tokens = word_tokenize(line)\n    #     tokens = [w.lower() for w in tokens]\n        # remove puntuations\n        table = str.maketrans('', '', string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove non alphabetic characters\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stop_words]\n        tweet_lines.append(words)\n    return tweet_lines\n\ntweet_lines = CleanTokenize(data)\ntweet_lines[0:10]","b32c1478":"# import string\n# fake_lines = [\"#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\",\n#               \"@bbcmtd Wholesale Markets ablaze https:\/\/t.co\/lHYXEOHY6C\"]\n\n# fake_tweet_lines = list()\n# for line in fake_lines:\n#     line = clean_text(str(line))\n#     # tokenize the text\n#     tokens = word_tokenize(line)\n#     #  tokens = [w.lower() for w in tokens]\n#     # remove puntuations\n#     table = str.maketrans('', '', string.punctuation)\n#     stripped = [w.translate(table) for w in tokens]\n#     # remove non alphabetic characters\n#     words = [word for word in stripped if word.isalpha()]\n#     stop_words = set(stopwords.words(\"english\"))\n#     words = [w for w in words if not w in stop_words]\n#     fake_tweet_lines.append(words)\n    \n# fake_tweet_lines","39668b50":"from collections import Counter\nfrom wordcloud import WordCloud, ImageColorGenerator\nimport matplotlib.pyplot as plt\nall_tweets = [j for sub in tweet_lines for j in sub] \nword_could_dict=Counter(all_tweets)\nword_could_dict.most_common(10)\n\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")","e483b935":"pos_data = data.loc[data['target'] == 1]\npos_tweet_lines = CleanTokenize(pos_data)\npos_tweets = [j for sub in pos_tweet_lines for j in sub] \nword_could_dict=Counter(pos_tweets)\nword_could_dict.most_common(10)\n\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")","e255cff9":"neg_data = data.loc[data['target'] == 0]\nneg_tweet_lines = CleanTokenize(neg_data)\nneg_tweets = [j for sub in neg_tweet_lines for j in sub] \nword_could_dict=Counter(neg_tweets)\nword_could_dict.most_common(10)\n\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")","2b9093c1":"VALIDATION_SPLIT = 0.2\nmax_length = 10\nEMBEDDING_DIM = 120\n\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(tweet_lines)\nsequences = tokenizer_obj.texts_to_sequences(tweet_lines)\n\nword_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))\nvocab_size = len(tokenizer_obj.word_index) + 1\nprint('vocab_size '+str(vocab_size))\n\nreview_pad = pad_sequences(sequences, maxlen=max_length, padding='post')\nsentiment =  data['target'].values\n\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\nreview_pad = review_pad[indices]\nsentiment = sentiment[indices]\n\nnum_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n\nX_train_pad = review_pad[:-num_validation_samples]\ny_train = sentiment[:-num_validation_samples]\nX_test_pad = review_pad[-num_validation_samples:]\ny_test = sentiment[-num_validation_samples:]","4a2d85d3":"print('Shape of X_train_pad tensor:', X_train_pad.shape)\nprint('Shape of y_train tensor:', y_train.shape)\n\nprint('Shape of X_test_pad tensor:', X_test_pad.shape)\nprint('Shape of y_test tensor:', y_test.shape)","e50b9820":"model = Sequential()\nmodel.add(Embedding(vocab_size, EMBEDDING_DIM, input_length=max_length))\nmodel.add(LSTM(units=32,  dropout=0.4, recurrent_dropout=0.3))\n# model.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint('Summary of the built model...')\nprint(model.summary())","682d3e43":"model.fit(X_train_pad, y_train, batch_size=32, epochs=2, validation_data=(X_test_pad, y_test), verbose=2)","4e1d7665":"test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nlen(test)","3f220606":"test_lines = CleanTokenize(test)\nlen(test_lines)\ntest_sequences = tokenizer_obj.texts_to_sequences(test_lines)\ntest_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\ntest_review_pad.shape","901ed9e2":"predictions = model.predict(test_review_pad)","21c98dbe":"submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmission[\"target\"] = predictions\nsubmission[\"target\"] = submission[\"target\"].apply(lambda x : 0 if x<=.5 else 1)","a56f5090":"submission.to_csv(\"submit_2.csv\", index=False)","c6bcfc57":"## *Train the model*","12b4a997":"## *Get the data*\n","4c17c2a1":"## *Make the model*","94f80658":"## *Clean and tokenize the text*","d778d890":"## *Predict*","e92f4219":"## *Train-Test split*"}}