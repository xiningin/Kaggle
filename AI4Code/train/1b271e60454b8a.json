{"cell_type":{"d45a89e8":"code","b6c548ec":"code","184fd044":"code","8778433d":"code","fcf76458":"code","ed6c992f":"code","f01548a0":"code","862c3a41":"code","d1f19b18":"code","a13d1062":"code","21e1ffda":"code","3aae9536":"markdown","39f32538":"markdown","1e4022e3":"markdown","3b4fb979":"markdown","4e3be460":"markdown","858349cb":"markdown","dd4f183f":"markdown","51d91586":"markdown"},"source":{"d45a89e8":"import numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nimport scipy\nfrom tqdm import tqdm_notebook as tqdm","b6c548ec":"dd0=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\nddtest0=pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\nddall=dd0.append(ddtest0, sort=False)\nnum_train=len(dd0)\nddall.head()","184fd044":"drop_cols=[\"bin_0\"]\n\n# Split 2 Letters; This is the only part which is not generic and would actually require data inspection\nddall[\"ord_5a\"]=ddall[\"ord_5\"].str[0]\nddall[\"ord_5b\"]=ddall[\"ord_5\"].str[1]\ndrop_cols.append(\"ord_5\")","8778433d":"for col in [\"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\"]:\n    train_vals = set(dd0[col].unique())\n    test_vals = set(ddtest0[col].unique())\n   \n    xor_cat_vals=train_vals ^ test_vals\n    if xor_cat_vals:\n        ddall.loc[ddall[col].isin(xor_cat_vals), col]=\"xor\"","fcf76458":"X=ddall[ddall.columns.difference([\"id\", \"target\"] + drop_cols)]","ed6c992f":"X_oh=X[X.columns.difference([\"ord_1\", \"ord_4\", \"ord_5a\", \"ord_5b\", \"day\", \"month\"])]\noh1=pd.get_dummies(X_oh, columns=X_oh.columns, drop_first=True, sparse=True)\nohc1=oh1.sparse.to_coo()","f01548a0":"from sklearn.base import TransformerMixin\nfrom itertools import repeat\nimport scipy\n\n\nclass ThermometerEncoder(TransformerMixin):\n    \"\"\"\n    Assumes all values are known at fit\n    \"\"\"\n    def __init__(self, sort_key=None):\n        self.sort_key = sort_key\n        self.value_map_ = None\n    \n    def fit(self, X, y=None):\n        self.value_map_ = {val: i for i, val in enumerate(sorted(X.unique(), key=self.sort_key))}\n        return self\n    \n    def transform(self, X, y=None):\n        values = X.map(self.value_map_)\n        \n        possible_values = sorted(self.value_map_.values())\n        \n        idx1 = []\n        idx2 = []\n        \n        all_indices = np.arange(len(X))\n        \n        for idx, val in enumerate(possible_values[:-1]):\n            new_idxs = all_indices[values > val]\n            idx1.extend(new_idxs)\n            idx2.extend(repeat(idx, len(new_idxs)))\n            \n        result = scipy.sparse.coo_matrix(([1] * len(idx1), (idx1, idx2)), shape=(len(X), len(possible_values)), dtype=\"int8\")\n            \n        return result","862c3a41":"thermos=[]\nfor col in [\"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5a\", \"day\", \"month\"]:\n    if col==\"ord_1\":\n        sort_key=['Novice', 'Contributor', 'Expert', 'Master', 'Grandmaster'].index\n    elif col==\"ord_2\":\n        sort_key=['Freezing', 'Cold', 'Warm', 'Hot', 'Boiling Hot', 'Lava Hot'].index\n    elif col in [\"ord_3\", \"ord_4\", \"ord_5a\"]:\n        sort_key=str\n    elif col in [\"day\", \"month\"]:\n        sort_key=int\n    else:\n        raise ValueError(col)\n    \n    enc=ThermometerEncoder(sort_key=sort_key)\n    thermos.append(enc.fit_transform(X[col]))","d1f19b18":"ohc=scipy.sparse.hstack([ohc1] + thermos).tocsr()\ndisplay(ohc)\n\nX_train = ohc[:num_train]\nX_test = ohc[num_train:]\ny_train = dd0[\"target\"].values","a13d1062":"clf=LogisticRegression(C=0.123456789, solver=\"lbfgs\", max_iter=5000)  # MODEL\n\nclf.fit(X_train, y_train)\n\npred=clf.predict_proba(X_test)[:,1]\n\npd.DataFrame({\"id\": ddtest0[\"id\"], \"target\": pred}).to_csv(\"submission.csv\", index=False)","21e1ffda":"from sklearn.model_selection import cross_validate\n\nscore=cross_validate(clf, X_train, y_train, cv=3, scoring=\"roc_auc\")[\"test_score\"].mean()\nprint(f\"{score:.6f}\")","3aae9536":"The recipe for today is just doing One-Hot most columns and Thermometer encoding of some ordinal columns. Finally, we apply plain Logistic Regression.\n\nDid anyone have success with more complex methods? Please let us know!","39f32538":"# Crazy feature engineering","1e4022e3":"## Thermometer encode some ordinal columns","3b4fb979":"## One-Hot Encode all","4e3be460":"# Make prediction","858349cb":"# Evaluate","dd4f183f":"## Combine sparse matrices","51d91586":"## Map cat vals which are not in both sets to single values"}}