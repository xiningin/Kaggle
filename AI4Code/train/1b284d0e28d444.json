{"cell_type":{"10f77266":"code","0f81a4de":"code","18e1dcd5":"code","3f05b263":"code","79abb13e":"code","719d3dea":"markdown"},"source":{"10f77266":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","0f81a4de":"import l5kit, os\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/visualisation_config.yaml\")","18e1dcd5":"from l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\ndm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","3f05b263":"import numpy as np\nfrom IPython.display import display, clear_output\nimport PIL\ncfg[\"raster_params\"][\"map_type\"]=\"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset=EgoDataset(cfg,zarr_dataset,rast)\nscene_idx=2\nindexes=dataset.get_scene_indices(scene_idx)\nimages=[]\nfor idx in indexes:\n    data=dataset[idx]\n    im=data[\"image\"].transpose(1,2,0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels=transform_points(data[\"target_positions\"]+data[\"centroid\"][:2],data[\"world_to_image\"])\n    center_in_pixles=np.asarray(cfg[\"raster_params\"][\"ego_center\"])*cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im,target_positions_pixels,data[\"target_yaws\"],TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","79abb13e":"from IPython.display import display, clear_output\nfrom IPython.display import HTML\n\nimport PIL\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\ndef animate_solution(images):\n\n    def animate(i):\n        im.set_data(images[i])\n \n    fig, ax = plt.subplots()\n    im = ax.imshow(images[0])\n    \n    return animation.FuncAnimation(fig, animate, frames=len(images), interval=60)\ncfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 34\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    images.append(PIL.Image.fromarray(im[::-1]))\nanim = animate_solution(images)\nHTML(anim.to_jshtml())","719d3dea":"Thanks for the inspiration https:\/\/www.kaggle.com\/omareltouny\/lyft-understanding-the-data-and-eda\/\nsource code: https:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/visualisation\/visualise_data.ipynb"}}