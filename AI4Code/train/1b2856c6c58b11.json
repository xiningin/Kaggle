{"cell_type":{"5c7a0201":"code","0181a7ca":"code","eb4c01d6":"code","04fd23e7":"code","ffbe3490":"code","cd121079":"code","38610433":"code","19634d13":"code","abc92fe2":"code","10f658ae":"code","9d8476e1":"code","8927c42c":"code","f1374d46":"code","c09874ef":"code","f827b91b":"code","4c611568":"code","d31c7b2b":"code","df73a8d8":"code","b0c572be":"code","d980bc17":"code","ad467e10":"code","87387e5d":"code","388db67f":"code","f364d290":"code","c34cb682":"markdown","f3c93049":"markdown","f23019a8":"markdown","e1f0e490":"markdown","e2ddc1f8":"markdown","d4651c0b":"markdown","1cbb921d":"markdown","111b7b9a":"markdown","5ed0319e":"markdown","330fd2d6":"markdown","ff14864a":"markdown","8bf0d2d6":"markdown","0990856e":"markdown","2f0bd4e7":"markdown","9c068bd7":"markdown","6679c5ee":"markdown","e9b53555":"markdown","26427659":"markdown","6e1051c1":"markdown","c58b9221":"markdown","1140c807":"markdown","e877c457":"markdown","0bf8b6fa":"markdown"},"source":{"5c7a0201":"# Let us load in the relavant python modules\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n# notebook\uc744 \uc2e4\ud589\ud55c \ube0c\ub77c\uc6b0\uc800\uc5d0\uc11c \ubc14\ub85c \uadf8\ub9bc\uc744 \ubcfc \uc218 \uc788\uac8c \ud574\uc8fc\ub294 \uac83\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')","0181a7ca":"train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')","eb4c01d6":"# taking a look at how many rows and columns the train dataset contains\nrows = train.shape[0]\ncolumns = train.shape[1]\nprint('train dataset contains {} rows and {} columns'.format(rows, columns))","04fd23e7":"# any() applied twice to check run the isnull check across all columns\ntrain.isnull().any().any()","ffbe3490":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","cd121079":"import missingno as msno\n# nullity or missing values by columns\nmsno.matrix(df = train_copy.iloc[:, 2:39], figsize = (20, 14),\n            color = (0.42, 0.1, 0.05))","38610433":"data = [go.Bar(\n    x = train['target'].value_counts().index.values,\n    y = train['target'].value_counts().values,\n    text = 'Distribution of target variable'\n)]\n\nlayout = go.Layout(title = 'target variable distribution')\n\nfig = go.Figure(data = data, layout = layout)\n\npy.iplot(fig, filename = 'basic-bar')","19634d13":"Counter(train.dtypes.values)","abc92fe2":"train_float = train.select_dtypes(include = ['float64'])\ntrain_int = train.select_dtypes(include = ['int64'])","10f658ae":"# Correlation of float features\n\ncolormap = plt.cm.magma\nplt.figure(figsize = (16,12))\nplt.title('pearson correlation of continuous features', y = 1.05, size = 15)\nsns.heatmap(train_float.corr(), linewidths = 0.1, vmax = 1.0,\n            square = True, cmap = colormap, linecolor = 'white', annot = True)","9d8476e1":"# ver.1.0\n\n#train_int = train_int.drop(['id', 'target'], axis = 1)\ncolormap = plt.cm.bone\nplt.figure(figsize = (21, 16))\nplt.title('pearson correlation of categorical features', y = 1.05, size = 15)\nsns.heatmap(train_int.corr(), linewidths = 0.1, vmax = 1.0, square = True,\n            cmap = colormap, linecolor = 'white', annot = False)","8927c42c":"data = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        opacity = 1.0 )\n]","f1374d46":"# ver.2.0\n\ndata = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        opacity = 1.0 )\n]\n\nlayout = go.Layout(title = 'pearson correlation of Integer-type features',\n                   xaxis = dict(ticks = '', nticks = 36),\n                   yaxis = dict(ticks = ''),\n                   width = 900, height = 700)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'labelled')","c09874ef":"import tqdm as tqdm","f827b91b":"mf = mutual_info_classif(train_float.values,train.target.values,n_neighbors=3, random_state=17 )\nprint(mf)","4c611568":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\n\nfor col in bin_col:\n    zero_list.append((train[col] == 0).sum())\n    one_list.append((train[col] == 1).sum())","d31c7b2b":"trace1 = go.Bar(x = bin_col,\n                y = zero_list,\n                name = 'zero count')\n\ntrace2 = go.Bar(x = bin_col,\n                y = one_list,\n                name = 'one count')\n\ndata = [trace1, trace2]\nlayout = go.Layout(barmode = 'stack',\n                   title = 'count of 1 and 0 in binary variables')\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'stacked-bar')","df73a8d8":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 150, max_depth = 8, min_samples_leaf = 4,\n                            max_features = 0.2, n_jobs = -1, random_state = 0)\nrf.fit(train.drop(['id', 'target'], axis = 1), train.target)\n\nfeatures = train.drop(['id', 'target'], axis = 1).columns.values\n\nprint('---- training done -----')","b0c572be":"# scatter plot\ntrace = go.Scatter(y = rf.feature_importances_, x = features,\n                   mode ='markers', marker = dict(sizemode = 'diameter', sizeref = 1,\n                                                  size = 13, color = rf.feature_importances_,\n                                                  colorscale = 'Portland', showscale = True), text = features)\ndata = [trace]\n\nlayout = go.Layout(autosize = True, title = 'random forest feature importance',\n                   xaxis = dict(ticklen = 5, showgrid = False, zeroline = False, showline = False),\n                   yaxis = dict(title = 'feature importance', showgrid = False, zeroline = False, ticklen = 5, gridwidth = 2),\n                   showlegend = False)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'scatter 2010')","d980bc17":"x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features),\n                                    reverse = False)))\n\ntrace2 = go.Bar(x = x, y = y, \n                marker = dict(color = x, colorscale = 'Viridis', reversescale = True),\n                name = 'random forest feature importance', orientation = 'h')\n\nlayout = dict(title = 'barplot of feature importances', width = 900, height = 2000,\n              yaxis = dict(showgrid = False, showline = False, showticklabels = True))\n\nfig1 = go.Figure(data = [trace2])\nfig1['layout'].update(layout)\n\npy.iplot(fig1, filename = 'plots')","ad467e10":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(train.drop(['id', 'target'], axis = 1), train.target)\n\n# export our trained model as a .dot file\nwith open('tree1.dot', 'w') as f:\n    f = tree.export_graphviz(decision_tree,\n                             out_file = f,\n                             max_depth = 4,\n                             impurity = False,\n                             feature_names = train.drop(['id', 'target'], axis =1).columns.values,\n                             class_names = ['no', 'yes'],\n                             rounded = True,\n                             filled = True)\n\n# convert .dot to .png to allow display in web notebook\ncheck_call(['dot', '-Tpng', 'tree1.dot', '-o', 'tree1.png'])    \n\n# annoting chart with PIL\nimg = Image.open('tree1.png')\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage('sample-out.png')","87387e5d":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators = 100, max_depth = 3, min_samples_leaf = 4,\n                                max_features = 0.2, random_state = 0)\n\n\ngb.fit(train.drop(['id', 'target'], axis = 1), train.target)\nfeatures = train.drop(['id', 'target'], axis = 1).columns.values\n\nprint('------- Training Done -------')","388db67f":"# scatter plot\ntrace = go.Scatter(y = gb.feature_importances_,\n                   x = features,\n                   mode = 'markers',\n                   marker = dict(sizemode = 'diameter', sizeref = 1, size = 13, color = gb.feature_importances_, colorscale = 'Portland', showscale = True),\n                   text = features)\n\ndata = [trace]\n\nlayout = go.Layout(autosize = True, title = 'gradient boosting machine feature importance',\n                   xaxis = dict(ticklen = 5, showgrid = False, zeroline = False, showline = False),\n                   yaxis = dict(title = 'feature importance', showgrid = False, zeroline = False, ticklen = 5, gridwidth = 2),\n                   showlegend = False)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'scatter 2010')","f364d290":"x, y = (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features), reverse = False)))\n\ntrace2 = go.Bar(x = x, y = y, marker = dict(color = x, colorscale = 'Viridis', reversescale = True),\n                name = 'gradient boosting classfier feature importance', orientation = 'h')\n\nlayout = dict(title = 'barplot of feature importance', width = 900, height = 2000,\n              yaxis = dict(showgrid = False, showline = False, showticklabels = True))\n\nfig1 = go.Figure(data = [trace2])\nfig1['layout'].update(layout)\n\npy.iplot(fig1, filename = 'plots')","c34cb682":"As alluded to above, there are a total of 59 columns that make up the train dataset and as we can observe from this check, the features\/columns consist of only two datatypes - Integer and floats.\n\nAnother point to note is that Porto Seguro has actually provided us data with headers that come suffixed with abbreviations such as \"_bin\", \"_cat\" and \"_reg\", where they have given us a rough explanation that _bin indicates binary features while _cat indicates categorical features whilst the rest are either continuous or ordinal features. \n\nHere I shall simplify this a bit further just by looking at float values (probably only the continuous features) and integer datatypes (binary, categorical and ordinal features).","f3c93049":"From the correlation plot, we can see that the majority of the features display zero or no correlation to one another. This is quite an interesting observation that will warrant our further investigation later down. For now, the paired features that display a positive linear correlation are listed as follows:\n\n1. (ps_reg_01, ps_reg_03)\n\n2. (ps_reg_02, ps_reg_03)\n\n3. (ps_car_12, ps_car_13)\n\n4. (ps_car_13, ps_car_15)","f23019a8":"#### Feature importance via Gradient Boosting model\n\nJust for curiosity, let us try another learning method in getting our feature importances. This time, we use a Gradient Boosting classifier to fit to the training data . Gradient Boosting proceeds in a forward stage-wise fashion, where at each stage regression tress are fitted on the gradient of the loss function (which defaults to the deviance in Sklearn implementation).","e1f0e490":"Next, we can use resident Kaggler's Aleksey Bilogur - creator of the \"Missingno\" package which is a most useful and convenient tool in visualising missing values in the dataset, so check it out.","e2ddc1f8":"* Our null values check returns False but however, this does not really mean that this case has been closed as the data is also described as \"Values of -1 indicate that the feature was missing from the observation\". Therefore I take it that Porto Seguro has simply conducted a blanket replacement of all null values in the data with the value of -1. Let us now inspect if there where any missing values in the data.","d4651c0b":"## Introduction\n\nThis competition is hosted by the third largest insurance company in Brazil: Porto Seguro with the task of predicting the probability that a driver will initiate an insurance claim in the next year.\n\nThis notebook will aim to provide some interactive charts and analysis of the competition data by way of the Python visualisation library Plot.ly and hopefully bring some insights and beautiful plots that others can take and replicate. Plot.ly is one of the main products offered by the software company - Plotly which specializes in providing online graphical and statistical visualisations (charts and dashboards) as well as providing an API to a whole rich suite of programming languages and tools such as Python, R, Matlab, Node.js etc.\n\nListed below for easy convenience are links to the various Plotly plots in this notebook:\n\n1. Simple horizontal bar plot - Used to inspect the Target variable distribution\n2. Correlation Heatmap plot - Inspect the correlation between the different features\n3. Scatter plot - Compare the feature importances generated by Random Forest and Gradient-Boosted model\n4. Vertical bar plot - List in Descending order, the importance of the various features\n5. 3D Scatter plot\n\nThe themes in this notebook can be briefly summarized follows:\n\n1. Data Quality Checks : Visualising and evaluating all missing\/Null values (values that are -1)\n\n2. Feature inspection and filtering : Correlation and feature Mutual information plots against the target variable. Inspection of the Binary, categorical and other variables.\n\n3. Feature importance ranking via learning models : Building a Random Forest and Gradient Boosted model to help us rank features based off the learning","1cbb921d":"#### Plot.ly Scatter Plot of feature importances\n\nHaving trained the Random Forest, we can obtain the list of feature importances by invoking the attribute \"featureimportances\" and plot our next Plotly plot, the Scatter plot.\n\nHere we invoke the command Scatter and as per the previous Plotly plots, we have to define our y and x-axes. However the one thing that we pay attention to in scatter plots is the marker attribute. It is the marker attribute where we define and hence control the size, color and scale of the scatter points embedded.","111b7b9a":" Here we can see that which columns contained -1 in their values so we could easily for example make a blanket replacement of all -1 with nulls first as follows:","5ed0319e":"#### Datatype check\n\nThis check is carried out to see what kind of datatypes the train set is comprised of : integers or characters or floats just to gain a better overview of the data we were provided with. \n\nOne trick to obtain counts of the unique types in a python sequence is to use the Counter method, when you import the Collections module as follows:","330fd2d6":"As we can see, the missing values now become much more apparent and clear when we visualise it, where the empty white bands (data that is missing) superposed on the vertical dark red bands (non-missing data) reflect the nullity of the data in that particular column. In this instance, we can observe that there are 7 features out of the 59 total features (although as rightly pointed out by Justin Nafe in the comments section there are really a grand total of 13 columns with missing values) that actually contained null values. This is due to the fact that the missingno matrix plot can only comfortable fit in approximately 40 odd features to one plot after which some columns may be excluded, and hence the remaining 5 null columns have been excluded. To visualize all nulls, try changing the figsize argument as well as tweaking how we slice the dataframe.\n\nFor the 7 null columns that we are able to observe, they are hence listed here as follows:\n\n**ps_ind_05_cat | ps_reg_03 | ps_car_03_cat | ps_car_05_cat | ps_car_07_cat | ps_car_09_cat | ps_car_14**\n\nMost of the missing values occur in the columns suffixed with _cat. One should really take further note of the columns ps_reg_03, ps_car_03_cat and ps_car_05_cat. Evinced from the ratio of white to dark bands, it is very apparent that a big majority of values are missing from these 3 columns, and therefore a blanket replacement of -1 for the nulls might not be a very good strategy.","ff14864a":"#### Decision Tree visualisation\n\nOne other interesting trick or technique oft used would be to visualize the tree branches or decisions made by the model. For simplicity, I fit a decision tree (of max_depth = 3) and hence you only see 3 levels in the decision branch, use the export to graph visualization attribute in sklearn \"export_graphviz\" and then export and import the tree image for visualization in this notebook.","8bf0d2d6":"#### Conclusion\n\nWe have performed quite an extensive inspection of the Porto Seguro dataset by inspecting for null values and data quality, investigated linear correlations between features, inspected some of the feature distributions as well as implemented a couple of learning models (Random forest and Gradient Boosting classifier) so as to identify features that the models deemed important.","0990856e":"#### Target variable inspection\n\nAnother standard check normally conducted on the data is with regards to our target variable, where in this case, the column is conveniently titled \"target\". The target value also comes by the moniker of class\/label\/correct answer and is used in supervised learning models along with the corresponding data that is given (in our case all our train data except the id column) to learn the function that best maps the data to our target in the hope that this learned function can generalize and predict well with new unseen data.","2f0bd4e7":"Interestingly we observe that in both Random forest and Gradient Boosted learning models, the most important feature that both models picked out was the column : ps_car_13.\n\nThis particular feature warrants further investigation so let us conduct a deep-dive into it.","9c068bd7":"Here we observe that there are 4 features : ps_ind_10_bin, ps_ind_11_bin, ps_ind_12_bin, ps_ind_13_bin which are completely dominated by zeros. This begs the question of whether these features are useful at all as they do not contain much information about the other class vis-a-vis the target.","6679c5ee":"### Correlation plots\n\nAs a starter, let us generate some linear correlation plots just to have a quick look at how a feature is linearly correlated to the next and perhaps start gaining some insights from here. At this juncture, I will use the seaborn statistical visualisation package to plot a heatmap of the correlation values. Conveniently, Pandas dataframes come with the corr() method inbuilt, which calculates the Pearson correlation. Also as convenient is Seaborn's way of invoking a correlation plot. Just literally the word \"heatmap\"","e9b53555":"#### Correlation of integer features\n\nFor the columns of interger datatype, I shall now switch to using the Plotly library to show how one can also generate a heatmap of correlation values interactively. Much like our earlier Plotly plot, we generate a heatmap object by simply invoking the \"go.Heatmap\". \n\nHere we have to provide values to three different axes, where x and y axes take in the column names while the correlation value is provided by the z-axis. The colorscale attribute takes in keywords that correspond to different color palettes that you will see in the heatmap where in this example, I have used the Greys colorscale (others include Portland and Viridis - try it for yourself).","26427659":"#### Binary features inspection\n\nAnother aspect of the data that we may want to inspect would be the columns that only contain binary values, i.e where values take on only either of the two values 1 or 0. Proceeding, we store all columns that contain these binary values and then generate a vertical plotly barplot of these binary values as follows:","6e1051c1":"#### Categorical and Ordinal feature inspection\n\nLet us first take a look at the features that are termed categorical as per their suffix \"_cat\".\n\n#### Feature importance via Random Forest\n\nLet us now implement a Random Forest model where we fit the training data with a Random Forest Classifier and look at the ranking of the features after the model has finished training. This is a quick way of using an ensemble model (ensemble of weak decision tree learners applied under Bootstrap aggregated) which does not require much parameter tuning in obtaining useful feature importances and is also pretty robust to target imbalances. We call the Random Forest as follows:","c58b9221":"Similarly, we can observe that there are a huge number of columns that are not linearly correlated with each other at all, evident from the fact that we observe quite a lot of 0 value cells in our correlation plot. This is quite a useful observation to us, especially if we are trying to perform dimensionality reduction transformations such as Principal Component Analysis (PCA), this would require a certain degree of correlation . We can note some features of interest are as follows:\n\n**Negatively correlated features :**\n\n* **ps_ind_06_bin, ps_ind_07_bin, ps_ind_08_bin, ps_ind_09_bin**\n\nOne interesting aspect to note is that in our earlier analysis on nullity, ps_car_03_cat and ps_car_05_cat were found to contain many missing or null values. Therefore it should come as no surprise that both these features show quite a strong positive linear correlation to each other on this basis, albeit one that may not really reflect the underlying truth for the data.","1140c807":"### 1. Data Quality checks\n#### Null or missing values\n\nAs part of our quality checks, let us quick look at whether there are any null values in the train dataset as follows:","e877c457":"### Mutual Information plots\nMutual information is another useful tool as it allows one to inspect the mutual information between the target variable and the corresponding feature it is calculated against. For classification problems, we can conveniently call Sklearn's mutual_info_classif method which measures the dependency between two random variables and ranges from zero (where the random variables are independent of each other) to higher values (indicate some dependency). This therefore will help give us an idea of how much information from the target may be contained within the features.\n\nThe sklearn implementation of the mutual_info_classif function tells us that it \"relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances\".","0bf8b6fa":"This kernel used from the Porto Seguro\u2019s Safe Driver Prediction and copied from the 'Data Preparation & Exploration' written by Anisotropic\n\nData Preparation & Exploration : [URL](https:\/\/www.kaggle.com\/arthurtok\/interactive-porto-insights-a-plot-ly-tutorial)\n\n*Thanks for sharing kernel, Anisotropic*"}}