{"cell_type":{"f290119a":"code","8e4275e1":"code","5303684a":"code","63956427":"code","30189031":"code","61a80835":"code","3c32e058":"code","bffbbe0a":"code","514cc343":"code","b56423c4":"code","648d77a1":"code","fcd0c1cf":"code","dddeb610":"code","aaf9dca1":"code","0e223cd1":"code","e58b3df9":"code","d8e4383f":"code","0dfafcc5":"code","f0fe3e7c":"markdown","c24dde9f":"markdown","94a66241":"markdown","f643ae48":"markdown","22b99e15":"markdown","379aaf76":"markdown","ecb73125":"markdown","1e174302":"markdown","5dd2c35a":"markdown","bf0800d5":"markdown","6410f19b":"markdown","28e263f3":"markdown","631101cb":"markdown","8647f2da":"markdown","fb23cd78":"markdown","db223ef7":"markdown","1f31d04c":"markdown","b6e1e2af":"markdown"},"source":{"f290119a":"'''install packages'''\n!pip install cnocr\n!pip install cnstd","8e4275e1":"import os,jieba,pickle\nimport numpy as np\nimport pandas as pd\nfrom os.path import join as joinp\n\n'''set path'''\n\nmod_path = '..\/input\/w2v-cn\/sgns.sogounews.bigram-char'\nsuicide_path = '..\/input\/tempstrenthen\/.xlsx'\ns_path = '..\/input\/sucide-text\/.xlsx'\nstopword_path = '..\/input\/stopwordch\/stopwordCh.txt'\npath_dict = ['anger.txt','neg.txt','pos.txt']\npath_dict=['..\/input\/emotion-word\/'+p for p in path_dict]\nother_txt_path='..\/input\/coversationzhihu\/zhihu.txt'\npic_path='..\/input\/pic-ocr-test\/pic.jpeg'","5303684a":"'''load stopword'''\nwith open(stopword_path,'r',encoding='utf8') as f:\n    stopword = [w.strip() for w in f.readlines()]\n    stopword.append(' ')","63956427":"'''recognize chinese character'''\nfrom cnstd import CnStd\nfrom cnocr import CnOcr\n\nstd = CnStd()\ncn_ocr = CnOcr(cand_alphabet=None)\n\nbox_info_list = std.detect(pic_path)\nTEXT=[]\nfor box_info in box_info_list:\n    cropped_img = box_info['cropped_img']  # \u68c0\u6d4b\u51fa\u7684\u6587\u672c\u6846\n    ocr_res = cn_ocr.ocr_for_single_line(cropped_img)\n    print('ocr result: %s' % ''.join(ocr_res))\n    TEXT.append(''.join(ocr_res))\nTEXT=''.join(TEXT)","30189031":"'''add vocabularies'''\ndef load_dict(path):\n    jieba.load_userdict(path)\n    with open(path,'r',encoding='utf8') as f:\n        word = f.readlines()\n        return [w.strip() for w in word]\n    \nword_lst_add = []\nfor path in path_dict:\n    word_lst_add.extend(load_dict(path))\nword_lst_add = list(set(word_lst_add))","61a80835":"'''enhancement'''\n'''load txt suicide'''\nimport xlrd\ndef load_text_cut(path):\n    import xlrd\n    txt_add=xlrd.open_workbook(path)\n    sheet=txt_add.sheet_by_index(0)\n    s=sheet.col_values(0)\n    txt,w = [],[]\n    for s_ in s:\n        txt.append([w for w in s_.split(' ') if w not in stopword])\n    for k in txt:\n        w.extend(k)\n    return txt,list(set(w))\n   \ntxt_c,word_lst_scd = load_text_cut(suicide_path)","3c32e058":"'''get zhihu context with string len more than 5 ans escape tag'''\nimport re\ndef get_other_txt(num):\n    others = []\n    f = open(other_txt_path,'r')\n    lines = f.readlines()\n    for s in lines:\n        s = re.sub('<(.*)>','',s).strip()\n        pres = re.search(r\"(?:_content\\\" \\: \\\")(.*)(?:\\\")\",s)\n        if pres != None:\n            if len(pres.group(1))>20:\n                lst = jieba.cut(pres.group(1))\n                others.append([w for w in list(lst) if w not in stopword])\n            if len(others)>num*0.7:\n                break\n    word_lst_other = []\n    for s in others:\n        word_lst_other.extend([w for w in s if w not in stopword])\n    word_lst_other = list(set(word_lst_other))\n    return others,word_lst_other\n\ntxt_c_other,word_lst_other = get_other_txt(len(txt_c))","bffbbe0a":"'''get word dictionary'''\nword_lst = list(set(word_lst_scd+word_lst_other+word_lst_add))\nd_w2i = {w:i for i,w in enumerate(word_lst)}","514cc343":"'''get digit data and organize'''\nfrom keras.preprocessing.sequence import pad_sequences\n\nLEN = 2000\ndef tokenizer(d_w2i,txt_c,len=LEN):\n    '''given dictionary word to digt , txt cut and pad LEN. return digt tok '''\n    tok = []\n    for i,s in enumerate(txt_c):\n        tok.append([])\n        for w in s:\n            try:\n                if d_w2i[w] != None:\n                    tok[i].append(d_w2i[w])\n                else:\n                    tok[i].append(0)\n            except:\n                tok[i].append(0)\n    tok = pad_sequences(tok,LEN)\n    return tok\n\ntok_pos = tokenizer(d_w2i,txt_c)\ntok_neg = tokenizer(d_w2i,txt_c_other)\n\ntok = np.vstack((tok_pos,tok_neg))\ntag = np.vstack(( np.ones((tok_pos.shape[0],1),dtype = 'int'),\n                 np.zeros((tok_neg.shape[0],1),dtype = 'int')\n               )) \nind = np.array(range(len(tok)))\nsample = list(zip(tok,tag,ind))","b56423c4":"'''shuffle the samples'''\nfrom keras.utils import to_categorical\nfrom random import shuffle as sf\nsf(sample)\ntok,tag,ind = tuple(zip(*sample))\n# tok,tag = np.array(tok),to_categorical(np.array(tag))\ntok,tag = np.array(tok),to_categorical(np.array(tag))","648d77a1":"'''load word embeding model'''\nfrom keras.preprocessing.sequence import pad_sequences\nimport codecs\ndef Load_model_emb(data_dict,embMod_path):\n    '''\u8f93\u5165\u8bcd\u5178,\u6a21\u578b\u8def\u5f84\u3002\u8fd4\u56de\u9700\u8981\u7684\u53ef\u4f9bkeras.layers.Embedding\u4f7f\u7528\u7684\u8bcd\u5411\u91cf\u7684\u77e9\u9635'''\n    with codecs.open(embMod_path,'r','utf-8') as f:\n        emb_mat = np.zeros((len(data_dict) + 1, 300))\n        size =None\n        num = 0 #\u8bb0\u5f55\u547d\u4e2d\u8bcd\u6570\u91cf\n        for line in f:\n            line = line.strip().split(' ') ## \u6709\u6bd2\n            if not size:\n                size=line\n                continue\n            if line[0] in data_dict.keys():\n                num+=1\n                emb_mat[data_dict[line[0]]] = np.asarray(line[1:], dtype='float32')\n    return emb_mat,len(data_dict)-num\nEmb_mod,oov = Load_model_emb(d_w2i,mod_path)\noov","fcd0c1cf":"'''save data'''\nimport pickle\nnp.save('tok',np.array(tok))\nnp.save('tag',np.array(tag))\nnp.save('emb',Emb_mod)\n","dddeb610":"'''build model'''\nfrom keras.models import Sequential,save_model,load_model\nfrom keras.layers import *\ndef build_mod(tok,tag,emb_mat,drop_rate=0.3):\n    model = Sequential()\n    # embedding with unknown method\n\n    #Embedding \u5982\u679c\u4e0d\u4f1a\u53ef\u4ee5\u76f4\u63a5\u5728\u8fd9\u4e2a\u6a21\u578b\u4e2d\u8f93\u5165\u8bcd\u5411\u91cf\u8bd5\u8bd5\n    model.add(Embedding(emb_mat.shape[0],emb_mat.shape[1],weights=[emb_mat],input_length=tok.shape[1],trainable = False,))\n    # \u968f\u673a\u5220\u6389\u4e00\u90e8\u5206\n    model.add(Dropout(drop_rate))\n\n    # GRU \u6a21\u578b\n    model.add(GRU(2**5))\n    model.add(Dropout(drop_rate))\n    # \u5168\u8fde\u63a5\n    model.add(Dense(2**6,activation='relu'))\n    model.add(Dropout(drop_rate))\n    \n    #\u6700\u540e\u4e00\u5c42softmax\u5206\u7c7b\n    model.add(Dense(tag.shape[1],activation = 'softmax'))\n\n    # \u6dfb\u52a0\u635f\u5931\u51fd\u6570\u4e3a\u5206\u7c7b\u7684\u4ea4\u53c9\u71b5\uff0c\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\n    model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n    return model\nmod = build_mod(np.array(tok),np.array(tag),Emb_mod)\nmod.summary()","aaf9dca1":"'''split data and train'''\ndef sep(tok,tag,k):\n    tok_train = tok[:-k]\n    tag_train = tag[:-k]\n    tok_test = tok[-k:]\n    tag_test = tag[-k:] \n    return (tok_train,tag_train),(tok_test,tag_test)\n(tok_train,tag_train),(tok_test,tag_test) = sep(tok,tag,100)\nmod.fit(x=tok_train,y=tag_train, validation_split=0.2,batch_size=2**4,epochs=10,workers=4)","0e223cd1":"'''test set'''\n# mod.predict(np.array([tok_train[1]]))\nmod.evaluate(tok_test,tag_test)","e58b3df9":"'''try OCR text'''\ntxt = TEXT\nif type(txt)==str:\n    txt = [txt]\nelif type(txt)!=list:\n    print('input txt must str or list')\ntok=[]\nfor i,t in enumerate(txt):\n    tok.append([])\n    txt_cut = [w for w in list(jieba.cut(t)) if w not in stopword]\n    for w in txt_cut:\n        if w in d_w2i.keys():\n            tok[i].append(d_w2i[w])\n        else:\n            tok[i].append(0)\nsequences = pad_sequences(np.array(tok),LEN)\nmod.predict(np.array(sequences))","d8e4383f":"'''save'''\nsave_model(mod,'category_scd-mod.h5')\npickle.dump(d_w2i,open('category_scd-d_w2i','wb'))","0dfafcc5":"import jieba\nimport numpy as np\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\n\nmod = load_model('.\/category_scd-mod.h5')\nd_w2i = pickle.load(open('.\/category_scd-d_w2i','rb'))\n\n# txt = input('please enter a sentence:\\n\\n')\ntxt = ['\u901a\u8fc7\u5bf9\u8bcd\u9891\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u53ef\u4ee5\u66f4\u8be6\u7ec6\u5730\u533a\u5206\u77ed\u671f\u3001\u957f\u671f\u4e0e\u5468\u671f\u6027\u70ed\u70b9\uff1b\u5bf9\u4e00\u4e9b\u66f4\u6709\u4ef7\u503c\u7684\u70ed\u8bcd\u505a\u70ed\u5ea6\u9884\u8b66\uff1b\u5bf9\u70ed\u8bcd\u7684\u589e\u957f\u8d8b\u52bf\u8fdb\u884c\u5206\u6790\u7b49\u7b49\u3002',\n      '\u5927\u6d77\u90a3\u4e48\u8fd1\uff0c\u4e4b\u524d\u4e5f\u6709\u51e0\u4eba\u505a\u4e86\uff0c\u4e3a\u4ec0\u4e48\u81ea\u5df1\u4e0d\u53bb\u505a\uff0c\u8fd8\u5728\u6d3b\u7740\u554a\uff0c\u80fd\u4e0d\u80fd\u6709\u4e2a\u610f\u5916\u8ba9\u6211\u79bb\u4e16\uff0c\u7b97\u662f\u6211\u8fd9\u4e00\u751f\u6536\u5230\u7684\u6700\u5927\u7684\u793c\u7269']\nif type(txt)==str:\n    txt = [txt]\nelif type(txt)!=list:\n    print('input txt must str or list:\\n\\n')\ntok=[]\nfor i,t in enumerate(txt):\n    tok.append([])\n    txt_cut = [w for w in list(jieba.cut(t)) if w not in stopword]\n    for w in txt_cut:\n        if w in d_w2i.keys():\n            tok[i].append(d_w2i[w])\n        else:\n            tok[i].append(0)\nsequences = pad_sequences(np.array(tok),LEN)\nres=mod.predict(np.array(sequences))\nfor k in range(len(res)):\n    print(\"+----------------------+\"+\n          \"\\n|\u6b63\u5e38\/\u81ea\u6740\uff1a{:.4f}\/{:.4f}|\\n\".format(*np.round(res,decimals=3)[k])+\n          \"+----------------------+\")\n","f0fe3e7c":"# Load Suicide Text\nLoad about 300 texts, then by the means of `Data enhancement`,we get about 3000(replace, exchange, delete some word in sentences as new ones)","c24dde9f":"# Test\nWe use test-data to test the generalization ability of the model. However, due to the Data enhancement process. the test may inreliable","94a66241":"# Add More Emotion Word\nanger,negative,positive words added to dictionary","f643ae48":"# Save Classfer & Vocabulary","22b99e15":"# RNN Model\nTo creat a classifier,we can organize a RNN model. Details are in code block below","379aaf76":"# Load Non-suicide Text\nwe use comments from Zhihu a huge community in China. And the topic of comments is about Employment.","ecb73125":"# OCR Recognize\nNot really cool with recognization","1e174302":"# Build Dictionary\ndictionary","5dd2c35a":"# Build Tokenize Matrix\ngive every word an index and then pad it we get a `tokenize matrix`.","bf0800d5":"# Save Temporary Data","6410f19b":"# Shuffle\nAfter Tokenizing we can shuffle it by row to make result reliable.","28e263f3":"# Try on OCR Text\nIt seems cool on classfy OCR text, although OCR text recognized is really uncool","631101cb":"# Get OCR Packages\n`cnocr`:recognize Chinese characters in image\n`cnstd`:get position of characters (assist cnocr to be more precise)","8647f2da":"# Load Word2vector Embeding Model\nLoad an embeding model from [Github](https:\/\/github.com\/Embedding\/Chinese-Word-Vectors),with 300 dimentions trained from sougou","fb23cd78":"# Split Data & Training\nDivide Data into two parts, train-data and test-data. We use train-data to Train model.","db223ef7":"# Load Stopword","1f31d04c":"# Use Model Independently\nApply Model into more text.","b6e1e2af":"# Set Path\nSet path of word2vector model, Zhihu comments, suicide text, emotion words, stopword and test image."}}