{"cell_type":{"55d3d52d":"code","c18e71a6":"code","001248f3":"code","d3be4ad0":"code","a91c7230":"code","c9319b37":"code","8364b6a6":"code","aada9116":"code","b9856333":"code","eff5bd23":"code","7693e89a":"code","fb8ace24":"code","7ee4ab2a":"code","024ae82d":"code","7a9c0d39":"code","bfa562dd":"code","8fa5d9ab":"code","3b642fa1":"code","1f57d163":"code","496e56b2":"code","7b8c034e":"code","d424cb49":"code","cea4b902":"code","95f6ff55":"code","b9e6edb2":"code","e71fe62b":"code","0c009ae7":"code","47f68cea":"code","3fa911e2":"code","c5c21609":"code","77727b13":"code","49ac326c":"code","b42cfca7":"code","fcf47bfd":"code","b7175908":"code","8f8b930b":"code","a5baea44":"code","d49a384a":"code","bef5e377":"code","47f362d3":"code","214d71d3":"code","2784b2f9":"code","227a7d6a":"code","e777ba3f":"code","5fa2ac36":"code","22598fe8":"code","b756a8e5":"code","e9a8bb81":"code","d673750e":"code","39aa3e6f":"code","72ebefeb":"code","1cae4e98":"code","527c89e9":"code","40ecdd52":"code","f439b49e":"code","e9363672":"code","c0145e8b":"code","e9e7e60a":"code","4dd451e5":"code","60ae3643":"code","b7404c81":"code","3d17b012":"code","269a45a5":"code","2186a17c":"code","f50d9d66":"code","6bd1bccb":"code","bb281651":"code","bc6d8975":"code","bdf3475d":"code","9f9a17aa":"code","7fcc1ecb":"code","7f693753":"markdown","491ba125":"markdown","297683bd":"markdown","232a81bc":"markdown","d694e8e1":"markdown","c5f8d36b":"markdown","20e1e7e5":"markdown","09414851":"markdown","cb0d6acf":"markdown","81abf7d9":"markdown","f07bc970":"markdown","0a7c0b36":"markdown","c486bb10":"markdown","166083d5":"markdown","248dfb98":"markdown","295fc1a3":"markdown","61a06747":"markdown","4fc5fa96":"markdown","2a618507":"markdown","3a2226e5":"markdown","9b6b753a":"markdown","5bc60d1c":"markdown","c5d118cb":"markdown","de675a84":"markdown","8fed3427":"markdown","6ec78dfe":"markdown","fc40b2d0":"markdown","c771c787":"markdown","e692f77a":"markdown","c5fcf1ac":"markdown","2bfac2ed":"markdown","f071c4aa":"markdown","0a8eb7b9":"markdown","e4b0825a":"markdown"},"source":{"55d3d52d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt # Not used as of now","c18e71a6":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","001248f3":"train_df.head()","d3be4ad0":"test_df.head()","a91c7230":"train_df.info()","c9319b37":"test_df.info()","8364b6a6":"corr = train_df.corr()\ncorr","aada9116":"# Impute, remove, encode, scale\n# (Only scale for ANN (for now))","b9856333":"# Age, cabin, & embarked are incomplete (in the training set)\n# Age, cabin, & fare are incomplete (in the test set)\n\n# age and fare can be imputed using the mean\n# embarked - most common (only missing 2)\n# cabin can probably be safely dropped due to how many are missing","eff5bd23":"train_labels = train_df['Survived'] # get separate labels array\nX_train = train_df.drop(['Survived'], axis=1) # drop labels out of df","7693e89a":"X_train # Missing 'Survived' now, good","fb8ace24":"# Import\nfrom sklearn.impute import SimpleImputer","7ee4ab2a":"# Imputer for age and fare\nmean_imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# Imputer for embarked\ncommon_imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent') # decent guess as any","024ae82d":"mean_imputer.fit(X_train[['Age', 'Fare']])\ncommon_imputer.fit(X_train[['Embarked']])","7a9c0d39":"X_train[['Age', 'Fare']] = mean_imputer.transform(X_train[['Age', 'Fare']]);\ntest_df[['Age', 'Fare']] = mean_imputer.transform(test_df[['Age', 'Fare']]);\n\nX_train[['Embarked']] = common_imputer.transform(X_train[['Embarked']]);\ntest_df[['Embarked']] = common_imputer.transform(test_df[['Embarked']]);","bfa562dd":"X_train.info()","8fa5d9ab":"col_to_drop = ['Name', 'Ticket', 'Cabin']","3b642fa1":"X_train = X_train.drop(col_to_drop, axis=1)\ntest_df = test_df.drop(col_to_drop, axis=1)","1f57d163":"X_train.info() #; test_df.info() # easier to comment out","496e56b2":"X_train.head()","7b8c034e":"# Need to encode sex & embarked\n# Drop Id","d424cb49":"X_train['AgeRange'] = ['child' if x < 13 else 'teenager' if x < 22 \n                       else 'adult' if x < 66 else 'senior' for x in X_train['Age']]\ntest_df['AgeRange'] = ['child' if x < 13 else 'teenager' if x < 22 \n                       else 'adult' if x < 66 else 'senior' for x in test_df['Age']]","cea4b902":"X_train.head()","95f6ff55":"# Make a new column that contains the # of family members aboard\nX_train['FamNum'] = X_train['SibSp'] + X_train['Parch']\ntest_df['FamNum'] = test_df['SibSp'] + test_df['Parch']\n\n# Remove SibSp and Parch as they were just used\n# Remove Age as we have AgeRange now\n# Remove PassengerId as it doesn't really tell us anything\nX_train = X_train.drop(['SibSp', 'Parch', 'PassengerId', 'Age'], axis=1)\ntest_df = test_df.drop(['SibSp', 'Parch', 'PassengerId', 'Age'], axis=1)","b9e6edb2":"# A quick idea of what is strongly correlated with Survived\npd.concat([X_train, train_labels], axis=1).corr()['Survived']","e71fe62b":"# Nothing super crazy, but Pclass is nice to see","0c009ae7":"need_encoding = ['Sex', 'Embarked', 'AgeRange'] # Marked for later removal","47f68cea":"# Manual one-hot encoding on train set using list comprehensions\n# Gender\nX_train['Male'] = [1 if x == 'male' else 0 for x in X_train['Sex']]\nX_train['Female'] = [1 if x == 'female' else 0 for x in X_train['Sex']]\n\n# Embarked\nX_train['S'] = [1 if x == 'S' else 0 for x in X_train['Embarked']]\nX_train['C'] = [1 if x == 'C' else 0 for x in X_train['Embarked']]\nX_train['Q'] = [1 if x == 'Q' else 0 for x in X_train['Embarked']]\n\n# AgeRange\nX_train['child'] = [1 if x == 'child' else 0 for x in X_train['AgeRange']]\nX_train['teenager'] = [1 if x == 'teenager' else 0 for x in X_train['AgeRange']]\nX_train['adult'] = [1 if x == 'adult' else 0 for x in X_train['AgeRange']]\nX_train['senior'] = [1 if x == 'senior' else 0 for x in X_train['AgeRange']]","3fa911e2":"# Manual one-hot encoding on test set using list comprehensions\n# Gender\ntest_df['Male'] = [1 if x == 'male' else 0 for x in test_df['Sex']]\ntest_df['Female'] = [1 if x == 'female' else 0 for x in test_df['Sex']]\n\n# Embarked\ntest_df['S'] = [1 if x == 'S' else 0 for x in test_df['Embarked']]\ntest_df['C'] = [1 if x == 'C' else 0 for x in test_df['Embarked']]\ntest_df['Q'] = [1 if x == 'Q' else 0 for x in test_df['Embarked']]\n\n# AgeRange\ntest_df['child'] = [1 if x == 'child' else 0 for x in test_df['AgeRange']]\ntest_df['teenager'] = [1 if x == 'teenager' else 0 for x in test_df['AgeRange']]\ntest_df['adult'] = [1 if x == 'adult' else 0 for x in test_df['AgeRange']]\ntest_df['senior'] = [1 if x == 'senior' else 0 for x in test_df['AgeRange']]","c5c21609":"# Can safely drop the original columns now out of both\nX_train = X_train.drop(need_encoding, axis=1)\ntest_df = test_df.drop(need_encoding, axis=1)","77727b13":"# Let's see what it looks like now\nX_train.head()","49ac326c":"from sklearn.ensemble import RandomForestClassifier\n\nrf_classifier = RandomForestClassifier()","b42cfca7":"# Grid Search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_estimators': [100], \n               'criterion': ['gini'],\n               'min_samples_split': [2, 3],\n               'min_samples_leaf': [1, 2],\n               'max_features': ['auto', None],\n               'max_depth': [None, 8, 10, 12]\n              }]\n# Get it ready\ngrid_search = GridSearchCV(rf_classifier, param_grid, cv=7, verbose=1)","fcf47bfd":"grid_search.fit(X_train, train_labels) # Train it (shouldn't take too long)","b7175908":"grid_search.best_params_ \n# Usually {gini, 8, None, 2, 2, 100}","8f8b930b":"grid_search.best_score_ \n# ~ 83.28%","a5baea44":"# Save the best random forest\nrf_classifier = grid_search.best_estimator_","d49a384a":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier()","bef5e377":"# Grid Search\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [{'n_neighbors': [2, 3, 4], \n               'algorithm': ['ball_tree', 'kd_tree'],\n               'leaf_size': [10, 20, 30]\n              }]\n# Get it ready\ngrid_search = GridSearchCV(knn_classifier, param_grid, cv=7, verbose=1)","47f362d3":"grid_search.fit(X_train, train_labels) # Fit it","214d71d3":"grid_search.best_params_ \n# ball_tree, 10, 3 last time","2784b2f9":"grid_search.best_score_ \n# ~ 80.36%","227a7d6a":"# Save the best K-NN\nknn_classifier = grid_search.best_estimator_","e777ba3f":"# This one is a bit messier","5fa2ac36":"# Quick function that gets how many out of 'preds' match 'labels'\ndef get_num_correct(preds, labels):\n    return preds.argmax(dim=1).eq(labels).sum().item()","22598fe8":"# Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset","b756a8e5":"# Scale the data for the ANN\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","e9a8bb81":"# Scale the data\nX_train_scaled = scaler.fit_transform(X_train)\ntest_ann = scaler.transform(test_df) # To be used later","d673750e":"# Want a train and val set now\nfrom sklearn.model_selection import train_test_split\n\n# Split the train set into train & val \n# Could try again w\/o splitting the train set w\/ a set NN architecture\nX_train_part, X_val, y_train, y_val = train_test_split(X_train_scaled, train_labels, test_size=0.10)","39aa3e6f":"X_train_part.shape","72ebefeb":"# Our Artificial Neural Network class\n# Could play around with this a lot more\nclass ANN(nn.Module):\n    def __init__(self):\n        super(ANN, self).__init__()\n      \n        self.fc1 = nn.Linear(in_features=12, out_features=32) # linear 1\n        self.fc1_bn = nn.BatchNorm1d(num_features=32)\n        self.drop1 = nn.Dropout(.1)\n        \n        self.fc2 = nn.Linear(in_features=32, out_features=8) # linear 2\n        self.fc2_bn = nn.BatchNorm1d(num_features=8)\n        \n        self.out = nn.Linear(in_features=8, out_features=2) # output\n    \n    def forward(self, t):\n        t = F.relu(self.fc1_bn(self.fc1(t)))\n        t = self.drop1(t)\n        t = F.relu(self.fc2_bn(self.fc2(t)))\n        \n        t = self.out(t)\n        return t","1cae4e98":"# Gets the GPU if there is one, otherwise the cpu\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","527c89e9":"batch_size = 64 # got a weird error if there was a different batch size\n\n# Some weird TensorDataset stuff\n# It wants tensors of type float, but they were numpy arrays of not all float\ntrain_set = TensorDataset(torch.from_numpy(X_train_part.astype(float)), \n                          torch.from_numpy(y_train.as_matrix().astype(float)))\nval_set = TensorDataset(torch.from_numpy(X_val.astype(float)), \n                        torch.from_numpy(y_val.as_matrix().astype(float)))\n\n# Load up the data and shuffle away\ntrain_dl = DataLoader(train_set, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(val_set, batch_size=batch_size, shuffle=True)","40ecdd52":"lr = 0.001 # initial learning rate\nepochs = 200 # number of epochs to run\n\nnetwork = ANN().float().to(device) # put the model on device (hopefully a GPU!)\noptimizer = optim.Adam(network.parameters(), lr=lr) # Could try a different optimizer\n\n# It wanted the X to be float and the y to be long, so I complied\nfor epoch in range(epochs):\n    network.train() # training mode\n    \n    # decrease the learning rate a bit\n    if epoch == 40:\n        optimizer = optim.Adam(network.parameters(), lr=0.0001)\n    \n    # decrease the learning rate a bit more\n    if epoch == 80:\n        optimizer = optim.Adam(network.parameters(), lr=0.00000000001)\n        \n    for features, labels in train_dl:\n        X, y = features.to(device), labels.to(device) # put X & y on device\n        y_ = network(X.float()) # get predictions\n        \n        optimizer.zero_grad() # zeros out the gradients\n        loss = F.cross_entropy(y_, y.long()) # computes the loss\n        loss.backward() # computes the gradients\n        optimizer.step() # updates weights\n          \n    # Evaluation with the validation set\n    network.eval() # eval mode\n    val_loss = 0\n    val_correct = 0\n    with torch.no_grad():\n        for features, labels in val_dl:\n            X, y = features.to(device), labels.to(device) # to device\n            \n            preds = network(X.float()) # get predictions\n            loss = F.cross_entropy(preds, y.long()) # calculate the loss\n            \n            val_correct += get_num_correct(preds, y.long())\n            val_loss = loss.item() * batch_size\n            \n    # Print the loss and accuracy for the validation set\n    if (epoch % 10) == 9: # prints every 10th epoch\n        print(\"Epoch: \", epoch+1)\n        print(\" Val Loss: \", val_loss)\n        print(\" Val Acc: \", (val_correct\/len(X_val))*100)\n\n# Output can get a bit long","f439b49e":"# Helps a bit to reduce dimensions for linear regression (or so I remember)\nX_lin = X_train.drop(['S', 'C', 'Q', 'teenager', 'adult'], axis=1)\ntest_lin = test_df.drop(['S', 'C', 'Q', 'teenager', 'adult'], axis=1) # need to use later","e9363672":"from sklearn.linear_model import LinearRegression\n\nlinearreg = LinearRegression(normalize=True, copy_X=True)","c0145e8b":"linearreg.fit(X_lin, train_labels)","e9e7e60a":"# rf_classifier, network, knn_classifier, linearreg\n# test_df = test set","4dd451e5":"# Get the test set ready for the ANN because it's special\ntest = TensorDataset(torch.from_numpy(test_ann.astype(float)) )\ntest_tensor = DataLoader(test, batch_size=batch_size, shuffle=False)","60ae3643":"# Get predictions\n\n# Predictions for the Random Forest ---------------\nrf_preds = rf_classifier.predict(test_df)\n\n# Predictions for the ANN -------------------------\nann_preds = torch.LongTensor().to(device) # Tensor for all predictions\nnetwork.eval() # safety\nfor batch in test_tensor:\n    batch = batch[0].to(device) # just batch is a [tensor] for some reason\n    predictions = network(batch.float()) # again with the float thing\n    ann_preds = torch.cat((ann_preds, predictions.argmax(dim=1)), dim=0) \n# bring it back to the cpu and convert to an array\nann_preds = ann_preds.cpu().numpy()\n\n# Predictions for the K-Nearest Neighbors ---------\nknn_preds = knn_classifier.predict(test_df)\n\n# Predictions for the Linear Regression -----------\nlin_preds = linearreg.predict(test_lin) # special test set with less columns","b7404c81":"lin_preds[:5] # Not quite how we want them","3d17b012":"lin_preds = np.around(lin_preds, decimals=0).astype(int) # Rounds them","269a45a5":"# Interesting to see\nprint(np.sum(rf_preds==ann_preds), \"\/\", rf_preds.shape[0], \" same predictions between Random Forest and ANN\")\nprint(np.sum(rf_preds==knn_preds), \"\/\", rf_preds.shape[0], \" same predictions between Random Forest and K-NN\")\nprint(np.sum(rf_preds==lin_preds), \"\/\", rf_preds.shape[0], \" same predictions between Random Forest and Linear Reg\")\nprint(np.sum(ann_preds==knn_preds), \"\/\", rf_preds.shape[0], \" same predictions between ANN and K-NN\")\nprint(np.sum(ann_preds==lin_preds), \"\/\", rf_preds.shape[0], \" same predictions between ANN and Linear Reg\")\nprint(np.sum(knn_preds==lin_preds), \"\/\", rf_preds.shape[0], \" same predictions between K-NN and Linear Reg\")","2186a17c":"# Add them all up\nagg_preds = rf_preds + ann_preds + lin_preds + knn_preds\nagg_preds # values 0-4 now","f50d9d66":"values, counts = np.unique(agg_preds, return_counts=True) # sum number of 0s..., 4s\nfor i in range(5):\n    print(values[i], \" classifiers predicted 'Survive'\", counts[i], \" times\", )","6bd1bccb":"# Time to get the final predictions\nfinal_preds = np.empty(len(agg_preds), dtype=int) # empty predictions array\n\n# Survived if agg_preds has 4 or 3\n# Didn't survive if agg_preds has 0 or 1\n# Up to the Random Forest if agg_preds is split at 2\nfor i in range(len(agg_preds)): # go through agg_preds\n    if agg_preds[i] < 2:\n        final_preds[i] = 0\n    elif agg_preds[i] > 2:\n        final_preds[i] = 1\n    else: # final call goes to random forest\n        final_preds[i] = rf_preds[i]","bb281651":"final_preds # Beautiful!","bc6d8975":"# Read in sample csv\nsample_df = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","bdf3475d":"# Edit it\nsample_df['Survived'] = final_preds","9f9a17aa":"# Write to a new csv\nsample_df.to_csv(\"predictions.csv\", index=False) # Be sure to not include the index","7fcc1ecb":"# 79.425 on test set (Kaggle) (2164th place when submitted)\n# Top 20%","7f693753":"# - Remove Columns","491ba125":"# A Few Final Remarks","297683bd":" # - Feature Engineering","232a81bc":"# ANN Classifier","d694e8e1":"Uses: scikit-learn, pandas, NumPy, PyTorch","c5f8d36b":"A nice and quick Linear Regression model, just using a few less columns ","20e1e7e5":"# Linear Regression","09414851":"Looking at the 'Survived' column, Pclass and Fare have the strongest correlations (negatively and positively, respectively). That doesn't mean everything else won't be useful, though. ","cb0d6acf":"Looking at the data initially, it looks like there's a few columns that I won't try to mess with. First is the name (which would be a place to come back to and grab the titles from it). There's also the passenger id which won't have any real correlation, with the ticket probably being in a similar position. Then there's the cabin, which is just missing from so many entries. Everything else we should be able to use, somehow. ","81abf7d9":"Looks good, no missing data points now. Still a few objects to deal with, though. ","f07bc970":"Definitely the hardest one here to implement, but it is quite a nice classifier. ","0a7c0b36":"# Ensemble","c486bb10":"# 3 - Train the Models","166083d5":"The first classifier up is a Random Forest. It'll go through a grid search to find a good set of hyperparameters and then we'll grab that best estimator for later use. ","248dfb98":"# - Imputing","295fc1a3":"- This served as a nice reminder that ANNs perform better when inputs are scaled\n- The K-NN did much better after the categorical variables were one-hot encoded\n- Ensemble learning is quite nice, even for a small task like this (and the things I had to do in pandas for this were a nice refresher from way back\/ learning experience)\n- List comprehensions in Python work quite nicely for quick, manual one-hot encoding\n- Also PyTorch can be really picky about a few things, like data type or batch size","61a06747":"Instead of using scikit-learn's LabelEncoder and OneHotEncoder classes, I'm going to use list comprehensions to encode the three columns mentioned in the next cell. It's a nice reminder that list comprehensions are a thing in Python and they are quite handy (even if doing them for all of that takes up more lines than using sci-kit-learn). ","4fc5fa96":"These results got 79.425% for a top 20% placement. Not too bad for what was done here. ","2a618507":"I left it up to the Random Forest classifier because it was one of the strongest classifiers when I tested them individually. Since then, the ANN might have closed the gap, but I imagine if anything they'd be able the same. I think the best way to settle this would probably be to introduce a fifth classifier. ","3a2226e5":"# Random Forest Classifier","9b6b753a":"Not the prettiest list comprehensions ever, but they get the job done. Tweaking the ages and number of categories here could potentially help performance a little bit. ","5bc60d1c":"Quick Note: I did have more options for some of those parameters, but I removed them afterwards so it wouldn't take so long to train. Rest assured I removed the ones that never got picked. ","c5d118cb":"Time to use two SimpleImputers from skklearn.impute to fill some missing data. ","de675a84":"I'm sure you're all aware of the Titanic (especially since you're here), so I'll spare you much of an introduction in that regards. Anyways, this notebook uses ensemble learning with a random forest, k-nearest neighbors, linear regression, and an artificial neural network. The first three are from scikit-learn while the ANN is from PyTorch. Some other things that are used are a grid search, simple imputers, and some manual one-hot encoding (mentioned for people quickly looking for an example of them). \n\nThis notebook got 79.425% for a top 20% placement here on Kaggle. \n\nIf you're looking to learn something from this then I hope you do! If you have any questions or comments, I'd love to hear them. ","8fed3427":"After the Random Forest, it's basically the same thing except with a different classifier and different parameters for the grid search. ","6ec78dfe":"# - Encode","fc40b2d0":"Thanks again for reading and I hope you learned something! ","c771c787":"# 2 - Data Preprocessing","e692f77a":"Training set: X_train && train_labels","c5fcf1ac":"Some possible future notebook improvements...\n- More visuals\n- Try and add more useful classifiers\n- Grab some info from the name \/ some more feature engineering\n- Mess with the ANN more","2bfac2ed":"# Write To CSV","f071c4aa":"# K-NN","0a8eb7b9":"# Some Quick Things I Learned","e4b0825a":"# 1 - Explore (A Little)"}}