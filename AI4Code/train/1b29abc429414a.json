{"cell_type":{"345ab63a":"code","12469d21":"code","7e6cccf7":"code","10a5584f":"code","5f649f6d":"code","3a62a886":"code","1b104f7a":"code","4cc703bc":"code","94bba8ba":"code","e4c3429f":"code","b47639c0":"code","0df3a4b9":"code","1d4fd1d4":"code","2f258494":"code","229f0d44":"code","10750c39":"code","6f03b0ac":"code","2bebb13d":"code","e851b799":"code","8eda0d67":"code","563f2456":"code","a89abf51":"code","991b6ca8":"code","caeeb908":"code","78c25bbf":"code","f413f006":"code","b1d04311":"code","0ac01df7":"code","78394519":"code","131f854f":"code","111af07e":"code","31dc5038":"code","19a6d090":"code","8b7ba2bf":"code","26072e40":"code","f70da622":"code","791cc039":"code","d5d3c857":"code","0e23b3f2":"code","45e609f9":"code","c978d260":"code","d914df21":"code","97a3c501":"code","26fcfcec":"code","110eac1d":"code","d4b018ce":"code","aff8e336":"code","e4d8497d":"code","0501b4f7":"code","fd175882":"code","a485b8c7":"code","ccf64511":"code","fac26796":"code","fc1e18ca":"code","fcf2fc4e":"code","a9786e73":"code","5e0e79be":"code","283cf8b2":"code","6e9990e3":"code","054c3c11":"code","78e362b3":"code","7d886ea9":"code","cad58eba":"code","625e4e06":"code","8212a9d9":"code","d36c2164":"code","22f5e829":"code","7f717eb0":"code","1a61bd4e":"code","518d387d":"code","b21fb1bf":"code","bb1fef20":"code","1ae7a3d4":"code","02386aa7":"code","b271a4e9":"code","a0bd8ceb":"code","cb30660a":"markdown","4975b902":"markdown","bdc9d0f4":"markdown","a881270c":"markdown","d2199cc2":"markdown","e959956a":"markdown","e2f789f0":"markdown","66590253":"markdown","9155e703":"markdown","7a09c122":"markdown","b152fbe2":"markdown","e64915fa":"markdown","6f21c6f9":"markdown","a49180da":"markdown","ed9de584":"markdown","9e667231":"markdown","5df8a2e4":"markdown","61c97e2a":"markdown","12079292":"markdown","1a0ff90b":"markdown"},"source":{"345ab63a":"import pandas as pd\nimport numpy as np","12469d21":"# Importing file\ndf = pd.read_csv('..\/input\/pokemon\/Pokemon.csv', index_col = '#')\n\n# Display number of columns and rows we want\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 20)","7e6cccf7":"df.info()","10a5584f":"df.columns","5f649f6d":"df.shape","3a62a886":"df.size","1b104f7a":"df.count()","4cc703bc":"df.dtypes","94bba8ba":"df.dtypes.value_counts()","e4c3429f":"df.describe()","b47639c0":"df['Type 1'].value_counts()","0df3a4b9":"df['Type 1'].value_counts(normalize = True)","1d4fd1d4":"df.loc[1:4]","2f258494":"df.loc[1:4, ['Name', 'Type 1', 'Type 2']]","229f0d44":"df.loc[[1,4], ['Name', 'Type 1', 'Type 2']]","10750c39":"df.iloc[1:3, 3] # Get value in row 1 to 3, and column 3","6f03b0ac":"df.iloc[1:3, 0:3] # Get value in row 1 to 3, and column 0 to 2","2bebb13d":"df[['Name', 'Type 1', 'Type 2']].head()","e851b799":"df.loc[df['Attack'] > 150, ['Name', 'Attack']].head()","8eda0d67":"# Add new column 'Information'\ndef info(df):\n    if df['Total'] < 400:\n        status = 'Weak'\n    elif df['Total'] < 500:\n        status = 'Medium'\n    else:\n        status = 'Strong'\n    return status\n\ndf['Information'] = df.apply(info, axis = 1)\n\n# Add new column type\ndf['Type'] = df['Type 1'] + \" and \" + df['Type 2']\n\n# Add new column attack + defense\ndf['Attack + Defense'] = df['Attack'] + df['Defense']","563f2456":"# Add new row with append\nnew_pokemon = pd.DataFrame({\n    'Name' : ['Mysterious', 'Legendaryous']\n}, index  = [722, 723])\n\ndf = df.append(new_pokemon)","a89abf51":"# Drop column 'Information', 'Type' and 'attack + defense'\ndf.drop(columns = ['Information', 'Type', 'Attack + Defense'], inplace = True)\n\n# Drop row 722nd index\ndf.drop(index = [722], inplace = True)\n\n# Another way to drop column\nfilt = df['Name'] == 'Legendaryous'\ndf.drop(index = df[filt].index, inplace = True)","991b6ca8":"# Find duplicated data on Name column\nduplicated = df['Name'].duplicated() == True\n\n# Drop duplicated data\ndf.drop(index = df[duplicated].index, inplace = True)","caeeb908":"# Copy the data, so we don't change the original one\ndf_copy = df.copy()\n\n# Remove all rows which contain null value\ncontain_null = df_copy[df_copy.isnull().any(axis = 1)]\ndf_copy.drop(index = contain_null.index, inplace = True)","78c25bbf":"df.rename(columns = {\n        'Type 1' : 'Type1',\n        'Type 2' : 'Type2'\n    }, inplace = True)\n\n# Same with column, if we want to change index name, we just change the columns with index","f413f006":"df.columns = df.columns.str.replace(' ', '_') # change space to underscore in column name\ndf.columns = [x.title() for x in df.columns] # change column name into title format","b1d04311":"df['Name'] = df['Name'].str.upper() # change value in 'Name' column into capital format\ndf.head()","0ac01df7":"# Change some values in row 1 \ndf.loc[1, 'Attack'] = 50\ndf.loc[1, ['Defense', 'Sp._Atk']] = [50, 64]\n\nfilt = df['Name'] == 'BULBASAUR'\ndf.loc[filt, 'Sp._Def'] = 64","78394519":"# Change value using replace -- values that unchanged are remained the same as previous\ndf['Legendary'] = df['Legendary'].astype('str')\ndf['Legendary'] = df['Legendary'].replace({'False' : 'No', 'True' : 'Yes'})","131f854f":"# Change only for certain value\n\n# Change value which contain 'BULBASAUR' in Name column or '1' in Generation column into 'XXX'\ndummy_df = df.replace({\n    'Name' : 'BULBASAUR',\n    'Generation' : 1\n}, 'XXX') \n\n# Change value which contain '50', '100', 'Yes' or 'Grass' in any columns, into Nan\ndummy_df = df.replace([50, 100, 'Yes', 'Grass'], np.nan)\n\n# Change value which contain 'Water' into 1, 'Normal' into 2, or 'Bug' into 3 \ndummy_df = df.replace(['Water', 'Normal', 'Bug'], [1,2,3])","111af07e":"# Change value using map -- values that unchanged will be turned into nan\ndf['Legendary'] = df['Legendary'].map({'Yes' : 'True'})","31dc5038":"df_sorted = df.sort_values(by = 'Total', ascending = False)\ndf_sorted.head(3)","19a6d090":"df_sorted = df.sort_values(by = ['Total', 'Attack'], ascending = [False, True])\ndf_sorted.head(3)","8b7ba2bf":"# Sorting by index\ndf_sorted.sort_index(inplace = True)\ndf_sorted.head(3)","26072e40":"df_sorted.nlargest(3, 'Total')","f70da622":"df_sorted.nsmallest(3, 'Total')","791cc039":"# Filtering legendary pokemon\nfilt = df['Legendary'] == 'True'\ndf.loc[filt].head()","d5d3c857":"# Filtering the strongest pokemon based on 'Total' column\nfilt = df['Total'] == df['Total'].max()\ndf.loc[filt][['Name', 'Total']]","0e23b3f2":"# Filtering legendary pokemon with the type 1 is water\nfilt = (df['Legendary'] == 'True') & (df['Type1'] == 'Water')\ndf.loc[filt, ['Name', 'Total']]","45e609f9":"# Filtering using isin\npokemon_type = ['Fire', 'Water', 'Grass']\nfilt = df['Type1'].isin(pokemon_type)\ndf.loc[filt, ['Name', 'Total', 'Type1']].head()","c978d260":"# Filtering using contain\nfilt = df['Type1'].str.contains('Grass', na = False)\ndf.loc[filt].head()","d914df21":"# Grouping by 'Type1'\ntype1_grp = df.groupby('Type1')\n\n# Get group\nfire = type1_grp.get_group('Fire')\ngrass = type1_grp.get_group('Grass')\nwater = type1_grp.get_group('Water')","97a3c501":"# Shows amount of other types in 'Type2' with 'Type1' is Fire\ntype1_grp['Type2'].value_counts().loc['Fire']\n\n# Same result with \nfilt = df['Type1'] == 'Fire'\ndf.loc[filt]['Type2'].value_counts()","26fcfcec":"# Shows amount of other types in 'Type1' with 'Type2' is Grass\ntype1_grp['Type2'].apply(lambda x : x.str.contains('Grass').sum())","110eac1d":"type1_grp['Type2'].apply(lambda x : x.str.contains('Grass').sum()).loc[['Fire', 'Water']]","d4b018ce":"# Shows amount of other types in 'Type2' with 'Type1' is Fire or Grass or Water\ntype1_grp['Type2'].value_counts().loc[['Fire', 'Grass', 'Water']]","aff8e336":"type1_grp.mean()","e4d8497d":"type1_grp['Total'].agg(['mean', 'median'])","0501b4f7":"type1_grp['Total'].agg(['mean', 'median']).loc[['Fire', 'Grass', 'Water']]","fd175882":"# 2D\npd.crosstab(df['Generation'], df['Type1'])\npd.crosstab(df['Generation'], df['Type1'], margins = True)","a485b8c7":"# 3D\npd.crosstab(df['Generation'], df['Type1'], values = df['Total'], aggfunc = np.max)","ccf64511":"pd.crosstab(df['Generation'], [df['Legendary'], df['Type1']], margins = True)","fac26796":"pd.crosstab([df['Generation'], df['Legendary']], df['Type1'], margins = True)","fc1e18ca":"weather = pd.DataFrame({\n    'Day' : ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'],\n    'Jakarta' : [32,33,32,31,34,32,33],\n    'Chicago' : [15,12,14,13,13,12,15],\n    'Singapore' : [24,24,22,23,24,24,25]\n})\nweather","fcf2fc4e":"# Transform by using Melt\nweather = pd.melt(weather, id_vars = 'Day', var_name = 'City', value_name = 'Temperature')\nweather","a9786e73":"weather[weather['City'] == 'Jakarta']","5e0e79be":"# Concating dataset\nasia_country = pd.DataFrame({\n    'Country' : ['Singapore', 'Indonesia', 'Malaysia'],\n    'Temperature' : [22,20,29],\n    'Humidity' : [12,11,11]\n})\n\neurope_country = pd.DataFrame({\n    'Country' : ['Germany', 'Italy', 'Spain'],\n    'Temperature' : [5,4,3],\n    'Humidity' : [6,6,7]\n})\n\ncountry = pd.concat([asia_country, europe_country], ignore_index = True)\ncountry","283cf8b2":"country = pd.concat([asia_country, europe_country], keys = ['Asia', 'Europe'])\ncountry","6e9990e3":"weather = pd.Series(['Sunny', 'Rain', 'Rain'], name = 'Weather')\n\ncountry = pd.concat([country, weather], axis = 1)\ncountry","054c3c11":"cap_city = pd.DataFrame({\n    'Country' : ['Indonesia', 'Germany', 'Malaysia'],\n    'Capital City' : ['Jakarta', 'Berlin', 'Kuala Lumpur']\n})\n\npopulation = pd.DataFrame({\n    'Country' : ['Indonesia', 'Germany', 'Malaysia'],\n    'Population (in million)' : [200, 50, 100]\n})\n\n# Merge inner join\ninner_join = pd.merge(cap_city, population, on = 'Country')\n\n# Merge left join\nleft_join = pd.merge(cap_city, population, on = 'Country', how = 'left')\n\n# Merge right join\nright_join = pd.merge(cap_city, population, on = 'Country', how = 'right')\n\n# Merge outer join\nouter_join = pd.merge(cap_city, population, on = 'Country', how = 'outer')","78e362b3":"weather = pd.DataFrame({\n    'Day' : ['1\/1\/2017', '1\/2\/2017', '1\/3\/2017', '1\/4\/2017', '1\/1\/2017', '1\/2\/2017', '1\/3\/2017', '1\/4\/2017', '1\/1\/2017', '1\/2\/2017', '1\/3\/2017', '1\/4\/2017'],\n    'City' : ['Jakarta', 'Jakarta', 'Jakarta', 'Jakarta', 'New York', 'New York', 'New York', 'New York', 'Singapore', 'Singapore', 'Singapore', 'Singapore'],\n    'Temp' : [15,12,14,13,2,3,4,2,11,13,12,15],\n    'Windspeed' : [6,3,7,8,9,2,3,4,2,3,6,8],\n    'Event' : ['Rain', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Sunny', 'Snow', 'Snow', 'Sunny', 'Sunny', 'Sunny', 'Rain']\n})\nweather","7d886ea9":"weather.pivot(index = 'Day', columns = 'City', values = 'Temp')","cad58eba":"# Pivot table\nweather.pivot_table(index = 'Day', columns = 'City', aggfunc = 'mean')","625e4e06":"# Pivot table\nweather.pivot_table(index = 'Day', columns = 'City', aggfunc = 'sum')","8212a9d9":"# Pivot table\nweather.pivot_table(index = 'Day', columns = 'City', aggfunc = 'sum', values = ['Temp'])","d36c2164":"# Pivot table\nweather.pivot_table(index = 'Day', columns = 'City', aggfunc = 'sum', margins = True)","22f5e829":"# Pivoting by weekly \nweather['Day'] = pd.to_datetime(weather['Day'])\n\nweather.pivot_table(index = pd.Grouper(freq = 'W', key = 'Day'))\nweather.pivot_table(index = pd.Grouper(freq = 'W', key = 'Day'), columns = 'City')\nweather.pivot_table(index = pd.Grouper(freq = 'W', key = 'Day'), columns = 'City', values = 'Temp')","7f717eb0":"datetime = \"2021-10-30 00:00:00\"\nprint(type(datetime))\nprint(datetime)\nprint('\\n')\n\ndatetime_1 = pd.datetime.strptime(datetime, '%Y-%m-%d %H:%M:%S')\nprint(type(datetime_1))\nprint(datetime_1)\nprint('\\n')\n\ndate = pd.datetime.strptime(datetime, '%Y-%m-%d %H:%M:%S').date()\nprint(type(date))\nprint(date)\nprint('\\n')\n\ntime = pd.datetime.strptime(datetime, '%Y-%m-%d %H:%M:%S').time()\nprint(type(time))\nprint(time)\n","1a61bd4e":"dates = ['2017-01-31', 'Jan 31, 2017', '31\/1\/2017', '01\/31\/2017', '2017\/01\/31', '20170131', '31\/01\/2017']\ndates = pd.to_datetime(dates)\nprint(dates)\n\ndates1 = ['2017-06-05', 'May 06, 2017', '6\/5\/2017', '05\/06\/2017', '2017\/05\/06', '20170506', '06\/05\/2017'] # After Year, if it can be turned as a month, then it will be\ndates_1 = pd.to_datetime(dates1)\nprint(dates_1)\n\ndates2 = ['2017-01-31 2:30:00 PM', 'Jan 31, 2017 14:30:00', '31\/1\/2017','01\/31\/2017 01:00:00', '2017\/01\/31', '20170131']\ndates2 = pd.to_datetime(dates2)\nprint(dates2)\n\ndates3 = ['2017-01-31 2:30:00 PM', 'abcdefgh','Jan 31, 2017 14:30:00', '31\/1\/2017','01\/31\/2017 01:00:00', '2017\/01\/31', '20170131']\ndates3 = pd.to_datetime(dates3, errors = 'coerce')\nprint(dates3)\n\ndates4 = '06\/05\/2017'\ndates4 = pd.to_datetime(dates4, format = '%m\/%d\/%Y')\nprint(dates4)\n\ndates5 = '25\/1\/2017'\ndates5 = pd.to_datetime(dates5, format = '%d\/%m\/%Y')\nprint(dates5)","518d387d":"df = pd.DataFrame({\n    'Data' : [\n        '1987_M_US _1',\n        '1990?_M_IK_1',\n        '1992_F_US_2',\n        '1970?_M_  IT_1',\n        '1985_F_I   T_2'\n    ]\n})\n\ndf['Data'].str.split('_')\ndf = df['Data'].str.split('_', expand = True)\ndf.columns = ['Year', 'Sex', 'Country', 'Number of Children']\ndf['Country'] = df['Country'].str.replace(' ', '')\ndf['Year'] = df['Year'].str.replace(r'(?P<year>\\d{4})\\?', lambda m : m.group('year'))\n\ndf","b21fb1bf":"employee = {\n    'first' : ['Eren', 'Mikasa', 'Levi', 'Armin', np.nan, None, 'Historia'],\n    'last' : ['Yeager', 'Arckman', 'Arckman', 'Arlert', np.nan, np.nan, 'Reiss'],\n    'email' : ['eren@gmail.com', 'mikasa@gmail.com', None, 'armin@gmail.com', np.nan, None, 'historia@gmail.com'],\n    'age' : [18, 20, 24, 'NA', None, 'Missing', 15]\n}\n\nemployee = pd.DataFrame(employee)\nemployee","bb1fef20":"# Checking Nan\nprint(employee.isna())\nprint(employee.isna().sum())\nprint(employee.isnull())\nprint(employee.isnull().sum())\n\n# Checking for non Nan\nprint(employee.notna())\nprint(employee.notna().sum())","1ae7a3d4":"# Number of ways to drop Nan or None\n\n# Drop NaN or None\nemployee.dropna()\nemployee.dropna(axis = 'index', how = 'any')\nemployee.dropna(axis = 'index', how = 'all')\nemployee.dropna(axis = 'columns', how = 'any')\nemployee.dropna(axis = 'columns', how = 'all')\n\n# Drop Nan or None with Subset (only certain columns)\nemployee.dropna(axis = 'index', how = 'all', subset = ['email'])\nemployee.dropna(axis = 'index', how = 'all', subset = ['email', 'age'])\nemployee.dropna(axis = 'index', how = 'any', subset = ['first', 'email', 'age'])\n\n# Drop Nan or None with thresh (atleast in a row must contain value as minimum as the thresh, otherwise the row will be dropped)\nemployee.dropna(thresh = 1)","02386aa7":"# Replace string to NaN\nemployee.replace('NA', np.nan, inplace = True)\nemployee.replace('Missing', np.nan, inplace = True)","b271a4e9":"# Number of ways to filling Nan value\nemployee.fillna('MISSING')\nemployee.fillna(0)\nemployee.fillna({\n    'first' : 'Missing',\n    'last' : 'Missing',\n    'email' : 'Missing',\n    'age' : employee['age'].mean()\n})","a0bd8ceb":"# Another way to filling Nan\n\n# Filling NaN with the same value as the cell above\nemployee.fillna(method = 'ffill')\nemployee.fillna(method = 'ffill', limit = 1)\n\n# Filling NaN with the same value as the cell below\nemployee.fillna(method = 'bfill')\n\n# Filling NaN with the same value as the cell next to\nemployee.fillna(method = 'ffill', axis = 'columns')\nemployee.fillna(method = 'bfill', axis = 'columns')\n\n# Filling NaN with linear value\nemployee.interpolate(method = 'linear')","cb30660a":"## 18. Text Handling","4975b902":"## Template For Basic Code In Data Analaysis","bdc9d0f4":"## 15. Merging the Dataset","a881270c":"## 12. Grouping","d2199cc2":"## 8. Changing Column and Index Name","e959956a":"## 9. Number of Ways to Changing the Value","e2f789f0":"## 10. Sorting","66590253":"## 1. Importing File","9155e703":"## 3. Indexing","7a09c122":"## 19. Handling Missing Data","b152fbe2":"## 6. Finding and Dropping Duplicated Data","e64915fa":"## 4. Adding New Column and Row","6f21c6f9":"## 14. Transforming Dataset Format","a49180da":"## 5. Removing Column and Row","ed9de584":"## 16. Pivoting","9e667231":"## 17. Time Series","5df8a2e4":"## 13. Crosstab 2D & 3D","61c97e2a":"## 11. Filtering","12079292":"## 7. Removing Null Value","1a0ff90b":"## 2. Basic Information"}}