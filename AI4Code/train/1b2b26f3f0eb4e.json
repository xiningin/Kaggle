{"cell_type":{"28f7aa8d":"code","953eacc8":"code","9990feb2":"code","c4c40387":"code","0ba97e89":"code","143b2600":"code","836ebb67":"code","07060cdc":"code","680fcc61":"code","7148ad3e":"code","42660028":"code","88c527e5":"code","6edf60c8":"code","38d9f537":"code","bfebd3e1":"code","59bcd264":"code","5bd53a06":"code","9e25ca1f":"code","21fe190a":"code","dc42aa75":"code","c7bcc7a7":"code","d9cdd17b":"code","18771027":"code","612c5866":"code","f35adee6":"code","af20c093":"code","5779ad53":"code","1c275632":"code","cc1326b6":"code","ea33022e":"code","9cfb5b17":"code","9d95383f":"code","1d332968":"code","0013c343":"code","44b6b500":"code","53d9c312":"code","a9ae90d9":"code","4afa777e":"code","27b08412":"code","d9246957":"code","216f2402":"markdown","4ca9034a":"markdown","bbe47bc7":"markdown","65b81a68":"markdown","4e6c07fb":"markdown","597a71d2":"markdown","fa72a833":"markdown","eee42adb":"markdown","081e5b4c":"markdown","886ffab5":"markdown","a0043ae2":"markdown","09aef7fb":"markdown","5f8a7024":"markdown","5ebcce98":"markdown","be56cbd7":"markdown","5952f294":"markdown","2de4000e":"markdown","1c1c457d":"markdown","e3135de0":"markdown","d8bbd49e":"markdown","e5a2d0f2":"markdown","add96679":"markdown","bb755d0b":"markdown","1b726f02":"markdown","6d47463a":"markdown","d45f2ede":"markdown","057dbb20":"markdown","ae089770":"markdown","cb168b70":"markdown","b723987b":"markdown","46fbe51a":"markdown","f972f449":"markdown","269a5d44":"markdown","061f114b":"markdown","14549f2c":"markdown","53733bc1":"markdown","e6694f00":"markdown","3fedc83d":"markdown","e139907b":"markdown","255efcb9":"markdown","d9da4aa8":"markdown","39c05c32":"markdown","60e9909c":"markdown","6d447c54":"markdown"},"source":{"28f7aa8d":"import os\nimport json\nimport collections\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt","953eacc8":"PATH_BASE = \"..\/input\/herbarium-2021-fgvc8\/\"\nPATH_TRAIN = os.path.join(PATH_BASE, \"train\/\")\nPATH_TRAIN_META = os.path.join(PATH_TRAIN, \"metadata.json\")\n\n\nwith open(PATH_TRAIN_META) as json_file:\n    metadata = json.load(json_file)","9990feb2":"metadata.keys()","c4c40387":"len(metadata[\"annotations\"]), len(metadata[\"images\"])","0ba97e89":"print(metadata[\"annotations\"][0])\nprint(metadata[\"images\"][0])\nprint(metadata[\"categories\"][0])\nprint(metadata[\"licenses\"][0])\nprint(metadata[\"institutions\"][0])","143b2600":"len(set([annotation[\"category_id\"] for annotation in metadata[\"annotations\"]]))","836ebb67":"ids = []\ncategories = []\npaths = []\n\nfor annotation, image in zip(metadata[\"annotations\"], metadata[\"images\"]):\n    assert annotation[\"image_id\"] == image[\"id\"]\n    ids.append(image[\"id\"])\n    categories.append(annotation[\"category_id\"])\n    paths.append(image[\"file_name\"])\n        \ndf_meta = pd.DataFrame({\"id\": ids, \"category\": categories, \"path\": paths})","07060cdc":"df_meta","680fcc61":"df_meta[\"category\"].value_counts()","7148ad3e":"d_categories = {category[\"id\"]: category[\"name\"] for category in metadata[\"categories\"]}\nd_families = {category[\"id\"]: category[\"family\"] for category in metadata[\"categories\"]}\nd_orders = {category[\"id\"]: category[\"order\"] for category in metadata[\"categories\"]}\n\ndf_meta[\"category_name\"] = df_meta[\"category\"].map(d_categories)\ndf_meta[\"family_name\"] = df_meta[\"category\"].map(d_families)\ndf_meta[\"order_name\"] = df_meta[\"category\"].map(d_orders)\ndf_meta","42660028":"def visualize_train_batch(paths, categories, families, orders):\n    plt.figure(figsize=(16, 16))\n    \n    for ind, info in enumerate(zip(paths, categories, families, orders)):\n        path, category, family, order = info\n        \n        plt.subplot(2, 3, ind + 1)\n        \n        image = cv2.imread(os.path.join(PATH_TRAIN, path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        plt.imshow(image)\n        \n        plt.title(\n            f\"FAMILY: {family} ORDER: {order}\\n{category}\", \n            fontsize=10,\n        )\n        plt.axis(\"off\")\n    \n    plt.show()","88c527e5":"def visualize_by_id(df, _id=None):\n    tmp = df.sample(6)\n    if _id is not None:\n        tmp = df[df[\"category\"] == _id].sample(6)\n\n    visualize_train_batch(\n        tmp[\"path\"].tolist(), \n        tmp[\"category_name\"].tolist(),\n        tmp[\"family_name\"].tolist(),\n        tmp[\"order_name\"].tolist(),\n    )","6edf60c8":"visualize_by_id(df_meta, 22344)","38d9f537":"visualize_by_id(df_meta, 42811)","bfebd3e1":"visualize_by_id(df_meta, 1719)","59bcd264":"visualize_by_id(df_meta, 1)","5bd53a06":"visualize_by_id(df_meta)","9e25ca1f":"df_submission = pd.read_csv(\n    \"..\/input\/herbarium-2021-fgvc8\/sample_submission.csv\",\n    index_col=0,\n)","21fe190a":"df_submission[\"Predicted\"] = 25229","dc42aa75":"df_submission.to_csv(\"submission.csv\")","c7bcc7a7":"pd.read_csv(\"submission.csv\", index_col=0)","d9cdd17b":"FULL_PIPELINE = False","18771027":"import os\nimport random\n\nimport numpy as np\nfrom numpy import save, load\nimport pandas as pd\nimport cv2\nimport albumentations as A\nfrom albumentations import pytorch as ATorch\nimport torch\nfrom torch.utils import data as torch_data\nfrom torch import nn as torch_nn\nfrom torch.nn import functional as torch_functional\nimport torchvision\nfrom tqdm import tqdm\nfrom sklearn.metrics.pairwise import euclidean_distances","612c5866":"class MobileNetV2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        tmp_net = torch.hub.load(\n            \"pytorch\/vision:v0.6.0\", \"mobilenet_v2\", pretrained=True\n        )\n        self.net = torch_nn.Sequential(*(list(tmp_net.children())[:-1]))\n\n    def forward(self, x):\n        return self.net(x)","f35adee6":"class DataRetriever(torch_data.Dataset):\n    def __init__(\n        self, \n        paths, \n        categories=None,\n        transforms=None,\n        base_path=PATH_TRAIN\n    ):\n        self.paths = paths\n        self.categories = categories\n        self.transforms = transforms\n        self.base_path = base_path\n          \n    def __len__(self):\n        return len(self.paths)\n    \n    def __getitem__(self, index):\n        img = cv2.imread(os.path.join(self.base_path, self.paths[index]))\n        img = cv2.resize(img, (224, 224))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transforms:\n            img = self.transforms(image=img)[\"image\"]\n        \n        if self.categories is None:\n            return img\n        \n        y = self.categories[index] \n        return img, y\n    \n    \ndef get_transforms():\n    return A.Compose(\n        [\n            A.Normalize(\n                mean=[0.485, 0.456, 0.406], \n                std=[0.229, 0.224, 0.225], \n                p=1.0\n            ),\n            ATorch.transforms.ToTensorV2(p=1.0),\n        ], \n        p=1.0\n    )","af20c093":"df_train = df_meta[[\"category\", \"path\"]].sort_values(by=\"category\")\n\ndf_train","5779ad53":"tmp_path = df_train[\"path\"].tolist()\ntmp_category = df_train[\"category\"].tolist()\n# If FULL_PIPELINE is False we use small subset of data\nif not FULL_PIPELINE:\n    tmp_path = tmp_path[:256 * 8]\n    tmp_category = tmp_category[:256 * 8]\n\ntrain_data_retriever = DataRetriever(\n    tmp_path,\n    tmp_category,\n    transforms=get_transforms(),\n)\n\ntrain_loader = torch_data.DataLoader(\n    train_data_retriever,\n    batch_size=256,\n    shuffle=False,\n    num_workers=8,\n)","1c275632":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = MobileNetV2()\nmodel.to(device)\nmodel.eval();","cc1326b6":"category_counts = collections.Counter(df_train[\"category\"].tolist())","ea33022e":"final_vectors = np.zeros((len(category_counts), 1280))\n\nwith torch.no_grad():\n    for batch in tqdm(train_loader):\n        X, y = batch\n        vectors = model(X.to(device)).mean(axis=(2, 3))\n        \n        _y = y.numpy().tolist()\n        for ind in range(len(_y)):\n            final_vectors[_y[ind]] += vectors[ind].cpu().numpy().copy() \/ category_counts[_y[ind]]","9cfb5b17":"save(\"average_vectors.npy\", final_vectors)","9d95383f":"final_vectors = load(\"average_vectors.npy\")","1d332968":"PATH_TEST = os.path.join(PATH_BASE, \"test\/\")\nPATH_TEST_META = os.path.join(PATH_TEST, \"metadata.json\")\n\n\nwith open(PATH_TEST_META) as json_file:\n    metadata = json.load(json_file)\n\n    \nid2path = {\n    img[\"id\"]: img[\"file_name\"] for img in metadata[\"images\"]\n}","0013c343":"df_submission = pd.read_csv(\n    \"..\/input\/herbarium-2021-fgvc8\/sample_submission.csv\",\n    index_col=0,\n)\n\ndf_submission[\"Id\"] = df_submission.index\ndf_submission[\"Path\"] = df_submission[\"Id\"].map(lambda x: id2path[x])","44b6b500":"tmp_path = df_submission[\"Path\"].tolist()\n# If FULL_PIPELINE is False we use small subset of data\nif not FULL_PIPELINE:\n    tmp_path = tmp_path[:256 * 2]\n\ntest_data_retriever = DataRetriever(\n    tmp_path,\n    transforms=get_transforms(),\n    base_path=PATH_TEST,\n)\n\ntest_loader = torch_data.DataLoader(\n    test_data_retriever,\n    batch_size=256,\n    shuffle=False,\n    num_workers=8,\n)","53d9c312":"res = []\n\nwith torch.no_grad():\n    for ind, X in enumerate(tqdm(test_loader)):\n        vectors = model(X.to(device)).mean(axis=(2, 3))\n        tmp = euclidean_distances(vectors.cpu().numpy(), final_vectors)\n        res.extend(list(tmp.argmin(axis=1)))","a9ae90d9":"df_submission.iloc[:len(res), 0] = res\n\ndf_submission[[\"Predicted\"]].to_csv(\"submission.csv\")\n\npd.read_csv(\"submission.csv\", index_col=0)","4afa777e":"PATH_PREPARED_SUBMISSION = \"..\/input\/herbarium-2021-submissions\/submission-mobilenetv2-mean.csv\"\n\nprepared_submission = pd.read_csv(PATH_PREPARED_SUBMISSION, index_col=0)\nprepared_submission.to_csv(\"prepared_submission.csv\")\n\npd.read_csv(\"prepared_submission.csv\", index_col=0)","27b08412":"PATH_RESNET18 = \"..\/input\/herbarium-2021-submissions\/submission-resnet18-model.csv\"\n\nresnet18_submission = pd.read_csv(PATH_RESNET18, index_col=0)\nresnet18_submission.to_csv(\"resnet18_submission.csv\")\n\npd.read_csv(\"resnet18_submission.csv\", index_col=0)","d9246957":"PATH_RESNET50 = \"..\/input\/herbarium-2021-submissions\/submission-resnet50-model.csv\"\n\nresnet50_submission = pd.read_csv(PATH_RESNET50, index_col=0)\nresnet50_submission.to_csv(\"resnet50_submission.csv\")\n\npd.read_csv(\"resnet50_submission.csv\", index_col=0)","216f2402":"### Read the metadata file","4ca9034a":"<a id=\"100\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Competition Metric<center><h2>","bbe47bc7":"<a id=\"1\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Overview<center><h2>","65b81a68":"### Check first samples from each key","4e6c07fb":"### Define the model\n\nYou can use any of the pretrained models, for example:\n- [PYTORCH HUB FOR RESEARCHERS](https:\/\/pytorch.org\/hub\/research-models)\n- [TORCHVISION.MODELS](https:\/\/pytorch.org\/vision\/stable\/models.html)","597a71d2":"The images are provided by the [New York Botanical Garden](https:\/\/www.nybg.org\/), [Bishop Museum](https:\/\/www.bishopmuseum.org\/), [Naturalis Biodiversity Center](https:\/\/www.naturalis.nl\/en), [Queensland Herbarium](https:\/\/www.qld.gov.au\/environment\/plants-animals\/plants\/herbarium), and [Auckland War Memorial Museum](https:\/\/www.aucklandmuseum.com\/).","fa72a833":"### Save output vectors from the model and average by category","eee42adb":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/25558\/logos\/header.png)","081e5b4c":"### Create DataFrame with main information","886ffab5":"### Get test output vectors and find the nearest train vector (by euclidean distance) and take its category","a0043ae2":"The training and test set contain images of herbarium specimens from nearly 65,000 species of vascular plants. Each image contains exactly one specimen. The text labels on the specimen images have been blurred to remove category information in the image.","09aef7fb":"### Classes distribution","5f8a7024":"<a id=\"200\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Modeling<center><h2>","5ebcce98":"### Create test data loader","be56cbd7":"The data has been approximately split 80%\/20% for training\/test. Each category has at least 1 instance in both the training and test datasets. Note that the test set distribution is slightly different from the training set distribution. The training set contains species with hundreds of examples, but the test set has the number of examples per species capped at a maximum of 10.","5952f294":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:brown; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [Overview](#1)\n* [Data Visualization](#2)\n    \n    \n* [Competition Metric](#100)\n* [Sample Submission](#101)\n    \n    \n* [Modeling](#200)\n* [Ensembling](#201)","2de4000e":"### Check the number of images and their annotations","1c1c457d":"$$F_1 = 2\\frac{precision \\cdot recall}{precision+recall}$$","e3135de0":"### First level elements","d8bbd49e":"### The idea: Create for each category abstract vector from some model (MobileNetV2) and find nearest vector for each train sample","e5a2d0f2":"### Find name and family of the classes by their ids","add96679":"### Import libraries","bb755d0b":"### Let's take for each category (target) all images from the train set and after processing average their vectors\n","1b726f02":"<a id=\"2\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Data Visualization<center><h2>","6d47463a":"### Save results to submission file","d45f2ede":"<a id=\"101\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Sample Submission<center><h2>","057dbb20":"$$precision = \\frac{TP}{TP+FP}$$","ae089770":"### Define your dataset class for getting image samples","cb168b70":"### Random samples","b723987b":"### Save and load category vectors (you can pretrain them)","46fbe51a":"![](https:\/\/i.postimg.cc\/htpxH99f\/Herbarium2021.png)","f972f449":"where:","269a5d44":"# Herbarium 2021: Half-Earth Challenge - FGVC8 - Exploratory Data Analysis\n\nQuick Exploratory Data Analysis for [Herbarium 2021: Half-Earth Challenge - FGVC8](https:\/\/www.kaggle.com\/c\/herbarium-2021-fgvc8) challenge    \n\nThe Herbarium 2021: Half-Earth Challenge is to identify vascular plant specimens provided by the [New York Botanical Garden (NY)](https:\/\/www.nybg.org\/), [Bishop Museum (BPBM)](https:\/\/www.bishopmuseum.org\/), [Naturalis Biodiversity Center (NL)](https:\/\/www.naturalis.nl\/en), [Queensland Herbarium (BRI)](https:\/\/www.qld.gov.au\/environment\/plants-animals\/plants\/herbarium), and [Auckland War Memorial Museum (AK)](https:\/\/www.aucklandmuseum.com\/).\n\n*The Herbarium 2021: Half-Earth Challenge* dataset includes more than 2.5M images representing nearly 65,000 species from the Americas and Oceania that have been aligned to a standardized plant list ([LCVP v1.0.2](https:\/\/www.nature.com\/articles\/s41597-020-00702-z)).","061f114b":"![](https:\/\/i.postimg.cc\/fbSLBX54\/Logos.png)","14549f2c":"Submissions are evaluated using the [macro F1 score](#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html).","53733bc1":"<a id=\"201\"><\/a>\n<h2 style='background:brown; border:0; color:white'><center>Ensembling<center><h2>","e6694f00":"### Calculate the total number of classes","3fedc83d":"### Initialize the model","e139907b":"### One of the most frequently class from train data","255efcb9":"In \"macro\" F1 a separate F1 score is calculated for each species value and then averaged.","d9da4aa8":"### Get test paths","39c05c32":"### I prepared processed by the algorithm described above submission file for all data, you can use it for fast submission","60e9909c":"## Work In Progress\n\nNext steps:\n- Add ensemble of different models","6d447c54":"$$recall = \\frac{TP}{TP+FN}$$"}}