{"cell_type":{"437b89eb":"code","0908cbf6":"code","42121a06":"code","3e12b048":"code","aa664f8b":"code","1eeb5963":"code","b76a5651":"code","dcaa0160":"code","1650582d":"code","3c5d2c38":"code","3ec9fc15":"code","7d24349d":"code","d7e4faf3":"code","d4e38d13":"code","bd15b449":"code","5de91847":"code","ce626a94":"code","b2cb116b":"code","478dd42a":"code","ca7e014b":"code","447100da":"code","ed18fded":"code","9803f0bf":"code","40366c96":"code","7df5d8b1":"code","fd16b714":"code","76d9d6a2":"code","5c0f4d7a":"markdown","cd5910db":"markdown","f217140c":"markdown","a1fdb96c":"markdown"},"source":{"437b89eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense, Activation, Convolution2D, MaxPooling2D, Dropout, Flatten,BatchNormalization\nfrom keras.models import Sequential, save_model\nfrom keras.utils import np_utils","0908cbf6":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","42121a06":"train_features.drop(['sig_id','cp_type','cp_dose','cp_time'], axis='columns', inplace=True)\ntrain_features.head(n=5)","3e12b048":"train_targets_scored.drop(['sig_id'], axis='columns', inplace=True)\ntrain_targets_scored.head(n=5)","aa664f8b":"x=train_features.values\ny=train_targets_scored.values\nprint(x.shape)\nprint(y.shape)\nnum_classes=y.shape[1]\nprint(num_classes)\ninput_dim=x.shape[1]\nprint(input_dim)","1eeb5963":"# model = Sequential()\n# model.add( Dense(32, input_dim = input_dim ) )\n# model.add( Activation('relu') )\n\n# model.add( Dense(64 ) )\n# model.add( Activation('relu') )\n\n# # model.add( Flatten() )\n\n# model.add( Dropout(0.25) )\n# model.add( Dense(num_classes))\n# # model.add( Activation('softmax') )\n# model.add( Activation('sigmoid') )","b76a5651":"model = Sequential()\nmodel.add( Dense(512, input_dim = input_dim ) )\nmodel.add( Activation('relu') )\n\nmodel.add( BatchNormalization() )\nmodel.add( Dropout(0.25) )\nmodel.add( Dense(256) )\nmodel.add( Activation('relu') )\n\nmodel.add( BatchNormalization() )\nmodel.add( Dropout(0.25) )\nmodel.add( Dense(128) )\nmodel.add( Activation('relu') )\n\nmodel.add( BatchNormalization() )\nmodel.add( Dropout(0.25) )\nmodel.add( Dense(64) )\nmodel.add( Activation('relu') )\n\nmodel.add( BatchNormalization() )\nmodel.add( Dropout(0.25) )\nmodel.add( Dense(num_classes))\n# output_bias = keras.initializers.Constant(4.6)\n# model.add( Dense(num_classes,bias_initializer=output_bias))\n\n# model.add( Activation('softmax') )\nmodel.add( Activation('sigmoid'))","dcaa0160":"# model = Sequential()\n# model.add( Dense(512, input_dim = input_dim ) )\n# model.add( Activation('relu') )\n\n# model.add( BatchNormalization() )\n# model.add( Dropout(0.25) )\n# model.add( Dense(256) )\n# model.add( Activation('relu') )\n\n# model.add( BatchNormalization() )\n# model.add( Dropout(0.25) )\n# model.add( Dense(128) )\n# model.add( Activation('relu') )\n\n# model.add( BatchNormalization() )\n# model.add( Dropout(0.25) )\n# model.add( Dense(64) )\n# model.add( Activation('relu') )\n\n# model.add( BatchNormalization() )\n# model.add( Dropout(0.25) )\n# model.add( Dense(128) )\n# model.add( Activation('relu') )\n\n# model.add( BatchNormalization() )\n# model.add( Dropout(0.25) )\n# model.add( Dense(num_classes))\n# # output_bias = keras.initializers.Constant(4.6)\n# # model.add( Dense(num_classes,bias_initializer=output_bias))\n\n# # model.add( Activation('softmax') )\n# model.add( Activation('sigmoid'))","1650582d":"model.summary()","3c5d2c38":"opt = keras.optimizers.Adam(learning_rate=3e-3)","3ec9fc15":"model.compile( optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'] )\n","7d24349d":"train_features=x\ntrain_labels=y","d7e4faf3":"history= model.fit( train_features, train_labels, batch_size=256, epochs=30)","d4e38d13":"plt.plot(history.history['accuracy'])\n# plt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n# plt.legend(['train', 'test'], loc='upper left')\nplt.legend(['train'], loc='upper left')\nplt.show()","bd15b449":"sample_submission.head(n=5)","5de91847":"col_names=sample_submission.columns\n# print(col_names)","ce626a94":"test_id=x_test=test_features['sig_id'].values\nx_test=test_features.drop(['sig_id','cp_type','cp_dose','cp_time'], axis='columns', inplace=False)\n# x_test.head(n=5)\n# test_id=x_test=test_features['sig_id'].values\nprint(test_id.shape)","b2cb116b":"y_pred=model.predict(x_test)\nprint(y_pred.shape)","478dd42a":"len(y_pred[0])","ca7e014b":"sub_dict={}\nfor i in col_names:\n    sub_dict[i]=[]\n# print(sub_dict.keys())","447100da":"total_pred=y_pred.shape[0]\nfor i in range(total_pred):\n    sub_dict['sig_id'].append(test_id[i])\n#     ans=y_pred[i].argmax()+1\n    ans=y_pred[i]\n    for j in range(1,207):\n        sub_dict[col_names[j]].append(ans[j-1])\n#         if j==ans:\n#             sub_dict[col_names[j]].append(1)\n#         else:\n#             sub_dict[col_names[j]].append(0)\n#     break\n#     for j","ed18fded":"df=pd.DataFrame(sub_dict)","9803f0bf":"df.head()","40366c96":"print(df.shape)","7df5d8b1":"df.to_csv('.\/submission.csv',index=False)\n# sample_submission.to_csv('.\/submission.csv',index=False)","fd16b714":"check_df=pd.read_csv('.\/submission.csv')","76d9d6a2":"check_df.head()","5c0f4d7a":"# Architecture 2","cd5910db":"# Architecture 3","f217140c":"# Architecture 1","a1fdb96c":"# Upvote if you like it\n1. vesrion 6: basic model (0.28029)\n2. version 7: try to improve increase epochs to 50 and replace softmax with sigmoid (0.02715)\n3. version 8: Change model architecture and learning rate (0.03493)\n4. version 9: Change model architecture and learning rate\n5. version 10: drop cp_time from dataframe(0.02053)\n6. version 11: change learning rate\n7. version 12: change architecture and learning rate(0.02103)\n8. version 13: increase batch size(0.02017)\n9. version 14: again increase batch size\n10. version 15: changing architecture and batch size"}}