{"cell_type":{"d36c3a4b":"code","6a5599a5":"code","a73b30aa":"code","cce8b16d":"code","8bb78bb5":"code","ed728bf3":"code","768a9208":"code","7eb7cc31":"code","d6813078":"code","9bab56b2":"code","a43f4c1f":"code","a5980ba9":"code","5765a770":"code","7ee40bf8":"code","95d99202":"code","a2a6146f":"code","998c069b":"code","55057e2a":"code","9fc0ab99":"code","e92203b0":"code","48ae35d8":"code","52f4326d":"code","906ca8f5":"code","0bd8af31":"code","41a2d936":"code","9f395932":"code","a91736a4":"code","a462e144":"code","243300a0":"code","86bcae02":"code","fa21d349":"code","aad1f559":"code","55a44708":"code","c8db69cb":"code","fcc774a5":"code","38aa7878":"code","ced78a00":"code","e459db9d":"code","b912f1fc":"code","d2b8737a":"code","a2d6ea3b":"code","52602cd7":"code","ffe8228d":"code","7fcdf1ea":"code","f85441c5":"code","16b607d2":"code","b94579bb":"code","f44679f7":"code","e8eb6897":"code","e3ae8858":"code","8f524acb":"code","6b276f51":"code","ecbfbabf":"code","bd4651ce":"code","f27df66b":"code","fa46a461":"code","632244a1":"code","3d140568":"code","ee3d5e8f":"code","cc71ee0a":"code","8f643b59":"code","7c7d26db":"code","9617ad30":"code","9f02fbe9":"code","66f6db85":"code","6bfa6320":"code","67a721ea":"code","5847ef95":"code","fcab7f90":"code","f9790d9c":"code","289835b7":"code","94dddffd":"code","3116b48c":"code","d92050cf":"code","82c315ba":"code","192d1f3f":"code","849523ac":"code","6297a83a":"code","8ec9ecb6":"code","e1618c4d":"code","2f23bcb0":"code","1a307a3c":"code","580f87ee":"code","56bdf424":"code","230ab6b7":"code","1b0f1fcf":"code","d8c99588":"code","4e7d86cb":"code","e19cbeda":"markdown","32ed3931":"markdown","39e65911":"markdown","d533f0e3":"markdown","56c07823":"markdown","3d810b98":"markdown","512315f1":"markdown","5a1bdeeb":"markdown","0a470819":"markdown","43b0d0b2":"markdown","bf107cd8":"markdown","4893990f":"markdown","bc1089d0":"markdown","7de62832":"markdown","e6f42276":"markdown","e61c5904":"markdown","d237eee7":"markdown","dce8541f":"markdown","e6dec91c":"markdown","627431f5":"markdown","58d2332f":"markdown","0e4cbe03":"markdown","bf7c8acf":"markdown","38ff0cb7":"markdown","6d06437a":"markdown","a0b83150":"markdown","f4d65b03":"markdown","13d408c8":"markdown","05c5ce0e":"markdown","7e216d85":"markdown","6a3b5b17":"markdown","4ccd2e59":"markdown","ae8a2333":"markdown","c39ad7fb":"markdown","3af5c06a":"markdown","85e15989":"markdown","8448a456":"markdown","d3a83241":"markdown","2ab163af":"markdown","04410e31":"markdown","214c8a9e":"markdown","6dbd7f04":"markdown","5b04e92f":"markdown","1b882893":"markdown","4d238c00":"markdown","96a57202":"markdown","70b4dfd8":"markdown","a35e0534":"markdown","04c146ca":"markdown","a18da247":"markdown","56b58b27":"markdown","0e96a843":"markdown","fa5210bb":"markdown","900ca2f3":"markdown","9bdb4612":"markdown","46c5147f":"markdown","cb9006ca":"markdown","4d3cf08c":"markdown","2132349a":"markdown","ced2f329":"markdown","db1898da":"markdown","d4f5a333":"markdown","b408c17b":"markdown","6ad3fc3a":"markdown","1e56749d":"markdown","6f9e814e":"markdown"},"source":{"d36c3a4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%matplotlib inline                 \n\nimport pandas as pd                # Implemennts milti-dimensional array and matrices\nimport numpy as np                 # For data manipulation and analysis\nimport matplotlib.pyplot as plt    # Plotting library for Python programming language and it's numerical mathematics extension NumPy\nimport seaborn as sns              # Provides a high level interface for drawing attractive and informative statistical graphics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a5599a5":"# load dataset\ntrain=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_submission=pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","a73b30aa":"len(train),len(test),len(gender_submission)","cce8b16d":"train.head()","8bb78bb5":"train.shape","ed728bf3":"train.describe(include='all')","768a9208":"train.info()","7eb7cc31":"train.isnull().values.any()","d6813078":"train.isnull().sum()","9bab56b2":"test.isnull().sum()","a43f4c1f":"plt.style.use('default')\ntotal=train.isnull().sum()\npercent=train.isnull().sum()\/train.isnull().count()\nmissing_data=pd.concat([total,percent],axis=1, keys=['total', 'percent'])\n#missing_data.sort_values(ascending=False)\nax = plt.subplots(figsize=(12, 6))\n#plt.xticks(rotation='90')\nsns.barplot(x=missing_data.index,y=missing_data['percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nplt.show()","a5980ba9":"import missingno as msno","5765a770":"msno.bar(train,figsize=(10,6),color=\"skyblue\")\nplt.show()","7ee40bf8":"msno.bar(test,figsize=(10,6),color=\"skyblue\")\nplt.show()","95d99202":"msno.heatmap(train,figsize=(10,6))\nplt.show()","a2a6146f":"msno.matrix(train,figsize=(12,8))\nplt.show()","998c069b":"msno.matrix(test,figsize=(12,8))\nplt.show()","55057e2a":"train['Age'].fillna(train['Age'].median(),inplace=True)\ntest['Age'].fillna(train['Age'].median(),inplace=True)","9fc0ab99":"train['Age']","e92203b0":"train['Cabin'].unique()","48ae35d8":"train['Cabin'].fillna('Unknown',inplace=True)\ntrain['Embarked'].fillna('Unknown',inplace=True)\ntest['Cabin'].fillna('Unknown',inplace=True)\ntest['Fare'].fillna(train['Fare'].median(),inplace=True)","52f4326d":"msno.bar(train,figsize=(10,6),color=\"skyblue\")\nplt.show()","906ca8f5":"msno.bar(test,figsize=(10,6),color=\"skyblue\")\nplt.show()","0bd8af31":"msno.matrix(train,figsize=(12,6))\nplt.show()","41a2d936":"train['Survived'].value_counts(normalize=True)","9f395932":"\nsns.countplot(x='Survived',data=train)\nplt.xticks( np.arange(2), ['drowned', 'survived'] )\nplt.title('Overall survival (training dataset)',fontsize= 18)\n# set x label\nplt.xlabel('Passenger status after the tragedy',fontsize = 15)\n# set y label\nplt.ylabel('Number of passengers',fontsize = 15)\nlabels = (train['Survived'].value_counts())\nfor i, v in enumerate(labels):\n    plt.text(i, v-40, str(v), horizontalalignment = 'center', size = 14, color = 'w', fontweight = 'bold')\n    \nplt.show()\n","a91736a4":"\nsns.barplot(x = \"Sex\", y = \"Survived\", data=train)\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize =10)\nlabels = ['Female', 'Male']\nplt.ylabel(\"% of passenger survived\", fontsize = 8)\nplt.xlabel(\"Gender\",fontsize = 8)\nplt.show()\n","a462e144":"print(\"% of women survived: \" , train[train.Sex == 'female'].Survived.sum()\/train[train.Sex == 'female'].Survived.count())\nprint(\"% of men survived:   \" , train[train.Sex == 'male'].Survived.sum()\/train[train.Sex == 'male'].Survived.count())","243300a0":"\nsns.catplot(x='Sex', col='Survived', kind='count', data=train)\n\nplt.show()","86bcae02":"train.groupby(['Survived','Sex']).count()","fa21d349":"train['Pclass'].unique()","aad1f559":"\nplt.subplots(figsize = (8,6))\nsns.countplot('Pclass',hue='Survived',data=train)\n\nplt.show()","55a44708":"plt.subplots(figsize = (8,6))\nsns.barplot('Pclass','Survived',data=train,hue='Sex',edgecolor=(0,0,0), linewidth=2)\nplt.show()","c8db69cb":"sns.catplot('Pclass','Survived', kind='point', data=train);","fcc774a5":"plt.subplots(figsize=(8,6))\nsns.kdeplot(train.loc[(train['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived' )\n\nlabels = ['First', 'Second', 'Third']\nplt.xticks(sorted(train.Pclass.unique()),labels)\nplt.show()","38aa7878":"print(\"% of survivals in\") \nprint(\"Pclass=1 : \", train.Survived[train.Pclass == 1].sum()\/train.Survived[train.Pclass == 1].count())\nprint(\"Pclass=2 : \", train.Survived[train.Pclass == 2].sum()\/train.Survived[train.Pclass == 2].count())\nprint(\"Pclass=3 : \", train.Survived[train.Pclass == 3].sum()\/train[train.Pclass == 3].Survived.count())","ced78a00":"plt.subplots(figsize=(8,6))\nsns.distplot(train.Age)\nplt.title('Distrubution of passengers age (all data)',fontsize= 14)\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","e459db9d":"bins = [ 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train,ci=None)\nplt.show()","b912f1fc":"train.Name.head()","d2b8737a":"train['Title'] = train['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\ntest['Title'] = test['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\nplt.figure(figsize=(8, 6))\nax = sns.countplot( x = 'Title', data = train, palette = \"hls\", order = train['Title'].value_counts().index)\n_ = plt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\n\nplt.title('Passengers distribution by titles',fontsize= 14)\nplt.ylabel('Number of passengers')\n\n# calculate passengers for each category\nlabels = (train['Title'].value_counts())\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+10, str(v), horizontalalignment = 'center', size = 10, color = 'black')\n\nplt.show()","a2d6ea3b":"plt.figure(figsize=(10, 6))\nsns.barplot(x=\"Title\", y=\"Survived\", data=train,ci=None) \nplt.xticks(\n    rotation=45, \n    horizontalalignment='right',\n    fontweight='light'  \n)\n\nplt.show()","52602cd7":"train['Cabin']","ffe8228d":"train['Cabin'].unique()\n","7fcdf1ea":"train['deck']=train['Cabin'].str.split('',expand=True)[1]\ntest['deck']=test['Cabin'].str.split('',expand=True)[1]","f85441c5":"train['deck'].unique()","16b607d2":"plt.figure(figsize=(12,8))\nsns.countplot(x=train['deck'],data=train,hue='Survived',order = train['deck'].value_counts().index)\nplt.title('Passengers distribution by deck',fontsize= 16)\nplt.ylabel('Number of passengers')\nplt.legend(( 'Drowned', 'Survived'), loc=(0.85,0.89))\nplt.xticks(rotation = False)\n\n\nplt.show()","b94579bb":"#draw a bar plot for Parch vs. survival\nplt.figure(figsize=(8,6))\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train,ci=None)\nplt.show()","f44679f7":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train,ci=None)\nplt.show()","e8eb6897":"train['SibSp'].sort_values().unique()","e3ae8858":"print(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 3 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of SibSp = 4 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","8f524acb":"plt.subplots(figsize=(8,6))\n\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'],color='r',shade=True,label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'],color='b',shade=True,label='Survived' )\nplt.title('Fare Distribution Survived vs Non Survived')\nplt.ylabel('Frequency of Passenger Survived')\nplt.xlabel('Fare')\nplt.show()","6b276f51":"sns.catplot(x=\"Pclass\", y=\"Fare\",hue='Survived', kind=\"swarm\", data=train)\nplt.show()","ecbfbabf":"train['Embarked'].unique()","bd4651ce":"train['Embarked'].describe()","f27df66b":"train['Embarked'] = train['Embarked'].replace('Unknown','S')","fa46a461":"sns.countplot(train.Embarked)\nlabels = (train['Embarked'].value_counts())\n","632244a1":"plt.figure(figsize=(10,6))\nsns.countplot(train['Embarked'],hue='Survived',data=train)\nplt.legend(( 'Drowned', 'Survived'), loc=(0.85,0.89))\nplt.show()","3d140568":"train.head()","ee3d5e8f":"total_data=train.append(test)","cc71ee0a":"total_data.head()","8f643b59":"total_data.shape","7c7d26db":"total_data['Sex'] =total_data['Sex'].replace('male',0)\ntotal_data['Sex'] =total_data['Sex'].replace('female',1)\ntotal_data['Embarked'] =total_data['Embarked'].replace('S',0)\ntotal_data['Embarked'] = total_data['Embarked'].replace('Q',1)\ntotal_data['Embarked'] = total_data['Embarked'].replace('C',2)","9617ad30":"mapping = {'Mlle': 'Miss', 'Major': 'Rare', 'Col': 'Rare', 'Sir': 'Rare', 'Don': 'Rare', 'Mme': 'Mrs',\n           'Jonkheer': 'Rare', 'Lady': 'Rare', 'Capt': 'Rare', 'Countess': 'Rare', 'Ms': 'Miss', 'Dona': 'Mrs', 'Rev':'Rare', 'Dr':'Rare'}\n\ntotal_data.replace({'Title': mapping}, inplace=True)\n\ntotal_data['Title'].value_counts(normalize=True)*100","9f02fbe9":"total_data['Title'] = total_data['Title'].map({'Mr':0, 'Miss':1, 'Mrs':2, 'Master':3, 'Rare':4})\ntotal_data['Title'].fillna(total_data['Title'].median(),inplace=True)","66f6db85":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\ntotal_data['deck'] = total_data['deck'].map(cabin_category)\n","6bfa6320":"total_data['Family_size'] = total_data['SibSp'] + total_data['Parch'] + 1\n","67a721ea":"total_data['Alone'] = 1\ntotal_data['Alone'].loc[total_data['Family_size'] > 1] = 0","5847ef95":"bins = [-1, 0, 18, 25, 35, 60, np.inf]\nlabels = ['Unknown', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\ntotal_data['AgeGroup'] = pd.cut(total_data[\"Age\"], bins, labels = labels)\nage_mapping = {'Unknown': None,'Child': 1, 'Teenager': 2, 'Young Adult': 3, 'Adult': 4, 'Senior': 5}\ntotal_data['AgeGroup'] = total_data['AgeGroup'].map(age_mapping)","fcab7f90":"fig,ax=plt.subplots(figsize=(14,6))\nsns.heatmap(total_data.corr(),annot=True,annot_kws={'size':12})","f9790d9c":"total_data.head()","289835b7":"total_data.isna().sum()","94dddffd":"features = ['Embarked','Fare','Pclass','Sex','Title','Family_size','Alone']","3116b48c":"#Modelos\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Metrics\nfrom sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n#Model Select\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","d92050cf":"df_train = total_data[0:891]\ndf_test =  total_data[891:]\nX = df_train[features]\ny = df_train['Survived'].astype(int)\n","82c315ba":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=78941)","192d1f3f":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)","849523ac":"logreg = LogisticRegression(solver= 'lbfgs',max_iter=400)\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\n","6297a83a":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = knn.predict(X_test) \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n","8ec9ecb6":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test) \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)","e1618c4d":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)","2f23bcb0":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)  \nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)","1a307a3c":"results = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              ' Support Vector Machine', \n              'Decision Tree'],\n    'Score': [ acc_knn, acc_log, \n              acc_random_forest, acc_gaussian,  \n              acc_linear_svc, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","580f87ee":"model= LogisticRegression(solver= 'lbfgs',max_iter=400)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n\ncm_logit = confusion_matrix(y_test, predictions)\nprint('Confusion matrix for Logistic\\n',cm_logit)\n\naccuracy_logit = accuracy_score(y_test,predictions)\nprecision_logit =precision_score(y_test, predictions)\nrecall_logit =  recall_score(y_test, predictions)\nf1_logit = f1_score(y_test, predictions)\nprint('accuracy_logistic : %.3f' %accuracy_logit)\nprint('precision_logistic : %.3f' %precision_logit)\nprint('recall_logistic : %.3f' %recall_logit)\nprint('f1-score_logistic : %.3f' %f1_logit)\nauc_logit = roc_auc_score(y_test,predictions)\nprint('AUC_logistic : %.2f' % auc_logit)","56bdf424":"randomForestFinalModel = RandomForestClassifier(random_state = 2, bootstrap=False,min_samples_split=2,min_samples_leaf= 5, criterion = 'entropy', max_depth = 13, max_features = 'sqrt', n_estimators = 200)\nrandomForestFinalModel.fit(X_train, y_train)\npredictions_rf = randomForestFinalModel.predict(X_test)\n\ncm_logit = confusion_matrix(y_test, predictions_rf)\nprint('Confusion matrix for Random Forest\\n',cm_logit)\n\naccuracy_logit = accuracy_score(y_test,predictions_rf)\nprecision_logit =precision_score(y_test, predictions_rf)\nrecall_logit =  recall_score(y_test, predictions_rf)\nf1_logit = f1_score(y_test,predictions_rf)\nprint('accuracy_random_Forest : %.3f' %accuracy_logit)\nprint('precision_random_Forest : %.3f' %precision_logit)\nprint('recall_random_Forest : %.3f' %recall_logit)\nprint('f1-score_random_Forest : %.3f' %f1_logit)\nauc_logit = roc_auc_score(y_test,predictions_rf)\nprint('AUC_random_Forest: %.2f' % auc_logit)\n","230ab6b7":"a=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nb=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nfig =plt.figure(figsize=(20,12),dpi=50)\nfpr, tpr, thresholds = roc_curve(y_test,predictions )\nplt.plot(fpr, tpr,color ='orange',label ='Logistic',linewidth=2 )\nfpr, tpr, thresholds = roc_curve(y_test,predictions_rf )\nplt.plot(fpr, tpr,color ='blue',label ='random Forest',linewidth=2 )\n\nplt.plot(a,b,color='black',linestyle ='dashed',linewidth=2)\nplt.legend(fontsize=15)\nplt.xlabel('False Positive Rate',fontsize=15)\nplt.ylabel('True Positive Rate',fontsize=15)","1b0f1fcf":"submission = pd.DataFrame({\n    \"PassengerId\": df_test[\"PassengerId\"],\n    \"Survived\": randomForestFinalModel.predict( df_test[features])\n})\n","d8c99588":"submission.head()","4e7d86cb":"submission.to_csv(\"titanic_s.csv\",index=False)","e19cbeda":"* People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","32ed3931":"* As predicted, females have a much higher chance of survival than males. ","39e65911":"* In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)","d533f0e3":"### Correlation is a statistical technique that can show whether and how strongly pairs of variables are related","56c07823":"*  Babies are more likely to survive than any other age group.","3d810b98":"## Bar Chart :\nThis bar chart gives you an idea about how many missing values are there in each column.","512315f1":"I would like to introduce one of the most popular algorithms for classification (but also regression, etc), Random Forest! In a nutshell, Random Forest is an ensembling learning algorithm which combines decision trees in order to increase performance and avoid overfitting.","5a1bdeeb":"As we see their is not any missing value","0a470819":"# 1.8 Embarked( Port of Embarkation )\n![](https:\/\/hainguyenau.github.io\/projects\/img\/titanic\/route.JPG)\nTitanic had 3 embarkation points before the ship started its route to New York:\n\n* Southampton\n* Cherbourg\n* Queenstown\n\nSome passengers could leave Titanic in Cherbourg or Queenstown and avoid catastrophe. Also, the point of embarkation could have an influence on ticket fare and location on the ship.","43b0d0b2":"Logistic Regression:","bf107cd8":"K Nearest Neighbor:","4893990f":"* We can observe that the distribution of prices for the second and third class is very similar. \n\n* The distribution of first-class prices is very different, has a larger spread, and on average prices are higher.","bc1089d0":"* We are going to deal missing value(in **Cabin** & **Embarked**) has categorical data by replace its by new category ie. 'unknown'","7de62832":" # **<h1 style=\"color:black\">Titanic<\/h1>**\n\n  ![](https:\/\/wallpapercave.com\/wp\/7wF5gkJ.jpg)\n   ## Introduction  \n   \nIn this notebook we examine the Titanic dataset and then we build a model that can predict if a passenger survived the sinking or not. We start with finding feature types, missing values and we continue with feature analysis and visualization of the data. Feature engineering is implemented to create new attributes, encoding and imputation of the missing values. At last we test several classifiers and we evaluate them with the help of the ROC and CAP curves.\n\n## History\n\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters.         \n  ","e6f42276":"# 1.4 Name ","e61c5904":"# Variables\nFrom the data overview of the competition, we have a description of each variable:\n* PassengerId - unique identifier\n* Survived:\n        0 = No\n        1 = Yes\n* Pclass: Ticket class\n        1 = 1st, Upper\n        2 = 2nd, Middle\n        3 = 3rd, Lower\n* Name: full name with a title\n* Sex: gender\n* Age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n* Sibsp: Number of siblings \/ spouses aboard the Titanic. The dataset defines family relations in this way:\n        Sibling = brother, sister, stepbrother, stepsister\n        Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n* Parch: Number of parents \/ children aboard the Titanic. The dataset defines family relations in this way:\n        Parent = mother, father\n        Child = daughter, son, stepdaughter, stepson\n        Some children travelled only with a nanny, therefore parch=0 for them.\n* Ticket: Ticket number.\n* Fare: Passenger fare.\n* Cabin: Cabin number.\n* Embarked: Port of Embarkation:\n        C = Cherbourg\n        Q = Queenstown\n        S = Southampton","d237eee7":"## Matrix:\nVisualising missing values for a sample of 150 Using this matrix you can very quickly find the pattern of missingness in the dataset.","dce8541f":"# Exploratory data analysis\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.","e6dec91c":"\n### What was the age of passengers, how it correlated with chances to survive\nWe have 263 missing values:\n\n* 177 missing in the training dataset(**which had filled by age mean value**)\n* 86 in the test dataset\nOverall age distribution (seaborn distplot) and descriptive statistics:","627431f5":"Checking Missing value is **present or not** in our dataset","58d2332f":"# 2. Feature Engineering\n![](https:\/\/img-a.udemycdn.com\/course\/750x422\/1304050_ee0f_8.jpg)","0e4cbe03":"Decision Tree","bf7c8acf":"## **Roc_curve**\n\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.","38ff0cb7":" Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other. Later on, we will use cross validation.","6d06437a":"# 1.3 Age \n![](https:\/\/lesleylyle.com\/wp-content\/uploads\/2017\/03\/Age-Democraphic.jpg)","a0b83150":"## 2.1 Creating Dummies Variables\n\nDummy variable is a categorical variable that has been transformed into numeric. For example the column Gender, we have \"male\" and \"female\" we will transform these variables into numeric. Creating a new column just for Men. and Women, where 1 will be set to positive and 0 to negative","f4d65b03":"Linear Support Vector Machine:","13d408c8":"As we can see, the Random Forest classifier goes on the first place. But first, let us check, how random-forest performs & Logistic_Regression","05c5ce0e":"## 1. Survivals( Survived (1) or died (0))","7e216d85":"* We have 891 passengers in train dataset, 549 (61,6%) of them drowned and only 342 (38,4%) survived.\n* more people died than survived (38% survived)\n\n\n\n\n\n\n","6a3b5b17":"## Heatmap\nThe missingno correlation heatmap measures nullity correlation: how strongly the presence or absence of one variable affects the presence of another:","4ccd2e59":"### Importing Libraries","ae8a2333":"* Most number of passengers  were embarked in Southampton. Also Southampton has the biggiest proportion of drowned passengers.\n\n* Passengers emarked in Cherbourg and more than 50% of them survived (in the training dataset).","c39ad7fb":"\n <center><h1 style=\"color:red\">Don't forget to upvote if you like it! :)<\/h1><\/center>","3af5c06a":"## **Let's submit our solutions**","85e15989":"# 1.7 Fare( Passenger Fare )\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-5ab46f31803d2242e89996144a228ab1.webp)","8448a456":"# Replacing With Mean\/Median\/mode\nMEAN: Suitable for continuous data without outliers\nMEDIAN : Suitable for continuous data with outliers\nMode: For categorical feature we can select to fill in the missing values with the most common value(mode) as illustrated below.","d3a83241":"* Most passengers don't have cabin numbers ('U').\n* The largest part of passengers with known cabin numbers were located on the 'C' deck . 'C' deck is fifth by a percentage of the survivor.\n* The largest surviving rate (among passengers with known cabin numbers in training dataset) had passengers from deck 'D'.","2ab163af":"# 1.6 SibSp( Number of Siblings\/Spouses Aboard )","04410e31":"* We are going to deal missing value(in **Age**) has numeric data by replace its **median value**","214c8a9e":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/d4\/Correlation_examples2.svg\/600px-Correlation_examples2.svg.png)","6dbd7f04":"Each passenger Name value contains the title of the passenger which we can extract and discover.\nTo create new variable \"Title\":\n\n1. I am using method 'split' by comma to divide Name in two parts and save the second part\n2. I am splitting saved part by dot and save first part of the result\n3. To remove spaces around the title I am using 'split' method\nTo visualize, how many passengers hold each title, I chose countplot.","5b04e92f":"**Missingno** library offers a very nice way to visualize the distribution of NaN values. Missingno is a Python library and compatible with Pandas.","1b882893":"Gaussian Naive Bayes:","4d238c00":"# Handle missing data\n![](https:\/\/miro.medium.com\/max\/763\/0*02gRztuKsDyEFQBB)","96a57202":"# 1.1 Sex \n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQNR1gaL8DqmzM9z_OTHHmng8P9ow3NDuLDcvA47Yv-ANlux3f8&usqp=CAU)","70b4dfd8":"# 1.5 Parch( Number of Parents\/Children Aboard )\n![](https:\/\/i.pinimg.com\/originals\/e5\/6d\/6b\/e56d6b2ca15caaf30d98da2c9a1ff081.jpg)","a35e0534":"## Which is the best Model ?","04c146ca":"## Hyperparameter Tuning\nBelow we set the hyperparameter grid of values with 4 lists of values:\n\n**'criterion'** : A function which measures the quality of a split.\n\n**'n_estimators'** : The number of trees of our random forest.\n\n**'max_features'** : The number of features to choose when looking for the best way of splitting.\n\n**'max_depth'** : the maximum depth of a decision tree.","a18da247":"So it clearly seems that,The survival of the people belong to 3rd class is very least. It looks like ...\n\n* 63% first class passenger survived titanic tragedy, while\n* 48% second class and\n* only 24% third class passenger survived.","56b58b27":"# **Building Machine Learning Models**","0e96a843":"## **Random_Forest Model**\n![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/random-forest-algorithm.png)\nRandom forest is a supervised learning algorithm which is used for both classification as well as regression. But however, it is mainly used for classification problems. As we know that a forest is made up of trees and more trees means more robust forest. Similarly, random forest algorithm creates decision trees on data samples and then gets the prediction from each of them and finally selects the best solution by means of voting. It is an ensemble method which is better than a single decision tree because it reduces the over-fitting by averaging the result.","fa5210bb":"# 1.4 Cabin  \n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/84\/Titanic_cutaway_diagram.png\/687px-Titanic_cutaway_diagram.png)","900ca2f3":"# 2.3 Feature selection\n\n![](https:\/\/miro.medium.com\/max\/1002\/1*68H8EsCwfqJNxzYdPYtEDw.png)\n\nWe will now select the features (X) for our model. These features will help our model identify patterns. The features will be columns.\n\n\"When feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential.\"","9bdb4612":"### Correlation","46c5147f":"Random Forest:","cb9006ca":"## Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\n* The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","4d3cf08c":"# 1.2 Pclss(Passenger\u2019s class )","2132349a":"## Logistic_Regression Model\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTOvSA2vtiTPKUjUl-yp5aNYlcAgrLz_2rX3camQuVYddefbxHE&usqp=CAU)\nLogistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.\n\nIn simple words, the dependent variable is binary in nature having data coded as either 1 (stands for success\/yes) or 0 (stands for failure\/no).","ced2f329":"## 2.2 Adding New Features and Filling the missing values","db1898da":"* From the number of the cabin we can extract first letter, which will tell us about placement of the cabin on the ship!\n\n* To the passengers without deck information I will imput U letter (as unknown).","d4f5a333":"In this case I will use the age that was provided from our dataset to create the groups to find out if the passenger was a child, youth, adult, etc. In this case we are doing a Feacture Engineer where we transform a column to get another one through it","b408c17b":"## **Dateset is completely ready now!**","6ad3fc3a":"# Contents \n1. Include Libraries\n2. Import DataSet\n3. Handle Missing Value\n4. EDA(Exploratory Data Analysis)\n5. Feature Engineering\n\nMachine learning Model\n6. Logistic Regression\n7. Random Forest Classifier\n8. ROC Curve\n9. Final Submittion","1e56749d":"Looks like the bigger passenger paid, the more chances to survive he had.","6f9e814e":"<center><h1 style=\"color:green\">Please Upvote!<\/h1><\/center>"}}