{"cell_type":{"a0f412f2":"code","fe9bd871":"code","801476d0":"code","50ca1196":"code","d7fda249":"code","81752a93":"code","8683093c":"code","3bfd7b5b":"code","12e17dd5":"code","4cb1da49":"code","51de91f9":"code","72be2f02":"code","7ceb111b":"code","81615b70":"code","214cb269":"code","bb8acae3":"code","8589eedf":"code","3e123bff":"code","58835d14":"code","58012519":"code","7f301dc6":"code","6f34b0d6":"code","3294f942":"code","9e8e0cf9":"code","4c5ad9e2":"code","e6151e28":"code","312fd714":"code","84f5b5e2":"code","70fbfbe8":"code","392fac86":"code","c2cd8449":"code","b62f4292":"code","55ac5bd3":"code","e821304f":"code","93635143":"code","61311286":"code","d8901d5c":"code","e434fea9":"code","e09b580f":"code","c58210a8":"code","ad2b8f23":"code","57a8c048":"code","7ee45d74":"code","b8043fae":"code","2771efa8":"code","a61fedcc":"code","05f6a66c":"code","10b38d53":"code","32b0e4a4":"code","eb85262d":"code","9eb87a11":"code","915d32f6":"code","3a1e4e72":"code","9dd4ed7e":"code","98975c26":"code","7e2040e0":"code","7c6e15fe":"markdown","adf97be5":"markdown","e660641a":"markdown","8e3ffb21":"markdown","ac6fde55":"markdown","f5c6956f":"markdown","e4b27750":"markdown","6a79f70e":"markdown"},"source":{"a0f412f2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.covariance import EllipticEnvelope\nimport seaborn as sns","fe9bd871":"data=pd.read_csv('\/content\/drive\/MyDrive\/part-088.csv')","801476d0":"data.head()","50ca1196":"#dimensions\ndata.shape","d7fda249":"#removing rows with constant values\n\ndef remove_constant_value_features(df):\n    return [e for e in df.columns if df[e].nunique() == 1]\ndata=data.drop(columns=remove_constant_value_features(data))","81752a93":"#null values\nnp.sum(np.sum(data.isna()))","8683093c":"#anomalies by hour\ntimedelta = pd.to_datetime(data['timestamp'])\ndata['Time_hour'] = (timedelta.dt.hour).astype(int)\n\nplt.figure(figsize=(12,5))\nsns.distplot(data[data['isAnomaly'] == 0][\"Time_hour\"], color='g')\nsns.distplot(data[data['isAnomaly'] == 1][\"Time_hour\"], color='r')\nplt.title('Fraud and Normal Transactions by Hours', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","3bfd7b5b":"target=data['isAnomaly']\ndata=data.drop(columns=['timestamp']) #dropping right now but will require in a later section","12e17dd5":"data['isAnomaly'].value_counts() #number of anomalies","4cb1da49":"#Feature Selection\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 0)\nrnd_clf.fit(data,target);\n\nnot_imp=[]\nfor name, importance in zip(data.columns, rnd_clf.feature_importances_):\n  if importance > 0.020 :\n    not_imp.append(name)\n\ndata=data.drop(columns=not_imp)","51de91f9":"list_of_tuples = list(zip(data.columns, rnd_clf.feature_importances_))\npd.DataFrame(list_of_tuples, columns = ['Columns', 'Importance']).sort_values(by='Importance', ascending=False)","72be2f02":"#Dropping high correlated columns\ncor_matrix=data.corr()\nupper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.95)]\ndata=data.drop(columns=to_drop)","7ceb111b":"factor=1723\/38797 #number of fraud cases","81615b70":"#Define the outlier detection methods\n\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(data), \n                                       contamination=factor,random_state=0, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=factor),\n    \"Support Vector Machine\":OneClassSVM(kernel='linear', degree=3, gamma=0.1,nu=0.05, \n                                         max_iter=-1),\n    \"Elliptic Envelope\":EllipticEnvelope(contamination=factor)\n   \n}","214cb269":"from sklearn.metrics import classification_report,accuracy_score\nn_outliers = 1723\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(data)\n        scores_prediction = clf.negative_outlier_factor_\n    elif clf_name == \"Support Vector Machine\":\n        clf.fit(data)\n        y_pred = clf.predict(data)\n    else:    \n        clf.fit(data)\n        scores_prediction = clf.decision_function(data)\n        y_pred = clf.predict(data)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != target).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(target,y_pred))\n    print(confusion_matrix(target, y_pred))","bb8acae3":"#Looking for clusters\nfrom sklearn.decomposition import PCA\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(data)\nplt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], alpha=.1, color='black')","8589eedf":"import tensorflow as tf\nimport random as rn\n# manual parameters\nRANDOM_SEED = 42\nVALIDATE_SIZE = 0.2\n\n# setting random seeds for libraries to ensure reproducibility\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","3e123bff":"from sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef tsne_scatter(features, labels, dimensions=2, save_as='graph.png'):\n    if dimensions not in (2, 3):\n        raise ValueError('tsne_scatter can only plot in 2d or 3d (What are you? An alien that can visualise >3d?). Make sure the \"dimensions\" argument is in (2, 3)')\n\n    # t-SNE dimensionality reduction\n    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n    \n    # initialising the plot\n    fig, ax = plt.subplots(figsize=(8,8))\n    \n    # counting dimensions\n    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n\n    # plotting data\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==1)]),\n        marker='o',\n        color='r',\n        s=2,\n        alpha=0.7,\n        label='Fraud'\n    )\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==0)]),\n        marker='o',\n        color='g',\n        s=2,\n        alpha=0.3,\n        label='Clean'\n    )\n\n    # storing it to be displayed later\n    plt.legend(loc='best')\n    plt.savefig(save_as);\n    plt.show;","58835d14":"tsne_scatter(data, target, dimensions=2, save_as='tsne_initial_2d.png')","58012519":"data=pd.concat([data,target],axis=1)","7f301dc6":"fraud = data[data.isAnomaly == 1]\nclean = data[data.isAnomaly == 0]","6f34b0d6":"print(f\"\"\"Shape of the datasets:\n    clean (rows, cols) = {clean.shape}\n    fraud (rows, cols) = {fraud.shape}\"\"\")","3294f942":"TRAINING_SAMPLE = 25952\n# shuffle our training set\nclean = clean.sample(frac=1).reset_index(drop=True)\n\n# training set: exlusively non-fraud transactions\nX_train = clean.iloc[:TRAINING_SAMPLE].drop('isAnomaly', axis=1)\n\n# testing  set: the remaining non-fraud + all the fraud \nX_test = clean.iloc[TRAINING_SAMPLE:].append(fraud).sample(frac=1)","9e8e0cf9":"print(f\"\"\"Our testing set is composed as follows:\n\n{X_test.isAnomaly.value_counts()}\"\"\")","4c5ad9e2":"from sklearn.model_selection import train_test_split\n\n# train \/\/ validate - no labels since they're all clean anyway\nX_train, X_validate,y_train, y_validate = train_test_split(X_train, \n                                       test_size=VALIDATE_SIZE, \n                                       random_state=RANDOM_SEED)\n\n# manually splitting the labels from the test df\nX_test, y_test = X_test.drop('isAnomaly', axis=1).values, X_test.isAnomaly.values","e6151e28":"print(f\"\"\"Shape of the datasets:\n    training (rows, cols) = {X_train.shape}\n    validate (rows, cols) = {X_validate.shape}\n    holdout  (rows, cols) = {X_test.shape}\"\"\")","312fd714":"from sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# configure our pipeline\npipeline = Pipeline([('normalizer', Normalizer()),\n                     ('scaler', MinMaxScaler())])","84f5b5e2":"pipeline.fit(X_train);","70fbfbe8":"X_train_transformed = pipeline.transform(X_train)\nX_validate_transformed = pipeline.transform(X_validate)","392fac86":"g = sns.PairGrid(X_train.iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Before:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","c2cd8449":"g = sns.PairGrid(pd.DataFrame(X_train_transformed).iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('After:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","b62f4292":"input_dim = X_train_transformed.shape[1]\nBATCH_SIZE = 256\nEPOCHS = 100\n\nautoencoder = tf.keras.models.Sequential([\n    \n    # deconstruct \/ encode\n    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )), \n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(2, activation='elu'),\n    \n    # reconstruction \/ decode\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(input_dim, activation='elu')\n    \n])\n\nautoencoder.compile(optimizer=\"adam\", \n                    loss=\"mse\",\n                    metrics=[\"acc\"])\n\n# print an overview of our model\nautoencoder.summary();","55ac5bd3":"from datetime import datetime\n\n# current date and time\nyyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n\n# new folder for a new run\nlog_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(autoencoder.layers)}'\n\n# define our early stopping\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001,\n    patience=10,\n    verbose=1, \n    mode='min',\n    restore_best_weights=True\n)\n\nsave_model = tf.keras.callbacks.ModelCheckpoint(\n    filepath='autoencoder_best_weights.hdf5',\n    save_best_only=True,\n    monitor='val_loss',\n    verbose=0,\n    mode='min'\n)\n\ntensorboard = tf.keras.callbacks.TensorBoard(\n    f'logs\/{log_subdir}',\n    batch_size=BATCH_SIZE,\n    update_freq='batch'\n)\n\n# callbacks argument only takes a list\ncb = [early_stop, save_model, tensorboard]","e821304f":"history = autoencoder.fit(\n    X_train_transformed, X_train_transformed,\n    shuffle=True,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=cb,\n    validation_data=(X_validate_transformed, X_validate_transformed)\n);","93635143":"# transform the test set with the pipeline fitted to the training set\nX_test_transformed = pipeline.transform(X_test)\n\n# pass the transformed test set through the autoencoder to get the reconstructed result\nreconstructions = autoencoder.predict(X_test_transformed)","61311286":"# calculating the mean squared error reconstruction loss per row in the numpy array\nmse = np.mean(np.power(X_test_transformed- reconstructions, 2), axis=1)","d8901d5c":"THRESHOLD = 3\n\ndef mad_score(points):\n    \n    m = np.median(points)\n    ad = np.abs(points - m)\n    mad = np.median(ad)\n    \n    return 0.6745 * ad \/ mad\n\nz_scores = mad_score(mse)\noutliers = z_scores > THRESHOLD","e434fea9":"print(f\"Detected {np.sum(outliers):,} outliers in a total of {np.size(z_scores):,} operations [{np.sum(outliers)\/np.size(z_scores):.2%}].\")","e09b580f":"from sklearn.metrics import (confusion_matrix, \n                             precision_recall_curve)\n\n# get (mis)classification\ncm = confusion_matrix(y_test, outliers)\n\n# true\/false positives\/negatives\n(tn, fp, \n fn, tp) = cm.flatten()","c58210a8":"print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n{cm}\n\n% of transactions labeled as fraud that were correct (precision): {tp}\/({fp}+{tp}) = {tp\/(fp+tp):.2%}\n% of fraudulent transactions were caught succesfully (recall):    {tp}\/({fn}+{tp}) = {tp\/(fn+tp):.2%}\"\"\")","ad2b8f23":"# Accuracy\n(9701\/12845)*100","57a8c048":"!pip install fbprophet\nfrom fbprophet import Prophet\nimport os","7ee45d74":"# View the data as a table\ndf_ = pd.DataFrame(data, columns=['timestamp', r'Available db connection activity : (d\/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'])\ndf_['ds']=df_['timestamp']\ndf_['y']=df_[r'Available db connection activity : (d\/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'].astype(float)\ndf_=df_.drop(['timestamp',r'Available db connection activity : (d\/dx (MXBean(com.bea:Name=source09,Type=JDBCDataSourceRuntime).NumAvailable))'],axis=1)\ndf_.head()","b8043fae":"def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n    m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n#                 seasonality_mode = 'multiplicative', \n                interval_width = interval_width,\n                changepoint_range = changepoint_range)\n    m = m.fit(dataframe)\n    \n    forecast = m.predict(dataframe)\n    forecast['fact'] = dataframe['y'].reset_index(drop = True)\n    print('Displaying Prophet plot')\n    fig1 = m.plot(forecast)\n    return forecast\n    \npred = fit_predict_model(df_)","2771efa8":"def detect_anomalies(forecast):\n    forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n    #forecast['fact'] = df['y']\n\n    forecasted['anomaly'] = 0\n    forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n    forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = 1 #-1\n\n    #anomaly importances\n    forecasted['importance'] = 0\n    forecasted.loc[forecasted['anomaly'] ==1, 'importance'] = \\\n        (forecasted['fact'] - forecasted['yhat_upper'])\/forecast['fact']\n    forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] = \\\n        (forecasted['yhat_lower'] - forecasted['fact'])\/forecast['fact']\n    \n    return forecasted\n\npred = detect_anomalies(pred)","a61fedcc":"pred.head()","05f6a66c":"pred[ r'anomaly'].value_counts()","10b38d53":"#Accuracy\n1361\/1723","32b0e4a4":"#Handling imbalance\nfrom imblearn.under_sampling import NearMiss\n\nnm = NearMiss()\n\nx_nm, y_nm = nm.fit_resample(data, target)","eb85262d":"print(x_nm.shape,y_nm.shape)","9eb87a11":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nx_scaled = scalar.fit_transform(x_nm)","915d32f6":"from sklearn.model_selection import train_test_split, cross_val_score\nx_train,x_test,y_train,y_test = train_test_split(x_scaled,y_nm, test_size = 0.25)","3a1e4e72":"scores = {}\nacc = []\ncv_scores = []\ndef model(model):\n    model.fit(x_train,y_train)\n    score = model.score(x_test,y_test)\n    print(\"Accuracy: {}\".format(score))\n    cv_score = cross_val_score(model,x_train,y_train,cv=5)\n    print(\"Cross Val Score: {}\".format(np.mean(cv_score)))\n    acc.append(score)\n    cv_scores.append(np.mean(cv_score))","9dd4ed7e":"from xgboost import XGBClassifier\nclf = XGBClassifier()\nmodel(clf)\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nmodel(clf)\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier()\nmodel(clf)\nfrom sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nmodel(clf)\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\nmodel(clf)\nfrom sklearn.svm import SVC\nclf = SVC()\nmodel(clf)\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nmodel(clf)\nfrom sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier()\nmodel(clf)\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier()\nmodel(clf)","98975c26":"models = [\"XGBClassifier\",\"LogisticRegression\",\"RandomForestClassifier\",\"DecisionTreeClassifier\",\"KNeighborsClassifier\",\"SVC\",\"GaussianNB\",\"AdaBoostClassifier\",\"GradientBoostingClassifier\"]\nscores = { \"Model Name\" : models , \"Accuracy Score\" : acc, \"Cross val Score\": cv_scores}\ndf1 = pd.DataFrame(scores)","7e2040e0":"df1","7c6e15fe":"### Auto Outlier Detection Algorithms","adf97be5":"### Supervised","e660641a":"Anomalies aren't apparent","8e3ffb21":"Peak in fraud transcations at 3 pm","ac6fde55":"### Data Preparation","f5c6956f":"### FBProphet","e4b27750":"We can tell the data is slightly more uniform and proportionally distributed. \nThe ranges were also shrunk to fit between 0 and 1.","6a79f70e":"### Auto-Encoders"}}