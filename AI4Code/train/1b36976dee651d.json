{"cell_type":{"f2d8f3b2":"code","df950e4f":"code","07113626":"code","44de009a":"code","3ed5b9f0":"code","9aeea07f":"code","61420601":"code","391b9044":"code","7d0bb119":"code","b6e07bdb":"code","87dd4d9d":"code","9d4771e5":"code","20ea8e61":"code","589ac346":"code","6f45302a":"code","acd7d17e":"code","ad83a105":"code","21ac6ec7":"code","7997c5b4":"code","223e44e1":"code","bd8296ab":"code","e5dd36c3":"code","54068724":"code","c7977641":"code","9025be1d":"code","7063545d":"code","c6c4b394":"code","d00f2edb":"code","d155ef2a":"code","73d92b4e":"code","752d9a58":"code","fe0a56cd":"code","eaed8362":"code","1a485f98":"code","cfaa6cd1":"code","ba16749f":"code","8321e3d4":"code","482820a4":"code","02737195":"code","2be49be9":"code","751bf247":"code","67f21862":"code","b0ba4afc":"code","2d7ebdd5":"code","a9fb298b":"code","2ea1613f":"code","3d3368dc":"code","6ec2c957":"code","e5086945":"code","31b5b457":"code","10b515af":"code","b96edbd7":"markdown","ed639bde":"markdown","242108cd":"markdown","2588af3b":"markdown","e0658bde":"markdown","bd06e1b7":"markdown","d5ba45b8":"markdown","3c03ecf7":"markdown","e4535b63":"markdown","b6276fff":"markdown","f4596c73":"markdown","853e053e":"markdown"},"source":{"f2d8f3b2":"\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib as mp\nimport scipy\nimport scipy.stats\nimport tensorflow as tf\n#import tensorflow_hub as hub\nimport json\nimport pickle\nimport urllib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\n\nprint(tf.__version__)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom wordcloud import WordCloud, STOPWORDS\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df950e4f":"\n\npath = os.path.join(dirname, filename)\ndf = pd.read_csv(path) \n\n","07113626":"df","44de009a":"\n\ndf.columns\n\n","3ed5b9f0":"data=df.Text","9aeea07f":"\n\ndf.head()\n\n","61420601":"wc=WordCloud(width=200,height=100,background_color='black',stopwords=STOPWORDS\n            ).generate(str(data))","391b9044":"\n\nfig=plt.figure(figsize=(10,10),facecolor='k',edgecolor='w')\nplt.imshow(wc,interpolation='bicubic')\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()\n\n","7d0bb119":"\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport string\nimport gensim\nfrom gensim import corpora\n\n","b6e07bdb":"stop = set(stopwords.words('english'))\n\nexclude = set(string.punctuation)\n\nlemmma= WordNetLemmatizer() #base word conversion for bbetter tuning and performance\n\ndef clean(doc):\n    stop_free=\" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free=\"\".join([char for char in stop_free if char not in exclude])\n    normalisation = \" \".join(lemmma.lemmatize(word) for word in punc_free.split(' '))\n    stop_free1=\" \".join([i for i in normalisation.lower().split() if i not in ['like','know','go','ah','okay','ok','so','thats','there','right','no','good','think','yes','yeah','thing','stuff','this','it','that','the']])\n    return stop_free1\n\ndocument=df.Text.to_list()\n\n\ndoc_clean=[clean(docu).split() for docu in document ]\n\ndoc_clean[:10]","87dd4d9d":"\n\ndf['text_clean']=pd.Series(doc_clean)\n\n","9d4771e5":"\n\ndf.head()\n\n","20ea8e61":"dictionary=corpora.Dictionary(doc_clean)\n\nprint(dictionary)","589ac346":"\n\ndoc_word_freqcies=[dictionary.doc2bow(term) for term in doc_clean]\ndoc_word_freqcies[:30]\n\n","6f45302a":"\n\nfrom gensim.models import LdaModel\n\n","acd7d17e":"model=LdaModel(doc_word_freqcies,num_topics=4,id2word=dictionary,passes=800) ","ad83a105":"types= model.show_topics()\nfor t in types:\n    print(t)\n    print('----------------')","21ac6ec7":"diction={}\nfor i in range(4):\n    words=model.show_topic(i,topn=20)\n    #print(words)\n    diction[\"Topic number\" + \"{}\".format(i)]=[i[0] for i in words]\n    \n    \npd.DataFrame(diction)","7997c5b4":"\n\nimport pyLDAvis.gensim\n\n","223e44e1":"Vis=pyLDAvis.gensim.prepare(model,doc_word_freqcies,dictionary,sort_topics=False)\n\n","bd8296ab":"\n\npyLDAvis.display(Vis)\n\n","e5dd36c3":"\n\n# Create Corpus: Term Document Frequency\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n\n","54068724":"def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=doc_clean)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","c7977641":"\n\nvc=df_dominant_topic.Dominant_Topic.value_counts()\nvc\n\n","9025be1d":"\n\n#topic 2 and 4  is \n#topic 3 is\n#topic 5 IS \n#topic 1 is\ndic={1.0:\"2\",2.0:\"3\",3.0:\"4\",0.0:'1'}#,4.0:\"5\",0.0:'1'}#,5.0:\"6\",6.0:'7',0.0:\"1\",8.0:'9',7.0:'8'}\nvc=df_dominant_topic.Dominant_Topic.value_counts()\n\n","7063545d":"dt=df_dominant_topic[[\"Dominant_Topic\"]]","c6c4b394":"\n\ndt\n\n","d00f2edb":"dt.Dominant_Topic=dt.Dominant_Topic.apply(lambda row: dic[row])","d155ef2a":"\n\npd.DataFrame(dt.Dominant_Topic.value_counts())\n\n","73d92b4e":"\n\n# lda is assumed to be the variable holding the LdaModel object\nimport matplotlib.pyplot as plt\nfor t in range(model.num_topics):\n    plt.figure()\n#   plt.imshow(WordCloud().fit_words(model.show_topic(t, 200)))\n    plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n    plt.axis(\"off\")\n    plt.title(\"Topic #\" + str(t))\n    plt.show()\n\n","752d9a58":"from collections import OrderedDict\n\ndata_lda = {i: OrderedDict(model.show_topic(i,25)) for i in range(4)}\n#data_lda","fe0a56cd":"import pandas as pd\n\ndf_lda = pd.DataFrame(data_lda)\nprint(df_lda.shape)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","eaed8362":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ng=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine'\n                 , linewidths=.75, figsize=(10, 10))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis","1a485f98":"\n\nfrom textblob import TextBlob\ndf['pop']=df.text_clean.apply(lambda tw: TextBlob(' '.join(tw)).sentiment.polarity)\ndf\n\n","cfaa6cd1":"df['sent']=df['pop'].apply(lambda p: 'positive' if p>0 else  \n                           ( 'negative' if p<0  else 'neutral'))","ba16749f":"df","8321e3d4":"plt.figure(figsize=(10, 10))\ndf['Time']=pd.to_datetime(df.Timestamp.str.strip(':'),format='[%H:%M:%S]')\n\ndf['Timest']=df.Time.apply(lambda x: x.hour*60 + x.second + x.minute*60)\nsns.scatterplot(x=df['Timest'], y=df['pop'],data=df,hue=df['sent']);","482820a4":"\n\ndf['mint']=df.Time.apply(lambda x:x.minute)\ndf.groupby('mint')['pop'].describe()['mean'].plot(figsize=(10,10))\n\n","02737195":"\n\ndf['hr']=df.Time.apply(lambda x:x.hour)\ndf.groupby('hr')['pop'].describe()['mean'].plot(figsize=(10,10))\n\n","2be49be9":"\n\nprint(df.sent.value_counts())\nsns.countplot(x='sent', data = df);\n\n","751bf247":"plt.figure(figsize=(10,6))\nsns.distplot(df['pop'], bins=30)\nplt.title('Sentiment Distribution',size = 10)\nplt.xlabel('Polarity',size = 10)\nplt.ylabel('Frequency',size = 10)\nplt.show();","67f21862":"count = df.groupby(['min','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount","b0ba4afc":"count = df.groupby(['hr','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount","2d7ebdd5":"times = count.hr.unique()\nneut=[]\nfor hr in count.hr.unique():\n    a=count[count['hr']==hr]\n    if len(list(a[a['sent']=='neutral'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='neutral']['text_clean'])[0]\n        neut.append(c)\n    else:\n        neut.append(0)\npos = count.loc[count['sent'] == 'positive']['text_clean'].reset_index(drop = True)\n\nneg=[]\nfor hr in count.hr.unique():\n    a=count[count['hr']==hr]\n    if len(list(a[a['sent']=='negative'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='negative']['text_clean'])[0]\n        neg.append(c)\n    else:\n        neg.append(0)                        \nplt.figure(figsize=(10,10))\nplt.xticks(rotation='45')\nlin1=plt.plot(times, pos, 'ro-', label='positive')\nlin2=plt.plot(times, neut, 'g^-', label='neutral')\nlin3=plt.plot(times, neg, 'b--', label='negative')\nplt.legend()\nplt.show","a9fb298b":"count = df.groupby(['mint','sent'])['text_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount\n\n\ntimes = count.mint.unique()\nneut=[]\nfor mint in count.mint.unique():\n    a=count[count['mint']==mint]\n    if len(list(a[a['sent']=='neutral'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='neutral']['text_clean'])[0]\n        neut.append(c)\n    else:\n        neut.append(0)\npos = count.loc[count['sent'] == 'positive']['text_clean'].reset_index(drop = True)\n\nneg=[]\nfor mint in count.mint.unique():\n    a=count[count['mint']==mint]\n    if len(list(a[a['sent']=='negative'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='negative']['text_clean'])[0]\n        neg.append(c)\n    else:\n        neg.append(0)                        \nplt.figure(figsize=(10,10))\nplt.xticks(rotation='45')\nlin1=plt.plot(times, pos, 'ro-', label='positive')\nlin2=plt.plot(times, neut, 'g^-', label='neutral')\nlin3=plt.plot(times, neg, 'b--', label='negative')\nplt.legend()\nplt.show","2ea1613f":"\n\ndf_dominant_topic.Dominant_Topic.shape\n\n","3d3368dc":"df['topic']=df_dominant_topic.Dominant_Topic","6ec2c957":"\n\ndf.columns\n\n","e5086945":"df_nlp=df[['Time','hr','mint','topic','pop','sent']]","31b5b457":"\n\nprint(df_nlp.topic.value_counts())\nsns.countplot(x='topic', data = df_nlp);\n\n","10b515af":"\n\nsns.heatmap(df_nlp.corr())\n\n","b96edbd7":"## create LDA Model in gensim","ed639bde":"## overall wordcloud","242108cd":"## cleanup of punctuations fillers stopwords etc","2588af3b":"## visualisation of heatmap between words","e0658bde":"## Word cloud of topics and add the topics into dataframe","bd06e1b7":"## create document for processing with word frequencies corpus","d5ba45b8":"## visualize topics with word distribution \ntopics found\nAI,\nCars,\nFlamehtrower,\nEnergy, solar and tunnel","3c03ecf7":"## Analyze the types of topics","e4535b63":"## visualisation of sentiment","b6276fff":"## we will also do some sentiment analysis","f4596c73":"## polarity went up with time in the podcast","853e053e":"## import the dataframe \n"}}