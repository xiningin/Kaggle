{"cell_type":{"4997c846":"code","03aa1885":"code","08cbccb2":"code","ec8213dd":"code","6f3ddf6e":"code","7b0f42d0":"code","a6d40dda":"code","b64096ab":"code","56ad02a6":"code","6918a06b":"code","17cdacbe":"code","acd52da3":"code","6c61a6c0":"code","4b4062af":"code","361a078f":"code","a15417bf":"code","71e7239e":"code","9293e259":"code","164895f4":"code","37bae468":"code","e1fe06b4":"code","59f0c608":"code","8a484ebb":"code","d1225b26":"code","64a68e2c":"code","a0a38625":"code","60d9d6bd":"code","31ce7083":"code","93022317":"code","2e037b1c":"code","c87fc2cb":"code","dd31d51b":"code","efe166d2":"code","2f58173b":"code","d3213af7":"code","ec645f9a":"code","68b33cf3":"code","01d334a7":"markdown","f622d30d":"markdown","2d342c2b":"markdown","0fe2b4fd":"markdown","22075ab0":"markdown","bfe4e759":"markdown","fd0ac324":"markdown","60a6811d":"markdown"},"source":{"4997c846":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1{\n    text-align: center;\n    background-color: #F999B7;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 80px;\n}\n\nh2 {\n    text-align: center;\n    background-color: #F9C5D5;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 80px;\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #F2789F;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 15px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: #EF2F88;\n}\n<\/style>\n\"\"\")","03aa1885":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08cbccb2":"train_data=pd.read_csv('..\/input\/rotten-tomatoes\/train.tsv',delimiter='\\t')","ec8213dd":"train_data.head()","6f3ddf6e":"train_data.info","7b0f42d0":"train_data=train_data.drop(['PhraseId', 'SentenceId'],axis=1)","a6d40dda":"train_data.head()","b64096ab":"type(train_data['Phrase'][0])","56ad02a6":"lowercase_data=[]\nfor i in range (40000):\n  phrase = (train_data['Phrase'][i]).lower()\n  lowercase_data.append(phrase)\n","6918a06b":"lowercase_data[:2]","17cdacbe":"from nltk.tokenize import word_tokenize\nimport nltk\nnltk.download(\"punkt\")","acd52da3":"tokenized_data=[]\nfor phrase in lowercase_data:\n  li=word_tokenize(phrase)\n  tokenized_data.append(li)\n","6c61a6c0":"tokenized_data[:2]","4b4062af":"no_punctuation_data=[]\nimport re\nfor li in tokenized_data:\n  clean=[]\n  for w in li:\n    res=re.sub(r'[^\\w\\s]',\"\",w)\n    if res!=\"\":\n      clean.append(res)\n  no_punctuation_data.append(li)","361a078f":"no_punctuation_data[:1]","a15417bf":"from nltk.corpus import stopwords\nnltk.download(\"stopwords\")\n ","71e7239e":"no_stopwords_data=[]\nfor phrase in no_punctuation_data:\n    clean=[]\n    for i in phrase: \n      if not i in stopwords.words(\"english\"):\n        clean.append(i)\n    no_stopwords_data.append(phrase)","9293e259":"no_stopwords_data[:2]","164895f4":"from nltk.stem.porter import PorterStemmer\nstemmer=PorterStemmer()\n\nstem_data=[]\nfor li in no_punctuation_data:\n  clean=[]\n  for word in li:\n    w=stemmer.stem(word)\n    clean.append(w)\n  stem_data.append(clean)\n","37bae468":"stem_data[:1]","e1fe06b4":"from nltk.stem.wordnet import WordNetLemmatizer\nnltk.download(\"wordnet\")\nlem = WordNetLemmatizer()\n\nlemmatized_data=[]\nfor li in stem_data:\n  clean=[]\n  for words in li:\n    w=lem.lemmatize(words)\n    clean.append(w)\n  lemmatized_data.append(clean)","59f0c608":"lemmatized_data[:4]","8a484ebb":"import gensim","d1225b26":"df=pd.read_json('..\/input\/musical-instruments-json-file\/Musical_Instruments_5.json',lines=True)","64a68e2c":"df.shape","a0a38625":"df.head()","60d9d6bd":"review_text=df.reviewText.apply(gensim.utils.simple_preprocess)","31ce7083":"review_text[:12]","93022317":"model=gensim.models.Word2Vec(\nwindow=10,\nmin_count=2,\nworkers=4)","2e037b1c":"model.build_vocab(review_text)","c87fc2cb":"model.epochs","dd31d51b":"model.corpus_count","efe166d2":"model.train(review_text,total_examples=model.corpus_count,epochs=model.epochs)","2f58173b":"model.wv.most_similar(\"bad\")","d3213af7":"model.wv.similarity(w1=\"model\",w2=\"product\")","ec645f9a":"vector=model.wv[\"good\"]","68b33cf3":"vector","01d334a7":"# NATURAL LANGUAGE PROCESSING\n#### 1. **WHAT IS NLP?**\nNatural Language Processing (NLP) is a branch of artificial intelligence that is concerned with giving computers the ability to understand the text and spoken words in pretty much the same way human beings can.\n#### 2. **USE CASES OF NLP** :- \n a. Auto-complete: In search engines (Google,Yahoo, Bing).\n \n b. Machine Translation: (e.g. Google Translate, Ms Translator). \n \n c. Spell checking: This works everywhere, in your browser, in your IDE (Visual Studio), desktop apps (Microsoft Word).\n\n d. NLP enables the recognition and prediction of diseases based on electronic health records and patient\u2019s own speech.\n\n e. Personal assistants: Cortana, Siri, and Google Assistant.\n\n f. Sentiment Analysis of Product Reviews\n \n g. Spam Classification of emails\n\n h. Text Summarization\n \n#### 3. **How Does NLP WORK?**\nIn natural language processing, human language is separated into fragments so that the grammatical structure of sentences and themeaning of words can be analyzed and understood in context.NLP combines computational linguistics\u2014rule-based modeling of humanlanguage\u2014with statistical, machine learning, and deep learning models.\n\n### THIS NOTEBOOK IS A NATURAL LANGUAGE PROCESSING TUTORIAL","f622d30d":"## STEP 2 :- Tokenization: converting data into tokens is very important so as to make data simplified for processing.","2d342c2b":"# Word2Vector","0fe2b4fd":"## STEP 4 :- Remove Stopwords: stopwords are words that do not add any meaning to the context and make the processing complex. So therefore we remove such words that do not add very much information to our data.","22075ab0":"## STEP 1 :-  Converting to lowercase: It is very important that we bring uniformity to our dataset so that our machines don\u2019t treat the capital and lowercase differently.","bfe4e759":"## STEP 6:- Lemmatization: It is an extension of stemming. Sometimes during stemming the word loses its meaning. For instance, the word \u2018university\u2019 after stemming might turn into \u2018universe\u2019, and therefore it loses its original meaning. So therefore in such cases, we perform lemmatization where we convert it into our root word by referring to the dictionary so that it doesn't lose its meaning.","fd0ac324":"## STEP 5 :- Stemming: Stemming is the process of converting the words to their root form. Let\u2019s take an example to understand it. The words\u2019 going\u2019, \u2018gone\u2019,\u2019goes\u2019 are the stem of the root word of \u2018go\u2019.We do so that our machine treats all these stem words with the same significance.","60a6811d":"## STEP 3 :- Removing punctuation Tags: punctuation tags make the data unnecessarily complicated and add meaningless significance to our textual data which makes the performance of the model poor. So it is wise to remove them and as a result, we get a set of clean text which makes the processing easy."}}