{"cell_type":{"b1ed4223":"code","7646fbeb":"code","fce8cf6f":"code","522a3b12":"code","02c2cf17":"code","bad12f3b":"code","34737363":"code","6abd8cc6":"code","7a8ceced":"code","93b7726a":"code","90a72f6e":"code","84e2d899":"code","ac2a1bc1":"code","1c3ed4ee":"markdown","bb17e6b6":"markdown","6f3130cc":"markdown","fa669d33":"markdown","e28585b3":"markdown","d1cfb862":"markdown","5ee6f5be":"markdown","78c082bc":"markdown","6f6a0838":"markdown"},"source":{"b1ed4223":"import numpy as np \nimport pandas as pd\nimport os\nimport re\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport keras.backend as K\nimport matplotlib.pyplot as plt\n\nfrom nltk.tokenize  import TweetTokenizer\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#MAX_FEATURES = 10000\nNUM_FEATURES = 2500\nMIN_NGRAM_RANGE = 1\nMAX_NGRAM_RANGE = 1","7646fbeb":"main_dir = '\/kaggle\/input\/nlp-getting-started'\ntrain_filename = \"train.csv\"\ntest_filename = \"test.csv\"\n\ntrain_df = pd.read_csv(os.path.join(main_dir,train_filename))\ntest_df = pd.read_csv(os.path.join(main_dir,test_filename))","fce8cf6f":"train_df.head()","522a3b12":"test_df.head()","02c2cf17":"tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n\ndef custom_preprocessor(doc):\n    \"\"\"Preprocess text before tokenization.\"\"\"\n    \n    doc = re.sub(r\"http\\S+\", \"this_was_an_url\", doc).lower()\n    doc = doc.replace(\"#\",\"\")\n    doc = doc.replace(\"%20\",\" \")\n    return doc\n\ndef custom_tokenizer(doc):\n    \"\"\"Tokenize text into list of words.\"\"\"\n    \n    tokenized_doc = tokenizer.tokenize(doc)\n    return tokenized_doc","bad12f3b":"all_texts = train_df[\"text\"].append(test_df[\"text\"]).reset_index(drop=True)\nvectorizer = CountVectorizer(analyzer='word', \n                             ngram_range=(MIN_NGRAM_RANGE, MAX_NGRAM_RANGE),\n                             preprocessor=custom_preprocessor,\n                             tokenizer=custom_tokenizer,\n                             #max_features=MAX_FEATURES\n                            )\nall_texts_vectorized = vectorizer.fit_transform(all_texts)\ntrain_texts = all_texts_vectorized[0:len(train_df)]\ntest_texts = all_texts_vectorized[len(train_df):]","34737363":"corr_df = pd.DataFrame(train_texts.todense(), columns=vectorizer.get_feature_names())\ncorr = corr_df.corrwith(train_df[\"target\"]).fillna(0).abs().sort_values()\nwords_with_highest_correlation = list(corr[-(NUM_FEATURES):].index)","6abd8cc6":"corr[-20:].plot.barh()","7a8ceced":"X = pd.DataFrame(train_texts.todense(), columns=vectorizer.get_feature_names())[words_with_highest_correlation]\ny = train_df[\"target\"]\nX_te = pd.DataFrame(test_texts.todense(), columns=vectorizer.get_feature_names())[words_with_highest_correlation]\n\nX.head()","93b7726a":"def f1(y_true, y_pred):\n    \"\"\"F1 score for Keras model.\n    \n    Copyright (c) 2018 Guglielmo Camporese.\n    \n    https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras\n    \n    \"\"\"\n    \n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","90a72f6e":"def build_model():\n    \"\"\"Build the model.\"\"\"\n    \n    model = Sequential([\n    Dense(64, input_shape=(NUM_FEATURES,),activation=\"relu\"),\n    Dense(1,activation=\"sigmoid\")\n    ])\n    \n    model.compile(optimizer=\"adam\",loss='binary_crossentropy', metrics=[f1])\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","84e2d899":"checkpoint = ModelCheckpoint('model.h5', monitor='val_f1', mode=\"max\", save_best_only=True)\nearlystopping = EarlyStopping(monitor='val_f1', min_delta=0, patience=10, verbose=0, mode='max', baseline=None, restore_best_weights=False)\nreducelronplateau = ReduceLROnPlateau(monitor='val_f1', factor=0.75, patience=5, verbose=0, mode='max', min_delta=0.0001, cooldown=0, min_lr=0)\n\ntrain_history = model.fit(\n    X, y,\n    validation_split=0.2,\n    epochs=50,\n    callbacks=[checkpoint, \n               earlystopping,\n               reducelronplateau],\n    batch_size=1500,\n    verbose=0\n)\nplt.plot(train_history.history[\"val_f1\"])\nplt.legend([\"val_f1\"])\nplt.show()\nplt.plot(train_history.history[\"val_loss\"])\nplt.legend([\"val_loss\"])\nplt.show()\n\nprint(\"Epochs Trained:\", len(train_history.history[\"val_f1\"]))\nprint(\"Best F1-Score:\", max(train_history.history[\"val_f1\"]))\nprint(\"Best Loss:\", min(train_history.history[\"val_loss\"]))","ac2a1bc1":"model.load_weights('model.h5')\n\nsubmission = pd.DataFrame()\nsubmission[\"id\"] = test_df[\"id\"]\nsubmission[\"target\"] = [int(el) for el in list(model.predict(X_te).round())]\nsubmission.to_csv(\"submission.csv\",index=False)\nprint(pd.read_csv(\"submission.csv\"))","1c3ed4ee":"# Data Import","bb17e6b6":"# Prediction","6f3130cc":"# Real or Not? NLP with Disaster Tweets\n\nPredicting disasters with tweet data by using significant word occurrences as feature.\n\nNote that I do not use the keyword or location column in this approach.","fa669d33":"# Train the model","e28585b3":"# Feature Selection - Identify the most significant expressions","d1cfb862":"# Data Preparation","5ee6f5be":"# Tokenize and count words in Tweet text","78c082bc":"# Build the Neural Network","6f6a0838":"# The 20 most significant expressions"}}