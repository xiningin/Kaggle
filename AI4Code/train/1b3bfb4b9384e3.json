{"cell_type":{"c6e3559e":"code","0634b615":"code","cd8bb30d":"code","8960fec4":"code","0dbf78f8":"code","de5aa1c2":"code","7579d6fd":"code","fb19e950":"code","a1e3b471":"code","c9848580":"code","d551383e":"code","59fcb771":"code","8ff8c15b":"code","55499334":"code","7d1dbb33":"code","69a7918e":"code","73f203ec":"code","b139849b":"code","3ddd2472":"code","6d459b1a":"code","93529195":"code","09cc3217":"code","43f28661":"code","9781048c":"code","1f05e37c":"code","bb03e335":"code","1717b948":"markdown","083c8f23":"markdown","7549aee7":"markdown","cf0e58fc":"markdown","cf59daf9":"markdown","b898c239":"markdown","0e593516":"markdown","f80ba2f4":"markdown","57da49f8":"markdown","5be3e015":"markdown","6a3794f7":"markdown","103ef64d":"markdown","3ebc7ed0":"markdown","f1d33dc0":"markdown","92678642":"markdown","29431199":"markdown","de666125":"markdown","2c18c4ec":"markdown"},"source":{"c6e3559e":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport warnings\nfrom sklearn.ensemble import BaggingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport os\nfrom pprint import pprint\n\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n# warnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)\n\nsns.set(style=\"ticks\")\nplt.rc('figure', figsize=(8, 5), dpi=100)\nplt.rc('axes', facecolor=\"#ffffff\", linewidth=0.4, grid=True, labelpad=8, labelcolor='#616161')\nplt.rc('patch', linewidth=0)\nplt.rc('xtick.major', width=0.2)\nplt.rc('ytick.major', width=0.2)\nplt.rc('grid', color='#9E9E9E', linewidth=0.4)\nplt.rc('text', color='#282828')\nplt.rc('savefig', pad_inches=0.3, dpi=300)","0634b615":"tr = pd.read_csv('..\/input\/train.csv')\nts = pd.read_csv('..\/input\/test.csv')","cd8bb30d":"tr.head(10)","8960fec4":"tr.shape, ts.shape","0dbf78f8":"train_null = tr.isna().sum()\nprint('Columns that contain missing values in training dataset')\ntrain_null[train_null > 0]\n\ntest_null = ts.isna().sum()\nprint('Columns that contain missing values in test dataset')\ntest_null[test_null > 0]","de5aa1c2":"tt = [tr, ts]\n\n# Filling in missing values for columns that contain missing values in both\n# training and test datasets\nfor df in tt:\n    df['LotFrontage'].fillna(df['LotFrontage'].mean(), inplace=True)\n    df['Alley'].fillna('No alley', inplace=True)\n    df['MasVnrArea'].fillna(df['MasVnrArea'].median(), inplace=True)\n    df['FireplaceQu'].fillna('No fireplace', inplace=True)\n    df['PoolQC'].fillna('No pool', inplace=True)\n    df['Fence'].fillna('No fence', inplace=True)\n    df['MiscFeature'].fillna('No', inplace=True)\n    df['GarageYrBlt'].fillna(1, inplace=True)\n    for a in ['MasVnrType', 'BsmtExposure']:\n        df[a].fillna(df[a].mode().iloc[0], inplace=True)\n    for a in ['BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2']:\n        df[a].fillna('No basement', inplace=True)\n    for a in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        df[a].fillna('No Garage', inplace=True)\n        \n# Some columns contain missing values in only one of the two datasets\ntr['Electrical'].fillna(tr['Electrical'].mode().iloc[0], inplace=True)\n\nfor a in ['Utilities', 'MSZoning', 'Exterior1st', 'Exterior2nd', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual',\n         'Functional', 'GarageCars', 'SaleType']:\n    ts[a].fillna(ts[a].mode().iloc[0], inplace=True)\n\nfor a in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'GarageArea']:\n    ts[a].fillna(ts[a].median(), inplace=True)\n\n# GarageYrBlt_years = tr['GarageYrBlt'].value_counts().index.tolist()\n# GarageYrBlt_prob = tr['GarageYrBlt'].value_counts(normalize=True).values.tolist()\n# for df in tt:\n#     df['GarageYrBlt'] = df['GarageYrBlt'].apply(lambda x: \n#                                                 np.random.choice(GarageYrBlt_years, p=GarageYrBlt_prob) \n#                                                 if (pd.isnull(x)) else x)","7579d6fd":"for df in tt:\n    df['MSSubClass'] = df['MSSubClass'].astype(str)\n    mp1 = {'Ex':4,'Gd':3,'TA':2,'Fa':1,'Po':0}\n    df['ExterQual'] = df['ExterQual'].map(mp1)\n    df['ExterCond'] = df['ExterCond'].map(mp1)\n    mp3 = {'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No basement':0}\n    df['BsmtQual'] = df['BsmtQual'].map(mp3)\n    df['BsmtCond'] = df['BsmtCond'].map(mp3)\n    df['BsmtExposure'] = df['BsmtExposure'].map({'Gd':4,'Av':3,'Mn':2,'No':1,'No basement':0})\n    mp2 = {'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'No basement':0}\n    df['BsmtFinType1'] = df['BsmtFinType1'].map(mp2)\n    df['BsmtFinType2'] = df['BsmtFinType2'].map(mp2)\n    df['HeatingQC'] = df['HeatingQC'].map(mp1)\n    df['CentralAir'] = df['CentralAir'].map({'Y':1,'N':0})\n    df['KitchenQual'] = df['KitchenQual'].map(mp1)\n    df['Functional'] = df['Functional'].map({'Typ':7,'Min1':6,'Min2':5,'Mod':4,'Maj1':3,'Maj2':2,'Sev':1,'Sal':0})\n    df['FireplaceQu'] = df['FireplaceQu'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No fireplace':0})\n    df['GarageFinish'] = df['GarageFinish'].map({'Fin':3,'RFn':2,'Unf':1,'No Garage':0})\n    df['GarageQual'] = df['GarageQual'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\n    df['GarageCond'] = df['GarageCond'].map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'No Garage':0})\n    df['PoolQC'] = df['PoolQC'].map({'Ex':4,'Gd':3,'TA':2,'Fa':1,'No pool':0})\n    df['Fence'] = df['Fence'].map({'GdPrv':4,'MnPrv':3,'GdWo':2,'MnWw':1,'No fence':0})\n    ","fb19e950":"tr['TotalSF'] = tr['TotalBsmtSF'] + tr['GrLivArea']\nts['TotalSF'] = ts['TotalBsmtSF'] + ts['GrLivArea']","a1e3b471":"tr.drop('Id', axis=1, inplace=True)\n# saving test dataset Ids for submission later\nts_ids = ts['Id']\nts.drop('Id', axis=1, inplace=True)","c9848580":"fig, ax = plt.subplots()\nax.scatter(x = tr['GrLivArea'], y = tr['SalePrice']);","d551383e":"# outlier_indexes = tr[(tr['GrLivArea'] > 4000) & (tr['SalePrice'] < 200000)].index\n# tr = tr.drop(outlier_indexes, axis=0)","59fcb771":"## Outlier removal\n\n# from pandas.api.types import is_numeric_dtype\n# from scipy import stats\n# for c in tr.columns.values:\n#     if is_numeric_dtype(tr[c]):\n#         z = np.abs(stats.zscore(tr[c]))\n#         s = tr[(z > 7) & (tr[c] > 1000)].shape[0]\n#         if s <= 5 and s > 0:\n#             tr.drop(tr[(z > 7) & (tr[c] > 1000)].index, inplace=True)\n\n# Or\n\n# for df in tt:\n#     for c in df.columns.values:\n#         if is_numeric_dtype(df[c]):\n#             Q1 = df[c].quantile(0.25)\n#             Q3 = df[c].quantile(0.75)\n#             IQR = Q3 - Q1\n#             tmp = df[(df[c] < (Q1 - 10 * IQR)) | (df[c] > (Q3 + 10 * IQR))].shape\n#             if tmp[0] > 0:\n#                 print(c)\n#                 print(tmp)\n#                 print()","8ff8c15b":"tr_X = tr.drop('SalePrice', axis=1)\ntr_y = tr['SalePrice']","55499334":"tr_X = pd.get_dummies(tr_X)\nts = pd.get_dummies(ts)\n# get names of columns that exist in tr_X but not in ts after applying get_dummies()\nts_diff_cols = set(tr_X.columns.values) - set(ts.columns.values)\ntr_diff_cols = set(ts.columns.values) - set(tr_X.columns.values)\n# add them to ts with values of zero\nfor c in ts_diff_cols:\n    ts[c] = 0\nfor c in tr_diff_cols:\n    tr_X[c] = 0\n# make sure that columns have the same order in tr_X and ts\ntr_X, ts = tr_X.align(ts, axis=1)\ntr_X_colnames = list(tr_X.columns.values)","7d1dbb33":"scaler = StandardScaler().fit(tr_X)\ntr_X = scaler.transform(tr_X)\nts = scaler.transform(ts)","69a7918e":"x_train, x_test, y_train, y_test = \\\n    train_test_split(tr_X, tr_y, test_size=0.25, random_state=3)","73f203ec":"def f(model, name):\n    '''\n    fits the model on x_train and y_train, then prints\n    the model prediction score on x_train and x_test\n    '''\n    print('-------- ', name, ' --------')\n    model.fit(x_train, y_train)\n    pred = model.predict(x_train)\n    score = np.sqrt(metrics.mean_squared_error(y_train, pred))\n    print('Training: ', score)\n    pred = model.predict(x_test)\n    all_pred.append(list(pred))\n    score = np.sqrt(metrics.mean_squared_error(y_test, pred))\n    print('Test: ', score)\n    print()\n\ndef get_predictions(model):\n    '''\n    fits the model on x_train and y_train, then\n    returns the predictions of the model on x_train\n    and x_test\n    '''\n    model.fit(x_train, y_train)\n    return (list(model.predict(x_train)), list(model.predict(x_test)))","b139849b":"def parameterSearch(search_type='random', parameter_space={}, \n                    model=None, cv=4, n_iter=100, scoring='neg_mean_squared_error', \n                    n_jobs=4, verbose=1, fitting_data=None, \n                    best_parameters=[], best_scores=[], \n                    n_repeat=1, print_rest=False, round_decimals=6,\n                    modify_score=None):\n    \n    '''\n    Parameters:\n        search_type should be 'random' or 'grid'\n        parameter_space is a dictionary\n    \n    best_parameters = [{'p1': v1, 'p2': v2}, {'p1': v1, 'p2': v2},...]\n    best_scores = [(test_mean, test_std, train_mean, train_std), \n                    (test_mean, test_std, train_mean, train_std),...]\n    '''\n    \n    for i in range(n_repeat):\n        if search_type == 'grid':\n            clf = GridSearchCV(model, parameter_space, cv=cv, return_train_score=True,\n                       scoring=scoring, n_jobs=n_jobs, verbose=verbose)\n        else:\n            clf = RandomizedSearchCV(model, parameter_space, cv=cv, n_iter=n_iter, \n                                     return_train_score=True, scoring=scoring, \n                                     n_jobs=n_jobs, verbose=verbose)\n            \n        clf.fit(*(fitting_data))\n        print(\"Best parameters:\" if n_repeat==1 else \"Round {} | Best parameters:\".format(i+1))\n        print()\n        pprint(clf.best_params_) \n        print()\n        best_parameters.append(clf.best_params_)\n        bi = clf.best_index_\n        tsmeans = np.round(clf.cv_results_['mean_test_score'], decimals=round_decimals)\n        tsstds = np.round(clf.cv_results_['std_test_score'], decimals=round_decimals)\n        trmeans = np.round(clf.cv_results_['mean_train_score'], decimals=round_decimals)\n        trstds = np.round(clf.cv_results_['std_train_score'], decimals=round_decimals)\n        if modify_score != None:\n            tsmeans = list(map(modify_score, tsmeans))\n            tsstds = list(map(modify_score, tsstds))\n            trmeans = list(map(modify_score, trmeans))\n            trstds = list(map(modify_score, trstds))\n        print(\"Mean test score = \", tsmeans[bi])\n        print(\"with std = \", tsstds[bi])\n        print(\"Mean train score = \", trmeans[bi])\n        print(\"with std = \", trstds[bi])\n        best_scores.append((tsmeans[bi], tsstds[bi], trmeans[bi], trstds[bi]))\n        print()\n        if print_rest:\n            print('*'*30)\n            print()\n            print(\"All parameters: \")\n            parameters = clf.cv_results_['params']\n            for tsmean, tsstd, trmean, trstd, params in zip(tsmeans, tsstds, trmeans, trstds, parameters):\n                print(params)\n                print(tsmean, tsstd, trmean, trstd, sep=' | ')\n                print('-'*20)\n        print('='*50)\n        print('='*50)\n        print()","3ddd2472":"xgb_tuned_parameters = {\n                     'gamma': np.round(np.random.uniform(0,100,size=(100,)), decimals=4),\n                     'subsample': np.round(np.random.uniform(0.5,0.9,(100,)), decimals=4),\n                     'colsample_bytree': np.round(np.random.uniform(0.5,0.9,(100,)), decimals=4),\n                     'colsample_bylevel': np.round(np.random.uniform(0.5,0.9,(100,)), decimals=4),\n                     'learning_rate': np.round(np.random.uniform(0.05, 0.2, (100,)), decimals=4),\n                     'n_estimators': np.random.randint(10,1500,(100,)),\n                     'reg_alpha': np.round(np.random.uniform(1,500,(100,)), decimals=4),\n                     'reg_lambda': np.round(np.random.uniform(1,500,(100,)), decimals=4),\n                     'random_state': np.random.randint(1,20,(100,)),\n                    }\n\nxgb_bestpars = []\nxgb_bestscores = []\n\n# Finding good parameters for XGBRegressor\n# parameterSearch(model=XGBRegressor(), best_parameters=xgb_bestpars, \n#                 best_scores=xgb_bestscores, parameter_space=xgb_tuned_parameters, \n#                 fitting_data=(tr_X, tr_y), cv=4, n_iter=1000, n_jobs=4, n_repeat=1, \n#                 search_type='random', print_rest=True, round_decimals=4, verbose=1, \n#                 modify_score=lambda v: np.sqrt(np.absolute(v)))\n","6d459b1a":"xgb = XGBRegressor(**{'colsample_bylevel': 0.5308,\n                     'colsample_bytree': 0.861,\n                     'gamma': 54.0637,\n                     'learning_rate': 0.0577,\n                     'n_estimators': 1291,\n                     'random_state': 11,\n                     'reg_alpha': 384.5839,\n                     'reg_lambda': 15.3661,\n                     'subsample': 0.8521})\nxgb.fit(tr_X, tr_y)\nfeat_imp = xgb.feature_importances_\nfeat_imp_s = pd.Series(feat_imp, index=tr_X_colnames).sort_values(ascending=False)\n# # plot a bar chart of feature importances\n# fig, ax = plt.subplots(figsize=(18,80))\n# sns.barplot(x=feat_imp_s, y=feat_imp_s.index, palette=sns.color_palette('deep', n_colors=284));\n# plt.xlabel('Feature Importance Score');\n# plt.ylabel('Features');","93529195":"fi = sorted( [i for i, x in enumerate(feat_imp) if x < 0.0001] )","09cc3217":"tr_X = np.delete(tr_X, fi, 1)\nts = np.delete(ts, fi, 1)\n# re-split the training data\nx_train, x_test, y_train, y_test = \\\n    train_test_split(tr_X, tr_y, test_size=0.25, random_state=0)","43f28661":"xgb_bestpars = []\nxgb_bestscores = []\n\n# Finding good parameters for XGBRegressor\n# parameterSearch(model=XGBRegressor(), best_parameters=xgb_bestpars, \n#                 best_scores=xgb_bestscores, parameter_space=xgb_tuned_parameters, \n#                 fitting_data=(tr_X, tr_y), cv=4, n_iter=5000, n_jobs=4, n_repeat=1, \n#                 search_type='random', print_rest=True, round_decimals=4, verbose=1, \n#                 modify_score=lambda v: np.sqrt(np.absolute(v)))","9781048c":"# Finding good parameters for Bagging with XGBRegressor\nops = [\n    {'colsample_bylevel': 0.5037, 'colsample_bytree': 0.5102, 'gamma': 8.5888, 'learning_rate': 0.0573,\n     'n_estimators': 1194, 'random_state': 17, 'reg_alpha': 12.3419, 'reg_lambda': 15.3661, 'subsample': 0.8243},\n    {'subsample': 0.5656, 'reg_lambda': 302.4913, 'reg_alpha': 482.5555, 'random_state': 2, 'n_estimators': 641, \n     'learning_rate': 0.1327, 'gamma': 50.2968, 'colsample_bytree': 0.8274, 'colsample_bylevel': 0.7549},\n    {'subsample': 0.8526, 'reg_lambda': 223.798, 'reg_alpha': 497.4114, 'random_state': 12, 'n_estimators': 245, \n     'learning_rate': 0.124, 'gamma': 58.3453, 'colsample_bytree': 0.8874, 'colsample_bylevel': 0.7565}]\nbes = [XGBRegressor(**op) for op in ops]\nbg_tuned_parameters = {\n    'base_estimator': bes,\n    'n_estimators': np.random.randint(7,30,(100,)),\n    'max_samples': np.round(np.random.uniform(0.7,1.0,(100,)), decimals=4),\n    'max_features': np.round(np.random.uniform(0.7,1.0,(100,)), decimals=4),\n    'bootstrap': (True, False),\n    'bootstrap_features': (True, False),\n    'random_state': np.random.randint(1,50,(100,))}\nbg_bestpars = []\nbg_bestscores = []\n# parameterSearch(model=BaggingRegressor(), best_parameters=bg_bestpars, \n#                 best_scores=bg_bestscores, parameter_space=bg_tuned_parameters, \n#                 fitting_data=(tr_X, tr_y), cv=4, n_iter=30, n_jobs=4, n_repeat=1, \n#                 search_type='random', print_rest=True, round_decimals=4, verbose=1, \n#                 modify_score=lambda v: np.sqrt(np.absolute(v)))","1f05e37c":"bg_s = [BaggingRegressor(**{'random_state': 36, 'n_estimators': 14, 'max_samples': 0.8894, 'max_features': 0.8545, \n                            'bootstrap_features': False, 'bootstrap': False, \n                            'base_estimator': XGBRegressor(**{'subsample': 0.5773, 'reg_lambda': 197.4628, \n                                                              'reg_alpha': 106.8185, 'random_state': 15, 'n_estimators': 990, \n                                                              'learning_rate': 0.07, 'gamma': 13.8871, 'colsample_bytree': 0.5363, \n                                                              'colsample_bylevel': 0.8607})})]*16\nfor i in range(1, 17):\n    if i % 2 != 0:\n        a = {'colsample_bylevel': 0.5037, 'colsample_bytree': 0.5102, 'gamma': 8.5888, 'learning_rate': 0.0573,\n         'n_estimators': 1194, 'random_state': 17, 'reg_alpha': 12.3419, 'reg_lambda': 15.3661, 'subsample': 0.8243}\n        a['random_state'] = i\n    else:\n        a = {'colsample_bylevel': 0.4045,\n             'colsample_bytree': 0.3997,\n             'gamma': 32.6015,\n             'learning_rate': 0.1286,\n             'n_estimators': 4190,\n             'random_state': 19,\n             'reg_alpha': 186.2586,\n             'reg_lambda': 505.4068,\n             'subsample': 0.3381}\n    b = {'base_estimator': XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.5037,\n                           colsample_bytree=0.5102, gamma=8.5888, learning_rate=0.0573,\n                           max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n                           n_estimators=1194, n_jobs=1, nthread=None, objective='reg:linear',\n                           random_state=17, reg_alpha=12.3419, reg_lambda=15.3661,\n                           scale_pos_weight=1, seed=None, silent=True, subsample=0.8243),\n         'bootstrap': False,\n         'bootstrap_features': True,\n         'max_features': 0.9193,\n         'max_samples': 0.948,\n         'n_estimators': 19,\n         'random_state': 16}\n    b['base_estimator'] = XGBRegressor(**a)\n    b['random_state'] = i + 3\n    bg_s.append(BaggingRegressor(**b))\npreds = []\nfor b in bg_s:\n    preds.append(list(b.fit(tr_X, tr_y).predict(ts)))\nfinal_p = list(np.mean(preds, axis=0))","bb03e335":"submission = pd.DataFrame({\n        \"Id\": ts_ids,\n        \"SalePrice\": final_p\n})\n\nsubmission.to_csv('submission.csv', index=False)","1717b948":"### Break the training dataset into predictor variables and target variable","083c8f23":"### Split the training data for model building","7549aee7":"### Checking for missing values","cf0e58fc":"### Perform one-hot encoding for categorical variables","cf59daf9":"### Removing outliers","b898c239":"## Importing packages","0e593516":"### Adding features","f80ba2f4":"### Dealing with categorical variables that represent ranking","57da49f8":"## Preprocessing","5be3e015":"### Feature scaling","6a3794f7":"### Creating and tuning models","103ef64d":"### Feature importances","3ebc7ed0":"### Creating a function to perform parameter search","f1d33dc0":"### Creating functions for fitting and evaluating models","92678642":"### Filling in missing values","29431199":"### Dropping Id columns","de666125":"## Reading datasets","2c18c4ec":"### Deleting features with low importances"}}