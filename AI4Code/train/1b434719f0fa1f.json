{"cell_type":{"893906b6":"code","75ab2d47":"code","fb80642b":"code","e2a46405":"code","d05847be":"code","828c8f51":"code","95b4ca6a":"code","4a54d75b":"code","133d096c":"code","548dcfb0":"code","8ddd0631":"code","9c0cac3d":"code","59459a24":"code","73cfe33f":"code","93e2c07b":"code","81bcc28c":"code","af2ff8f6":"code","b9aa5579":"code","2c1dbfc7":"code","86c09d06":"code","7ed3ce6b":"code","b5bf05e6":"code","0029b8cb":"code","09d5bfee":"code","75c489ae":"code","0fba029e":"code","980069d6":"code","94b1f5bb":"code","78965427":"code","7f5830bd":"code","a68864d1":"code","7229374a":"code","e58669a6":"code","b251352e":"code","50811fe9":"code","9488ba30":"code","589e0417":"code","51664216":"code","e8b815a5":"code","8a1eb949":"code","209ba6ef":"code","52f1e4c6":"code","97ab4b08":"code","4794409f":"code","f964f715":"code","02f4274e":"code","77e55c1f":"code","c5ca3a98":"code","fa009e88":"code","c9b0ec2b":"code","825884f6":"code","083bee11":"code","86ccfc17":"code","2951db51":"code","62765e46":"code","df6ee2b0":"code","3cb32f4a":"code","84ed8248":"code","27f471d0":"code","8684e9db":"code","c62c49a4":"code","6b2ebcbe":"markdown","e8baa066":"markdown","823461e0":"markdown","3404348c":"markdown","a99c2881":"markdown","58a0c061":"markdown","9e49f2ed":"markdown","ac13d629":"markdown","108bbcbd":"markdown","f76a11c0":"markdown","a9460e59":"markdown","6ae1f453":"markdown","243f2a2b":"markdown","f7fe5142":"markdown","7730734c":"markdown","bfc0beef":"markdown","f78f4084":"markdown","cfcf8a31":"markdown","e7f42383":"markdown","c1bf9b72":"markdown","d865d328":"markdown","c7e87421":"markdown","a3298bab":"markdown","7fa1b487":"markdown","287fcc27":"markdown","fa05ff85":"markdown","92d59ddf":"markdown","a3607c45":"markdown","18ac62ad":"markdown","4dd29cff":"markdown","6c6ad8c4":"markdown","bb1419ee":"markdown","c09c3341":"markdown","613a3c0f":"markdown","89521c92":"markdown","6a2bde58":"markdown","49c51442":"markdown","d31e0ec1":"markdown","3531f9bf":"markdown","539210aa":"markdown","703151d0":"markdown","9840f3d2":"markdown","3d34fa17":"markdown","8082d8aa":"markdown","4a963c18":"markdown","b2390f6a":"markdown","bde8d8e4":"markdown","c88d47d8":"markdown","448857b8":"markdown","fa550f3b":"markdown","113d949a":"markdown"},"source":{"893906b6":"!pip install pytorch-tabnet;\n!pip install wget;\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.plotting import scatterplotmatrix\nfrom itertools import combinations\nimport warnings\nimport gc\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, validation_curve, GridSearchCV\nfrom sklearn.feature_selection import RFECV, SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.base import clone\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier, StackingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.notebook import tqdm\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom pytorch_tabnet.tab_model import TabNetClassifier","75ab2d47":"def outlier_function(df, col_name):\n    first_quartile = np.percentile(np.array(df[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(3*IQR)\n    lower_limit = first_quartile-(3*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","fb80642b":"def evalMod(df, model=LGBMClassifier(n_estimators=2000, max_depth=20, learning_rate=0.05)):\n    X_df = df.iloc[:, :-1]\n    y_df = df.iloc[:, -1]\n    X = df.values[:, :-1]\n    y = df.values[:, -1] - 1\n\n    sc = StandardScaler()\n    \n\n    kfold = StratifiedKFold(n_splits=5)\n    kfold = kfold.split(X, y)\n    scores = []\n    for k, (train, test) in enumerate(kfold):\n        sc.fit(X[train])\n        X_train_std = sc.transform(X[train])       \n        model.fit(X_train_std, y[train])\n        X_test_std = sc.transform(X[test])\n        score = accuracy_score(model.predict(X_test_std), y[test])\n        scores.append(score)\n        #print( score)   \n    print('\\nCV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))\n    ","e2a46405":"def plotHistograms(df, Feature):\n    dct = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[]}\n    elev = df[Feature]\n    cov_type = df['Cover_Type']\n    for i in range(df.shape[0]):\n        dct[cov_type[i]].append(elev[i])\n\n    plt.figure(figsize=(8,6))\n    for i in range(7):\n        plt.hist(dct[i+1], bins=100, alpha=0.35, label=\"Cov_Type\"+str(i+1))\n\n    plt.xlabel(Feature, size=14)\n    plt.ylabel(\"Count\", size=14)\n    plt.title(Feature + \"Histograms\")\n    plt.legend(loc='upper right')\n    plt.savefig(Feature + \"  Histograms.png\")","d05847be":"def plotStackedHistograms(df, Feature, n_attr):\n    dct = {}\n    cov_type = df['Cover_Type']\n    for i in range(n_attr):\n        dct[ Feature +str(i+1)] = [0,0,0,0,0,0,0]\n\n    for i in range(n_attr):\n        for j in range(df.shape[0]):\n            dct[ Feature+str(i+1)][cov_type[j]-1] += df[Feature+str(i+1)][j]\n\n\n    XX = tuple(dct.keys())\n    YY = np.transpose(np.array(list(dct.values()))).tolist()\n    \n    dct_type = {}\n    for i in range(7):\n        dct_type[ \"Cover_Type\" + str(i+1)] = YY[i]\n    \n    \n    ddff = pd.DataFrame(dct_type, index=pd.Index(XX, name=Feature))\n    ax = ddff.plot(kind='bar', stacked=True, figsize=(10, 6))\n    ax.set_ylabel('foo')\n    plt.legend(title='labels', bbox_to_anchor=(1.0, 1), loc='upper left')\n    # plt.savefig('stacked.png')  # if needed\n    plt.show()","828c8f51":"#read the training data\ndf = pd.read_csv('..\/input\/forest-cover-type-kernels-only\/train.csv.zip')\ndf.head()","95b4ca6a":"df = df.drop(columns='Id')","4a54d75b":"df.describe()","133d096c":"np.bincount(df['Cover_Type'])[1:]","548dcfb0":"#number of unique values for every feature\ndf.nunique(axis=0)","8ddd0631":"df['Soil_Type7'].sum()","9c0cac3d":"df['Soil_Type15'].sum()","59459a24":"if(df.isnull().sum().sum() == 0):\n  print('There is no missing values')\nelse:\n  print(df.isnull().sum())","73cfe33f":"corr = df.iloc[:, :10].corr()\nfig, ax = plt.subplots(figsize=(20,20)) \nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(2200,200,as_cmap=True), ax=ax)","93e2c07b":"sns.set_theme(style=\"ticks\")\n\nsns.pairplot(df.iloc[:, [0,1,2,3,4,5,6,7,8,9,54]], hue='Cover_Type', )\n\nplt.xticks(rotation=90)\n    \nplt.yticks(rotation=90)\n\nplt.show()","81bcc28c":"for col in list(df.columns[:10]):\n    plotHistograms(df, col)","af2ff8f6":"plotStackedHistograms(df, 'Soil_Type', 40)","b9aa5579":"plotStackedHistograms(df, 'Wilderness_Area', 4)","2c1dbfc7":"print(\"Cross validation score for the original training data:\")\nevalMod(df)","86c09d06":"df = df.drop(columns='Hillshade_9am')\nevalMod(df)","7ed3ce6b":"# loop through all columns to see if there are any outliers\nfor column in df.columns:\n    if outlier_function(df, column)[2] > 0:\n        print(\"There are {} outliers in {}\".format(outlier_function(df, column)[2], column))","b5bf05e6":"df = df[(df['Horizontal_Distance_To_Fire_Points'] > outlier_function(df, 'Horizontal_Distance_To_Fire_Points')[0]) & (df['Horizontal_Distance_To_Fire_Points'] < outlier_function(df, 'Horizontal_Distance_To_Fire_Points')[1])]\n# evalMod(df)\n# 80%","0029b8cb":"# df = df[(df['Horizontal_Distance_To_Hydrology'] > outlier_function(df, 'Horizontal_Distance_To_Hydrology')[0]) & (df['Horizontal_Distance_To_Hydrology'] < outlier_function(df, 'Horizontal_Distance_To_Hydrology')[1])]\n# evalMod(df)\n# 79.7%","09d5bfee":"# df = df[(df['Vertical_Distance_To_Hydrology'] > outlier_function(df, 'Vertical_Distance_To_Hydrology')[0]) & (df['Vertical_Distance_To_Hydrology'] < outlier_function(df, 'Vertical_Distance_To_Hydrology')[1])]\n# evalMod(df)\n# 79.7%","75c489ae":"# df = df[(df['Hillshade_Noon'] > outlier_function(df, 'Hillshade_Noon')[0]) & (df['Hillshade_Noon'] < outlier_function(df, 'Hillshade_Noon')[1])]\n# evalMod(df)\n# 79.6%","0fba029e":"df.insert(0, 'Hydro_Dist', np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2))\ndf.insert(0, 'HydroRoad_horz', df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Roadways'])\ndf.insert(0, 'HydroFire_horz', df['Horizontal_Distance_To_Hydrology']+df['Horizontal_Distance_To_Fire_Points'])\ndf.insert(0, 'FireRoad_horz', df['Horizontal_Distance_To_Roadways']+df['Horizontal_Distance_To_Fire_Points'])\n# evalMod(df)\n# 80.6%","980069d6":"# df.insert(0, 'HydroRoad_Vert', np.abs(df['Vertical_Distance_To_Hydrology'])+df['Horizontal_Distance_To_Roadways'])\n# df.insert(0, 'HydroFire_Vert', np.abs(df['Vertical_Distance_To_Hydrology'])+df['Horizontal_Distance_To_Fire_Points'])\n# evalMod(df)\n# 80.3%","94b1f5bb":"df.insert(0, 'HydroRoad_horz_diff', abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways']))\ndf.insert(0, 'HydroFire_horz_diff', abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points']))\ndf.insert(0, 'FireRoad_horz_diff', abs(df['Horizontal_Distance_To_Roadways'] - df['Horizontal_Distance_To_Fire_Points']))\n# evalMod(df)\n# 81.2%","78965427":"df.insert(0, 'HydroRoad_horz_ratio', df['Horizontal_Distance_To_Hydrology'] \/ (df['Horizontal_Distance_To_Roadways'] + 1))\ndf.insert(0, 'HydroFire_horz_ratio', df['Horizontal_Distance_To_Hydrology'] \/ (df['Horizontal_Distance_To_Fire_Points'] + 1))\ndf.insert(0, 'FireRoad_horz_ratio', df['Horizontal_Distance_To_Roadways'] \/ (df['Horizontal_Distance_To_Fire_Points'] + 1))\n# evalMod(df)\n# 81.3%","7f5830bd":"# df.insert(0, 'Hydro_Vert_abs', np.abs(df['Vertical_Distance_To_Hydrology']))\n# df = df.drop(columns='Vertical_Distance_To_Hydrology')\n# evalMod(df)\n# 81%","a68864d1":"# df.insert(0, 'Hishade_mean',  (df['Hillshade_Noon'] + df['Hillshade_3pm']) \/ 2)\n# evalMod(df)\n# 80.%","7229374a":"# df.insert(0, 'Hishade_max',  np.max( [list(df['Hillshade_Noon']), list(df['Hillshade_3pm'])], axis=0))\n# evalMod(df)\n# 80.6%","e58669a6":"# df.insert(0, 'Hishade_ratio',  df['Hillshade_3pm'] \/ df['Hillshade_Noon'])\n# evalMod(df)\n# 80.9%","b251352e":"df.insert(0, 'Elevation_log',  np.log(df['Elevation']))\n#evalMod(df)\n# 81.3% +\/- 0.036","50811fe9":"df.insert(0, 'Elevation_inv',  1\/ df['Elevation'])\n# evalMod(df)\n# 81.4% +\/- 0.033","9488ba30":"# df.insert(0, 'ElevHydro_vert', df['Elevation'] + df['Vertical_Distance_To_Hydrology'])\n# evalMod(df)\n# 81.1%","589e0417":"# df = df.drop(columns='ElevHydro_vert')","51664216":"# df.insert(0, 'ElevHydro_vert', df['Elevation'] + np.abs(df['Vertical_Distance_To_Hydrology']))\n# evalMod(df)\n# 81.1%","e8b815a5":"# df = df.drop(columns='ElevHydro_vert')","8a1eb949":"# df.insert(0, 'ElevHydro_vert', df['Elevation'] + 4*np.abs(df['Vertical_Distance_To_Hydrology']))\n# evalMod(df)\n# 81.3% +\/- 0.035","209ba6ef":"df.insert(0, 'Angle', (np.arctan(df['Slope']) \/ np.pi) * 180)\ndf = df.drop(columns='Slope') #no effect when removing\n# evalMod(df)\n# 81.4% +\/- 0.032 best","52f1e4c6":"# df.insert(0, 'AspectAngle_ratio', (df['Aspect']+1) \/ (df['Angle']+1))\n# evalMod(df)\n# 81.2% ","97ab4b08":"df.iloc[:,:15].describe()","4794409f":"# train-test split\nX = df.values[:, :-1]\ny = df.values[:, -1] - 1\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train_std = sc.transform(X_train)\nX_test_std = sc.transform(X_test)\n\n#For Voting Classifier\npreds = pd.DataFrame()","f964f715":"#fit lightGBM\nlgbm = LGBMClassifier(n_estimators=2000, max_depth=20, learning_rate=0.06, min_child_samples=20, reg_alpha=0 , reg_lambda=0.001, subsample=0.8, colsample_bytree=.6, subsample_for_bin=1000)\nlgbm.fit(X_train_std, y_train)\npreds['lgbm'] = lgbm.predict(X_test_std)\nprint(\"lgbm\", accuracy_score(preds['lgbm'].values , y_test))\ndel lgbm","02f4274e":"#fit xgboost\nxgb = XGBClassifier(n_estimators=2000, max_depth=20, learning_rate=0.05, use_label_encoder=False, verbosity = 0)\nxgb.fit(X_train_std, y_train)\npreds['xgb'] = xgb.predict(X_test_std)\nprint(\"xgb\", accuracy_score(preds['xgb'].values , y_test))\ndel xgb","77e55c1f":"#fit randomForest\nforest = RandomForestClassifier(n_estimators=3000, max_depth=20)\nforest.fit(X_train_std, y_train)\npreds['forest'] = forest.predict(X_test_std)\nprint(\"forest\", accuracy_score(preds['forest'].values , y_test))\ndel forest","c5ca3a98":"#fit extraTrees\nextraTrees = ExtraTreesClassifier(n_estimators=5000, max_depth=50, criterion='entropy')\nextraTrees.fit(X_train_std, y_train)\npreds['extraTrees'] = extraTrees.predict(X_test_std)\nprint(\"extraTrees\", accuracy_score(preds['extraTrees'].values , y_test))\ndel extraTrees","fa009e88":"class ForestCoverDataset(Dataset):\n\n    def __init__(self, X, y, transform=None):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return self.y.shape[0]\n\n    def __getitem__(self, idx):\n        sample_features = self.X[idx]\n        sample_label = self.y[idx]\n        sample = {'sample_features': sample_features, 'sample_label':sample_label}\n        return sample  ","c9b0ec2b":"# Network class\nclass NN(nn.Module):\n  #best 800 hidden units\n  def __init__(self, input_size, num_classes):\n    super(NN,self).__init__()\n\n    self.fc1 = nn.Linear(input_size, 1600)\n    self.fc1_bn = nn.BatchNorm1d(1600)\n    self.dropout1 = nn.Dropout(p=0.5)\n    \n    self.fc2 = nn.Linear(1600, 200)\n    self.fc2_bn = nn.BatchNorm1d(200)\n    self.dropout2 = nn.Dropout(p=0.5)\n\n    \n    self.fc3 =  nn.Linear(200, num_classes)\n    self.fc3_bn = nn.BatchNorm1d(num_classes)\n\n  def forward(self,x):\n    x = F.leaky_relu(self.dropout1(self.fc1_bn(self.fc1(x))))\n    x = F.leaky_relu(self.dropout2(self.fc2_bn(self.fc2(x))))\n    x = self.fc3_bn(self.fc3(x))\n    return x","825884f6":"def multi_acc(y_pred, y_test):\n  y_pred_softmax = torch.log_softmax(y_pred, dim=1)\n  _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n  correct_pred = (y_pred_tags == y_test).float()\n  acc = correct_pred.sum() \/ len(correct_pred)\n  acc = acc * 100\n  return acc","083bee11":"def train_model(train_loader, test_loader, model, optimizer, criterion, num_epochs, device='cpu', validation=True): \n    accuracy_stats = {'train': [], 'val': []}\n    loss_stats = {'train': [], 'val': []}\n    for e in tqdm(range(1, num_epochs+1)):\n        #training\n        train_epoch_loss = 0\n        train_epoch_acc = 0\n        model.train()\n\n        for data in train_loader:\n            X_train_batch = data['sample_features']\n            y_train_batch = data['sample_label']\n            X_train_batch = X_train_batch.to(device=device)\n            y_train_batch = y_train_batch.to(device=device)\n\n            optimizer.zero_grad()\n\n            y_train_pred = model(X_train_batch)\n\n            train_loss = criterion(y_train_pred, y_train_batch)\n            train_acc = multi_acc(y_train_pred, y_train_batch)\n\n            train_loss.backward()\n            optimizer.step()\n\n            train_epoch_loss += train_loss.item()\n            train_epoch_acc += train_acc.item()\n        \n        #validation\n        if(validation):        \n            with torch.no_grad():\n                val_epoch_loss = 0\n                val_epoch_acc = 0\n\n                model.eval()\n\n                for data_test in test_loader:\n                    X_test_batch = data_test['sample_features']\n                    y_test_batch = data_test['sample_label']\n                    X_test_batch = X_test_batch.to(device=device)\n                    y_test_batch = y_test_batch.to(device=device)\n\n                    y_test_pred = model(X_test_batch)\n\n                    val_loss = criterion(y_test_pred, y_test_batch)\n                    val_acc = multi_acc(y_test_pred, y_test_batch)\n\n                    val_epoch_loss += val_loss.item()\n                    val_epoch_acc += val_acc.item()\n\n            loss_stats['train'].append(train_epoch_loss\/len(train_loader))\n            loss_stats['val'].append(val_epoch_loss\/len(test_loader))\n            accuracy_stats['train'].append(train_epoch_acc\/len(train_loader))\n            accuracy_stats['val'].append(val_epoch_acc\/len(test_loader))\n            if(e % 25 == 0 or e == 1):\n                print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss\/len(train_loader) : .5f} | Val Loss: {val_epoch_loss\/len(test_loader):.5f}  | Train Acc: {train_epoch_acc\/len(train_loader):.3f} | Val Acc:  {val_epoch_acc\/len(test_loader):.3f}')\n    return model, accuracy_stats, loss_stats","86ccfc17":"def evalNet(model, X_test, device='cpu'):\n    with torch.no_grad():\n        model.eval()\n        y_test_pred = model(torch.from_numpy(X_test).float().to(device))\n        y_pred_softmax = torch.log_softmax(y_test_pred, dim=1)\n        _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n    return y_pred_tags.cpu().numpy()","2951db51":"nn_train_data = ForestCoverDataset(torch.from_numpy(X_train_std).float(), torch.from_numpy(y_train).long())\nnn_test_data = ForestCoverDataset(torch.from_numpy(X_test_std).float(), torch.from_numpy(y_test).long())\n\n\nnum_epochs = 1000\ntrain_batch_size = 128\ntest_batch_size = len(nn_test_data)\nlearning_rate = 0.005\nnum_features = 65\nnum_classes = 7\ndevice = 'cuda'\n\ntrain_loader = DataLoader(nn_train_data, \n                          batch_size=train_batch_size,\n                          shuffle=True)\n\ntest_loader = DataLoader(nn_test_data, \n                          batch_size=test_batch_size,\n                          shuffle=True)\n\nmodel = NN(input_size=num_features, num_classes=num_classes)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nmodel.to(device=device)\ntrained_model, accuracy_stats, loss_stats = train_model(train_loader, test_loader, model, optimizer, criterion, num_epochs, device='cuda', validation=True)\npreds['net'] = evalNet(model, X_test_std, 'cuda')\nprint(\"net\", accuracy_score(preds['net'].values , y_test))\ndel trained_model, model, accuracy_stats, loss_stats","62765e46":"tabnet = TabNetClassifier(\n    n_d=64, n_a=64, n_steps=8,\n    gamma=1.5, n_independent=2, n_shared=2,\n    lambda_sparse=1e-4, momentum=0.3, clip_value=2.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params = {\"gamma\": 0.95,\n                     \"step_size\": 20},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15\n)\n\nmax_epochs = 500\n\ntabnet.fit(\n    X_train=X_train_std, y_train=y_train,\n    max_epochs=max_epochs, patience=500,\n    batch_size=512, virtual_batch_size=128\n) \n\ny_pred= tabnet.predict(X_test_std)\nprint(\"tabnet\", accuracy_score(y_pred , y_test))\ndel tabnet","df6ee2b0":"### Voting ###\ny_pred = preds.mode(axis=1).values[:,0]\nprint(\"Voting\", accuracy_score(y_pred , y_test))","3cb32f4a":"# Reading the entire training data\nX = df.values[:, :-1]\ny = df.values[:, -1] - 1\nsc = StandardScaler()\nsc.fit(X)\nX_std = sc.transform(X)","84ed8248":"# Training ExtraTrees Classifier\nextraTrees = ExtraTreesClassifier(n_estimators=5000, max_depth=50, criterion='entropy')\nextraTrees.fit(X_std, y)\nprint(\"ExtraTrees Done!\")","27f471d0":"# Preprocess Test Data\ndf_submit = pd.read_csv('..\/input\/forest-cover-type-kernels-only\/test.csv.zip')\nid_submit = df_submit.values[:, 0]\ndf_submit = df_submit.drop(columns='Id')\n\ndf_submit = df_submit.drop(columns='Hillshade_9am')\ndf_submit.insert(0, 'Hydro_Dist', np.sqrt(df_submit['Horizontal_Distance_To_Hydrology']**2 + df_submit['Vertical_Distance_To_Hydrology']**2))\ndf_submit.insert(0, 'HydroRoad_horz', df_submit['Horizontal_Distance_To_Hydrology'] + df_submit['Horizontal_Distance_To_Roadways'])\ndf_submit.insert(0, 'HydroFire_horz', df_submit['Horizontal_Distance_To_Hydrology'] + df_submit['Horizontal_Distance_To_Fire_Points'])\ndf_submit.insert(0, 'FireRoad_horz', df_submit['Horizontal_Distance_To_Roadways'] + df_submit['Horizontal_Distance_To_Fire_Points'])\ndf_submit.insert(0, 'HydroRoad_horz_diff', abs(df_submit['Horizontal_Distance_To_Hydrology'] - df_submit['Horizontal_Distance_To_Roadways']))\ndf_submit.insert(0, 'HydroFire_horz_diff', abs(df_submit['Horizontal_Distance_To_Hydrology'] - df_submit['Horizontal_Distance_To_Fire_Points']))\ndf_submit.insert(0, 'FireRoad_horz_diff', abs(df_submit['Horizontal_Distance_To_Roadways'] - df_submit['Horizontal_Distance_To_Fire_Points']))\ndf_submit.insert(0, 'HydroRoad_horz_ratio', df_submit['Horizontal_Distance_To_Hydrology'] \/ (df_submit['Horizontal_Distance_To_Roadways'] + 1))\ndf_submit.insert(0, 'HydroFire_horz_ratio', df_submit['Horizontal_Distance_To_Hydrology'] \/ (df_submit['Horizontal_Distance_To_Fire_Points'] + 1))\ndf_submit.insert(0, 'FireRoad_horz_ratio', df_submit['Horizontal_Distance_To_Roadways'] \/ (df_submit['Horizontal_Distance_To_Fire_Points'] + 1))\ndf_submit.insert(0, 'Elevation_log',  np.log(df_submit['Elevation']))\ndf_submit.insert(0, 'Elevation_inv',  1\/ df_submit['Elevation'])\ndf_submit.insert(0, 'Angle', (np.arctan(df_submit['Slope']) \/ np.pi) * 180)\ndf_submit = df_submit.drop(columns='Slope') #no effect when removing\ndf_submit.head()","8684e9db":"id_submit.shape","c62c49a4":"X_submit = df_submit.values\nX_submit_std = sc.transform(X_submit)\n\n\nprint(\"Evaluation..\")\npreds = pd.DataFrame()\ny_submit = extraTrees.predict(X_submit_std)\n\nsubmission = {'Id':id_submit, 'Cover_Type': (y_submit + 1).astype(int)}\ndf_submission = pd.DataFrame(submission)\ndf_submission.to_csv('submission.csv', index=False)","6b2ebcbe":"## Removing Outliers","e8baa066":"Again observe cleary that elevation does good goob seperating the classes compared to other features, we also observe that Wilderness_Area is constrained with only 3 cover types which make it an important feature seperating the classes.","823461e0":"LightGBM and XGBoost are different implementations of Gradient Boosting Decision Trees (GBDT), these are ensemble models that are based on boosting, which means building stronger models from weaker models. LightGBM provies faster and parallelized implementation in which the tree grows in leaf-wise approach. Whereas XGBoost is slower and its trees grow in level-wise approch. \n\nRandom Forest and ExtraTreeClassifier are kinda similar. Both of them build randomized trees, every tree is built with subset of features and subset of training data. Then in test time voting is done between the decision tree to determine the correct class. The difference between them is as follows, Random forest uses bootstrap replicas, that is to say, it subsamples the input data with replacement, whereas Extra Trees use the whole original sample. Another difference is the selection of cut points in order to split nodes. Random Forest chooses the optimum split while Extra Trees chooses it randomly. However, once the split points are selected, the two algorithms choose the best one between all the subset of features. Therefore, Extra Trees adds randomization but still has optimization. \n\nAs it is observed the performance of these 4 models is quite similar with ExtraTreeClassifier having the lead.","3404348c":"To see how many unique values each attribute has we run the following ","a99c2881":"It is balanced training data","58a0c061":"## Voting Classifier","9e49f2ed":"# Feature Engineering\n","ac13d629":"# Utilities","108bbcbd":"# Submission","f76a11c0":"Now we will look with more details into the distribution of the cover types with respect to every feature in the data set","a9460e59":"\nWe shall see brief description of our dataset","6ae1f453":"An outlier is a data point that is noticeably different from the rest. They represent errors in measurement, bad data collection, or simply show variables not considered when collecting the data. Removing outliers from the training data will make the trained model more robust as it will focus on data that represent the real distribution without any disruption. Our approach to remove outlirs is as stated in this article [https:\/\/people.richland.edu\/james\/lecture\/m170\/ch03-pos.html](http:\/\/)\n\nThe outlier_function detects first and third quartile and interquartile range for a given column of a dataframe then calculates upper and lower limits to determine outliers conservatively\nreturns the number of lower and uper limit and number of outliers respectively\n","243f2a2b":"Now we can construct the correlation matrix, to see the relationships between the attributes and how they affect each other. One thing to look for is highly correlated features. We shall remove highly correlated features for memory and storage concerns. In addition to that we want to remove any redundant features to avoid the curse of dimensionality and make the feature space more simpler for our model. A rule of thumb is to remove any feature that is correlated with +\/- 0.75 or higher with any other feature.","f7fe5142":"Now comes the interesting part, Feature Engineering. We will try to make use of the observations we obtained from visualizing the data to create new features that help the model to seperate will between the classes. As we remember, there wasn't enough good seperation between the classess in almost all the features except for the Elevation feature, thus we will try to make some meaningful combination of the features to enhance the seperation. Our approach as stated before is to evluate the performance of every step we do in feature engineering and undone this step if it leads to worse accuracy.","7730734c":"Dropping the feature Hillshade_9am improved the accuracy as expected","bfc0beef":"We can make a stronger classifier if we united our chosen classifiers together. This could be done a Voting Classifier. Simply we will train our all of our chosen models and in test time we will make predictions with all of our chosen models, then for each test example we will get its prediction from all of the models, the test example will be asigned to a particular class according to the majority voting of all the models. This is called Hard Voting.\n\nAll of the previous models will be chosen except for TabNet. As we saw, ExtraTreesClassifier achieved the best results, with LightGBM, XGBoost and Neural Network achieved quite good and similar results, RandomForest achieved lower results but it is still comparable to the previous classifiers, when it comes to TabNet, it took so long to be trained and generated worest result of them all, thus we get rid of it in favor of achieving better training and testing time and also to constrain the voting to reliable models.\n\nNow we will compare the performance of our Voting Classifier with the performance of the other models. We won't do it using cross validation as we don't have enough time for it. so we will split our training data and get validation set to compare the results. Of course cross validation assures that our model isn't biased towards part of the examples in the dataset but using a validation set still generates somewhat reliable result.\n","f78f4084":"# Exploratory Data Analysis","cfcf8a31":"From the confusion matrix we could see that there are some correlations, the stongest of them -which exceeded the +\/- 0.75 threshold- is Hillshade_9am with Hillshade_3pm. we will more into this when we plot the scatter matrix","e7f42383":"Now comes the model selection section. In this section will we evaluate the models that perform well in structured data. It's known that tree based methods are the state of the art when we are talking about structured data thus we will train and evaluate LightGBM, XGboost, Random Forests and ExtraTreeClassifier. We won't use tree methods only, Neural Networks is thought to give a promising performance in this problem so we will also train and evaluate fully connected neural networks and its new variation TabNet. Linear models like logistic regression and support vector machines didn't perform well in this problem so I won't consider them in my final submission. \n\nFor the tree-based methods I will sklearn interface. For neural networks and tabnet I will use pytorch. The models hyberparameters were optimized manually by conducting alot of different trails on each model,the models were evaluated using cross validation score during hyberparameter optimization as it provides more accurate result with no bias.\n","c1bf9b72":"# **Forest Cover Type Classification**\nThe forest cover type problem is mainly about classifying the type of trees that cover certain areas. The dataset is taken from 30m by 30m areas of forest that are classified as one of seven cover types:\n1. Spruce\/Fir\n1. Lodgepole Pine\n1. Ponderosa Pine\n1. Cottonwood\/Willow\n1. Aspen\n1. Douglas-fir\n1. Krummholz\n\nOur data set contains a training set of 15,120 examples and a test set of 565,892 examples. As we see the size of the test set is far larger than the size of the training set which make it an intersting challenge to get a model that performs well on the test data. We will start our building our model by importing some libraries and defining some utilities that will be used through this notebook then we will dive into data exploration.","d865d328":"We see that the accuracy in the neural net models are higher than the tree-based models, but it doesn't reflect the truth as we use cross validation with tree-based methods whcih is more reliable than the validation set we use with the neural net models. In the next model (Voting Classifier) we will establish a fair comparison between these models. ","c7e87421":"# Model Selection and Optimization\n","a3298bab":"## Random Forest","7fa1b487":"1- Elevation\n\nQuantitative - measured in meters - its the height of the forest above the sea level\n\n2- Aspect\n\nQuantitative - measured in azimuth degrees - it answers the question of \"With respect to the sun, which ways the hill facing?\"\n\n\n3- Slope \n\nQuantitative - measured in degrees - the slope of the hill\n\n\n![image.png](attachment:11b6e987-192d-413c-be12-671f676a40f8.png)\n\n4 - Horizontal Distance To Hydrology\t\n\nQuantitative - measured in meters - Horz Dist to nearest surface water features\n\n5 - Vertical Distance To Hydrology\n\nQuantitative - measured in meters - Vert Dist to nearest surface water features\n\n6 - Horizontal Distance To Roadways\t\n\nQuantitative - measured in meters - Horz Dist to nearest roadway\n\n7 - Horizontal_Distance_To_Fire_Points \n\nQuantitative - measured in meters - Horz Dist to nearest wildfire ignition points\n\n8 - Hillshade_9am \n\nQuantitative - 0 to 255 index - Hillshade index at 9am, summer solstice\n\n9 - Hillshade_Noon \n\nQuantitative - 0 to 255 index - Hillshade index at noon, summer soltice\n\n10 - Hillshade_3pm \n\nQuantitative -  0 to 255 index - Hillshade index at 3pm, summer solstice\n\n> Hillshade simulates the shadows cast by the sun upon a three-dimensional representation of terrain\n\n11 - Wilderness_Area \n\nQualitative - 0 (absence) or 1 (presence) - 4 binary columns - Wilderness area designation\n\n12 - Soil_Type \n\nQualitative - 0 (absence) or 1 (presence) - 40 binary columns - Soil Type designation\n\n13 - Cover_Type \n\ninteger - 1 to 7 - 7 types - Forest Cover Type designation","287fcc27":"As we see, our dataset has no missing values","fa05ff85":"The ExtraTreeClassifier outperforms all other models and also outperforms the Voting classifier! It turns out that the ExtraTreeClassifier classifier managed to correctly classify hard test examples that all other classes failed to do classifiy correctly, in the final submission I submited one time the voting classifier and obtained 79.75% but with the ExtraTreeClassifier I obtained 80.7%, which makes it a clear winner!","92d59ddf":"## Neural Networks","a3607c45":"# Conclusion of the previous steps","18ac62ad":"## LightGBM\n","4dd29cff":"Next we check if our dataset has some missing values","6c6ad8c4":"As we see, the dataset is somewhat diverse, but we also notice that there are no examples of it have Soil_Type7 and Soil_Type15","bb1419ee":"As we see from our output, the attributes of the dataset are as follows:","c09c3341":"In the beginning we explored the dataset which contains 10 quantitative features and two categorical features onehot-encoded into 4 and 40 binary features respectively. We observed from the visualization of the data that there some correlation between the hillshade features thus we removed one of them. We also removed the outliers that make our model perform worse. In addition to that we generated some new features from combinations of the existed features, all of made our data better for the models. We trained several machine learning algorithms but from all the tree based methods achieved the highest accuracy, specially ExtraTreeClassifier which outperorms all other classifiers and also outperformed the voting classifier. We used used this ExtraTreeClassifier for our final submission and obtained 80.7%.","613a3c0f":"# Data Cleaning","89521c92":"As observed in the data exploration part there was no missing values and we droped ID column. Two things will be done in the data cleanring section. First thing we will remove highly correlated features and then we will remove outliers. Our approach is to test a baseline model after every modification we do in the data, if the modification improves the accuracy then we keep it, else we undo the modification. I left the modification that I have undone commented to see the effect of it and compare it with other modifications. Also, every modification is evaluated by cross validation score using the function evalMod()","6a2bde58":"We will start our exploration journey by using pandas to read the dataset then display its first examples.","49c51442":"# Imports","d31e0ec1":"Next, we will drop the \"Id\" column as it won't help in modeling.","3531f9bf":"## Evaluating Baseline Model","539210aa":"## XGBoost","703151d0":"## ExtraTreeClassifier","9840f3d2":"Removing the outliers of Horizontal_Distance_To_Fire_Points generated a decent improvement in accuracy. But for the other features there weren't any improvments at all, thus we I have undone these modifications. It could be justified as these outliers of Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology and Hillshade_Noon holds some other valuable information for the classifier, and getting rid of them means also getting rid of these valuable information, so we just keep them. ","3d34fa17":"One thing to notice from the description is that the attribute Vertical_Distance_To_Hydrology has negative value. After some searching I found out that negative values of Vertical_Distance_To_Hydrology mean the hydrology lies beneth the specific area of study, positive values mean the hydrology lies above the specific area of study. This information may provide the classifier with some insight but it may also confuses the classifier, thus we will try to eleminate this attribute later and see what happens. ","8082d8aa":"First thing to notice is the Elevation feature. Clearly we can see that it does the best job trying to seperate the classes compared to the other features. \n\nWe can't see any clear seperation in the other features, thus we will try to combine some of them in the feature engineering, hoping to create features that try to seperate the classes better than that.\n\nThe correlation between Hillshade_9am and Hillshade_3pm is cleary visible in the plot.\n\nThere is some kind of trend between Aspect and Hillshade features, also between Slope and Hillshade features. After some digging it turns out that the hillshade at time t various as a factor of:\n\n                                 cos(slope)cos(90\u2212Altitude)+sin(slope)sin(90\u2212Altitude)cos(Azimuth\u2212Aspect)\n                                                         \nAltitude is the angle of the Sun relative to the horizon and Azimuth relates to the direction the Sun is facing: North, South, East, or West. Azimuth of 90 degrees corresponds to East.","4a963c18":"## TabNet","b2390f6a":"We will consider the outliers of the quantitative features \n* Horizontal_Distance_To_Fire_Points\n* Horizontal_Distance_To_Hydrology\n* Vertical_Distance_To_Hydrology\n* Hillshade_Noon","bde8d8e4":"As expected, different combinations of Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Horizontal_Distance_To_Fire_Points improved the performance a lot. In addtion to that, we made use of our observation of the elevation feature importance and added its log and its inverse to the features. Accidentally, I took the arctan() of the slope angle instead of calculating it tan(), but it was a good accident! Surprisingly, the arctan of the slope performed better than the slope itself, the arctan of the slope doesn't make much sense, but in any case it performed better and made sense for the classifier, thus I decided to keep it. This could be justified, the arctan of the slope angle is a function of the slope angle, this function gives us new feature that provides -in combination with the other features- a clearer seperation between the classes. \n\n","c88d47d8":"Let's see if our training data is balanced","448857b8":"## Removing Highly Correlated Features","fa550f3b":"TabNet is a deep neural network specialized to deal with tabular data. It consists of sevaral steps, every step contains two blocks:\n\nThe Feature Transformer block is used to process the features\n\nThe Attentive Transformer block is used for automatic feature selection\n\nMore info could be found here https:\/\/arxiv.org\/abs\/1908.07442\n","113d949a":"It's time to build the scatter matrix! What we are looking for is some kind of correlation, some kind of class seperation based on some those features, we also look for interesting patterns that rise in the dataset and see why they rise."}}