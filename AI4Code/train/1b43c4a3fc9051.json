{"cell_type":{"8fe81e96":"code","9e651119":"code","7cf377fc":"code","3b38fcc8":"code","66c86a32":"code","aa625d25":"code","4d0190eb":"code","149d915f":"code","c3dc21fa":"code","0d2cdd30":"code","f819a6ce":"code","13ac4e50":"code","080e1dc3":"code","86542371":"code","72a19848":"code","a762aad1":"code","4581f32c":"code","6d962010":"code","ff515b9b":"code","79b0abc0":"code","ed8de35f":"code","585cae34":"code","c54e3d35":"code","f0234dce":"code","19f526d6":"code","163104a3":"code","d8bc1f5d":"code","55a3bf62":"code","8f1973c2":"code","a30fa5ab":"code","f1143094":"code","9fdc17d5":"code","cb068c09":"code","b9f93357":"code","3a5c9e96":"code","7e204ce7":"code","7de6d28a":"code","d04f1e8d":"code","377f015c":"code","c535f905":"code","bb2b9db1":"code","3cbc243d":"markdown","f4dd6c30":"markdown","c732a7d9":"markdown","9d5d1174":"markdown","f2e2040c":"markdown","4496c31b":"markdown","875b9a9c":"markdown","e6e1b69c":"markdown","ea1d0837":"markdown","34e6f025":"markdown","bdd40d37":"markdown","7a2dc2c1":"markdown","f10d30b8":"markdown","734539e8":"markdown","1b731f24":"markdown","d33e8e28":"markdown","bbe244e2":"markdown","2c681215":"markdown","9890b19d":"markdown","f9cc210f":"markdown","ae909359":"markdown","41e3666c":"markdown","45b9be0e":"markdown","ac8b078d":"markdown"},"source":{"8fe81e96":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport gc\nimport os\nimport time\n\nimport lightgbm as lgb\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","9e651119":"seed_val = 42","7cf377fc":"print(os.listdir(\"..\/input\"))","3b38fcc8":"# Original data for holdout and test:\nx_hold = pd.read_csv('..\/input\/creditensemble\/x_hold.csv', index_col=0)\ntest_x = pd.read_csv('..\/input\/creditensemble\/test_x.csv', index_col=0)\n\n# Predictions and target for holdout:\ndf_holdout = pd.read_csv('..\/input\/creditensemble\/holdout_res.csv', index_col=0).clip(0, None)\ndf_holdout.set_index('SK_ID_CURR', inplace=True, drop=True)\n\ndf_tst = pd.read_csv('..\/input\/creditensemble\/test_res.csv', index_col=0).clip(0, None)\ndf_tst.set_index('SK_ID_CURR', inplace=True, drop=True)\n\n# Target for holdout:\ny_hold = pd.read_csv('..\/input\/creditensemble\/y_hold.csv', index_col=0, header=None)","66c86a32":"for column in df_holdout.columns:\n    print(column, roc_auc_score(y_hold, df_holdout[column]))","aa625d25":"df_corr = df_holdout.corr()\ndf_corr.style.background_gradient().set_precision(2)","4d0190eb":"df_holdout.drop(labels='pred_lgb', axis=1, inplace = True)\ndf_tst.drop(labels='pred_lgb', axis=1, inplace = True)\n\ndf_corr = df_holdout.corr()\ndf_corr.style.background_gradient().set_precision(2)","149d915f":"# Hold out score:\nensemble_holdout = df_holdout.mean(axis=1)\nroc_auc_score(y_hold, ensemble_holdout)","c3dc21fa":"# Calculate test submission:\nensemble_sub = df_tst.mean(axis=1)","0d2cdd30":"# Save:\nsub_train = pd.DataFrame(df_tst.index)\nsub_train['TARGET'] = ensemble_sub.values\nsub_train[['SK_ID_CURR', 'TARGET']].to_csv('sub_average.csv', index=False)","f819a6ce":"from scipy.optimize import nnls","13ac4e50":"# Find weigths by solving linear regression with constraint:\nweights = nnls(df_holdout.values, y_hold.values.ravel())[0]\nweights","080e1dc3":"# Hold out score: multiply, sum, clip values out of range.\nensemble_holdout = (df_holdout.values*weights).sum(axis=1).clip(0,1)\nroc_auc_score(y_hold, ensemble_holdout)","86542371":"from sklearn.linear_model import LogisticRegression","72a19848":"# Put test and hold in one frame:\nframes = [df_holdout, df_tst]\nhold_test = pd.concat(frames)\n\n# Compute ranking:\nranked_hold_test = hold_test.rank(axis=0)\/hold_test.shape[0]\n\n# Split the frames:\nranked_hold = ranked_hold_test.loc[df_holdout.index,:]\nranked_test = ranked_hold_test.loc[df_tst.index,:]","a762aad1":"# Match test values to holdout ranks: (this will take some time)\nfrom tqdm import tqdm, trange\n\ndef historical_ranking(df_tst, ranked_holdout):\n    ranked_test = df_tst.copy()\n    for c in df_tst.columns:\n        for i in tqdm(df_tst.index):\n            value_to_find = df_tst.loc[i,c]\n            ranked_test.loc[i,c] = ranked_holdout[c].iloc[(df_holdout[c]-value_to_find).abs().values.argmin()]\n    return ranked_test","4581f32c":"# Train logistic regression on holdout:\nclf = LogisticRegression()\nclf.fit(ranked_hold.values, y_hold.values.ravel())","6d962010":"# Hold out score: multiply, sum, stretch values within range.\nensemble_holdout = clf.predict_proba(ranked_hold.values)[:, 1]\nensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\nroc_auc_score(y_hold, ensemble_holdout)","ff515b9b":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n\ndef preprocessing(X, degree):\n\n    poly = PolynomialFeatures(degree)\n    scaler = MinMaxScaler()  \n    lin_scaler = StandardScaler()\n    poly_df = pd.DataFrame(lin_scaler.fit_transform(poly.fit_transform(scaler.fit_transform(X))))\n    poly_df['SK_ID_CURR'] = X.index\n    poly_df.set_index('SK_ID_CURR', inplace=True, drop=True)\n    return poly_df","79b0abc0":"# Compute poly features:\ndegree = 2\npoly_hold_test = preprocessing(hold_test, degree)\n\n# Split the frames:\npoly_hold = poly_hold_test.loc[df_holdout.index,:]\npoly_test = poly_hold_test.loc[df_tst.index,:]","ed8de35f":"# Train logistic regression on holdout:\nclf = LogisticRegression()\nclf.fit(poly_hold.values, y_hold.values.ravel())","585cae34":"# Hold out score: multiply, sum, clip values out of range.\nensemble_holdout = clf.predict_proba(poly_hold.values)[:, 1]\nroc_auc_score(y_hold, ensemble_holdout)","c54e3d35":"frames = [x_hold, test_x]\nx_hold_test = pd.concat(frames)","f0234dce":"from sklearn.decomposition import PCA\npca = PCA(n_components=100, random_state=seed_val)\npca.fit(x_hold_test)\npca.explained_variance_ratio_.sum()","19f526d6":"# Reduce dimensions add ranks:\npca_hold_test = pd.DataFrame(pca.transform(x_hold_test))\npca_hold_test.set_index(x_hold_test.index, inplace=True)\n\nranks_pca = pd.concat([ranked_hold_test, pca_hold_test], axis=1)","163104a3":"# Split the frames:\nranks_pca_hold = ranks_pca.loc[df_holdout.index,:]\nranks_pca_test = ranks_pca.loc[df_tst.index,:]","d8bc1f5d":"# Train logistic regression:\nclf = LogisticRegression()\nclf.fit(ranks_pca_hold.values, y_hold.values.ravel())\nensemble_holdout = clf.predict_proba(ranks_pca_hold.values)[:, 1]\n\n#Linear stretch:\nensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\nroc_auc_score(y_hold, ensemble_holdout)","55a3bf62":"# Train on test and ave:\nsub_train = clf.predict_proba(ranks_pca_test.values)[:, 1]\n\nsub_train = pd.DataFrame(test_x.index)\nsub_train['TARGET'] = ensemble_sub.values\nsub_train[['SK_ID_CURR', 'TARGET']].to_csv('sub_log_pca_rank.csv', index=False)","8f1973c2":"wf_hold_test = pd.DataFrame(pca_hold_test.index)\nfor feature in pca_hold_test.columns:\n    for predictor in hold_test.columns:\n        col_name = str(predictor)+str(feature)\n        wf_hold_test[col_name] = (pca_hold_test[feature]*hold_test[predictor]).values\n\nwf_hold_test.set_index('SK_ID_CURR', inplace=True, drop=True)\nwf_hold_test.head()","a30fa5ab":"# Split the frames:\nwf_hold = wf_hold_test.loc[df_holdout.index,:]\nwf_test = wf_hold_test.loc[df_tst.index,:]","f1143094":"# Train logistic regression:\nclf = LogisticRegression()\nclf.fit(wf_hold.values, y_hold.values.ravel())\nensemble_holdout = clf.predict_proba(wf_hold.values)[:, 1]\n\n#Linear stretch:\nensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\nroc_auc_score(y_hold, ensemble_holdout)","9fdc17d5":"from sklearn.linear_model import LinearRegression","cb068c09":"lr = LinearRegression()\nlr.fit(wf_hold.values, y_hold.values.ravel())\nensemble_holdout = lr.predict(wf_hold.values)\n\n#Linear stretch:\nensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\nroc_auc_score(y_hold, ensemble_holdout)","b9f93357":"import lightgbm as lgb\n\ndef kfold_lightgbm(trn_x, trn_y, num_folds=3):\n       \n    # Cross validation model\n    in_folds = StratifiedShuffleSplit(n_splits= num_folds, random_state=seed_val)\n        \n    # Create arrays and dataframes to store results\n    for train_idx, valid_idx in in_folds.split(trn_x, trn_y):\n        dtrain = lgb.Dataset(data=trn_x[train_idx], \n                             label=trn_y[train_idx], \n                             free_raw_data=False, silent=True)\n        dvalid = lgb.Dataset(data=trn_x[valid_idx], \n                             label=trn_y[valid_idx], \n                             free_raw_data=False, silent=True)\n\n        # LightGBM parameters found by Bayesian optimization\n        params = {\n            'objective': 'binary',\n            'boosting_type': 'gbdt',\n            'nthread': 4,\n            'learning_rate': 0.02,  # 02,\n            'num_leaves': 20,\n            'colsample_bytree': 0.9497036,\n            'subsample': 0.8715623,\n            'subsample_freq': 1,\n            'max_depth': 8,\n            'reg_alpha': 0.041545473,\n            'reg_lambda': 0.0735294,\n            'min_split_gain': 0.0222415,\n            'min_child_weight': 60, # 39.3259775,\n            'seed': seed_val,\n            'verbose': -1,\n            'metric': 'auc',\n        }\n        \n        clf = lgb.train(\n            params=params,\n            train_set=dtrain,\n            num_boost_round=1000,\n            valid_sets=[dtrain, dvalid],\n            early_stopping_rounds=200,\n            verbose_eval=False\n        )\n\n        del dtrain, dvalid\n        gc.collect()\n    \n    return clf","3a5c9e96":"# Train lightGBM:\nmodel = kfold_lightgbm(ranks_pca_hold.values, y_hold.values.ravel())\nensemble_holdout = model.predict(ranks_pca_hold.values)\n\n#Linear stretch:\n#ensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\nroc_auc_score(y_hold, ensemble_holdout)","7e204ce7":"# Train on test and ave:\nsub_train = model.predict(ranks_pca_test.values)\n\nsub_train = pd.DataFrame(test_x.index)\nsub_train['TARGET'] = ensemble_sub.values\nsub_train[['SK_ID_CURR', 'TARGET']].to_csv('sub_lgb_pca_rank.csv', index=False)","7de6d28a":"estimators = ['lgb']\nestimator = estimators[0] #, 'xgb','ridge','f10_dnn']\nj=0\ntrain_x = ranks_pca_hold\ntrain_y = y_hold\n\n#train_len = train_x.shape[0]","d04f1e8d":"folds = StratifiedShuffleSplit(n_splits= 3,\n                                random_state=seed_val,\n                                test_size = 1\/3,\n                                train_size = 2\/3)\n\nhalf_folds = StratifiedShuffleSplit(n_splits= 1,\n                                random_state=seed_val,\n                                test_size = 0.5,\n                                train_size = 0.5)","377f015c":"test_len = ranks_pca_test.shape[0]\ntest_probas = np.zeros((test_len, len(estimators)*3))\ntest_proba = np.zeros(test_len)\n\n# For every fold:\nfor i, (train, test) in enumerate(folds.split(train_x, train_y)):\n    trn_x = train_x.iloc[train, :]\n    trn_y = train_y.iloc[train].values.ravel()\n    val_x = train_x.iloc[test, :]\n    val_y = train_y.iloc[test].values.ravel()  \n    \n    val_len = val_x.shape[0]\n    estimators_probas = np.zeros((val_len, len(estimators)*3))\n    \n    for i, (half_train, half_test) in enumerate(half_folds.split(trn_x, trn_y)):\n        half_trn_x = trn_x.iloc[half_train, :].values\n        half_trn_y = trn_y[half_train].ravel()\n        half_val_x = trn_x.iloc[half_test, :].values\n        half_val_y = trn_y[half_test].ravel()\n\n        #Train on one part, predict the other:\n        if estimator == 'lgb':\n            #Train on halves and on the whole set:\n            model_a = kfold_lightgbm(half_trn_x, half_trn_y)\n            model_b = kfold_lightgbm(half_val_x, half_val_y)\n            model = kfold_lightgbm(ranks_pca_hold.values, y_hold.values.ravel())\n            \n            #Predict val set:\n            estimators_probas[:, j*3] = model_a.predict(val_x)\n            estimators_probas[:, j*3+1] = model_b.predict(val_x) \n            estimators_probas[:, j*3+2] = model.predict(val_x)\n            \n            #Predict test set:\n            test_probas[:, j*3] = model_a.predict(ranks_pca_test)\n            test_probas[:, j*3+1] = model_b.predict(ranks_pca_test) \n            test_probas[:, j*3+2] = model.predict(ranks_pca_test)            \n    \n    # Train logistic regression on holdout:\n    clf = LogisticRegression()\n    clf.fit(estimators_probas, val_y.ravel())\n    \n    # Hold out score: multiply, sum, stretch values within range.\n    ensemble_holdout = clf.predict_proba(estimators_probas)[:, 1]\n    ensemble_holdout = (ensemble_holdout - ensemble_holdout.min()) \/ (ensemble_holdout.max() - ensemble_holdout.min())\n    print(roc_auc_score( val_y.ravel(), ensemble_holdout))\n    \n    ensemble_test = clf.predict_proba(test_probas)[:, 1]\n    ensemble_test = (ensemble_test - ensemble_test.min()) \/ (ensemble_test.max() - ensemble_test.min())    \n    test_proba += ensemble_test*1\/3","c535f905":"# Train on test and ave:\nsub_train = test_proba\n\nsub_train = pd.DataFrame(test_x.index)\nsub_train['TARGET'] = ensemble_sub.values\nsub_train[['SK_ID_CURR', 'TARGET']].to_csv('dragons.csv', index=False)","bb2b9db1":"import pandas as pd\nholdout_res = pd.read_csv(\"..\/input\/creditensemble\/holdout_res.csv\")\ntest_res = pd.read_csv(\"..\/input\/creditensemble\/test_res.csv\")\ntest_x = pd.read_csv(\"..\/input\/creditensemble\/test_x.csv\")\nx_hold = pd.read_csv(\"..\/input\/creditensemble\/x_hold.csv\")\ny_hold = pd.read_csv(\"..\/input\/creditensemble\/y_hold.csv\")","3cbc243d":"**Once again we'll snoop the test, to get a better score on LB.** Clean solution would rank holdout set, train on it. Then match values from test set to ranks in holdout and use them to predict target. ","f4dd6c30":"## LightGBM with ranks and PCA","c732a7d9":"Better than sole xgb, but not for the leader board.","9d5d1174":"Worse than xgb.","f2e2040c":"First check and drop any overcorrelated results:","4496c31b":"# Universal Blender\n## Part 2. Potions 101.\n\nInspired by Henk van Veen \nhttps:\/\/www.kdnuggets.com\/2015\/06\/ensembles-kaggle-data-science-competition-p3.html\n\nIn this part I go by the book and try all the blending techniques mentioned by Henk. This part shows that stacking and blending offers a lot of ways to be creative and overfit. :) I think I might have made a mistake when chosing holdout, so I have not achieved a higher score on test, despite the good results on cross validation. Try it on your own, maybe you will be luckier than me. If you spot a mistake in the code, I will much appreciate if you point it out in the comments.","875b9a9c":"## Weighted vote\nSame as linear regression with constraint on weights >0.","e6e1b69c":"## Dragons","ea1d0837":"Nope.","34e6f025":"Let\u2019s say you want to do 2-fold stacking:\n\n* Split the train set in 2 parts: train_a and train_b\n* Fit a first-stage model on train_a and create predictions for train_b\n* Fit the same model on train_b and create predictions for train_a\n* Finally fit the model on the entire train set and create predictions for the test set.\nNow train a second-stage stacker model on the probabilities from the first-stage model(s) (using CV).","bdd40d37":"**Yet again it scored 0.788.** That is dissapointing. Perhaps, there is a flaw in implementation. For instance, I finish by using lgbm only. Maybe, that is not the best choise and a bunch of weaker estimators have to be blended\/stacked instead. I also suspect that the hold out is not representative of the test set. The fact that the score is stuck at 0.788 hints that there is a simple mistake somewhere. Anyway, I hope this will serve as starter guide for creative ensamblimg and blending. Have fun and good luck! I'll go and retrain the whole damn thing, yet again; with different holdout.\n\nPS: One other thing, that I is worth trying is to train a blender on the validation sets in the models training loop. This will make the code more cumbersome, but allow to train models on the whole set without need for holdout.","7a2dc2c1":"\"Here be dragons. With 7 heads. Standing on top of 30 other dragons.\" Henk van Veen ","f10d30b8":"That has not gone well; pass.","734539e8":"## Logistic regression with ranks and PCA\n1. Use PCA to reduce dimension of original holdout and test data\n2. add the reduced dimensions to the ranked datesets,\n3. train simple logistic regression based on the ranks + reduced dimensions.","1b731f24":"**Lorgistic regression with linear stretch on pca + rank features:**","d33e8e28":"### Data load\nNote we won't touch train set at all. Only holdout and test sets.","bbe244e2":"Let's drop lightGBM:","2c681215":"**That is somewhat promising, but can we do better?\n**Actually scored 0.788**","9890b19d":"## Ranked logistic regression\nHere we do two things: \n1. substitute rank for probability in every column\n2. train simple logistic regression based on the ranks","f9cc210f":"That is very promising, but actually it just overfits and scores 0.788. Ok, as we deal with overfit, lets try to train a bunch of classifiers and then train logistic regreesion on their outcome. As we can only use holdout set, we will have to find weights with CV.","ae909359":"## Avergaing\nSimply average all probabilities, in this case a single confident classifier may overrun several non-confident classifiers.","41e3666c":"Awful!","45b9be0e":"## Quadratic linear stacking\nSame as above, but we add quadratic features of the ranked dataset:","ac8b078d":"## Feature-Weighted Linear Stacking\nhttps:\/\/arxiv.org\/pdf\/0911.0460.pdf\n\nAs far as I understand it is what it sounds like. Generate meta features for the dataset, then multiply them by predictions. And build linear regression on top of it."}}