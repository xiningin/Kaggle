{"cell_type":{"e9afdb53":"code","2c519ce2":"code","c6af6f61":"code","f67eb14d":"code","97a1bab1":"code","368c6b21":"code","402f77d5":"code","d4a725e5":"code","81f72df4":"code","47d383ec":"code","9cb8658b":"code","291c0e55":"code","030c37ed":"code","11276e87":"markdown","e77dd53a":"markdown","b9bc9e32":"markdown","6227b391":"markdown","5281cf4c":"markdown","198a77c4":"markdown","a576c3cc":"markdown"},"source":{"e9afdb53":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.feature_selection import RFECV, VarianceThreshold\nfrom sklearn import linear_model, datasets\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c519ce2":"X = np.array([[1,1,1],\n              [2,2,0],\n              [3,3,1],\n              [4,4,0],\n              [5,5,1],\n              [6,6,0],\n              [7,7,1],\n              [8,7,0],\n              [9,7,1]])\n\n# Create dataframe\ndata_frm = pd.DataFrame(X)\n\ndata_frm","c6af6f61":"# Get the correlation matrix\ncorr_matrix = data_frm.corr().abs()\n\ncorr_matrix","f67eb14d":"# Select the upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nprint(upper)\n\n# Pick the column that has correlation more than 0.95\nto_drop = [col for col in upper.columns if any(upper[col] > 0.95)]","97a1bab1":"data_frm.drop(to_drop, axis=1)","368c6b21":"X, y = make_regression(n_samples=10000, \n                      n_features=100,\n                      n_informative=2,\n                      random_state=1)\n\nprint(X.shape, y.shape)\n# Creating linear model\n\nlr_model = linear_model.LinearRegression()","402f77d5":"# Recursive feature elimination\n\nrfecv = RFECV(estimator=lr_model, step=1, scoring='neg_mean_squared_error')\n\nrfecv.fit(X, y)\n\nrfecv.transform(X)\n","d4a725e5":"rfecv.n_features_","81f72df4":"X = [[0,1,0],\n    [0,1,1],\n    [0,1,0],\n    [0,1,1],\n    [0,1,0],\n    [1,0,0]]","47d383ec":"# Variance Threshold\nthresholder = VarianceThreshold(threshold=(0.75 * (0.25)))\nthresholder.fit_transform(X)","9cb8658b":"iris_data = datasets.load_iris()\n\nX = iris_data.data\ny = iris_data.target\n","291c0e55":"# Create VarianceThreshold object with variance threshold above 0.5\nthresholder = VarianceThreshold(threshold=0.5)\n\n# Conduct variance threshold\nX_high_variance = thresholder.fit_transform(X)","030c37ed":"print(len(X_high_variance))\nX_high_variance[0:5]","11276e87":"# Feature Selection Techniques\n\n## Definition\n\nFeature selection is a process of choosing independent variables that have more influence in predicting the target or response variable. \n\n> In **machine learning** and **statistics**, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction.\n\nThis notebook is compiled from various sources and one of the primary sources is [Chris Alban](https:\/\/chrisalbon.com\/)\n\n## Types of Feature Selection\n\nThe feature selection methods are grouped into three based on how they combine selection algorithm and the model building.","e77dd53a":"### Non-Binary Features","b9bc9e32":"## 2. Recursive Feature Elimination\n\n### Create Data","6227b391":"## Import Libraries","5281cf4c":"## 1. Drop highly correlated features","198a77c4":"## 3. Variance Thresholding \n\nThis method removes all low-variance features. This feature algorithm looks only the features(X) not the desired outputs(y) and thus can be used for unsupervised learning. \n\n### Binary Features\n\nIn Bernauli trail there will be only two outcomes, 1-success and 0-failure. The variance will be calculated as: $Var(x) = p(1-p)$\n    \nWhere, p - Probability of success, q or (1-p) - Probability of failure","a576c3cc":"![FeatureSelection.png](attachment:FeatureSelection.png)"}}