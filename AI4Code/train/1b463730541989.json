{"cell_type":{"6d40d9f9":"code","93db1e7d":"code","8c411860":"code","7991837a":"code","aaac27c2":"code","29d5e3c9":"code","5440eb4d":"code","c3ecc068":"code","d6d76b3e":"code","f48096ea":"code","ef32dcae":"code","35d4fe9b":"code","1280c344":"code","2ecedad7":"code","e6824aec":"code","6a0cf7b2":"code","4473e56a":"code","66288941":"code","3475960a":"code","3354fbe7":"code","538817b5":"code","93688929":"code","1979c5a6":"code","d75d0c7e":"code","1c04a6a0":"code","ab2dcbfc":"code","25fecd06":"code","1fa7ea83":"code","b42fd96d":"code","91a28212":"code","2899e568":"code","ff4916bd":"code","5d362493":"code","65aa1611":"code","bc46898c":"code","87e33677":"code","4ea05d58":"code","e357548d":"code","9e7d7da5":"code","d8f11171":"code","d740072a":"code","bc461471":"code","318acb37":"code","531b1b2b":"code","130feda3":"code","74007921":"markdown","546dff0b":"markdown","9d375b5d":"markdown","9464209a":"markdown","fce7713d":"markdown","72a8a73c":"markdown","243e4f18":"markdown","cd6d1acf":"markdown","bcd5ec19":"markdown","0503240a":"markdown","1b368e21":"markdown","90fb06a3":"markdown","e8d0b46e":"markdown","b6208541":"markdown","9032cc6e":"markdown","8652e201":"markdown","e3836c58":"markdown","9d7ed2e4":"markdown","6a0c7083":"markdown","9c3ea95f":"markdown","95c52ecb":"markdown","c3125b60":"markdown","fc43e7ee":"markdown","465783e2":"markdown","abfb006c":"markdown"},"source":{"6d40d9f9":"use_review_text = True  # Change it to False, if you don't want review text included for training\nuse_count_vectorization = True  # Change it to False to exclude count_vectorization","93db1e7d":"if not use_review_text:\n    # Without review text.\n    df_types_filename = '..\/input\/airline-reviews-eda-and-preprocessing-pt-1\/PreprocessedDataLightTypes.csv'\n    df_filename = '..\/input\/airline-reviews-eda-and-preprocessing-pt-1\/PreprocessedDataLight.csv'\n    df_out_filename = '.\/Preds-WithoutText.csv'\nelse:\n    # With review text.\n    df_types_filename = '..\/input\/airline-review-data-preprocessing-pt-2-nlp\/NLPFinalDataLightTypes.csv'\n    df_filename = '..\/input\/airline-review-data-preprocessing-pt-2-nlp\/NLPFinalDataLight.csv'\n    df_out_filename = '.\/Preds-WithText.csv'","8c411860":"# Define numerical and categorical features.\nif not use_review_text:\n    # Without review text.\n    num_feats = ['date_flown_month',\n                 'date_flown_year',\n                 'review_date_date_flown_distance_days',\n                 'review_characters',\n                 'has_layover_num',\n                 'seat_comfort',\n                 'cabin_service',\n                 'food_bev',\n                 'entertainment',\n                 'ground_service',\n                 'value_for_money']\n    cat_feats = ['airline',\n                 'traveller_type',\n                 'cabin']\nelse:\n    # With review text.\n    if not use_count_vectorization:\n        num_feats = ['date_flown_month',\n                     'date_flown_year',\n                     'review_date_date_flown_distance_days',\n                     'review_characters',\n                     'has_layover_num',\n                     'seat_comfort',\n                     'cabin_service',\n                     'food_bev',\n                     'entertainment',\n                     'ground_service',\n                     'value_for_money',\n                     'polarity']\n    else:\n        with open('..\/input\/airline-review-data-preprocessing-pt-2-nlp\/VecReviewTextCleanFeats.csv','r') as f:\n            vec_feats = f.read()\n            vec_feats = vec_feats.split(', ')\n        num_feats = ['date_flown_month',\n                     'date_flown_year',\n                     'review_date_date_flown_distance_days',\n                     'review_characters',\n                     'has_layover_num',\n                     'seat_comfort',\n                     'cabin_service',\n                     'food_bev',\n                     'entertainment',\n                     'ground_service',\n                     'value_for_money',\n                     'polarity'] + vec_feats\n    cat_feats = ['airline',\n                 'traveller_type',\n                 'cabin']\n\nfeats = num_feats + cat_feats","7991837a":"# Set this variable to the desired method for data transformation.\n# Possible options are: scaling_and_one_hot_encoding, label_encoding, no_transformation.\ntransform_dataset = 'label_encoding'","aaac27c2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette('Set2')\nimport scipy.sparse\n\nimport datetime as dt\nimport dateutil\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.metrics import roc_curve, accuracy_score, roc_auc_score, confusion_matrix \n\nimport lightgbm as lgb\n\nimport importlib","29d5e3c9":"# Type of each field in the input data.\ndf_dtype = pd.read_csv(df_types_filename)\ndict_dtype = df_dtype[['index','dtypes']].set_index('index').to_dict()['dtypes']\ndict_dtype['recommended'] = 'bool'","5440eb4d":"# Input data.\ndf = pd.read_csv(df_filename, dtype=dict_dtype, keep_default_na=False, na_values=['_'])\ndf.drop(columns=['Unnamed: 0'],inplace=True)","c3ecc068":"df.head()","d6d76b3e":"df.shape","f48096ea":"n_reviews = df.shape[0]\nprint('Number of customer reviews in the dataset: {:d}'.format(n_reviews))","ef32dcae":"# Utility function to assign the label to our dataset\ndef assign_label_recommended(df_row):\n    \"\"\"\n    Return 0 if not recommended and 1 otherwise.\n    \"\"\"\n    label_recommended = None\n    if df_row['recommended'] == True:\n        label_recommended = 1\n    elif df_row['recommended'] == False:\n        label_recommended = 0\n    else:\n        label_recommended = None\n    return label_recommended","35d4fe9b":"df['label'] = df.apply(lambda x: assign_label_recommended(x), axis=1)","1280c344":"df.head()","2ecedad7":"df['has_layover_num'] = df['has_layover'].astype(int)\ndf['date_flown_day'] = df['date_flown_day'].astype(int)\ndf['date_flown_month'] = df['date_flown_month'].astype(int)\ndf['date_flown_year'] = df['date_flown_year'].astype(int)\n\ndf['seat_comfort'] = df['seat_comfort'].astype(int)\ndf['cabin_service'] = df['cabin_service'].astype(int)\ndf['ground_service'] = df['ground_service'].astype(int)\ndf['food_bev'] = df['food_bev'].astype(int)\ndf['value_for_money'] = df['value_for_money'].astype(int)\ndf['entertainment'] = df['entertainment'].astype(int)\n\nfor feat in num_feats:\n    if 'polarity' not in feat:\n        df[feat] = df[feat].astype(int)","e6824aec":"df.head()","6a0cf7b2":"X = df[feats]\ny = df['label'].values","4473e56a":"f_rec = (y[y==1].shape[0])\/y.shape[0]\nf_not_rec = (y[y==0].shape[0])\/y.shape[0]\nprint('Fraction of customers that recommeded the service: {:.2f}'.format(f_rec))\nprint('Fraction of customers that did not recommed the service: {:.2f}'.format(f_not_rec))","66288941":"# Create a pipeline for numerical features and a pipeline for categorical features.\nnum_proc = make_pipeline(SimpleImputer(missing_values=np.nan, strategy='mean'), StandardScaler())\ncat_proc = make_pipeline(SimpleImputer(strategy='constant', fill_value='missing'), OneHotEncoder(handle_unknown='ignore'))\n\n# Create a preprocessing step for all features.\npreprocessor = make_column_transformer((num_proc, num_feats),\n                                       (cat_proc, cat_feats))","3475960a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)","3354fbe7":"X_train_transformed = preprocessor.fit_transform(X_train)","538817b5":"cat_feats_one_hot = preprocessor.transformers_[1][1]['onehotencoder'].get_feature_names(cat_feats)\n# print(cat_feats_one_hot)\n\nall_feats = list(num_feats)+list(cat_feats_one_hot)\n# print(all_feats)\n\ndict_for_renaming_cols = {}\nfor i in range(len(all_feats)):\n    dict_for_renaming_cols[i] = all_feats[i]\n# print(dict_for_renaming_cols)","93688929":"if scipy.sparse.issparse(X_train_transformed):\n    X_train_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_train_transformed)\nelse:\n    X_train_transformed_2 = pd.DataFrame(X_train_transformed)\nX_train_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)\n\nX_test_transformed = preprocessor.transform(X_test)\nif scipy.sparse.issparse(X_test_transformed):\n    X_test_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_test_transformed)\nelse:\n    X_test_transformed_2 = pd.DataFrame(X_test_transformed)\nX_test_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)\n\nX_transformed = preprocessor.transform(X)\nif scipy.sparse.issparse(X_transformed):\n    X_transformed_2 = pd.DataFrame.sparse.from_spmatrix(X_transformed)\nelse:\n    X_transformed_2 = pd.DataFrame(X_transformed)\nX_transformed_2.rename(columns=dict_for_renaming_cols,inplace=True)","1979c5a6":"X_train.shape","d75d0c7e":"X_train_transformed_2.shape","1c04a6a0":"X_test.shape","ab2dcbfc":"X_test_transformed_2.shape","25fecd06":"le = LabelEncoder()","1fa7ea83":"# Make copies so that original aren't changed\nX_label_enc = X.copy()\nX_train_label_enc = X_train.copy()\nX_test_label_enc = X_test.copy()","b42fd96d":"for feat in cat_feats:\n    print('Feature:', feat)\n    X_label_enc[feat] = le.fit_transform(X_label_enc[feat])\n    X_train_label_enc[feat] = le.fit_transform(X_train_label_enc[feat])\n    X_test_label_enc[feat] = le.fit_transform(X_test_label_enc[feat])","91a28212":"X_label_enc[cat_feats].head()","2899e568":"if transform_dataset == 'scaling_and_one_hot_encoding':\n    print('Method for data tranformation: scaling and one hot encoding')\n    X_train_for_model = X_train_transformed_2\n    X_test_for_model = X_test_transformed_2\n    X_for_model = X_transformed_2\n    X_test_for_shap = X_test_transformed_2\n    X_for_shap = X_transformed_2\nelif transform_dataset == 'label_encoding':\n    print('Method for data transformation: label encoding')\n    X_train_for_model = X_train_label_enc\n    X_test_for_model = X_test_label_enc\n    X_for_model = X_label_enc\n    X_test_for_shap = X_test_label_enc\n    X_for_shap = X_label_enc\nelif transform_dataset == 'no_transformation':\n    print('Method for data transformation: no transformation')\n    X_train_for_model = X_train\n    X_test_for_model = X_test \n    X_for_model = X\n    X_test_for_shap = X_test\n    X_for_shap = X","ff4916bd":"cat_feats","5d362493":"if transform_dataset == 'scaling_and_one_hot_encoding':\n    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n    test_data=lgb.Dataset(X_test_for_model,label=y_test)\nelif transform_dataset == 'label_encoding':    \n    train_data=lgb.Dataset(X_train_for_model,label=y_train,categorical_feature=cat_feats)\n    test_data=lgb.Dataset(X_test_for_model,label=y_test,categorical_feature=cat_feats)\nelif transform_dataset == 'no_transformation':\n    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n    test_data=lgb.Dataset(X_test_for_model,label=y_test)\nelse:\n    train_data=lgb.Dataset(X_train_for_model,label=y_train)\n    test_data=lgb.Dataset(X_test_for_model,label=y_test)\n    \nparams = {'metric': 'binary_logloss', \n          'boosting_type': 'gbdt', \n          'objective': 'binary',\n          'feature_fraction': 0.5,\n          'num_leaves': 15,\n          'max_depth': 10,\n          'n_estimators': 200,\n          'min_data_in_leaf': 200, \n          'min_child_weight': 0.1,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.8,\n          'verbose': -1,\n}","65aa1611":"lgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=test_data,\n                 early_stopping_rounds= 100,\n                 verbose_eval= 20\n                 )\n\ny_prob = lgbm.predict(X_for_model)\ny_pred = y_prob.round(0)\n\nclf_roc_auc_score = roc_auc_score(y, y_prob)\nclf_accuracy_score = accuracy_score(y, y_pred)\n\nprint('Model overall ROC AUC score: {:.3f}'.format(clf_roc_auc_score))\nprint('Model overall accuracy: {:.3f}'.format(clf_accuracy_score))","bc46898c":"# Verify if the model has predicted a value between 1 and 0\nprint('Min value of prediction: {:.3f}'.format(y_pred.min()))\nprint('Max value of prediction: {:.3f}'.format(y_pred.max()))\nprint('Min value of probability: {:.3f}'.format(y_prob.min()))\nprint('Max value of probability: {:.3f}'.format(y_prob.max()))","87e33677":"# Getting all the accuracy metrics\ntn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n\nsensitivity = tp \/ (tp+fn) # Recall.\nspecificity = tn \/ (tn+fp)\nprecision = tp \/ (tp+fp)\n\nprint('Sensitivity\/Recall: %.2f' % sensitivity)\nprint('Specificity: %.2f' % specificity)\nprint('Precision: %.2f' % precision)","4ea05d58":"def plot_confusion_matrix(y, y_pred, normalize_str, figsize_w, figsize_h, filename):\n    \"\"\"\n    Plot the confusion matrix of a classifier.\n    \"\"\"\n    plt.figure(figsize=(figsize_w,figsize_h))\n    plt.title('Confusion matrix')\n    cm = confusion_matrix(y, y_pred, normalize=normalize_str)\n    df_cm = pd.DataFrame(cm, columns=np.unique(y), index = np.unique(y))\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_cm, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16})\n    plt.savefig(filename)\n    plt.show()\n    return","e357548d":"plot_confusion_matrix(y=y, y_pred=y_pred, normalize_str='true', figsize_w=4, figsize_h=4, filename='.\/ConfusionMatrix.png')","9e7d7da5":"# True positive rate and false positive rate.\nfpr, tpr, _ = roc_curve(y, y_prob)","d8f11171":"def plot_roc_curve(fpr, tpr, clf_name, figsize_w, figsize_h, filename):\n    \"\"\"\n    Plot the ROC curve of a classifier.\n    \"\"\"\n    plt.figure(figsize=(figsize_w,figsize_h))\n    sns.set(style=\"whitegrid\")\n    plt.plot([0, 1], [0, 1], 'k--', label='random')\n    plt.plot(fpr, tpr, label=clf_name)\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve')\n    plt.legend(loc='best')\n    plt.savefig(filename)\n    plt.show()\n    return","d740072a":"plot_roc_curve(fpr=fpr, tpr=tpr, clf_name='LightGBM', figsize_w=6, figsize_h=6, filename='.\/ROCCurve.png')","bc461471":"# Saving results in a fresh dataframe\ndf_out = pd.DataFrame()\ndf_out['y_pred'] = y_pred\ndf_out['y_prob'] = y_prob","318acb37":"def plot_hist_sns(df,feat,bins,title,x_label,y_label,filename):\n    \"\"\"\n    Plot the histogram of a given feature.\n    \"\"\"\n    plt.figure(figsize=(6,6))\n    sns.distplot(df[feat],bins=bins,kde=False)\n    plt.title(title)\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.grid(False)\n    plt.savefig(filename)\n    plt.show()\n    return","531b1b2b":"plot_hist_sns(df=df_out,\n             feat='y_prob',\n             bins=30,\n             title='Distribution of model prediction',\n             x_label='Predicted probability of being recommended',\n             y_label='Entries \/ bin',\n             filename='.\/HistModelPredictions.png')","130feda3":"df_out.to_csv(df_out_filename)","74007921":"# 7. Plotting the recommendation probability","546dff0b":"# 6. Saving the predictions in a new dataframe","9d375b5d":"# 4. Model training and prediction for the recommendation of airline by customers","9464209a":"# 5. Evaluation of trained model on different classification metrics ","fce7713d":"## 5.3 Plotting the ROC Curve","72a8a73c":"### 4.7.1 Choosing the transformation configuration","243e4f18":"### 4.6.1 Transform the data before training using the pipeline made in section 4.5","cd6d1acf":"# 2. Necessary Imports","bcd5ec19":"## 4.6 Dataset for training and testing","0503240a":"### 4.6.2 Label Encoding for categorical features","1b368e21":"## 4.2 Convert Boolean features to numerical","90fb06a3":"# About the Data\n\n![](https:\/\/cdn.britannica.com\/41\/123141-050-E6229449\/Air-New-Zealand-Boeing-747-400.jpg)\n\nThe data used here comes from a 2-part preprocessing of the original data.\n\nThe original data was preprocessed in 2 different stages, the preprocessing notebooks could be found here:\n\n   * [Part 1](https:\/\/www.kaggle.com\/divyansh22\/airline-reviews-eda-and-preprocessing-pt-1)\n   \n   * [Part 2](https:\/\/www.kaggle.com\/divyansh22\/airline-review-data-preprocessing-pt-2-nlp)\n   \nThe original data could be found [here](https:\/\/www.kaggle.com\/efehandanisman\/skytrax-airline-reviews).","e8d0b46e":"## 4.7 Model Training","b6208541":"# 3. Load the input data","9032cc6e":"### 4.7.3 Training and Predicting using the LGBM classifier ","8652e201":"# 8. Saving the predictions in an output file","e3836c58":"**Thanks a lot for taking out time to read my work! Please feel free to leave any comments and recommendations for me to improve my work. If you liked the work, please feel free to press that \"little upward arrow\" button :) !!!**\n\n**Cheers!!**","9d7ed2e4":"## 5.2 Plotting the confusion matrix ","6a0c7083":"## 4.1 Label the data on the basis of 'recommended' column","9c3ea95f":"## 4.5 Scaling numerical features and encoding the categorical features\nWe might want to scale numerical features, so that they have values in a common range.\n\nHere, we use the StandardScaler available in the sklearn library to normalize the features, that is, to subtract their mean and divide by their standard deviation. We transform x to z = (x-u)\/s. We can specify whether or no we want to subtract the mean with the option with_mean=True\/False and whether or not we want to divide by the standard deviation with the option with_std=True\/False. As a result, all the numerical features will have mean zero and unit standard deviation.\n\nWe also need to transform categorical features as well. Two common options are one-hot encoding and label encoding.\n\n1.  **One-hot encoding** allows to encode categorical features as one-hot vectors. The categorical feature is transformed into binary features, one for each category.\n\n    For example, the categorical feature 'cabin' can have four possible values: Economy Class, Premium Economy, Business Class and First Class. The one-hot encoding transform this feature, with four possible values, into four new features, called cabin_Economy, cabin_Premium Economy, cabin_Business and cabin_First, with each new feature having two possible values, 0 or 1, depending on the value of the original feature. A record with cabin equal to Economy Class will have cabin_Economy Class equal to 1 and all the other three features equal to 0.\n    This could lead to sparse data (most of the elements in the dataset will have the value 0) if the features can have many possible values.\n\n2. **Label encoding** allows to encode categorical features as numbers.\n\n    For example, the categorical feature cabin can be encoded as one feature with values 0, 1, 2 and 3.\n\nHere, we use a pipeline to define the data processing, so that we can repeat the same steps for the training and test datasets. In particular, the parameters of the data processing are defined based on the training dataset and are then applied to the test dataset.","95c52ecb":"### 4.7.2 Converting the dataset into an lgb Dataset and setting the parameters for model training\nLightGBM model works on a specific datatype. The normal Pandas dataframe could be easily converted into that specific type by using lgb.Dataset() function","c3125b60":"## 5.1 Getting Recall, Precision and Specificity scores","fc43e7ee":"# 1. Decide the model configuration","465783e2":"## 4.3 Select features for training and labels for prediction","abfb006c":"## 4.4 Check for class imbalance"}}