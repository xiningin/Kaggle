{"cell_type":{"e42816e4":"code","43d0ffd2":"code","4dc2e4df":"code","660f38a2":"code","dfbaf8d2":"code","2a263a27":"code","e1097dce":"code","d6f8144e":"code","3e51354c":"code","176b55c9":"code","a8a06b01":"code","a3dc3a19":"code","27d4a6c0":"code","41d22c65":"code","b275cd56":"code","99a0eead":"code","7b5f3caa":"code","f9d4e180":"code","c094d99b":"code","8f82a3b2":"code","b61efebe":"code","b0d0c6cf":"code","28a324fb":"code","725935e8":"code","7d819a66":"code","fc4818c5":"code","47447658":"code","4be21fab":"code","4425c6d7":"code","4bf9e6f6":"code","3a9710e7":"code","350f0d1e":"code","d0916e1b":"code","9e70f546":"code","1483dfe7":"code","95148dd6":"code","2ef456e3":"code","517d9331":"code","707bd53f":"code","faab9429":"code","71c9025a":"code","186590e4":"code","d401515f":"code","7e9d68ae":"code","241e7552":"code","095189e9":"code","5dceb0d2":"code","860b6f1a":"code","4eff4c46":"markdown","383c487a":"markdown","4fb25f22":"markdown","f973280c":"markdown","6d9a67ac":"markdown","f8253b14":"markdown","f98a5b30":"markdown","0243dcbd":"markdown","d8f3ae5a":"markdown","b13ca53c":"markdown","825424ef":"markdown","32ab5c6a":"markdown","6daa2883":"markdown","827be9a4":"markdown","146257f0":"markdown","c0da6b87":"markdown","a6ecb7eb":"markdown"},"source":{"e42816e4":"from nltk.tokenize import TweetTokenizer","43d0ffd2":"cols = ['sentiment','id','date','query_string','user','text']\ndf = pd.read_csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",header=None, names=cols)\n# above line will be different depending on where you saved your data, and your file name\ndf.head()","4dc2e4df":"df.info()","660f38a2":"df.describe()","dfbaf8d2":"df.sentiment.value_counts()","2a263a27":"val_count = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_count.index, val_count.values)\nplt.title(\"Sentiment Data Distribution\")","e1097dce":"df.drop(['id','date','query_string','user'],axis=1,inplace=True)","d6f8144e":"df[df.sentiment == 0].head(10)","3e51354c":"df[df.sentiment == 4].head(10)","176b55c9":"df['pre_clean_len'] = [len(t) for t in df.text]\nfig, ax = plt.subplots(figsize=(5, 5))\nplt.boxplot(df.pre_clean_len)\nplt.show()","a8a06b01":"# Set log\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","a3dc3a19":"df[df.pre_clean_len > 140].head(10)","27d4a6c0":"df.text[279]","41d22c65":"example1 = BeautifulSoup(df.text[279], 'lxml')\nprint(example1.get_text())","b275cd56":"df.text[343]","99a0eead":"import re\nre.sub(r'@[A-Za-z0-9]+','',df.text[343])","7b5f3caa":"df.text[0]","f9d4e180":"re.sub('https?:\/\/[A-Za-z0-9.\/]+','',df.text[0])","c094d99b":"from nltk.tokenize import WordPunctTokenizer\ntok = WordPunctTokenizer()\npat1 = r'@[A-Za-z0-9]+'\npat2 = r'https?:\/\/[A-Za-z0-9.\/]+'\ncombined_pat = r'|'.join((pat1, pat2))\ndef tweet_cleaner(text):\n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    stripped = re.sub(combined_pat, '', souped)\n    clean = stripped\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n    lower_case = letters_only.lower()\n    # During the letters_only process two lines above, it has created unnecessay white spaces,\n    # I will tokenize and join together to remove unneccessary white spaces\n    words = tok.tokenize(lower_case)\n    return (\" \".join(words)).strip()\ntesting = df.text[:100]\ntest_result = []\nfor t in testing:\n    test_result.append(tweet_cleaner(t))\ntest_result","8f82a3b2":"nums = [0,400000,800000,1200000,1600000]\nprint(\"Cleaning and parsing the tweets... \\n\")\nclean_tweet_texts = []\nfor i in range(nums[0],nums[4]):\n    if( (i+1)%10000 == 0 ):\n        print(\"Tweets %d of %d has been processed\" % ( i+1, nums[1] ))                                                                    \n    clean_tweet_texts.append(tweet_cleaner(df['text'][i]))","b61efebe":"clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\nclean_df['target'] = df.sentiment\nclean_df.head()","b0d0c6cf":"clean_df.to_csv('clean_tweet.csv',encoding='utf-8')\ncsv = 'clean_tweet.csv'\ndf = pd.read_csv(csv,index_col=0)\ndf.head()","28a324fb":"df.info()","725935e8":"df=df.dropna()\ndf.info()","7d819a66":"start_time = time()\n\nfrom nltk.tokenize import TweetTokenizer\n# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\ntk = TweetTokenizer(reduce_len=True)\n\ndata = []\n\n# Separating our features (text) and our labels into two lists to smoothen our work\nX = df['text'].tolist()\nY = df['target'].tolist()\n\n# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n# and its corresponding label\nfor x, y in zip(X, Y):\n    if y == 4:\n        #print(type(x))\n        data.append((tk.tokenize(x), 1))\n    else:\n        #print(type(x))\n        #print(x)\n        data.append((tk.tokenize(x), 0))\n        \n# Printing the CPU time and the first 5 elements of our 'data' list\nprint('CPU Time:', time() - start_time)\ndata[:5]","fc4818c5":"from nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\n# Previewing the pos_tag() output\nprint(pos_tag(data[0][0]))","47447658":"import re, string\n\n# Stopwords are frequently-used words (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that do not hold any meaning useful to extract sentiment.\n# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words('english')\n\n# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance. \ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '\u00bd25':\n        return ''\n    return token\n\n# This function will be our all-in-one noise removal function\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Eliminating the token if it is a link\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        # Eliminating the token if it is a mention\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        cleaned_token = cleaned(token.lower())\n        \n        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens\n\n# Prevewing the remove_noise() output\nprint(remove_noise(data[0][0]))","4be21fab":"#!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n#!unzip .\/glove.6B.zip","4425c6d7":"def read_glove_vecs(glove_file):\n    with open(glove_file, 'r', encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map","4bf9e6f6":"word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('.\/glove.6B.50d.txt')","3a9710e7":"word_to_index['hello']","350f0d1e":"word_to_vec_map['hello']","d0916e1b":"word_to_index['unk']","9e70f546":"def cosine_similarity(u, v):\n    dot = np.dot(u, v)\n    norm_u = np.sqrt(np.sum(u**2))\n    norm_v = np.sqrt(np.sum(v**2))\n    cosine_similarity = dot \/ (norm_u * norm_v)\n    return cosine_similarity","1483dfe7":"cosine_similarity(word_to_vec_map['cucumber'], word_to_vec_map['tomato'])","95148dd6":"cosine_similarity(word_to_vec_map['cucumber'], word_to_vec_map['phone'])","2ef456e3":"import re, string\n\n# Stopwords are frequently-used words (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that do not hold any meaning useful to extract sentiment.\n# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nSTOP_WORDS = stopwords.words('english')\n\n# A custom function defined in order to fine-tune the cleaning of the input text. This function is highly dependent on each usecase.\n# Note: Only include misspelling or abbreviations of commonly used words. Including many minimally present cases would negatively impact the performance. \ndef cleaned(token):\n    if token == 'u':\n        return 'you'\n    if token == 'r':\n        return 'are'\n    if token == 'some1':\n        return 'someone'\n    if token == 'yrs':\n        return 'years'\n    if token == 'hrs':\n        return 'hours'\n    if token == 'mins':\n        return 'minutes'\n    if token == 'secs':\n        return 'seconds'\n    if token == 'pls' or token == 'plz':\n        return 'please'\n    if token == '2morow':\n        return 'tomorrow'\n    if token == '2day':\n        return 'today'\n    if token == '4got' or token == '4gotten':\n        return 'forget'\n    if token == 'amp' or token == 'quot' or token == 'lt' or token == 'gt' or token == '\u00bd25':\n        return ''\n    return token\n\n# This function will be our all-in-one noise removal function\ndef remove_noise(tweet_tokens):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(tweet_tokens):\n        # Eliminating the token if it is a link\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n        # Eliminating the token if it is a mention\n        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        cleaned_token = cleaned(token.lower())\n        \n        # Eliminating the token if its length is less than 3, if it is a punctuation or if it is a stopword\n        if cleaned_token not in string.punctuation and len(cleaned_token) > 2 and cleaned_token not in STOP_WORDS:\n            cleaned_tokens.append(cleaned_token)\n            \n    return cleaned_tokens\n\n# Prevewing the remove_noise() output\nprint(remove_noise(data[0][0]))","517d9331":"start_time = time()\n\n# As the Naive Bayesian classifier accepts inputs in a dict-like structure,\n# we have to define a function that transforms our data into the required input structure\ndef list_to_dict(cleaned_tokens):\n    return dict([token, True] for token in cleaned_tokens)\n\ncleaned_tokens_list = []\n\n# Removing noise from all the data\nfor tokens, label in data:\n    cleaned_tokens_list.append((remove_noise(tokens), label))\n\nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nfinal_data = []\n\n# Transforming the data to fit the input structure of the Naive Bayesian classifier\nfor tokens, label in cleaned_tokens_list:\n    final_data.append((list_to_dict(tokens), label))\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n# Previewing our final (tokenized, cleaned and lemmatized) data list\nfinal_data[:5]","707bd53f":"\nstart_time = time()\n\nunks = []\nUNKS = []\n\n# This function will act as a \"last resort\" in order to try and find the word\n# in the words embedding layer. It will basically eliminate contiguously occuring\n# instances of a similar character\ndef cleared(word):\n    res = \"\"\n    prev = None\n    for char in word:\n        if char == prev: continue\n        prev = char\n        res += char\n    return res\n\n\ndef sentence_to_indices(sentence_words, word_to_index, max_len, i):\n    global X, Y\n    sentence_indices = []\n    for j, w in enumerate(sentence_words):\n        try:\n            index = word_to_index[w]\n        except:\n            UNKS.append(w)\n            w = cleared(w)\n            try:\n                index = word_to_index[w]\n            except:\n                index = word_to_index['unk']\n                unks.append(w)\n        X[i, j] = index\n\n        \n# Here we will utilize the already computed 'cleaned_tokens_list' variable\n   \nprint('Removed Noise, CPU Time:', time() - start_time)\nstart_time = time()\n\nlist_len = [len(i) for i, j in cleaned_tokens_list]\nmax_len = max(list_len)\nprint('max_len:', max_len)\n\nX = np.zeros((len(cleaned_tokens_list), max_len))\nY = np.zeros((len(cleaned_tokens_list), ))\n\nfor i, tk_lb in enumerate(cleaned_tokens_list):\n    tokens, label = tk_lb\n    sentence_to_indices(tokens, word_to_index, max_len, i)\n    Y[i] = label\n    \nprint('Data Prepared for model, CPU Time:', time() - start_time)\n\n\nprint(X[:5])\nprint(Y[:5])","faab9429":"import keras\nfrom keras import Sequential\nfrom keras.models import Model\nfrom keras.layers import Dense, Dropout, LSTM, Bidirectional\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split","71c9025a":"# Defining a function that will initialize and populate our embedding layer\n\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n    vocab_len = len(word_to_index) + 1\n    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n    \n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    for word, idx in word_to_index.items():\n        emb_matrix[idx, :] = word_to_vec_map[word]\n        \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","186590e4":"model = Sequential()\n\nmodel.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=False)))\nmodel.add(Dense(units=1, activation='sigmoid'))\n\nmodel.summary()","d401515f":"# Compiling our model with a binary cross-entropy loss function, using the default adam optimizer\n# and setting the accurary as the metric to track and ameliorate\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","7e9d68ae":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)","241e7552":"model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 20, batch_size = 128, shuffle=True)","095189e9":"def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.RdBu):\n    \"\"\"\n    Source: \n         http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \n    Input:\n         cm: confusion matrix\n         classes: output classes name\n         normalize: normalization can be applied by setting `normalize=True`\n    \n    Output:\n         This function prints and plots the confusion matrix.\n    \"\"\"\n    plt.figure(figsize=(20,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"green\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","5dceb0d2":"def plot_acc_loss(history):\n\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n    plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.figure()\n    plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n    plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","860b6f1a":"plot_acc_loss(model.history)","4eff4c46":"## Preprocessing","383c487a":"### class distribution","4fb25f22":"### Data Description","f973280c":"##### Data Preparation","6d9a67ac":"### Word Embeddings","f8253b14":"### DataFrame","f98a5b30":"I first started by dropping the columns that I don\u2019t need for the specific purpose of sentiment analysis.\n\u201cid\u201d column is unique ID for each tweet\n\u201cdate\u201d column is for date info for the tweet\n\u201cquery_string\u201d column indicates whether the tweet has been collected with any particular query key word, but for this column, 100% of the entries are with value \u201cNO_QUERY\u201d\n\u201cuser\u201d column is the twitter handle name for the user who tweeted\nI decided to drop above four columns.","0243dcbd":"By looking at some entries for each class, it seems like that all the negative class is from 0~799999th index, and the positive class entries start from 800000 to the end of the dataset.","d8f3ae5a":"### DataCleaning FUnction","b13ca53c":"### Data Cleaning","825424ef":"### Tokenization","32ab5c6a":"#### Data Preparation","6daa2883":"### Imported Libraries","827be9a4":"### URL links","146257f0":"#### \u2018@\u2019mention","c0da6b87":"### HTML Decoding","a6ecb7eb":"Dataset has 1.6million entries, with no null entries, and importantly for the \u201csentiment\u201d column, even though the dataset description mentioned neutral class, the training set has no neutral class.\n50% of the data is with negative label, and another 50% with positive label.\nWe can see there\u2019s no skewness on the class division."}}