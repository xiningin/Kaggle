{"cell_type":{"f386959d":"code","8e2b5601":"code","4b1dda59":"code","df7aeca9":"code","ad7ffe42":"code","f750fa6d":"code","0ad59c1f":"code","f31b377f":"markdown","f7354cc2":"markdown","7f0485d7":"markdown","0f455bb2":"markdown","fc329a5a":"markdown","1e49be65":"markdown","1cc66a84":"markdown"},"source":{"f386959d":"import numpy as np","8e2b5601":"from keras.datasets import mnist\n(x_train, _), (x_test, _) = mnist.load_data()\nx_train = (x_train \/ 255).reshape(-1, 784)\nx_test = (x_test \/ 255).reshape(-1, 784)","4b1dda59":"def sigmoid(z, d=False):\n    return sigmoid(z) * (1 - sigmoid(z)) + 1e-12 if d else 1 \/ (1 + np.exp(-z))\n\ndef relu(z, d=False):\n    return (z > 0)+1e-12 if d else  z * (z > 0)","df7aeca9":"layers = [\n    # activation, shape:(out,in)\n    {\"act\":relu, \"shape\":(1024,784)},\n    {\"act\":relu, \"shape\":(50,1024)},\n    {\"act\":relu, \"shape\":(1024,50)},\n    {\"act\":sigmoid, \"shape\":(784,1024)}\n]","ad7ffe42":"# global properties\nl, errors, epochs = len(layers), [], 30\n# adam properties\nlr, b1, b2 = 0.002, 0.9, 0.999\nrw,mw,rb,mb = {},{},{},{}\n# layer properties\na,w,b,f, = {},{},{},{}\nfor i, layer in zip(range(1,l+1), layers):\n    n_out, n_in = layer[\"shape\"]\n    f[i] = layer[\"act\"]\n    # Xavier Initialization of weights\n    w[i] = np.random.randn(n_out, n_in) \/ n_in**0.5\n    b[i], rb[i], mb[i] = [np.zeros((n_out,1)) for i in [1,2,3]]\n    rw[i], mw[i] = [np.zeros((n_out, n_in)) for i in [1,2]]","f750fa6d":"for t in range(1, epochs+1):\n    # Train\n    for batch in np.split(x_train, 30):\n        # Forward pass\n        a[0] = batch.T\n        for i in range(1,l+1):\n            a[i] = f[i]((w[i] @ a[i-1]) + b[i])\n        # Backpropagation\n        dz,dw,db = {},{},{}\n        for i in range(1,l+1)[::-1]:\n            d = w[i+1].T @ dz[i+1] if l-i else 0.5*(a[l]-a[0])\n            dz[i] = d * f[i](a[i],d=1)\n            dw[i] = dz[i] @ a[i-1].T\n            db[i] = np.sum(dz[i], 1, keepdims=True)\n        # Adam updates\n        def adam(m, r, z, dz, i):\n            m[i] = b1 * m[i] + (1 - b1) * dz[i]\n            r[i] = b2 * r[i] + (1 - b2) * dz[i]**2\n            m_hat = m[i] \/ (1. - b1**t)\n            r_hat = r[i] \/ (1. - b2**t) \n            z[i] -= lr * m_hat \/ (r_hat**0.5 + 1e-12)\n        for i in range(1,l+1):\n            adam(mw, rw, w, dw, i)\n            adam(mb, rb, b, db, i)\n    # Validate\n    a[0] = x_test.T\n    for i in range(1,l+1):\n        a[i] = f[i]((w[i] @ a[i-1]) + b[i])\n    errors += [np.mean((a[l]-a[0])**2)]\n    print(\"Val loss - \", errors[-1])","0ad59c1f":"import matplotlib.pyplot as plt\ny_pred = []\na[0] = x_train[:20].T\n#forward pass\nfor i in range(1,l+1):\n    a[i] = f[i](w[i] @ a[i-1] + b[i])\ny_pred = a[l]\n\nplt.figure(figsize=(20,5))\n\nfor i in range(20):\n    plt.subplot(3, 20, i + 1)\n    plt.imshow(x_train[i].reshape(28,28), cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.grid(b=False)\n\nfor i in range(20):\n    plt.subplot(3, 20, i + 1 + 20)\n    plt.imshow(a[l-2].T[i].reshape(5,-1), cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.grid(b=False)\n    \nfor i in range(20):\n    plt.subplot(3, 20, i + 1 + 40)\n    plt.imshow(y_pred.T[i].reshape(28,28), cmap=\"gray\")\n    plt.axis(\"off\")\n    plt.grid(b=False)\n\nplt.show()","f31b377f":"# Auto Encoder in Raw Numpy\nFeatures: \n\n+ Fully Connected ReLU layers \n+ Sigmoid output \n+ MSE Loss \n+ Adam optimizer\n+ Xavier Layer initialization for Dense networks\n+ Example run on mnist\n\nBy Tobias Schiele @mailto[53513@hs-aalen.de]","f7354cc2":"## Print Results","7f0485d7":"## Define Activation Functions","0f455bb2":"## Train","fc329a5a":"## Initialize Layers","1e49be65":"## Data","1cc66a84":"## Define Architecture"}}