{"cell_type":{"1ce0b491":"code","ec17b1bf":"code","f2320c6d":"code","ccf8968c":"code","cadbd055":"code","a49a4a44":"code","5c443968":"code","7d82f4b8":"code","5dfd4b9d":"code","92b76c9f":"code","06711e0c":"code","5c5eab19":"code","9a80e074":"code","67587498":"code","5dbf19e1":"code","5769acc3":"code","dfcd23c4":"code","7107e109":"code","6fae77f0":"code","2c6d94f6":"code","cee7052e":"code","878ffb27":"code","549ff991":"code","9af87126":"code","6569aa9d":"code","3fa1695b":"code","904e1c2d":"code","3adb1924":"code","8a750b83":"code","03106e73":"code","df2113ac":"code","323b5aa7":"code","28235fce":"code","ced44568":"code","1b3b9939":"code","f0d59706":"code","ad38bafd":"code","dd8745b8":"code","f6ffe553":"code","6adde135":"code","c168cd3b":"code","ab4276eb":"code","e3448182":"code","3e2d7f98":"code","4d827f4f":"code","cb3843f3":"code","2c2b0cf2":"markdown","840ae260":"markdown","13531102":"markdown","62fa2923":"markdown","e96b1564":"markdown"},"source":{"1ce0b491":"#!conda create -n rapids-0.16 -c rapidsai -c nvidia -c conda-forge \\\n#    -c defaults rapids=0.16 python=3.7 cudatoolkit=10.1 -y","ec17b1bf":"seed=66","f2320c6d":"import os\nimport re\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n#import cudf\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 5);\nsns.set_style('whitegrid')","ccf8968c":"items = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ncats = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ntrain = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nsub_df = pd.read_csv(\n    \"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")","cadbd055":"train.head(3)\ntest.head(3)","a49a4a44":"items.head(3)\nshops.head(3)\ncats.head(3)","5c443968":"shops[shops['shop_id']==0]\nshops[shops['shop_id']==57]","7d82f4b8":"train.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\ntrain.loc[train.shop_id == 23, 'shop_id'] = 24\ntest.loc[test.shop_id == 23, 'shop_id'] = 24\ntrain.loc[train.item_id == 69, 'item_id'] = 70\ntest.loc[test.item_id == 69, 'item_id'] = 70\n\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops.loc[shops.shop_name.str.contains(\n    '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c', case=False),\"shop_name\"] = '\u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c'\nshops.loc[shops.shop_name.str.contains(\n    '\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56', case=False),\"shop_name\"] = '\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56'\nshops.loc[shops.shop_name.str.contains(\n    '!\u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\" \u0444\u0440\u0430\u043d', case=False),\"shop_name\"] = '\u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\" \u0444\u0440\u0430\u043d'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map(lambda x: x[0])\nshops[\"category\"] = shops.shop_name.str.split(\" \").map(lambda x: x[1])\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"","5dfd4b9d":"shops.category.value_counts()","92b76c9f":"counts = shops['category'].value_counts()\nidx = counts[counts.lt(4)].index\n\nshops.loc[shops['category'].isin(idx), 'category'] = 'OTHER'\nshops.loc[shops[shops.city=='\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434'].index, 'city'] = '\u0422\u0426'\nshops.head()","06711e0c":"shops.city.unique()\nshops.category.unique()","5c5eab19":"shops.head(3)","9a80e074":"set(train['item_id']) == set(test['item_id'])\nset(train['shop_id']) == set(test['shop_id'])","67587498":"train['item_in_test'] = train['item_id'].isin(list(test['item_id']))\ntrain['shop_in_test'] = train['shop_id'].isin(list(test['shop_id']))","5dbf19e1":"train.shape[0]","5769acc3":"train['item_in_test'].sum()\ntrain['shop_in_test'].sum()","dfcd23c4":"items.head(3)","7107e109":"items['item_name'].unique()","6fae77f0":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport re\n\nstop_words_en = set(stopwords.words('english'))\nstop_words_ru = set(stopwords.words('russian'))\n\ndef get_fdist(df):\n    fdist = FreqDist()\n    for item in df['item_name'].unique():\n        for w in word_tokenize(item):\n            word = re.sub(r\"(`|\\*|'|!)+\", '', w)\n            if len(word) < 2:\n                continue\n            if word.lower() in stop_words_en or word.lower() in stop_words_ru:\n                continue\n            fdist[word.lower()] += 1\n            \n    return fdist\n            \nfdist = get_fdist(items)","2c6d94f6":"fdist.most_common(10)","cee7052e":"def broad_category(string):\n    if '-' in string:\n        cat = string.split('-')[0].strip(' ')\n    elif '(' in string:\n        cat = string.split('(')[0].strip(' ')\n    else:\n        cat = string\n    return cat\n\ndef narrow_category(string):\n    if '-' in string:\n        cat = string.split('-')[1].strip(' ')\n    elif '(' in string:\n        cat = string.split('(')[1].strip(' ').replace(')', '')\n    else:\n        cat = 'UNK'\n    return cat\n\ncats['super_cat'] = cats['item_category_name'].apply(broad_category)\ncats['sub_cat'] = cats['item_category_name'].apply(narrow_category)","878ffb27":"super_cat_dict = {x:y for x, y in zip(\n    cats['super_cat'].unique(), range(1, cats['super_cat'].nunique()+1))}\nsub_cat_dict = {x:y for x, y in zip(\n    cats['sub_cat'].unique(), range(1, cats['sub_cat'].nunique()+1))}\n\nsuper_cat_index = {y:x for x, y in super_cat_dict.items()}\nsub_cat_index = {y:x for x, y in sub_cat_dict.items()}","549ff991":"cats_m = pd.merge(items, cats, on='item_category_id').drop('item_category_id', axis=1)\ncats_m.head()","9af87126":"items_train = pd.merge(cats_m, train, on='item_id')\nitems_test = pd.merge(cats_m, test, on='item_id').drop('item_category_name', axis=1)\nitems_train = pd.merge(items_train, shops, on='shop_id')\nitems_test = pd.merge(items_test, shops, on='shop_id')\n\nassert items_train.shape[0]==train.shape[0]\nassert items_test.shape[0]==test.shape[0]\nitems_train.head(3)\nitems_test.head(3)","6569aa9d":"items_train.super_cat.unique()","3fa1695b":"fig, ax = plt.subplots(1, 2)\nsns.distplot(items_train['item_price'].dropna(), ax=ax[0]);\nsns.distplot(items_train['item_price'].fillna(items_train['item_price'].median()), ax=ax[1]);","904e1c2d":"fig, ax = plt.subplots(1, 2)\nsns.distplot(np.log(items_train['item_price'].dropna()), ax=ax[0]);\nsns.distplot(np.log(items_train['item_price'].fillna(items_train['item_price'].median())), ax=ax[1]);","3adb1924":"# for now what we want is less memory usage :)\nto_drop = ['item_name', 'date', \n           'shop_name', 'item_category_name']\nitems_train = items_train.drop(to_drop, axis=1)","8a750b83":"# items_train = cudf.DataFrame.from_pandas(items_train)\ngrouped = items_train.groupby(['item_id', 'shop_id', 'date_block_num'], as_index=False)","03106e73":"def mean_log(values):\n    if len(values)==1:\n        return np.log(values)\n    else:\n        return np.log(np.mean(values))\n    \ndef to_int(values):\n    if len(values)==1:\n        return values.astype('int')\n    else:\n        return values.astype('int').mode()[0]\n    \ndef encode_super_cats(string, codes=super_cat_dict):\n    if len(string)==1:\n        s = string.values[0]\n    else:\n        s = string.tolist()[0]\n    return codes[s]\n\ndef encode_sub_cats(string, codes=sub_cat_dict):\n    if len(string)==1:\n        s = string.values[0]\n    else:\n        s = string.tolist()[0]\n    return codes[s]\n        \n\nitems_train = grouped.agg({'super_cat': encode_super_cats, \n             'sub_cat': encode_sub_cats, \n             'item_price': mean_log, \n             'item_cnt_day': np.sum,\n            'item_in_test': to_int,\n            'shop_in_test': to_int})\n\nitems_train.head()","df2113ac":"grouped = items_test.groupby(['item_id', 'shop_id'], as_index=False)\nitems_test = grouped.agg({'super_cat': encode_super_cats, \n             'sub_cat': encode_sub_cats,})\n\nitems_test.head()","323b5aa7":"def get_price_dict(i_df=items, ref_df=items_train):\n    p_dict = dict()\n    for i in tqdm(i_df.item_id.unique()):\n        if i in ref_df.item_id:\n            p_dict[i] = ref_df[ref_df.item_id==i].item_price.mean()\n        else:\n            p_dict[i] = None\n            \n    return p_dict\n\ndef get_cnt_dict(ref_df=items_train):\n    p_dict = dict()\n    for i in tqdm(ref_df.item_id.unique()):\n        p_dict[i] = ref_df[ref_df.item_id==i].item_cnt_day.tolist()[0]\n    return p_dict\n\ncnt_dict = get_cnt_dict()\nprice_dict = get_price_dict()","28235fce":"# price_dict = get_price_dict()\ndef get_price(row):\n    i = row['item_id']\n    if price_dict[i] is not None:\n        return price_dict[i] #row['item_price']\n    i = items_train[items_train.sub_cat==row['sub_cat']].item_price.mean()\n    return i\n\ndef get_cnt(row):\n    i = row['item_id']\n    #if cnt_dict[i] is not None:\n    if i in cnt_dict.keys():\n        return cnt_dict[i] #row['item_price']\n    i = items_train[items_train.sub_cat==row['sub_cat']].item_cnt_day.mode()[0]\n    return i\n\n#assert items_train.shape[0]==train.shape[0]\n#assert items_test.shape[0]==test.shape[0]\n    \nitems_test.loc[:, 'item_price'] = items_test.apply(get_price, axis=1)\nitems_test.loc[:, 'item_cnt_day'] = items_test.apply(get_cnt, axis=1)","ced44568":"items_test #.loc[items_test, 'item_price']","1b3b9939":"items_train.sample(3)\nitems_test.sample(3)","f0d59706":"X = items_train.drop(['item_cnt_day', \n                      'date_block_num', \n                      'item_id', \n                      'item_in_test', \n                      'shop_in_test',\n                     'item_price'], axis=1) \n#[['super_cat', 'sub_cat']]\ny = items_train.item_cnt_day\nX\ny","ad38bafd":"from catboost import CatBoostRegressor","dd8745b8":"from sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nkf = KFold(n_splits=5)\nmodel = CatBoostRegressor(cat_features=['shop_id', 'super_cat', 'sub_cat'])\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=seed)\n\nmodel = model.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint('rmse:', mean_squared_error(y_test, preds, squared=False))\n\"\"\"\nfor train_index, test_index in kf.split(X):\n\n    X_train, X_test = X.loc[train_index, :], X.loc[test_index, :]\n    y_train, y_test = y[train_index], y[test_index]\n    clf = model.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n    print('rmse:', mean_squared_error(y_test, preds, squared=False))\n\"\"\"","f6ffe553":"items_test.item_id","6adde135":"error_ids = []\nfor ind in tqdm(sub_df.index):\n    item = sub_df.loc[ind, 'ID']\n    vals = items_train[items_train.item_id==item][['shop_id', 'super_cat', 'sub_cat']]\n    if len(vals)==0:\n        vals = items_test[items_test.item_id==item][['shop_id', 'super_cat', 'sub_cat']]\n    #print(vals)\n    try:\n        sub_df.loc[ind, 'shop_id'] = vals.shop_id.tolist()[-1]\n        sub_df.loc[ind, 'super_cat'] = vals.super_cat.tolist()[-1]\n        sub_df.loc[ind, 'sub_cat'] = vals.sub_cat.tolist()[-1]\n    except Exception:\n        sub_df.loc[ind, 'shop_id'] = items_train.shop_id.mode()[0]\n        sub_df.loc[ind, 'super_cat'] = items_train.super_cat.mode()[0]\n        sub_df.loc[ind, 'sub_cat'] = items_train.sub_cat.mode()[0]\n        error_ids.append(item)","c168cd3b":"sub_df\nerror_ids","ab4276eb":"X","e3448182":"sub_df[['shop_id', 'super_cat', 'sub_cat']].values","3e2d7f98":"preds = model.predict(sub_df[['shop_id', 'super_cat', 'sub_cat']].astype('int'))","4d827f4f":"sub_df.item_cnt_month = preds\nsub_df","cb3843f3":"sub_df = sub_df[['ID', 'item_cnt_month']]\nsub_df.to_csv('submission.csv', index=False)","2c2b0cf2":"### Introduction\n\nData Science is nothing without data and Machine Learning does not drive far without feature engineering. Data for <a href='https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales'>Predict Future Sales<\/a> competition reminds us about it.\n\nIn this notebook I plan to:\n* explore data given for the competition\n* get some insights based on (at least quick) EDA\n* generate some additional (hopefully usefull) features for the dataset\n* make some model and train it\n* test\/validate the results\n* make at least baseline predictions and submission\n\nP.S. I update the notebook as I can devote some time, if there are parts not yet covered from my plan, I am on my way to cover those in the next versions.","840ae260":"Thanks @abdalazez for spoting some duplicates the set! Check out <a href='https:\/\/www.kaggle.com\/abdalazez\/predict-future-sales-2020'>his notebook<\/a> Overall we discover many duplicates throught the dataset, maybe those I deal with is not the full list. But take a look at a few of these.","13531102":"Let's first see if we can get some information from categories alone.","62fa2923":"Bad, I know. But let's build a submission pipeline so do not worry about it later.","e96b1564":"With my attempt to get an idea what kind of shops we are dealing with some archtifacts arose. Seems like some shops did not contain any information regarding the shop category, a few categories we get seem to be to rare to be meaningfull. For now I will consider one category 'OTHER' for all these cases."}}