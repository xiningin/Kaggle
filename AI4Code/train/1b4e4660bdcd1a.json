{"cell_type":{"10b45dcb":"code","931f9c03":"code","4ed45f89":"code","1c5188ea":"code","5caafbf4":"code","984fadb0":"code","ce7bb45b":"code","8ab4857b":"code","85b15fb9":"code","19f9ed58":"code","9ab2473c":"code","002e8a21":"code","5203155d":"code","29e6240f":"code","cacdd298":"code","65b2a3c8":"code","658235b4":"code","6956b007":"code","f14f5e2c":"code","1773b811":"code","f155b8b0":"code","9b4f21b5":"code","d591407a":"code","2cddeadf":"markdown","038da4a2":"markdown","5ba2dfdc":"markdown","ac854b61":"markdown","95c60552":"markdown","ec8fef0c":"markdown","c1127e57":"markdown","aff898a4":"markdown","bddca5e8":"markdown","c6e57f87":"markdown","2f0b368a":"markdown"},"source":{"10b45dcb":"import pandas as pd\nimport numpy as np\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score","931f9c03":"%%time\napp = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv')\ncredit = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')","4ed45f89":"app.head()","1c5188ea":"app.columns","5caafbf4":"# dropping duplicate rows\napp.drop_duplicates(subset = ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY', 'CNT_CHILDREN',\n       'AMT_INCOME_TOTAL', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',\n       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'DAYS_BIRTH',\n       'DAYS_EMPLOYED', 'FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE',\n       'FLAG_EMAIL', 'OCCUPATION_TYPE', 'CNT_FAM_MEMBERS'], keep = 'first', inplace = True)","984fadb0":"app.drop('FLAG_OWN_REALTY', axis = 1, inplace = True)\napp.columns","ce7bb45b":"app.shape","8ab4857b":"app.info()","85b15fb9":"credit.head()","19f9ed58":"credit.shape","9ab2473c":"credit.info()","002e8a21":"# Data frame to analyze length of time since initial approval of credit card\n# Shows number of past dues, paid off and no loan status.\ngrouped = credit.groupby('ID')\n\npivot_tb = credit.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min()\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max()\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month']\npivot_tb['window'] += 1 # Adding 1 since month starts at 0.\n\n#Counting number of past dues, paid offs and no loans.\npivot_tb['paid_off'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'C'].count(axis = 1)\npivot_tb['pastdue_1-29'] = pivot_tb[pivot_tb.iloc[:,0:61] == '0'].count(axis = 1)\npivot_tb['pastdue_30-59'] = pivot_tb[pivot_tb.iloc[:,0:61] == '1'].count(axis = 1)\npivot_tb['pastdue_60-89'] = pivot_tb[pivot_tb.iloc[:,0:61] == '2'].count(axis = 1)\npivot_tb['pastdue_90-119'] = pivot_tb[pivot_tb.iloc[:,0:61] == '3'].count(axis = 1)\npivot_tb['pastdue_120-149'] = pivot_tb[pivot_tb.iloc[:,0:61] == '4'].count(axis = 1)\npivot_tb['pastdue_over_150'] = pivot_tb[pivot_tb.iloc[:,0:61] == '5'].count(axis = 1)\npivot_tb['no_loan'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'X'].count(axis = 1)\n#Setting Id column to merge with app data.\npivot_tb['ID'] = pivot_tb.index\n\n","5203155d":"pivot_tb.head(10)","29e6240f":"def data_cleansing(data):\n    # Adding number of family members with number of children to get overall family members.\n    data['CNT_FAM_MEMBERS'] = data['CNT_FAM_MEMBERS'] + data['CNT_CHILDREN']\n    dropped_cols = ['FLAG_MOBIL', 'FLAG_WORK_PHONE', 'FLAG_PHONE',\n       'FLAG_EMAIL','OCCUPATION_TYPE','CNT_CHILDREN']\n    data = data.drop(dropped_cols, axis = 1)\n    \n    #converting birth years and days employed to years.\n    data['DAYS_BIRTH'] = np.abs(data['DAYS_BIRTH']\/365)\n    data['DAYS_EMPLOYED'] = data['DAYS_EMPLOYED']\/365 \n    \n    #Cleaning up categorical values to lower the count of dummy variables.\n    housing_type = {'House \/ apartment' : 'House \/ apartment',\n                   'With parents': 'With parents',\n                    'Municipal apartment' : 'House \/ apartment',\n                    'Rented apartment': 'House \/ apartment',\n                    'Office apartment': 'House \/ apartment',\n                    'Co-op apartment': 'House \/ apartment'}\n              \n    income_type = {'Commercial associate':'Working',\n                  'State servant':'Working',\n                  'Working':'Working',\n                  'Pensioner':'Pensioner',\n                  'Student':'Student'}\n    education_type = {'Secondary \/ secondary special':'secondary',\n                     'Lower secondary':'secondary',\n                     'Higher education':'Higher education',\n                     'Incomplete higher':'Higher education',\n                     'Academic degree':'Academic degree'}\n    family_status = {'Single \/ not married':'Single',\n                     'Separated':'Single',\n                     'Widow':'Single',\n                     'Civil marriage':'Married',\n                    'Married':'Married'}\n    data['NAME_HOUSING_TYPE'] = data['NAME_HOUSING_TYPE'].map(housing_type)\n    data['NAME_INCOME_TYPE'] = data['NAME_INCOME_TYPE'].map(income_type)\n    data['NAME_EDUCATION_TYPE']=data['NAME_EDUCATION_TYPE'].map(education_type)\n    data['NAME_FAMILY_STATUS']=data['NAME_FAMILY_STATUS'].map(family_status)\n    return data","cacdd298":"cleansed_app = data_cleansing(app)","65b2a3c8":"def feature_engineering_target(data):\n    good_or_bad = []\n    for index, row in data.iterrows():\n        paid_off = row['paid_off']\n        over_1 = row['pastdue_1-29']\n        over_30 = row['pastdue_30-59']\n        over_60 = row['pastdue_60-89'] \n        over_90 = row['pastdue_90-119']\n        over_120 = row['pastdue_120-149'] + row['pastdue_over_150']\n        no_loan = row['no_loan']\n            \n        overall_pastdues = over_1+over_30+over_60+over_90+over_120    \n            \n        if overall_pastdues == 0:\n            if paid_off >= no_loan or paid_off <= no_loan:\n                good_or_bad.append(1)\n            elif paid_off == 0 and no_loan == 1:\n                good_or_bad.append(1)\n        \n        elif overall_pastdues != 0:\n            if paid_off > overall_pastdues:\n                good_or_bad.append(1)\n            elif paid_off <= overall_pastdues:\n                good_or_bad.append(0)\n        \n        elif paid_off == 0 and no_loan != 0:\n            if overall_pastdues <= no_loan or overall_pastdues >= no_loan:\n                good_or_bad.append(0)\n\n        else:\n            good_or_bad.append(1)\n                \n        \n    return good_or_bad","658235b4":"target = pd.DataFrame()\ntarget['ID'] = pivot_tb.index\ntarget['paid_off'] = pivot_tb['paid_off'].values\ntarget['#_of_pastdues'] = pivot_tb['pastdue_1-29'].values+ pivot_tb['pastdue_30-59'].values + pivot_tb['pastdue_60-89'].values +pivot_tb['pastdue_90-119'].values+pivot_tb['pastdue_120-149'].values +pivot_tb['pastdue_over_150'].values\ntarget['no_loan'] = pivot_tb['no_loan'].values\ntarget['target'] = feature_engineering_target(pivot_tb)\ncredit_app = cleansed_app.merge(target, how = 'inner', on = 'ID')\ncredit_app.drop('ID', axis = 1, inplace = True)","6956b007":"x = credit_app[credit_app.drop('target', axis = 1).columns]\ny = credit_app['target']\nxtrain, xtest, ytrain, ytest = train_test_split(x,y, train_size = 0.8, random_state = 0)","f14f5e2c":"def feature_engineering_dummies(train, test):\n    ohe = OneHotEncoder(drop = 'if_binary')\n    xtrain_cat = train.select_dtypes(['object'])\n    xtest_cat = test.select_dtypes(['object'])\n    \n    train_dummies = pd.DataFrame(ohe.fit_transform(xtrain_cat).todense(), \n                              columns = ohe.get_feature_names(xtrain_cat.columns))\n    test_dummies = pd.DataFrame(ohe.transform(xtest_cat).todense(), \n                                columns = ohe.get_feature_names(xtest_cat.columns))\n    train_dummies.index = train.index\n    test_dummies.index = test.index\n    \n    train_with_dummies = pd.concat([train,train_dummies], axis = 1)\n    train_with_dummies.drop(xtrain_cat.columns, axis = 1, inplace = True)\n    \n    test_with_dummies = pd.concat([test,test_dummies], axis = 1)\n    test_with_dummies.drop(xtest_cat.columns, axis = 1, inplace = True)\n    \n    return train_with_dummies, test_with_dummies","1773b811":"xtrain_with_dummies, xtest_with_dummies = feature_engineering_dummies(xtrain,xtest)","f155b8b0":"print(ytrain.value_counts())\nprint(ytest.value_counts())","9b4f21b5":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 2)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\ngrid_rf = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf = RandomForestClassifier()\ngrid = RandomizedSearchCV(rf, grid_rf, cv = 5,verbose = True, n_jobs = -1)\ngrid.fit(xtrain_with_dummies,ytrain)\nparams = grid.best_params_\nprint(params)","d591407a":"rf_grid = RandomForestClassifier(n_estimators = 200, min_samples_split = 5, min_samples_leaf = 2, max_features = 'sqrt', max_depth = 80, bootstrap = True, random_state = 0) \nrf.fit(xtrain_with_dummies, ytrain) \npredictions_test = rf.predict(xtest_with_dummies)\nroc_auc_test = roc_auc_score(ytest,predictions_test) \naccuracy_test = accuracy_score(ytest,predictions_test)\nf1_test = f1_score(ytest, predictions_test)\n\nprint('roc_auc test: ', roc_auc_test) \nprint('accuracy test: ', accuracy_test) \nprint('f1 test: ', f1_test)\n\n","2cddeadf":"## Feature Engineering","038da4a2":"# Predicting Good or Bad From Credit Card Approvals","5ba2dfdc":"## Brief Exploration of Applications \/ Credit","ac854b61":"## Machine Learning with Random Forest","95c60552":"## Data Cleansing","ec8fef0c":"A ratio based method was used to create the target variable. For example, given a client with a time period of 60 months, if the client had paid off loan 40 times and was late 20 times, this would be considered a fairly good client given that there were more loans that were paid off on time compared to late payments. If a client had no loans throughout the initial approval of the credit card account, by default, this would be considered a good client as well. To identify a bad client, the number of past dues would exceed the number of loans paid off  or if the client only has past dues. It may be better to incorporate a set difference between number of paid off loans and number of past dues. Meaning, there needs to be a significant gap between paid off loans and past dues. If a person has 50 past dues and 51 paid off loans, based on the ratio method, this would be considered good. However the difference is only 1 and this may not be a good sign of a good client. For simplicity sake, I will not adjust the algorithm further and keep it at ratio decisioning. Code is also not optimal, adjustment may be needed for the code to compute faster.","c1127e57":"Application data contains information from clients when applying for credit card. The credit card data contains information on the length of time the credit card account was opened since the initial approval as well as the status of the loan during each month. The purpose of this machine learning task is to predict whether or not a client is good or bad based on the application\/ credit card data. The target variable is not given so we will need to find a method to determine good or bad.","aff898a4":"There is data on clients in the credit data that intersect with the application data. the following is a merge between the two data frames given on the data of clients that exist in both data sets. Featured engineered additional columns from the credit data.","bddca5e8":"rf_grid = RandomForestClassifier(n_estimators = 200, min_samples_split = 5, min_samples_leaf = 2, max_features = 'sqrt',\n                                 max_depth = 80, bootstrap = True, random_state = 0)\nrf.fit(xtrain_with_dummies, ytrain)\npredictions_test = rf.predict(xtest_with_dummies)\nroc_auc_test = roc_auc_score(ytest,predictions_test)\naccuracy_test = accuracy_score(ytest,predictions_test)\nf1_test = f1_score(ytest, predictions_test)\n\nprint('roc_auc test: ', roc_auc_test)\nprint('accuracy test: ', accuracy_test)\nprint('f1 test: ', f1_test)","c6e57f87":"Flag_own_realty will be drop from dataset in order to consider credit applicants asking for a mortagage loan, but using credit card risk criteria, i,e without collateral","2f0b368a":"Imported SMOTE in case of high imbalance of target variable. However, it does not seem to be highly imbalanced so it is fine not to use an imbalanced technique. "}}