{"cell_type":{"4d8a890b":"code","40ce7ed7":"code","b0212fd7":"code","0ed12c4f":"code","550e103d":"code","d455d470":"code","26d55eea":"code","4749ff70":"code","eacfebe2":"markdown","71de063d":"markdown","fe6801a8":"markdown","46d6641f":"markdown","e055d67c":"markdown","f8dca63d":"markdown"},"source":{"4d8a890b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","40ce7ed7":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.utils import to_categorical \nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Dropout, Dense, Flatten, Activation, concatenate\nfrom keras.initializers import glorot_uniform, he_uniform, Zeros\nimport matplotlib.pyplot as plt","b0212fd7":"train_X = np.load(\"\/kaggle\/input\/train-data\/train_X.npy\")\n\ntrain_y = np.asarray(pd.read_csv(\"\/kaggle\/input\/train-data\/train.csv\").iloc[:,1])\n\ntrain_y = to_categorical(train_y)","0ed12c4f":"kernel_init = glorot_uniform()\nbias_init = Zeros()\ndef inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5, filters_pool_proj, name=None):\n    conv_1x1 = Conv2D(filters_1x1, kernel_size=(1,1), padding='same', activation='relu', kernel_initializer=kernel_init, bias_initializer = bias_init)(x)\n    \n    conv_3x3 = Conv2D(filters_3x3_reduce, kernel_size=(1,1), padding='same', activation ='relu', kernel_initializer=kernel_init, bias_initializer = bias_init)(x)\n    conv_3x3 = Conv2D(filters_3x3, kernel_size=(3,3), padding='same', activation ='relu', kernel_initializer=kernel_init, bias_initializer = bias_init)(x)\n    \n    conv_5x5 = Conv2D(filters_5x5_reduce, kernel_size=(1,1), padding='same', activation ='relu', kernel_initializer=kernel_init, bias_initializer = bias_init)(x)\n    conv_5x5 = Conv2D(filters_5x5, kernel_size=(3,3), padding='same', activation ='relu', kernel_initializer=kernel_init, bias_initializer = bias_init)(x)\n    \n    pool_proj = MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)\n    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', kernel_initializer=kernel_init)(pool_proj)\n    \n    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n    \n    return output\n    ","550e103d":"input_layer = Input(shape=(100, 100, 3))\n\nx = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7\/2', kernel_initializer=kernel_init)(input_layer)\nx = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_1_3x3\/2')(x)\nx = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv_2a_3x3\/1')(x)\nx = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2b_3x3\/1')(x)\nx = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_2_3x3\/2')(x)\n\n\nx = inception_module(x, filters_1x1=64, filters_3x3_reduce=96, filters_3x3=128, filters_5x5_reduce=16, filters_5x5=32, filters_pool_proj=32, name='inception_3a')\n\nx = inception_module(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=192, filters_5x5_reduce=32, filters_5x5=96, filters_pool_proj=64, name='inception_3b')\n\nx = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_3_3x3\/2')(x)\n\nx = inception_module(x, filters_1x1=192, filters_3x3_reduce=96,filters_3x3=208, filters_5x5_reduce=16, filters_5x5=48, filters_pool_proj=64, name='inception_4a')\n\n\nx1 = AveragePooling2D((5, 5), strides=3)(x)\nx1 = Conv2D(128, (1, 1), padding='same', activation='relu')(x1)\nx1 = Flatten()(x1)\nx1 = Dense(1024, activation='relu')(x1)\nx1 = Dropout(0.7)(x1)\nx1 = Dense(103, activation='softmax', name='auxilliary_output_1')(x1)\n\nx = inception_module(x, filters_1x1=160, filters_3x3_reduce=112, filters_3x3=224, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64, name='inception_4b')\n\nx = inception_module(x, filters_1x1=128, filters_3x3_reduce=128, filters_3x3=256, filters_5x5_reduce=24, filters_5x5=64, filters_pool_proj=64, name='inception_4c')\n\nx = inception_module(x, filters_1x1=112, filters_3x3_reduce=144, filters_3x3=288, filters_5x5_reduce=32,filters_5x5=64, filters_pool_proj=64, name='inception_4d')\n\n\nx2 = AveragePooling2D((5, 5), strides=3)(x)\nx2 = Conv2D(128, (1, 1), padding='same', activation='relu')(x2)\nx2 = Flatten()(x2)\nx2 = Dense(1024, activation='relu')(x2)\nx2 = Dropout(0.7)(x2)\nx2 = Dense(103, activation='softmax', name='auxilliary_output_2')(x2)\n\nx = inception_module(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128, name='inception_4e')\n\nx = MaxPooling2D((3, 3), padding='same', strides=(2, 2), name='max_pool_4_3x3\/2')(x)\n\nx = inception_module(x, filters_1x1=256, filters_3x3_reduce=160, filters_3x3=320, filters_5x5_reduce=32, filters_5x5=128, filters_pool_proj=128, name='inception_5a')\n\nx = inception_module(x, filters_1x1=384, filters_3x3_reduce=192, filters_3x3=384, filters_5x5_reduce=48, filters_5x5=128, filters_pool_proj=128, name='inception_5b')\n\nx = GlobalAveragePooling2D(name='avg_pool_5_3x3\/1')(x)\nx = Dropout(0.4)(x)\nx = Dense(103, activation='softmax', name='output')(x)\n","d455d470":"model = Model(input_layer, [x1, x2, x], name='inception_v1')\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","26d55eea":"model.fit(train_X, [train_y, train_y, train_y], validation_split=0.2, epochs = 100, batch_size=64)","4749ff70":"plt.plot(model.history.history['val_auxilliary_output_1_accuracy'])\nplt.plot(model.history.history['val_auxilliary_output_2_accuracy'])\nplt.plot(model.history.history['val_output_accuracy'])\nplt.plot(model.history.history['auxilliary_output_1_accuracy'])\nplt.plot(model.history.history['auxilliary_output_2_accuracy'])\nplt.plot(model.history.history['output_accuracy'])\nplt.legend(['validation accuracy 1','validation accuracy 2', 'validation accuracy 3', 'training accuracy 1','training accuracy 2', 'training accuracy 3'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')","eacfebe2":"There are 100+ types of flowers which are to be classified. A labeled set is given for training. In this kernel I will make a GoogLeNet for the problem statement. In a previous kernel I used VGG16 network to train on the same dataset. ","71de063d":"![image.png](attachment:image.png)","fe6801a8":"Let's import packages","46d6641f":"Inception Block","e055d67c":"To save some time and more importantly space. I already saved the images as numpy arrays of size 100 * 100. Let's get the data.","f8dca63d":"GoogLeNet \/ Inception V1 Network"}}