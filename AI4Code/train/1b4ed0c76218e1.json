{"cell_type":{"69afb812":"code","9beca102":"code","e433def2":"code","fc478d8f":"code","2f3110ee":"code","83629482":"code","d8f175c6":"code","430c0b02":"code","c92bac04":"code","d1932cb8":"code","a89fdfa7":"code","6e234fa1":"code","428d4114":"code","acac0c66":"code","9f04f22d":"code","b82a19d7":"code","8dd1ad3e":"code","432b5f95":"code","e87b492c":"code","5e39112f":"code","bffd3bbe":"code","2bd9947d":"code","bfc95188":"code","8614d186":"code","28b9ffb8":"code","beabb460":"markdown","d82afc8c":"markdown","fafd779e":"markdown","7b8b5f8b":"markdown","977b5043":"markdown","8c207d42":"markdown","adb6ccf2":"markdown","06740cf6":"markdown","765eeecd":"markdown","ccd45b61":"markdown","2a8ee965":"markdown","1103f9e8":"markdown"},"source":{"69afb812":"\nimport pandas as pd\nimport numpy as np\nimport time\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nsns.set(style=\"darkgrid\")\nplt.style.use(\"fivethirtyeight\")\n\n\n","9beca102":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n\nprint(os.listdir(\"..\/input\"))","e433def2":"%%time\ntrain_df = load_df(nrows=600000)\ntrain_corr = train_df.copy()\ntest_df = load_df(\"..\/input\/test.csv\", nrows=1000000)\n# train_df.to_csv(\"500000.csv\", index=False)","fc478d8f":"#","2f3110ee":"train_df.isnull().sum().plot(kind=\"bar\", figsize = (20,8))\nplt.xlabel(\"Count\")\nplt.ylabel(\"Column Name\")\nplt.title(\"Missing Value Count By Column for 500000 Rows\")","83629482":"plt.figure(figsize=(20,8))\nax = sns.countplot(x=train_df.dtypes, data=train_df)\nplt.xlabel(\"Data Types\")\nplt.ylabel(\"Counts\")\nplt.title(\"Column Count By Datatypes\")","d8f175c6":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\n\ntarget = target.apply(lambda x: np.log(x) if x > 0 else x)\n\n\ndel train_df['totals.transactionRevenue']\n# plt.figure(figsize = (15,10))\nsns.jointplot(target.values,target.index,kind=\"regg\")\nplt.show()","430c0b02":"train_df.select_dtypes('object').describe()","c92bac04":"train_df.select_dtypes(exclude=('object')).describe().boxplot(figsize=(20,8))\ntrain_df.select_dtypes(exclude=('object')).describe()","d1932cb8":"plt.figure(figsize=(15,15))\nsns.heatmap(train_corr.corr(method=\"kendall\"), annot=True)\ntrain_corr.corr(method='kendall').style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","a89fdfa7":"columns = [col for col in train_df.columns if train_df[col].nunique() > 1]\ntrain_df = train_df[columns]\ntest_df = test_df[columns]","6e234fa1":"trn_len = train_df.shape[0]\nmerged_df = pd.concat([train_df, test_df])\n\nfor col in merged_df.columns:\n    if col in ['fullVisitorId']: continue\n    if merged_df[col].dtypes == object or merged_df[col].dtypes == bool:\n        merged_df[col], indexer = pd.factorize(merged_df[col])\n\ntrain_df = merged_df[:trn_len]\ntest_df = merged_df[trn_len:]","428d4114":"train_df['totals.transactionRevenue']=target\ntrain_df.head(5)","acac0c66":"test_df.head()","9f04f22d":"#train_df[\"fullVisitorId\"] = train_df.fullVisitorId.astype(float)\n#test_df[\"fullVisitorId\"] = test_df[\"fullVisitorId\"].astype(float)","b82a19d7":"import xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error\n","8dd1ad3e":"train_df.dtypes","432b5f95":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_df.drop(['totals.transactionRevenue','fullVisitorId'], axis=1),\n                                                    train_df['totals.transactionRevenue'], test_size=0.25)\n#del(df)\ndtrain = xgb.DMatrix(X_train, label=y_train)\n#del(X_train)\ndtest = xgb.DMatrix(X_test)\n#del(X_test)","e87b492c":"# def xgb_evaluate(max_depth, gamma,min_child_weight,max_delta_step,subsample,colsample_bytree):\n#     params = {'eval_metric': 'rmse',\n#               'max_depth': int(max_depth),\n#               'subsample': subsample,\n#               'eta': 0.1,\n#               'gamma': gamma,\n#               'colsample_bytree': colsample_bytree,   \n#               'min_child_weight': min_child_weight ,\n#               'max_delta_step':max_delta_step\n#              }\n#     # Used around 1000 boosting rounds in the full model\n#     cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n    \n#     # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n#     return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","5e39112f":"# xgb_bo = BayesianOptimization(xgb_evaluate, {\n#                                     'max_depth': (2, 12),\n#                                      'gamma': (0.001, 10.0),\n#                                      'min_child_weight': (0, 20),\n#                                      'max_delta_step': (0, 10),\n#                                      'subsample': (0.4, 1.0),\n#                                      'colsample_bytree' :(0.4, 1.0)})\n# # Use the expected improvement acquisition function to handle negative numbers\n# # Optimally needs quite a few more initiation points and number of iterations\n# xgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","bffd3bbe":"#params = xgb_bo.res['max']['max_params']\n#print(params)\nparams = {'max_depth': 6.714941854933043, 'gamma': 1.3250360141843498, 'min_child_weight': 13.0958516960316, 'max_delta_step': 8.88492863796954, 'subsample': 0.9864199446951019, 'colsample_bytree': 0.8376539278239742}\n#params = {'max_depth': 12.0, 'gamma': 0.001, 'min_child_weight': 8.740952582296343, 'max_delta_step': 10.0, 'subsample': 0.4, 'colsample_bytree': 1.0}\nparams['max_depth'] = int(params['max_depth'])","2bd9947d":"# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)\n\n# Report testing and training RMSE\nprint(np.sqrt(mean_squared_error(y_test, y_pred)))\nprint(np.sqrt(mean_squared_error(y_train, y_train_pred)))","bfc95188":"submission = test_df[['fullVisitorId']].copy()\n#test = transform((test_df.drop('fullVisitorId', axis=1)))\ndtest = xgb.DMatrix(test_df.drop('fullVisitorId', axis=1))\npredictions = model2.predict(dtest)","8614d186":"\nsubmission.loc[:, 'PredictedLogRevenue'] = predictions\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsubmission[\"PredictedLogRevenue\"] = submission[\"PredictedLogRevenue\"].fillna(0.0)\ngrouped_test = submission[['fullVisitorId', 'PredictedLogRevenue']].groupby('fullVisitorId').sum().reset_index()\ngrouped_test.to_csv('submit.csv',index=False)","28b9ffb8":"grouped_test.head(5)","beabb460":"### Categorical Variable Columns Statistics","d82afc8c":"## 1.3 Investigate the Target Columns","fafd779e":"## 1.4 Investigate the Statistics of Data\n\nDescriptive statistics can give you great insight into the shape of each attribute. Often you can create more summaries than you have time to review. The describe() function on the Pandas DataFrame lists 8 statistical properties of each attribute. They are:\n* Count.\n* Mean.\n* Standard Deviation.\n*  Minimum Value.\n* 25th Percentile.\n* 50th Percentile (Median).\n* 75th Percentile.\n* Maximum Value.\n","7b8b5f8b":"# 1. Data Investigation","977b5043":"# 9.Final Submission","8c207d42":"# 3. Categorical to Numeric Variable Conversion for Model Training(Label Encoding)","adb6ccf2":"# Outline\n1. [**Data Investigation**](#Data-Investigation**)\n    1. [Count the Missing Values](#Count-the-Missing-Values)\n    1. [Datatypes wise Columns Counts](#Datatypes-wise-Columns-Counts)\n    1. [Investigate the target variable distribution](#Investigate-the-target-variable-distribution)\n    1. [Investigate Statistics of Data](Investigate-Statistics-of-Data)\n    1. [Correlation of dataset](#Correlation-of-dataset)\n2. [**Remove Varible That Contrain Same Class**](#Remove-Varible-That-Contrain-Same-Class)\n3. [**Categorical to Numeric Variable Conversion for Model Training(Label Encoding)**](#Categorical-to-Numeric-Variable-Conversion-for-Model-Training(Label-Encoding)\n4. [**Bayesian Optimization for Best Parameter Tuning**](#Bayesian-Optimization-for-Best-Parameter-Tuning)\n5. [**Parameter as per Understanding of Model**](#Parameter-as-per-Understanding-of-Model)\n6. [**Model Training With KFold Cross Validation**](#Model-Training-With-KFold-Cross-Validation)\n7. [**Best CV Score Return By Model**](#Best-CV-Score-Return-By-Model)\n8. [**Features Importance**](#Features-Importance)\n9. [**Final Submission**](#Final-Submission)","06740cf6":"## 1.2 Count the Datatypes Columnwise\n\n* We can see that most of columns datatype is **object**.\n* The type of each attribute is important. Strings may need to be converted to \ufb02oating point values or integers to represent categorical or ordinal values. \n* we can get an idea of the types of attributes by peeking at the raw data. You can also list the data types used by the DataFrame to characterize each attribute using the dtypes property.\n","765eeecd":"# 1.5 Correlation of dataset\n\n* only for numeric variable","ccd45b61":"### Numeric Statistics","2a8ee965":"# 2. Remove Variable That Contain Same Class","1103f9e8":"## 1.1 Count Missing Values\n* We can see that so missing Values are to high.\n* Missing data are a **common occurrence and can have a significant effect** on the conclusions that can be drawn from the** data.**"}}