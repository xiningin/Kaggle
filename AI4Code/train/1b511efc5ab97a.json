{"cell_type":{"d67ab0ac":"code","bc7b00a3":"code","f097ab6a":"code","a87f4d93":"code","0af25798":"code","87d6dfc7":"code","335f3226":"code","3ada8f96":"code","365dfd64":"code","a70ccb0d":"code","b3f339ea":"code","19348313":"code","da83eb85":"code","99b3266b":"code","62e562df":"code","8abf2e76":"code","75985882":"code","eb89a616":"code","f77ce86a":"code","081028d0":"code","66986b63":"code","4135e31f":"code","5bf5b5b3":"code","229791b8":"code","0b8f6454":"code","e67528b9":"code","36aa4873":"markdown","e5a12a16":"markdown","cb73893a":"markdown","93af41d2":"markdown","3774796f":"markdown","92464d5f":"markdown","cc76c989":"markdown","25eebb67":"markdown","345e2c3c":"markdown","0ca3c9a8":"markdown","88e27daf":"markdown","eef11185":"markdown","064e8df3":"markdown","fbd84d13":"markdown","4bfd0e43":"markdown","03acbe67":"markdown","87b50f40":"markdown","719e40f3":"markdown","0314d434":"markdown","819791ad":"markdown","b42aa8dd":"markdown","733d5686":"markdown"},"source":{"d67ab0ac":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","bc7b00a3":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2 as cv\nfrom numpy.random import seed\nseed(45)\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n%matplotlib inline","f097ab6a":"train_dirname = '\/kaggle\/input\/histopathologic-cancer-detection\/train'","a87f4d93":"train_labels = pd.read_csv('\/kaggle\/input\/histopathologic-cancer-detection\/train_labels.csv')\ntrain_labels.head()","0af25798":"train_labels['label'].value_counts()","87d6dfc7":"positive_samples = train_labels.loc[train_labels['label'] == 1].sample(4)\nnegative_samples = train_labels.loc[train_labels['label'] == 0].sample(4)\npositive_images = []\nnegative_images = []\nfor sample in positive_samples['id']:\n    path = os.path.join(train_dirname, sample+'.tif')\n    img = cv.imread(path)\n    positive_images.append(img)\n        \nfor sample in negative_samples['id']:\n    path = os.path.join(train_dirname, sample+'.tif')\n    img = cv.imread(path)\n    negative_images.append(img)\n\nfig,axis = plt.subplots(2,4,figsize=(20,8))\nfig.suptitle('Dataset samples presentation plot',fontsize=20)\nfor i,img in enumerate(positive_images):\n    axis[0,i].imshow(img)\n    rect = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='g',facecolor='none', linestyle=':', capstyle='round')\n    axis[0,i].add_patch(rect)\naxis[0,0].set_ylabel('Positive samples', size='large')\nfor i,img in enumerate(negative_images):\n    axis[1,i].imshow(img)\n    rect = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='r',facecolor='none', linestyle=':', capstyle='round')\n    axis[1,i].add_patch(rect)\naxis[1,0].set_ylabel('Negative samples', size='large')\n    ","335f3226":"preprocessed_dir = 'train_for_tf'\nos.mkdir(preprocessed_dir)\n\nvalidation_dir = 'val_dir'\ntraining_dir = 'train_dir'\ntest_dir = 'test_dir'\npositive_label_dir = 'positive'\nnegative_label_dir = 'negative'\nos.mkdir(os.path.join(preprocessed_dir,validation_dir))\nos.mkdir(os.path.join(preprocessed_dir,training_dir))\nos.mkdir(os.path.join(preprocessed_dir,test_dir))\nos.mkdir(os.path.join(preprocessed_dir,validation_dir,positive_label_dir))\nos.mkdir(os.path.join(preprocessed_dir,validation_dir,negative_label_dir))\nos.mkdir(os.path.join(preprocessed_dir,training_dir,positive_label_dir))\nos.mkdir(os.path.join(preprocessed_dir,training_dir,negative_label_dir))\nos.mkdir(os.path.join(preprocessed_dir,test_dir,positive_label_dir))\nos.mkdir(os.path.join(preprocessed_dir,test_dir,negative_label_dir))","3ada8f96":"IMG_SIZE = 96\nIMG_CHANNELS = 3\nTRAIN_SIZE=80000\n# TRAIN_SIZE=89000\nBATCH_SIZE = 16\nEPOCHS = 30","365dfd64":"train_neg = train_labels[train_labels['label']==0].sample(TRAIN_SIZE,random_state=45)\ntrain_pos = train_labels[train_labels['label']==1].sample(TRAIN_SIZE,random_state=45)\n\ntrain_data = pd.concat([train_neg, train_pos], axis=0).reset_index(drop=True)\n\ntrain_data = shuffle(train_data)","a70ccb0d":"train_data['label'].value_counts()\ntrain_data.head()","b3f339ea":"y = train_data['label']\ntrain_df, val_df = train_test_split(train_data, test_size=0.3, random_state=45, stratify=y)\ny = val_df['label']\nval_df, test_df = train_test_split(val_df, test_size=0.5, random_state=45, stratify=y)\nprint(train_df.shape)\nprint(val_df.shape)\nprint(test_df.shape)","19348313":"for sample in train_df.iterrows():\n    source = os.path.join(train_dirname, sample[1]['id']+'.tif')\n    if sample[1]['label'] == 0:\n        label = 'positive'\n    else:\n        label = 'negative'\n    target = os.path.join(preprocessed_dir,training_dir,label,sample[1]['id']+'.tif')\n    shutil.copyfile(source, target)\n    \nfor sample in val_df.iterrows():\n    source = os.path.join(train_dirname, sample[1]['id']+'.tif')\n    if sample[1]['label'] == 0:\n        label = 'positive'\n    else:\n        label = 'negative'\n    target = os.path.join(preprocessed_dir,validation_dir,label,sample[1]['id']+'.tif')\n    shutil.copyfile(source, target)\n\nfor sample in test_df.iterrows():\n    source = os.path.join(train_dirname, sample[1]['id']+'.tif')\n    if sample[1]['label'] == 0:\n        label = 'positive'\n    else:\n        label = 'negative'\n    target = os.path.join(preprocessed_dir,test_dir,label,sample[1]['id']+'.tif')\n    shutil.copyfile(source, target)","da83eb85":"print(len(os.listdir('train_for_tf\/train_dir\/positive')))\nprint(len(os.listdir('train_for_tf\/train_dir\/negative')))\nprint(len(os.listdir('train_for_tf\/val_dir\/positive')))\nprint(len(os.listdir('train_for_tf\/val_dir\/negative')))\nprint(len(os.listdir('train_for_tf\/test_dir\/positive')))\nprint(len(os.listdir('train_for_tf\/test_dir\/negative')))","99b3266b":"TRAIN_PATH = 'train_for_tf\/train_dir'\nVAL_PATH = 'train_for_tf\/val_dir'\nTEST_PATH = 'train_for_tf\/test_dir'\ntotal_train = len(os.listdir('train_for_tf\/train_dir\/positive')) + len(os.listdir('train_for_tf\/train_dir\/negative'))\ntotal_val = len(os.listdir('train_for_tf\/val_dir\/positive')) + len(os.listdir('train_for_tf\/val_dir\/negative'))\ntotal_test = len(os.listdir('train_for_tf\/test_dir\/positive')) + len(os.listdir('train_for_tf\/test_dir\/negative'))","62e562df":"train_image_generator = ImageDataGenerator(rescale=1.\/255)\ntrain_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                           directory=TRAIN_PATH,\n                                                           shuffle=True,\n                                                           target_size=(IMG_SIZE, IMG_SIZE),\n                                                           class_mode='binary')\n\nvalidation_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our validation data\nval_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                              directory=VAL_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                              class_mode='binary')\n\ntest_image_generator = ImageDataGenerator(rescale=1.\/255) # Generator for our validation data\ntest_data_gen = test_image_generator.flow_from_directory(batch_size=1,\n                                                              directory=TEST_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                              class_mode='binary',shuffle=False)","8abf2e76":"simple_model = Sequential([\n    Conv2D(16, 3, padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE ,3)),\n    MaxPooling2D(),\n    Dropout(0.3),\n    Conv2D(32, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.3),\n    Conv2D(64, 3, padding='same', activation='relu'),\n    MaxPooling2D(),\n    Dropout(0.3),\n    Flatten(),\n    Dense(512, activation='relu'),\n    Dense(1)\n])\n\nsimple_model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nsimple_model.summary()","75985882":"EPOCHS = 25\ncheckpoint_filepath = 'checkpoint_simple_model.hdf5'\ncheckpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \nearly_stop = EarlyStopping(\n    monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=True\n)                             \ncallbacks_list = [checkpoint, reduce_lr, early_stop]\n\nsimple_history = simple_model.fit_generator(train_data_gen, steps_per_epoch=total_train\/\/BATCH_SIZE, \n                    validation_data=val_data_gen,\n                    validation_steps=total_val\/\/BATCH_SIZE,\n                    epochs=EPOCHS, verbose=1,\n                   callbacks=callbacks_list)","eb89a616":"resnet50_train_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\nresnet50_train_data_gen = resnet50_train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                           directory=TRAIN_PATH,\n                                                           shuffle=True,\n                                                           target_size=(IMG_SIZE, IMG_SIZE),\n                                                           class_mode='binary')\nresnet50_validation_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\nresnet50_val_data_gen = resnet50_validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                              directory=VAL_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                              class_mode='binary')\nresnet50_test_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.resnet50.preprocess_input)\nresnet50_test_data_gen = resnet50_test_image_generator.flow_from_directory(batch_size=1,\n                                                              directory=TEST_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                            class_mode='binary',shuffle=False)","f77ce86a":"from tensorflow.keras.applications.resnet50 import ResNet50\n\ndropout_fc = 0.3\n\nresnet50_base_model = ResNet50(weights = 'imagenet', include_top = False,pooling = max, input_shape = (IMG_SIZE,IMG_SIZE,3))\nresnet50_model = Sequential([\n    resnet50_base_model,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(dropout_fc),\n    Dense(1,activation=\"sigmoid\")\n])\nresnet50_model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nresnet50_model.summary()","081028d0":"EPOCHS = 60\n# needs generator that zero centers the data, without rescaling\nresnet50_filepath = \"checkpoint_resnet50_model.h5\"\ncheckpoint = ModelCheckpoint(resnet50_filepath, monitor='val_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \nearly_stop = EarlyStopping(\n    monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=True\n)                             \ncallbacks_list = [checkpoint, reduce_lr, early_stop]\n\nresnet50_history = resnet50_model.fit_generator(resnet50_train_data_gen, steps_per_epoch=total_train\/\/BATCH_SIZE, \n                    validation_data=resnet50_val_data_gen,\n                    validation_steps=total_val\/\/BATCH_SIZE,\n                    epochs=EPOCHS, verbose=1,\n                   callbacks=callbacks_list)","66986b63":"mobilenetv2_train_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input)\nmobilenetv2_train_data_gen = mobilenetv2_train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                           directory=TRAIN_PATH,\n                                                           shuffle=True,\n                                                           target_size=(IMG_SIZE, IMG_SIZE),\n                                                           class_mode='binary')\nmobilenetv2_validation_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) # Generator for our validation data\nmobilenetv2_val_data_gen = mobilenetv2_validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n                                                              directory=VAL_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                              class_mode='binary')\nmobilenetv2_test_image_generator = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input) # Generator for our validation data\nmobilenetv2_test_data_gen = mobilenetv2_test_image_generator.flow_from_directory(batch_size=1,\n                                                              directory=TEST_PATH,\n                                                              target_size=(IMG_SIZE,IMG_SIZE),\n                                                              class_mode='binary',shuffle=False)","4135e31f":"from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n\ndropout_fc = 0.3\nmobilenetv2_base_model = MobileNetV2(weights = 'imagenet', include_top = False, pooling = max, input_shape = (IMG_SIZE,IMG_SIZE,3))\nmobilenetv2_model = Sequential([\n    mobilenetv2_base_model,\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(dropout_fc),\n    Dense(1,activation=\"sigmoid\")\n])\nmobilenetv2_model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nmobilenetv2_model.summary()","5bf5b5b3":"EPOCHS = 50\nmobilenetv2_filepath = \"mobilenetv2_model.h5\"\ncheckpoint = ModelCheckpoint(mobilenetv2_filepath, monitor='val_accuracy', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \nearly_stop = EarlyStopping(\n    monitor='val_accuracy', min_delta=0, patience=5, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=True\n)                             \ncallbacks_list = [checkpoint, reduce_lr, early_stop]\n\nmobilenetv2_history = mobilenetv2_model.fit_generator(mobilenetv2_train_data_gen, steps_per_epoch=total_train\/\/BATCH_SIZE, \n                    validation_data=mobilenetv2_val_data_gen,\n                    validation_steps=total_val\/\/BATCH_SIZE,\n                    epochs=EPOCHS, verbose=1,\n                   callbacks=callbacks_list)","229791b8":"plt.figure(figsize=(20,5))\nplt.subplot(221)\nplt.plot(resnet50_history.history['loss'], color='orange', label=\"training_loss\")\nplt.plot(resnet50_history.history['val_loss'], color='blue', label=\"validation_loss\")\nplt.legend(loc='best')\nplt.title('training plot -  - ResNet50')\nplt.xlabel('epoch')\nplt.savefig(\"training.png\", bbox_inches='tight')\n\nplt.subplot(222)\nplt.plot(resnet50_history.history['accuracy'], color='orange', label=\"training_accuracy\")\nplt.plot(resnet50_history.history['val_accuracy'], color='blue',label=\"validation_accuracy\")\nplt.legend(loc='best')\nplt.title('validation plot - ResNet50')\nplt.xlabel('epoch')\nplt.savefig(\"validation.png\", bbox_inches='tight')\nplt.show()\nplt.figure(figsize=(20,5))\nplt.subplot(223)\nplt.plot(mobilenetv2_history.history['loss'], color='orange', label=\"training_loss\")\nplt.plot(mobilenetv2_history.history['val_loss'], color='blue', label=\"validation_loss\")\nplt.legend(loc='best')\nplt.title('training plot -  - MobileNetV2')\nplt.xlabel('epoch')\nplt.savefig(\"training.png\", bbox_inches='tight')\n\nplt.subplot(224)\nplt.plot(mobilenetv2_history.history['accuracy'], color='orange', label=\"training_accuracy\")\nplt.plot(mobilenetv2_history.history['val_accuracy'], color='blue',label=\"validation_accuracy\")\nplt.legend(loc='best')\nplt.title('validation plot - MobileNetV2')\nplt.xlabel('epoch')\nplt.savefig(\"validation.png\", bbox_inches='tight')\nplt.show()","0b8f6454":"from sklearn.metrics import roc_curve, auc, roc_auc_score\nimport matplotlib.pyplot as plt\n\n# make a prediction resnet50\nresnet50_model_predictions = resnet50_model.predict_generator(resnet50_test_data_gen, steps=total_test, verbose=1)\nfpr_resnet50, tpr_resnet50, thresholds_resnet50 = roc_curve(resnet50_test_data_gen.classes, resnet50_model_predictions)\nresnet50_model_auc = roc_auc_score(resnet50_test_data_gen.classes, resnet50_model_predictions)\nprint(f'ResNet50 AUC = {resnet50_model_auc}')\n\n# make a prediction MobileNetV2\nmobilenetv2_model_predictions = mobilenetv2_model.predict_generator(mobilenetv2_test_data_gen, steps=total_test, verbose=1)\nfpr_mobilenetv2, tpr_mobilenetv2, thresholds_mobilenetv2 = roc_curve(mobilenetv2_test_data_gen.classes, mobilenetv2_model_predictions)\nmobilenetv2_model_auc = roc_auc_score(mobilenetv2_test_data_gen.classes, mobilenetv2_model_predictions)\nprint(f'MobileNetV2 AUC = {mobilenetv2_model_auc}')","e67528b9":"plt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_resnet50, tpr_resnet50, label='area = {:.4f}'.format(resnet50_model_auc))\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.title('ResNet50 ROC curve')\nplt.legend(loc='best')\nplt.show()\nplt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_mobilenetv2, tpr_mobilenetv2, label='area = {:.4f}'.format(mobilenetv2_model_auc))\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.title('MobileNetV2 ROC curve')\nplt.legend(loc='best')\nplt.show()","36aa4873":"# Setting up learning constants","e5a12a16":"## Train ResNet50 model","cb73893a":"## Training process plots","93af41d2":"# Dataset exploration","3774796f":"# Splitting dataset","92464d5f":"Data is not entirely balanced, there is more negative samples than positive, by about 30 percent","cc76c989":"## ","25eebb67":"## Train simple model","345e2c3c":"## Prepare image generators paths","0ca3c9a8":"## Image generators for the simple CNN model","88e27daf":"## Area Under the Receiver Operating Characteristic curve","eef11185":"## Creating a suitable folder structure","064e8df3":"## Create ResNet Model","fbd84d13":"## Check if copied correctly","4bfd0e43":"## Copying splitted data to respective directories","03acbe67":"## Train MobileNetV2 model","87b50f40":"### Create image generators for MobileNetV2","719e40f3":"## Image generators for the ResNet CNN model","0314d434":"## Create MobileNetV2 model","819791ad":"## Splitting the dataset","b42aa8dd":"## Create a simple model","733d5686":"## Balancing the dataset"}}