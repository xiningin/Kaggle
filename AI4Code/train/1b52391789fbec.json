{"cell_type":{"34f7ccdf":"code","b57923bf":"code","5ab70247":"code","7a0ff0a4":"code","d9ed25d2":"code","0a6b710a":"code","e218a171":"code","34eb9ea7":"code","f03cf4c2":"code","17f82300":"code","92771db4":"code","af4a856b":"code","6e1d666c":"code","1cc6a835":"code","2d10b3ef":"code","759ffec9":"code","a22987e7":"code","2a8281b9":"code","e8e4f772":"code","b08671d5":"code","cc0221ba":"code","807a5f91":"code","99314533":"code","05fbe000":"markdown","a2712a48":"markdown","043159c7":"markdown","6b25254c":"markdown","c81569dd":"markdown","463255fb":"markdown","63a1fbab":"markdown","819a1a2c":"markdown","7a7ef822":"markdown","a82208f5":"markdown","68197eb9":"markdown","411c1c42":"markdown","4ccc4c5f":"markdown","2bc0432b":"markdown","2f346496":"markdown","608ae9c1":"markdown","2d28a1ac":"markdown","5f0a4d5b":"markdown","a5f60342":"markdown","9317be02":"markdown"},"source":{"34f7ccdf":"# Basic library\nimport numpy as np \nimport pandas as pd \n\n# Data preprocessing\nimport cv2 # Open cv\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nfrom matplotlib import pyplot as plt\n\n# Machine learning library\nimport keras\nfrom keras.models import Sequential, Model, load_model\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, Activation, Input\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator","b57923bf":"# dataframe data\nsample_submission = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/plant-pathology-2020-fgvc7\/train.csv\")","5ab70247":"# train image data\nsize = 64\ntrain_image_data = []\n\n# loading\nfor _id in train[\"image_id\"]:\n    path = '..\/input\/plant-pathology-2020-fgvc7\/images\/'+_id+'.jpg'\n    img = cv2.imread(path)\n    image = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n    train_image_data.append(image)","7a0ff0a4":"# test image data\nsize = 64\ntest_image_data = []\n\n# loading\nfor _id in test[\"image_id\"]:\n    path = '..\/input\/plant-pathology-2020-fgvc7\/images\/'+_id+'.jpg'\n    img = cv2.imread(path)\n    image = cv2.resize(img, (size, size), interpolation=cv2.INTER_AREA)\n    test_image_data.append(image)","d9ed25d2":"# sample_submission data\nsample_submission.head()","0a6b710a":"# train data\ntrain.head()","e218a171":"# data information\ndef data_info(data):\n    print(\"-\"*20, \"data_info\", \"-\"*20)\n    print(data.info())\n    print(\"-\"*20, \"data_info\", \"-\"*20)\n\ndata_info(train)","34eb9ea7":"# test data\ntest.head()","f03cf4c2":"# train image data size\nlen(train_image_data)","17f82300":"# visualization, train_data\nfig, ax = plt.subplots(1,3,figsize=(10,10))\nfor i in range(3):\n    ax[i].imshow(train_image_data[i])","92771db4":"# visualization, test_data\nfig, ax = plt.subplots(1,3,figsize=(10,10))\nfor i in range(3):\n    ax[i].imshow(test_image_data[i])","af4a856b":"# Data dimension\nX_Train = np.ndarray(shape=(len(train_image_data), size, size, 3),\n                     dtype=np.float32)\n# Change to nu.ndarray\ni=0\nfor image in train_image_data:\n    X_Train[i]=train_image_data[i]\n    i=i+1\n    \n# Scaling\nX_Train = X_Train\/255\n\n# Checking dimension\nprint(\"Train_shape:{}\".format(X_Train.shape))","6e1d666c":"# Data dimension adjust\nX_Test = np.ndarray(shape=(len(test_image_data), size, size, 3),\n                     dtype=np.float32)\n# Change to np.ndarray\ni=0\nfor image in test_image_data:\n    X_Test[i]=test_image_data[i]\n    i=i+1\n    \n# Scaling\nX_Test = X_Test\/255\n\n# Checking dimension\nprint(\"Train_shape:{}\".format(X_Test.shape))","1cc6a835":"y = train.iloc[:,1:]\n\n# change to np.array\ny = np.array(y.values)\nprint(\"y_shape:{}\".format(y.shape))","2d10b3ef":"# data split\nX_train, X_val, y_train, y_val = train_test_split(X_Train,\n                                                  y,\n                                                  test_size=0.2,\n                                                  random_state=10)","759ffec9":"# target data\ny_train1 = [y[0] for y in y_train]\ny_train2 = [y[1] for y in y_train]\ny_train3 = [y[2] for y in y_train]\ny_train4 = [y[3] for y in y_train]\n\n# val data\ny_val1 = [y[0] for y in y_val]\ny_val2 = [y[1] for y in y_val]\ny_val3 = [y[2] for y in y_val]\ny_val4 = [y[3] for y in y_val]\n\n# convert class vectors to binary class metrices\ny_train1 = keras.utils.to_categorical(y_train1, 2)\ny_train2 = keras.utils.to_categorical(y_train2, 2)\ny_train3 = keras.utils.to_categorical(y_train3, 2)\ny_train4 = keras.utils.to_categorical(y_train4, 2)\n\ny_val1 = keras.utils.to_categorical(y_val1, 2)\ny_val2 = keras.utils.to_categorical(y_val2, 2)\ny_val3 = keras.utils.to_categorical(y_val3, 2)\ny_val4 = keras.utils.to_categorical(y_val4, 2)","a22987e7":"def define_model():\n    inputs = Input(shape=(size, size, 3))\n    \n    # 1st layer\n    x = BatchNormalization()(inputs)\n    x = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # 2nd layer\n    x = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=256, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # 3rd layer\n    x = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), strides=(1,1))(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(0.2)(x)\n    \n    # Flatten\n    x = Flatten()(x)\n    \n    # Dens layer\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    \n    output1 = Dense(2, activation=\"softmax\", name='output1')(x)\n    output2 = Dense(2, activation=\"softmax\", name='output2')(x)\n    output3 = Dense(2, activation=\"softmax\", name='output3')(x)\n    output4 = Dense(2, activation=\"softmax\", name='output4')(x)\n    \n    multiModel = Model(inputs, [output1, output2, output3, output4])\n    \n    # initiate Adam optimizer\n    opt = keras.optimizers.adam(lr=0.0001, decay=0.00001)\n    \n    # Compile\n    multiModel.compile(loss={'output1':'categorical_crossentropy',\n                            'output2':'categorical_crossentropy',\n                            'output3':'categorical_crossentropy',\n                            'output4':'categorical_crossentropy'},\n                      optimizer=opt,\n                      metrics=[\"accuracy\"])\n    return multiModel","2a8281b9":"# data augmentation, This is dropped because GPU is down.\ndatagen = ImageDataGenerator(rotation_range=360,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n                             horizontal_flip=True)\ndatagen.fit(X_train)\n\n# define early stopping\nes_cb = EarlyStopping(monitor='val_loss',\n                    patience=15,\n                    verbose=1)\ncp_cb = ModelCheckpoint(\"cnn_model_02.h5\",\n                        monitor='val_loss',\n                        verbose=1,\n                        save_best_only=True)\n# parameters\nbatch_size = 8\nepochs = 100\n\n# train model\nmodel = define_model()\nhistory = model.fit(X_train,\n                   {'output1':y_train1,\n                    'output2':y_train2,\n                    'output3':y_train3,\n                    'output4':y_train4},\n                   batch_size=batch_size,\n                   epochs=epochs,\n                   validation_data=(X_val,\n                                   {'output1':y_val1,\n                                    'output2':y_val2,\n                                    'output3':y_val3,\n                                    'output4':y_val4}),\n                   callbacks=[es_cb, cp_cb])","e8e4f772":"# train_loss\ntrain1_loss = history.history[\"output1_loss\"]\ntrain2_loss = history.history[\"output2_loss\"]\ntrain3_loss = history.history[\"output3_loss\"]\ntrain4_loss = history.history[\"output4_loss\"]\n\n# val_loss\nval1_loss = history.history[\"val_output1_loss\"]\nval2_loss = history.history[\"val_output2_loss\"]\nval3_loss = history.history[\"val_output3_loss\"]\nval4_loss = history.history[\"val_output4_loss\"]\n\n# train_accuracy\ntrain1_acc = history.history[\"output1_accuracy\"]\ntrain2_acc = history.history[\"output2_accuracy\"]\ntrain3_acc = history.history[\"output3_accuracy\"]\ntrain4_acc = history.history[\"output4_accuracy\"]\n\n# val_accuracy\nval1_acc = history.history[\"val_output1_accuracy\"]\nval2_acc = history.history[\"val_output2_accuracy\"]\nval3_acc = history.history[\"val_output3_accuracy\"]\nval4_acc = history.history[\"val_output4_accuracy\"]\n\n# Visualization\nfig, ax = plt.subplots(2,4,figsize=(25,10))\nplt.subplots_adjust(wspace=0.3)\n\n# train1 loss\nax[0,0].plot(range(len(train1_loss)), train1_loss, label='train1_loss')\nax[0,0].plot(range(len(val1_loss)), val1_loss, label='val1_loss')\nax[0,0].set_xlabel('epoch', fontsize=16)\nax[0,0].set_ylabel('loss', fontsize=16)\nax[0,0].set_yscale('log')\nax[0,0].legend(fontsize=16)\n\n# train2 loss\nax[0,1].plot(range(len(train2_loss)), train2_loss, label='train2_loss')\nax[0,1].plot(range(len(val2_loss)), val2_loss, label='val2_loss')\nax[0,1].set_xlabel('epoch', fontsize=16)\nax[0,1].set_ylabel('loss', fontsize=16)\nax[0,1].set_yscale('log')\nax[0,1].legend(fontsize=16)\n\n# train3 loss\nax[0,2].plot(range(len(train3_loss)), train3_loss, label='train3_loss')\nax[0,2].plot(range(len(val2_loss)), val3_loss, label='val3_loss')\nax[0,2].set_xlabel('epoch', fontsize=16)\nax[0,2].set_ylabel('loss', fontsize=16)\nax[0,2].set_yscale('log')\nax[0,2].legend(fontsize=16)\n\n# train4 loss\nax[0,3].plot(range(len(train4_loss)), train4_loss, label='train4_loss')\nax[0,3].plot(range(len(val4_loss)), val4_loss, label='val4_loss')\nax[0,3].set_xlabel('epoch', fontsize=16)\nax[0,3].set_ylabel('loss', fontsize=16)\nax[0,3].set_yscale('log')\nax[0,3].legend(fontsize=16)\n\n# train1 accuracy\nax[1,0].plot(range(len(train1_acc)), train1_acc, label='train1_accuracy')\nax[1,0].plot(range(len(val1_acc)), val1_acc, label='val1_accuracy')\nax[1,0].set_xlabel('epoch', fontsize=16)\nax[1,0].set_ylabel('accuracy', fontsize=16)\nax[1,0].set_yscale('log')\nax[1,0].legend(fontsize=16)\n\n# train2 accuracy\nax[1,1].plot(range(len(train2_acc)), train2_acc, label='train2_accuracy')\nax[1,1].plot(range(len(val2_acc)), val2_acc, label='val2_accuracy')\nax[1,1].set_xlabel('epoch', fontsize=16)\nax[1,1].set_ylabel('accuracy', fontsize=16)\nax[1,1].set_yscale('log')\nax[1,1].legend(fontsize=16)\n\n# train3 accuracy\nax[1,2].plot(range(len(train3_acc)), train3_acc, label='train3_accuracy')\nax[1,2].plot(range(len(val3_acc)), val3_acc, label='val3_accuracy')\nax[1,2].set_xlabel('epoch', fontsize=16)\nax[1,2].set_ylabel('accuracy', fontsize=16)\nax[1,2].set_yscale('log')\nax[1,2].legend(fontsize=16)\n\n# train4 accuracy\nax[1,3].plot(range(len(train4_acc)), train4_acc, label='train4_accuracy')\nax[1,3].plot(range(len(val4_acc)), val4_acc, label='val4_accuracy')\nax[1,3].set_xlabel('epoch', fontsize=16)\nax[1,3].set_ylabel('accuracy', fontsize=16)\nax[1,3].set_yscale('log')\nax[1,3].legend(fontsize=16)","b08671d5":"model = load_model('cnn_model_02.h5')","cc0221ba":"predict = model.predict(X_Test)\nhealthy = [y_test[1] for y_test in predict[0]]\nmultiple_diseases = [y_test[1] for y_test in predict[1]]\nrust = [y_test[1] for y_test in predict[2]]\nscab = [y_test[1] for y_test in predict[3]]","807a5f91":"submit = pd.DataFrame({\"image_id\":test[\"image_id\"],\n                    \"healthy\":healthy,\n                    \"multiple_diseases\":multiple_diseases,\n                    \"rust\":rust,\n                    \"scab\":scab})\nsubmit.tail()","99314533":"submit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","05fbe000":"## 2. Preporcessing","a2712a48":"## Libraries","043159c7":"## 5. Check the result","6b25254c":"Preparing Training data and validation data","c81569dd":"Train data shape adjusting for learning","463255fb":"I try to take basic \"Convolutional neural network\".<br>\nHow to improve prediction accuracy, 'Data augmentation', 'Batchnormalization', 'Dropout'<br>\nI'm a starter, so there's nothing special about it, but I hope it's good information for the same beginner.<br>\n*I dropped Data Augmentation, because GPU is down.","63a1fbab":"The image is blurry, but features, Leaf shape\/contour\/color,  are almost captured.","819a1a2c":"# Convolutional neural network, Plant pathology","7a7ef822":"## Data loading","a82208f5":"## 7. Creation submit data","68197eb9":"Train targe","411c1c42":"## 4. Calculation","4ccc4c5f":"This time, image size is 64\u00d764, increasing the size will increase accuracy, but I decided here because the memory would die.","2bc0432b":"Although over-fitting tendency is seen for each label, the accuracy of the evaluation data has reached almost 90%.","2f346496":"## 6. Prediction from test data","608ae9c1":"I used representative technique, Data augmentation, BatchNormalization, Dropout in Convolutional neural network.","2d28a1ac":"Test data shape adjusting for predicting","5f0a4d5b":"## Data checking","a5f60342":"## 3. Define the model","9317be02":"### 1. Data loading and Data checking\n### 2. Preporcessing\n### 3. Define the model\n### 4. Calculation\n### 5. Check the result\n### 6. Prediction from test data\n### 7. Creation submit data"}}