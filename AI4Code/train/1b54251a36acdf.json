{"cell_type":{"7be81e19":"code","9ecbe403":"code","eba74ab2":"code","256cb26e":"code","bab9467e":"code","d6232b95":"code","0ca75e13":"code","8900fb0c":"code","e9cc5267":"code","0bacae9a":"code","4e130ff4":"code","3ebe5824":"markdown"},"source":{"7be81e19":"import sys\npackage_dir = \"..\/input\/ppbert\/pytorch-pretrained-BERT\/pytorch_pretrained_bert\/\"\nsys.path.append(package_dir)","9ecbe403":"from __future__ import absolute_import, division, print_function\nimport torch.utils.data\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport os\nimport warnings\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertConfig, BertAdam\n\nwarnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\n","eba74ab2":"## Function to convert sentence into tokenized format\n\ndef convert_lines(example, max_seq_length, tokenizer):\n    max_seq_length -=2\n    all_tokens =[]\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a) > max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"]) +[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)","256cb26e":"MAX_SEQUENCE_LENGTH = 220\nSEED  = 7 \nBATCH_SIZE = 32\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'","bab9467e":"np.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","d6232b95":"bert_config = BertConfig('..\/input\/bert-inference\/bert\/bert_config.json')\ntokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir = None, do_lower_case = True)","0ca75e13":"# reading test dataset\ntest_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\ntest_df['comment_text'] = test_df['comment_text'].astype('str')\nX_test = convert_lines(example = test_df[\"comment_text\"].fillna(\"DUMMY_VALUE\"), max_seq_length = MAX_SEQUENCE_LENGTH, tokenizer = tokenizer)","8900fb0c":"model = BertForSequenceClassification(bert_config , num_labels = 1)\nmodel.load_state_dict(torch.load('..\/input\/bert-inference\/bert\/bert_pytorch.bin'))\nmodel.to(device)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.eval()","e9cc5267":"test_preds = np.zeros(len(X_test))\ntest = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype = torch.long))\ntest_loader =  torch.utils.data.DataLoader(test,batch_size = 32, shuffle = False)\ntk0 = tqdm(test_loader)\nfor i, (x_batch,) in enumerate(tk0):\n    pred = model(x_batch.to(device), attention_mask = (x_batch > 0).to(device), labels = None)\n    test_preds[i*32:(i+1)*32] = pred[:,0].detach().cpu().squeeze().numpy()\ntest_pred = torch.sigmoid(torch.tensor(test_preds)).numpy().ravel()","0bacae9a":"submission = pd.DataFrame.from_dict({\n    'id':test_df['id'],\n    'prediction':test_pred\n})\nsubmission.to_csv(\"submission.csv\", index = False)","4e130ff4":"submission.head()","3ebe5824":"Reference:\nhttps:\/\/www.kaggle.com\/abhishek\/pytorch-bert-inference\/notebook"}}