{"cell_type":{"efc39251":"code","d18560dd":"code","33609b40":"code","327e1c97":"code","f4925052":"code","d18949d2":"code","8779de48":"code","241f16f6":"code","9f11d03e":"code","0f104737":"code","c96630a5":"code","cc74a610":"code","d5c8b73d":"code","927c1c6e":"code","11bcdb45":"markdown","ba93e4a3":"markdown","10727d89":"markdown","f89a33ad":"markdown","0219d5cd":"markdown","58d24f25":"markdown","cd36f0e5":"markdown","e73cd728":"markdown","ad5d219f":"markdown","bf020017":"markdown","69273ef7":"markdown","e44bcd68":"markdown","9afcc346":"markdown","e8d9a5a2":"markdown"},"source":{"efc39251":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d18560dd":"from catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","33609b40":"df=pd.read_csv('\/kaggle\/input\/house-prices-data-categorical-features-encoded\/Train.csv')\nef=pd.read_csv('\/kaggle\/input\/house-prices-data-categorical-features-encoded\/Test.csv')\nef=ef.drop(['Id','Unnamed: 0'],axis=1)","327e1c97":"df.mean()","f4925052":"ef.mean()","d18949d2":"df=df.fillna(df.mean())","8779de48":"ef=ef.fillna(ef.mean())","241f16f6":"X=df.drop(['SalePrice','Id','Unnamed: 0'],axis=1)\ny=pd.DataFrame(df['SalePrice'])","9f11d03e":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)","0f104737":"model=RandomForestRegressor()\nmodel.fit(X_train,y_train)","c96630a5":"predictions=model.predict(X_test)","cc74a610":"mse= mean_squared_error(y_test,predictions)\nmse","d5c8b73d":"test_pred=pd.DataFrame(model.predict(ef))","927c1c6e":"test_pred.to_csv('testpred1.csv')","11bcdb45":"# Train test split","ba93e4a3":"# Prediction csv file","10727d89":"# Import Requirements","f89a33ad":"##  Hooray! We are going places just by cleaning our data.","0219d5cd":"# Replace missing values in each column with its mean","58d24f25":"##  1) Impute missing values by using better approaches. Mean, median, mode, fillna(0) are too simplistic.\n##  2) Feature Engineering.\n##  3) Feature Selection.\n##  4) Model Selection.\n##  5) Hyper Optimization","cd36f0e5":"![sub.png](attachment:874328af-d136-469d-a240-e42cbbf55196.png)","e73cd728":"# Call Train and Test Datasets","ad5d219f":"# Declare target and features","bf020017":"# The End","69273ef7":"# Modeling","e44bcd68":"# Way Forward","9afcc346":"![House.png](attachment:ec54a40d-f84e-4ec8-b18a-9143d6ee9498.png)","e8d9a5a2":"##  Go here (https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) and download the sample_submission csv file and paste your predictions"}}