{"cell_type":{"11d85207":"code","34c4f31d":"code","f0a5e1ea":"code","9f0c960a":"code","abb23f29":"code","2fd38c34":"code","c80c4f97":"code","c015f487":"code","bee46560":"code","8c7497fb":"code","06899a8b":"markdown","f36e2fa1":"markdown","cddc57fe":"markdown","af806165":"markdown","3929ea00":"markdown","fda57b7c":"markdown","8f1431f9":"markdown","78399b0d":"markdown","7d6831b2":"markdown","049c1c12":"markdown"},"source":{"11d85207":"# Import all the necessary packages: \nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss \nimport scipy.optimize as so\nfrom sklearn import linear_model\n\n%matplotlib inline","34c4f31d":"# Read in the data with pandas\npossum_data = pd.read_csv('..\/input\/possum-data\/possum.csv')                      \n\n# Make the scatter plot (don't forget the axis labels)\n\npossum_data.plot(x=\"age\",y=\"tailL\",kind=\"scatter\",alpha=0.5,c=\"green\",s=75)\nplt.title(\"Age vs tail length in possum\")\nplt.show()","f0a5e1ea":"def linearModelPredict(b,X):\n    yp = print(X@b.T)\n    return yp\n# Always important: Test the new function you have written! \nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\nlinearModelPredict(b,X)\n# the predicted y is a 1d-array.\n# By the way: What happens when b is a 2d-array? \n\n","9f0c960a":"\ndef linearModelLossRSS(b,X,y):\n    predY = np.array([ 0.1, -0.2,  0.7])   #from last question\n    residual=y-predY\n    residual_sum_of_squares = sum(residual**2)\n    gradient = [-2*sum(residual),-2*sum(residual@X)]\n    return (residual_sum_of_squares,gradient )\n\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\ny = np.array([0,0.4,2]) \n\nlinearModelLossRSS(b,X,y)\n\n#To minimize the cost we need to increase the value of the two parameters since the partial derivatives \n#of loss with respect to both are negative.","abb23f29":"bstart=[0,0]\nR = so.minimize(linearModelLossRSS,bstart,args=(X,y),jac=True)\n","2fd38c34":"def linearModelFit(X,y,lossfcn = linearModelLossRSS):\n    bstart=[0,0]\n    result = so.minimize(linearModelLossRSS,bstart,args=(X,y),jac=True)\n    return result\n\nX = np.array([[1,0],[1,-1],[1,2]])\ny = np.array([0,0.4,2]) \n\nlinearModelFit(X,y,lossfcn = linearModelLossRSS)","c80c4f97":"# Make the design matrix using np.c_ \n\n# Call your fitting function \n\n\n# Create the scatter plot (see question 1.1)\n\n# Create a new X matrix with equally space data \n\n# Add the line to the graph \n\n# Report R2 ","c015f487":"def linearModelLossLAD(b,X,y):\n    predY = np.array([ 0.1, -0.2,  0.7])   \n    residual=y-predY\n    sum_abs_dev = sum(residual)\n    return sum_abs_dev\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\ny = np.array([0,0.4,2])\n\nlinearModelLossLAD(b,X,y)\n\n","bee46560":"def linearModelLossLAD(b,X,y):\n    predY = np.array([ 0.1, -0.2,  0.7])   \n    residual=y-predY\n    gradient = [-sum(sgn(residual)),-sum(sgn(residual)*X)]\n    return gradient\n\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\ny = np.array([0,0.4,2])\n\nlinearModelLossLAD(b,X,y)","8c7497fb":"# The L2 cost function penalize for large influential points, in this case, L2 loss has a higher R2","06899a8b":"### Question 1.4:  \/15 points. \n\nNow that you've implemented a loss function in question 1.3, it is now time to minimize it!\n\nWrite a function `linearModelFit` to fit a linear model.  The function should take as its first argument the design matrix `X` as a 2d-array, as its second argument a 1d-array `y` of outcomes, and as its third argument a function  `lossfcn` which returns as a tuple the value of the loss, as well as the gradient of the loss. As a result, it should return the estimated betas and the R2. \n\nTest the function with the values: \n```\nX = np.array([[1,0],[1,-1],[1,2]])\ny = np.array([0,0.4,2]) \n```\n\nReport best parameters and the fitted R2 \n","f36e2fa1":"# Grade: \/100 points\n\n# Assignment 01: Supervised learning, Linear models, and Loss functions\n\nIn this assignment, you're going to write your own methods to fit a linear model using either an OLS or LAD cost function.  \n\n## Data set \n\nFor this assignment, we will examine some data representing possums in Australia and New Guinea. The data frame contains 46 observations on the following 6 variables:\n\n* sex: Sex, either m (male) or f (female).\n* age: Age in years.\n* headL: Head length, in mm.\n* skullW: Skull width, in mm.\n* totalL: Total length, in cm.\n* tailL: Tail length, in cm.\n\n## Follow These Steps Before Submitting\nOnce you are finished, ensure to complete the following steps.\n\n1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n\n2.  Fix any errors which result from this.\n\n3.  Repeat steps 1. and 2. until your notebook runs without errors.\n\n4.  Submit your completed notebook to OWL by the deadline.\n\n\n## Preliminaries","cddc57fe":"### Question 1.5: \/15 points\n\nUse the above functions to fit your model to the possum data. Then use your model and the fitted parameters to make predictions along a grid of equally spaced possum ages.  \n\nPlot the data and add a line for the predicted values. You can get these by generating a new X-matrix with equally space ages (using for example np.linspace). Also report the R2 value for the fit. You can do this by either printing out the R2 of the fit or putting it on your plot via the `annotate` function in matplotlib.\n","af806165":"### Question 1.2: \/5 point\n\nRecall that the linear model, we obtain predictions by computing \n\n$$ \\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\beta} $$\n\nHere, $\\mathbf{X}$ is a design matrix which includes a column of ones, $\\hat{\\beta}$ are coefficients, and $\\hat{\\mathbf{y}}$ are outcomes.  Write a function `linearModelPredict` to compute linear model predictions given data and a coefficient vector.  The function should take as it's arguments a 1d-array of coefficients `b` and the design matrix `X` as a 2d-array and return linear model predictions `yp`.\n\nTest the function by setting \n\n```\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\n```\nand call your function with these values! \n\nReport $\\hat{\\mathbf{y}}$. \nWhat is the dimensionality of the numpy-array that you get back? \n\nHint:  Read the documentation for `np.dot` or the `@` operator in `numpy`.","3929ea00":"### Question 2.2: \/10 points\n\n\nUse the above functions to fit your LAD model. Use your model to make predictions along a grid of equally spaced possum ages.  Once fit, add the fitted line to the scatter plot as in question 1.5.  Also report the R2-value. \n\n**Written answer**: What is the difference in the fit obtained with an L1 as compared to the L2 cost function? Which one has a higher R2 value? Why?  \n\nNote: If you recieve an error from the optimizer, it may be because the loss function for the LAD model is not differentiable at its minimum.  This will lead to some gradient based optimizers to fail to converge.  If this happens to you then pass `method=\"Powell\"` to `scipy.optimize.minimize`.\n\n","fda57b7c":"### Question 1.3: \/15 points\n\nWrite a function `linearModelLossRSS` which computes and returns the loss function for an OLS model parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n\nTest the function with the values \n\n```\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\ny = np.array([0,0.4,2]) \n```\n\nReport the loss and the gradient. \n\n**Written answer**: To minimize the cost do you need increase or decrease the value of the parameters? ","8f1431f9":"## Part 2: LAD Regression\n\n### Question 2.1:  \/15 points\n\nIn the previous section, we worked with the squared loss.  Now, we'll implement a linear model with least absolute deviation loss.\n\nWrite a function `linearModelLossLAD` which computes the least absolute deviation loss function for a linear model  parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n\nTest the function with the values \n\n```\nX = np.array([[1,0],[1,-1],[1,2]])\nb = np.array([0.1,0.3])\ny = np.array([0,0.4,2]) \n```\n\nReport the loss and the gradient. ","78399b0d":"Written answer: The LAD fit does not give as much weight to the outlier (9,55) as the OLS fit. The R2 value is lower, however. This is because OLS minimized the RSS, and therefore maximizes R2.  ","7d6831b2":"\n## Part 1\n### Question 1.1:  \/10 points\n\n\nRead in the `possum.csv` file as a `pandas.DataFrame`.  Investigate the relationship between the possum's age and its tail length by plotting a scatter plot of the `age` and `tailL` columns. Add an `alpha`(transparency of the plotted dots) in case some data are overlapping. ","049c1c12":"### Question 2.3: \/15 points\n\nFit an OLS model to the possum data with the `linear_model` module from the `sklearn` package by using the `LinearRegression` class.  In no more than two sentences, comment on the rsquared values from `sklearn` and the rsquared values from your models. Are they similar?"}}