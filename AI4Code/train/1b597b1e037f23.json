{"cell_type":{"d83d3505":"code","5127e995":"code","76cda34b":"code","0af927a9":"markdown","7ae7cd34":"markdown","e54fc074":"markdown","14302178":"markdown","1960f2b0":"markdown","5db9a5ce":"markdown"},"source":{"d83d3505":"# Import the neccessary modules\/libraries\n!pip install autocorrect\nimport spacy\nimport pandas as pd\nimport nltk\nimport collections\nfrom nltk.stem.porter import *\nfrom textblob import TextBlob\nfrom autocorrect import Speller\nfrom collections import Counter\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer,TfidfTransformer\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline","5127e995":"# Import dataset\n# Using Amazon Alexa reviews as our data\ndf_amazon = pd.read_csv(\"\/kaggle\/input\/amazon-alexa-reviews\/amazon_alexa.tsv\",sep=\"\\t\")\n\n# print(f'Shape of data: {df_amazon.shape}')\n\n# Set the number reviews that you want to examine\n# You can increase the number but it will take longer to run\/complete\nnum_reviews = 200\n\n# Initialize Speller object for English to correct spelling mistakes\nspell = Speller(lang = 'en')\n\n# Initialize stemmer\nstemmer = PorterStemmer()\n\n# Initialize lists\ntotalWords = []\npartsOfSpeech = []\nlemma = []\nstems = []\nwords = []\nsentences = []\n\n# Iterate through reviews\nfor index in range(0, num_reviews):\n    test = df_amazon.verified_reviews[index]\n    nlp = spacy.load('en_core_web_sm')\n    review = nlp(test)\n    currentWords = []\n    for token in review:\n        # Ensure that the token isn't a stopword, punctuation, or empty space for better analysis\n        if not token.is_stop and not token.is_punct and not token.text == ' ':\n            partsOfSpeech.append(token.pos_)\n            if token.pos_ == 'PROPN':\n                # If any words in the reviews are misspelled, correct the spelling before finding their stems or lemma\n                totalWords.append(spell(token.text))\n                stems.append(stemmer.stem(spell(token.text)))\n                token.lemma_ = spell(token.lemma_)\n                currentWords.append(token.text)\n            else:\n                # If any words in the reviews are misspelled, correct the spelling before finding their stems or lemma\n                totalWords.append(spell(token.text).lower())\n                stems.append(stemmer.stem(spell(token.text).lower()))\n                token.lemma_ = spell(token.lemma_.lower())\n                currentWords.append(token.text)\n            lemma.append(token.lemma_)\n    sentences.append(' '.join(currentWords))\n\n# print(totalWords)\n\n# Calculate the frequencies of the words, parts of speech, stems, and lemmas\nword_freq = Counter(totalWords)\npartOfSpeech_freq = Counter(partsOfSpeech)\nstem_freq = Counter(stems)\nlemma_freq = Counter(lemma)\n\n# Print statements \nprint(\"Words: \")\nprint(word_freq)\nprint(\"\")\nprint(\"Parts of Speech: \")\nprint(partOfSpeech_freq)\nprint(\"\")\nprint(\"Stems: \")\nprint(stem_freq)\nprint(\"\")\nprint(\"Lemma: \")\nprint(lemma_freq)\nprint(\"\")\n# print(sentences)\n\n# Creates word associations for bigrams that appear more than 3 times\nfinder = nltk.BigramCollocationFinder.from_words(lemma)\nfinder.apply_freq_filter(3)\n\n# Prints the 10 n-grams with the highest PMI\nprint(\"Top 10 Word Associations: \")\nprint(finder.nbest(nltk.collocations.BigramAssocMeasures().pmi, 10))","76cda34b":"# Generate TF-IDF table\ntfIdfVectorizer = TfidfVectorizer(use_idf=True)\ntfIdf = tfIdfVectorizer.fit_transform(sentences)\ndf = pd.DataFrame(tfIdf[0].T.todense(), index = tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n# Sort values in descending order\ndf = df.sort_values('TF-IDF', ascending=False)\n# Print the TF-IDF table\nprint(df.head(5))\nprint(\" \")\n\n# Find bigrams and their likelihood ratios\nbgm = nltk.collocations.BigramAssocMeasures()\nscored = finder.score_ngrams(bgm.likelihood_ratio)\nprefix_keys = collections.defaultdict(list)\n# print(scored)\n# print(\" \")\n\nfor key, scores in scored:\n    prefix_keys[key[0]].append((key[1], scores))\n\n# Sort keyed bigrams by strongest association.                                  \nfor key in prefix_keys:\n    prefix_keys[key].sort(key = lambda x: -x[1])\n# print(prefix_keys)\n# print(\" \")\n\n# Print statements for bigrams\nprint(\"easy: \")\nprint(prefix_keys['easy'])\n\nprint(\"sound: \")\nprint(prefix_keys['sound'])\n\nprint(\"play: \")\nprint(prefix_keys['play'])\n\nprint(\"turn: \")\nprint(prefix_keys['turn'])","0af927a9":"The outputs of our code have very distinct characteristics and follow all of the steps within Chapter 2 to transform words into vectors for them to be ready to be processed by predictive methods. The first output is simply a frequency table that displays how often each word appears in the Amazon reviews, beginning our tokenization process. We will later use these tokens for text analysis. The next output provides a frequency distribution of the parts of speech that the words in the reviews belong to, which is an analytical use of the generated tokens. The two outputs that follow take care of stemming and lemmatization, which normalizes the previously generated tokens. Stemming and lemmatization reduces the different inflectional forms and derivationally related forms of words to a common base form. The outputs after that in Deliverable 2 display the TF-IDF table and bigram word associations with a likelihood ratio (PMI) score to measure the strength of the associations which are sorted in descending order.","7ae7cd34":"**Deliverable 3**","e54fc074":"**Deliverable 2**","14302178":"**Deliverable 4**","1960f2b0":"**Deliverable 1:**","5db9a5ce":"The use of a single token enhances the reliability of identifying the important concepts in a document\/corpus because it allows us to identify words with the same meaning and eliminate any unnecessary differences. Stemming and lemmata allow us to group all the different forms of each word together and a SynSet allows us to group all the synonyms with each other as well. By stemming, the ends of words are chopped off so that the common stems can be found. Lemmata analyzes and obtains the base forms of the words to eliminate any of the differing forms. By using stems, lemma, and SynSets the data of text is cleaned up and the reliability of the model that is being run is enhanced. This prepares text, words, and documents for further processing."}}