{"cell_type":{"e4843404":"code","05eb4e4f":"code","f0427555":"code","2c2c6082":"code","d0d51cab":"code","94c68d6a":"code","a99b4cf4":"code","465e4af6":"code","6623d95a":"code","a5944163":"code","4f417588":"code","1cec2540":"code","96680dfc":"code","1da5eca8":"code","cf083bb1":"code","756cb1f9":"code","0db6e9ee":"code","abbce06f":"code","6c06ec3e":"code","f362f928":"markdown","850a5663":"markdown","057d5874":"markdown","dd27a6cc":"markdown","275b85ef":"markdown","e5e9277e":"markdown","b3b76475":"markdown","f1110356":"markdown","7b8677e1":"markdown","10f3e0d5":"markdown"},"source":{"e4843404":"import numpy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport keras\nimport keras.backend as K","05eb4e4f":"#PyTorch\nclass DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)\/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice","f0427555":"#Keras\ndef DiceLoss(targets, inputs, smooth=1e-6):\n    \n    #flatten label and prediction tensors\n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    intersection = K.sum(K.dot(targets, inputs))\n    dice = (2*intersection + smooth) \/ (K.sum(targets) + K.sum(inputs) + smooth)\n    return 1 - dice","2c2c6082":"#PyTorch\nclass DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)\/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE","d0d51cab":"#Keras\ndef DiceBCELoss(targets, inputs, smooth=1e-6):    \n       \n    #flatten label and prediction tensors\n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    BCE =  binary_crossentropy(targets, inputs)\n    intersection = K.sum(K.dot(targets, inputs))    \n    dice_loss = 1 - (2*intersection + smooth) \/ (K.sum(targets) + K.sum(inputs) + smooth)\n    Dice_BCE = BCE + dice_loss\n    \n    return Dice_BCE","94c68d6a":"#PyTorch\nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return 1 - IoU","a99b4cf4":"#Keras\ndef IoULoss(targets, inputs, smooth=1e-6):\n    \n    #flatten label and prediction tensors\n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    intersection = K.sum(K.dot(targets, inputs))\n    total = K.sum(targets) + K.sum(inputs)\n    union = total - intersection\n    \n    IoU = (intersection + smooth) \/ (union + smooth)\n    return 1 - IoU","465e4af6":"#PyTorch\nALPHA = 0.8\nGAMMA = 2\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n                       \n        return focal_loss","6623d95a":"#Keras\nALPHA = 0.8\nGAMMA = 2\n\ndef FocalLoss(targets, inputs, alpha=ALPHA, gamma=GAMMA):    \n    \n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    BCE = K.binary_crossentropy(targets, inputs)\n    BCE_EXP = K.exp(-BCE)\n    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n    \n    return focal_loss","a5944163":"#PyTorch\nALPHA = 0.5\nBETA = 0.5\n\nclass TverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(TverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n       \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        \n        return 1 - Tversky","4f417588":"#Keras\nALPHA = 0.5\nBETA = 0.5\n\ndef TverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, smooth=1e-6):\n        \n        #flatten label and prediction tensors\n        inputs = K.flatten(inputs)\n        targets = K.flatten(targets)\n        \n        #True Positives, False Positives & False Negatives\n        TP = K.sum((inputs * targets))\n        FP = K.sum(((1-targets) * inputs))\n        FN = K.sum((targets * (1-inputs)))\n       \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        \n        return 1 - Tversky","1cec2540":"#PyTorch\nALPHA = 0.5\nBETA = 0.5\nGAMMA = 1\n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalTverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n        \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        FocalTversky = (1 - Tversky)**gamma\n                       \n        return FocalTversky","96680dfc":"#Keras\nALPHA = 0.5\nBETA = 0.5\nGAMMA = 1\n\ndef FocalTverskyLoss(targets, inputs, alpha=ALPHA, beta=BETA, gamma=GAMMA, smooth=1e-6):\n    \n        #flatten label and prediction tensors\n        inputs = K.flatten(inputs)\n        targets = K.flatten(targets)\n        \n        #True Positives, False Positives & False Negatives\n        TP = K.sum((inputs * targets))\n        FP = K.sum(((1-targets) * inputs))\n        FN = K.sum((targets * (1-inputs)))\n               \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        FocalTversky = K.pow((1 - Tversky), gamma)\n        \n        return FocalTversky","1da5eca8":"#PyTorch\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection \/ union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n    return loss\n\n#=====\n#Multi-class Lovasz loss\n#=====\n\ndef lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes='present'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n    \"\"\"\n    if probas.numel() == 0:\n        # only void pixels, the gradients should be 0\n        return probas * 0.\n    C = probas.size(1)\n    losses = []\n    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n    for c in class_to_sum:\n        fg = (labels == c).float() # foreground for class c\n        if (classes is 'present' and fg.sum() == 0):\n            continue\n        if C == 1:\n            if len(classes) > 1:\n                raise ValueError('Sigmoid output possible only with 1 class')\n            class_pred = probas[:, 0]\n        else:\n            class_pred = probas[:, c]\n        errors = (Variable(fg) - class_pred).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)","cf083bb1":"#PyTorch\nclass LovaszHingeLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(LovaszHingeLoss, self).__init__()\n\n    def forward(self, inputs, targets):\n        inputs = F.sigmoid(inputs)    \n        Lovasz = lovasz_hinge(inputs, targets, per_image=False)                       \n        return Lovasz","756cb1f9":"import tensorflow as tf\nimport numpy as np\n\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection \/ union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), grad, 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels","0db6e9ee":"#Keras\n# not working yet\n# def LovaszHingeLoss(inputs, targets):\n#     return lovasz_hinge_loss(inputs, targets)","abbce06f":"#PyTorch\nALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\nCE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n\nclass ComboLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(ComboLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, eps=1e-9):\n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        intersection = (inputs * targets).sum()    \n        dice = (2. * intersection + smooth) \/ (inputs.sum() + targets.sum() + smooth)\n        \n        inputs = torch.clamp(inputs, eps, 1.0 - eps)       \n        out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n        weighted_ce = out.mean(-1)\n        combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n        \n        return combo","6c06ec3e":"#Keras\nALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\nCE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n\ndef Combo_loss(targets, inputs, eps=1e-9):\n    targets = K.flatten(targets)\n    inputs = K.flatten(inputs)\n    \n    intersection = K.sum(targets * inputs)\n    dice = (2. * intersection + smooth) \/ (K.sum(targets) + K.sum(inputs) + smooth)\n    inputs = K.clip(inputs, eps, 1.0 - eps)\n    out = - (ALPHA * ((targets * K.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * K.log(1.0 - inputs))))\n    weighted_ce = K.mean(out, axis=-1)\n    combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n    \n    return combo","f362f928":"# Dice Loss\n---\nThe Dice coefficient, or Dice-S\u00f8rensen coefficient, is a common metric for pixel segmentation that can also be modified to act as a loss function:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/a80a97215e1afc0b222e604af1b2099dc9363d3b)","850a5663":"# Usage Tips\n---\n\nIn my experience testing and debugging these losses, I have some observations that may be useful to beginners experimenting with different loss functions. These are not rules that are set in stone; they are simply my findings and your results may vary.\n\n* Tversky and Focal-Tversky loss benefit from very low learning rates, of the order 5e-5 to 1e-4. They would not see much improvement in my kernels until around 7-10 epochs, upon which performance would improve significantly.\n\n* In general, if a loss function does not appear to be working well (or at all), experiment with modifying the learning rate before moving on to other options.\n\n* You can easily create your own loss functions by combining any of the above with Binary Cross-Entropy or any combination of other losses. Bear in mind that loss is calculated for every batch, so more complex losses will increase runtime.\n\n* Care must be taken when writing loss functions for PyTorch. If you call a function to modify the inputs that doesn't entirely use PyTorch's numerical methods, the tensor will 'detach' from the the graph that maps it back through the neural network for the purposes of backpropagation, making the loss function unusable. Discussion of this is available [here.](https:\/\/discuss.pytorch.org\/t\/some-problems-in-custom-loss-functions-and-so-on\/36618)\n\nI hope this kernel is of use to you. Good luck with your work!","057d5874":"# BCE-Dice Loss\n---\nThis loss combines Dice loss with the standard binary cross-entropy (BCE) loss that is generally the default for segmentation models. Combining the two methods allows for some diversity in the loss, while benefitting from the stability of BCE. The equation for multi-class BCE by itself will be familiar to anyone who has studied logistic regression:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/80f87a71d3a616a0939f5360cec24d702d2593a2)","dd27a6cc":"# Focal Loss\n---\nFocal Loss was introduced by *Lin et al* of Facebook AI Research in 2017 as a means of combatting extremely imbalanced datasets where positive cases were relatively rare. Their paper \"Focal Loss for Dense Object Detection\" is retrievable here: https:\/\/arxiv.org\/abs\/1708.02002. In practice, the researchers used an alpha-modified version of the function so I have included it in this implementation.","275b85ef":"# Focal Tversky Loss\n---\n\nA variant on the Tversky loss that also includes the gamma modifier from Focal Loss.","e5e9277e":"# Lovasz Hinge Loss\n---\nThis complex loss function was introduced by Berman, Triki and Blaschko in their paper \"The Lovasz-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks\", retrievable here: https:\/\/arxiv.org\/abs\/1705.08790. It is designed to optimise the Intersection over Union score for semantic segmentation, particularly for multi-class instances. Specifically, it sorts predictions by their error before calculating cumulatively how each error affects the IoU score. This gradient vector is then multiplied with the initial error vector to penalise most strongly the predictions that decreased the IoU score the most. This procedure is detailed by [jeandebleu](https:\/\/www.kaggle.com\/jeandebleau) in his excellent summary [here](https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/67791).\n\nThis code is taken directly from the author's github repo here: https:\/\/github.com\/bermanmaxim\/LovaszSoftmax and all credit is to them.\n\nIn this kernel I have implemented the flat variant that uses reshaped rank-1 tensors as inputs for PyTorch. You can modify it accordingly with the dimensions and class number of your data as needed. This code takes raw logits so ensure your model does not contain an activation layer prior to the loss calculation.\n\nI have hidden the researchers' own code below for brevity; simply load it into your kernel for the losses to function. In the case of their tensorflow implementation, I am still working to make it compatible with Keras. There are differences between the Tensorflow and Keras function libraries that complicate this.","b3b76475":"# Combo Loss\n---\nThis loss was introduced by Taghanaki et al in their paper \"Combo loss: Handling input and output imbalance in multi-organ segmentation\", retrievable here: https:\/\/arxiv.org\/abs\/1805.02798. Combo loss is a combination of Dice Loss and a modified Cross-Entropy function that, like Tversky loss, has additional constants which penalise either false positives or false negatives more respectively.\n\nSince my GPU quota has run out this week, as of V16 these functions have not been tested, so please leave any debugging notes in the comments section below.","f1110356":"# Loss Function Reference for Keras & PyTorch\n---\nThis kernel provides a reference library for some popular custom loss functions that you can easily import into your code.\n\nLoss functions define how neural network models calculate the overall error from their residuals for each training batch. This in turn affects how they adjust their internal weights when performing backpropagation, so the choice of loss function has a direct influence on model performance.\n\nThe default choice of loss function for segmentation and other classification tasks is Binary Cross-Entropy (BCE). In situations where a particular metric, like the Dice Coefficient or Intersection over Union (IoU), is being used to judge model performance, competitors will sometimes experiment with loss functions that derive from these metrics - typically in the form `1 - f(x)` where `f(x)` is the metric in question. \n\nThese functions cannot simply be written in NumPy, as they must operate on tensors that also have gradient parameters which need to be calculated throughout the model during backpropagation. Accordingly, loss functions must be written using backend functions from the respective model library. This is less complicated than it sounds! For example in Keras, you would simply use the same familiar mathematical functions, albeit using the Keras backend imported as `K`, i.e `K.sum()`. Gradient calculation is handled automatically by the model libraries, although (at least in PyTorch) you can define this manually if you are confident in your mathematical ability and wish to make alterations of your own.\n\nWith multi-class classification or segmentation, we sometimes use loss functions that calculate the average loss for each class, rather than calculating loss from the prediction tensor as a whole. This kernel is meant as a template reference for the basic code, so all examples calculate loss on the entire tensor, but it should be trivial for you to modify it for multi-class averaging. \n\nI hope this kernel will be of use to you, and any corrections or suggestions are welcome.","7b8677e1":"# Jaccard\/Intersection over Union (IoU) Loss\n---\nThe IoU metric, or Jaccard Index, is similar to the Dice metric and is calculated as the ratio between the overlap of the positive instances between two sets, and their mutual combined values:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7)\n\nLike the Dice metric, it is a common means of evaluating the performance of pixel segmentation models.","10f3e0d5":"# Tversky Loss\n---\nThis loss was introduced in \"Tversky loss function for image segmentationusing 3D fully convolutional deep networks\", retrievable here: https:\/\/arxiv.org\/abs\/1706.05721. It was designed to optimise segmentation on imbalanced medical datasets by utilising constants that can adjust how harshly different types of error are penalised in the loss function. From the paper:\n\n>... in the case of \u03b1=\u03b2=0.5 the Tversky index simplifies to be the same as the Dice coefficient, which is also equal to the F1 score.  With \u03b1=\u03b2=1, Equation 2 produces Tanimoto coefficient, and setting \u03b1+\u03b2=1 produces the set of F\u03b2 scores. Larger \u03b2s weigh recall higher than precision (by placing more emphasis on false negatives).\n\nTo summarise, this loss function is weighted by the constants 'alpha' and 'beta' that penalise false positives and false negatives respectively to a higher degree in the loss function as their value is increased. The beta constant in particular has applications in situations where models can obtain misleadingly positive performance via highly conservative prediction. You may want to experiment with different values to find the optimum. With alpha==beta==0.5, this loss becomes equivalent to Dice Loss."}}