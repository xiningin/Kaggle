{"cell_type":{"52420460":"code","688a1b21":"code","289d59a4":"code","67f1872f":"code","6a4b2827":"code","e78ffca1":"code","ebd11491":"code","9bbe7f72":"code","bc3e779f":"code","88b65aaf":"code","c7298b4f":"code","2ec25e6a":"code","90411c57":"code","fd56274a":"code","54eed263":"code","13e8ae55":"code","8ba740ef":"code","cf751da6":"code","8037dbf8":"code","5764cd9a":"code","192837ed":"code","3bd90818":"code","0201fdd4":"code","74e733bc":"code","4ee72ff1":"code","a8cc6602":"code","a1c59291":"code","64bfbf24":"code","b960441e":"code","009ba673":"code","3d1e337c":"code","3f17b6e8":"code","27e06286":"code","159365c9":"code","7926a9c2":"code","f9559131":"code","b8e01263":"code","a8fbb41c":"code","bfeb492a":"code","b3383e98":"markdown","f60e74a6":"markdown","eeb34066":"markdown","aa93fabc":"markdown","17ee1e38":"markdown"},"source":{"52420460":"!pip install -qq transformers","688a1b21":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader , Dataset\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns \nfrom pylab import rcParams\nimport re\nfrom sklearn import metrics\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertModel , BertTokenizer , AdamW ","289d59a4":"rcParams[\"figure.figsize\"]=12,9","67f1872f":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","6a4b2827":"sub=\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\"\ntrain_file =\"\/kaggle\/input\/nlp-getting-started\/train.csv\"\ntest_file =\"\/kaggle\/input\/nlp-getting-started\/test.csv\"","e78ffca1":"df_train = pd.read_csv(train_file)\ndf_train.head(3)","ebd11491":"df_train=df_train.drop([\"id\",\"keyword\",\"location\"] , axis=1)","9bbe7f72":"df_train.head(3)","bc3e779f":"sns.countplot(df_train.target)","88b65aaf":"def preprocessing(text):\n    text=re.sub( '[^a-zA-Z0-9]', ' ', text)\n    return text\n ","c7298b4f":"df_train[\"text\"]=df_train[\"text\"].apply(preprocessing)","2ec25e6a":"df_train.head()","90411c57":"class TweetDataset(Dataset):\n    def __init__(self , texts , targets , max_len):\n        self.texts = texts \n        self.targets = targets\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self ,item):\n        text = str(self.texts[item])\n        target = self.targets[item]\n        tokenized_text = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len ,\n            padding='max_length' ,\n            truncation=True , \n           \n        )\n        \n        ids = tokenized_text[\"input_ids\"]\n        mask = tokenized_text[\"attention_mask\"]\n        token_type_ids = tokenized_text[\"token_type_ids\"]\n\n        return{\n            \"ids\":torch.tensor(ids , dtype=torch.long) ,\n            \"attention_mask\":torch.tensor(mask , dtype=torch.long),\n            \"token_type_ids\":torch.tensor(token_type_ids , dtype=torch.long),\n            \"target\":torch.tensor(target , dtype=torch.float)\n        }\n    \n","fd56274a":"def TweetDataLoader(data, batch_size  , max_len):\n    dataset =  TweetDataset(data.text.values , data.target.values , max_len)\n    dataloader = DataLoader(dataset , batch_size , shuffle=True)\n    \n    return dataloader\n    ","54eed263":"train_data , valid_data =train_test_split(df_train , test_size = 0.2)","13e8ae55":"print(f\"the length of train_data  : {len(train_data)}\" )","8ba740ef":"train_loader = TweetDataLoader(train_data ,20 , 100 )\nvalid_loader = TweetDataLoader(valid_data ,20 , 100 )","cf751da6":"data = next(iter(train_loader))","8037dbf8":"data.keys()","5764cd9a":"data","192837ed":"class TweetModel(nn.Module):\n    def __init__(self , num_classes):\n        super(TweetModel,self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased' ,  return_dict =False)\n        self.drop = nn.Dropout(p=0.1)\n        self.classifier = nn.Linear(768 , num_classes)\n        \n    def forward(self , ids , mask , token_type_ids):\n        _ , outpooled = self.bert(ids , mask , token_type_ids)\n        out = self.drop (outpooled)\n        \n        return self.classifier(out)","3bd90818":"classe_name =[\"negative\",\"positive\"]","0201fdd4":"def loss_fn(outputs , targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))","74e733bc":"def train_step(model , data , device):\n    model.train()\n    \n    optimizer = AdamW(model.parameters() , lr=2e-5)\n    \n    \n    for step , data in tqdm(enumerate(data) , total=len(data)):\n        optimizer.zero_grad()\n        ids =  data[\"ids\"].to(device)\n        mask= data[\"attention_mask\"].to(device)\n        token_type_ids =  data[\"token_type_ids\"].to(device)\n        targets = data[\"target\"].to(device)\n        outputs =  model (ids , mask , token_type_ids)\n        \n        loss = loss_fn(outputs, targets)\n        \n         # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()","4ee72ff1":"def eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            \n            ids =data[\"ids\"].to(device)\n            mask = data[\"attention_mask\"].to(device)\n            token_type_ids =  data[\"token_type_ids\"].to(device)\n            targets =  data[\"target\"].to(device)\n     \n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n        \n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","a8cc6602":"model = TweetModel(1)\nmodel.to(device)","a1c59291":"best_metric = 0\nfor epoch in range(12):\n    train_step(model ,train_loader, device)\n    outputs, targets = eval_fn(valid_loader, model, device)\n    targets = np.array(targets) >= 0.5\n    metric = metrics.roc_auc_score(targets, outputs)\n    print(f\"AUC Score = {metric}\")\n    if metric > best_metric:\n        torch.save(model.state_dict(), 'model.bin')\n        best_metric = metric","64bfbf24":"df_test =  pd.read_csv(test_file)\ndf_test.head(3)","b960441e":"df_test =  df_test.drop([\"id\",\"keyword\",\"location\"] , axis=1)","009ba673":"df_test[\"text\"] =  df_test[\"text\"].apply(preprocessing)","3d1e337c":"df_test.head(3)","3f17b6e8":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")","27e06286":"model = TweetModel(1)\nmodel.load_state_dict(torch.load('.\/model.bin'))\nmodel = model.to(device)","159365c9":"sortie_predic = []","7926a9c2":"def get_predict(text):\n    \n \n    encoded_review = tokenizer(\n\n                  text,\n                  max_length=100,\n                  add_special_tokens=True,\n                  return_token_type_ids=True,\n                  pad_to_max_length=True,\n                  return_attention_mask=True,\n                  return_tensors='pt')\n\n    input_ids = encoded_review['input_ids'].to(device)\n    token_type_ids = encoded_review['token_type_ids'].to(device)\n    attention_mask = encoded_review['attention_mask'].to(device)\n    output = model(input_ids ,attention_mask , token_type_ids)\n    \n    for i in output:\n        if torch.sigmoid(i).item() > 0.5:\n            sortie_predic.append(1)\n        else:\n            sortie_predic.append(0)\n    \n    return sortie_predic\n    \n  ","f9559131":"for text in df_test.text :\n    get_predict(text)\n    \n    ","b8e01263":"sub = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\nsub.head(3)","a8fbb41c":"sub.target=sortie_predic","bfeb492a":"sub.to_csv(\"submission6.csv\" , index=False)","b3383e98":"### The DataLoader ","f60e74a6":"### We remove all special character from the text\n","eeb34066":"### Check the data look like","aa93fabc":"### The Prediction and Submission ","17ee1e38":"### Create the Database"}}