{"cell_type":{"5f5a826e":"code","b5916b04":"code","ae43d41e":"code","24707f36":"code","fb811f96":"code","641f7971":"code","3fd6e882":"code","29821c0b":"code","40714a6a":"code","e17a3838":"code","951ae59d":"code","bdc973cf":"code","474b73b4":"code","4dfa689d":"code","14e2fa63":"code","1bf01675":"code","bc764406":"code","9253e6f2":"code","678c15fd":"code","e616fdd2":"code","14072b61":"code","cde9383e":"code","e7ac9e03":"code","17a5c122":"code","3b5254dc":"code","373aec51":"code","b9ab4d9c":"code","e406dc5e":"code","0a3e7bd8":"markdown","704fb3d6":"markdown","00050209":"markdown","46fd591d":"markdown","a3159e6b":"markdown","69f75c01":"markdown","efb5db54":"markdown","1b342a08":"markdown","ad91c20a":"markdown","099de47c":"markdown","c821cea4":"markdown","358f8789":"markdown","283c5b26":"markdown","90eda855":"markdown","e8a0b016":"markdown","2ca07ddc":"markdown","2c93d8cc":"markdown","820ffddb":"markdown","91743520":"markdown","da0a8c36":"markdown","26561e64":"markdown","d07d23a3":"markdown","f5935647":"markdown","e30b4814":"markdown","d4e49ff9":"markdown","8ebb56ab":"markdown"},"source":{"5f5a826e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))","b5916b04":"df = pd.read_csv(\"..\/input\/heart.csv\")\ndf.head()","ae43d41e":"df.columns","24707f36":"# CHEST PAIN TYPE\n\ncp = df[\"cp\"]\nplt.hist(cp)\nplt.title(\"Distribution of Chest Pain Type\")\nplt.xlabel(\"Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.show()","fb811f96":"cp1 = df[df[\"target\"] == 1][\"cp\"]\ncp0 = df[df[\"target\"] == 0][\"cp\"]\n\nbar_width = 0.25\n\nplt.hist(cp1 - bar_width, color = \"red\", rwidth=1, label=\"Disease\")\nplt.hist(cp0, color=\"blue\", rwidth=1, label=\"No Disease\")\nplt.title(\"Distribution of Chest Pain Type\")\nplt.xlabel(\"Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.xticks([])\nplt.legend()\nplt.show()","641f7971":"cp_probs = {}\nfor i in set(df[\"cp\"]):\n    total = df[df[\"cp\"] == i]\n    cp_probs[i] = sum(total[\"target\"] == 1) \/ len(total), sum(total[\"target\"] == 0) \/ len(total)\nkeys = np.array(list(cp_probs.keys()))\nvalues= np.array([np.array(w) for w in cp_probs.values()])\nwidth = 0.25\nplt.bar(keys, values[:, 0], width, label=\"Disease\", color=\"red\")\nplt.bar(keys, values[:, 1], width, bottom=values[:, 0], label=\"No Disease\", color=\"blue\")\nplt.title(\"Probability Distribution of Disease with Chest Pain Type\")\nplt.xlabel(\"Pain Type\")\nplt.ylabel(\"Probability\")\nplt.xticks(keys, [1, 2, 3, 4])\nplt.legend()\nplt.show()","3fd6e882":"\nfrom sklearn.model_selection import train_test_split\n\n#Getting all the feature columns\nfeatures = list(df.columns)\nfeatures.remove(\"target\")\n\nX_train, X_test, y_train, y_test = train_test_split(df[\"cp\"], df[\"target\"])","29821c0b":"from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\n\nd_clf = DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train)\npredicted = d_clf.predict(X_test)\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\nd_fpr, d_tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(d_fpr, d_tpr)\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","40714a6a":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression().fit(np.array(X_train).reshape(-1, 1), y_train)\npredicted = clf.predict(np.array(X_test).reshape(-1, 1))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\nfpr, tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(fpr, tpr, label=\"Logistic Classifier\")\nplt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","e17a3838":"ag1 = df[df[\"target\"] == 1][\"age\"]\nag0 = df[df[\"target\"] == 0][\"age\"]\n\nbar_width = 0.25\n\nplt.hist(ag1 - bar_width, color = \"red\", bins=25, rwidth=1, label=\"Disease\")\nplt.hist(ag0, color=\"blue\", rwidth=1, bins=50, label=\"No Disease\")\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()","951ae59d":"ag_probs = {}\nfor i in set(df[\"age\"]):\n    total = df[df[\"age\"] == i]\n    ag_probs[i] = sum(total[\"target\"] == 1) \/ len(total), sum(total[\"target\"] == 0) \/ len(total)\nkeys = np.array(list(ag_probs.keys()))\nvalues= np.array([np.array(w) for w in ag_probs.values()])\n# width = 0.25\nplt.bar(keys, values[:, 0], label=\"Disease\", color=\"red\")\nplt.bar(keys, values[:, 1], bottom=values[:, 0], label=\"No Disease\", color=\"blue\")\nplt.title(\"Probability Distribution of Disease with Age\")\nplt.xlabel(\"Pain Type\")\nplt.ylabel(\"Probability\")\nplt.legend()\nplt.plot()","bdc973cf":"X_train, X_test, y_train, y_test = train_test_split(df[\"age\"], df[\"target\"])\n\nclf = LogisticRegression().fit(np.array(X_train).reshape(-1, 1), y_train)\npredicted = clf.predict(np.array(X_test).reshape(-1, 1))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\nfpr, tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(fpr, tpr, label=\"Logistic Classifier\")\nplt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","474b73b4":"ch1 = df[df[\"target\"] == 1][\"chol\"]\nch0 = df[df[\"target\"] == 0][\"chol\"]\n\nbar_width = 0.25\n\nplt.hist(ch1, color = \"red\", bins=25, rwidth=1, label=\"Disease\")\nplt.hist(ch0, color=\"blue\", rwidth=1, bins=50, label=\"No Disease\")\nplt.title(\"Distribution of Cholesterol\")\nplt.xlabel(\"Cholesterol\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()","4dfa689d":"X_train, X_test, y_train, y_test = train_test_split(df[\"chol\"], df[\"target\"])\n\nclf = LogisticRegression().fit(np.array(X_train).reshape(-1, 1), y_train)\npredicted = clf.predict(np.array(X_test).reshape(-1, 1))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\nfpr, tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(fpr, tpr, label=\"Logistic Classifier\")\nplt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","14e2fa63":"bp1 = df[df[\"target\"] == 1][\"trestbps\"]\nbp0 = df[df[\"target\"] == 0][\"trestbps\"]\n\nbar_width = 0.25\n\nplt.hist(bp1, color = \"red\", bins=25, rwidth=1, label=\"Disease\")\nplt.hist(bp0, color=\"blue\", rwidth=1, bins=50, label=\"No Disease\")\nplt.title(\"Distribution of Blood Pressure with Disease\")\nplt.xlabel(\"Blood Pressure\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()","1bf01675":"X_train, X_test, y_train, y_test = train_test_split(df[\"trestbps\"], df[\"target\"])\n\nclf = LogisticRegression().fit(np.array(X_train).reshape(-1, 1), y_train)\npredicted = clf.predict(np.array(X_test).reshape(-1, 1))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\nfpr, tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(fpr, tpr, label=\"Logistic Classifier\")\nplt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","bc764406":"name_dict = {\"restecg\":\"resting electrocardiographic results (values 0,1,2)\", \"thalach\":\"maximum heart rate achieved\",\"exang\":\"exercise induced angina\", \"oldpeak\" :\"ST depression induced by exercise relative to rest\", \"slope\":\"the slope of the peak exercise ST segment\"}\n\nfor feat in ['restecg', 'thalach', 'exang', 'oldpeak', 'slope']:\n    feat1 = df[df[\"target\"] == 1][feat]\n    feat0 = df[df[\"target\"] == 0][feat]\n\n    bar_width = 0.25\n    if feat in ['restecg', 'exang', 'slope']:\n        plt.hist(feat1 - bar_width, color = \"red\", rwidth=1, label=\"Disease\")\n        plt.hist(feat0, color=\"blue\", rwidth=1, label=\"No Disease\")\n    else:\n        plt.hist(feat1 - bar_width, color = \"red\", bins=50, rwidth=1, label=\"Disease\")\n        plt.hist(feat0, color=\"blue\", rwidth=1, bins=50, label=\"No Disease\")        \n    plt.title(\"Distribution of {} with Disease\".format(name_dict[feat]))\n    plt.xlabel(name_dict[feat])\n    plt.ylabel(\"Frequency\")\n    plt.legend()\n    plt.show()","9253e6f2":"for feat in ['restecg', 'exang', 'slope']:\n    feat_probs = {}\n    for i in set(df[feat]):\n        total = df[df[feat] == i]\n        feat_probs[i] = sum(total[\"target\"] == 1) \/ len(total), sum(total[\"target\"] == 0) \/ len(total)\n    keys = np.array(list(feat_probs.keys()))\n    values= np.array([np.array(w) for w in feat_probs.values()])\n    # width = 0.25\n    plt.bar(keys, values[:, 0], label=\"Disease\", color=\"red\")\n    plt.bar(keys, values[:, 1], bottom=values[:, 0], label=\"No Disease\", color=\"blue\")\n    plt.title(\"Probability Distribution of Disease with {}\".format(name_dict[feat]))\n    plt.xlabel(name_dict[feat])\n    plt.ylabel(\"Probability\")\n    plt.legend()\n    plt.show()","678c15fd":"df.dtypes","e616fdd2":"df.head(2)","14072b61":"df = df.astype({\"age\": \"int64\", \"sex\": \"int64\", \"cp\": \"object\", \"trestbps\": \"int64\", \"chol\": \"int64\", \"fbs\": \"int64\", \"restecg\": \"object\", \"thalach\": \"int64\", \"exang\": \"int64\", \"oldpeak\": \"float64\", \"slope\": \"object\", \"ca\": \"int\", \"thal\": \"object\", \"target\": \"int64\"})     \ndf.head()","cde9383e":"df.dtypes","e7ac9e03":"df = pd.get_dummies(df)\ndf.head()","17a5c122":"df.dtypes","3b5254dc":"X = df.loc[:, ['age', 'sex', 'trestbps', 'chol', 'fbs', 'thalach', 'exang', 'oldpeak',\n       'ca', 'cp_0', 'cp_1', 'cp_2', 'cp_3', 'restecg_0',\n       'restecg_1', 'restecg_2', 'slope_0', 'slope_1', 'slope_2', 'thal_0',\n       'thal_1', 'thal_2', 'thal_3']]\ny = df[\"target\"]","373aec51":"X.head(2)","b9ab4d9c":"X_train, X_test, y_train, y_test = train_test_split(X, y)\n\nclf = LogisticRegression().fit(np.array(X_train), y_train)\n\npredicted = clf.predict(np.array(X_test))\n\nprint(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\nprint(\"Recall: {}\".format(recall_score(y_test, predicted)))\nprint(\"Precision: {}\".format(precision_score(y_test, predicted)))\n# print(\"AUC: {}\".format(auc(y_test, predicted)))\n\nfpr, tpr, d_thresholds = roc_curve(y_test, predicted)\nplt.plot(fpr, tpr, label=\"Logistic Classifier\")\nplt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\nprint(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))","e406dc5e":"from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nclf_names = {SVC: \"SVM Classifier\", DecisionTreeClassifier: \"Tree\", RandomForestClassifier: \"Random Forest\"}\nfor classifier in [SVC, DecisionTreeClassifier, RandomForestClassifier]:\n    clf = classifier().fit(np.array(X_train), y_train)\n\n    predicted = clf.predict(np.array(X_test))\n    print(\"Classifier: {}\".format(clf_names[classifier]))\n    print(\"Accuracy: {}\".format(accuracy_score(y_test, predicted)))\n    print(\"Recall: {}\".format(recall_score(y_test, predicted)))\n    print(\"Precision: {}\".format(precision_score(y_test, predicted)))\n    # print(\"AUC: {}\".format(auc(y_test, predicted)))\n    print(\"AUC Score: {}\".format(roc_auc_score(y_test, predicted)))\n    fpr, tpr, d_thresholds = roc_curve(y_test, predicted)\n    plt.plot(fpr, tpr, label=clf_names[classifier])\n    plt.plot(d_fpr, d_tpr, color=\"green\", linestyle=\"--\", label=\"Dummy Classifier\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(\"ROC Curve\")\n    plt.legend()\n    plt.show()\n","0a3e7bd8":"### Dummy Variables\nNow we will start working with dummy variables, which are used to make processing categorical variables easy. So if there is 4 categories of a variable, and we use dummy variables, 4 new variables will be created, with every variable having value one for the observation where it was observed, zero otherwise. So it is like one hot encoding categorical variables.","704fb3d6":"We can see that age is not a good indicator of heart disease. Although the probability of heart disease increasing with age, visually the difference does not look significant.\nComparitively, the last variable \"cp\" did a better job. \nLets see how a model trained only on age performs:","00050209":"# Exploration\nThe first part of the kernel will be simple exploration of the data and visual analysis\nLets see how we can go about it.","46fd591d":"## Reading the data and seeing the features available","a3159e6b":"Okay the logistic regression did a good job, getting the accuracy, precision and recall all above 80%, which is good. Now we will explore other types of models now","69f75c01":"Visually it doesnt seem like cholesterol is a very good indicator of heart disease. Lets see how it performs as a feature.","efb5db54":"So majority of the pain types are 0.\nNow we should see the distribution of the target with this variable","1b342a08":"### Blood Pressure","ad91c20a":"## Classifier\nNow we will get into the most interesting part, making a good classifier. We will explore different types of algorithms for the purpose. Lets begin, but first, we will need to do some preprocessing, i.e. make dummy variables.","099de47c":"# Introduction\nIn this kernel, I will be trying to analyze the HCI data and try to construct a good classifier which can predict heart disease.\nIf youre new, you will learn the following from this kernel:\n1. Simple Matplotlib data visualization\n2. Making a baseline dummy classfier\n3. Constructing and training classifiers of different types.\n4. Converting categorical variables to dummy variables\n5. Comparing classifiers (using ROC curves and other means).","c821cea4":"As expected, cholesterol only can not significantly predict heart disease","358f8789":"### Now we will train a logistic regression classifier using only the cp feature and then compare it to the baseline model","283c5b26":"### Train Test Split\nHere we will train a simple Logistic Regression Classification model to predict disease using Chest Pain (\"cp\") feature\nFirst we need to split the data into training and testing parts so that we infer model performance when given unseen data.","90eda855":"## Stress Tests:\nFeatures 7 - 11 ('restecg', 'thalach', 'exang', 'oldpeak', 'slope') are results of heart under stress test\nThe full forms are: \n1. restecg - resting electrocardiographic results (values 0,1,2)\n2. thalach - maximum heart rate achieved \n3. exang - exercise induced angina \n4. oldpeak - ST depression induced by exercise relative to rest \n5. slope - the slope of the peak exercise ST segment \n\nThese features are believed to be very good indicators:\nLets see more:","e8a0b016":"### Explanation of the feature names\nThe features serially are:\n1. age \n2. sex \n3. chest pain type (4 values) \n4. resting blood pressure \n5. serum cholestoral in mg\/dl \n6. fasting blood sugar > 120 mg\/dl\n7. resting electrocardiographic results (values 0,1,2)\n8. maximum heart rate achieved \n9. exercise induced angina \n10. oldpeak = ST depression induced by exercise relative to rest \n11. the slope of the peak exercise ST segment \n12. number of major vessels (0-3) colored by flourosopy \n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n\nFrom the above description we can see that most of the features have continuous values, but a few (chest pain type, resting electrocardiogram, number of major vessels and thal) are discrete type. We need to evaluate these the continuous and the discrete ones differently.","2ca07ddc":"## Baseline models\nIn this section, we will train a baseline model to compare our more feature rich models. The baseline models would just predict the most frquent class for all observations. Lets have a go.","2c93d8cc":"## Discerete Data Exploration\nFirst we will explore the discrete features, mainly visually and see if we can find any insights.","820ffddb":"Again we see that the resting blood pressure is not a very good indicator of heart disease","91743520":"## Cholesterol\nLets have a look at cholesterol feature, which has been reported to be one of the best indicators of heart disease","da0a8c36":"We can see that the model slightly outperforms the baseline model. The scores also point out the same.","26561e64":"### Chest Pain Type","d07d23a3":"## Exploring Age\nIt is commonly known that the risk of heart disease increases with age. So, we will explore the age feature a little now","f5935647":"#### Inference:\nThe above graph is very informative. We get a very good sense of the probability of having disease goes up if the chest pain is of any type other than 0.\nIf we train a classifier based on only this property, it will give decent results. Lets try and do that.","e30b4814":"So it is evident that the chance of having the disease increases as the pain type number increases. Lets have a look at the probability distribution of the same","d4e49ff9":"### Result:\nThe best classifier seems to be the simple logistic regression and the slightly more complex random forest classifier. With that I will end this notebook. Please feel free to comment in case of any queries. Thank You...","8ebb56ab":"### Improvement\nThe above simple models performs pretty well with accuracy, precision and recall scores approaching 80%.\nLets try and explore other features now."}}