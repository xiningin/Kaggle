{"cell_type":{"7194da17":"code","79dfb743":"code","1c77067f":"code","2031a2ff":"code","13bf6504":"code","492815f2":"code","cfebf6a0":"code","b50d4137":"code","4ac0ad1c":"code","82edf053":"code","da4bf076":"code","0d2717a0":"code","b5ff78ad":"code","23363c56":"code","b88bdac5":"code","1da28202":"code","49560599":"code","7d509e7e":"code","da0d7d5f":"code","07833069":"code","d9bc02a0":"code","60628f66":"markdown","b87f8875":"markdown","36ed5c68":"markdown","5f4b7933":"markdown","270f1068":"markdown","41187f00":"markdown","d6a26c39":"markdown","262013f5":"markdown","28435ab8":"markdown"},"source":{"7194da17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","79dfb743":"import numpy as np\nnp.random.seed(1337)  # for reproducibility","1c77067f":"def load_data(path, num_words=None, skip_top=0, seed=1337):\n    with np.load(path) as f:\n        x_train, labels_train = f['x_train'], f['y_train']\n        x_test, labels_test = f['x_test'], f['y_test']\n\n    np.random.seed(seed)\n    \n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n    \n    indices = np.arange(len(x_test))\n    np.random.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n    \n    xs = np.concatenate([x_train, x_test])\n    labels = np.concatenate([labels_train, labels_test])\n    \n    if not num_words:\n        num_words = max([max(x) for x in xs])\n\n    xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n    \n    idx = len(x_train)\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n    \n    return (x_train, y_train), (x_test, y_test)","2031a2ff":"from keras.preprocessing import sequence\ndef get_data(maxlen = 80, max_features = 20000):\n    #print('Loading data...')\n    (X_train, y_train), (X_test, y_test) = load_data('..\/input\/keras-imdb-reviews\/imdb.npz',\n                                                     num_words=max_features)\n    #print(len(X_train), 'train sequences')\n    #print(len(X_test), 'test sequences')\n    \n    #print('Pad sequences (samples x time)')\n    X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n    X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n    #print('X_train shape:', X_train.shape)\n    #print('X_test shape:', X_test.shape)\n    return (X_train, y_train), (X_test, y_test)","13bf6504":"max_features = 20000\nprint('Loading word index...')\nimport json\nwith open('..\/input\/keras-imdb-reviews\/imdb_word_index.json') as f:\n    word_index = json.load(f)\n    \nprint('Loading embeddings...')\nembeddings_index = dict()\nf = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt')\n\nfor line in f:\n    # Note: use split(' ') instead of split() if you get an error.\n    values = line.split(' ')\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Loaded %s word vectors.' % len(embeddings_index))\n\n# create a weight matrix\nembedding_matrix = np.zeros((max_features, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        if i < max_features:\n            embedding_matrix[i] = embedding_vector\n            \nprint('Done loading embeddings')","492815f2":"import numpy as np\n\nfrom keras import backend as K\nfrom keras import activations, initializers, regularizers, constraints\nfrom keras.layers import Layer, InputSpec\n\nfrom keras.utils.conv_utils import conv_output_length\n\n#import theano\n#import theano.tensor as T\n\n\ndef _dropout(x, level, noise_shape=None, seed=None):\n    x = K.dropout(x, level, noise_shape, seed)\n    x *= (1. - level) # compensate for the scaling by the dropout\n    return x\n\n\nclass QRNN(Layer):\n    '''Quasi RNN\n\n    # Arguments\n        units: dimension of the internal projections and the final output.\n\n    # References\n        - [Quasi-recurrent Neural Networks](http:\/\/arxiv.org\/abs\/1611.01576)\n    '''\n    def __init__(self, units, window_size=2, stride=1,\n                 return_sequences=False, go_backwards=False, \n                 stateful=False, unroll=False, activation='tanh',\n                 kernel_initializer='uniform', bias_initializer='zero',\n                 kernel_regularizer=None, bias_regularizer=None,\n                 activity_regularizer=None,kernel_constraint=None, bias_constraint=None, \n                 dropout=0, use_bias=True, input_dim=None, input_length=None,**kwargs):\n        # Setup\n        self.return_sequences = return_sequences\n        self.go_backwards = go_backwards\n        self.stateful = stateful\n        self.unroll = unroll\n\n        self.units = units \n        self.window_size = window_size\n        self.strides = (stride, 1)\n\n        self.use_bias = use_bias\n        self.activation = activations.get(activation)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.supports_masking = True\n        self.input_spec = [InputSpec(ndim=3)]\n        self.input_dim = input_dim\n        self.input_length = input_length\n        if self.input_dim:\n            kwargs['input_shape'] = (self.input_length, self.input_dim)\n        super(QRNN, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        batch_size = input_shape[0] if self.stateful else None #Stateful RNNs need to know BS\n        self.input_dim = input_shape[2]\n        self.input_spec = InputSpec(shape=(batch_size, None, self.input_dim))\n        self.state_spec = InputSpec(shape=(batch_size, self.units))\n\n        self.states = [None]\n        if self.stateful:\n            self.reset_states()\n\n        kernel_shape = (self.window_size, 1, self.input_dim, self.units * 3)\n        self.kernel = self.add_weight(name='kernel',\n                                      shape=kernel_shape,\n                                      initializer=self.kernel_initializer,\n                                      regularizer=self.kernel_regularizer,\n                                      constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(name='bias', \n                                        shape=(self.units * 3,),\n                                        initializer=self.bias_initializer,\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        if isinstance(input_shape, list):\n            input_shape = input_shape[0]\n\n        length = input_shape[1]\n        if length:\n            length = conv_output_length(length + self.window_size - 1,\n                                        self.window_size, 'valid',\n                                        self.strides[0])\n        if self.return_sequences:\n            return (input_shape[0], length, self.units)\n        else:\n            return (input_shape[0], self.units)\n\n    def compute_mask(self, inputs, mask):\n        if self.return_sequences:\n            return mask\n        else:\n            return None\n\n    def get_initial_states(self, inputs):\n        # build an all-zero tensor of shape (samples, units)\n        initial_state = K.zeros_like(inputs)  # (samples, timesteps, input_dim)\n        initial_state = K.sum(initial_state, axis=(1, 2))  # (samples,)\n        initial_state = K.expand_dims(initial_state)  # (samples, 1)\n        initial_state = K.tile(initial_state, [1, self.units])  # (samples, units)\n        initial_states = [initial_state for _ in range(len(self.states))]\n        return initial_states\n\n    def reset_states(self, states=None):\n        if not self.stateful:\n            raise AttributeError('Layer must be stateful.')\n        if not self.input_spec:\n            raise RuntimeError('Layer has never been called '\n                               'and thus has no states.')\n\n        batch_size = self.input_spec.shape[0]\n        if not batch_size:\n            raise ValueError('If a QRNN is stateful, it needs to know '\n                             'its batch size. Specify the batch size '\n                             'of your input tensors: \\n'\n                             '- If using a Sequential model, '\n                             'specify the batch size by passing '\n                             'a `batch_input_shape` '\n                             'argument to your first layer.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a '\n                             '`batch_shape` argument to your Input layer.')\n\n        if self.states[0] is None:\n            self.states = [K.zeros((batch_size, self.units))\n                           for _ in self.states]\n        elif states is None:\n            for state in self.states:\n                K.set_value(state, np.zeros((batch_size, self.units)))\n        else:\n            if not isinstance(states, (list, tuple)):\n                states = [states]\n            if len(states) != len(self.states):\n                raise ValueError('Layer ' + self.name + ' expects ' +\n                                 str(len(self.states)) + ' states, '\n                                 'but it received ' + str(len(states)) +\n                                 'state values. Input received: ' +\n                                 str(states))\n            for index, (value, state) in enumerate(zip(states, self.states)):\n                if value.shape != (batch_size, self.units):\n                    raise ValueError('State ' + str(index) +\n                                     ' is incompatible with layer ' +\n                                     self.name + ': expected shape=' +\n                                     str((batch_size, self.units)) +\n                                     ', found shape=' + str(value.shape))\n                K.set_value(state, value)\n\n    def __call__(self, inputs, initial_state=None, **kwargs):\n        # If `initial_state` is specified,\n        # and if it a Keras tensor,\n        # then add it to the inputs and temporarily\n        # modify the input spec to include the state.\n        if initial_state is not None:\n            if hasattr(initial_state, '_keras_history'):\n                # Compute the full input spec, including state\n                input_spec = self.input_spec\n                state_spec = self.state_spec\n                if not isinstance(state_spec, list):\n                    state_spec = [state_spec]\n                self.input_spec = [input_spec] + state_spec\n\n                # Compute the full inputs, including state\n                if not isinstance(initial_state, (list, tuple)):\n                    initial_state = [initial_state]\n                inputs = [inputs] + list(initial_state)\n\n                # Perform the call\n                output = super(QRNN, self).__call__(inputs, **kwargs)\n\n                # Restore original input spec\n                self.input_spec = input_spec\n                return output\n            else:\n                kwargs['initial_state'] = initial_state\n        return super(QRNN, self).__call__(inputs, **kwargs)\n\n    def call(self, inputs, mask=None, initial_state=None, training=None):\n        # input shape: `(samples, time (padded with zeros), input_dim)`\n        # note that the .build() method of subclasses MUST define\n        # self.input_spec and self.state_spec with complete input shapes.\n        if isinstance(inputs, list):\n            initial_states = inputs[1:]\n            inputs = inputs[0]\n        elif initial_state is not None:\n            pass\n        elif self.stateful:\n            initial_states = self.states\n        else:\n            initial_states = self.get_initial_states(inputs)\n\n        if len(initial_states) != len(self.states):\n            raise ValueError('Layer has ' + str(len(self.states)) +\n                             ' states but was passed ' +\n                             str(len(initial_states)) +\n                             ' initial states.')\n        input_shape = K.int_shape(inputs)\n        if self.unroll and input_shape[1] is None:\n            raise ValueError('Cannot unroll a RNN if the '\n                             'time dimension is undefined. \\n'\n                             '- If using a Sequential model, '\n                             'specify the time dimension by passing '\n                             'an `input_shape` or `batch_input_shape` '\n                             'argument to your first layer. If your '\n                             'first layer is an Embedding, you can '\n                             'also use the `input_length` argument.\\n'\n                             '- If using the functional API, specify '\n                             'the time dimension by passing a `shape` '\n                             'or `batch_shape` argument to your Input layer.')\n        constants = self.get_constants(inputs, training=None)\n        \n        preprocessed_input = self.preprocess_input(inputs, training=None) #Convolutional Step\n\n        last_output, outputs, states = K.rnn(self.step, preprocessed_input, #fo-gate step\n                                            initial_states,\n                                            go_backwards=self.go_backwards,\n                                            mask=mask,\n                                            constants=constants,\n                                            unroll=self.unroll,\n                                            input_length=input_shape[1])\n        if self.stateful:\n            updates = []\n            for i in range(len(states)):\n                updates.append((self.states[i], states[i]))\n            self.add_update(updates, inputs)\n\n        if self.return_sequences:\n            return outputs\n        else:\n            return last_output\n\n    def preprocess_input(self, inputs, training=None):\n        if self.window_size > 1: # Pad\n            inputs = K.temporal_padding(inputs, (self.window_size-1, 0))\n        inputs = K.expand_dims(inputs, 2)  # add a dummy dimension\n\n        output = K.conv2d(inputs, self.kernel, strides=self.strides, # Conv Step\n                          padding='valid',\n                          data_format='channels_last')\n        output = K.squeeze(output, 2)  # remove the dummy dimension\n        \n        if self.use_bias: #Add bias\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        \n        return output\n\n    def step(self, inputs, states): #Fo-pool implementation\n        prev_output = states[0]\n\n        z = inputs[:, :self.units]\n        f = inputs[:, self.units:2 * self.units]\n        o = inputs[:, 2 * self.units:]\n\n        z = self.activation(z)\n        f = K.sigmoid(f) \n        o = K.sigmoid(o)\n\n        output = f * prev_output + (1 - f) * z\n        output = o * output\n\n        return output, [output]\n\n    def get_constants(self, inputs, training=None):\n        return []\n \n    def get_config(self):\n        config = {'units': self.units,\n                  'window_size': self.window_size,\n                  'stride': self.strides[0],\n                  'return_sequences': self.return_sequences,\n                  'go_backwards': self.go_backwards,\n                  'stateful': self.stateful,\n                  'unroll': self.unroll,\n                  'use_bias': self.use_bias,\n                  'activation': activations.serialize(self.activation),\n                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n                  'bias_initializer': initializers.serialize(self.bias_initializer),\n                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n                  'bias_constraint': constraints.serialize(self.bias_constraint),\n                  'input_dim': self.input_dim,\n                  'input_length': self.input_length}\n        base_config = super(QRNN, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","cfebf6a0":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Embedding, Dropout\nfrom keras.layers import CuDNNLSTM, LSTM\nfrom keras.regularizers import l2\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import RMSprop","b50d4137":"def build_QRNN():\n    #print('Build model QRNN')\n    model = Sequential()\n    model.add(Embedding(max_features, 300,weights=[embedding_matrix],trainable=False))\n    model.add(Dropout(0.3))\n\n    model.add(QRNN(256, window_size=3, dropout=0,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(QRNN(256, window_size=3, dropout=0,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(QRNN(256, window_size=3, dropout=0,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(QRNN(256, window_size=3, dropout=0,return_sequences=False, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    # try using different optimizers and different optimizer configs\n    model.compile(loss='binary_crossentropy',\n                  optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-8),\n                  metrics=['accuracy'])\n    \n    return model","4ac0ad1c":"def build_LSTM():\n    \n    model = Sequential()\n    model.add(Embedding(max_features, 300,weights=[embedding_matrix],trainable=False))\n    model.add(Dropout(0.3))\n\n    model.add(LSTM(256,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(LSTM(256,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(LSTM(256,return_sequences=True, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(LSTM(256 ,return_sequences=False, \n                   kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                   kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-8),\n                  metrics=['accuracy'])\n    \n    return model","82edf053":"import time\nfrom sklearn.utils import resample\ndef test_model(model, maxlen, batch_size,n_samples = 10000):\n    print('Testing with Seq Len: {}, Batch Size: {}'.format(maxlen,batch_size))\n    \n    (X_train, y_train), (X_test, y_test) = get_data(maxlen=maxlen)\n    X_train, y_train, X_test, y_test = resample(X_train, y_train, X_test, y_test,n_samples=n_samples,random_state = 1337)\n    print('Dry Run')\n    model.fit(X_train, y_train, \n              batch_size=batch_size, \n              epochs=1)\n    \n    print('On Time')\n    start = time.time()\n    model.fit(X_train, y_train, \n              batch_size=batch_size, \n              epochs=1)\n    finish = time.time()\n    total = finish-start\n    print('Evaluating')\n    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n    return total, score, acc","da4bf076":"model = build_QRNN()\nqrnn_512_8_total, score, qrnn_512_8_acc = test_model(model,maxlen=512,batch_size=8, n_samples=1000)\n\nprint('QRNN Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(qrnn_512_8_total,qrnn_512_8_acc))","0d2717a0":"model = build_QRNN()\nqrnn_32_256_total, score, qrnn_32_256_acc = test_model(model,maxlen=32,batch_size=256, n_samples=1000)\n\nprint('QRNN Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(qrnn_32_256_total,qrnn_32_256_acc))","b5ff78ad":"model = build_LSTM()\nlstm_512_8_total, score, lstm_512_8_acc = test_model(model,maxlen=512,batch_size=8,n_samples=1000)\n\nprint('LSTM Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(lstm_512_8_total,lstm_512_8_acc))","23363c56":"model = build_LSTM()\nlstm_32_256_total, score, lstm_32_256_acc = test_model(model,maxlen=32,batch_size=256,n_samples=1000)\n\nprint('LSTM Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(lstm_32_256_total,lstm_32_256_acc))","b88bdac5":"print('Speedup with Seq Len 512 and BS 8: {:.2f}'.format(lstm_512_8_total \/ qrnn_512_8_total))","1da28202":"print('Speedup with Seq Len 32 and BS 256: {:.2f}'.format(lstm_32_256_total \/ qrnn_32_256_total))","49560599":"def build_CuDNNLSTM():\n    \n    model = Sequential()\n    model.add(Embedding(max_features, 300,weights=[embedding_matrix],trainable=False))\n    model.add(Dropout(0.3))\n\n    model.add(CuDNNLSTM(256,return_sequences=True, \n                       kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                       kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(CuDNNLSTM(256,return_sequences=True, \n                       kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                       kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(CuDNNLSTM(256,return_sequences=True, \n                       kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                       kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dropout(0.3))\n\n    model.add(CuDNNLSTM(256 ,return_sequences=False, \n                       kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6),\n                       kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10)))\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=RMSprop(lr=0.001, rho=0.9, epsilon=1e-8),\n                  metrics=['accuracy'])\n    \n    return model","7d509e7e":"model = build_CuDNNLSTM()\nculstm_512_8_total, score, culstm_512_8_acc = test_model(model,maxlen=512,batch_size=8,n_samples=1000)\n\nprint('CuDNNLSTM Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(culstm_512_8_total,culstm_512_8_acc))","da0d7d5f":"model = build_CuDNNLSTM()\nculstm_32_256_total, score, culstm_32_256_acc = test_model(model,maxlen=32,batch_size=256,n_samples=1000)\n\nprint('CuDNNLSTM Run Time Per Epoch: {:.4f}, Accuracy after two epochs: {:.4f}'.format(culstm_32_256_total,culstm_32_256_acc))","07833069":"print('Speedup with Seq Len 512 and BS 8: {:.2f}'.format(culstm_512_8_total \/ qrnn_512_8_total))","d9bc02a0":"print('Speedup with Seq Len 32 and BS 256: {:.2f}'.format(culstm_32_256_total \/ qrnn_32_256_total))","60628f66":"![QRNN](https:\/\/preview.ibb.co\/gtukuJ\/Screen_Shot_2018_07_16_at_16_14_03.png)","b87f8875":"# QRNN implementation\nThe cell below implements the QRNN. ","36ed5c68":"Firstly, we are interested if we see anything like the 17x speedup from the paper on long sequences with small batch sizes. Note that we use the hyperparameters proposed by the authors for the QRNN as well as the LSTM, so take the accuracy comparisons with a bit of salt.","5f4b7933":"The QRNN seems to be faster indeed. The speedup factor lies between 2.5x and 2x. Not quite the 17x we hoped for, but still pretty good. QRNNs accuracy scores are slightly below those of the LSTM but the difference is small and QRNN might catch up if we train for longer or used more data. That said, Keras `LSTM` layer is not the best way to use LSTMs in Keras. The `CuDNNLSTM` is a highly optimized LSTM layer that only works on CUDA enabled GPUs. Lucky that Kernels come with a Tesla-K80!","270f1068":"# Getting Data\n\nWe use the IMDB movie review dataset, with 25,000 training and 25,000 test examples.","41187f00":"The optimized `CuDNNLSTM` beats the QRNN, no wonder, it is optimized for the hardware we are running this test on.\n\n## So should I only use QRNNs now?\nQRNNs work well on long sequences. So, if your task requires working with a very long sequence, they might be a tool to consider. Yet, QRNNs do not perform better on short sequences. If you can make use of a highly optimized LSTM implementation, you should stick with the LSTM.","d6a26c39":"# Experiments\nFrom the paper: \n> Our best performance on a held-out development set was achieved using a four-layer denselyconnected\nQRNN with 256 units per layer and word vectors initialized using 300-dimensional cased\nGloVe embeddings (Pennington et al., 2014). Dropout of 0.3 was applied between layers, and we\nused L2 regularization of 4 \u00d7 10\u22126. Optimization was performed on minibatches of 24 examples\nusing RMSprop (Tieleman & Hinton, 2012) with learning rate of 0.001, alpha = 0.9, and epsilon = 10\u22128.\n\nWe will use these hyperparameters in our experiments as well, for both QRNN and LSTM networks. This skews the accuracy metrics in favor of QRNNs, but gives us a better idea of how fast each layer is. To test the training speed of the model, we let the model train for two epochs with 1000 samples. We only measure the time of the second epoch, as Keras builds the computational graph lazily and the first epoch usually takes a little bit longer.","262013f5":"# Benchmarking QRNNs in Sentiment Analysis\nIn this Kernel we will benchmark Quasi Recurrent Neural Networks against LSTM's on the IMDB sentiment classification task. This Kernel uses a Keras implementation by Ding Ke. If you enjoy this work, please also visit his GitHub and consider citing the QRNN paper in your research:\n\nOriginal QRNN Paper: [Bradbury et al. 2016, Quasi-Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1611.01576)\n\nOriginal QRNN Implementation: [Ding Ke's QRNN Implementation](https:\/\/github.com\/DingKe\/nn_playground\/tree\/master\/qrnn)\n\n## What is a QRNN and why should I care\n**tl;dr**: QRNNs replace the sequential computation of an LSTM with parallelizable computations based on a convolutional architecture. This makes them faster, especially on long sequences.\n\nIf you work with sequence data like text, your favorite neural network layer is probably the [Long Short Term Memory](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/) (LSTM) Layer. LSTMs are a form of recurrent neural networks. Recurrent neural networks use their output from a previous sequence element as an input to the next sequence element. This allows them to use the temporal order of words as features. In a sentences like:  \"I do recommend this move, not bad\" and \"I do not recommend this bad movie\", the order of words makes all the difference. \n\nThe downside of recurrent neural networks is that they need to compute the output for one sequence element before they can move on to the next one. LSTMs can only process text sequentially, not in parallel. LSTMs can therefore not access the speedups from parallel computing on a GPU as much as other networks can. On long sequences LSTMs can be quite slow.\n\nAn alternative to the LSTM are [Convolutional Neural Networks](http:\/\/cs231n.github.io\/convolutional-networks\/) (CNNs). CNNs work like filters that get slided over a sequence. They can make use of small, local features but a CNN by itself struggles to make use of larger structures in sequences. An important ingedient in a CNN is a [pooling layer](http:\/\/cs231n.github.io\/convolutional-networks\/#pool). Pooling layers shorten a sequence by just keeping the maximum (Max-Pool) or average (Average-Pool) activation of the previous layer. By then running a CNN layer over the shortened sequence, the CNN can make use of larger structures in sequences like text. Yet, CNNs often underperform compared to LSTMs on text. They simply lack the means to make use of sequence features in which one thing clearly comes after the other.\n\nQRNNs are an attempt to marry the best of both worlds. They combine a convolutional layer with a \"dynamic average pool layer\", see [Balduzzi & Ghifary 2016 Strongly-Typed Recurrent Neural Networks](https:\/\/arxiv.org\/abs\/1602.02218) for details. This pooling layer effectively computes a weighted average of features, where the weighting is dependent on features that come earlier in the sequence. This way, the QRNN can make use of the sequentiality of the data. The averaging process leads to a learned memory, similar to a 'forget gate', which is why the authors call their pooling layer a forget-pool or fo-pool.\n\n\n","28435ab8":"In their experiments section, the authors find speedups QRNNs up to 16.9x faster than LSTMs for tasks with long sequences and a small batch size, while archieving similar accuracy.\n![QRNN Experiements Results](https:\/\/preview.ibb.co\/fCPqTd\/Screen_Shot_2018_07_16_at_17_04_44.png)\n\n[Salesforce Research published a pytorch implementation](https:\/\/github.com\/salesforce\/pytorch-qrnn) and seems to find the same speedups in their experiments. But, as diligent scientists, we should run our own benchmarks, against LSTMs as well as other architectures!\n\nIn the Kernel below you will find the details of the implementation we use for benchmarking. For results, scroll to the bottom."}}