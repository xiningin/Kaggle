{"cell_type":{"bb5b20f2":"code","5ab2dafc":"code","78fc4899":"code","6bad808e":"code","33d155fa":"code","9e1e0d45":"code","30c8a9bd":"code","3cae0e15":"code","11a12efb":"code","11fd3f8c":"code","fa55c490":"code","4b5b36e3":"code","e32a0be7":"code","47030afe":"code","2feb0cc4":"code","ceab896c":"code","1e04959c":"code","1e9043c6":"code","99e0e590":"code","b016a6a7":"code","0c84eb40":"code","d5b7500f":"code","7ea9ce68":"code","259a0774":"code","79059256":"code","000f6e16":"code","a1ecaf64":"code","67acabbd":"markdown","989e9be9":"markdown","2eafef7f":"markdown","b95ddc57":"markdown","a068e4af":"markdown","77f444e0":"markdown","36f3f2f2":"markdown","2e7c5b70":"markdown","af0433c4":"markdown","2d293f64":"markdown","61a9e496":"markdown","829303bd":"markdown","b5cb3819":"markdown"},"source":{"bb5b20f2":"# Basic Libraries\n\nimport pandas as pd\nimport numpy as np\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.preprocessing import MinMaxScaler","5ab2dafc":"# Libraries for Classification and building Models\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D, Dropout\nfrom tensorflow.keras.utils import to_categorical \n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","78fc4899":"# Project Specific Libraries\n\nimport os\nimport librosa\nimport librosa.display\nimport glob \nimport skimage","6bad808e":"df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\n\n'''We will extract classes from this metadata.'''\n\ndf.head()","33d155fa":"dat1, sampling_rate1 = librosa.load('..\/input\/urbansound8k\/fold5\/100032-3-0-0.wav')\ndat2, sampling_rate2 = librosa.load('..\/input\/urbansound8k\/fold5\/100263-2-0-117.wav')","9e1e0d45":"plt.figure(figsize=(20, 10))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(dat1)), ref=np.max)\nplt.subplot(4, 2, 1)\nlibrosa.display.specshow(D, y_axis='linear')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Linear-frequency power spectrogram')","30c8a9bd":"plt.figure(figsize=(20, 10))\nD = librosa.amplitude_to_db(np.abs(librosa.stft(dat2)), ref=np.max)\nplt.subplot(4, 2, 1)\nlibrosa.display.specshow(D, y_axis='linear')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Linear-frequency power spectrogram')","3cae0e15":"'''Using random samples to observe difference in waveforms.'''\n\narr = np.array(df[\"slice_file_name\"])\nfold = np.array(df[\"fold\"])\ncla = np.array(df[\"class\"])\n\nfor i in range(192, 197, 2):\n    path = '..\/input\/urbansound8k\/fold' + str(fold[i]) + '\/' + arr[i]\n    data, sampling_rate = librosa.load(path)\n    plt.figure(figsize=(10, 5))\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(data)), ref=np.max)\n    plt.subplot(4, 2, 1)\n    librosa.display.specshow(D, y_axis='linear')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(cla[i])","11a12efb":"'''EXAMPLE'''\n\ndat1, sampling_rate1 = librosa.load('..\/input\/urbansound8k\/fold5\/100032-3-0-0.wav')\narr = librosa.feature.melspectrogram(y=dat1, sr=sampling_rate1)\narr.shape","11fd3f8c":"feature = []\nlabel = []\n\ndef parser(row):\n    # Function to load files and extract features\n    for i in range(8732):\n        file_name = '..\/input\/urbansound8k\/fold' + str(df[\"fold\"][i]) + '\/' + df[\"slice_file_name\"][i]\n        # Here kaiser_fast is a technique used for faster extraction\n        X, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n        # We extract mfcc feature from data\n        mels = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)        \n        feature.append(mels)\n        label.append(df[\"classID\"][i])\n    return [feature, label]","fa55c490":"temp = parser(df)","4b5b36e3":"temp = np.array(temp)\ndata = temp.transpose()","e32a0be7":"X_ = data[:, 0]\nY = data[:, 1]\nprint(X_.shape, Y.shape)\nX = np.empty([8732, 128])","47030afe":"for i in range(8732):\n    X[i] = (X_[i])","2feb0cc4":"Y = to_categorical(Y)","ceab896c":"'''Final Data'''\nprint(X.shape)\nprint(Y.shape)","1e04959c":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 1)","1e9043c6":"X_train = X_train.reshape(6549, 16, 8, 1)\nX_test = X_test.reshape(2183, 16, 8, 1)","99e0e590":"input_dim = (16, 8, 1)","b016a6a7":"model = Sequential()","0c84eb40":"model.add(Conv2D(64, (3, 3), padding = \"same\", activation = \"tanh\", input_shape = input_dim))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, (3, 3), padding = \"same\", activation = \"tanh\"))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.1))\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"tanh\"))\nmodel.add(Dense(10, activation = \"softmax\"))","d5b7500f":"model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","7ea9ce68":"model.fit(X_train, Y_train, epochs = 90, batch_size = 50, validation_data = (X_test, Y_test))","259a0774":"model.summary()","79059256":"predictions = model.predict(X_test)\nscore = model.evaluate(X_test, Y_test)\nprint(score)","000f6e16":"preds = np.argmax(predictions, axis = 1)","a1ecaf64":"result = pd.DataFrame(preds)\nresult.to_csv(\"UrbanSound8kResults.csv\")","67acabbd":"# Feature Extraction and Database Building","989e9be9":"# Creating Keras Model and Testing","2eafef7f":"This dataset contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. For a detailed description of the dataset and how it was compiled please refer to our paper.\nAll excerpts are taken from field recordings uploaded to www.freesound.org. The files are pre-sorted into ten folds (folders named fold1-fold10) to help in the reproduction of and comparison with the automatic classification results reported in the article above.\n\nIn addition to the sound excerpts, a CSV file containing metadata about each excerpt is also provided.","b95ddc57":"# Importing Necessary Libraries","a068e4af":"1. There are 3 basic methods to extract features from audio file :\n    a) Using the mffcs data of the audio files\n    b) Using a spectogram image of the audio and then converting the same to data points (As is done for images). This is easily done using mel_spectogram function of Librosa\n    c) Combining both features to build a better model. (Requires a lot of time to read and extract data).\n2. I have chosen to use the second method.\n3. The labels have been converted to categorical data for classification.\n4. CNN has been used as the primary layer to classify data","77f444e0":"# Intro","36f3f2f2":"# Analysing Data Type and Format","2e7c5b70":"### Methodology","af0433c4":"#### Using Librosa to analyse random sound sample - SPECTOGRAM","2d293f64":"#### Analysing CSV Data","61a9e496":"#### Method\n\n1. I have used Librosa to extract features.\n2. To do so, I will go through each fold and extract the data for each file. Then I have used the mel_spectogram function of librosa to extract the spectogram data as a numpy array.\n3. After reshaping and cleaning the data, 75-25 split has been performed.\n4. Classes (Y) have been converted to Categorically Encoded Data usng Keras.utils\n\nNote : Running the parser function may take upto 45 minutes depending on your system since it has to extract spectogram data for 8732 audio files","829303bd":"##### Column Names\n\n* slice_file_name: \nThe name of the audio file. The name takes the following format: [fsID]-[classID]-[occurrenceID]-[sliceID].wav, where:\n[fsID] = the Freesound ID of the recording from which this excerpt (slice) is taken\n[classID] = a numeric identifier of the sound class (see description of classID below for further details)\n[occurrenceID] = a numeric identifier to distinguish different occurrences of the sound within the original recording\n[sliceID] = a numeric identifier to distinguish different slices taken from the same occurrence\n\n* fsID:\nThe Freesound ID of the recording from which this excerpt (slice) is taken\n\n* start\nThe start time of the slice in the original Freesound recording\n\n* end:\nThe end time of slice in the original Freesound recording\n\n* salience:\nA (subjective) salience rating of the sound. 1 = foreground, 2 = background.\n\n* fold:\nThe fold number (1-10) to which this file has been allocated.\n\n* classID:\nA numeric identifier of the sound class:\n0 = air_conditioner\n1 = car_horn\n2 = children_playing\n3 = dog_bark\n4 = drilling\n5 = engine_idling\n6 = gun_shot\n7 = jackhammer\n8 = siren\n9 = street_music\n\n* class:\nThe class name: air_conditioner, car_horn, children_playing, dog_bark, drilling, engine_idling, gun_shot, jackhammer, \nsiren, street_music.","b5cb3819":"#### Model 1:\n\n1. CNN 2D with 64 units and tanh activation.\n2. MaxPool2D with 2*2 window.\n3. CNN 2D with 128 units and tanh activation.\n4. MaxPool2D with 2*2 window.\n5. Dropout Layer with 0.2 drop probability.\n6. DL with 1024 units and tanh activation.\n4. DL 10 units with softmax activation.\n5. Adam optimizer with categorical_crossentropy loss function.\n\n90 epochs have been used."}}