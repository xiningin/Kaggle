{"cell_type":{"6934c06b":"code","5705fa95":"code","f761847c":"code","9f1a8c97":"code","6893eff9":"code","9f3f26d0":"code","9e054a5b":"code","d94cde7e":"code","4279c40a":"code","1a91ada3":"code","5531e557":"code","0e9585e5":"code","7b6b8111":"code","917e838c":"code","2d035fe2":"code","d8a0dba7":"code","5eafb644":"code","4d04005f":"code","e6ec8ff3":"code","50506c3d":"markdown","d7c914fd":"markdown","fea61061":"markdown","d1bca46b":"markdown","7a51c57b":"markdown","5e66c9d8":"markdown","c590fe14":"markdown","c5f05240":"markdown","fc2a8c86":"markdown","9dc5a05d":"markdown","1b72ef94":"markdown","f9a7c58b":"markdown","4b0e01cb":"markdown","6b000c3b":"markdown"},"source":{"6934c06b":"import torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom sklearn import model_selection\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","5705fa95":"class MNIST(Dataset):\n    def __init__(self, df, labels=True, transform=None):\n        \n        self.df = df\n        self.labels = labels\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        if torch.is_tensor(index):\n            index = index.tolist()\n   \n        if self.labels:\n            label = self.df.iloc[index, 0]\n            data  = self.df.iloc[index, 1:].to_numpy(np.uint8).reshape(28,28, 1)\n            if self.transform:\n                data = self.transform(data)\n            return data, label\n    \n        data  = self.df.iloc[index].to_numpy(dtype=np.uint8).reshape(28,28, 1)\n        \n        if self.transform:\n            data = self.transform(data)\n        \n        return data","f761847c":"augment = transforms.Compose([transforms.ToPILImage(),\n                                transforms.RandomRotation(10, fill=(0,)),\n                                transforms.RandomPerspective(),\n                                transforms.RandomAffine(10),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\ntfs = transforms.Compose([transforms.ToTensor(),\n                          transforms.Normalize((0.5,), (0.5,))])","9f1a8c97":"train_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\nX, y = train_df.iloc[:, 1:], train_df.iloc[:, 0]\nx_train, x_valid, y_train, y_valid = model_selection.train_test_split(X, y)\n\ntrainset = MNIST(pd.concat([y_train, x_train], axis=1), transform=augment)\nvalidset = MNIST(pd.concat([y_valid, x_valid], axis=1), transform=tfs)\n\ntrainload = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=4)\nvalidload = DataLoader(validset, batch_size=32, shuffle=True, num_workers=4)","6893eff9":"images, labels = iter(validload).next()\n\nfig = plt.figure(figsize=(25,4))\n\nfor i in range(20):\n    ax = fig.add_subplot(2, 20\/2, i+1, xticks=[], yticks=[])\n    ax.set_title(f'Label: {labels[i].item()}')\n    ax.imshow(images[i].squeeze())\n\nplt.tight_layout()\nplt.show()\n\nimages[0].dtype, images[0].shape, images.shape","9f3f26d0":"class LeNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 6, 5),\n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2),\n        )\n        \n        self.conv2 = nn.Sequential(\n            nn.Conv2d(6, 16, 5),\n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2)\n        )\n        \n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(4*4*16, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU()\n        )\n        \n        self.out = nn.Linear(84, 10)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.fc(x)\n        return self.out(x)","9e054a5b":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LeNet()\nmodel = model.to(device)","d94cde7e":"opt = optim.Adam(model.parameters(), lr=5e-4)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=7)\ncriterion = nn.CrossEntropyLoss()\nepochs = 40\n\ntrain_loss = []\nvalid_loss = []\naccuracy   = []\nvalid_low = np.Inf\n\n\nfor e in range(epochs):\n    running_tl = 0\n    running_vl = 0\n    running_ac = 0\n    \n    # backprop and and update\n    model.train()\n    for images, labels in trainload:\n        images, labels = images.to(device), labels.to(device)\n        opt.zero_grad()\n        t_cel = criterion(model(images.float()), labels)\n        t_cel.backward()\n        opt.step()\n        running_tl += t_cel.item()\n        \n    # validation pass    \n    with torch.no_grad():\n        model.eval()\n        for images, labels in validload:\n            images, labels = images.to(device), labels.to(device)\n            scores = model(images.float())\n            ps = F.softmax(scores, dim=1)\n            v_cel = criterion(scores, labels)\n            pred = torch.argmax(ps, dim=1)\n            running_ac += (pred == labels).cpu().numpy().mean()\n            running_vl += v_cel.item()\n    \n    # Decay Learning Rate:\n    print(f'Epoch {e} Learning Rate: {opt.param_groups[0][\"lr\"]} Validation Loss: {v_cel}')\n    scheduler.step(v_cel)\n\n    # get loss metrics for plotting later\n    train_loss.append(running_tl\/len(trainload))\n    valid_loss.append(running_vl\/len(validload))\n    accuracy.append(running_ac\/len(validload))","4279c40a":"plt.title('Loss vs Epochs')\nplt.xlabel('Epochs')\nplt.plot(train_loss, label='Training')\nplt.plot(valid_loss, label='Validation')\nplt.legend()","1a91ada3":"plt.title('Accuracy on Validation Set by Epoch')\nplt.xlabel('Epochs')\nplt.plot(accuracy)","5531e557":"torch.save(model.state_dict(), 'alltrans_50e.pt')\naccuracy[-1]","0e9585e5":"comp_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ncomp_data.head()","7b6b8111":"comp_set = MNIST(comp_data, labels=False, transform=tfs)\ncomp_loader = DataLoader(comp_set, batch_size=32, num_workers=0, shuffle=False)","917e838c":"predictions = np.array([])\n\nfor images in comp_loader:\n    images = images.to(device).float()\n    scores = model(images)\n    ps = F.softmax(scores, dim=1)\n    preds = torch.argmax(ps, dim=1).cpu()\n    predictions = np.append(predictions, preds)\n    ","2d035fe2":"predictions = predictions.astype(np.int)","d8a0dba7":"sub_df = pd.DataFrame({'ImageId': np.arange(1, len(predictions) + 1),\n                       'Label': predictions})\n\nsub_df.head(10)","5eafb644":"images = iter(comp_loader).next()\n\nfig = plt.figure(figsize=(25,4))\n\nfor i in range(20):\n    ax = fig.add_subplot(2, 20\/2, i+1, xticks=[], yticks=[])\n    ax.set_title(f'Label: {sub_df.loc[i, \"Label\"]}')\n    ax.imshow(images[i].squeeze())\n\nplt.tight_layout()\nplt.show()","4d04005f":"sub_df.to_csv('submission_LeNet.csv', index=False)","e6ec8ff3":"# ! kaggle competitions submit -c digit-recognizer -f submission_LeNet.csv -m \"same as last +10 epochs\"","50506c3d":"Now that our model is defined, let's create an instance of it and move it to GPU:","d7c914fd":"# Final Visualization\nLet's visually sample our predicted labels. This is a good idea, because we want to know if our data is formatted correctly for submission. ","fea61061":"## Defining a Model\nI'm going to define my model based on LeNet-5. This is a modified version, where all 6 features of C1 and S2 are run through C3. The original version only had a partial mapping. LeNet-5 was my attempt to take what I learned in the Udacity PyTorch course, read up on a new architecture and implement it myself. There are many others out there, but this one is quite famous for being a \"classic\" architecture.","d1bca46b":"## Custom Dataset Object\nSince the dataset provided by Kaggle is not in image format, we can define a custom MNIST dataset object in PyTorch. In the Kaggle dataset, each row of the .csv file represents a single image and its label. The first entry in the row is the image label, while the other 784 entries in the row are the unstacked image pixel values. \n\nFor a convolutional network, we will need to transform those row vectors back into 3D tensors representing actual image data.","7a51c57b":"Let's read in our Kaggle training data, split it (25% validation hold out) and get it into `DataLoaders`:","5e66c9d8":"Looks pretty good!","c590fe14":"__Note:__ There are several bugs in IPython that cause PyTorch DataLoaders to hang when `num_workers > 0`. I encountered that issue when loading the competition test set for inference. For whatever reason, I was able to use 4 workers on the training data, but not here.","c5f05240":"If you have installed the Kaggle API, and are sure you are ready to submit, run this last cell:","fc2a8c86":"# Convolutional Network for MNIST Predictions. \nIn this notebook, I will be using a convolutional architecture rather than a fully connected network. The architecture I am adopting is LeNet-5, which has recorded an error rate as low as 0.95 (or %99.05 accurate). I don't know if I will get quite those results, but let's try!","9dc5a05d":"## Transforms, Augmentation and Datasets and DataLoaders\n\nData augmentation is a way to artificially increase the size of the training data. I say artificially, because if you look at the custom dataset class above, you will see that augmentations and transforms are randomly applied on the fly in the `__getitem__` method. What this means is that the amount of data is actually the same, but each epoch a randomly different version of that data will be seen. For this reason, as we add data augmentation, we must also increase the number of epochs to combat under-fitting. \n\nFinding the right number of augmentations and epoch increases was done by holding all variables in strict control, and introducing one augmentation at a time. After a new augmentation was added, I slowly increased the number of training epochs while monitoring validation loss. I stopped adding epochs when validation loss reached it's previous minimum (IOW the minimum loss before the new augmentation was added). I repeated this process for each new augmentation. This process is slow and tedious but ensures rigorous results. \n\nBelow, we also define very basic transforms that will also apply to the validation set. First, we transform each 28x28 numpy array into a 1x28x28 torch image tensor. Then we normalize the data to help convergence.","1b72ef94":"## Training the Network:\nNow that the model has been defined, initialized and moved to GPU, we can begin training. \n\n### What you can't see from the cell below:\nI ran this cell multiple times to get my final result. Here is my process: \n\n1. I started with a learning rate of `0.0005` and applied learning rate decay through `lr_scheduler`. My initial run was 30 epochs, which I settled on after experimenting with adding a single augmentation at a time, then measuring the number of additional epochs required to combat under-fitting. I'll add a note at the bottom to expand on this.\n\n2. Once the initial 30 epochs are run, repeat:\n    1. Take note of final learning rate\n    2. Modify optimizer learning rate to equal final learning rate from last run\n    3. Run for 10 more epochs.\n\nUsing this process, my validation loss and leaderboard rankings continued to improve with this process, but I ran out of submission attempts on Kaggle. You may try to continue this process to get an even better score. ","f9a7c58b":"# Predictions of Kaggle Competition Data:\nNow that we have a model that is doing quite well on validation set, let's put it to the test in our competition.","4b0e01cb":"## Plotting Our Metrics:\n\nNote that validation loss is lower than training loss because the validation pass is run __after__ weights are updated by back-propagation. In other words, the model is improved before validation loss is measured, and so it gets a little boost.","6b000c3b":"Just to make sure that everything is looking good before we get started, let's take a look at a few images:\n"}}