{"cell_type":{"b0cc6a44":"code","ba704988":"code","cfd12e2a":"code","872ab9d2":"code","3f60566c":"code","1910e407":"code","06212eb2":"code","726a721e":"code","6d5e2a91":"code","febc9366":"code","e26c76b1":"code","c899905b":"code","43696da6":"code","a7a3a9f3":"code","a4dd905d":"code","9ef1da42":"code","50d69490":"code","ea099fa4":"code","3a0d85bc":"code","98ae726d":"code","d2ed5b0c":"code","9283c7b9":"code","5d39393d":"code","59c13574":"code","5f2e1540":"code","57fca057":"code","fd12ccdc":"code","3d95b74e":"code","c0f15197":"code","0664513f":"code","145dccbe":"code","ab92d81d":"code","5f2066f2":"code","0dea028e":"code","85015209":"code","49065af7":"code","aeba4cb0":"code","a13d2a22":"code","b4064ed9":"code","9bdd094b":"code","64c6a573":"code","3feb07fb":"code","e63d0326":"markdown","0fe975de":"markdown","63329df5":"markdown","c1b59d1f":"markdown","b9813273":"markdown","2c771c9a":"markdown","1bda13c3":"markdown","1470eb5c":"markdown","20466c92":"markdown","0ba10349":"markdown","515228ae":"markdown","6f069891":"markdown","70ce5769":"markdown","0d74c013":"markdown","69e2bda7":"markdown","7eb71d61":"markdown","4abb556d":"markdown","dc3703f5":"markdown","8262518c":"markdown","759195d3":"markdown","4b478550":"markdown","a5646aad":"markdown","576f594d":"markdown","45a23817":"markdown","0e3be9b6":"markdown","e7613b7b":"markdown","1ad5ddfc":"markdown","ca05ff3e":"markdown","5970670a":"markdown","3555ef9c":"markdown","ce031e8d":"markdown","e60ca86a":"markdown","fe810f14":"markdown"},"source":{"b0cc6a44":"#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\nsns.despine(left=True)","ba704988":"#bring in the six packs\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","cfd12e2a":"highCorrFeatures = train[train.columns[1:]].corr()['SalePrice'].sort_values(ascending=False)[0:15].to_frame().transpose().columns.tolist()\nhighCorrFeatures.append('Id')\ntrain = train[[c for c in highCorrFeatures]]\ntest = test[[c for c in highCorrFeatures if c!='SalePrice']]\nfig, axs = plt.subplots(1,1,figsize=(12,12))\nsns.heatmap(train.corr(), vmax=.8, square=True, ax=axs);\nplt.close(2)","872ab9d2":"train.drop(['TotRmsAbvGrd','GarageArea','YearRemodAdd', 'GarageYrBlt','1stFlrSF'],axis=1,inplace=True)\ntest.drop(['TotRmsAbvGrd','GarageArea','YearRemodAdd', 'GarageYrBlt','1stFlrSF'],axis=1,inplace=True)\n\nsns.heatmap(train.corr(), vmax=.8, square=True);","3f60566c":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\n\ntotal_ = test.isnull().sum().sort_values(ascending=False)\npercent_ = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent, total_, percent_], axis=1, keys=['Total', 'Percent', 'Total_','Percent_'])\nmissing_data.head(20)","1910e407":"#dealing with missing data\ntrain['MasVnrArea'].fillna(method=\"ffill\", inplace=True)\nprint(train.isnull().sum().max()) #just checking that there's no missing data missing...\n\n#dealing with missing data\ntest['BsmtFinSF1'].fillna(0, inplace=True)\ntest['GarageCars'].fillna(0, inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)\ntest['TotalBsmtSF'].fillna(0, inplace=True)\n\nprint(test.isnull().sum().max()) #just checking that there's no missing data missing...","06212eb2":"fig, axs = plt.subplots(1,2,figsize=(16,6))\nax = sns.distplot(train['SalePrice'],fit=norm, ax=axs[0])\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","726a721e":"# Normality : applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])\nfig, axs = plt.subplots(1,2,figsize=(16,6))\nax = sns.distplot(train['SalePrice'],fit=norm, ax=axs[0])\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.close(2)\n\n# new skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","6d5e2a91":"fig, axs = plt.subplots(1,2,figsize=(24,6))\nfig = sns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['OverallQual'], ax=axs[1]);\nplt.close(2)","febc9366":"fig, axs = plt.subplots(1,2,figsize=(16,6))\nfig = sns.boxplot(x=\"GarageCars\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['GarageCars'], ax=axs[1]);\nplt.close(2)","e26c76b1":"fig, axs = plt.subplots(1,2,figsize=(16,6))\nfig = sns.boxplot(x=\"FullBath\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['FullBath'], ax=axs[1]);\nplt.close(2)","c899905b":"fig, axs = plt.subplots(1,2,figsize=(24,6))\nax1 = sns.boxplot(x=\"YearBuilt\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['YearBuilt'], ax=axs[1],fit=norm);\nplt.close(2)","43696da6":"fig, axs = plt.subplots(1,2,figsize=(16,6))\nfig = sns.boxplot(x=\"Fireplaces\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['Fireplaces'], ax=axs[1]);\nplt.close(2)","a7a3a9f3":"fig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['GrLivArea'], ax=axs[1],fit=norm)\nres = stats.probplot(train['GrLivArea'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % train['GrLivArea'].kurt())","a4dd905d":"#applying log transformation\ntrain['GrLivArea'] = np.log(train['GrLivArea'])\ntest['GrLivArea'] = np.log(test['GrLivArea'])\nfig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.regplot(x=\"GrLivArea\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['GrLivArea'], ax=axs[1],fit=norm)\nres = stats.probplot(train['GrLivArea'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % train['GrLivArea'].kurt())","9ef1da42":"fig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.scatterplot(x=\"TotalBsmtSF\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['TotalBsmtSF'], ax=axs[1],fit=norm);\nres = stats.probplot(train['TotalBsmtSF'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['TotalBsmtSF'].skew())\nprint(\"Kurtosis: %f\" % train['TotalBsmtSF'].kurt())","50d69490":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ntrain['HasBsmt'] = pd.Series(len(train['TotalBsmtSF']), index=train.index)\ntrain['HasBsmt'] = 0 \ntrain.loc[train['TotalBsmtSF']>0,'HasBsmt'] = 1\n\ntest['HasBsmt'] = pd.Series(len(test['TotalBsmtSF']), index=test.index)\ntest['HasBsmt'] = 0 \ntest.loc[test['TotalBsmtSF']>0,'HasBsmt'] = 1","ea099fa4":"#transform data\ntrain.loc[train['HasBsmt']==1,'TotalBsmtSF'] = np.log(train['TotalBsmtSF'])\ntest.loc[test['HasBsmt']==1,'TotalBsmtSF'] = np.log(test['TotalBsmtSF'])\nfig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.regplot(x=train[train['TotalBsmtSF']>0]['TotalBsmtSF'], y=train[train['TotalBsmtSF']>0][\"SalePrice\"], ax=axs[0])\nax = sns.distplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], ax=axs[1],fit=norm)\nres = stats.probplot(train[train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train[train['TotalBsmtSF']>0]['TotalBsmtSF'].skew())\nprint(\"Kurtosis: %f\" % train[train['TotalBsmtSF']>0]['TotalBsmtSF'].kurt())","3a0d85bc":"fig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.scatterplot(x=\"MasVnrArea\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['MasVnrArea'].fillna(method='ffill'), ax=axs[1],fit=norm);\nres = stats.probplot(train['MasVnrArea'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['MasVnrArea'].skew())\nprint(\"Kurtosis: %f\" % train['MasVnrArea'].kurt())","98ae726d":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if MasVnrArea>0 it gets 1, for MasVnrArea==0 it gets 0\ntrain['HasMasVnr'] = pd.Series(len(train['MasVnrArea']), index=train.index)\ntrain['HasMasVnr'] = 0 \ntrain.loc[train['MasVnrArea']>0,'HasMasVnr'] = 1\n\ntest['HasMasVnr'] = pd.Series(len(test['MasVnrArea']), index=test.index)\ntest['HasMasVnr'] = 0 \ntest.loc[test['MasVnrArea']>0,'HasMasVnr'] = 1","d2ed5b0c":"#transform data\ntrain.loc[train['HasMasVnr']==1,'MasVnrArea'] = np.log(train['MasVnrArea'])\ntest.loc[test['HasMasVnr']==1,'MasVnrArea'] = np.log(test['MasVnrArea'])\nfig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.regplot(x=train[train['MasVnrArea']>0]['MasVnrArea'], y=train[train['MasVnrArea']>0][\"SalePrice\"], ax=axs[0])\nax = sns.distplot(train[train['MasVnrArea']>0]['MasVnrArea'], ax=axs[1],fit=norm)\nres = stats.probplot(train[train['MasVnrArea']>0]['MasVnrArea'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train[train['MasVnrArea']>0]['MasVnrArea'].skew())\nprint(\"Kurtosis: %f\" % train[train['MasVnrArea']>0]['MasVnrArea'].kurt())","9283c7b9":"fig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.scatterplot(x=\"BsmtFinSF1\", y=\"SalePrice\", data=train, ax=axs[0])\nax = sns.distplot(train['BsmtFinSF1'].fillna(method=\"ffill\"), ax=axs[1],fit=norm);\nres = stats.probplot(train['BsmtFinSF1'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train['BsmtFinSF1'].skew())\nprint(\"Kurtosis: %f\" % train['BsmtFinSF1'].kurt())","5d39393d":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if BsmtFinSF1>0 it gets 1, for BsmtFinSF1==0 it gets 0\ntrain['HasBsmtFinSF1'] = pd.Series(len(train['BsmtFinSF1']), index=train.index)\ntrain['HasBsmtFinSF1'] = 0 \ntrain.loc[train['BsmtFinSF1']>0,'HasBsmtFinSF1'] = 1\n\ntest['HasBsmtFinSF1'] = pd.Series(len(test['BsmtFinSF1']), index=test.index)\ntest['HasBsmtFinSF1'] = 0 \ntest.loc[test['BsmtFinSF1']>0,'HasBsmtFinSF1'] = 1","59c13574":"#transform data\ntrain.loc[train['HasBsmtFinSF1']==1,'BsmtFinSF1'] = np.log(train['BsmtFinSF1'])\ntest.loc[test['HasBsmtFinSF1']==1,'BsmtFinSF1'] = np.log(test['BsmtFinSF1'])\nfig, axs = plt.subplots(1,3,figsize=(24,6))\nax = sns.regplot(x=train[train['BsmtFinSF1']>0]['BsmtFinSF1'], y=train[train['BsmtFinSF1']>0][\"SalePrice\"], ax=axs[0])\nax = sns.distplot(train[train['BsmtFinSF1']>0]['BsmtFinSF1'], ax=axs[1],fit=norm)\nres = stats.probplot(train[train['BsmtFinSF1']>0]['BsmtFinSF1'], plot=plt)\nplt.close(2)\n\n#skewness and kurtosis\nprint(\"Skewness: %f\" % train[train['BsmtFinSF1']>0]['BsmtFinSF1'].skew())\nprint(\"Kurtosis: %f\" % train[train['BsmtFinSF1']>0]['BsmtFinSF1'].kurt())","5f2e1540":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\ny_train = train.SalePrice.values\ntrain.drop(['SalePrice'], axis=1, inplace=True)","57fca057":"from sklearn.model_selection import RandomizedSearchCV\n\n# This class will be used for Hyperparameter tuning of all Algorithms\nclass TunedModel:\n    def __init__(self, X,y):\n        self.X = X\n        self.y = y\n        self.clf = None\n        self.params = None\n    \n    def run(self,clf, params):\n        self.clf = clf\n        self.params = params\n        # run randomized search\n        n_iter_search = 20\n        tunedModel = RandomizedSearchCV(self.clf, param_distributions=self.params,n_iter=n_iter_search, cv=5, iid=False)\n        tunedModel.fit(self.X, self.y)\n        #self.report(tunedModel.cv_results_)\n        return tunedModel\n        \n    # Utility function to report best scores\n    def report(results, n_top=3):\n        for i in range(1, n_top + 1):\n            candidates = np.flatnonzero(results['rank_test_score'] == i)\n            for candidate in candidates:\n                print(\"Model with rank: {0}\".format(i))\n                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                      results['mean_test_score'][candidate],\n                      results['std_test_score'][candidate]))\n                print(\"Parameters: {0}\".format(results['params'][candidate]))\n                print(\"\")\n                \ntunedObj = TunedModel(train,y_train)","fd12ccdc":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","3d95b74e":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","c0f15197":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","0664513f":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","145dccbe":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","ab92d81d":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","5f2066f2":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","0dea028e":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","85015209":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","49065af7":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","aeba4cb0":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","a13d2a22":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","b4064ed9":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","9bdd094b":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","64c6a573":"''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","3feb07fb":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","e63d0326":"**SalePrice** : Selling Price of the House","0fe975de":"So we're left with Eight features.\n\nAwesome. Let's dig deep into these eight features and let's get to know these features more (statistical analysis)","63329df5":"Look deep into the Heatmap, and you'll see that a low of features have high correlation between each other (search for lighter squares) which we don't want because they don't add any value to the model. Find more [here](https:\/\/newonlinecourses.science.psu.edu\/stat501\/node\/346\/)\n\nWe note the following high correlations between feature vectors : \n* <b>GrLivArea<\/b> --> TotRmsAbvGrd\n* <b>TotalBsmtSF<\/b> --> 1stFlrSF\n* <b>GarageCars<\/b> --> GarageArea\n* <b>YearBuilt<\/b> --> YearRemodAdd, GarageYrlt\n\nHence we delete the right side names as they do not provide much of uniqueness for us currenty.","c1b59d1f":"### 4. YearBuilt: Original construction date","b9813273":"Inpiration taken from [Stacked Regression](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by [Serigne](https:\/\/www.kaggle.com\/serigne)","2c771c9a":"## Relation with Numerical variables\n\n### 6. GrLivArea : Above grade (ground) living area square feet","1bda13c3":"### 3. FullBath: Full bathrooms above grade","1470eb5c":"**Normality** - The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","20466c92":"**Normality** - The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","0ba10349":"**Normality** - Done\n\n**Homoscedasticity** - The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nOlder versions of this scatter plot (previous to log transformations), had a conic shape. As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\n**Linearity** - We can see a very well defined linear relationship between SalePrice and GrLivArea, hence linearity is proved.\n\n**Absence of correlated errors** - Not sure. Need help","515228ae":"### 8. MasVnrArea : Masonry veneer area in square feet","6f069891":"**Normality** - Done\n\n**Homoscedasticity** - Older versions of this scatter plot (previous to log transformations), had a conic shape. As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\n**Linearity** - We can see a very well defined linear relationship between SalePrice and GrLivArea, hence linearity is proved.\n\n**Absence of correlated errors** - Not sure. Need help","70ce5769":"## Univariate study","0d74c013":"**Normality** - The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","69e2bda7":"**Normality** - The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","7eb71d61":"**Normality** - Done\n\n**Homoscedasticity** - Older versions of this scatter plot (previous to log transformations), had a conic shape. As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\n**Linearity** - We can see a very well defined linear relationship between SalePrice and GrLivArea, hence linearity is proved.\n\n**Absence of correlated errors** - Not sure. Need help","4abb556d":"### 2. GarageCars : Size of garage in car capacity","dc3703f5":"### 7. TotalBsmtSF : Total basement area in square feet","8262518c":"### 9. BsmtFinSF1 : Type 1 finished square feet","759195d3":"1. <b>Understand the problem<\/b>. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n2. <b>Basic cleaning<\/b>. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n3. <b>Univariable study<\/b>. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it. We'll check if our data meets the assumptions required by most multivariate techniques.\n4. <b>Multivariate study<\/b>. We'll try to understand how the dependent variable and independent variables relate. We'll check if our data meets the assumptions required by most multivariate techniques.\n5. <b>Modeling<\/b>. Using different models and parameters, will try to find the best possible answer.\n\nAs mentioned by [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino), for each analysis that we perform, we will test these four assumptions :\n\n\n**Normality** - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n**Homoscedasticity** - Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n**Linearity**- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n**Absence of correlated errors** - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.","4b478550":"Better the quality of the house, more the price rise. Majority of the house have (above?) average quality houses (5-7 quality)","a5646aad":"**Normality** - Done\n\n**Homoscedasticity** - N\/A for Univariate analysis\n\n**Linearity** - N\/A for Univariate analysis\n\n**Absence of correlated errors** - N\/A for Univariate analysis","576f594d":"#### Define a cross validation strategy","45a23817":"## Basic cleaning","0e3be9b6":"\n\n# Multivariate Study\n\n## Relation with Categorical variables\n\n### **1. OverallQual** : Rates the overall material and finish of the house","e7613b7b":"**Normality** - *SalePrice* is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line. But in case of positive skewness, log transformations usually works well.","1ad5ddfc":"### Base Models","ca05ff3e":"## Modeling","5970670a":"**Normality** - Done\n\n**Homoscedasticity** - Older versions of this scatter plot (previous to log transformations), had a conic shape. As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\n**Linearity** - We can see a very well defined linear relationship between SalePrice and GrLivArea, hence linearity is proved.\n\n**Absence of correlated errors** - Not sure. Need help","3555ef9c":"## Understanding the Problem","ce031e8d":"Before we start any of our work, let's understand how the independent variables are related to each other.\nWe are looking for the features with high correlation with \"SalePrice\".\n\nExtract the correlation of all columns with \"SalePrice\" and pick the top 20 features.","e60ca86a":"### 5. Fireplaces: Number of fireplaces","fe810f14":"# Meaning & Relevance + Modeling\n[Saurabh Chakrabarty](https:\/\/linkedin.com\/in\/iamsaurabhc) - July 2019\n\nInspiration taken from: [Comprehensive Data with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino)\n\n----------"}}