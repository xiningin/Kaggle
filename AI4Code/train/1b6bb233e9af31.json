{"cell_type":{"62918174":"code","2fc09eaf":"code","6ecb1e40":"code","6b9968f1":"code","a071b02c":"code","c2c2392c":"code","5790a229":"code","5671b118":"code","2da1d1b3":"code","d6251fa5":"code","91211f07":"code","c99f4344":"code","445d9743":"code","d48a152c":"code","5f246988":"code","0cdd20e9":"code","8f5ff85a":"code","14ac89c4":"code","3de2d7d3":"markdown","9897a0db":"markdown","faff6d89":"markdown","84d049c5":"markdown","b8d4d2cf":"markdown"},"source":{"62918174":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nfrom time import perf_counter\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\n\ndef printmd(string):\n    # Print with Markdowns    \n    display(Markdown(string))","2fc09eaf":"# Create a list with the filepaths for training and testing\ndir_ = Path('..\/input\/chest-xray-pneumonia\/chest_xray\/train')\ntrain_file_paths = list(dir_.glob(r'**\/*.jpeg'))\n\ndir_ = Path('..\/input\/chest-xray-pneumonia\/chest_xray\/test')\ntest_file_paths = list(dir_.glob(r'**\/*.jpeg'))\n\ndir_ = Path('..\/input\/chest-xray-pneumonia\/chest_xray\/val')\nval_file_paths = list(dir_.glob(r'**\/*.jpeg'))","6ecb1e40":"def proc_img(filepath):\n    \"\"\" Create a DataFrame with the filepath and the labels of the pictures\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1,random_state=0).reset_index(drop = True)\n    \n    return df\n\ntrain_df = proc_img(train_file_paths)\nval_df = proc_img(val_file_paths)\n# Combine train_df and val_df\ntrain_df = pd.concat([train_df,val_df]).reset_index(drop = True)\ntest_df = proc_img(test_file_paths)\n\n\nprint(f'Number of pictures in the training set: {train_df.shape[0]}')\nprint(f'Number of pictures in the test set: {test_df.shape[0]}')\nprint(f'Number of pictures in the validation set: {val_df.shape[0]}\\n')\n\n\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')\n\n# The DataFrame with the filepaths in one column and the labels in the other one\ntrain_df.head(5)","6b9968f1":"# Display the number of pictures of each category in the training set\nvc = train_df['Label'].value_counts()\nplt.figure(figsize=(20,5))\nsns.barplot(x = sorted(vc.index), y = vc, palette = \"rocket\")\nplt.title(\"Number of pictures of each category in the training set\", fontsize = 15)\nplt.show()","a071b02c":"# Display some pictures of the dataset\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(15, 7),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(train_df.Filepath[i]))\n    ax.set_title(train_df.Label[i], fontsize = 15)\nplt.tight_layout(pad=0.5)\nplt.show()","c2c2392c":"def create_gen():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n        validation_split=0.1\n    )\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n    )\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n#         rotation_range=30, # Uncomment to use data augmentation\n#         zoom_range=0.15,\n#         width_shift_range=0.2,\n#         height_shift_range=0.2,\n#         shear_range=0.15,\n#         horizontal_flip=True,\n#         fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n#         rotation_range=30, # Uncomment to use data augmentation on the validation set\n#         zoom_range=0.15,\n#         width_shift_range=0.2,\n#         height_shift_range=0.2,\n#         shear_range=0.15,\n#         horizontal_flip=True,\n#         fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","5790a229":"def get_model(model):\n# Load the pretained model\n    kwargs =    {'input_shape':(224, 224, 3),\n                'include_top':False,\n                'weights':'imagenet',\n                'pooling':'avg'}\n    \n    pretrained_model = model(**kwargs)\n    pretrained_model.trainable = False\n    \n    inputs = pretrained_model.input\n\n    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n\n    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy']\n    )\n    \n    return model","5671b118":"# Dictionary with the models\nmodels = {\n    \"DenseNet121\": {\"model\":tf.keras.applications.DenseNet121, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"DenseNet169\": {\"model\":tf.keras.applications.DenseNet169, \"perf\":0},\n    \"DenseNet201\": {\"model\":tf.keras.applications.DenseNet201, \"perf\":0},\n    \"EfficientNetB0\": {\"model\":tf.keras.applications.EfficientNetB0, \"perf\":0},\n    \"EfficientNetB1\": {\"model\":tf.keras.applications.EfficientNetB1, \"perf\":0},\n    \"EfficientNetB2\": {\"model\":tf.keras.applications.EfficientNetB2, \"perf\":0},\n    \"EfficientNetB3\": {\"model\":tf.keras.applications.EfficientNetB3, \"perf\":0},\n    \"EfficientNetB4\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB5\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB6\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"EfficientNetB7\": {\"model\":tf.keras.applications.EfficientNetB4, \"perf\":0},\n    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n    \"MobileNetV2\": {\"model\":tf.keras.applications.MobileNetV2, \"perf\":0},\n    \"MobileNetV3Large\": {\"model\":tf.keras.applications.MobileNetV3Large, \"perf\":0},\n    \"MobileNetV3Small\": {\"model\":tf.keras.applications.MobileNetV3Small, \"perf\":0},\n    \"NASNetMobile\": {\"model\":tf.keras.applications.NASNetMobile, \"perf\":0},\n    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n    \"ResNet101V2\": {\"model\":tf.keras.applications.ResNet101V2, \"perf\":0},\n    \"ResNet152\": {\"model\":tf.keras.applications.ResNet152, \"perf\":0},\n    \"ResNet152V2\": {\"model\":tf.keras.applications.ResNet152V2, \"perf\":0},\n    \"ResNet50\": {\"model\":tf.keras.applications.ResNet50, \"perf\":0},\n    \"ResNet50V2\": {\"model\":tf.keras.applications.ResNet50V2, \"perf\":0},\n    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n    \"VGG19\": {\"model\":tf.keras.applications.VGG19, \"perf\":0},\n    \"Xception\": {\"model\":tf.keras.applications.Xception, \"perf\":0}\n}\n\n# Create the generators\ntrain_generator,test_generator,train_images,val_images,test_images=create_gen()\nprint('\\n')\n\n# Fit the models\nfor name, model in models.items():\n    \n    # Get the model\n    m = get_model(model['model'])\n    models[name]['model'] = m\n    \n    start = perf_counter()\n    \n    # Fit the model\n    history = m.fit(train_images,validation_data=val_images,epochs=1,verbose=0)\n    \n    # Sav the duration and the val_accuracy\n    duration = perf_counter() - start\n    duration = round(duration,2)\n    models[name]['perf'] = duration\n    print(f\"{name:20} trained in {duration} sec\")\n    \n    val_acc = history.history['val_accuracy']\n    models[name]['val_acc'] = [round(v,4) for v in val_acc]","2da1d1b3":"for name, model in models.items():\n    \n    # Predict the label of the test_images\n    pred = models[name]['model'].predict(test_images)\n    pred = np.argmax(pred,axis=1)\n\n    # Map the label\n    labels = (train_images.class_indices)\n    labels = dict((v,k) for k,v in labels.items())\n    pred = [labels[k] for k in pred]\n\n    y_test = list(test_df.Label)\n    acc = accuracy_score(y_test,pred)\n    models[name]['acc'] = round(acc,4)","d6251fa5":"# Create a DataFrame with the results\nmodels_result = []\n\nfor name, v in models.items():\n    models_result.append([ name, models[name]['val_acc'][-1], \n                          models[name]['acc'],\n                          models[name]['perf']])\n    \ndf_results = pd.DataFrame(models_result, \n                          columns = ['model','val_accuracy','accuracy (test set)','Training time (sec)'])\ndf_results.sort_values(by='accuracy (test set)', ascending=False, inplace=True)\ndf_results.reset_index(inplace=True,drop=True)\ndf_results","91211f07":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'accuracy (test set)', data = df_results)\nplt.title('Accuracy on the test set (after 1 epoch))', fontsize = 15)\nplt.ylim(0,1)\nplt.xticks(rotation=90)\nplt.show()","c99f4344":"plt.figure(figsize = (15,5))\nsns.barplot(x = 'model', y = 'Training time (sec)', data = df_results)\nplt.title('Training time for each model in sec', fontsize = 15)\nplt.xticks(rotation=90)\nplt.show()","445d9743":"df_results","d48a152c":"acc = df_results.iloc[0]['accuracy (test set)']\nbest_model = df_results.iloc[0]['model']\nprintmd(f'# Best model: {best_model}')\nprintmd(f'# Accuracy on the test set: {acc * 100:.2f}%')","5f246988":"# Predict the labels of the test_images for the best model\npred = models[best_model]['model'].predict(test_images)\npred = np.argmax(pred,axis=1)\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\ny_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\nmodels[name]['acc'] = round(acc,4)","0cdd20e9":"# Display a confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(y_test, pred, normalize='true')\nplt.figure(figsize = (8,6))\nsns.heatmap(cf_matrix, annot=True, xticklabels = sorted(set(y_test)), yticklabels = sorted(set(y_test)),cbar=False)\nplt.title('Normalized Confusion Matrix\\n', fontsize = 23)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","8f5ff85a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","14ac89c4":"# Display picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=4, ncols=6, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df.Filepath.iloc[i]))\n    ax.set_title(f\"True: {test_df.Label.iloc[i].split('_')[0]}\\nPredicted: {pred[i].split('_')[0]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","3de2d7d3":"# 3. Test 27 canned architectures with pre-trained weights<a class=\"anchor\" id=\"3\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n\nMore info about the architectures under: [Module: tf.keras.applications](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=enhttps:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications?hl=en)","9897a0db":"# 2. Load the Images with a generator and Data Augmentation<a class=\"anchor\" id=\"2\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","faff6d89":"# 1. Data preprocessing and visualization<a class=\"anchor\" id=\"1\"><\/a><a class=\"anchor\" id=\"1\"><\/a>","84d049c5":"# 5. Examples of prediction<a class=\"anchor\" id=\"5\"><\/a><a class=\"anchor\" id=\"1\"><\/a>\n","b8d4d2cf":"# Image recognition from chest X-Rays to classify if Patients have pneumonia\n\n## *Find the best model*\n\n# Table of contents\n\n[<h3>1. Data preprocessing and visualization<\/h3>](#1)\n\n[<h3>2. Load the Images with a generator and Data Augmentation<\/h3>](#2)\n\n[<h3>3. Test 27 canned architectures with pre-trained weights<\/h3>](#3)\n\n[<h3>4. Train the best architecture<\/h3>](#4)\n\n[<h3>5. Examples of prediction<\/h3>](#5)\n\n\n## Context\n\n![Chest X-RayChest X-Rays](https:\/\/i.imgur.com\/jZqpV51.png)\n\nFigure S6. Illustrative Examples of Chest X-Rays in Patients with Pneumonia, Related to Figure 6\nThe normal chest X-ray (left panel) depicts clear lungs without any areas of abnormal opacification in the image. Bacterial pneumonia (middle) typically exhibits a focal lobar consolidation, in this case in the right upper lobe (white arrows), whereas viral pneumonia (right) manifests with a more diffuse \u2018\u2018interstitial\u2019\u2019 pattern in both lungs.\n\n## Content\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (Pneumonia\/Normal). There are 5,863 X-Ray images (JPEG) and 2 categories (Pneumonia\/Normal). Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care. For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n\n## Acknowledgements\n\nLicense: CC BY 4.0\n\n![](https:\/\/i.imgur.com\/8AUJkin.png)\n\n## Inspiration\nAutomated methods to detect and classify human diseases from medical images.\n \n  "}}