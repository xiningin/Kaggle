{"cell_type":{"ed456df9":"code","87df726f":"code","56259ece":"code","e2c2a773":"code","4bffaa8c":"code","4c8a512c":"code","779843c8":"code","4e3971c5":"code","99633b91":"code","80905245":"code","35cc76c1":"code","ed989a05":"code","f5361d67":"code","b0a97f17":"code","e1c0a47a":"code","8080cfbf":"code","e2272364":"code","1d960ac7":"code","d72d4570":"code","a6719d9d":"code","f94eaaf3":"code","ed7f9882":"code","2b736809":"code","20e76cbf":"code","164b1164":"code","563ea430":"code","30b993ec":"code","17cb6246":"code","f187e1fb":"code","5539bea8":"code","fdb7d504":"code","de822de3":"code","2112cbf8":"code","73d09d0a":"code","4b0b6b41":"code","c8951bcd":"code","a75ac54d":"code","675cba64":"code","c0722479":"code","6ee57972":"markdown","7170e064":"markdown","cd6c0ed0":"markdown","15191fd1":"markdown","5000850b":"markdown","d9ef850c":"markdown","de85353a":"markdown","48f875ad":"markdown","479b516e":"markdown","db487d63":"markdown","c0eba8a3":"markdown","5b31dfd4":"markdown","322356b9":"markdown","7b12eeb6":"markdown","ea2caf56":"markdown","a3bb626e":"markdown","1ae8d293":"markdown","bb0530c1":"markdown","e1419188":"markdown","5fb0c877":"markdown","98e1bd7b":"markdown","cee8edae":"markdown","a615e38d":"markdown","ffac7500":"markdown","75f958b9":"markdown","e48f5e13":"markdown","f4e745a0":"markdown","8e90fd70":"markdown"},"source":{"ed456df9":"%matplotlib inline\n\nimport os\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport plotly\nimport plotly.graph_objs as go\nplotly.offline.init_notebook_mode(connected=False)\n\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif, GenericUnivariateSelect\nfrom sklearn.preprocessing import Imputer, StandardScaler\n# from sklearn.impute import SimpleImputer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, fbeta_score, make_scorer, f1_score\n# from sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.externals import joblib\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline, make_pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier, EasyEnsembleClassifier\n\nimport lightgbm as lgb\nfrom lightgbm.sklearn import LGBMClassifier","87df726f":"def challenge_metric(y_true, y_pred):\n    \"\"\"\n     Predicted class |      True class       |\n                     |    pos    |    neg    |\n     -----------------------------------------\n      pos            |     -     |  Cost_1   |\n     -----------------------------------------\n      neg            |  Cost_2   |     -     |\n     -----------------------------------------\n     Cost_1 = 10 and cost_2 = 500\n\n     Total_cost = Cost_1*No_Instances + Cost_2*No_Instances.\n    \"\"\"\n\n    Cost_1 = 10\n    Cost_2 = 500\n\n    tn, fp, fn, tp = confusion_matrix(y_true=y_true, y_pred=y_pred).ravel()\n    return Cost_1 * fp + Cost_2 * fn\n\n\ndef negative_challenge_metric(y_true, y_pred):\n    return -challenge_metric(y_true=y_true, y_pred=y_pred)\n\n\ndef impute_target_class(df, strategy=\"mean\"):\n    \"\"\"\n    Impute a pandas dataframe by the statistic (mean\/median\/mode) of the target class\n    \"\"\"\n\n    if strategy != \"mean\" and strategy != \"median\" and strategy != \"mode\":\n        raise Exception(\"Invalid strategy: {}\".format(strategy))\n\n    df1 = df.copy()  # Make a copy of the original df\n\n    df_groupby = df1.groupby(\"class\")\n\n    df_groups = []\n    for group, df_group in df_groupby:\n        if strategy == \"mean\":\n            df_group.fillna(df_group.mean(), inplace=True)\n        elif strategy == \"median\":\n            df_group.fillna(df_group.median(), inplace=True)\n        elif strategy == \"mode\":\n            df_group.fillna(df_group.mode(), inplace=True)\n\n        df_groups.append(df_group)\n\n    df1 = pd.concat(df_groups, axis=0)\n    return df1.iloc[np.argsort(df1.index)]","56259ece":"class PandasColumnsSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select a pandas dataframe using columns.\n    \"\"\"\n\n    def __init__(self, columns=\"all\"):\n        if isinstance(columns, str) and columns != \"all\":\n            raise ValueError(\"Invalid columns: {}\".format(columns))\n\n        self.columns = columns\n\n    def fit(self, *_):\n        return self\n\n    def transform(self, X, *_):\n        if isinstance(self.columns, str) and self.columns == \"all\":\n            self.columns = X.columns\n\n        return X[self.columns]\n\n\nclass PandasCorrelatedFeaturesDropper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Drop highly correlated features from a pandas dataframe.\n    \"\"\"\n\n    def __init__(self, corr_threshold=0.95):\n        self.corr_threshold = corr_threshold\n\n    def fit(self, X, *_):\n        corr_matrix = X.corr().abs()\n        upper = corr_matrix.where(\n            np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n        to_drop = [col for col in upper.columns if any(\n            upper[col] >= self.corr_threshold)]\n        self.to_drop = to_drop\n        return self\n\n    def transform(self, X, *_):\n        return X.drop(self.to_drop, axis=1)\n\n\nclass PandasHighNullRatioFeaturesDropper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Drop high null ratio features from a dataframe.\n    \"\"\"\n\n    def __init__(self, null_ratio_threshold=0.4):\n        self.null_ratio_threshold = null_ratio_threshold\n\n    def fit(self, X, *_):\n        drop_mask = ((X.isnull().sum() \/ len(X)) >= self.null_ratio_threshold)\n        self.drop_columns = X.columns[drop_mask]\n        return self\n\n    def transform(self, X, *_):\n        return X.drop(self.drop_columns, axis=1)\n\n\nclass PandasNullMarkerFeaturesCreator(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Create new features by marking null values in a pandas dataframe.\n    \"\"\"\n\n    def __init__(self, columns=\"all\", null_threshold=None):\n        if isinstance(columns, str) and columns != \"all\":\n            raise ValueError(\"Invalid columns: {}\".format(columns))\n\n        self.columns = columns\n        self.null_threshold = null_threshold\n\n    def fit(self, X, *_):\n        if self.null_threshold is None:\n            self.null_threshold = 0\n\n        over_threshold = (X.isnull().sum() \/ len(X)) >= self.null_threshold\n        self.cols_over_threhold = over_threshold[over_threshold].index\n\n        return self\n\n    def transform(self, X, *_):\n        if isinstance(self.columns, str) and self.columns == \"all\":\n            self.columns = X.columns\n\n        null_markers_df = X[self.columns][self.cols_over_threhold].isnull().astype(\n            \"int\")\n        null_markers_df.columns = [\"{}_is_null\".format(\n            col) for col in null_markers_df.columns]\n        return pd.concat([X, null_markers_df], axis=1)","e2c2a773":"USE_LGB = False # Whether to fit the lightgbm model","4bffaa8c":"# df_train = pd.read_csv(\"..\/input\/aps_failure_training_set.csv\", skiprows=20, na_values=[\"na\"])\n# df_test = pd.read_csv(\"..\/input\/aps_failure_test_set.csv\", skiprows=20, na_values=[\"na\"])\n\ndf_train = pd.read_csv(\"..\/input\/aps_failure_training_set.csv\", na_values=[\"na\"])\ndf_test = pd.read_csv(\"..\/input\/aps_failure_test_set.csv\", na_values=[\"na\"])","4c8a512c":"df_train.shape","779843c8":"df_test.shape","4e3971c5":"df_train.columns","99633b91":"df_test.columns","80905245":"df_train[\"class\"] = (df_train[\"class\"] == \"pos\").astype(\"int\")\ndf_test[\"class\"] = (df_test[\"class\"] == \"pos\").astype(\"int\")","35cc76c1":"df_train.describe()","ed989a05":"class_value_counts = df_train[\"class\"].value_counts()\n\ntrace = go.Pie(labels=class_value_counts.index, \n               values=class_value_counts.values,\n               marker={\n                   \"colors\": [\"blue\", \"red\"]\n               })\n\ndata = [trace]\nlayout = go.Layout(title=\"Target Class Distribution for Training Set\")\n\nfig = go.Figure(data=data, layout=layout)\nplotly.offline.iplot(fig)","f5361d67":"NULL_RATIO_TRHESHOLD = 0 # Set the null ratio threshold required\n\n\nnull_ratios = (df_train.isnull().sum() \/ df_train.shape[0])\nnull_ratios_over_threshold = null_ratios[null_ratios > NULL_RATIO_TRHESHOLD].sort_values(ascending=False)\n\ndata = [\n    go.Bar(\n        x=null_ratios_over_threshold.index,\n        y=null_ratios_over_threshold\n    )\n]\n\nfig = go.Figure(data=data, layout={\n    \"title\": \"Null Ratio for Features with Null Ratio Exceeding {}\".format(NULL_RATIO_TRHESHOLD)\n})\n\nplotly.offline.iplot(fig)","b0a97f17":"def plot_plotly_scatter_matrix(df, features):\n    \"\"\"\n    Plot plotly scatter matrix given pandas dataframe and list of required features.\n    Assumes dataframe has a binary valued column called \"class\".\n    \"\"\"\n    \n    rows = len(features)\n    cols = rows\n\n    fig = plotly.tools.make_subplots(rows=rows, cols=cols)\n\n    neg_class = df[df[\"class\"] == 0]\n    pos_class = df[df[\"class\"] == 1]\n\n    for i, f1 in enumerate(features):\n        for j, f2 in enumerate(features):\n#             print(f1, f2)\n\n            trace0 = go.Scattergl(\n                x = neg_class[f2],\n                y = neg_class[f1],\n                name = '0',\n                mode = 'markers',\n                marker = dict(\n                    size = 2,\n                    color = 'blue'\n                ),\n            )\n\n            trace1 = go.Scattergl(\n                x = pos_class[f2],\n                y = pos_class[f1],\n                name = '1',\n                mode = 'markers',\n                marker = dict(\n                    size = 2,\n                    color = 'red'\n                ),\n            )\n\n            fig.append_trace(trace0, i+1, j+1)\n            fig.append_trace(trace1, i+1, j+1)\n\n            fig['layout']['xaxis{}'.format(i * cols + j+1)].update(title=f2)\n            fig['layout']['yaxis{}'.format(i * cols + j+1)].update(title=f1)\n\n    fig[\"layout\"][\"showlegend\"] = False\n\n    plotly.offline.iplot(fig)","e1c0a47a":"corr = df_train[df_train.columns.difference([\"class\"])].corr()\ncorr.shape","8080cfbf":"trace = go.Heatmap(z=corr.values,\n                   x=corr.columns,\n                   y=corr.index)\n\nfig = go.Figure([trace], layout={\n    \"title\": \"Feature Correlations\"\n})\nplotly.offline.iplot(fig)","e2272364":"tree = RandomForestClassifier(class_weight=\"balanced\")\ntree.fit(Imputer(strategy=\"median\").fit_transform(df_train[df_train.columns.difference([\"class\"])]), \n         df_train[\"class\"])\n\nfeature_importances = pd.DataFrame([ [feature, importance] for feature, importance in zip(df_train.columns.difference([\"class\"]), tree.feature_importances_) ], columns=[\"feature\", \"importance\"])\nfeature_importances = feature_importances.sort_values(\"importance\", ascending=False)\nfeature_importances.reset_index(inplace=True)\nprint(feature_importances)","1d960ac7":"# plot_plotly_scatter_matrix(df_train, list(feature_importances[:5].feature))","d72d4570":"feature_importances[:5][\"feature\"]","a6719d9d":"def get_univariate_feature_importances(df, score_func=chi2):\n    \"\"\"\n    Get feature importances dataframe.\n    \"\"\"\n    \n    feature_selector = GenericUnivariateSelect(score_func, mode=\"percentile\", param=100) # Select all features\n    feature_selector.fit(Imputer(strategy=\"median\").fit_transform(df[df.columns.difference([\"class\"])]), \n                         df_train[\"class\"])\n    score_df = pd.DataFrame([ [ feature, importance ] for feature, importance in zip(df.columns[1:], feature_selector.scores_) ], \n                            columns=[\"feature\", \"importance\"])\n    score_df = score_df.sort_values(\"importance\", ascending=False)\n    score_df.reset_index(drop=True, inplace=True)\n    return score_df","f94eaaf3":"score_chi2 = get_univariate_feature_importances(df_train)\nprint(score_chi2.head())","ed7f9882":"score_f_classif = get_univariate_feature_importances(df_train, score_func=f_classif)\nprint(score_f_classif.head())","2b736809":"df_train[df_train.columns.difference([\"class\"])[89]].describe()","20e76cbf":"score_mutual_info_classif = get_univariate_feature_importances(df_train, score_func=mutual_info_classif)\nprint(score_mutual_info_classif.head())","164b1164":"score_chi2[\"rank\"] = score_chi2[\"importance\"].rank(ascending=False)\nscore_f_classif[\"rank\"] = score_f_classif[\"importance\"].rank(ascending=False)\nscore_mutual_info_classif[\"rank\"] = score_mutual_info_classif[\"importance\"].rank(ascending=False)","563ea430":"df_feature_scores = pd.merge(pd.merge(score_chi2, score_f_classif, how=\"left\", on=\"feature\", suffixes=[\"_chi2\", \"_f_classif\"]), score_mutual_info_classif, how=\"left\", on=\"feature\")\ndf_feature_scores.columns = [\n    \"feature\",\n    \n    \"score_chi2\",\n    \"rank_chi2\",\n    \n    \"score_f_classif\",\n    \"rank_f_classif\",\n\n    \"score_mutual_info_classif\",\n    \"rank_mutual_info_classif\"\n]\ndf_feature_scores[\"mean_rank\"] = (df_feature_scores[\"rank_chi2\"] + df_feature_scores[\"rank_f_classif\"] + df_feature_scores[\"rank_mutual_info_classif\"]) \/ 3\ndf_feature_scores = df_feature_scores.sort_values(\"mean_rank\", ascending=True)\ndf_feature_scores[\"mean_rank1\"] = df_feature_scores[\"mean_rank\"].rank(ascending=True)\ndf_feature_scores.reset_index(drop=True, inplace=True)\ndf_feature_scores","30b993ec":"# model = joblib.load(\"models\/searcher_lgb-23830-9050.joblib\")","17cb6246":"# model.best_params_","f187e1fb":"# test_score = model.score(df_test, df_test[\"class\"])\n# print(\"challenge metric (test set):\", test_score)","5539bea8":"# confusion_matrix(model.predict(df_test), df_test[\"class\"])","fdb7d504":"scorer = make_scorer(negative_challenge_metric)","de822de3":"pipeline_lgb = make_pipeline(\n    PandasColumnsSelector(columns=df_train.columns.difference([\"class\"])),\n    \n    PandasHighNullRatioFeaturesDropper(), # We can either drop high null ratio features\n#     PandasNullMarkerFeaturesCreator() # ...or we keep them, but create new features indicating null locations\n    \n    PandasCorrelatedFeaturesDropper(), # Drop highly correlated features\n    \n    Imputer(), # Replace null values, will put this step as low as possible to avoid any distortion to data\n    \n    GenericUnivariateSelect(chi2), # Feature selection based on univariate statistical testing\n\n#     RandomUnderSampler(),\n    BalancedBaggingClassifier(base_estimator=LGBMClassifier(num_threads=8), n_jobs=-1) # Ensembles of resampling LGBMClassifiers \n)\npipeline_lgb","2112cbf8":"list(pipeline_lgb.get_params().keys())","73d09d0a":"searcher_lgb = GridSearchCV(estimator=pipeline_lgb, \n                            param_grid=[\n                                {\n#                                     \"simpleimputer__strategy\": [\"median\"],\n                                    \"imputer__strategy\": [\"median\"],\n                                    \n#                                     \"pandashighnullratiofeaturesdropper__null_ratio_threshold\": [0.7, 1],\n                                    \"pandashighnullratiofeaturesdropper__null_ratio_threshold\": [0.3, 0.5, 0.7],\n#                                     \"pandascorrelatedfeaturesdropper__corr_threshold\": [0.9, 1],\n                                    \"pandascorrelatedfeaturesdropper__corr_threshold\": [0.85, 0.9, 0.95],\n                                    \n                                    \"genericunivariateselect__mode\": [\"percentile\",],\n#                                     \"genericunivariateselect__param\": [75, 100],\n                                    \"genericunivariateselect__param\": [25, 50, 75],\n                                    \n#                                     \"lgbmclassifier__boosting\": [\"gbdt\", \"dart\", \"goss\"],\n#                                     \"lgbmclassifier__num_iterations\": [100, 200, 500, 1000],\n#                                     \"balancedbaggingclassifier__base_estimator__boosting\": [\"gbdt\", \"dart\", \"goss\"],\n                                    \"balancedbaggingclassifier__base_estimator__boosting\": [\"gbdt\"],\n                                    \"balancedbaggingclassifier__base_estimator__num_iterations\": [200, 300, 400],\n#                                     \"balancedbaggingclassifier__base_estimator__max_depth\": [5, 7, -1],\n                                    \"balancedbaggingclassifier__base_estimator__max_depth\": [6, 7, 8],\n#                                     \"balancedbaggingclassifier__base_estimator__min_data_in_leaf\": [10, 20, 30],\n                                    \"balancedbaggingclassifier__base_estimator__min_data_in_leaf\": [30, 40, 50],\n#                                     \"balancedbaggingclassifier__base_estimator__early_stopping_round\": [0, 50],\n#                                     \"balancedbaggingclassifier__base_estimator__eval_metric\": [\"binary_logloss\"],\n#                                     \"balancedbaggingclassifier__base_estimator__feature_fraction\": [0.8, 0.9, 1],\n                                    \"balancedbaggingclassifier__base_estimator__feature_fraction\": [1],\n#                                     \"balancedbaggingclassifier__base_estimator__bagging_fraction\": [0.8, 0.9, 1],\n                                    \"balancedbaggingclassifier__base_estimator__bagging_fraction\": [1],\n                                    \n                                    \"balancedbaggingclassifier__n_estimators\": [20,]\n                                },\n#                                 {\n#                                     \"simpleimputer__strategy\": [\"mean\", \"median\"],\n                                    \n#                                     \"pandashighnullratiofeaturesdropper__null_ratio_threshold\": [0.25, 0.5, 0.75],\n#                                     \"pandascorrelatedfeaturesdropper__corr_threshold\": [0.8, 0.9, 1],\n                                    \n#                                     \"genericunivariateselect__mode\": [\"fdr\", \"fwe\"],\n#                                     \"genericunivariateselect__param\": [0.05, 0.01, 0.1],\n                                    \n# #                                     \"lgbmclassifier__boosting\": [\"gbdt\", \"dart\", \"goss\"],\n# #                                     \"lgbmclassifier__num_iterations\": [100, 200, 500, 1000],\n#                                     \"balancedbaggingclassifier__base_estimator__boosting\": [\"gbdt\", \"dart\", \"goss\"],\n#                                     \"balancedbaggingclassifier__base_estimator__num_iterations\": [100, 200, 500, 1000],\n#                                 }\n                            ],\n                            cv=3,\n                            scoring=scorer)","4b0b6b41":"if USE_LGB:\n    searcher_lgb.fit(df_train, df_train[\"class\"])","c8951bcd":"if USE_LGB:\n    print(searcher_lgb.best_params_)","a75ac54d":"if USE_LGB:\n    print(searcher_lgb.score(df_train, df_train[\"class\"]))","675cba64":"if USE_LGB:\n    print(searcher_lgb.score(df_test, df_test[\"class\"]))","c0722479":"if USE_LGB:\n    score_train = challenge_metric(y_pred=searcher_lgb.predict(df_train),\n                                   y_true=df_train[\"class\"])\n    print(\"competition score (train set):\", score_train)\n    score_test = challenge_metric(y_pred=searcher_lgb.predict(df_test),\n                                  y_true=df_test[\"class\"])\n    print(\"competition score (test set) :\", score_test)\n    \n    # Save model\n    joblib.dump(searcher_lgb, 'searcher_lgb-{}-{}.joblib'.format(score_train, score_test))","6ee57972":"We have quite a few features over <b>70%<\/b> null ratio! We will be integrating dropping of high null ratio features into cross-validation grid search later.","7170e064":"Thanks for letting us know feature \"<b>cd_000<\/b>\" is useless. Useless as it does not help to discriminate the target class.","cd6c0ed0":"We will be using the negative challenge metric as the scorer in sklearn cross-validation grid search instead of the usual accuracy.","15191fd1":"#### Target Class Distribution\nLet's have a look at the target class distribution.","5000850b":"### Imports","d9ef850c":"Seems like we have a highly <b>imbalanced<\/b> dataset! We will be using ensembles of classifiers with random under\/over sampling to handle this directly in the cross-validation grid search later.\n<br>\n<br>\nAn ensemble of classifiers is required to avoid sampling the wrong stuffs if we do resampling only once.","de85353a":"We can see some blocks of highly correlated features, we will try using cross-validation grid search to see if removing highly correlated features helps.","48f875ad":"### Data Loading\n<hr>\nLoad data from the csv files located in the \"<b>input<\/b>\" directory.","479b516e":"Here are the top five important features according to the model.","db487d63":"Let's see what univariate feature selection says regarding feature importances.","c0eba8a3":"Let's fit a random forest and see what it thinks are the importance features.","5b31dfd4":"Compute the feature correlation matrix.","322356b9":"First, let's have a look at some statistics.","7b12eeb6":"Load trained model","ea2caf56":"Confusion matrix","a3bb626e":"Feature importances according to chi2, ANOVA F-value, and mutual information","1ae8d293":"#### Lightgbm","bb0530c1":"Best model parameters after a grid search","e1419188":"#### Grid Search\nHere this is the code for cross-validation grid search for the model Lightgbm.","5fb0c877":"### Model Development\n<hr>\nWe will be integrating the following functions into cross-validation grid search to find the best model:\n<ol>\n    <li><b>Pre-processing<\/b> (dropping of high null ratio features, imputation, etc)<\/li>\n    <li><b>Feature engineering<\/b> (creating new features by marking null values, etc)<\/li>\n    <li><b>Feature selection<\/b> (select best features based on univariate statistical tests, dimensionality reduction techniques (PCA) etc)<\/li>\n    <li><b>Handling target class imbalance<\/b> (ensembles of balanced random sampling, etc)<\/li>\n    <li><b>Model hyperparameter tuning<\/b><\/li>\n<\/ol>\nThe reason for putting everything in the cross-validation grid search is so we can collectlively optimize all the parameters, e.g, what feature null ratio before dropping feature, what imputation strategy, etc.\n<br>\n<br>\nWe will be saving trained models in the following format \"<b>searcher_model-{TRAIN_SCORE}-{TEST_SCORE}.joblib<\/b>\".\nI have put trained models in the \"<b>models<\/b>\" directory.\nTo load a particular model, do: <b>model = joblib.load(\"{MODEL_JOBLIB_FILE}\")<\/b>\n<br>\n<br>\nDue to extremely long computation time, I have splitted model training codes into separate jupyter notebooks to allow for parallel computation.\n<br>\n<br>\nI will be focusing the following models:\n<ul>\n    <li><b>xgboost<\/b>: Winners of many Kaggle challanges<\/li>\n    <li><b>lightgbm<\/b>: Same as above<\/li>\n    <li><b>Random Forest<\/b><\/li>\n    <li><b>Neural Network<\/b><\/li>\n<\/ul>\nI will not be using the following models:\n<ul>\n    <li>K Nearest Neighbors: too slow for grid search<\/li>\n    <li>Support Vector Machine: too slow for grid search<\/li>\n<\/ul>","98e1bd7b":"#### Feature Importances","cee8edae":"Let's plot a scatter matrix of the top 5 most important features according to the model.","a615e38d":"List model parameters","ffac7500":"#### Null Ratio\nLets have a look at the null ratio of the features.","75f958b9":"### Configurations","e48f5e13":"#### Correlation\nLet's have a look at the feature correlations.","f4e745a0":"### Exploration\n<hr>\nWe will be using mostly plotly for visualizations due to its interactivity.","8e90fd70":"Convert \"<b>class<\/b>\" column to binary"}}