{"cell_type":{"36dcb203":"code","cb5b667b":"code","1f4c7d65":"code","33cf94fd":"code","4bcc6bc4":"code","2f0e3e72":"code","19113a0d":"code","524b097f":"code","0e30b8c7":"code","e5fe9cd0":"code","42e6f04f":"code","9a4162f9":"code","60a89212":"code","b6ee8026":"code","0932976b":"code","5a46d343":"code","e238df3d":"code","86550e95":"code","0f23d0ca":"code","5e8f7742":"code","d936e68a":"code","1734918d":"code","05d0e8c2":"code","75b2b4fe":"code","881b0428":"code","764c7460":"code","b619e714":"code","c16a7aeb":"code","24261927":"code","94e4607f":"code","50b1682f":"markdown","57b2422a":"markdown","98a6f194":"markdown","67e19d4f":"markdown","91c9c749":"markdown","6e627917":"markdown","f3f42a1a":"markdown","e5689f42":"markdown","9f471860":"markdown","6222ff3b":"markdown","bf0d9a8a":"markdown","e948b2bc":"markdown","49c51332":"markdown","3dcbfd4a":"markdown","1ff73da9":"markdown","9c92eb53":"markdown","2d45b160":"markdown","e11bcf55":"markdown","30d9ea19":"markdown","05edfdca":"markdown","a35de094":"markdown","5a8d9b0c":"markdown","be6993a8":"markdown","2b876af3":"markdown","26f75956":"markdown","e59a77d7":"markdown","bd4f4feb":"markdown","cd94cd7a":"markdown","4ddb83af":"markdown","9686d817":"markdown"},"source":{"36dcb203":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport gc\nimport copy\nimport pandas as pd\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom sklearn.model_selection import (train_test_split, \n                                     KFold, \n                                     StratifiedKFold, \n                                     StratifiedShuffleSplit)\n\nfrom sklearn.metrics import mean_squared_error, accuracy_score\n\nimport transformers\nfrom transformers import get_linear_schedule_with_warmup\n\nprint(transformers.__version__)\nnp.random.seed(42)","cb5b667b":"!cp ..\/input\/huggingfacemodelsnew\/roberta_base_tokenizer\/* .\n!cp ..\/input\/huggingfacemodelsnew\/roberta_base_model\/* .","1f4c7d65":"class CFG:\n    stratified = True\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained('.')\n    bert_model = transformers.AutoModel.from_pretrained('.')\n    max_length = 300\n    batch_size = 32\n    dropout = 0.3 # [None, float]\n    fc_dim = None # [None, int]\n    pool = 'CLS' # ['avg', 'CLS']","33cf94fd":"class TextDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer, mode=\"train\", max_length=None):\n        self.dataframe = dataframe\n        if mode != \"test\":\n            self.targets = dataframe['target'].values\n        self.encodings = tokenizer(list(dataframe['excerpt'].values), \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        self.mode = mode\n        \n        \n    def __getitem__(self, idx):\n        # putting each tensor in front of the corresponding key from the tokenizer\n        # HuggingFace tokenizers give you whatever you need to feed to the corresponding model\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n        # when testing, there are no targets so we won't do the following\n        if self.mode != \"test\":\n            item['labels'] = torch.tensor(self.targets[idx]).float()\n        return item\n    \n    def __len__(self):\n        return len(self.dataframe)","4bcc6bc4":"def make_loaders(dataframe, tokenizer, mode=\"train\", max_length=CFG.max_length):\n    dataset = TextDataset(dataframe, tokenizer, mode, max_length=max_length)\n    dataloader = torch.utils.data.DataLoader(dataset, \n                                             batch_size=CFG.batch_size, \n                                             shuffle=True if mode == \"train\" else False,\n                                             num_workers=2)\n    return dataloader","2f0e3e72":"class CustomModel(nn.Module):\n    def __init__(self,\n                 bert_model,\n                 bert_hidden_dim=768,\n                 fc_dim=CFG.fc_dim,\n                 pool=CFG.pool,\n                 dropout=CFG.dropout):\n        \n        super().__init__()\n        self.pool = pool\n        self.bert_model = bert_model\n        self.head = nn.Sequential(\n            nn.Dropout(dropout) if dropout else nn.Identity(),\n            nn.Linear(bert_hidden_dim, fc_dim) if fc_dim else nn.Identity(),\n            nn.Dropout(dropout) if fc_dim and dropout else nn.Identity(),\n            nn.Linear(fc_dim, 1) if fc_dim else nn.Linear(bert_hidden_dim, 1)\n        )\n    \n    def forward(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], \n                                 attention_mask=batch['attention_mask'])\n        \n        last_hidden_state = output.last_hidden_state\n        if self.pool == \"CLS\":\n            features = last_hidden_state[:, 0, :]\n        elif self.pool == \"avg\":\n            features = last_hidden_state.mean(dim=1)\n            \n        logits = self.head(features)\n        return logits, features","19113a0d":"def load_model(fold, mode=\"score\"):\n    model = CustomModel(CFG.bert_model).to(CFG.device)\n    mode = \"scoring_model\" if mode == 'score' else \"loss\"\n    model.load_state_dict(torch.load(f'..\/input\/commonlitrobertabase\/best_{mode}_fold_{fold}.pt',\n                                     map_location=CFG.device))\n    model.eval()\n    return model","524b097f":"def make_folds(n_splits=5, stratified=CFG.stratified):\n    df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\n    print(f\"Building stratified folds: {stratified}\")\n    if stratified:\n        bins = int(np.floor(1 + np.log2(len(df))))\n        df['target_bin_label'] = pd.cut(df['target'].values, bins, labels=range(bins))\n\n        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        for i, (_, valid_idx) in enumerate(kfold.split(X=df, y=df['target_bin_label'].astype(int))):\n            df.loc[valid_idx, 'fold'] = int(i)\n    else:\n        \n        kfold = KFold(n_splits=n_splits, shuffle=True, random_state=123)\n        for i, (_, valid_idx) in enumerate(kfold.split(df)):\n            df.loc[valid_idx, 'fold'] = int(i)\n    \n    return df","0e30b8c7":"def get_train_valid(df, fold):\n    train_df = df[df['fold'] != fold].reset_index(drop=True)\n    valid_df = df[df['fold'] == fold].reset_index(drop=True)\n    return train_df, valid_df","e5fe9cd0":"df = make_folds()","42e6f04f":"fold = 4\n\n_, valid_df = get_train_valid(df, fold=fold)\nvalid_loader = make_loaders(valid_df, CFG.tokenizer, \"valid\")\nmodel = load_model(fold)\nfold_preds = []\nfold_features = []\nwith torch.no_grad():\n    for batch in tqdm(valid_loader):\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        preds, features = model(batch)\n        fold_preds.append(preds)\n        fold_features.append(features)\n\nfold_preds = torch.cat(fold_preds).cpu()\nfold_features = torch.cat(fold_features, dim=0).cpu()\nprint(fold_preds.shape, fold_features.shape)","9a4162f9":"mean_squared_error(fold_preds.numpy(), \n                   df[df['fold'] == fold].target, \n                   squared=False)","60a89212":"num_bins = 10\ndf['class_label'], bins = pd.cut(df['target'].values, num_bins, labels=range(num_bins), retbins=True)\nmain_labels = df['class_label']\ndf['idx'] = list(range(len(df)))","b6ee8026":"df['preds'] = \"nan\"\ndf['preds_class_label'] = \"nan\"\ndf.loc[df['fold'] == fold, 'preds'] = fold_preds.numpy()\ndf.loc[df['fold'] == fold, 'preds_class_label'] = pd.cut(fold_preds.numpy().reshape(-1), \n                                                         bins=bins, labels=range(num_bins))","0932976b":"def accuracy(preds, targets):\n    return np.mean((preds == targets).astype(float))\n\naccuracy(df.loc[df['fold'] == fold, 'preds_class_label'].values, \n         df.loc[df['fold'] == fold, 'class_label'].astype(int).values)","5a46d343":"augmented = pd.read_csv(\"..\/input\/commonliteasydataaugmentation\/eda_data.txt\", \n                        sep=\"\\t\", \n                        header=None, \n                        names=['idx', 'excerpt'])\nprint(augmented.shape, augmented.shape[0] \/\/ df.shape[0])\naugmented.head()","e238df3d":"to_drop = [i for i in range(9, len(augmented), 10)]\naugmented_only = augmented.drop(to_drop).reset_index(drop=True)","86550e95":"augmented_only = pd.merge(augmented_only, df[['idx', 'class_label', 'fold', 'target']], \n                          how='left', on='idx')\naugmented_only.head()","0f23d0ca":"_, valid_df = get_train_valid(augmented_only, fold=fold)\nvalid_loader = make_loaders(valid_df, CFG.tokenizer, \"valid\")\nmodel = load_model(fold)\naugmented_fold_preds = []\naugmented_fold_features = []\nwith torch.no_grad():\n    for batch in tqdm(valid_loader):\n        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n        preds, features = model(batch)\n        augmented_fold_preds.append(preds)\n        augmented_fold_features.append(features)\n            \naugmented_fold_preds = torch.cat(augmented_fold_preds).cpu()\naugmented_fold_features = torch.cat(augmented_fold_features, dim=0).cpu()","5e8f7742":"mean_squared_error(augmented_fold_preds.numpy(), \n                   augmented_only[augmented_only['fold'] == fold].target, \n                   squared=False)","d936e68a":"augmented_only['preds'] = \"nan\"\naugmented_only['preds_class_label'] = \"nan\"\naugmented_only.loc[augmented_only['fold'] == fold, 'preds'] = augmented_fold_preds.numpy()\naugmented_only.loc[augmented_only['fold'] == fold, 'preds_class_label'] = pd.cut(augmented_fold_preds.numpy().reshape(-1), \n                                                                                 bins=bins, labels=range(num_bins))","1734918d":"accuracy(augmented_only.loc[augmented_only['fold'] == fold, 'preds_class_label'], \n         augmented_only.loc[augmented_only['fold'] == fold, 'class_label'].astype(int))","05d0e8c2":"import cudf, cuml\nimport cupy as cp\n\nfrom cuml.manifold import TSNE, UMAP\nimport matplotlib.pyplot as plt","75b2b4fe":"fold_labels = df[df['fold'] == fold]['class_label'].values\naugmented_fold_labels = augmented_only[augmented_only['fold'] == fold]['class_label'].values","881b0428":"%%time\ntsne = TSNE(n_components=2, perplexity=10, random_state=42)\ntrain_2D = tsne.fit_transform(fold_features.numpy())\n\nmarkers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\nplt.rc('legend',**{'fontsize':10})\nclasses_to_visual = list(set(fold_labels))\nC = len(classes_to_visual)\nwhile True:\n    if C <= len(markers):\n        break\n    markers += markers\n\nclass_ids = dict(zip(classes_to_visual, range(C)))\n\nplt.figure(figsize=(10, 10), facecolor='white')\n\nfor c in classes_to_visual:\n    idx = np.array(fold_labels) == c\n    plt.plot(train_2D[idx, 0], train_2D[idx, 1], linestyle='None', alpha=1, marker=markers[class_ids[c]],\n             markersize=10, label=c)\nlegend = plt.legend(loc='upper right', shadow=True, title=\"class id\")\nplt.title(\"Main Datapoints\")\nplt.axis(\"off\")\nplt.show()","764c7460":"%%time\n\noffset = 7\nidxs = [i for i in range(offset, len(augmented_fold_features), 9)]\n# idxs = [i for i in range(0, len(augmented_fold_features))]\n\ntsne = TSNE(n_components=2, perplexity=10, random_state=42)\ntrain_2D = tsne.fit_transform(augmented_fold_features[idxs, :].numpy())\n\nmarkers = [\"o\", \"v\", \"8\", \"s\", \"p\", \"*\", \"h\", \"H\", \"+\", \"x\", \"D\"]\nplt.rc('legend',**{'fontsize':10})\nclasses_to_visual = list(set(augmented_fold_labels))\nC = len(classes_to_visual)\nwhile True:\n    if C <= len(markers):\n        break\n    markers += markers\n\nclass_ids = dict(zip(classes_to_visual, range(C)))\n\nplt.figure(figsize=(10, 10), facecolor='white')\n\nfor c in classes_to_visual:\n    idx = np.array(augmented_fold_labels[idxs]) == c\n    plt.plot(train_2D[idx, 0], train_2D[idx, 1], linestyle='None', alpha=1, marker=markers[class_ids[c]],\n             markersize=10, label=c)\nlegend = plt.legend(loc='upper right', shadow=True, title=\"class id\")\nplt.title(\"Augmented Datapoints\")\nplt.axis(\"off\")\nplt.show()","b619e714":"selected_labels = [0, 8] # I chose some obviously far apart classes\nmask = (fold_labels == selected_labels[0]) + (fold_labels == selected_labels[1])\nselected_fold_features = fold_features[mask, :]\nselected_fold_labels = fold_labels[mask]\nprint(selected_fold_features.shape, selected_fold_labels.shape)\n\nmask = (augmented_fold_labels[idxs] == selected_labels[0]) + (augmented_fold_labels[idxs] == selected_labels[1])\nselected_augmented_fold_features = augmented_fold_features[idxs][mask, :]\nselected_augmented_fold_labels = augmented_fold_labels[idxs][mask]\nprint(selected_augmented_fold_features.shape, selected_augmented_fold_labels.shape)","c16a7aeb":"perplexity = 2\ntsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\ntrain_class_0 = tsne.fit_transform(selected_fold_features[selected_fold_labels == selected_labels[0], :].numpy())\nperplexity = 5\ntsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\ntrain_class_1 = tsne.fit_transform(selected_fold_features[selected_fold_labels == selected_labels[1], :].numpy())","24261927":"perplexity = 2\ntsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\naugmented_class_0 = tsne.fit_transform(selected_augmented_fold_features[selected_augmented_fold_labels == selected_labels[0], :].numpy())\nperplexity = 5\ntsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\naugmented_class_1 = tsne.fit_transform(selected_augmented_fold_features[selected_augmented_fold_labels == selected_labels[1], :].numpy())","94e4607f":"plt.figure(figsize=(10, 10), facecolor='white')\n\n# , \"v\", \"8\", \"s\"\nplt.plot(train_class_0[:, 0], train_class_0[:, 1], linestyle='None', alpha=1, marker=\"o\",\n         markersize=10, label='main_class_0')\n\nplt.plot(train_class_1[:, 0], train_class_1[:, 1], linestyle='None', alpha=1, marker=\"v\",\n         markersize=10, label='main_class_1')\n\nplt.plot(augmented_class_0[:, 0], augmented_class_0[:, 1], linestyle='None', alpha=1, marker=\"8\",\n         markersize=10, label='augmented_class_0')\n\nplt.plot(augmented_class_1[:, 0], augmented_class_1[:, 1], linestyle='None', alpha=1, marker=\"+\",\n         markersize=10, label='augmented_class_1')\n    \n    \nlegend = plt.legend(loc='upper right', shadow=True, title=\"mode\/class\")\nplt.title(\"Plotting main vs augmented datapoints of two specific classes\")\nplt.axis(\"off\")\nplt.show()","50b1682f":"Its very tricky how to interpret this result: it seems that the representations of the augmented view are near the main one for individual classes but I don't encourage you to accept this hypothesis as the randomness and perplexity change the plot dramatically and tSNE itslef is not a very relibale tool for this purpose. But I think you agree that the results seem interesting :)","57b2422a":"Okay, thanks if you've stayed with me until this part. Here I got some excitement after the disappointing results before. I don't know how much we can trust the following results but I wanted to do some experiments like the ones in the paper to see if EDA hurts the embeddings of the model.","98a6f194":"I wanted to something similar here: to check if the hidden_states of two specific labels for the main data and augmented data lie near each other.","67e19d4f":"## Config","91c9c749":"As you see, for the main datapoints it seems that the model is powerful enough to cluster really positive targets far from really negative ones. You can also see horizontal splits for other classes in somewhat a good order","6e627917":"## The Cool part: magic with tSNE","f3f42a1a":"Here it is!!! <br>\nThis was the cool thing that I wanted to tell you about. Although the predictions of the model on the augmented version were far off the real targets (terrible RMSE and accuracy), here we see that the hidden_states of the model seem to be representative enough! <br>\nThey are able to separate the far classes (0, 1, 2 from 8, 9 for example) from each other and we see a somewhat similar pattern as the main data here.","e5689f42":"I'll drop the main versions as we have them in df","9f471860":"As you might be thinking now, I first thought that EDA will harm the readibilty and hence change the labels drastically; but I continued on the experiment and found interesting results. Stay tuned :)","6222ff3b":"assigning the same target value and fold for each augmented version of a sample","bf0d9a8a":"even worse! accuracy is near random guessing (we have 10 bins or class ~ 0.1 chance by randomly guessing)","e948b2bc":"## Folds","49c51332":"By the way, as noted [in this awesome blog post in Distilpub](https:\/\/distill.pub\/2016\/misread-tsne\/) interpreting tSNE results is tricky and we should be very careful about what conclusions we drive with it. My main goal to share this notebook was to spread the this idea of checking if augmentations hurt or not with these tools.\n\nIf you want to know more about tSNE, how to interpret the results and the perplexity hyperparameter you definitly want to check that blog post out.","3dcbfd4a":"## Checking two specific classes","1ff73da9":"### Augmented Datapoints\n\nHere I do the same for the augmented ones. As I said, we have 9 augmented versions of the main datapoints. Using the variable \"offset\" below I'm using only one augmneted version of each main datapoint and plotting it with tSNE","9c92eb53":"## Binning the target and predictions\n\nIn order to be able to plot the datapoints more easily, I'm binning the target values into 10 bins and turning it into a classification task. I'm also binning the predictions with the same bins returned by pandas","2d45b160":"## Dataset","e11bcf55":"Inspired by this plot from the paper:\n\n![](https:\/\/i.ibb.co\/7Qr1r2L\/Screenshot-2021-06-16-005757.png)","30d9ea19":"## Loading the augmented dataframe\n\n[In my other notebook](https:\/\/www.kaggle.com\/moeinshariatnia\/commonlit-easy-data-augmentation), I augmented the competitions data using EDA github repo and got 9 augmented versions (using the 4 methods descibed earlier) of each of the samples in the main data of competition; so, the augmented version is 10 (9 + 1) times the previous df. The order of \"augmented\" dataframe is as follows:\n\n1. idx, excerpt\n2. 0, augmented_version_0\n3. 0, augmented_version_1\n4. ...\n5. 0, augmented_version_9\n6. 0, main_version\n7. 1, augmented_version_0,\n8. 1, augmented_version_1\n9. ...\n10. 1, augmented_version_9\n11. 1, main_version","05edfdca":"## Imports","a35de094":"## Checking the accuracy of model on main data","5a8d9b0c":"## Intro\n\nIn this notebook, we are going to check whether augmenting the data changes model's interpretation of the texts dratsically. As the task of this competition is to predict texts readibility, we should be very careful about augmentations we use as they can simply change the target.\n\nTo do so, first we will get the hidden_states of a pretrained RoBERTa model (pretrained on the competition's task with simple RMSE regression) for the provided data by the host. Then, we will do the same and get hidden_states of multiple augmented versions of the provided data. \n\nIn this notebook I'm using **EDA** (**Easy Data Augmentation**) from paper: [EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks](https:\/\/arxiv.org\/abs\/1901.11196)\n\nThis augmentation technique uses four methods to augment the data which all are really simple:\n1. synonym replacement (SR)\n2. random insertion (RI)\n3. random swap (RS)\n4. random deletion (RD)\n\nLook at the figure below for examples of each method:\n\n![](https:\/\/i.ibb.co\/k6k5HdM\/Screenshot-2021-06-15-235611.png)","be6993a8":"Oh, it seems that augmenting the data doesn't that much perseve the target :( --> compare it with 0.5 RMSE of main data\n\nit's not that shocking actually because some of the techniques in EDA remove some words and add noise which obviously hurt the texts.\n\nbut let's keep on going to visualization part and see if the hidden_states are useless (or not!)","2b876af3":"## Model","26f75956":"## Getting Hidden Representations of Augmented Data","e59a77d7":"### Main Datapoints","bd4f4feb":"As you see, although the model had an RMSE of 0.5 on this fold, the accuracy is not that high. But that's okay because we need the classification part only for visualization purposes later in this notebook ","cd94cd7a":"Here, I will first convert the 768 dimensional output of RoBERTa to 2D in order to be able to visualiza it. Then, I'll tag each datapoint with its associated class (0, 1, 2, ..., 9) to recognize it better. As you know the classes are built with binning so that the most positive targets get a class of 9 and the most negative targets get a class of 0 and others in between get their classes in regarding their target\n\n0 --> most negative (-3.x) <br>\n9 --> most positive (+1.4x and above)","4ddb83af":"Here I'm loading one of my previously trained models on the competition's data with regression loss. In addition to predictions, I'm also returning the hidden_states of last layer for CLS token or avg of tokens (you can change this behaviour in CFG, but as the model is trained using CLS, it is used here as well)\n\nFor simplicity, I'm only getting the predictions and hidden_states for fold number 4 ignoring other samples.","9686d817":"## Getting Hidden Representations of Main Data"}}