{"cell_type":{"c1295b9a":"code","33c532e5":"code","4c2d37e0":"code","ecae2c25":"code","d6812d0c":"code","cf507986":"code","0fce61e5":"code","014965c9":"code","afe09281":"code","cebfa2b7":"code","c29a0163":"code","8a560bc5":"code","38153b9f":"code","ed8ba8f7":"code","f7189d1f":"code","b2bd558a":"code","ffd19ec8":"code","4fed8d51":"code","2101368d":"code","93fbe836":"code","46addf77":"code","370e95d3":"code","80e2bf1f":"code","913e0233":"code","787b39e6":"code","940f56fb":"code","facfd69b":"code","2086959f":"code","98761b07":"code","fc0b2320":"code","e7831d58":"code","547bbd43":"code","b3fa6595":"code","902c9129":"code","4dfffc68":"code","bca34ae9":"code","27544b1b":"code","c0f5d93f":"code","644c2c14":"code","c83873a0":"code","53f12bfa":"code","939eb03c":"code","43544417":"code","bbb8dd3a":"code","fc0a2910":"code","482e86e7":"code","d3cd6811":"code","b46d153f":"code","a246ce7a":"code","a3d23ff9":"code","1bf67cb8":"code","c547ffc3":"code","064f23bf":"code","b5aff0af":"code","5748a170":"code","94e106b0":"code","02832590":"code","e2fa0e49":"code","629e7b8e":"code","d53fcd4f":"code","9c5089cd":"code","77240586":"code","07fe97b0":"code","ff4dd061":"code","47ef4fc6":"code","b1a8eb8f":"code","78af7ff5":"code","004edd39":"code","bd788774":"code","fe95b49a":"code","b48abae5":"markdown","1d6086cf":"markdown","fc34fb62":"markdown","d11f9169":"markdown","372d1685":"markdown","1fd91fff":"markdown","33076612":"markdown","c119b621":"markdown","adb1ef04":"markdown","1cc0e795":"markdown","0ad08eb1":"markdown","550b9366":"markdown","0e63934f":"markdown","b3456fb8":"markdown","53238487":"markdown","4f1e70a7":"markdown","b97a9842":"markdown","00bf9ce5":"markdown","cdb4f1a1":"markdown","8f4ec02b":"markdown","577dea0a":"markdown","a78f8889":"markdown","db9013d5":"markdown","8df22e54":"markdown","878a4428":"markdown","7ffb4f70":"markdown","c9a85c1d":"markdown","911348d8":"markdown","c6a2bf21":"markdown","dd76d3f9":"markdown","c23120b2":"markdown","89df8c66":"markdown","55931967":"markdown","bf3e3b56":"markdown","d8cca10f":"markdown","86751f5e":"markdown","f6ddbaa7":"markdown"},"source":{"c1295b9a":"#loading dataset\nimport pandas as pd\nimport numpy as np\n#visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#EDA\nfrom collections import Counter\n# data preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# data splitting\nfrom sklearn.model_selection import train_test_split\n# data modeling\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n#ensembling\nimport warnings\nwarnings.filterwarnings(\"ignore\")","33c532e5":"data = pd.read_csv (r'..\/input\/heart-disease-uci\/heart.csv')","4c2d37e0":"df=data","ecae2c25":"df.head(5)","d6812d0c":"df.tail(5)","cf507986":"df.dtypes","0fce61e5":"duplicate_rows_df = df[df.duplicated()]\nprint(duplicate_rows_df.shape)","014965c9":"# Checking for null values:\n","afe09281":"print(df.isnull().sum())","cebfa2b7":"df.columns = ['age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol', 'fasting_blood_sugar', 'rest_ecg', 'max_heart_rate_achieved',\n       'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels', 'thalassemia', 'target']","c29a0163":"df.head(5)","8a560bc5":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"age\"])\nplt.show()","38153b9f":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"resting_blood_pressure\"])\nplt.show()","ed8ba8f7":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"cholesterol\"])\nplt.show()","f7189d1f":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"max_heart_rate_achieved\"])\nplt.show()","b2bd558a":"f, ax = plt.subplots(figsize=(8, 6))\nsns.boxplot(x=df[\"st_depression\"])\nplt.show()","ffd19ec8":"df.target.value_counts()","4fed8d51":"df.describe()","2101368d":"sns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()","93fbe836":"plt.figure( figsize= (25,25) )\ndf[['age'] ].hist(bins=10)\nplt.show()","46addf77":"plt.figure( figsize= (25,25) )\ndf[['resting_blood_pressure'] ].hist(bins=10)\nplt.show()","370e95d3":"plt.figure( figsize= (25,25) )\ndf[['cholesterol'] ].hist(bins=10)\nplt.show()","80e2bf1f":"plt.figure( figsize= (25,25) )\ndf[['max_heart_rate_achieved'] ].hist(bins=10)\nplt.show()","913e0233":"plt.figure( figsize= (25,25) )\ndf[['st_depression'] ].hist(bins=10)\nplt.show()","787b39e6":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"chest_pain_type\", data=df)\nplt.show()","940f56fb":"f, ax = plt.subplots(figsize=(10,6))\nx = df['max_heart_rate_achieved']\nax = sns.distplot(x, bins=10)\nplt.show()","facfd69b":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"rest_ecg\", data=df)\nplt.show()","2086959f":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"chest_pain_type\", hue=\"target\", data=df)\nplt.show()","98761b07":"f, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"max_heart_rate_achieved\", data=df, jitter = 0.01)\nplt.show()","fc0b2320":"f, ax = plt.subplots(figsize=(8, 6))\nsns.stripplot(x=\"target\", y=\"age\", data=df, jitter = 0.01)\nplt.show()","e7831d58":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"age\", y=\"resting_blood_pressure\", data=df)\nplt.show()","547bbd43":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"age\", y=\"cholesterol\", data=df)\nplt.show()","b3fa6595":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.regplot(x=\"cholesterol\", y=\"max_heart_rate_achieved\", data=df)\nplt.show()","902c9129":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","4dfffc68":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"sex\", hue=\"target\", data=df)\nplt.show()","bca34ae9":"f, ax = plt.subplots(figsize=(8, 6))\nax = sns.countplot(x=\"fasting_blood_sugar\", hue=\"target\", data=df)\nplt.show()","27544b1b":"pd.crosstab(df.st_depression,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for ST depression')\nplt.xlabel('ST_Depression')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndSTdepression.png')\nplt.show()","c0f5d93f":"pd.crosstab(df.resting_blood_pressure,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Resting Blood Pressure')\nplt.xlabel('Resting Blood Pressure')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndRestingbps.png')\nplt.show()","644c2c14":"num_var = ['age', 'resting_blood_pressure', 'cholesterol', 'max_heart_rate_achieved', 'st_depression', 'target' ]\nsns.pairplot(df[num_var], kind='scatter', diag_kind='hist')\nplt.show()","c83873a0":"plt.figure(figsize=(16,12))\nplt.title('Correlation Heatmap of Heart Disease Dataset')\na = sns.heatmap(df.corr(), square=True, annot=True, fmt='.2f', linecolor='white')\na.set_xticklabels(a.get_xticklabels(), rotation=90)\na.set_yticklabels(a.get_yticklabels(), rotation=30)\nplt.show()\n","53f12bfa":"corr_matrix = df.corr()\ncorr_matrix['target'].sort_values( ascending = False )","939eb03c":"y = data[\"target\"]\nX = data.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","43544417":"print(y_test.unique())\nCounter(y_train)","bbb8dd3a":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","fc0a2910":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))","482e86e7":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': rf.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'], color = 'blue')\nplt.show()","d3cd6811":"m6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))","b46d153f":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': dt.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'blue')\nplt.show()","a246ce7a":"m4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=500, max_depth=10,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=28,  \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))","a3d23ff9":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': xgb.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'blue')\nplt.show()","1bf67cb8":"m1 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))","c547ffc3":"m2 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))","064f23bf":"m5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))","b5aff0af":"from sklearn.svm import SVC\nm7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))","5748a170":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','Extreme Gradient Boost',\n                    'K-Nearest Neighbour','Decision Tree','Support Vector Machine'], 'Accuracy': [lr_acc_score*100,\n                    nb_acc_score*100,rf_acc_score*100,xgb_acc_score*100,knn_acc_score*100,dt_acc_score*100,svc_acc_score*100]})\nmodel_ev","94e106b0":"from mlxtend.classifier import StackingCVClassifier","02832590":"scv=StackingCVClassifier(classifiers=[rf,knn,svc],meta_classifier= knn,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","e2fa0e49":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","629e7b8e":"scv=StackingCVClassifier(classifiers=[rf,xgb,svc],meta_classifier= knn,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","d53fcd4f":"scv=StackingCVClassifier(classifiers=[xgb,knn,svc],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","9c5089cd":"scv=StackingCVClassifier(classifiers=[rf,knn,xgb],meta_classifier= xgb,random_state=42)\nscv.fit(X_train,y_train)\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))","77240586":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\n  \n# print prediction results\npredictions = model.predict(X_test)\nprint(classification_report(y_test, predictions))","07fe97b0":"from sklearn.model_selection import GridSearchCV\n# defining parameter range\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf']} \n  \ngrid = GridSearchCV(XGBClassifier(), param_grid, refit = True, verbose = 3)\n  \n# fitting the model for grid search\ngrid.fit(X_train, y_train)","ff4dd061":"# print best parameter after tuning\nprint(grid.best_params_)\n  \n# print how our model looks after hyper-parameter tuning\nprint(grid.best_estimator_)","47ef4fc6":"grid_predictions = grid.predict(X_test)\n  \n# print classification report\nprint(classification_report(y_test, grid_predictions))","b1a8eb8f":"import plotly.express as px\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = XGBClassifier()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\nfpr, tpr, thresholds = roc_curve(y, y_score)\n\n# The histogram of scores compared to true labels\nfig_hist = px.histogram(\n    x=y_score, color=y, nbins=50,\n    labels=dict(color='True Labels', x='Score')\n)\n\nfig_hist.show()\n\n\n# Evaluating model performance at various thresholds\ndf = pd.DataFrame({\n    'False Positive Rate': fpr,\n    'True Positive Rate': tpr\n}, index=thresholds)\ndf.index.name = \"Thresholds\"\ndf.columns.name = \"Rate\"\n\nfig_thresh = px.line(\n    df, title='TPR and FPR at every threshold',\n    width=700, height=500\n)\n\nfig_thresh.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig_thresh.update_xaxes(range=[0, 1], constrain='domain')\nfig_thresh.show()","78af7ff5":"import plotly.express as px\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import precision_recall_curve, auc\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=500, random_state=0)\n\nmodel = XGBClassifier()\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","004edd39":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","bd788774":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = KNeighborsClassifier(n_neighbors=10)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","fe95b49a":"X, y = make_classification(n_samples=500, random_state=0)\n\nmodel = StackingCVClassifier(classifiers=[rf,knn,svc],meta_classifier= xgb,random_state=42)\nmodel.fit(X, y)\ny_score = model.predict_proba(X)[:, 1]\n\nprecision, recall, thresholds = precision_recall_curve(y, y_score)\n\nfig = px.area(\n    x=recall, y=precision,\n    title=f'Precision-Recall Curve (AUC={auc(fpr, tpr):.4f})',\n    labels=dict(x='Recall', y='Precision'),\n    width=700, height=500\n)\nfig.add_shape(\n    type='line', line=dict(dash='dash'),\n    x0=0, x1=1, y0=1, y1=0\n)\nfig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\nfig.update_xaxes(constrain='domain')\n\nfig.show()","b48abae5":"# Bivariate Analysis:\n","1d6086cf":"# Feature importance","fc34fb62":"# Random forest + KNN + SVM\n","d11f9169":"# GridSearchCV","372d1685":"# ROC curve","1fd91fff":"# Outlier detection:\n","33076612":"# Correlation Matrix:\n","c119b621":"# KNN-\nIt is a lazy learning algorithm that stores all instances corresponding to training data in n-dimensional space. It is a lazy learning algorithm as it does not focus on constructing a general internal model, instead, it works on storing instances of training data.","adb1ef04":"# Logistic Regression-\nIt is a classification algorithm in machine learning that uses one or more independent variables to determine an outcome. The outcome is measured with a dichotomous variable meaning\u00a0it will have only two possible outcomes.\n","1cc0e795":"# Data split","0ad08eb1":"# Statistical details from the data:\n","550b9366":"# Decision Tree-\nDecision Trees predict the value of a target variable by learning simple decision rules inferred from the input data features. The decision tree algorithm builds the classification model in the form of a tree structure. It utilizes the if-then rules which are equally exhaustive and mutually exclusive in classification.\n","0e63934f":"# Standardization","b3456fb8":"# Random Forest-\nIt is an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputs the class that is the mode of the classes or classification or mean prediction(regression) of the individual trees.\n","53238487":"# PR curve for Stacking- Random forest, KNN, SVM","4f1e70a7":"# Xgboost + KNN + SVM","b97a9842":"# Random forest + KNN + Xgboost","00bf9ce5":"# Feature importance","cdb4f1a1":"# SVM-\nIt is a lazy learning algorithm that stores all instances corresponding to training data in n-dimensional space. It is a lazy learning algorithm as it does not focus on constructing a general internal model, instead, it works on storing instances of training data.\n","8f4ec02b":"# PR curve for Random Forest","577dea0a":"# Checking the datatypes and shape of the data:\n","a78f8889":"# Xgboost + KNN + SVM","db9013d5":"# Random forest + XGB + SVM","8df22e54":"# PR curve for Xgboost","878a4428":"# PR curve for SVM","7ffb4f70":"# Feature importance","c9a85c1d":"# PR curve for KNN","911348d8":"# Univariate Analysis:\n","c6a2bf21":"# Multivariate Analysis:\n","dd76d3f9":"# **Import Libraries**","c23120b2":"# Checking for duplicate rows:\n","89df8c66":"# Model Comparison","55931967":"# Xgboost-\nXGBoost is a popularly used algorithm which seeks to push the limit of computational resources used in conventional boosted trees. XGBoost is short for \u201cExtreme Gradient Boosting\u201d and is based on the original gradient boosting model\nThe key difference between the two is that XGBoost uses a more regularized model formalization to control over-fitting, giving it better performance.XGBoost uses a combination of bagging and boosting which can reduce variance and bias respectively.","bf3e3b56":"# Naive Bayes-\nIt is a classification algorithm based on Bayes\u2019s theorem which gives an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n","d8cca10f":"# Ensemble technique-Model Stacking-\nStacking, also known as stacked generalization, is an ensemble method where the models are combined using another machine learning algorithm. The basic idea is to train machine learning algorithms with training dataset and then generate a new dataset with these models. Then this new dataset is used as input for the combiner machine learning algorithm.","86751f5e":"# Import data","f6ddbaa7":"# Renaming columns:\n"}}