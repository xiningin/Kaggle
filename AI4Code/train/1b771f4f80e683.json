{"cell_type":{"477d194a":"code","08906220":"code","52a8269a":"code","7933064c":"code","1e9959b2":"code","31028810":"code","9da82e25":"code","b1504653":"code","939e7709":"code","29a6bb13":"code","59cbe312":"code","5cce6721":"code","5fbd020c":"code","b4b3a20e":"code","b932492e":"code","170aa8a0":"code","9ccb71a8":"code","a23e5a50":"code","a839759d":"code","91be6ee6":"code","e74ab04d":"code","ebfb3e91":"code","e1718f6c":"code","0dc87f01":"code","0d2844b3":"code","12c530d0":"code","87601741":"code","4cb2f689":"code","1c8b7132":"code","edf86f56":"code","95199714":"code","fe9b158c":"markdown","79cd44d7":"markdown","94ff61eb":"markdown","79debd2a":"markdown","f3f5e77a":"markdown","9034d92b":"markdown","6f7db75c":"markdown","d280ac9f":"markdown","f2ebaa6e":"markdown","5f07d243":"markdown","34fd4136":"markdown","8f16b8bb":"markdown","d60cdeb8":"markdown","06d7cfe0":"markdown","eedca888":"markdown","78a49242":"markdown","9d39370f":"markdown","8ce40c62":"markdown","e97a90de":"markdown","45f509e5":"markdown","070dce2a":"markdown","64f37560":"markdown","13986435":"markdown","091f7ed0":"markdown","3bd0d88d":"markdown","70c25d9e":"markdown","48ffc844":"markdown","72a7bbb3":"markdown","14af7e47":"markdown","944b61dc":"markdown","fc05bbf4":"markdown","cfe9b0e6":"markdown","6721e1be":"markdown","5677647f":"markdown","902af560":"markdown","0d32e13b":"markdown","82e5632e":"markdown","d40b2c64":"markdown","f310f95a":"markdown","67aea531":"markdown","af515ea3":"markdown","441cbc66":"markdown","f4e49fba":"markdown","1dfa3449":"markdown"},"source":{"477d194a":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', 400)\n%matplotlib inline\n\n# Any results you write to the current directory are saved as output.","08906220":"train = pd.read_csv('..\/input\/train.csv', parse_dates=[\"first_active_month\"])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=[\"first_active_month\"])\n\ntrain.head(5)\n","52a8269a":"train_test = pd.concat([train, test], ignore_index=False, keys=['train', 'test'], sort=True)","7933064c":"print('Rows in train: {}'.format(len(train_test[['card_id']])))\nprint('Unique card_ids in train: {}'.format(len(train_test.card_id.value_counts())))\n\n# no issues regarding duplicate card_id's","1e9959b2":"print('train missing values:')\nprint(train_test.loc['train'].isnull().sum())\n\nprint('test missing values:')\nprint(train_test.loc['test'].isnull().sum())\n\nprint('missing value:')\nprint(train_test[train_test['first_active_month'].isnull()])","31028810":"# calculate mean of datetime series\n\nmean_date = (train_test.first_active_month - train_test.first_active_month.min()).mean() + train_test.first_active_month.min()\n\n# fill with mean\ntrain_test['first_active_month'] = train_test['first_active_month'].fillna(mean_date)","9da82e25":"# data pull seems to be 2018-04-30 as this is max in new_merchant_transactions\n\ntrain_test['Days_since_first_active'] = (pd.to_datetime('2018-04-30') - train_test['first_active_month']).dt.days\n\ntrain_test['Months_since_first_active'] = train_test['Days_since_first_active'] \/\/ 30 #floor division\n\n#clean dataset\n\nkeep_columns = ['card_id', 'first_active_month', 'feature_1', 'feature_2', 'feature_3', 'target', \n                'Months_since_first_active']\n\ntrain_test = train_test[keep_columns]","b1504653":"import seaborn as sns; sns.set() #set plot theme\nimport matplotlib.pyplot as plt\n\nprint(train_test.loc['train'].describe())\nprint(train_test.loc['test'].describe())","939e7709":"#feature_1\nfeature_1_train = train_test.loc['train'].feature_1.value_counts().sort_index()\nfeature_1_train = feature_1_train \/ len(train_test.loc['train'])\n\nfeature_1_test = train_test.loc['test'].feature_1.value_counts().sort_index()\nfeature_1_test = feature_1_test \/ len(train_test.loc['test'])\n\n#feature_2\nfeature_2_train = train_test.loc['train'].feature_2.value_counts().sort_index()\nfeature_2_train = feature_2_train \/ len(train_test.loc['train'])\n\nfeature_2_test = train_test.loc['test'].feature_2.value_counts().sort_index()\nfeature_2_test = feature_2_test \/ len(train_test.loc['test'])\n\n#feature_3\nfeature_3_train = train_test.loc['train'].feature_3.value_counts().sort_index()\nfeature_3_train = feature_3_train \/ len(train_test.loc['train'])\n\nfeature_3_test = train_test.loc['test'].feature_3.value_counts().sort_index()\nfeature_3_test = feature_3_test \/ len(train_test.loc['test'])\n\n#Months_since_first_active\nMonths_since_first_active_train = train_test.loc['train'].Months_since_first_active.value_counts().sort_index()\nMonths_since_first_active_train = Months_since_first_active_train \/ len(train_test.loc['train'])\n\nMonths_since_first_active_test = train_test.loc['test'].Months_since_first_active.value_counts().sort_index()\nMonths_since_first_active_test = Months_since_first_active_test \/ len(train_test.loc['test'])","29a6bb13":"fig, ax = plt.subplots(2, 2, figsize=(14,14))\n\n# =============================================================================\n# #plotting independt variables checking for consistency vs train and test data\n# =============================================================================\n\n#plotting feature_1\nax[0,0].bar(feature_1_train.index, feature_1_train, label='train', alpha=0.3)\nax[0,0].bar(feature_1_test.index, feature_1_test, label='test', alpha=0.3)\nax[0,0].legend()\nax[0,0].set_title('feature_1')\n\n#plotting feature_2\nax[0,1].bar(feature_2_train.index, feature_2_train, label='train', alpha=0.3)\nax[0,1].bar(feature_2_test.index, feature_2_test, label='test', alpha=0.3)\nax[0,1].legend()\nax[0,1].set_title('feature_2')\n\n#plotting feature_3\nax[1,0].bar(feature_3_train.index, feature_3_train, label='train', alpha=0.3)\nax[1,0].bar(feature_3_test.index, feature_3_test, label='test', alpha=0.3)\nax[1,0].legend()\nax[1,0].set_title('feature_3')\n\n#plotting Months_since_first_active\nax[1,1].bar(Months_since_first_active_train.index, Months_since_first_active_train, label='train', alpha=0.3)\nax[1,1].bar(Months_since_first_active_test.index, Months_since_first_active_test, label='test', alpha=0.3)\nax[1,1].legend()\nax[1,1].set_title('Months_since_first_active');","59cbe312":"f, ax = plt.subplots(figsize=(14,7))\nsns.distplot(train_test.loc['train'].target)\nplt.title('target distribution');","5cce6721":"#plotting feature_1 - feature_2 vs target\nfig, ((ax1, ax2)) = plt.subplots(nrows=1, ncols=2, figsize=(14,7))\n\nfig = sns.boxplot(x='feature_1', y='target', data=train_test.loc['train'],ax=ax1)\nax1.set_title('target distribution [feature_1]')\n\nsns.boxplot(x='feature_2', y='target', data=train_test.loc['train'],ax=ax2)\nax2.set_title('target distribution [feature_2]')\n\n#plotting feature_1 - feature_2 vs target\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nfig = sns.boxplot(x='feature_3', y='target', data=train_test.loc['train'])\nplt.title('target distribution [feature_3]')\n\n#plotting jointplot, taking every 60th observation to lower load time\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nfig = sns.swarmplot(x='Months_since_first_active', y='target', data=train_test.loc['train'::60])\nplt.title('Months_since_first_active vs. target')\nplt.xticks(rotation='90');","5fbd020c":"hist_transactions = pd.read_csv('..\/input\/historical_transactions.csv', parse_dates=['purchase_date'])\n\n#function found in https:\/\/www.kaggle.com\/fabiendaniel\/elo-world\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\nhist_transactions = reduce_mem_usage(hist_transactions)\n                                \nprint('missing values %:') \nprint(hist_transactions.isnull().sum() \/ len(hist_transactions))","b4b3a20e":"##### authorized_flag\nhist_transactions.authorized_flag.head()\nhist_transactions['authorized_flag'] = hist_transactions['authorized_flag'].map({'Y' : 1, 'N' : 0})\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['authorized_flag'].value_counts().index,\n            y = hist_transactions['authorized_flag'].value_counts() \/ len(hist_transactions),\n            order = hist_transactions['authorized_flag'].value_counts().index)\nplt.ylabel('freq %')\nplt.title('authorized_flag');","b932492e":"##### city_id\nhist_transactions.city_id.head(10)\nprint(hist_transactions.city_id.value_counts().head()) #-1 probalbly missing values, we treet them as a seperate group for now\n\nshow = 20\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['city_id'].value_counts().index[:show],\n            y = hist_transactions['city_id'].value_counts()[:show] \/ len(hist_transactions['city_id']),\n            order = hist_transactions['city_id'].value_counts().index[:show])\nplt.ylabel('freq %')\nplt.title('Most {}th frequent city_ids'.format(show))\n\n#cumulative barplot\n\ncity_id_freq = hist_transactions['city_id'].value_counts() \/ len(hist_transactions['city_id'])\n\ncity_id_cum = pd.DataFrame(city_id_freq)\ncity_id_cum.columns = ['city_id_freq']\n \ncity_id_cum = city_id_cum.reset_index()\n\ncum_var = np.empty((0,1),float)\n\nfor i in city_id_cum.index:\n    if i == 0:\n        cum_var = np.append(cum_var,(city_id_cum.loc[i,'city_id_freq']))\n    else: \n        cum_var = np.append(cum_var, (cum_var[i-1] + city_id_cum.loc[i,'city_id_freq']))\n\ncity_id_cum['city_id_cum'] = cum_var    \n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = city_id_cum['index'][:show],\n            y = city_id_cum['city_id_cum'][:show],\n            order = city_id_cum['index'][:show])\nplt.ylabel('freq %')\nplt.title('Most {}th frequent city_ids (cumulative)'.format(show)); #we see that a relatively small number of cities make up a large part of the transactions","170aa8a0":"##### state_id\nhist_transactions.state_id.head(10)\n\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['state_id'].value_counts().index,\n            y = hist_transactions['state_id'].value_counts() \/ len(hist_transactions['state_id']),\n            order = hist_transactions['state_id'].value_counts().index)\nplt.ylabel('freq %')\nplt.title('state_id')\n\n#cumulative barplot\n\nstate_id_freq = hist_transactions['state_id'].value_counts() \/ len(hist_transactions['state_id'])\n\nstate_id_cum = pd.DataFrame(state_id_freq)\nstate_id_cum.columns = ['state_id_freq']\n \nstate_id_cum = state_id_cum.reset_index()\n\ncum_var = np.empty((0,1),float)\n\nfor i in state_id_cum.index:\n    if i == 0:\n        cum_var = np.append(cum_var,(state_id_cum.loc[i,'state_id_freq']))\n    else: \n        cum_var = np.append(cum_var, (cum_var[i-1] + state_id_cum.loc[i,'state_id_freq']))\n\nstate_id_cum['state_id_cum'] = cum_var    \n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = state_id_cum['index'],\n            y = state_id_cum['state_id_cum'],\n            order = state_id_cum['index'])\nplt.ylabel('freq %')\nplt.title('state_id cummulative dist'); #we see that a relatively small number of states make up a large part of the transactions","9ccb71a8":"##### category_1\nhist_transactions.category_1.head()\nhist_transactions.category_1.unique()\n\nhist_transactions['category_1'] = hist_transactions['category_1'].map({'Y' : 1, 'N' : 0})\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['category_1'].value_counts().index,\n            y = hist_transactions['category_1'].value_counts() \/ len(hist_transactions['category_1']),\n            order = hist_transactions['category_1'].value_counts().index)\nplt.ylabel('freq %')\nplt.title('category_1')\n\n##### category_2\nhist_transactions.category_2.head()\nhist_transactions.category_2.value_counts(dropna=False)\n\nhist_transactions['category_2'] = hist_transactions.category_2.fillna(6.0)\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['category_2'].value_counts().index,\n            y = hist_transactions['category_2'].value_counts() \/ len(hist_transactions['category_2']),\n            order = hist_transactions['category_2'].value_counts().index)\nplt.ylabel('freq %')\nplt.title('category_2')\n\n\n##### category_3\nhist_transactions.category_3.head()\nhist_transactions.category_3.value_counts(dropna=False)\n\n#we see relatively small number of NAs. We handle them as a seperate group.\nhist_transactions['category_3'] = hist_transactions.category_3.fillna('NA')\n\nhist_transactions.category_3.value_counts(dropna=False)\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['category_3'].value_counts().index,\n            y = hist_transactions['category_3'].value_counts() \/ len(hist_transactions['category_3']),\n            order = hist_transactions['category_3'].value_counts().index)\nplt.ylabel('% freq')\nplt.title('category_3');\n","a23e5a50":"##### installments\nhist_transactions.installments.unique()\nprint('Values:')\nprint(hist_transactions.installments.value_counts()) \n\nhist_transactions['installments'] = hist_transactions.installments.replace({-1 : 0, 999 : 0})\nhist_transactions.installments.value_counts()\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['installments'].value_counts().index,\n            y = hist_transactions['installments'].value_counts() \/ len(hist_transactions['installments']),\n            order = hist_transactions['installments'].value_counts().index)\nplt.xlabel('# of installments')\nplt.ylabel('% freq')\nplt.title('installments');","a839759d":"##### merchant_category_id\nhist_transactions.merchant_category_id.value_counts().head(15)\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['merchant_category_id'].value_counts().index[:30],\n            y = hist_transactions['merchant_category_id'].value_counts()[:30] \/ len(hist_transactions['merchant_category_id']),\n            order = hist_transactions['merchant_category_id'].value_counts().index[:30])\nplt.ylabel('freq %')\nplt.xlabel('merchant_category_id')\nplt.title('merchant_category_id')\n\n#cumulative barplot\n\nmerchant_category_id_freq = hist_transactions['merchant_category_id'].value_counts() \/ len(hist_transactions['merchant_category_id'])\n\nmerchant_category_id_cum = pd.DataFrame(merchant_category_id_freq)\nmerchant_category_id_cum.columns = ['merchant_category_id_freq']\n \nmerchant_category_id_cum = merchant_category_id_cum.reset_index()\n\ncum_var = np.empty((0,1),float)\n\nfor i in merchant_category_id_cum.index:\n    if i == 0:\n        cum_var = np.append(cum_var,(merchant_category_id_cum.loc[i,'merchant_category_id_freq']))\n    else: \n        cum_var = np.append(cum_var, (cum_var[i-1] + merchant_category_id_cum.loc[i,'merchant_category_id_freq']))\n\nmerchant_category_id_cum['merchant_category_id_cum'] = cum_var    \n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = merchant_category_id_cum['index'][:30],\n            y = merchant_category_id_cum['merchant_category_id_cum'][:30],\n            order = merchant_category_id_cum['index'][:30])\nplt.xlabel('merchant_category_id')\nplt.ylabel('freq %')\nplt.title('merchant_category_id cummulative dist'); #we see that a relatively small number of merchant_category_id's make up a large part of the transactions","91be6ee6":"##### month_lag \/ purchase_date\nhist_transactions['month_lag'].head()\nhist_transactions['month_lag'].value_counts()\n\n#calculate so it is consistent with the formula used on the training set\n\nhist_transactions['Days_since_trans'] = (pd.to_datetime('2018-03-01') - hist_transactions['purchase_date']).dt.days\n\nhist_transactions['MonthsSince_trans'] = hist_transactions['Days_since_trans'] \/\/ 30 #floor division\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(7,7))\nsns.barplot(x = hist_transactions['MonthsSince_trans'].value_counts().index,\n            y = hist_transactions['MonthsSince_trans'].value_counts() \/ len(hist_transactions['MonthsSince_trans']),)\nplt.ylabel('% freq')\nplt.xlabel('# months since transaction')\nplt.title('MonthsSince_trans');\n","e74ab04d":"show = 20\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['purchase_date'].dt.date.value_counts().index[:show],\n            y = hist_transactions['purchase_date'].dt.date.value_counts()[:show] \/ len(hist_transactions['purchase_date']),\n            order = hist_transactions['purchase_date'].dt.date.value_counts().index[:show])\nplt.xticks(rotation='45')\nplt.ylabel('% freq')\nplt.title('Most {}th frequent purchase_dates'.format(show));","ebfb3e91":"##### purchase_amount\n\nprint(hist_transactions['purchase_amount'].describe())","e1718f6c":"##### subsector_id\n\nhist_transactions.subsector_id.value_counts().head(15)\n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['subsector_id'].value_counts().index[:30],\n            y = hist_transactions['subsector_id'].value_counts()[:30] \/ len(hist_transactions['subsector_id']),\n            order = hist_transactions['subsector_id'].value_counts().index[:30])\nplt.xlabel('subsector_id')\nplt.ylabel('freq %')\nplt.title('subsector_id')\n\n#cumulative barplot\n\nsubsector_id_freq = hist_transactions['subsector_id'].value_counts() \/ len(hist_transactions['subsector_id'])\n\nsubsector_id_cum = pd.DataFrame(subsector_id_freq)\nsubsector_id_cum.columns = ['subsector_id_freq']\n \nsubsector_id_cum = subsector_id_cum.reset_index()\n\ncum_var = np.empty((0,1),float)\n\nfor i in subsector_id_cum.index:\n    if i == 0:\n        cum_var = np.append(cum_var,(subsector_id_cum.loc[i,'subsector_id_freq']))\n    else: \n        cum_var = np.append(cum_var, (cum_var[i-1] + subsector_id_cum.loc[i,'subsector_id_freq']))\n\nsubsector_id_cum['subsector_id_cum'] = cum_var    \n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = subsector_id_cum['index'][:30],\n            y = subsector_id_cum['subsector_id_cum'][:30],\n            order = subsector_id_cum['index'][:30])\nplt.xlabel('subsector_id')\nplt.ylabel('freq %')\nplt.title('subsector_id cummulative dist');\n","0dc87f01":"#### merchant_id\n\nhist_transactions.merchant_id.value_counts().head(15)\nprint('Number of unique merchant_id~s: {}'.format(len(hist_transactions.merchant_id.unique())))\n\nshow = 20\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = hist_transactions['merchant_id'].value_counts().index[:show],\n            y = hist_transactions['merchant_id'].value_counts()[:show] \/ len(hist_transactions['merchant_id']),\n            order = hist_transactions['merchant_id'].value_counts().index[:show])\nplt.xticks(rotation='60')\nplt.ylabel('freq %')\nplt.xlabel('merchant_id')\nplt.title('Most {}th frequent merchant_id~s'.format(show))\n\n#cumulative barplot\n\nmerchant_id_freq = hist_transactions['merchant_id'].value_counts() \/ len(hist_transactions['merchant_id'])\n\nmerchant_id_cum = pd.DataFrame(merchant_id_freq)\nmerchant_id_cum.columns = ['merchant_id_freq']\n \nmerchant_id_cum = merchant_id_cum.reset_index()\n\ncum_var = np.empty((0,1),float)\n\nfor i in merchant_id_cum.index:\n    if i == 0:\n        cum_var = np.append(cum_var,(merchant_id_cum.loc[i,'merchant_id_freq']))\n    else: \n        cum_var = np.append(cum_var, (cum_var[i-1] + merchant_id_cum.loc[i,'merchant_id_freq']))\n\nmerchant_id_cum['merchant_id_cum'] = cum_var    \n\nfig = plt.subplots(nrows=1, ncols=1, figsize=(14,7))\nsns.barplot(x = merchant_id_cum['index'][:show],\n            y = merchant_id_cum['merchant_id_cum'][:show],\n            order = merchant_id_cum['index'][:show])\nplt.xticks(rotation='60')\nplt.ylabel('freq %')\nplt.xlabel('merchant_id')\nplt.title('merchant_id cummulative dist'); ","0d2844b3":"agg_func = {\n        'MonthsSince_trans' : ['min', 'max', 'mean', 'std'],\n        'purchase_date' : ['count'],\n        'authorized_flag': ['min', 'max', 'sum', 'mean'],\n        'category_3': ['nunique'],\n        'installments': ['min', 'max', 'mean', 'sum', 'std'],\n        'category_1' : ['min', 'max', 'mean'],\n        'merchant_category_id' : ['nunique'],\n        'subsector_id' : ['nunique'],\n        'merchant_id' : ['nunique'],\n        'purchase_amount' : ['min', 'max', 'sum', 'mean', 'std'],\n        'city_id' : ['nunique'],\n        'state_id' : ['nunique'],\n        'category_2' : ['nunique', 'min', 'max', 'mean']\n        }\n\nhist_trans_agg = hist_transactions.groupby(['card_id']).agg(agg_func)\n\nhist_trans_agg.columns = ['hist_' + '_'.join(col).strip() \n                           for col in hist_trans_agg.columns.values]\n\nhist_trans_agg.reset_index(inplace=True)\n\ndel hist_transactions","12c530d0":"train = train_test.loc['train'].set_index('card_id').drop('first_active_month', axis=1)\ntest = train_test.loc['test'].set_index('card_id').drop('first_active_month', axis=1)\n\ntrain = train.merge(hist_trans_agg, left_on='card_id', right_on='card_id',\n                              how='left').set_index('card_id')\n\ntest = test.merge(hist_trans_agg, left_on='card_id', right_on='card_id',\n                              how='left').set_index('card_id')","87601741":"# =============================================================================\n# #transforming categorical features\n# =============================================================================\nfeatures = list(train.columns)\ncategorical_feats = [col for col in features if 'feature_' in col]\nfor col in categorical_feats:\n    print(col, 'have', train[col].value_counts().shape[0], 'categories.')\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in categorical_feats:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))\n\ndf_all = pd.concat([train, test])\ndf_all = pd.get_dummies(df_all, columns=categorical_feats)\n\nlen_train = train.shape[0]\n\ntrain = df_all[:len_train]\ntest = df_all[len_train:]","4cb2f689":"#inspired by https:\/\/www.kaggle.com\/garethjns\/microsoft-lightgbm-with-parameter-tuning-0-823\n\nimport lightgbm as lgb\n\n#set params\n\nlgb_params = {'max_depth' : 6,\n          'objective': 'regression',\n          'num_leaves': 55,\n          'max_bin' : 60,\n          'learning_rate': 0.05,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'subsample_for_bin': 200,\n          'reg_alpha': 1,\n          'reg_lambda': 1,    \n          'min_child_weight': 1,\n          'min_child_samples': 12,\n          'min_split_gain': 0.5,\n          'scale_pos_weight': 1,\n          'metric' : 'rmse'}\n\ngridParams = {\n    'learning_rate': [0.05],\n    'n_estimators': [40],\n    'num_leaves': [8,16,32,64],\n    'objective' : ['regression'],\n    'random_state' : [501], # Updated from 'seed'\n    'colsample_bytree' : [0.6, 0.8],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.5],\n    'reg_lambda' : [1,1.2,1.5],\n    }\n\n\nmdl = lgb.LGBMRegressor(boosting_type= 'gbdt',\n          n_jobs = 3, # Updated from 'nthread'\n          silent = True,\n          **lgb_params)\n","1c8b7132":"# =============================================================================\n# #grid search\n# =============================================================================\n\n#dropping 10 features with low importance from first run\nto_drop = ['hist_authorized_flag_max', 'hist_category_1_min', 'hist_authorized_flag_min', \n            'feature_3_1', 'feature_1_3', 'hist_category_1_max', 'feature_3_0', 'feature_1_2', \n            'hist_category_2_max', 'feature_1_0']\n\ntarget = train['target']\n\ntrain = train.drop(to_drop + ['target'], axis=1)\ntest = test.drop(to_drop + ['target'], axis=1)\n\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n       \nmean_squared_error_ = make_scorer(mean_squared_error, greater_is_better=False)\n\n#setting up GridSearchCV\ngrid = GridSearchCV(mdl, gridParams,\n                    verbose=4,\n                    cv=4,\n                    n_jobs=2,  #paralel computing\n                    scoring=mean_squared_error_) \n\ngrid.fit(train, target)\n    \n# Print the best parameters found\n\nprint('mean square error for best params {}'.format(np.abs(grid.best_score_)))\nprint('root mean square error for best params {}'.format(np.sqrt(np.abs(grid.best_score_))))\n\nlgb_params['colsample_bytree'] = grid.best_params_['colsample_bytree']\nlgb_params['learning_rate'] = grid.best_params_['learning_rate']\n# params['max_bin'] = grid.best_params_['max_bin']\nlgb_params['num_leaves'] = grid.best_params_['num_leaves']\nlgb_params['reg_alpha'] = grid.best_params_['reg_alpha']\nlgb_params['reg_lambda'] = grid.best_params_['reg_lambda']\nlgb_params['subsample'] = grid.best_params_['subsample']\n\n\nprint('Fitting with params: ')\nprint(lgb_params)","edf86f56":"# =============================================================================\n# Model Build \/ Best Params\n# =============================================================================\n\n# inspiration from https:\/\/www.kaggle.com\/chocozzz\/simple-data-exploration-with-python-lb-3-764\n\nfrom sklearn.model_selection import KFold\n\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1987)\n\noof_lgb = np.zeros(len(train))\npredictions_lgb = np.zeros(len(test))\n\nfeatures_lgb = list(train.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train)):\n    trn_data = lgb.Dataset(train.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 10000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 60)\n    oof_lgb[val_idx] = clf.predict(train.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))\n\n# =============================================================================\n# inspecting var importance\n# =============================================================================\n\n\ncols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","95199714":"# =============================================================================\n# best_features dataframe\n# =============================================================================\n\n#best_features = best_features.drop('fold', axis=1).groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=True)\n\n#print(best_features.index[:10])\n\n# =============================================================================\n# submission\n# =============================================================================\n\nsub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = predictions_lgb \n#sub_df.to_csv(\"submission_lgb.csv\", index=False)\n\nprint(sub_df.head(10))","fe9b158c":"We see relatively small number of NAs in category_3. We handle them as a seperate group ('NA').","79cd44d7":"Setting up GridSearch:","94ff61eb":"### Explanatory data analysis:","79debd2a":"    *subsector_id:","f3f5e77a":"Checking for duplicates on card_id:","9034d92b":"Merging:","6f7db75c":"    *purchase_amount","d280ac9f":"Importing train and test data:\n","f2ebaa6e":"<a id=\"6\"><\/a>\n## 6. Submission","5f07d243":"Again we see that a relatively small number of merchant_category_id's make up a large part of the transactions.","34fd4136":"<a id=\"3\"><\/a>\n## 3. Merging and Preparing Data for Modelling","8f16b8bb":"Load numpy and pandas:","d60cdeb8":"    *merchant_id:","06d7cfe0":"Visualizing variables in historical_transactions dataset:","eedca888":"Setting up for barplots:","78a49242":"Transforming categorical features:","9d39370f":"<a id=\"1\"><\/a>\n## 1. Data Preperation and Exploration for *train.csv* and *train.csv*\n","8ce40c62":"    *merchant_category_id","e97a90de":"Quite irrational distributiion, we would expect the number of transactions to increase with time it seems. Lets have a another look at it.","45f509e5":"999  and -1 could be missing values, given installments should be known by the client we will assume 0 installments for these cases.","070dce2a":"We create a 'grid' to test for most optimal tuning parameters.\n\nPrepering parameters for Light Gradient Boosting.","64f37560":"Concatenate train and test for easier merge later:","13986435":"Adding 'months since' active feature:","091f7ed0":"We see some missing values, we will have this in consideration when inspecting the variables.","3bd0d88d":"Variable description:\n    * first_active_month = 'YYYY-MM', month of first purchase\n    * feature_1\t= Anonymized card categorical feature\n    * feature_2\t= Anonymized card categorical feature\n    * feature_3\t= Anonymized card categorical feature\n    * target = Loyalty numerical score calculated 2 months after historical and evaluation period\n\n\n","70c25d9e":"    *authorized_flag:","48ffc844":"Plotting feature_1 to Months_since_first_active: ","72a7bbb3":"We see christmas time is popular, why monthsince 2 peaks.","14af7e47":"Load data and inspect missing values:","944b61dc":"Plotting dependent variable vs independent (full sample):","fc05bbf4":"Calculate mean of datetime series and substitute missing value:","cfe9b0e6":"    *city_id:","6721e1be":"    *state_id:","5677647f":"<a id=\"2\"><\/a>\n# 2. Data Preperation and Exploration for *historical_transactions.csv*","902af560":"We see that a relatively small number of cities make up a large part of the transactions. Also -1 is probably missing values, we will treat them as a seperate group for now.","0d32e13b":"Checking for missing values:","82e5632e":"    *installments:","d40b2c64":"<a id=\"4\"><\/a>\n## 4. GridSearch:","f310f95a":"Plotting target variable:","67aea531":"We see distribution is rougly the same between train and test data, why we should no be to concerned about differences between training and test data.","af515ea3":"    *category_1, category_2, category_3:","441cbc66":"<a id=\"5\"><\/a>\n## 5. Light Gradient Boosting and Feature Importance","f4e49fba":"### Introductory comments:\n\nThis is my first time using python for other than data wrangling and committing a kernel. I tried to arrange the kernel and comment so that beginners should be able to understand and run the individual steps fairly easy. If something is unclear i suggest running the code bit by bit and inspecting the output. In order to not be biased by previous kernels i tried to limit the time spend in discussions and reading existing kernels, although i had a look at few (if steps taken from existing kernels, this is outlined in the code). The kernel only takes the *'train.csv'*, *'test.csv'* and *'historical_transactions.csv'* into consideration, why features from *'new_merchants_transaction.csv'* and *'merchants.csv'* is not introduced. Although *'new_merchants_transaction.csv' *can be largely introduced by the same approach as *'hitorical_transactions.csv'*, i chose not to do this as i believe a different approach would be better.\n\nThe purpose of the notebook is more leaned towards providing a **framework** and giving ideas for **further feature engineering** than performance itself.\n\n\n## Notebook content\n\n## [1. Data Preperation and Exploration for *train.csv* and *train.csv*](#1)\n\n## [2. Data Preperation and Exploration for *historical_transactions.csv*](#2)\n\n## [3. Merging and Preparing Data for Modelling](#3)\n\n## [4. GridSearch](#4)\n\n## [5. Light Gradient Boosting and Feature Importance](#5)\n\n## [6. Submission](#6)\n\n","1dfa3449":"    *month_lag \/ purchase_date:"}}