{"cell_type":{"94c5efb3":"code","19ae1212":"code","d00d6530":"code","e56bf7df":"code","5de20f6b":"code","f77e87a8":"code","a31c7f8d":"code","8103f9ad":"code","345c30b6":"code","92f99caf":"code","a490633b":"code","2de0f192":"code","bb12f5db":"code","50a88d67":"code","561597b4":"code","d4ab6335":"code","2958fc59":"code","5b7e282b":"code","d80605b9":"code","081efc47":"code","5bb3cf0e":"code","faa54191":"code","d61885c8":"code","a898199d":"code","0f29c98a":"markdown","e2201ee1":"markdown","60ebb798":"markdown","9c01d374":"markdown","3ade8b7b":"markdown","a4073272":"markdown","84a79af0":"markdown","348a4f14":"markdown","aba69e79":"markdown","583ba335":"markdown","9607cccc":"markdown","ff797e6e":"markdown","072489f1":"markdown","f34a6d96":"markdown","8c018b38":"markdown","0663e154":"markdown","cf013272":"markdown","c3b20296":"markdown","fc540714":"markdown","2ab90c9c":"markdown","a1d19c23":"markdown","164eb048":"markdown","c077f321":"markdown","ac37b96d":"markdown","61aa5caa":"markdown","067f4b06":"markdown","d9061d4b":"markdown","c76df0ff":"markdown","d9352bbd":"markdown","a91b91ee":"markdown","21186f3c":"markdown","ae4e76ac":"markdown","936fa86c":"markdown","6117e355":"markdown","bf3a009a":"markdown","b87e4839":"markdown","cb4070db":"markdown","10f49be7":"markdown","5dd5df89":"markdown","e0dc7a35":"markdown","f516e5ea":"markdown"},"source":{"94c5efb3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk\nimport matplotlib.pyplot as plt\nimport sys","19ae1212":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d00d6530":"df = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')\ndf.head()","e56bf7df":"df.info()","5de20f6b":"try:\n    df.horsepower =df.horsepower.astype('float')\nexcept:\n    print(\"an error occured\")","f77e87a8":"df.horsepower.unique()","a31c7f8d":"df[df['horsepower']=='?']","8103f9ad":"df = df[df.horsepower != '?']","345c30b6":"df.horsepower =df.horsepower.astype('float')\ndf.cylinders =df.cylinders.astype('float')\n","92f99caf":"df.info()","a490633b":"df.describe()","2de0f192":"import warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nsns.pairplot(df, diag_kind='kde');","bb12f5db":"cor=df.corr()\ncor","50a88d67":"sns.heatmap(cor,center=0,cmap=\"vlag\")","561597b4":"Y=df['mpg']\nX=df.drop(['mpg','car name'],axis=1)\nX=pd.get_dummies(X, columns=[\"origin\"])\nX.head()","d4ab6335":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=1\/5,random_state=0)","2958fc59":"import statsmodels.formula.api as smf\ndf_train=pd.concat([y_train, X_train], axis = 1)\nreg = smf.ols('mpg ~ X_train', data = df_train)","5b7e282b":"res=reg.fit()\nprint(res.summary())","d80605b9":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nx_temp = sm.add_constant(X_train.drop(['origin_1','origin_2','origin_3'],axis=1))\n\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(x_temp.values, i) for i in range(x_temp.values.shape[1])]\nvif[\"features\"] = x_temp.columns\nprint(vif.round(1))","081efc47":"cols = list(X_train.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X_train[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y_train,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features= cols\nprint(selected_features)","5bb3cf0e":"newX_train=X_train[selected_features]","faa54191":"reg2 = smf.ols('mpg ~ newX_train', data = df_train)\nres2=reg2.fit()\nprint(res2.summary())","d61885c8":"from sklearn.linear_model import LinearRegression\nregression_model = LinearRegression()\nX_train2=X_train[selected_features]\nregression_model.fit(X_train2, y_train)\nin_sampleScore = regression_model.score(X_train2, y_train)\n\nX_test2=X_test[selected_features]\nout_sampleScore = regression_model.score(X_test2, y_test)\nprint(in_sampleScore)\nprint(out_sampleScore)","a898199d":"model_accuracies = []\nimport statistics\nfor repetition in range(1000):\n    (training_inputs,\n     testing_inputs,\n     training_values,\n     testing_values) = train_test_split(X, Y, test_size=0.2)\n    cols = list(X_train.columns)\n    pmax = 1\n    while (len(cols)>0):\n        p= []\n        X_1 = training_inputs[cols]\n        X_1 = sm.add_constant(X_1)\n        model = sm.OLS(training_values,X_1).fit()\n        p = pd.Series(model.pvalues.values[1:],index = cols)      \n        pmax = max(p)\n        feature_with_p_max = p.idxmax()\n        if(pmax>0.05):\n            cols.remove(feature_with_p_max)\n        else:\n            break\n    selected_features= cols\n\n    training_inputs=training_inputs[selected_features]\n    testing_inputs=testing_inputs[selected_features]\n    regression_model = LinearRegression()\n    regression_model.fit(training_inputs, training_values)\n    classifier_accuracy = regression_model.score(testing_inputs, testing_values)\n    model_accuracies.append(classifier_accuracy)\n    \nprint(\"Min. model accuracy\",min(model_accuracies)*100)\nprint(\"Max. model accuracy\",max(model_accuracies)*100)\nprint(\"Median. model accuracy\",statistics.median(model_accuracies)*100)\nx = np.array(range(10000))\nplt.plot(x,model_accuracies) ;\n","0f29c98a":"Looking at the result, we can draw some conclusions about our dataset:\n* **mpg** is strongly correlated with **cylinders**, **displacement** , **horsepower** and **weight** .An increase in their values means a decrease in the mileage per gallon which seems right: a heavy vehicule, with a powerful engine and great horsepower would consume more energy, thus the low mileage per gallon\n* The independent variables that we are intending to use in our regression are also correlated.We suspect a multicollinearity. We will test and correct the eventual problem as we come further down our analysis.","e2201ee1":"The goal of our model is to look for the coefficient related to each independent variable. we will be using [the Ordinary Least Squares](https:\/\/en.wikipedia.org\/wiki\/Ordinary_least_squares) to estimate our parameters.\nBut first, we will encode the variable **origin** using the dummy variables. We will also drop the variable **car name** to keep things simple.","60ebb798":"Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to observed data. For all values of our independent variables (x1,x2,x3....xn) is associated with a value of the dependent variable y","9c01d374":"Our R-squared has dropped a little . That is mainly due to the fact that we reduced the number of variables of our model. Also, the p-values are lower than 5%, which means that the coefficients are significantly non null","3ade8b7b":"First things first, we will have a look at the data we are dealing with. Before diving into building a model, it is essential that we know more about the variables contained in our dataset.\n\nWe will start by checking the data, deal with missing values (if there are any) ,some descriptive statistics, and some vizualisation","a4073272":"<H1> Step 3:Building the model:","84a79af0":"![1%2039FkA9sgT6E_txFT_KANjw.png](attachment:1%2039FkA9sgT6E_txFT_KANjw.png)","348a4f14":"The value of our R-squared is 82% . However the p_values of a number of independent is far greater than 5%. This may lead to an error while interpreting our coefficents and how a change of 1 unit in a variable, all else being equal, can affect **mpg**.\nAlso, we suspected muticollinearity in our variables, let's have a look at it.\n","aba69e79":"The variable \"horsepower\" has a type \"object\" . Let's convert it to float type.","583ba335":"<H1> Problem of training and test sets choice:","9607cccc":"<H3> Feature selection","ff797e6e":"<H1> Importing the required libraries :","072489f1":"It's obviously a problem that our model performs quite differently depending on the subset of the data it's trained on. This phenomenon is known as overfitting: The model is learning to classify the training set so well that it doesn't generalize and perform well on data it hasn't seen before.\nIn our case, most of the models simulated have a good accuracy, yet many models have low accuracy.\nMore can be done  by tuning the parameters","f34a6d96":"<H3> Summary of the results:","8c018b38":"More can be found here: https:\/\/thomasttam.medium.com\/learn-machine-learning-algorithms-908e42d97a97","0663e154":"To detect the multicolinearity, we are using variable inflation factor or VIF. \nVIF determines the strength of the correlation between the independent variables. It is predicted by taking a variable and regressing it against every other variable.\nMore on VIF and multicollinearity : \nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/what-is-multicollinearity\/\n\nThe next part is mainly inspired from the following link : https:\/\/www.datasklr.com\/ols-least-squares-regression\/multicollinearity","cf013272":"<H3> What is multicollinearity and why is it a problem in a regression analysis:","c3b20296":"<H3> Dummy variables","fc540714":"<H1> Conclusion and insights","2ab90c9c":"* The mean value for mpg is is 23.445918\n* 50% of the vehicules in our sample have a mpg less than 22.75 (more than half of the cars are efficient)","a1d19c23":"Since there aren't many, I chose to delete them","164eb048":"Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high, it can cause problems when you fit the model and interpret the results.\n\nThe idea behind a regression is to capture the effect of each variable on our dependent variable.That being said,we aim to change the value of an independent variable whereas the others are constant, and then study the effect. However, when independent variables are correlated, it indicates that changes in one variable are associated with shifts in another variable(s). Thus, you will not be able to isolate the effect of your independent variable.\n\nMore on multicollinearity : https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/","c077f321":"Since we have a problem of multicollinearity, we will try to fit a new model by only keeping the variables that have significant effect on **mpg**\nFeature selection is the process of identifying and selecting a subset of input variables that are most relevant to the target variable. To do so, we will fit our model recursively. At each iteration,we will drop the variable that is not significant to our model.","ac37b96d":"Now that we have our subset of input variables, let's refit our model.","61aa5caa":"<H3> Detect the multicollinearity:","067f4b06":"The accuracy of the results depend more on the training set we use to learn the relationship between then the variable **mpg** and the independant variables. Hence, it appears that tuning the parameters is necessary to avoid the problem of overfitting.","d9061d4b":"<H3> Creating our Train and Test sets:","c76df0ff":"Using the statsmodels library we will fit our model using the linear regression.","d9352bbd":"<H1> Step2: Exploratory data analysis:","a91b91ee":"<H1> Step 1: Loading the data :","21186f3c":"<H2> Data vizualisation","ae4e76ac":"An error occured while trying to convert \"horsepower\" . It may be something related to the values of the variable. Let's have a look at the distinct values of our variable","936fa86c":"![pvalue.png](attachment:pvalue.png)","6117e355":"In th third line of our output, we see that we have '?' as a value. Maybe the creators of the dataset used it as a way to indicate that there are missing values. We can either try to impute by using the median value of \"horsepower\", or delete the observations where \"horsepower\" has '?' as a value.\nLet's have a look at those observations:","bf3a009a":"Our dataset now contains 392 observations.\nNext, we are going to look at the distribution of our data by showing some summary statistics about the data set","b87e4839":"<H2> Variable Correlation:","cb4070db":"<H2> Summary Statistics","10f49be7":"Using the dataset \"auto-mpg.csv\" we will try to create a model to predict car consumption(MPG or miles per gallon)\n\nOur aim iwill be to predict tha value of **mpg** .\n\nDifferent approaches can be used to tackle the problem, but in this notebook we will do the following:\n* Get our data\n* Run a exploratory data analysis on our data\n* Use linear regression to build our models\n* Finish with a conclusion and insights on other models that can be used\nWe will use the linear regression mode to predict the value of **mpg** .","5dd5df89":"<H2> Checking the data","e0dc7a35":"<H1> Aim of the notebook","f516e5ea":"In litterature, for large datasets, a VIF value of 10 and above indicates multicollinearity. From the results, we clearly have a multicollinearity problem in our variables."}}