{"cell_type":{"dc1c42f5":"code","c5e143b1":"code","4425af0f":"code","62fdaaac":"code","e3e4779b":"code","db9c6969":"code","5352b6e1":"code","5e557c63":"code","c0f56118":"code","f286c8c9":"code","3520ad1a":"code","3242cf4b":"code","1a75ed45":"code","5596b3ff":"code","b89e4993":"code","15575d47":"code","94e8374a":"code","f21f9d13":"code","beca7e7d":"code","47d42409":"code","f82ac831":"code","34d32a60":"code","2b89f617":"code","67c312e4":"code","4102c3a9":"code","3312933a":"code","d9a48fb6":"code","8b04301b":"code","e8df8697":"code","c0fa4a81":"code","fafeaf8f":"code","e8dccb5f":"code","4d064737":"code","c8b5fa62":"code","7a51f2ee":"code","0663d806":"code","c516d392":"code","c1e02765":"code","59f3e9dd":"code","1953ceb1":"code","0f272359":"code","29fde5ff":"code","7b941c7f":"markdown","bf43858f":"markdown","521a35e5":"markdown","e578edc9":"markdown","222f2b89":"markdown","1c7fa385":"markdown","60cda142":"markdown","9af1f76c":"markdown","a7191d7f":"markdown","ee19b099":"markdown","a7842b0e":"markdown","20cb4e17":"markdown","519ecf43":"markdown","de1260e3":"markdown","3577eb24":"markdown","a0f56403":"markdown","b73d9d45":"markdown","741f9e7f":"markdown","8b80b1ca":"markdown","efdec889":"markdown","15142d34":"markdown","eadabedc":"markdown","8aecbdc6":"markdown","e0a32f5f":"markdown","e711c644":"markdown","f66ab048":"markdown","a34d42f6":"markdown","32948a35":"markdown","7160e716":"markdown","a776b2de":"markdown","c812d5f8":"markdown","df9750cf":"markdown","dadba7a1":"markdown","e0840897":"markdown","82ce06e5":"markdown","cae29bd3":"markdown","a94a7ce6":"markdown","7fe14c0c":"markdown","f8eebe58":"markdown","1766705c":"markdown","b4a607cf":"markdown"},"source":{"dc1c42f5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nimport itertools\nwarnings.filterwarnings(\"ignore\")\n\n\n### Modelling\nfrom sklearn.model_selection import train_test_split,cross_val_score,KFold,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve,auc,classification_report,confusion_matrix,accuracy_score,make_scorer,f1_score,auc\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import display","c5e143b1":"kaggle=1\nif kaggle==0:\n    diab=pd.read_csv(\"diabetes.csv\")\nelse:\n    diab=pd.read_csv(\"..\/input\/diabetes.csv\")","4425af0f":"print(\"Number of rows:{} and Number of column:{}\".format(diab.shape[0],diab.shape[1]))","62fdaaac":"diab.info()","e3e4779b":"diab.describe()","db9c6969":"plt.figure(figsize=(8,8))\nax=sns.countplot(diab['Outcome'])\nax.set_xlabel(\"Outcome\")\nax.set_ylabel('Count')\nax.set_title(\"Countplot of Predictor Variable\")\n","5352b6e1":"count_non_diab=len(diab[diab['Outcome']==0])\ncount_diab=len(diab[diab['Outcome']==1])\nprint(\"The dataset has {} % of non-diabetic cases and {} % of diabetic cases\".format(round(count_non_diab\/len(diab['Outcome'])*100,2),round(count_diab\/len(diab['Outcome'])*100,2)))","5e557c63":"diab.isnull().values.any()","c0f56118":"## Defining a function to plot histogram for all the independent variable\ndef plot_variables(variable):\n\n    f, axes = plt.subplots(3, 1, figsize=(10, 10))\n    \n    sns.distplot(diab[variable][diab['Outcome']==1],ax=axes[0],color='green')\n    axes[0].set_title(r\"Distribution of {}-Diabetic Outcome\".format(variable))\n    sns.distplot(diab[variable][diab['Outcome']==0],ax=axes[1],color='red')\n    axes[1].set_title(r\"Distribution of {}-Non Diabetic Outcome\".format(variable))\n    sns.boxplot(x=diab['Outcome'],y=diab[variable],ax=axes[2],palette=\"Set3\")\n    axes[2].set_title(r\"Boxplot of {}\".format(variable))\n    f.tight_layout()","f286c8c9":"plot_variables('Age')","3520ad1a":"plot_variables('Pregnancies')","3242cf4b":"plot_variables('Glucose')","1a75ed45":"plot_variables('BloodPressure')","5596b3ff":"plot_variables('SkinThickness')","b89e4993":"plot_variables('Insulin')","15575d47":"plot_variables('BMI')","94e8374a":"plot_variables('DiabetesPedigreeFunction')","f21f9d13":"mut_vari=['BloodPressure','Glucose','SkinThickness','BMI']\nfor i in mut_vari:\n    print('Imputing column {} with median value {} \\n'.format(i,diab[i].median()))\n    diab[i]=diab[i].replace(0,value=diab[i].median())","beca7e7d":"X=diab.drop('Outcome',axis=1)\ny=diab['Outcome']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=100)","47d42409":"X_train.shape","f82ac831":"X_test.shape","34d32a60":"### Function to plot confusion matrix.\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2b89f617":"prediction={}  ### Create a dictionary of prediction for comparison.","67c312e4":"def print_score(m):\n    res = {'F1 Score for Train:':f1_score(m.predict(X_train), y_train),'F1 Score for Test:':f1_score(m.predict(X_test), y_test),\n           'Accuracy Score for Train:':m.score(X_train, y_train),'Accuracy Score for Test:':m.score(X_test, y_test)}\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","4102c3a9":"###https:\/\/www.kaggle.com\/tunguz\/just-some-simple-eda\n\n#param_grid = {\n           # 'n_estimators': [100, 200, 500],\n           # 'max_features': [2, 3, 4],\n           # 'min_samples_leaf': [1, 2, 4],\n           # 'min_samples_split': [2, 5, 10]\n          #  }\nrfm=RandomForestClassifier(random_state=100,n_jobs=-1,n_estimators=40,min_samples_leaf=5,max_features=0.5)\nrfm.fit(X_train,y_train)","3312933a":"estimator=rfm.estimators_[5]","d9a48fb6":"prediction['RF']=rfm.predict(X_test)","8b04301b":"print_score(rfm)","e8df8697":"## Following code is taken from fast.ai libraries function by Jermey Howard .Thanks Jermey for the awesome package .\n\ndef rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","c0fa4a81":"class_names = set(diab['Outcome'])\ncnf_matrix = confusion_matrix(y_test, prediction['RF'])\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, Random Forest Model(W\/O CV)')","fafeaf8f":"###https:\/\/stats.stackexchange.com\/questions\/117654\/what-does-the-numbers-in-the-classification-report-of-sklearn-mean\n###https:\/\/medium.com\/greyatom\/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b\nprint(classification_report(y_test,prediction['RF']))","e8dccb5f":"fi=rf_feat_importance(rfm,X_train)\nfi","4d064737":"to_keep=fi[fi['imp']>0.05].cols;len(to_keep)","c8b5fa62":"diab_keep=diab[to_keep]","7a51f2ee":"X=diab_keep\ny=diab['Outcome']","0663d806":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,test_size=0.2,random_state=100)\nprint(\"Dimensions of train set:{} and Dimensions of test set:{}\".format(X_train.shape,X_test.shape))","c516d392":"rfm=RandomForestClassifier(random_state=100,n_jobs=-1,n_estimators=40,min_samples_leaf=5,max_features=0.5)\nrfm.fit(X_train,y_train)","c1e02765":"prediction['RF_FI']=rfm.predict(X_test)","59f3e9dd":"print_score(rfm)","1953ceb1":"class_names = set(diab['Outcome'])\ncnf_matrix = confusion_matrix(y_test, prediction['RF_FI'])\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, Random Forest Model(After Feature Selection)')","0f272359":"print(classification_report(y_test,prediction['RF_FI']))","29fde5ff":"#Borrowed from https:\/\/www.kaggle.com\/gpayen\/building-a-prediction-model\ncmp = 0\ncolors = ['b', 'g', 'y', 'm', 'k']\nfor model, predicted in prediction.items():\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,predicted)\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n    cmp += 1\n\nplt.title('Classifiers comparaison with ROC')\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","7b941c7f":"We have 5 columns whose importance are more than 0.05 . We recreate the dataset with these columns.","bf43858f":"### Pregnancies Vs Outcome ","521a35e5":"* There are two blood pressures - Systolic and Diastolic . The reading provided here is the diastolic blood pressure .Diastolic blood pressure is the pressure in the arteries when the heart rests between beats. This is the time when the heart fills with blood and gets oxygen.A normal diastolic blood pressure is lower than 80. A reading of 90 or higher means you have high blood pressure.\n\n* From the histogram it is understood that the are values where the blood pressure is 0 !!! . The mode for diabetic outcome is on the higher side when compared to non-diabetic outcome . The median value is slightly different between the outcomes . \n\n* This variable is also significant in predicting the outcome.\n","e578edc9":"There data doesnt seem to have any missing information.","222f2b89":"The distribution for each of the outcomes seem to be similar with the histograms skewed towards right . There are outliers for both the outcomes .The median value of insulin is close to zero in case of diabetic outcome.There is a significant difference between the outcomes with respect to insulin levels.Therefore this might be a good predictor of outcome.","1c7fa385":"Thanks for reading my kernel . Pls provide your valuable feedback through comments\/upvotes.","60cda142":"We use the Random Forest algorithm for our modelling and use the confusion matrix,accuracy score , f1 score as our scoring metrics.Since it is a unbalanced dataset , using accuracy score will not be a good metric . Hence we give f1 score more importance.","9af1f76c":"The description of the variable is as follows:\n\n*Pregnancies* - Number of times pregnant\n\n*Glucose* - Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n*BloodPressure* - Diastolic blood pressure (mm Hg)\n\n*SkinThickness* - Triceps skin fold thickness (mm)\n\n*Insulin* - 2-Hour serum insulin (mu U\/ml)\n\n*BMI* - Body mass index (weight in kg\/(height in m)^2)\n\n*DiabetesPedigreeFunction* - Diabetes pedigree function\n\n*Age* - Age (years)\n\n*Outcome* - Class variable (0 or 1) \n\n\nFrom the summary of the data ,we understand that all the columns are numeric . The predictor variable  Outcome is binary (0,1).","a7191d7f":"Going by the first look ,it seems that for a diabetic outcome , the data is skewed towards right .The median number of pregnancies is higher for a diabetic when compared to a non-diabetic outcome .Thus the outcome with respect to pregnancies is also significant.","ee19b099":"The dataset we have taken is on Pima Indians Diabetes data consisting of diagnostic measurements of females of atleast 21 years of age . We are asked to predict based on these diagnostic measurements whether a person is diabetic or not .In this work , I have done an extensive EDA on all the variables to understand how each variable is related to the predictor variable . I will implement Random forest algorithm to model the classification problem and compare the metrics before and after feature selection .","a7842b0e":"Distribution of skin thickness shows that it is multimodal .The median values are close to each other for both the outcomes.This might not have a good influence in predicting the outcomes .","20cb4e17":"The model has done a good job in improving the f1 score . Lets plot the ROC curve and check the AUC .","519ecf43":"## Loading the required libraries","de1260e3":"The histogram shows that the BMI for a diabetic outcome is higher compared to a non-diabetic outcome . There are zero values in both cases.The median BMI value for a non-diabetic outcome is higher compared to a non-diabetic outcome .A significant difference between the two is seen . ","3577eb24":"### DiabetesPedigreeFunction Vs Outcome","a0f56403":"Given the nature of the dataset it may be wise to plot histogram and boxplot inorder to determine which factors will have maximum influence in predicting the outcome of the model .Therefore a function is created first to plot histogram and outcome.","b73d9d45":"Distribution patterns for both the outcomes seem to be similar - skewed towards right . The median is almost equal between the two . There are visible outliers between the two .","741f9e7f":"From the scores , we understand that the F1 score for test is 0.55 whereas the accuracy score is 72 %.","8b80b1ca":"* I did a quick google search to understand more about this variable .According to the data describtion the values in the column represent Plasma glucose concentration a 2 hours in an oral glucose tolerance test . According to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Glucose_tolerance_test#Results) the glucose tolerance test is a medical test in which glucose is given and blood samples taken afterward to determine how quickly it is cleared from the blood.The test is usually used to test for diabetes, insulin resistance, impaired beta cell function,and sometimes reactive hypoglycemia and acromegaly, or rarer disorders of carbohydrate metabolism. In the most commonly performed version of the test, an oral glucose tolerance test (OGTT), a standard dose of glucose is ingested by mouth and blood levels are checked two hours later.\n\n* The normal blood glucose level (tested while fasting) for non-diabetics, should be between 3.9 and 7.1 mmol\/L (70 to 130 mg\/dL).Blood sugar levels for those without diabetes and who are not fasting should be below 6.9 mmol\/L (125 mg\/dL).\n\n* A reading of more than 200 mg\/dL (11.1 mmol\/L) after two hours indicates diabetes.\n\n* Thus going by our distribution plot , we see that the median value of glucose levels after 2 hour period for a non-diabetic person is around 100 mg\/dL whereas for a diabetic it has been close to 140 mg\/dL.There are also some outliers in the non-diabetic where the glucose levels were on the extreme end.","efdec889":"### References","15142d34":"From the countplot , it is seen that there are more values of '0' than '1' which means the dataset has more non-diabetic diagnostic related information than diabetic related information .","eadabedc":"## Reading the dataset ","8aecbdc6":"The classification report here shows that the model has a precision score of 0.61 and a recall value of 0.51 .The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The recall is intuitively the ability of the classifier to find all the positive samples.","e0a32f5f":"### Modelling","e711c644":"## Introduction","f66ab048":"###  Blood Pressure Vs Outcome","a34d42f6":"### Insulin Vs Outcome","32948a35":"The accuracy of the model has marginally improved from 0.72 to 0.77 .But the F1 score has improved from 0.55 to 0.66.Lets print the confusion matrix and check the classification report .","7160e716":"### Age Vs Outcome:","a776b2de":"From the feature importance we see that to determine the outcome , glucose , BMI ,AGE and diabetes pedigree function are considered as most important features.Lets consider only those variables and see if we can improve the accuracy of the model .","c812d5f8":"### Glucose Vs Outcome","df9750cf":"We split the dataset into train and test dataset by 80-20 ratio.This is not a stratified split and it is a random split .During our EDA we saw that there are some 0 values which might be some data error .We can impute those variables with median values in the column.","dadba7a1":"* [fastais Machine Learning class](https:\/\/course.fast.ai\/ml)\n* [Glucose Tolerance Test](https:\/\/www.mayoclinic.org\/diseases-conditions\/diabetes\/diagnosis-treatment\/drc-20371451)\n* [Insulin](https:\/\/emedicine.medscape.com\/article\/2089224-overview)\n* [Stackexchange discussion on classification metrics](https:\/\/stats.stackexchange.com\/questions\/117654\/what-does-the-numbers-in-the-classification-report-of-sklearn-mean)\n* [Medium blog on classification metrics](https:\/\/medium.com\/greyatom\/performance-metrics-for-classification-problems-in-machine-learning-part-i-b085d432082b)\n\n**Kaggle Kernels:**\n\n* [Guillaume Payen Kernel - Building a prediction model](https:\/\/www.kaggle.com\/gpayen\/building-a-prediction-model)\n* [Bojan Tunguz Kernel - Just some simple EDA](https:\/\/www.kaggle.com\/tunguz\/just-some-simple-eda)","e0840897":"### Random Forest","82ce06e5":"# Working on Pima Indians Diabetes","cae29bd3":"### Skin Thickness Vs Outcome","a94a7ce6":"From the histogram it is seen that the distribution is non-normal .The histogram for age -diabetic outcome seems to be multimodal with modes centered around 20-30 years and again at 50 years whereas the histogram for age-non diabetic outcome is at 20 years .The distribution is skewed towards right for age-non diabetic outcome .There is a significant difference between the outcome and age as seen from the boxplot .The median age for a diabetic is higher compared to a non-diabetic .There are also many outliers in age for non-diabetic outcome compared to a diabetic outcome.","7fe14c0c":"### BMI Vs Outcome","f8eebe58":"Thus after selecting the variables after feature importance , we see that there is an improvement in the AUC curve from 0.48 to 0.74 .","1766705c":"Lets plot the feature importance of the model and check which variables are contributing to the model accuracy.","b4a607cf":"## Exploratory Data Analysis"}}