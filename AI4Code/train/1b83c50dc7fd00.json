{"cell_type":{"fb5aa5d3":"code","4f9661a1":"code","803e4fc5":"code","ccbce335":"code","6a76a05e":"code","ee3c3edd":"code","c8cacf2d":"code","076ba506":"code","29cfe744":"code","63488a02":"code","0c43dba0":"code","f75c1bf6":"code","23901fe5":"code","ae162562":"code","542f92c6":"code","afd25b99":"code","0645bbdc":"code","9dc43240":"code","8da806b2":"code","50582c91":"code","89a038d5":"code","9823f6f5":"code","c490020e":"code","8c2455d6":"code","d82bde8f":"code","eebf8e1c":"code","20b14673":"code","c743ff11":"code","36165b7b":"code","a50c2398":"code","2d158132":"code","d8f0614e":"code","a86cd3b4":"code","04fec005":"code","4bd4904e":"code","16b478a6":"code","c6ccca15":"code","4949eb71":"code","b1b28319":"code","eb0e607b":"code","0d84cf4e":"code","fa959cd2":"code","799eeef2":"code","930c8521":"code","fcc7a9b7":"code","b3ba5039":"code","b5a5b191":"code","16929637":"code","a618828b":"code","9921b83f":"code","c64b7003":"code","2dc4d089":"code","73d756a6":"code","101fa0c2":"code","62e48c05":"code","e05ea296":"code","166c724f":"code","37f1362e":"code","0c593ef9":"code","7ef30f3e":"code","6f3fe846":"code","e2970332":"code","08aa8358":"code","2a12a36c":"code","5ae89008":"code","a5513070":"code","75ef3aaa":"code","b86255e0":"code","4c59d4df":"code","bb14509b":"code","03cd71e4":"code","a56cfe71":"code","bef2bac6":"code","c61e997d":"code","01d6e103":"code","3bef4f53":"code","5d931607":"code","b109904d":"code","58445dd0":"code","504bdb8b":"code","461673aa":"code","7aed5ed0":"code","0f0450d7":"markdown","cf68a98b":"markdown","561d431b":"markdown","037047df":"markdown","7efdb6ef":"markdown","d69f3d6d":"markdown","23d990e2":"markdown","fa9cf74c":"markdown","417682d8":"markdown","45c9fdc6":"markdown","0c0b53e3":"markdown","52ff9285":"markdown","6ef42267":"markdown","b712915f":"markdown","340ffb0d":"markdown","cf3928e8":"markdown","e8aa41d3":"markdown","16875b6c":"markdown","4108a36a":"markdown","e47de0be":"markdown","dba59faf":"markdown","7fff2921":"markdown","75aaf789":"markdown","f9f54bcd":"markdown","cec45405":"markdown","822fb105":"markdown","95e4f0f2":"markdown","2cb05b46":"markdown","65758e50":"markdown","23b7ad54":"markdown","650a9a22":"markdown"},"source":{"fb5aa5d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f9661a1":"#pip install collinearity","803e4fc5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\n\n\n","ccbce335":"df=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf.head(10)","6a76a05e":"print(df['Alley'].unique())\n## categorical feature\n","ee3c3edd":"print(df['PoolQC'].unique())\n#categorical feature\n","c8cacf2d":"print(df['Fence'].unique())\n\n## categorical","076ba506":"print(df['MiscFeature'].unique())\n## categorical feature","29cfe744":"corr = df.corr()\nplt.subplots(figsize = (15, 12))\nsns.heatmap(corr, vmax = 0.9, square = True)","63488a02":"df.info()","0c43dba0":"df.duplicated().sum()\n# there are no duplicated values","f75c1bf6":"cols=[feature for feature in df.columns ]\n","23901fe5":"# variables having atleast one missing value\nfor i in cols:\n    if df[i].isnull().sum()>0:\n        print(\"the feature {} has a total of {} missing values\".format(i, df[i].isnull().sum()))","ae162562":"# now we make a record of the features\/columns which have missing values\nmiss_cols=[]\n\nfor i in cols:\n     if df[i].isnull().sum()>0:\n            miss_cols.append(i)","542f92c6":"\n#miss_cols # the list of features having missiing values\ndf[miss_cols].head()","afd25b99":"# varibales having more than 50% missing values\nfor i in cols:\n    if df[i].isnull().sum()\/len(df[i])>0.5:\n        print(\"the feature {} has a total of {} missing values and percentage is {}\".format(i, df[i].isnull().sum(),df[i].isnull().sum()\/len(df[i])))","0645bbdc":"drop_cols= []\n\nfor i in cols:\n    if df[i].isnull().sum()\/len(df[i])>0.5:\n        drop_cols.append(i)\n        \ndrop_cols\n        ","9dc43240":"df[\"WoodDeckSF\"].isnull().sum()","8da806b2":"# Handle missing values for features where median\/mean or most common value doesn't make sense\n\n# Alley : data description says NA means \"no alley access\"\ndf.loc[:, \"Alley\"] = df.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ndf.loc[:, \"BedroomAbvGr\"] = df.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ndf.loc[:, \"BsmtQual\"] = df.loc[:, \"BsmtQual\"].fillna(\"No\")\ndf.loc[:, \"BsmtCond\"] = df.loc[:, \"BsmtCond\"].fillna(\"No\")\ndf.loc[:, \"BsmtExposure\"] = df.loc[:, \"BsmtExposure\"].fillna(\"No\")\ndf.loc[:, \"BsmtFinType1\"] = df.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ndf.loc[:, \"BsmtFinType2\"] = df.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ndf.loc[:, \"BsmtFullBath\"] = df.loc[:, \"BsmtFullBath\"].fillna(0)\ndf.loc[:, \"BsmtHalfBath\"] = df.loc[:, \"BsmtHalfBath\"].fillna(0)\ndf.loc[:, \"BsmtUnfSF\"] = df.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ndf.loc[:, \"CentralAir\"] = df.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ndf.loc[:, \"Condition1\"] = df.loc[:, \"Condition1\"].fillna(\"Norm\")\ndf.loc[:, \"Condition2\"] = df.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ndf.loc[:, \"EnclosedPorch\"] = df.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ndf.loc[:, \"ExterCond\"] = df.loc[:, \"ExterCond\"].fillna(\"TA\")\ndf.loc[:, \"ExterQual\"] = df.loc[:, \"ExterQual\"].fillna(\"TA\")\n# Fence : data description says NA means \"no fence\"\ndf.loc[:, \"Fence\"] = df.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ndf.loc[:, \"FireplaceQu\"] = df.loc[:, \"FireplaceQu\"].fillna(\"No\")\ndf.loc[:, \"Fireplaces\"] = df.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ndf.loc[:, \"Functional\"] = df.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ndf.loc[:, \"GarageType\"] = df.loc[:, \"GarageType\"].fillna(\"No\")\ndf.loc[:, \"GarageFinish\"] = df.loc[:, \"GarageFinish\"].fillna(\"No\")\ndf.loc[:, \"GarageQual\"] = df.loc[:, \"GarageQual\"].fillna(\"No\")\ndf.loc[:, \"GarageCond\"] = df.loc[:, \"GarageCond\"].fillna(\"No\")\ndf.loc[:, \"GarageArea\"] = df.loc[:, \"GarageArea\"].fillna(0)\ndf.loc[:, \"GarageCars\"] = df.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ndf.loc[:, \"HalfBath\"] = df.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ndf.loc[:, \"HeatingQC\"] = df.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ndf.loc[:, \"KitchenAbvGr\"] = df.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ndf.loc[:, \"KitchenQual\"] = df.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ndf.loc[:, \"LotFrontage\"] = df.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ndf.loc[:, \"LotShape\"] = df.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ndf.loc[:, \"MasVnrType\"] = df.loc[:, \"MasVnrType\"].fillna(\"None\")\ndf.loc[:, \"MasVnrArea\"] = df.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ndf.loc[:, \"MiscFeature\"] = df.loc[:, \"MiscFeature\"].fillna(\"No\")\ndf.loc[:, \"MiscVal\"] = df.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ndf.loc[:, \"OpenPorchSF\"] = df.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ndf.loc[:, \"PavedDrive\"] = df.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ndf.loc[:, \"PoolQC\"] = df.loc[:, \"PoolQC\"].fillna(\"No\")\ndf.loc[:, \"PoolArea\"] = df.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ndf.loc[:, \"SaleCondition\"] = df.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ndf.loc[:, \"ScreenPorch\"] = df.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ndf.loc[:, \"TotRmsAbvGrd\"] = df.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ndf.loc[:, \"Utilities\"] = df.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ndf.loc[:, \"WoodDeckSF\"] = df.loc[:, \"WoodDeckSF\"].fillna(0)","50582c91":"# Some numerical features are actually really categories\ndf = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","89a038d5":"# Encode some categorical features as ordered numbers when there is information in the order\ndf = df.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )\n","9823f6f5":"df.head(10)","c490020e":"sum(df.isna().sum())\n\n# meaning that there are still 82 missing values!","8c2455d6":"sns.set_style(\"white\")\nsns.set_color_codes(palette = 'deep')\nf, ax = plt.subplots(figsize = (8, 7))\nsns.distplot(df['SalePrice'], color = 'g');\nax.xaxis.grid(False)\nax.set(ylabel = \"Frequency\")\nax.set(title = \"SalePrice distribution\")\nsns.despine(trim = True, left = True)\nplt.show()","d82bde8f":"#for feature in cols:\n #   plt.scatter(df[feature], df['SalePrice'])\n  #  plt.show()\n   # plt.xlabel(feature)\n    #plt.ylabel('SalePrice')\n    #plt.title(feature)","eebf8e1c":"#df.drop(['Alley','MiscFeature','PoolQC','Fence'],axis=1).head()\ndf.drop(['Alley','MiscFeature','PoolQC','Fence','GarageQual', 'BsmtCond', 'GrLivArea', 'Fireplaces', 'PoolArea', 'GarageCars', 'BsmtFinType2', 'BsmtQual', 'BsmtFinType1'],axis=1).head()\n\n","20b14673":"df.shape","c743ff11":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = df.select_dtypes(include = [\"object\"]).columns\nnumerical_features = df.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ndf_num = df[numerical_features]\ndf_cat = df[categorical_features]\n\n# now we have differentiated the categorical and numerical values and also we give the no. of diff variables","36165b7b":"year_feat = [features for features in df_num if 'Yr' in features or 'Year' in features]\nyear_feat","a50c2398":"\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    df[feature]=df['YrSold']-df[feature]\n# we are replacing the years with difference in years\n# because the price is highly dependent on the diff rather than the year in which was made \n# this can be seen from my code on EDA in line 15 and below also","2d158132":"df.groupby('YrSold')['SalePrice'].mean().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Mean House Price')\nplt.title(\"House Price vs YearSold\")","d8f0614e":"df.groupby('YrSold')['SalePrice'].median().plot()\nplt.xlabel('Year Sold')\nplt.ylabel('Median House Price')\nplt.title(\"House Price vs YearSold\")","a86cd3b4":"for feature in df_num.columns:\n    if df_num[feature].isnull().sum()>0:\n        print(feature)","04fec005":"for feature in df_cat.columns:\n    if df_cat[feature].isnull().sum()>0:\n        print(feature)","4bd4904e":"df_cat[\"Electrical\"].head()","16b478a6":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))\ndf_num = df_num.fillna(df_num.median())\nprint(\"NAs for numerical features in train : \" + str(df_num.isnull().values.sum()))\n\n","c6ccca15":"\nprint(\"NAs for categorical features in train : \" + str(df_cat.isnull().values.sum()))\ndf_cat = pd.get_dummies(df_cat)\nprint(\"NAs for categorical features in train : \" + str(df_cat.isnull().values.sum()))\n\n# Create dummy features for categorical values via pd.get_dummies encoding\n\n","4949eb71":"from scipy.stats import skew\n\n# Log transform of the right skewed numerical features to lessen impact of outliers\n# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nskewness = df_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\ndf_num[skewed_features] = np.log1p(df_num[skewed_features])\n#Calculates log(1 + x) : log1p to handle the case where we have zero values present\n\n\n#Top skewed columns\n#numeric_features = data.dtypes[data.dtypes != 'object'].index\n#skewed_features = data[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n##high_skew = skewed_features[abs(skewed_features) > 0.5]\n#print(high_skew)\n","b1b28319":"df_num.head()","eb0e607b":"#df_num","0d84cf4e":"#from collinearity import SelectNonCollinear\n#selector = SelectNonCollinear(0.4)\n\n## always work only for the numerical data\ncorr_list = []\nn =len(df_num.columns)\ncorr_matrix = df_num.corr()\n\ncorr_set = set()\nfor i in range(n):\n    for j in range(i+1,n):\n        corr = corr_matrix.iloc[i,j]\n        if corr > 0.8:\n            corr_list.append((df_num.columns[i],df_num.columns[j],corr))\n            corr_set.add(df_num.columns[i])\nprint(corr_set)\n","fa959cd2":"print(corr_list)","799eeef2":"#df.drop(['GarageQual', 'BsmtCond', 'GrLivArea', 'Fireplaces', 'PoolArea', 'GarageCars', 'BsmtFinType2', 'BsmtQual', 'BsmtFinType1'],axis=1).head()\n","930c8521":"df.SalePrice = np.log1p(df.SalePrice)\ny = df.SalePrice","fcc7a9b7":"#df.SalePrice","b3ba5039":"data = pd.concat([df_num, df_cat], axis = 1)\ndata.head(10)","b5a5b191":"data.shape","16929637":"#data['Fence'].head()\n# means the missing values have been deleted","a618828b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.3, random_state = 0)\nprint(\"X_train : \" + str(X_train.shape))\nprint(\"X_test : \" + str(X_test.shape))\nprint(\"y_train : \" + str(y_train.shape))\nprint(\"y_test : \" + str(y_test.shape))","9921b83f":"from sklearn.preprocessing import StandardScaler\nstdSc = StandardScaler()\nX_train.loc[:, numerical_features] = stdSc.fit_transform(X_train.loc[:, numerical_features])\nX_test.loc[:, numerical_features] = stdSc.transform(X_test.loc[:, numerical_features])\n\n## .loc for column names\n## .iloc for numerical value of columns \/ rows","c64b7003":"from sklearn.metrics import mean_squared_error, make_scorer\n\n\n#  RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\n#greater is better is false because we wish to minimize the error between actual and predicted house prices\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","2dc4d089":"from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score\n","73d756a6":"# 1. Lasso\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","101fa0c2":"# 2. Ridge\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","62e48c05":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators=300)\nrfr.fit(X_train, y_train)","e05ea296":"\nprint(\"RMSE on Training set :\", rmse_cv_train(rfr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(rfr).mean())\ny_train_rfr = rfr.predict(X_train)\ny_test_rfr = rfr.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rfr, y_train_rfr - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rfr, y_test_rfr - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rfr, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rfr, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","166c724f":"rfr.score(X_test,y_test)\n","37f1362e":"prediction = rfr.predict(X_test)\nprint(prediction)","0c593ef9":"\narr=np.array(np.expm1(prediction)).T\nprint(arr)\n\n","7ef30f3e":"print(arr.size)","6f3fe846":"df_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","e2970332":"# Handle missing values for features where median\/mean or most common value doesn't make sense\n\n# Alley : data description says NA means \"no alley access\"\ndf_test.loc[:, \"Alley\"] = df_test.loc[:, \"Alley\"].fillna(\"None\")\n# BedroomAbvGr : NA most likely means 0\ndf_test.loc[:, \"BedroomAbvGr\"] = df_test.loc[:, \"BedroomAbvGr\"].fillna(0)\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ndf_test.loc[:, \"BsmtQual\"] = df_test.loc[:, \"BsmtQual\"].fillna(\"No\")\ndf_test.loc[:, \"BsmtCond\"] = df_test.loc[:, \"BsmtCond\"].fillna(\"No\")\ndf_test.loc[:, \"BsmtExposure\"] = df_test.loc[:, \"BsmtExposure\"].fillna(\"No\")\ndf_test.loc[:, \"BsmtFinType1\"] = df_test.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ndf_test.loc[:, \"BsmtFinType2\"] = df_test.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ndf_test.loc[:, \"BsmtFullBath\"] = df_test.loc[:, \"BsmtFullBath\"].fillna(0)\ndf_test.loc[:, \"BsmtHalfBath\"] = df_test.loc[:, \"BsmtHalfBath\"].fillna(0)\ndf_test.loc[:, \"BsmtUnfSF\"] = df_test.loc[:, \"BsmtUnfSF\"].fillna(0)\n# CentralAir : NA most likely means No\ndf_test.loc[:, \"CentralAir\"] = df_test.loc[:, \"CentralAir\"].fillna(\"N\")\n# Condition : NA most likely means Normal\ndf_test.loc[:, \"Condition1\"] = df_test.loc[:, \"Condition1\"].fillna(\"Norm\")\ndf_test.loc[:, \"Condition2\"] = df_test.loc[:, \"Condition2\"].fillna(\"Norm\")\n# EnclosedPorch : NA most likely means no enclosed porch\ndf_test.loc[:, \"EnclosedPorch\"] = df_test.loc[:, \"EnclosedPorch\"].fillna(0)\n# External stuff : NA most likely means average\ndf_test.loc[:, \"ExterCond\"] = df_test.loc[:, \"ExterCond\"].fillna(\"TA\")\ndf_test.loc[:, \"ExterQual\"] = df_test.loc[:, \"ExterQual\"].fillna(\"TA\")\n# Fence : data description says NA means \"no fence\"\ndf_test.loc[:, \"Fence\"] = df_test.loc[:, \"Fence\"].fillna(\"No\")\n# FireplaceQu : data description says NA means \"no fireplace\"\ndf_test.loc[:, \"FireplaceQu\"] = df_test.loc[:, \"FireplaceQu\"].fillna(\"No\")\ndf_test.loc[:, \"Fireplaces\"] = df_test.loc[:, \"Fireplaces\"].fillna(0)\n# Functional : data description says NA means typical\ndf_test.loc[:, \"Functional\"] = df_test.loc[:, \"Functional\"].fillna(\"Typ\")\n# GarageType etc : data description says NA for garage features is \"no garage\"\ndf_test.loc[:, \"GarageType\"] = df_test.loc[:, \"GarageType\"].fillna(\"No\")\ndf_test.loc[:, \"GarageFinish\"] = df_test.loc[:, \"GarageFinish\"].fillna(\"No\")\ndf_test.loc[:, \"GarageQual\"] = df_test.loc[:, \"GarageQual\"].fillna(\"No\")\ndf_test.loc[:, \"GarageCond\"] = df_test.loc[:, \"GarageCond\"].fillna(\"No\")\ndf_test.loc[:, \"GarageArea\"] = df_test.loc[:, \"GarageArea\"].fillna(0)\ndf_test.loc[:, \"GarageCars\"] = df_test.loc[:, \"GarageCars\"].fillna(0)\n# HalfBath : NA most likely means no half baths above grade\ndf_test.loc[:, \"HalfBath\"] = df_test.loc[:, \"HalfBath\"].fillna(0)\n# HeatingQC : NA most likely means typical\ndf_test.loc[:, \"HeatingQC\"] = df_test.loc[:, \"HeatingQC\"].fillna(\"TA\")\n# KitchenAbvGr : NA most likely means 0\ndf_test.loc[:, \"KitchenAbvGr\"] = df_test.loc[:, \"KitchenAbvGr\"].fillna(0)\n# KitchenQual : NA most likely means typical\ndf_test.loc[:, \"KitchenQual\"] = df_test.loc[:, \"KitchenQual\"].fillna(\"TA\")\n# LotFrontage : NA most likely means no lot frontage\ndf_test.loc[:, \"LotFrontage\"] = df_test.loc[:, \"LotFrontage\"].fillna(0)\n# LotShape : NA most likely means regular\ndf_test.loc[:, \"LotShape\"] = df_test.loc[:, \"LotShape\"].fillna(\"Reg\")\n# MasVnrType : NA most likely means no veneer\ndf_test.loc[:, \"MasVnrType\"] = df_test.loc[:, \"MasVnrType\"].fillna(\"None\")\ndf_test.loc[:, \"MasVnrArea\"] = df_test.loc[:, \"MasVnrArea\"].fillna(0)\n# MiscFeature : data description says NA means \"no misc feature\"\ndf_test.loc[:, \"MiscFeature\"] = df_test.loc[:, \"MiscFeature\"].fillna(\"No\")\ndf_test.loc[:, \"MiscVal\"] = df_test.loc[:, \"MiscVal\"].fillna(0)\n# OpenPorchSF : NA most likely means no open porch\ndf_test.loc[:, \"OpenPorchSF\"] = df_test.loc[:, \"OpenPorchSF\"].fillna(0)\n# PavedDrive : NA most likely means not paved\ndf_test.loc[:, \"PavedDrive\"] = df_test.loc[:, \"PavedDrive\"].fillna(\"N\")\n# PoolQC : data description says NA means \"no pool\"\ndf_test.loc[:, \"PoolQC\"] = df_test.loc[:, \"PoolQC\"].fillna(\"No\")\ndf_test.loc[:, \"PoolArea\"] = df_test.loc[:, \"PoolArea\"].fillna(0)\n# SaleCondition : NA most likely means normal sale\ndf_test.loc[:, \"SaleCondition\"] = df_test.loc[:, \"SaleCondition\"].fillna(\"Normal\")\n# ScreenPorch : NA most likely means no screen porch\ndf_test.loc[:, \"ScreenPorch\"] = df_test.loc[:, \"ScreenPorch\"].fillna(0)\n# TotRmsAbvGrd : NA most likely means 0\ndf_test.loc[:, \"TotRmsAbvGrd\"] = df_test.loc[:, \"TotRmsAbvGrd\"].fillna(0)\n# Utilities : NA most likely means all public utilities\ndf_test.loc[:, \"Utilities\"] = df_test.loc[:, \"Utilities\"].fillna(\"AllPub\")\n# WoodDeckSF : NA most likely means no wood deck\ndf_test.loc[:, \"WoodDeckSF\"] = df_test.loc[:, \"WoodDeckSF\"].fillna(0)","08aa8358":"# Some numerical features are actually really categories\ndf_test = df_test.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","2a12a36c":"# Encode some categorical features as ordered numbers when there is information in the order\ndf_test = df_test.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}})","5ae89008":"df_test.head()","a5513070":"df_test.drop(['GarageQual', 'BsmtCond', 'GrLivArea', 'Fireplaces', 'PoolArea', 'GarageCars', 'BsmtFinType2', 'BsmtQual', 'BsmtFinType1','PoolQC','Alley','MiscFeature','Fence'],axis=1)","75ef3aaa":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features_test = df_test.select_dtypes(include = [\"object\"]).columns\nnumerical_features_test = df_test.select_dtypes(exclude = [\"object\"]).columns\nprint(\"Numerical features : \" + str(len(numerical_features_test)))\nprint(\"Categorical features : \" + str(len(categorical_features_test)))\ndf_test_num = df_test[numerical_features]\ndf_test_cat = df_test[categorical_features]","b86255e0":"year_feat_test = [features for features in df_test_num if 'Yr' in features or 'Year' in features]\nyear_feat_test","4c59d4df":"\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    df_test[feature]=df_test['YrSold']-df_test[feature]\n# we are replacing the years with difference in years\n# because the price is highly dependent on the diff rather than the year in which was made \n# this can be seen from my code on EDA in line 15 and below also","bb14509b":"df_test.head()","03cd71e4":"print(\"NAs for numerical features in train : \" + str(df_test_num.isnull().values.sum()))\ndf_test_num = df_test_num.fillna(df_test_num.median())\nprint(\"NAs for numerical features in train : \" + str(df_test_num.isnull().values.sum()))\nprint(\"NAs for categorical features in train : \" + str(df_test_cat.isnull().values.sum()))\ndf_test_cat = pd.get_dummies(df_test_cat)\nprint(\"NAs for categorical features in train : \" + str(df_test_cat.isnull().values.sum()))\n\n# Create dummy features for categorical values via pd.get_dummies encoding\n\n\n","a56cfe71":"\n\n# Log transform of the right skewed numerical features to lessen impact of outliers\n# As a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed\nskewness = df_test_num.apply(lambda x: skew(x))\nskewness = skewness[abs(skewness) > 0.5]\nprint(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\nskewed_features = skewness.index\ndf_test_num[skewed_features] = np.log1p(df_test_num[skewed_features])","bef2bac6":"data_test = pd.concat([df_test_num, df_test_cat], axis = 1)\ndata_test.head(10)","c61e997d":"data_test.loc[:, numerical_features_test] = stdSc.transform(data_test.loc[:, numerical_features_test])","01d6e103":"data_test.shape","3bef4f53":"data_test.describe()","5d931607":"final_data_train, final_data_test = data.align(data_test,\n                                                                    join='left', \n                                                                    axis=1)\n\n","b109904d":"final_data_train.shape","58445dd0":"final_data_test.shape","504bdb8b":"## since we have NAN values after doing the above thing, we can set the nan values to zero since they all come from the categorical \n# variables only\n#df.replace(np.nan, 0)\n# \ntestdata = final_data_test.fillna(0)\ntestdata.head()\n","461673aa":"test_pred=np.expm1(rfr.predict(testdata))\ntest_pred","7aed5ed0":"\nprice=pd.DataFrame()\nprice['Id']= df_test['Id']\nprice['SalePrice']= test_pred\nprice.to_csv('submission.csv',index=False)\nprice\n\n","0f0450d7":"https:\/\/www.yourdatateacher.com\/2021\/06\/28\/a-python-library-to-remove-collinearity\/","cf68a98b":"ridge deleted only a small no. of features and hence it resulted in a better RMSE score comapred to LASSO !","561d431b":"\nWe cannnot do standardization before the partitioning as the model will learn about the test data set, which we don't want to happen!","037047df":"## 2. L2 or Ridge regression:\n#### this adds regularization term given by squared sum of weights to our cost function","7efdb6ef":"## highly right skewed data, will have to normalize them!","d69f3d6d":"## doing a train test split in order to see how well our algorithm\/regressions perform!\n### we perform our test train - test split only after we have taken care of the data set ( i.e. cleaned, deleted rows\/columns and other preprocessing techniques)","23d990e2":"## Our data is now ready and pre-processed. We will now run regressions and also work usinf Random Forest Regressor to achieve the best results!","fa9cf74c":"https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding\n### How can I align pandas get_dummies across training \/ validation \/ testing? :https:\/\/stackoverflow.com\/a\/60784347","417682d8":"![WhatsApp-Image-2020-07-01-at-2.09.25-AM.jpeg](attachment:8e1c0198-22d7-44f0-9f58-43dc9787c85b.jpeg)","45c9fdc6":"### we will definitely drop these columns","0c0b53e3":"one can check that the rmse score is the least for random forest regressor when it comes to the test data set!","52ff9285":"https:\/\/www.geeksforgeeks.org\/replace-nan-values-with-zeros-in-pandas-dataframe\/","6ef42267":"## 1. LASSO Regression ( or L1 penalty):\n#### lasso stands for least absolute shrinkage and selection operator, so Lasso itself will search for the important features and work with it!\n","b712915f":"i believe that something is wrong here!\nRMSE is very very poor compared to previous regularization techniques. i will try to see what goes possibly wrong here! till then i will try my hands using the ##Random Forest Regressor##","340ffb0d":"## 4. Random Forest regressor","cf3928e8":"now, we will drop the above features along with those which had more than 50% missing values which were: ['Alley', 'PoolQC', 'Fence', 'MiscFeature']","e8aa41d3":"Parameters:\ncorrelation_threshold : float (between 0 and 1), default = 0.4\nOnly those features that produce a correlation matrix with off-diagonal elements that are, in absolute value, less than this threshold will be chosen.\nscoring : callable, default=f_classif\nThe scoring function for supervised problems. It must be the same accepted by sklearn.feature_selection.SelectKBest.","16875b6c":"Let\u2019s calculate our model\u2019s score using x_test and y_test.\n","4108a36a":"## so now our data set set also has the same no. of features. Earlier we were facing a issue that we couldn't predict since the no. of features were differnt in both the cases!","e47de0be":"we have obtained good RMSE values on both training and testing data set. The lasso regression deleted more than 50% of the features and worked with less than 50% of the total features. ","dba59faf":"The align command makes sure the columns show up in the same order in both datasets (it uses column names to identify which columns line up in each dataset.) The argument join='left' specifies that we will do the equivalent of SQL's left join. That means, if there are ever columns that show up in one dataset and not the other, we will keep exactly the columns from our training data. The argument join='inner' would do what SQL databases call an inner join, keeping only the columns showing up in both datasets. That's also a sensible choice.","7fff2921":"### Let us see that happens if we didnt add any regularizaition? will the RMSE score be better or poor?\n","75aaf789":"so we will use the random forest regressor to predict the values on the test data set!","f9f54bcd":"## Let's start working on our test data set (test.csv) to output the required sale price prediction!\nwe will work in a similar fashion as we had done for the train.csv case!\n","cec45405":"### this is the new thing that i learned in this case i.e. how to tackle a situation where after apply encoding (pd.get_dummies) we get a different number of features !\n","822fb105":"this is the logarithmic values since we had applied log1p. now lets see how the test values behave when we undo the logarithm","95e4f0f2":"## 3. Linear regression","2cb05b46":"- Best for Visualization: https:\/\/www.kaggle.com\/fightingmuscle\/the-power-of-normality-and-visualization\n- Example for Lasso and Ridge: https:\/\/www.kaggle.com\/fightingmuscle\/eda-more-technical\n- EDA and filling missing values: https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\/notebook\n\n","65758e50":"1. Since there are many variables , we will first perform data cleaning\n2. Delete those columns which have high no. of missing values\n3. After that we fill the missing data in other rows? By median in numerical case as the data is highly - boxplot-outlier\n4. For categorical data, we can replace them with ????\n5. Perform lasso regression so that other unnecessary data is deleted on its own and only the important ones are selected!\n6. Similarly for test data we delete the one that we deleted as they had lot of missing values\n7. By doing so dataset is equal and then we can fill the missing values in data set by mean and for categorical in a similar fashion, but is it the right way to do so?\n8. Then we can use XGBooster also! This way we will have two ML algo updated in the same code\n\n\n","23b7ad54":"#Linear Regression\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n#Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n\n######Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n####Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","650a9a22":"Lasso linear model with iterative fitting along a regularization path.\n\nSee glossary entry for cross-validation estimator.\n\nThe best model is selected by cross-validation.\n\nThe optimization objective for Lasso is:"}}