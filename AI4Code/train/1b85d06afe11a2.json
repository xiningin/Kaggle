{"cell_type":{"72b0953e":"code","97c50a2d":"code","cd7e2b80":"code","65732be5":"code","151044ac":"code","c1a104d4":"code","0f396b08":"code","efcba68e":"code","d30332e6":"code","7c3c1a28":"code","e6827069":"code","62a0f5a8":"code","4cfebadd":"code","6a2225a3":"code","8173be19":"code","dd842708":"code","82813b09":"code","c52b8ae1":"markdown"},"source":{"72b0953e":"debug = 1\nseed = 42\n\ndevice = 'cuda'\nbert_model = 'sentence-transformers\/stsb-bert-large'\nmaxlen=128\nfreeze_bert=True\nearly_stop=True\nfinetune_units =1024\ndropout_rate = 0.2\nes_counts_MAX = 3\n\nbs = 16\nlr = 2e-5  \nif debug:\n    epochs = 4\n    num_warmup_steps = 0\nelse:\n    epochs = 15\n    num_warmup_steps = 4","97c50a2d":"from scipy.spatial import distance\nfrom scipy.spatial.distance import cosine\nimport nltk\nfrom scipy.stats import pearsonr\n\nimport os\nimport numpy as np \nimport pandas as pd\nimport copy\nimport matplotlib.pyplot as plt\nimport random\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, Dataset\n\nimport torch\nfrom torch import nn\nimport transformers\nfrom transformers import AutoTokenizer,AutoModel, AdamW, get_linear_schedule_with_warmup\n\n#float16\u548cfloat32\u81ea\u52a8\u6df7\u5408\u7cbe\u5ea6\u52a0\u901f\u8ba1\u7b97\uff0c\u5b98\u65b9\u6587\u6863\uff1ahttps:\/\/pytorch.org\/docs\/stable\/amp.html\nfrom torch.cuda.amp import autocast\nfrom torch.cuda.amp import GradScaler","cd7e2b80":"def set_seed(seed = 42):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return seed\n\nseed = set_seed(seed)","65732be5":"ss=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-sample_submission.csv')\ntrain=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-train.csv')\ntest=pd.read_csv('..\/input\/kkb-repl4nlp-assignment0\/sts-kaggle-test.csv')\ntrain","151044ac":"train.drop('id',axis=1,inplace=True)\ntrain['similarity'] -= 2.5\ntrain['similarity'] \/= 2.5\ntrain\n\n","c1a104d4":"train.describe()","0f396b08":"train.info()","efcba68e":"df_train,df_val = train_test_split(train,random_state = seed)\ndf_train.reset_index(inplace=True,drop=True)\ndf_val.reset_index(inplace=True,drop=True)\ndf_train","d30332e6":"df_val","7c3c1a28":"class LoadDataset(Dataset):\n    def __init__(self, data, maxlen, with_labels=True, bert_model='bert-base-uncased'):\n        self.data = data\n        self.tokenizer = AutoTokenizer.from_pretrained(bert_model,output_loading_info = False)  \n        self.maxlen = maxlen\n        self.with_labels = with_labels \n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        # Selecting sentence1 and sentence2 at the specified index in the data frame\n        sent1 = str(self.data.loc[index,'sentence_a'])\n        sent2 = str(self.data.loc[index,'sentence_b'])\n\n        # Tokenize the pair of sentences to get token ids, attention masks and token type ids\n        encoded_input1 = self.tokenizer(sent1, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        encoded_input2 = self.tokenizer(sent2, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n        \n        token_ids1 =  encoded_input1['input_ids'].squeeze(0) \n        attn_masks1 =  encoded_input1['attention_mask'].squeeze(0)  \n        token_type_ids1 =  encoded_input1['token_type_ids'].squeeze(0) \n\n        token_ids2 =  encoded_input2['input_ids'].squeeze(0)  \n        attn_masks2 =  encoded_input2['attention_mask'].squeeze(0) \n        token_type_ids2 =  encoded_input2['token_type_ids'].squeeze(0) \n        \n        if self.with_labels:  # True if the dataset has labels\n            label = self.data.loc[index, 'similarity']\n            return token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2,label  \n        else:\n            return token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2\n        ","e6827069":"def val_lossF(net, device, criterion, dataloader):\n    net.eval()\n    mean_loss = 0\n    count = 0\n    true_labelss =[]\n    list_val_outputs = []\n    val_metric = 0\n    \n    with torch.no_grad():\n        for  i, (token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2,labels) in enumerate(dataloader):\n            token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n            token_ids2, attn_masks2, token_type_ids2 =  token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n            labels = labels.to(device)\n            \n#             ot1,ot2 = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n#             cs = nn.CosineSimilarity()\n#             cs = nn.CosineSimilarity()\n#             val_output = cs(ot1,ot2)\n#             print('val_output = ',output)\n            val_output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n            mean_loss += criterion(val_output, labels.float()).item()\n            count += 1\n\n            val_outputs = val_output.cpu().numpy()\n            list_val_outputs += val_outputs.tolist()\n            labelss = labels.cpu().numpy()\n            true_labelss += labelss.tolist()  \n\n        val_metric,_ = pearsonr(list_val_outputs,true_labelss)       \n    return mean_loss \/ count, val_metric","62a0f5a8":"class TextSimilarityModel(nn.Module):\n    def __init__(self, dropout_rate=0.2, finetune_units=1024, bert_model='bert-base-uncased', freeze_bert=False):\n        super(TextSimilarityModel, self).__init__()\n        self.bert_layer = AutoModel.from_pretrained(bert_model,output_loading_info = False)\n        \n        if bert_model == \"albert-base-v2\":  # 12M parameters\n            hidden_size = 768\n        elif bert_model == \"albert-large-v2\":  # 18M parameters\n            hidden_size = 1024\n        elif bert_model == \"albert-xlarge-v2\":  # 60M parameters\n            hidden_size = 2048\n        elif bert_model == \"albert-xxlarge-v2\":  # 235M parameters\n            hidden_size = 4096\n        elif bert_model == \"bert-base-uncased\": # 110M parameters\n            hidden_size = 768\n        elif bert_model == 'sentence-transformers\/stsb-bert-large':\n            hidden_size = 1024\n            \n        if freeze_bert:\n            for p in self.bert_layer.parameters():\n                p.requires_grad = False\n\n        self.dropout0 = nn.Dropout(p=dropout_rate)\n        self.linear1 = nn.Linear(hidden_size, finetune_units)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=dropout_rate)\n        self.linear2 = nn.Linear(finetune_units, finetune_units)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(p=dropout_rate)\n        self.vec_layer = nn.Linear(finetune_units,hidden_size)\n        \n        self.cs_layer = nn.CosineSimilarity(dim=1)\n        \n    def mean_pooling(self,all_vecs, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(all_vecs.size()).float()\n        sum_embeddings = torch.sum(all_vecs * input_mask_expanded, 1)\n        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n        return sum_embeddings \/ sum_mask\n    \n    @autocast()\n    def forward(self,  token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2):\n        all_vecs1, cls_vecs1 = self.bert_layer( token_ids1, attn_masks1, token_type_ids1)\n        all_vecs2, cls_vecs2 = self.bert_layer(token_ids2, attn_masks2, token_type_ids2)\n        \n        #Perform pooling. In this case, mean pooling\n        sent_embed1 = self.mean_pooling(all_vecs1, attn_masks1)\n        sent_embed2 = self.mean_pooling(all_vecs2, attn_masks2)\n\n        # vec1\n        x1 = self.dropout0(sent_embed1)\n#         x1 = self.relu1(self.linear1(x1))\n#         x1 = self.dropout1(x1)\n#         x1 = self.relu2(self.linear2(x1))\n#         x1 = self.dropout2(x1)\n        x1 = self.vec_layer(x1)\n        \n        # vec2\n        x2 = self.dropout0(sent_embed2)\n#         x2 = self.relu1(self.linear1(x2))\n#         x2 = self.dropout1(x2)\n#         x2 = self.relu2(self.linear2(x2))\n#         x2 = self.dropout2(x2)\n        x2 = self.vec_layer(x2)\n    \n#         print('x1 = ',x1)\n#         print('x1.shape = ',x1.shape)\n#         print('x2 = ',x2)\n#         print('x2.shape = ',x2.shape)\n        output = self.cs_layer(x1,x2)\n#         print('output = ',output)\n#         print('output.shape = ',output.shape) \n        return output\n#         return x1, x2","4cfebadd":"device = torch.device(device if torch.cuda.is_available() else \"cpu\")\nnet = TextSimilarityModel(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model,freeze_bert=freeze_bert)\nnet.to(device)","6a2225a3":"train_set = LoadDataset(df_train, maxlen, bert_model)\nval_set = LoadDataset(df_val, maxlen, bert_model)\ntrain_loader = DataLoader(train_set, batch_size=bs)\nval_loader = DataLoader(val_set, batch_size=bs)\n\ncriterion = nn.MSELoss()\nopti = AdamW(net.parameters(), lr=lr, weight_decay=1e-2)\nnum_training_steps = epochs * len(train_loader)  # The total number of training steps\nlr_scheduler = get_linear_schedule_with_warmup(optimizer=opti, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps )\nscaler = GradScaler()\n\nbest_loss = np.Inf\nbest_ep = 1\niters = []\ntrain_losses = []\nval_losses = []\nval_metrics = []\nes_count = 0\nfor ep in range(epochs):\n    net.train()\n    for it, (token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2,labels) in enumerate(train_loader):\n        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n        token_ids2, attn_masks2, token_type_ids2 = token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n        labels = labels.to(device)\n\n        opti.zero_grad()\n        with autocast():\n#             ot1,ot2 = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n#             cs=nn.CosineSimilarity()\n#             #output =  1.- cs(ot1,ot2)\n#             output = cs(ot1,ot2)\n            output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n            loss = criterion(output, labels.float())\n        scaler.scale(loss).backward()\n        scaler.step(opti)\n        scaler.update()\n        \n        lr_scheduler.step()\n        if it%50 ==0:\n            val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss\n            print(\"it = {}, train_loss = {}, val_loss = {}, val_metric= {}\".format(it+1,loss,val_loss,val_metric))\n            \n    val_loss, val_metric = val_lossF(net, device, criterion, val_loader)  # Compute validation loss  \n    print(\"Epoch {} complete! Train Loss : {} , Validation Loss : {} , Validation Metric - Pearsonr : {} \".format(ep+1, loss, val_loss, val_metric))\n    train_losses.append(loss)\n    val_losses.append(val_loss)  \n    val_metrics.append(val_metric)\n    if val_loss < best_loss:       \n        print(\"Best validation loss improved from {} to {}\".format(best_loss, val_loss))\n        net_copy = copy.deepcopy(net)  # save a copy of the model\n        best_loss = val_loss\n        best_ep = ep + 1\n        path_to_model='lr_{}_val_loss_{}_ep_{}.pt'.format(lr, round(best_loss, 5), best_ep)\n        torch.save(net_copy.state_dict(), path_to_model)\n        print(\"The model has been saved in {}\".format(path_to_model))\n    else:\n        es_count += 1\n    \n    if early_stop and es_count>es_counts_MAX:\n        print('Early Stop Train in Epoch : {} '.format(ep+1))\n        break\n\ndel loss\ntorch.cuda.empty_cache()","8173be19":"p1 = plt.plot(range(ep+1),train_losses,'b--',label='train_loss')\np2 = plt.plot(range(ep+1),val_losses,'r--',label='validation_loss')\np3 = plt.plot(range(ep+1),val_metrics,'g--',label='validation_metric')\nplt.plot(range(ep+1),train_losses,'bo-',range(ep+1),val_losses,'r+-',range(ep+1),val_metrics,'g^-')\nplt.title('Loss')\nplt.xlabel('epoch')\nplt.ylabel('loss & metric')\nplt.legend()\nplt.show()","dd842708":"net = TextSimilarityModel(dropout_rate=dropout_rate, finetune_units=finetune_units,bert_model=bert_model)\nnet.load_state_dict(torch.load(path_to_model))\nnet.to(device)\n\ntest_set = LoadDataset(test, maxlen, with_labels=False, bert_model = bert_model)\ntest_loader = DataLoader(test_set, batch_size=bs)\n\nnet.eval()\nresults = []\nwith torch.no_grad():\n    for token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2 in tqdm(test_loader):\n        token_ids1, attn_masks1, token_type_ids1 = token_ids1.to(device), attn_masks1.to(device), token_type_ids1.to(device)\n        token_ids2, attn_masks2, token_type_ids2 =  token_ids2.to(device), attn_masks2.to(device), token_type_ids2.to(device)\n#         ot1,ot2 = net(token_ids1, attn_masks1, token_type_ids1, token_ids2, attn_masks2, token_type_ids2)\n#         cs=nn.CosineSimilarity()\n#         output = cs(ot1,ot2)\n        output = net(token_ids1, attn_masks1, token_type_ids1,token_ids2, attn_masks2, token_type_ids2)\n        output = output.cpu().numpy()\n        results += output.tolist()\n\ntest['similarity'] = results","82813b09":"ss['similarity'] = (test['similarity'] * 2.5) + 2.5\nss.set_index('id',inplace = True)\nss.to_csv('submission.csv')\nss","c52b8ae1":"finetune\u4e4b\u540e\u7684\u7ed3\u679c\u5f88\u5dee\uff1a\nfreezebert\u7684\u6700\u5dee\uff0c\n\u53eafinetune linear\u5c42\u4f1a\u597d\u4e00\u70b9\uff0c\u4f46\u8fd8\u662f\u4e0d\u5982\u76f4\u63a5\u7528pretrianed model\u505a\u63a8\u7406\u5f3a\uff01\uff01\uff01\n\n\u6709\u65f6\u95f4\u8bd5\u8bd5textcnn\u5427\n"}}