{"cell_type":{"367ef12d":"code","dc24e530":"code","bb7344fb":"code","1d45fcae":"code","e93241d6":"code","cf1e62a1":"code","a9f40226":"code","8a1cf280":"code","b3b137cd":"code","03ffaa99":"code","4dad1934":"code","cda08966":"code","a73d3ea2":"code","976ade93":"code","2b40987b":"code","eaa39c24":"code","686a6895":"code","16eb0ea4":"code","b0a700af":"code","7c336c63":"markdown","f3fa54f2":"markdown","2a60d6d8":"markdown","9cde8e37":"markdown","45671f53":"markdown","18b5a7b1":"markdown","0ad0ee00":"markdown","b02543df":"markdown","31d825cc":"markdown","c7c771b8":"markdown"},"source":{"367ef12d":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nfrom sklearn.metrics import f1_score as f1\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import *\nfrom imblearn.over_sampling import SMOTE\n\n#-- Pytorch specific libraries import -----#\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader","dc24e530":"df_data=pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf_data.columns","bb7344fb":"df_data.shape","1d45fcae":"#OHE of Categorical features\ndf_data.Attrition_Flag = df_data.Attrition_Flag.replace({'Attrited Customer':1,'Existing Customer':0})\ndf_data.Gender = df_data.Gender.replace({'F':1,'M':0})\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Education_Level']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Income_Category']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Marital_Status']).drop(columns=['Unknown'])],axis=1)\ndf_data = pd.concat([df_data,pd.get_dummies(df_data['Card_Category']).drop(columns=['Platinum'])],axis=1)\ndf_data.drop(columns = ['Education_Level','Income_Category','Marital_Status','Card_Category','CLIENTNUM'],inplace=True)","e93241d6":"df_data.shape","cf1e62a1":"#SMOTE upsampling\noversample = SMOTE()\nX, y = oversample.fit_resample(df_data[df_data.columns[1:]], df_data[df_data.columns[0]])","a9f40226":"df_data.columns[1:]","8a1cf280":"df_data.columns[0]","b3b137cd":"upsampled_df = pd.DataFrame(data=X,columns=df_data.columns[1:])\nupsampled_df = upsampled_df.assign(Churn = y)\nohe_data =upsampled_df[upsampled_df.columns[15:-1]].copy()\nupsampled_df = upsampled_df.drop(columns=upsampled_df.columns[15:-1])\nupsampled_df.shape","03ffaa99":"upsampled_df.dtypes","4dad1934":"#Train & Test Set\nX= upsampled_df.loc[: , upsampled_df.columns != 'Churn']\n#y = upsampled_df['Churn']\ny = pd.DataFrame(upsampled_df['Churn'])\n\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,random_state=42)\nprint(test_x.shape)\nprint(test_y.shape)","cda08966":"###First use a MinMaxscaler to scale all the features of Train & Test dataframes\n\nscaler = preprocessing.MinMaxScaler()\nx_train = scaler.fit_transform(train_x.values)\nx_test =  scaler.fit_transform(test_x.values)\n\nprint(\"Scaled values of Train set \\n\")\nprint(x_train)\nprint(\"\\nScaled values of Test set \\n\")\nprint(x_test)\n\n\n###Then convert the Train and Test sets into Tensors\n\nx_tensor =  torch.from_numpy(x_train).float()\ny_tensor =  torch.from_numpy(train_y.values.ravel()).float()\nxtest_tensor =  torch.from_numpy(x_test).float()\nytest_tensor =  torch.from_numpy(test_y.values.ravel()).float()\n\nprint(\"\\nTrain set Tensors \\n\")\nprint(x_tensor)\nprint(y_tensor)","a73d3ea2":"#Define a batch size , hyperparameter can be further tuned\nbs = 64\n#Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice\ny_tensor = y_tensor.unsqueeze(1)\ntrain_ds = TensorDataset(x_tensor, y_tensor)\n#Pytorch\u2019s DataLoader is responsible for managing batches. \n#You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches\ntrain_dl = DataLoader(train_ds, batch_size=bs)\n\n\n#For the validation\/test dataset\nytest_tensor = ytest_tensor.unsqueeze(1)\ntest_ds = TensorDataset(xtest_tensor, ytest_tensor)\ntest_loader = DataLoader(test_ds, batch_size=32)","976ade93":"n_input_dim = train_x.shape[1]\n\n#Layer size\nn_hidden1 = 120  # Number of hidden nodes\nn_hidden2 = 100\nn_output =  1   # Number of output nodes = for binary classifier\n\n\nclass ChurnModel(nn.Module):\n    def __init__(self):\n        super(ChurnModel, self).__init__()\n        self.layer_1 = nn.Linear(n_input_dim, n_hidden1) \n        self.layer_2 = nn.Linear(n_hidden1, n_hidden2)\n        self.layer_out = nn.Linear(n_hidden2, n_output) \n        \n        \n        self.relu = nn.ReLU()\n        self.sigmoid =  nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.1)\n        self.batchnorm1 = nn.BatchNorm1d(n_hidden1)\n        self.batchnorm2 = nn.BatchNorm1d(n_hidden2)\n        \n        \n    def forward(self, inputs):\n        x = self.relu(self.layer_1(inputs))\n        x = self.batchnorm1(x)\n        x = self.relu(self.layer_2(x))\n        x = self.batchnorm2(x)\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer_out(x))\n        \n        return x\n    \n\nmodel = ChurnModel()\nprint(model)","2b40987b":"#Loss Computation\nloss_func = nn.BCELoss()\n#Optimizer\nlearning_rate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\nepochs = 50","eaa39c24":"model.train()\ntrain_loss = []\nfor epoch in range(epochs):\n    #Within each epoch run the subsets of data = batch sizes.\n    for xb, yb in train_dl:\n        y_pred = model(xb)            # Forward Propagation\n        loss = loss_func(y_pred, yb)  # Loss Computation\n        optimizer.zero_grad()         # Clearing all previous gradients, setting to zero \n        loss.backward()               # Back Propagation\n        optimizer.step()              # Updating the parameters\n        #optimizer.zero_grad() \n    #print(\"Loss in iteration :\"+str(epoch)+\" is: \"+str(loss.item()))\n    train_loss.append(loss.item())\nprint('Last iteration loss value: '+str(loss.item()))","686a6895":"plt.plot(train_loss)\nplt.show()","16eb0ea4":"import itertools\n\ny_pred_list = []\nmodel.eval()\n#Since we don't need model to back propagate the gradients in test set we use torch.no_grad()\n# reduces memory usage and speeds up computation\nwith torch.no_grad():\n    for xb_test,yb_test  in test_loader:\n        y_test_pred = model(xb_test)\n        y_pred_tag = torch.round(y_test_pred)\n        y_pred_list.append(y_pred_tag.detach().numpy())\n\n#Takes arrays and makes them list of list for each batch        \ny_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n#flattens the lists in sequence\nytest_pred = list(itertools.chain.from_iterable(y_pred_list))","b0a700af":"y_true_test = test_y.values.ravel()\nconf_matrix = confusion_matrix(y_true_test ,ytest_pred)\nprint(\"Confusion Matrix of the Test Set\")\nprint(\"-----------\")\nprint(conf_matrix)\nprint(\"Precision of the MLP :\\t\"+str(precision_score(y_true_test,ytest_pred)))\nprint(\"Recall of the MLP    :\\t\"+str(recall_score(y_true_test,ytest_pred)))\nprint(\"F1 Score of the Model :\\t\"+str(f1_score(y_true_test,ytest_pred)))","7c336c63":"## Pytorch - MLP implementation","f3fa54f2":"### Dataloader to pass data in batches","2a60d6d8":"### Test Dataset prediction on trained NN","9cde8e37":"### MLP (Model) \n\nDefine the Layers , Activation function , Number of nodes for the MultiLayerPerceptron\n\nStructure of MLP\n\n* 2 Hidden Layers\n* Normalizing the batch data usign batchnorm in between each layer\n* Using ReLU Activation function between the layers\n* Using dropout before sending to output\n* Sigmoid to make probabilities between 0 to 1","45671f53":"## Data Preprocessing \n\n* OHE of Categorical features\n* Up-sampling using SMOTE\n* Dropping redundant and unwanted fields","18b5a7b1":" * This Notebook is intended to showcase a Multilayer Perceptron **(MLP) implementation in Pytorch using structured dataset** such as this - Credit Card data.\n * EDA etc of the data is not carried out , there are plenty of good notebooks for this dataset depicting  the same , you can refer them over : [here](https:\/\/www.kaggle.com\/thomaskonstantin\/bank-churn-data-exploration-and-churn-prediction)\n","0ad0ee00":"### Defining \n\n* Loss computation function : Here using Binary Cross Entropy (BCE) which is defacto for Binary class problems\n* Learning rate : Setting as 0.001 (can be optimized further)\n* Optimizer : Using Adam and\n* Epochs of Training : setting as 50 ","b02543df":"### Training the MLP Model\n\nNN Steps\n1. Forward Propagation\n2. Loss computation\n3. Backpropagation\n4. Updating the parameters","31d825cc":"### Converting Data into Pytorch Tensors","c7c771b8":"Plotting the loss function shows it stabilized after 20th epoch itself"}}