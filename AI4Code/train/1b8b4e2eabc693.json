{"cell_type":{"3d5e343b":"code","fc177f87":"code","f29cc407":"code","33944ba0":"code","316aba84":"code","d884d1fd":"code","c98f19a6":"code","4ad6a46a":"code","ae67d9a9":"code","99bcb47c":"code","f45d6d05":"code","50c30cab":"code","d66e7c28":"code","0e72b610":"code","33fdd90e":"code","a443f523":"code","3984a21f":"code","ddd5cdc5":"code","c2715295":"code","b081c515":"code","1e192d93":"code","6d692ab2":"code","9e8565d1":"code","285c3e13":"code","39165796":"code","ccd99fe6":"code","4e933dfc":"code","f6b599c2":"code","ca08a74d":"code","e8085307":"code","661e916d":"code","d2c12187":"code","1e01fdce":"code","cb0b6a88":"code","bcfd250f":"code","47f34b52":"code","db705982":"code","5e7ec5d5":"code","a1c9d992":"code","308a33ca":"code","26f9172a":"code","22d98936":"code","f35aa1a5":"code","e48dcb41":"code","a168499d":"code","0555a62a":"code","b702b7af":"code","70bc469a":"code","c490996d":"code","9a25dc59":"code","8f8aee76":"code","960ad0f4":"code","3fca67f0":"code","ef50896d":"code","5db2c127":"code","a8400273":"code","70467525":"code","890d43c8":"code","5bed5cad":"code","1506398c":"code","59be6427":"code","2d4d388a":"code","50b4787c":"markdown","f73275e6":"markdown","eea5c360":"markdown","e2c8000c":"markdown","cde20da7":"markdown","783a33ba":"markdown","761558e9":"markdown","8cd95541":"markdown","34c7ad3e":"markdown","17b78605":"markdown","b4e4e66d":"markdown","3c6c8e06":"markdown","57dcfd05":"markdown","897359eb":"markdown","9d45485c":"markdown","9cfbfcf3":"markdown","6bcfe550":"markdown","bec1aa4f":"markdown","439b0a29":"markdown","e85167b5":"markdown","07791b0e":"markdown","c6971d0c":"markdown","28e501ce":"markdown","20fc0e5e":"markdown","898aafaf":"markdown"},"source":{"3d5e343b":"# making the initial imports\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#print the directory items\nprint(os.listdir('..\/input\/telstra-recruiting-network'))\n","fc177f87":"#reading the files\n\ntrain = pd.read_csv('..\/input\/telstra-recruiting-network\/train.csv')\ntest = pd.read_csv('..\/input\/telstra-recruiting-network\/test.csv')\nseverity_type = pd.read_csv('..\/input\/telstra-recruiting-network\/severity_type.csv', error_bad_lines= False, warn_bad_lines= False)\nresource_type = pd.read_csv('..\/input\/telstra-recruiting-network\/resource_type.csv', error_bad_lines= False, warn_bad_lines= False)\nlog_failure = pd.read_csv('..\/input\/telstra-recruiting-network\/log_feature.csv', error_bad_lines= False, warn_bad_lines= False)\nevent_type = pd.read_csv('..\/input\/telstra-recruiting-network\/event_type.csv', error_bad_lines=False, warn_bad_lines= False)","f29cc407":"#printing the shape of all given files\n\nprint('The shape of test set is: {}\\n'.format(test.shape))\nprint('The shape of train set is: {}\\n'.format(train.shape))\nprint('The shape of severity_type is: {}\\n'.format(severity_type.shape))\nprint('The shape of resource_type is: {}\\n'.format(resource_type.shape))\nprint('The shape of log_failure is: {}\\n'.format(log_failure.shape))\nprint('The shape of event_type is: {}'.format(event_type.shape))","33944ba0":"#id column in event_types is an object\n\nevent_type.dtypes","316aba84":"#convert the id column to numeric data type\n\nevent_type['id']=pd.to_numeric(event_type['id'],errors='coerce')","d884d1fd":"#checking the shape of training set\n\ntrain.shape","c98f19a6":"#checking the head of training file before merging it with other files\n\ntrain.head()","4ad6a46a":"#merging the data sets to have all the available info\n\ntrain_1 = train.merge(severity_type, how = 'left', left_on='id', right_on='id')\ntrain_2 = train_1.merge(resource_type, how = 'left', left_on='id', right_on='id')\ntrain_3 = train_2.merge(log_failure, how = 'left', left_on='id', right_on='id')\ntrain_4 = train_3.merge(event_type, how = 'left', left_on='id', right_on='id')","ae67d9a9":"#checking the head after merging\n\ntrain_4.head()","99bcb47c":"#checking the nulls in each column\n\ntrain_4.isnull().sum()","f45d6d05":"#do the head method on training file.\ntrain_4.head(20)","50c30cab":"#dropping the duplicate records\n\ntrain_4.drop_duplicates(subset= 'id', keep= 'first', inplace = True)","d66e7c28":"#checking the shape of training file after dropping duplicate records\n\ntrain_4.shape","0e72b610":"#count plot for fault severity\n\nplt.figure(figsize = (8,6))\nsns.countplot(train_4['fault_severity'])\nplt.show()","33fdd90e":"#count plot for severity type\n\nplt.figure(figsize = (8,6))\nsns.countplot(train_4['severity_type'])\nplt.show()","a443f523":"#count plot for resource type\n\nplt.figure(figsize = (14,6))\nsns.countplot(train_4['resource_type'])\nplt.tight_layout()\nplt.show()","3984a21f":"#plotting the correlation matrix\n\nplt.figure(figsize = (8,6))\nsns.heatmap(train_4.corr(), vmax = 0.8, linewidths= 0.01, square= True, \n           annot= True, cmap= 'viridis', linecolor= 'white')\n\nplt.title('Correlation Matrix', fontsize = 15)\nplt.show()","ddd5cdc5":"#importing the catboost and train test split\n\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split","c2715295":"#splitting into X and y (training data and training labels)\n\nX = train_4[['id', 'location', 'severity_type', 'resource_type',\n       'log_feature', 'volume', 'event_type']]\ny = train_4.fault_severity","b081c515":"#divide the training set into train\/validation set with 20% set aside for validation. \n\nfrom sklearn.model_selection import train_test_split\nX_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.20, random_state=101)","1e192d93":"#as we know that we can give categorical features to catboost to make best use of its performance. \n\n\ncategorical_features_indices = np.where(X_train.dtypes == object)[0]","6d692ab2":"#using pool to make the training and validation sets\n\ntrain_dataset = Pool(data=X_train,\n                     label=y_train,\n                     cat_features=categorical_features_indices)\n\neval_dataset = Pool(data=X_validation,\n                    label=y_validation,\n                    cat_features=categorical_features_indices)\n","9e8565d1":"# Initialize CatBoostClassifier\n\nmodel = CatBoostClassifier(iterations=1000,\n                           learning_rate=1,\n                           depth=2,\n                           loss_function='MultiClass',\n                           random_seed=1,\n                           bagging_temperature=22,\n                           od_type='Iter',\n                           metric_period=100,\n                           od_wait=100)","285c3e13":"# Fit model\n\nmodel.fit(train_dataset, eval_set= eval_dataset, plot= True)","39165796":"# Get predicted classes\n\npreds_class = model.predict(eval_dataset)","ccd99fe6":"# Get predicted probabilities for each class\n\npreds_proba = model.predict_proba(eval_dataset)","4e933dfc":"#we are getting the probabilities in this format.\n\npreds_proba","f6b599c2":"#checking the head\n\ntest.head()","ca08a74d":"#checking the shape of test set before merging with other files. \n\ntest.shape","e8085307":"#merging the data sets to combine all the needed info\n\ntest_1 = test.merge(severity_type, how = 'left', left_on='id', right_on='id')\ntest_2 = test_1.merge(resource_type, how = 'left', left_on='id', right_on='id')\ntest_3 = test_2.merge(log_failure, how = 'left', left_on='id', right_on='id')\ntest_4 = test_3.merge(event_type, how = 'left', left_on='id', right_on='id')","661e916d":"#checkingk the head 20 records\n\ntest_4.head(20)","d2c12187":"#removing the duplicates.\n\ntest_4.drop_duplicates(subset= 'id', keep= 'first', inplace = True)","1e01fdce":"#checkingk the shape of test set again\n\ntest_4.shape","cb0b6a88":"#checking for any null values. \n\ntest_4.isnull().sum()","bcfd250f":"#making predictions on test set\n\npredict_test=model.predict_proba(test_4)\npred_df=pd.DataFrame(predict_test,columns=['predict_0', 'predict_1', 'predict_2'])\nsubmission_cat=pd.concat([test[['id']],pred_df],axis=1)\nsubmission_cat.to_csv('sub_cat_1.csv',index=False,header=True)","47f34b52":"#having a look at the submission file\n\nsubmission_cat.head()","db705982":"#making the imports\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics","5e7ec5d5":"#checking the head of training file\n\ntrain_4.head()","a1c9d992":"#initialize the label encoder\n\nlb = LabelEncoder()","308a33ca":"#apply the label encoder to all the categorical columns\n\ntrain_4['location'] = lb.fit_transform(train_4['location'])\ntrain_4['severity_type'] = lb.fit_transform(train_4['severity_type'])\ntrain_4['resource_type'] = lb.fit_transform(train_4['resource_type'])\ntrain_4['log_feature'] = lb.fit_transform(train_4['log_feature'])\ntrain_4['event_type'] = lb.fit_transform(train_4['event_type'])","26f9172a":"#checking the head of encoded training set\n\ntrain_4.head(20)","22d98936":"#divide the data into X and y\n\ny = train_4['fault_severity']\nX = train_4.drop('fault_severity', axis = 1)","f35aa1a5":"#making training and test sets with 75% and 25% ratio\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","e48dcb41":"#instantiate and train the model\n\nrfc = RandomForestClassifier()\nrfc.fit(X_train,y_train)","a168499d":"#making predictions on test set\n\npred = rfc.predict(X_test)","0555a62a":"#confusion matrix and classification report\n\nprint('Confusion matrix \\n')\nprint(metrics.confusion_matrix(y_test,pred))\nprint('*'*80)\nprint('\\n')\nprint('Classification report \\n')\nprint(metrics.classification_report(y_test,pred))","b702b7af":"#making the needed imports\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV","70bc469a":"#divide the data into 5 folds\n\nfolds = KFold(n_splits= 5, shuffle= True, random_state= 101)","c490996d":"#give the range for parameters to check for grid search\n\nparams = {'max_depth':[3,5,7,9],\n         'n_estimators':[500,800,1100,1400],\n         'min_samples_leaf': [150, 200, 250, 300], \n         'min_samples_split': [300, 350, 400, 450]}\n\nrf = RandomForestClassifier()","9a25dc59":"#define the model with gridsearch parameters\n\nrf_fin = GridSearchCV(estimator= rf, cv = folds, param_grid= params, scoring= 'accuracy', return_train_score= True)\n","8f8aee76":"#traint the model (will take some time)\n\nrf_fin.fit(X_train,y_train)","960ad0f4":"#getting the results in a dataframe\n\nscores = rf_fin.cv_results_\n\nscores = pd.DataFrame(scores)\n\nscores.head()","3fca67f0":"#getting the best score\n\nprint('The best score was achieved using the parameters: {}'.format(rf_fin.best_params_))","ef50896d":"#so now we will use the above parameters to build the model\n\nrandom_final = RandomForestClassifier(max_depth= 3,\n                                      min_samples_leaf= 150,\n                                      min_samples_split= 300,\n                                      n_estimators= 500)","5db2c127":"#fit the model and get the predictions\n\nrandom_final.fit(X_train,y_train)\n\npred_fin = random_final.predict(X_test)","a8400273":"#printing the confusion matrix and classification reports\n\nprint('Confusion matrix \\n')\nprint(metrics.confusion_matrix(y_test,pred_fin))\nprint('*'*80)\nprint('\\n')\nprint('Classification report \\n')\nprint(metrics.classification_report(y_test,pred_fin))","70467525":"#checking the orignal form of test_4 (without label encoding the data)\n\ntest_4.head()","890d43c8":"#applying the label encode to categorical columns\n\ntest_4['location'] = lb.fit_transform(test_4['location'])\ntest_4['severity_type'] = lb.fit_transform(test_4['severity_type'])\ntest_4['resource_type'] = lb.fit_transform(test_4['resource_type'])\ntest_4['log_feature'] = lb.fit_transform(test_4['log_feature'])\ntest_4['event_type'] = lb.fit_transform(test_4['event_type'])","5bed5cad":"#label encoded test set\n\ntest_4.head()","1506398c":"# we will use the predict_proba as this is needed format for kaggle submission. \n\npred_fin = rfc.predict_proba(test_4)","59be6427":"#making the submission file ready\n\npred_df=pd.DataFrame(pred_fin,columns=['predict_0', 'predict_1', 'predict_2'])\nsubmission_rf=pd.concat([test[['id']],pred_df],axis=1)\nsubmission_rf.to_csv('sub_random_forest.csv',index=False,header=True)","2d4d388a":"#checking the submission file. \n\nsubmission_rf.head()","50b4787c":"## Making Predictions on Test data","f73275e6":"The tuned model is only good at predicting calss '0'. On other classes it is performing terribly.\nSo we will just stick to our default random forest model. ","eea5c360":"# Exploratory Data Analysis (EDA)","e2c8000c":"**Warn bad lines** has been used to avoid the un-necessary warnings generated by pandas. ","cde20da7":"## Getting the Test Set Ready to feed into the Model","783a33ba":"This plot only shows the correlation between the numeric columns. To see the complete correlation matrix we would have to convert the categorical columns to ints using label encoding etc and then visualize the correlation matrix again. ","761558e9":"# Conclusion\n\n**As a conclusion we can say that it is a good dataset for practice and you can try different approaches.**\n\n**We did not try Support Vector Machines (SVM) classifier. Usually it can also perform good on classification tasks**.\n\n**Happy Machine Learning.**","8cd95541":"# Data Preprocessing\/ Cleaning","34c7ad3e":"# Telstra Network Disruptions Challenge","17b78605":"As we can see that there are many duplicates in the test set as a result of merging with other files. Lets get rid of these duplicate records. ","b4e4e66d":"# Random Forest","3c6c8e06":"# Catboost","57dcfd05":"The mode performs better for class 0 and not so much for other classes. As the dataset was imbalanced (more training samples for class 0) so the model is biased towards predicting the class 0.","897359eb":"# Making predictions using Random Forest","9d45485c":"We can see that the one id is being repeated multiple times resulting in high number of records in training file. ","9cfbfcf3":"Duplicates have been removed. ","6bcfe550":"type 1 and type 2 and more frequent than others. These are just coded values we don't actually know what these types imply. ","bec1aa4f":"**In this challenge we are required to predict the fault severity i.e, if the fault is a normal glitch or it is critical and will result in total loss of service. As we know that Telecom Operatores strive to provide the best possible service to customers and network outages are taken very seriously as they effect user experience and also impact the revenue.**\n\n**We are given a training data where the fault_severity is already provided (labels). And we are required to predict the fault severity (ranging from 0 to 2) for the test data. Fault severity of 0 means no fault and 2 means critical (total loss of service)**","439b0a29":"Most of the records are with the type 2 and type 8. ","e85167b5":"# Grid Search CV","07791b0e":"# Label Encoding the test data","c6971d0c":"Not very balanced data set as the number of 0 values (indicating no fault) is high as compared with others.","28e501ce":"**These are the predicted probabilites. The column with the highest value is the predicted class of severity.**\n**We can use numpy.argmax to get the predicted classes**\n\n**But for this challenge the submission file is already in the desired format (as they will calculate the logloss against our given probabilities).**","20fc0e5e":"Now we will try random forest as usually it is a good algorithm for classification tasks. ","898aafaf":"As the model was getting overfit after initial iterations so it was stopped by overfitting detector in catboost."}}