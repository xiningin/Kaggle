{"cell_type":{"382ffe80":"code","b0e9ef31":"code","012aebcd":"code","4ae4df9e":"code","80cdb3e4":"code","6c2b50d8":"code","6ce3733c":"code","a1ae51cc":"code","4705052a":"code","dbd5ab71":"code","516989e8":"code","78f390f5":"code","456e2ba7":"code","20537420":"code","b2c4c323":"code","d7d4669d":"code","ee34e03c":"markdown","4be77380":"markdown"},"source":{"382ffe80":"#####################\n# Importing Library:\n#####################\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor,ExtraTreesRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate,validation_curve\nfrom sklearn.model_selection import GridSearchCV, cross_validate, RandomizedSearchCV, validation_curve\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder","b0e9ef31":"###########\n# Setting:\n###########\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 50)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","012aebcd":"############################################\n# Some sets of functions for EDA processing:\n############################################\n\n#1\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n    \n#2\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n#3\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n#4\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n    \n#5\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \n#6\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\n# 7\n\ndef label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n# 8\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal de\u011fi\u015fkenlerin isimlerini verir.\n    Not: Kategorik de\u011fi\u015fkenlerin i\u00e7erisine numerik g\u00f6r\u00fcn\u00fcml\u00fc kategorik de\u011fi\u015fkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                De\u011fi\u015fken isimleri al\u0131nmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n        car_th: int, optinal\n                kategorik fakat kardinal de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik de\u011fi\u015fken listesi\n        num_cols: list\n                Numerik de\u011fi\u015fken listesi\n        cat_but_car: list\n                Kategorik g\u00f6r\u00fcn\u00fcml\u00fc kardinal de\u011fi\u015fken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam de\u011fi\u015fken say\u0131s\u0131\n        num_but_cat cat_cols'un i\u00e7erisinde.\n        Return olan 3 liste toplam\u0131 toplam de\u011fi\u015fken say\u0131s\u0131na e\u015fittir: cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131\n\n    \"\"\"\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n","4ae4df9e":"#Importing File:\ndf = pd.read_csv('\/kaggle\/input\/hitters\/hitters.csv')\ndf.head()","80cdb3e4":"################\n# EDA\n################\n# (322, 20)\n# NewLeague;Division;League data type is \"object\":\n# Pre-processing control-check step : There are 59 NA of salary values:\n\ncheck_df(df)\n\n# The next step is determining of categoric, numeric columns: \ncat_cols, num_cols, cat_but_car =grab_col_names(df)\n\n# Missing value of Salary is filled via KNN imputatin method:\n\ndf_knn = df.select_dtypes(include=[\"float64\",\"int64\"])\nimputer = KNNImputer(n_neighbors=15)\ndf_knn = imputer.fit_transform(df_knn)\ndf_knn = pd.DataFrame(df_knn,columns=num_cols)\ndf[\"Salary\"] = df_knn[\"Salary\"]","6c2b50d8":"# Num_col histogram:\nfor col in num_cols:\n    plt.hist(df[col], align='mid',color = \"skyblue\")\n    plt.title(col)\n    plt.show()\n\n# Some of graphs are right - skewness based on Empirical Analysis so the values can be marked as Outliers\n# Salary < 1500\n# CRBI < 1500\n# CRuns <1500\n# CHits < 3000\n\ndf = df[(df['Salary'] < 1500) & (df['CHits']<3000) & (df[\"CRBI\"]< 1500) & (df[\"CRuns\"]<1500)]","6ce3733c":"########################################\n#New Feature Engineering:\n########################################\n\ndf['AtBat*RBI'] = df['AtBat'] * df['RBI']\ndf['Walks*Years'] = df['Walks'] * df['Years']\ndf['AtBat*RBI'] = df['AtBat'] * df['RBI']\ndf['Walks*Years'] = df['Walks'] * df['Years']\ndf['AtBat\/Hits'] = df['AtBat'] \/ df['Hits']\ndf['AtBat\/Runs'] = df['AtBat'] \/ df['Runs']\ndf['Hits\/Runs'] = df['Hits'] \/ df['Runs']\ndf['HmRun\/RBI'] = df['HmRun'] \/ df['RBI']\ndf['Runs\/RBI'] = df['Runs'] \/ df['RBI']\ndf['Years\/CAtBat'] = df['Years'] \/ df['CAtBat']\ndf['Years\/CHits'] = df['Years'] \/ df['CHits']\ndf['Years\/CHmRun'] = df['Years'] \/ df['CHmRun']\ndf['Years\/CRuns'] = df['Years'] \/ df['CRuns']\ndf['Years\/CRBI'] = df['Years'] \/ df['CRBI']\ndf['CAtBat\/CHits'] = df['CAtBat'] \/ df['CHits']\ndf['CAtBat\/CRuns'] = df['CAtBat'] \/ df['CRuns']\ndf['CAtBat\/CRBI'] = df['CAtBat'] \/ df['CRBI']\ndf['CAtBat\/CWalks'] = df['CAtBat'] \/ df['CWalks']\ndf['CHits\/CRuns'] = df['CHits'] \/ df['CRuns']\ndf['CHits\/CRBI'] = df['CHits'] \/ df['CRBI']\ndf['CHits\/CWalks'] = df['CHits'] \/ df['CWalks']\ndf['CHmRun\/CRuns'] = df['CHmRun'] \/ df['CRuns']\ndf['CHmRun\/CRBI'] = df['CHmRun'] \/ df['CRBI']\ndf['CHmRun\/CWalks'] = df['CHmRun'] \/ df['CWalks']\ndf['CRuns\/CRBI'] = df['CRuns'] \/ df['CRBI']\ndf['CRuns\/CWalks'] = df['CRuns'] \/ df['CWalks']\ndf['CHmRun\/CRBI'] = df['CHmRun'] \/ df['CRBI']\ndf.replace([np.inf, -np.inf], 0, inplace=True)","a1ae51cc":"# Determining numeric and categorical columns:\ncat_cols, num_cols, cat_but_car =grab_col_names(df)","4705052a":"# Determing Outliers \n\noutlier_list= []\nfor col in num_cols:\n    if check_outlier(df, col) == True:\n        outlier_list.append(col)\n        print(col, check_outlier(df, col))\n\n# Check step:\n\noutlier_list\n\n# Replace thresholds for outlier values:\nfor col in num_cols:\n    replace_with_thresholds(df,col)","dbd5ab71":"# Checking for Missing Values via the function:\n\nmissing_values_table(df)\n\n#               n_miss   ratio\n#HmRun\/RBI           2 0.65000\n#CHmRun\/CRBI         1 0.33000\n#CHmRun\/CWalks       1 0.33000\n\n# A few rows are dropped.\n\ndf.dropna(subset=[\"HmRun\/RBI\",\"CHmRun\/CWalks\",\"CHmRun\/CRBI\"], axis=0, inplace=True)","516989e8":"# Controlling whether or not there is multi correlated with features:\ndrop_list = high_correlated_cols(df,corr_th=0.80)\n\n# Let's control:\ndrop_list\n\n# Deleted:\ndf.drop(drop_list, axis=1, inplace=True)","78f390f5":"### Label encoding ###\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\n# ['League', 'Division', 'NewLeague']\nfor col in binary_cols:\n    df = label_encoder(df, col)","456e2ba7":"#Robust Scale:\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\nnum_cols.remove(\"Salary\") #Targeted feature exlude from num_cols\n\nfor col in num_cols:\n    transformer = RobustScaler().fit(df[[col]])\n    df[col] = transformer.transform(df[[col]])","20537420":"# Let's checking the DataFrame\ndf.shape\ndf.info()\ndf.head()","b2c4c323":"# The result of EDA:\ny = df[\"Salary\"]\nX = df.drop([\"Salary\"], axis=1)","d7d4669d":"######################################################\n# Automated Hyperparameter Optimization\n######################################################\n\n# Defining parameters:\n\ncart_params = {'max_depth': range(1, 20),\n                \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n                \"max_features\": [5, 7, \"auto\"],\n                \"min_samples_split\": [8, 15, 20],\n                 \"n_estimators\": [200, 500]}\n\n\nlightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n                    \"n_estimators\": [500, 1000],\n                    \"colsample_bytree\": [0.1, 0.3, 0.7, 1]}\n\n\ncatboost_params = {\"iterations\": [500,1000],\n                     \"learning_rate\": [0.01, 0.1],\n                     \"depth\": [3, 6]}\n\nextraTrees_params = {\n        'n_estimators': [500, 1000],\n        'max_depth': [2, 16, 50],\n        'min_samples_split': [2, 10],\n        'min_samples_leaf': [1, 2],\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'bootstrap': [True, False],\n        'warm_start': [True, False],\n     }\n\n\n\nregressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n                (\"RF\", RandomForestRegressor(), rf_params),\n                ('LightGBM', LGBMRegressor(), lightgbm_params),\n                ('Catboost',CatBoostRegressor(verbose=False),catboost_params),\n                ('ExtraTrees', ExtraTreesRegressor(), extraTrees_params),\n                ]\n#################################################################################################\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    # After hyper parameters tunned\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model\n","ee34e03c":"# HITTERS:\n\n# Baseball Data\nDescription\nMajor League Baseball Data from the 1986 and 1987 seasons.\n\n# Usage\nHitters\n\n# Format\n\nA data frame with 322 observations of major league players on the following 20 variables.\n\n* AtBat: Number of times at bat in 1986\n\n* Hits: Number of hits in 1986\n\n* HmRun: Number of home runs in 1986\n\n* Runs: Number of runs in 1986\n\n* RBI: Number of runs batted in in 1986\n\n* Walks: Number of walks in 1986\n\n* Years: Number of years in the major leagues\n\n* CAtBat: Number of times at bat during his career\n\n* CHits: Number of hits during his career\n\n* CHmRun: Number of home runs during his career\n\n* CRuns: Number of runs during his career\n\n* CRBI: Number of runs batted in during his career\n\n* CWalks: Number of walks during his career\n\n* League: A factor with levels A and N indicating player's league at the end of 1986\n\n* Division: A factor with levels E and W indicating player's division at the end of 1986\n\n* PutOuts: Number of put outs in 1986\n\n* Assists: Number of assists in 1986\n\n* Errors: Number of errors in 1986\n\n* Salary: 1987 annual salary on opening day in thousands of dollars\n\n* NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987\n\n# Source\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.","4be77380":"# The Results:\n\n*** ########## CART ##########**\n* RMSE: 287.532 (CART) \n* RMSE (After): 248.1274 (CART) \n* CART best params: {'max_depth': 5, 'min_samples_split': 17}\n\n* ########## RF ##########\n* RMSE: 193.4727 (RF) \n* RMSE (After): 190.6059 (RF) \n* RF best params: {'max_depth': 8, 'max_features': 7, 'min_samples_split': 8, 'n_estimators': 200}\n\n*** ########## LightGBM ##########**\n* RMSE: 196.0429 (LightGBM) \n* RMSE (After): 188.5556 (LightGBM) \n* LightGBM best params: {'colsample_bytree': 0.7, 'learning_rate': 0.01, 'n_estimators': 500}\n\n*** ########## Catboost ##########**\n* RMSE: 185.9416 (Catboost) \n* RMSE (After): 186.1933 (Catboost) \n* Catboost best params: {'depth': 6, 'iterations': 1000, 'learning_rate': 0.01}\n\n*** ########## ExtraTrees ##########**\n* RMSE: 182.8924 (ExtraTrees) \n* RMSE (After): 178.7743 (ExtraTrees) \n* ExtraTrees best params: {'bootstrap': False, 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 1000, 'warm_start': True}"}}