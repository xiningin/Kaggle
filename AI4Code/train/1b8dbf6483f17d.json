{"cell_type":{"4ffcfbee":"code","8673bef4":"code","b0216ad5":"code","90b3ecba":"code","e202bbb9":"code","007d2c1b":"code","db7c089b":"code","158766c1":"code","22baaed7":"code","add63e10":"code","8ca12a81":"code","4c65f0ea":"code","3e1345bc":"code","bc77c8a9":"code","a5ee6834":"code","82069514":"code","95fc67e9":"code","b4b57a14":"code","cd723a99":"code","56c5b286":"code","57fef49b":"code","06c1e23d":"code","efe9cbd2":"code","50eba2e0":"code","160ec3a1":"code","61a5987e":"code","effe9207":"code","485daaca":"code","e4a57748":"code","aa952b8b":"code","946df626":"code","ea9fac4d":"code","8eb37954":"code","af56f2e7":"code","f8785e05":"code","43f55069":"code","7a71b74e":"code","37c73b02":"code","e3920b59":"code","02fb1ef9":"code","31fa4739":"code","8e92bfc4":"code","e4dea5bc":"code","72b0db42":"code","11ed9ff8":"code","c8822935":"code","38d6db4f":"code","44d1e6b3":"code","a09d3ae9":"code","a8940ed3":"code","058b70c8":"code","cdb4d209":"code","c48d38d8":"code","a6e4d008":"code","81e6af77":"code","cf0139b7":"code","261f102a":"code","2649ff99":"code","62ed2159":"code","a6f356e2":"code","78c446b2":"code","e058c267":"code","35caf69b":"code","71a9fbdd":"code","18772d21":"code","4d768325":"code","1fb3997e":"code","90df2a96":"code","aeb0eec5":"code","891ff100":"code","4653d63c":"code","ca5fa4a5":"markdown","95bed737":"markdown","df03a57e":"markdown","fde9b113":"markdown","101f8238":"markdown","b97f3590":"markdown","d5cb329b":"markdown","33bae839":"markdown","8df36fb9":"markdown","42f02a0e":"markdown","f805671f":"markdown","dacd3c8e":"markdown","f6357dec":"markdown","eb6467b1":"markdown","1fb4b8eb":"markdown","52c28e15":"markdown","f133e505":"markdown","9204ed41":"markdown","71dd7ad9":"markdown","3f123441":"markdown","761abeb6":"markdown","923a6b6b":"markdown","3ec4153a":"markdown","305c56d5":"markdown","1a79eafa":"markdown","5e28ef0d":"markdown","8fd3f5d8":"markdown","d7152021":"markdown","5dd287f7":"markdown","bee31d05":"markdown","9a70bab1":"markdown","aaff1264":"markdown","36510014":"markdown","a6d3b3f6":"markdown","aa22d2b6":"markdown","9ced060f":"markdown","e6a10875":"markdown","30891103":"markdown","8ca8f970":"markdown","97d68f6a":"markdown","b843d22f":"markdown","d8eba3b9":"markdown","a0ab16d7":"markdown"},"source":{"4ffcfbee":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nfrom matplotlib import cm\nimport seaborn as sns\nimport tqdm\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nsns.set(rc={\"figure.figsize\": (10, 12)})\nnp.random.seed(sum(map(ord, \"palettes\")))","8673bef4":"metadata = pd.read_csv(\"..\/input\/heroes_information.csv\", index_col=0)\npowers = pd.read_csv(\"..\/input\/super_hero_powers.csv\")","b0216ad5":"print(\"Heroes information data shape: \", metadata.shape)\nprint(\"Hero super powers data shape: \", powers.shape)","90b3ecba":"metadata.head()","e202bbb9":"powers.head()","007d2c1b":"def clean_repeated_heroes(metadata, powers):\n    \n    print(\"Initial shape of metadata and powers: \")\n    print(\"Powers:\", powers.shape)\n    print(\"Metadata\", metadata.shape)\n    \n    print(\"\\nStart cleaning...\")\n    \n    powers.drop_duplicates(inplace=True)\n    metadata.drop_duplicates(inplace=True)\n    \n    # Handle Goliath\n    goliath_idxs_to_drop = [100, 289, 290] # not dropping Goliath IV, it will be used to join powers\n    metadata.drop(goliath_idxs_to_drop, inplace=True)\n    metadata.loc[metadata.name == \"Goliath IV\", \"Race\"] = \"Human\"\n    \n    # Avoid outersected entries. i.e. appearing in metadata, but not in powers. And viceversa.\n    metadata = metadata[metadata.name.isin(powers.hero_names)]\n    powers = powers[powers.hero_names.isin(metadata.name)]\n    \n    # Spider-Man\n    metadata.loc[metadata.name.str.contains(\"Spider-Man\")] = metadata[metadata.name.str.contains(\"Spider-Man\")].mode().values[0]\n    metadata.drop(623, inplace=True)\n    metadata.drop(624, inplace=True)\n\n    # Nova\n    metadata.drop(497, inplace=True)\n\n    # Angel\n    metadata.loc[metadata.name == \"Angel\", \"Race\"] = \"Vampire\"\n    metadata.drop(23, inplace=True)\n\n    # Blizzard\n    metadata.loc[metadata.name == \"Blizzard\"] = metadata.loc[metadata.name == \"Blizzard II\"].values\n    metadata.at[115, 'name'] = \"Blizzard\"\n    metadata.at[116, 'Race'] = \"Human\"\n    metadata.at[115, 'Race'] = \"Human\"\n    metadata.drop(117, inplace=True)\n\n    # Black Canary\n    metadata.drop(97, inplace=True)\n\n    # Captain Marvel\n    metadata.at[156, 'Race'] = \"Human\"\n    metadata.drop(155, inplace=True)\n\n    # Blue Beettle\n    metadata.at[122, 'Race'] = \"Human\"\n    metadata.at[124, 'Race'] = \"Human\"\n    metadata.at[122, 'Height'] = 183.0\n    metadata.at[125, 'Height'] = 183.0\n    metadata.at[122, 'Weight'] = 86.0\n    metadata.at[125, 'Weight'] = 86.0\n    metadata.drop(123, inplace=True)\n\n    # Vindicator\n    metadata.drop(696, inplace=True)\n\n    # Atlas\n    metadata.drop(48, inplace=True)\n\n    # Speedy\n    metadata.drop(617, inplace=True)\n\n    # Firestorm\n    metadata.drop(260, inplace=True)\n\n    # Atom\n    metadata.drop(50, inplace=True)\n    metadata.at[49, 'Race'] = \"Human\"\n    metadata.at[53, 'Race'] = \"Human\"\n    metadata.at[54, 'Race'] = \"Human\"\n    metadata.at[49, 'Race'] = \"Human\"\n    metadata.at[54, 'Height'] = 183.0\n    metadata.at[49, 'Height'] = 183.0\n    metadata.at[53, \"Weight\"] = 72.0\n\n    # Batman\n    metadata.drop(69, inplace=True)\n\n    # Toxin\n    metadata.drop(673, inplace=True)\n\n    # Namor\n    metadata.drop(481, inplace=True)\n\n    # Batgirl\n    metadata.drop(62, inplace=True)\n    \n    print(\"Final shape of metadata and powers: \")\n    print(\"Powers:\", powers.shape)\n    print(\"Metadata\", metadata.shape)\n    \n    print(\"\\nCleaning done\")\n    \n    return metadata, powers","db7c089b":"# if you run it twice, it won't work due to hard-coded indexers won't match.\n# you need to get the data and run it again\n\n# metadata, powers = clean_repeated_heroes(metadata, powers)","158766c1":"powers.drop_duplicates(inplace=True)\nmetadata.drop_duplicates(inplace=True)","22baaed7":"print(\"Number of rows with more than 1 entry per hero name in metadata \", (metadata.name.value_counts() > 1).sum()  )\nprint(\"Number of rows with more than 1 entry per hero name in powers \", (powers.hero_names.value_counts() > 1).sum() )","add63e10":"mask = metadata.name.value_counts() > 1\nmetadata.name.value_counts()[mask].sum() - mask.sum() # get excessive number of rows from repeated names","8ca12a81":"# Does it match with difference in table length?\nmetadata.shape[0] - powers.shape[0]","4c65f0ea":"repeated_heroes = mask.index[mask]\nrepeated_heroes[~repeated_heroes.isin(powers.hero_names)]","3e1345bc":"powers[powers.hero_names.str.contains(\"Goliath\")]","bc77c8a9":"metadata[metadata.name.str.contains(\"Goliath\")]","a5ee6834":"goliath_idxs_to_drop = [100, 289, 290] # not dropping Goliath IV, it will be used to join powers\nmetadata.drop(goliath_idxs_to_drop, inplace=True)\n\n# modify Goliath IV row\nmetadata.loc[metadata.name == \"Goliath IV\", \"Race\"] = \"Human\"","82069514":"metadata[metadata.name.str.contains(\"Goliath\")]","95fc67e9":"# How many superheroes that appear in metadata, do not have an entry in powers?\n(~metadata.name.isin(powers.hero_names)).sum()","b4b57a14":"# How many superheroes that appear in powers, do not have an entry in metadata?\n(~powers.hero_names.isin(metadata.name)).sum()","cd723a99":"metadata = metadata[metadata.name.isin(powers.hero_names)]\npowers = powers[powers.hero_names.isin(metadata.name)]","56c5b286":"metadata.shape","57fef49b":"powers.shape","06c1e23d":"(metadata.name.value_counts() > 1).sum() ","efe9cbd2":"repeated_heroes = repeated_heroes.drop(\"Goliath\")\n\n# let's go 1 by 1\nfor rh in repeated_heroes:\n    print(\"*********** \", rh, \" **************\")\n    print(metadata[metadata.name.str.contains(rh)])\n    print(\"\\n\\n\")","50eba2e0":"# Spider-Man\n# As all three instances seem similar, let's just take the mode of each column as new values\nmetadata.loc[metadata.name.str.contains(\"Spider-Man\")] = metadata[metadata.name.str.contains(\"Spider-Man\")].mode().values[0]\nmetadata.drop(623, inplace=True)\nmetadata.drop(624, inplace=True)\n\n# Nova\n# They are different superheroes, but with the same name. It is not clear which one is represented in powers df\n# We cannot keep both as both would have same superpowers and thus confuse the classifier. For simplicity,\n# let's only keep the human Nova. And to choose between male or female, let's keep the female.\nmetadata.drop(497, inplace=True)\n\n# Angel\n# There are 2 Angels. Both rows seemed to have been split from one single row. Let's merge it back.\nmetadata.loc[metadata.name == \"Angel\", \"Race\"] = \"Vampire\"\nmetadata.drop(23, inplace=True)\n\n# Blizzard\n# Blizzard II has only 1 difference in superpower, therefore we claim both Blizzard and Blizzard II will have same \n# characteristics. And it's considered Human, according to Wikipedia.\nmetadata.loc[metadata.name == \"Blizzard\"] = metadata.loc[metadata.name == \"Blizzard II\"].values\nmetadata.at[115, 'name'] = \"Blizzard\"\nmetadata.at[116, 'Race'] = \"Human\"\nmetadata.at[115, 'Race'] = \"Human\"\nmetadata.drop(117, inplace=True)\n\n# Black Canary\n# Let's take only one of them, as they're practically similar\nmetadata.drop(97, inplace=True)\n\n# Captain Marvel\n# All are same, in exception of Captain Marvel II, which is showed in powers df. Let's keep CM and CM II, but in case \n# of Captain Marvel we will keep the original one by DC (the one from Marvel is copied)\nmetadata.at[156, 'Race'] = \"Human\"\nmetadata.drop(155, inplace=True)\n\n# Blue Beettle\n# In powers df, there are the three blue beetles, and they are indeed different in terms of powers.\n# Let's keep all of them but they will all have the same characteristics as they are really similar.\nmetadata.at[122, 'Race'] = \"Human\"\nmetadata.at[124, 'Race'] = \"Human\"\nmetadata.at[122, 'Height'] = 183.0\nmetadata.at[125, 'Height'] = 183.0\nmetadata.at[122, 'Weight'] = 86.0\nmetadata.at[125, 'Weight'] = 86.0\nmetadata.drop(123, inplace=True)\n\n# Vindicator\n# keep only the one that does not have null values\nmetadata.drop(696, inplace=True)\n\n# Atlas\n# Keep only one of them\nmetadata.drop(48, inplace=True)\n\n# Speedy\n# Searched in Google, they are mainly the same, but male version introduced 1941 and female on 2001. Let's \n# just keep the female as it has more characteristics\nmetadata.drop(617, inplace=True)\n\n# Firestorm\n# keep the one that doesn't have null values\nmetadata.drop(260, inplace=True)\n\n# Atom\n# All atoms shown there are covered in powers df. Let's drop the row that has Atom and few null values. And add Human\n# as race, plus other characteristics (all will have similar ones)\nmetadata.drop(50, inplace=True)\nmetadata.at[49, 'Race'] = \"Human\"\nmetadata.at[53, 'Race'] = \"Human\"\nmetadata.at[54, 'Race'] = \"Human\"\nmetadata.at[49, 'Race'] = \"Human\"\nmetadata.at[54, 'Height'] = 183.0\nmetadata.at[49, 'Height'] = 183.0\nmetadata.at[53, \"Weight\"] = 72.0\n\n# Batman\n# let's only drop the short and skinny Batman. Because he is just not.\nmetadata.drop(69, inplace=True)\n\n# Toxin\n# let's just keep one of them, as the second, for example\nmetadata.drop(673, inplace=True)\n\n# Namor\n# keep the one without null values\nmetadata.drop(481, inplace=True)\n\n# Batgirl\n# drop the one with null values\nmetadata.drop(62, inplace=True)","160ec3a1":"metadata.shape, powers.shape","61a5987e":"metadata = metadata.replace('-', np.nan) \nmetadata = metadata.replace(-99, np.nan)\n\nmetadata.isnull().sum()","effe9207":"metadata.dropna(subset=['Race'], inplace=True)","485daaca":"metadata.isnull().sum() ","e4a57748":"# drop Skin color because it has too many null values\nmetadata.drop(\"Skin color\", axis=1, inplace=True)","aa952b8b":"# transform Human- race into Human (as they are not mutations)\nmetadata.loc[:, \"Race\"] = metadata.apply(lambda x: \"Human\" if(x.Race.startswith(\"Human-\")) else x.Race, axis=1)\n# add label for modeling\nmetadata['label'] = metadata.apply(lambda x: \"No-Human\" if(x.Race != \"Human\") else x.Race, axis=1)","946df626":"fig, ax = pyplot.subplots(figsize=(14,8))\nsns.boxplot(x=\"Weight\", y=\"label\", hue=\"Gender\", data=metadata, ax=ax)\n\nfig, ax = pyplot.subplots(figsize=(14,8))\nsns.boxplot(x=\"Height\", y=\"label\",  hue=\"Gender\",data=metadata, ax=ax)","ea9fac4d":"fig, ax = pyplot.subplots(figsize=(12,14))\nsns.boxplot(x=\"Height\", y=\"Race\", data=metadata[metadata.Gender == 'Male'])","8eb37954":"fig, ax = pyplot.subplots(figsize=(12,14))\nsns.boxplot(x=\"Weight\", y=\"Race\", data=metadata[metadata.Gender == 'Male'])","af56f2e7":"# height and weight can be replaced by the mean of the same gender and race\n\nw_means = metadata.groupby([\"label\", \"Gender\"])[\"Weight\"].mean().unstack()\nh_means = metadata.groupby([\"label\", \"Gender\"])[\"Height\"].mean().unstack()\n\nw_fh = w_means.loc[\"Human\",\"Female\"]\nw_mh = w_means.loc[\"Human\",\"Male\"]\nw_fn = w_means.loc[\"No-Human\",\"Female\"]\nw_mn = w_means.loc[\"No-Human\",\"Male\"]\n\nh_fh = h_means.loc[\"Human\",\"Female\"]\nh_mh = h_means.loc[\"Human\",\"Male\"]\nh_fn = h_means.loc[\"No-Human\",\"Female\"]\nh_mn = h_means.loc[\"No-Human\",\"Male\"]\n\n# Fill null values with means\nmetadata.loc[(metadata.label == \"Human\") & (metadata.Gender == \"Female\") & (metadata.Weight.isnull()), \"Weight\"] = w_fh\nmetadata.loc[(metadata.label == \"Human\") & (metadata.Gender == \"Male\") & (metadata.Weight.isnull()), \"Weight\"] = w_mh\nmetadata.loc[(metadata.label == \"No-Human\") & (metadata.Gender == \"Female\") & (metadata.Weight.isnull()), \"Weight\"] = w_fn\nmetadata.loc[(metadata.label == \"No-Human\") & (metadata.Gender == \"Male\") & (metadata.Weight.isnull()), \"Weight\"] = w_mn\n\nmetadata.loc[(metadata.label == \"Human\") & (metadata.Gender == \"Female\") & (metadata.Height.isnull()), \"Height\"] = h_fh\nmetadata.loc[(metadata.label == \"Human\") & (metadata.Gender == \"Male\") & (metadata.Height.isnull()), \"Height\"] = h_mh\nmetadata.loc[(metadata.label == \"No-Human\") & (metadata.Gender == \"Female\") & (metadata.Height.isnull()), \"Height\"] = h_fn\nmetadata.loc[(metadata.label == \"No-Human\") & (metadata.Gender == \"Male\") & (metadata.Height.isnull()), \"Height\"] = h_mn\n\n# plot to see clearer differences\nfig, (ax1,ax2) = pyplot.subplots(1,2, figsize=(12,6))\nax1.set_title(\"Weight\")\nax2.set_title(\"Height\")\nw_means.plot(kind=\"bar\", ax=ax1)\nh_means.plot(kind=\"bar\", ax=ax2)\n","f8785e05":"metadata.isnull().sum()","43f55069":"metadata[metadata.Gender.isnull()]","7a71b74e":"metadata.drop(metadata[metadata.Gender.isnull()].index, axis=0, inplace=True)","37c73b02":"metadata.isnull().sum()","e3920b59":"sns.pairplot(x_vars=[\"Height\"], y_vars=[\"Weight\"], data=metadata, hue=\"label\", height=10)","02fb1ef9":"metadata[(metadata.Height > 400)]","31fa4739":"metadata = metadata[(metadata.Weight < 450)]","8e92bfc4":"for col in metadata.columns:\n    if col in (\"Eye color\", \"Hair color\"):\n        fig, ax = pyplot.subplots(figsize=(12,10))\n        values = metadata.groupby([col, \"label\"]).count()['name'].unstack().sort_values(by=\"Human\", ascending=False)\n        values.plot(kind='barh', stacked=True, ax=ax)\n        plt.show()","e4dea5bc":"aux_eyes_colors = [\"blue\", \"brown\", \"green\"]\naux_hair_colors = [\"Black\", \"Brown\", \"Blond\", \"Red\", \"No Hair\"]\nlen_aux_eyes_colors = len(aux_eyes_colors)\nlen_aux_hair_colors = len(aux_hair_colors)\n\nfor ix in metadata[metadata[\"Eye color\"].isnull()].index:\n    metadata.at[ix, \"Eye color\"] = aux_eyes_colors[np.random.choice(len_aux_eyes_colors)]\n    \nfor ix in metadata[metadata[\"Hair color\"].isnull()].index:\n    metadata.at[ix, \"Hair color\"] = aux_hair_colors[np.random.choice(len_aux_hair_colors)]\n","72b0db42":"for col in metadata.columns:\n    if col in (\"Eye color\", \"Hair color\"):\n        fig, ax = pyplot.subplots(figsize=(12,10))\n        values = metadata.groupby([col, \"label\"]).count()['name'].unstack().sort_values(by=\"Human\", ascending=False)\n        values.plot(kind='barh', stacked=True, ax=ax)\n        plt.show()","11ed9ff8":"metadata.isnull().sum()","c8822935":"metadata.drop(metadata[metadata.Publisher.isnull()].index, axis=0, inplace=True)\nmetadata.drop(metadata[metadata.Alignment.isnull()].index, axis=0, inplace=True)","38d6db4f":"metadata.isnull().sum()","44d1e6b3":"metadata = metadata.drop(['Race'], axis=1)\n\nhigh_card = [\"Gender\", \"Alignment\"]\nlow_card = [\"Eye color\", \"Hair color\", \"Publisher\"]\n\nfor hc in high_card:\n    one_hot = pd.get_dummies(metadata[hc])\n    metadata.drop(hc, axis=1, inplace=True)\n    metadata = metadata.join(one_hot)\n\nfor lc in low_card:\n    metadata[lc] = metadata[lc].astype('category').cat.codes\n\n# transform label into 0 (Human) or 1 (No-Human)\nmetadata['label'] = metadata['label'].astype('category').cat.codes","a09d3ae9":"# transform powers data into 0,1 binary features\ncols = powers.select_dtypes(['bool']).columns\nfor col in cols:\n    powers[col] = powers[col].astype(int)","a8940ed3":"metadata.head()","058b70c8":"powers.head()","cdb4d209":"heroes = pd.merge(metadata, powers, how='inner', left_on = 'name', right_on = 'hero_names')\n\nheroes.drop([\"hero_names\",\"name\"], axis=1, inplace=True)\n\npowers_cols = powers.columns.drop(\"hero_names\")\nmetadata_cols = metadata.columns.drop(\"name\")","c48d38d8":"heroes.shape","a6e4d008":"heroes.head()","81e6af77":"# store dataframe\nmetadata.to_pickle(\"metadata.pkl\")\npowers.to_pickle(\"powers.pkl\")\nheroes.to_pickle(\"heroes.pkl\")","cf0139b7":"# load back again\nmetadata = pd.read_pickle(\"metadata.pkl\")\npowers = pd.read_pickle(\"powers.pkl\")\nheroes = pd.read_pickle(\"heroes.pkl\")","261f102a":"from sklearn import preprocessing\n\nX = heroes.drop([\"label\"], axis=1).values\ny = heroes[\"label\"].values\n\nX = preprocessing.scale(X)\n\nprint( \"X - training data shape \", X.shape)\nprint( \"y - label \", y.shape )","2649ff99":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier","62ed2159":"# Initialize a stratified split of our dataset for the validation process\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","a6f356e2":"models = [\"LogReg\", \"SVM\", \"RF\", \"XGB\"]\n\nfor model in models:\n    print( \"Training \", model)\n    if model == \"LogReg\":\n        clf = LogisticRegression(random_state=0, solver='liblinear')\n    elif model == \"SVM\":\n        clf = svm.SVC(kernel='linear',C=1)\n    elif model == 'RF':\n        clf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=1)\n    else:\n        warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n        clf = XGBClassifier(n_estimators=50, max_depth=6)\n    \n    results = cross_val_score(clf, X, y, cv=5).mean()\n    print( model, \" CV accuracy score: {:.2f}%\".format(results.mean()*100) )\n","78c446b2":"# Random Forest optimization\nclf_rf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=1)\n\nrf_params = {'max_features': [4, 7, 10, 13], \n             'min_samples_leaf': [1, 3, 5, 7], \n             'max_depth': [5,8,10,15], \n             \"n_estimators\": [50, 100] }\n\ngcv = GridSearchCV(clf_rf, rf_params, n_jobs=-1, cv=skf, verbose=1)\ngcv.fit(X, y)","e058c267":"gcv.best_estimator_, gcv.best_score_","35caf69b":"# XGBoost optimization\nclf_xgb = XGBClassifier(n_estimators=50, max_depth=8, random_state=1)\n\nxgb_params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 8, 10],\n        'n_estimators' : [50,100]\n        }\n\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\ngcv_xgb = GridSearchCV(clf_xgb, xgb_params, n_jobs=-1, cv=skf, verbose=1)\ngcv_xgb.fit(X, y)","71a9fbdd":"gcv_xgb.best_estimator_, gcv_xgb.best_score_","18772d21":"importances = gcv_xgb.best_estimator_.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importancies\nfeatures = dict()\ncount = 1\nfor col in heroes.drop(\"label\",axis=1).columns:\n    index = \"f\"+str(count)\n    features[index] = col\n    count+=1\n\nnum_to_plot = 20\nfeature_indices = [ind+1 for ind in indices[:num_to_plot]]\ntop_features = list()\n# Print the feature ranking\nprint(\"Feature ranking:\")\n  \nfor f in range(num_to_plot):\n    print(\"%d. %s %f \" % (f + 1, \n            features[\"f\"+str(feature_indices[f])], \n            importances[indices[f]]))\n    top_features.append(features[\"f\"+str(feature_indices[f])])\nplt.figure(figsize=(15,5))\nplt.title(u\"Feature Importance\")\nbars = plt.bar(range(num_to_plot), \n               importances[indices[:num_to_plot]],\n       color=([str(i\/float(num_to_plot+1)) \n               for i in range(num_to_plot)]),\n               align=\"center\")\nticks = plt.xticks(range(num_to_plot), \n                   feature_indices)\nplt.xlim([-1, num_to_plot])\nplt.legend(bars, [u''.join(features[\"f\"+str(i)]) \n                  for i in feature_indices]);","4d768325":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=1, n_iter=1000)\ntsne_results = tsne.fit_transform(X)\n","1fb3997e":"df_tsne = pd.DataFrame(data=tsne_results, columns=[\"tsne1\", \"tsne2\"])\ndf_tsne['label'] = y\n\nsns.pairplot(x_vars=[\"tsne1\"], y_vars=[\"tsne2\"], data=df_tsne, hue=\"label\", height=10)","90df2a96":"from sklearn.decomposition import PCA","aeb0eec5":"models = [\"LogReg\", \"SVM\", \"RF\", \"XGB\"]\nreductions = [20, 30, 40, 50, 70, 100, 150]\n\nfor red in reductions:\n    \n    print( \"Applying PCA on \", red, \" components\")\n    pca = PCA(n_components=red)\n    X_reduced = pca.fit_transform(X)\n\n    for model in models:\n        print( \"Training \", model )\n        if model == \"LogReg\":\n            clf = LogisticRegression(random_state=0, solver='liblinear')\n        elif model == \"SVM\":\n            clf = svm.SVC(kernel='linear',C=1)\n        elif model == 'RF':\n            clf = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=1)\n        else:\n            clf = XGBClassifier(n_estimators=50, max_depth=6)\n\n        results = cross_val_score(clf, X_reduced, y, cv=5).mean()\n        print( model, \" CV accuracy score: {:.2f}%\".format(results.mean()*100) )\n        \n    print( \"\\n\\n\" )","891ff100":"# let's try to train xgboost with 50 PCA component\n\npca_50 = PCA(n_components=50)\nX_reduced_p50 = pca_50.fit_transform(X)\n\n# XGBoost optimization\nclf_xgb = XGBClassifier(n_estimators=50, max_depth=8, random_state=1)\n\nxgb_params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 8, 10],\n        'n_estimators' : [50]\n        }\n\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)\ngcv_xgb = GridSearchCV(clf_xgb, xgb_params, n_jobs=-1, cv=skf, verbose=1)\ngcv_xgb.fit(X_reduced_p50, y)","4653d63c":"gcv_xgb.best_estimator_, gcv_xgb.best_score_","ca5fa4a5":"### Training classifiers","95bed737":"## Modeling\n\n### Plan\n\n#### Target\n\nThe idea is to use several classifiers to predict whether the hero is Human or No-Human. \n\n#### Input Data\n\nAs **input data**, I will use the processed and cleaned `heroes` dataframe, which contains 388 superheroes and 177 different characteristics of each. The target is balanced between rows, but the main problem I expect to happen is that the data is highly complex (high dimensional) in comparison with the amount of examples we possess. We will tackle that issue later on.\n\nWe also standardize the data as it is shown that helps the model optimization in terms of faster convergence due to better gradient flow, and a possible increase in performance. Standardizing data means scaling the feature values so that the resulting data has zero mean and unit variance.\n\n#### Evaluation\n\nAs the target is quite balanced, it is okay to use actual accurcacy as a main way to evaluate our models through cross validation. As a further way of evaluation, it would be interesting to look at the confusion matrix to understand a bit better what is our model doing.\n\n#### Models\n\nAs a baseline, we use **Logistic Regression** as it is easy to implement, understand, and should provide an already decent predictive power.\n\nWe will continue with using **SVM**, **Random Forest Classifier**, and **XGBoost**. The reason behind using those is because they are known and proven to be the most powerful conventional machine learning algorithms. We won't use neural networks in this dataset due to the obvious little amount of data we have.\n\n#### Feature Reduction\n\nAs our dataset has little amount of examples in comparison with the amount of features, it would be interesting to try to reduce the complexity of the dataset, to see if we predict better or not. One of the main assumptions behind dimensionality reduction is that the high complex datasets capture a lot of unnecessary information which can be reduced into a arbitrable amount of principal components that explain the variance of the underlying data, with the ideal case that the model will be trained with more rellevant information. To get a glampse of whether the dimensionality reduction will work or not, we can use t-SNE algorithm to plot high dimensionality data into two dimensions, colored by the label value (Human \/ Not-Human) and see if we can already infer a way to discriminate our dataset.\n\nFor the sake of reducing dimensionality, we will use the classic PCA with different amount of components and study what is the impact that it does to our dataset.","df03a57e":"This looks better. Let's now check how many heroes do not have an entry in `metadata` but in `powers`, and viceversa.","fde9b113":"Going in the good direction, but it still looks like there are more entries in metadata, this might be related to the number of repeated entries that we did not handle yet. Let's find out.","101f8238":"## Outline\n\n- [Import Libraries and Data](#Import-Libraries-and-Data) \n- [Feature Engineering](#Feature-Engineering) \n    - [Data Overview](#Data-Overview) \n    - [Repeated Heroes](#Repeated-Heroes) \n    - [Handling Null Values](#Handling-Null-values) \n    - [Categorical Variables and One-hot encoding](#Categorical-Variables-and-One-Hot-Encoding) \n    - [All data together](#All-data-together) \n- [Modeling](#Modeling) \n    - [Plan](#Plan)\n    - [Training Classifiers](#Training-classifiers) \n    - [Feature Importance](#Feature-Importance)\n    - [Dimensionality Reduction](#Dimensionality-Reduction)","b97f3590":"### Categorical Variables and One-Hot Encoding\n\nHigh cardinality categorical features, we will just convert them to their corresponding code (0,1,2,...).\n\n- Gender\n- Alignment\n\nFor low cardinality features, one-hot encoding will be implemented.\n\n- Eye color\n- Hair color\n- Publisher\n","d5cb329b":"Race, which is our target in this exercise, has 232 (out of 634 rows) rows with nulls. But as for the exercise, we will ignore these. Therefore, let's drop them and then see how many null values are there still in the dataframe.","33bae839":"#### Handle Weight and Height null values\n\nThe idea is to set, for those rows which height and weight are null, the mean of the same gender and race (human \/ no-human). That way we will provide a good value yet still having more training data.\n\nBut first, **convert Race into label column with -> Human \/ No-Human**. The steps followed are:\n\n- Everything that is Human, will be considered human. \n- Those Races that are Human-\\* will also be considered human.\n- All the rest, No-Human.","8df36fb9":"Indeed, as I said. Let's then delete those rows.","42f02a0e":"#### Checking distribution of height and weight per gender and race","f805671f":"As seen, most values lie on the same range for height, for weight the deviation is a bit higher, but I would say that it is still valid to use the mean.","dacd3c8e":"### Repeated Heroes ","f6357dec":"`metadata` has null values, represented as either '-' for all columns except for height and weight, which is `-99.0`. \n\nLet's take a look at how many null values there are per each column.","eb6467b1":"-------------\n\n#### Explanation","1fb4b8eb":"Looks like there are some superheroes that need to be removed from both tables as we won't have their entire information.","52c28e15":"----------------\n\n## Import Libraries and Data","f133e505":"Let's just forget about the remaining rows as the number is insignificant.","9204ed41":"It decreased drastically. But\n\n- Eye color and hair color still have some percentage of values missing.\n- Skin color has too many values being null. It could be a potential feature to be removed. So let's just remove it.\n- Height and weight have quite some null values, but they can be filled by different techniques, like the median\/mean of same race and gender.\n\nNext steps:\n\n- Look at feature distribution of each variable to see if there can be applied any quick wins for the features with many null values.","71dd7ad9":"Practically no difference between the barplots shown before the methodology.\n\nLet's now finally see what is left to handle in terms of null values:","3f123441":"### Dimensionality Reduction","761abeb6":"It seems like `Goliath` is a bit messed up in `metadata` and it can be cleaned up. `Goliath` and `Black Goliath` are the same superhero. `Goliath IV` seems to be an evolution or something from `Goliath`, but for simplicity we will say that all `Goliaths` appearing in `metadata` are the same one, hence it allows it to be merged with `powers`.\n\nTherefore, let's unify `Goliath` into one. We will take mostly the characteristics from `Goliath IV` but the Race from the others (`Human`).","923a6b6b":"# Super Heroes Dataset\n\nThe goal of the task is to **predict whether a superhero is Human or not** based on their characteristics and super powers.","3ec4153a":"As mentioned in the exercise wording, both tables do not have a 1-1 mapping between them. Logically this would not make sense, assuming that the super heroes between two tables have same names. This 1-\\* correspondance (being 1 `powers` and \\* `metadata`) might be caused by `metadata` containing repeated name entries for several heroes, or some data disturbances such as errors in the scraping, etc. Let's try to quickly get a glance of this by \n\n- Getting repeated values from `metadata`, or `powers`, in case of existing.\n- Try to make sense of the difference between `metadata` and `powers` number of rows by summing the entries of the repeated rows from `metadata`.","305c56d5":"As a first round for classifers, we already get a 76.77% accuracy. It is not bad, but we can do better. Let's grid search best combination of parameters for Random Forest and XGBoost as those classifiers showed the best performance so far.","1a79eafa":"-------------------\n\n## Feature Engineering\n\n### Data Overview","5e28ef0d":"### Handling Null values","8fd3f5d8":"Let's get a final check of how the values of height and weight correlate to each other, to see if there are still some weird things happening:","d7152021":"#### Handling Eye and Hair color\n\nThe idea would be similar as of in Height and Weight. Let's take the most common eye and hair color","5dd287f7":"**Done!** Let's now continue by merging `powers` Dataframe and our brand new cleaned `metadata` dataframe.","bee31d05":"### All data together","9a70bab1":"#### Merge repeated heroes into one","aaff1264":"Looking that the values are quite balanced between human and no-human between the most popular eye colors and hair colors, it will be fair to sample those null values with a random choice of the 3 most popular eye colors (green, brown, blue) and the 5 most popular hair colors (no-hair, red, blond, brown, black). \n\nThe method would be the following\n\n- For each entry in Eye color that is null in metadata, set a value from a random choice of an array of values (green, brown, blue). \n- Similarly, for Hair color, but for array (no-hair, red, blond, brown, black).\n","36510014":"By fine tuning the parameters of both models we have got a really nice improvement in both, RF and XGB with a curiosly same performance of **78.86%**.","a6d3b3f6":"Some non-humans are really tall, but weight not too much. As they are only non-humans, let's just leave them because it won't confuse the classifier. \n\nWhat might confuse the classifier, though, it's the weight to be more than 450kg, as there are the same number of human and non-human instances. Let's take them out of the data.","aa22d2b6":"Check if we made a difference in terms of null values.","9ced060f":"There is a difference indeed between both genders and race. Therefore it makes sense to use such granularity. One could argue that for no-human and male, we should use median as the height and weight are quite spread. But no-human males are higher and heavier, therefore by using mean we would take outliers into account and hence preserve this relationship between both groups. \n\nTo completely prove this, let's look at the height and weight boxplot of all No-Human races, to see if the values are more or less well distributed or they are indeed different:","e6a10875":"*Voila!*, 1-1 correspondence between both dataframes.","30891103":"Let's call the datasets in the following way throughout the Notebook:\n\n- Heroes Information : **`metadata`**\n- Heroes Superpowers : **`powers`**","8ca8f970":"As we can see, we went from 72 and 89 values for height and weight, respectivelly, to 5 and 6. And, most likely, those 5 and 6 values are due to missing Gender.\n\nFor now we could say that we can drop those rows from Gender, Height, and Weight that are null, as it will most likely be the same. Let's check it out quickly:","97d68f6a":"As the process of dealing with Repeated Heroes is quite extensive, I created a function in the cell below that does all the work in a single step. If you would like to know the why of each action, please continue scrolling down, if not, jump to the [next section](#Handling-Null-values) .","b843d22f":"### Feature Importance\n\nOf the best model seen, which is XGBoost.\n\nCode obtained in kernel published by sendohchang https:\/\/www.kaggle.com\/sendohchang\/classify-superhero-is-a-human ","d8eba3b9":"Looks like there are still 48 heroes that either do not have an entry in powers but in metadata, and viceversa. Let's figure them out. But first let's check if all repeated heroes are in powers dataset to avoid bias in the result.","a0ab16d7":"Let's check how many null values do we have missing:"}}