{"cell_type":{"3c4661c0":"code","dcdccee1":"code","1f143782":"code","886e6b98":"markdown"},"source":{"3c4661c0":"import pandas as pd\n\ntest_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")","dcdccee1":"from transformers import pipeline\n\nmodel_name = \"..\/input\/huggingface-qa-models\/mrm8488\/bert-multi-cased-finedtuned-xquad-tydiqa-goldp\"\n\nqa_pl = pipeline('question-answering', model=model_name, tokenizer=model_name, device=0)\n\npredictions = []\n\n# batches might be faster \nfor ctx, q in test_df[[\"context\", \"question\"]].to_numpy():\n\n    result = qa_pl(context=ctx, question=q)\n    \n    predictions.append(result[\"answer\"])","1f143782":"submission_df = pd.DataFrame(data={\"id\": test_df[\"id\"], \"PredictionString\": predictions})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\nsubmission_df","886e6b98":"### Real basic. Just trying to get a sense of how good existing multilingual models are.\n\n#### [deepset\/xlm-roberta-large-squad2](https:\/\/huggingface.co\/deepset\/xlm-roberta-large-squad2): 0.571  \n#### [deepset\/xlm-roberta-base-squad2](https:\/\/huggingface.co\/deepset\/xlm-roberta-base-squad2): 0.493\n#### [seongju\/squadv2-xlm-roberta-base](https:\/\/huggingface.co\/seongju\/squadv2-xlm-roberta-base): 0.349\n#### [mrm8488\/xlm-multi-finetuned-xquadv1](https:\/\/huggingface.co\/mrm8488\/xlm-multi-finetuned-xquadv1): 0.007 \ud83d\ude02"}}