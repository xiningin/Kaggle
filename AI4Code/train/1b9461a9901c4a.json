{"cell_type":{"eb359763":"code","4b4a09a1":"code","9725bf5c":"code","a7f73008":"code","01f2c47c":"code","be23e9eb":"code","da73c8fe":"code","376a5edc":"code","f58278d5":"code","09d3b90a":"code","eebc3b7e":"code","1a4a1455":"code","197859a9":"code","3d570659":"code","2a7630da":"code","e3505732":"code","33aae84b":"code","e04e81ef":"code","d4ae1e70":"code","e757ccf4":"code","5d740584":"code","701391f7":"code","89af8b03":"code","ce164b3b":"code","ad5bd754":"code","6e4877dc":"code","f36ea20c":"code","71c030cf":"code","4964064c":"code","6a73c8f4":"code","e6fc6953":"code","b8439732":"code","610861a9":"code","ee492dc9":"code","753355d6":"code","520c37f3":"code","45a3ffe9":"code","d29ec630":"code","8457dc42":"code","194806ad":"code","0831c8e8":"code","5996d3fc":"code","6aa94efa":"code","a5de27b6":"code","7237d595":"code","e7d00304":"code","5feff60c":"code","96a30fd4":"code","5ecddcb9":"code","7f7e57c6":"code","abfb0fcf":"code","c7dddd7f":"code","2d955c65":"code","b7a7a124":"code","223e88e7":"code","7acd3d74":"code","ff285a96":"code","689134e5":"code","ae5761d6":"code","3467c26e":"code","bbc2d55a":"code","01fe72c4":"code","eaccb630":"code","63c84c2e":"code","a316457d":"code","454f15f4":"code","2cb95189":"code","af289e45":"code","bf558c3b":"code","09686467":"code","3540ff1c":"code","9b2b4c04":"code","0af17ca7":"code","b21cd6de":"code","e48c61eb":"code","53487528":"code","ebd15744":"code","d4d790b7":"code","ea1ef92b":"code","537f6fd6":"code","ca6c8f9f":"code","ed6ee0d3":"code","8bcee97c":"markdown","2f846760":"markdown","7ec4ac53":"markdown","e179ca08":"markdown","fbaaaa1c":"markdown","f347b2e4":"markdown","bfc8ee95":"markdown","733d961a":"markdown","30908d50":"markdown","364f6446":"markdown","8b34ffbe":"markdown","b9054b1f":"markdown","771b95f7":"markdown","bec4c35e":"markdown"},"source":{"eb359763":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4b4a09a1":"import plotly as py\nimport cufflinks as cf\nfrom plotly.offline import iplot","9725bf5c":"py.offline.init_notebook_mode(connected=True)\ncf.go_offline()","a7f73008":"df = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\" , index_col=0)\ndf.head()","01f2c47c":"df.drop(labels=['Title', 'Clothing ID'], axis=1 , inplace=True)","be23e9eb":"df.head()","da73c8fe":"#check nan Values\ndf.isnull().sum()","376a5edc":"#drop null values\ndf.dropna(subset=['Review Text','Division Name'], inplace=True)","f58278d5":"df.isnull().sum()","09d3b90a":"# print(' '.join(df['Review Text'].tolist()))[:1000]","eebc3b7e":"contractions = { \n\"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren't\": \"are not \/ am not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had \/ he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he shall \/ he will\",\n\"he'll've\": \"he shall have \/ he will have\",\n\"he's\": \"he has \/ he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has \/ how is \/ how does\",\n\"I'd\": \"I had \/ I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I shall \/ I will\",\n\"I'll've\": \"I shall have \/ I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \/ it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it shall \/ it will\",\n\"it'll've\": \"it shall have \/ it will have\",\n\"it's\": \"it has \/ it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she had \/ she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she shall \/ she will\",\n\"she'll've\": \"she shall have \/ she will have\",\n\"she's\": \"she has \/ she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as \/ so is\",\n\"that'd\": \"that would \/ that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that has \/ that is\",\n\"there'd\": \"there had \/ there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there has \/ there is\",\n\"they'd\": \"they had \/ they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they shall \/ they will\",\n\"they'll've\": \"they shall have \/ they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we had \/ we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what shall \/ what will\",\n\"what'll've\": \"what shall have \/ what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what has \/ what is\",\n\"what've\": \"what have\",\n\"when's\": \"when has \/ when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where has \/ where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who shall \/ who will\",\n\"who'll've\": \"who shall have \/ who will have\",\n\"who's\": \"who has \/ who is\",\n\"who've\": \"who have\",\n\"why's\": \"why has \/ why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you had \/ you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you shall \/ you will\",\n\"you'll've\": \"you shall have \/ you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","1a4a1455":"def cont_to_exp(x):\n    if type(x) is str:\n        x=x.replace('\\\\', '')\n        for key in contractions:\n            value=contractions[key]\n            x=x.replace(key, value)\n        return x\n    else:\n        return x\n            \n            ","197859a9":"x=\"i don't  know what date is today, I'm 5'8\\\"\"","3d570659":"print(cont_to_exp(x))","2a7630da":"%%time\ndf['Review Text']=df['Review Text'].apply(lambda x: cont_to_exp(x))","e3505732":"# ' '.join(df['Review Text'].tolist())","33aae84b":"df.head()","e04e81ef":"from textblob import TextBlob","d4ae1e70":"df[\"polarity\"]=df['Review Text'].apply(lambda x: TextBlob(x).sentiment.polarity)","e757ccf4":"df['review_len']=df['Review Text'].apply(lambda x: len(x))","5d740584":"df['word_count']=df['Review Text'].apply(lambda x: len(x.split()))","701391f7":"def get_avg_word_len(x):\n    words=x.split()\n    word_len=0\n    for word in words:\n        word_len = word_len+len(word)\n    return word_len\/len(words)","89af8b03":"df['avg_word_len']=df['Review Text'].apply(lambda x:get_avg_word_len(x))","ce164b3b":"df.head()","ad5bd754":"df['polarity'].iplot(kind='hist', colors='red', bins=50, xTitle='Polarity', yTitle='Count',title='Sentiment Polarity Distribution'  )","6e4877dc":"df['Rating'].iplot(kind='hist', xTitle='Rating',yTitle='Count', title='Review Rating distribution ')","f36ea20c":"df['Age'].iplot(kind='hist',bins=40,xTitle='Age', yTitle='Count', title='Reviewrs Age distribution ')","71c030cf":"df['review_len'].iplot(kind='hist' ,bins=40,  xTitle='Review len', yTitle='count', title ='Review Text Len Distribution')","4964064c":"df['word_count'].iplot(kind='hist' ,bins=40,  xTitle='Word Count', yTitle='count', title ='Word Count Distribution')","6a73c8f4":"df['avg_word_len'].iplot(kind='hist' ,bins=40,  xTitle='avg_word len', yTitle='count', title ='Avg word Distribution')","e6fc6953":"df['review_len'].iplot(kind='hist' ,bins=40,  xTitle='Review len', yTitle='count', title ='Review Text Len Distribution')","b8439732":"df['Department Name'].value_counts()","610861a9":"df.groupby('Department Name').count()","ee492dc9":"df['Department Name'].value_counts().iplot(kind=\"bar\" , yTitle='Count', xTitle='Department ', title=' Bar Chart Department\\'s Name')","753355d6":"df['Division Name'].value_counts().iplot(kind=\"bar\" , yTitle='Count', xTitle='Division ', title=' Bar Chart Division\\'s Name')","520c37f3":"df['Class Name'].value_counts().iplot(kind=\"bar\" , yTitle='Count', xTitle='Class ', title=' Bar Chart Class\\'s Name')","45a3ffe9":"x='this is a test example'\n\n#unigram= this , is , a , test , example\n#bigram =this is , is a, a test , test example\n#trigram= this is a , is a test , a test example","d29ec630":"from sklearn.feature_extraction.text import CountVectorizer\n\n","8457dc42":"x=['this is the list list this']","194806ad":"vec=CountVectorizer().fit(x)\nbow=vec.transform(x)\nsum_words=bow.sum(axis=0)\n\nwords_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\nwords_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\nwords_freq","0831c8e8":"def get_top_n_words(x, n):\n    vec=CountVectorizer().fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    ","5996d3fc":"get_top_n_words(x,3)","6aa94efa":"words=get_top_n_words(df['Review Text'], 20)","a5de27b6":"words","7237d595":"df1=pd.DataFrame(words, columns=['Unigram', 'Frequency'])\ndf1","e7d00304":"df1=df1.set_index('Unigram')\ndf1.iplot(kind='bar', xTitle='Unigram', yTitle='Count', title='Top 20 Unigram Words')","5feff60c":"def get_top_n_words(x, n):\n    vec=CountVectorizer(ngram_range=(2,2)).fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    ","96a30fd4":"get_top_n_words(x,3)","5ecddcb9":"words=get_top_n_words(df['Review Text'], 20)","7f7e57c6":"df1=pd.DataFrame(words, columns=['Bigram', 'Frequency'])\n","abfb0fcf":"df1=df1.set_index('Bigram')\ndf1.iplot(kind='bar', xTitle='Bigram', yTitle='Count', title='Top 20 Bigram Words')","c7dddd7f":"def get_top_n_words(x, n):\n    vec=CountVectorizer(ngram_range=(3,3)).fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    ","2d955c65":"get_top_n_words(x,3)","b7a7a124":"words=get_top_n_words(df['Review Text'], 20)","223e88e7":"df1=pd.DataFrame(words, columns=['Trigram', 'Frequency'])\n","7acd3d74":"df1=df1.set_index('Bigram')\ndf1.iplot(kind='bar', xTitle='Bigram', yTitle='Count', title='Top 20 Bigram Words')","ff285a96":"def get_top_n_words(x, n):\n    vec=CountVectorizer(ngram_range=(1,1), stop_words='english').fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    ","689134e5":"get_top_n_words(x,3)","ae5761d6":"words=get_top_n_words(df['Review Text'], 20)\n","3467c26e":"df1=pd.DataFrame(words, columns=['Unigram', 'Frequency'])\n","bbc2d55a":"df1=df1.set_index('Unigram')\ndf1.iplot(kind='bar', xTitle='Unigram', yTitle='Count', title='Top 20 Unigram Words')","01fe72c4":"def get_top_n_words(x, n):\n    vec=CountVectorizer(ngram_range=(2,2), stop_words='english').fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    \nget_top_n_words(x,3)\n\nwords=get_top_n_words(df['Review Text'], 20)\n\ndf1=pd.DataFrame(words, columns=['Bigram', 'Frequency'])\n\ndf1=df1.set_index('Bigram')\ndf1.iplot(kind='bar', xTitle='Bigram', yTitle='Count', title='Top 20 Bigram Words')\n","eaccb630":"def get_top_n_words(x, n):\n    vec=CountVectorizer(ngram_range=(3,3), stop_words='english').fit(x)\n    bow=vec.transform(x)\n    sum_words=bow.sum(axis=0)\n\n    words_freq=[(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq=sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]\n    \n# get_top_n_words(x,3)\n\nwords=get_top_n_words(df['Review Text'], 20)\n\ndf1=pd.DataFrame(words, columns=['Trigram', 'Frequency'])\n\ndf1=df1.set_index('Trigram')\ndf1.iplot(kind='bar', xTitle='Trigram', yTitle='Count', title='Top 20 Trigram Words')\n","63c84c2e":"import nltk","a316457d":"nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","454f15f4":"blob= TextBlob(str(df['Review Text']))","2cb95189":"print(nltk.help.upenn_tagset())","af289e45":"blob.tags","bf558c3b":"pos_df=pd.DataFrame(blob.tags, columns=['words','pos'])\npos_df=pos_df['pos'].value_counts()\npos_df","09686467":"pos_df.iplot(kind='bar')","3540ff1c":"df.head(2)","9b2b4c04":"sns.pairplot(df)","0af17ca7":"sns.catplot(x='Division Name', y='polarity',data=df)","b21cd6de":"sns.catplot(x='Division Name', y='polarity',data=df, kind='box')","e48c61eb":"sns.catplot(x='Department Name', y='polarity',data=df)","53487528":"sns.catplot(x='Department Name', y='polarity',data=df, kind='box' )","ebd15744":"sns.catplot(x='Division Name', y='review_len',data=df, kind='box')","d4d790b7":"import plotly.express as px\nimport plotly.graph_objects as go","ea1ef92b":"x1=df[df['Recommended IND']==1]['polarity']\nx0=df[df['Recommended IND']==0]['polarity']\n","537f6fd6":"type(x1)","ca6c8f9f":"trace1=go.Histogram(x=x0, name='Not Recommended ', opacity=0.8)\ntrace0=go.Histogram(x=x1, name='Recommended ', opacity=0.8)","ed6ee0d3":"data=[trace0,trace1]\nlayout=go.Layout(barmode='overlay', title='Distribution of sentiment Polarity of Reviews Based On The Recommendation ')\nfig=go.Figure(data=data,layout=layout)\nfig.show()","8bcee97c":"# Distribution of Review Text Length and Word Length","2f846760":"# Trigram","7ec4ac53":"# TEXT CLEARNING","e179ca08":"# Distribution of unigram, bogram and trigram without stop words","fbaaaa1c":"# Distribution of Sentiment Polarity","f347b2e4":"# # Unigram","bfc8ee95":"# Distribution of Department Division and Class","733d961a":"# Bivariate Analysis","30908d50":"# Distribution of Unigram , Bigram and Trigram","364f6446":"# Bigram","8b34ffbe":"# Distribution of top 20 Parts of speech POS tags","b9054b1f":"# Bigram","771b95f7":"# Feature Engineering ","bec4c35e":"# Distribution of Reviews Rating and Reviewers Age"}}