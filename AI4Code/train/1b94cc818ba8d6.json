{"cell_type":{"c61e07d0":"code","03bdfe88":"code","944c5525":"code","883d2ad9":"code","28b022f8":"code","dd46457b":"code","92812e1d":"code","de465ea4":"code","64b9bc42":"code","85b6a477":"code","8da234f3":"code","84d4be7d":"code","29b3ece5":"code","3c8619d3":"code","1afd74a8":"code","194f4e27":"code","9d5e277f":"code","5a861321":"code","e451386e":"code","5eefb2a7":"code","5a4f9de8":"code","20ca7d08":"code","5275f9c8":"code","08f0d9b0":"code","f97efb37":"code","a2457638":"code","9eb5008c":"code","b5a03f96":"code","a140f23d":"code","181afd0a":"code","39849412":"markdown","6ed2e4a1":"markdown","5f88dcac":"markdown","4b8c440e":"markdown","8d3369c2":"markdown","7b3ae325":"markdown","1db5cf2f":"markdown","d140c723":"markdown","8bec9af8":"markdown","f12ec0dd":"markdown","338eaf88":"markdown","7629d31c":"markdown","59e08ff3":"markdown","653d972f":"markdown","b340cdd3":"markdown","f96485ab":"markdown","ca4b09b6":"markdown","68a40976":"markdown","54e27b37":"markdown","1dc4be07":"markdown","4b87cb26":"markdown","37fc23c4":"markdown","51951ac0":"markdown","c602518a":"markdown","eaaa2f31":"markdown","46cac5b9":"markdown","9752dbb9":"markdown","58e9072d":"markdown","45591907":"markdown","cca98151":"markdown","2f734442":"markdown","b9052f29":"markdown"},"source":{"c61e07d0":"# imports!\nimport numpy as np\nimport pandas as pd\nimport xarray as xr  # easier handling of multi-dimensional arrays than numpy\n\nimport matplotlib.gridspec as gridspec\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pl\nfrom matplotlib import colors","03bdfe88":"directory = '\/kaggle\/input\/meteonet\/'\n\nzone = 'NW'\nyear = 2016\nmonth = 1\nsubmonth = 1\nnan_val = -1\n\n# rainfall files are located in a structured format, so we can easily locate them\n\ndef fetch_rainfall(directory, zone, year, month, submonth, ind=15):\n    \"\"\"\n    Returns: rainfall data in xarray format, with coordinates.\n    \n    Precondition: directory is the directory to locate the file in, in string format\n                  zone is a two-letter string corresponding to a zone of the dataset (default NW)\n                  year is an int representing a valid year\n                  month is an int representing a valid month\n                  submonth is an int from 1 to 3, inclusive\n                  ind is the 5-minute period to plot\n    \"\"\"\n    \n    s1 = f'{zone}_rainfall_{year}'\n    s2 = f'rainfall-{zone}-{year}-' + f'{month}'.zfill(2)\n    s3 = f'rainfall_{zone}_{year}_' + f'{month}'.zfill(2) + f'.{submonth}.npz'\n    \n    filepath = directory + s1 + '\/' + s1 + '\/' + s2 + '\/' + s2 + '\/' + s3\n    coords_path = directory + 'Radar_coords\/'*2 + f'radar_coords_{zone}.npz'\n    \n    # load the data\n    data = np.load(filepath,allow_pickle=True)['data'][ind,:,:]\n    coordinates = np.load(coords_path,allow_pickle=True)\n    \n    # uses spacial resolution of data (in degrees) to center coordinates\n    res = 0.01\n    lat = coordinates['lats']-res\/2\n    lon = coordinates['lons']+res\/2\n    \n    data = xr.DataArray(data,coords=[lat[:,0],lon[0,:]],dims=['latitude','longitude'])\n    radar = data.to_dataset(name = 'rainfall')\n    \n    return radar,lat,lon\n    ","944c5525":"# testing the function to see if we can plot the data\nradar_test,lat_test,lon_test = fetch_rainfall(directory, zone, year, month, submonth)","883d2ad9":"fig = plt.figure()\n\nif (np.max(radar_test['rainfall'].values) > 65):\n    borne_max = np.max(radar_test['rainfall'].values)\nelse:\n    borne_max = 65 + 10\ncmap = colors.ListedColormap(['silver','white', 'darkslateblue', 'mediumblue','dodgerblue', 'skyblue','olive','mediumseagreen'\n                              ,'cyan','lime','yellow','khaki','burlywood','orange','brown','pink','red','plum'])\nbounds = [-1,0,2,4,6,8,10,15,20,25,30,35,40,45,50,55,60,65,borne_max]\nnorm = colors.BoundaryNorm(bounds, cmap.N)\n\n# take out nan values\nnan_radar_test = radar_test.where(radar_test[\"rainfall\"]!=nan_val)\n\nfig.set_size_inches(11,8)\nplt.imshow(nan_radar_test[\"rainfall\"].values,cmap=cmap, norm=norm)\nfig.suptitle('Rainfall')","28b022f8":"ground2016 = pd.read_csv(directory + 'NW_Ground_Stations\/'*2 + 'NW_Ground_Stations_2016.csv')\nground2016.describe()","dd46457b":"# select a random sample of sample_size rows\nsample_size = 10000\nsample_weather = ground2016.sample(n=sample_size, random_state=42)\nsample_weather.isnull().sum()\/sample_size","92812e1d":"# these are essentially useless. we want to predict weather regardless of location!\nsample_weather.drop(columns = ['lat', 'lon', 'number_sta', 'height_sta'], inplace=True)\n\n# time is also not important\nsample_weather.drop(columns = ['date'], inplace=True)\nprint(sample_weather.columns)","de465ea4":"sample_filtered = sample_weather[sample_weather.isna()['psl'] == False]\nprint(sample_filtered.isnull().sum()\/len(sample_filtered))\nprint(sample_filtered.describe()-ground2016.describe())","64b9bc42":"# interpolate missing values\n# we don't have a lot, but we should still do it\nsample_filtered = sample_filtered.interpolate(method='linear')\nprint(sample_filtered.isnull().sum())\nprint(sample_filtered.isnull())","85b6a477":"from sklearn.manifold import TSNE\nimport seaborn as sns\nimport time\n\nfeat_cols = ['dd','ff','hu','td','t', 'psl']\n\ndf_subset = sample_filtered.copy()\n\ntsne_features = df_subset[feat_cols].values\n\nperplexities = [2,3,5,10,15,30]\n\ntime_start = time.time()\n\nfor val in perplexities:\n    \n    tsne = TSNE(n_components=2, verbose=1, perplexity=val, n_iter=5000)\n    tsne_results = tsne.fit_transform(tsne_features)\n    \n    # organize results\n    df_subset[f'tsne-2d-x-{val}'] = tsne_results[:,0]\n    df_subset[f'tsne-2d-y-{val}'] = tsne_results[:,1]\nprint('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))","8da234f3":"### Graphing and whatnot ###\n\nfig = plt.figure()\nfig.set_figheight(10*1.5)\nfig.set_figwidth(16*len(perplexities)\/3)\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\ncmap = sns.cubehelix_palette(dark=0.3, light=0.8, as_cmap=True)\n\nfor n in range(len(perplexities)):\n    ax = fig.add_subplot(2, len(perplexities)\/2, n+1)\n    sns.scatterplot(\n        x=f\"tsne-2d-x-{perplexities[n]}\", y=f\"tsne-2d-y-{perplexities[n]}\",\n        palette=cmap,\n        hue='precip',\n        hue_norm=(0, 0.05),\n        size='precip',\n        data=df_subset,\n        legend=\"full\",\n        alpha=0.6,\n        ax=ax\n    )\nplt.show()","84d4be7d":"# we can use a larger sample than we did with t-SNE\nN = 200000\ndf = ground2016.sample(n=N, random_state=42)\n\ndf.drop(columns = ['lat', 'lon', 'number_sta', 'height_sta'], inplace=True)\n\ndf.drop(columns = ['date'], inplace=True)\n\ndf = df[df.isna()['psl'] == False]\n\ndf = df.interpolate(method='linear')\n\ndf_feats = df[feat_cols]","29b3ece5":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\n\nbest_r = (0, -1)\nr_squares = []\n\nfor k in range(1,201):\n    X_train, X_test, Y_train, Y_test = train_test_split(df_feats, df['precip'], test_size=0.2, random_state=42)\n\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n\n    Y_pred = knn.predict(X_test)\n    r = r2_score(Y_test, Y_pred)\n    r_squares.append(r)\n    if r > best_r[0]:\n        best_r = (r, k)\n\nprint(f'Greatest coef. of determination: {best_r[0]} occured at k = {best_r[1]}')","3c8619d3":"plt.plot(range(1,201), r_squares)\nplt.title('R^2 vs. k')\nplt.xlabel('k value')\nplt.ylabel('coef. of determination')\nplt.show()","1afd74a8":"from sklearn.decomposition import PCA\n\n# duplicate code, after PCA applied\nbest_r = (0, -1)\nr_squares = []\n\npca = PCA(n_components=2)\npca_df = pca.fit_transform(X=df_feats)\n\nfor k in range(1,201):\n    X_train, X_test, Y_train, Y_test = train_test_split(pca_df, df['precip'], test_size=0.2, random_state=42)\n\n    knn = KNeighborsRegressor(n_neighbors=k)\n    knn.fit(X_train, Y_train)\n\n    Y_pred = knn.predict(X_test)\n    r = r2_score(Y_test, Y_pred)\n    r_squares.append(r)\n    if r > best_r[0]:\n        best_r = (r, k)\n\nprint(f'Greatest coef. of determination: {best_r[0]} occured at k = {best_r[1]}')\n\nplt.plot(range(1,201), r_squares)\nplt.title('R^2 vs. k after PCA')\nplt.xlabel('k value')\nplt.ylabel('coef. of determination')\nplt.show()","194f4e27":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist \n\ndistortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nks = range(2,15)\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(df_feats)\n    \n    distortions.append(sum(np.min(cdist(df_feats, model.cluster_centers_, \n                      'euclidean'),axis=1)) \/ df_feats.shape[0])\n    inertias.append(model.inertia_)\n    \n    mapping1[k] = sum(np.min(cdist(df_feats, model.cluster_centers_, \n                      'euclidean'),axis=1)) \/ df_feats.shape[0]\n    mapping2[k] = model.inertia_\n    ","9d5e277f":"for k, dist in mapping1.items():\n    print(str(k) + ': ' + str(dist))","5a861321":"plt.plot(ks, distortions, 'bx-')\nplt.title('distortion elbow')\nplt.xlabel('k = # of clusters')\nplt.ylabel('distortion value')\nplt.show()","e451386e":"plt.plot(ks, inertias, 'bx-')\nplt.title('inertia elbow')\nplt.xlabel('k = # of clusters')\nplt.ylabel('inertia value')\nplt.show()","5eefb2a7":"# NOTE: the package for hdbscan is build ontop of sci-kit learn, but is NOT native to the library.\n#       Thus, we will have to perform an inline import; comment this out if you are running on an environment with hdbscan installed.\n!pip install hdbscan","5a4f9de8":"import hdbscan\n\nclusterer= hdbscan.HDBSCAN(min_cluster_size=30, prediction_data=True)\nclusterer.fit(df)","20ca7d08":"num_of_clusters=clusterer.labels_.max()\ndf_cols=clusterer.labels_\nclusterValues, occurCount=np.unique(df_cols, return_counts=True)\nprint('cluster numbers are', clusterValues)\nprint('number of data in a cluster', occurCount)","5275f9c8":"x=0\nlst=[]\nwhile x < num_of_clusters+1:\n    precip=[]\n    for i in range(0,len(df_cols)):\n        if df_cols[i]==x:\n            precip_amt=df['precip'].iloc[i]\n            precip.append(precip_amt)\n    mean=sum(precip)\/len(precip)\n    lst.append(mean)\n    x=x+1\n        \nlst","08f0d9b0":"data=df['precip']\ncolor_palette = sns.color_palette('deep',4)\ncluster_colors = [color_palette[x] if x >= 0\n                else (0.5,0.5,0.5)\n                for x in clusterer.labels_]\ncluster_member_colors = [sns.desaturate(x,p) for x, p in \n                        zip(cluster_colors, clusterer.probabilities_)]\n\n#plt.scatter(data.T[0],data.T[1], s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)\n\n","f97efb37":"clusterer.condensed_tree_\nclusterer.condensed_tree_.plot()","a2457638":"clusterer.condensed_tree_.plot(select_clusters=True,\n                              selection_palette=sns.color_palette('deep',11999))","9eb5008c":"clusterer.single_linkage_tree_\nclusterer.single_linkage_tree_.plot()","b5a03f96":"clusterer=hdbscan.HDBSCAN(min_samples=3, prediction_data=True)\n\nx=df.drop('precip', axis=1)\ny=df['precip']\nx_train, x_test, y_train, y_test=train_test_split(x,y, test_size=0.2, random_state=42)\nclusterer.fit(x_train, y_train)\nx=0\nlst=[]\ncols=clusterer.labels_\nwhile x < clusterer.labels_.max()+1:\n    precip=[]\n    for i in range(0,len(cols)):\n        if cols[i]==x:\n            precip_amt=df['precip'].iloc[i]\n            precip.append(precip_amt)\n    mean=sum(precip)\/len(precip)\n    lst.append(mean)\n    x=x+1\n        \nlst\n","a140f23d":"test_labels, strengths=hdbscan.approximate_predict(clusterer, x_test)\npredictions=[]\nnon_clustered = 0\nfor i in range(0,len(test_labels)):\n    if test_labels[i]==0:\n        predictions.append(0.0)\n    elif test_labels[i]==1:\n        predictions.append(0.014285714285714287)\n    elif test_labels[i]==2:\n        predictions.append(0.003250975292587777)\n    else:\n        predictions.append(y_test.iloc[i])\n        if test_labels[i] == -1:\n            non_clustered += 1\n\nprint(non_clustered\/len(test_labels))\nprecip_predict=pd.Series(predictions)\n\n\n","181afd0a":"from sklearn.metrics import mean_absolute_error\n\nprint(\"sklearn's mean absolute error  for predicting precipitation is:\", mean_absolute_error(y_test, precip_predict))\nprint(\"sklearn's R score for predicting precipitation is:\", r2_score(y_test, precip_predict))\n","39849412":"We ran an iterative diagnostic of the k-NN method, but it is easy to see that k-NN is not suited for this problem, under the current conditions. We observe a plateau around 0 in the coefficient of determination for k values higher than 50. This is not good! k-NN is having a hard time predicting the data better than a horizontal hyperplane of the dimension of the data.\n\nBut as discussed [here](https:\/\/www.cs.cornell.edu\/courses\/cs4780\/2018fa\/lectures\/lecturenote02_kNN.html), we have essentially broken the main assumption of the k-NN algorithm; our model has fallen victim to the \"curse of dimensionality.\" Thus, we will retry this iterative approach after some Principal Component Analysis (PCA) to reduce the dimension of our data.","6ed2e4a1":"As we can see here, the model generates four clusters for us with reasonable sizes. However, we do see a great amount of outliers which we might have to adjust later when we try to use the model to actually predict. ","5f88dcac":"**NOTE: This project was completed purely for educational purposes. We leave our work unlicensed, so you are free to do what you would like with it.**\n\nAs it stands, the weather forecasting paradigm has yet to be truly touched by machine learning. The Global Forecast System (GFS) used by the National Weather Service relies on mathematical models of the atmosphere, as does the forecasting model of the European Centre for Medium-Range Weather Forecasts (ECMWF). Physical models rule over computer-learned models. In fact, it would be redundant for a program to learn an atmopheric model if we already know the model. Why teach software to do something that we can already do with supercomputing power?\n\nBut as hinted at above, these physical models are computationally expensive. Companies like [Google](https:\/\/ai.googleblog.com\/2020\/03\/a-neural-weather-model-for-eight-hour.html) have already produced work showing incredible speedups over state-of-the-art physical methods by engineering neural networks to predict local weather. The venture shows promise, but as of now, not enough is known about the efficacy of these methods for them to be implemented on a large scale.\n\nSo we posed the question to ourselves, \"Can we predict the current amount of precipitation based on other initial conditions?\" We hypothesize that we can, and that a proababilistic clustering model based on the HDBSCAN* algorithm will perform better than a k-nearest neighbors model at making predictions.\n\nBefore testing this, we will have to clean the data.","4b8c440e":"Wow, this dataset has 22 MILLION rows in just one year! Since the dataset is so large, we need to take a random sample so that it is possible to take an exploratory approach to getting a better set of initial centroids. Running diagnostics on the entire dataset multiple times would become far too costly.","8d3369c2":"### Load our time series","7b3ae325":"Link to our project: https:\/\/www.kaggle.com\/bradysites\/forecasting-model-comparison-k-nn-and-hdbscan","1db5cf2f":"combine train_test_split with clustering allowed us to predict, now let's see how good it is","d140c723":"Clearly, k-NN is not the correct model for forecasting with our given dataset; the coefficient of determination sticking around 0 means we are taking the wrong approach. Now, we will see how well clustering can predict weather.","8bec9af8":"# HDBSCAN*\nHDBSCAN* is a hierarchical clustering algorithm which extends from DBSCAN. It uses a technique to extract a flat clustering based in the stability of clusters. Due to its hierarchical nature, it is inevitably very sensitive to noise in the data. However, it is very neat for our case because it allows clustering without specifying the number of clusters. In addition, this density-based algorithm can be very powerful because it is indifferent to the shape of clusters and robust with respect to lcusters with different density.","f12ec0dd":"Great, we now have a much better idea of what our data looks like, in a two dimensional space. Should we cluster on the output on one of these t-SNE models?\n\nThe answer is no! t-SNE does not preserve distances or densities, so performing any type of machine learning on t-SNE output would give  distorted results. Instead, we want to intepret these results within the specifications of t-SNE. Based on overall structure, some clusters naturally form within the data. However, these clusters are very strange-looking. Since t-SNE does not preserve distances, it only clusters neighbors, these clusters can definitely manifest in strange forms.\n\nTherefore, we will train our clustering model on the original data, not the t-SNE output.","338eaf88":" ### Radar plotting","7629d31c":"### Determining optimal cluster count (supplemental)\n\n##### as we'll learn in the next section, we do not actually need to do this. but this provides us the opportunity to potentially improve our model based on what is known as the Elbow Method. if we had decided to create a Gaussian Mixture Model, for instance, this would be a necessary step.","59e08ff3":"### t-SNE visualization","653d972f":"The next step we want to take is visualizing the data that we will eventually fit a clustering model to. We can do this using T-distributed Stochastic Neighbor Embedding (t-SNE). We have high dimensional data that we would like to visualize in a low dimensional space. The reason we want to do this is so we can observe if the data naturally exhibits some degree of clustering.","b340cdd3":"Since we have so much initial data (in the whole dataset, not the sample), we can reduce our sample to only entries with an air pressure reading without worrying about making our feature space too small. ~80% of entries are missing a pressure measurement, but if we take 20% of 22 million, we still get over 4 million data points. And that is just for one year! We can safely assume that pressure readings are missing at random due to some equipment limitations; it is not like they only took pressure measurements when it was raining. Nevertheless, we can compare the distributions before and after dropping, just to be sure.","f96485ab":"Now we have a better idea how our data fits into this specific type of model, we can now train and test our sample. ","ca4b09b6":"For effective use of t-SNE, we want to assure an adherence to tradition. We want the perplexity value to be less than the number of samples. The implementation will behave in an unpredictable way if we do not set the perplexity low enough. When van der Maaton & Hinton proposed this method, they recommended a typical range from 5 to 50 [[1]](http:\/\/www.jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf). This will also depend on the sample size.","68a40976":"To determine the ideal k value, we will check the coefficient of determination for models trained with a range of k values, and select the one that gives us the best results.","54e27b37":"### Manipulating the data","1dc4be07":"## k-NN","4b87cb26":"But before we train our clustering model, let's see how a k-nearest neighbors model performs on the data.","37fc23c4":"# Comparitive Analysis of Weather Forecasting Models\n###### an INFO 1998 final project by Ashley Jiang (yj387@cornell.edu) and Brady Sites (bas339@cornell.edu)","51951ac0":"What this tells us is this: the NaN columns were the ones we don't care about. Some boundary values have a seemingly significant difference, but this is a result of sampling. The difference in mean, quartiles, and standard deviation is so small compared to the actual values we saw for the dataset at the beginning that we don't have to worry about selection bias due to preserving the pressure measurement.","c602518a":"# Conclusion\n\nk-NN is not really built for data of this kind. Our feature space is too high-dimensional, and feature space reduction only introduces more noise into our dataset.\n\nClustering is also a very roundabout way of doing what we are trying to do. Noisey datasets are terrible for clustering models, and clustering also suffers from the curse of dimensionality.\n\nUltimately, while clustering performs better than k-NN, neither are suitable for weather forecasting in this feature space.","eaaa2f31":"The value after which rate of change becomes linear occurs around k = 9. Subjectively, it could occur anywhere between 7 and 9 inclusive, but 9 would be the safest choice.","46cac5b9":"Luckily the hdbscan algorithm has its own prediction function; however, it only predicts which cluster a datapoint will fall into not its expected precipitation. Therefore, we have to assign the precipitation values for each datapoint. We also want to take into account how many datapoints were not able to fall in a cluster. ","9752dbb9":"This allows us to find out the average precipitation for the datapoints that are in the same cluster. We can therefore predict the precipitation of a datapoint based on which cluster it falls into.","58e9072d":"A good way to verify if our parameters are set properly is to check the condensed tree graph. For example, if  there is one cluster that takes up the majority of the color and space while we can barely see it for other clusters that means our parameters are not chosen properly.","45591907":"Successful imputation!","cca98151":"We modify our model a bit so that it will actually give us predicted values. Our features will be everything except the 'precip' column and our goal is to predict the amount of precipitation based on the other factors we have. After clustering, we calculate the average precipitation for each cluster.","2f734442":"it is safe for us to conclude that clustering is not a good way to predict precipitation","b9052f29":"Let's first test the algorithm with the entire sampled dataset from before to understand better how it works and what our dataset looks like."}}