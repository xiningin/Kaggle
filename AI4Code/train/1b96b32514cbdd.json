{"cell_type":{"5e6ce68f":"code","725d70e8":"code","46d67b08":"code","98d02ea9":"code","a80646ce":"code","caa01641":"code","220cc63f":"code","be4fabb4":"code","f7afcfa6":"code","632ce760":"code","5d718de2":"code","8fb2762e":"code","8f5ddc1b":"code","a81277dc":"code","02a31638":"code","024d7f16":"code","dd15060e":"code","51bd0761":"code","25e48ac6":"code","375c6fa8":"code","8fb71b61":"code","61490d0e":"code","a0eb9380":"code","0ac661e0":"code","3e72b5c0":"code","3f2efde2":"code","c8245c54":"code","752c7ec5":"markdown","6ddfb0cc":"markdown","114952df":"markdown","69d9f86a":"markdown","bbc78736":"markdown","127231a1":"markdown","9742fd90":"markdown","3a43143d":"markdown","3b1e12c7":"markdown","60689072":"markdown","1e0b772d":"markdown","da47d68b":"markdown","40a4bc2f":"markdown","13a4735d":"markdown","ad2521de":"markdown","f000b147":"markdown","38fb4d8f":"markdown"},"source":{"5e6ce68f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","725d70e8":"#visualize\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n#preprocessing\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler,OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\n\npd.set_option('display.max_columns',None)","46d67b08":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nss = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')","98d02ea9":"print(train.shape)\ntrain.head()","a80646ce":"print(test.shape)\ntest.head()","caa01641":"train.info()","220cc63f":"train.describe()","be4fabb4":"print(train.isnull().sum())\nsns.heatmap(train.isnull(),cbar=False,yticklabels=False)","f7afcfa6":"sns.pairplot(train)","632ce760":"train_df= train.drop('id',axis=1)","5d718de2":"num_data = train_df.select_dtypes('number')\ncat_data = train_df.select_dtypes('object')\nnum_data.head()","8fb2762e":"plt.figure(figsize=(10,6))\nsns.heatmap(num_data.corr(),annot=True,cbar=True)\nplt.show()","8f5ddc1b":"fig = plt.figure(figsize=(20,15))\nfor i,j in enumerate(num_data.columns):\n    fig.add_subplot(4,4,i+1)\n    sns.boxplot(x=j,data=num_data)\n    \nplt.show()","a81277dc":"fig = plt.figure(figsize=(20,15))\nfor i,j in enumerate(num_data.columns):\n    fig.add_subplot(4,4,i+1)\n    ax=sns.kdeplot(x=j,data=num_data)\n    ax = sns.histplot(x=j,data=num_data)\nplt.show()","02a31638":"fig = plt.figure(figsize=(20,15))\nfor i,j in enumerate(cat_data.columns):\n    fig.add_subplot(4,4,i+1)\n    ax=sns.barplot(x=j,y=train['target'],data=cat_data)\nplt.show()","024d7f16":"fig = plt.figure(figsize=(20,15))\nfor i,j in enumerate(cat_data.columns):\n    fig.add_subplot(4,3,i+1)\n    ax=sns.boxplot(x=j,y='target',data=train)\nplt.show()","dd15060e":"def get_unique_sum(cat_list):\n    for i in cat_list:\n        print(train[i].unique())\n        \ncat_lists = list(train.select_dtypes('object').columns)\nget_unique_sum(cat_lists)","51bd0761":"X = train.drop(['id','target'],axis=1)\ny = train['target']\ntest_df = test.drop('id',axis=1)","25e48ac6":"ct = make_column_transformer(\n    (OrdinalEncoder(),cat_lists),\n    (StandardScaler(),['cont1','cont2','cont3','cont4','cont5','cont7','cont9','cont10','cont11','cont12','cont13']),\n    (RobustScaler(),['cont0','cont6','cont8']),\n    remainder='passthrough'\n)\nX_train = pd.DataFrame(ct.fit_transform(X))\ntest = pd.DataFrame(ct.fit_transform(test_df)) ","375c6fa8":"X_train.head()","8fb71b61":"test.head()","61490d0e":"print(X_train.shape)\nprint(test.shape)","a0eb9380":"train.head()","0ac661e0":"train['kfold'] = -1\ntrain.head()","3e72b5c0":"kf = KFold(n_splits=10,shuffle=True,random_state=23)\nfor fold,(train_idx,valid_idx) in enumerate(kf.split(train)):\n    train.loc[valid_idx,'kfold'] = fold","3f2efde2":"figure = plt.figure(figsize=(15,10))\nfor i in range(10):\n    figure.add_subplot(4,3,i+1)\n    sns.histplot(x='target',data=train[train['kfold']==3])\nplt.show()","c8245c54":"train.to_csv('10fold_30dayml.csv',index=False)","752c7ec5":"**part_2 based of Hyperparameter tuning using randomsearchcv and optuna for randomforest**\nif this notebook is useful please Upvote it","6ddfb0cc":"from this boxplot , cont0,cont6,cont8  have some oultiers","114952df":"# Importing Libraries","69d9f86a":"from correlation matrix , we conclude that the independent variables are not not strongly correlated.","bbc78736":"# Preprocessing","127231a1":"### USING COLUMNTRANSFORMER FOR PREPROCESSING","9742fd90":"### Catgorical data VS Targetvariable","3a43143d":"# Visualization","3b1e12c7":"### Categorical data","60689072":"from the pairplot, we declare that all the numrical features are scatter well. so there will be no that much outliers","1e0b772d":"Therefore, there is no null value in the train and test data frame","da47d68b":"from this histogram we saw that the, most of them are not normally distributed .. and the all polynomial distribution.","40a4bc2f":"### scatter plot between numerical_data","13a4735d":"### SPLITTING THE FOLDS AND MAKING CSV FOR FURTHER USE ","ad2521de":"all folds are equally distributed. so there is no problem while cross validation","f000b147":"### dependent varibale visualisation(numerical_data)","38fb4d8f":"### Finding Correlation using Heatmap"}}