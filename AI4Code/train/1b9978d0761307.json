{"cell_type":{"269b2b60":"code","502b2648":"code","c3cceb1f":"code","fd1e478b":"code","ce998ed7":"code","f306f543":"code","cf6d740c":"code","891f2335":"code","7edc0390":"code","ccd665e4":"code","d5f395dc":"code","ae1c7e4b":"code","b79e1882":"code","88060951":"code","fbd0e6b4":"code","b5fbc22c":"code","2b5db18a":"code","41dd47e2":"code","334d2c58":"code","0cd089d7":"code","c28493dd":"code","f90a1e4e":"code","092040fa":"code","756a32d6":"code","2a8bbb6f":"code","171ffc7f":"code","aaebe3a3":"code","ff268b18":"code","0f22231b":"code","e83cb9f7":"code","076d566c":"code","19816be9":"code","ad9a14cf":"code","9a5a76a8":"code","193d44f0":"code","8436982a":"code","c29785c0":"code","b7273b0d":"code","eaf6fc8d":"code","651e6241":"code","e7b25a4b":"code","65100c2f":"code","422afc75":"code","f0f31048":"code","9c5b2342":"code","22b9b84e":"code","fafffd7d":"markdown","c9f3e6a9":"markdown","59bae33d":"markdown","7e35dd17":"markdown","49fac38f":"markdown","4d837d9b":"markdown","7437a969":"markdown","b444e2b0":"markdown","954153c1":"markdown","934ad27a":"markdown","88551b61":"markdown","bfd14184":"markdown","049f6ddf":"markdown","e041ea92":"markdown","cdb12215":"markdown","c84b4790":"markdown","f3cd67cd":"markdown","d68dbbf7":"markdown","cfab6ce8":"markdown"},"source":{"269b2b60":"import numpy as np\nimport keras \nimport keras.backend as K\nimport tensorflow as tf\nimport math","502b2648":"def naive_bce(y_true,y_pred):\n    loss = (-1)* (y_true * K.log(y_pred) + (1-y_true)*K.log(1- y_pred))\n    return K.mean(loss)\n\n\n\ndef bce_numpy_equivalent(y_true,y_pred):     # Important Note :- numpy equivalent Functions are there for understanding only, they shouldn't be passed to the models!!!!!\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+= y_true[i]*(math.log(y_pred[i]))+(1-y_true[i])*math.log(1-y_pred[i])\n    loss*=-1\n    return loss\/len(y_true)\n    \n    ","c3cceb1f":"y_true=K.variable([[0, 1],[0, 0]])    # batch size = 2\ny_pred=K.variable([[0.43, 0.51],[0.32, 0.49]])\nnaive_bce(y_true,y_pred).numpy()","fd1e478b":"bce_numpy_equivalent(y_true,y_pred)","ce998ed7":"# Comparing with Keras BCE\nBCE=keras.losses.BinaryCrossentropy(from_logits=False)\nBCE(y_true,y_pred).numpy()","f306f543":"from tensorflow.python.ops import clip_ops\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.ops import math_ops","cf6d740c":"def cce(y_true,y_pred):\n    if len(y_true.shape)==2:     #batch_size=1\n        samples=y_true.shape[0]\n        batch=1\n    else:                        #batch_size>1  \n        samples=y_true.shape[1]\n        batch=y_true.shape[0]\n    loss=(-1)*(y_true*K.log(y_pred))\n    loss=(math_ops.reduce_sum(loss))\/(samples*batch)\n    return loss\n\n\ndef cce_numpy_equivalent(y_true,y_pred):\n    if len(y_true.shape)==2:     #batch_size=1\n        samples=y_true.shape[0]\n        batch=1\n    else:                        #batch_size>1  \n        samples=y_true.shape[1]\n        batch=y_true.shape[0]\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+= y_true[i]*(math.log(y_pred[i]))\n    loss*=-1\n    return loss\/(samples*batch)\n    \n    \n    ","891f2335":"y_true=K.variable([[[0, 1, 0], [0, 0, 1]],[[0, 1, 0], [0, 0, 1]]])    # batch size = 2\ny_pred=K.variable([[[0.05, 0.9499, 0.0001], [0.1, 0.8, 0.1]],[[0.05, 0.9499, 0.0001], [0.1, 0.8, 0.1]]])\ncce(y_true,y_pred).numpy()","7edc0390":"cce_numpy_equivalent(y_true,y_pred)","ccd665e4":"# Comparing with Keras CCE\nCCE=keras.losses.CategoricalCrossentropy(from_logits=False)\nCCE(y_true,y_pred).numpy()","d5f395dc":"def mse(y_true,y_pred):\n    loss = K.mean(K.square(y_true - y_pred), axis=-1)\n    return K.mean(loss)\n\ndef mse_numpy_equivalent(y_true,y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=(y_true[i]-y_pred[i])*(y_true[i]-y_pred[i])\n    return loss\/len(y_true)","ae1c7e4b":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\nmse(y_true,y_pred).numpy()","b79e1882":"mse_numpy_equivalent(y_true,y_pred)","88060951":"# Comparing with Keras MSE\nMSE = keras.losses.MeanSquaredError()\nMSE(y_true,y_pred).numpy()","fbd0e6b4":"def mae(y_true,y_pred):\n    loss = K.mean(abs(y_true - y_pred), axis=-1)\n    return K.mean(loss)\n\ndef mae_numpy_equivalent(y_true,y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=abs(y_true[i]-y_pred[i])\n    return loss\/len(y_true)","b5fbc22c":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\nmae(y_true,y_pred).numpy()","2b5db18a":"mae_numpy_equivalent(y_true,y_pred)","41dd47e2":"# Comparing with Keras MAE\nMAE = keras.losses.MeanAbsoluteError()\nMAE(y_true,y_pred).numpy()","334d2c58":"def custom_loss(y_true, y_pred):\n    loss=K.mean((pow(y_true,3)+pow(y_pred,3))-(pow(y_true,2)+pow(y_pred,2))-(pow(y_true,1)+pow(y_pred,1)))\n    return K.mean(loss)\n\n\n\ndef custom_loss_numpy_equivalent(y_true, y_pred):\n    y_true=np.asarray(y_true).flatten()\n    y_pred=np.asarray(y_pred).flatten()\n    loss=0\n    for i in range(len(y_true)):\n        loss+=(pow(y_true[i],3)+pow(y_pred[i],3)) - (pow(y_true[i],2)+pow(y_pred[i],2)) -(pow(y_true[i],1)+pow(y_pred[i],1))\n    return loss\/len(y_true)\n    \n    ","0cd089d7":"y_true = K.variable([[0., 1.], [0., 0.]])\ny_pred =K.variable( [[1., 1.], [1., 0.]])\ncustom_loss(y_true,y_pred).numpy()","c28493dd":"custom_loss_numpy_equivalent(y_true, y_pred)","f90a1e4e":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n\ndata = pd.read_csv('..\/input\/oranges-vs-grapefruit\/citrus.csv')\ncol=data.columns","092040fa":"data","756a32d6":"target=[]\nfor i in range(len(data)):\n    if data['name'][i]=='orange':\n        target.append(0)\n    else :\n        target.append(1)","2a8bbb6f":"target=np.asarray(target).astype(np.float32)\nnp.unique(target)","171ffc7f":"X=data[col[1:]]\nX.head()\nX=np.asarray(X).astype(np.float32)\n","aaebe3a3":"X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.2, random_state=42)","ff268b18":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation","0f22231b":"def create_model():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=naive_bce, optimizer='adam', metrics=['accuracy'])\n    return model\n    ","e83cb9f7":"model=create_model()\nmodel.summary()","076d566c":"model.fit(X_train,y_train,epochs=3,batch_size=128)","19816be9":"def bce(y_true,y_pred):\n    loss = K.max(y_pred,0)-y_pred * y_true + K.log(1+K.exp((-1)*K.abs(y_pred)))\n    return K.mean(loss)","ad9a14cf":"def create_modelv2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=bce, optimizer='adam', metrics=['accuracy'])\n    return model\n","9a5a76a8":"modelv2=create_modelv2()\nmodelv2.fit(X_train,y_train,epochs=3,batch_size=128)","193d44f0":"modelv2.evaluate(X_test,y_test)","8436982a":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=5,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","c29785c0":"model2=create_model2()\nmodel2.fit(X_train,y_train,epochs=3,batch_size=128)","b7273b0d":"model2.evaluate(X_test,y_test)","eaf6fc8d":"\nfrom sklearn.datasets import make_regression\nfrom matplotlib import pyplot\n\nX, y = make_regression(n_samples=10000, n_features=10,  n_targets=1, bias=0.1, noise=0.1)","651e6241":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e7b25a4b":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=10,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss=mse, optimizer='adam', metrics=['mse'])\n    return model","65100c2f":"model2=create_model2()\nmodel2.fit(X_train,y_train,epochs=3,batch_size=12)","422afc75":"model2.evaluate(X_test,y_test)","f0f31048":"def create_model2():\n    model = Sequential()\n    model.add(Dense(30,input_dim=10,activation='relu'))\n    model.add(Dense(5,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n    return model","9c5b2342":"model=create_model2()\nmodel.fit(X_train,y_train,epochs=3,batch_size=12)","22b9b84e":"model.evaluate(X_test,y_test)","fafffd7d":"Say we define Our Loss Function as mean ([ y^3 + p^3 ] - [ y^2 + p^2 ] -[ y + p ]) .\nWe can take any other mathematical definition too, that simply depends upon the task you have to perform.","c9f3e6a9":"### Binary Cross Entropy","59bae33d":"As we can see we have got exactly the same result with our custom mse loss function and standard keras mse loss function","7e35dd17":"Now as y_pred approaches 1 or more, 1-y_pred becomes negative and hence k.log(1-y_pred) -> Nan. Similarly when y_pred is 0, k.log(y_pred)-> Nan","49fac38f":"# Regression Example","4d837d9b":"# Mean Squared Eroor","7437a969":"# Mean Absolute Eroor","b444e2b0":"# Using Our loss Functions In a Deep Learning Model","954153c1":"Categorical Cross entropy can be defined as \u2212  (y[i] * log(y_pred[i])) for all i in 1 to n_class\n","934ad27a":"# Comparing it with Standard Keras BCE Loss Function\nWe expect a lower acccuracy cause of the alternate implementation as it doesn't give same loss valuse as standard keras BCE loss function","88551b61":"# Your own Loss Function\nIn this Notebook, we will look at how can we write our very own loss functions from scratch. I will be using keras framework for this purpose. You Guys can implement this notebook in other frameworks \ud83d\ude09 \n\nFirst we will look at how some standard loss functions are written, like binary cross entropy, categorical cross entropy etc, then how can we write a arbitary loss function based on some mathematical formula.","bfd14184":"# Comparing it with Standard Keras BCE Loss Function","049f6ddf":"We can see as y_pred is reaching 0 or 1, so the loss is becoming *nan* , to combat this we can use alternative implementation of bce ","e041ea92":"# IF You got to learn something or you enjoyed the notebook. Please Do upvote \ud83d\ude05\n## Follow me for upcoming tutorials and please show some love to my previous notebooks.\ud83d\ude05\n### Peace Out!!\n### Sourabh Yadav!","cdb12215":"### Categorical Cross entropy","c84b4790":"The code for the Custom Loss function depends first and foremost on its definition. So for writing a custom loss function its mathematical definition should be pretty damn clear.","f3cd67cd":"Our bce function works!!!!!","d68dbbf7":"Binary Cross Entropy is defined as: y * log(p) + (1-y) * log(1-p)","cfab6ce8":"# Custom Loss Functions"}}