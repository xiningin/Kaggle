{"cell_type":{"047242ce":"code","e21ae906":"code","4634cfb3":"code","4c7572a7":"code","85b4e00d":"code","c0b35f7b":"code","f1f9e990":"code","69c74505":"code","54d3035c":"code","60354f4c":"code","68fbe162":"code","fb0a283b":"code","a793e78a":"code","fb6b352c":"code","f1879b29":"code","4181cf70":"code","e521ccf5":"code","e09c0740":"code","dbf0485e":"code","79db3891":"code","ac509f21":"code","338a5654":"code","5dd52813":"code","fbf90e44":"code","5433a673":"code","cd07999b":"code","953c0daa":"code","59ebc30b":"code","5a0fdfd0":"code","1d49d08f":"code","f2a1a7cf":"code","fdf341db":"code","757a6394":"code","b6c50aa2":"code","b8662e06":"code","5ae7da05":"code","a18dc6bd":"code","e5810030":"code","ae4da063":"code","74673a2d":"code","6e403d2d":"code","f1ebd592":"code","5b8c5282":"code","997e53db":"code","64e4e0fe":"code","f6364bd8":"code","4a0e8680":"code","28119ce0":"markdown","33e93a2b":"markdown","9631b856":"markdown","aedfb50a":"markdown","b75a276e":"markdown","735470ba":"markdown","82d4a466":"markdown","35f68554":"markdown","a9b663f1":"markdown","cb93e0c9":"markdown","1819f59d":"markdown","87a1013f":"markdown"},"source":{"047242ce":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport os\n\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport catalyst\nfrom catalyst import dl, utils\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, accuracy_score\n\nimport gc\n\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","e21ae906":"data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ndata.drop('Id', axis=1, inplace=True)\n\ndata","4634cfb3":"test_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\ntest_ids = test_data['Id']\ntest_data.head()","4c7572a7":"len(data.columns)","85b4e00d":"data.dtypes","c0b35f7b":"data.describe()","f1f9e990":"for column in data.columns:\n    unique_num = data[column].nunique()\n    print('Column {} has {} unique values'.format(column, unique_num))","69c74505":"for column in data.columns:\n    nan_num = data[column].isnull().sum()\n    print('Column {} has {} NaN values'.format(column, nan_num))","54d3035c":"data.hist(figsize=(40,40))","60354f4c":"item_counts = data['Cover_Type'].value_counts(sort=False)\nitem_counts","68fbe162":"item_counts_frequencies = data['Cover_Type'].value_counts(sort=False, normalize=True)\ni = 1\nfor item in item_counts_frequencies:\n  print('{}: {:.8f}%'.format(i, item*100))\n  i += 1","fb0a283b":"cont_features = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points']","a793e78a":"data[cont_features] = data[cont_features].astype(np.int16)\ndata[data.columns[~data.columns.isin(cont_features)]] = data[data.columns[~data.columns.isin(cont_features)]].astype(np.int8)\ndata.dtypes","fb6b352c":"corr_matr = data[cont_features + ['Cover_Type']].corr()\ncorr_matr","f1879b29":"plt.figure(figsize=(10,10))\nsns.heatmap(corr_matr)","4181cf70":"data = data.drop(data.loc[data['Cover_Type'].isin([5])].index)","e521ccf5":"for i in tqdm(range(0, len(data['Cover_Type']))):\n    if i == 3403875:\n        continue\n    if data['Cover_Type'][i] < 5:\n        data['Cover_Type'][i] = data['Cover_Type'][i] - 1\n    else:\n        data['Cover_Type'][i] = data['Cover_Type'][i] - 2\ndata['Cover_Type']","e09c0740":"X = data.drop('Cover_Type', axis=1)\ny = data['Cover_Type']","dbf0485e":"class TPS_Dec_2021(Dataset):\n    \n    def __init__(self, X, y, dataset_type):\n        self.dataset_type = dataset_type\n        if self.dataset_type in ['train', 'valid', 'test']:\n            self.X = np.asarray(X)\n            print(self.X.shape)\n            self.y = np.asarray(y)\n            print(self.y.shape)\n        else: \n            self.X = np.asarray(X)\n        \n    def __len__(self):\n        return self.X.shape[0]\n    \n    def __getitem__(self, idx):\n        if self.dataset_type in ['train', 'valid', 'test']:\n            return torch.tensor(self.X[idx], dtype=torch.float).to(device), torch.tensor(self.y[idx], dtype=torch.long).to(device)\n        else:\n            return torch.tensor(self.X[idx], dtype=torch.float).to(device)","79db3891":"# model definition\nclass model_catalyst(nn.Module):\n    # define model elements\n    def __init__(self):\n        super(model_catalyst, self).__init__()\n        self.linear1 = nn.Linear(54, 108)\n        self.linear2 = nn.Linear(108, 108)\n        self.linear3 = nn.Linear(108, 108)\n        self.linear4 = nn.Linear(108, 108)\n        self.linear5 = nn.Linear(108, 108)\n        self.linear6 = nn.Linear(108, 108)\n        self.out = nn.Linear(108, 6)\n \n    # forward propagate input\n    def forward(self, X):\n        logits = self.linear1(X)\n        logits = self.linear2(logits)\n        logits = self.linear3(logits)\n        logits = self.linear4(logits)\n        logits = self.linear5(logits)\n        logits = self.linear6(logits)\n        logits = self.out(logits)\n        \n        return logits","ac509f21":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","338a5654":"X_train, X_buf, y_train, y_buf = train_test_split(X, y, test_size=0.3, random_state=42)\nX_valid, X_test, y_valid, y_test = train_test_split(X_buf, y_buf, test_size=0.5, random_state=42)","5dd52813":"train_dataset = TPS_Dec_2021(X_train, y_train, 'train')\nvalid_dataset = TPS_Dec_2021(X_valid, y_valid, 'valid')\ntest_dataset = TPS_Dec_2021(X_test, y_test, 'test')","fbf90e44":"trainloader = DataLoader(train_dataset, batch_size=512)\nvalidloader = DataLoader(valid_dataset, batch_size=512)\ntestloader = DataLoader(test_dataset, batch_size=1)\n\nloaders = {\n    \"train\": trainloader,\n    \"valid\": validloader,\n}","5433a673":"model = model_catalyst().to(device)\nmodel","cd07999b":"gc.collect()","953c0daa":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters())","59ebc30b":"runner = dl.SupervisedRunner(\n    input_key=\"features\", output_key=\"logits\", target_key=\"targets\", loss_key=\"loss\"\n)","5a0fdfd0":"runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    num_epochs=10,\n    callbacks=[\n        dl.CriterionCallback(\n            input_key=\"logits\", target_key=\"targets\", metric_key=\"loss\"\n        ),\n        dl.AccuracyCallback(input_key=\"logits\", target_key=\"targets\", num_classes=6, topk_args=[1,2,3,4,5,6])\n    ],\n    logdir=\".\/logs\",\n    valid_loader=\"valid\",\n    valid_metric=\"loss\",\n    minimize_valid_metric=True,\n    verbose=True,\n    load_best_on_end=True,\n    seed=42,\n)","1d49d08f":"preds = []\nwith torch.no_grad():\n    model.eval()\n    for data in tqdm(testloader):\n        features = data\n        outputs = model(features[0])\n        outputs = outputs.detach().cpu().numpy()\n        preds.append(outputs)","f2a1a7cf":"preds[:10]","fdf341db":"test_preds = []\nfor pred in preds:\n    test_preds.append(np.argmax(pred[0]))","757a6394":"test_preds[:10]","b6c50aa2":"balanced_accuracy_score(y_test, test_preds)","b8662e06":"accuracy_score(y_test, test_preds)","5ae7da05":"gc.collect()","a18dc6bd":"out_data = pd.DataFrame()\nout_data['Id'] = test_ids\ntest_data = test_data.drop('Id', axis=1)\nX_subm = test_data.to_numpy()\nX_subm","e5810030":"submission_dataset = TPS_Dec_2021(X_subm, None, 'submission')","ae4da063":"submissionloader = DataLoader(submission_dataset, batch_size=1)","74673a2d":"preds = []\nwith torch.no_grad():\n    model.eval()\n    for data in tqdm(submissionloader):\n        features = data\n        outputs = model(features)\n        outputs = outputs.detach().cpu().numpy()\n        preds.append(outputs)","6e403d2d":"preds[:10]","f1ebd592":"final_preds = []\nfor pred in preds:\n    if np.argmax(pred[0]) > 3:\n        final_preds.append(np.argmax(pred[0])+2)\n    else:\n        final_preds.append(np.argmax(pred[0])+1)","5b8c5282":"final_preds[:10]","997e53db":"out_data['Cover_Type'] = final_preds","64e4e0fe":"out_data","f6364bd8":"out_data.to_csv('torch_baseline.csv',index=None)","4a0e8680":"gc.collect()","28119ce0":"# LB is 0.90618","33e93a2b":"### We need to move all targets to the left because, according to Docs, CrossEntropyLoss gets class indices from 0 to C-1. Our class indices after deletion of row with target '5' are 1, 2, 3, 4, 6, 7 which is not correct input for loss function","9631b856":"# EDA","aedfb50a":"### Submission","b75a276e":"### Inference","735470ba":"### No NaN values - great!","82d4a466":"# Let's make a simple PyTorch model with training in Catalyst framework","35f68554":"# Load data","a9b663f1":"### Train our model","cb93e0c9":"### All features have low correlation.","1819f59d":"### We can make two conclusions\n1.   Two features (Wilderness_Area and Soil_Type are one-hot encoded)\n2.   Cover_Type is very imbalanced. Class '6' is at noise level, class '5' has only one value! Let's delete class '5'","87a1013f":"### Split dataset to train, valid and test"}}