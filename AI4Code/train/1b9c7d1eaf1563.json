{"cell_type":{"f0e0eae2":"code","4dcd4c4c":"code","77035c6d":"code","eddeae00":"code","e471a5c6":"code","f87471b9":"code","84bcda24":"code","c6e5b082":"code","a49e6abf":"code","451f75df":"code","bb35544b":"code","9d0481ee":"code","a1f3bb73":"code","21c42500":"code","0fe43d28":"code","59ac93be":"code","098da8a0":"code","ca7ac9e1":"code","dffc927b":"code","b2c13bb9":"code","4e524790":"code","1e088440":"code","16e0ff96":"code","28eee618":"code","07c96254":"code","528ec4c5":"code","f95d866d":"code","a2b09e79":"code","f25a6c6e":"code","4db2fcd9":"markdown","1a9e9218":"markdown","0a1c5fce":"markdown","3949f2f4":"markdown","b870eceb":"markdown","3c881ad4":"markdown","3030b664":"markdown","7ae514a1":"markdown","2809cb14":"markdown","0f5418f4":"markdown","63dd50c8":"markdown","39d73ccd":"markdown","882fb974":"markdown","be0241f1":"markdown","d5f926b2":"markdown"},"source":{"f0e0eae2":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","4dcd4c4c":"data=pd.read_csv(\"\/kaggle\/input\/credit-card-customers\/BankChurners.csv\")\ndata.head()","77035c6d":"data=data.iloc[:,:-2]#deleting last two rows as mentioned in database\ndata.head()","eddeae00":"data[data.select_dtypes(['object']).columns] = data.select_dtypes(['object']).apply(lambda x: x.astype('category'))","e471a5c6":"data.info()","f87471b9":"data.Attrition_Flag.value_counts()","84bcda24":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.countplot(x='Attrition_Flag', data=data, hue='Gender')\nplt.title(\"Distribution of Gender Among Attrited and Existing Customers\")\nplt.show()","c6e5b082":"sns.countplot(x='Attrition_Flag', data=data, hue='Marital_Status')\nplt.title(\"Distribution of Marital Status Among Attrited and Existing Customers\")\nplt.show()","a49e6abf":"sns.countplot(x='Attrition_Flag', data=data, hue='Income_Category')\nplt.title(\"Distribution of Income Category Among Attrited and Existing Customers\")\nplt.show()","451f75df":"sns.countplot(x='Attrition_Flag', data=data, hue='Card_Category')\nplt.title(\"Distribution of Card_Category Among Attrited and Existing Customers\")\nplt.show()","bb35544b":"from sklearn.model_selection import train_test_split\nX=data.drop(\"Attrition_Flag\",axis=1)\ny=data.Attrition_Flag\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","9d0481ee":"X_train.shape","a1f3bb73":"X_train_num=X_train[X_train.select_dtypes(['int64',\"float64\"]).columns]\nX_test_num=X_test[X_test.select_dtypes(['int64',\"float64\"]).columns]","21c42500":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train_num)\nX_train_scaled=pd.DataFrame(scaler.transform(X_train_num),columns=X_train_num.columns,index=X_train_num.index)\nX_test_scaled=pd.DataFrame(scaler.transform(X_test_num),columns=X_test_num.columns,index=X_test_num.index)","0fe43d28":"X_train_dum=pd.get_dummies(X_train[X_train.select_dtypes(['category']).columns],drop_first=True)\nX_test_dum=pd.get_dummies(X_test[X_test.select_dtypes(['category']).columns],drop_first=True)","59ac93be":"X_train_pd=pd.concat([X_train_scaled, X_train_dum], axis=1)\nX_test_pd=pd.concat([X_test_scaled, X_test_dum], axis=1)","098da8a0":"X_train_pd.drop(\"CLIENTNUM\",axis=1,inplace=True)\nX_test_pd.drop(\"CLIENTNUM\",axis=1,inplace=True)","ca7ac9e1":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\nlogmodel.fit(X_train_pd,y_train)\nlrpredictions = logmodel.predict(X_test_pd)","dffc927b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,lrpredictions))","b2c13bb9":"from sklearn.naive_bayes import MultinomialNB\n\nnbmodel = MultinomialNB().fit(X_train_pd, y_train)\nnbpredictions=nbmodel.predict(X_test_pd)\n\nprint(classification_report(y_test,nbpredictions))\n\n","4e524790":"from sklearn.neighbors import KNeighborsClassifier\nknnmodel = KNeighborsClassifier(n_neighbors=3)\nknnmodel.fit(X_train_pd,y_train)\nknnpredictions=knnmodel.predict(X_test_pd)\nprint(classification_report(y_test,knnpredictions))","1e088440":"from sklearn.tree import DecisionTreeClassifier\ndtmodel = DecisionTreeClassifier(random_state=0)\ndtmodel.fit(X_train_pd,y_train)\ndtpredictions=dtmodel.predict(X_test_pd)\nprint(classification_report(y_test,dtpredictions))","16e0ff96":"from sklearn.ensemble import RandomForestClassifier\nrfmodel = RandomForestClassifier(random_state=0,max_depth=100,n_estimators=50)\nrfmodel.fit(X_train_pd,y_train)\nrfpredictions=rfmodel.predict(X_test_pd)\nprint(classification_report(y_test,rfpredictions))","28eee618":"from sklearn.ensemble import GradientBoostingClassifier\ngbmodel = GradientBoostingClassifier(random_state=0)\ngbmodel.fit(X_train_pd,y_train)\ngbpredictions=gbmodel.predict(X_test_pd)\nprint(classification_report(y_test,gbpredictions))","07c96254":"import xgboost as xgb\nxgbmodel = xgb.XGBClassifier(random_state=0)\nxgbmodel.fit(X_train_pd,y_train)\nxgbpredictions=xgbmodel.predict(X_test_pd)\nprint(classification_report(y_test,xgbpredictions))","528ec4c5":"#dummyfying the target variable since Deep Learning doesn't accept categorical variables\ny_train_dum=pd.get_dummies(y_train,drop_first=True)\ny_test_dum=pd.get_dummies(y_test,drop_first=True)\n\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42)#performing SMOTE to resolve class imbalances\nX_train_oversampled, y_train_oversampled = sm.fit_sample(X_train_pd, y_train_dum)\nX_train_dumsmote = pd.DataFrame(X_train_oversampled, columns=X_train_pd.columns)","f95d866d":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras import regularizers\nimport keras\n\nannmodel = Sequential()\nannmodel.add(Dense(64, input_dim=32, activation='relu'))\nannmodel.add(Dense(32,kernel_regularizer=regularizers.l2(0.01), activation='relu'))\nannmodel.add(Dropout(0.1))\nannmodel.add(Dense(1, activation='sigmoid'))\n\n# compile the keras model\nannmodel.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n# fit the keras model on the dataset\nannmodel.fit(X_train_dumsmote, y_train_oversampled, epochs=150, batch_size=32,verbose=0)","a2b09e79":"annpredictions=annmodel.predict(X_test_pd)\nannpredictions=[1 if x>0.7 else 0 for x in annpredictions]\nprint(classification_report(y_test_dum,annpredictions))","f25a6c6e":"feature_important = xgbmodel.get_booster().get_score(importance_type='weight')\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\ndata.plot(kind='barh',figsize=(5,10))","4db2fcd9":"# Model Building","1a9e9218":"From the above Graph we notice that the most important feature that decides between the churning customer and the existing customer is the Total_Trans_Amt. It makes logical sense since anyone who is planning on churning will try out another bank's services before closing down the current account. ","0a1c5fce":"# Exploratory Data Analysis","3949f2f4":"# **2. Naive Bayes**","b870eceb":"# **5. Random Forest Classifier**","3c881ad4":"# **7. Extreme Gradient Boosting Classifier(XGBoost)**","3030b664":"# Feature Importances\nNow Let's look at which features contributed most to the classification algorithm","7ae514a1":"# **6. Gradient Boosting Classifier**","2809cb14":"# **3. KNN Classifier**","0f5418f4":"# **1. Logistic Regression**","63dd50c8":"# 8. ANN","39d73ccd":"In this notebook we will try to find the most important reasons that a customer would churn and also devise multiple models that would predict churning customers","882fb974":"**Preprocessing**\n <br>Here the most important metric is recall since we want most of if not all the customers who want to churn so that the bank manager can propose plans to minimise the churning rate. ","be0241f1":"# **4. Decision Tree Classifier**","d5f926b2":"Here we can see that even after performing various pre processing and adjusting various hyperparameters, the maximum recall and accuracy we are able to achieve is only 86% and 90% respectively. After a lot of adjustments, I have come to the conclusion that there isn't enough data to acheive better results than the XGBoost Algorithm here for deep learning to perform better. Hence, there seems to be no point going any further. If anyone has any better algorithms, please write it down in the comments and I'll try that out. "}}