{"cell_type":{"790d5aaf":"code","2ee4bb6c":"code","c78ed183":"code","9323aa99":"code","24931592":"code","030634fe":"code","7b493208":"code","bf3ef0c6":"code","e56b28d1":"code","376fbfd7":"code","56b0e8e3":"code","63023493":"code","f82091a7":"code","9bdbc9bc":"code","2c84ffcb":"code","6d8cebf5":"code","2bce9ce1":"code","6c3f9f09":"code","52922a9c":"code","1e38e86e":"code","1d38ff05":"code","00a5d9da":"code","c004c950":"code","2c811fef":"code","0e20974b":"code","d76d9ee5":"code","04894779":"markdown","e599e3cc":"markdown","9fb5e51c":"markdown","cea44ebc":"markdown","fdf20c7d":"markdown","5fea46a1":"markdown","d42ece17":"markdown","0998d91f":"markdown","05d6df87":"markdown","dd4a3db6":"markdown","f3a7d1dd":"markdown","58f50ff7":"markdown","b27b7685":"markdown","67cb0544":"markdown","94fbb13c":"markdown","5d9c62fa":"markdown","b5c1e742":"markdown","f8eda399":"markdown","3aea35fe":"markdown","b526cf26":"markdown","634a303f":"markdown","8c06d924":"markdown","beb7400f":"markdown","40746483":"markdown","9d23f147":"markdown","1d110f2a":"markdown","4c691834":"markdown"},"source":{"790d5aaf":"!pip install -q timm albumentations wandb\n!pip install pandas-profiling[notebook] -q\n!pip install autoviz -q","2ee4bb6c":"import gc\nimport os\nimport glob\nimport sys\nimport cv2\nimport imageio\nimport joblib\nimport math\nimport random\nimport wandb\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\n\nfrom scipy.stats import kstest\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom statsmodels.graphics.gofplots import qqplot\n\nplt.rcParams.update({'font.size': 18})\nplt.style.use('fivethirtyeight')\n\nimport seaborn as sns\nimport matplotlib\n\nfrom termcolor import colored\n\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import pearsonr\n\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA \nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE \nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport timm\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\nfrom albumentations.pytorch import ToTensorV2\n\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Wandb Login\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning import Trainer\n\n\n#Importing the class function\nfrom autoviz.AutoViz_Class import AutoViz_Class\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nsns.color_palette('pastel')\n\ntqdm.pandas()","c78ed183":"user_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"wandb-api\")\nwandb.login(key=secret_value_0)\nrun = wandb.init(project=\"PetFinder\", entity=\"gusevski\")\n\n\nwandb_logger = WandbLogger(project=\"PetFinder\")","9323aa99":"class CFG:\n    TRAIN_CSV_PATH = '..\/input\/petfinder-pawpularity-score\/train.csv'\n    TEST_CSV_PATH = '..\/input\/petfinder-pawpularity-score\/test.csv'\n    sample_submission = '..\/input\/petfinder-pawpularity-score\/sample_submission.csv'\n    SEED=2021","24931592":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nset_seed(CFG.SEED)","030634fe":"# Efficient Data Types\ndtype = {\n    'Id': 'string',\n    'Subject Focus': np.uint8, 'Eyes': np.uint8, 'Face': np.uint8, 'Near': np.uint8,\n    'Action': np.uint8, 'Accessory': np.uint8, 'Group': np.uint8, 'Collage': np.uint8,\n    'Human': np.uint8, 'Occlusion': np.uint8, 'Info': np.uint8, 'Blur': np.uint8,\n    'Pawpularity': np.uint8,\n}\n\ntrain = pd.read_csv(CFG.TRAIN_CSV_PATH, dtype=dtype)\ntest = pd.read_csv(CFG.TEST_CSV_PATH, dtype=dtype)","7b493208":"train.head()","bf3ef0c6":"test.head()","e56b28d1":"train.info()","376fbfd7":"test.info()","56b0e8e3":"profile_train = ProfileReport(train, title=\"Train Profiling Report\")\nprofile_train.to_file(\"Train Profiling Report.html\")\nprofile_train","63023493":"profile_test = ProfileReport(test, title=\"Test Profiling Report\")\nprofile_test.to_file(\"Test Profiling Report.html\")\n# profile for 8 row is non informative\n#profile_test","f82091a7":"AV = AutoViz_Class()\n#Automatically produce dataset\nAV.AutoViz(\"\",dfte = train )","9bdbc9bc":"print(f\"Training Dataset Shape: {colored(train.shape, 'yellow')}\")\nprint(f\"Test Dataset Shape: {colored(test.shape, 'yellow')}\")","2c84ffcb":"def get_image_file_path(df_type,image_id):\n    return f'\/kaggle\/input\/petfinder-pawpularity-score\/{df_type}\/{image_id}.jpg'\n\ntrain['file_path'] = train['Id'].apply(lambda x: get_image_file_path('train',x))\ntest['file_path'] = test['Id'].apply(lambda x: get_image_file_path('test',x))","6d8cebf5":"# Kolmogorov-Smirnov test with Scipy\nstat, p = kstest(train['Pawpularity'],'norm')\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n\nalpha = 0.05\nif p > alpha:\n    print(f'Sample looks Gaussian (fail to reject H0 at {int(alpha*100)}% test level)')\nelse:\n    print(f'Sample does not look Gaussian (reject H0 at {int(alpha*100)}% test level)')","2bce9ce1":"plt.figure(figsize=(10, 10))\nfor i, img_path in enumerate(train[train.Eyes == 0].file_path.head(4)):\n    ax = plt.subplot(2, 2, i+1)\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    plt.axis(\"off\")\n    plt.imshow(image)","6c3f9f09":"plt.figure(figsize=(10, 10))\nfor i, img_path in enumerate(train[(train.Eyes == 0) & (train.Blur == 0) ].file_path.head(4)):\n    ax = plt.subplot(2, 2, i+1)\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    \n    plt.axis(\"off\")\n    plt.imshow(image)","52922a9c":"feature_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\ny = train['Pawpularity']\ntrain_features = train[feature_columns]\ntest_features = test[feature_columns]\n\n\nx = train_features.to_numpy(\"float64\")\ny = y.to_numpy(\"float64\")\ntest = test_features.to_numpy(\"float64\")","1e38e86e":"# Melt function\n#Unpivot a DataFrame from wide to long format, optionally leaving identifiers set.\n#This function is useful to massage a DataFrame into a format where one or more columns are identifier variables (id_vars),\n#while all other columns, considered measured variables (value_vars), are \u201cunpivoted\u201d to the row axis, \n#leaving just two non-identifier columns, \u2018variable\u2019 and \u2018value\u2019.\ndf_train = train_features.melt(value_vars=feature_columns)\nplt.figure(figsize = (15, 7))\nsns.countplot(data=df_train, y=\"variable\", hue=\"value\" , palette =sns.color_palette([\"#E4916C\", \"#BCE6EF\"]))\nplt.show()","1d38ff05":"tsne = TSNE(n_components=2, random_state=CFG.SEED)\nt_train_2 = tsne.fit_transform(train_features)\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    x=t_train_2[:, 0], y=t_train_2[:, 1],\n    hue=train['Pawpularity'],\n    alpha=0.3\n)\nplt.show()","00a5d9da":"tsne = TSNE(n_components=3, random_state=CFG.SEED)\nt_train_3 = tsne.fit_transform(train_features)\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    x=t_train_3[:, 0], y=t_train_3[:, 1],\n    hue=train['Pawpularity'],\n    alpha=0.3\n)\nplt.show()","c004c950":"t_train_2 = pd.DataFrame(t_train_2, index=train.Id, columns=[\"c\"+str(c) for c in range(2)])\nkm = KMeans(n_clusters=2, random_state=CFG.SEED).fit(t_train_2)\ny_km = km.predict(t_train_2)\nt_train_2[\"cluster\"] = y_km\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    data=t_train_2,\n    x=\"c0\",\n    y=\"c1\",\n    hue=\"cluster\",\n    alpha=0.3,\n    palette=sns.color_palette([\"#E4916C\", \"#BCE6EF\"])\n)\nplt.show()","2c811fef":"n_comp = 2\npca = PCA(n_components=n_comp, svd_solver='full', random_state=CFG.SEED)\nX_pca = pca.fit_transform(train_features)\nplt.figure(figsize=(15, 7))\nsns.scatterplot(\n    data=X_pca,\n    x=X_pca[:, 0], y=X_pca[:, 1],\n    hue=train['Pawpularity'],\n    alpha=0.3\n    )\nplt.show()","0e20974b":"columns = train.columns.tolist()\ndata_at = wandb.Table(columns=columns) # W&B Code 1\n\nfor i in tqdm(range(len(train))):# W&B Code 2\n    row = train.loc[i]\n    img_id = row.Id\n    data_at.add_data(img_id, wandb.Image(f'\/kaggle\/input\/petfinder-pawpularity-score\/train\/{img_id}.jpg'),*tuple(row.values[2:])) # W&B Code 3\n\nwandb.log({'Raw Petfinder data': data_at}) # W&B Code 4\nwandb.finish() # W&B Code 5","d76d9ee5":"# This is just to display the W&B run page in this interactive session.\nfrom IPython import display\n\n# we create an IFrame and set the width and height\niF = display.IFrame(run.url, width=1080, height=720)\niF","04894779":"<a id=\"tabular-exploration\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>EDA<\/center><\/h2>","e599e3cc":"We notice the deviation at this QQPlot which seems to indicate a non-Gaussian distribution. We will check with the Kolmogorov-Smirnov test (Shapiro-Wilks is not suitable for a dataset greater than 5000 items).","9fb5e51c":"## **<span style=\"color:lightgreen;\">PCA<\/span>**","cea44ebc":"## **<span style=\"color:lightgreen;\">Evaluation Criteria<\/span>**\n\nSubmissions are scored on the **Root mean squared error**. ","fdf20c7d":"<a id=\"distribution-plots\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> t-SNE | Clusters | Plots<\/center><\/h2>","5fea46a1":"<a id=\"competition-overview\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Competition Overview<\/center><\/h2>","d42ece17":"## **<span style=\"color:lightgreen;\">Clustering<\/span>**","0998d91f":"<a id=\"tableswb\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center> Tables in W&B<\/center><\/h2>","05d6df87":"> ### **<span style=\"color:lightgreen;\">Goal of Competition<\/span>**\n> In this competition, your task is to predict engagement with a pet's profile based on the photograph for that profile. You are also provided with hand-labelled metadata for each photo. The dataset for this competition therefore comprises both images and tabular data.\n> \n> ### **<span style=\"color:lightgreen;\">How Pawpularity Score Is Derived<\/span>**\n> The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.  \n> Duplicate clicks, crawler bot accesses and sponsored profiles are excluded from the analysis.\n>\n>---","dd4a3db6":"## **<span style=\"color:lightgreen;\">Description<\/span>**\n\nIn this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos.   \n  \nYou'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.\n  \nIf successful, your solution will be adapted into AI tools that will guide shelters and rescuers around the world to improve the appeal of their pet profiles, automatically enhancing photo quality and recommending composition improvements.   \n  \nAs a result, stray dogs and cats can find their \"furever\" homes much faster. With a little assistance from the Kaggle community, many precious lives could be saved and more happy families created.\n\n---","f3a7d1dd":"It only requires 5 lines of extra code to get the power of W&B Tables.\n\n\n1. Create a wandb.Table object. Imagine this to be an empty Pandas Dataframe.\n1. Iterate through each row of the train.csv file and add_data to the wandb.Table object. Imagine this to be appending new rows to your Dataframe.\n1. Log the W&B Tables using wandb.log API. You will use this API to log almost anything to W&B.\n1. In a Juypter like interactive session, you need to call wandb.finish to close the initialized W&B run.\n\n","58f50ff7":"## **<span style=\"color:lightgreen;\">KMeans<\/span>**","b27b7685":"<a id=\"libraries\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Libraries<\/center><\/h2>","67cb0544":"<a id=\"global-config\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>CFG<\/center><\/h2>","94fbb13c":"> ### **<span style=\"color:lightgreen;\">Training Data<\/span>**\n> - train\/ - Folder containing training set photos of the form {id}.jpg, where {id} is a unique Pet Profile ID.\n> - train.csv - Metadata (described below) for each photo in the training set as well as the target, the photo's Pawpularity score. The Id column gives the photo's unique Pet Profile ID corresponding the photo's file name.\n> \n> ### **<span style=\"color:lightgreen;\">Example Test Data<\/span>**\n> In addition to the training data, we include some randomly generated example test data to help you author submission code. When your submitted notebook is scored, this example data will be replaced by the actual test data (including the sample submission).\n> \n> - test\/ - Folder containing randomly generated images in a format similar to the training set photos. The actual test data comprises about 6800 pet photos similar to the training set photos.\n> - test.csv - Randomly generated metadata similar to the training set metadata.\n> - sample_submission.csv - A sample submission file in the correct format.\n>\n>---\n\n> ### **<span style=\"color:lightgreen;\">Photo Metadata<\/span>**\n> The train.csv and test.csv files contain metadata for photos in the training set and test set, respectively. Each pet photo is labeled with the value of 1 (Yes) or 0 (No) for each of the following features:\n> \n> - Focus - Pet stands out against uncluttered background, not too close \/ far.\n> - Eyes - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n> - Face - Decently clear face, facing front or near-front.\n> Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n> - Action - Pet in the middle of an action (e.g., jumping).\n> - Accessory - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n> - Group - More than 1 pet in the photo.\n> - Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n> - Human - Human in the photo.\n> - Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n> - Info - Custom-added text or labels (i.e. pet name, description).\n> - Blur - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n>\n>---","5d9c62fa":"<a id=\"eof\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>work in progress<\/center><\/h2>","b5c1e742":"<a id=\"weights-and-biases\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Weights and Biases<\/center><\/h2>","f8eda399":"<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Contents<\/center><\/h2>","3aea35fe":"## **<span style=\"color:lightgreen;\">Understanding the Structure of the Dataset<\/span>**","b526cf26":"## **<span style=\"color:lightgreen;\">Visualize Dataset Interactively using W&B Tables<\/span>**","634a303f":"**Weights & Biases** is the machine learning platform for developers to build better models faster. \n\nYou can use W&B's lightweight, interoperable tools to \n- quickly track experiments, \n- version and iterate on datasets, \n- evaluate model performance, \n- reproduce models, \n- visualize results and spot regressions, \n- and share findings with colleagues. \n\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.","8c06d924":"1. [Competition Overview](#competition-overview)  \n2. [Libraries](#libraries)  \n3. [Weights and Biases](#weights-and-biases)   \n4. [CFG](#global-config)\n5. [Load Datasets](#load-datasets)  \n6. [EDA](#tabular-exploration) \n7. [Conclusion about meta data](#conclusion)\n8. [t-SNE | Clusters | Plots](#distribution-plots)\n9. [Tables in W&B](#tableswb)\n8. [Work in progress](#eof )","beb7400f":"<a id=\"load-datasets\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Load Datasets<\/center><\/h2>","40746483":"> ### **<span style=\"color:lightgreen;\">Top Visualization for your Data<\/span>**\n> AutoViz is an open-source visualization package under the AutoViML package library designed to automate many data scientists\u2019 works. Many of the projects were quick and straightforward but undoubtedly helpful, including AutoViz.\n> \n> AutoViz is a one-liner code visualization package that would automatically produce data visualization. Let\u2019s try this package to show why AutoViz could help your work. First, we need to install AutoViz.\n","9d23f147":"<a id=\"conclusion\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:lightgreen; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Small conclusion about metadata<\/center><\/h2>\n    \n* Train\/Test meta data have no missing values\n* The pawpularity score is centered around 40 and has a peak on 0 and 100.\n* The test clearly indicates that the distribution does not follow a Gaussian law. It will therefore be important to normalize the data according to the modeling chosen.\n* We have pictures that meta table Eyes == 0 & Blur == 0, but actual picture has eyes (Look \ud83d\udc47 images)\n* Correlation between: \n    1. \"Eyes\" and \"Face\";\n    1. \"Occlusion\" and \"Human\";\n    1. \"Eyes\" and \"Blur\"\n    1. \"Group\" and \"Near\"","1d110f2a":"<center><img src = \"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\"\/><\/center>  ","4c691834":"<h1><center>Pawpular : PetFinder + W&B<\/center><\/h1>\n                                                      \n<center><img src = \"https:\/\/scontent-hel3-1.xx.fbcdn.net\/v\/t1.6435-9\/95999292_3051385531573716_8920255664832380928_n.png?_nc_cat=111&ccb=1-5&_nc_sid=6e5ad9&_nc_ohc=64hmGZb8bXAAX_AoecS&_nc_ht=scontent-hel3-1.xx&oh=6964e3c4b5d29fc4fea6f3fdd9fd1f67&oe=61A85FDF\" width = \"750\" height = \"500\"\/><\/center>                                                                                               "}}