{"cell_type":{"728b50b9":"code","75ff7839":"code","94465b38":"code","8f2a1e31":"code","d5ccae4f":"code","bdffbbaf":"code","7ac151eb":"code","bad5a0d4":"code","eda68398":"code","4af761ce":"code","377fa820":"code","b9439cf3":"code","95dd1a16":"code","45affb22":"code","19aa689a":"code","74f1fb37":"code","d4463fa0":"markdown","84b24783":"markdown","ab24c44e":"markdown","c0be891f":"markdown","2ed7acca":"markdown","a3bd2cf8":"markdown","95a614f3":"markdown","b3dc685c":"markdown","82e24ccc":"markdown","dd4f8629":"markdown","003408b1":"markdown"},"source":{"728b50b9":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date\nimport scipy.stats\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.pipeline import make_pipeline\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, InputLayer, Add\n","75ff7839":"# Plot training history\ndef plot_history(history, *, n_epochs=None, plot_lr=False, plot_acc=True, title=None, bottom=None, top=None):\n    \"\"\"Plot (the last n_epochs epochs of) the training history\"\"\"\n    plt.figure(figsize=(15, 6))\n    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n    \n    # Plot training and validation losses\n    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n    try:\n        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n        best_epoch = np.argmin(np.array(history['val_loss']))\n        best_val_loss = history['val_loss'][best_epoch]\n        if best_epoch >= from_epoch:\n            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n        if best_epoch > 0:\n            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n            almost_val_loss = history['val_loss'][almost_epoch]\n            if almost_epoch >= from_epoch:\n                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n    except KeyError:\n        pass\n    if bottom is not None: plt.ylim(bottom=bottom)\n    if top is not None: plt.ylim(top=top)\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(loc='lower left')\n    if title is not None: plt.title(title)\n        \n    # Plot learning rate\n    if plot_lr and 'lr' in history:\n        ax2 = plt.gca().twinx()\n        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n        ax2.set_ylabel('Learning rate')\n        ax2.legend(loc='upper right')\n        \n    plt.show()\n    ","94465b38":"# Read the data\noriginal_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\noriginal_train_df.head(2)\n\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\n# qgdp_df = pd.read_csv('..\/input\/tsp-jan2022-gdp-per-quarter\/GDP_Quarterly.csv')\n# qgdp_df['GDP'] = qgdp_df['GDP'].apply(lambda s: int(s.replace(',', '')))\n# qgdp_df.set_index('Base_Key', inplace=True)\n# qgdp_df","8f2a1e31":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return tf.abs(y_true - y_pred) \/ (y_true + tf.abs(y_pred)) * 200\n","d5ccae4f":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        \"\"\"GDP from yearly GDP dataset\"\"\"\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n#     def get_gdp(row):\n#         \"\"\"GDP from quarterly GDP dataset\"\"\"\n#         key = f\"{row.country}_{row.date.year}_Q{(row.date.month+2)\/\/3}\"\n#         return qgdp_df.loc[key, 'GDP']\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}),\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n                                      (df.country == 'Norway')\n                                      for d in list(range(18, 28))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n                                      (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) &\n                                      (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) +\n                                      list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = list(test_df.columns)\nprint(list(features))\n","bdffbbaf":"#%%time\nEPOCHS = 300\nEPOCHS_COSINEDECAY = 120\nVERBOSE = 0 # set to 0 for less output, or to 2 for more output\nRUNS = 5 # set to 1 for quick experiments\nDIAGRAMS = True\nUSE_PLATEAU = True\nINFERENCE = False\n\n# We split the features into subsets so that we can apply different\n# regularization schemes for the subsets\nwd_features = [f for f in features if f.startswith('wd')]\nother_features = [f for f in features if f not in wd_features]\n\n# def tpsjan_model():\n#     \"\"\"Linear model with flexible regularization\n    \n#     The model is to be used with a log-transformed target.\n#     \"\"\"\n#     wd = Input(shape=(len(wd_features), ))\n#     other = Input(shape=(len(other_features), ))\n#     wd_contribution = Dense(1, kernel_regularizer=tf.keras.regularizers.l2(1e-7),\n#                             use_bias=False)(wd)\n#     other_contribution = Dense(1, kernel_regularizer=tf.keras.regularizers.l2(1e-7),\n#                                use_bias=True,\n#                                bias_initializer=tf.keras.initializers.Constant(value=5.7))(other)\n#     output = Add()([wd_contribution, other_contribution])\n#     model = Model([wd, other], output)\n#     return model\n\n\ndef tpsjan_model_2():\n    \"\"\"Linear model\n    \n    The model is to be used with a log-transformed target.\n    \"\"\"\n    other = Input(shape=(len(features), ))\n    output = Dense(1, #kernel_regularizer=tf.keras.regularizers.l2(1e-6),\n                   use_bias=True,\n                   bias_initializer=tf.keras.initializers.Constant(value=5.74))(other)\n    model = Model(other, output)\n    return model\n\n\ndef fit_model(X_tr, X_va=None):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data (select columns and scale)\n    preproc = make_pipeline(MinMaxScaler(), StandardScaler(with_std=False))\n    X_tr_f = pd.DataFrame(preproc.fit_transform(X_tr[features]), columns=features, index=X_tr.index)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = pd.DataFrame(preproc.transform(X_va[features]), columns=features, index=X_va.index)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n        validation_data = ([X_va_f[features]], np.log(y_va))\n    else:\n        validation_data = None\n\n    # Define the learning rate schedule and EarlyStopping\n    if USE_PLATEAU and X_va is not None:\n        epochs = EPOCHS\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, \n                               patience=4, verbose=VERBOSE) # 4\n        es = EarlyStopping(monitor=\"val_loss\",\n                           patience=25, \n                           verbose=1,\n                           mode=\"min\", \n                           restore_best_weights=True)\n        callbacks = [lr, es, tf.keras.callbacks.TerminateOnNaN()]\n\n    else:\n        epochs = EPOCHS_COSINEDECAY\n        lr_start=0.02\n        lr_end=0.00001\n        def cosine_decay(epoch):\n            if epochs > 1:\n                w = (1 + math.cos(epoch \/ (epochs-1) * math.pi)) \/ 2\n            else:\n                w = 1\n            return w * lr_start + (1 - w) * lr_end\n\n        lr = LearningRateScheduler(cosine_decay, verbose=0)\n        callbacks = [lr, tf.keras.callbacks.TerminateOnNaN()]\n        \n    # Construct and compile the model\n    model = tpsjan_model_2()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n    #model.compile(optimizer=tf.keras.optimizers.SGD(), loss='mse')\n\n    # Train the model\n    history = model.fit([X_tr_f[features]], np.log(y_tr), \n                        validation_data=validation_data, \n                        epochs=epochs,\n                        verbose=VERBOSE,\n                        batch_size=512,\n                        shuffle=True,\n                        callbacks=callbacks)\n\n    history_list.append(history.history)\n    callbacks, es, lr, history = None, None, None, None\n    #print(f\"Loss:            {history_list[-1]['loss'][-1]:.6f}\")\n    #print(f\"Bias:  {model.get_layer(index=-1).get_weights()[1]}\")\n    \n    if X_va is not None:\n        # Inference for validation\n        y_va_pred = np.exp(model.predict([X_va_f[features]]))\n        oof_list[run][val_idx] = y_va_pred\n        \n        # Evaluation: Execution time and SMAPE\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f} validated on {X_va.iloc[0].date.year}\")\n        score_list.append(smape)\n        \n        if DIAGRAMS and fold == 0 and run == 0:\n            # Plot training history\n            plot_history(history_list[-1], title=f\"Validation SMAPE = {smape:.5f}\",\n                         plot_lr=True, n_epochs=110)\n\n            # Plot y_true vs. y_pred\n            plt.figure(figsize=(10, 10))\n            plt.scatter(y_va, y_va_pred, s=1, color='r')\n            #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n            plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n            plt.gca().set_aspect('equal')\n            plt.xlabel('y_true')\n            plt.ylabel('y_pred')\n            plt.title('OOF Predictions')\n            plt.show()\n\n    return preproc, model\n\n\n# Make the results reproducible\nnp.random.seed(2022)\n\ntotal_start_time = datetime.now()\nhistory_list, score_list, test_pred_list = [], [], []\noof_list = [np.full((len(train_df), 1), -1.0, dtype='float32') for run in range(RUNS)]\nfor run in range(RUNS):\n    preproc, model = None, None\n    kf = GroupKFold(n_splits=4)\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_df, groups=train_df.date.dt.year)):\n        X_tr = train_df.iloc[train_idx]\n        X_va = train_df.iloc[val_idx]\n        print(f\"Fold {run}.{fold}\")\n        preproc, model = fit_model(X_tr, X_va)\n        if INFERENCE:\n            test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n            test_pred_list.append(np.exp(model.predict([test_df_f[wd_features], test_df_f[other_features]])))\n\n\nprint(f\"Average SMAPE: {sum(score_list) \/ len(score_list):.5f}\") # Average over all runs and folds\nwith open('oof.pickle', 'wb') as handle: pickle.dump(oof_list, handle) # for further analysis\n    \nif RUNS > 1:\n    y_va = train_df.num_sold\n    print(f\"Ensemble SMAPE: {np.mean(smape_loss(y_va, sum(oof_list).ravel() \/ len(oof_list))):.5f}\")\nprint(f\"Total time: {str(datetime.now() - total_start_time)[:-7]}\")\n","7ac151eb":"w = pd.Series(model.get_layer(index=-1).get_weights()[0].ravel(), index=features)\nws = w * preproc.named_steps['minmaxscaler'].scale_\n\ndef plot_feature_weights_numbered(prefix):\n    prefix_features = [f for f in features if f.startswith(prefix)]\n    plt.figure(figsize=(12, 2))\n    plt.bar([int(f[len(prefix):]) for f in prefix_features], ws[prefix_features])\n    plt.title(f'Feature weights for {prefix}')\n    plt.ylabel('weight')\n    plt.xlabel('day')\n    plt.show()\n    \nplot_feature_weights_numbered('easter')\n","bad5a0d4":"# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df_f = pd.DataFrame(preproc.transform(demo_df[features]), columns=features, index=demo_df.index)\n    demo_df['num_sold'] = np.exp(model.predict([demo_df_f[features]]))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\nplot_five_years_combination(engineer)\n\n","eda68398":"# Retrain the network on the full training data several times\nRETRAIN_RUNS = 33\nif RETRAIN_RUNS > 0:\n    total_start_time = datetime.now()\n    test_pred_list = []\n    for run in range(RETRAIN_RUNS):\n        preproc, model = None, None\n        print(f\"Retraining {run}\")\n        preproc, model = fit_model(train_df)\n        print(f\"Training loss:            {history_list[-1]['loss'][-1]:.6f}\")\n        test_df_f = pd.DataFrame(preproc.transform(test_df[features]), columns=features, index=test_df.index)\n        test_pred_list.append(np.exp(model.predict([test_df_f[features]])))\n    print(f\"Total time: {str(datetime.now() - total_start_time)[:-7]}\")\n","4af761ce":"# Ensemble the test predictions\nsub = None\nif len(test_pred_list) > 0:\n    # Create the submission file\n    print(f\"Ensembling {len(test_pred_list)} predictions...\")\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\n    sub.to_csv('submission_keras.csv', index=False)\n    \n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201), density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201), density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n\nsub","377fa820":"# Create a rounded submission file\nsub_rounded = None\nif sub is not None:\n    sub_rounded = sub.copy()\n    sub_rounded['num_sold'] = sub_rounded['num_sold'].round()\n    sub_rounded.to_csv('submission_keras_rounded.csv', index=False)\nsub_rounded\n","b9439cf3":"# Compute the residuals using the SMAPE formula without abs() and mean()\ntrain_preds = np.exp(model.predict([preproc.transform(train_df[features])])).ravel()\nresiduals = (train_df.num_sold - train_preds) \/ (train_df.num_sold + train_preds) * 200\n\nplt.figure(figsize=(20,6))\nplt.scatter(residuals.index, residuals, s=1, color='b')\nplt.hlines([0], 0, residuals.index.max(), color='k')\nplt.title('Residuals for all 26298 training samples')\nplt.ylabel('Residual (percent)')\nplt.xlabel('row_id')\nplt.show()\n","95dd1a16":"mu, std = scipy.stats.norm.fit(residuals)\n\nplt.figure(figsize=(20,4))\nplt.hist(residuals, bins=100, color='b', density=True)\nx = np.linspace(plt.xlim()[0], plt.xlim()[1], 200)\nplt.plot(x, scipy.stats.norm.pdf(x, mu, std), 'r', linewidth=2)\nplt.title(f'Histogram of residuals; mean = {residuals.mean():.4f}, '\n          f'$\\sigma = {residuals.std():.1f}$, SMAPE = {residuals.abs().mean():.5f}')\nplt.xlabel('Residual (percent)')\nplt.ylabel('Density')\nplt.show()\n","45affb22":"def plot_unexplained(residuals, groups, labels=None, label_z_score=False, title=None):\n    residuals_grouped = residuals.groupby(groups)\n    means = residuals_grouped.mean()\n    counts = residuals_grouped.count()\n    z_score = np.sqrt(counts) * means \/ residuals.std()\n    z_threshold = scipy.stats.norm.ppf(1 - 0.25 \/ len(means))\n    m_threshold = z_threshold * residuals.std() \/ np.sqrt(counts.mean())\n    outliers = np.abs(z_score) > z_threshold\n    plt.figure(figsize=(17, 4))\n    #plt.hlines([-z_threshold, +z_threshold] if label_z_score else [-m_threshold, +m_threshold], 0, len(means)-1, color='k')\n    plt.bar(range(len(means)), z_score if label_z_score else means,\n            color=outliers.apply(lambda b: 'r' if b else 'b'), width=0.6)\n    if labels is not None: plt.xticks(ticks=range(len(means)), labels=labels)\n    plt.ylabel('z score' if label_z_score else 'percent')\n    plt.title(title)\n    plt.show()    \n\nplot_unexplained(residuals, [train_df.date.dt.day],\n                 labels=np.arange(1, 32),\n                 title='Residuals for the 31 days of the month')\nplot_unexplained(residuals, [(train_df.date.dt.dayofyear) \/\/ 7],\n                 labels=None,\n                 title='Residuals for the 53 weeks of a year')\nplot_unexplained(residuals, [train_df.date.dt.month],\n                 labels=\"JFMAMJJASOND\",\n                 title='Residuals for the 12 months')\n","19aa689a":"plot_unexplained(residuals, [(train_df.date - train_df.date.min()).dt.days \/\/ 7],\n                 labels=None,\n                 title='Mean residuals of all 213 weeks of the training data')\nplot_unexplained(residuals, [train_df.date.dt.year, train_df.date.dt.month],\n                 labels=\"JFMAMJJASOND\" * 4,\n                 title='Mean residuals of all 48 months')\nplot_unexplained(residuals, [train_df.date.dt.year, train_df.date.dt.quarter],\n                 labels=[f\"{q\/\/4}Q{q%4+1}\" for q in range(60, 76)],\n                 title='Mean residuals of all 16 Quarters')\n","74f1fb37":"def plot_unexplained(residuals, groups, labels=None, label_z_score=False, title=None, label=None):\n    residuals_grouped = residuals.groupby(groups)\n    means = residuals_grouped.mean()\n    plt.plot(range(len(means)), z_score if label_z_score else means,\n            label=label)\n    if labels is not None: plt.xticks(ticks=range(len(means)), labels=labels)\n\n\nplt.figure(figsize=(17, 6))\nplt.subplot(2, 1, 1)\nfor i, c in enumerate(original_train_df.country.unique()):\n    selection = original_train_df.country == c\n    plot_unexplained(residuals[selection], [train_df.date.dt.year[selection], train_df.date.dt.month[selection]],\n                     labels=\"JFMAMJJASOND\" * 4,\n                     title='Mean residuals of all 48 months',\n                     label=c)\nplt.legend()\n\nplt.subplot(2, 1, 2)\nfor i, c in enumerate(original_train_df['product'].unique()):\n    selection = original_train_df['product'] == c\n    plot_unexplained(residuals[selection], [train_df.date.dt.year[selection], train_df.date.dt.month[selection]],\n                     labels=\"JFMAMJJASOND\" * 4,\n                     title='Mean residuals of all 48 months',\n                     label=c)\nplt.legend()\nplt.show()","d4463fa0":"# Training and validation\n\nWe validate using a 4-fold GroupKFold with the years as groups. We show\n- The execution time and the SMAPE\n- The training and validation loss curves with the learning rate\n- A scatterplot y_true vs. y_pred (ideally all points should lie near the diagonal)\n","84b24783":"As a second step, we plot the histogram of the residuals. The histogram looks like a normal distribution which has a standard deviation of 5.2 and is (almost) centered at 0. Almost all 26298 residuals are contained within \u00b14 standard deviations, i.e. there are no outliers.","ab24c44e":"# Feature engineering","c0be891f":"# Keras quickstart model for the January TPS\n\nThis notebook shows how to use a Keras network in the January 2022 TPS competition.\n\nOn the first of January, I implemented a Keras network for the TPS competition. The network fluctuated between overfitting and divergence. This let me realize that I had to understand the data before implementing the network. I did an [EDA](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense), implemented a [linear model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model) and a [LightGBM model](https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-06-lightgbm-quickstart). Now I'm returning to Keras.\n\nThe network consists of a single dense layer, i.e. it is a linear model. So what is the **advantage of a single-layer neural network** compared to the scikit-learn regressors? The main advantage is that the network is more flexible for experimentation and ready for future improvements: We can play with various regularization schemes or add a hidden layer.\n\nSome points to note:\n- Although Keras could handle SMAPE as a custom loss, I'm using an MSE loss and a log-transformed target. See [this post](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298473) and in particular [this post](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/300611#1649132) for an explanation why MSE with a log-transformed target is the best choice.\n- The network basically uses the same features as my linear model. This contrasts with the LightGBM model, which works with completely other features.\n- Initializing the bias to a suitable non-zero value reduces training time massively.\n- Cross-validation uses a 4-fold GroupKFold with the years as groups.\n- For good test predictions, we need to retrain the network on the full training data (all four years). In this retraining, we cannot use early stopping because there is no data left for validation. I train the network for a fixed number of epochs and use cosine learning rate decay.\n\nExperiment with the notebook - I wish you good luck!\n\nBug reports: Please report all bugs in the comments section of the notebook.\n\nRelease notes:\n- V2: Retrain on full data\n- V3: Feature engineering\n- V4: Quarterly GDP -> cv is much too high -> don't use this dataset!\n- V5: Other scaling\n- V6: Residuals analysis\n- V7: Residuals grouped by country and product\n","2ed7acca":"## Other trends\n\nHaving seen that the model captures the seasonality well, we'll do a similar test to see how the model deals with trends which do not depend on season. Again we average residuals and do a z-test. This time we see lots of red bars, most conspicuously in the last quarter of 2017, where sales are 2 % higher than expected. \n\nThe result of this analysis means that the sales figures are affected by some time-dependent influence which is unknown to our model. This influence might be, for instance:\n- Some macro-economic indicator with monthly or better granularity, perhaps customer confidence index, unemployment rate or an exchange rate\n- Marketing campaigns of Kaggle or its competitors - maybe Google Trends knows more\n- The weather, as [investigated by](https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/301486) @adamwurdits","a3bd2cf8":"# Demonstration","95a614f3":"# Feature importance","b3dc685c":"# Retraining and submission","82e24ccc":"We can group the same residuals by country or product to show that we are looking for an influence which affects all countries and products equally.","dd4f8629":"## Seasonality\n\nIn the third step of our residual analysis, we want to test whether the seasonality of the data is well modeled. For this purpose, we calculate the average residual\n- for every day of the month (31 values)\n- for every week of the year (53 values)\n- for every month of the year (12 values)\n\nThe standard deviation of these averages should be much smaller than the standard deviation of the individual residuals (which was 5.2). The diagrams show that all averages are between -0.6 and +0.6.\n\nWe now could do a [one-sample t-test](https:\/\/en.wikipedia.org\/wiki\/Student%27s_t-test#One-sample_t-test) to decide whether any average significantly deviates from zero, but the sample size is large enough to approximate the t-test by a [z-test](https:\/\/en.wikipedia.org\/wiki\/Z-test). \n\nIn the diagrams, the bars are colored red if the average significantly deviates from zero, whereby the significance level is chosen so that only one or two bars per diagram should be red if the residuals are independent. Indeed most of the bars are blue, which shows that the data's seasonality is captured well by the model.","003408b1":"# Analyzing the residuals\n\nA residuals analysis can give an indication how the model might be improved. We start by computing the residuals and plotting all 26298 residuals. The plot looks quite homogeneous, except maybe between row_id 18000 and 20000, where the residuals are slightly above average."}}