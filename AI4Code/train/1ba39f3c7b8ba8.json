{"cell_type":{"2d0e6056":"code","e30359bb":"code","62431688":"code","256a1f93":"code","461505e5":"code","7d29378e":"code","233906cf":"code","9df6c2c5":"code","e4e61b10":"code","cb53f974":"code","85b39771":"code","20aa1721":"code","2b26e318":"code","bed7560f":"code","171e63ea":"code","2ab05c6f":"markdown","974cccee":"markdown","b4901319":"markdown","974f626c":"markdown","74b5c7ee":"markdown","c4dadf63":"markdown","1757e2db":"markdown","fda45a0c":"markdown","f57be912":"markdown","6128b1a8":"markdown","229760e0":"markdown"},"source":{"2d0e6056":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization \n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\ndf= pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ndf.head(10)\ndf.shape\ndf.info()\n\n\ndf.isna().sum() # so lots of missing values are there","e30359bb":"\n## Lets start by handling missing values\ndf['gender'].loc[df['gender'].isnull()==True]='Undefined' #since nan values is really high we will treat it as a seperate category\ndf.gender.value_counts()\ndf.gender.isna().sum()\n\n\n\ndf.enrolled_university .value_counts()\ndf.enrolled_university .isna().sum()\ndf['enrolled_university'].loc[df['enrolled_university'].isnull()==True]='no_enrollment'  \n#Here the nan values are small so we just add it to majority class\n\n\n\n\ndf.education_level.value_counts()\ndf.education_level.isna().sum()\ndf['education_level'].loc[df['education_level'].isnull()==True]='Graduate' \n# the nan values are very small so we will just add into graduate category\n\n\n\ndf.major_discipline.value_counts()\ndf['major_discipline'].loc[df['major_discipline'].isnull()==True]='STEM'\ndf.major_discipline.isna().sum()\n\n\n\n\ndf.experience.replace('>20','22',inplace=True) # replacing special chars(like >,+) with numbers\ndf.experience.replace('<1','0',inplace=True)\ndf.experience=pd.to_numeric(df.experience)\ndf['experience']=np.where(df['experience']>10,'Senior-level', np.where(df['experience']>3,'Intermediate-level' ,'Entry-Level'))\n# here we are creating a class interval for each level of experience\ndf['experience'].value_counts().sum() \n\n\n\ndf.company_size.replace('<10','9',inplace=True) # replacing special chars(like >,+) with numbers\ndf.company_size.replace('10\/49','20',inplace=True)\ndf.company_size.replace('50-99','55',inplace=True)\ndf.company_size.replace('100-500','300',inplace=True)\ndf.company_size.replace('10000+','10001',inplace=True)\ndf.company_size.replace('500-999','600',inplace=True)\ndf.company_size.replace('5000-9999','6000',inplace=True)\ndf.company_size.replace('1000-4999','3000',inplace=True)\ndf.company_size= pd.to_numeric(df.company_size)\ndf['company_size']=np.where(df['company_size']>2000,'Large-org.', np.where(df['company_size']>1,'Small & Medium-org.','Undefined'))\n# here we are creating a class interval for various company sizes\ndf['company_size'].value_counts()\n\n\ndf.company_type.value_counts()\ndf.company_type.isna().sum()\ndf['company_type'].loc[df['company_type'].isnull()==True]='Pvt Ltd'\n\n\n\n\ndf.last_new_job.value_counts()      \ndf.last_new_job.replace('>4','5',inplace=True)\ndf.last_new_job.replace('never','0',inplace=True)\ndf.last_new_job.fillna(1,inplace=True)    #Replace with majority category of 1 year diff\ndf.last_new_job=df.last_new_job.astype(int)\n\ndf.isna().sum() # no column has nan value left we can proceed now","62431688":"\n## Time to encode the categories\n\ncategorical_cols=[name for name in df.columns if df[name].dtype not in ['int','float']]  \ncategorical_cols\n#fetches categorical features needed to be encoded\n\n\n\nnew_df=df.copy() #lets make a new df to which we want to make changes\n\nnew_df.relevent_experience= new_df.relevent_experience.map({\"Has relevent experience\":1,\"No relevent experience\":0})\n\n\n\n#Ordinal encoder because these categories follow a rank like 1st,2nd,3rd etc\nfrom sklearn.preprocessing import OrdinalEncoder\nenc_ordered_cat=OrdinalEncoder(categories=[['Undefined',\"Small & Medium-org.\",'Large-org.'],[\"Entry-Level\",\"Intermediate-level\" ,'Senior-level'],['Primary School','High School',\"Graduate\",'Masters','Phd']])\nordinal_feat=enc_ordered_cat.fit_transform(new_df[['company_size','experience','education_level']])\nenc_ordered_cat.categories_\n\n\n\n#OneHot encode because these categories are independent of each other\none_hot_cat=new_df[['gender','major_discipline', 'company_type','enrolled_university']]\none_hot=pd.get_dummies(one_hot_cat)\n\n\nnew_df.drop([\"company_type\",\"gender\",\"major_discipline\",\"enrolled_university\"],axis=1,inplace=True)\nnew_df.drop(['company_size','experience','education_level'],axis=1,inplace=True)","256a1f93":"\nfull_df=pd.concat([new_df,one_hot],axis=1)\n\n\nordinal_feat_df= pd.DataFrame(data=ordinal_feat,dtype='int32')      #converting the np array of ordinal encoded feat to dataframe\nordinal_feat_df.columns=['company_size','experience','education_level']\n\nall_df= pd.concat([full_df,ordinal_feat_df],axis=1) # here is the final dateframe that we are going to work with \nall_df.head() \n\n\nall_df.drop(['enrollee_id', 'city'],inplace=True,axis=1)\nall_df.head()","461505e5":"from sklearn.preprocessing import scale  #training hours can have a magnitude as it shows relatively high numbers\n\nall_df['training_hours']=scale(all_df['training_hours'])     #scales down to unit variance\nall_df['training_hours']= np.floor(all_df['training_hours'])\n\nall_df.corr()['target']","7d29378e":"y=all_df['target']\nX=all_df.drop('target',axis=1)\n\n\ny.value_counts()   # the number people that will change job are way less than those who wont -DATA IS IMBALANCED\nsns.countplot(y) \n\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\nrand=RandomOverSampler(random_state=42)\nx_ros, y_ros = rand.fit_resample(X, y)\nprint(f\"Imbalanced target class: {Counter(y)} Balanced target class:{Counter(y_ros)}\")\n","233906cf":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(x_ros,y_ros,test_size=0.3,shuffle=True)\n#make sure shuffle is set to true because we dont want to get data belonging to one class","9df6c2c5":"from sklearn.ensemble import RandomForestClassifier  # In my opinion RF will be ideal as it we have many feat like yes or no questions which can be use in decision making \nrf=RandomForestClassifier()\nrf.get_params() #fetches rf params\n\nrf.fit(X_train,y_train)\n\npredictions= rf.predict(X_test)","e4e61b10":"\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,classification_report\nprint(f\"Roc-Auc score: {roc_auc_score(y_test,predictions)},f1_score: {f1_score(y_test,predictions)},Accuracy: {accuracy_score(y_test,predictions)}\")\nprint(classification_report(y_test,predictions))\n# The precision-recall, accuracy scores are very promising but wait until we cross check it with the validation set\n","cb53f974":"from sklearn.model_selection import cross_val_score,StratifiedKFold        # lets validate our val_accuracy\nskfold = StratifiedKFold(n_splits=3, random_state=42,shuffle=True)\nscores=cross_val_score(rf,X_test,y_test,cv=skfold)\nprint(\"best score:{:.3f}\".format(np.mean(scores)))              # validation looks good","85b39771":"all_df.to_csv(\"all_test.csv\")\nnew_test= pd.read_csv('.\/all_test.csv')\nnew_test_X=new_test.drop([\"Unnamed: 0\",'target'],axis=1)\nnew_test_y=new_test[\"target\"]","20aa1721":"pred=rf.predict(new_test_X) \n#fitting our random forest into new test data and geting the probability of candidate leaving or staying \npred","2b26e318":"\nfrom sklearn.metrics import roc_auc_score,f1_score,accuracy_score,classification_report\nprint(f\"Roc-Auc score: {roc_auc_score(new_test_y,pred)},f1_score: {f1_score(new_test_y,pred)},Accuracy: {accuracy_score(new_test_y,pred)}\")\nprint(classification_report(new_test_y,pred))","bed7560f":"from sklearn.model_selection import cross_val_score,StratifiedKFold  # lets validate our val_accuracy\nskfold = StratifiedKFold(n_splits=5, random_state=42,shuffle=True)\nscores=cross_val_score(rf,new_test_X,new_test_y,cv=skfold)\nprint(\"best score:{:.3f}\".format(np.mean(scores)))","171e63ea":"import pickle\n# save your precious model :)\nfilename = 'finalized_model.sav'\npickle.dump(rf, open(filename, 'wb'))","2ab05c6f":"## Testing on new testset","974cccee":"## Correlation of features with target class ","b4901319":"\n## Combining all dataframes to get a final df","974f626c":"## Training and testing data  split","74b5c7ee":"## Encoding Categorical variables","c4dadf63":"## Lets see how well our model performed!","1757e2db":"## Model building","fda45a0c":"## Cross validation","f57be912":"## Treating imbalanced targets\n","6128b1a8":"#### Please upvote this so i can make moreof these. Cheers!!","229760e0":"## Handling Missing values "}}