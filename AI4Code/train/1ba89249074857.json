{"cell_type":{"f07bd9ef":"code","aa0eb704":"code","53ba31f5":"code","0f0b2f90":"code","ebbecfcd":"code","a79e8f05":"code","ab80c3b7":"code","70d3584e":"code","947cbd66":"code","f172ec90":"code","7249121c":"code","b1db9c4a":"code","a381b4e7":"code","f1d099ad":"code","e3234224":"code","df0722a3":"code","32602c9c":"code","97e11411":"code","cb2f99d2":"code","e999b2b1":"code","54ef3998":"code","86d993a0":"code","9b731f9d":"code","a5d718b3":"code","396e0e8b":"code","906733b2":"code","7cba22a6":"code","133c1f48":"code","14ddbb57":"code","b175dc5b":"code","7f1d49ab":"code","30984425":"code","13732632":"code","7a88fa27":"code","24c3e57c":"code","ba85f35c":"code","37047eeb":"code","c61a939a":"code","f68da883":"code","56875487":"code","f4d104f4":"code","2261cc39":"code","588cef75":"code","7dfcfd2e":"code","11be3f3d":"code","ae136dc0":"markdown","d535f644":"markdown","75b8b020":"markdown","9f324cb0":"markdown","bfbe81b1":"markdown","01391ba1":"markdown","b21dc153":"markdown","8e7504db":"markdown","b42c1b94":"markdown","0048142f":"markdown","c537cb97":"markdown","0e47996c":"markdown","e83b0373":"markdown","fd4c539b":"markdown","22277ce9":"markdown","6a2ba42a":"markdown","2fb3e235":"markdown","fa464c8c":"markdown","f6660625":"markdown","5394bce5":"markdown","0e8839bc":"markdown","0dbbb443":"markdown","629c6233":"markdown"},"source":{"f07bd9ef":"# Import data analysis libraries\nimport pandas as pd\nimport numpy as np\n\n# Import libraries for visualisation\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Show all columns\npd.set_option('display.max_columns', None)\npd.set_option('mode.chained_assignment', None)\n\nprint('Libraries Imported!')","aa0eb704":"# Read into dataframe\ndf = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndf.head()","53ba31f5":"# Get column names\ncols = df.columns\nprint(cols)","0f0b2f90":"# Seperate target variable from dataframe\ny = df.blueWins\n\n# Drop target and unnecessary features\ndrop_cols = ['gameId','blueWins']\nx = df.drop(drop_cols, axis=1)\n\nx.head()","ebbecfcd":"# Visualise blueWins using countplot\nax = sns.countplot(y, label='Count', palette='RdBu')\nW, L = y.value_counts()\n\nprint('Red Wins: {} ({}%), Blue Wins: {}({}%)'.format(W,round(100*W\/(W+L),3),L,round(100*L\/(W+L),3)))","a79e8f05":"x.describe()","ab80c3b7":"# Drop unnecessary features (same as blueFirstBlood, blueDeaths etc.)\ndrop_cols = ['redFirstBlood','redKills','redDeaths'\n             ,'redGoldDiff','redExperienceDiff', 'blueCSPerMin',\n            'blueGoldPerMin','redCSPerMin','redGoldPerMin']\nx.drop(drop_cols, axis=1, inplace=True)\nx.head()","70d3584e":"# Copy feature matrix and standardise\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:, 0:9]], axis=1)\ndata = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')\n\nfig, ax = plt.subplots(1,2,figsize=(15,5))\n\n# Create violin plot of features\n#plt.figure(figsize=(8,5))\nsns.violinplot(x='Features', y='Values', hue='blueWins', data=data, split=True,\n               inner='quart', ax=ax[0], palette='Blues')\nfig.autofmt_xdate(rotation=45)\n\ndata = x\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std.iloc[:, 9:18]], axis=1)\ndata = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')\n\n# Create violin plot\n#plt.figure(figsize=(8,5))\nsns.violinplot(x='Features', y='Values', hue='blueWins', \n               data=data, split=True, inner='quart', ax=ax[1], palette='Blues')\nfig.autofmt_xdate(rotation=45)\n\nplt.show()","947cbd66":"plt.figure(figsize=(18,14))\nsns.heatmap(round(x.corr(),2), cmap='Blues', annot=True)\nplt.show()","f172ec90":"# Drop unnecessary features\ndrop_cols = ['redAvgLevel','blueAvgLevel']\nx.drop(drop_cols, axis=1, inplace=True)","7249121c":"sns.set(style='whitegrid', palette='muted')\n\nx['wardsPlacedDiff'] = x['blueWardsPlaced'] - x['redWardsPlaced']\nx['wardsDestroyedDiff'] = x['blueWardsDestroyed'] - x['redWardsDestroyed']\n\ndata = x[['blueWardsPlaced','blueWardsDestroyed','wardsPlacedDiff','wardsDestroyedDiff']].sample(1000)\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std], axis=1)\ndata = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')\n\nplt.figure(figsize=(10,6))\nsns.swarmplot(x='Features', y='Values', hue='blueWins', data=data)\nplt.xticks(rotation=45)\nplt.show()","b1db9c4a":"# Drop unnecessary features\ndrop_cols = ['blueWardsPlaced','blueWardsDestroyed','wardsPlacedDiff',\n            'wardsDestroyedDiff','redWardsPlaced','redWardsDestroyed']\nx.drop(drop_cols, axis=1, inplace=True)","a381b4e7":"x['killsDiff'] = x['blueKills'] - x['blueDeaths']\nx['assistsDiff'] = x['blueAssists'] - x['redAssists']\n\nx[['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists']].hist(figsize=(12,10), bins=20)\nplt.show()","f1d099ad":"sns.set(style='whitegrid', palette='muted')\n\ndata = x[['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists']].sample(1000)\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std], axis=1)\ndata = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')\n\nplt.figure(figsize=(10,6))\nsns.swarmplot(x='Features', y='Values', hue='blueWins', data=data)\nplt.xticks(rotation=45)\nplt.show()","e3234224":"data = pd.concat([y, x], axis=1).sample(500)\n\nsns.pairplot(data, vars=['blueKills','blueDeaths','blueAssists','killsDiff','assistsDiff','redAssists'], \n             hue='blueWins')\n\nplt.show()","df0722a3":"data = pd.concat([y, x], axis=1)\n\nfig, ax = plt.subplots(1,2, figsize=(15,6))\nsns.scatterplot(x='killsDiff', y='assistsDiff', hue='blueWins', data=data, ax=ax[0])\n\nsns.scatterplot(x='blueKills', y='blueAssists', hue='blueWins', data=data, ax=ax[1])\nplt.show()","32602c9c":"# Drop unnecessary features\ndrop_cols = ['blueFirstBlood','blueKills','blueDeaths','blueAssists','redAssists']\nx.drop(drop_cols, axis=1, inplace=True)","97e11411":"x['dragonsDiff'] = x['blueDragons'] - x['redDragons']\nx['heraldsDiff'] = x['blueHeralds'] - x['redHeralds']\nx['eliteDiff'] = x['blueEliteMonsters'] - x['redEliteMonsters']\n\ndata = pd.concat([y, x], axis=1)\n\neliteGroup = data.groupby(['eliteDiff'])['blueWins'].mean()\ndragonGroup = data.groupby(['dragonsDiff'])['blueWins'].mean()\nheraldGroup = data.groupby(['heraldsDiff'])['blueWins'].mean()\n\nfig, ax = plt.subplots(1,3, figsize=(15,4))\n\neliteGroup.plot(kind='bar', ax=ax[0])\ndragonGroup.plot(kind='bar', ax=ax[1])\nheraldGroup.plot(kind='bar', ax=ax[2])\n\nprint(eliteGroup)\nprint(dragonGroup)\nprint(heraldGroup)\n\nplt.show()","cb2f99d2":"# Drop unnecessary features\ndrop_cols = ['blueEliteMonsters','blueDragons','blueHeralds',\n            'redEliteMonsters','redDragons','redHeralds']\nx.drop(drop_cols, axis=1, inplace=True)","e999b2b1":"x['towerDiff'] = x['blueTowersDestroyed'] - x['redTowersDestroyed']\n\ndata = pd.concat([y, x], axis=1)\n\ntowerGroup = data.groupby(['towerDiff'])['blueWins']\nprint(towerGroup.count())\nprint(towerGroup.mean())\n\nfig, ax = plt.subplots(1,2,figsize=(15,5))\n\ntowerGroup.mean().plot(kind='line', ax=ax[0])\nax[0].set_title('Proportion of Blue Wins')\nax[0].set_ylabel('Proportion')\n\ntowerGroup.count().plot(kind='line', ax=ax[1])\nax[1].set_title('Count of Towers Destroyed')\nax[1].set_ylabel('Count')","54ef3998":"# Drop unnecessary features\ndrop_cols = ['blueTowersDestroyed','redTowersDestroyed']\nx.drop(drop_cols, axis=1, inplace=True)","86d993a0":"data = pd.concat([y, x], axis=1)\n\ndata[['blueGoldDiff','blueExperienceDiff']].hist(figsize=(15,5))\nplt.show()","9b731f9d":"plt.figure(figsize=(8,6))\nsns.scatterplot(x='blueExperienceDiff', y='blueGoldDiff', hue='blueWins', data=data)","a5d718b3":"# Drop unnecessary features\ndrop_cols = ['blueTotalGold','blueTotalExperience','redTotalGold','redTotalExperience']\nx.drop(drop_cols, axis=1, inplace=True)\n\nx.rename(columns={'blueGoldDiff':'goldDiff', 'blueExperienceDiff':'expDiff'}, inplace=True)","396e0e8b":"data = pd.concat([y, x], axis=1)\n\ndata[['blueTotalMinionsKilled','blueTotalJungleMinionsKilled',\n      'redTotalMinionsKilled','redTotalJungleMinionsKilled']].hist(figsize=(15,10))\nplt.show()","906733b2":"sns.set(style='whitegrid', palette='muted')\n\ndata = x[['blueTotalMinionsKilled','blueTotalJungleMinionsKilled',\n      'redTotalMinionsKilled','redTotalJungleMinionsKilled']].sample(1000)\ndata_std = (data - data.mean()) \/ data.std()\ndata = pd.concat([y, data_std], axis=1)\ndata = pd.melt(data, id_vars='blueWins', var_name='Features', value_name='Values')\n\nplt.figure(figsize=(10,6))\nsns.swarmplot(x='Features', y='Values', hue='blueWins', data=data)\nplt.xticks(rotation=45)\nplt.show()","7cba22a6":"# Drop unnecessary features\ndrop_cols = ['blueTotalMinionsKilled','blueTotalJungleMinionsKilled',\n      'redTotalMinionsKilled','redTotalJungleMinionsKilled']\nx.drop(drop_cols, axis=1, inplace=True)","133c1f48":"# Import libraries for machine learning models\nfrom sklearn import preprocessing, metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nprint('Machine Learning Libraries Imported!')","14ddbb57":"print(x.shape,y.shape)\nx.head()","b175dc5b":"X = preprocessing.StandardScaler().fit(x).transform(x.astype(float))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","7f1d49ab":"from prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = ['Algorithm', 'Accuracy', 'Recall', 'Precision', 'F-Score']","30984425":"def get_confusion_matrix(algorithm, y_pred, y_actual):\n    # Create confusion matrix and interpret values\n    con = confusion_matrix(y_test, y_pred)\n    tp, fn, fp, tn = con[0][0], con[0][1], con[1][0], con[1][1]\n    algorithm = algorithm\n    accuracy = (tp + tn) \/ (tp + tn + fp + fn)\n    recall = tp \/ (tp + fn)\n    precision = tp \/ (tp + fp)\n    f_score = (2 * precision * recall) \/ (recall + precision)\n    return algorithm, accuracy, recall, precision, f_score","13732632":"# Test different values of k\nKs = 10\nmean_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    kneigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    y_pred = kneigh.predict(X_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, y_pred)\n\n# Use most accurate k value to predict test values\nk = mean_acc.argmax()+1\nneigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\ny_pred = neigh.predict(X_test)","7a88fa27":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('KNN', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","24c3e57c":"# Initialise Decision Tree classifier and predict\ndrugTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4)\ndrugTree.fit(X_train,y_train)\ny_pred = drugTree.predict(X_test)","ba85f35c":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('Decision', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","37047eeb":"# Train and predict logistic regression model\nLR = LogisticRegression(C=0.01, solver='liblinear')\ny_pred = LR.fit(X_train,y_train).predict(X_test)","c61a939a":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('LR', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","f68da883":"clf = svm.SVC(kernel='rbf')\ny_pred = clf.fit(X_train, y_train).predict(X_test)","56875487":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('SVM', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","f4d104f4":"gnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)","2261cc39":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('Bayes', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","588cef75":"# Instantiate Random Forest Classifier and predict values\nclf = RandomForestClassifier(max_depth=2, random_state=0)\ny_pred = clf.fit(X_train, y_train).predict(X_test)","7dfcfd2e":"# Call confusion matrix and accuracy\nalgorithm, accuracy, recall, precision, f_score = get_confusion_matrix('R Forest', y_pred, y_test)\n\n# Add values to table\ntable.add_row([algorithm, round(accuracy,5), round(recall,5),\n               round(precision,5), round(f_score,5)])","11be3f3d":"print(table)","ae136dc0":"## [Game Basics](#1)\nI imagine the matter that you are reading a post on League of Legends suggests you may be more than familiar with the rules, and have a much more in depth understanding of strategies and influences than I do. However, I will briefly explain some of the basics. Feel free to skip this part.\n\n\n- Players accumulate gold and experience from a mixture of killing minions, monsters, other players and towers. \n       More gold -> better items -> easier killing.\n       More experience -> higher levels -> easier killing.\n- Wards provide map vision so we can see people coming to kill us.\n       More wards -> better vision -> less deathing.\n- Main objective of the game is to destroy a number of towers leading to the destruction of the opponents base.\n       Kill towers -> kill base -> win game.","d535f644":"## [Data Exploration](#2)\nThe aim of this project is to try and predict a class for **blueWins**, which is the respective outcome of the game. We can do this by visualising the features in the following dataframe and subsequently using machine learning techniques to find the best predictions.  \n\nBegin by viewing the different features we have available in the dataset provided.","75b8b020":"# League of Legends Diamond Classification Problem\nHi all, I am new to the data science community on all platforms and would appreciate any help and guidance you can provide in my projects.\n\nLeague of Legends. Possibly the biggest online game of all time and a life choice for some people, but is it possible to predict the outcome of a game based on the statistics in the first 10 minutes?\n\nAccording to the <a href='https:\/\/leagueoflegends.com'>leagueoflegends.com<\/a>\n> League of Legends is a team-based strategy game where two teams of five powerful champions face off to destroy the other\u2019s base. Choose from over 140 champions to make epic plays, secure kills, and take down towers as you battle your way to victory.","9f324cb0":"### Analysis of basic statistics\nThe numeric data in our dataset have very different ranges which could effect machine learning models effectiveness by applying different weights to different features.\n\n**Discrete Data**\n- Blue\/red wards placed\/destroyed have a massive range and sdev.\n- Blue\/red Elite Monsters equal to sum of Dragons + Heralds, dragons more popular kill.\n- Blue\/red total gold\/minions killed have low sdev (<10% mean)\n- Blue gold diff \/ experience diff is exact negative of red gold diff \/ experience diff.\n\n**Binary Data**\n- Blue\/red First blood is yes\/no with approx 50% reliability.","bfbe81b1":"### Naive Bayes","01391ba1":"### Decision Trees","b21dc153":"The importance of each feature on the outcome of a game can be pictured below, where the outcome isn't solely represented by these features, there is a clear correlation.  \n\nInclude **killsDiff** and **assistsDiff** in modelling.","8e7504db":"### Evaluation of Machine Learning Models\n**Logistic Regression** had the highest accuracy score of the machine learning models used with a prediction accuracy of 74.646%. Since we are trying to predict an outcome of a game, there isn't much risk involved with false positives or negatives and therefore we look to the accuracy for the greattest predicting model.","b42c1b94":"### Kills, Assists and Deaths\nThe distribution of the kills, deaths and assists appear similar, assists of course scale with kills (or red assists with blue deaths) so the histograms are as expected.  ","0048142f":"### Violin and Box Plots\nViolin plots allow us to visualise the distribution of each features simply and seperate data points based on the final outcome of a game.\n\n**Observations from plots**\n- Blue kills appears to have a large positive impact on winning the game.\n- Similarly, blue deaths has a large negative impact on winning the game (i.e positive on losing).\n- Blue assists similar plot to blue kills, need to get kills to get assists so scales with kills.\n- First blood is positively correlated with outcome but also mirrors blue kills.\n- Gold and experience differences have major influence.\n- Minions and Jungle minions do not have much impact.","c537cb97":"### Random Forest","0e47996c":"### Minions and Jungle Minions","e83b0373":"### Support Vector Machines","fd4c539b":"### Gold and Experience","22277ce9":"### Towers\nA major objective for each team and we should therefore expect to be heavily influential with the outcome of the game.\n\nThe plots below show that although it is unlikely there will be any towers destroyed in the first ten minutes of the game, the destruction of a tower provides a great advantage to a team, and therefore will be included in my model as **towerDiff**.","6a2ba42a":"### Ward Data\nWe can see the seperation of data points is pretty well randomised in the plot of ward data below. From knowledge of the game I would suggest that ward placing and destruction in Diamond is quite systematic and therefore there isn't much variance in the data as suggested by the violin plots above.  \n\nWith this in mind, I will not use ward data in my learning model.","2fb3e235":"- Notice that numeric data has very different ranges, which gives higher weights to larger in machine learning models. So standardise.","fa464c8c":"### Logistic Regression","f6660625":"### Elite Monsters\nIncluding all three of the features that are **blueEliteMonsters**, **blueDragons** and **blueHeralds** would be unadvisable since the first of these is an accumulation of the others. Grouping the data below shows that having a dragon advantage gives a larger advantage than having a herald advantage.  \n\nThe dragon group shows a 64% chance of winning if killing the dragon before 10 minutes, 50% if equal on dragons and 37% chance if the opposite team has killed the dragon. S\n\nDragons pose more influence than heralds on the outcome of the game, therefore choose to include both **heralds** and **dragons** individually in my machine learning model.","5394bce5":"### K-Nearest Neighbours ","0e8839bc":"## [Machine Learning](#3)\nBelow I use some machine learning algorithms from the Scikit-Learn library to see how effective the features I have selected above are for predicting the outcome of a Diamond League of Legends match.","0dbbb443":"Good news! Our dataset provides almost 50\/50 data for our target variable, this means there is no data imbalance.","629c6233":"#### Target Variable \"blueWins\"\nLet's seperate the target variable from the dataframe into a new variable \"y\". \"gameId\" can also be dropped as it is randomised and provides no information gain to the observations in the data."}}