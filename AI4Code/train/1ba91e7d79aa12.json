{"cell_type":{"51f2123e":"code","fefa1305":"code","e8749f3e":"code","da0fc1df":"code","4edee1db":"code","05a1eea8":"code","27fcf333":"code","34bd675b":"code","0ed3d431":"code","945bcf8f":"code","46aeb88a":"code","e7729664":"code","918d1073":"code","3d84b722":"code","3191857d":"code","4f0a74d4":"code","6b0847c7":"code","789ff843":"code","84d85443":"code","c642a543":"code","83e7918c":"code","0105b075":"code","c170ab4f":"code","4dff1adc":"code","80ed97a7":"code","cf033751":"code","4c10635d":"code","73c3aab6":"code","ff9e92d2":"code","78ad301a":"code","ed58b1bc":"code","badec0eb":"code","f7c52c70":"code","b9f89676":"code","e86cbb53":"code","d37f434b":"code","62536531":"code","080747cc":"code","1fcc35a2":"code","cf1e49f4":"markdown"},"source":{"51f2123e":"!pip install iterative-stratification","fefa1305":"# ==================\n# Library\n# ==================\nimport warnings\nwarnings.simplefilter('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport sys\nimport pickle\nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.nn import LayerNorm\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom torch.optim import lr_scheduler\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.nn import TransformerEncoder\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","e8749f3e":"# ==========================\n# Constant\n# ==========================\nTRAIN_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/train.csv\"\nTEST_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/test_hard.csv\"\nACTION_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/actions.csv\"\nACTION_HARD_PATH = \"..\/input\/data-science-spring-osaka-hard-mode\/actions_hard.csv\"\nSUB_PATH =\"..\/input\/data-science-spring-osaka-hard-mode\/sample_submission.csv\"","da0fc1df":"# =========================\n# main\n# =========================\ntrain = pd.read_csv(TRAIN_PATH)\ntest = pd.read_csv(TEST_PATH)","4edee1db":"train.head()","05a1eea8":"test.head()","27fcf333":"train_all = pd.DataFrame()\ntest_all = pd.DataFrame()\nfor i in tqdm(train[\"file_path\"]):\n    train_ = pd.read_csv(f\"..\/input\/data-science-spring-osaka-hard-mode\/train\/{i}\")\n    train_[\"file\"] = i\n    train_all = pd.concat([train_all,train_]).reset_index(drop=True)\n    \nfor i in tqdm(test[\"file_path\"]):\n    test_ = pd.read_csv(f\"..\/input\/data-science-spring-osaka-hard-mode\/test_hard\/{i}\")\n    test_[\"file\"] = i\n    test_all = pd.concat([test_all,test_]).reset_index(drop=True)","34bd675b":"len_train = len(train_all)\ndf_all = pd.concat([train_all, test_all]).reset_index(drop=True)","0ed3d431":"sc = StandardScaler()\ncols = df_all.columns[1:-1]","945bcf8f":"df_all[cols] = sc.fit_transform(df_all[cols].values)","46aeb88a":"train_all = df_all.iloc[:len_train].reset_index(drop=True)\ntest_all = df_all.iloc[len_train:].reset_index(drop=True)","e7729664":"train_all[\"file\"].value_counts()","918d1073":"test_all[\"file\"].value_counts()","3d84b722":"train_all[\"file\"].nunique()","3191857d":"train_seq = np.zeros([len(train),596,len(cols)])\ntest_seq = np.zeros([len(test),596,len(cols)])","4f0a74d4":"for n,f in enumerate(train[\"file_path\"]):\n    train_value = train_all[train_all.file == f].iloc[:,1:-1].values\n    train_seq[n,-len(train_value):,:] = train_value","6b0847c7":"for n,f in enumerate(test[\"file_path\"]):\n    if f == '\/test_hard\/0161.csv':\n        test_value = test_all[test_all.file == f].iloc[224:,1:-1].values\n    else:\n        test_value = test_all[test_all.file == f].iloc[:,1:-1].values\n    test_seq[n,-len(test_value):,:] = test_value","789ff843":"# ==========================\n# Settings\n# ==========================\nex = \"005\"\nSEED = 0\nN_SPLITS = 5\nSHUFFLE = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 32\nEPOCH = 2\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u5185\u306b\u5b58\u5728\u3059\u308b\u3001straight,hook,upper(&bodyupper),sway,jab,jab\u3068\u306e\u30b3\u30f3\u30d3\u30cd\u30fc\u30b7\u30e7\u30f3\u306e9\u30a2\u30af\u30b7\u30e7\u30f3\nLABEL_NUM=9","84d85443":"# ==========================\n# Function\n# ==========================\n# ====================\n# Function\n# ====================\ndef process_data(data_seq):\n    # attention\u306emask\n    mask = data_seq[:,0] == 0\n    return {\n        'input_data_seq': data_seq,\n        \"mask\":mask\n    }\n\nclass DSPO_Dataset(Dataset):\n    \n    def __init__(self, data_seq, train = True, y = None):\n        self.data_seq = data_seq\n        self.train = train\n        self.y = y\n    \n    def __len__(self):\n        return len(self.data_seq)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.data_seq[item],\n            \n        )\n\n        # Return the processed data where the lists are converted to `torch.tensor`s\n        if self.train : \n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool),  \n              \"y\":torch.tensor(self.y[item], dtype=torch.float32)\n               }\n        else:\n            return {\n              'input_data_seq': torch.tensor(data[\"input_data_seq\"], dtype=torch.float32),\n              'mask': torch.tensor(data[\"mask\"], dtype=torch.bool), \n               }\n        \nclass Transformer_model(nn.Module):\n    def __init__(\n        self, dropout=0.2, con_size = 20, linear_emb1 = 240, dim_feedforward = 720, linear_emb2 = 100, nhead=4):\n        super(Transformer_model, self).__init__()\n        self.linear1 = nn.Sequential(\n            nn.Linear(con_size , linear_emb1),\n            nn.LayerNorm(linear_emb1 ),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        \n        self.transformer_encoder = TransformerEncoder(encoder_layer=nn.TransformerEncoderLayer(d_model=linear_emb1, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout),\n                                                      num_layers=2)\n        \n        # dense\n        self.linear2 = nn.Sequential(\n            nn.Linear(linear_emb1, linear_emb2),\n            nn.LayerNorm(linear_emb2),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        )\n        self.linear_final = nn.Linear(linear_emb2,LABEL_NUM)\n\n    def forward(self, data_seq, mask):\n        data_seq = self.linear1(data_seq)\n        # pytorch\u306etransformer encoder\u306einput\u306eshape\u306b\u6c17\u3092\u3064\u3051\u3066\u304f\u3060\u3055\u3044\n        data_seq = data_seq.permute(1, 0, 2).contiguous()\n        output = self.transformer_encoder(data_seq, src_key_padding_mask=mask)\n        output = output.permute(1, 0, 2).contiguous()\n        output = torch.mean(output, 1)\n        output = self.linear2(output)\n        output = self.linear_final(output)\n        return output\n    \ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef sigmoid(value):\n    return 1 \/ (1 + np.exp(-value))","c642a543":"# ==========================\n# Main\n# ==========================\naction = pd.read_csv(ACTION_PATH)\naction_hard = pd.read_csv(ACTION_HARD_PATH)\nsub = pd.read_csv(SUB_PATH)","83e7918c":"train[\"action_seq\"] = train[\"action_seq\"].str.replace('bodyupper', 'upper')","0105b075":"train[\"action_seq_str\"] = train[\"action_seq\"].str.split('-').tolist()","c170ab4f":"train","4dff1adc":"for i,l in enumerate(train[\"action_seq_str\"]):\n    train.iat[i, 2].append(l[0]+l[1])\n    train.iat[i, 2].pop(0)","80ed97a7":"train","cf033751":"train[\"action_seq_str\"].value_counts()","4c10635d":"from sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ntarget = mlb.fit_transform(train[\"action_seq_str\"].values)\ntarget","73c3aab6":"mlb.classes_","ff9e92d2":"y_oof = np.empty([len(train),LABEL_NUM])\ntest_preds = np.empty([len(test),LABEL_NUM])\ntest_ = DSPO_Dataset(test_seq, train = False, y = None)\ntest_loader = DataLoader(dataset=test_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n#kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\n#kf = KFold(n_splits=N_SPLITS, shuffle=SHUFFLE,random_state=SEED)\nkf = MultilabelStratifiedKFold(n_splits=N_SPLITS, shuffle=SHUFFLE, random_state=SEED)\nseed_everything(SEED)\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(train_seq,y = target)):\n    x_train_seq = train_seq[train_idx]\n    y_train = target[train_idx]\n    x_val_seq = train_seq[valid_idx]\n    y_val = target[valid_idx]\n    train_ = DSPO_Dataset(x_train_seq, train = True, y = y_train)\n    val_ = DSPO_Dataset(x_val_seq,train = True, y = y_val)\n    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True , num_workers=2)\n    val_loader = DataLoader(dataset=val_, batch_size=BATCH_SIZE, shuffle = False , num_workers=2)\n    model = Transformer_model()\n    model = model.to(device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters,\n                      lr=1e-3,\n                      weight_decay=0.1,\n                      )\n    num_train_optimization_steps = int(len(train_loader) * EPOCH)\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=5,\n                                                num_training_steps=num_train_optimization_steps)\n    \n    criterion = nn.BCEWithLogitsLoss()\n    best_val = None\n    for epoch in tqdm(range(EPOCH)):\n        model.train() \n        train_losses_batch = []\n        val_losses_batch = []\n        epoch_loss = 0\n\n        # ==========================\n        # train\n        # ==========================\n        for d in train_loader:\n\n            # =========================\n            # data loader\n            # =========================\n\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            y = d[\"y\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            y = y.to(device)\n            optimizer.zero_grad()\n\n            output = model(input_data_seq,mask)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses_batch.append(loss.item())\n\n        train_loss = np.mean(train_losses_batch)\n        \n        # ==========================\n        # eval\n        # ==========================\n        model.eval()  # switch model to the evaluation mode\n        val_preds = np.ndarray((0,LABEL_NUM))\n        with torch.no_grad():  # Do not calculate gradient since we are only predicting\n            # Predicting on validation set\n            for d in val_loader:\n                # =========================\n                # data loader\n                # =========================\n                input_data_seq = d['input_data_seq']\n                mask = d[\"mask\"]\n                y = d[\"y\"]\n                input_data_seq = input_data_seq.to(device)\n                mask = mask.to(device)\n                y = y.to(device)\n                output = model(input_data_seq,mask)\n\n                loss = criterion(output, y)\n                val_preds = np.concatenate([val_preds, output.detach().cpu().numpy()], axis=0)\n                val_losses_batch.append(loss.item())\n\n\n        val_loss = np.mean(val_losses_batch)\n        #acc = accuracy_score(np.argmax(y_val,axis=1), np.argmax(val_preds,axis=1))\n        acc = accuracy_score(y_val, np.round(sigmoid(val_preds)))\n        print(epoch, \"train loss:\", train_loss, \"val loss:\",val_loss, \"val acc:\",acc)\n        \n        if not best_val:\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            continue\n\n        if val_loss <= best_val:\n            best_epoch = epoch\n            best_val = val_loss  # So any validation roc_auc we have is the best one for now\n            best_acc = acc\n            torch.save(model.state_dict(), f\"ex{ex}_{fold}.pth\")  # Saving the model\n            y_oof[valid_idx,:] = val_preds\n            \n    print(f\"{fold}_best_poch:{best_epoch},best_val:{best_val}, best_acc:{best_acc}\")\n    \n    # ===================================\n    # test\n    # ===================================\n    model = Transformer_model()\n    model.load_state_dict(torch.load(f\"ex{ex}_{fold}.pth\"))\n    model.to(device)\n    model.eval()\n    test_preds_ = np.ndarray((0,LABEL_NUM))\n    with torch.no_grad():  # Do not calculate gradient since we are only predicting\n        # Predicting on test set\n        for d in test_loader:\n            # =========================\n            # data loader\n            # =========================\n            input_data_seq = d['input_data_seq']\n            mask = d[\"mask\"]\n            input_data_seq = input_data_seq.to(device)\n            mask = mask.to(device)\n            output = model(input_data_seq,mask)\n\n            test_preds_ = np.concatenate([test_preds_, output.detach().cpu().numpy()], axis=0)\n\n    #torch.save(best_model.state_dict(), f\"..\/ex\/ex{ex}\/ex{ex}_{b}_{fold}.pth\")  # Saving the model\n    test_preds += test_preds_ \/  N_SPLITS\n#acc = accuracy_score(train[\"action_seq_num\"].values,np.argmax(y_oof,axis=1))\nacc = accuracy_score(target,np.round(sigmoid(y_oof)))\nprint(f\"cv:{acc}\")\nnp.save(f\"ex{ex}_test_pred.npy\",test_preds)\nnp.save(f\"ex{ex}_oof.npy\",y_oof)","78ad301a":"test_preds_list = np.round(sigmoid(test_preds))","ed58b1bc":"test_preds_list=np.nan_to_num(test_preds_list)","badec0eb":"test[\"action_list\"] = mlb.inverse_transform(test_preds_list)","f7c52c70":"test[\"action_list\"].value_counts()","b9f89676":"test","e86cbb53":"action_hard","d37f434b":"test[\"action_seq\"]=''\nfor i,l in enumerate(test[\"action_list\"]):\n    if len(l)>9: #[secret]\u304c\u5206\u304b\u3089\u306a\u3044\u306e\u3067\u4e00\u65e6\u610f\u5473\u306e\u306a\u3044if\u6587 \u95be\u5024\u3092\u4e0b\u3052\u308b\u3068[secret]\u5019\u88dc\u304c\u51fa\u3066\u304f\u308b\n        test.iat[i, 2]='[secret]'\n    else:\n        if (('sway' in l) and ('upper' in l)) or (('jabsway' in l) and ('upper' in l)):\n            if 'jabstraight' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            else:\n                test.iat[i, 2]='stepinjab-backstep-sway-upper'\n                \n        elif 'jabstraight' in l:\n            if 'upper' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            elif 'jabjab' in l:\n                test.iat[i, 2]='jab-jab-jab-straight'\n            elif 'hook' in l:\n                test.iat[i, 2]='jab-straight-hook-bodyhook'\n            else:\n                test.iat[i, 2]='stepinjab-straight-backstep-straight'  \n\n        elif ('hook' in l) or ('jabhook' in l):\n            test.iat[i, 2]='jab-straight-hook-bodyhook'\n        elif 'upper' in l:\n            if 'straight' in l:\n                test.iat[i, 2]='jab-straight-upper-ducking'\n            else:\n                test.iat[i, 2]='jab-upper-ducking'\n\n        else:\n            if (l == ('jab', 'straight')) or l==('jab', 'jabjab', 'straight'):\n                test.iat[i, 2]='jab-jab-jab-straight'\n            else:\n                test.iat[i, 2]='jab-jab-jab-jab'","62536531":"pd.set_option('display.max_rows', 200)\ntest[test['action_seq'] == 'jab-jab-jab-jab']","080747cc":"test[\"action_seq\"].value_counts()","1fcc35a2":"test[['file_path','action_seq']].to_csv(f\"ex{ex}.csv\",index=False)","cf1e49f4":"### Transformer"}}