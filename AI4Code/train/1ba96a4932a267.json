{"cell_type":{"0bdff6f6":"code","1aa0968d":"code","f7f372f8":"code","7d09f885":"code","feb61b30":"code","d449cf7f":"code","5f907a17":"code","a41ff11f":"code","cf30cfe9":"markdown","68b56288":"markdown","4e489f36":"markdown","5049b201":"markdown","34b58277":"markdown","1e5b2568":"markdown","c4403038":"markdown","b182cd99":"markdown"},"source":{"0bdff6f6":"# General import\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras import layers\nfrom keras import Model\nfrom keras import Sequential\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.math import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\n\n# Import classification models\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n# from sklearn.metrics import ConfusionMatrixDisplay\n# from sklearn.metrics import confusion_matrix","1aa0968d":"def plot_loss_accuracy(history):\n    fig, axes = plt.subplots(1, 2, figsize=(10,5))\n    axes[0].plot(history.history['loss'], label='train')\n    axes[0].plot(history.history['val_loss'], label='test')\n    axes[0].legend()\n    axes[0].title.set_text('Loss')\n    axes[1].plot(history.history['accuracy'], label='train')\n    axes[1].plot(history.history['val_accuracy'], label='test')\n    axes[1].title.set_text('Accuracy')\n    axes[1].legend()\n\n    fig.show()\n    \n    \ndef plot_confusion_matrix(cm, target_classes):\n    cm_df = pd.DataFrame(\n        cm,\n        index = target_classes, \n        columns = target_classes)\n\n    figure = plt.figure(figsize=(5, 5))\n    sns.heatmap(cm_df, annot=True, cmap=plt.cm.Blues)\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","f7f372f8":"sonar = pd.read_csv('..\/input\/mines-vs-rocks\/sonar.all-data.csv', header=None)\n\n# There is some correlation between some variables, for so the dataset is\n# candidate for dimensionality reduction\n\nX = sonar.loc[:,:59]\ny = sonar.loc[:,60]\nle = preprocessing.LabelEncoder()\nle.fit(y)\ny = le.transform(y)\ntarget_names = le.classes_\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1)\n\nencoder = load_model('..\/input\/minesvsrocks-encoder\/encoder.h5')\n\n# Perform Feature Extraction\/Dimensionality Reduction with trained autoencoder\nX_encode = encoder.predict(X)\nX_train_encode = encoder.predict(X_train)\nX_test_encode = encoder.predict(X_test)\nn_obs, n_inputs = X_train_encode.shape","7d09f885":"models = [\n    ('LogReg', LogisticRegression()),\n    ('DT', tree.DecisionTreeClassifier()),\n    ('RF', RandomForestClassifier()),\n    ('KNN', KNeighborsClassifier()),\n    ('SVM', SVC(probability=True)),\n    ('GNB', GaussianNB()),\n    ('XGB', XGBClassifier(verbosity=0, use_label_encoder=False))\n]\n\ntarget_names = le.classes_\n\nscoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\nresults = []\nnames = []\nmodel_cv_results = []\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=1986)\n    cv_result = model_selection.cross_validate(model, X_train_encode, y_train,\n                                               cv=kfold, scoring=scoring)\n\n    results.append(cv_result)\n    names.append(name)\n\n    df = pd.DataFrame(cv_result)\n    df['model'] = name\n    model_cv_results.append(df)\n\nmodel_cv_results = pd.concat(model_cv_results, ignore_index=True)\nmodel_cv_results.groupby(['model']).mean()","feb61b30":"# Using EarlyStopper to avoid overfitting\ncallback = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)\n# Model a Binary classifier\nmodel_encoded = Sequential()\nmodel_encoded.add(layers.Dense(8, input_dim=n_inputs, activation='relu'))\nmodel_encoded.add(layers.Dense(5, activation='relu'))\nmodel_encoded.add(layers.Dense(3, activation='relu'))\nmodel_encoded.add(layers.Dense(1, activation='sigmoid'))\n# Define optimizer and loss function\nmodel_encoded.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Train the model\nhistory_encoded = model_encoded.fit(X_train_encode, y_train, epochs=1000, batch_size=n_obs,\n                                    validation_data=(X_test_encode, y_test), verbose=0,\n                                    callbacks=[callback])\n\nprint(f\"Validation accuracy: {np.max(history_encoded.history['val_accuracy'])}\")","d449cf7f":"plot_loss_accuracy(history_encoded)","5f907a17":"# Model a Binary classifier\nmodel_standard = Sequential()\nmodel_standard.add(layers.Dense(30, input_dim=60, activation='relu'))\nmodel_standard.add(layers.Dense(20, activation='relu'))\nmodel_standard.add(layers.Dense(10, activation='relu'))\nmodel_standard.add(layers.Dense(5, activation='relu'))\nmodel_standard.add(layers.Dense(3, activation='relu'))\nmodel_standard.add(layers.Dense(1, activation='sigmoid'))\n# Define optimizer and loss function\nmodel_standard.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# Train the model\nhistory_standard = model_standard.fit(X_train, y_train, epochs=1000, batch_size=n_obs,\n                                      validation_data=(X_test, y_test), verbose=0,\n                                      callbacks=[callback])\nprint(f\"Validation accuracy: {np.max(history_standard.history['val_accuracy'])}\")\nplot_loss_accuracy(history_standard)","a41ff11f":"yhat = model_encoded.predict(X_test_encode)\n# Map probablities to classes\nyhat = np.where(yhat > 0.5, 1, 0)\ncm = confusion_matrix(labels=y_test, predictions=yhat).numpy()\nplot_confusion_matrix(cm, target_names)\n","cf30cfe9":"# 3.2 Comparing the Results with a not Compressed Input Representation\n\nNow, let's train another model with the 60 non compressed inputs and compare the results. In this way, we'll see if the model accuracy is positively affected by the use of an autoencoder.","68b56288":"# 3.3 Confusion Matrix\n\nFinally let's plot a confusion matrix for the compressed model to evaluate the classification results.","4e489f36":"## 1.1 Let's define some helper functions","5049b201":"# 1. Setup\nInitializing useful libraries","34b58277":"# 3.Classical Machine Learning Models Training\nWe'll instatiate several Machine Learning models and train them to compare the results with a baseline.","1e5b2568":"# 3.1 Building the Neural Network for Binary Classification\n\nNow it's the time to build the classification neural network and train it.","c4403038":"# 2. Splitting the Dataset and Loading the Encoder\nWe'll load the encoder which was trained in the previous [notebook](https:\/\/www.kaggle.com\/augustodenevreze\/mines-vs-rocks-autoencoder-for-feat-extraction\/). For sake of simplicity, the encoder training was done in the aforementioned notebook. Should not be the case, the model training will take hours!\n\nWe'll generate the reduced dimensionality representation of the inputs (10 inputs instead of 60) with\n\n```X_encode = encoder.predict(X)```","b182cd99":"Now we can plot the training results with the validation set. A comment here, in this notebook and the other, we're using test and validation interchangeably. In both cases the dataset mentioned is a subset which has never been used for training. Sometimes, while dealing with classic ML algorithms the regular convention is to referto this subset as the test one (such is the name of the splitting sklearn method) but in deep learning nomenclature, the term used generally is validation. Since the testing subset is used for loss calculation during the model training."}}