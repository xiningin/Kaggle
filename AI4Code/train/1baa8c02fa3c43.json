{"cell_type":{"10dd4700":"code","2ac7ea22":"code","3485a997":"code","8b9153b2":"code","71bfd3aa":"code","6d9ed52f":"code","d499ae6f":"code","f4fc41c2":"code","00dc5da3":"code","f377d568":"code","ba454f98":"code","d1847f61":"code","f40c4fd7":"markdown","fe6aa8c0":"markdown","25aa388b":"markdown","ca14aad1":"markdown","3973e11d":"markdown","527e6ba3":"markdown","00f2a1d3":"markdown","0620c446":"markdown","f7a90d14":"markdown","cf4b04a2":"markdown","63bb0a5c":"markdown","67c14b2a":"markdown","b342648b":"markdown","00dafd7d":"markdown","5d0ab0b4":"markdown","583a3c7f":"markdown","887ddd83":"markdown","afca7bf4":"markdown","9b4f8364":"markdown","502f6c1c":"markdown","98814d29":"markdown","d70e74d3":"markdown","5299bc8f":"markdown","78c19a83":"markdown","28db5bc4":"markdown","9a6de9b8":"markdown","1e0c57f8":"markdown","d2cc4231":"markdown","9ba54cf8":"markdown","0b501709":"markdown","ca42db73":"markdown","2aa88f58":"markdown","bdd15798":"markdown","5fa20d0d":"markdown","84bf12db":"markdown","be80700d":"markdown","76bf63c7":"markdown","188d58dd":"markdown","4c126bfd":"markdown","a6e67f0f":"markdown","bcbc9fe7":"markdown","2ed36cad":"markdown","1ce4e80b":"markdown","06116b3a":"markdown","70f284ba":"markdown","a75fc7c1":"markdown","b5f42fde":"markdown","9bdf9dee":"markdown","5fa3bebd":"markdown","6c5ffd4e":"markdown","d2a49779":"markdown","2f4a8795":"markdown"},"source":{"10dd4700":"# import pandas for data wrangling\nimport pandas as pd\n\n\n# import numpy for Scientific computations\nimport numpy as np\n\n\n# import machine learning libraries\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\n\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","2ac7ea22":"data = '\/kaggle\/input\/wholesale-customers-data-set\/Wholesale customers data.csv'\n\ndf = pd.read_csv(data)","3485a997":"X = df.drop('Channel', axis=1)\n\ny = df['Channel']","8b9153b2":"X.head()","71bfd3aa":"y.head()","6d9ed52f":"# convert labels into binary values\n\ny[y == 2] = 0\n\ny[y == 1] = 1","d499ae6f":"# again preview the y label\n\ny.head()","f4fc41c2":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","00dc5da3":"space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 180,\n        'seed': 0\n    }","f377d568":"def objective(space):\n    clf=xgb.XGBClassifier(\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( X_train, y_train), ( X_test, y_test)]\n    \n    clf.fit(X_train, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(X_test)\n    accuracy = accuracy_score(y_test, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","ba454f98":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","d1847f61":"print(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)\n","f40c4fd7":"### **3.1 Import libraries** <a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Table of Contents](#0.1)","fe6aa8c0":"### **2.2.3 max_depth** <a class=\"anchor\" id=\"2.2.3\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **max_depth [default=6]**\n\n    - The maximum depth of a tree, same as GBM.\n    - It is used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n    - Increasing this value will make the model more complex and more likely to overfit. \n    - The value 0 is only accepted in lossguided growing policy when tree_method is set as hist and it indicates no limit on depth. \n    - We should be careful when setting large value of max_depth because XGBoost aggressively consumes memory when training a deep tree.\n    - range: [0,\u221e] (0 is only accepted in lossguided growing policy when tree_method is set as hist.\n    - Should be tuned using CV.\n    - Typical values: 3-10","25aa388b":"### **2.2.9 alpha** <a class=\"anchor\" id=\"2.2.9\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **alpha [default=0, alias: reg_alpha]**\n\n    - L1 regularization term on weights (analogous to Lasso regression).\n    - It can be used in case of very high dimensionality so that the algorithm runs faster when implemented.\n    - Increasing this value will make model more conservative.","ca14aad1":"### **3.4 Split data into separate training and test set** <a class=\"anchor\" id=\"3.4\"><\/a>\n\n[Table of Contents](#0.1)","3973e11d":"### **2.2.11 scale_pos_weight** <a class=\"anchor\" id=\"2.2.11\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **scale_pos_weight [default=1]**\n\n     - It controls the balance of positive and negative weights, \n     - It is useful for imbalanced classes. \n     - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n     - A typical value to consider: `sum(negative instances) \/ sum(positive instances)`.\n","527e6ba3":"# **4. Bayesian Optimization with HYPEROPT** <a class=\"anchor\" id=\"4\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **Bayesian optimization** is optimization or finding the best parameter for a machine learning or deep learning algorithm. \n\n- **Optimization** is the process of finding a minimum of cost function , that determines an overall better performance of a model on both train-set and test-set.\n\n- In this process, we train the model with various possible range of parameters until a best fit model is obtained. \n\n- **Hyperparameter tuning** helps in determining the optimal tuned parameters and return the best fit model, which is the best practice to follow while building an ML or DL model.\n\n- In this section, we discuss one of the most accurate and successful hyperparameter tuning method, which is **Bayesian Optimization with HYPEROPT**.\n\n- Please see my kernel [Bayesian Optimization using HYPEROPT](https:\/\/www.kaggle.com\/prashant111\/bayesian-optimization-using-hyperopt), for more information on the optimization process using HYPEROPT.\n\n- So, we will start with **HYPEROPT**.\n","00f2a1d3":"- Now, let's take a look at feature vector(X) and target variable(y).","0620c446":"### **4.3.1 Initialize domain space for range of values** <a class=\"anchor\" id=\"4.3.1\"><\/a>\n\n[Table of Contents](#0.1)","f7a90d14":"### **2.2.12 max_leaves** <a class=\"anchor\" id=\"2.2.11\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **max_leaves [default=0]**\n\n  - Maximum number of nodes to be added. \n  - Only relevant when `grow_policy=lossguide` is set.","cf4b04a2":"### **2.2.1 eta** <a class=\"anchor\" id=\"2.2.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **eta [default=0.3, alias: learning_rate]**\n\n  - It is analogous to learning rate in GBM.\n  - It is the step size shrinkage used in update to prevent overfitting. \n  - After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n  - It makes the model more robust by shrinking the weights on each step.\n  - range : [0,1]\n  - Typical final values : 0.01-0.2.\n\n","63bb0a5c":"### **4.3.3 Optimization algorithm** <a class=\"anchor\" id=\"4.3.3\"><\/a>\n\n[Table of Contents](#0.1)","67c14b2a":"- There are other hyperparameters like `sketch_eps`,`updater`, `refresh_leaf`, `process_type`, `grow_policy`, `max_bin`, `predictor` and `num_parallel_tree`.\n\n- For detailed discussion of these hyperparameters, please visit [Parameters for Tree Booster](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster)","b342648b":"There are other general parameters like **disable_default_eval_metric [default=0]**, **num_pbuffer [set automatically by XGBoost, no need to be set by user]** and **num_feature [set automatically by XGBoost, no need to be set by user]**.\n\nSo, these parameters are taken care by XGBoost algorithm itself. Hence,we will not discuss these further.","00dafd7d":"### **2.3.2 eval_metric** <a class=\"anchor\" id=\"2.3.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **eval_metric [default according to objective]**\n\n\n- The metric to be used for validation data.\n- The default values are **rmse for regression**, **error for classification** and **mean average precision for ranking**.\n- We can add multiple evaluation metrics.\n- Python users must pass the metrices as list of parameters pairs instead of map.\n- The most common values are given below -\n\n   - **rmse** : [root mean square error](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation)\n   - **mae** : [mean absolute error](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error)\n   - **logloss** : [negative log-likelihood](https:\/\/en.wikipedia.org\/wiki\/Likelihood_function#Log-likelihood)\n   - **error** : Binary classification error rate (0.5 threshold).  It is calculated as `#(wrong cases)\/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n   - **merror** : Multiclass classification error rate. It is calculated as `#(wrong cases)\/#(all cases)`.\n   - **mlogloss** : [Multiclass logloss](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.log_loss.html)\n   - **auc**: [Area under the curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic#Area_under_curve)\n   - **aucpr** : [Area under the PR curve](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall)\n","5d0ab0b4":"<a class=\"anchor\" id=\"0\"><\/a>\n# **A Guide on XGBoost hyperparameters tuning**\n\n\nHello friends,\n\n\nIn my previous kernel [XGBoost + k-fold CV + Feature Importance](https:\/\/www.kaggle.com\/prashant111\/xgboost-k-fold-cv-feature-importance), we have discussed XGBoost and develop a simple baseline XGBoost model. \n\n\nNow, XGBoost algorithm provides large range of hyperparameters. We should know how to tune these hyperparameters to improve and take full advantage of the XGBoost model.\n\n\nHence, in this kernel, we will discuss main hyperparameters of the XGBoost model and how to tune these hyperparameters.\n\n\nSo, let's get started.\n\n","583a3c7f":"### **2.3.1 objective** <a class=\"anchor\" id=\"2.3.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n\n- **objective [default=reg:squarederror]**\n\n- It defines the loss function to be minimized. Most commonly used values are given below -\n\n     - **reg:squarederror** : regression with squared loss.\n     \n     - **reg:squaredlogerror**: regression with squared log loss 1\/2[log(pred+1)\u2212log(label+1)]2.               - All input labels are required to be greater than -1. \n     \n     - **reg:logistic** : logistic regression\n     \n     - **binary:logistic** : logistic regression for binary classification, output probability\n     \n     - **binary:logitraw**: logistic regression for binary classification, output score before logistic transformation\n\n     - **binary:hinge** : hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities.\n     \n     - **multi:softmax** : set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)\n\n     - **multi:softprob** : same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.                            \n                             \n      \n      ","887ddd83":"## **4.3 Bayesian Optimization implementation** <a class=\"anchor\" id=\"4.3\"><\/a>\n\n[Table of Contents](#0.1)","afca7bf4":"### **3.3 Declare feature vector and target variable** <a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Table of Contents](#0.1)","9b4f8364":"The available hyperopt optimization algorithms are -\n\n- **hp.choice(label, options)** \u2014 Returns one of the options, which should be a list or tuple.\n\n- **hp.randint(label, upper)** \u2014 Returns a random integer between the range [0, upper).\n\n- **hp.uniform(label, low, high)** \u2014 Returns a value uniformly between low and high.\n\n- **hp.quniform(label, low, high, q)** \u2014 Returns a value round(uniform(low, high) \/ q) * q, i.e it rounds the decimal values and returns an integer.\n\n- **hp.normal(label, mean, std)** \u2014 Returns a real value that\u2019s normally-distributed with mean and standard deviation sigma.","502f6c1c":"- Here **best_hyperparams** gives us the optimal parameters that best fit model and better loss function value. \n\n- **trials** is an object that contains or stores all the relevant information such as hyperparameter, loss-functions for each set of parameters that the model has been trained. \n\n- **\u2018fmin\u2019** is an optimization function that minimizes the loss function and takes in 4 inputs - fn, space, algo and max_evals.\n\n- Algorithm used is **tpe.suggest**.","98814d29":"## **4.2 4 parts of Optimization Process** <a class=\"anchor\" id=\"4.2\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe optimization process consists of 4 parts which are as follows-\n\n\n- **1. Initialize domain space**\n\nThe domain space is the input values over which we want to search.\n\n\n- **2. Define objective function**\n  \nThe objective function can be any function which returns a real value that we want to minimize. In this case, we want to minimize the validation error of a machine learning model with respect to the hyperparameters. If the real value is accuracy, then we want to maximize it. Then the function should return the negative of that metric.\n\n\n- **3. Optimization algorithm**\n\nIt is the method used to construct the surrogate objective function and choose the next values to evaluate.\n\n\n- **4. Results**\n\nResults are score or value pairs that the algorithm uses to build the model.","d70e74d3":"- We can see that the y label contain values as 1 and 2.\n\n- We will need to convert it into 0 and 1 for further analysis.\n\n- We will do it as follows -","5299bc8f":"# **5. Results and Conclusion** <a class=\"anchor\" id=\"5\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- In this kernel, we have discussed the XGBoost hyperparameters which are divided into 3 categories - general parameters, booster parameters and learning task parameters.\n\n- We have discussed **Bayesian Optimization with HYPEROPT**.\n\n- We have discussed the 4 parts of optimization process.\n\n- We have found the best hyperparameters for the XGBoost ML model. \n\n- The same technique can be applied to find the optimum hyperparameters for any other ML model.","78c19a83":"# **1. What are hyperparameters** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- In this kernel, we will discuss the critical problem of hyperparameter tuning in XGBoost model.\n\n- **Hyperparameters** are certain values or weights that determine the learning process of an algorithm.\n\n- As stated earlier, XGBoost provides large range of hyperparameters. We can leverage the maximum power of XGBoost by tuning its hyperparameters.\n\n- The most powerful ML algorithm like XGBoost is famous for picking up patterns and regularities in the data by automatically tuning thousands of learnable parameters. \n\n- In tree-based models, like XGBoost the learnable parameters are the choice of decision variables at each node.\n\n- XGBoost is a very powerful algorithm. So, it will have more design decisions and hence large hyperparameters. These are parameters specified by hand to the algo and fixed throughout a training phase.\n\n- In tree-based models, hyperparameters include things like the maximum depth of the tree, the number of trees to grow, the number of variables to consider when building each tree, the minimum number of samples on a leaf and the fraction of observations used to build a tree.\n\n- Although we focus on optimizing XGBoost hyperparameters in this kernel, the concepts discussed in this kernel applies to any other advanced ML algorithm as well.\n","28db5bc4":"### **2.3.3 seed** <a class=\"anchor\" id=\"2.3.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **seed [default=0]**\n\n  - The random number seed.\n  - This parameter is ignored in R package, use set.seed() instead.\n  - It can be used for generating reproducible results and also for parameter tuning.","9a6de9b8":"### **4.3.4 Print Results** <a class=\"anchor\" id=\"4.3.4\"><\/a>\n\n[Table of Contents](#0.1)","1e0c57f8":"### **2.2.8 lambda** <a class=\"anchor\" id=\"2.2.8\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **lambda [default=1, alias: reg_lambda]**\n\n    - L2 regularization term on weights  (analogous to Ridge regression).\n    - This is used to handle the regularization part of XGBoost. \n    - Increasing this value will make model more conservative.","d2cc4231":"## **2.2 Booster Parameters** <a class=\"anchor\" id=\"2.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- We have 2 types of boosters - **tree booster** and **linear booster**.\n- We will limit our discussion to **tree booster** because it always outperforms the **linear booster** and thus the later is rarely used.\n- Please visit, [Parameters for Tree Booster](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster), for detailed discussion on booster parameters.\n","9ba54cf8":"# **3. Basic Setup** <a class=\"anchor\" id=\"3\"><\/a>\n\n[Table of Contents](#0.1)","0b501709":"### **2.1.2 verbosity** <a class=\"anchor\" id=\"2.1.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **verbosity[default = 1]**\n\n    - Verbosity of printing messages. \n    - Valid values are 0 (silent), 1 (warning), 2 (info), 3 (debug).","ca42db73":"### **2.2.6 subsample** <a class=\"anchor\" id=\"2.2.6\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **subsample [default=1]**\n\n   - It denotes the fraction of observations to be randomly samples for each tree.\n   - Subsample ratio of the training instances. \n   - Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees.      - This will prevent overfitting. \n   - Subsampling will occur once in every boosting iteration.\n   - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n   - Typical values: 0.5-1\n   - range: (0,1]","2aa88f58":"We can see that our target variable (y) has been converted into 0 and 1.","bdd15798":"### **2.1.1 booster** <a class=\"anchor\" id=\"2.1.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **booster[default = gbtree]**\n\n   - **booster** parameter helps us to choose which booster to use.\n   - It helps us to select the type of model to run at each iteration. \n   - It has 3 options - **gbtree**, **gblinear** or **dart**.\n   \n       - **gbtree** and **dart** - use tree-based models, while\n       - **gblinear** uses linear models. \n   ","5fa20d0d":"I will skip the EDA part, as I have done it in previous kernel - [XGBoost + k-fold CV + Feature Importance](https:\/\/www.kaggle.com\/prashant111\/xgboost-k-fold-cv-feature-importance).","84bf12db":"# **6. References** <a class=\"anchor\" id=\"6\"><\/a>\n\n[Table of Contents](#0.1)\n\nThe ideas and concepts in this kernel are taken from the following websites.\n\n\n-\thttps:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/param_tuning.html\n\n\n-\thttps:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#general-parameters\n\n\n-\thttps:\/\/medium.com\/analytics-vidhya\/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9\n\n\n-   https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n\n\n-\thttps:\/\/www.kaggle.com\/yassinealouini\/hyperopt-the-xgboost-model\n","be80700d":"# **Table of Contents** <a class=\"anchor\" id=\"0.1\"><\/a>\n\n\n- 1 [What are hyperparameters](#1)\n- 2 [XGBoost hyperparameters](#2)\n   - 2.1 [General Parameters](#2.1)\n      - 2.1.1 [booster](#2.1.1)\n      - 2.1.2 [verbosity](#2.1.2)\n      - 2.1.3 [nthread](#2.1.3)\n   - 2.2 [Booster Parameters](#2.2)\n      - 2.2.1 [eta](#2.2.1)\n      - 2.2.2 [gamma](#2.2.2)\n      - 2.2.3 [max_depth](#2.2.3)\n      - 2.2.4 [min_child_weight](#2.2.4)\n      - 2.2.5 [max_delta_step](#2.2.5)\n      - 2.2.6 [subsample](#2.2.6)\n      - 2.2.7 [colsample_bytree, colsample_bylevel, colsample_bynode](#2.2.7) \n      - 2.2.8 [lambda](#2.2.8)\n      - 2.2.9 [alpha](#2.2.9)\n      - 2.2.10 [tree_method](#2.2.10)\n      - 2.2.11 [scale_pos_weight](#2.2.11)\n      - 2.2.12 [max_leaves](#2.2.12)\n   - 2.3 [Learning Task Parameters](#2.3)\n      - 2.3.1 [objective](#2.3.1)\n      - 2.3.2 [eval_metric](#2.3.2)\n      - 2.3.3 [seed](#2.3.3)\n- 3 [Basic Setup](#3)\n   - 3.1 [Import libraries](#3.1)\n   - 3.2 [Read dataset](#3.2)\n   - 3.3 [Declare feature vector and target variable](#3.3)\n   - 3.4 [Split data into separate training and test set](#3.4)\n- 4 [Bayesian Optimization with HYPEROPT](#4)\n   - 4.1 [What is HYPEROPT](#4.1)\n   - 4.2 [4 Parts of Optimization Process](#4.2)\n   - 4.3 [Bayesian Optimization Implementation](#4.3)\n      - 4.3.1 [Initialize domain space for range of values](#4.3.1)\n      - 4.3.2 [Define objective function](#4.3.2)\n      - 4.3.3 [Optimization algorithm](#4.3.3)\n      - 4.3.4 [Print Results](#4.3.4)\n- 5 [Results and Conclusion](#5)\n- 6 [References](#6)\n\n\n\n\n","76bf63c7":"[Go to Top](#0)","188d58dd":"**If this helped in your learning, then please <font color=\"red\"><b>UPVOTE<\/b><\/font>  \u2013 as they are the source of motivation!**\n\n**Happy Learning**\n","4c126bfd":"### **2.2.7 colsample_bytree, colsample_bylevel, colsample_bynode** <a class=\"anchor\" id=\"2.2.7\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **colsample_bytree, colsample_bylevel, colsample_bynode [default=1]**\n\n   - This is a family of parameters for subsampling of columns.\n\n   - All **colsample_by** parameters have a range of (0, 1], the default value of 1, and specify the fraction of columns to be subsampled.\n\n   - **colsample_bytree** is the subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n\n   - **colsample_bylevel** is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n\n   - **colsample_bynode** is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level.\n\n   - **colsample_by*** parameters work cumulatively. For instance, the combination **{'colsample_bytree':0.5, 'colsample_bylevel':0.5, 'colsample_bynode':0.5}** with 64 features will leave 8 features to choose from at each split.\n","a6e67f0f":"### **2.2.5 max_delta_step** <a class=\"anchor\" id=\"2.2.5\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **max_delta_step [default=0]**\n\n   - In maximum delta step we allow each tree\u2019s weight estimation to be. \n   - If the value is set to 0, it means there is no constraint. \n   - If it is set to a positive value, it can help making the update step more conservative.\n   - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n   - Set it to value of 1-10 might help control the update.\n   - range: [0,\u221e]\n\n\n","bcbc9fe7":"- The above result give best set of hyperparameters.\n\n","2ed36cad":"## **4.1 What is HYPEROPT** <a class=\"anchor\" id=\"4.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **HYPEROPT** is a powerful python library that search through an hyperparameter space of values and find the best possible values that yield the minimum of the loss function. \n\n- Bayesian Optimization technique uses Hyperopt to tune the model hyperparameters. Hyperopt is a Python library which is used to tune model hyperparameters.\n\n- More information on Hyperopt can be found at the following link:-\n\nhttps:\/\/hyperopt.github.io\/hyperopt\/?source=post_page\n  \n","1ce4e80b":"### **2.2.2 gamma** <a class=\"anchor\" id=\"2.2.2\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **gamma [default=0, alias: min_split_loss]**\n\n   - A node is split only when the resulting split gives a positive reduction in the loss function. \n   - Gamma specifies the minimum loss reduction required to make a split.\n   - It makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n   - The larger gamma is, the more conservative the algorithm will be.\n   - Range: [0,\u221e]","06116b3a":"## **2.3 Learning Task Parameters** <a class=\"anchor\" id=\"2.3\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- These parameters are used to define the optimization objective the metric to be calculated at each step.\n\n- They are used to specify the learning task and the corresponding learning objective. The objective options are below:\n\n","70f284ba":"### **2.2.10 tree_method** <a class=\"anchor\" id=\"2.2.10\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **tree_method string [default= auto]**\n\n   - The tree construction algorithm used in XGBoost. \n\n   - XGBoost supports `approx`, `hist` and `gpu_hist` for distributed training. Experimental support for external memory is available for `approx` and `gpu_hist`.\n\n   - Choices: `auto`, `exact`, `approx`, `hist`, `gpu_hist`\n\n      - **auto**: Use heuristic to choose the fastest method.\n         - For small to medium dataset, exact greedy (exact) will be used.\n\n         - For very large dataset, approximate algorithm (approx) will be chosen.\n\n         - Because old behavior is always use exact greedy in single machine, user will get a message when approximate algorithm is chosen to notify this choice.\n\n     - **exact**: Exact greedy algorithm.\n\n     - **approx**: Approximate greedy algorithm using quantile sketch and gradient histogram.\n\n     - **hist**: Fast histogram optimized approximate greedy algorithm. It uses some performance improvements such as bins caching.\n\n     - **gpu_hist**: GPU implementation of hist algorithm.","a75fc7c1":"So, now we will come to the end of this kernel.\n\nI hope you find this kernel useful and enjoyable.\n\nYour comments and feedback are most welcome.\n\nThank you\n","b5f42fde":"# **2. XGBoost hyperparameters** <a class=\"anchor\" id=\"2\"><\/a>\n\n[Table of Contents](#0.1)\n\n- Generally, the XGBoost hyperparameters have been divided into 4 categories. They are as follows -\n\n  - 1. General parameters\n  - 2. Booster parameters\n  - 3. Learning task parameters\n  - 4. Command line parameters\n \n- Before running a XGBoost model, we must set three types of parameters - **general parameters**, **booster parameters** and **task parameters**.\n\n- The fourth type of parameters are **command line parameters**. They are only used in the console version of XGBoost. So, we will skip these parameters and limit our discussion to the first three type of parameters.","9bdf9dee":"### **2.1.3 nthread** <a class=\"anchor\" id=\"2.1.3\"><\/a>\n\n[Table of Contents](#0.1)\n\n- **nthread [default = maximum number of threads available if not set]**\n\n   - This is number of parallel threads used to run XGBoost.\n   - This is used for parallel processing and number of cores in the system should be entered.\n   - If you wish to run on all cores, value should not be entered and algorithm will detect automatically.","5fa3bebd":"## **2.1 General Parameters** <a class=\"anchor\" id=\"2.1\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- These parameters guide the overall functioning of the XGBoost model. \n\n- In this section, we will discuss three hyperparameters - **booster**, **verbosity** and **nthread**.\n\n- Please visit [XGBoost General Parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#general-parameters) for detailed discussion on general parameters.","6c5ffd4e":"### **2.2.4 min_child_weight** <a class=\"anchor\" id=\"2.2.4\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **min_child_weight [default=1]**\n\n   - It defines the minimum sum of weights of all observations required in a child.\n   - This is similar to min_child_leaf in GBM but not exactly. This refers to min \u201csum of weights\u201d of observations while GBM has min \u201cnumber of observations\u201d.\n   - It is used to control over-fitting. \n   - Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n   - Too high values can lead to under-fitting. \n   - Hence, it should be tuned using CV.\n   - The larger min_child_weight is, the more conservative the algorithm will be.\n   - range: [0,\u221e]","d2a49779":"### **4.3.2 Define objective function** <a class=\"anchor\" id=\"4.3.2\"><\/a>\n\n[Table of Contents](#0.1)","2f4a8795":"### **3.2 Read dataset** <a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Table of Contents](#0.1)"}}