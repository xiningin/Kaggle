{"cell_type":{"cee3c88d":"code","a5b5ff45":"code","666dca1f":"code","327b9766":"code","eddde5b1":"code","bc9ea708":"code","25931394":"code","4b72e0a9":"code","5140107e":"code","11fec16e":"code","812f242c":"code","e1ade1d0":"code","ce0e0b4d":"code","75b70dee":"code","3acff782":"code","43156c9a":"code","7c4e21f5":"code","ff8fa64b":"code","6ca36635":"code","4ecb289a":"code","0b9f1344":"code","f938dedd":"code","42d67b3d":"code","49eab543":"code","e8e2de81":"code","67c38a58":"code","7d5b0406":"code","42e08d2a":"code","2b829040":"code","63abe982":"code","4b9014da":"markdown","f52d7ccc":"markdown","b1a31d37":"markdown"},"source":{"cee3c88d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport random\nfrom pprint import pprint","a5b5ff45":"from sklearn.datasets import load_iris\ndata = load_iris()","666dca1f":"df = pd.DataFrame(data.data)\ndf['label'] = data.target\ndf.head()","327b9766":"df.info()","eddde5b1":"def train_test_split(df, test_size=0.2):\n    test_size = round(test_size * len(df))\n    idxs = df.index.tolist()\n    test_idxs = random.sample(idxs, test_size)\n    test_df = df.loc[test_idxs]\n    train_df = df.drop(test_idxs)\n    \n    return train_df, test_df","bc9ea708":"random.seed(0)\ntrain_df, test_df = train_test_split(df)","25931394":"print(len(train_df))\nprint(len(test_df))","4b72e0a9":"data = train_df.values\ndata[:5]","5140107e":"def check_purity(data):\n    if len(np.unique(data[:,-1])) == 1:\n        return True\n    else:\n        return False","11fec16e":"check_purity(data)","812f242c":"def classify_data(data):\n    labels = data[:,-1]\n    unique_lbl, label_counts = np.unique(labels, return_counts = True)\n    idx = label_counts.argmax()\n    clf = unique_lbl[idx]\n    \n    return clf","e1ade1d0":"def get_potential_splits(data):\n    potential_splits = {}\n    rows, cols = data.shape\n    for col in range(cols - 1):\n        potential_splits[col] = []\n        values = data[:, col]\n        unique_vals = np.unique(values)\n        \n        for i in range(1, len(unique_vals)):\n            curr, prev = unique_vals[i], unique_vals[i-1]\n            potential_splits[col].append((curr+prev)\/2)\n    \n    return potential_splits","ce0e0b4d":"potential_splits = get_potential_splits(data)","75b70dee":"sns.scatterplot(data=train_df, x=3, y=2, hue='label')\nplt.vlines(potential_splits[3],1,7, 'grey')\nplt.hlines(potential_splits[2],0,2.5,'grey')","3acff782":"def split_data(data, split_col, split_val):\n    splitcol_vals = data[:, split_col]\n    below = data[splitcol_vals <= split_val]\n    above = data[splitcol_vals > split_val]\n    \n    return below, above","43156c9a":"def calc_entropy(data):\n    _, counts = np.unique(data[:, -1], return_counts = True)\n    ps = counts \/ counts.sum()\n    entropy = -sum(ps * np.log2(ps))\n    \n    return entropy","7c4e21f5":"def information_gain(below, above):\n    total = len(below) + len(above)\n    bel_frac = len(below)\/total; abo_frac = len(above)\/total\n    info_gain = 1 - bel_frac*calc_entropy(below) + abo_frac*calc_entropy(above)\n    return info_gain","ff8fa64b":"def get_best_split(data, potential_splits):\n    info_gain = 9999\n    for col in potential_splits:\n        for val in potential_splits[col]:\n            below, above = split_data(data, col, val)\n            curr_info_gain = information_gain(below, above)\n            \n            if curr_info_gain <= info_gain:\n                info_gain = curr_info_gain\n                best_split_col = col; best_split_val = val\n    \n    return best_split_col, best_split_val","6ca36635":"def decision_tree(df, count=0):\n    if count == 0:\n        data = df.values\n    else:\n        data = df\n    \n    # base criterion\n    if check_purity(data):\n        clf = classify_data(data)\n        return clf\n    # recursive criterion\n    else:\n        count += 1\n        \n        potential_splits = get_potential_splits(data)\n        best_split_col, best_split_val = get_best_split(data, potential_splits)\n        below, above = split_data(data, best_split_col, best_split_val)\n        \n        # sub-tree\n        criteria = \"{} <= {}\".format(best_split_col, best_split_val)\n        tree = {criteria: []}\n        \n        ans_below = decision_tree(below, count)\n        ans_above = decision_tree(above, count)\n        \n        tree[criteria].append(ans_below)\n        tree[criteria].append(ans_above)\n        \n        return tree","4ecb289a":"tree = decision_tree(train_df)\npprint(tree)","0b9f1344":"def decision_tree_pruned(df, count=0, min_samples=2, max_depth=5):\n    if count == 0:\n        data = df.values\n    else:\n        data = df\n    \n    # base criterion\n    if check_purity(data) or len(data)<min_samples or count == max_depth:\n        clf = classify_data(data)\n        return clf\n    # recursive criterion\n    else:\n        count += 1\n        \n        potential_splits = get_potential_splits(data)\n        best_split_col, best_split_val = get_best_split(data, potential_splits)\n        below, above = split_data(data, best_split_col, best_split_val)\n        \n        # sub-tree\n        criteria = \"{} <= {}\".format(best_split_col, best_split_val)\n        tree = {criteria: []}\n        \n        ans_below = decision_tree_pruned(below, count, min_samples, max_depth)\n        ans_above = decision_tree_pruned(above, count, min_samples, max_depth)\n        \n        if ans_below == ans_above:\n            tree = ans_below\n        else:\n            tree[criteria].append(ans_below)\n            tree[criteria].append(ans_above)\n        \n        return tree","f938dedd":"tree = decision_tree_pruned(train_df)\npprint(tree)","42d67b3d":"from sklearn.datasets import load_boston\nraw = load_boston()\ndf = pd.DataFrame(raw.data)\ndf['label'] = raw.target\ndf.head()","49eab543":"random.seed(0)\ntrain_df, test_df = train_test_split(df)","e8e2de81":"tree = decision_tree_pruned(train_df)\npprint(tree)","67c38a58":"from sklearn.datasets import load_iris\ndata = load_boston()","7d5b0406":"df = pd.DataFrame(data.data)\ndf['label'] = data.target\ndf.head()","42e08d2a":"df.info()","2b829040":"train_df, test_df = train_test_split(df, 0.2)","63abe982":"tree = decision_tree_pruned(train_df)\npprint(tree)","4b9014da":"## Decision Tree Algorithm","f52d7ccc":"## Decision Tree Formulation","b1a31d37":"The objective is to have a method to solve tabular decision problems in 2 lines of code, provided that the data is in the form of a python dataframe and does not have any missing values.\n1. train_df, test_df = train_test_split(df, 0.2);\n2. tree = decision_tree(train_df);"}}