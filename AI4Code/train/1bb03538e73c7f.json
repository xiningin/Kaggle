{"cell_type":{"ac7a512a":"code","102e88da":"code","a4477182":"code","7ed95697":"code","8781bf7e":"code","8e5d0c2b":"code","20a7ad1b":"code","98b7c53b":"code","cab1a831":"code","4fc80e5a":"code","6442517f":"code","7eb93183":"code","2a685fa2":"code","36a16943":"code","6b1279b9":"code","13107ece":"code","bec6d261":"code","fafa7249":"code","d1a6bc07":"code","ca405043":"code","efc64adb":"code","422a27f5":"code","710b9b03":"code","c514aa11":"code","a73f6fba":"code","bd8f7cc0":"code","4ec1725e":"code","731b98fe":"code","73482575":"code","b297c32b":"code","2507db83":"code","452242f6":"code","69e2a123":"code","57e5d2e1":"code","492f8356":"code","8692add8":"code","7b95a145":"code","c98b43c0":"code","6c5fd17e":"code","775fccc5":"code","c878899e":"code","1e0f06fc":"code","184ca09e":"code","088d6e5f":"code","9f12a58a":"code","bd88febf":"code","d87a7366":"code","d5934b55":"code","0031e33e":"code","4c53526f":"code","65a53fb4":"code","89b26ffe":"code","7cf15e3a":"code","789125a5":"code","e0803f01":"code","cea1849b":"code","efd5040c":"code","124b3c6e":"markdown","1b6c9e71":"markdown","ef1bf6eb":"markdown","c872b335":"markdown","073327e1":"markdown","b7449b3b":"markdown","ae85396f":"markdown","82a1af39":"markdown","5fe494bb":"markdown","6a92d5ee":"markdown","48d6fe1a":"markdown","fda82d1e":"markdown","15ed1004":"markdown","047b9c70":"markdown","0984b8b1":"markdown"},"source":{"ac7a512a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","102e88da":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","a4477182":"data=pd.read_csv('\/kaggle\/input\/vehicle-dataset-from-cardekho\/car data.csv', encoding='latin-1')","7ed95697":"data.head()","8781bf7e":"data.shape","8e5d0c2b":"data.columns","20a7ad1b":"data.describe()","98b7c53b":"%matplotlib inline\ndata.hist(bins=50, figsize=(30,15))\nplt.show()","cab1a831":"data.info()","4fc80e5a":"data.isnull().sum()","6442517f":"data.nunique()","7eb93183":"data.corr()","2a685fa2":"#At time of reselling a product, how old the product is a main factor\ndata['Car_Age'] = 2021 - data.Year","36a16943":"data.shape","6b1279b9":"data.info()","13107ece":"data.nunique()","bec6d261":"#Owner seems to be categorical value, change its data type\ndata['Owner']=data['Owner'].astype(object)","fafa7249":"#We will drop Year attribute as well since as long as we know the age we don't require year\ndata.drop(['Year'],inplace=True, axis=1)\n#here we are going to remove car_name as we will not require if for the analysis\ndata.drop(['Car_Name'], axis=1, inplace=True)","d1a6bc07":"data.shape","ca405043":"#plot outliers for each numeric data\nsns.boxplot(x=data['Selling_Price'])","efc64adb":"sns.boxplot(x=data['Present_Price'])","422a27f5":"sns.boxplot(x=data['Kms_Driven'])","710b9b03":"sns.boxplot(x=data['Car_Age'])","c514aa11":"#Handling outliers\n\ndef cap_data(data2):\n    for col in data2.columns:\n        print(\"capping the \",col)\n        if (((data2[col].dtype)=='float64') | ((data2[col].dtype)=='int64')):\n            Q1 = data2.quantile(0.25)\n            Q3 = data2.quantile(0.75)\n            IQR = Q3 - Q1\n            data2 = data2[~((data2 < (Q1 - 1.5 * IQR)) |(data2 > (Q3 + 1.5 * IQR))).any(axis=1)]\n        else:\n            data2[col]=data2[col]\n    return data2\ndata=cap_data(data)","a73f6fba":"sns.boxplot(x=data['Car_Age'])","bd8f7cc0":"sns.boxplot(x=data['Kms_Driven'])","4ec1725e":"sns.boxplot(x=data['Selling_Price'])","731b98fe":"#one hot encoding for categorical data\ndata = pd.get_dummies(data, drop_first=True)\ndata.head()","73482575":"data.shape","b297c32b":"data.corr()","2507db83":"corelation=data.corr()\nsns.heatmap(corelation,xticklabels=corelation.columns,yticklabels=corelation.columns,annot=True)","452242f6":"data['present_price_cat']=pd.cut(data['Present_Price'],bins=[0,2.0,4.0,6.0,np.inf],labels=[1,2,3,4])\ndata['present_price_cat'].hist()","69e2a123":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit=StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=42)\nfor train_index, test_index in split.split(data,data['present_price_cat']):\n    strat_train_set=data.iloc[train_index]\n    strat_test_set=data.iloc[test_index]","57e5d2e1":"strat_test_set[\"present_price_cat\"].value_counts()\/len(strat_test_set)","492f8356":"data[\"present_price_cat\"].value_counts()\/len(data)","8692add8":"strat_train_set[\"present_price_cat\"].value_counts()\/len(strat_train_set)","7b95a145":"y_train=strat_train_set['Selling_Price']","c98b43c0":"y_train.head()","6c5fd17e":"strat_train_set.drop(['present_price_cat', 'Selling_Price'],inplace=True,axis=1)\nX_train=strat_train_set\nX_train.head()","775fccc5":"y_test=strat_test_set['Selling_Price']","c878899e":"y_test.head()","1e0f06fc":"strat_test_set.drop(['present_price_cat', 'Selling_Price'],inplace=True,axis=1)\nX_test=strat_test_set\nX_test.head()","184ca09e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nlin_reg=LinearRegression()\nlin_reg.fit(X_train,y_train)\n\ny_pred=lin_reg.predict(X_train)\nlin_mse=mean_squared_error(y_train,y_pred)\nlin_rmse=np.sqrt(lin_mse)\nlin_rmse","088d6e5f":"\nscores=cross_val_score(lin_reg,X_train,y_train,scoring=\"neg_mean_squared_error\",cv=10)\nlin_rmse_scores=np.sqrt(-scores)\n\ndef display_scores(scores):\n    print(\"Scores:\",scores)\n    print(\"Mean:\",scores.mean())\n    print(\"Standard deviation:\",scores.std())\n\ndisplay_scores(lin_rmse_scores)\n","9f12a58a":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg=DecisionTreeRegressor()\ntree_reg.fit(X_train,y_train)\n\n\ny_pred=tree_reg.predict(X_train)\ntree_mse=mean_squared_error(y_train,y_pred)\ntree_rmse=np.sqrt(tree_mse)\ntree_rmse\n","bd88febf":"\nscores=cross_val_score(tree_reg,X_train,y_train,scoring=\"neg_mean_squared_error\",cv=10)\ntree_rmse_scores=np.sqrt(-scores)\n\n    \ndisplay_scores(tree_rmse_scores)","d87a7366":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg=RandomForestRegressor()\nforest_reg.fit(X_train,y_train)\n\n\ny_pred=forest_reg.predict(X_train)\nforest_mse=mean_squared_error(y_train,y_pred)\nforest_rmse=np.sqrt(forest_mse)\nforest_rmse\n","d5934b55":"scores=cross_val_score(forest_reg,X_train,y_train,scoring=\"neg_mean_squared_error\",cv=10)\nforest_rmse_scores=np.sqrt(-scores)\n\ndisplay_scores(forest_rmse_scores)","0031e33e":"from sklearn.model_selection import GridSearchCV\n\nparam_grid=[\n    {'n_estimators':[3,10,30],'max_features':[2,4,6,8]},\n    {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]},\n]\n\nforest_reg=RandomForestRegressor()\n\ngrid_search=GridSearchCV(forest_reg,param_grid,cv=5,scoring='neg_mean_squared_error',return_train_score=True)\ngrid_search.fit(X_train,y_train)","4c53526f":"grid_search.best_params_","65a53fb4":"grid_search.best_estimator_","89b26ffe":"cvers=grid_search.cv_results_\n\nfor mean_score, params in zip(cvers[\"mean_test_score\"],cvers[\"params\"]):\n    print(np.sqrt(-mean_score),params)","7cf15e3a":"final_model=grid_search.best_estimator_\nfinal_pred_tuning=final_model.predict(X_test)\nfinal_mse_tuning=mean_squared_error(y_test,final_pred_tuning)\nfinal_rmse_tuning=np.sqrt(final_mse_tuning)","789125a5":"final_rmse_tuning","e0803f01":"from sklearn.metrics import r2_score\nprint(\"Linear Regression R_Square Score: \" ,r2_score(y_test,final_pred_tuning))","cea1849b":"sns.distplot(y_test-final_pred_tuning)","efd5040c":"plt.scatter(y_test,final_pred_tuning)","124b3c6e":"# Importing libraries and loading the data in pandas dataframe\n","1b6c9e71":"# Stratified division of data \nWe can see from the correlation matrics that present price is heavly co-related to selling price\nwe will go for stratified sampling,the population is divided into homogeneous sub-groups called strata, and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population","ef1bf6eb":"Checking if he data is split in the same proportion in train, test ","c872b335":"# Evaluating the model with rmse, r2 and graphs","073327e1":"# Exploratory Data Analysis","b7449b3b":"# Model 3:Random Forest \n\n1. Importing DecisionTreeRegressor model and fit the training data on it\n2. Check the model suitability using mean_squared_error\n3. Using cross-validation to evaluate the model more precisely","ae85396f":"1. RandomForestRegressor seems to be promising model, we will fine tune it get the best resulta\n2. Use GridSearchCV to test the model with various parameters\n3. grid_search.best_params_ gives you the best parameters for which model is performing the best\n4. cvers=grid_search.cv_results_ shows the output for all the combinations of parameter","82a1af39":"\n\n\n1. Look at the first few observations of the data-data.head()\n2. Get the no. of observations and no. of features(row x columns)-data.shape\n3. Get the features list-data.columns\n4. Get spread across idea of the numerical data type-data.describe()\n5. Using histogram, spread of the values for numeric variables\n6. Get the different data type present-data.info()\n7. Check for any missing values-data.isnull().sum()\n8. Check unique values(gives idea about categorical values)-data.nunique()\n9. Check corr-data.corr()","5fe494bb":"# Model 1:Linear Regression\n1. Importing Linear Regression model and fit the training data on it\n2. Check the model suitability using mean_squared_error\n3. Using cross-validation to evaluate the model more precisely","6a92d5ee":"\n1. dropping, adding, updating, modifying features\n2. Identifying outliers for numerical datatypes\n3. Handling outliers-Feature Scaling\n4. Check if the outliers are tackled\n5. Handling Categorical data using One-Hot Encoding\n6. Graphical Represnetation of corr","48d6fe1a":"# Splitting the datasets into Training and Testing sets\n\n","fda82d1e":"# Preprocessing Data","15ed1004":"# Model 2: Decision Tree\n\n1. Importing DecisionTreeRegressor model and fit the training data on it\n2. Check the model suitability using mean_squared_error\n3. Using cross-validation to evaluate the model more precisely","047b9c70":"# Testing the model on testing data","0984b8b1":"# Tuning the model to get the best results\n"}}