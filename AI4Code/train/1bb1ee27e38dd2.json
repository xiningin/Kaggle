{"cell_type":{"38bebd55":"code","5621f56d":"code","5de60298":"code","469054f3":"code","4af70b39":"code","e33486e3":"code","445d07d8":"code","aa1a0452":"code","ce2ac7e5":"code","f2ea313f":"code","99d7047e":"code","12393872":"code","97d235c4":"code","6ce50d0c":"code","5f0dcf85":"code","d3e2eb1d":"markdown","f9448fa7":"markdown","cd813710":"markdown","6c3c5b34":"markdown"},"source":{"38bebd55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5621f56d":"from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\n","5de60298":"df_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\n","469054f3":"#simmple Encoding\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\nfinal_test_preds = []\nfinal_valid_preds = {}\npreds = []\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n    \n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[objects_col])\n    X_valid_ohe = ohe.transform(X_valid[objects_col])\n    X_test_ohe = ohe.transform(X_test[objects_col])\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train, X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid, X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test, X_test_ohe], axis=1)\n    \n    # this part is missing in the video:\n    X_train = X_train.drop(objects_col, axis=1)\n    X_valid = X_valid.drop(objects_col, axis=1)\n    X_test = X_test.drop(objects_col, axis=1)\n    \n    scaler = preprocessing.StandardScaler(with_mean=True)\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n   \n    params = params = {'n_estimators': 8000, 'max_depth': 3, 'learning_rate': 0.03628302216953097, \n              'gamma': 0, 'min_child_weight': 1, 'subsample': 0.7875490025178415, \n              'colsample_bytree': 0.11807135201147481, 'reg_alpha': 23.13181079976304, \n              'reg_lambda': 0.0008746338866473539}\n    \n    model = XGBRegressor(**params, objective ='reg:squarederror',\n                         max_delta_step=0.7, random_state=42,\n                         tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    \n    model.fit(X_train, y_train, early_stopping_rounds=300, eval_set=[(X_valid, y_valid)], verbose=1000)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n    \nprint(np.mean(preds), np.std(preds))\nfinal_valid_preds = pd.DataFrame.from_dict(final_valid_preds, orient=\"index\").reset_index()\nfinal_valid_preds.columns = [\"id\", \"models_pred_1\"]\nfinal_valid_preds.to_csv(\"train_pred_1.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_preds), axis=1)\nsub_df.columns = ['id', 'models_pred_1']\nsub_df.to_csv(\"test_pred_1.csv\", index=False)","4af70b39":"#polynomial\ndf_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\n\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\npoly = preprocessing.PolynomialFeatures(degree=3, interaction_only=True, include_bias=False)\ntrain_poly = poly.fit_transform(df_fold[numerical_cols])\ntest_poly = poly.transform(test_df[numerical_cols])\n\ndf_poly = pd.DataFrame(train_poly, columns=[f\"poly_{i}\" for i in range(train_poly.shape[1])])\ndf_test_poly = pd.DataFrame(test_poly, columns=[f\"poly_{i}\" for i in range(test_poly.shape[1])])\n\ndf_fold = pd.concat([df_fold, df_poly], axis=1)\ntest_df = pd.concat([test_df, df_test_poly], axis=1)\n\n\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\n# numerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\nfinal_test_preds = []\nfinal_valid_preds = {}\npreds = []\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n    \n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    X_train[objects_col] = ordinal_encoder.fit_transform(X_train[objects_col])\n    X_valid[objects_col] = ordinal_encoder.transform(X_valid[objects_col])\n    X_test[objects_col] = ordinal_encoder.transform(X_test[objects_col])\n    \n    scaler = preprocessing.StandardScaler(with_mean=True)\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n   \n    params = params = {'n_estimators': 10000, 'max_depth': 3, 'learning_rate': 0.03628302216953097, \n              'gamma': 0, 'min_child_weight': 1, 'subsample': 0.7875490025178415, \n              'colsample_bytree': 0.11807135201147481, 'reg_alpha': 23.13181079976304, \n              'reg_lambda': 0.0008746338866473539}\n\n    model = XGBRegressor(**params, objective ='reg:squarederror',\n                         max_delta_step=0.7, random_state=42,\n                         tree_method='gpu_hist', gpu_id=0, predictor=\"gpu_predictor\")\n    \n    model.fit(X_train, y_train, early_stopping_rounds=300, eval_set=[(X_valid, y_valid)], verbose=1000)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n    \nprint(np.mean(preds), np.std(preds))\nfinal_valid_preds = pd.DataFrame.from_dict(final_valid_preds, orient=\"index\").reset_index()\nfinal_valid_preds.columns = [\"id\", \"models_pred_2\"]\nfinal_valid_preds.to_csv(\"train_pred_2.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_preds), axis=1)\nsub_df.columns = ['id', 'models_pred_2']\nsub_df.to_csv(\"test_pred_2.csv\", index=False)\n\n","e33486e3":"#cahnge param at set 4\ndf =pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ndf_test =  pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat \/= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n    params = {'n_estimators': 8000, 'max_depth': 3, 'learning_rate': 0.03628302216953097, \n              'gamma': 0, 'min_child_weight': 1, 'subsample': 0.7875490025178415, \n              'colsample_bytree': 0.11807135201147481, 'reg_alpha': 23.13181079976304, \n              'reg_lambda': 0.0008746338866473539}\n    \n    model = XGBRegressor(\n        random_state=42,\n        **params,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n    )\n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"models_pred_3\"]\nfinal_valid_predictions.to_csv(\"train_pred_3.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsub_df.columns = [\"id\", \"models_pred_3\"]\nsub_df.to_csv(\"test_pred_3.csv\", index=False)","445d07d8":"\n#model 5\n#simple Decision Tree\ndf_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\nfinal_test_preds = []\nfinal_valid_preds = {}\npreds = []\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n    \n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n   \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    X_train[objects_col] = ordinal_encoder.fit_transform(X_train[objects_col])\n    X_valid[objects_col] = ordinal_encoder.transform(X_valid[objects_col])\n    X_test[objects_col] = ordinal_encoder.transform(X_test[objects_col])\n    \n    \n    scaler = preprocessing.StandardScaler(with_mean=True)\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n    \n    model = DecisionTreeRegressor(max_depth=3, random_state=1)\n    \n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n    \nprint(np.mean(preds), np.std(preds))\nfinal_valid_preds = pd.DataFrame.from_dict(final_valid_preds, orient=\"index\").reset_index()\nfinal_valid_preds.columns = [\"id\", \"models_pred_4\"]\nfinal_valid_preds.to_csv(\"train_pred_4.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_preds), axis=1)\nsub_df.columns = ['id', 'models_pred_4']\nsub_df.to_csv(\"test_pred_4.csv\", index=False)","aa1a0452":"\nfrom sklearn.linear_model import SGDRegressor\n\ndf_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\nfinal_test_preds = []\nfinal_valid_preds = {}\npreds = []\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n    \n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[objects_col])\n    X_valid_ohe = ohe.transform(X_valid[objects_col])\n    X_test_ohe = ohe.transform(X_test[objects_col])\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train, X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid, X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test, X_test_ohe], axis=1)\n    \n    # this part is missing in the video:\n    X_train = X_train.drop(objects_col, axis=1)\n    X_valid = X_valid.drop(objects_col, axis=1)\n    X_test = X_test.drop(objects_col, axis=1)\n    \n    \n    scaler = preprocessing.StandardScaler(with_mean=True)\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n    \n    model = SGDRegressor(alpha=0, random_state=42)\n    \n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n    \nprint(np.mean(preds), np.std(preds))\nfinal_valid_preds = pd.DataFrame.from_dict(final_valid_preds, orient=\"index\").reset_index()\nfinal_valid_preds.columns = [\"id\", \"models_pred_6\"]\nfinal_valid_preds.to_csv(\"train_pred_6.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_preds), axis=1)\nsub_df.columns = ['id', 'models_pred_6']\nsub_df.to_csv(\"test_pred_6.csv\", index=False)","ce2ac7e5":"import catboost as cb\n\ndf_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\n\nfeatures = [c for c in df_fold.columns if c not in (\"id\", \"target\", \"kfold\")]\nobjects_col = [col for col in features if 'cat' in col]\nnumerical_cols = [col for col in features if col.startswith(\"cont\")]\ntest_df = test_df[features]\n\nfinal_test_preds = []\nfinal_valid_preds = {}\npreds = []\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n    \n    valid_ids = X_valid.id.values.tolist()\n    \n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[features]\n    X_valid = X_valid[features]\n    \n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe = ohe.fit_transform(X_train[objects_col])\n    X_valid_ohe = ohe.transform(X_valid[objects_col])\n    X_test_ohe = ohe.transform(X_test[objects_col])\n    \n    X_train_ohe = pd.DataFrame(X_train_ohe, columns=[f\"ohe_{i}\" for i in range(X_train_ohe.shape[1])])\n    X_valid_ohe = pd.DataFrame(X_valid_ohe, columns=[f\"ohe_{i}\" for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe = pd.DataFrame(X_test_ohe, columns=[f\"ohe_{i}\" for i in range(X_test_ohe.shape[1])])\n    \n    X_train = pd.concat([X_train, X_train_ohe], axis=1)\n    X_valid = pd.concat([X_valid, X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test, X_test_ohe], axis=1)\n    \n    # this part is missing in the video:\n    X_train = X_train.drop(objects_col, axis=1)\n    X_valid = X_valid.drop(objects_col, axis=1)\n    X_test = X_test.drop(objects_col, axis=1)\n    \n    \n    scaler = preprocessing.StandardScaler(with_mean=True)\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n    \n    model = cb.CatBoostRegressor()\n    \n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_test_preds.append(preds_test)\n    final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n    \nprint(np.mean(preds), np.std(preds))\nfinal_valid_preds = pd.DataFrame.from_dict(final_valid_preds, orient=\"index\").reset_index()\nfinal_valid_preds.columns = [\"id\", \"models_pred_5\"]\nfinal_valid_preds.to_csv(\"train_pred_5.csv\", index=False)\n\nsub_df.target = np.mean(np.column_stack(final_test_preds), axis=1)\nsub_df.columns = ['id', 'models_pred_5']\nsub_df.to_csv(\"test_pred_5.csv\", index=False)","f2ea313f":"df_fold = pd.read_csv('\/kaggle\/input\/folding-30days\/folds_train.csv')\ntest_df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsub_df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n\n\ndf1 = pd.read_csv(\"train_pred_1.csv\")\ndf2 = pd.read_csv(\"train_pred_2.csv\")\ndf3 = pd.read_csv(\"train_pred_3.csv\")\ndf4 = pd.read_csv(\"train_pred_4.csv\")\ndf5 = pd.read_csv(\"train_pred_5.csv\")\ndf6 = pd.read_csv(\"train_pred_6.csv\")\n\ndf_test1 = pd.read_csv(\"test_pred_1.csv\")\ndf_test2 = pd.read_csv(\"test_pred_2.csv\")\ndf_test3 = pd.read_csv(\"test_pred_3.csv\")\ndf_test4 = pd.read_csv(\"test_pred_4.csv\")\ndf_test5 = pd.read_csv(\"test_pred_5.csv\")\ndf_test6 = pd.read_csv(\"test_pred_6.csv\")\n\ndf_fold = df_fold.merge(df1, on=\"id\", how=\"left\")\ndf_fold = df_fold.merge(df2, on=\"id\", how=\"left\")\ndf_fold = df_fold.merge(df3, on=\"id\", how=\"left\")\ndf_fold = df_fold.merge(df4, on=\"id\", how=\"left\")\ndf_fold = df_fold.merge(df5, on=\"id\", how=\"left\")\ndf_fold = df_fold.merge(df6, on=\"id\", how=\"left\")\n\ntest_df = test_df.merge(df_test1, on=\"id\", how=\"left\")\ntest_df = test_df.merge(df_test2, on=\"id\", how=\"left\")\ntest_df = test_df.merge(df_test3, on=\"id\", how=\"left\")\ntest_df = test_df.merge(df_test4, on=\"id\", how=\"left\")\ntest_df = test_df.merge(df_test5, on=\"id\", how=\"left\")\ntest_df = test_df.merge(df_test6, on=\"id\", how=\"left\")\n\ntest_df.head()\n","99d7047e":"from sklearn.tree import DecisionTreeRegressor\n# , \"models_pred_2\"\nuseful_features = [c for c in test_df.columns if c in (\"models_pred_1\",\"models_pred_2\",\"models_pred_3\",\"models_pred_4\",\"models_pred_5\",\"models_pred_6\")]\n# useful_features = [c for c in test_df.columns if c in (\"models_pred_1\")]\n\ntest_df = test_df[useful_features]\n\nfinal_predictions = []\npreds = []","12393872":"test_df.head()","97d235c4":"\n\nfor i in range(5):\n    X_train = df_fold[df_fold['kfold'] != i].reset_index(drop=True)\n    X_valid = df_fold[df_fold['kfold'] == i].reset_index(drop=True)\n    X_test = test_df.copy()\n\n    y_train  = X_train['target']\n    y_valid  = X_valid['target']\n    \n    X_train = X_train[useful_features]\n    X_valid = X_valid[useful_features]\n   \n    #Decision Tree \n#     model = DecisionTreeRegressor(max_depth=4, random_state=42)\n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    preds_valid = model.predict(X_valid)\n    preds_test = model.predict(X_test)\n    final_predictions.append(preds_test)\n#     final_valid_preds.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    preds.append(rmse)\n    print(i, rmse)\n\nprint(np.mean(scores), np.std(scores))","6ce50d0c":"sub_df.target = np.mean(np.column_stack(final_predictions), axis=1)\nsub_df.to_csv(\"submission.csv\", index=False)","5f0dcf85":"test_df","d3e2eb1d":"# Run","f9448fa7":"0.7181683000017911 remove model 5\n","cd813710":"# catboost","6c3c5b34":"# Run"}}