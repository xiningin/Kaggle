{"cell_type":{"0301f9e6":"code","1bb522d9":"code","c0eb1e2d":"code","2b064c7d":"code","9b38686c":"code","3eed7c2f":"code","14002e9a":"code","e9e5ddae":"markdown","b65f186a":"markdown","3649d15e":"markdown","9addc522":"markdown","c3f2a477":"markdown","33168474":"markdown","6951ae78":"markdown","b51a087a":"markdown"},"source":{"0301f9e6":"!pip install attrdict\n!pip install git+https:\/\/github.com\/openai\/CLIP.git\n!pip install git+https:\/\/github.com\/YoadTew\/zero-shot-image-to-text.git\n!git clone https:\/\/github.com\/YoadTew\/zero-shot-image-to-text.git","1bb522d9":"import torch\nimport clip\nfrom model.ZeroCLIP import CLIPTextGenerator\nfrom IPython.display import Image, display\n\ndef calc_best_clip(text_generator, captions, image_features):\n    encoded_captions = [text_generator.clip.encode_text(clip.tokenize(c).to(text_generator.device)) for c in captions]\n    encoded_captions = [x \/ x.norm(dim=-1, keepdim=True) for x in encoded_captions]\n    best_clip_idx = (torch.cat(encoded_captions) @ image_features.t()).squeeze().argmax().item()\n    best_clip = captions[best_clip_idx]\n    \n    return best_clip","c0eb1e2d":"from attrdict import AttrDict\nfrom enum import Enum\n\nclass GenType(Enum):\n    Captioning = 0\n    Arithmetic = 1\n    RealWorld = 2\n    OCR = 3\n    \ndef get_args(gen_type):\n    args = {}\n    args['lm_model']=\"gpt-2\"\n    args['forbidden_tokens_file_path']='zero-shot-image-to-text\/forbidden_tokens.npy'\n    args['target_seq_length']=15\n    args['reset_context_delta']=True\n    args['num_iterations']=5\n    args['clip_loss_temperature']=0.01\n    args['clip_scale']=1\n    args['ce_scale']=0.2\n    args['stepsize']=0.3\n    args['repetition_penalty']=1\n    args['end_token']=\".\"\n    args['forbidden_factor']=20\n    args['beam_size']=5\n\n    if gen_type == GenType.Captioning:\n        args['cond_text']=\"Image of a\"\n        args['end_factor']=1.01\n        args['fusion_factor']=0.99\n        args['grad_norm_factor']=0.9\n    elif gen_type == GenType.Arithmetic:\n        args['cond_text']=\"Image of a\"\n        args['end_factor']=1.06\n        args['fusion_factor']=0.95\n        args['grad_norm_factor']=0.95\n    elif gen_type == GenType.RealWorld:\n        args['cond_text']=\"Image of\"\n        args['end_factor']=1.04\n        args['fusion_factor']=0.99\n        args['grad_norm_factor']=0.9\n    elif gen_type == GenType.OCR:\n        args['cond_text']=\"Image of text that says\"\n        args['end_factor']=1.04\n        args['fusion_factor']=0.99\n        args['grad_norm_factor']=0.9\n        \n    return AttrDict(args)","2b064c7d":"img_path = 'zero-shot-image-to-text\/example_images\/captions\/COCO_val2014_000000097017.jpg'\n\nprint('Input image:')\ndisplay(Image(img_path, width = 300, height = 300))\n\ncaptioning_args = get_args(GenType.Captioning)\n\ntext_generator = CLIPTextGenerator(**dict(captioning_args))\nimage_features = text_generator.get_img_feature([img_path], None)\ncaptions = text_generator.run(image_features, captioning_args.cond_text, beam_size=captioning_args.beam_size)\n\nbest_clip = calc_best_clip(text_generator, captions, image_features)\n\nprint('all captions:', captions)\nprint('best clip:', captioning_args.cond_text + best_clip)","9b38686c":"arithmetics_imgs=[\"zero-shot-image-to-text\/example_images\/arithmetics\/woman2.jpg\", \"zero-shot-image-to-text\/example_images\/arithmetics\/king2.jpg\", \"zero-shot-image-to-text\/example_images\/arithmetics\/man2.jpg\"]\narithmetics_weights=[1, 1, -1]\narithmetics_args = get_args(GenType.Arithmetic)\n\ntext_generator = CLIPTextGenerator(**dict(arithmetics_args))\n\nprint(\"Input images:\")\nfor i, img_path in enumerate(arithmetics_imgs):\n    display(Image(img_path, width = 100, height = 100))\n\nimage_features = text_generator.get_combined_feature(arithmetics_imgs, [], arithmetics_weights, None)\ncaptions = text_generator.run(image_features, arithmetics_args.cond_text, beam_size=arithmetics_args.beam_size)\n\nbest_clip = calc_best_clip(text_generator, captions, image_features)\n\nprint('all captions:', captions)\nprint('best clip:', arithmetics_args.cond_text + best_clip)","3eed7c2f":"img_path = 'zero-shot-image-to-text\/example_images\/real_world\/london.jpg'\nrw_args = get_args(GenType.RealWorld)\n\ntext_generator = CLIPTextGenerator(**dict(rw_args))\n\nprint('Input image:')\ndisplay(Image(img_path, width = 300, height = 300))\n\nimage_features = text_generator.get_img_feature([img_path], None)\ncaptions = text_generator.run(image_features, rw_args.cond_text, beam_size=rw_args.beam_size)\n\nbest_clip = calc_best_clip(text_generator, captions, image_features)\n\nprint('all captions:', captions)\nprint('best clip:', rw_args.cond_text + best_clip)","14002e9a":"img_path = 'zero-shot-image-to-text\/example_images\/OCR\/welcome_sign.jpg'\nocr_args = get_args(GenType.OCR)\n\ntext_generator = CLIPTextGenerator(**dict(ocr_args))\n\nprint('Input image:')\ndisplay(Image(img_path, width = 300, height = 300))\n\nimage_features = text_generator.get_img_feature([img_path], None)\ncaptions = text_generator.run(image_features, ocr_args.cond_text, beam_size=ocr_args.beam_size)\n\nbest_clip = calc_best_clip(text_generator, captions, image_features)\n\nprint('all captions:', captions)\nprint('best clip:', ocr_args.cond_text + best_clip)","e9e5ddae":"The following cell imports the relevant packages, and defines a method that returns the caption that is most aligned with the input image features. The alignment score is based on the cosine distance between the text's and image's CLIP features.","b65f186a":"Example for OCR:","3649d15e":"Example for arithmetic with images:","9addc522":"Example for real-world knowledge:","c3f2a477":"The following method returns the values of the hyper-parameters, used in the paper, for a given scenario.","33168474":"Example for image captioning:","6951ae78":"The following cell installs the package and the required dependencies.","b51a087a":"**Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic**\n\nThis notebook shows how to install and run the model for a variety of scenarios presented in the paper."}}