{"cell_type":{"db0f5db5":"code","4017f28e":"code","a3434c68":"code","bb970551":"code","ea852290":"code","e8c4dc5a":"code","ab61d7db":"code","0907ecf9":"code","21abb32d":"code","1e0078e5":"code","aa04d23e":"code","63f7f47e":"code","c0ad3128":"code","e2c98ae4":"code","9bc16f72":"code","191b909c":"markdown","64d714ba":"markdown","4859837e":"markdown","5a65e590":"markdown","f4354d6f":"markdown","e87d7d9a":"markdown","58238e80":"markdown","d2828fc6":"markdown","d8a278eb":"markdown","09a3ddad":"markdown","8de7282a":"markdown","1ddf53e1":"markdown","a4ccb674":"markdown","92ecce24":"markdown","f100001e":"markdown","02e9dece":"markdown","731d9146":"markdown","73ac8f1c":"markdown","d415a6f1":"markdown"},"source":{"db0f5db5":"from sklearn import linear_model\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n%matplotlib inline","4017f28e":"## Read data\ndf = pd.read_csv(\"..\/input\/studentperformance\/student-appended-mat-por.csv\")\n\n## Summarize data\ndf.describe()","a3434c68":"## Selecting appropriate X for Y=G3 (Final Grade)\n# here we are picking a few of 'probable' continuous variables as X\ncdf = df[['absences', 'G1', 'G2', 'G3']]\ncdf.head(9)","bb970551":"# Plotting histogram on the selected variables to explore its distribution\n\nviz = cdf[['absences', 'G1', 'G2', 'G3']]\nfig = plt.figure(figsize = (15,10))\naxes = fig.gca() # get current axes\nviz.hist(ax=axes)\nplt.show()\n\n# erm, not sure why we get the warning... but oh well, the sizing works! haha","ea852290":"# copying the cdf\ncdf2 = cdf.copy()\ncdf2","e8c4dc5a":"# unpivoting the copied cdf to try to plot multiple scatterplots using facetgrid (this is also why we copy cdf, lest we altered the original data while trying to plot the facetgrid)\nmelted2 = cdf2.melt(id_vars=['G3'], value_vars=['absences', 'G1', 'G2'])\nmelted2","ab61d7db":"# ref on seaborn docs: https:\/\/seaborn.pydata.org\/tutorial\/axis_grids.html\n# ref on setting indv xlim: https:\/\/stackoverflow.com\/questions\/31170480\/set-axis-limits-on-individual-facets-of-seaborn-facetgrid\n\ng = sns.FacetGrid(melted2, col='variable', sharex=False, sharey='row', hue='variable', height=5, margin_titles=True) # hue=changes the color based on variable\ng.map(sns.scatterplot, 'value', 'G3', edgecolor='w')\ng.fig.subplots_adjust(wspace=.02, hspace=.02)\n\n# manually adjust the x axis range of subplots\n# goal was to have xlim(0,20) only for G1 and G2 but not absences\n# g.axes[0,1].set_xlim(0,20)\n# g.axes[0,2].set_xlim(0,20) # these 2 worked, but also changes the absences xlim to 20. which is not what i want\ng.set(xlim=(0, None)) # only works when we have sharex=False :D\n\n# changing the font size of titles of all subplots\ng.set_titles(size=25)\n\n# manually set the label for each of the plot in the facet\n# g.axes[0,0].set_xlabel('absence')\n# g.axes[0,1].set_xlabel('G1')\n# g.axes[0,2].set_xlabel(melted2.variable)\n\n# dynamically changing the xlabel of all subplots (tried to use the '{col_name}' format but doesnt work for me...)\nfor i in range(3):\n    xlabel = g.axes[0, i].get_title() # get the title from each subplot\n    xlabel_trunc = xlabel[11:] # truncating the titles :) \n    g.axes[0,i].set_xlabel(xlabel_trunc) # now this worked! :D \n#     g.axes[0,i].set_title(' ') # if we wanted to remove the now redundant title.. but lets keep it :D","0907ecf9":"# Plot each selected X variables against Y (G3) to see their relationship (not using facetgrid)\n\n# 1: Absences vs Final Grade\nplt.scatter(cdf.absences, cdf.G3, color='blue')\nplt.xlabel('Absences')\nplt.ylabel('Final Grade')\nplt.title('Absence against Final Grade')\nplt.show()\n# conclusion: No obvious relationship\n\n\n# 2: 1st Grade vs Final Grade\nplt.scatter(cdf.G1, cdf.G3, color='orange')\nplt.xlabel('1st Grade')\nplt.ylabel('Final Grade')\nplt.title('1st Grade against Final Grade')\nplt.show()\n# conclusion: seems like a possible positive correlation\n\n\n# 3: 2nd Grade vs Final Grade\nplt.scatter(cdf.G2, cdf.G3, color='green')\nplt.xlabel('2nd Grade')\nplt.ylabel('Final Grade')\nplt.title('2nd Grade against Final Grade')\nplt.show()\n# conclusion: seems like a possible positive correlation","21abb32d":"# Chossing G1 as X\n\n# create a mask to select random rows\nmsk = np.random.rand(len(df)) < 0.8\n\n# split the dataset (80% training, 20% testing)\ntrain = cdf[msk]\ntest = cdf[~msk]\n\n# viweing train data distribution\nplt.scatter(train.G1, train.G3, color='orange')\nplt.xlabel('1st Grade')\nplt.ylabel('Final Grade')\nplt.title('1st Grade against Final Grade')\nplt.show()","1e0078e5":"# extracting one column (G1) from the train dataset\ntrain[['G1']]","aa04d23e":"# matplotlib linecolor\/marker ref: https:\/\/matplotlib.org\/2.1.1\/api\/_as_gen\/matplotlib.pyplot.plot.html\n\n# Modeling\nregr = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['G1']])  # selecting a column from train (G1) as X\ntrain_y = np.asanyarray(train[['G3']])  # selecting a column from train (G3) as Y\nregr.fit(train_x, train_y) # fitting the linear regression model\n\n# Viewing the coefficients\nprint('\ud835\udf031, Coefficients:', regr.coef_)\nprint('\ud835\udf030, Intercept:', regr.intercept_)\n\ntheta1 = regr.coef_\ntheta0 = regr.intercept_\n\n# Plot the best fit line over the data\nplt.scatter(train.G1, train.G3, color='orange') # plot scatterplot\nplt.plot(train_x, theta0+theta1[0][0]*train_x, '-k') \nplt.xlabel('1st Grade')\nplt.ylabel('Final Grade')\nplt.title('1st Grade against Final Grade')","63f7f47e":"# Evaluation\n\ntest_x = np.asanyarray(test[['G1']])\ntest_y = np.asanyarray(test[['G3']])\ntest_y_hat = regr.predict(test_x)\n\nprint('Mean absolute error: %.2f' % np.mean(np.absolute(test_y_hat - test_y)))\nprint('Residual sum of squares (MSE): %.2f' % np.mean((test_y_hat - test_y)**2))\nprint('R2-score: %.2f' % r2_score(test_y_hat, test_y))\n\n# only 0.42 accuracy score - so not that good..","c0ad3128":"## Chossing G2 as X\n\n# we have split the test and train dataset previously when doing it for G1\n# so we can straight away proceed..\n\n# viweing train data distribution\nplt.scatter(train.G2, train.G3, color='green')\nplt.xlabel('2nd Grade')\nplt.ylabel('Final Grade')\nplt.title('2nd Grade against Final Grade')\nplt.show()","e2c98ae4":"# Modeling\n\nregrG2 = linear_model.LinearRegression()\ntrain_xG2 = np.asanyarray(train[['G2']])  # selecting a column from train (G1) as X\ntrain_yG2 = np.asanyarray(train[['G3']])  # selecting a column from train (G3) as Y\nregrG2.fit(train_xG2, train_yG2) # fitting the linear regression model\n\ntheta1G2 = regrG2.coef_\ntheta0G2 = regrG2.intercept_\n\n# Viewing the coefficients\nprint('\ud835\udf031, Coefficients:', theta1G2)\nprint('\ud835\udf030, Intercept:', theta0G2)\n\n# Plot the best fit line over the data\n\nplt.scatter(train.G2, train.G3, color='green') # plot scatterplot\nplt.plot(train_xG2, theta0G2+theta1G2[0][0]*train_xG2, '-k')\nplt.xlabel('2nd Grade')\nplt.ylabel('Final Grade')\nplt.title('2nd Grade against Final Grade')","9bc16f72":"# Evaluation\n\ntest_xG2 = np.asanyarray(test[['G2']])\ntest_yG2 = np.asanyarray(test[['G3']])\ntest_y_hatG2 = regrG2.predict(test_xG2)\n\nprint('Mean absolute error: %.2f' % np.mean(np.absolute(test_y_hatG2 - test_yG2)))\nprint('Residual sum of squares (MSE): %.2f' % np.mean((test_y_hatG2 - test_yG2)**2))\nprint('R2-score: %.2f' % r2_score(test_y_hatG2, test_yG2))\n\n# only 0.83 accuracy score - so better than using x=G1! Bravo!","191b909c":"(1d) Plotting the scatterplot of each X candidates against Y (Final Grade), to observe their relationship\n","64d714ba":"### Step 4: Fit the Linear Model ###\n\nSince, we are using Simple Linear Regression. The model can be calculated using this polynomial regression equation:\n<br>\n \u0177 = \ud835\udf030 + \ud835\udf031(X)\n \n Where, \n  <br>\u0177 = the response variable, in our case, it is the Final Grade variable\n  <br>\ud835\udf030 = the intercept of the best fit line\n  <br>\ud835\udf031 = the gradient of the best fit line\n  \n <br>Now, we want to try to find the coefficient and intercept that best fit our data...","4859837e":"So, plugging our numbers in, we get:\n\n\u0177 = \ud835\udf030 + \ud835\udf031(X)\n<br>\u0177 = -0.66 + (1.06)(X)","5a65e590":"(1a) Descriptive statistical analysis of the whole dataset","f4354d6f":"We get a much better score than option 1! yahoo! \u2728","e87d7d9a":"So, using G1 as the predictor variable for Final Grade only gives R2-score of 0.42... which is a little low..\n<br> Now, let's look at the next option! \ud83e\uddd0","58238e80":"### Step 5: Evaluate our prediction model ###","d2828fc6":"##### Option 1: Choosing G1 as independent variable ######","d8a278eb":"### Conclusion: ###\n<b> Yes! We can predict the student's final grade using one of their attributes.. Yay!<\/b>\n<br><b> In this notebook, we have shown that G2 is a better predictor of student's Final Grade with R2 score of 0.83! <\/b>\n\nThat's it for my first attempt at creating a simple linear regression model! \ud83e\udd38\ud83c\udffb\u200d\u2640\ufe0f yay!!! \u2728\u2728\n##### Next step: Save the model and test it using other out-of-samples datasets... ##### \nLet me know what I can improve on in the comment below! Looking forward to reading your suggestions! \ud83d\ude01\n<br>That's it for now, thank you for reading! \ud83d\ude04\nHave a nice day!","09a3ddad":"### Step 3: Split the dataset into Test and Train sets ###\n##### Option 2: Choosing G2 as independent variable ######","8de7282a":"So, plugging our numbers in, we get:\n\n\u0177 = \ud835\udf030 + \ud835\udf031(X)\n<br>\u0177  = -0.42 + (1.04)(X)","1ddf53e1":"### Step 1: Explore the dataset ###","a4ccb674":"### Step 3: Split the dataset into Test and Train sets ###\n\nFrom previous step, we have narrowed down 2 possible X variable candidates (G1 and G2).\n<br> Now, let's look at them separately and try to identify which one of them can better predict Final Grade.\n\nSo here we go! \ud83d\udd75\ud83c\udffb\u200d","92ecce24":"### Step 4: Fit the Linear Model ###","f100001e":"(1c) Plotting the histogram of all X candidates (absences, G1, G2) and Y (G3) to observe their distribution\n<br> Looking at these distribution roughly, we can somewhat observe normal distribution for G1, G2 and G3.. but not absences..","02e9dece":"### Step 2: Choose the independent variable, X (the variable which we use to try to predict the Final Grade)###\n\nFrom previous step, we can observe:\n<br>1. No obvious correlation between Absences and Final Grade   \u274c\n<br>2. Possible positive correlation between 1st Grade and Final Grade   \u2705\n<br>3. Possible positive correlation between 2nd Grade and Final Grade   \u2705\n\nSo, we will proceed with G1 and G2","731d9146":"(1b) Selecting a few candidates as X (Independent variable)\n<br> So from the dataset, these 3 variables looks promising...\n<br>\n<br> They are:\n - absences = total days students are absent from school \n - G1 = 1st term grades\n - G2 = 2nd term grades\n<br>\n<br> Additionally, we also have\n - G3 = Final Grade, which is our Y, the value we wanted to try to predict!\n \ud83d\ude03","73ac8f1c":"<b>Hello there!<\/b>\n<br>I am new to ML, and in this notebook I want to try to answer whether..\n\n### Can we predict student's Final Grade based on their other attributes? ###\n\nIn this notebook, I will be using the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/Student+Performance\">Student Performance Data Set <\/a> from UCI ML Repository. \n\n<b> So, let's start! \ud83d\udc31\u200d\ud83c\udfcd <\/b>\n\n<i>Step 1: Explore the dataset<i>\n<i><br>Step 2: Choose the independent variable, X (the variable which we use to try to predict the Final Grade)<i>\n<i><br>Step 3: Split the dataset into Test and Train sets<i>\n<i><br>Step 4: Fit the Linear Model<i>\n<i><br>Step 5: Evaluate our prediction model<i>","d415a6f1":"### Step 5: Evaluate our prediction model ###\n\nThere are many ways to evaluate our model, here we will look at 3 of them:"}}