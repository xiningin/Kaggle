{"cell_type":{"fda84596":"code","46f723d9":"code","ed57cdd4":"code","7697abfb":"code","f709e9e0":"code","7affc0f8":"code","56d43099":"code","623629d2":"code","c94ae39e":"code","a821193c":"code","f0e94971":"code","241e6991":"code","0fd11895":"code","6e06cd09":"code","2e8718d1":"code","39fe2c4f":"code","375a45e6":"code","59145bdf":"code","4779b538":"code","4c891f0f":"code","1272cfff":"code","13881df8":"code","f3632069":"code","7826de92":"code","cf6c3e12":"code","9990fdbd":"code","af0c1fcf":"code","5a26ca95":"code","e9e2c036":"code","797e88a5":"code","93ad1a4d":"code","e38cb251":"code","047dd1ae":"code","081ab207":"code","54884b35":"code","49c2e216":"code","a097deaf":"code","519607c6":"code","80236022":"code","6a2764c4":"code","d464ce03":"code","3fcd66ac":"code","836a8666":"code","bf5334e6":"code","c3bdb3ab":"code","c4372df1":"code","8bbcd819":"code","ce126b21":"code","5a7faf19":"code","3a55476e":"code","c979682a":"code","3483276b":"code","d1f3da81":"code","181bcd55":"code","712e3fb6":"code","7934e184":"code","16fad788":"markdown","6fc3fb40":"markdown","f238cc78":"markdown","75ccee72":"markdown","e2ea8a48":"markdown","beb67153":"markdown","407c0731":"markdown","efae1bab":"markdown","a880c6f7":"markdown","fff850fb":"markdown","11878ed7":"markdown","bf6e4a4d":"markdown","0d77f28c":"markdown","27980498":"markdown","63013ac4":"markdown","756cb0fd":"markdown","1479c63f":"markdown","130bfba0":"markdown","38d9ee14":"markdown","fe4c5ae9":"markdown"},"source":{"fda84596":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n% matplotlib inline","46f723d9":"import warnings\nwarnings.filterwarnings('ignore')","ed57cdd4":"import gc\nimport pyarrow.parquet as pq","7697abfb":"import os\nprint(os.listdir(\"..\/input\"))","f709e9e0":"metadata_train = pd.read_csv(\"..\/input\/metadata_train.csv\")\nmetadata_train.info()","7affc0f8":"metadata_train.head(12)","56d43099":"metadata_test = pd.read_csv(\"..\/input\/metadata_test.csv\")\nmetadata_test.info()","623629d2":"metadata_test.head(12)","c94ae39e":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.info()","a821193c":"sample_submission.head(12)","f0e94971":"%%time\ntrain = pq.read_pandas('..\/input\/train.parquet').to_pandas()","241e6991":"train.info()","0fd11895":"train.iloc[0:7,0:10]","6e06cd09":"x = train.index\ny = train.iloc[:,0]","2e8718d1":"fig = plt.figure(figsize=(12,4))\nax1 = fig.add_axes([0,0,1,1])\nax1.plot(x,y,color='lightblue')","39fe2c4f":"fig = plt.figure(figsize=(12,4))\nax1 = fig.add_axes([0,0,1,1])\nax1.set_xlim([10,300])\nax1.set_ylim([10,30])\nax1.plot(x,y,marker='o', color='orange')\nax2 = fig.add_axes([0.7,0.7,0.2,0.2])\nax2.plot(x,y, color='lightblue')","375a45e6":"x = train.index\ny0 = train.iloc[:,0]\ny1 = train.iloc[:,1]\ny2 = train.iloc[:,2]","59145bdf":"fig = plt.figure(figsize=(12,4))\nax1 = fig.add_axes([0,0,1,1])\nax1.plot(x,y0,color='blue')\nax1.plot(x,y1,color='red')\nax1.plot(x,y2,color='green')","4779b538":"np.mean(y0)","4c891f0f":"np.min(y0)","1272cfff":"np.max(y0)","13881df8":"np.std(y0)","f3632069":"y0 = train.iloc[:,3]\ny1 = train.iloc[:,4]\ny2 = train.iloc[:,5]","7826de92":"fig = plt.figure(figsize=(12,4))\nax1 = fig.add_axes([0,0,1,1])\nax1.plot(x,y0,color='blue')\nax1.plot(x,y1,color='red')\nax1.plot(x,y2,color='green')","cf6c3e12":"np.mean(y0)","9990fdbd":"np.min(y0)","af0c1fcf":"np.max(y0)","5a26ca95":"np.std(y0)","e9e2c036":"row_nr = train.shape[0]\nrow_nr","797e88a5":"index_group_size=100","93ad1a4d":"time_sample_idx=np.arange(0,row_nr,index_group_size)\ntime_sample_idx[0:10]","e38cb251":"train_down=train.iloc[time_sample_idx,:]\ntrain_down.iloc[:,0:10].head()","047dd1ae":"import numpy.fft as ft","081ab207":"def Amplitude(z):\n    return np.abs(z)","54884b35":"def Phase(z):\n    return (np.arctan2(z.imag,z.real))","49c2e216":"df_harm=pd.DataFrame()","a097deaf":"def find_dfa(df_source, df_dest,num_harm,base_col):\n    # init\n    dfa=df_dest.iloc[:,0:base_col]\n    num_ap_cols = int(num_harm\/2)\n    for j in range(0,num_ap_cols) :\n        dfa['Amp'+str(j)] = 0\n        dfa['Pha'+str(j)] = 0\n    dfa['ErrFun'] = 0\n    dfa['ErrGen'] = 0\n    # calc\n    for i in range(0,len(df_source.columns)):\n        dfa.loc[i]=0\n        s=df_source.iloc[:,base_col+i]\n        SF=ft.rfft(s)\n        SF_Fundam=np.zeros(SF.size, dtype=np.complex_)\n        SF_Filtered=np.zeros(SF.size, dtype=np.complex_)\n        SF_Fundam[0:2]=SF[0:2]\n        SF_Filtered[0:num_harm]=SF[0:num_harm]\n        s_fun_rec=ft.irfft(SF_Fundam)\n        s_gen_rec=ft.irfft(SF_Filtered)\n        for j in range(0,num_ap_cols):\n            dfa.iloc[i,base_col+2*j] = Amplitude(SF_Filtered[j])\n            dfa.iloc[i,base_col+2*j+1] = Phase(SF_Filtered[j])\n        dfa.iloc[i,base_col+2*num_ap_cols] = np.sqrt(np.mean((s-s_fun_rec)**2))\n        dfa.iloc[i,base_col+2*num_ap_cols+1] = np.sqrt(np.mean((s-s_gen_rec)**2))\n    return dfa","519607c6":"%%time\ntrain_max = train.apply(np.max)\ntrain_min = train.apply(np.min)","80236022":"%%time\ntrain_mean = train_down.apply(np.mean)\ntrain_std = train_down.apply(np.std)","6a2764c4":"%%time\ndf_harm=pd.DataFrame()\nnum_harm=10\ndf_harm=find_dfa(train_down,df_harm,num_harm,0)","d464ce03":"df_harm.iloc[:,0:10].head()","3fcd66ac":"metadata_train['mean']=train_mean.values\nmetadata_train['max']=train_max.values\nmetadata_train['min']=train_min.values\nmetadata_train['std']=train_std.values\nfor j in range(0,int(num_harm\/2)) :\n    metadata_train['Amp'+str(j)] = df_harm['Amp'+str(j)]\n    metadata_train['Pha'+str(j)] = df_harm['Pha'+str(j)]\nmetadata_train['ErrFun'] = df_harm['ErrFun']\nmetadata_train['ErrGen'] = df_harm['ErrGen']","836a8666":"metadata_train.head()","bf5334e6":"df_train = metadata_train\ndf_train.to_csv('df_train.csv', index=False)","c3bdb3ab":"col_group_size=2000","c4372df1":"gc.collect()","8bbcd819":"metadata_test = pd.read_csv(\"..\/input\/metadata_test.csv\")","ce126b21":"metadata_test['target']=-1\nmetadata_test['mean']=0\nmetadata_test['max']=0\nmetadata_test['min']=0\nmetadata_test['std']=0\nfor j in range(0,int(num_harm\/2)) :\n    metadata_test['Amp'+str(j)] = 0\n    metadata_test['Pha'+str(j)] = 0\nmetadata_test['ErrFun'] = 0\nmetadata_test['ErrGen'] = 0","5a7faf19":"metadata_test.shape","3a55476e":"metadata_test.head()","c979682a":"def add_info_test(metadata_df,time_sample_idx_1,col_group_size):\n    col_id_start_0=np.min(metadata_test['signal_id'])\n    col_id_start=col_id_start_0\n    col_id_last=np.max(metadata_test['signal_id'])+1\n    n_groups = int(np.round((col_id_last-col_id_start)\/col_group_size))\n    print('Steps = {}'.format(n_groups))\n    for i in range(0,n_groups):\n        col_id_stop = np.minimum(col_id_start+col_group_size,col_id_last)\n        col_numbers = np.arange(col_id_start,col_id_stop)\n        print('Step {s} - cols = [{a},{b})'.format(s=i,a=col_id_start,b=col_id_stop))\n        print('   Adding Stats...',end=\"\")\n        col_names = [str(col_numbers[j]) for j in range(0,len(col_numbers))]\n        test_i = pq.read_pandas('..\/input\/test.parquet',columns=col_names).to_pandas()\n        test_i_d1=test_i.iloc[time_sample_idx_1,:]\n        test_mean_i = test_i_d1.apply(np.mean)\n        test_max_i  = test_i.apply(np.max)\n        test_min_i  = test_i.apply(np.min)\n        test_std_i  = test_i_d1.apply(np.std)\n        r_start = col_id_start - col_id_start_0\n        r_stop = r_start + (col_id_stop-col_id_start)\n        metadata_df.iloc[r_start:r_stop,4] = test_mean_i[0:col_id_stop-col_id_start].values\n        metadata_df.iloc[r_start:r_stop,5] = test_max_i[0:col_id_stop-col_id_start].values\n        metadata_df.iloc[r_start:r_stop,6] = test_min_i[0:col_id_stop-col_id_start].values\n        metadata_df.iloc[r_start:r_stop,7] = test_std_i[0:col_id_stop-col_id_start].values\n        print('   Adding FFT...')\n        df_harm=pd.DataFrame()\n        df_harm=find_dfa(test_i_d1,df_harm,10,0)\n        num_ap_cols = int(num_harm\/2)\n        fft_base_col=8\n        for j in range(0, num_ap_cols) :\n            metadata_df.iloc[r_start:r_stop,fft_base_col+2*j] = df_harm.iloc[r_start:r_stop,2*j]\n            metadata_df.iloc[r_start:r_stop,fft_base_col+2*j+1] = df_harm.iloc[r_start:r_stop,2*j+1]\n        metadata_df.iloc[r_start:r_stop,fft_base_col+num_harm] = df_harm.iloc[r_start:r_stop, num_harm]\n        metadata_df.iloc[r_start:r_stop,fft_base_col+num_harm+1] = df_harm.iloc[r_start:r_stop, num_harm+1]\n        col_id_start=col_id_stop\n    return (metadata_df)","3483276b":"%%time\nmetadata_test1=add_info_test(metadata_test,time_sample_idx,col_group_size)","d1f3da81":"metadata_test1.head()","181bcd55":"metadata_test1.shape","712e3fb6":"metadata_test1.iloc[0:12,:]","7934e184":"metadata_test1.to_csv('df_test.csv', index=False)","16fad788":"Now, let's put our attention on trios, starting from a \"good\" one.","6fc3fb40":"The test set has about **3 times** the rows of the training set.","f238cc78":"Let's start loading the train.parquet file, which is actually a \"second level\" train set, containing information to put into the \"usual\" train set, metadata_train.","75ccee72":"It's impossibile to load the test set as a whole, let's define a chunk size:","e2ea8a48":"I'd say that a failure is linked to peaks in two (or more phases), but let's go on. ","beb67153":"In addition, let's free all the avalable resources:","407c0731":"# Train Set Preparation (and a bit of EDA)","efae1bab":"Now let's restart from the metadata_test:","a880c6f7":"Here is the function definition:","fff850fb":"I joined this competition only a **few days ago** and the first problem I faced has been the data loading. My kernels always crashed due to RAM limitations while loading \"parquet\" files. This notebook has a **simple aim**: summarising the competition data and **generate two sets** (train and test) **easily usable in other kernels**, mainly focused on modelling.\n\nThanks to [bluexleoxgreen](https:\/\/www.kaggle.com\/bluexleoxgreen\/simple-feature-lightgbm-baseline), especially for the idea of **splitting** the huge test set into subsets of **2K** columns.\n\nChanges from the previous version: I deleted the part where I collected samples to put them into the exported sets. The sampling rates where too low and a lot of information about peaks and errors were lost. Here, instead, I try to add the information about amplitudes and phases of the first harmonics, with a downsampling much more \"rich\" than the previous one.\n\nRunning times of the kernel: **15 minutes**.\n\nIn any case, I'd highly appreciate comments or suggestions to improve my Python or the efficiency of this notebook (better slicing, multithreading or whatever).","11878ed7":"So we have, finally, our exportable test set:","bf6e4a4d":"The columns are the FK (signal_id). The values are voltage levels. The rows are sampling points (one every 20ms). Let's look at one signal:","0d77f28c":"# Test Set Preparation","27980498":"At this point we want to add some common indexes such as mean, std etc ... ","63013ac4":"And this is its call:","756cb0fd":"# Introduction","1479c63f":"# Data Loading","130bfba0":"Now it's time to have fun with a new notebook based on these data and focused on modelling. \n\nLike said Pink some years ago, \"[Let's get the party started!](https:\/\/www.youtube.com\/watch?v=QRINgISPUWQ)\"","38d9ee14":"# Imports","fe4c5ae9":"And now, here is one with target=1, i. e. a \"bad example\":"}}