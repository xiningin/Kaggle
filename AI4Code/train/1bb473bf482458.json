{"cell_type":{"b9b37e5e":"code","e25f6eb1":"code","ed380f61":"code","142d7d7d":"code","c5c909aa":"code","5a74b858":"code","5a4d2b3c":"code","8cba6c3c":"code","b83d405a":"code","ca241a02":"code","ded61db8":"code","97319a20":"code","941b7568":"code","45e48f37":"code","39d4a60d":"code","f37e28e5":"code","fd9d6c65":"code","0cbf980b":"code","415269be":"code","688648aa":"code","57f06504":"code","f4aa3b89":"code","789ebc50":"code","215a60fc":"code","c0e5efef":"code","63779544":"code","ffbd6e46":"code","db77d6c1":"code","becd7f26":"code","ee7cde52":"code","c012026d":"code","84413d97":"code","d9dd71a0":"code","5702d4cd":"code","632ddded":"markdown","8f0fbc4f":"markdown","47546dab":"markdown","6fdefe95":"markdown","0405ba36":"markdown","f919efc7":"markdown","72caafdb":"markdown","fc67f3ee":"markdown","f1629212":"markdown","6cbb5c60":"markdown","25bbe240":"markdown","ddba1949":"markdown","5fefd36c":"markdown","74bb8644":"markdown","d98d8cd5":"markdown"},"source":{"b9b37e5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e25f6eb1":"df = pd.read_csv(\"\/kaggle\/input\/tmdb-movie-metadata\/tmdb_5000_movies.csv\")\ndf.head()","ed380f61":"df['original_language'] = df['original_language'].apply(lambda x : 1 if x=='en' else 0 )","142d7d7d":"df['original_language'].unique()","c5c909aa":"df.rename(columns={\"original_language\": \"English\"},inplace=True)","5a74b858":"a = df['production_countries'][0]\na","5a4d2b3c":"import ast\na = ast.literal_eval(a)\na","8cba6c3c":"df['release_date']=df['release_date'].fillna('1992-09-04')","b83d405a":"df['release_date'].isna().sum()","ca241a02":"df['release_date']= pd.to_datetime(df['release_date']) \ndf['release_date']=df['release_date'].apply(lambda x: int(x.year))\ndf['release_date'].head()","ded61db8":"df","97319a20":"df.drop(['homepage', 'id','keywords','original_title','overview','status','tagline','title','English'], axis=1,inplace=True)","941b7568":"df['production_companies']=df['production_companies'].apply(lambda x: ast.literal_eval(x))\n\ndf['production_companies']=df['production_companies'].apply(lambda x: len(x))\ndf['production_companies'].head()","45e48f37":"df['genres']=df['genres'].apply(lambda x: ast.literal_eval(x))\n\ndf['genres']=df['genres'].apply(lambda x: len(x))\ndf['genres'].head()","39d4a60d":"df['production_countries']=df['production_countries'].apply(lambda x: ast.literal_eval(x))\n\ndf['production_countries']=df['production_countries'].apply(lambda x: len(x))\ndf['production_countries'].head()","f37e28e5":"df['spoken_languages']=df['spoken_languages'].apply(lambda x: ast.literal_eval(x))\ndf['spoken_languages']=df['spoken_languages'].apply(lambda x: len(x))\ndf['spoken_languages'].head()\n\n","fd9d6c65":"#df.rename(columns={\"spoken_languages\": \"Number of spoken_languages\"},inplace=True)\n#df.rename(columns={\"production_countries\": \"Number of countries produced in\"},inplace=True)\n#df.rename(columns={\"production_companies\": \"Number of producers\"},inplace=True)","0cbf980b":"df['runtime']=df['runtime'].fillna(df['runtime'].mean())","415269be":"df['popularity']=df['popularity'].apply(lambda x: int(x))\ndf['runtime']=df['runtime'].apply(lambda x: int(x))\ndf","688648aa":"df['production_companies']=df['production_companies'].replace(0,1)\ndf['production_countries']=df['production_countries'].replace(0,1)\n\nquant = 0.0156\ndf['revenue']=df['revenue'].replace(0,df['revenue'].quantile(quant))\ndf['budget']=df['budget'].replace(0,df['budget'].quantile(quant))\ndf['popularity']=df['popularity'].replace(0,df['popularity'].quantile(quant))\ndf['runtime']=df['runtime'].replace(0,df['runtime'].quantile(quant))\n\ndf['spoken_languages']=df['spoken_languages'].replace(0,1)","57f06504":"#df.drop(['runtime'], axis=1,inplace=True)","f4aa3b89":"df.info()","789ebc50":"df.columns","215a60fc":"X = df.drop(['revenue'],axis=1)\ny = df['revenue']","c0e5efef":"df.describe()","63779544":"from sklearn.preprocessing import MaxAbsScaler\nscaler = MaxAbsScaler()\nX=scaler.fit_transform(X)\n#y=scaler.fit_transform(y)","ffbd6e46":"\n# import matplotlib.pyplot as plt\n# from sklearn.decomposition import PCA\n\n# pca = PCA()\n# principalComponents = pca.fit_transform(X)\n\n# plt.figure()\n# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n# plt.xlabel('Number of Components')\n# plt.ylabel('Variance (%)') #for each component\n# plt.title('Explained Variance')\n# plt.show()","db77d6c1":"#pca = PCA(n_components=5)\n#X = pca.fit_transform(X)\n","becd7f26":"from sklearn.ensemble import RandomForestRegressor as forest\nclf = forest(max_depth=40,max_features=0.4,n_estimators=45,random_state=42)","ee7cde52":"from sklearn.model_selection import train_test_split \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","c012026d":"clf.fit(X_train,y_train)","84413d97":"clf.score(X_test,y_test)","d9dd71a0":"import pickle\n\nfilename = 'RandomForest_model.pickle'\npickle.dump(clf, open(filename, 'wb'))\n\nfilename_scaler = 'scaler_model.pickle'\npickle.dump(scaler, open(filename_scaler, 'wb'))\n","5702d4cd":"##Hyper parameter tuning \n\n# n_estimators = [int(x) for x in np.linspace(start = 40, stop = 120, num = 10)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 75, num = 10)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5, 10]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                }\n\n# from sklearn.model_selection import GridSearchCV\n# grid_search = GridSearchCV(estimator=clf,param_grid=random_grid,cv=2,n_jobs =-1,verbose = 3)\n# grid_search.fit(X_train, y_train)\n# grid_search.best_params_\n\n\n\n# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n# [Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   49.3s\n# [Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  6.5min\n# [Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 15.5min\n# [Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 26.2min\n# [Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 33.7min\n# \/opt\/conda\/lib\/python3.6\/site-packages\/joblib\/externals\/loky\/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n#   \"timeout or by a memory leak.\", UserWarning\n\n# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n# [Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   49.3s\n# [Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  6.5min\n# [Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 15.5min\n# [Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 26.2min\n# [Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 33.7min\n# \/opt\/conda\/lib\/python3.6\/site-packages\/joblib\/externals\/loky\/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n#   \"timeout or by a memory leak.\", UserWarning\n# [Parallel(n_jobs=-1)]: Done 1144 tasks      | elapsed: 53.7min\n","632ddded":"# Saving pickles","8f0fbc4f":"# Importing the regression Model ","47546dab":"# Splitiing the data for validation after trainin g","6fdefe95":"# Figuring out how to accept string data as lists in python ","0405ba36":"# Training and evaluating the model ","f919efc7":"# RENAMING SOME COLUMNS","72caafdb":"# DROP NO ESSENTIAL FEATURES","fc67f3ee":"# Scaling the data ","f1629212":"# Creating features and target label ","6cbb5c60":"# Filling nan values and changing datatypes ","25bbe240":"# REPLACING 0s ","ddba1949":"# Some Data needs to be length of a list instead of whole list ","5fefd36c":"# HYPER PARAMETER TUNING","74bb8644":"# Trying PCA","d98d8cd5":"# READ THE DATA"}}