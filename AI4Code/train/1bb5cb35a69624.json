{"cell_type":{"0ef88e2e":"code","98809941":"code","d0223dc0":"code","839b32da":"code","fc335ade":"code","6f8fb54e":"code","e48959db":"code","a90a444d":"code","bb694e12":"code","355b28e1":"code","d8a63db0":"code","8c8b6647":"code","5a219c73":"code","6b863971":"code","8804de10":"code","f1bfd7fb":"code","633b8f29":"code","96601466":"code","9df949c1":"code","17a8dcdf":"code","3618dc74":"code","c5a44e48":"code","7515decf":"code","7312631a":"code","f27d8ff6":"code","b56e811f":"code","c88e609f":"code","8f47e692":"markdown","7e2dfd9b":"markdown","6c76ceef":"markdown","445d2726":"markdown","3ff06439":"markdown","45e4ec2a":"markdown","5fb2d964":"markdown","3ae1a3db":"markdown","bf07fb6d":"markdown"},"source":{"0ef88e2e":"import time\nimport concurrent.futures\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport matplotlib.pyplot as plt\nimport PIL\nfrom PIL.ImageDraw import Draw","98809941":"# Image IDs and target values.\nMETA_FILE = '..\/input\/crowd-counting\/labels.csv'","d0223dc0":"# EfficientDet model\nMODEL_PATH = 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d0\/1'","839b32da":"def reconstruct_path(image_id: int) -> str:\n    \"\"\"Function transforms numerical image ID\n    into a relative file path filling in leading zeros\n    and adding file extension and directory.\n    :param image_id: Image ID\n    :return: Relative path to the image\n    \"\"\"\n    image_id = str(image_id).rjust(6, '0')\n    return f'..\/input\/crowd-counting\/frames\/frames\/seq_{image_id}.jpg'\n\n\ndef detect_objects(path: str, model) -> dict:\n    \"\"\"Function extracts image from a file, adds new axis\n    and passes the image through object detection model.\n    :param path: File path\n    :param model: Object detection model\n    :return: Model output dictionary\n    \"\"\"\n    image_tensor = tf.image.decode_jpeg(\n        tf.io.read_file(path), channels=3)[tf.newaxis, ...]\n    return model(image_tensor)\n\n\ndef count_persons(path: str, model, threshold=0.) -> int:\n    \"\"\"Function counts the number of persons in an image\n    processing \"detection_classes\" output of the model\n    and taking into account confidence threshold.\n    :param path: File path\n    :param model: Object detection model\n    :param threshold: Threshold for confidence scores\n    :return: Number of people for one image\n    \"\"\"\n    results = detect_objects(path, model)\n    # Class ID 1 = \"person\"\n    return (results['detection_classes'].numpy()[0] == 1)[np.where(\n        results['detection_scores'].numpy()[0] > threshold)].sum()\n\n\ndef draw_bboxes(image_path, data: dict, threshold=0.) -> PIL.Image:\n    \"\"\"Function displays an image with bounding boxes\n    overlaid for every detected person.\n    :param image_path: File path to an image\n    :param data: Output of objects detection model for this image\n    :param threshold: Threshold for confidence scores\n    :return: PIL.Image object\n    \"\"\"\n    image = PIL.Image.open(image_path)\n    draw = Draw(image)\n\n    im_width, im_height = image.size\n\n    boxes = data['detection_boxes'].numpy()[0]\n    classes = data['detection_classes'].numpy()[0]\n    scores = data['detection_scores'].numpy()[0]\n\n    for i in range(int(data['num_detections'][0])):\n        if classes[i] == 1 and scores[i] > threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            draw.line([(left, top), (left, bottom), (right, bottom), (right, top), (left, top)],\n                      width=4, fill='red')\n\n    return image\n\n\ndef set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format","fc335ade":"set_display()","6f8fb54e":"# Input data\ndata = pd.read_csv(META_FILE)\ndata['path'] = data['id'].apply(reconstruct_path)\ndata.head()","e48959db":"stats = data.describe()\nstats","a90a444d":"plt.hist(data['count'], bins=20)\nplt.axvline(stats.loc['mean', 'count'], label='Mean value', color='green')\nplt.legend()\nplt.xlabel('Number of people')\nplt.ylabel('Frequency')\nplt.title('Target Values')\nplt.show()","bb694e12":"# Load the model.\ndetector = hub.load(MODEL_PATH)","355b28e1":"# Object detection with no confidence threshold results in\n# duplicate bounding boxes and false positives.\n# Total number of people in an image is overestimated.\n# Some mannequins are erroneously marked as people.\nexample_path = '..\/input\/crowd-counting\/frames\/frames\/seq_000010.jpg'\nresults = detect_objects(example_path, detector)\ndraw_bboxes(example_path, results)","d8a63db0":"# With high threshold the model underestimates the number of people\n# selecting only the most obvious objects at the foreground.\ndraw_bboxes(example_path, results, threshold=0.5)","8c8b6647":"# With relatively low threshold the model is most accurate counting people\n# that are located at the foreground and the middle of the picture.\n# Objects at the background are mostly ignored.\ndraw_bboxes(example_path, results, threshold=0.25)","5a219c73":"# Test the model on an image with small number of people.\nexample_path = data.loc[data['count'] == data['count'].min(), 'path'].iloc[0]\nresults = detect_objects(example_path, detector)\ndraw_bboxes(example_path, results, threshold=0.25)","6b863971":"# Test the model on a very crowded image.\nexample_path = data.loc[data['count'] == data['count'].max(), 'path'].iloc[0]\nresults = detect_objects(example_path, detector)\ndraw_bboxes(example_path, results, threshold=0.25)","8804de10":"# Performing object detection for 2,000 images takes a considerable time.\n# The model processes images one at a time. Batch inference is not available.\n# We can speed up the model by using multiprocessing, however it still takes a while.\n# For testing purposes we will check the model performance on a randomly selected\n# sample of 200 images (10% of the original data set).\nsample = data.sample(frac=0.1)\nstart = time.perf_counter()\nobjects = []\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = [executor.submit(count_persons, path, detector, 0.25) for path in sample['path']]\n    for f in tqdm(concurrent.futures.as_completed(results)):\n        objects.append(f.result())\n\nfinish = time.perf_counter()\nprint(f'Finished in {round(finish - start, 2)} second(s).')","f1bfd7fb":"# Compare predicted values with the ground truth.\nsample['prediction'] = objects\nsample.head(10)","633b8f29":"sample['mae'] = (sample['count'] - sample['prediction']).abs()\nsample['mse'] = sample['mae'] ** 2\n\nprint(f'MAE = {sample[\"mae\"].mean()}\\nMSE = {sample[\"mse\"].mean()}')\nplt.hist(sample['mae'], bins=20)\nplt.title('Absolute Errors')\nplt.show()","96601466":"plt.scatter(sample['count'], sample['prediction'])\nplt.xlabel('Actual person count')\nplt.ylabel('Predicted person count')\nplt.title('Predicted vs. Actual Count')\nplt.show()","9df949c1":"import os\nimport random","17a8dcdf":"# TensorFlow settings and training parameters\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nEPOCHS = 500\nBATCH_SIZE = 16\nPATIENCE = 10\nLEARNING_RATE = 1e-3\nIMAGE_SIZE = 299","3618dc74":"def load_image(is_labelled: bool, is_training=True):\n    \"\"\"Wrapper function that returns a function\n    for loading a single image if is_labelled=False\n    or a function for loading image in an image-label pair\n    if is_labelled=True.\n    :param is_labelled: Boolean argument defining the return\n    :param is_training: Boolean argument for image augmentation\n    :return: Function\n    \"\"\"\n    def _get_image(path: str) -> tf.Tensor:\n        \"\"\"Function loads image from a file.\n        :param path: Path to image file\n        :return: Tensor with preprocessed image\n        \"\"\"\n        image = tf.image.decode_jpeg(tf.io.read_file(path), channels=3)\n        image = tf.cast(image, dtype=tf.int32)\n        image = tf.image.resize_with_pad(image, IMAGE_SIZE, IMAGE_SIZE)\n        if is_training:\n            image = tf.image.random_flip_left_right(image)\n            image = tf.image.random_brightness(image, 0.1)\n            image = tf.image.random_contrast(image, 0.1, 0.2)\n            image = tf.image.random_saturation(image, 0.9, 1.1)\n            image = tf.image.random_hue(image, 0.1)\n        return tf.keras.applications.inception_resnet_v2.preprocess_input(image)\n\n    def _get_image_label(img: tf.Tensor, label: int) -> tuple:\n        \"\"\"Function loads the image in an image-label pair.\n        :param img: Tensor with original image\n        :param label: Target value\n        :return: Tuple with TF tensor and label\n        \"\"\"\n        return _get_image(img), label\n\n    return _get_image_label if is_labelled else _get_image\n\n\ndef prepare_dataset(dataset, is_training=True, is_labeled=True):\n    \"\"\"Function transforms a TF dataset containing file paths\n    or file paths and labels to a dataset with image tensors and labels.\n    :param dataset: Original dataset\n    :param is_training: Argument defines if shuffling and image augmentation should be applied\n    :param is_labeled: Argument defines if the dataset contains labels\n    :return: Updated dataset\n    \"\"\"\n    image_read_fn = load_image(is_labeled, is_training)\n    dataset = dataset.map(image_read_fn, num_parallel_calls=AUTOTUNE)\n    return dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n\n\ndef create_model() -> tf.keras.Model:\n    \"\"\"Function initializes and compiles a regression model\n    with pretrained feature extractor.\n    :return: TF Model object\n    \"\"\"\n    feature_model = tf.keras.applications.InceptionResNetV2(\n        include_top=False, pooling='avg')\n    feature_model.trainable = False\n\n    model = tf.keras.Sequential([\n        tf.keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3)),\n        feature_model,\n        tf.keras.layers.Dense(512, activation='selu'),\n        tf.keras.layers.Dense(1)\n    ])\n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n                  loss=tf.keras.losses.MeanSquaredError(),\n                  metrics=[tf.keras.metrics.MeanAbsoluteError()])\n\n    return model\n\n\ndef plot_history(hist):\n    \"\"\"Function plots a chart with training and validation metrics.\n    :param hist: Tensorflow history object from model.fit()\n    \"\"\"\n    mae = hist.history['mean_absolute_error']\n    val_mae = hist.history['val_mean_absolute_error']\n    x_axis = range(1, len(mae) + 1)\n    plt.plot(x_axis, mae, 'bo', label='Training')\n    plt.plot(x_axis, val_mae, 'ro', label='Validation')\n    plt.title('MAE')\n    plt.legend()\n    plt.xlabel('Epochs')\n    plt.tight_layout()\n    plt.show()\n\n\ndef set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'","c5a44e48":"set_seed()","7515decf":"# Create train and validation data sets.\ndata_train = data.head(1700)\ndata_valid = data.tail(300)\n\nds_train = tf.data.Dataset.from_tensor_slices((data_train['path'], data_train['count']))\nds_valid = tf.data.Dataset.from_tensor_slices((data_valid['path'], data_valid['count']))\n\nds_train = prepare_dataset(ds_train)\nds_valid = prepare_dataset(ds_valid, is_training=False)","7312631a":"# Load the model.\nmodel = create_model()\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=PATIENCE,\n    restore_best_weights=True)\n\nlr_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', patience=1, cooldown=1, verbose=1,\n    factor=0.75, min_lr=1e-8)\n\nhistory = model.fit(ds_train, validation_data=ds_valid,\n                    epochs=EPOCHS, callbacks=[early_stop, lr_reduction],\n                    use_multiprocessing=True, workers=os.cpu_count())","f27d8ff6":"plot_history(history)","b56e811f":"mse, mae = model.evaluate(ds_valid)\nprint(f'Validation MSE = {mse}\\n'\n      f'Validation MAE = {mae}')","c88e609f":"model.save('model.h5')","8f47e692":"**EfficientDet model:** SSD with EfficientNet + BiFPN feature extractor, shared box predictor and focal loss, trained on COCO 2017 dataset. Several models of various sizes could be found at TF Hub. We will use the smallest model d0.\n\n**Model inputs:** a three-channel image of variable size - a tf.uint8 tensor with shape [1, height, width, 3] with values in [0, 255].\n\n#### Algorithm\n- Extract example images from .jpg files and convert to tf.Tensor without resizing or preprocessing.\n- Visualize model predictions overlaying predicted bounding boxes over the example images.\n- Select the minimum confidence score to improve the model accuracy.\n- Check the accuracy on randomly selected subset of images:\n  - EfficientDet model cannot process batches of images, so we process them one by one using multiprocessing for time optimization.\n  - Postprocess the model output for each image counting the number of objects identified as \"person\" with a selected confidence threshold.","7e2dfd9b":"# Counting the Crowds\nThis notebook demonstrates several approaches to automatic counting of people using images from indoor video cameras in a shopping center.\n\n- **Out-of-the-box solution: EfficientDet** object detection model loaded from **TensorFlow Hub**. The model is capable of\u00a0detecting a wide range of objects returning predicted class, bounding box coordinates and confidence score for each object. Benefits: doesn't require training, multiple models are available, could be easily deployed on various devices. Drawbacks: model is prone to errors when detecting multiple objects, objects partly accluded or located at the background, model is difficult to retrain and fine-tune.\n- **Transfer learning: InceptionResNetV2** as feature extractor with a new regression head. Benefits: model is relatively easy to fine-tune for a new task while retaining the useful knowledge of the original classifier. Drawbacks: despite higher accuracy compared to the previous solution, this model is not perfect and could not be used in environments where high precision is important.","6c76ceef":"We can conclude that out-of-the-box solution cannot be used for this task. The model is good at detecting objects at the foreground and middle of the image and fails at \"counting\" numerous objects of the same class located all over the image. Despite the attempts to improve performance with confidence scores, the eror rate is high.","445d2726":"Target values (people count) vary between 13 and 53 with a mean of 31.16. Values are normally distributed with the median value close to the mean.\n\nTotal number of images is 2,000.","3ff06439":"We will load **InceptionResNetV2 model** from **Keras** applications and freeze the original weights. The model will be trained with a new regression head. The learning rate will be adjusted whenever validation loss is getting worse.\n\nThe original model was trained on images of size 299 x 299. We will resize the images accordingly using padding to avoid distorting the objects. To compensate for small number of training samples we will apply various image augmentation techniques randomly changing brightness, contract, saturation and hue and flipping the images left-to-right.","45e4ec2a":"## Part 2: Transfer learning","5fb2d964":"## Part 1: Using out-of-the-box model","3ae1a3db":"We can see that the model is not perfect and occasionally drows duplicate bounding boxes or fails to detect a person at the background or in partially occluded areas of the image.\n\nLet's check the model on a randomly selected subset of images using predicted class 1 (\"person\") with confidence scores above 0.25.","bf07fb6d":"The output dictionary of the model contains:\n- num_detections: a tf.int tensor with only one value, the number of detections [N].\n- detection_boxes: a tf.float32 tensor of shape [N, 4] containing bounding box coordinates in the following order: [ymin, xmin, ymax, xmax].\n- detection_classes: a tf.int tensor of shape [N] containing detection class index from the label file.\n- detection_scores: a tf.float32 tensor of shape [N] containing detection scores.\n- raw_detection_boxes: a tf.float32 tensor of shape [1, M, 4] containing decoded detection boxes without Non-Max suppression. M is the number of raw detections.\n- raw_detection_scores: a tf.float32 tensor of shape [1, M, 90] and contains class score logits for raw detection boxes. M is the number of raw detections.\n- detection_anchor_indices: a tf.float32 tensor of shape [N] and contains the anchor indices of the detections after NMS.\n- detection_multiclass_scores: a tf.float32 tensor of shape [1, N, 90] and contains class score distribution (including background) for detection boxes in the image including background class.\n\nWe will use \"detection_classes\" to count the number of persons detected in the image. Index 1 corresponds to \"person\"."}}