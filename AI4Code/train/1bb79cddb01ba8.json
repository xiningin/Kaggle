{"cell_type":{"17df8332":"code","365ef70d":"code","281b85d3":"code","e1ca034e":"code","1adc2184":"code","19da6e3b":"code","d064781d":"code","48fd11ba":"code","e2088af0":"code","ef6821e3":"code","c8e0641f":"code","3a9e7d53":"code","63b6f37a":"markdown","1a0a3b2b":"markdown","5f50c40a":"markdown","55b7b1f4":"markdown","70703da1":"markdown","31edab46":"markdown","f2c7fbe3":"markdown","9a613853":"markdown","3741d84e":"markdown","37a4543a":"markdown"},"source":{"17df8332":"import torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom torchvision.utils import make_grid\n\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm","365ef70d":"# Function to visualizing images from tensor\n\ndef show_tensor_images(tensor_img, num_img=25, size=(1, 28, 28)):\n    tensor_img = (tensor_img + 1) \/ 2\n    unflat_img = tensor_img.detach().cpu()\n    img_grid = make_grid(unflat_img[:num_img], nrow=5)\n    plt.imshow(img_grid.permute(1, 2, 0).squeeze())\n    plt.show()","281b85d3":"# More transforms can be added\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_loader = DataLoader(\n    MNIST('data\/', download=True, transform=transform),\n    batch_size=128,\n    shuffle=True)","e1ca034e":"# Display images from train_loader\n\ndataiter = iter(train_loader)\nimages,labels = dataiter.next()\nprint('Shape of loading one batch:', images.shape)\nprint('Total no. of batches present in trainloader:', len(train_loader))\n\nshow_tensor_images(images)","1adc2184":"class Generator(nn.Module):\n\n    def __init__(self, z_dim=10, img_channel=1, hidden_dim=64):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        self.gen = nn.Sequential(\n            self.gen_block(z_dim, hidden_dim * 4),\n            self.gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.gen_block(hidden_dim * 2, hidden_dim),\n            self.gen_block(hidden_dim, img_channel, kernel_size=4, final_layer=True),\n        )\n\n    def gen_block(self, input_channel, output_channel, kernel_size=3, stride=2, final_layer=False):\n        '''\n        Function that return a basic Sequential block for the Generator of DCGAN, corresponding to\n            - Transposed Convolution\n            - Batchnorm (except for in the last layer)\n            - Activation\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channel, output_channel, kernel_size, stride),\n                nn.BatchNorm2d(output_channel),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channel, output_channel, kernel_size, stride),\n                nn.Tanh()\n            )\n\n    def forward(self, x):\n        x = x.view(len(x), self.z_dim, 1, 1)      # change the shape: (batch_size, channels=z_dim, width=1, height=1)\n        return self.gen(x)\n","19da6e3b":"class Discriminator(nn.Module):\n\n    def __init__(self, img_channel=1, hidden_dim=16):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            self.disc_block(img_channel, hidden_dim, kernel_size=4),\n            self.disc_block(hidden_dim, hidden_dim * 2),\n            self.disc_block(hidden_dim * 2, hidden_dim * 4, kernel_size=4, stride=1),\n            self.disc_block(hidden_dim * 4, 1, final_layer=True),\n        )\n\n    def disc_block(self, input_channel, output_channel, kernel_size=3, stride=2, final_layer=False):\n        '''\n        Function that return a basic Sequential block for the Discriminator of DCGAN, corresponding to\n            - Convolution\n            - Batchnorm (except for in the last layer)\n            - LeakyReLU activation with 0.2 slope of the leak\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channel, output_channel, kernel_size, stride),\n                nn.BatchNorm2d(output_channel),\n                nn.LeakyReLU(0.2, inplace=True)\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channel, output_channel, kernel_size, stride)\n            )\n  \n    def forward(self, image):\n        disc_pred = self.disc(image)\n        return disc_pred.view(len(disc_pred), -1)    # returns 1-dimension tensor representing fake\/real","d064781d":"# required parameters for training\n\nz_dim = 64\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","48fd11ba":"G = Generator(z_dim).to(device)\nD = Discriminator().to(device)\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        nn.init.normal_(m.weight, 0.0, 0.02)\n        nn.init.constant_(m.bias, 0)\n\nG = G.apply(weights_init)\nD = D.apply(weights_init)","e2088af0":"beta_1 = 0.5         # parameters control the optimizer's momentum\nbeta_2 = 0.999       # parameters control the optimizer's momentum\n\nG_opt = torch.optim.Adam(G.parameters(),lr=0.0002, betas=(beta_1, beta_2))\nD_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(beta_1, beta_2))\n\n# 'BinaryCrossEntropy Loss' is used to evaluate how well dicriminator and generator performs\ndef real_loss(disc_pred):\n    criterion = nn.BCEWithLogitsLoss()\n    loss = criterion(disc_pred, torch.ones_like(disc_pred))\n    return loss\n\ndef fake_loss(disc_pred):\n    criterion = nn.BCEWithLogitsLoss()\n    loss = criterion(disc_pred, torch.zeros_like(disc_pred))\n    return loss\n","ef6821e3":"display_step = 1000      # how often to display\/visualize the images\n\nepochs = 50\ncur_step = 0\nmean_generator_loss = 0\nmean_discriminator_loss = 0","c8e0641f":"for epoch in range(epochs):\n    for real_img,_ in tqdm(train_loader):\n\n        batch_size = real_img.size(0)\n        real_img = real_img.to(device)\n\n        #  Update Discriminator  #\n        D_opt.zero_grad()\n\n        noise = torch.randn(batch_size, z_dim, device=device)\n        fake_img = G(noise)\n        D_pred = D(fake_img.detach())\n        D_fake_loss = fake_loss(D_pred)\n\n        D_pred = D(real_img)\n        D_real_loss = real_loss(D_pred)\n        D_loss = (D_fake_loss + D_real_loss) \/ 2\n        mean_discriminator_loss += D_loss.item() \/ display_step    # Keep track of the average discriminator loss\n\n        D_loss.backward(retain_graph=True)\n        D_opt.step()\n\n        # Update Generator #\n        G_opt.zero_grad()\n        noise = torch.randn(batch_size, z_dim, device=device)\n        fake_img = G(noise)\n        D_pred = D(fake_img)\n        G_loss = real_loss(D_pred)\n        mean_generator_loss += G_loss.item() \/ display_step     # Keep track of the average generator loss\n\n        G_loss.backward()\n        G_opt.step()\n\n        # Visualization #\n        if cur_step % display_step == 0 and cur_step > 0:\n            print(f\"Step {cur_step}: Generator loss: {mean_generator_loss}, discriminator loss: {mean_discriminator_loss}\")\n            show_tensor_images(fake_img)\n            show_tensor_images(real_img)\n            mean_generator_loss = 0\n            mean_discriminator_loss = 0\n        cur_step += 1\n","3a9e7d53":"num_image = 30\nnoise = torch.randn(num_image, z_dim, device=device)\nwith torch.no_grad():\n    fake_img = G(noise)\nshow_tensor_images(fake_img)","63b6f37a":"## Training\n\nHere's roughly progression of DCGAN. On GPU this takes about 17-18 seconds per thousand steps. On CPU, this can take more than 4 hours per thousand steps.\n\n![MNIST_DCGAN_Progression.png](attachment:MNIST_DCGAN_Progression.png)","1a0a3b2b":"DCGAN is a very successful and influential GAN model developed in 2015. Most GANs today are at least loosely based on the DCGAN architecture.\n\n![DCGAN.png](attachment:DCGAN.png)\n\nHere are the main features of DCGAN:\n*   Use convolutions without any pooling layers\n*   Use batchnorm in both the generator and the discriminator\n*   Don't use fully connected hidden layers\n*   Use ReLU activation in the generator for all layers except for the output, which uses a Tanh activation.\n*   Use LeakyReLU activation in the discriminator for all layers except for the output, which does not use an activation\n*   Both Generator and Discriminator use 4 layers as 3 hidden layers and 1 output layer\n","5f50c40a":"## Load data\n\nIn these cells we are going to load MNIST dataset from torchvision datasets and apply transforms and creating Dataloader. After that visualization has been done so we can be familier with the dataset.","55b7b1f4":"## Create more samples","70703da1":"## Initialize Generator and Discriminator","31edab46":"## Define loss and Optimizer ","f2c7fbe3":"## Generator","9a613853":"# Deep Convolutional GAN (DCGAN)\n\nGenerative Adversarial Networks, or GANs for short, are an approach to **Generative Modeling** using deep learning methods. Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset.\n\nThe GAN model architecture involves two sub-models: a generator model for generating new examples and a discriminator model for classifying whether generated examples are real, from the domain, or fake, generated by the generator model.\n- **Generator:** Model that is used to generate new plausible examples from the problem domain.\n- **Discriminator:** Model that is used to classify examples as real (from the domain) or fake (generated).\n\n**Application of GANs:**\n* Generate Examples for Image Datasets(i.e, medical data such as X-ray, MRI)\n* Super Resolution\n* Clothing Translation\n* 3D Object Generation\n* Text-to-Image Translation\n\n\nHere is the implementation of a **Deep Convolutional GAN(DCGAN)** on MNIST dataset.\nIn this Notebook, we will see how to,\n   * Load torchvision dataset\n   * Create Generator network\n   * Create Discriminator network\n   * Train GAN\n   * Evaluate GAN during training by plotting results of Generator","3741d84e":"## Import Packages","37a4543a":"## Discriminator"}}