{"cell_type":{"6935a36f":"code","d7afb165":"code","9813c5ca":"code","f26ebea8":"code","5233eb69":"code","1e1e3eb4":"code","f5adf910":"code","8f13009a":"code","1411d8b6":"code","fd1f7c20":"code","24caed72":"code","0eda138e":"code","41dffe0b":"code","6a5e751a":"code","b6091077":"code","3d989bad":"code","7fe34446":"code","2a9c8393":"code","1de2c13b":"code","e4a05788":"code","9eae465b":"code","1d217178":"code","0712e00d":"code","f49a9f2a":"code","869d9be3":"code","516cec43":"code","9441306e":"code","f8185827":"code","bdb75dcf":"code","77edfb2c":"code","a9351563":"code","5f922eb1":"code","8f72d11c":"code","1d733bfd":"code","038b7c92":"code","81c7f47a":"code","6f3820b9":"code","fa9aa92d":"code","671f3612":"code","37e1172e":"code","27d55e9c":"code","95144e10":"markdown","4fc28c0a":"markdown","191a69c1":"markdown","a59995b8":"markdown","00ad2529":"markdown","fbf528ae":"markdown","692dffae":"markdown","8c4f0188":"markdown","e7d2f256":"markdown","05271ff7":"markdown","e0ee0704":"markdown","fd8680ef":"markdown","83f9c551":"markdown","f2fc95a3":"markdown","0f6a92d8":"markdown","cf643610":"markdown","32783bd5":"markdown","7d773709":"markdown"},"source":{"6935a36f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt","d7afb165":"df = pd.read_csv('..\/input\/goodreadsbooks\/books.csv', error_bad_lines=False)\ndf.head()","9813c5ca":"df.describe() # Generate the summary table of the data","f26ebea8":"df.dtypes # Check the data types of all columns","5233eb69":"df.isnull().sum() # Check if there's any missing value","1e1e3eb4":"from sklearn.preprocessing import OrdinalEncoder\n\nencoding = {'language_code':{'en-US': 'eng', 'en-GB': 'eng', 'en-CA': 'eng'}} # Unify the langauge codes\ndf.replace(encoding, inplace=True)\n\nenc = OrdinalEncoder()\nenc.fit(df[['language_code']])\ndf[['language_code']] = enc.fit_transform(df[['language_code']]) # Apply ordinal encoding on language_code to convert it into numerical column","f5adf910":"df['publication_date'] = pd.to_datetime(df['publication_date'], format='%m\/%d\/%Y', errors='coerce') # Convert data type of publication_date from object into date type\ndf[df['publication_date'].isnull()]","8f13009a":"df.loc[df.bookID == 31373, 'publication_date'] = '1999-10-01 00:00:00'\ndf.loc[df.bookID == 45531, 'publication_date'] = '1975-10-01 00:00:00'","1411d8b6":"df['year'] = pd.DatetimeIndex(df['publication_date']).year # Extract year of publication in a separate column\n\ndf.rename(columns = {'  num_pages': 'num_pages'}, inplace=True) # Rename the column to remove leading whitespaces","fd1f7c20":"df['num_occ'] = df.groupby('title')['title'].transform('count') # Add a new feature which has the number of occurences of each book","24caed72":"df['rate_occ'] = df['average_rating'] * df['num_occ']\ndf['rate_weight'] = df['average_rating'] * df['text_reviews_count']\ndf['rate_weight_2'] = df['average_rating'] * df['ratings_count']\ndf['rate_per_pages'] = df['average_rating'] * df['num_pages']","0eda138e":"fig = plt.gcf()\nfig.set_size_inches(26, 10)\ncorr = df.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True)","41dffe0b":"sns.relplot(x=\"num_occ\", y=\"average_rating\", data=df, height=7, aspect = 2)","6a5e751a":"fig = plt.gcf()\nfig.set_size_inches(26, 10)\nsns.lineplot(x=\"year\", y=\"average_rating\", data=df)","b6091077":"sns.relplot(x=\"language_code\", y=\"average_rating\", data=df, height=9, aspect = 2)","3d989bad":"sns.relplot(x=\"text_reviews_count\", y=\"average_rating\", data=df, height=9, aspect = 2)","7fe34446":"sns.relplot(x=\"num_pages\", y=\"average_rating\", data=df, height=9, aspect = 2)","2a9c8393":"fig = plt.gcf()\nfig.set_size_inches(26, 10)\nsns.lineplot(x=\"year\", y=\"text_reviews_count\", data=df)","1de2c13b":"label = df['average_rating'].values\ndf.drop(['bookID', 'title', 'authors', 'isbn', 'isbn13', 'publication_date', 'publisher', 'average_rating'], axis=1, inplace=True)","e4a05788":"# Split the Data into 70% - 30%\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df, label, test_size=0.3)","9eae465b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nmodel = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4))\n\nparameters = {\n    'learning_rate': [0.001, 0.01, 0.02, 0.1, 0.2, 1.0],\n    'n_estimators': [10, 50, 100, 200]\n}\n\ngrad_Ada = GridSearchCV(model, parameters, refit=True)\ngrad_Ada.fit(X_train, y_train)\n\nprint('Best Score: ', grad_Ada.best_score_*100, '\\nBest Parameters: ', grad_Ada.best_params_)","1d217178":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\n\nmodel  = LinearRegression()\n\nparameters = {\n    'fit_intercept': [True, False],\n    'normalize': [True, False],\n    \n}\n\ngrad_Linear = GridSearchCV(model, parameters, refit=True)\ngrad_Linear.fit(X_train, y_train)\n\nprint('Best Score: ', grad_Linear.best_score_*100, '\\nBest Parameters: ', grad_Linear.best_params_)","0712e00d":"from sklearn.linear_model import Ridge\n\nmodel = Ridge()\n\nparameters = {\n    'fit_intercept': [True, False],\n    'normalize': [True, False],\n    'max_iter': [1000, 100, 10000],\n    'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\n\ngrad_ridge = GridSearchCV(model, parameters, refit=True)\ngrad_ridge.fit(X_train, y_train)\n\nprint('Best Score: ', grad_ridge.best_score_*100, '\\nBest Parameters: ', grad_ridge.best_params_)","f49a9f2a":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\n\nparameters = {\n    'n_estimators': [50, 100, 150, 200],\n    'max_depth': [3, 5, 7, 10, 12, 15],\n    'min_samples_split': [5, 10, 15],\n    'min_samples_leaf': [5, 10, 15]\n}\n\ngrad_rf = GridSearchCV(model, parameters, refit=True, cv=10)\ngrad_rf.fit(X_train, y_train)\n\nprint('Best Score: ', grad_rf.best_score_*100, '\\nBest Parameters: ', grad_rf.best_params_)","869d9be3":"l = []\nl.append(('AdaBoost', grad_Ada.best_score_*100))\nl.append(('Linear Regression', grad_Linear.best_score_*100))\nl.append(('Ridge Regression', grad_ridge.best_score_*100))\nl.append(('Random Forest', grad_rf.best_score_*100))\nscores = pd.DataFrame(l, columns =['Model', 'Train Score'])","516cec43":"from sklearn.metrics import r2_score, mean_squared_error, accuracy_score","9441306e":"# AdaBoost Model\npred_adaboost = grad_Ada.predict(X_test)\n\n# Check Model Score\nprint(\"Residual sum of squares: \",  np.mean((pred_adaboost - y_test) ** 2))\nprint('RMSE: '+str(np.sqrt(mean_squared_error(y_test, pred_adaboost))))\nprint('Model Score on Test Data: ', grad_Ada.score(X_test, y_test))","f8185827":"from eli5.sklearn import PermutationImportance\nimport eli5\nperm = PermutationImportance(grad_Ada.best_estimator_, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","bdb75dcf":"plt.figure(figsize=(19,10))\nsns.regplot(pred_adaboost, y_test, marker=\"+\", line_kws={'color':'darkred','alpha':1.0})","77edfb2c":"# Linear Regression Model\npred_lr = grad_Linear.predict(X_test)\n\n# Check Model Score\nprint(\"Residual sum of squares: \",  np.mean((pred_lr - y_test) ** 2))\nprint('RMSE: '+str(np.sqrt(mean_squared_error(y_test, pred_lr))))\nprint('Model Score on Test Data: ', grad_Linear.score(X_test, y_test))","a9351563":"perm = PermutationImportance(grad_Linear.best_estimator_, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","5f922eb1":"plt.figure(figsize=(19,10))\nsns.regplot(pred_lr, y_test, marker=\"+\", line_kws={'color':'darkred','alpha':1.0})","8f72d11c":"# Ridge Regression Model\npred_ridge = grad_ridge.predict(X_test)\n\n# Check Model Score\nprint(\"Residual sum of squares: \",  np.mean((pred_ridge - y_test) ** 2))\nprint('RMSE: '+str(np.sqrt(mean_squared_error(y_test, pred_ridge))))\nprint('Model Score on Test Data: ', grad_ridge.score(X_test, y_test))","1d733bfd":"perm = PermutationImportance(grad_ridge.best_estimator_, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","038b7c92":"plt.figure(figsize=(19,10))\nsns.regplot(pred_ridge,y_test, marker=\"+\", line_kws={'color':'darkred','alpha':1.0})","81c7f47a":"# Random Forest Model\npred_rf = grad_rf.predict(X_test)\n\n# Check Model Score\nprint(\"Residual sum of squares: \",  np.mean((pred_rf - y_test) ** 2))\nprint('RMSE: '+str(np.sqrt(mean_squared_error(y_test, pred_rf))))\nprint('Model Score on Test Data: ', grad_rf.score(X_test, y_test))","6f3820b9":"perm = PermutationImportance(grad_ridge.best_estimator_, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","fa9aa92d":"plt.figure(figsize=(19,10))\nsns.regplot(pred_rf,y_test, marker=\"+\", line_kws={'color':'darkred','alpha':1.0})","671f3612":"l2 = []\nl2.append(('AdaBoost', grad_Ada.score(X_test, y_test)*100))\nl2.append(('Linear Regression', grad_Linear.score(X_test, y_test)*100))\nl2.append(('Ridge Regression', grad_ridge.score(X_test, y_test)*100))\nl2.append(('Random Forest', grad_rf.score(X_test, y_test)*100))\n\ntest_scores = pd.DataFrame(l2, columns =['Model', 'Test Score'])","37e1172e":"scores['Test Score'] = test_scores['Test Score']\nscores","27d55e9c":"scores.plot.bar()","95144e10":"# 6. Make Predictions using the 4 Models","4fc28c0a":"#### Features Importance","191a69c1":"#### Features Importance","a59995b8":"#### Features Importance","00ad2529":"# 1. Importing and Exploring Dataset","fbf528ae":"#### The upper visual says that any book appeared more than once has a good\/high rate","692dffae":"## Linear Regression Model","8c4f0188":"## Random Forest Model","e7d2f256":"# 3. Data Analysis & Visualizations","05271ff7":"#### Calculating New Features","e0ee0704":"# 2. Data Cleaning & Feature Engineering","fd8680ef":"## AdaBoost Model","83f9c551":"#### Features Importance","f2fc95a3":"## Ridge Regression Model","0f6a92d8":"# Goodreads Book Ratings Predictions\n![GoodReads](https:\/\/s.gr-assets.com\/assets\/facebook\/goodreads_wide-e23f6858b6bf20dcaf8493237a214a0e.png)\n\n### Content\n1. Exploring Data.\n2. Analyze Data through visualizations.\n3. Data Preparation e.g.: [Ordinal Encoding, Handling Missing Values].\n4. Feature Engineering.\n5. Building Multiple Machine Learning Models.\n6. Compare models accuracy on training data.\n7. Make predictions using each model.\n8. Compare models accuracy on test data.\n9. Compare training VS test score for each model","cf643610":"### We can see from the visual above that starting from the 80s, the rate\/number of reviews is getting higher than before, We can say that this's the effect of the computer & internet","32783bd5":"# 5. Creating Model","7d773709":"### Since there are only 2 books with courrpted dates, I googled these 2 books to get the publication dates and put them manually"}}