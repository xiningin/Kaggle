{"cell_type":{"13036cd1":"code","f3864dea":"code","cdce7008":"code","9acf110e":"code","a9350622":"code","d444bc35":"code","be80caa4":"code","9e9ca2f8":"code","f8fae4fb":"code","5dd09075":"code","9ee47920":"code","2ae11fca":"code","6995c0f2":"code","537ab9f1":"code","a78b8322":"code","9c7ca692":"code","95d916b5":"code","1c85dd75":"code","f031682d":"code","2873d969":"code","37fc3f15":"code","93caaabb":"code","111e1d2d":"code","706f9e78":"code","f1be3a03":"code","d4d463e4":"code","9fbbf8c3":"code","be07bacf":"code","fb328b62":"code","b5cea08a":"code","41c4cdb8":"code","4f98799c":"code","2bde79da":"code","129f0c04":"code","cc308b7e":"markdown","c82f09cd":"markdown","996df90d":"markdown","ca79a1a4":"markdown","2debb3ca":"markdown","46ba23eb":"markdown","4bbe727b":"markdown","18efa574":"markdown","fbbd08d0":"markdown","b7100388":"markdown","df414be9":"markdown","9af2acce":"markdown","52e9db73":"markdown","92e8f8e0":"markdown","13d7d577":"markdown","3dfcdb43":"markdown","2f3f6c7c":"markdown","2f8477ba":"markdown","28c3c843":"markdown","6c7d9b85":"markdown","7ce1f421":"markdown","2276854c":"markdown","5911d53a":"markdown","6b32994d":"markdown","06bc0bd8":"markdown","bd7ac3ef":"markdown","f39a1421":"markdown","63395049":"markdown","067b8b03":"markdown"},"source":{"13036cd1":"from skimage import io\nimport os\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random","f3864dea":"# function to plot n images using subplots\ndef plot_image(images, captions=None, cmap=None ):\n    f, axes = plt.subplots(1, len(images), sharey=True)\n    f.set_figwidth(15)\n    for ax,image in zip(axes, images):\n        ax.imshow(image, cmap)","cdce7008":"# path to your dataset\nDATASET_PATH = '\/kaggle\/input\/flowers-recognition\/flowers'\nflowers_cls = ['daisy', 'rose']","9acf110e":"# globbing example\n# help(glob)\nflower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\nprint(flower_path)\n\n# glob through the directory (returns a list of all file paths)\nflower_path = glob.glob(flower_path)\nprint(flower_path[3]) # access an individual file","a9350622":"# run this block multiple times to look at some randomly chosen images of roses\nrand_index = random.randint(0, len(flower_path))\nimage = io.imread(flower_path[rand_index])\nplt.imshow(image)\nplt.show()","d444bc35":"# plot a sample image\nflower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\nflower_path = glob.glob(flower_path)\n\n# access some element (a file) from the list\nimage = io.imread(flower_path[99])\nplt.imshow(image)\nplt.show()","be80caa4":"print(image.shape)","9e9ca2f8":"# plotting the original image and the RGB channels\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True)\nf.set_figwidth(15)\nax1.imshow(image)\n\n# RGB channels\nax2.imshow(image[:, : , 0])\nax3.imshow(image[:, : , 1])\nax4.imshow(image[:, : , 2])\nf.suptitle('Different Channels of Image')\nplt.show()","f8fae4fb":"# bin_image will be a (240, 320) True\/False array\nbin_image = image[:, :, 0] > 125\nplot_image([image, bin_image], cmap='gray')","5dd09075":"from skimage.morphology import binary_closing, binary_dilation, binary_erosion, binary_opening\nfrom skimage.morphology import selem\n\n# use a disk of radius 3\nselem = selem.disk(3)\n\n# oprning and closing\nopen_img = binary_opening(bin_image, selem)\nclose_img = binary_closing(bin_image, selem)\n\n# erosion and dilation\neroded_img = binary_erosion(bin_image, selem)\ndilated_img = binary_dilation(bin_image, selem)\n\nplot_image([bin_image, open_img, close_img, eroded_img, dilated_img], cmap='gray')","9ee47920":"norm1_image = image\/255\nnorm2_image = image - np.min(image)\/np.max(image) - np.min(image)\nnorm3_image = image - np.percentile(image,5)\/ np.percentile(image,95) - np.percentile(image,5)\n\nplot_image([image, norm1_image, norm2_image, norm3_image], cmap='gray')","2ae11fca":"from skimage import transform\n\n# flip left-right, up-down\nimage_flipr = np.fliplr(image)\nimage_flipud = np.flipud(image)\n\nplot_image([image, image_flipr, image_flipud])","6995c0f2":"# specify x and y coordinates to be used for shifting (mid points)\nshift_x, shift_y = image.shape[0]\/2, image.shape[1]\/2\n\n# translation by certain units\nmatrix_to_topleft = transform.SimilarityTransform(translation=[-shift_x, -shift_y])\nmatrix_to_center = transform.SimilarityTransform(translation=[shift_x, shift_y])\n\n# rotation\nrot_transforms =  transform.AffineTransform(rotation=np.deg2rad(45))\nrot_matrix = matrix_to_topleft + rot_transforms + matrix_to_center\nrot_image = transform.warp(image, rot_matrix)\n\n# scaling \nscale_transforms = transform.AffineTransform(scale=(2, 2))\nscale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\nscale_image_zoom_out = transform.warp(image, scale_matrix)\n\nscale_transforms = transform.AffineTransform(scale=(0.5, 0.5))\nscale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\nscale_image_zoom_in = transform.warp(image, scale_matrix)\n\n# translation\ntransaltion_transforms = transform.AffineTransform(translation=(50, 50))\ntranslated_image = transform.warp(image, transaltion_transforms)\n\n\nplot_image([image, rot_image, scale_image_zoom_out, scale_image_zoom_in, translated_image])","537ab9f1":"# shear transforms\nshear_transforms = transform.AffineTransform(shear=np.deg2rad(45))\nshear_matrix = matrix_to_topleft + shear_transforms + matrix_to_center\nshear_image = transform.warp(image, shear_matrix)\n\nbright_jitter = image*0.999 + np.zeros_like(image)*0.001\n\nplot_image([image, shear_image, bright_jitter])","a78b8322":"image_fliplr = np.fliplr(image)\nimage_final = np.flipud(image_fliplr)\n\nplot_image([image,image_fliplr,image_final])","9c7ca692":"norm3_image = (image - np.percentile(image,25))\/ (np.percentile(image,75) - np.percentile(image,25))\nplot_image([image,norm3_image])","95d916b5":"# First, define the shifting transformations\nshift_x, shift_y = image.shape[0]\/2, image.shape[1]\/2\nmatrix_to_topleft = transform.SimilarityTransform(translation=[-shift_x, -shift_y])\nmatrix_to_center = transform.SimilarityTransform(translation=[shift_x, shift_y])\n\n# Then, perform rotation transform\nrot_transforms =  transform.AffineTransform(rotation=np.deg2rad(90))\n\n# Then, perform the scaling transform with 4X zoom-in\nscale_transforms = transform.AffineTransform(scale=(0.25, 0.25))\n\n# Add up the transforms\nrot_plus_scale_matrix = matrix_to_topleft + rot_transforms + scale_transforms + matrix_to_center\n\n# Finally, apply the added-up transformation \nfinal_image = transform.warp(image, rot_plus_scale_matrix)\n\n# Plot the image\nplot_image([image,final_image])","1c85dd75":"import six\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input,\n    Activation,\n    Dense,\n    Flatten,\n    add,\n    BatchNormalization,\n    Conv2D,\n    MaxPooling2D,\n    AveragePooling2D\n)\n\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import backend as K\n\n\ndef _bn_relu(input):\n    \"\"\"Helper to build a BN -> relu block\n    \"\"\"\n    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n    return Activation(\"relu\")(norm)\n\n\ndef _conv_bn_relu(**conv_params):\n    \"\"\"Helper to build a conv -> BN -> relu block\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(input)\n        return _bn_relu(conv)\n\n    return f\n\n\ndef _bn_relu_conv(**conv_params):\n    \"\"\"Helper to build a BN -> relu -> conv block.\n    This is an improved scheme proposed in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\n    \"\"\"\n    filters = conv_params[\"filters\"]\n    kernel_size = conv_params[\"kernel_size\"]\n    strides = conv_params.setdefault(\"strides\", (1, 1))\n    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n    padding = conv_params.setdefault(\"padding\", \"same\")\n    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n\n    def f(input):\n        activation = _bn_relu(input)\n        return Conv2D(filters=filters, kernel_size=kernel_size,\n                      strides=strides, padding=padding,\n                      kernel_initializer=kernel_initializer,\n                      kernel_regularizer=kernel_regularizer)(activation)\n\n    return f\n\n\ndef _shortcut(input, residual):\n    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n    \"\"\"\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    input_shape = K.int_shape(input)\n    residual_shape = K.int_shape(residual)\n    stride_width = int(round(input_shape[ROW_AXIS] \/ residual_shape[ROW_AXIS]))\n    stride_height = int(round(input_shape[COL_AXIS] \/ residual_shape[COL_AXIS]))\n    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    if stride_width > 1 or stride_height > 1 or not equal_channels:\n        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1),\n                          strides=(stride_width, stride_height),\n                          padding=\"valid\",\n                          kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2(0.0001))(input)\n\n    return add([shortcut, residual])\n\n\ndef _residual_block(block_function, filters, repetitions, is_first_layer=False):\n    \"\"\"Builds a residual block with repeating bottleneck blocks.\n    \"\"\"\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            input = block_function(filters=filters, init_strides=init_strides,\n                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n        return input\n\n    return f\n\n\ndef basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n    Follows improved proposed scheme in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n                           strides=init_strides,\n                           padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n                                  strides=init_strides)(input)\n\n        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n    \"\"\"Bottleneck architecture for > 34 layer resnet.\n    Follows improved proposed scheme in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\n    Returns:\n        A final conv layer of filters * 4\n    \"\"\"\n    def f(input):\n\n        if is_first_block_of_first_layer:\n            # don't repeat bn->relu since we just did bn->relu->maxpool\n            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n                              strides=init_strides,\n                              padding=\"same\",\n                              kernel_initializer=\"he_normal\",\n                              kernel_regularizer=l2(1e-4))(input)\n        else:\n            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n                                     strides=init_strides)(input)\n\n        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n        return _shortcut(input, residual)\n\n    return f\n\n\ndef _handle_dim_ordering():\n    global ROW_AXIS\n    global COL_AXIS\n    global CHANNEL_AXIS\n    #if K.image_dim_ordering() == 'tf':\n    ROW_AXIS = 1\n    COL_AXIS = 2\n    CHANNEL_AXIS = 3\n    #else:\n    #CHANNEL_AXIS = 1\n    #ROW_AXIS = 2\n    #COL_AXIS = 3\n\n\ndef _get_block(identifier):\n    if isinstance(identifier, six.string_types):\n        res = globals().get(identifier)\n        if not res:\n            raise ValueError('Invalid {}'.format(identifier))\n        return res\n    return identifier\n\n\nclass ResnetBuilder(object):\n    @staticmethod\n    def build(input_shape, num_outputs, block_fn, repetitions):\n        \"\"\"Builds a custom ResNet like architecture.\n        Args:\n            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n            num_outputs: The number of outputs at final softmax layer\n            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n                The original paper used basic_block for layers < 50\n            repetitions: Number of repetitions of various block units.\n                At each block unit, the number of filters are doubled and the input size is halved\n        Returns:\n            The keras `Model`.\n        \"\"\"\n        _handle_dim_ordering()\n        if len(input_shape) != 3:\n            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n\n        # Permute dimension order if necessary\n        # if K.image_dim_ordering() == 'tf':\n        input_shape = (input_shape[1], input_shape[2], input_shape[0])\n\n        # Load function from str if needed.\n        block_fn = _get_block(block_fn)\n\n        input = Input(shape=input_shape)\n        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n\n        block = pool1\n        filters = 64\n        for i, r in enumerate(repetitions):\n            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n            filters *= 2\n\n        # Last activation\n        block = _bn_relu(block)\n\n        # Classifier block\n        block_shape = K.int_shape(block)\n        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n                                 strides=(1, 1))(block)\n        flatten1 = Flatten()(pool2)\n        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n                      activation=\"softmax\")(flatten1)\n\n        model = Model(inputs=input, outputs=dense)\n        return model\n\n    @staticmethod\n    def build_resnet_18(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n\n    @staticmethod\n    def build_resnet_34(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_50(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n\n    @staticmethod\n    def build_resnet_101(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n\n    @staticmethod\n    def build_resnet_152(input_shape, num_outputs):\n        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])","f031682d":"\n# specify image size and channels\nimg_channels = 3\nimg_rows = 100\nimg_cols = 100\n\n# number of classes\nnb_classes = 2","2873d969":"import numpy as np\nimport tensorflow as tf\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    \n    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n        \"\"\"\n        Initialise the data generator\n        \"\"\"\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = {}\n        self.list_IDs = []\n        \n        # glob through directory of each class \n        for i, cls in enumerate(flowers_cls):\n            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n            brk_point = int(len(paths)*0.8)\n            if mode == 'train':\n                paths = paths[:brk_point]\n            else:\n                paths = paths[brk_point:]\n            if ablation is not None:\n                paths = paths[:ablation]\n            self.list_IDs += paths\n            self.labels.update({p:i for p in paths})\n            \n        self.n_channels = n_channels\n        self.n_classes = len(flowers_cls)\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n        \n        delete_rows = []\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            img = io.imread(ID)\n            img = img\/255\n            if img.shape[0] > 100 and img.shape[1] > 100:\n                h, w, _ = img.shape\n                img = img[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n            else:\n                delete_rows.append(i)\n                continue\n            \n            X[i,] = img\n          \n            # Store class\n            y[i] = self.labels[ID]\n        \n        X = np.delete(X, delete_rows, axis=0)\n        y = np.delete(y, delete_rows, axis=0)\n        return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)","37fc3f15":"# using resnet 18\nmodel = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy', optimizer='SGD',\n              metrics=['accuracy'])\n\n# create data generator objects in train and val mode\n# specify ablation=number of data points to train on\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\n# fit: this will fit the net on 'ablation' samples, only 1 epoch\nmodel.fit(training_generator, epochs=1, validation_data=validation_generator)","93caaabb":"# resnet 18\nmodel = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer='SGD',\n              metrics=['accuracy'])\n\n# generators\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\n# fit\nmodel.fit(training_generator, epochs=20, validation_data=validation_generator)","111e1d2d":"# generic way to create custom callback\nclass LossHistory(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))","706f9e78":"# help(tf.keras.callbacks.Callback)","f1be3a03":"from tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import *\n\n# range of learning rates to tune\nhyper_parameters_for_lr = [0.1, 0.01, 0.001]\n\n# callback to append loss\nclass LossHistory(tf.keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.losses.append(logs.get('loss'))\n\n# instantiate a LossHistory() object to store histories\nhistory = LossHistory()\nplot_data = {}\n\n# for each hyperparam: train the model and plot loss history\nfor lr in hyper_parameters_for_lr:\n    print ('\\n\\n'+'=='*20 + '   Checking for LR={}  '.format(lr) + '=='*20 )\n    sgd = optimizers.SGD(lr=lr, clipnorm=1.)\n    \n    # model and generators\n    model = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n    model.compile(loss='categorical_crossentropy',optimizer= sgd,\n                  metrics=['accuracy'])\n    training_generator = DataGenerator('train', ablation=100)\n    validation_generator = DataGenerator('val', ablation=100)\n    model.fit(training_generator, epochs=5, validation_data=validation_generator, callbacks=[history])\n    \n    # plot loss history\n    plot_data[lr] = history.losses","d4d463e4":"# plot loss history for each value of hyperparameter\nf, axes = plt.subplots(1, 3, sharey=True)\nf.set_figwidth(15)\n\nplt.setp(axes, xticks=np.arange(0, len(plot_data[0.01]), 1)+1)\n\nfor i, lr in enumerate(plot_data.keys()):\n    axes[i].plot(np.arange(len(plot_data[lr]))+1, plot_data[lr])\n    axes[i].grid()\n","9fbbf8c3":"# learning rate decay\nclass DecayLR(tf.keras.callbacks.Callback):\n    def __init__(self, base_lr=0.001, decay_epoch=1):\n        super(DecayLR, self).__init__()\n        self.base_lr = base_lr\n        self.decay_epoch = decay_epoch \n        self.lr_history = []\n        \n    # set lr on_train_begin\n    def on_train_begin(self, logs={}):\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n\n    # change learning rate at the end of epoch\n    def on_epoch_end(self, epoch, logs={}):\n        new_lr = self.base_lr * (0.5 ** (epoch \/\/ self.decay_epoch))\n        self.lr_history.append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        tf.keras.backend.set_value(self.model.optimizer.lr, new_lr)\n\n# to store loss history\nhistory = LossHistory()\nplot_data = {}\n\n# start with lr=0.1\ndecay = DecayLR(base_lr=0.1)\n\n# model\nsgd = optimizers.SGD()\nmodel = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer= sgd,\n              metrics=['accuracy'])\ntraining_generator = DataGenerator('train', ablation=100)\nvalidation_generator = DataGenerator('val', ablation=100)\n\nmodel.fit(training_generator, epochs=5, validation_data=validation_generator, callbacks=[history, decay])\n\nplot_data[lr] = decay.lr_history","be07bacf":"plt.plot(np.arange(len(decay.lr_history)), decay.lr_history)\nplt.grid()","fb328b62":"# keras data generator\n# help(ImageDataGenerator)\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)","b5cea08a":"import numpy as np\nimport tensorflow as tf\n\n# data generator with augmentation\nclass AugmentedDataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = {}\n        self.list_IDs = []\n        self.mode = mode\n        \n        for i, cls in enumerate(flowers_cls):\n            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n            brk_point = int(len(paths)*0.8)\n            if self.mode == 'train':\n                paths = paths[:brk_point]\n            else:\n                paths = paths[brk_point:]\n            if ablation is not None:\n                paths = paths[:ablation]\n            self.list_IDs += paths\n            self.labels.update({p:i for p in paths})\n        \n            \n        self.n_channels = n_channels\n        self.n_classes = len(flowers_cls)\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size), dtype=int)\n        \n        delete_rows = []\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            img = io.imread(ID)\n            img = img\/255\n            if img.shape[0] > 100 and img.shape[1] > 100:\n                h, w, _ = img.shape\n                img = img[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n            else:\n                delete_rows.append(i)\n                continue\n            \n            X[i,] = img\n          \n            # Store class\n            y[i] = self.labels[ID]\n        \n        X = np.delete(X, delete_rows, axis=0)\n        y = np.delete(y, delete_rows, axis=0)\n        \n        # data augmentation\n        if self.mode == 'train':\n            aug_x = np.stack([datagen.random_transform(img) for img in X])\n            X = np.concatenate([X, aug_x])\n            y = np.concatenate([y, y])\n        return X, tf.keras.utils.to_categorical(y, num_classes=self.n_classes)\n        ","41c4cdb8":"from sklearn.metrics import roc_auc_score\n\nclass roc_callback(Callback):\n    \n    def on_train_begin(self, logs={}):\n        logs['val_auc'] = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        y_p = []\n        y_v = []\n        for i in range(len(validation_generator)):\n            x_val, y_val = validation_generator[i]\n            y_pred = self.model.predict(x_val)\n            y_p.append(y_pred)\n            y_v.append(y_val)\n        y_p = np.concatenate(y_p)\n        y_v = np.concatenate(y_v)\n        roc_auc = roc_auc_score(y_v, y_p)\n        print ('\\nVal AUC for epoch{}: {}'.format(epoch, roc_auc))\n        logs['val_auc'] = roc_auc\n","4f98799c":"# model\nmodel = ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\nmodel.compile(loss='categorical_crossentropy',optimizer= sgd,\n              metrics=['accuracy'])\ntraining_generator = AugmentedDataGenerator('train', ablation=32)\nvalidation_generator = AugmentedDataGenerator('val', ablation=32)\n\n# checkpoint \nfilepath = '\/kaggle\/working\/models\/best_model.hdf5'\ncheckpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\nauc_logger = roc_callback()\n\n# fit\nmodel.fit(training_generator, epochs=20, validation_data=validation_generator, callbacks=[auc_logger, history, decay, checkpoint])","2bde79da":"plt.imshow(image)\nplt.show()","129f0c04":"h, w, _ = image.shape\nimg = image[int(h\/2)-50:int(h\/2)+50, int(w\/2)-50:int(w\/2)+50, : ]\n\nmodel.predict(img[np.newaxis,: ])","cc308b7e":"Write code to perform a left-right flip, followed by an up-down flip to the same image.","c82f09cd":"## Final Run\n\nLet's now train the final model. Note that we will keep saving the best model's weights at `models\/best_models.hdf5`, so you will need to create a directory `models`. Note that model weights are usually saved in hdf5 files.\n\n**Saving the best model** is done using the callback functionality that comes with `ModelCheckpoint`. We basically specify the `filepath` where the model weights are to be saved, ` monitor='val_auc'` specifies that you are choosing the best model based on validation accuracy, `save_best_only=True` saves only the best weights, and `mode='max'` specifies that the validation accuracy is to be maximised.","996df90d":"In the code below, we have created a custom callback to append the loss to a list at the end of every epoch. Note that `logs` is an attribute (a dictionary) of `tf.keras.callbacks.Callback`, and we are using it to get the value of the key 'loss'. Some other keys of this dict are `acc`, `val_loss` etc.\n\nTo tell the model that we want to use a callback, we create an object of `LossHistory` called `history` and pass it to `model.fit_generator` using `callbacks=[history]`. In this case, we only have one callback `history`, though you can pass multiple callback objects through this list (an example of multiple callbacks is in the section below - see the code block of `DecayLR()`).\n\nWe highly recommend you to <a href=\"https:\/\/keras.io\/callbacks\/\">read the documentation of keras callbacks here<\/a>. For a gentler introduction to callbacks, you can read this <a href=\"https:\/\/machinelearningmastery.com\/check-point-deep-learning-models-keras\/\"> nice blog post by Jason Brownlee.<\/a>\n\n","ca79a1a4":"### Hyperparameter Tuning\n\nFirst let's make a list the hyper-parameters we want to tune:\n\n1. Learning Rate & Variation + Optimisers \n2. Augmentation Techniques\n\nThe basic idea is to track the validation loss with increasing epochs for various values of a hyperparameter. \n\n#### Keras Callbacks ####\n\nBefore you move ahead, let's discuss a bit about **callbacks**. Callbacks are basically actions that you want to perform at specific instances of training. For example, we want to perform the action of storing the loss at the end of every epoch (the instance here is the end of an epoch).\n\n\nFormally, a callback is simply a function (if you want to perform a single action), or a list of functions (if you want to perform multiple actions), which are to be executed at specific events (end of an epoch, start of every batch, when the accuracy plateaus out, etc.). Keras provides some very useful callback functionalities through the class `tf.keras.callbacks.Callback`. \n\nKeras has many builtin callbacks (<a href=\"https:\/\/keras.io\/callbacks\/\">listed here<\/a>). The generic way to **create a custom callback in keras** is:\n","2debb3ca":"The results above show that a learning rate of 0.1 is the best, though using such a high learning rate for the entire training is usually not a good idea. Thus, we should use **learning rate decay** - starting from a high learning rate and decaying it with every epoch.\n\nWe use another **custom callback** (`DecayLR`) to decay the learning rate at the end of every epoch. The decay rate is specified as 0.5 ^ epoch. Also, note that this time we are telling the model to **use two callbacks** (passed as a list `callbacks=[history, decay]` to `model.fit_generator`).\n\n\nAlthough we have used out own custom decay implementation here, you can use the ones built into <a href=\"https:\/\/keras.io\/optimizers\/\">keras optimisers<\/a> (using the `decay` argument).","46ba23eb":"## Morphological Transformations","4bbe727b":"#### Checking that the network is 'working'\n\nThe first part of building a network is to get it to run on your dataset. Let's try fitting the net on only a few images and just one epoch. Note that since `ablation=100` is specified, 100 images of each class are used, so total number of batches is `np.floor(200\/32)` = 6. \n\nNote that the `DataGenerator`  class 'inherits' from the `tf.keras.utils.Sequence` class, so it has all the functionalities of the base `tf.keras.utils.Sequence` class (such as the `model.fit_generator` method).","18efa574":"The results show that the training accuracy increases consistently with each epoch. The validation accuracy also increases and then plateaus out - this is a sign of 'good fit', i.e. we know that the model is at least able to learn from a small dataset, so we can hope that it will be able to learn from the entire set as well.","fbbd08d0":"### Overfitting on the Training Data\n\nLet's now perform another important step which should be done before training the full-fledged model-  trying to **deliberately overfit the model** on a small dataset.\n\nWe'll use ablation=100 (i.e. training on 100 images of each class), so it is still a very small dataset, and we will use 20 epochs. In each epoch, 200\/32=6 batches will be used.","b7100388":"Normalise the image between the 25th and 75th percentiles.","df414be9":"### Data Generator ###\n\nLet's now set up the **data generator**. The code below sets up a custom data generator which is slightly different than <a href=\"https:\/\/keras.io\/preprocessing\/image\/\">the one that comes with the keras API<\/a>. The reason to use a custom generator is to be able to modify it according to the problem at hand (customizability). \n\n\nWe won't be going through the entire code step-by-step in the lectures, though the code is explained below.\n\nTo start with, we have the training data stored in $n$ directories (if there are $n$ classes). For a given batch size, we want to generate batches of data points and feed them to the model.\n\n\nThe first `for` loop 'globs' through each of the classes (directories). For each class, it stores the path of each image in the list `paths`. In training mode, it subsets `paths` to contain the first 80% images; in validation mode it subsets the last 20%. In the special case of an ablation experiment, it simply subsets the first `ablation` images of each class.\n\nWe store the paths of all the images (of all classes) in a combined list `self.list_IDs`. The dictionary `self.labels` contains the labels (as key:value pairs of `path: class_number (0\/1)`).\n\nAfter the loop, we call the method `on_epoch_end()`, which creates an array `self.indexes` of length `self.list_IDs` and shuffles them (to shuffle all the data points at the end of each epoch).\n\nThe `_getitem_` method uses the (shuffled) array `self.indexes` to select a `batch_size` number of entries (paths) from the path list `self.list_IDs`. \n\nFinally, the method `__data_generation` returns the batch of images as the pair X, y where X is of shape `(batch_size, height, width, channels)` and y is of shape `(batch size, )`. Note that `__data_generation` also does some preprocessing - it normalises the images (divides by 255) and crops the center 100 x 100 portion of the image. Thus, each image has the shape `(100, 100, num_channels)`. If any dimension (height or width) of an image less than 100 pixels, that image is deleted.\n","9af2acce":"Using https:\/\/github.com\/raghakot\/keras-resnet\/resnet.py","52e9db73":"## Augmentations","92e8f8e0":"Perform a 90-degree rotation, and follow it up with a 4x zoom-in.","13d7d577":"# Network Building\n\nLet's now build the network. We'll import the resnet architecture from the module `resnet.py`.","3dfcdb43":"We'll now use the `glob` module of python to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Glob_(programming)\">glob<\/a> through the directory where the data is stored, i.e. to walk through the directory, subdirectories and the files. It uses regular expressions to access files having names matching some pattern. In our case, we want to access all the files in the path `flowers\/rose\/` and `flowers\/daisy\/`, so we'll just use the regex `*` (used as a 'wildcard' to catch everything). \n\nAn example of how the glob module works is given below - you first join the base directory path with the subdirectory (e.g. `flowers\/rose\/`) and then `glob` through it to access all the individual files (images here).","2f3f6c7c":"**Erosion** shrinks bright regions and enlarges dark regions. **Dilation** on the other hand is exact opposite side - it shrinks dark regions and enlarges the bright regions. \n\n**Opening** is erosion followed by dilation. Opening can remove small bright spots (i.e. \u201csalt\u201d) and connect small dark cracks. This tends to \u201copen\u201d up (dark) gaps between (bright) features.\n\n**Closing** is dilation followed by erosion. Closing can remove small dark spots (i.e. \u201cpepper\u201d) and connect small bright cracks. This tends to \u201cclose\u201d up (dark) gaps between (bright) features.\n\nAll these can be done using the `skimage.morphology` module. The basic idea is to have a **circular disk** of a certain size (3 below) move around the image and apply these transformations using it.","2f8477ba":"##  Image Augmentation & Preprocessing\n\nIn the following section, we'll look at some common image preprocessing techniques.","28c3c843":"### Understanding images and channels\n\nAs these images are RGB images they would constitute three channels - one for each of the color channels","6c7d9b85":"## Augmentation Techniques\n\nLet's now write some code to implement data augmentation. Augmentation is usually done with data generators, i.e. the augmented data is generated batch-wise, on the fly. \n\nYou can either use the built-in keras `ImageDataGenerator` or write your own data generator (for some custom features etc if you want). The two cells below show how to implement these respectively. ","7ce1f421":"There are multiple types of augmentations possible. The basic ones transform the original image using one of the following types of transformations:\n\n1. Linear transformations\n2. Affine transformations","2276854c":"## Normalisation ","5911d53a":"### Metrics to optimise\n\nAUC is often a better metric than accuracy. So instead of optimising for accuracy, let's monitor AUC and choose the best model based on AUC on validaton data. We'll use the callbacks `on_train_begin` and `on_epoch_end` to initialise  (at the start of each epoch) and store the AUC (at the end of epoch).","6b32994d":"# Working with Images\n\nIn this notebook, we will go through the end-to-end pipeline of training conv nets, i.e. organising the data into directories, preprocessing, data augmentation, model building etc.  ","06bc0bd8":"If you want to implement your own customized data generator (with augmentation), you can add the augmentation step easily to the `DataGenerator` class created above. The only change is that we stack the augmented images to the X, y arrays (as done in the last section of the code below).","bd7ac3ef":"### Thresholding\n\nOne of the simpler operations where we take all the pixels whose intensities are above a certain threshold, and convert them to ones; the pixels having value less than the threshold are converted to zero. This results in a *binary image*.","f39a1421":"Normalisation is the most crucial step in the pre-processing part. There are multiple ways to normalise images which we will be talking about.","63395049":"### Erosion, Dilation, Opening & Closing","067b8b03":"### Running Ablation Experiments \n\nBefore training the net on the entire dataset, you should always try to first run some experiments to check whether the net is fitting on a small dataset or not. "}}