{"cell_type":{"f74fa112":"code","bc7e3bbb":"code","100a0a00":"code","81b0fbbc":"code","b8c3fef1":"code","9a6a1ff1":"code","6b339081":"code","ce969a43":"code","062572ae":"code","9d7da173":"code","5dda0c57":"code","6f3a1de1":"code","c33f2fb2":"code","7f471c97":"code","44506e75":"code","5917a6c5":"code","7b2fd1ed":"code","48e91ec7":"code","a5ea8e0c":"code","849a525e":"code","2d02060a":"code","468528d2":"code","47ea9cee":"code","dd0703e0":"code","df5294d6":"code","57b01447":"code","810d868a":"code","3ef2133f":"code","4aea5fa0":"code","53462e26":"code","7b63dcf0":"code","0c40248a":"code","1d8f298a":"code","618da3a8":"code","01212233":"code","1366f29e":"code","95872b6a":"code","4919bef5":"code","c841a0dc":"code","c90e7298":"code","f9819e52":"code","2e8da1b0":"code","d4b36a27":"code","def3e483":"code","9351c51b":"code","f1463702":"code","6d7e26d4":"code","55461e0f":"code","41560db0":"code","78eeafa2":"code","c14913c4":"code","1703d36b":"code","31a7ee04":"code","de3fd086":"code","64c65ac6":"code","696e0b8d":"code","106e1467":"code","8b36274f":"code","6afe964d":"code","c1c653af":"code","fa375831":"code","718829b5":"code","ae94e6e4":"code","4fdbfba8":"code","a9d17b9c":"code","e623fcab":"code","19dd1016":"code","57ad6146":"code","f1ff6f47":"code","26d29e15":"code","c01f342d":"code","8f5852ff":"code","ca0c5594":"code","fe20226d":"code","d3bdf78b":"code","5d48a350":"code","00042dcd":"code","69c3e4fc":"code","5173f51a":"code","425a73ae":"code","536222a2":"code","9f58292a":"code","3229088b":"code","96d83054":"code","26d31308":"code","68748bc7":"code","e3eef930":"code","ecf32a43":"code","ec2cab1b":"code","961e3142":"code","d04dc0f8":"code","c5a03591":"code","16354640":"code","348a689b":"code","f207ab9a":"markdown","af177666":"markdown","27edbec2":"markdown","0536a50f":"markdown","14a9e5f4":"markdown","49e85956":"markdown","b42291cd":"markdown","6d00278f":"markdown","ca32c55c":"markdown","6e9474d8":"markdown","0e27c5c3":"markdown","4a3c6fb2":"markdown","0b5595d1":"markdown","110a066c":"markdown","e56c2406":"markdown","671d4d73":"markdown","212c375a":"markdown","336e5f9f":"markdown","1a210430":"markdown","cbd044d0":"markdown","3d567f30":"markdown","4aa05e41":"markdown","7634d575":"markdown","ee5288ef":"markdown","70df706b":"markdown","0910f67b":"markdown","de79c64c":"markdown","cb01edec":"markdown","fce1242d":"markdown","0a938396":"markdown","d9f3db1c":"markdown","18dd5f49":"markdown","8919c78a":"markdown","587b7e81":"markdown","fbc3149c":"markdown","66927d84":"markdown","53dc332f":"markdown","f48087c1":"markdown","57496529":"markdown","1e6446bc":"markdown","8e3d1afb":"markdown","4147a8df":"markdown","8725c330":"markdown","15abe265":"markdown","3fc34cae":"markdown","fee74e05":"markdown","da07e830":"markdown","6b7282dd":"markdown","b4b02ace":"markdown","ca3f6b98":"markdown","d9b93e8f":"markdown","2df76224":"markdown","1c026b09":"markdown","a2063766":"markdown","e015b729":"markdown","19183edc":"markdown","a0eb7b2a":"markdown","1dcab86b":"markdown","67d7f3c1":"markdown","244b24b9":"markdown","10612a50":"markdown","4967089c":"markdown","dcd9031b":"markdown","36577012":"markdown","0fafc88c":"markdown","7e07c2b8":"markdown","194a073e":"markdown","c1f4c1f8":"markdown"},"source":{"f74fa112":"#Data Processing\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n\n#Fix Seaborn Kaggle Bug\n!pip install seaborn --upgrade\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt \nimport matplotlib.dates as mdates\nimport seaborn as sns\n%matplotlib inline\n\n#Set visualization style\nsns.set(style=\"whitegrid\")\nmpl.rcParams['figure.figsize'] = (6, 6)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings('ignore')","bc7e3bbb":"#Creates a count plot for 1 feature with percentages for each category\n\ndef plt_count(df, cat, title=None, ax=None):\n    \n    #Count values and percentages for each category in a categorical variable\n    val_counts = df[cat].value_counts()\n    per_counts = df[cat].value_counts(normalize=True)\n    \n    if ax==None:\n        fig, ax = plt.subplots()\n    \n    val_counts.plot.bar(ax=ax)\n    \n    max_height = max([p.get_height() for p in ax.patches])\n    min_height = min([p.get_height() for p in ax.patches])\n    \n    for i,p in enumerate(ax.patches):\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{per_counts.iloc[i]:.1%}', (x + width\/2, y+height+min_height*0.1), ha='center')\n    \n    ylim=max_height+max_height\/\/2.5\n    ax.set_ylim(0, ylim)\n    \n    if title==None:\n        title=cat\n    ax.set_title(title)\n    plt.tight_layout()","100a0a00":"data = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\n\n#The last 2 columns are predictions from previous models. We'll delete them, as they are irrelevant\ndata.drop(data.columns[-2:], axis=1, inplace=True) \n\n#Drop Client Id. It can create data leakage. A complex model may detect patterns in the customer id that will not be relevant in\n#the future\ndata.drop(data.columns[0], axis=1, inplace=True) \n\n#Quick look at the Data\ndata","81b0fbbc":"data.info()","b8c3fef1":"round((data=='Unknown').sum()\/len(data)*100,1).astype(str)+'%'","9a6a1ff1":"plt_count(data,'Attrition_Flag')","6b339081":"#Use stratified sampling to split the dataset\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=10)\nfor train_index, test_index in split.split(data, data[\"Attrition_Flag\"]):\n    train_set = data.loc[train_index]\n    test_set = data.loc[test_index]","ce969a43":"#Check the proportions are the same\nfig, axes = plt.subplots(2,1,figsize=(6,6), sharex=True)\nplt_count(df=train_set,cat='Attrition_Flag', title='Churn Train Set', ax=axes[0])\nplt_count(df=test_set,cat='Attrition_Flag', title='Churn Test Set', ax=axes[1])","062572ae":"target = 'Attrition_Flag'\ntrain_set[target] = train_set[target].map({'Existing Customer':0, 'Attrited Customer':1})\ntest_set[target] = test_set[target].map({'Existing Customer':0, 'Attrited Customer':1})","9d7da173":"#Create a copy to avoid messing up the train dataset\nchurn = train_set.copy()","5dda0c57":"#Create separate lists of numeric and categorical features\nnum_vars = list(churn.select_dtypes(include=[np.number]).columns)\ncat_vars = list(churn.select_dtypes(include=['object']).columns)\nnum_vars.remove('Attrition_Flag')","6f3a1de1":"#Plots the relationship between all the possible values in a categorical column and the target variable.\ndef plt_hue_cat(df, cat, target='Attrition_Flag', title=None, ax=None):\n    #Create crosstab\n    table = pd.crosstab(index=df[cat], columns=churn[target], normalize='index')\n    \n    if ax==None:\n        fig, ax = plt.subplots()\n    table.plot.bar(ax=ax)\n    \n    max_height = max([p.get_height() for p in ax.patches])\n    min_height = min([p.get_height() for p in ax.patches])\n    \n    for i,p in enumerate(ax.patches):\n        width = p.get_width()\n        height = p.get_height()\n        x, y = p.get_xy() \n        ax.annotate(f'{table.unstack().iloc[i]:.1%}', (x + width\/2, y+height+min_height*0.1), ha='center')\n    \n    ylim=max_height+max_height\n    ax.set_ylim(-.1, 1.1)\n    y_vals = ax.get_yticks()\n    ax.set_yticklabels(['{:3.0f}%'.format(x * 100) for x in y_vals])\n\n    if title==None:\n        title=cat\n        \n    ax.set_title(title)\n    ax.legend(loc=(1.1,0.5))\n    plt.tight_layout()","c33f2fb2":"rows = len(cat_vars)\/\/2+1\nfig, axes = plt.subplots(rows, 2, figsize=(12,10))\naxes = axes.ravel()\naxes[-1].remove()\n\nfor i in range(len(cat_vars)):\n    cat = cat_vars[i]\n    plt_count(df=churn, cat=cat, ax=axes[i])","7f471c97":"rows = len(cat_vars)\/\/2+1\nfig, axes = plt.subplots(rows, 2, figsize=(12,12))\naxes = axes.ravel()\naxes[-1].remove()\n\nfor i in range(len(cat_vars)):\n    cat = cat_vars[i]\n    plt_hue_cat(df=churn, cat=cat, ax=axes[i])\n    \nplt.show()","44506e75":"churn.pivot_table(index='Income_Category', columns='Education_Level', \n                  values=target, aggfunc=lambda x: round(sum(x)\/len(x),2)).iloc[[4,1,2,3,0]]","5917a6c5":"def create_dummies(df, feature):\n    dummies = pd.get_dummies(df[feature], prefix=feature)\n    df_mod = pd.concat([df, dummies], axis=1)\n    df_mod = df_mod.drop([feature], axis=1)\n    return(df_mod) \n\nfor cat in cat_vars:\n    churn = create_dummies(churn, cat)\n    \nchurn","7b2fd1ed":"#Plot the distribution of a numeric variable with respect to the target variable\ndef plt_hue_num(df, num_var, target='Attrition_Flag', title=None, ax=None):\n    \n    if ax==None:\n        fig, ax = plt.subplots()\n    \n    sns.histplot(df, x=num_var, hue=target, ax=ax)\n\n    if title==None:\n        title=num_var\n        \n    ax.set_title(title)\n    plt.tight_layout()","48e91ec7":"rows = len(num_vars)\/\/3+1\nfig, axes = plt.subplots(rows, 3, figsize=(12,12))\naxes = axes.ravel()\naxes[-1].remove()\n\nfor i in range(len(num_vars)):\n    num_var = num_vars[i]\n    plt_hue_num(churn, num_var, target='Attrition_Flag', ax=axes[i])    \n    plt.tight_layout()\nplt.show()","a5ea8e0c":"def plot_correlation_heatmap(df):\n    corr = df.corr()\n    \n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(11, 9))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()\n    \nplot_correlation_heatmap(churn)","849a525e":"#Revert to a clean dataset\ny_train = train_set[target].copy()\nX_train = train_set.drop(target, axis=1)","2d02060a":"#Prepare Test Set for later\ny_test = test_set[target].copy()\nX_test = test_set.drop(target, axis=1)","468528d2":"#Import Scikit-Learn Libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer","47ea9cee":"#Get sets of categories for each categorical variable\n\ncategories = []\n\nfor cat in cat_vars:\n    cat_add = train_set[cat].unique()\n    cat_add = cat_add[cat_add!='Unknown']\n    cat_add = cat_add[:-1]\n    categories.append(cat_add)","dd0703e0":"#Create different pipelines for categorical and numeric features\n\ncat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('encoder', OneHotEncoder(categories=categories, handle_unknown='ignore')),\n    ])\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        #('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])","df5294d6":"#Join the transformations for categorical and numeric features\n\nprep_pipeline = ColumnTransformer([\n    ('cat', cat_pipeline, cat_vars),\n    ('num', num_pipeline, num_vars),\n])\n\nX_train_prep = prep_pipeline.fit_transform(X_train)","57b01447":"#Check that the transformed dataset shape is correct\nX_train_prep.shape","810d868a":"#Import Scikit-Learn Libraries\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_curve","3ef2133f":"#Plots a confusion matrix for a binary classification problem\ndef plot_confusion_matrix(y_actual, y_pred, title=None):\n    fig,ax = plt.subplots(figsize=(9,6))\n    sns.heatmap(confusion_matrix(y_actual,y_pred),\n            annot=True, fmt='d',annot_kws={'size':16})\n    ax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\n    ax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\n    ax.set_xlabel('Prediction',fontsize=16)\n    ax.set_ylabel('Actual',fontsize=16)\n    ax.set_title(title, fontsize=17, fontweight='bold')","4aea5fa0":"from sklearn.linear_model import LogisticRegression","53462e26":"#Train model on whole train set\nlr = LogisticRegression()\nlr.fit(X_train_prep, y_train)","7b63dcf0":"#Get accuracy on train set after training\ny_pred = lr.predict(X_train_prep)\naccuracy_score(y_pred, y_train)","0c40248a":"#Get cross-validated score on the train set\nnp.mean(cross_val_score(lr, X_train_prep, y_train, cv=10, scoring=\"accuracy\"))","1d8f298a":"#Generate confusion matrix\ny_pred = cross_val_predict(lr, X_train_prep, y_train, cv=10)\nconf_matrix = confusion_matrix(y_train, y_pred)\nplot_confusion_matrix(y_train, y_pred, 'Confusion Matrix')","618da3a8":"precision_score(y_train, y_pred)","01212233":"recall_score(y_train, y_pred)","1366f29e":"f1_score(y_train, y_pred)","95872b6a":"#Prepare data for plotting Precision-Recall Curve\ny_scores = cross_val_predict(lr, X_train_prep, y_train, cv=10,\n                             method=\"decision_function\")\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)","4919bef5":"def plot_precision_vs_recall(precisions, recalls, color='b', label=None, title=None, ax=None):\n    if ax==None:\n        fig, ax = plt.subplots(figsize=(8, 6))\n    #Calculate points of 80% Recall and 80% Precision\n    recall_80_precision = recalls[np.argmax(precisions >= 0.80)]\n    precision_80_recall = precisions[np.argmin(recalls >= 0.80)]\n    \n    ax.plot([recall_80_precision, recall_80_precision], [0., 0.8], \"r:\")\n    ax.plot([0.0, recall_80_precision], [0.8, 0.8], \"r:\")\n    ax.plot([recall_80_precision], [0.8], \"ro\")\n    ax.plot([0., 0.8], [precision_80_recall, precision_80_recall], \"g:\")\n    ax.plot([0.8, 0.8], [0.0, precision_80_recall], \"g:\")\n    ax.plot([0.8],[precision_80_recall], \"go\")\n    ax.plot(recalls, precisions, color, linewidth=1.5, label=label)\n    ax.set_xlabel(\"Recall\", fontsize=14)\n    ax.set_ylabel(\"Precision\", fontsize=14)\n    ax.set_title(title, fontsize=16)\n    ax.axis([0, 1.05, 0, 1.05])\n    plt.grid(True)","c841a0dc":"plot_precision_vs_recall(precisions, recalls, title='1st Try at Logistic Regression')\nplt.show()","c90e7298":"precision_80_recall = precisions[np.argmin(recalls >= 0.80)]\nprecision_80_recall","f9819e52":"recall_80_precision = recalls[np.argmax(precisions >= 0.80)]\nrecall_80_precision","2e8da1b0":"#Look at feature importances\nlr_coefs = lr.coef_[0]\nfeature_importances = abs(lr_coefs)\ncat_encoder = prep_pipeline.named_transformers_[\"cat\"]['encoder']\ncat_one_hot_attribs = np.concatenate(cat_encoder.categories_).ravel().tolist()\nattributes = cat_one_hot_attribs + num_vars #+ extra_attributes\nsorted(zip(feature_importances, attributes), reverse=True)","d4b36a27":"#Plot feature coefficients\nax = pd.Series(index=attributes, data=lr_coefs).sort_values().plot.barh()\nax.set_title('Feature Coefficients Logistic Regression')\nplt.show()","def3e483":"from sklearn.ensemble import RandomForestClassifier","9351c51b":"rfc = RandomForestClassifier(random_state=10)","f1463702":"#Train model on whole train set\nrfc.fit(X_train_prep, y_train)","6d7e26d4":"#Get accuracy on train set after training\ny_pred = rfc.predict(X_train_prep)\naccuracy_score(y_train, y_pred)","55461e0f":"#Look at feature importances\nfeature_importances = rfc.feature_importances_\nax = pd.Series(index=attributes, data=feature_importances).sort_values().plot.barh()\nax.set_title('Feature Importances RFC')\nplt.show()","41560db0":"#Get cross-validated score on the train set\nnp.mean(cross_val_score(rfc, X_train_prep, y_train, cv=10, scoring=\"accuracy\"))","78eeafa2":"#Generate confusion matrix\ny_pred = cross_val_predict(rfc, X_train_prep, y_train, cv=10)\nconf_matrix = confusion_matrix(y_train, y_pred)\nplot_confusion_matrix(y_train, y_pred, 'Confusion Matrix')","c14913c4":"accuracy_score(y_train, y_pred)","1703d36b":"precision_score(y_train, y_pred)","31a7ee04":"recall_score(y_train, y_pred)","de3fd086":"f1_score(y_train, y_pred)","64c65ac6":"#Prepare data for plotting Precision-Recall Curve\ny_scores = cross_val_predict(rfc, X_train_prep, y_train, cv=10,\n                             method=\"predict_proba\")\ny_scores = y_scores[:,1]\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)","696e0b8d":"plot_precision_vs_recall(precisions, recalls, title='1st Try at Random Forest Classifier')\nplt.show()","106e1467":"precision_80_recall = precisions[np.argmin(recalls >= 0.80)]\nprecision_80_recall","8b36274f":"recall_80_precision = recalls[np.argmax(precisions >= 0.80)]\nrecall_80_precision","6afe964d":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#Get indexes of top features\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\n#Select the best k-features\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]    \n\n#Get pipeline with feature selection and model predictor\ndef get_fs_pred_pipeline(model):\n    feature_select_and_predict_pipeline = Pipeline([\n        ('feature_selection', TopFeatureSelector(feature_importances,k=1)),\n        ('model', model)\n    ])\n    return feature_select_and_predict_pipeline\n\n#Get pipeline with data preparation, feature selection and model predictor\ndef create_full_pipeline(model):\n    pipeline = Pipeline([\n        ('prep', prep_pipeline),\n        ('fs_pred', model)\n    ])\n    return pipeline","c1c653af":"#Get pipeline with feature selection and RFC predictor\n\nfs_pred_rfc = get_fs_pred_pipeline(rfc)","fa375831":"param_grid = [{\n   'feature_selection__k': list(range(7,len(feature_importances)-14)),\n   'model__max_depth': list(range(2, 50)),\n    'model__max_features': ['auto', 'none']\n}]\n\ngrid_search = GridSearchCV(fs_pred_rfc, param_grid, cv=5,\n                                scoring='accuracy', verbose=0)\n\ngrid_search.fit(X_train_prep, y_train)","718829b5":"grid_search.best_params_","ae94e6e4":"grid_search.best_score_","4fdbfba8":"best_fs_rfc = grid_search.best_estimator_\nbest_rfc_full = create_full_pipeline(best_fs_rfc)\n\ny_test_scores_rfc = best_rfc_full.predict_proba(X_test)[:,1]\nprecisions_rfc, recalls_rfc, thresholds_rfc = precision_recall_curve(y_test, y_test_scores_rfc)\n\nplot_precision_vs_recall(precisions_rfc, recalls_rfc, title='RFC Regression Optimized')\nplt.show()","a9d17b9c":"precision_80_recall_rfc = precisions_rfc[np.argmin(recalls_rfc >= 0.80)]\nprecision_80_recall_rfc","e623fcab":"recall_80_precision_rfc = recalls_rfc[np.argmax(precisions_rfc >= 0.80)]\nrecall_80_precision_rfc","19dd1016":"from sklearn.neural_network import MLPClassifier","57ad6146":"mlp = MLPClassifier(hidden_layer_sizes=(10,10), activation='relu', max_iter=1000)","f1ff6f47":"#Train model on whole train set\nmlp.fit(X_train_prep, y_train)","26d29e15":"#Get accuracy on the training set\ny_pred = mlp.predict(X_train_prep)\naccuracy_score(y_train, y_pred)","c01f342d":"#Get accuracy on train set after training\nnp.mean(cross_val_score(mlp, X_train_prep, y_train, cv=10, scoring=\"accuracy\"))","8f5852ff":"#Generate confusion matrix\ny_pred = cross_val_predict(mlp, X_train_prep, y_train, cv=10)\nconf_matrix = confusion_matrix(y_train, y_pred)\nplot_confusion_matrix(y_train, y_pred, 'Confusion Matrix')","ca0c5594":"accuracy_score(y_train, y_pred)","fe20226d":"precision_score(y_train, y_pred)","d3bdf78b":"recall_score(y_train, y_pred)","5d48a350":"f1_score(y_train, y_pred)","00042dcd":"#Prepare data for plotting Precision-Recall Curve\ny_scores = cross_val_predict(mlp, X_train_prep, y_train, cv=10,\n                             method=\"predict_proba\")\ny_scores = y_scores[:,1]\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)","69c3e4fc":"plot_precision_vs_recall(precisions, recalls, title='1st Try at Neural Network Classifier')\nplt.show()","5173f51a":"precision_80_recall = precisions[np.argmin(recalls >= 0.80)]\nprecision_80_recall","425a73ae":"recall_80_precision = recalls[np.argmax(precisions >= 0.80)]\nrecall_80_precision","536222a2":"param_grid = [{\n        'activation': ['identity', 'logistic', 'tanh', 'relu'],\n        'hidden_layer_sizes':[(6,6),(8,8),(10,10)]\n}]","9f58292a":"grid_search = GridSearchCV(mlp, param_grid, cv=5,\n                                scoring='accuracy', verbose=0)\n\ngrid_search.fit(X_train_prep, y_train)","3229088b":"grid_search.best_params_","96d83054":"grid_search.best_score_","26d31308":"best_mlp = grid_search.best_estimator_\nbest_mlp_full= create_full_pipeline(grid_search.best_estimator_)","68748bc7":"y_test_scores_mlp = best_mlp_full.predict_proba(X_test)[:,1]\nprecisions_mlp, recalls_mlp, thresholds_mlp = precision_recall_curve(y_test, y_test_scores_mlp)\n\nplot_precision_vs_recall(precisions_mlp, recalls_mlp, title='MLP Classifier Optimized')\nplt.show()","e3eef930":"precision_80_recall_mlp = precisions_mlp[np.argmin(recalls_mlp >= 0.80)]\nprecision_80_recall_mlp","ecf32a43":"recall_80_precision_mlp = recalls_mlp[np.argmax(precisions_mlp >= 0.80)]\nrecall_80_precision_mlp","ec2cab1b":"from sklearn.ensemble import VotingClassifier","961e3142":"#Combine the 2 predictors\nvoting_clf = VotingClassifier(\n    estimators=[('rf', best_fs_rfc), ('mlp', best_mlp)],\n    voting='soft')\n\n#Create full pipeline for preparing data, feature selection and output prediction.\nfull_ensembled_clf = create_full_pipeline(voting_clf)\nfull_ensembled_clf.fit(X_train, y_train)","d04dc0f8":"#Plot precision-recall curve for the individual algorithms and the ensemble.\ny_test_scores_ens = full_ensembled_clf.predict_proba(X_test)[:,1]\nprecisions_ens, recalls_ens, thresholds_ens = precision_recall_curve(y_test, y_test_scores_ens)\n\nfig, ax = plt.subplots(figsize=(10,10))\n\nplot_precision_vs_recall(precisions_ens, recalls_ens, color='blue', label='Ensemble', ax=ax)\nplot_precision_vs_recall(precisions_rfc, recalls_rfc, color='orange', label='RFC',ax=ax)\nplot_precision_vs_recall(precisions_mlp, recalls_mlp, color='purple', label='MLP', \n                         title='Precision-Recall Trade-off Comparison\\n(Ensemble, RFC, MLP)', ax=ax)\nax.legend()\nplt.show()","c5a03591":"y_pred_ens = full_ensembled_clf.predict(X_test)\naccuracy_score(y_pred_ens, y_test)","16354640":"precision_80_recall_ens = precisions_ens[np.argmin(recalls_ens >= 0.80)]\nprecision_80_recall_ens","348a689b":"recall_80_precision_ens = recalls_ens[np.argmax(precisions_ens >= 0.80)]\nrecall_80_precision_ens","f207ab9a":"For the ensemble model of RFC and MLP, on the test set we get 94.8% precision for 80% recall, and 95.7% for 80% precision. These are the scores we present for our estimate accuracies of this model. \n\nWe successfully  created an ensemble that has higher accuracies than the individual models it's composed of. This is because both models make predictions differently, so the combination tends to be more accurate. It would be a good idea for expanding this project to add some new models to the ensemble such as Support Vector Machines and Gradient Boosting Classifiers. The more different these models are, the better the ensemble will be.","af177666":"We can see that the accuracy on the training data is 100%. The model is clearly overfitting. We will sort it by eliminating useless features and constraining the model.","27edbec2":"Before we start trying models on the data we need to explore the relationship between the target variable and the predictor features.","0536a50f":"Now we will optimize the model, tuning the hyperparameters with GridSearch.","14a9e5f4":"Let's start with Logistic Regression, which is a simple linear model. ","49e85956":"For the optimal hyperparametres, on the test set we get 90.2% precision for 80% recall, and 91.4% for 80% precision. These are the scores we present for our estimate accuracies of this model.","b42291cd":"# 6. Create Ensembled Model","6d00278f":"## Optimization","ca32c55c":"We could create a more granular set of categories, but we've got a small dataset so we risk overfitting the data. The more complex algorithms we'll use will pick up on these patterns anyway.","6e9474d8":"We get a cross-validated calculated accuracy of 96.4%, which is higher than the first try, that had 96.0%. Because we have tried many different combinations of parameters on the same data, we risk having got a good result just by random chance, so we need to check the generalization error on the test set, which the algorithm has never seen:","0e27c5c3":"Let's look at the features importances:","4a3c6fb2":"Let's take a look at how many missing values there are in each column:","0b5595d1":"# 5.3 Neural Network Classifier","110a066c":"The logistic regression model does not seem to be overfitting. 90% accuracy would be high if we had a balanced dataset. However, as 84% of the observations are negative, a classifier that only outputs negatives would have 84% accuracy, so this is not the best metric. Below we'll plot the confusion matrix and look at the precision and recall.","e56c2406":"If we want 80% recall we'll get 57.5% precision, and if we want 80% precision, we'll get 52.9% recall.","671d4d73":"The parameters that produce the best cross-validated accuracy are:\n- Using the hyperbolic tangent function as an activation function.\n- 2 layers of 8 neurons each.","212c375a":"# 1. Import Libraries","336e5f9f":"Below we can see the churn rate for each combination of Education Level to Income:","1a210430":"Now we need to create a test set and don't look at it. All the exploratory data analysis, experimenting with models and fine-tuning has to be done on the train set only. We should only use the test to get an estimate of the generalization error once we are ready to deploy a model, to prevent data snooping. We'll split the data so there's the same proportion of positives and negatives in the train set and the test sets.","cbd044d0":"The Logistic Regression model outputs a probability for each instance that it belongs to a determined class. By default, if p>=0.5, the instance is labelled as positive, and if p<0.5, it's labelled as negative. We can tweak this threshold to increase recall at the expense of reducing precision and vice versa. Note that the overall accuracy might vary. \n\nLet's plot recall against precision to asses this trade-off: ","3d567f30":"We'll try a Neural Network Classifier, which are great for capturing complex patterns on non-linear data.","4aa05e41":"# 4. Prepare the Data for Machine Learning Algorithms","7634d575":"Now we'll plot the distribution of the numeric features with respect to the target variable.","ee5288ef":"We only need to use 13 features out of 29, and have a depth of 17 to get the best precision. Using a random subset of features on each branch split also reduces overfitting.","70df706b":"# 5. Try different Models","0910f67b":"Below we plot the number of instances and the percentage they represent of the total for each categorical feature:","de79c64c":"The dataset we used is unbalanced so we focused on the precision-recall trade-off more than in the overall precision.\n\nThe results for each model on the test set are:\n- Logistic regression has a 90% accuracy (which is not high for an unbalanced dataset). It only had 57.5% precision for 80% recall, 52.9% recall for 80% precision, so we discarded it.\n- Random Forest Classifier (RFC) has a 96.4% accuracy. It has 93.5% precision for 80% recall, and 92.6% for 80% precision.\n- Neural Network Classifier (MLP) has a 95.3% accuracy. It has 90.2% precision for 80% recall, and 91.4% for 80% precision.\n- The Ensembled model of Random Forest Classifier has a 96.7% accuracy. It has 94.8% precision for 80% recall, and 95.7% for 80% precision.\n\nWe found out that by combining the RFC and MLP the accuracy increased, and we reduced the trade-off between precision and recall.","cb01edec":"# 2. Read in the Data","fce1242d":"## Summary of Results","0a938396":"We can see that the churn rate differs across different demographics but not greatly. For instance, women are more likely to churn than men. Platinum customers are twice as likely than Silver customers. However, due to the low number of instances for these values, they are likely not to be significant and instead introduce noise on the model. In the rest of variables, the difference in churn rate is just a few percentage points.","d9f3db1c":"It's a good idea to write functions and pipelines for this purpose:\n- It will automate the process if we get a new dataset. \n- It also allows us to build a library of functions that we can reuse in future projects.\n- We can use this functions to transform the data on a live system.\n- It makes it easier to try different transformations and see which combination yields the best results.","18dd5f49":"We'll make the target variable numerical:","8919c78a":"We can see that the numeric features related to the transactions behaviour have the highest coefficients, as it was expected. We can also see the issues that multicollinearity causes. Both the Total Transaction Amount and the Count are negatively correlated to the target variable. However, the coefficient for Total Transaction Amount is positive. This happens as a result of the high linear correlation with the Total Transaction Count, which distorts the model fitting. \n\nTo improve the performance of this model we could get rid of multicorrelated features, get rid of irrelevant features (the ones with the lowest absolute value for the logistic regression coefficients), and try both constraining and unconstraining the model. We could also break down some of the numeric features into categories, which would help capture the nonlinear relationships. However, there are more powerful models available that do this automatically, so let's explore them. ","587b7e81":"Of all the instances labelled as positive, only 76% actually are, and it only identifies 59% of the customers who churn.","fbc3149c":"The variables that relate to the customers' transactions behaviour seem like way better predictors than the demographic features. The distribution for variables like the Average Utilization Rate and the Total Transaction amount are significantly skewed to the left for not churned customers than for not churned customers.","66927d84":"# 5.2 Random Forest Classifier","53dc332f":"Below we perform One-Hot-Encoding to plot the linear correlation between categorical and numeric features later:","f48087c1":"We'll now try a more complex model: Random Forest Classifier. It is an ensemble of Decision Tree Classifiers, which don't assume that the data is linear, as opposed to Logistic Regression, but are prone to overfit. That is why we use an ensemble of them, to take advantage of how well they fit the training data, but reducing the overfitting to the noise.","57496529":"We get a cross-validated calculated accuracy of 96.4%, which is higher than the first try, that had 96.0%. Because we have tried many different combinations of parameters on the same data, we risk having got a good result just by random chance, so we need to check the generalization error on the test set, which the algorithm has never seen:","1e6446bc":"The data used in this project is publicly available on Kaggle and it's anonymized, so there are no privacy issues.","8e3d1afb":"# 3.3 Correlation Matrix","4147a8df":"We'll get rid of the missing values at the same time we perform OneHotEncoding. To avoid issues with collinearity, when we encode the categorical variables, we'll get rid of one of the possible values. We'll also get rid of the missing values by not including an Unknown category. ","8725c330":"We get a lot less false positives and false negatives.","15abe265":"Now we'll find the best combination of number of features, maximum depth of the classifier trees and maximum number of features used on each tree branch split using GridSearch:","3fc34cae":"## Optimization","fee74e05":"The objective of this project is to develop a Machine Learning Algorithm to predict which customers of a bank will leave their credit card services. We will use a dataset that has demographic information about the clients, records of the services they have hired with the bank and information about past transactions. The algorithm will take this data as input and predict if a client will churn or not. The data is publicly available on [Kaggle](https:\/\/www.kaggle.com\/sakshigoyal7\/credit-card-customers).\n\nWe will explore 3 different algorithms: Logistic Regression, Random Forest Classifiers and Neural Network Classifiers.","da07e830":"Let's start with how the categorical variables are distributed:","6b7282dd":"The cross-validated accuracy score is a lot higher than for Logistic Regression, as well as the recall and precision:","b4b02ace":"# 3. Explore the Data","ca3f6b98":"# 5.1 Logistic Regression","d9b93e8f":"**Descriptions of the variables:**\n\n**- Target Variable:**\n- Attrition_Flag: 'Existing Customer' if the account is not closed, and 'Attrited Customer' if the account was closed.\n\n\n**- Numeric Features:**\n\n   - CLIENTNUM: Client number. Unique identifier for the customer holding the account.\n   - Customer_Age: Demographic variable - Customer's Age in Years.\n   - Customer_Age: Demographic variable - Number of dependents.\n   - Marital_Status: Demographic variable - Married, Single, Divorced, Unknown.\n   - Months_on_book: Period of relationship with bank.\n   - Total_Relationship_Count: Total no. of products held by the customer.\n   - Months_Inactive_12_mon: No. of months inactive in the last 12 months.\n   - Contacts_Count_12_mon: No. of Contacts in the last 12 months.\n   - Credit_Limit: Credit Limit on the Credit Card.\n   - Total_Revolving_Bal: Total Revolving Balance on the Credit Card.\n   - Avg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months).\n   - Total_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1).\n   - Total_Trans_Amt: Total Transaction Amount (Last 12 months).\n   - Total_Trans_Ct: Total Transaction Count (Last 12 months).\n   - Total_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1).\n   - Avg_Utilization_Ratio: Average Card Utilization Ratio.\n   \n**- Categorical Features:**\n\n   - Gender: Demographic variable - M=Male, F=Female.\n   - Education_Level: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.).\n   - Marital_Status: Demographic variable - Married, Single, Divorced, Unknown.\n   - Income_Category: Demographic variable - Annual Income Category of the account holder.\n   - Card_Category: Product Variable - Type of Card (Blue, Silver, Gold, Platinum).","2df76224":"If we want 80% recall we'll get 94% precision, and if we want 80% precision we'll get 95% recall.","1c026b09":"# 3.2 Numerical Features","a2063766":"# Credit Card Customers Churning Prediction using Machine Learning","e015b729":"# 4.1 Categorical Features","19183edc":"Below is a plot of the target variable. This is a binary classification problem. We'll consider a customer churning as a positive result and not churning a negative result. We can see that there is an imbalance in the number of instances for each class. When we split the data into a train and a test sets we need to be careful about this imbalance.","a0eb7b2a":"We can see the RFC model identifies which features are the most important more accurately than Logistic Regression. All the top features are bank behaviour variables.","1dcab86b":"Now we will optimize the model, tuning the hyperparameters with GridSearch. Similar to what we did for Random Forest Classifiers. We'll find the best activation function and number of neurons in each layer. We won't perform feature selection for this algorithm, as it does it automatically.","67d7f3c1":"For the optimal hyperparametres, on the test set we get 93.5% precision for 80% recall, and 92.6% for 80% precision. These are the scores we present for our estimate accuracies of this model.","244b24b9":"The first try doesn't seem to overfit the model as much as the first RFC so it's a good start:","10612a50":"If we want 80% recall we'll get 93.8% precision, and if we want 80% precision we'll get 94.5% recall.","4967089c":"Now we'll create a model that combines the predictions of the RFC and the MLP classifiers. It will work by averaging the output probability of each model. Because they are different algorithms, we expect that the accuracy of the ensemble will be higher than of the individual predictors.","dcd9031b":"Let's plot the churn rate for each possible value of each category:","36577012":"Below we plot the correlation matrix for all variables:","0fafc88c":"We've got missing values in 3 columns, which are all categorical. We'll remove them when we pre-process the data.","7e07c2b8":"# 3.1 Categorical Features","194a073e":"We don't know where this bank is located, so we can't find out if the demographic data is representative of the area it's located. The Gender variable seems balanced. The Card Category variable is likely to introduce noise in the model, due to the low number of observations for 3 out of 4 possible categories.","c1f4c1f8":"We can see that the target variable is linearly correlated only to the transaction information variables. There is some great collinearity between some of the features such as the Total Transaction Amount and the Average Transaction amount, which is logical. We need to be careful with it, as it could distort the model, especially simple linear models. More complex models like Decision Trees, which perform feature selection almost automatically, are way less affected. "}}