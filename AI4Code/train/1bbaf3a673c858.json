{"cell_type":{"39676693":"code","9e862019":"code","dbe30985":"code","0844fdae":"code","c3f5007b":"code","4cc180ff":"code","04bd6fc7":"code","9d48e0ed":"code","894091fc":"code","32bfb988":"code","2bfe0870":"code","848128b7":"code","553f55ab":"code","86f3cb31":"code","42e3f046":"code","ec36ba8f":"code","1d87eb0f":"code","2e60924c":"code","e7b5a2dd":"code","25715294":"code","7b45c559":"code","799d7f3c":"code","12bd2482":"code","cc67c8a0":"code","84bf3719":"code","e4dfb3dc":"markdown","0b6e5e3a":"markdown","b0d9a6c1":"markdown","d2a2adc7":"markdown","c3e858b2":"markdown"},"source":{"39676693":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette('viridis')\n\n# Any results you write to the current directory are saved as output.","9e862019":"# you can load one by one depends on your demand, or load them one time. \nprior = pd.read_csv(\"..\/input\/order_products__prior.csv\")\ntrain = pd.read_csv(\"..\/input\/order_products__train.csv\")\norders = pd.read_csv(\"..\/input\/orders.csv\")\nproducts =  pd.read_csv(\"..\/input\/products.csv\")\naisles = pd.read_csv(\"..\/input\/aisles.csv\")\ndepartments = pd.read_csv(\"..\/input\/departments.csv\")","dbe30985":"print('the shape of prior:', prior.shape)\nprint('cols of prior:', prior.columns)\nprint('cols of train:', train.columns)\nprint('the shape of orders:', orders.shape)\nprint('cols of orders:' ,orders.columns)\nprint('cols of products:', products.columns)\n","0844fdae":"orders.eval_set.value_counts().plot(kind ='bar',color = color,figsize = (8,6),fontsize =12 )","c3f5007b":"# see how many unique user in each group\norders.groupby('eval_set')['user_id'].apply(lambda x: len(x.unique()))","4cc180ff":"print('cols of orders:' ,orders.columns)","04bd6fc7":"fig,axes = plt.subplots(4,1,figsize = (16,24))\norders.order_dow.value_counts().plot(kind ='bar',color = 'c',ax = axes[0],title = 'Distribution of Day of Week orders')\norders.order_hour_of_day.value_counts().plot(kind ='bar',color = 'c',ax = axes[1],title = 'Distribution of Hour of Day orders')\n\ntmp = orders.groupby(['order_dow', 'order_hour_of_day'])[\"order_number\"].aggregate('count').reset_index()\ntmp = tmp.pivot('order_dow', 'order_hour_of_day', 'order_number')\nax = axes[2]\nsns.heatmap(tmp,ax = axes[2])\n\norders.days_since_prior_order.value_counts().plot(kind ='bar',color = 'c',ax = axes[3],title = 'Distribution of days_since_prior_order')\n","9d48e0ed":"print('The re-ordered percentage in train dataset is: ', round(train.reordered.sum()\/len(train) *100,2))\nprint('The re-ordered percentage in prior dataset is: ', round(prior.reordered.sum()\/len(prior) *100,2))","894091fc":"tmp = train.groupby('order_id')['reordered'].aggregate('sum').reset_index()\ntmp['reordered'].loc[tmp['reordered'] >= 1] =1\nprint('the percentage of non-reorders in train is ',tmp['reordered'].value_counts()\/ len(tmp)) \n      \ntmp = prior.groupby('order_id')['reordered'].aggregate('sum').reset_index()\ntmp['reordered'].loc[tmp['reordered'] >= 1] =1\nprint('the percentage of non-reorders in prior is ',tmp['reordered'].value_counts()\/ len(tmp)) \n","32bfb988":"tmp = train.groupby('order_id')['add_to_cart_order'].aggregate('max').reset_index()\ntmp['add_to_cart_order'].value_counts()[:50].plot(kind = 'bar',legend = 'train',color = 'b',figsize =(16,6))\n","2bfe0870":"tmp1 = prior.groupby('order_id')['add_to_cart_order'].aggregate('max').reset_index()\ntmp1['add_to_cart_order'].value_counts()[:50].plot(kind = 'bar',legend = 'prior',color = 'orange',figsize =(16,6))","848128b7":"cols =[products,departments, aisles]\nfor c in cols:\n    print(c.columns)\n","553f55ab":"# Build a complete products dataset\nproducts = pd.merge(products,departments,on ='department_id', how = 'left')\nproducts = pd.merge(products,aisles,on ='aisle_id', how = 'left')\nproducts.head(2)","86f3cb31":"# Merge products dataset to prior dataset\n#prior.columns\nprior = pd.merge(prior,products,on = 'product_id',how ='left')\nprior.head(2)","42e3f046":"prior.product_name.value_counts()[:10]","ec36ba8f":"prior.department.value_counts()[:10]","1d87eb0f":"prior.aisle.value_counts()[:10]","2e60924c":"tmp =prior.groupby('department')['reordered'].aggregate('mean').reset_index()\nprint('The top 10 goods easy to be reordered:')\ndisplay(tmp.sort_values(by ='reordered',ascending = False).head(10))\nprint('The top 10 goods hard to be reordered:')\ndisplay(tmp.sort_values(by ='reordered',ascending = False).tail(10))","e7b5a2dd":"print(len(departments),len(aisles))\n# let's just pick aisles as viarable","25715294":"# merge all together, because then we will do some observation between user and their order\nall = pd.merge(orders,prior, on=['order_id','order_id'])\nuser_ai = pd.crosstab(all['user_id'], all['aisle'])\nuser_ai.head()","7b45c559":"from sklearn.decomposition import PCA\npca = PCA(n_components= 10)\npca.fit(user_ai)\npca_samples = pca.transform(user_ai)\nuser_ai_s = pd.DataFrame(pca_samples)\nuser_ai_s.head()","799d7f3c":"from matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d import proj3d\ntocluster = pd.DataFrame(user_ai_s[[5,1]])\nprint (tocluster.shape)\n#print (tocluster.head())\n\nfig = plt.figure(figsize=(8,8))\nplt.plot(tocluster[5], tocluster[1], 'o', markersize=4, color='c', label='class1')\n\nplt.xlabel('x_values')\nplt.ylabel('y_values')\nplt.legend()\nplt.show()","12bd2482":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nclusterer = KMeans(n_clusters=5,random_state=42).fit(tocluster)\ncenters = clusterer.cluster_centers_\nc_preds = clusterer.predict(tocluster)\nprint(centers)","cc67c8a0":"c_preds ","84bf3719":"fig = plt.figure(figsize=(8,8))\ncolors = ['orange','g','r','cyan','yellow']\ncolored = [colors[k] for k in c_preds]\nprint (colored[0:5])\nplt.scatter(tocluster[5],tocluster[1],  color = colored)\nfor ci,c in enumerate(centers):\n    plt.plot(c[0], c[1], 'o', markersize=5, label=''+str(ci))\n\nplt.xlabel('x_values')\nplt.ylabel('y_values')\nplt.legend()\n","e4dfb3dc":"Ok. Everything so far looks fine. What could we find from the new prior dataset ?\n\nLet us see:\n1.  The top 10 best-selling products\n2. The top 10 best-selling department\n3. Explore of reorder ratio\n","0b6e5e3a":"No matter in train or prior dataset, it is easily to see the long tail distribution. \n\nGenerally, people will buy 5 - 10 goods in one purchase,  more than 30 goods is pretty rare.","b0d9a6c1":"## Observe data and see how much we can know","d2a2adc7":" Not bad,  nearly 60% goods will be reordered.\n \n However, there are still 6% goods not be reordered in train dataset while in prior dataset is 12%.","c3e858b2":"1.   Generally, Sunday and Monday are the grocery day\n2.  9-17 hour of day are grocery peaking hour, same as our working hour \n    (This is out of my expectation, I thought people do grocery always after work or before work, but it might be my bias)\n3.  After the first purchase, there are two obvious re-purchase peaking days: the 7th day and the 30th day.\n     Since the target is to figure out the re-orders, let us check out the re-order percentage in other tables (train and prior).\n    "}}