{"cell_type":{"5b8cad05":"code","378936b5":"code","74836ddc":"code","00ea4a7e":"code","2df025c5":"code","585ad21c":"code","06f6f9f1":"code","cdea61ac":"code","af9a83cb":"code","0c29524d":"code","43a4c874":"code","e47472e7":"code","6a731f89":"code","1f30a18e":"code","c389e8ed":"code","4ca4705f":"code","e08ff40e":"code","ea626c6e":"markdown","fd76ce1d":"markdown","bea6d64e":"markdown","42a66f20":"markdown","06d49611":"markdown","ec24ac1e":"markdown","0a9b31ad":"markdown","0ff5e61e":"markdown","f9a7cb2a":"markdown","5a28cc73":"markdown","18a37e6e":"markdown","dd9cf5e3":"markdown","fc92ec09":"markdown","ae152908":"markdown","fb7ca4ad":"markdown"},"source":{"5b8cad05":"# Arrays, matrizes, c\u00e1lculos e dataframes\nimport numpy as np\nimport pandas as pd\n\n# Importando a m\u00e9trica\nfrom sklearn.metrics import accuracy_score\n\n# Divis\u00e3o dataframe\nfrom sklearn.model_selection import train_test_split\n\n# M\u00e9todos de \u00e1rvores\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# GridSearch e Cross-validation\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n\n# Gr\u00e1ficos\nimport matplotlib.pyplot as plt\n\n# Importo o arquivo\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        df = pd.read_csv(os.path.join(dirname, filename))","378936b5":"# Visualizo\ndf.head()","74836ddc":"# Renomeio as colunas\ndf.columns = ['inadim', 'vl_emp', 'vl_dev_hip', 'vl_prop', 'mot_emp', 'prof', \n              'anos_emp', 'nr_rel_dep', 'lin_cred_inadim', 'meses_lin_ant', 'lin_cred_rec', \n              'lin_cred_atuais', 'tx_div_mes']        \n\nprint('Qtd inicial de linhas: ', df.shape)\n\n# Removo as linhas que possuem menos de 5 colunas preenchidas\ndf = df[~(df.count(1) <= 5)]\nprint('Qtd final de linhas: ', df.shape)\n\n# Seto 'Other' em prof (Profiss\u00e3o), quando estiver nula\ndf['prof'] = np.where(df['prof'].isna(), 'Other', df['prof'])\n\n# Seto 0 em vl_dev_hip (valor devido de hipoteca), quando estiver nulo\ndf['vl_dev_hip'] = np.where(df['vl_dev_hip'].isna(), 0, df['vl_dev_hip'])\n\n# Para os casos onde existe inadimpl\u00eancia e o valor da casa est\u00e1 nulo, seto 0, isto porque, possivelmente, a pessoa j\u00e1 tenha perdido o im\u00f3vel\ndf['vl_prop'] = np.where(df['vl_prop'].isna(), 0, df['vl_prop'])\n\n# Se o motivo do empr\u00e9stimo estiver nulo e o valor da hipoteca for zero, seto HomeImp, caso contr\u00e1rio, DebtCon\ndf['mot_emp'] = np.where((df['mot_emp'].isna()) & (df['vl_dev_hip'] == 0), 'HomeImp', df['mot_emp'])\ndf['mot_emp'] = np.where((df['mot_emp'].isna()) & (df['vl_dev_hip'] > 0), 'DebtCon', df['mot_emp'])\n\n# Seto 0 em anos_emp quando for nulo\ndf['anos_emp'] = np.where(df['anos_emp'].isna(), 0, df['anos_emp'])\n\n# Preencho a coluna tx_div_mes com a m\u00e9dia da mesma coluna, isso porque ningu\u00e9m tem 0% da renda comprometida, ent\u00e3o a m\u00e9dia \u00e9 mais interessante\ndf['tx_div_mes'].fillna(df['tx_div_mes'].mean(), inplace=True)\n\n# Demais colunas, preencho com 0\ndf.fillna(0, inplace=True)\n\n# Transformo texto em n\u00famero\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        df[col + '_n'] = df[col].astype('category').cat.codes","00ea4a7e":"# Visualizo como ficaram os dados\ndf.head()","2df025c5":"# Visualizo o tipo dos dados\ndf.info()","585ad21c":"# Separo as colunas que vou utilizar\nfeats = [c for c in df.columns if c not in ['inadim', 'mot_emp', 'prof']]\nfeats_wt = [c for c in df.columns if c not in ['mot_emp', 'prof']]\n\n# Separo as bases de treino e teste\ntrain, test = train_test_split(df[feats_wt], test_size=0.15, random_state=42)\n\n# Verifico o tamanho dos dataframes de treino e teste\ntrain.shape, test.shape","06f6f9f1":"# Testo com RandomForest\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], rf.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis neste modelo\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","cdea61ac":"# Testo com GBM\ngbm = GradientBoostingClassifier(n_estimators=200, random_state=42)\ngbm.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], gbm.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis neste modelo\npd.Series(gbm.feature_importances_, index=feats).sort_values().plot.barh()","af9a83cb":"# Testo com XGBoost\nxgb = XGBClassifier(n_estimators=200, random_state=42)\nxgb.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], xgb.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis neste modelo\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()","0c29524d":"# seto os valores que quero testar\nparams = {\n    'n_estimators': [200, 400],\n    'max_depth': [None, 0, 2, 4],\n    'min_samples_split': [1, 2, 4],\n    'min_samples_leaf': [1, 2, 4]\n    #'max_features': [None, 'auto', 'log2'] removi, sen\u00e3o nem roda\n}\n\n# executo antes para conseguir uma melhor combina\u00e7\u00e3o de valores\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\n\n# valida\u00e7\u00e3o cruzada que retorna dobras estratificadas\n# as dobras s\u00e3o feitas preservando a porcentagem de amostras para cada classe\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n\n# aplica\u00e7\u00e3o da grid\ngsc = GridSearchCV(estimator=rf, param_grid=params, cv=skf.split(train[feats], train['inadim'].values), n_jobs=4)\ngsc.fit(train[feats], train['inadim'])\nprint (gsc.best_params_)","43a4c874":"# Utilizo os par\u00e2metros sugeridos e testo\nrf = RandomForestClassifier(n_estimators=200, min_samples_leaf=1, max_depth=None, min_samples_split=4, random_state=42)\nrf.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], rf.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","e47472e7":"# Grid para usar com XGBoost\nparams = {\n        'min_child_weight': [1, 3, 5, 8, 10],\n        'gamma': [0, 0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [2, 4, 6, 8, 10],\n        'learning_rate': [0.2, 0.4, 0.5, 0.6, 1.0],\n        'max_delta_step': [0, 3, 5, 10],\n        'base_score': [0, 0.3, 0.5, 0.7, 1.0],\n        'n_estimators': [200, 400, 600]\n        }\n\nxgb = XGBClassifier(random_state=42)\n\n# cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n\nrandom_search = RandomizedSearchCV(xgb, \n                                   param_distributions=params,\n                                   n_iter=6,\n                                   scoring='roc_auc', \n                                   n_jobs=4, \n                                   cv=skf.split(train[feats], train['inadim'].values), \n                                   verbose=3, \n                                   random_state=42)\n\nrandom_search.fit(train[feats], train['inadim'].values)\n\nprint('\\n Melhor resultado:')\nprint(random_search.best_estimator_)","6a731f89":"# Utilizo o resultado do RandomSearch\nxgb = XGBClassifier(base_score=0.7, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, gamma=1.5, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.5, max_delta_step=10, max_depth=10,\n              min_child_weight=5, monotone_constraints=None,\n              n_estimators=400, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=42, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1.0, tree_method=None,\n              validate_parameters=False, verbosity=None)\n\nxgb.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], xgb.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis com os novos par\u00e2metros\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()","1f30a18e":"# Grid para usar com XGBoost\nparams = {\n        'learning_rate': [0.2, 0.4, 0.5, 0.6, 1.0],\n        'n_estimators': [200, 400, 600]\n        }\n\nxgb = XGBClassifier(random_state=42)\n\n# cross-validation\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 42)\n\nrandom_search = RandomizedSearchCV(xgb, \n                                   param_distributions=params,\n                                   n_iter=6,\n                                   scoring='roc_auc', \n                                   n_jobs=4, \n                                   cv=skf.split(train[feats], train['inadim'].values), \n                                   verbose=3, \n                                   random_state=42)\n\nrandom_search.fit(train[feats], train['inadim'].values)\n\nprint('\\n Melhor resultado:')\nprint(random_search.best_estimator_)","c389e8ed":"# Utilizo o resultado do RandomSearch\nxgb = XGBClassifier(base_score=0.5, booster=None, colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints=None,\n              learning_rate=0.4, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints=None,\n              n_estimators=600, n_jobs=0, num_parallel_tree=1,\n              objective='binary:logistic', random_state=42, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n              validate_parameters=False, verbosity=None)\n\nxgb.fit(train[feats], train['inadim'])\nprint('Porcentagem de acerto: ', accuracy_score(test['inadim'], xgb.predict(test[feats])))\n\n# Checo a import\u00e2ncia das vari\u00e1veis\npd.Series(xgb.feature_importances_, index=feats).sort_values().plot.barh()","4ca4705f":"# Apresento os valores referentes aos registros previstos e n\u00e3o previstos\ntest['t_inadim'] = xgb.predict(test[feats]).astype(int)\ntest['acerto'] = test['t_inadim'] == test['inadim']\ntest['acerto'] = np.where(test['acerto'] == True, 'Previsto', 'N\u00e3o Previsto')\nprint('Previstos: ', str(np.round(test['acerto'].value_counts(normalize=True)[0], 3)) + '%', '-', test['acerto'].value_counts()[0])\nprint('N\u00e3o Previstos: ', str(np.round(test['acerto'].value_counts(normalize=True)[1], 3)) + '%', '-', test['acerto'].value_counts()[1])","e08ff40e":"# Apresento o resultado em gr\u00e1fico\npd.value_counts(test['acerto']).plot.bar(color='#4682B4')\nplt.xticks(rotation=0)\nplt.title('Total de registros previstos e n\u00e3o previstos')\nplt.show()","ea626c6e":"Apresentou o mesmo resultado, portanto tentarei evoluir outro m\u00e9todo.","fd76ce1d":"\n# <center>![](https:\/\/catho-edu-ssr-assets.eduadvice.co\/images\/holders\/iesb-centro-universitario-do-instituto-de-educacao-superior-de-brasilia-iesb@logo2x.jpeg)<\/center>\n\n# <center>DATA MINING E MACHINE LEARNING II<\/center>\n# <center>PROJETO FINAL<\/center>\n\n> Prof.: Marcos\n> \n> Aluno: Ramonn\n","bea6d64e":"### Contexto\n\n> O departamento de cr\u00e9dito ao consumidor de um banco deseja automatizar o processo de tomada de decis\u00e3o para aprova\u00e7\u00e3o das linhas de cr\u00e9dito do patrim\u00f4nio l\u00edquido. Para fazer isso, eles seguir\u00e3o as recomenda\u00e7\u00f5es da Lei da Igualdade de Oportunidades de Cr\u00e9dito para criar um modelo de pontua\u00e7\u00e3o de cr\u00e9dito derivado empiricamente e estatisticamente s\u00f3lido. O modelo ser\u00e1 baseado em dados coletados de solicitantes recentes concedidos cr\u00e9dito atrav\u00e9s do processo atual de subscri\u00e7\u00e3o de empr\u00e9stimos. O modelo ser\u00e1 constru\u00eddo a partir de ferramentas de modelagem preditiva, mas o modelo criado deve ser suficientemente interpret\u00e1vel para fornecer um motivo para qualquer a\u00e7\u00e3o adversa (rejei\u00e7\u00e3o).\n\n\n### Conte\u00fado\n\n> O conjunto de dados de Home Equity (HMEQ) cont\u00e9m informa\u00e7\u00f5es de linha de base e de desempenho de empr\u00e9stimos para 5.960 empr\u00e9stimos recentes de home equity. O alvo (BAD) \u00e9 uma vari\u00e1vel bin\u00e1ria que indica se um requerente acabou por falhar ou se foi gravemente delinq\u00fcente. Esse desfecho adverso ocorreu em 1.189 casos (20%). Para cada candidato, foram registradas 12 vari\u00e1veis de entrada.\n\n\n### Inspira\u00e7\u00e3o\n> E se voc\u00ea puder prever clientes que n\u00e3o pagam seus empr\u00e9stimos?\n\n\n### Descri\u00e7\u00e3o dos dados:\n* BAD\/RUIM: 1 = cliente inadimplente no empr\u00e9stimo 0 = empr\u00e9stimo reembolsado\n* LOAN\/EMPR\u00c9STIMO: Montante do pedido de empr\u00e9stimo\n* MORTDUE: Valor devido da hipoteca existente\n* VALUE: valor da propriedade atual\n* REASON\/MOTIVO: DebtCon = consolida\u00e7\u00e3o da d\u00edvida HomeImp = melhoria da casa\n* JOB\/TRABALHO: Seis categorias profissionais (Manager, Office, Other, Prof.Executive, Sales, Self)\n* YOJ: Anos no emprego atual\n* DEROG: N\u00famero de principais relat\u00f3rios depreciativos\n* DELINQ: n\u00famero de linhas de cr\u00e9dito inadimplentes\n* CLAGE: Idade da linha comercial mais antiga em meses\n* NINQ: N\u00famero de linhas de cr\u00e9dito recentes\n* CLNO: N\u00famero de linhas de cr\u00e9dito\n* DEBTINC\/D\u00cdVIDA: Taxa de receita da d\u00edvida (\u00e9 a porcentagem da renda bruta mensal de um consumidor que \u00e9 destinada ao pagamento de d\u00edvidas)\n\n### Dados renomeados para melhor entendimento:\n* inadim: 1 = cliente inadimplente no empr\u00e9stimo 0 = empr\u00e9stimo reembolsado\n* vl_emp: Montante do pedido de empr\u00e9stimo\n* vl_dev_hip: Valor devido da hipoteca existente\n* vl_prop: valor da propriedade atual\n* mot_emp: DebtCon = consolida\u00e7\u00e3o da d\u00edvida HomeImp = melhoria da casa\n* prof: Seis categorias profissionais (Manager, Office, Other, Prof.Executive, Sales, Self)\n* anos_emp: Anos no emprego atual\n* nr_rel_dep: N\u00famero de principais relat\u00f3rios depreciativos\n* lin_cred_inadim: n\u00famero de linhas de cr\u00e9dito inadimplentes\n* meses_lin_ant: Idade da linha comercial mais antiga em meses\n* lin_cred_rec: N\u00famero de linhas de cr\u00e9dito recentes\n* lin_cred_atuais: N\u00famero de linhas de cr\u00e9dito\n* tx_div_mes: Taxa de receita da d\u00edvida (\u00e9 a porcentagem da renda bruta mensal de um consumidor que \u00e9 destinada ao pagamento de d\u00edvidas)","42a66f20":"## Explorando o RandomForestClassifier","06d49611":"## Visualizo como ficaram os dados","ec24ac1e":"## In\u00edcio dos testes com \u00e1rvores","0a9b31ad":"## Trato os dados","0ff5e61e":"## Separa\u00e7\u00e3o das vari\u00e1veis a serem utilizadas","f9a7cb2a":"## Importo as bibliotecas","5a28cc73":"Sem adicionar mais par\u00e2metros, sen\u00e3o o random_state, RandomForestClassifier retornou um resultado melhor, por isso vou explor\u00e1-lo.","18a37e6e":"## Explorando XGBoost com menos par\u00e2metros","dd9cf5e3":"Utilizando menos par\u00e2metros, tive uma porcentagem de acerto maior, portanto irei utilizar este \u00faltimo modelo.","fc92ec09":"Nesse caso, tive um resultado pior do que antes, vou ent\u00e3o testar com a mudan\u00e7a de menos par\u00e2metros.","ae152908":"## Conclus\u00e3o\nPor fim, resolvo utilizar o XGBoost, com a altera\u00e7\u00e3o de apenas dois par\u00e2metros, obtendo ent\u00e3o mais de 94% de acerto.","fb7ca4ad":"## Explorando tamb\u00e9m o XGBoost\nXGBoost me retornou o segundo melhor resultado, ent\u00e3o vou explorar as op\u00e7\u00f5es dele tamb\u00e9m"}}