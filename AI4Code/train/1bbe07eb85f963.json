{"cell_type":{"a69e3749":"code","04e10f0d":"code","92d7f27a":"code","e89a2802":"code","9b0b4a5f":"code","9d482fbd":"code","0ad04866":"code","753a243d":"code","fccaac15":"code","6712d3a1":"code","ade2b70c":"code","4c137e4c":"code","dbd09581":"code","25b0f173":"code","c6de3dae":"code","df5ae27c":"code","77d31fed":"code","1316dd3b":"code","63dcea36":"code","0af8031a":"code","548f72ef":"code","83bdfa59":"code","72158bef":"code","9230bc8d":"code","788accf6":"code","c2bddd9c":"code","f9991362":"code","551f76b2":"code","96012b43":"code","d88c9f05":"code","fc8c8831":"code","8a000698":"code","b520960e":"code","46821f9d":"code","2391f7a8":"code","1e4c365b":"markdown","d81fb679":"markdown","83f09c27":"markdown","f32a3361":"markdown","8fe7bd37":"markdown","63248e7d":"markdown","c13a2491":"markdown"},"source":{"a69e3749":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8\nimport gc\n\nimport numpy as np\nnp.random.seed(71)\nimport scipy as sp\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom category_encoders import OrdinalEncoder\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom rgf.sklearn import FastRGFClassifier\nfrom rgf.utils import cleanup\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\nfrom keras.layers import Input, Dense ,Dropout, BatchNormalization\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping","04e10f0d":"def CountEncoder(X_train, X_test, col):\n    X_concat = pd.concat([X_train, X_test])\n    \n    summary = X_concat[col].value_counts()\n    \n    del X_concat\n    gc.collect()\n    \n    return X_train[col].map(summary), X_test[col].map(summary)","92d7f27a":"def TargetEncoder(X_train, y_train, X_test, target, col, alpha = 0.5, min_samples_leaf = 10, smooth_coeff = 1.0):\n    X_temp = X_train.copy()\n    X_temp[target] = y_train\n    \n    global_mean = X_temp[target].astype(float).mean()\n    summary = X_temp[[col, target]].groupby([col])[target].agg(['mean', 'count'])\n    \n    smoove = 1 \/ (1 + np.exp(-(summary['count'] - min_samples_leaf) \/ smooth_coeff))\n    smoothing = global_mean * (1 - smoove) + summary['mean'] * smoove\n    smoothing[summary['count'] == 1] = global_mean \n    \n    del X_temp\n    \n    return X_test[col].map(smoothing)","e89a2802":"def negative_downsample(X_train, y_train, text_train, ratio=1, seed=71):\n    X_train0 = X_train[y_train==0]\n    X_train1 = X_train[y_train==1]\n    \n    n_samples = int(ratio*len(X_train1))\n    \n    X_train0_sample = X_train0.sample(n=n_samples, replace=False, random_state=seed)\n    \n    X_train = pd.concat([X_train0_sample, X_train1])\n    \n    y_train = y_train.loc[X_train.index]\n    text_train = text_train.loc[X_train.index]\n    \n    return X_train, y_train, text_train","9b0b4a5f":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ndf_train = pd.read_csv('..\/input\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])\ndf_test = pd.read_csv('..\/input\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])","9d482fbd":"df_train = df_train[df_train.issue_d.dt.year >= 2011]","0ad04866":"# X, y\u306b\u5206\u96e2\u3059\u308b\u3002\ny_train = df_train.loan_condition\nX_train = df_train.drop(['loan_condition'], axis=1)\n\nX_test = df_test.copy()\n\ndel df_train, df_test\ngc.collect()","753a243d":"# \u30af\u30ec\u30b8\u30c3\u30c8\u30e9\u30a4\u30f3\u958b\u59cb\u65e5\u304b\u3089\u7533\u8acb\u65e5\u307e\u3067\u306e\u65e5\u6570\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\nX_train['sinse_earliest_cr_line'] = (X_train.issue_d - X_train.earliest_cr_line) \/ pd.Timedelta(days=1)\nX_test['sinse_earliest_cr_line'] = (X_test.issue_d - X_test.earliest_cr_line) \/ pd.Timedelta(days=1)\n\nX_train.drop(['issue_d', 'earliest_cr_line'], axis=1, inplace=True)\nX_test.drop(['issue_d', 'earliest_cr_line'], axis=1, inplace=True)","fccaac15":"# \u30c6\u30ad\u30b9\u30c8\u7279\u5fb4\u91cf\u3092\u5206\u96e2\u3059\u308b\ntext_train = X_train[['emp_title']].fillna('#').copy()\ntext_test = X_test[['emp_title']].fillna('#').copy()","6712d3a1":"# emp_title\u306etrain\u3068test\u306b\u5171\u901a\u3057\u3066\u51fa\u73fe\u3059\u308b\u3082\u306e\u4ee5\u5916\u306fall_others\u3068\u3057\u3066\u307e\u3068\u3081\u308b\ncommon_titles = np.intersect1d(X_train.emp_title.dropna().unique(), X_test.emp_title.dropna().unique())\nprint(len(common_titles))\n\nX_train.loc[~X_train.emp_title.isin(common_titles), 'emp_title'] = 'all others'\nX_test.loc[~X_test.emp_title.isin(common_titles), 'emp_title'] = 'all others'","ade2b70c":"# loan_amnt\u306f\u3044\u304f\u3064\u304b\u306e\u5178\u578b\u7684\u306a\u8cb8\u4ed8\u984d\u306b\u30d4\u30fc\u30af\u304c\u7acb\u3063\u3066\u3044\u308b\u306e\u3067\u30ab\u30c6\u30b4\u30ea\u306b\u3057\u3066\u62fe\u3063\u3066\u307f\u308b\n\nX_train['annual_inc_cat'] = X_train.annual_inc.astype(str)\nX_test['annual_inc_cat'] = X_test.annual_inc.astype(str)\n\nX_train['loan_amnt_cat'] = X_train.loan_amnt.astype(str)\nX_test['loan_amnt_cat'] = X_test.loan_amnt.astype(str)\n\nX_train['annual_inc_cat_concat_loan_amnt_cat'] = X_train['annual_inc_cat'] + '_' + X_train['loan_amnt_cat']\nX_test['annual_inc_cat_concat_loan_amnt_cat'] = X_test['annual_inc_cat'] + '_' + X_test['loan_amnt_cat']\n\n\ncols = ['annual_inc_cat', 'loan_amnt_cat', 'annual_inc_cat_concat_loan_amnt_cat']\nX_concat = pd.concat([X_train, X_test])\n\nfor col in cols:\n    summary = X_concat[col].value_counts()\n    X_train[col] = X_train[col].map(summary)\n    X_test[col] = X_test[col].map(summary)\n    \ndel X_concat\ngc.collect()","4c137e4c":"# \u30ed\u30fc\u30f3\u984d\u3092\u6708\u3005\u306e\u8fd4\u6e08\u984d\u3067\u5272\u3063\u3066\u8fd4\u6e08\u671f\u9593\u3092\u898b\u7a4d\u3082\u308b\nX_train['loan_period'] = X_train.loan_amnt \/ X_train.installment\nX_test['loan_period'] = X_test.loan_amnt \/ X_test.installment","dbd09581":"# \u307e\u3060dtype\u304cobject\u306e\u30ab\u30e9\u30e0\u3092\u4eee\u306b\u30ab\u30c6\u30b4\u30ea\u3068\u307f\u306a\u3059\ncat = []\n\nfor col in tqdm(X_train.columns):\n    if X_train[col].dtype == 'object':\n        \n        cat.append(col)","25b0f173":"# X_test\u3092 Count Encoding\nfor col in cat:\n    X_test[col + '_te'] = TargetEncoder(X_train, y_train, X_test, 'loan_condition', col)","c6de3dae":"# X_train\u3092oof\u3067TargetEncoding\nfor col in cat:\n    skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\n    temp = np.zeros(len(X_train))\n\n    for i, (train_ix, test_ix) in enumerate(skf.split(X_train, y_train)):\n        X_train_, y_train_ = X_train.iloc[train_ix], y_train.iloc[train_ix]\n        X_val, y_val = X_train.iloc[test_ix], y_train.iloc[test_ix]\n        \n        temp[test_ix] = TargetEncoder(X_train_, y_train_, X_val, 'loan_condition', col)\n    \n    X_train[col + '_te'] = temp","df5ae27c":"# X_test\u3092 Count Encoding\nfor col in cat:\n    X_train[col + 'ce'], X_test[col + 'ce'] = CountEncoder(X_train, X_test, col)","77d31fed":"# \u5143\u306ecategorical\u3092drop\nX_train.drop(cat, axis=1, inplace=True)\nX_test.drop(cat, axis=1, inplace=True)","1316dd3b":"X_train.to_csv('..\/X_train_tree.csv')\nX_test.to_csv('..\/X_test_tree.csv')\n\ntext_train.to_csv('..\/text_train_tree.csv')\ntext_test.to_csv('..\/text_test_tree.csv')","63dcea36":"# \u30c7\u30fc\u30bf\u306e\u8aad\u307f\u8fbc\u307f\ndf_train = pd.read_csv('..\/input\/train.csv', index_col=0, parse_dates=['issue_d'])\ndf_test = pd.read_csv('..\/input\/test.csv', index_col=0, parse_dates=['issue_d'])","0af8031a":"# \u53e4\u3059\u304e\u3066\u50be\u5411\u306e\u7570\u306a\u308b\u30c7\u30fc\u30bf\u3092drop\ndf_train = df_train[df_train.issue_d.dt.year >= 2011]","548f72ef":"# X, y\u306b\u5206\u96e2\u3059\u308b\u3002\ny_train = df_train.loan_condition\nX_train = df_train.drop(['loan_condition'], axis=1)\n\nX_test = df_test.copy()\n\ndel df_train, df_test\ngc.collect()","83bdfa59":"# \u7279\u5fb4\u91cf\u306e\u578b\u3092\u3056\u3063\u304f\u308a\u30c1\u30a7\u30c3\u30af\ncat = []\nnum = []\ntxt = ['emp_title', 'title']\n\nfor col in X_train.columns:\n    if X_train[col].dtype == 'object':\n        if col not in txt:\n            cat.append(col)\n    else:\n        if col != 'issue_d':\n            num.append(col)","72158bef":"# \u7279\u5fb4\u91cf\u306e\u578b\u3054\u3068\u306b\u5206\u3051\u308b\ncat_train = X_train[cat]\ntxt_train = X_train[txt]\nX_train = X_train[num]\n\ncat_test = X_test[cat]\ntxt_test = X_test[txt]\nX_test = X_test[num]","9230bc8d":"# \u6570\u5024\u3092rankGauss\nscaler = QuantileTransformer(copy=True, ignore_implicit_zeros=False, n_quantiles=1000,\n          output_distribution='normal', random_state=71,\n          subsample=100000)\nX_train = scaler.fit_transform(X_train.fillna(X_train.median()))\nX_test = scaler.transform(X_test.fillna(X_test.median()))","788accf6":"# \u30ab\u30c6\u30b4\u30ea\u3092One-hot\nfor col in tqdm(cat):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=True)\n    oe = OrdinalEncoder(return_df=False)\n    \n    cat_train[col] = oe.fit_transform(cat_train[[col]])\n    cat_test[col] = oe.transform(cat_test[[col]])    \n    \n    train = ohe.fit_transform(cat_train[[col]])\n    test = ohe.transform(cat_test[[col]])\n    \n    X_train = sp.sparse.hstack([X_train, train])\n    X_test = sp.sparse.hstack([X_test, test])","c2bddd9c":"# \u30c6\u30ad\u30b9\u30c8\u3092TFIDF\nfor (analyzer, n) in [('word',(1, 2)) , ('char',(2, 5))]:\n    for f in txt:\n        tfidf = TfidfVectorizer(max_features=100000, analyzer=analyzer, ngram_range=n)\n        \n        train = tfidf.fit_transform(txt_train[f].fillna('#'))\n        test = tfidf.transform(txt_test[f].fillna('#'))\n        \n        X_train = sp.sparse.hstack([X_train, train])\n        X_test = sp.sparse.hstack([X_test, test])\n        \nX_train = X_train.tocsr()\nX_test = X_test.tocsr()\n\ndel cat_train, cat_test, txt_train, txt_test\ngc.collect()","f9991362":"# \u30b7\u30f3\u30d7\u30eb\u306aMLP\ndef create_model(input_len):\n    inp = Input(shape=(input_len,), sparse=True) # \u758e\u884c\u5217\u3092\u5165\u308c\u308b\n    x = Dense(194+np.random.randint(5), activation='relu')(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64+np.random.randint(5), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64+np.random.randint(5), activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outp = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","551f76b2":"%%time\n# 30\u56de\u5b66\u7fd2\u30fb\u4e88\u6e2c\u3092\u884c\u3063\u3066\u3001Averaging\nnum_ave = 30\ny_pred_nn = np.zeros(X_test.shape[0])\n\nfor i in range(num_ave):\n    model = create_model(X_train.shape[1])\n    model.fit(X_train, y_train, batch_size=512, epochs=2)\n    y_pred_nn += model.predict(X_test).ravel()\n    \ny_pred_nn \/= num_ave","96012b43":"X_train = pd.read_csv('..\/X_train_tree.csv', index_col=0)\nX_test = pd.read_csv('..\/X_test_tree.csv', index_col=0)\n\ntext_train = pd.read_csv('..\/text_train_tree.csv', index_col=0)\ntext_test = pd.read_csv('..\/text_test_tree.csv', index_col=0)","d88c9f05":"tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=100000, use_idf=False)\n\ntext_train = tfidf.fit_transform(text_train.emp_title)\ntext_test = tfidf.transform(text_test.emp_title)\n\nX_train = sp.sparse.hstack([X_train, text_train])\nX_test = sp.sparse.hstack([X_test, text_test])  ","fc8c8831":"%%time\n# 20\u56de\u5b66\u7fd2\u30fb\u4e88\u6e2c\u3092\u884c\u3063\u3066\u3001Seed Averaging\nnum_ave = 20\ny_pred_lgb = np.zeros(X_test.shape[0])\n\nfor i in range(num_ave):\n    seed = np.random.randint(99999)\n    clf = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.71,\n                                        importance_type='split', learning_rate=0.05, max_depth=-1,\n                                        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n                                        n_estimators=1000, n_jobs=-1, num_leaves=31, objective=None,\n                                        random_state=seed, reg_alpha=1.0, reg_lambda=1.0, silent=True,\n                                        subsample=0.9, subsample_for_bin=200000, subsample_freq=0)\n\n    clf.fit(X_train, y_train)\n    y_pred_lgb += clf.predict_proba(X_test)[:,1]\n    \ny_pred_lgb \/= num_ave","8a000698":"X_train = pd.read_csv('..\/X_train_tree.csv', index_col=0).fillna(-99999)\nX_test = pd.read_csv('..\/X_test_tree.csv', index_col=0).fillna(-99999)\n\ntext_train = pd.read_csv('..\/text_train_tree.csv', index_col=0)\ntext_test = pd.read_csv('..\/text_test_tree.csv', index_col=0)","b520960e":"%%time\n# 20\u56de\u5b66\u7fd2\u30fb\u4e88\u6e2c\u3092\u884c\u3063\u3066\u3001Balanced Bootstrap\nnum_ave = 20\ny_pred_rgf = np.zeros(X_test.shape[0])\n\nfor i in range(num_ave):\n    seed = np.random.randint(99999)\n    \n    # negative downsampling\n    X_train_, y_train_, text_train_ = negative_downsample(X_train, y_train, text_train, seed=seed)\n    \n    tfidf = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=5000, use_idf=False)\n    \n    text_train_ = tfidf.fit_transform(text_train_.emp_title)\n    text_test_ = tfidf.transform(text_test.emp_title)\n    \n    X_train_ = sp.sparse.hstack([X_train_, text_train_])\n    X_test_ = sp.sparse.hstack([X_test, text_test_])   \n    \n    \n    clf = FastRGFClassifier(calc_prob='sigmoid', data_l2=2.0, l1=1.0, l2=1000.0,\n                                 learning_rate=0.1, loss='LS', max_bin=None, max_depth=6,\n                                 max_leaf=50, min_child_weight=5.0, min_samples_leaf=5,\n                                 n_estimators=1000, n_jobs=-1, opt_algorithm='rgf',\n                                 sparse_max_features=80000, sparse_min_occurences=5,\n                                 tree_gain_ratio=1.0, verbose=1)\n\n    clf.fit(X_train_, y_train_)\n    y_pred_rgf += clf.predict_proba(X_test_)[:,1]\n    cleanup()\n    \ny_pred_rgf \/= num_ave","46821f9d":"y_pred = 4*sp.stats.rankdata(y_pred_nn) + 4*sp.stats.rankdata(y_pred_lgb) + 3*sp.stats.rankdata(y_pred_rgf)\ny_pred \/= y_pred.max()","2391f7a8":"# sample submission\u3092\u8aad\u307f\u8fbc\u3093\u3067\u3001\u4e88\u6e2c\u5024\u3092\u4ee3\u5165\u306e\u5f8c\u3001\u4fdd\u5b58\u3059\u308b\nsubmission = pd.read_csv('..\/input\/sample_submission.csv', index_col=0)\n\nsubmission.loan_condition = y_pred\nsubmission.to_csv('submission.csv')","1e4c365b":"## LightGBM\u306e\u5b66\u7fd2\u30fb\u4e88\u6e2c","d81fb679":"## RGF\u306e\u5b66\u7fd2\u30fb\u4e88\u6e2c","83f09c27":"## Weighted Averaging","f32a3361":"## \u30e9\u30a4\u30d6\u30e9\u30ea\u306eimport\u3068\u5fc5\u8981\u306a\u95a2\u6570\u306e\u5b9a\u7fa9","8fe7bd37":"## Features for Tree Algos","63248e7d":"## Features for NN","c13a2491":"## NN\u306e\u5b66\u7fd2\u30fb\u4e88\u6e2c"}}