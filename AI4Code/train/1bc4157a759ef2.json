{"cell_type":{"bfe46cbb":"code","7305d819":"code","8f81ad10":"code","63712e4c":"code","aefad7de":"code","31d186b4":"code","84e51c69":"code","4bd0face":"code","3bed71cd":"code","5a32e018":"code","c967a89c":"code","d1c42037":"code","890bf8fa":"code","3086eca6":"code","a74c51af":"code","1764e3fd":"code","5c133fc2":"code","0050131f":"code","e6e30249":"code","e4da14e0":"code","e4f467f6":"code","6da8c0b2":"code","02c42059":"code","6ec107d2":"code","b06d87e8":"code","c57b5cb3":"code","4cd64c5e":"code","650461e0":"code","5c646751":"code","93eb09e6":"code","6b80158e":"code","a7ab762e":"code","411ee681":"code","7358e11b":"code","a28072bd":"code","0c4898f7":"code","0fd1ffcf":"code","84e2e78a":"code","89d98724":"code","e2e588d6":"code","e096b062":"code","ea1c7e96":"code","224757bb":"code","15ba7a7d":"code","22965dd7":"code","498bd6be":"code","16d61101":"code","178eee27":"code","680b7627":"code","bfd7590e":"code","7e9675b2":"code","6e4b5c81":"code","aca38ac1":"code","9cc20ac4":"code","21ab5cd4":"code","8ecb8a89":"code","57ccb90b":"code","6725ee90":"code","ae7fdfe7":"code","47fc2661":"code","c5cec00b":"code","33e723ff":"code","306d6d1e":"code","2f1f73b4":"code","357dbf8b":"code","a16387c5":"code","648318d3":"code","bb25e4ba":"code","10e341bc":"code","56d12975":"code","189fe512":"code","257e5474":"code","cede53ee":"code","fd4018f0":"code","9784c68a":"code","8445c9f3":"code","ff2adc38":"code","c1e64c30":"code","3f183a28":"code","0d2bcb62":"code","8a87c335":"code","a3c385cd":"code","7cb6d4c7":"code","f5f69e32":"code","592ab4e2":"code","e9da4e55":"code","2886ba1e":"code","33f683cc":"code","ae996a97":"code","2392ab22":"code","e8a3d2a6":"code","dcd6f030":"code","c6ea908a":"code","037b47d3":"markdown","26d7a5bd":"markdown","81ee6022":"markdown","bf989fe7":"markdown","7a617e96":"markdown","823038ee":"markdown","fed55cf6":"markdown","22ae2862":"markdown","99a2bb8e":"markdown","499faedc":"markdown","013f1750":"markdown","5124bfc7":"markdown","32dbea47":"markdown","18f0bd6c":"markdown","9ed730f0":"markdown","8ddf712f":"markdown","01b1f42b":"markdown","c4a7907f":"markdown","689b66fd":"markdown","ae538530":"markdown","745cc766":"markdown","2f23af27":"markdown","0bec01d5":"markdown","e384e25f":"markdown","55b4731c":"markdown","0a33a5f3":"markdown","9cfe6162":"markdown","3d44fa4d":"markdown","2eea2fdd":"markdown","62fa68c1":"markdown","dde4a998":"markdown","cb637ad1":"markdown","c02223f5":"markdown","779d1d48":"markdown","72be0e2f":"markdown","d867a1c7":"markdown","dacc8a86":"markdown","7a0dcf9f":"markdown","7540621b":"markdown","fb9832cd":"markdown","0656db0a":"markdown","ba3ed681":"markdown"},"source":{"bfe46cbb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nfrom scipy import stats\nfrom scipy.stats import norm, skew ","7305d819":"from scipy.stats import shapiro #for checking for skewed data - Shapiro Wilks Test\nimport matplotlib\nimport seaborn as sns","8f81ad10":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')\ntrain.head()","63712e4c":"test.head()\n#train.dtype","aefad7de":"train.isnull().sum()","31d186b4":"len(train.columns)","84e51c69":"shapiro(train.SalePrice)[1]#if the value is < 0.05 then it is skewed","4bd0face":"# To calculate skewedness of all the columns , we use Fisher Pearson Standard Moment Co-efficient\nnum_feats = train.dtypes[train.dtypes!= 'object'].index # determine index for continuous variables\nskew_feats = train.skew().sort_values(ascending = False)# calculate skewness & sort descending\nskewness = pd.DataFrame({'skew': skew_feats})","3bed71cd":"skewness","5a32e018":"resp = train.SalePrice\nsqrt_resp = resp ** (.5)\nsns.distplot(sqrt_resp)","c967a89c":"recip = 1\/resp\nsns.distplot(recip)","d1c42037":"log_resp = np.log(resp)\nsns.distplot(log_resp)","890bf8fa":"from scipy.stats import boxcox\nskewed_features = skewness.index\nlambd = 0.15\nnum_feat=train.columns[train.dtypes!=object]\nposdata = train[train > 0]\nfor feat in skewed_features:\n    train[feat], _ = boxcox(posdata[feat])","3086eca6":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())#log transformation is usually good for positive skewedness\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())#tail","a74c51af":"#correlation in data\ncorr=train.corr()[\"SalePrice\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","1764e3fd":"OverallQual ,GrLivArea ,GarageCars,GarageArea ,TotalBsmtSF, 1stFlrSF ,FullBath,TotRmsAbvGrd,YearBuilt, YearRemodAdd have more than 0.5 correlation with SalePrice.\n\nEnclosedPorch and KitchenAbvGr have little negative correlation with target variable.\n\nThese can prove to be important features to predict SalePrice.","5c133fc2":"#plotting correlations\nnum_feat=train.columns[train.dtypes!=object]\nnum_feat=num_feat[1:-1] \nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train[col].values, train.SalePrice.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(values), color='red')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Sale Price\");","0050131f":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n#'SalePrice' and 'GrLivArea' have a linear relationship.","e6e30249":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n# linear relationship- gradually leading to exponential","e4da14e0":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","e4f467f6":"var = 'YearBuilt'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","6da8c0b2":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n#there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, \n#and the second one refers to the 'GarageX' variables. Both cases show how significant the correlation is between these variables.\n#Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, \n#we can conclude that they give almost the same information so multicollinearity really occurs.","02c42059":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","6ec107d2":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();\n#The pairs plot builds on two basic figures, the histogram and the scatter plot. The histogram on the diagonal allows\n#us to see the distribution of a single variable while \n#the scatter plots on the upper and lower triangles show the relationship (or lack thereof) between two variables.","b06d87e8":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","c57b5cb3":"train.isnull().sum().sort_values(ascending=False)","4cd64c5e":"#dealing with missing data\ntrain = train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain = train.drop(train.loc[train['Electrical'].isnull()].index)\ntrain.isnull().sum().max() #just checking that there's no missing data missing...","650461e0":"Univariate analysis- The primary concern here is to establish a threshold that defines an observation as an outlier.\nTo do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and\na standard deviation of 1.\n","5c646751":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\n#Simply put, the newaxis is used to increase the dimension of the existing array by one more dimension, when used once.\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","93eb09e6":"train['SalePrice'].describe()","6b80158e":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","a7ab762e":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","411ee681":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","7358e11b":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","a28072bd":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","0c4898f7":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","0fd1ffcf":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","84e2e78a":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","89d98724":"#Data Correlation\n#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","e2e588d6":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","e096b062":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","ea1c7e96":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","224757bb":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","15ba7a7d":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","22965dd7":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","498bd6be":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","16d61101":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","178eee27":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","680b7627":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","bfd7590e":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","7e9675b2":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","6e4b5c81":"all_data = all_data.drop(['Utilities'], axis=1)\n","aca38ac1":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n","9cc20ac4":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","21ab5cd4":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","8ecb8a89":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","57ccb90b":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n","6725ee90":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n","ae7fdfe7":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","47fc2661":"print(\"all_data size is : {}\".format(all_data.shape))","c5cec00b":"all_data.head()","33e723ff":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","306d6d1e":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","2f1f73b4":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","357dbf8b":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\n","a16387c5":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","648318d3":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","bb25e4ba":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","10e341bc":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n#import xgboost as xgb\n#import lightgbm as lgb","56d12975":"import xgboost as xgb\nimport lightgbm as lgb","189fe512":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","257e5474":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n","cede53ee":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","fd4018f0":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n","9784c68a":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","8445c9f3":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","ff2adc38":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","c1e64c30":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3f183a28":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0d2bcb62":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8a87c335":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a3c385cd":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","7cb6d4c7":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","f5f69e32":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","592ab4e2":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","e9da4e55":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","2886ba1e":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","33f683cc":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","ae996a97":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","2392ab22":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","e8a3d2a6":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","dcd6f030":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15\n","c6ea908a":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","037b47d3":"Adding one more important feature\n\nSince area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","26d7a5bd":"LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","81ee6022":"Label Encoding some categorical variables that may contain information in their ordering set","bf989fe7":"'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables.Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).","7a617e96":"MSSubClass : Na most likely means No building class. We can replace missing values with None","823038ee":"Alley : data description says NA means \"no alley access\"","fed55cf6":"Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","22ae2862":"BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement","99a2bb8e":"Elastic Net Regression :\nagain made robust to outliers","499faedc":"Wow ! It seems even the simplest stacking approach really improve the score . This encourages us to go further and explore a less simple stacking approch.","013f1750":"BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.","5124bfc7":"MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","32dbea47":"The target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","18f0bd6c":"Fence : data description says NA means \"no fence\"","9ed730f0":"GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None","8ddf712f":"One of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).","01b1f42b":"Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","c4a7907f":"GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)","689b66fd":"Outliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extremely large areas for very low prices).\n\nThere are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why , instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that.","ae538530":"KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","745cc766":"Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","2f23af27":"Box Cox Transformation of (highly) skewed features\n\nWe use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.","0bec01d5":"Getting dummy categorical features","e384e25f":"Target Variable\nSalePrice is the variable we need to predict. So let's do some analysis on this variable first.","55b4731c":"FireplaceQu : data description says NA means \"no fireplace\"","0a33a5f3":"MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","9cfe6162":"Imputing missing values\nWe impute them by proceeding sequentially through features with missing values\n\nPoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.","3d44fa4d":"Functional : data description says NA means typical","2eea2fdd":"We get again a better score by adding a meta learner\n\nEnsembling StackedRegressor, XGBoost and LightGBM\nWe add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","62fa68c1":"Final Training and Prediction\nStackedRegressor:","dde4a998":"It remains no missing value.\n\nMore features engeneering\nTransforming some numerical variables that are really categorical","cb637ad1":"Gradient Boosting Regression :\nWith huber loss that makes it robust to outliers","c02223f5":"MiscFeature : data description says NA means \"no misc feature\"","779d1d48":"LASSO Regression :\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","72be0e2f":"The skew seems now corrected and the data appears more normally distributed.\n\nFeatures engineering\nlet's first concatenate the train and test data in the same dataframe","d867a1c7":"Stacking models\nSimplest Stacking approach : Averaging base models\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","dacc8a86":"Modelling\nImport librairies","7a0dcf9f":"Data Processing\nOutliers\nLooking out for outliers present in the training data\n\n","7540621b":"Base models scores\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","fb9832cd":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers. Therefore, we can safely delete them.","0656db0a":"Getting the new train and test sets.","ba3ed681":"SaleType : Fill in again with most frequent which is \"WD\""}}