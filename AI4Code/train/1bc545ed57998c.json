{"cell_type":{"68aea9a7":"code","0fb55ed4":"code","45887b70":"code","859616a6":"code","cacf19df":"code","06f69e20":"code","40249189":"code","28d9f3b8":"code","ff9ca508":"code","335f43ae":"code","5c8359c0":"code","634f7ec3":"code","760329f6":"code","403bfd8c":"code","9fcbb439":"code","f409651e":"code","c1c0a128":"code","c4882302":"code","ac25139f":"code","28c0e21d":"code","4644ba55":"code","50e1c998":"code","03d94cf2":"code","1af5f907":"code","060be54e":"code","33c40b67":"code","7df52b22":"code","62005f9c":"code","6383726d":"code","27de81d6":"code","033ab65a":"code","253cb415":"code","899a72a8":"code","af94cca2":"code","b408e208":"code","5d54424a":"code","4843dfbc":"code","c4be4a83":"code","221dd3f6":"code","56e2a72c":"code","de3256ff":"code","9b1c3375":"code","7036ffe6":"code","a917f5f5":"code","54ab08b2":"code","cdde0e3e":"code","67eaa858":"code","acb8faed":"code","1f3a4bee":"code","01645107":"code","a38e97a0":"code","ce57d27f":"code","932d1f11":"code","a380418e":"code","b249017b":"code","a368197d":"code","d91fecb4":"code","381d8431":"code","feead988":"code","d3cb0bdb":"code","09eec7d5":"code","9e9c5b20":"code","41b782fc":"code","dac031e8":"code","9f17e26e":"code","f202037a":"code","cce05ee8":"code","08c4a11a":"code","2fea322c":"code","c824e307":"code","670c7e85":"code","8b3b4430":"code","76c55b4a":"code","adbf783b":"code","4deaab0f":"code","c2375c19":"code","bcb6afdb":"code","20059822":"code","75a322bf":"code","e3c6f12b":"code","65e9b20f":"code","a28c80bb":"code","b7c485a3":"code","6a2b39ba":"code","6a516f47":"code","cba58319":"code","59832b1d":"code","1e75144b":"code","9fb1f8ca":"markdown","f410a52f":"markdown","67eccafb":"markdown","c1b277a0":"markdown","7f80cd4f":"markdown","3a284f10":"markdown"},"source":{"68aea9a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.feature_selection import SelectFwe, f_regression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import OneHotEncoder, StackingEstimator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0fb55ed4":"train = pd.read_csv('\/kaggle\/input\/janatahack\/train_8wry4cB.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack\/test_Yix80N0.csv')\nsample = pd.read_csv('\/kaggle\/input\/janatahack\/sample_submission_opxHi4g.csv')","45887b70":"print(train.shape)\nprint(train.info)","859616a6":"train.apply(lambda x:len(x.unique()))","cacf19df":"train.isna().sum()","06f69e20":"sns.countplot(train['gender'])","40249189":"train['startTime'] = pd.to_datetime(train['startTime'])\ntrain['endTime'] = pd.to_datetime(train['endTime'])","28d9f3b8":"train","ff9ca508":"df = train.append(test)","335f43ae":"df = df[['session_id','startTime','endTime','ProductList','gender']]","5c8359c0":"df['ProductCount'] = df.ProductList.str.count(';')+1","634f7ec3":"df['ProductList']","760329f6":"df['startTime'] = pd.to_datetime(df['startTime'])\ndf['endTime'] = pd.to_datetime(df['endTime'])","403bfd8c":"import numpy as np\nfrom itertools import chain\n\n# return list from series of comma-separated strings\ndef chainer(s):\n    return list(chain.from_iterable(s.str.split(';')))\n\n# calculate lengths of splits\nlens = df['ProductList'].str.split(';').map(len)\n\n# create new dataframe, repeating or chaining as appropriate\ndf1 = pd.DataFrame({'session_id': np.repeat(df['session_id'], lens),\n                    'startTime': np.repeat(df['startTime'], lens),\n                    'endTime':np.repeat(df['endTime'],lens),\n                    'ProductCount': np.repeat(df['ProductCount'], lens),\n                    'ProductList': chainer(df['ProductList']),\n                    'gender':np.repeat(df['gender'],lens)})\n\nprint(df1)","9fcbb439":"df1.head()","f409651e":"df1['TimeTaken'] = abs(df1['endTime'] - df1['startTime']).astype('timedelta64[m]')","c1c0a128":"df1[['Date','Time']] = df1['startTime'].astype(str).str.split(\" \",expand=True) ","c4882302":"df1['Date'] = pd.to_datetime(df1['Date'])","ac25139f":"df1['Day'] = df1['Date'].apply(lambda x: x.weekday())","28c0e21d":"df1.head()","4644ba55":"df1['TimeTaken'].max()","50e1c998":"df1[['Category','SubCategory','SubSubCategory','SubSubSubCategory','Extra']] = df1['ProductList'].str.split(\"\/\",expand=True) ","03d94cf2":"del df1['Extra']\ndel df1['ProductList']\n","1af5f907":"del df1['Time']\ndel df1['Date']","060be54e":"len(df1['session_id'].unique())","33c40b67":"df1.columns","7df52b22":"df1 = df1[['session_id','TimeTaken','Day','ProductCount','Category','SubCategory','SubSubCategory','SubSubSubCategory','gender']]","62005f9c":"df1['TimeTaken'] = df1.TimeTaken.apply(lambda x:int(x))","6383726d":"df1.head()","27de81d6":"from sklearn import preprocessing \ncolumns = ['Category','SubCategory','SubSubCategory','SubSubSubCategory']\nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    df1[i]= label_encoder.fit_transform(df1[i]) \n  ","033ab65a":"df1['session_id'].describe()","253cb415":"test1 = df1[df1['gender'].isnull() == True]","899a72a8":"train1 = df1[df1['gender'].isnull() == False]\n","af94cca2":"train1.head()","b408e208":"test1.head()","5d54424a":"from sklearn import preprocessing \ncolumns = ['gender']\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    train1[i]= label_encoder.fit_transform(train1[i]) \n  ","4843dfbc":"train1.corr()","c4be4a83":"sns.countplot(train1['Category'])","221dd3f6":"sns.countplot(train1['Day'])","56e2a72c":"train1.groupby('Day')['gender'].size()","de3256ff":"sns.countplot(train1['ProductCount'])","9b1c3375":"pd.crosstab(train1['Day'],train1['gender'])","7036ffe6":"del test1['gender']","a917f5f5":"def extra_tree(Xtrain,Ytrain,Xtest):\n    extra = ExtraTreesClassifier()\n    extra.fit(Xtrain, Ytrain) \n    extra_prediction = extra.predict(Xtest)\n    return extra_prediction\ndef Xg_boost(Xtrain,Ytrain,Xtest):\n    xg = XGBClassifier(loss='exponential', learning_rate=0.05, n_estimators=1000, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=1, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n    xg.fit(Xtrain, Ytrain) \n    xg_prediction = xg.predict(Xtest)\n    return xg_prediction\ndef LGBM(Xtrain,Ytrain,Xtest):\n    lgbm = LGBMClassifier(boosting_type='gbdt', num_leaves=40,\n                            max_depth=5, learning_rate=0.05, n_estimators=1000, subsample_for_bin=200, objective='binary', \n                            min_split_gain=0.0, min_child_weight=0.001, min_child_samples=10,\n                            subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0,\n                            reg_lambda=0.0, random_state=None, n_jobs=1, silent=True, importance_type='split')\n    #lgbm = LGBMClassifier(n_estimators= 500)\n    lgbm.fit(X_train, Y_train)\n    lgbm_preds = lgbm.predict(X_test)\n    return lgbm_preds","54ab08b2":"print(train1.columns)\nprint(test1.columns)","cdde0e3e":"X_train = train1[['TimeTaken','Day','ProductCount','Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]\nY_train = train1['gender']\nX_test = test1[['TimeTaken','Day','ProductCount', 'Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]","67eaa858":"from autoviml.Auto_ViML import Auto_ViML","acb8faed":"target = 'gender'\nscoring_parameter = 'balanced-accuracy'","1f3a4bee":"\nm, feats, trainm, testm = Auto_ViML(train1, target, test1,\n                                    scoring_parameter=scoring_parameter,\n                                    hyper_param='GS',feature_reduction=True,\n                                     Boosting_Flag='Boosting_Flag',Binning_Flag=False)","01645107":"sam = pd.read_csv('\/kaggle\/input\/sample\/gender_Binary_Classification_submission.csv')","a38e97a0":"\ntest1['gender'] = sam['gender_predictions']\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Auto.csv',index = False)","ce57d27f":"X_train.head()\ncate_features_index = np.where(X_train.dtypes != float)[0]","932d1f11":"xtrain,xtest,ytrain,ytest = train_test_split(X_train,Y_train,train_size=0.99,random_state=1236)","a380418e":"from catboost import Pool, CatBoostClassifier, cv, CatBoostRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","b249017b":"model = CatBoostClassifier(iterations=7000, learning_rate=0.001, l2_leaf_reg=3.5, depth=5, \n                           rsm=0.99, loss_function= 'Logloss', eval_metric='AUC',use_best_model=True,random_seed=50)","a368197d":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","d91fecb4":"predss = model.predict(X_test)","381d8431":"\ntest1['gender'] = predss\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('FinalSubmission.csv',index = False)","feead988":"#pred_xg = Xg_boost(X_train,Y_train,X_test)\n#pred_et = extra_tree(X_train,Y_train,X_test)\npred_l = LGBM(X_train,Y_train,X_test)\n","d3cb0bdb":"# 0 - female, 1 male","09eec7d5":"test1['gender'] = pred_xg\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DXG.csv',index = False)","9e9c5b20":"test1['gender'] = pred_et\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DETC.csv',index = False)","41b782fc":"\ntest1['gender'] = pred_l\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DPCLGBM.csv',index = False)\n","dac031e8":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, Y_train)\nans = clf.predict(X_test)\n","9f17e26e":"print(len(pred_l))\ntest1['gender'] = ans\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DLR.csv',index = False)","f202037a":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=100).fit(X_train, Y_train)\nprediction_of_ada = ada.predict(X_test)","cce05ee8":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(loss='exponential', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001).fit(X_train, Y_train)\nprediction_of_gbc = gbc.predict(X_test)","08c4a11a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=10).fit(X_train, Y_train)\nprediction_of_rf = rf.predict(X_test)","2fea322c":"\ntest1['gender'] = prediction_of_ada\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DADA.csv',index = False)","c824e307":"\ntest1['gender'] = prediction_of_gbc\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dgbc.csv',index = False)","670c7e85":"\ntest1['gender'] = prediction_of_rf\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DRF.csv',index = False)","8b3b4430":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train,Y_train)\n\n# Predicted class\nnri = neigh.predict(X_test)\n","76c55b4a":"\ntest1['gender'] = nri\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dknn.csv',index = False)","adbf783b":"from sklearn.calibration import CalibratedClassifierCV","4deaab0f":"model = XGBClassifier()\nmetLearn=CalibratedClassifierCV(model, method='isotonic', cv=2)\nmetLearn.fit(X_train, Y_train)\ntestPredictions = metLearn.predict(X_test)","c2375c19":"def submissions(predictions_by_model,string):\n    test1['gender'] = predictions_by_model\n    testn = test1[['session_id','gender']]\n    print(testn.isna().sum())\n    test_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\n    dic = {1:'male',0:'female'}\n    test_final['gender'] = test_final['gender'].map(dic)\n    test_final.to_csv(string.csv,index = False)\n","bcb6afdb":"test1['gender'] = testPredictions\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DCCV.csv',index = False)","20059822":"import pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\n# sklearn tools for model training and assesment\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\nfrom sklearn.metrics import (roc_curve, auc, accuracy_score)\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': {'binary_logloss', 'auc'},\n    'metric_freq': 1,\n    'is_training_metric': True,\n    'max_bin': 255,\n    'learning_rate': 0.1,\n    'num_leaves': 63,\n    'tree_learner': 'serial',\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 5,\n    'is_enable_sparse': True,\n    'use_two_round_loading': False,\n    'is_save_binary_file': False,\n    'output_model': 'LightGBM_model.txt',\n    'num_machines': 1,\n    'local_listen_port': 12400,\n    'machine_list_file': 'mlist.txt',\n    'verbose': 0,\n    'subsample_for_bin': 200000,\n    'min_child_samples': 20,\n    'min_child_weight': 0.001,\n    'min_split_gain': 0.0,\n    'colsample_bytree': 1.0,\n    'reg_alpha': 0.0,\n    'reg_lambda': 0.0\n}\n\n\nlgb_train = lgb.Dataset(X_train, Y_train)\n\n ","75a322bf":"lgb_train","e3c6f12b":"lgb_eval = lgb.Dataset(X_test)","65e9b20f":"# train\ngbm = lgb.train(params,\n                lgb_train,\n                valid_sets=lgb_eval)","a28c80bb":"\n\n\ngridParams = {\n    'learning_rate': [ 0.1],\n    'num_leaves': [63],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary']\n}\n\nmdl = lgb.LGBMClassifier(\n    task = params['task'],\n    metric = params['metric'],\n    metric_freq = params['metric_freq'],\n    is_training_metric = params['is_training_metric'],\n    max_bin = params['max_bin'],\n    tree_learner = params['tree_learner'],\n    feature_fraction = params['feature_fraction'],\n    bagging_fraction = params['bagging_fraction'],\n    bagging_freq = params['bagging_freq'],\n    min_data_in_leaf = params['min_data_in_leaf'],\n    min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n    is_enable_sparse = params['is_enable_sparse'],\n    use_two_round_loading = params['use_two_round_loading'],\n    is_save_binary_file = params['is_save_binary_file'],\n    n_jobs = -1\n)\n\nscoring = {'AUC': 'roc_auc'}\n\n# Create the grid\n#grid = GridSearchCV(mdl, gridParams, verbose=2, cv=5, scoring=scoring, n_jobs=-1, refit='AUC')\n# Run the grid\n\n\n#print('Best parameters found by grid search are:', grid.best_params_)\n#print('Best score found by grid search is:', grid.best_score_)\n","b7c485a3":"yes = gbm.predict(X_test)","6a2b39ba":"yess =[]\nfor i in yes:\n    if i>=0.5:\n        yess.append(1)\n    else:\n        yess.append(0)","6a516f47":"test1['gender'] = yess\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DDCCV.csv',index = False)","cba58319":"import h2o\nh2o.init()\ntrain2 = h2o.H2OFrame(train1)\ntest2 = h2o.H2OFrame(X_test)\ntrain1.columns\ny = 'gender'\nx = train2.col_names\nx.remove(y)\ntrain2['gender'] = train2['gender'].asfactor()\ntrain2['gender'].levels()\nfrom h2o.automl import H2OAutoML\naml1 = H2OAutoML(max_models = 30, max_runtime_secs=200, seed = 1)\naml1.train(x = x, y = y, training_frame = train2)\npreds = aml1.predict(test2)\nprint(sample.columns)\ntest1['gender'] = preds\n#ans=h2o.as_list(preds) \n#sample['gender'] = ans['predict']\n#sample.to_csv('Solution_H2O(Divided).csv',index=False)\n#lb = aml.leaderboard\n#lb.head()\n#lb.head(rows=lb.nrows)","59832b1d":"test1['gender'] = (h2o.as_list(preds['predict']))","1e75144b":"\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DH2o.csv',index = False)","9fb1f8ca":"Gender - Male and Female\nUnique session id\n9402 unique products","f410a52f":"No null values","67eccafb":"More females view then male","c1b277a0":"All other models for refference","7f80cd4f":"We could also see that there is some inconsitency in start and end time","3a284f10":"Changing the orders"}}