{"cell_type":{"c60344c7":"code","985455cd":"code","95982716":"code","eb25489d":"code","ec93d22c":"code","a364da9c":"code","80d80976":"code","96d56504":"code","ebdc8128":"code","e1cfdd97":"code","939df007":"code","c73f902c":"code","78d51323":"code","3935880f":"markdown","6e46e295":"markdown","82d426e1":"markdown","e81aaa24":"markdown","b06da918":"markdown","d6ee36a5":"markdown","1e05de8c":"markdown","52fa9a02":"markdown","186d786f":"markdown"},"source":{"c60344c7":"import pandas as pd\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer, r2_score, accuracy_score,classification_report,confusion_matrix\nfrom sklearn.ensemble import AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression","985455cd":"def graph_distance(regressor, x, y, title=\"\"):\n  predictions = regressor.predict(x)\n  differences = y - predictions\n  distance = np.linalg.norm(y-predictions)\n  print('Euclidean Distance :',distance)\n  print('Mean Squared :',mean_squared_error(predictions,y))\n  plt.plot(differences)\n  plt.title(title)\n  plt.ylim((-30,30))\n  plt.legend(['Difference'])\n  plt.show()","95982716":"def plot_predictions(predictions=None,actual=None,x_start=1380,x_stop=40,title=''):\n  x_stop = x_start+x_stop\n  plt.plot(actual[x_start:x_stop])\n  plt.grid(True, which='both')\n  plt.title(title)\n  plt.plot(predictions[x_start:x_stop])\n  plt.legend(['open','Prediction'])\n  plt.show()","eb25489d":"ticks_in_future=2\n# load dataset\ndf = pd.read_csv(\"..\/input\/tsla-dal-withfeatures\/Apr29_p3_.TSLA.fs5.15824.ds.csv\")\ndf['datetime'] = pd.to_datetime(df['datetime'])\ndf['open_next']=df['open'].shift(-ticks_in_future) # this is the 'future' value\ndf.dropna(inplace=True)\n#print(df.isnull().sum())\nprint('shape: %s \\nstart date: %s \\nend date: %s'%(df.shape,df['datetime'].head(1)[0],df['datetime'].tail(1)))\nplt.plot(df['open'].head(50))\nplt.plot(df['open_next'].head(50))\nplt.legend(['open','to_predict'])\nplt.show()\ndf[['open','open_next']].head()","ec93d22c":"plt.figure(1 , figsize = (17 , 8))\ncor = sns.heatmap(df.iloc[:,:25].corr(), annot = True)","a364da9c":"y = df['open_next'].to_numpy()\n# we dont use all the features, because it makes everything worse ! \n#X = df.drop(columns=['open_next','datetime']).to_numpy()\nX = df[['open', 'high', 'low', 'close', 'volume']].to_numpy()\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,shuffle=False)\nprint('xtrain size: %s xtest size: %s'%(x_train.shape[0],x_test.shape[0]))","80d80976":"lr = LinearRegression()\nlr.fit(x_train,y_train)\nlr.coef_","96d56504":"lr.score(x_test,y_test)","ebdc8128":"graph_distance(lr, x_test, y_test, title=\"Linear Regression\")\npredictions = lr.predict(x_test)\nplot_predictions(predictions,y_test,x_start=1350, x_stop=50, title=\"Linear Regression\")\n\n\n","e1cfdd97":"regressors = [AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor]\nfor r in regressors:\n  reg = r(random_state=0, n_estimators=100)\n  params = reg.get_params()\n  if 'base_estimator' in params:\n    reg = r(lr,random_state=0, n_estimators=100)\n  name=reg.__class__.__name__\n  print(name)\n  reg.fit(x_train,y_train)\n  graph_distance(reg, x_test, y_test, title=name)\n  predictions = reg.predict(x_test)\n  plot_predictions(predictions,y_test, x_start=1350, x_stop=50, title=name)\n  #plot_predictions(predictions,x_start, title=name)","939df007":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2","c73f902c":"lr=LinearRegression()\nlr.fit(X_train, y_train)\nypl=lr.predict(X_val)\nprint('valMSE for LinearRegression:',mean_squared_error(y_val, ypl))\nypt=lr.predict(X_test)\nprint('TestMSE for LinearRegression:',mean_squared_error(y_test, ypt)) # To check if the model is overfitting","78d51323":"regressors = [AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,RandomForestRegressor]\nfor r in regressors:\n  reg = r(random_state=0, n_estimators=100)\n  params = reg.get_params()\n  if 'base_estimator' in params:\n    reg = r(lr,random_state=0, n_estimators=100)\n  name=reg.__class__.__name__\n  #print(name)\n  reg.fit(X_train,y_train)\n  ypv=reg.predict(X_val)\n  print(name,':The MSE error for validation:',mean_squared_error(y_val, ypv))\n  ypt=reg.predict(X_test)  \n  print(name,':The MSE error for testing:',mean_squared_error(y_test, ypt))","3935880f":"# Metric functions for linear regression and emsemble estimator ","6e46e295":"# First we load a dataset of tsla stock ticker\n* Dataset filled with the default features","82d426e1":"# heatmap to see correlations in the data","e81aaa24":"# Test all regressors (ensemble)\n","b06da918":"# Linear regression metric test","d6ee36a5":"# Emsemble estimators validation and testing MSE","1e05de8c":"# Train linear regression and fit to training data","52fa9a02":"# Split data into X and y, and do train\/test split\n* y is the open 2 minutes in the future\n* X is the dataset with the future and datetime removed\n* Make sure shuffle is false because this is time series data","186d786f":"# Developing a robust model\u00b6\n* For the LinearRegression\n* For ensemble of estimators"}}