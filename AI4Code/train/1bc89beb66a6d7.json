{"cell_type":{"968a724c":"code","0874e2d3":"code","3835b8c8":"code","f706caef":"code","eeb1f711":"code","6bd7c5d0":"code","9101b6eb":"code","d10ac2fb":"code","7cccb5dd":"code","2aa62867":"code","3e0fb828":"code","c441933e":"code","1fd443a0":"code","07717c84":"code","63202929":"code","cbf68760":"code","aeeb675a":"code","0b5ae2e0":"code","c3572b5e":"code","49bd1b64":"code","0c76be7e":"code","04c07a3d":"code","1443d2d2":"code","19017251":"code","d37c90ee":"code","857a7017":"code","042d12cd":"code","4fa46033":"code","b9b8aa06":"code","73741bd1":"code","80c34dd7":"code","9fe11fd6":"markdown","9e9e0b64":"markdown","b4c9cbf2":"markdown","3431e371":"markdown","386eb446":"markdown","dfa755c4":"markdown"},"source":{"968a724c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0874e2d3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3835b8c8":"import os\nimport gc\nimport random\nimport math\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import log_loss\n\nimport category_encoders as ce\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport warnings\n#warnings.filterwarnings(\"ignore\")","f706caef":"sample_submission=pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_targets_scored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_features=pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features=pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","eeb1f711":"train_features.head()","6bd7c5d0":"train_features.shape","9101b6eb":"train_features.sig_id.nunique()","d10ac2fb":"train_targets_scored.head()","7cccb5dd":"train_targets_scored.sum()[1:].sort_values()","2aa62867":"g = train_features[:1][[col for col in train_features if 'g-' in col]]\ng","3e0fb828":"g= g.values.reshape(-1,1)\nplt.plot(g)","c441933e":"print(\"Number of features (g-) = \",train_features.columns.str.startswith('g-').sum())","1fd443a0":"c = train_features[:1][[col for col in train_features if 'c-' in col]]\nc","07717c84":"c=c.values.reshape(-1,1)\nplt.plot(c)","63202929":"print(\"Number of features (c-) = \",train_features.columns.str.startswith('c-').sum())","cbf68760":"plt.figure(figsize=(5,5))\nplt.subplot(1, 2, 1)\nx_value = train_features.cp_type.value_counts()\nplt.title(\"Training data\")\nplt.bar(x_value.index,x_value.values)","aeeb675a":"sns.heatmap(train_features.loc[:, ['g-0', 'g-1', 'g-2','g-3','g-4', 'c-95', 'c-96', 'c-97','c-98', 'c-99']].corr(), annot=True)\nplt.show()","0b5ae2e0":"train_targets_scored.head(10)","c3572b5e":"train_targets_nonscored.head(10)","49bd1b64":"# for g- feature columns\ng = [col for col in train_features if 'g-' in col]\ng = sns.pairplot(train_features[g[:10]])\nplt.show()","0c76be7e":"# for c- feature columns\ng_vis = sns.pairplot(train_features[[col for col in train_features if 'c-' in col][:10]])\nplt.show()","04c07a3d":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n \nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","1443d2d2":"SEED = 123\nEPOCHS = 49\nBATCH_SIZE = 256\nFOLDS =5\nREPEATS = 5\ntarget_cols = train_targets_scored.columns[1:]\ninput_shape = len(target_cols)","19017251":"def seed(seed):\n    np.random.seed(seed)\n    os.environ['seed'] = str(seed)\n    tf.random.set_seed(seed)\ndef multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","d37c90ee":"def preprocess_input(data):\n    data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n    data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n    return data\nx_train = preprocess_input(train_features.drop(columns=\"sig_id\"))\nx_test =preprocess_input(test_features.drop(columns=\"sig_id\"))\ny_train = train_targets_scored.drop(columns=\"sig_id\")\nN_FEATURES = x_train.shape[1]","857a7017":"def dnn_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATURES),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),  \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(input_shape, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    model.summary()\n    return model\n\n","042d12cd":"print(x_train.shape)\nprint(y_train.shape)","4fa46033":"def build_train(resume_models = None, repeat_number = 0, folds = 5, skip_folds = 0):\n    \n    models = []\n    preds_df = y_train.copy()\n    \n\n    kfold = KFold(folds, shuffle = True)\n    for fold, (train_ind, val_ind) in enumerate(kfold.split(x_train)):\n        print('\\n')\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        \n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'binary_crossentropy', factor = 0.4, patience = 2, verbose = 1, min_delta = 0.0001, mode = 'auto')\n        checkpoint_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n        cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n\n        model = dnn_model()\n        model.fit(x_train.values[train_ind],\n              y_train.values[train_ind],\n              validation_data=(x_train.values[val_ind], y_train.values[val_ind]),\n              callbacks = [cb_lr_schedule, cb_checkpt],\n              epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1\n             )\n        model.load_weights(checkpoint_path)\n        preds_df.loc[val_ind, :] = model.predict(x_train.values[val_ind])\n        models.append(model)\n\n    return models, preds_df","b9b8aa06":"models = []\npreds_df = []\n# seed everything\nseed(SEED)\nfor i in range(REPEATS):\n    m, f = build_train(repeat_number = i, folds=FOLDS)\n    models = models + m\n    preds_df.append(f)","73741bd1":"mean_oof_preds = y_train.copy()\nmean_oof_preds.loc[:, target_cols] = 0\nfor i, p in enumerate(preds_df):\n    print(f\"Iterate{i + 1} Log Loss: {multi_log_loss(y_train, p)}\")\n    mean_oof_preds.loc[:, target_cols] += p[target_cols]\n\nmean_oof_preds.loc[:, target_cols] \/= len(preds_df)\nprint(f\"Mean Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")\nmean_oof_preds.loc[x_train['cp_type'] == 0, target_cols] = 0\nprint(f\"Mean Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")","80c34dd7":"test_preds = sample_submission.copy()\ntest_preds[target_cols] = 0\nfor model in models:\n    test_preds.loc[:,target_cols] += model.predict(x_test)\ntest_preds.loc[:,target_cols] \/= len(models)\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv('submission.csv', index=False)","9fe11fd6":"Model Prediction","9e9e0b64":"Lets see cp_type distribution","b4c9cbf2":"Correlation representation among features of 'g-' and 'c-'","3431e371":"> sample_submission=pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n> train_targets_scored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n> train_targets_nonscored=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\n> train_features=pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\n> test_features=pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","386eb446":"We can see that data is highly imbalanced","dfa755c4":"Visualizing training features vectors\/values"}}