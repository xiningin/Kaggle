{"cell_type":{"52ff443b":"code","2967ca5e":"code","238ec001":"code","1babe475":"code","34eca485":"code","64328cf5":"code","9d20dd8a":"code","c2cde157":"code","d3652a77":"code","073414c5":"code","d2782589":"code","105ef325":"code","27bf3117":"code","58e730e1":"code","6eb9d9fb":"code","437ddee1":"markdown","85430a20":"markdown","3e3bb06b":"markdown","9c3f75b0":"markdown","1a27de32":"markdown","cb91990f":"markdown","e64f2aff":"markdown","f1cb9d74":"markdown","faa59aa5":"markdown","71ac4161":"markdown"},"source":{"52ff443b":"import numpy as np\nimport cv2\nfrom os import listdir\nimport tensorflow as tf\nimport pickle\nfrom sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers.normalization import BatchNormalization\nfrom tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom tensorflow.python.keras.layers.core import Activation, Flatten, Dropout, Dense, Reshape\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.preprocessing.image import img_to_array\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing import image\nfrom tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.python.keras import regularizers\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","2967ca5e":"# Initialize hyperparameters.\nEPOCHS = 15\nINIT_LR = 1e-4\nDECAY = 1e-6\nBS = 32\ndefault_image_size = tuple((256, 256))\nresized_image_size = tuple((224,224))\nimage_size = 0\ndirectory_root = '..\/input\/plantvillage\/'","238ec001":"# Function to convert image to array.\ndef convert_image_to_array(image_dir):\n    try:\n        image = cv2.imread(image_dir)\n        if image is not None :\n            image = cv2.resize(image, resized_image_size)   \n            return img_to_array(image)\n        else :\n            return np.array([])\n    except Exception as e:\n        print(f\"Error : {e}\")\n        return None","1babe475":"# Reading the images from the dataset folder.\nimage_list, label_list = [], []\ntry:\n    print(\"[INFO] Loading images ...\")\n    root_dir = listdir(directory_root)\n    for directory in root_dir :\n        # remove .DS_Store from list\n        if directory == \".DS_Store\" :\n            root_dir.remove(directory)\n\n    for plant_folder in root_dir :\n        plant_disease_folder_list = listdir(f\"{directory_root}\/{plant_folder}\")\n        \n        for disease_folder in plant_disease_folder_list :\n            # remove .DS_Store from list\n            if disease_folder == \".DS_Store\" :\n                plant_disease_folder_list.remove(disease_folder)\n\n        for plant_disease_folder in plant_disease_folder_list:\n            print(f\"[INFO] Processing {plant_disease_folder} ...\")\n            plant_disease_image_list = listdir(f\"{directory_root}\/{plant_folder}\/{plant_disease_folder}\/\")\n                \n            for single_plant_disease_image in plant_disease_image_list :\n                if single_plant_disease_image == \".DS_Store\" :\n                    plant_disease_image_list.remove(single_plant_disease_image)\n\n            for image in plant_disease_image_list[:200]:\n                image_directory = f\"{directory_root}\/{plant_folder}\/{plant_disease_folder}\/{image}\"\n                if image_directory.endswith(\".jpg\") == True or image_directory.endswith(\".JPG\") == True:\n                    image_list.append(convert_image_to_array(image_directory))\n                    label_list.append(plant_disease_folder)\n    print(\"[INFO] Image loading completed!\")  \nexcept Exception as e:\n    print(f\"Error : {e}\")","34eca485":"image_size = len(image_list)","64328cf5":"# Assign the classes to images.\nlabel_binarizer = LabelBinarizer()\nimage_labels = label_binarizer.fit_transform(label_list)\nn_classes = len(label_binarizer.classes_)","9d20dd8a":"print(label_binarizer.classes_)","c2cde157":"# Convert the array to a NumPy list and normalise it.\nnp_image_list = np.array(image_list, dtype = np.float16) \/ 223.0","d3652a77":"print(\"[INFO] Spliting data into train set and test set ...\")\nx_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size = 0.2, random_state = 42) \nprint(\"[INFO] Spliting complete!\")","073414c5":"aug = ImageDataGenerator(\n    rotation_range = 20,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    shear_range = 0.2, \n    zoom_range = 0.2,\n    horizontal_flip = True, \n    fill_mode = \"nearest\")","d2782589":"# Initialize the model.\nmodel = Sequential()\n          \n# 1st Convolutional Layer\nmodel.add(Conv2D(filters = 96, input_shape = (224,224,3), kernel_size = (11,11), strides = (4,4), padding = 'valid'))\nmodel.add(Activation('relu'))\n# Batch Normalisation before passing it to the next layer\nmodel.add(BatchNormalization())\n# Pooling Layer\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n\n# 2nd Convolutional Layer\nmodel.add(Conv2D(filters = 256, kernel_size = (5,5), strides = (1,1), padding = 'same'))\nmodel.add(Activation('relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n# Pooling Layer\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n\n# 3rd Convolutional Layer\nmodel.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(Activation('relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n# Dropout\nmodel.add(Dropout(0.5))\n\n# 4th Convolutional Layer\nmodel.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(Activation('relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n# Dropout\nmodel.add(Dropout(0.5))\n\n# 5th Convolutional Layer\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3), strides = (1,1), padding = 'same'))\nmodel.add(Activation('relu'))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n# Pooling Layer\nmodel.add(MaxPooling2D(pool_size = (3,3), strides = (2,2), padding = 'valid'))\n# Dropout\nmodel.add(Dropout(0.5))\n\n# Passing it to a dense layer\nmodel.add(Flatten())\n\n# 1st Dense Layer\nmodel.add(Dense(4096, input_shape = (224*224*3,)))\nmodel.add(Activation('relu'))\n# Add Dropout to prevent overfitting\nmodel.add(Dropout(0.25))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 2nd Dense Layer\nmodel.add(Dense(4096))\nmodel.add(Activation('relu'))\n# Add Dropout\nmodel.add(Dropout(0.5))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# 3rd Dense Layer\nmodel.add(Dense(1000))\nmodel.add(Activation('relu'))\n# Add Dropout\nmodel.add(Dropout(0.5))\n# Batch Normalisation\nmodel.add(BatchNormalization())\n\n# Output Layer\nmodel.add(Dense(n_classes))\nmodel.add(Activation('softmax'))\n\n# Get the model summary.\nmodel.summary()","105ef325":"# Compile \nopt = tf.keras.optimizers.Adam(lr = INIT_LR, decay = DECAY)\nmodel.compile(loss=\"binary_crossentropy\", optimizer = opt,metrics = [\"accuracy\"])\nprint(\"[INFO] Training network...\")\n\n# Train\ncheckpoint = ModelCheckpoint(\"AlexNet.h5\", monitor = 'accuracy', verbose = 1, save_best_only = True, save_weights_only = False, mode = 'auto', period = 1)\n\nhistory = model.fit_generator(\n    aug.flow(x_train, y_train, batch_size = BS, shuffle = False),\n    validation_data = (x_test, y_test),\n    steps_per_epoch = len(x_train) \/\/ BS,\n    callbacks = [checkpoint],\n    epochs = EPOCHS,\n    verbose=1 )","27bf3117":"import matplotlib.pyplot as plt\nxmin = 0\nxmax = 15\nymin = 0.0\nymax = 1.0\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\naxes = plt.gca()\naxes.set_xlim([xmin,xmax])\naxes.set_ylim([ymin,ymax])\n\nplt.plot(epochs, acc, 'r', label = 'Training accuracy')\nplt.plot(epochs, val_acc, 'b', label = 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label = 'Training Loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","58e730e1":"print(\"[INFO] Calculating model accuracy\")\nscores = model.evaluate(x_test, y_test)\nprint(f\"Test Accuracy: {scores[1]*100}\")","6eb9d9fb":"print(\"[INFO] Saving model...\")\nmodel.save(\"Model_AlexNet.h5\")\nprint(\"[INFO] Saved model to disk!\")","437ddee1":"## Save the model to disk.","85430a20":"# AlexNet","3e3bb06b":"## Load the dataset for use.","9c3f75b0":"## Build an AlexNet.","1a27de32":"## Initialize a Image Data Generator object.","cb91990f":"## Compile the model.","e64f2aff":"## Split the NumPy list into test and train set.","f1cb9d74":"## Calculate model accuracy.","faa59aa5":"## Import the required libraries and dependencies.","71ac4161":"## Plot the accuracy and loss curves.\nThis is done to identify if the model is performing well or is overfitting\/underfitting the dataset."}}