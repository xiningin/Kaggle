{"cell_type":{"787e90f3":"code","e1c966e1":"code","538066ae":"code","5adcc8fa":"code","f6b99e8c":"code","9fca8c0d":"code","40e1cd24":"code","a83a1126":"code","05e8f981":"code","c500d9d4":"code","878a4aeb":"code","b7c96ec5":"code","290a251a":"code","42fc77c5":"code","acf472e1":"code","f4f78e9f":"code","15cdd9ef":"code","afe62e0b":"code","9bd56e10":"code","078897d6":"code","8e08f004":"code","27ea0da6":"code","ffec0b1d":"code","f1ed2eb6":"code","c6f04b9c":"code","4bcba5cf":"code","8fa90b46":"code","0f9e3a18":"code","227a8416":"markdown","66c14e10":"markdown","673a8c49":"markdown","b82ee8a0":"markdown"},"source":{"787e90f3":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport cv2\nfrom tqdm import tqdm\nimport time\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array","e1c966e1":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","538066ae":"train_dir='..\/input\/animal-dataset\/animal_dataset_intermediate\/train\/'\ntest_dir='..\/input\/animal-dataset\/animal_dataset_intermediate\/test'","5adcc8fa":"Name=[]\nfor file in os.listdir(train_dir):\n    Name+=[file]\nprint(Name)\nprint(len(Name))","f6b99e8c":"N=[]\nfor i in range(len(Name)):\n    N+=[i]\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","9fca8c0d":"trainx0=[]\ntrainy0=[]\ntrainpaths=[]\ncount=0\nfor file in Name:\n    path=os.path.join(train_dir,file)\n    for im in tqdm(os.listdir(path)):\n        if im[-4:]!='.txt':\n            trainpaths.append(os.path.join(path,im))    \n            image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=(48,48))\n            image=img_to_array(image)\n            image=image\/255.0\n            trainx0.append(image)\n            trainy0.append(count)\n    count=count+1","40e1cd24":"testx0=[]\ntestpaths=[]\nfor im in tqdm(os.listdir(test_dir)):\n    if im[-4:]!='.txt':\n        testpaths.append(os.path.join(test_dir,im))    \n        image=load_img(os.path.join(test_dir,im), grayscale=False, color_mode='rgb', target_size=(48,48))\n        image=img_to_array(image)\n        image=image\/255.0\n        testx0.append(image)","a83a1126":"num0=random.sample(range(len(trainpaths)),k=9)\nprint(num0)","05e8f981":"fig,axs = plt.subplots(3,3,figsize=(14,14))\nfor i in range(9):\n    image_input = cv2.imread(trainpaths[num0[i]])\n    r=i\/\/3\n    c=i%3\n    axs[r][c].set_xticks([])\n    axs[r][c].set_yticks([])\n    axs[r][c].set_title(reverse_mapping[trainy0[num0[i]]])\n    ax=axs[r][c].imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()","c500d9d4":"labels = open('..\/input\/yolo-coco-data\/coco.names').read().strip().split('\\n')\nprint(labels)","878a4aeb":"weights_path = '..\/input\/yolo-coco-data\/yolov3.weights'\nconfiguration_path = '..\/input\/yolo-coco-data\/yolov3.cfg'\nprobability_minimum = 0.5\nthreshold = 0.3","b7c96ec5":"network = cv2.dnn.readNetFromDarknet(configuration_path, weights_path)\nlayers_names_all = network.getLayerNames()\nlayers_names_output = [layers_names_all[i[0]-1] for i in network.getUnconnectedOutLayers()]","290a251a":"image_input = cv2.imread(trainpaths[6000])\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8,8)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()","42fc77c5":"blob = cv2.dnn.blobFromImage(image_input, 1\/255.0, (416,416), swapRB=True, crop=False)\nblob_to_show = blob[0,:,:,:].transpose(1,2,0)\nnetwork.setInput(blob)\noutput_from_network = network.forward(layers_names_output)\nnp.random.seed(42)\ncolours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')","acf472e1":"bounding_boxes = []\nconfidences = []\nclass_numbers = []\nh,w = image_input.shape[:2]\n\nfor result in output_from_network:\n    for detection in result:\n        scores = detection[5:]\n        class_current = np.argmax(scores)\n        confidence_current = scores[class_current]\n        if confidence_current > probability_minimum:\n            box_current = detection[0:4] * np.array([w, h, w, h])\n            x_center, y_center, box_width, box_height = box_current.astype('int')\n            x_min = int(x_center-(box_width\/2))\n            y_min = int(y_center-(box_height\/2))\n            bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n            confidences.append(float(confidence_current))\n            class_numbers.append(class_current)\n\n#print(class_numbers[-1])                  \n#print(labels[class_numbers[-1]])            ","f4f78e9f":"results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n\nif len(results) > 0:\n    for i in results.flatten():\n        x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n        box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n        colour_box_current = [int(j) for j in colours[class_numbers[i]]]\n        cv2.rectangle(image_input, (x_min, y_min), (x_min + box_width, y_min + box_height),\n                      colour_box_current, 5)\n        text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])], confidences[i])\n        cv2.putText(image_input, text_box_current, (x_min, y_min - 7), cv2.FONT_HERSHEY_SIMPLEX,\n                    1.5, colour_box_current, 5)","15cdd9ef":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (8,8)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()","afe62e0b":"def ImagePath(path):\n    \n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n    \n    image_input = cv2.imread(path)\n    blob = cv2.dnn.blobFromImage(image_input, 1\/255.0, (416,416), swapRB=True, crop=False)\n    blob_to_show = blob[0,:,:,:].transpose(1,2,0)\n    network.setInput(blob)\n    output_from_network = network.forward(layers_names_output)\n    h,w = image_input.shape[:2]\n\n    for result in output_from_network:\n        for detection in result:\n            scores = detection[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                box_current = detection[0:4] * np.array([w, h, w, h])\n                x_center, y_center, box_width, box_height = box_current.astype('int')\n                x_min = int(x_center-(box_width\/2))\n                y_min = int(y_center-(box_height\/2))\n                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n                confidences.append(float(confidence_current))\n                class_numbers.append(class_current)\n\n    %matplotlib inline\n    plt.rcParams['figure.figsize'] = (8,8)\n    plt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\n    plt.show()\n    \n    labels2=[]\n    for item in sorted(set(class_numbers)):\n        labels2+=[labels[item]]\n        \n    return labels2","9bd56e10":"ImagePath(trainpaths[6000])","078897d6":"tobjects=[]\n\ndef ExtractImage(path, show=False):\n    \n    bounding_boxes = []\n    confidences = []\n    class_numbers = []\n\n    image_input = cv2.imread(path)\n    blob = cv2.dnn.blobFromImage(image_input, 1\/255.0, (416,416), swapRB=True, crop=False)\n    blob_to_show = blob[0,:,:,:].transpose(1,2,0)\n    network.setInput(blob)\n    output_from_network = network.forward(layers_names_output)\n    h,w = image_input.shape[:2]\n\n    for result in output_from_network:\n        for detection in result:\n            scores = detection[5:]\n            class_current = np.argmax(scores)\n            confidence_current = scores[class_current]\n            if confidence_current > probability_minimum:\n                box_current = detection[0:4] * np.array([w, h, w, h])\n                x_center, y_center, box_width, box_height = box_current.astype('int')\n                x_min = int(x_center-(box_width\/2))\n                y_min = int(y_center-(box_height\/2))\n                \n                if x_min<0:\n                    x_min=0\n                if y_min<0:\n                    y_min=0\n                \n                obj=image_input[int(y_min):int(y_min+box_height),int(x_min):int(x_min+box_width)]\n              \n                obj1 = Image.fromarray(np.uint8(obj))\n                obj2 = np.asarray(obj1.resize((64,64)))\/255.0 \n                tobjects.append(obj2)\n\n                if show:\n                    _ = plt.figure(figsize=(3,3))\n                    _ = plt.xticks([])\n                    _ = plt.yticks([])\n                    _ = plt.imshow(cv2.cvtColor(obj, cv2.COLOR_BGR2RGB))\n\n    return None","8e08f004":"ExtractImage(trainpaths[6000],show=True)","27ea0da6":"for i in tqdm(range(len(trainpaths))):\n    ExtractImage(trainpaths[i],show=False)","ffec0b1d":"trainobjects_ar=np.array(tobjects)\nprint(trainobjects_ar.shape)\nnp.save('trainobjects',trainobjects_ar)\ndel trainobjects_ar","f1ed2eb6":"for i in tqdm(range(len(testpaths))):\n    ExtractImage(testpaths[i],show=False)","c6f04b9c":"testobjects_ar=np.array(tobjects)\nprint(testobjects_ar.shape)\nnp.save('testobjects',testobjects_ar)","4bcba5cf":"# loaded_ar=np.load('tobjects.npy')\n# print(loaded_ar.shape)","8fa90b46":"num1=random.sample(range(len(testobjects_ar)),k=9)\nprint(num1)","0f9e3a18":"fig,axs = plt.subplots(3,3,figsize=(14,14))\nfor i in range(9):\n    r=i\/\/3\n    c=i%3\n    axs[r][c].set_xticks([])\n    axs[r][c].set_yticks([])\n    ax=axs[r][c].imshow(cv2.cvtColor(testobjects_ar[num1[i]].astype('float32'),cv2.COLOR_BGR2RGB))\nplt.show()","227a8416":"# Output detected object","66c14e10":"# Set rectangle at detected area","673a8c49":"# Extract rectangle image","b82ee8a0":"# Show images"}}