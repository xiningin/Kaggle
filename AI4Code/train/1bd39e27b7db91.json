{"cell_type":{"266eb2df":"code","bd762d52":"code","c40bc47c":"code","cfecdbd8":"code","396f4309":"code","b147fa58":"code","7d55d129":"code","0c83e6f5":"code","fd3fa65a":"code","7ab9f63b":"code","103a3bea":"markdown","4a397796":"markdown","20d48d26":"markdown","683e9425":"markdown","e5715170":"markdown","400d05bc":"markdown","0d497f18":"markdown","d0d97ef4":"markdown","cf16a1c1":"markdown","fcbb579b":"markdown","2188f996":"markdown","583dbff6":"markdown","553a9051":"markdown","bc63495d":"markdown"},"source":{"266eb2df":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport os\n%matplotlib inline\nprint(os.listdir(\"..\/input\"))\n\ninputData = pd.read_csv(r\"..\/input\/diabetes.csv\");","bd762d52":"print(inputData.dtypes)\nprint(inputData.columns)\nprint(\"Data shape:\",inputData.shape)\nprint(inputData.head())\nprint(inputData.describe())\nprint(inputData.info())","c40bc47c":"print(inputData.isnull().sum())","cfecdbd8":"\nprint (\"***************************************\")\nprint (\"VISUALIZATIONS IN DATA SET\")\nprint (\"***************************************\")\n\nfig = plt.figure(figsize = (10,10))\nax = fig.gca()\nsns.heatmap(inputData.corr(), annot=True, fmt=\".2f\")\nplt.title(\"Correlation\",fontsize=5)\nplt.show()\n\nfig = plt.figure(figsize = (10,10))\nax = fig.gca()\nsns.scatterplot(x=\"Age\", y=\"Glucose\", hue=\"Outcome\",data=inputData)\nplt.title(\"Age vs Glucose\",fontsize =10)\nplt.show()\n\n\nsns.pairplot(data=inputData,hue=\"Outcome\")\nplt.title(\"Skewness\",fontsize =10)\nplt.show()\n\nfig = plt.figure(figsize=(15,15))\nax = fig.gca()\ninputData['Age'].value_counts().sort_values(ascending=False).plot.bar(width=0.5,edgecolor='k',align='center',linewidth=1)\nplt.xlabel('Age',fontsize=10)\nplt.ylabel('Counts',fontsize=10)\nax.tick_params(labelsize=10)\nplt.title('Age records',fontsize=10)\nplt.grid()\nplt.ioff()\n\n\nplt.figure( figsize=(10,10))\ninputData['Outcome'].value_counts().plot.pie(autopct=\"%1.1f%%\")\nplt.title(\"Data division on Outcome of diabetes\",fontsize=10)\nplt.show()\n\nlength  = len(inputData.columns[:-1])\ncolors  = [\"r\",\"g\",\"b\",\"m\",\"y\",\"c\",\"k\",\"orange\"] \n\nprint (\"***************************************\")\nprint (\"DISTIBUTION OF VARIABLES IN DATA SET\")\nprint (\"***************************************\")\nplt.figure(figsize=(13,20))\n# Leavout the last column of Outcome\nfor i,j,k in itertools.zip_longest(inputData.columns[:-1],range(length),colors):\n    plt.subplot(length\/2,length\/4,j+1)\n    sns.distplot(inputData[i],color=k)\n    plt.title(i)\n    plt.subplots_adjust(hspace = .3)\n    plt.axvline(inputData[i].mean(),color = \"k\",linestyle=\"dashed\",label=\"MEAN\")\n    plt.axvline(inputData[i].std(),color = \"b\",linestyle=\"dotted\",label=\"STANDARD DEVIATION\")\n    plt.legend(loc=\"upper right\")\nplt.show()    \n\n","396f4309":"print (\"***************************************\")\nprint (\"MODEL FUNCTION\")\nprint (\"***************************************\")\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,roc_curve,auc\n\ndef model(algorithm,dtrain_x,dtrain_y,dtest_x,dtest_y,of_type):\n    \n    print (\"*****************************************************************************************\")\n    print (\"MODEL - OUTPUT\")\n    print (\"*****************************************************************************************\")\n    algorithm.fit(dtrain_x,dtrain_y)\n    predictions = algorithm.predict(dtest_x)\n    \n    print (algorithm)\n    print (\"\\naccuracy_score :\",accuracy_score(dtest_y,predictions))\n    \n    print (\"\\nclassification report :\\n\",(classification_report(dtest_y,predictions)))\n        \n    plt.figure(figsize=(13,10))\n    plt.subplot(221)\n    sns.heatmap(confusion_matrix(dtest_y,predictions),annot=True,fmt = \"d\",linecolor=\"k\",linewidths=3)\n    plt.title(\"CONFUSION MATRIX\",fontsize=20)\n    predicting_probabilites = algorithm.predict_proba(dtest_x)[:,1]\n    fpr,tpr,thresholds = roc_curve(dtest_y,predicting_probabilites)\n    plt.subplot(222)\n    plt.plot(fpr,tpr,label = (\"Area_under the curve :\",auc(fpr,tpr)),color = \"r\")\n    plt.plot([1,0],[1,0],linestyle = \"dashed\",color =\"k\")\n    plt.legend(loc = \"best\")\n    plt.title(\"ROC - CURVE & AREA UNDER CURVE\",fontsize=20)\n    if  of_type == \"feat\":\n        \n        dataframe = pd.DataFrame(algorithm.feature_importances_,dtrain_x.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\",0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\",ascending = False)\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\" ,y =\"features\",data=dataframe,palette=\"husl\")\n        plt.title(\"FEATURE IMPORTANCES\",fontsize =20)\n        for i,j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011,i,j,weight = \"bold\")\n        plt.show()\n    elif of_type == \"coef\" :\n        \n        dataframe = pd.DataFrame(algorithm.coef_.ravel(),dtrain_x.columns).reset_index()\n        dataframe = dataframe.rename(columns={\"index\":\"features\",0:\"coefficients\"})\n        dataframe = dataframe.sort_values(by=\"coefficients\",ascending = False)\n        plt.subplot(223)\n        ax = sns.barplot(x = \"coefficients\" ,y =\"features\",data=dataframe,palette=\"husl\")\n        plt.title(\"FEATURE IMPORTANCES\",fontsize =20)\n        for i,j in enumerate(dataframe[\"coefficients\"]):\n            ax.text(.011,i,j,weight = \"bold\")\n        plt.show()    \n    elif of_type == \"none\" :\n        plt.show()\n        return (algorithm)\n\ndef run_classifiers(train_X,train_Y,test_X,test_Y):\n    print (\"***************************************\")\n    print (\"RANDOM FOREST CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.ensemble import RandomForestClassifier\n    rf =RandomForestClassifier()\n    model(rf,train_X,train_Y,test_X,test_Y,\"feat\")\n    \n    print (\"***************************************\")\n    print (\"EXTRA TREE CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.tree import ExtraTreeClassifier\n    etc = ExtraTreeClassifier()\n    model(etc,train_X,train_Y,test_X,test_Y,\"feat\")\n    \n    print (\"***************************************\")\n    print (\"GRADIENT BOOST CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.ensemble import GradientBoostingClassifier\n    gbc = GradientBoostingClassifier()\n    model(gbc,train_X,train_Y,test_X,test_Y,\"feat\")\n    \n    print (\"***************************************\")\n    print (\"GAUSSIAN NAIVES BAYES CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.naive_bayes import GaussianNB\n    nb = GaussianNB()\n    model(nb,train_X,train_Y,test_X,test_Y,\"none\")\n    \n    print (\"***************************************\")\n    print (\"K-NEAREST NEIGHBOUT CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier()\n    model(knn,train_X,train_Y,test_X,test_Y,\"none\")\n    \n    print (\"***************************************\")\n    print (\"ADA-BOOST CLASSIFIER\")\n    print (\"***************************************\")\n    \n    from sklearn.ensemble import AdaBoostClassifier\n    ada = AdaBoostClassifier()\n    model(ada,train_X,train_Y,test_X,test_Y,\"feat\")","b147fa58":"from sklearn.model_selection import train_test_split\nsplitRatio = 0.2\ntrain , test = train_test_split(inputData,test_size = splitRatio,random_state = 123)\n\nplt.figure(figsize=(12,6))\nplt.subplot(121)\ntrain[\"Outcome\"].value_counts().plot.pie(labels = [\"1\",\"0\"],\n                                              autopct = \"%1.0f%%\",\n                                              shadow = True,explode=[0,.1])\nplt.title(\"proportion of target class in train data\")\nplt.ylabel(\"\")\nplt.subplot(122)\ntest[\"Outcome\"].value_counts().plot.pie(labels = [\"1\",\"0\"],\n                                             autopct = \"%1.0f%%\",\n                                             shadow = True,explode=[0,.1])\nplt.title(\"proportion of target class in test data\")\nplt.ylabel(\"\")\nplt.show()\n","7d55d129":"print (\"************************\")\nprint (\"RAW DATA AND  PREDICTION\")\nprint (\"************************\")\n#Seperating Predictor and target variables\ntrain_X = train[[x for x in train.columns if x not in [\"Outcome\"]]]\ntrain_Y = train[[\"Outcome\"]]\ntest_X  = test[[x for x in test.columns if x not in [\"Outcome\"]]]\ntest_Y  = test[[\"Outcome\"]]\n\nrun_classifiers(train_X,train_Y,test_X,test_Y)","0c83e6f5":"print (\"******************************************************\")\nprint (\"ROWISE UNIT NORM NORMALIZATION OF DATA AND  PREDICTION\")\nprint (\"******************************************************\")\noriginal = inputData.copy();\nfrom sklearn import preprocessing\nv = inputData.loc[:, inputData.columns != 'Outcome']\nv = preprocessing.normalize(v, norm='l2',axis =1)\ninputData.loc[:, inputData.columns != 'Outcome'] = v;\ntrain , test = train_test_split(inputData,test_size = splitRatio,random_state = 123)\n#Seperating Predictor and target variables\ntrain_X = train[[x for x in train.columns if x not in [\"Outcome\"]]]\ntrain_Y = train[[\"Outcome\"]]\ntest_X  = test[[x for x in test.columns if x not in [\"Outcome\"]]]\ntest_Y  = test[[\"Outcome\"]]\nrun_classifiers(train_X,train_Y,test_X,test_Y)","fd3fa65a":"print (\"********************************************\")\nprint (\"MINMAX NORMALIZATION OF DATA AND  PREDICTION\")\nprint (\"********************************************\")\ninputData = original.copy()\nv = inputData.loc[:, inputData.columns != 'Outcome']\nv = v.apply(lambda x:(x.astype(float) - min(x))\/(max(x)-min(x)), axis = 1)\ninputData.loc[:, inputData.columns != 'Outcome'] = v;\ntrain , test = train_test_split(inputData,test_size = splitRatio,random_state = 123)\n#Seperating Predictor and target variables\ntrain_X = train[[x for x in train.columns if x not in [\"Outcome\"]]]\ntrain_Y = train[[\"Outcome\"]]\ntest_X  = test[[x for x in test.columns if x not in [\"Outcome\"]]]\ntest_Y  = test[[\"Outcome\"]]\nrun_classifiers(train_X,train_Y,test_X,test_Y)\n","7ab9f63b":"\nprint (\"***************************\")\nprint (\"PCA OF DATA AND  PREDICTION\")\nprint (\"***************************\")\nfrom sklearn.decomposition import PCA\ninputData = original.copy()\nn_components = 2;\npca = PCA(n_components = n_components).fit(inputData.loc[:, inputData.columns != 'Outcome'])\npcaTransformedData = pca.transform(inputData.loc[:, inputData.columns != 'Outcome'])\npcaDictionary = {}\n# Create the dictionarry\nfor i in range(n_components):\n    key = \"PCA_component_\"+str(i)\n    value = pcaTransformedData[:,i]\n    pcaDictionary[key] = value\np = pd.DataFrame.from_dict(pcaDictionary)\np[\"Outcome\"] = inputData.Outcome\ntrain , test = train_test_split(p,test_size = splitRatio,random_state = 123)\n#Seperating Predictor and target variables\ntrain_X = train[[x for x in train.columns if x not in [\"Outcome\"]]]\ntrain_Y = train[[\"Outcome\"]]\ntest_X  = test[[x for x in test.columns if x not in [\"Outcome\"]]]\ntest_Y  = test[[\"Outcome\"]]\nrun_classifiers(train_X,train_Y,test_X,test_Y)\n\n\n","103a3bea":"# Raw Data classification","4a397796":"Predicting diabetics is performed for the following\n* Raw data\n* Normalized data\n* PCA data\n","20d48d26":"#  PCA reduction","683e9425":"# Predictions on Data","e5715170":"# Classifier Models","400d05bc":"# Check for nulls","0d497f18":"# Normalization - MinMax ","d0d97ef4":"# Analysis of data","cf16a1c1":"# Predicting diabetics using supervised machine leanring on three aspects of the data\n## Raw, Normalized and PCA'd data","fcbb579b":"# Acknowledgement","2188f996":"# Prediction\n## Data splits","583dbff6":"# Visualizations","553a9051":"# Normalization - Unit norm","bc63495d":"Would like to thank [Pavan Raj](\/https:\/\/www.kaggle.com\/pavanraj159\/predicting-pulsar-star-in-the-universe) for providing a nifty code to peform predictions and visualization ROC's and Confusion matrices and feature importance plots.  Plus I have used some visualizations as well from the code."}}