{"cell_type":{"bdb02bcd":"code","b09a3d3d":"code","4b56fc44":"code","aa876fb6":"code","5ed3de2a":"code","c4080c03":"code","aae3772f":"code","794c27d4":"code","d3d5fa1f":"code","a4bd6ca3":"code","fc596ad8":"code","3571b603":"code","82fa09be":"code","0e6adfe2":"code","3ce45fa8":"code","903f4471":"code","36677544":"code","40dd7c6f":"code","4a8b7e16":"code","e29f4be4":"code","cc44de65":"code","1fef139c":"code","e099c7ea":"code","39b1be0f":"code","e6a80407":"code","9eba483c":"code","26a5fca6":"code","718a5321":"code","624a8199":"code","6aebed52":"code","4fb2a956":"code","917ab69e":"code","77bb9e32":"code","6aed3ff2":"code","8d1e4b85":"code","938d2b53":"code","aa3fc92f":"code","7e3add12":"code","c2f1409a":"code","f7f2fed0":"code","cb273037":"code","a7045fbd":"code","51a29dcb":"code","13b885d7":"code","2716f71f":"code","1f21f373":"code","e15655cc":"code","54e716e5":"code","16fe9446":"code","cd20a8cb":"code","e718adfc":"code","5cc2f548":"code","39dba144":"code","60c03802":"code","e1e26ced":"code","540b21d7":"code","8ddebacc":"code","fcb1c83c":"code","783c4757":"code","48ce38fc":"code","9e169c26":"code","f1149344":"code","d5a45eb5":"code","4089a60b":"code","b2b4620f":"code","a56f9ef9":"code","da4094fa":"code","cc0ba9d2":"code","b4a71c62":"code","914152e4":"code","5659c758":"code","25218758":"code","a5621ff6":"code","5121105d":"code","eae91d07":"code","abc77a7c":"code","f0433475":"code","f69416a9":"code","1d9070fe":"code","3fd70df5":"code","c41d18f8":"code","fdadc493":"code","48583196":"code","42a0f952":"code","07b3a488":"code","d144b671":"code","ba9ae441":"code","29b1f1fe":"code","acc930d4":"code","2f6a11bc":"code","5fda4afe":"code","28497ab7":"code","13ca153c":"code","2bc2a851":"code","f332e0a1":"code","d748b053":"code","8eff31df":"code","ec2532cf":"code","a8b3028b":"code","85d39d86":"code","50b276f6":"code","4920e290":"code","f4fa060f":"code","4b80c3be":"code","d13c3443":"code","0591108c":"code","022f947a":"code","01316e4b":"code","2a509d7b":"code","1307251c":"code","efbdd6ea":"code","3d709c9d":"code","6c8c9bfb":"code","171f027c":"code","ddb4bd9a":"code","f0adc088":"code","e8b2c74e":"code","ec5d5fe4":"code","8daf0935":"code","286eea67":"code","f5dd503d":"code","695cab30":"code","965f4b24":"code","e909c550":"code","18b16d01":"code","b81dfa62":"code","a94efc8f":"code","7e34e42b":"code","2d92f55e":"code","7fcb44ed":"code","817b66af":"code","12077ee9":"markdown","24cc872e":"markdown","e9389039":"markdown","0af5f44c":"markdown","5b571031":"markdown","a3283ece":"markdown","39b0e3d7":"markdown","a4abdfce":"markdown","7059b134":"markdown","c25d2e61":"markdown","445d9a48":"markdown","310c96a0":"markdown","9338bb2c":"markdown","f20b2571":"markdown","1982528f":"markdown","4b53d3bf":"markdown","7ac5a853":"markdown","d2da641c":"markdown","bc0d3ff6":"markdown","49ea7118":"markdown","a32f12dc":"markdown","ace07207":"markdown","d1deaa2c":"markdown","82ab59c1":"markdown","78ef581e":"markdown","d5816c4d":"markdown","423938a0":"markdown","a3e55ca8":"markdown","269624ad":"markdown","f1e1dbe1":"markdown","69c8d807":"markdown","d9b36660":"markdown","d4bdc922":"markdown","578b6d57":"markdown","a7a6e3f0":"markdown","f7618a8c":"markdown","135e4927":"markdown","418e2558":"markdown","a7756b5b":"markdown"},"source":{"bdb02bcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b09a3d3d":"# basic function of python\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sb\n%matplotlib inline\nimport numpy as np\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nimport xlrd\nfrom scipy import stats\nfrom datetime import datetime\n\n# feature hashing\nfrom sklearn.feature_extraction import FeatureHasher\n\n# target encoder\nimport category_encoders as ce\n\n# feature selection\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import feature_selection\n\n# oversampling\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\n# building the models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nimport tensorflow\n# from tensorflow.contrib.keras import models, layers\n# from tensorflow.contrib.keras import activations, optimizers, losses\n\n# standardize the vaiable\nfrom sklearn.preprocessing import StandardScaler\n\n# from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# validation\nfrom sklearn.metrics import confusion_matrix,classification_report","4b56fc44":"train = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\nsubmission = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')","aa876fb6":"train.head()","5ed3de2a":"test.head()","c4080c03":"submission.head()","aae3772f":"train.drop(['id'],axis=1,inplace=True)","794c27d4":"test.drop(['id'],axis=1,inplace=True)","d3d5fa1f":"train['target'].value_counts()","a4bd6ca3":"train.dtypes","fc596ad8":"train.shape","3571b603":"test.shape","82fa09be":"train.head()","0e6adfe2":"test.head()","3ce45fa8":"train.isnull() # Checking missing values","903f4471":"train.isnull().sum() # check the missing values","36677544":"sb.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","40dd7c6f":"missing_data = train.isnull()\nmissing_data.head(5)","4a8b7e16":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","e29f4be4":"test.isnull() # Checking missing values","cc44de65":"test.isnull().sum() # check the missing values","1fef139c":"sb.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","e099c7ea":"missing_data = test.isnull()\nmissing_data.head(5)","39b1be0f":"for column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","e6a80407":"train.head()","9eba483c":"train.dtypes","26a5fca6":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features=8, input_type='string')\nsp = fh.fit_transform(train['ord_5'])\ndf = pd.DataFrame(sp.toarray(), columns=['fh1', 'fh2', 'fh3', 'fh4', 'fh5', 'fh6', 'fh7', 'fh8'])\npd.concat([train, df], axis=1)\ntrain.drop('ord_5',axis=1,inplace=True)\ntrain","718a5321":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features=8, input_type='string')\nsp = fh.fit_transform(test['ord_5'])\ndf = pd.DataFrame(sp.toarray(), columns=['fh1', 'fh2', 'fh3', 'fh4', 'fh5', 'fh6', 'fh7', 'fh8'])\npd.concat([test, df], axis=1)\ntest.drop('ord_5',axis=1,inplace=True)\ntest","624a8199":"train = pd.get_dummies(train, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4','ord_3', 'ord_4'],drop_first=True, sparse=True)","6aebed52":"train.shape","4fb2a956":"test = pd.get_dummies(test, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4','ord_3', 'ord_4'],drop_first=True, sparse=True)","917ab69e":"test.shape","77bb9e32":"cols_ = ['nom_5','nom_6','nom_7','nom_8','nom_9']\nce_target_encoder = ce.TargetEncoder(cols = cols_, smoothing=0.50)\nce_target_encoder.fit(train[cols_], train['target'])\ntrain_nom = ce_target_encoder.transform(train[cols_])\ntrain.drop(['nom_5','nom_6','nom_7','nom_8','nom_9'],axis=1,inplace=True)\ntrain = pd.concat([train, train_nom], axis=1)\ntrain","6aed3ff2":"ce_target_encoder = ce.TargetEncoder(cols = ['nom_5','nom_6','nom_7','nom_8','nom_9'], smoothing=0.50)\ncols = ['nom_5','nom_6','nom_7','nom_8','nom_9']\nce_target_encoder.fit(train[cols], train['target'])\n#train = oof.sort_index() \ntest_nom = ce_target_encoder.transform(test[cols])\ntest_nom","8d1e4b85":"test.drop(['nom_5','nom_6','nom_7','nom_8','nom_9'],axis=1,inplace=True)\ntest = pd.concat([test, test_nom], axis=1)\ntest","938d2b53":"train.head()","aa3fc92f":"# Category variables -> Numerical variables\nlist_feat=['bin_3','bin_4','ord_1','ord_2']","7e3add12":"for feature in list_feat:\n    labels = train[feature].astype('category').cat.categories.tolist()\n    replace_map_comp = {feature : {k: v for k,v in zip(labels,list(range(0,len(labels)+1)))}}\n\n    train.replace(replace_map_comp, inplace=True)","c2f1409a":"list_feat=['bin_3','bin_4','ord_1','ord_2']","f7f2fed0":"for feature in list_feat:\n    labels = test[feature].astype('category').cat.categories.tolist()\n    replace_map_comp = {feature : {k: v for k,v in zip(labels,list(range(0,len(labels)+1)))}}\n\n    test.replace(replace_map_comp, inplace=True)","cb273037":"# Day\ntrain['day_sin'] = np.sin(2 * np.pi * train['day']\/7)\ntrain['day_cos'] = np.cos(2 * np.pi * train['day']\/7)\n# Month\ntrain['month_sin'] = np.sin(2 * np.pi * train['month']\/12)\ntrain['month_cos'] = np.cos(2 * np.pi * train['month']\/12)","a7045fbd":"# Day\ntest['day_sin'] = np.sin(2 * np.pi * test['day']\/7)\ntest['day_cos'] = np.cos(2 * np.pi * test['day']\/7)\n# Month\ntest['month_sin'] = np.sin(2 * np.pi * test['month']\/12)\ntest['month_cos'] = np.cos(2 * np.pi * test['month']\/12)","51a29dcb":"train.head()","13b885d7":"test.head()","2716f71f":"train.drop(['day','month'],axis=1,inplace=True)","1f21f373":"test.drop(['day','month'],axis=1,inplace=True)","e15655cc":"train.head()","54e716e5":"# train_target = train['target']\n# train_target\n# train.drop('target',axis=1,inplace=True)\n# train = pd.concat([train, train_target], axis=1)\n# train","16fe9446":"test.head()","cd20a8cb":"# checking the imbalance\nsb.countplot(x='target',data=train,palette='RdBu_r') # Barplot for the dependent variable","e718adfc":"train['target'].value_counts()","5cc2f548":"# # Separate the majority of data and the minority of data\ndf_majority = train[train['target']==0]\ndf_minority = train[train['target']==1]","39dba144":"# oversampling minority data\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # replace the original data\n                                 n_samples=208236,    # the number of data to match with majority\n                                 random_state=123) # reproducible results","60c03802":"# Combine majority class with upsampled minority class\ndf_upsampled = pd.concat([df_majority, df_minority_upsampled])","e1e26ced":"sb.countplot(x='target',data=df_upsampled,palette='RdBu_r')","540b21d7":"# Display new class counts\ndf_upsampled['target'].value_counts()","8ddebacc":"# dataset=df_upsampled._get_values","fcb1c83c":"# Separate input features (X) and target variable (y)\ny = df_upsampled.target\nX = df_upsampled.drop('target', axis=1)","783c4757":"train.shape","48ce38fc":"# # Build a forest and compute the feature importances\n# model1 = ExtraTreesClassifier(n_estimators=250,\n#                               random_state=0)\n\n# model1.fit(dataset_train,dataset_label)\n# importances = model1.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in model1.estimators_],\n#              axis=0)\n# indices = np.argsort(importances)[::-1]","9e169c26":"# Unbalanced dataset\n# X = train.iloc[:,np.r_[:,0:8,9:76]]  #independent columns\n# y = train.iloc[:,np.r_[:,8]]    #target column\n# Balanced dataset\ny = df_upsampled.target\nX = df_upsampled.drop('target', axis=1)\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel1 = ExtraTreesClassifier()\nmodel1.fit(X,y)\nprint(model1.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model1.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","f1149344":"# X = dataset[:,np.r_[:,0:8,9:76]]   #independent columns\n# y = dataset[:,np.r_[:,8]]    #target column\ny = df_upsampled.target\nX = df_upsampled.drop('target', axis=1)\n#get correlations of each features in dataset\ncorrmat = train.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sb.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d5a45eb5":"# from xgboost import XGBClassifier\n# from xgboost import plot_importance\n\n# # X = dataset[:,np.r_[:,0:8,9:76]]   #independent columns\n# # y = dataset[:,np.r_[:,8]]    #target column\n# y = df_upsampled.target\n# X = df_upsampled.drop('target', axis=1)\n# # fit model no training data\n# model2 = XGBClassifier()\n# model2.fit(X,y)\n# # feature importance\n# print(model2.feature_importances_)\n# # plot feature importance\n\n# plt.figure(figsize=(3,6))\n# plot_importance(model2,max_num_features=20)\n# plt.show()","4089a60b":"# from numpy import sort\n# from xgboost import XGBClassifier\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n# from sklearn.feature_selection import SelectFromModel\n\n# # X = train.iloc[:,np.r_[:,0:8,9:76]]  #independent columns\n# # Y = train.iloc[:,np.r_[:,8]]    #target column\n# y = df_upsampled.target\n# X = df_upsampled.drop('target', axis=1)\n\n# # split data into train and test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n# # fit model on all training data\n# model = XGBClassifier()\n# model.fit(X_train, y_train)\n# # make predictions for test data and evaluate\n# y_pred = model.predict(X_test)\n# predictions = [round(value) for value in y_pred]\n# accuracy = accuracy_score(y_test, predictions)\n# print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# # Fit model using each importance as a threshold\n# thresholds = sort(model.feature_importances_)\n# for thresh in thresholds:\n#     # select features using threshold\n#     selection = SelectFromModel(model, threshold=thresh, prefit=True)\n#     select_X_train = selection.transform(X_train)\n#     # train model\n#     selection_model = XGBClassifier()\n#     selection_model.fit(select_X_train, y_train)\n#     # eval model\n#     select_X_test = selection.transform(X_test)\n#     y_pred = selection_model.predict(select_X_test)\n#     predictions = [round(value) for value in y_pred]\n#     accuracy = accuracy_score(y_test, predictions)\n#     print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","b2b4620f":"## Import the random forest model.\nfrom sklearn.ensemble import RandomForestClassifier \n## This line instantiates the model. \nmodel3 = RandomForestClassifier() \n## Fit the model on your training data.\nmodel3.fit(X, y)","a56f9ef9":"feature_importances = pd.DataFrame(model3.feature_importances_,\n                                   index = X.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances","da4094fa":"(pd.Series(model3.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))","cc0ba9d2":"# display the relative importance of each attribute\noutput1=model1.feature_importances_","b4a71c62":"# output2=model2.feature_importances_","914152e4":"output3=model3.feature_importances_","5659c758":"output = output1 + output3 #  + output2","25218758":"n=18\nimportant_features=np.argsort(output)[::-1][:n]","a5621ff6":"important_features","5121105d":"training_data = X.iloc[:,important_features]\ntraining_label = y","eae91d07":"testing_data=test.iloc[:,important_features]","abc77a7c":"# train.shape","f0433475":"# training_data = train.iloc[:,np.r_[:,0:8,9:76]]  #independent columns\n# training_label = train.iloc[:,np.r_[:,8]]   #target column\ntraining_label = df_upsampled.target\ntraining_data = df_upsampled.drop('target', axis=1)","f69416a9":"X_train, X_test, y_train, y_test = train_test_split(training_data,training_label,test_size=0.33,random_state=101)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)","1d9070fe":"logmodel = LogisticRegression(C=0.01, solver='liblinear')\nlogmodel.fit(X_train,y_train)","3fd70df5":"predictions = logmodel.predict(X_test)","c41d18f8":"print(\"Accuracy is\", accuracy_score(y_test,predictions)*100)","fdadc493":"cm1 = confusion_matrix(y_test,predictions)","48583196":"print(cm1)","42a0f952":"print(classification_report(y_test,predictions))","07b3a488":"plt.clf()\nplt.imshow(cm1, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm1[i][j]))\nplt.show()","d144b671":"# scaler = StandardScaler()","ba9ae441":"# scaler.fit(training_data)","29b1f1fe":"# scaled_features = scaler.transform(training_data)","acc930d4":"# scaled_features","2f6a11bc":"# X_train, X_test, y_train, y_test = train_test_split(scaled_features,training_label,test_size=0.30)\n# print ('Train set:', X_train.shape,  y_train.shape)\n# print ('Test set:', X_test.shape,  y_test.shape)","5fda4afe":"# error_rate = []\n\n# # Will take some time\n# for i in range(1,20):\n    \n#     knn = KNeighborsClassifier(n_neighbors=i)\n#     knn.fit(X_train,y_train)\n#     pred_i = knn.predict(X_test)\n#     error_rate.append(np.mean(pred_i != y_test))","28497ab7":"# knn = KNeighborsClassifier(n_neighbors=1) # n_neighbors = k","13ca153c":"# knn.fit(X_train,y_train)","2bc2a851":"# predictions = knn.predict(X_test)","f332e0a1":"# from sklearn import metrics\n# print(\"Train set Accuracy: \", metrics.accuracy_score(y_train, knn.predict(X_train)))\n# print(\"Test set Accuracy: \", metrics.accuracy_score(y_test, predictions))","d748b053":"# print(\"Accuracy is\", accuracy_score(y_test,predictions)*100)","8eff31df":"# cm2 = confusion_matrix(y_test,predictions)","ec2532cf":"# print(cm2)","a8b3028b":"# print(classification_report(y_test,predictions))","85d39d86":"# plt.clf()\n# plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.Wistia)\n# classNames = ['Negative','Positive']\n# plt.title('Confusion Matrix')\n# plt.ylabel('True label')\n# plt.xlabel('Predicted label')\n# tick_marks = np.arange(len(classNames))\n# plt.xticks(tick_marks, classNames, rotation=45)\n# plt.yticks(tick_marks, classNames)\n# s = [['TN','FP'], ['FN', 'TP']]\n# for i in range(2):\n#     for j in range(2):\n#         plt.text(j,i, str(s[i][j])+\" = \"+str(cm2[i][j]))\n# plt.show()","50b276f6":"X_train, X_test, y_train, y_test = train_test_split(training_data,training_label,test_size=0.3,random_state=101)","4920e290":"dtree = DecisionTreeClassifier(criterion='entropy')","f4fa060f":"dtree.fit(X_train,y_train)","4b80c3be":"predictions = dtree.predict(X_test)","d13c3443":"print(\"Accuracy is\", accuracy_score(y_test,predictions)*100)","0591108c":"print(classification_report(y_test,predictions))","022f947a":"cm3 = confusion_matrix(y_test,predictions)","01316e4b":"print(cm3)","2a509d7b":"plt.clf()\nplt.imshow(cm3, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm3[i][j]))\nplt.show()","1307251c":"rfc = RandomForestClassifier(n_estimators=170)\nrfc.fit(X_train,y_train)","efbdd6ea":"rfc_pred = rfc.predict(X_test)","3d709c9d":"print(\"Accuracy is\", accuracy_score(y_test,rfc_pred)*100)","6c8c9bfb":"print(classification_report(y_test,rfc_pred))","171f027c":"cm4 = confusion_matrix(y_test,rfc_pred)","ddb4bd9a":"print(cm4)","f0adc088":"plt.clf()\nplt.imshow(cm4, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nplt.title('Confusion Matrix')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm4[i][j]))\nplt.show()","e8b2c74e":"# X_train, X_test, y_train, y_test = train_test_split(training_data, training_label, test_size=0.30, random_state=101)\n# print ('Train set:', X_train.shape,  y_train.shape)\n# print ('Test set:', X_test.shape,  y_test.shape)","ec5d5fe4":"# model = SVC()","8daf0935":"# model.fit(X_train,y_train) # If C is 0, we can have no margin kernel ='Radial Basis Functions'(Big cone located in all points of data set)","286eea67":"# predictions = model.predict(X_test)","f5dd503d":"# cm5 = confusion_matrix(y_test,predictions)","695cab30":"# print(\"Accuracy is\", accuracy_score(y_test,predictions)*100)","965f4b24":"# print(cm5)","e909c550":"# print(classification_report(y_test,predictions))","18b16d01":"# plt.clf()\n# plt.imshow(cm5, interpolation='nearest', cmap=plt.cm.Wistia)\n# classNames = ['Negative','Positive']\n# plt.title('Confusion Matrix')\n# plt.ylabel('True label')\n# plt.xlabel('Predicted label')\n# tick_marks = np.arange(len(classNames))\n# plt.xticks(tick_marks, classNames, rotation=45)\n# plt.yticks(tick_marks, classNames)\n# s = [['TN','FP'], ['FN', 'TP']]\n# for i in range(2):\n#     for j in range(2):\n#         plt.text(j,i, str(s[i][j])+\" = \"+str(cm5[i][j]))\n# plt.show()","b81dfa62":"submission = pd.read_csv('..\/input\/cat-in-the-dat\/sample_submission.csv')","a94efc8f":"final_prediction=rfc.predict(test)","7e34e42b":"submission[\"target\"] = rfc.predict_proba(test)[:, 1]","2d92f55e":"# submission[\"target\"] =final_prediction","7fcb44ed":"submission.head()","817b66af":"submission.to_csv('submission.csv', index=False)","12077ee9":"## Evaluation\n### confusion matrix\nAnother way of looking at accuracy of classifier is to look at __confusion matrix__.","24cc872e":"As you can see, the new DataFrame has more observations than the original, and the ratio of the two classes is now 1:1.","e9389039":"### Choosing a K Value\nUse the elbow method.","0af5f44c":"## 2. Building a K Nearest Neighbors model\nIt works very slow so I didn't use it.","5b571031":"## One-hot coding for nomial features","a3283ece":"# 1. Building a Logistic Regression model\nLet's build our model using LogisticRegression from Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including \u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019 solvers. You can find extensive information about the pros and cons of these optimizers if you search it in internet.\nThe version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models. C parameter indicates inverse of regularization strength which must be a positive float. Smaller values specify stronger regularization. Now lets fit our model with train set:","39b0e3d7":"# Import Libraries","a4abdfce":"## Label-encoder","7059b134":"## Train Test Split\nLet's start by splitting our data into a training set and test set","c25d2e61":"# Feature Importance\n\nWe can get the feature importance of each feature of our dataset by using the feature importance property of the model. Feature importance gives you a score for each feature of our data, the higher the score more important or relevant is the feature towards our output variable. Feature importance is an inbuilt class that comes with Tree Based Classifiers, we will be using Extra Tree Classifier for extracting the top 10 features for the dataset.<br>\n\nThere are 4 different feature selection techniques: univariate selection, recursive feature elimination, principle component analysis, and feature importance. So, we need to select the important features: Extra Trees Classifier and XGBClassifier","445d9a48":"Based on the count of each section, we can calculate precision and recall of each label:\n\n\n- __Precision__ is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP\u00a0\/\u00a0(TP\u00a0+\u00a0FP)\n\n- __Recall__ is true positive rate. It is defined as: Recall = \u00a0TP\u00a0\/\u00a0(TP\u00a0+\u00a0FN)\n\n    \nSo, we can calculate precision and recall of each class.\n\n__F1 score:__\nNow we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n\nThe F1 score is the harmonic average of the\u00a0precision and recall, where an F1\u00a0score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n\n\nAnd finally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 0.72 in our case.","310c96a0":"# Is there a cat in your data?\nBecause this is such a common task and important skill to master, we've put together a dataset that contains only categorical features, and includes:\n\nbinary features\nlow- and high-cardinality nominal features\nlow- and high-cardinality ordinal features\n(potentially) cyclical features\n\nhttps:\/\/www.kaggle.com\/alexisbcook\/categorical-variables <br>\nhttps:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63","9338bb2c":"### Predictions and Evaluations\nNow let's predict using the trained model.","f20b2571":"## 5. Building the Support Vector Machines model\nIt works very slow so I didn't use it.","1982528f":"## 4. Building the Random Forests model","4b53d3bf":"# Oversampling\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).\nDespite the advantage of balancing classes, these techniques also have their weaknesses (there is no free lunch). The simplest implementation of over-sampling is to duplicate random records from the minority class, which can cause overfitting. In under-sampling, the simplest technique involves removing random records from the majority class, which can cause loss of information.","7ac5a853":"<h4>Evaluating for Missing Data<\/h4>\n\nThe missing values are converted to Python's default. We use Python's built-in functions to identify these missing values. There are two methods to detect missing data:\n<ol>\n    <li><b>.isnull()<\/b><\/li>\n    <li><b>.notnull()<\/b><\/li>\n<\/ol>\nThe output is a boolean value indicating whether the value that is passed into the argument is in fact missing data.","d2da641c":"The SVM algorithm offers a choice of kernel functions for performing its processing. Basically, mapping data into a higher dimensional space is called kernelling. The mathematical function used for the transformation is known as the\u00a0kernel\u00a0function, and can be of different types, such as:\n\n    1.Linear\n    2.Polynomial\n    3.Radial basis function (RBF)\n    4.Sigmoid\nEach of these functions has its characteristics, its pros and cons, and its equation, but as there's no easy way of knowing which function performs best with any given dataset, we usually choose different functions in turn and compare the results. Let's just use the default, RBF (Radial Basis Function) for this lab.","bc0d3ff6":"<h4>Count missing values in each column<\/h4>\n<p>\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value, \"False\"  means the value is present in the dataset.  In the body of the for loop the method  \".value_counts()\"  counts the number of \"True\" values. \n<\/p>","49ea7118":"## Target encoder\nTarget-based encoding is numerization of categorical variables via target. In this method, we replace the categorical variable with just one new numerical variable and replace each category of the categorical variable with its corresponding probability of the target (if categorical) or average of the target (if numerical). The main drawbacks of this method are its dependency to the distribution of the target, and its lower predictability power compare to the binary encoding method.","a32f12dc":"## Let's check the dimension of dataset","ace07207":"### Using KNN","d1deaa2c":"### Predictions and Evaluations","82ab59c1":"## Feature hashing","78ef581e":"In conclusion, there is no missing value in this dataset. We don't need to handle the missing value:)","d5816c4d":"## Training and Predicting","423938a0":"## Handling cyclical features: Day, month","a3e55ca8":"# Methology","269624ad":"# Converting Categorical Features\nWe'll need to convert categorical features to numerical features. Otherwise our machine learning algorithm won't be able to directly take in those features as inputs.","f1e1dbe1":"<h4>Count missing values in each column<\/h4>\n<p>\nUsing a for loop in Python, we can quickly figure out the number of missing values in each column. As mentioned above, \"True\" represents a missing value, \"False\"  means the value is present in the dataset.  In the body of the for loop the method  \".value_counts()\"  counts the number of \"True\" values. \n<\/p>","69c8d807":"## 3. Building the Decision Tree\nWe'll start just by training a single decision tree.","d9b36660":"## Prediction and Evaluation\nLet's evaluate our decision tree.","d4bdc922":"### Train Test Split","578b6d57":"# Get the Data","a7a6e3f0":"# Submission ","f7618a8c":"### Standardize the Variables","135e4927":"### Train Test Split","418e2558":"# Exploratory Data Analysis\nIn this section, I deal with the data cleaning and check out some missing data or imbalance data.","a7756b5b":"# Correlation Matrix with Heatmap\n\nCorrelation states how the features are related to each other or the target variable. Correlation can be positive (increase in one value of feature increases the value of the target variable) or negative (increase in one value of feature decreases the value of the target variable) Heatmap makes it easy to identify which features are most related to the target variable, we will plot heatmap of correlated features using the seaborn library."}}