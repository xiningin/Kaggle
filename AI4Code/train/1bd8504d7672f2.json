{"cell_type":{"6452f129":"code","e1c86e40":"code","a12b1360":"code","102a1389":"code","aa0b5c1d":"code","2f206744":"code","da01f1b9":"code","6f20eb8d":"code","bad49d0d":"code","16dccb09":"code","10971789":"code","1b55028a":"code","3f8b5015":"code","276a3cb8":"code","226763e7":"code","3ebbb87e":"code","75861604":"code","8c5355b6":"code","b32dc02e":"code","8adf4575":"code","5a9e1ecc":"code","f71373d7":"code","c44726fd":"code","98d7657d":"code","43eba6c3":"code","e5cc6dea":"code","e56673c6":"code","3a6d393b":"code","1f1f9d06":"code","51432a77":"code","19873326":"code","6dface04":"code","dca7ff07":"code","b1ec542b":"code","b72340f4":"code","da68f9fc":"code","2dc00b5b":"code","64a291ba":"code","bd026e63":"code","e9b86432":"code","c197449c":"code","bf8d1b4a":"code","cb00e1b7":"code","822a96fe":"code","c431a7b3":"code","34b08699":"code","e4db9e53":"code","6fb50b3e":"markdown","f1fd7189":"markdown","3cc8e285":"markdown","42dec089":"markdown","aa78e7ac":"markdown","89d58570":"markdown","39ea7a7d":"markdown","d0e1037f":"markdown","6928939c":"markdown","32cfd303":"markdown","3e688329":"markdown","a407c7d6":"markdown","a44851bb":"markdown","be31e2ee":"markdown","2f612f90":"markdown","e55848ea":"markdown","d0640713":"markdown","a0832350":"markdown"},"source":{"6452f129":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e1c86e40":"train_df = pd.read_csv(\"\/kaggle\/input\/predict-demand\/train.csv\")\n# train[6479:6490]\ntrain  = train_df.drop(train_df.index[6480:]) # after row 6479, all the values are Nan.","a12b1360":"train.tail()","102a1389":"test = pd.read_csv(\"\/kaggle\/input\/predict-demand\/test.csv\")\ntest.tail()","aa0b5c1d":"train['date'] = pd.to_datetime(train['date']) # change date to datetime data type\ntrain.info()","2f206744":"train.head()","da01f1b9":"pd.DataFrame(train.groupby('city').count()['id'])\n# Here we see that these are cities in Greece.","6f20eb8d":"pd.DataFrame(train.groupby('brand').count()['id'])\n# Since there are multiple brands, we will just focus on the demand ","bad49d0d":"gazoza = train[train['brand'] == 'gazoza'].reset_index(drop=True)","16dccb09":"gazoza.head()","10971789":"# q = gazoza.groupby(['date'], as_index=False).sum()\n# sns.lineplot(x =q['date'], y=q['quantity'])","1b55028a":"gazoza.describe()","3f8b5015":"# let's see how the demand of this product is distributed\nsns.displot(gazoza['quantity'], kde = True, color = 'g')","276a3cb8":"sns.displot(data = gazoza, x = 'quantity', kde = True, col = 'container', color = 'g')","226763e7":"# find the sample mean and sample standard deviation\nmean = np.mean(gazoza['quantity'])\nstd = np.std(gazoza['quantity'])\n","3ebbb87e":"# Plot the histogram.\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(x = gazoza['quantity'], \n         density=True, alpha = 0.6, \n         color='g', bins = 25, linewidth=1, edgecolor='black')\n\n\n# Plot the PDF.\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p, 'k', linewidth=2)\ntitle = \"Fit results: mu = %.2f,  std = %.2f\" % (mean, std)\nplt.title(title)\n\nplt.show()","75861604":"def orderQuan(mean, std, Cu, Co):\n    # let y be the optimal order quanity\n    criticalVal = Cu\/(Cu+Co) # Critical Value for the optimal order quantity\n    y = norm.ppf(criticalVal, mean, std) # Calculation for optimal order quanity\n    return y","8c5355b6":"orderQuan(mean, std, 0.3, 0.4)","b32dc02e":"mu, sigma = 1000, 200\nsample = np.random.normal(mu, sigma, size=1000)\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(sample, bins = 25, edgecolor='black')","8adf4575":"sample_mean = np.mean(sample)\nsample_std = np.std(sample)\nprint('Mean = %.3f,  Standard Deviation = %.3f' % (sample_mean, sample_std))","5a9e1ecc":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(sample, bins = 25, edgecolor='black', alpha = 0.6, density = True) \n\n# Plot the PDF.\nxmin2, xmax2 = plt.xlim()\nx2 = np.linspace(xmin2, xmax2, 100)\np2 = norm.pdf(x2, sample_mean, sample_std)\nplt.plot(x2, p2, 'k', linewidth=2)\ntitle2 = \"Fit results: mu = %.2f,  std = %.2f\" % (sample_mean, sample_std)\nplt.title(title2)\n\nplt.show()","f71373d7":"# for Cu = 190 and Co = 274\norderQuan(sample_mean, sample_std, Cu=190, Co=274)","c44726fd":"%matplotlib inline\nimport warnings\nimport matplotlib\nimport scipy.stats as st\nimport statsmodels as sm","98d7657d":"matplotlib.rcParams['figure.figsize'] = (16.0, 12.0)\nmatplotlib.style.use('ggplot')\n\n# Create models from data\ndef best_fit_distribution(data, bins=200, ax=None):\n    \"\"\"Model data by finding best fit distribution to data\"\"\"\n    # Get histogram of original data\n    y, x = np.histogram(data, bins=bins, density=True)\n    x = (x + np.roll(x, -1))[:-1] \/ 2.0\n\n    # Distributions to check\n    DISTRIBUTIONS = [        \n        st.alpha,st.beta,st.betaprime,st.chi,st.chi2,st.cosine,st.dgamma,st.dweibull,st.erlang,\n        st.expon,st.exponnorm,st.exponweib,st.exponpow,st.f,st.genlogistic,st.genpareto,st.gennorm,\n        st.genexpon,st.genextreme,st.gausshyper,st.gamma,st.gengamma,st.invgamma,st.invgauss,\n        st.invweibull,st.johnsonsb,st.johnsonsu,st.laplace,st.logistic,st.loggamma,st.loglaplace,\n        st.lognorm,st.lomax,st.maxwell,st.nakagami,st.norm,st.pareto,st.pearson3,st.powerlaw,\n        st.powerlognorm,st.reciprocal,st.triang,st.tukeylambda,st.uniform,st.weibull_min,st.weibull_max\n    ]\n\n    # Best holders\n    best_distribution = st.norm\n    best_params = (0.0, 1.0)\n    best_sse = np.inf\n\n    # Estimate distribution parameters from data\n    for distribution in DISTRIBUTIONS:\n\n        # Try to fit the distribution\n        try:\n            # Ignore warnings from data that can't be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings('ignore')\n\n                # fit dist to data\n                params = distribution.fit(data)\n\n                # Separate parts of parameters\n                arg = params[:-2]\n                loc = params[-2]\n                scale = params[-1]\n\n                # Calculate fitted PDF and error with fit in distribution\n                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n                sse = np.sum(np.power(y - pdf, 2.0))\n\n                # if axis pass in add to plot\n                try:\n                    if ax:\n                        pd.Series(pdf, x).plot(ax=ax)\n                    end\n                except Exception:\n                    pass\n\n                # identify if this distribution is better\n                if best_sse > sse > 0:\n                    best_distribution = distribution\n                    best_params = params\n                    best_sse = sse\n\n        except Exception:\n            pass\n\n    return (best_distribution.name, best_params)\n","43eba6c3":"# Load data from statsmodels datasets\ndata = pd.Series(gazoza['quantity'])\n\n# Find best fit distribution\nbest_fit_name, best_fit_params = best_fit_distribution(data, 200)\nprint(\"The best Distibution is: \" + best_fit_name)\nprint(\"The parameters are: \")\nprint(best_fit_params)\nbest_dist = getattr(st, best_fit_name)","e5cc6dea":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(data, bins = 25, edgecolor='black', alpha = 0.6, density = True) \n\n# Plot the PDF.\nxmin5, xmax5 = plt.xlim()\nx5 = np.linspace(xmin5, xmax5, 100)\nparamSample = st.johnsonsu.fit(data)\np5 = st.johnsonsu.pdf(x5, paramSample[0], paramSample[1], paramSample[2], paramSample[3])\nplt.plot(x5, p5, 'k', linewidth=2)\ntitle5 = \"\"\nplt.title(title5)\n\nplt.show()","e56673c6":"Cu=0.3\nCo=0.4\ncriticalVal = Cu\/(Cu+Co) # Critical Value for the optimal order quantity\nst.johnsonsu.ppf(criticalVal, paramSample[0], paramSample[1], paramSample[2], paramSample[3]) \n# Calculation for optimal order quanity","3a6d393b":"gazozaTest = test[test['brand'] == 'gazoza'].reset_index(drop=True)","1f1f9d06":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(gazozaTest['quantity'], bins = 25, edgecolor='black', alpha = 0.6, density = True) \nplt.hist(data, bins = 25, edgecolor='black', alpha = 0.2, density = True) \n\n# Plot the PDF.\nxmin6, xmax6 = plt.xlim()\nx6 = np.linspace(xmin6, xmax6, 100)\nparamSample = st.johnsonsu.fit(data)\np6 = st.johnsonsu.pdf(x6, paramSample[0], paramSample[1], paramSample[2], paramSample[3])\nplt.plot(x6, p6, 'k', linewidth=2)\ntitle6 = \"\"\nplt.title(title6)\n\nplt.show()","51432a77":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(gazoza[pd.DatetimeIndex(gazoza['date']).year ==2014]['quantity'], \n         bins = 25, edgecolor='black', alpha = 0.6, density = True) \nplt.hist(data, bins = 25, edgecolor='black', alpha = 0.2, density = True) \n\n# Plot the PDF.\nxmin6, xmax6 = plt.xlim()\nx6 = np.linspace(xmin6, xmax6, 100)\nparamSample = st.johnsonsu.fit(data)\np6 = st.johnsonsu.pdf(x6, paramSample[0], paramSample[1], paramSample[2], paramSample[3])\nplt.plot(x6, p6, 'k', linewidth=2)\ntitle6 = \"\"\nplt.title(title6)\n\nplt.show()","19873326":"def sse_johnsonsu(sse_data,train_data, bins =200):\n    y, x = np.histogram(sse_data, bins=bins, density=True)\n    x = (x + np.roll(x, -1))[:-1] \/ 2.0\n    # Ignore warnings from data that can't be fit\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        # fit dist to data\n        params = st.johnsonsu.fit(train_data)\n        # Separate parts of parameters\n        arg = params[:-2]\n        loc = params[-2]\n        scale = params[-1]\n        # Calculate fitted PDF and error with fit in distribution\n        pdf = st.johnsonsu.pdf(x, loc=loc, scale=scale, *arg)\n        sse = np.sum(np.power(y - pdf, 2.0))\n    return sse\n\nprint(\"SSE for train data is: %.9f\" % (sse_johnsonsu(data, data)))\nprint(\"SSE for test data is: %.9f\" % (sse_johnsonsu(gazozaTest['quantity'], data)))\nprint(\"SSE for 2014 data is: %.9f\" % (sse_johnsonsu(gazoza[pd.DatetimeIndex(gazoza['date']).year ==2014]['quantity'], data)))\nprint(\"SSE for 2015 data is: %.9f\" % (sse_johnsonsu(gazoza[pd.DatetimeIndex(gazoza['date']).year ==2015]['quantity'], data)))\nprint(\"SSE for 2016 data is: %.9f\" % (sse_johnsonsu(gazoza[pd.DatetimeIndex(gazoza['date']).year ==2014]['quantity'], data)))\nprint(\"SSE for 2017 data is: %.9f\" % (sse_johnsonsu(gazoza[pd.DatetimeIndex(gazoza['date']).year ==2015]['quantity'], data)))","6dface04":"from sklearn.neighbors import KernelDensity","dca7ff07":"# the library espects the data to be 2D, so let's reshape it\nsampleArray = sample.reshape((len(sample), 1))","b1ec542b":"# we can try multiple bandwidth values and kernels. Let's start with bandwidth=2, kernel='gaussian'\nmodel = KernelDensity(bandwidth=50, kernel='gaussian')\n# I kept inccreasing the bandwidth, \n# but it is important to note that using a low bandwidth can result in overfitting.\n\nmodel.fit(sampleArray)","b72340f4":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(sampleArray, bins = 25, edgecolor='black', alpha = 0.6, density = True) \n\n# Plot the PDF.\n# values = np.asarray([value for value in range(200, 1600)])\n# values = values.reshape((len(values), 1))\nxmin3, xmax3 = plt.xlim()\nx3 = np.linspace(xmin3, xmax3, 100)\nx3 = x3.reshape((len(x3), 1))\npKernel = model.score_samples(x3)\npKernel = np.exp(pKernel)\ntitle3 = \"Fit results using Kernel Density Estimators\"\nplt.plot(x3[:], pKernel)\nplt.title(title3)\n\nplt.show()","da68f9fc":"model.get_params(deep=True)","2dc00b5b":"kernel_sample = model.sample(n_samples=1000, random_state=None)","64a291ba":"plt.hist(kernel_sample, bins = 25, edgecolor='black', alpha = 0.6, density = False, color = 'c') ","bd026e63":"print('Kernel Sample Means = %.2f, and Standard Deviation = %.2f'% (np.mean(kernel_sample), np.std(kernel_sample)))","e9b86432":"model2 = KernelDensity(bandwidth=1000, kernel='gaussian')\n\ngazozaArray = gazoza.loc[:,'quantity'].values\ngazozaArray = gazozaArray.reshape((len(gazozaArray),1))\n\nmodel2.fit(gazozaArray)","c197449c":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(gazozaArray, bins = 25, edgecolor='black', alpha = 0.6, density = True, color = 'g') \n\n# Plot the PDF.\n# values = np.asarray([value for value in range(200, 1600)])\n# values = values.reshape((len(values), 1))\nxmin4, xmax4 = plt.xlim()\nx4 = np.linspace(xmin4, xmax4, 100)\nx4 = x4.reshape((len(x4), 1))\npGazoza = model2.score_samples(x4)\npGazoza = np.exp(pGazoza)\ntitle4 = \"Fit results using Kernel Density Estimators for Gazoza Data\"\nplt.plot(x4[:], pGazoza)\nplt.title(title4)\n\nplt.show()","bf8d1b4a":"# Plot the Histogram\nplt.figure(figsize=(8, 6), dpi=80)\nplt.hist(gazozaTest['quantity'], bins = 25, edgecolor='black', alpha = 0.5, density = True, color = 'm') \nplt.hist(gazozaArray, bins = 25, edgecolor='black', alpha = 0.2, density = True, color = 'g') \n\n# Plot the PDF.\n# values = np.asarray([value for value in range(200, 1600)])\n# values = values.reshape((len(values), 1))\nxmin4, xmax4 = plt.xlim()\nx4 = np.linspace(xmin4, xmax4, 100)\nx4 = x4.reshape((len(x4), 1))\npGazoza = model2.score_samples(x4)\npGazoza = np.exp(pGazoza)\ntitle4 = \"Fit results using Kernel Density Estimators for Gazoza Data\"\nplt.plot(x4[:], pGazoza)\nplt.title(title4)\n\nplt.show()","cb00e1b7":"qty = pd.DataFrame(gazoza.groupby(['date'], axis =0, as_index=True).sum()['quantity'])","822a96fe":"sns.displot(qty, bins=8)","c431a7b3":"# Find best fit distribution\nqty_fit_name, qty_fit_params = best_fit_distribution(qty, 200)\nprint(\"The best Distibution is: \" + qty_fit_name)\nprint(\"The parameters are: \")\nprint(qty_fit_params)\nqty_dist = getattr(st, qty_fit_name)","34b08699":"# Plot the Histogram\nplt.figure(figsize=(10, 6), dpi=80)\nplt.hist(qty, bins = 8, edgecolor='black', alpha = 0.6, density = True, label = \"Quantity Sold Data\") \n\n# Plot the PDF.\nxmin_qty, xmax_qty = plt.xlim()\nx_qty = np.linspace(xmin_qty, xmax_qty, 100)\np_qty = norm.pdf(x_qty, np.mean(qty), np.std(qty))\nplt.plot(x_qty, p_qty, 'k', linewidth=2, label = \"Normal Dist\")\ntitle_qty = \"Best Fit for Gazoza Sale Aggregated by Month\"\nplt.legend(loc='best')\nplt.title(title_qty)\n\nplt.show()","e4db9e53":"orderQuan(np.mean(qty), np.std(qty), 0.3, 0.4)","6fb50b3e":"(The distribution is not really normal. We'll deal with that in a bit.)\n\nNow that we have fit a normal disribution to the demand data, we can calculate the optimal order quanity.","f1fd7189":"#### Let's see how the distribution fits for the demand from the test set (Demand for 2018).","3cc8e285":"## Non-Parametric Density Estimation\n1. Using Kernel Density Estimation\n\nread more: https:\/\/machinelearningmastery.com\/probability-density-estimation\/","42dec089":"This is a straightforward process when the distribution is normal. \n\nWhen the distribution is not normal, it may not be as easy to compute the parameters of the distribution. But we can have the computer do the computation.\n\n### In the following code, we will try to fit multiple distribution to find the distribution with the smallest sse.\n\nThe ``stats.<distribution>.fit()`` function reutrns the MLEs for shape, location, and scale parameters from data. MLE stands for Maximum Likelihood Estimate.  Starting estimates for the fit are given by input arguments.\n\nFor the normal distribution, the same results could have been achieved by using ``stats.norm.fit()``. We can do this for multiple distibution and see which one fits the best.\n\n\n[I also wanna point out there a risk of data snooping with this method here.]","aa78e7ac":"We can see that the non-parametric method also yielded a good estimate of the true mean and true standard deviation.\n\nLet's apply this non-parametric approach to the gazoza demand data.","89d58570":"#### Let us consider the demand of only one product, Gazoza. Why did I choose that? Because Gazoza is a fun name.","39ea7a7d":"We could have also gotten the same result by utilizing the ``stats.norm.fit()`` function. More on that later.","d0e1037f":"#### But, how can be use this non-parametric approach in the Newsvendor model?\n\n- The critical value Cu\/Cu+Co is equvalent to the area under the curve (AUC) of a probability desnity function to the left of the optimal order quanity. Thus, once we know the critical calue, we can find which order quantity gives an AUC of that value.","6928939c":"Now let's assume that the cost of underage Cu (!not under-age!) is 0.30 euros and the cost of underage Cu (not under-age) is 0.4 euros.\n\nThe optimal order quantity can be calculated as follows.","32cfd303":"Now we can generate a sample from the Kenerl density model that we have gotten.","3e688329":"## Import Sales\/Damand Data and Clean\n\nThe factor quantity represents demand of the products.","a407c7d6":"For the same values of Cu and Co, using the normal distribution, we got an order quantity of 40431.270, which is 3366.1535 more than what we got using the JohnsonSu distribution.","a44851bb":"#### And let's see how it fits for other years.","be31e2ee":"### Now let us try to fit a normal distribution to the demand of Gazoza in Greece\nTo do this, we find the sample average of the demand to approximate the mean.\nWe also find the sample standard deviation to approximate the true standard deviation.","2f612f90":"## Parametric Probability Density Distribution Estimation\n\nHere, we can find the average and standard deviation of the sample data and use them to estimate the true distribution.\n\nLet's assume that we know the true distribution of the demand to be a normal with mean of 1000 and standard deviation of 200.\n\nHow can we use different techniques to approximate the distribution of the demand?\n\nLet us generate a sample data of size 1000 from a distribution of our choice.","e55848ea":"However, the histogram is skewed. This suggests that the true distribution of the demand is not a normal distribution. Hence we have to find other ways to estimate the distribution of the demand. Afterall, we need the distribution to solve for the optimal order quantity.","d0640713":"### Knowing the probability distiburion of the demand of products can help determine the optimal order quantity.\n\nThe Newsvendor problem is a situation where there is uncertain demand for a perishable product.\n\nNewspaper is an example. A newsvendor does not know how many newspapers they will sell that day, and they have to estimate the order quanity. If the amount the newsvendor ordered is too little and the demand is too large, then they lose potential income. If the demand is too little and they ordered too much newspaper, then they will not get a profit on the money they spent to buy the papers.\n\nThis problem can be optimized if we know the distribution of the demand *D*, *Co* or the costs of ordering one more newspaper over the demand (*d+1*), and *Cu* the cost of ordering one less newspaper under the demand (*d-1*).\n\nSuppose the CDF of the demand distribution is *F(.)* and *y* is the order quantity. \n\nThe optinal *y = F^-1 (Cu \/ (Cu + Co))*\n\nWe do not always know how the demand is distributed.\n\n**But how can be estimate the distribution?**","a0832350":"**Now that we know the parameters of the distrubution, we can easily compute the order quanity using the Newsvendor model formula**"}}