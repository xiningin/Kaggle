{"cell_type":{"bde39405":"code","2bbb2c81":"code","b5628642":"code","750b6c23":"code","0e5db30f":"code","4c841b2e":"code","dcd107c8":"code","83651946":"code","bcf70a53":"code","3fae3fde":"code","d9149178":"code","5a8023d7":"code","f41486c0":"code","baff418b":"code","1b12a4f8":"code","e41e6bf5":"code","f4c73e06":"code","7ded9811":"code","086ed91b":"code","d3a7699a":"code","52ac9ba1":"code","d27e4b8a":"code","8b69b338":"code","7bf5f7f0":"code","5ea3f6a9":"code","730f577f":"code","0930668c":"code","a0f4c77c":"code","8f0ece50":"code","bf50f821":"markdown","fa462dfa":"markdown","43dfc853":"markdown","67cceebb":"markdown","cf24a90d":"markdown","7b8d72bd":"markdown","30d3d7f7":"markdown","93fe5f0b":"markdown","dddbafff":"markdown","fca50b67":"markdown","c5ef6d87":"markdown","d3c67b69":"markdown","7911d15f":"markdown","64a11fc1":"markdown","cabd9bbf":"markdown","6bf73dee":"markdown","9d06e227":"markdown","b131622d":"markdown","afa36805":"markdown","6a319d9a":"markdown","34c0820b":"markdown"},"source":{"bde39405":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2bbb2c81":"import os\nimport json\nfrom pandas.io.json import json_normalize\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b5628642":"# Loading data and flattening JSON columns\ndef load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     # Set the date, fullVisitorId, sessionId as string of constant\n                     dtype={'date': str, 'fullVisitorId': str, 'sessionId': str}, \n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(list(df[column]))\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","750b6c23":"%%time\ntrain_df = load_df()\ntest_df = load_df('..\/input\/test.csv')","0e5db30f":"def na_detect(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum() \/ df.isnull().count() * 100 ).sort_values(ascending = False)\n    df_opt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    \n    plt.figure(figsize=(20,20))\n    fig, ax = plt.subplots()\n    col_na = total[total>0]\n    bar_na = ax.barh(col_na.index, col_na.values, 0.8)\n    for i, v in enumerate(col_na.values):\n        ax.text(v + 5, i - .15 , str(v), color='red')#, fontweight='bold')\n    plt.title('Variables with Missing Value')\n    plt.xlabel('Quantity of Missing Value')\n    plt.ylabel('Columns')\n    plt.show()\n    \n    print (df_opt[~(df_opt['Total'] == 0)])\n    \n    return","4c841b2e":"na_detect(train_df)","dcd107c8":"set(train_df.columns).difference(set(test_df.columns))","83651946":"def date_convert(df):\n    df['visitdate'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    #df['visitdate'] = pd.datetime.utcfromtimestamp(test_df['visitStartTime'])\n    df['wday'] = df['visitdate'].dt.weekday\n    df['hour'] = df['visitdate'].dt.hour\n    df['day'] = df['visitdate'].dt.day\n    df['month'] = df['visitdate'].dt.month\n    return    ","bcf70a53":"for df in [train_df, test_df]:\n    date_convert(df)\nprint('TrainSet:', train_df.shape)\nprint('TestSet:', test_df.shape)","3fae3fde":"def constant_process(df):\n    num_constant = 0\n    constant_cols = []\n    for col in df.columns:\n        if df[col].nunique()==1:\n            constant_cols.append(col)\n            num_constant = num_constant+1\n            \n    print('Number of Constant Variables:', num_constant)\n    print(constant_cols)\n    df = df.drop(constant_cols, axis=1)\n    print('Shape: ', df.shape)\n    return df","d9149178":"ctrain_df = constant_process(train_df)\nctest_df = constant_process(test_df)","5a8023d7":"print('Unique Variables in Train:', ctrain_df['sessionId'].nunique())\nprint('Unique Variables in Test:', ctest_df['sessionId'].nunique())","f41486c0":"dup_session = ctrain_df[ctrain_df.duplicated(subset='sessionId', keep=False)].sort_values('sessionId',ascending = False)\ndup_session.head(2)","baff418b":"ctrain_df[\"totals.transactionRevenue\"].fillna(0, inplace=True)\nctrain_df['totals.transactionRevenue'] = ctrain_df['totals.transactionRevenue'].astype(int)\nctrain_df['totals.hits'] = ctrain_df['totals.hits'].astype(int)\nctrain_df['totals.pageviews'] = ctrain_df['totals.hits'].astype(int)\nctest_df['totals.hits'] = ctest_df['totals.hits'].astype(int)\nctest_df['totals.pageviews'] = ctest_df['totals.hits'].astype(int)","1b12a4f8":"plt.figure(figsize=(14,5))\nplt.subplot(1,2,2)\nax = sns.distplot(np.log1p(ctrain_df[ctrain_df['totals.transactionRevenue'] > 0][\"totals.transactionRevenue\"]), kde=True)\nax.set_xlabel('Transaction Revenue Log', fontsize=15)\nax.set_ylabel('Distribuition', fontsize=15)\nax.set_title(\"Distribuition of Revenue Log\", fontsize=20)\nplt.subplot(1,2,1)\nsns.distplot(ctrain_df[\"totals.transactionRevenue\"], kde=True)\nplt.xlabel('Transaction Revenue', fontsize=15)\nplt.ylabel('Distribuition', fontsize=15)\nplt.title(\"Distribuition of Revenue\", fontsize=20)","e41e6bf5":"valid_df = ctrain_df[ctrain_df['totals.transactionRevenue'] > 0]\nna_detect(valid_df)","f4c73e06":"ctrain_df = ctrain_df.drop(['trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot'], axis=1)\nprint('Train Shape: ' ,ctrain_df.shape)\nctest_df = ctest_df.drop(['trafficSource.adContent', 'trafficSource.adwordsClickInfo.adNetworkType', 'trafficSource.adwordsClickInfo.gclId', 'trafficSource.adwordsClickInfo.page', 'trafficSource.adwordsClickInfo.slot'], axis=1)\nprint('Test Shape:' ,ctest_df.shape)","7ded9811":"plt.figure(figsize=(14,5))\nplt.subplot(1,2,1)\nsns.distplot(ctrain_df[\"totals.hits\"], kde=True)\nplt.xlabel('Hits', fontsize=15)\nplt.ylabel('Distribuition', fontsize=15)\nplt.title(\"Distribuition of Hits\", fontsize=20)\nplt.subplot(1,2,2)\nsns.distplot(ctrain_df[\"totals.pageviews\"], kde=True)\nplt.xlabel('Page Views', fontsize=15)\nplt.ylabel('Distribuition', fontsize=15)\nplt.title(\"Distribuition of Page Views\", fontsize=20)","086ed91b":"ctrain_df[\"totals.pageviews\"].fillna(value=ctrain_df['totals.pageviews'].median(), inplace=True)\nctest_df[\"totals.pageviews\"].fillna(value=ctest_df['totals.pageviews'].median(), inplace=True)","d3a7699a":"plt.figure(figsize=(10,5))\nsns.distplot(ctrain_df[\"visitNumber\"], kde=True)\nplt.xlabel('Visit Number', fontsize=15)\nplt.ylabel('Distribuition', fontsize=15)\nplt.title(\"Distribuition of Visit Number\", fontsize=20)","52ac9ba1":"non_relevant = [\"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\", \"visitdate\", \"totals.transactionRevenue\"]","d27e4b8a":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategorical_cols = [c for c in ctrain_df.columns if not c.startswith(\"total\")]\ncategorical_cols = [c for c in categorical_cols if c not in non_relevant]\nfor c in categorical_cols:\n\n    le = LabelEncoder()\n    train_vals = list(ctrain_df[c].values.astype(str))\n    test_vals = list(ctest_df[c].values.astype(str))\n    \n    le.fit(train_vals + test_vals)\n    \n    ctrain_df[c] = le.transform(train_vals)\n    ctest_df[c] = le.transform(test_vals)","8b69b338":"train_y = ctrain_df['totals.transactionRevenue']\ndel ctrain_df['totals.transactionRevenue']","7bf5f7f0":"def get_folds(df=None, n_splits=5):\n    unique_sessions = np.array(sorted(df['sessionId'].unique()))\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for dev_s, val_s in folds.split(X=unique_sessions, y=unique_sessions, groups=unique_sessions):\n        fold_ids.append(\n            [\n                ids[df['sessionId'].isin(unique_sessions[dev_s])],\n                ids[df['sessionId'].isin(unique_sessions[val_s])]\n            ]\n        )\n\n    return fold_ids\n","5ea3f6a9":"import lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn import metrics","730f577f":"%%time\nfeatures = [f for f in ctrain_df.columns if f not in non_relevant]\nprint(features)\n\nfolds = get_folds(df=ctrain_df, n_splits=5)\n\nimportances = pd.DataFrame()\ndev_reg_preds = np.zeros(ctrain_df.shape[0])\nval_reg_preds = np.zeros(ctest_df.shape[0])\n\nfor f, (dev, val) in enumerate(folds):\n    dev_x, dev_y = ctrain_df[features].iloc[dev], train_y.iloc[dev]\n    val_x, val_y = ctrain_df[features].iloc[val], train_y.iloc[val]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    \n    reg.fit(\n        dev_x, np.log1p(dev_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    \n    importance_df = pd.DataFrame()\n    importance_df['feature'] = features\n    importance_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    importance_df['fold'] = f + 1\n    importances = pd.concat([importances, importance_df], axis=0, sort=False)\n    dev_reg_preds[val] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    dev_reg_preds[dev_reg_preds < 0] = 0\n    preds = reg.predict(ctest_df[features], num_iteration=reg.best_iteration_)\n    preds[preds < 0] = 0\n    val_reg_preds += np.expm1(preds)\/len(folds)\nprint('RMSE=' ,metrics.mean_squared_error(np.log1p(train_y), dev_reg_preds) ** .5)\n","0930668c":"val_reg_preds.shape","a0f4c77c":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","8f0ece50":"ctest_df[\"PredictedLogRevenue\"] = val_reg_preds\nsubmission = ctest_df.groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"sum\"}).reset_index()\nsubmission[\"PredictedLogRevenue\"] = np.log1p(submission[\"PredictedLogRevenue\"])\nsubmission[\"PredictedLogRevenue\"] =  submission[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsubmission.to_csv(\"baseline.csv\", index=False)\nsubmission.head()","bf50f821":"# Response Variable","fa462dfa":"Besides the response variable 'totals.transactionRevenue', the training set also has a column 'trafficSource.campaignCode'  which doesn't exist in test set.","43dfc853":"# LightBM","67cceebb":"The Revenue is typically long-tail distributed but the effective revenue which is greater than 0 is approximately norally distributed. We also check the missing values in the valid Revenue rows as follow.","cf24a90d":"# EDA\n## Missing Value Detection","7b8d72bd":"# Discrete Variables Processing - One Hot Encoding","30d3d7f7":"We remove the columns with over 95% missing values.","93fe5f0b":"+ In addition, all numerical features including visitNumber, pageviews, and hits are measured in same scale level. Therefore, we don't need to normalize them.\n+ All numerical variables we concern are long-tail distributed and continuous, therefore we can't use Correlation Coefficient to measure the correlations. Therefore, we decide to build a baseline tree-based model to measure the feature importance.\n","dddbafff":"# Data Imput","fca50b67":"# Numerical Feature Distribution","c5ef6d87":"We found the 'sessionId' exists duplicate which is wired because it is supposed to be unique as identifier.","d3c67b69":"## Difference Between Train and Test Dataset","7911d15f":"The rest of columns with missing values are trafficSource.keyword and trafficSource.referralPath.","64a11fc1":"# Feature Importance","cabd9bbf":"## TimeStamp\/Date Conversion  ","6bf73dee":"## Constant Variable Detection and Removal","9d06e227":"# Prediction and Submission","b131622d":"# Numerical Variables Processing\nConsidering the 'totals' of nunerical variables, we convert them into numerical type of float and replace the NAs in 'totals.transactionRevenue' with 0.","afa36805":"We use Label Encoding to save memory because of the tree-based models.","6a319d9a":"# Cross Validation ","34c0820b":"According  to the long-tail distribution of 'totals.pageviews', we replace the missing values with medians. "}}