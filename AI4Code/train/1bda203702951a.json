{"cell_type":{"39ce86dc":"code","746e8aef":"code","e537d472":"code","d02cf8d8":"code","98f72719":"code","55efbd54":"code","fc9d1286":"code","62dcab2d":"code","a9f32eb2":"code","652ee994":"code","e1085512":"code","78e05416":"markdown","9538e09f":"markdown","7a22e103":"markdown","efc8ac4a":"markdown","62ebc084":"markdown","5653d505":"markdown","f0f54ccb":"markdown","f667a77d":"markdown","02f6a806":"markdown","a67220fd":"markdown","672ecb90":"markdown"},"source":{"39ce86dc":"import os\nimport shutil\n# Dataset parameters:\nINPUT_DIR = os.path.join('..', 'input')\n\nDATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2020')\nTEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\nTRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\nTRAIN_LABELMAP_PATH = os.path.join(DATASET_DIR, 'train.csv')\nupload_tar_or_plainImages = True #set False to upload plain images","746e8aef":"import numpy as np\nimport PIL\nfrom PIL import Image\nimport tensorflow as tf\n\ndef load_image_tensor(image_path):\n  return tf.convert_to_tensor(\n      np.array(PIL.Image.open(image_path).convert('RGB')))\n\ndef to_hex(image_id) -> str:\n  return '{0:0{1}x}'.format(image_id, 16)\n\ndef get_image_path(subset, image_id):\n  name = to_hex(image_id)\n  return os.path.join(DATASET_DIR, subset, name[0], name[1], name[2],'{}.jpg'.format(name))","e537d472":"#for file upload, download, decode\nimport base64\nimport io\n!pip install PyGithub requests\nfrom github import Github\nfrom pprint import pprint\n\ndef get_repo(username=\"uname\",password=\"pass\"):\n    # authenticate to github\n    g = Github(username, password)\n    # get the authenticated user\n    user = g.get_user()\n    repos = user.get_repos()\n    !git config --global user.email \"XXXXX@gmail.com\"\n    return(repos[0]) #return the first available repo, change index as per your needs","d02cf8d8":"import csv\nImgPerClass_columns = ['imagecount','path']\ndef write_images_perclass_csv(count_path_dictionary,csv_file):\n    try:\n        with open(csv_file, 'w') as csvfile:\n            writer = csv.DictWriter(csvfile, fieldnames=ImgPerClass_columns)\n            writer.writeheader()\n            for imgcnt, pathArray in count_path_dictionary.items():\n                for imgpath in pathArray[0]:\n                    writer.writerow({'path':imgpath, 'imagecount':imgcnt})#path becomes the key as it is unique\n            csvfile.close()\n    except IOError:\n        print(\"I\/O error while creating csv file\")\n    \ndef upload_csv_gitGCP(repo,bucket_name,csvfilename):\n    try:\n        if(upload_to_git):\n            with open(csvfilename, 'rb') as csvFile:\n                repo.create_file(csvfilename, \"initial commit\", csvFile.read())\n        if(upload_to_gcp):\n            upload_blob(bucket_name, csvfilename, csvfilename)\n    except:\n        print(\"upload CSV error\")","98f72719":"import pandas as pd\nimport pathlib\nfrom pathlib import Path\nfrom collections import defaultdict\ndef prepare_imageGroups():\n    traincsv = pd.read_csv(DATASET_DIR + '\/train.csv')#Remove head to upload whole dataset\n    sorteddf = traincsv.groupby('landmark_id').count()\n    imgdict = defaultdict(list)\n    for landmark,imgcnt in (traincsv.groupby('landmark_id').count()).iterrows():\n        landmarks = traincsv.loc[traincsv['landmark_id'] == landmark]\n        imgidlist = list(landmarks['id'])\n        imgdict[imgcnt['id'].astype(str)].extend(imgidlist)#image count in a class becomes key\n    #replace imageid with image path\n    for imgcnt, landmark_list in imgdict.items():\n        imgdict[imgcnt] = [Path(get_image_path('train',int(landmark,16))) for landmark in landmark_list]\n        print(\"total {} images under all classes having {} images in each class\".format(len(imgdict[imgcnt]),imgcnt))\n        #some sets have 30k images, divide them into smaller lists\n        setsize = len(imgdict[imgcnt])\/3000 + 1\n        imgdict[imgcnt] = np.array_split(imgdict[imgcnt],setsize)\n    return imgdict","55efbd54":"import random\nimport subprocess\n\ndef upload_imageset_repo(image_paths,repo):\n  #image_paths = [x for x in pathlib.Path(TRAIN_IMAGE_DIR).rglob('*.jpg')]\n  for index,image_path in enumerate(image_paths):\n    #print(index, \"original image:\",load_image_tensor(image_path).shape, tf.size(load_image_tensor(image_path)), image_path)\n    resized_image = tf.image.resize(load_image_tensor(image_path), (224,320), method='bilinear', preserve_aspect_ratio=True)\n    #print(\"resized image:\", resized_image.shape, tf.size(resized_image),\"\\n\")\n\n    np_array = resized_image.numpy()#.eval(session=tf.compat.v1.Session())\n    imgarray = Image.fromarray(np_array.astype('uint8'),mode='RGB')\n    filepath = os.path.normpath(image_path).replace(DATASET_DIR+\"\/\",'')  \n    try:\n        directory = os.path.dirname(filepath)\n        pathlib.Path(directory).mkdir(parents=True, exist_ok=True) \n        imgarray.save(filepath)\n    except:\n        print(\"file save failed: \",filepath)\n\n    if(upload_tar_or_plainImages == False): #False to upload plain images\n        try:\n            with open(filepath, \"rb\") as imageFile:\n                repo.create_file(filepath, \"initial commit\", imageFile.read())\n                imageFile.close()\n            subprocess.call(['rm', filepath])\n        except:\n            print(\"Git repo file create exception \", filepath)","fc9d1286":"import tarfile\nupload_to_git = True\nupload_to_gcp = False\ndef create_tarzip(directory_to_tar,tarfilename): \n    #os.system(\"!find \"+directory_to_tar+\" -name *.jpg | tar -cvzf \"+tarfilename+\" -T -\")\n\n    def make_tarfile(output_filename, source_dir):\n        with tarfile.open(output_filename, \"w:gz\") as tar:\n            tar.add(source_dir, arcname=os.path.basename(source_dir))\n    make_tarfile(tarfilename,directory_to_tar)\n    try:\n        if(upload_to_git):\n            with open(tarfilename, \"rb\") as tarFile:\n                repo.create_file(tarfilename, \"initial commit\", tarFile.read())\n                tarFile.close()\n        if(upload_to_gcp):\n            upload_blob(bucket_name, tarfilename, tarfilename)\n        #os.system(\"!rm -rf \"+directory_to_tar+\"train\/\")\n    except:\n        print(\"Git repo file create exception \", tarfilename, Path(tarfilename).stat().st_size>>20 )\n    subprocess.run(['rm', tarfilename])\n    subprocess.call(['rm', '-rf', directory_to_tar+'train'])\n    #shutil.rmtree(directory_to_tar+\"train\")\n    \n#get all tar files\n#untar and mege folders, similar to original dataset\ndef get_tarzip_from_repo(repo):\n    !git clone https:\/\/github.com\/jkreddy123\/gld.git\n    basedirpath = \"\/kaggle\/working\/gld\"\n    tar_paths = [x for x in pathlib.Path(basedirpath).rglob('*.tar.gz')]\n    for tar_path in tar_paths:\n        subprocess.call(['tar', '-xvkf', tar_path]) # use -P if error with \/","62dcab2d":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\ndef plot_images(image_paths,samples_to_plot):\n    plt.figure()\n    f, axarr = plt.subplots(samples_to_plot,1) \n\n    #plot few samples, by downloading from GiHub\n    #image_paths used from the previous cell\n    img_paths = random.sample(image_paths,samples_to_plot)\n    for index, img_path in enumerate(img_paths):\n        filepath = os.path.normpath(img_path).replace(DATASET_DIR+\"\/\",'')\n        git_file = repo.get_contents(filepath)\n        file_data_string = git_file.content\n        image_data = base64.b64decode(file_data_string)\n        image = Image.open(io.BytesIO(image_data))\n\n        axarr[index].imshow(image)\n    plt.draw()\n    plt.show()\n    plt.close()","a9f32eb2":"def delete_files_in_repo(repo):\n    !git clone https:\/\/github.com\/jkreddy123\/gld.git\n    basedirpath = \"\/kaggle\/working\/gld\"\n    if(upload_tar_or_plainImages == False):\n        image_paths = [x for x in pathlib.Path(basedirpath).rglob('*.jpg')]\n    else:\n        image_paths = [x for x in pathlib.Path(basedirpath).rglob('*.tar.gz')]#*.tar.gz incase of tarballs\n    for image_path in image_paths:\n      filepath = str(image_path).replace(basedirpath+\"\/\",\"\")#format as per repo path\n      print(\"removing {} from git\".format(str(filepath)))\n      #os.system(\"!git rm -f \"+filepath)\n      try:\n        contents = repo.get_contents(filepath)\n        repo.delete_file(contents.path, \"remove file\", contents.sha)\n      except:\n        print(\"repo file delete failed: \",filepath)\n    #!git commit\n    #!git push -f origin master\n    shutil.rmtree(basedirpath,ignore_errors=True)","652ee994":"import pandas_profiling as pp\n# Set your own project id here\nPROJECT_ID = 'mlkaggle-288509'\nbucket_name = 'gldv2-train-validate-1580470tfrecords' #should be lower case letters\n\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)\n\ndef create_bucket(dataset_name):\n    \"\"\"Creates a new bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.create_bucket(dataset_name)\n    print('Bucket {} created'.format(bucket.name))\n\ndef upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}.'.format(\n        source_file_name,\n        destination_blob_name))\ndef list_blobs(bucket_name):\n    \"\"\"Lists all the blobs in the bucket. https:\/\/cloud.google.com\/storage\/docs\/\"\"\"\n    blobs = storage_client.list_blobs(bucket_name)\n    for blob in blobs:\n        print(blob.name)\n        \ndef download_to_kaggle(bucket_name,destination_directory,file_name):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    blobs = storage_client.list_blobs(bucket_name)\n    for blob in blobs:\n        full_file_path = os.path.join(destination_directory, blob.name)\n        blob.download_to_filename(full_file_path)\n \n#from colab use the below lines to upload file to GCP bucket\n#from google.colab import auth\n#auth.authenticate_user()\n#project_id = 'XXXXXXXX'\n#bucket_name = 'gldv2-train-validate-1580470tfrecords'\n#!gcloud config set project {project_id}\n#!gsutil cp  ReducedGLD2020.tar.gz gs:\/\/{bucket_name}\/ReducedGLD2020.tar.gz","e1085512":"#log the time elapsed, useful when running in background\/batch\nfrom timeit import default_timer as timer\nfrom datetime import timedelta\nstart = timer()\nrepo = get_repo(username=\"jkreddy123\",password=\"XXXXX\")\nimagedict = prepare_imageGroups()\nfor images_per_landmark, image_paths_arr in imagedict.items():\n    for index, image_paths in enumerate(image_paths_arr):\n        print(\"uploading {} images of classes having {} images each\".format(len(image_paths),images_per_landmark))\n        upload_imageset_repo(image_paths,repo)\n        create_tarzip(\"\/kaggle\/working\/\",str(images_per_landmark)+\"-\"+str(index)+\"images\"+str(len(image_paths))+\".tar.gz\")\n \nif(False):\n    csv_file = \"images_per_class.csv\"\n    write_images_perclass_csv(imagedict,csv_file)\n    upload_csv_gitGCP(repo,bucket_name,csv_file)\n    upload_csv_gitGCP(repo,bucket_name,\"train.csv\")#take care of input path name\n    get_tarzip_from_repo(repo)#get all tarfiles and unzip into same folder structure\n    plot_images(image_paths,4)#download the uploaded images and plot\n    delete_files_in_repo(repo)#clear local and github repo\n\nend = timer()\nprint('time taken to upload files to GitHub ', timedelta(seconds=end-start))","78e05416":"# Main code\n*Note:* Replace XXXX with your GitHub account details, for the below code to work.","9538e09f":"# Preprocess images into groups, for k-fold training later\nCreate one tar file for each set of images, having same number of images per class\/landmark for ex: if landmark 9 & 10, has 2 images each in traning set. All these 4 images will be part of one tar\/group file.\nThis way, we can train model with all classes having only 2 images once, and in next round train model with all classes having 3 images in training set etc ....This will create 430 groups. However we shall combine few sets having less image count per landmark, to restrict groups to around 50\ndict = {imgcnt: list of image paths}","7a22e103":"# Tarzip and upload to Git or GCP\nTar all the files in this set, upload to Git and clear all the local files. Clearing local files is important to not to exceed working directory limits. Files paths need to be relative paths, before sending to tar. This helps while untar into some other VM or floder path. Use below command to untar and merge all files under same tree structure\n> !ls gld\/*.gz |xargs -n1 tar -P -xvzf","efc8ac4a":"# File Upload\/Download to GitHub from Kaggle Notebooks\n![githubkaggle.jpeg](attachment:githubkaggle.jpeg)\nComputer vision challenge such as this, with 100GB sized dataset and a state of the art base model, will hit many developers straight out of the park. Relieing on local bandwidth and computing power will no longer an option. and cloud is the way to go. However cloud charges can be an inhibiting factor to rapidly prototype multiple models on this kind of challenge. \n\nIt will be a blessing to be able to use both kaggle and colab runtimes along with GitHub.\nThe following solution can be used in current GLD or any other future projects and can certainly add a feather to ones *data engineering* capabilities. This can help in overcoming the limitations of Kaggle\/Colab by using GitHub as an intermediate storage. \n\n> This notebook is an experiment in trying GitHub as one more tool to be considered in data pre\/processing activites along with GCP Cloud, and not as a complete alternative.\n\n# Things required to understand this notebook\n* Python\/Conversions btwn(Numpy, Pandas, strings, tensors)\n* GitHub API\/Account\n* Basic level of Tensorflow usage\n* Kaggle\/Colab directory structure\n* Matplotlib to view images for sanity check\n* UNIX\/bash commands \n\n# GitHub Vs GCP\n* Data preprocessing activities involves reading the dataset multiple times. This can generate huge data transfer charges from the cloud\/GCP. Using GitHub incurs no data transfer and storage chages.\n* Training a model involves reading\/writing checkpoints, 100's of EPOCHs over dataset, which can again generate considerable cloud storage and data transfer bills.\n* However GCP can be an alternative for solutions beyond GitHub's 100GB\/repo or 100MB\/file limit.\n* Competition rules should allow one to upload dataset to public repositories.\n\n# What does this notebook do?\nResize each image to size ~(224,320) preserving the aspect ratio, before uploading to GitHub. After resizing the images, the resultant dataset size is ~15GB, which can easily be accomadated in either kaggle\/colab or can be downloaded locally. Either plain images or tar of multiple files can be uploaded\n\n# Motivation: create small dataset\nGLD dataset size, 99GB, has been a huge limiting factor in many ways to compete in this image recognition challenge, in a way\n* It occupies the whole available user disk space *on kaggle*, which means, no space left out for data manipulation or intermediate data generation related activites.(5GB working directory data limit anyway)\n* Not enough Kaggle computing time to validate various ensembling or stacking models\n* SSH is not allowed from Kaggle notebook to Colab notebook server(worked briefly and stopped working from next day)\n* Can't use *google colab* to train custom models, as the GLD competition dataset size will easily cross the available limits(77GB for CPU\/TPU, 37GB for GPU)\n* Cannnot edit\/remove files in input Dataset. Can only remove entire input dataset, but not individual files, to make space for new edited files.\n\n\n*Note* Internet switch should be \"on\" to use this notebook","62ebc084":"Below, reads entire image list from the dataset and iterate through each image and upload to Git. To check the upload process with just few(100) images, we have taken head(100) into dataframe from csv, before for loop is added. Edit this line, to upload the entire dataset. \n\n# Caution\nUploading individual images can be a quite time taking. To make this script run faster, tar and upload one single file, for every ~10,000 image files. set upload_tar_or_plainImages to True to upload tar files.\n","5653d505":"# Copy CSV files to dataset","f0f54ccb":"# Image resize and save to working\/Git directory\nIf in case of plain images, upload to repository is done below. If uploading tar files, it is done in a create_tarzip function","f667a77d":"Pls Upvote, comment if you find this analysis and code useful in your Machine Learning tasks.\nThanks.","02f6a806":"# Plot images for sanity check\nDownload few samples images from GitHub and display them using matplotlib for sanity check and correctness. Change samples_to_plot to download\/plot desired number of images.","a67220fd":"# Deleting all files from Git\nCall below function to delete all the files in the repository. It can be used while testing or to remove complete dataset after project is over. Clone repo to local and prepare list of filenames to delete. CD into repository home directory and edit email, username\/XXXX, and gitname\/gld(twice) before calling","672ecb90":"# Upload to GCP storage"}}