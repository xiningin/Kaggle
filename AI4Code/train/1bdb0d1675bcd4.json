{"cell_type":{"a9d32c78":"code","2895de9b":"code","dff819ce":"code","407ad928":"code","bf7c3ee1":"code","b1a82568":"code","5c955e6b":"code","6748625a":"code","444827a0":"code","1a9ece10":"code","c20bb335":"code","3735db10":"code","5fcb9b84":"code","39edb8cb":"code","0ff90136":"code","b637420a":"code","a12e1d6a":"code","e4d46b8b":"code","97ee959e":"code","7bf6beff":"code","0675a9a6":"code","1459aa17":"code","a6483dd3":"code","1e3054c6":"code","4a535be3":"code","0dbf2556":"code","c0fbd805":"code","62dd7a93":"code","b253d49f":"code","fcf2c46d":"code","7808444c":"code","06148ce3":"code","00a42db4":"code","d7ab9a22":"code","2cf45fd9":"code","6c43f123":"code","860449f4":"code","7db2a517":"code","17167635":"code","6f39048a":"code","c4d367ca":"code","6a792c3f":"code","fa99c264":"code","93e96c32":"code","a5a870ae":"code","d343391e":"code","1f3cc171":"code","8fcd3281":"code","8952123d":"code","3529f61d":"code","3687a6a3":"code","4216cbb3":"code","cb4b6eda":"code","5bd495a3":"code","98bee6e9":"code","2d56129c":"code","055a84c1":"code","5f68853f":"code","691d0096":"code","e949c73a":"code","823cb8dd":"code","101cab38":"code","27cdb759":"code","7193afa5":"code","c9351172":"code","384ec2b8":"code","80e60873":"code","9385f906":"code","4c5f31b9":"code","4fbc6008":"code","e69ce2c2":"code","808f5735":"code","201fbcfc":"code","1585243d":"code","6baeebde":"code","4f4d50bd":"code","27de85af":"markdown","0e718e38":"markdown","0b42cf88":"markdown","7814cd55":"markdown","71c224c5":"markdown","6d467d1c":"markdown","e76166eb":"markdown","8b7a4dbf":"markdown","f25c2e7a":"markdown","68a76e54":"markdown","ee320718":"markdown","5f0cc99f":"markdown","a252647e":"markdown","84c4e667":"markdown","76c04aea":"markdown","91772a8c":"markdown","4ab9c252":"markdown","f1ccaf02":"markdown","3eb9bb91":"markdown","81cc3927":"markdown","0bcd6222":"markdown","8a1319c9":"markdown","16f7d956":"markdown","e60e70cd":"markdown","f1251632":"markdown"},"source":{"a9d32c78":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","2895de9b":"#importation of data manipulation, plotting and grid formating Modules\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n#importing feature engineering and preprocessing modules\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Importing Classifier Modules\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.dummy import DummyClassifier\n\n#metrics evaluation Modules\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import precision_recall_curve,roc_auc_score\nfrom sklearn.metrics import f1_score,auc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","dff819ce":"#Reading in the data\ntrain = pd.read_csv('\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv')","407ad928":"#size of train and test data respectively\ntrain.shape, test.shape","bf7c3ee1":"#snapshot of data\ntrain.head()","b1a82568":"test.head()","5c955e6b":"train.info()","6748625a":"test.info()","444827a0":"#checking for dupliated rows\ntrain.duplicated().sum()","1a9ece10":"#snapshot at numerical variables\ntrain.describe()","c20bb335":"#select numerical predictor variables\nnum_cols = ['Age','Vintage','Avg_Account_Balance']\nnum_data=train[num_cols]","3735db10":"# Plotting histograms plots for the variables\nnum_data.hist(bins=30, color = 'cyan',figsize=(12,12))\nplt.show()","5fcb9b84":"#snapshot at the categorical columns\ntrain.describe(include='O')","39edb8cb":"#A look the target variable\ntrain['Is_Lead'].value_counts()","0ff90136":"plt.suptitle('Pie chart showing distribution of clients by percentage in the Is_lead Target variable', fontsize=20)\ntrain['Is_Lead'].value_counts().plot(kind='pie',autopct=\"%.1f%%\",shadow=True,startangle=90,figsize=(7,7), fontsize=12,ylabel='')\nplt.show()","b637420a":"train_labels = train['Is_Lead']","a12e1d6a":"#Plot of probabbilty density function Age vs Is_lead for bank customers\nfig, ax = plt.subplots(figsize=(8,8))\n\nsns.kdeplot(train[train[\"Is_Lead\"]==0][\"Age\"],shade=True, color=\"red\", label=\"No Lead\", ax=ax)\nsns.kdeplot(train[train[\"Is_Lead\"]==1][\"Age\"],shade=True, color=\"green\", label=\"Yes Lead\",ax=ax)\n\nax.set_xlabel(\"Age\")\nax.set_ylabel(\"Density\")\n\nfig.suptitle(\"Age vs. Is_Lead for bank customers\")\nax.legend();\n","e4d46b8b":"#Plot of probabbilty density function Vintage vs Is_lead for bank customers\nfig, ax = plt.subplots(figsize=(8,8))\n\nsns.kdeplot(train[train[\"Is_Lead\"]==0][\"Vintage\"],shade=True, color=\"blue\", label=\"No Lead\", ax=ax)\nsns.kdeplot(train[train[\"Is_Lead\"]==1][\"Vintage\"],shade=True, color=\"red\", label=\"Yes Lead\",ax=ax)\n\nax.set_xlabel(\"Vintage\")\nax.set_ylabel(\"Density\")\n\nfig.suptitle(\"Vintage vs. Is_Lead for bank customers\")\nax.legend();","97ee959e":"#Plot of probabbilty density function Avg_Account_Balance vs Is_lead for bank customers\nfig, ax = plt.subplots(figsize=(8,8))\n\nsns.kdeplot(train[train[\"Is_Lead\"]==0][\"Avg_Account_Balance\"],shade=True, color=\"purple\", label=\"No Lead\", ax=ax)\nsns.kdeplot(train[train[\"Is_Lead\"]==1][\"Avg_Account_Balance\"],shade=True, color=\"green\", label=\"Yes Lead\",ax=ax)\n\nax.set_xlabel(\"Avg_Account_Balance\")\nax.set_ylabel(\"Density\")\n\nfig.suptitle(\"Avg_Account_Balance vs. Is_Lead for bank customers\")\nax.legend();","7bf6beff":"cat_data = train.select_dtypes(exclude=[np.number])\nprint(list(cat_data.columns))","0675a9a6":"train.groupby(['Gender','Is_Lead']).ID.count()","1459aa17":"fig, ax = plt.subplots(figsize=(8,8))\nsns.countplot(x=\"Is_Lead\", hue=\"Gender\", data=train,ax=ax)\nax.legend(title=\"gender\")\nax.set_xticklabels([\"Unlikely Lead\", \"Likely Lead\"])\nax.set_xlabel(\"Is_Lead\")\nfig.suptitle(\"Gender vs. Is_Lead Variable categories\");","a6483dd3":"#Has many unique values\ntrain['Region_Code'].nunique()","1e3054c6":"train.groupby(['Region_Code','Is_Lead']).ID.count()","4a535be3":"train.groupby(['Occupation','Is_Lead']).ID.count()","0dbf2556":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x=\"Is_Lead\", hue=\"Occupation\", data=train,ax=ax)\nax.legend(title=\"Occupation\")\nax.set_xticklabels([\"Unlikely Lead\", \"Likely Lead\"])\nax.set_xlabel(\"Is_Lead\")\nfig.suptitle(\"Occupation vs. Is_Lead Variable categories\");","c0fbd805":"train.groupby(['Channel_Code','Is_Lead']).ID.count()","62dd7a93":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x=\"Is_Lead\", hue=\"Channel_Code\", data=train,ax=ax)\nax.legend(title=\"Channel_Code\")\nax.set_xticklabels([\"Unlikely Lead\", \"Likely Lead\"])\nax.set_xlabel(\"Is_Lead\")\nfig.suptitle(\"Channel_Code vs. Is_Lead Variable categories\");","b253d49f":"train.groupby(['Credit_Product','Is_Lead']).ID.count()","fcf2c46d":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x=\"Is_Lead\", hue=\"Credit_Product\", data=train,ax=ax)\nax.legend(title=\"Credit_Product\")\nax.set_xticklabels([\"Unlikely Lead\", \"Likely Lead\"])\nax.set_xlabel(\"Is_Lead\")\nfig.suptitle(\"Credit_Product vs. Is_Lead Variable \");","7808444c":"train.groupby(['Is_Active','Is_Lead']).ID.count()","06148ce3":"fig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(x=\"Is_Lead\", hue=\"Is_Active\", data=train,ax=ax)\nax.legend(title=\"Is_Active\")\nax.set_xticklabels([\"Unlikely Lead\", \"Likely Lead\"])\nax.set_xlabel(\"Is_Lead\")\nfig.suptitle(\"Is_Active vs. Is_Lead target variable \");","00a42db4":"#select the numerical columns\nnum_data=train.loc[:,train.dtypes!=np.object]\n# heat map of correlation of features\ncorrelation_matrix = num_data.corr()\nfig = plt.figure(figsize=(8,4))\nsns.heatmap(correlation_matrix,vmax=1,annot=True) \nplt.show()","d7ab9a22":"# Make a new dataframe for polynomial features\npoly_features = train[['Age', 'Vintage', 'Is_Lead']]\npoly_features_test = test[['Age', 'Vintage']]\n\npoly_target = poly_features['Is_Lead']\n\npoly_features = poly_features.drop(columns = ['Is_Lead'])\n\n                                  \n# Create the polynomial object with specified degree\npoly_transformer = PolynomialFeatures(degree = 3,include_bias=False)","2cf45fd9":"# Train the polynomial features\npoly_features=poly_transformer.fit_transform(poly_features)\n\n# Transform the test features\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)\nprint('Polynomial Features shape: ', poly_features_test.shape)","6c43f123":"#This creates a considerable number of new features. To get the names we have to use the polynomial features get_feature_names method.\nprint(list(poly_transformer.get_feature_names(input_features = ['Age', 'Vintage'])[:9]))","860449f4":"# Create a dataframe of the features \npoly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['Age', 'Vintage']))\n\n# Add in the target\npoly_features['Is_Lead'] = poly_target\n\n# Find the correlations with the target\npoly_corrs = poly_features.corr()\nfig = plt.figure(figsize=(10,6))\nsns.heatmap(poly_corrs,vmax=1,annot=True) \nplt.show()","7db2a517":"poly_features.head()","17167635":"# Put test features into dataframe\npoly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['Age', 'Vintage']))","6f39048a":"poly_features_test.head()","c4d367ca":"# Merge polynomial features into training dataframe\npoly_features['ID'] = train['ID']\ntrain_poly = train.merge(poly_features, on = 'ID', how = 'left')\n\n# Merge polnomial features into testing dataframe\npoly_features_test['ID'] = test['ID']\ntest_poly = test.merge(poly_features_test, on = 'ID', how = 'left')","6a792c3f":"train_poly.shape, test_poly.shape","fa99c264":"# Align the training and testing data, keeps only columns present in both dataframes\n# Align the dataframes\ntrain_poly, test_poly = train_poly.align(test_poly, join = 'inner', axis = 1)","93e96c32":"train_poly.shape, test_poly.shape","a5a870ae":"#columns to drop before  modelling \ncols_to_drop= ['ID','Region_Code']","d343391e":"train = train_poly.drop(columns=cols_to_drop,axis=1)\ntesting = test_poly.drop(columns=cols_to_drop,axis=1).copy()","1f3cc171":"# combining  train and test dataset for preprocessing\ndata_combined = [train, testing]","8fcd3281":"for dataset in data_combined:\n    dataset.Credit_Product.fillna(value='No',inplace=True)","8952123d":"#Shows that there are nolonger null values in the train set.\ntrain['Credit_Product'].isnull().sum()","3529f61d":"#Shows that there are nolonger null values in the test set as well.\ntesting['Credit_Product'].isnull().sum()","3687a6a3":"#selection of categorical variables from the dataset\ncat_cols = [cname for cname in dataset.columns \n                    if  dataset[cname].dtype == \"object\"]","4216cbb3":"train= pd.get_dummies(train,columns=cat_cols,drop_first=True)\ntesting= pd.get_dummies(testing,columns=cat_cols,drop_first=True)","cb4b6eda":"train.head()","5bd495a3":"testing.head()","98bee6e9":"#Select numerical columns\nnum_cols = [cname for cname in dataset.columns \n            if dataset[cname].dtype in ['int64', 'float64']]","2d56129c":"#define the scaler object\nscaler = StandardScaler()\n\n# scale and transform numeric features n\ntrain[num_cols] = scaler.fit_transform(train[num_cols])\ntesting[num_cols] = scaler.transform(testing[num_cols])","055a84c1":"#final training features\ntrain.head()","5f68853f":"#final test features\ntesting.head()","691d0096":"#splitting the data to create validation splits.\n#stratified sampling ensures the even distribution of classes of the target variable.\nX_train , X_val , y_train , y_val = train_test_split(train,train_labels,test_size = 0.2, random_state = 1, stratify=train_labels)","e949c73a":"X_train.shape , y_train.shape, X_val.shape,y_val.shape","823cb8dd":"#Function to train and evaulate a given model\ndef fit_Evaulate(model,X_train,y_train):\n    \n    model.fit(X_train, y_train)\n    y_pred=model.predict_proba(X_val)[:,1]#predict and retrieve probabilities for positive class\n    y_pred2 = model.predict(X_val)# predict class values\n    auc = roc_auc_score(y_val, y_pred)\n    confusion = confusion_matrix(y_val,y_pred2)\n    f_score = f1_score(y_val,y_pred2)\n    \n    return auc,confusion,f_score","101cab38":"#train and make  a preduction using a dumpy classifier\nfrom sklearn.dummy import DummyClassifier\nmodel = DummyClassifier()\ndummy_auc,Dammy_confusion,f_score_dummy = fit_Evaulate(model,X_train,y_train)\nprint(\"Model performance metric report:\")\nprint('The dummy ROC AUC for validation data: ROC AUC=%.3f' %(dummy_auc))\nprint('The harmonic mean of  the dumy classifier, f1_score {:.2f}'.format(f_score_dummy))","27cdb759":"RF_clf = RandomForestClassifier(class_weight=\"balanced\",random_state=3)\nRF_auc,RF_confusion,f_score_RF = fit_Evaulate(RF_clf,X_train,y_train)\nprint(\"Model performance metric report:\")\nprint('Random Forest ROC AUC for vaalidation data: ROC AUC=%.2f' %(RF_auc))\nprint('The harmonic mean for the Random forest Classifier, f1_score : {:.2f}'.format(f_score_RF))","7193afa5":"GB_clf = GradientBoostingClassifier(random_state=2)\nGB_auc,GB_confusion,f_score_GB = fit_Evaulate(GB_clf,X_train,y_train)\nprint(\"Model performance metric report:\")\nprint('Gradient Boosting ROC AUC for validation data: ROC AUC=%.2f' %(GB_auc))\nprint('The harmonic mean for the Random forest Classifier, f1_score : {:.2f}'.format(f_score_GB))","c9351172":"XGB_clf = XGBClassifier(random_state=4)\nXGB_auc,XGB_confusion,f_score_XGB = fit_Evaulate(XGB_clf,X_train,y_train)\nprint(\"Model performance metric report:\")\nprint('Gradient Boosting ROC AUC for validation data: ROC AUC=%.2f' %(XGB_auc))\nprint('The harmonic mean for the XGB Classifier, f1_score : {:.2f}'.format(f_score_XGB))","384ec2b8":"print(\"\\n The Dammy class matrix  \\n {} \".format(Dammy_confusion))\nprint(\"\\n The Random Forest confusion matrix  \\n {} \".format(RF_confusion))\nprint(\"\\n The Gradient Boosting confusion matrix  \\n {} \".format(GB_confusion))\nprint(\"\\n The XGBClassifier confusion matrix   \\n {}\".format(XGB_confusion))","80e60873":"#taking the model that had better results being the random forest model and fitting it\nmodel = RandomForestClassifier(class_weight=\"balanced\",random_state=3)\nmodel.fit(X_train, y_train)","9385f906":"# precision-recall curve and f1 for an imbalanced dataset using the random forest model \ny_pred=model.predict_proba(X_val)[:,1]#predict and retrieve probabilities for positive class\n#The F-Measure can be calculated by calling the f1_score() function \n#that takes the true class values and the predicted class values as arguments.\n\ny_pred2 = model.predict(X_val)# predict class values\n# calculate precision and recall for each threshold\nrf_precision, rf_recall, _ = precision_recall_curve(y_val, y_pred)\n# calculate scores\nrf_f1, rf_auc = f1_score(y_val, y_pred2), auc(rf_recall, rf_precision)\n# summarize scores\nprint('Gradient Boosting metrics: f1=%.3f auc=%.3f' % (rf_f1,rf_auc))\n# plot the precision-recall curves\nno_skill = len(y_val[y_val==1]) \/ len(y_val)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\nplt.plot(rf_recall,rf_precision, marker='.', label='Random_Forest')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","4c5f31b9":"#classification report at the defualt threshold of 0.5 for the positive class\ny_pred = np.where(y_pred > 0.5, 1,0)\nprint(classification_report(y_val,y_pred))\n","4fbc6008":"#sample prediction for the first 10 data points\ny_pred[:10] ","e69ce2c2":"#evaluate the model on the performance of the validation set\ny_pred=model.predict_proba(X_val)[:,1]\ny_pred = np.where(y_pred > 0.2, 1,0)\nprint(classification_report(y_val,y_pred))# summary of all the three metrics","808f5735":"#sample prediction for the first 10 data points\ny_pred[:10] ","201fbcfc":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist())","1585243d":"#evaluate the model on the performance of the test set\ny_pred_test=model.predict_proba(testing)[:,1]\ny_pred_test = np.where(y_pred_test > 0.2, 1,0)","6baeebde":"#Create a submission file on Kaggle\nSubmit = pd.DataFrame({\n        \"ID\": test[\"ID\"],\n        \"Is_Lead\": y_pred_test\n    })\n\nSubmit.to_csv('Submit.csv', index=False)","4f4d50bd":"Submit.head(10).set_index('ID')","27de85af":"# **4. MODEL SELECTION AND TRAINING**\n\nWe shall not use the train accuracy for evaulating our models here since it's not so informative in an imbalanced dataset like the one we are dealing with here","0e718e38":"**2. DATA INSPECTION,PREPARATION AND FORMATTING**","0b42cf88":"Credit Card Lead Prediction\n\nHappy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings.\n\nThe bank also cross-sells products to its existing customers and to do so they use different kinds of communication like telecasting, e-mails, recommendations on net banking, mobile banking, etc.\n\nIn this case, the Happy Customer Bank wants to cross-sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards.\n\nNow, the bank is looking for your help in identifying customers that could show higher intent towards a recommended credit card, given:\n\nThis dataset was part of May 2021 Jobathon conducted my analytics vidhya, for more info check:https:\/\/datahack.analyticsvidhya.com\/contest\/job-a-thon-2\/","7814cd55":"Lets preprocess the numerical features in both datasets..\nwe are going to standardize the features for better performance during the modeling phase","71c224c5":"# **5. Evaluation and interpretation**","6d467d1c":"From the code above, we can deduce: \n1. We have 7 variables of type object(categorical columns) and 4 numerical columns\n2. We have one column of credit_product with missing values\n3. We have a total of 10 variables to act as our predictor variables and one target variable Is_lead.","e76166eb":"# **1. Importation of libraries to be used in this project**","8b7a4dbf":"Since avg_Account_Balance appears to have a very low correlation to the target compared to the age and vintage, we shall not consider it in the feature engineering phase.we shall consider Age and Vintage in feature engineering","f25c2e7a":"We can see that there are no repeated IDs in the ID variable,the  gender with the highest frequency is male,and that the highest number of it's clients are self employed among other factors that can be noted.","68a76e54":"**OBJECTIVE: \nTo build a predictive model that will be able to classify customers who will show a higher intent towards a credit card.**","ee320718":"**Python \u00b7 JOB-A-THON - May 2021- Credit Card Lead Prediction**","5f0cc99f":"**It can be observed that the three most important features that the model considered in classifying which customers are credit card lead and which are not were occupation_salaried, occupation_self_employed and channel code**","a252647e":"# **WORKING WITH IMBALANCE DATA**","84c4e667":"# **3. EXPLANATORY DATA ANALYSIS**","76c04aea":"The test dataset also has some missing values in the Credit product column. we shall fill in these missing values as well","91772a8c":"**Since we are interested in having a higher recall that is , identifying all the potential Is_lead clients who may also include some false positives. It's important for us to have higher recall since it's an imbalanced data set for our business case here and that we need to therefore identify  all the positive cases(all is lead customers). we shall threshold the probabilities from the default 0.5 for the positive class to 0.2.**","4ab9c252":"This notebook is still a work in  progress, i will be updating it as soon as i have time\n**please!you can upvote for this note book if you find it insightful.**","f1ccaf02":"**This is a binary classification problem considering our target column which has two possibilities, a possible lead customer and not a possible lead.We are going to build a binary classification model to solve this problem.\nThis is going to be an end to end classification machine learning model for this problem.**","3eb9bb91":"Considering the visualizations above, it can be concluded that there are some visible relationships between the target variable and the categorical variables.","81cc3927":"# **3. FEATURE ENGINEERING AND SELECTION PHASE.**","0bcd6222":"Lets explore the relationship between categorical variables and the target variable.","8a1319c9":"From the graphs above, it can be observed that the most variation between the distributions are those between target variable and age, target variable and vintage while the variations in the distribution between the target variable and the ave_account_balance is minimum.","16f7d956":"We can see that there is a class imbalance between the Is_lead and No_lead labels. Those are who are no leads are more than twice as many as those who are have been labeled as Yes leads. This class imbalance will be highly taken into consideration when modelling.","e60e70cd":"**Data columns**\nID :- Unique Identifier for a row\n\nGender :- Gender of the Customer\n\nAge :- Age of the Customer (in Years)\n\nRegion_Code :- Code of the Region for the customers\n\nOccupation :- Occupation Type for the customer\n\nChannel_Code :- Acquisition Channel Code for the Customer (Encoded)\n\nVintage :- Vintage for the Customer (In Months)\n\nCredit_Product :- If the Customer has any active credit product (Home loan,Personal loan, Credit Card etc.)\n\nAvgAccountBalance :- Average Account Balance for the Customer in last 12 Months\n\nIs_Active :- If the Customer is Active in last 3 Months\n\n**Target**\n**Is_Lead :- If the Customer is interested for the Credit Card\n0 : Customer is not interested\n1 : Customer is interested**","f1251632":"**It can be observed that the random forest model had the better results in the confusion matrix for the positive class. It has identified more True postives than the rest of the models**"}}