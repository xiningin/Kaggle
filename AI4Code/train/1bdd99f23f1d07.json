{"cell_type":{"9e3871c5":"code","231819f6":"code","bcf3e3c2":"code","9f63bc82":"code","767ffd52":"code","27becbd3":"code","64b21e4e":"code","85db1073":"code","4d3c21b1":"code","2f0188ef":"code","e49a55bc":"code","209ecac6":"code","4da07fc7":"code","5d0c79d6":"code","a5be5a03":"code","94e9c82e":"code","8c4a75f2":"code","5786b747":"code","e833ae9c":"code","7b05ecb6":"code","d7f55ae2":"code","95a4a1fd":"code","a766a7f2":"code","e413ecb7":"code","dd8716d8":"code","57600b3c":"code","d69a0d54":"markdown","142ebc9c":"markdown","66f189dc":"markdown","d49c1de3":"markdown","4365979f":"markdown","e2f0759c":"markdown","2d6c441d":"markdown"},"source":{"9e3871c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","231819f6":"historical_tx_df = pd.read_csv('..\/input\/historical_transactions.csv')","bcf3e3c2":"historical_tx_df.info(memory_usage='deep')","9f63bc82":"for dtype in ['float','int','object']:\n    selected_dtype = historical_tx_df.select_dtypes(include=[dtype])\n    mean_usage_b = selected_dtype.memory_usage(deep=True).mean()\n    mean_usage_mb = mean_usage_b \/ 1024 ** 2\n    print(\"Average memory usage for {} columns: {:03.2f} MB\".format(dtype,mean_usage_mb))","767ffd52":"def mem_usage(pandas_obj):\n    if isinstance(pandas_obj,pd.DataFrame):\n        usage_b = pandas_obj.memory_usage(deep=True).sum()\n    else: # we assume if not a df it's a series\n        usage_b = pandas_obj.memory_usage(deep=True)\n    usage_mb = usage_b \/ 1024 ** 2 # convert bytes to megabytes\n    return \"{:03.2f} MB\".format(usage_mb)","27becbd3":"'''\nFunction to count unique values in the columsn. \n\nArguments:\ndf_columns: List of columns in `his_int` dataframe\n'''\n\ndef count_unique( df_columns):\n    for i in df_columns:\n        print(\"Number of unique vals in\",i,\"are\",len(his_int[i].unique()))","64b21e4e":"his_int = historical_tx_df.select_dtypes(include=['integer'])","85db1073":"count_unique(his_int.columns)","4d3c21b1":"converted_obj = pd.DataFrame()\n\nfor col in his_int.columns:\n    num_unique_values = len(his_int[col].unique())\n    num_total_values = len(his_int[col])\n    if num_unique_values \/ num_total_values < 0.5:\n        converted_obj.loc[:,col] = his_int[col].astype('category')\n    else:\n        converted_obj.loc[:,col] = his_int[col]","2f0188ef":"print(mem_usage(his_int))\nprint(mem_usage(converted_obj))","e49a55bc":"converted_obj.info()","209ecac6":"optimized_hist_df = pd.DataFrame()","4da07fc7":"optimized_hist_df[converted_obj.columns] = converted_obj\nmem_usage(optimized_hist_df)","5d0c79d6":"his_int = historical_tx_df.select_dtypes(include=['float'])","a5be5a03":"his_int_na = his_int.category_2.fillna(6)\nconv_obj = his_int_na.astype('int')\nmem_usage(conv_obj)","94e9c82e":"conv_obj2 = conv_obj.astype('category')\nmem_usage(conv_obj2)","8c4a75f2":"his_int[\"category_2\"] = conv_obj2","5786b747":"optimized_hist_df[his_int.columns] = his_int\nmem_usage(optimized_hist_df)\n","e833ae9c":"his_int = historical_tx_df.select_dtypes(include=['object'])\nhis_int.head()","7b05ecb6":"count_unique(his_int.columns)","d7f55ae2":"converted_obj = pd.DataFrame()\n\nfor col in his_int.columns:\n    num_unique_values = len(his_int[col].unique())\n    num_total_values = len(his_int[col])\n    if num_unique_values\/num_total_values < 0.33:\n        converted_obj.loc[:,col] = his_int[col].astype('category')\n    else:\n        converted_obj.loc[:,col] = his_int[col]","95a4a1fd":"print(mem_usage(his_int))\nprint(mem_usage(converted_obj))","a766a7f2":"optimized_hist_df[converted_obj.columns] = converted_obj\nmem_usage(optimized_hist_df)","e413ecb7":"optimized_hist_df['purchase_date'] =  pd.to_datetime(historical_tx_df.purchase_date, format='%Y%m%d %H:%M:%S')\nmem_usage(optimized_hist_df)","dd8716d8":"optimized_hist_df.head()","57600b3c":"per_df_red = (float(mem_usage(optimized_hist_df).split()[0]) \/ float(mem_usage(historical_tx_df).split()[0]) )*100\nper_df_red","d69a0d54":"**As per the above computation, you can clearly see that we have reduced the 13+GB pandas dataframe to 1064MB dataframe. This leads to a reduction in size to around 8%.**","142ebc9c":"### Let's read the historical_transactions\n- Read the csv as a pandas dataframe using `read_csv()` function of pandas\n- Get info about each of the attribute of that dataframe along with memory usage using `info()` function.","66f189dc":"The integer columns memory usage is now reduced from 1332 MB to 222 MB.","d49c1de3":"### Preprocessing of Integer columns \nThe dataframe reads these columns as integer `city_id, installments, merchant_category_id, month_lag, state_id,subsector_id`. As you can see most of the columns are contain specific ids. These can be mapped to categorical data. \n\nIn the next couple of code cells, I will be downcasting the integer columns to category. ","4365979f":"## Reduction in the size of our dataframe","e2f0759c":"### Preprocessing of 29 million transaction records\n\nThe dataset provides access to 2 files. One of the files `new_merchant_transactions.csv` contains over 29million records. Pandas is considered as a great choice for solving data science problems. It provides some unique features like automatically detecting the datatype of attributes but it's not that efficient. Here comes the role of data engineers to correctly cast datatype for each of the attribute. \n\nIn this notebook, I will be sharing my approach of reducing the size of a pandas df for further Machine Learning\/Deep Learning implementation on it. \nI achieved reduction in the dataframe's memory usage to around 8% of original size. ","2d6c441d":"### Creation of optimized transaction records dataframe\nWe will be creating a new dataframe `optimized_hist_df` and mapping all the processed columns to that dataframe. "}}