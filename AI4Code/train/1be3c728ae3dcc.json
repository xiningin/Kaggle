{"cell_type":{"6c236960":"code","2a39894f":"code","9956d2ee":"code","8ef9a5dc":"code","da9a4a42":"code","c29265a4":"code","1e918bc9":"code","1ca51e2b":"code","31412ea7":"code","966e15a3":"code","6d30529b":"code","0a59337d":"code","d374effc":"code","30896cbf":"code","f5d4ef4b":"code","1eb7e7a8":"code","f1afcde1":"code","c963e7a2":"code","9d12b39f":"code","946a26d7":"code","692d7a6a":"code","ca3aeb0c":"code","b54add29":"code","f50f450b":"code","8da57ab6":"code","eb364fa2":"code","f6f6de44":"code","5f4b6e4a":"code","8c632231":"code","3584590a":"markdown","b2e15e1d":"markdown"},"source":{"6c236960":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a39894f":"df = pd.read_csv('..\/input\/hamlet-english-german-portuguese\/hamlets.csv')\ndf.head()","9956d2ee":"#Save for next work in case I need to remove some column.\n\n# remove useless column\n#df = df.drop(['link'], axis=1)                   \n#df.head(2)","8ef9a5dc":"languages_list = df['language'].unique()\nprint(len(languages_list))                           # prints the number of languages ","da9a4a42":"#Code by Allie K.  https:\/\/www.kaggle.com\/blankaf\/text-and-sentiment-analysis-vector-model\n\nsns.set_context(\"notebook\", font_scale=1.0)\nsns.set_palette('cubehelix',4)         \nplt.figure(figsize=(13,3))\nplt.title('Hamlet by Number of Languages')\ng = sns.countplot(df['language'])\nrotg = g.set_xticklabels(g.get_xticklabels(), rotation=10)                                        \ndf.head()","c29265a4":"port = df[(df['language']=='Portuguese')].reset_index(drop=True)\nport.tail()","1e918bc9":"eng = df[(df['language']=='English')].reset_index(drop=True)\neng.tail()","1ca51e2b":"#Codes by Ragnar https:\/\/www.kaggle.com\/rowhitswami\/starter-load-stopwords\n\ndef get_stopwords_list(stop_file_path):\n    \"\"\"load stop words \"\"\"\n    \n    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n        stopwords = f.readlines()\n        stop_set = set(m.strip() for m in stopwords)\n        return list(frozenset(stop_set))","31412ea7":"stopwords_path = \"\/kaggle\/input\/hamlet-english-german-portuguese\/hamlets.csv\"\nstopwords = get_stopwords_list(stopwords_path)","966e15a3":"stopwords[0:10]","6d30529b":"print(f\"Total number of stopwords: {len(stopwords)}\")","0a59337d":"from gensim.models import Word2Vec\nimport gensim","d374effc":"corpus = ['Follow my Mother',\n 'durch euer L\u00e4cheln zu verstehen zu geben scheint.',\n 'Que especie de imbecil me julga ent\u00e3o? Sou a s...',\n 'When I had seene this hot loue on the wing',\n 'Der Prologus tritt hier hervor',\n 'anbefohlen habt, liegen l\u00e4\u00dft?',\n 'amor infeliz que assim o faz padecer.',\n 'The thin and wholsome blood so did it mine']","30896cbf":"stop_words = ['Follow',\n 'euer',\n 'zu',\n 'Que',\n 'de',\n 'me',\n 'a',\n 'this',\n 'on',\n 'the',\n 'Der',\n 'hier',\n 'o',\n 'and',\n 'so',\n 'it']","f5d4ef4b":"#Codes by Ragnar https:\/\/www.kaggle.com\/rowhitswami\/starter-load-stopwords\n\ndef remove_stop_words(corpus):\n    results = []\n    for text in corpus:\n        tmp = text.split(' ')\n        for stop_word in stop_words:\n            if stop_word in tmp:\n                tmp.remove(stop_word)\n        results.append(\" \".join(tmp))\n        \n    return results","1eb7e7a8":"corpus = remove_stop_words(corpus)","f1afcde1":"#Codes by Ragnar https:\/\/www.kaggle.com\/rowhitswami\/starter-load-stopwords\n\nwords = []\nfor text in corpus:\n    for word in text.split(' '):\n        words.append(word)\n        \nwords = set(words)","c963e7a2":"words","9d12b39f":"\"\"\"Data Generation\"\"\"\n\nword2int = {}\n\nfor i,word in enumerate(words):\n    word2int[word] = i\n    \nsentences = []\nfor sentence in corpus:\n    sentences.append(sentence.split())\n    \nWINDOW_SIZE = 2\n\ndata = []\nfor sentence in sentences:\n    for idx, word in enumerate(sentence):\n        for neighbor in sentence[max(idx - WINDOW_SIZE, 0): min(idx + WINDOW_SIZE, len(sentence) + 1)]:\n            if neighbor !=word:\n                data.append([word, neighbor])","946a26d7":"import pandas as pd\nfor text in corpus:\n    print(text)\n\ndf = pd.DataFrame(data, columns = ['input', 'label'])","692d7a6a":"df.head(10)","ca3aeb0c":"df.shape","b54add29":"word2int","f50f450b":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nx = tf.placeholder(shape=[None, 2], dtype=tf.float32)","8da57ab6":"\"\"\"Define Tensorflow Graph\"\"\"\n\nONE_HOT_DIM = len(words)\n\n# function to convert numbers to one hot vectors\ndef to_one_hot_encoding(data_point_index):\n    one_hot_encoding = np.zeros(ONE_HOT_DIM)\n    one_hot_encoding[data_point_index] = 1\n    return one_hot_encoding\n\nX = [] # input word\nY = [] # target word\n\nfor x, y in zip(df['input'], df['label']):\n    X.append(to_one_hot_encoding(word2int[ x ]))\n    Y.append(to_one_hot_encoding(word2int[ y ]))\n\n# convert them to numpy arrays\nX_train = np.asarray(X)\nY_train = np.asarray(Y)\n\n# making placeholders for X_train and Y_train\nx = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\ny_label = tf.placeholder(tf.float32, shape=(None, ONE_HOT_DIM))\n\n# word embedding will be 2 dimension for 2d visualization\nEMBEDDING_DIM = 2 \n\n# hidden layer: which represents word vector eventually\nW1 = tf.Variable(tf.random_normal([ONE_HOT_DIM, EMBEDDING_DIM]))\nb1 = tf.Variable(tf.random_normal([1])) #bias\nhidden_layer = tf.add(tf.matmul(x,W1), b1)\n\n# output layer\nW2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, ONE_HOT_DIM]))\nb2 = tf.Variable(tf.random_normal([1]))\nprediction = tf.nn.softmax(tf.add( tf.matmul(hidden_layer, W2), b2))\n\n# loss function: cross entropy\nloss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), axis=[1]))\n\n# training operation\ntrain_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)","eb364fa2":"\"\"\"Training\"\"\"\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init) \n\niteration = 20000\nfor i in range(iteration):\n    # input is X_train which is one hot encoded word\n    # label is Y_train which is one hot encoded neighbor word\n    sess.run(train_op, feed_dict={x: X_train, y_label: Y_train})\n    if i % 3000 == 0:\n        print('iteration '+str(i)+' loss is : ', sess.run(loss, feed_dict={x: X_train, y_label: Y_train}))","f6f6de44":"# Now the hidden layer (W1 + b1) is actually the word look up table\nvectors = sess.run(W1 + b1)\nprint(vectors)","5f4b6e4a":"\"\"\"Word Vector in Table\"\"\"\n\nw2v_df = pd.DataFrame(vectors, columns = ['x1', 'x2'])\nw2v_df['word'] = words\nw2v_df = w2v_df[['word', 'x1', 'x2']]\nw2v_df","8c632231":"#After 'set' type is unordered I gave up","3584590a":"#Re-writing Hamlet: My mother verstehe durch L\u00e4cheln zu geben scheint. Esp\u00e9cie imbecil julga ent\u00e3o?\n\n#Amor infeliz que assim faz padecer. The thin wholsome blood did mine.\n\n#Shakespeare would hate this version.","b2e15e1d":"#Dead Bodies in Shakespeare.\n\n![](https:\/\/www.researchgate.net\/profile\/Dustin-Stoltz\/publication\/334847086\/figure\/fig8\/AS:963468723249175@1606720252520\/Closeness-to-death-and-dead-bodies-in-Shakespeare-Note-plays-are-colored-by-how-they.png)https:\/\/www.researchgate.net\/figure\/Closeness-to-death-and-dead-bodies-in-Shakespeare-Note-plays-are-colored-by-how-they_fig8_334847086"}}