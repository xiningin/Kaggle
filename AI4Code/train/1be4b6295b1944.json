{"cell_type":{"1e1bbb75":"code","ea13c761":"code","1fd80ab9":"code","9356884a":"code","20364ecd":"code","70cddce8":"code","2ec709bb":"code","b82418a0":"code","ca065abb":"code","c99b2e35":"code","1d893743":"markdown","80d192f1":"markdown","f0925454":"markdown","1e04a6b8":"markdown","60988ead":"markdown","fd27c3fd":"markdown","46e5c1be":"markdown","ff00e430":"markdown","15f82078":"markdown","ca6e9d96":"markdown"},"source":{"1e1bbb75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","ea13c761":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')\n\ndf.head()","1fd80ab9":"encoder = LabelEncoder()\n\n# Apply the encoder to each of the columns\ndf = df.apply(encoder.fit_transform)\n\ndf.head()","9356884a":"# Seperating our target and features\nX = df.drop(columns = ['class'])\ny = df['class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n\nprint(\"X_train = \", X_train.shape)\nprint(\"y_train = \", y_train.shape)\nprint(\"X_test = \", X_test.shape)\nprint(\"y_test = \", y_test.shape)","20364ecd":"# First, we calculate the prior probability which is just the percentage of data points belonging to the mentioned class\n# For example, if our training dataset has 60% edible mushrooms, then the prior probability will be 0.6 when calculating \n# in the testing side.\n\ndef prior(y_train, label):\n    \n    total_points = y_train.shape[0]\n    class_points = np.sum(y_train == label)\n    \n    return class_points\/float(total_points)","70cddce8":"## Next, we will define a function to calculate the conditional probability that we will use then to calculate the\n## likelihood,\n\ndef cond_prob(X_train, y_train, feat_col, feat_val, label):\n    \"\"\"\n    In this function, we will calculate the conditional probability which will be used to calculate likelihood.\n    The value it returns is of the form \n        P(x_i | y = C)\n    which is the probability of the current feature (given by feat_col x_i) having the current value (given by feat_val)\n    given that it belongs to the target class C\n    \n    Effectively, it reduces to the form\n        all points belongig to class C which have the given value for the feature column \/ all points belonging to class C\n    \"\"\"\n    \n    # Getting all the \n    X_filtered = X_train[y_train == label]\n    \n    numerator = np.sum(X_filtered[feat_col] == feat_val)\n    denominator = np.sum(y_train == label)\n    \n    return numerator\/float(denominator)","2ec709bb":"## Now time to calculate the posterior probability and make predictions\n\ndef predict(X_train, y_train, xtest):\n    \n    # Get the number of target classes\n    classes = np.unique(y_train)\n    \n    # All the features for our dataset\n    features = [x for x in X_train.columns]\n    \n    \n    # Compute posterior probabilites for each class\n    post_probs = []\n    \n    for label in classes:\n        \n        # Since, posterior = prior * likelihood\n        # We'll calculate likelihood by calculating the product of the conditional probabilities for each of the features\n        \n        likelihood = 1.0\n        \n        for f in features:\n            cond = cond_prob(X_train, y_train, f, xtest[f], label)\n            likelihood *= cond\n        \n        prior_prob = prior(y_train, label)\n        \n        posterior = prior_prob * likelihood\n        \n        post_probs.append(posterior)\n        \n    # Return the label for which the posterior probability was the maximum\n    prediction = np.argmax(post_probs)\n    \n    return prediction    ","b82418a0":"# First, let's check on a random example\n\nrand_example = 6\n\noutput = predict(X_train, y_train, X_test.iloc[rand_example])\n\nprint(\"Naive Bayes Classifier predicts \", output)\nprint(\"Current Answer \", y_test.iloc[rand_example])","ca065abb":"## Now, we'll check the results on each of the test data point and calculate \n## an accuracy-based score for our classifier\n\ndef accuracy_score(X_train, y_train, xtest, ytest):\n    \n    preds = []\n    \n    for i in range(xtest.shape[0]):\n        pred_label = predict(X_train, y_train, xtest.iloc[i])\n        preds.append(pred_label)\n        \n    preds = np.array(preds)\n    \n    accuracy = np.sum(preds == ytest)\/ytest.shape[0]\n    \n    return accuracy","c99b2e35":"print(\"Accuracy Score for our classifier == \", accuracy_score(X_train, y_train, X_test, y_test))","1d893743":"## Load the Dataset","80d192f1":"## Building Our Classifier\n\nFinally the time has come when we start building our classifer step-by-step. As we have seen earlier, to classify any data point in the Naive Bayes Classifier, we need the likelihood and prior probability which we will compare and assign the class to the one with higher posterior probability.","f0925454":"# Naive Bayes Classifier from Scratch\n\u200b\nIn this notebook, we will look at the overview of the Naive Bayes classifier and implement it from scratch and try to fit it on this mushroom classification dataset to predict if a mushroom is poisonous or edible.","1e04a6b8":"**This brings us to the end of this notebook. We built a naive bayes classifier from scratch, trained it on our data and then tested it to find out that it has an accuracy of 99.7% which is really very good.**","60988ead":"The probabilistic model for this classifier looks like:\n\\begin{equation}\nP(C_k | x) = \\frac{P(C_k) * P(x | C_k)}{P(x)}\n\\end{equation}\n\n\nIn plain English, using Bayesian probability terminology, the above equation can be written as\n\\begin{equation}\nposterior = \\frac{prior * likelihood}{evidence}\n\\end{equation}","fd27c3fd":"## Time to test our classifer","46e5c1be":"Here,, we can see from the dataset that \n\n- All the features are categorical and\n- the values for the features need to be encoded into numeric values for our classifier.\n","ff00e430":"## Encode the features into Numerical data\n\nWe will use the LabelEncoder for this specific task which creates ordinal values for all the categorical features. \n\nWhile it may not be the best approach but it is really simple","15f82078":"As we can see the values for each column are from 0 to the number of categories for that feature. Next, we will define our functions for prior and likelihood to compute the posterior probability and weigh against each of the target variable to see which data point fits where.","ca6e9d96":"## Split the dataset into train and test parts"}}