{"cell_type":{"dfc25bf3":"code","cb2a4a32":"code","f7f3141c":"code","f2ef9d7d":"code","94ad72e8":"code","a42a67df":"code","86eea012":"code","718a142d":"code","fdff4359":"code","0ba0b099":"code","94da0b53":"code","5fd51dc1":"code","84fa6937":"code","3991eb5b":"code","e2f13332":"code","611e3b1c":"code","df6aa90c":"code","feb80efe":"code","2ad8660b":"code","4ee24655":"code","d8e6a662":"code","e3c81d7d":"code","90242fff":"code","624d67f4":"code","e72de5f6":"code","1e2a9f8e":"code","f6acc893":"code","19997cb1":"code","39ac6c2b":"code","d62701c5":"code","7e0b0688":"code","041fd941":"code","ccafb5cf":"code","7bbb0f3c":"code","ddcf1eef":"code","774d8ad3":"code","d8d0b8cd":"code","c2dcb375":"code","79980da5":"code","7d29f7c2":"code","52a84f6f":"code","b6e234ed":"code","5e31ee9e":"code","4bdded72":"markdown","2e2f2324":"markdown","b4b37285":"markdown","1279ffaf":"markdown","c35b22e1":"markdown"},"source":{"dfc25bf3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \n\nfrom sklearn.metrics import mean_squared_error,mean_squared_log_error\nfrom sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score,RepeatedKFold\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer,RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\n\nimport xgboost as xgb\nimport lightgbm as lgb\nimport sklearn.ensemble as ensemble\nimport sklearn.metrics as metrics\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge,LogisticRegressionCV,RidgeCV,LassoCV,ElasticNetCV,OrthogonalMatchingPursuit,ElasticNet,LassoLarsCV,BayesianRidge\nfrom sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC,SVR\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.kernel_ridge import KernelRidge\n\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom category_encoders.sum_coding import SumEncoder\nfrom category_encoders.m_estimate import MEstimateEncoder\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom category_encoders.helmert import HelmertEncoder\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.james_stein import JamesSteinEncoder\nfrom category_encoders.one_hot import OneHotEncoder\nfrom scipy.special import boxcox1p\nfrom bayes_opt import BayesianOptimization","cb2a4a32":"warnings.filterwarnings('ignore')","f7f3141c":"import os\nprint(os.listdir(\"..\/input\"))","f2ef9d7d":"train=pd.read_csv('..\/input\/train_1.csv')\ntest=pd.read_csv('..\/input\/test_1.csv')\nsubmission=pd.read_csv('..\/input\/sample_submission_1.csv')","94ad72e8":"train.head()","a42a67df":"test.head()","86eea012":"submission.head()","718a142d":"train.describe()","fdff4359":"train.dtypes","0ba0b099":"train.isna().sum()","94da0b53":"#Imputing missing value with the relevant total price\ntrain.total_price=train.total_price.fillna(469.5375)","5fd51dc1":"print(train.isna().sum().sum())\nprint(test.isna().sum().sum())","84fa6937":"train.columns","3991eb5b":"#New Feature Creation functions\n\ndef gen_count_id(train,test,col,name):\n    temp=train.groupby(col)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name]=train[name].astype(float)\n    test[name]=test[name].astype(float)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test\n\ndef gen_average_units(train,test,col,name):\n    temp=train.groupby(col)['units_sold'].mean().reset_index().rename(columns={'units_sold':name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test\n\ndef gen_average_price(train,test,col,price='base_price',name='name'):\n    temp=train.groupby(col)[price].mean().reset_index().rename(columns={price:name})\n    train=pd.merge(train,temp,how='left',on=col)\n    test=pd.merge(test,temp,how='left',on=col)\n    train[name].fillna(np.median(temp[name]),inplace=True)\n    test[name].fillna(np.median(temp[name]),inplace=True)\n    return train,test","e2f13332":"train,test = gen_count_id(train,test,col=['sku_id','store_id'],name='count_id_sku_store') #Genearting count of records per 'sku-id & store-id' \ntrain,test = gen_count_id(train,test,col=['sku_id'],name='count_id_sku') #Genearting count of records per 'sku-id'\ntrain,test = gen_count_id(train,test,col=['store_id'],name='count_id_store') #Genearting count of records per 'store-id'\n\ntrain,test = gen_average_units(train,test,col=['sku_id','store_id'],name='count_sku_store_id') #Genearting average units sold per 'sku-id & store-id'\ntrain,test = gen_average_units(train,test,col=['store_id'],name='count_store_id') #Genearting average units sold per 'store-id'\ntrain,test = gen_average_units(train,test,col=['sku_id'],name='count_sku_id') #Genearting average units sold per 'sku-id'\n\ntrain,test = gen_average_price(train,test,col=['sku_id','store_id'],price='base_price',name='price_sku_store') #Genearting average base price per 'sku-id & store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id','store_id'],price='total_price',name='price_to_sku_store') #Genearting average total price per 'sku-id & store-id'\ntrain,test = gen_average_price(train,test,col=['store_id'],price='base_price',name='price_store_id') #Genearting average base price per 'store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id'],price='base_price',name='price_sku_id') #Genearting average base price per 'sku-id'\ntrain,test = gen_average_price(train,test,col=['store_id'],price='total_price',name='price_to_store_id') #Genearting average total price per 'store-id'\ntrain,test = gen_average_price(train,test,col=['sku_id'],price='total_price',name='price_to_sku_id') #Genearting average total price per 'sku-id'","611e3b1c":"#Converting week feature\nle = OrdinalEncoder()\ntrain['week_1']=le.fit_transform(train['week'])\nle = OrdinalEncoder()\ntest['week_1']=le.fit_transform(test['week'])+130\n\n#Creating week number feature\ntrain['week_num']=train.week_1%52\ntest['week_num']=test.week_1%52\n\ntrain['week_num1']=train.week_1%4\ntest['week_num1']=test.week_1%4\n\n# Encoding 'week' it using sine and cosine transform; considering it as a cyclic feature \ntrain['week_sin'] = np.sin(2 * np.pi * train['week_1'] \/ 52.143)\ntrain['week_cos'] = np.cos(2 * np.pi * train['week_1'] \/ 52.143)\ntest['week_sin'] = np.sin(2 * np.pi * test['week_1'] \/ 52.143)\ntest['week_cos'] = np.cos(2 * np.pi * test['week_1'] \/ 52.143)\n\n#Creating feature: percent difference between base price and checkout price.\ntrain['price_diff_percent'] = (train['base_price'] - train['total_price']) \/ train['base_price']\ntest['price_diff_percent'] = (test['base_price'] - test['total_price']) \/ test['base_price']","df6aa90c":"train.tail()","feb80efe":"test.head()","2ad8660b":"X=train[list(set(train.columns)-set(['record_ID','units_sold','week']))]\nY= np.log1p(train['units_sold'])\nX_test=test[list(set(test.columns)-set(['record_ID','week']))]","4ee24655":"X.dtypes","d8e6a662":"X['sku_id'] = X['sku_id'].astype('category')\nX['store_id'] = X['store_id'].astype('category')","e3c81d7d":"X.info()","90242fff":"print(len(X_test.columns))\nprint(len(X.columns))","624d67f4":"print(X_test.isna().sum().sum())\nprint(X.isna().sum().sum())","e72de5f6":"category_list=['store_id','sku_id']","1e2a9f8e":"encoder_final=MEstimateEncoder()\nencoder_final.fit(X[category_list], Y)\n\ncat_enc = encoder_final.transform(X[category_list], Y)\ncontinuous_train = X.drop(columns= category_list)\nX = pd.concat([cat_enc,continuous_train],axis=1)\n\ntest_enc=encoder_final.transform(X_test[category_list])\ncontinuous_test=X_test.drop(columns= category_list)\nX_test=pd.concat([test_enc,continuous_test],axis=1)","f6acc893":"X.head()","19997cb1":"X.info()","39ac6c2b":"X_test.head()","d62701c5":"X.columns","7e0b0688":"del X['week_num1']","041fd941":"x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2,random_state=23)","ccafb5cf":"len(x_train.columns)","7bbb0f3c":"rf_base = RandomForestRegressor()\nrf_base.fit(x_train,y_train)\n\n\nrf_tuned = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=10,\n                      min_weight_fraction_leaf=0.0, n_estimators=600,\n                      n_jobs=None, oob_score=True, random_state=None,\n                      verbose=0, warm_start=False)\nrf_tuned.fit(x_train,y_train)","ddcf1eef":"model_lgb_base=lgb.LGBMRegressor(objective='regression')\nmodel_lgb_base.fit(x_train,y_train)\n\nmodel_lgb_tuned=lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.1, max_depth=30,\n              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n              min_split_gain=0.0001, n_estimators=200, n_jobs=-1,\n              num_leaves=1200, objective=None, random_state=None, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)\n\nmodel_lgb_tuned.fit(x_train,y_train)","774d8ad3":"prediction_rfb_valid=rf_base.predict(x_valid)\nprediction_rft_valid=rf_tuned.predict(x_valid)\nprediction_lgbmb_valid=model_lgb_base.predict(x_valid)\nprediction_lgbmt_valid=model_lgb_tuned.predict(x_valid)\n\nrf_base_msle=100*mean_squared_log_error(y_valid,prediction_rfb_valid)\nrf_tuned_msle=100*mean_squared_log_error(y_valid,prediction_rft_valid)\nlgbm_base_msle=100*mean_squared_log_error(y_valid,prediction_lgbmb_valid)\nlgbm_tuned_msle=100*mean_squared_log_error(y_valid,prediction_lgbmt_valid)\n\nprediction_ensemble_base=(((1-rf_base_msle)*prediction_rfb_valid)+((1-lgbm_base_msle)*prediction_lgbmb_valid))\/(2-rf_base_msle-lgbm_base_msle)\nprediction_ensemble_tuned=(((1-rf_tuned_msle)*prediction_rft_valid)+((1-lgbm_tuned_msle)*prediction_lgbmt_valid))\/(2-rf_tuned_msle-lgbm_tuned_msle)\n\nensemble_base_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_base)\nensemble_tuned_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_tuned)\n\n\nprint(\"RF Base: {}; RF Tuned: {}\".format(rf_base_msle,rf_tuned_msle))\nprint(\"LGBM Base: {}; LGBM Tuned: {}\".format(lgbm_base_msle,lgbm_tuned_msle))\nprint(\"Ensemble Base: {}; Ensemble Tuned: {}\".format(ensemble_base_msle,ensemble_tuned_msle))","d8d0b8cd":"model = lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.1, max_depth=30,\n              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n              min_split_gain=0.0001, n_estimators=100, n_jobs=-1,\n              num_leaves=1400, objective=None, random_state=None, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)\n\nmodel.fit(X,Y)","c2dcb375":"X_test.head()","79980da5":"del X_test['week_num1']","7d29f7c2":"prediction=model.predict(X_test)","52a84f6f":"final_prediction=np.round(np.expm1(prediction))\nsubmission['units_sold']=final_prediction","b6e234ed":"submission.head()","5e31ee9e":"#submission.to_csv('AV_DemandForecast_05.csv',index=False)","4bdded72":"# Problem Statement\n\nOne of the largest retail chains in the world wants to use their vast data source to build an efficient forecasting model to predict the sales for each SKU in its portfolio at its 76 different stores using historical sales data for the past 3 years on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise.\n\nHowever, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product\/SKU-store combination for the next 12 weeks accurately? If yes, then dive right in","2e2f2324":"# Variable Definition\n\nrecord_ID: Unique ID for each week store sku combination\nweek: Starting Date of the week\nstore_id: Unique ID for each store (no numerical order to be assumed)\nsku_id: Unique ID for each product (no numerical order to be assumed)\ntotal_price: Sales Price of the product \nbase_price: Base price of the product\nis_featured_sku: Was part of the featured item of the week\nis_display_sku: Product was on display at a prominent place at the store\nunits_sold(Target): Total Units sold for that week-store-sku combination","b4b37285":"# Approach (high level)\n\nConsidered this as a regression problem with 'units_sold' as a target\nGenerated following new features:\n(a) Count of records per 'sku-id','store-id' and combination of both\n(b) Average units sold per 'sku-id','store-id' and combination of both\n(c) Average base-price & total-price per 'sku-id','store-id' and combination of both\n(d) Week of the year\n(e) Week number from start of data\n(f) Week of the month\n(g) Sine & Cosine transform of week number to capture cyclic nature\n(e) Price difference percent between base price & total-price\nCategorical Encoded 'sku-id' & 'store-id' with MEstimateEncoder()\nTrained the data on RandomForest & LGBM Regressor\nTuned the above models","1279ffaf":"# Feature Engineering","c35b22e1":"# Model Building"}}