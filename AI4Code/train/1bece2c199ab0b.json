{"cell_type":{"e65e9299":"code","4ba73e4c":"code","459767ce":"code","b1a2e732":"code","a871356b":"code","c6713422":"code","03101b3f":"code","fcce5a58":"code","2ff31b1d":"code","73a98373":"code","c52130c7":"code","6ebaa0c6":"code","a7bcc08e":"code","d0ac2e3f":"code","27e756b9":"code","5c57e9b1":"code","c9b5f812":"code","ea893e84":"code","c15390ce":"code","c4783df9":"code","563b020f":"code","6152b845":"code","e43e6fb7":"code","34f922ea":"code","dfd84c34":"code","8c8a9863":"code","c1662710":"code","0f1122aa":"code","a2bd7159":"code","6320de3f":"code","aefd12cc":"code","6a906a89":"code","a210ccc9":"code","1fe21c7b":"code","89dd1a98":"code","d7b83cac":"code","470e63b5":"markdown","7c29300a":"markdown","dbe614a0":"markdown","f436cf4f":"markdown","e66daa26":"markdown","3e319931":"markdown","ac8f62bc":"markdown","43eb9f3d":"markdown","5436788f":"markdown","e6c980fb":"markdown","f58d8e57":"markdown"},"source":{"e65e9299":"# for basic operations\nimport numpy as np \n\n# Widen the display of python output\n# This is done to avoid ellipsis appearing which restricts output view in row or column\nimport pandas as pd\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n\n# for modeling \nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.model_selection import RepeatedKFold, train_test_split, RandomizedSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import  KNeighborsClassifier as knn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\nfrom sklearn.linear_model import Lasso\nfrom sklearn.svm import OneClassSVM\n\n# to avoid warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n\nfrom collections import Counter\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom IPython.core.display import HTML","4ba73e4c":"# reading the data\ndata = pd.read_csv('..\/input\/uci-semcom\/uci-secom.csv')\n\n# getting the shape of the data\n# Dataset has 1,567 rows and 592 columns\nprint(data.shape)","459767ce":"#Add a prefix to the column names for easeof understanding\ndata.columns = 'feature_' + data.columns","b1a2e732":"#Rename the time column and Pass_Fail column as they are not features \ndata.rename(columns = {'feature_Time':'Time'}, inplace = True) \ndata.rename(columns = {'feature_Pass\/Fail':'Pass_Fail'}, inplace = True)","a871356b":"#All variables except Pass\/Fail (Wether the process entity passed or not) is float. \n#Pass\/Fail is a integer variable\n\n#data.dtypes","c6713422":"# checking if the dataset contains any NULL values\n# we do have NaN values if we see the output above\n\ndata.isnull().any().any()","03101b3f":"# Replacing all the NaN values with 0 as the values correspond to the test results.\n# since, the values are not present that means the values are not available or calculated\n# Absence of a signal is assumed to be no signal in the dataset\n# so better we not take median or mean and replace them with zeros\n\ndata = data.replace(np.NaN, 0)\n\n# again, checking if there is any NULL values left\ndata.isnull().any().any()","fcce5a58":"unique_vals = data['Pass_Fail'].unique()  # [0, 1, 2]\ntargets = [data.loc[data['Pass_Fail'] == val] for val in unique_vals]","2ff31b1d":"fig = plt.figure(figsize=(20,20))\n\nplt.subplot(2, 2, 1)\nfor target in targets:\n    sns.histplot(target['feature_1'])\nplt.title('First Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 2)\nfor target in targets:\n    sns.histplot(target['feature_2'])\nplt.title('Second Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 3)\nfor target in targets:\n    sns.histplot(target['feature_3'])\nplt.title('Third Sensor Measurements', fontsize = 20)\n\nplt.subplot(2, 2, 4)\nfor target in targets:\n    sns.histplot(target['feature_4'])\nplt.title('Fourth Sensor Measurements', fontsize = 20)\n\n#sns.add_legend()\n#plt.legend()\nfig.legend(labels=['Pass','Fail'])\nplt.show()\n","73a98373":"# pie chart\n# We have highly imbalanced class with only 6.6% failures and 93.4% pass\n\nlabels = ['Pass', 'Fail']\nsize = data['Pass_Fail'].value_counts()\ncolors = ['blue', 'green']\nexplode = [0, 0.1]\n\nplt.style.use('seaborn-deep')\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.pie(size, labels =labels, colors = colors, explode = explode, autopct = \"%.2f%%\", shadow = True)\nplt.axis('off')\nplt.title('Target: Pass or Fail', fontsize = 20)\nplt.legend()\nplt.show()\ndata['Pass_Fail'].value_counts().plot(kind=\"bar\");","c52130c7":"from datetime import datetime\ndata['year'] = pd.DatetimeIndex(data['Time']).year\ndata['month'] = pd.DatetimeIndex(data['Time']).month\ndata['date'] = pd.DatetimeIndex(data['Time']).day\ndata['week_day'] = pd.DatetimeIndex(data['Time']).weekday\ndata['start_time'] = pd.DatetimeIndex(data['Time']).time\ndata['hour'] = pd.DatetimeIndex(data['Time']).hour\ndata['min'] = pd.DatetimeIndex(data['Time']).minute","6ebaa0c6":"#This consists of only year 2008\nprint(\"Year: \", data.year.unique())\n#This consists of all the months of 2008\nprint(\"Month: \", data.month.unique())\n#All the dates of the month are not there, might be related to production on certain days only\nprint(\"Date: \", data.date.unique())\n#All the weekdays of the month are here, so production happens on all 7 days\n#0 stand for Sunday, 1 for Monday ... 6 for Saturday\nprint(\"Weekday: \", data.week_day.unique())","a7bcc08e":"#We see that the failures (Pass_Fail=1) peak in August which is also the peak for pass.\n#August and September are months with most product and most failures as well\n#The failures seem to subside from September onwards post some correction \n#(May-Aug we see more failures than passes)\nsns.histplot( data[data.Pass_Fail == -1]['month'], color = 'g');\nsns.histplot( data[data.Pass_Fail == 1]['month'], color = 'r');","d0ac2e3f":"#The failures tend to decrease towards month end and is in close sync with pass population\nsns.histplot( data[data.Pass_Fail == -1]['date'], color = 'g');\nsns.histplot( data[data.Pass_Fail == 1]['date'], color = 'r');","27e756b9":"#Failures appear to be more towards start and end of the week rather than in the middle of the week\nsns.histplot( data[data.Pass_Fail == -1]['week_day'], color = 'g');\nsns.histplot( data[data.Pass_Fail == 1]['week_day'], color = 'r');","5c57e9b1":"#There is no specific trend in terms of hours, it seems to be fairly distributed\nsns.distplot( data[data.Pass_Fail == -1]['hour'], color = 'g');\nsns.distplot( data[data.Pass_Fail == 1]['hour'], color = 'r');","c9b5f812":"#There is no specific trend in terms of minutes, it seems to be fairly distributed\nsns.distplot( data[data.Pass_Fail == -1]['min'], color = 'g');\nsns.distplot( data[data.Pass_Fail == 1]['min'], color = 'r');","ea893e84":"# heatmap to get correlation\n\nplt.rcParams['figure.figsize'] = (18, 18)\nsns.heatmap(data.corr(), cmap = \"YlGnBu\")\nplt.title('Correlation heatmap for the Data', fontsize = 20)","c15390ce":"# deleting the time column \ndata = data.drop(columns = ['Time','year','month','date','week_day','start_time','hour','min'], axis = 1)","c4783df9":"#Remove the highly collinear features from data\ndef remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model \n        to generalize and improves the interpretability of the model.\n\n    Inputs: \n        x: features dataframe\n        threshold: features with correlations greater than this value are removed\n\n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n\n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i+1):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n\n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns=drops)\n\n    return x","563b020f":"#Remove columns having more than 70% correlation\n#Both positive and negative correlations are considered here\ndata = remove_collinear_features(data,0.70)","6152b845":"# separating the dependent and independent data\n\nx = data.iloc[:,:306]\ny = data[\"Pass_Fail\"]\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","e43e6fb7":"#Observation is that most of the variables distribution are right skewed with long tails and outliers \ndef draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure(figsize=(20,10))\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\n#Most the variables are approximately normally distributed except for feature_4, feature_7,\n#feature_11, feature_12, feature_15, feature_16\ndraw_histograms(data, data.iloc[:,0:15], 5, 3)","34f922ea":"# separating the dependent and independent data\n\nx = data.iloc[:,:306]\ny = data[\"Pass_Fail\"]\n\n# getting the shapes of new data sets x and y\nprint(\"shape of x:\", x.shape)\nprint(\"shape of y:\", y.shape)","dfd84c34":"# splitting them into train test and split\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1)\n\n# gettiing the shapes\nprint(\"shape of x_train: \", x_train.shape)\nprint(\"shape of x_test: \", x_test.shape)\nprint(\"shape of y_train: \", y_train.shape)\nprint(\"shape of y_test: \", y_test.shape)","8c8a9863":"# standardization\n\nfrom sklearn.preprocessing import StandardScaler\n\n# creating a standard scaler\nsc = StandardScaler()\n\n# fitting independent data to the model\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","c1662710":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX_train, Y_train = oversample.fit_resample(x_train, y_train)","0f1122aa":"# Hyperparameter tuning takes a lot of time. If this variable is False, the tuning process will be omitted and the learning will proceed \n# with the hyperparameters already obtained. If this variable is true, you can proceed with the tuning process directly.\nallow_tuning = True","a2bd7159":"if allow_tuning:\n    params_knn = {\n        'n_neighbors' : range(1, 10),\n        'weights' : ['uniform', 'distance'],\n        'algorithm' : ['auto', 'ball_tree','kd_tree'],\n        'p' : [1,2]\n    }\n    model_knn = knn()\n    search_knn = GridSearchCV(model_knn, params_knn, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_knn.best_params_)\n    \n    params_logistic = {\n        'max_iter': [2000],\n        'penalty': ['l1', 'l2'],\n        'C': np.logspace(-4, 4, 20),\n        'solver': ['liblinear']\n    }\n    model_logistic = LogisticRegression()\n    search_logistic = GridSearchCV(model_logistic, params_logistic, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_logistic.best_params_)\n    \n    params_svc = {'kernel': ['rbf'], 'gamma': [i\/10000 for i in range(90, 110)], 'C': range(50, 80, 10), 'probability': [True]}\n    model_svc = SVC()\n    search_svc = GridSearchCV(model_svc, params_svc, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    print(search_svc.best_params_)\n    \n    params_rf = {\n        'n_estimators': [95, 100, 105],\n        'criterion':['entropy'],\n        'bootstrap': [True, False],\n        'max_depth': [40, 45, 50],\n        'max_features': [4, 5, 6],\n        'min_samples_leaf': [1, 2, 3],\n        'min_samples_split': [9, 10, 11],\n        'random_state': [734]}\n    model_rf = RandomForestClassifier()\n    search_rf = GridSearchCV(model_rf, params_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1).fit(X_train, Y_train)\n    search_rf.best_params_['random_state']=242\n    search_rf.best_estimator_.random_state=242\n    print(search_rf.best_params_)","6320de3f":"if allow_tuning:\n    model_knn = search_knn.best_estimator_\n    model_logistic = search_logistic.best_estimator_\n    model_svc = search_svc.best_estimator_\n    model_rf = search_rf.best_estimator_\n    model_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                              colsample_bynode=1, colsample_bytree=0.8,\n                              enable_categorical=False, eval_metric='logloss', gamma=0.8,gpu_id=-1, importance_type=None, interaction_constraints='',\n                              learning_rate=0.15, max_delta_step=0, max_depth=5,\n                              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n                              n_estimators=15, n_jobs=-1, num_parallel_tree=1, predictor='auto',\n                              random_state=0, reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1,\n                              subsample=0.8, tree_method='exact', use_label_encoder=False,\n                              validate_parameters=1, verbosity=0)\nelse:\n    model_knn = knn(algorithm='auto', \n                    n_neighbors=9,\n                    p=1, \n                    weights='uniform')\n    \n    model_logistic = LogisticRegression(C=0.08858667904100823,\n                                        max_iter=2000, \n                                        penalty='l2', \n                                        solver='liblinear')\n    model_svc = SVC(C=70,\n                    gamma=0.0106,\n                    kernel='rbf',\n                    probability=True)\n    \n    model_rf = RandomForestClassifier(bootstrap=True,\n                                      criterion='entropy',\n                                      max_depth=50, max_features=6, \n                                      min_samples_leaf=1, \n                                      min_samples_split=10, \n                                      n_estimators=100,\n                                      random_state=734)\n    \n    model_xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                              colsample_bynode=1, colsample_bytree=0.8,\n                              enable_categorical=False, eval_metric='logloss', gamma=0.8,gpu_id=-1, importance_type=None, interaction_constraints='',\n                              learning_rate=0.15, max_delta_step=0, max_depth=5,\n                              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n                              n_estimators=15, n_jobs=-1, num_parallel_tree=1, predictor='auto',\n                              random_state=0, reg_alpha=1e-05, reg_lambda=1, scale_pos_weight=1,\n                              subsample=0.8, tree_method='exact', use_label_encoder=False,\n                              validate_parameters=1, verbosity=0)\n\nmodels = {\n    'knn': model_knn,\n    'logistic': model_logistic,\n    'svc': model_svc,\n    'rf': model_rf,\n    'xgb': model_xgb\n}","aefd12cc":"import copy\n\n# goal: The number of models to combine.\n# estimaors: empty list.\n# voting: voting method.\ndef select_models(start, cnt, goal, estimators, voting):\n    if cnt == goal:\n        estimators_copy = copy.deepcopy(estimators)\n        voting_name = f'{voting}_' + '_'.join([i[0] for i in list(estimators_copy)])\n        models[voting_name] = VotingClassifier(estimators=estimators_copy, voting=voting)\n        return\n    for i in range(start, 5):\n        estimators.append(list(models.items())[i])\n        select_models(i + 1, cnt + 1, goal, estimators, voting)\n        estimators.pop()","6a906a89":"# create voting models\nselect_models(0, 0, 2, [], 'hard')\nselect_models(0, 0, 3, [], 'hard')\nselect_models(0, 0, 4, [], 'hard')\nselect_models(0, 0, 5, [], 'hard')\n\nselect_models(0, 0, 2, [], 'soft')\nselect_models(0, 0, 3, [], 'soft')\nselect_models(0, 0, 4, [], 'soft')\nselect_models(0, 0, 5, [], 'soft')","a210ccc9":"# Dictionary for storing results for each model.\nresult_by_model = pd.DataFrame({'model name': models.keys(), 'model': models.values(), 'score': 0})","1fe21c7b":"# Cross-validation progresses for all models.\nfor name, model in models.items():\n    result_by_model.loc[result_by_model['model name'] == name, 'score'] = cross_val_score(model, X_train,Y_train,cv=5).mean()","89dd1a98":"# Cross validation scores of all models.\nresult_by_model.sort_values('score', ascending=False).reset_index(drop=True)","d7b83cac":"model_name = 'soft_knn_rf_xgb'\nmodels[model_name].fit(X_train, Y_train)\ny_pred = models[model_name].predict(x_test).astype('int')","470e63b5":"<a id='preliminary_eda'><\/a>\n# Preliminary Data Exploration\n## Install and Import Libraries","7c29300a":"## Table of Contents\n* [Dataset Information](#data_information)\n    - Data description\n    - Files\n    - Steps\n* [Preliminary Data Exploration](#preliminary_eda)\n    - Install and Import Libraries\n    - Load Data\n    - General Dataset Information\n* [Exploratory Data Analysis](#eda)\n* [Data Cleaning](#data_cleaning)\n    - Drop Unused Column\n    - Split Data\n    - Standard Scaling\n* [Modeling](#modeling)","dbe614a0":"<a id='data_cleaning'><\/a>\n# Data Cleaning","f436cf4f":"## Load Data","e66daa26":"![maxence-pira-KpHTwMls5cs-unsplash.png](attachment:e865d32d-0528-4621-bd65-cab58e2e005b.png)\n<br>\nSource: Maxence Pira - Unsplash\n<br>\n#### Signals that acquired from sensors and process measurement sites are generally continuously monitored in a complicated semiconductor production process. In a monitoring system, not all of these signals are equally important. \n\n#### The measured signals comprise a mixture of insightful data, useless data, and noise. Engineers frequently have many signals than the necessary. If each form of signal is considered as a feature, <br> \"Feature Selection\" may be used to find the most relevant signals. \n\n#### These signals can then be used by Process Engineers to identify critical elements that contribute to yield excursions downstream in the process. This will allow for higher process throughput, shorter learning times, and lower takt time production costs. It can be used to forecast the yield type using characteristics. Essential signals that influence the yield type can be determined by evaluating and experimenting with different combinations of attributes.","3e319931":"<a id='data_information'><\/a>\n# Dataset Information\n## Data Description\n<div>The dataset contains containing 1567 examples each with 591 features.The dataset presented in this case represents a selection of such features where each example represents a single production entity with associated measured features and the labels represent a simple pass\/fail yield for in house line testing. Target column \u201c \u20131\u201d corresponds to a pass and \u201c1\u201d corresponds to a fail and the data time stamp is for that specific test point.  \n<br><br>\n<b>Goal<\/b>:\nBuild a classifier to predict the Pass\/Fail yield of a particular process entity and analyze whether all the features are required to build the model or not.\n\n## Files\n<span style=\"background-color:#e1e6e3;\">uci-secom.csv<\/span> (1567, 592)<\/br>\n\n## Steps\n1. Import the necessary liberraries and read the provided CSV as a dataframe and\nperform the below steps. \na. Check a few observations and shape of the dataframe\nb. Check for missing values. Impute the missing values if there is any\nc. Univariate analysis - check frequency count of target column and\ndistribution of the first few features (sensors)\nd. Perform bivariate analysis and check for the correlation\ne. Drop irrelevant columns\n2. Standardize the data \n3. Segregate the dependent column (\"Pass\/Fail\") from the data frame. And split\nthe dataset into training and testing set ( 70:30 split) \n4. Apply sampling techniques to handle the imbalanced classes \n5. Modeling","ac8f62bc":"## Data Splitting","43eb9f3d":"<a id='modeling'><\/a>\n# Modeling","5436788f":"## General Dataset Information","e6c980fb":"<a id='eda'><\/a>\n# Exploratory Data Analysis","f58d8e57":"<div style='color:white;background-color:#5b77ff; height: 100px; border-radius: 15px;'><h1 style='text-align:center;padding: 2%'>Yield Prediction in Semiconductor Manufacturing<\/h1><\/div>"}}