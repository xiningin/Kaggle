{"cell_type":{"5f943a1c":"code","92484e93":"code","dd2df389":"code","70d74548":"code","d541bddf":"code","dbc761c3":"code","c624a4cb":"code","6d292e87":"code","3bb960be":"code","5a6dde4c":"code","5506281b":"code","5abbbe38":"code","a0b3a535":"code","8447e015":"code","854ca754":"code","994c19d0":"code","b7dc4319":"code","0a711e12":"code","66813968":"code","299426bd":"markdown","67474ef9":"markdown","16db1d7e":"markdown","05d062d5":"markdown","047ea4a1":"markdown","02c61bd8":"markdown","168c1727":"markdown","6af2c3ee":"markdown","4317f833":"markdown","75756e77":"markdown","086d4570":"markdown","246d8263":"markdown","32e90207":"markdown","57a772e6":"markdown"},"source":{"5f943a1c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","92484e93":"#Loading dataset\ndf = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","dd2df389":"df.info()","70d74548":"df['quality'].unique()","d541bddf":"df1 = df.select_dtypes([np.int, np.float])\n\nfor i, col in enumerate(df1.columns):\n    plt.figure(i)\n    sns.barplot(x='quality',y=col, data=df1)","dbc761c3":"# quality > 6 is good and less is bad\nbins = [2, 6.5, 8] \nprint(bins)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\nprint(df['quality'])","c624a4cb":"# converting 'bad' and 'good' to labels 0 and 1\nfrom sklearn.preprocessing import LabelEncoder\nencode = LabelEncoder()\ndf.quality = encode.fit_transform(df.quality)\ndf['quality'].value_counts()","6d292e87":"y_data = df['quality']\nx_data = df\nx_data.drop(['residual sugar', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'quality'], axis=1, inplace=True)\n#x_data.drop(['quality'], axis=1, inplace=True)","3bb960be":"x_data.head()","5a6dde4c":"y_data.head()","5506281b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=1)","5abbbe38":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\n\ny_pred = logreg.predict(x_test)\n\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","a0b3a535":"from sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix","8447e015":"sns.heatmap(cnf_matrix, annot=True)\nplt.title(\"Confusion Matrix\")\nplt.show()","854ca754":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\n\ny_pred = dt.predict(x_test)\n\naccuracy_score(y_test, y_pred)","994c19d0":"from sklearn.svm import SVC\n\nclf = SVC(kernel='linear')\nclf.fit(x_train, y_train)\n\ny_pred = clf.predict(x_test)\n\naccuracy_score(y_test, y_pred)","b7dc4319":"from sklearn.neighbors import KNeighborsClassifier\nk = 2\n#Train Model and Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(x_train,y_train)\nyhat = neigh.predict(x_test)\n\nfrom sklearn import metrics\nprint(\"Train set Accuracy: \", metrics.accuracy_score(y_train, neigh.predict(x_train)))\nprint(\"Test set Accuracy (real acc): \", metrics.accuracy_score(y_test, yhat))\n","0a711e12":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nmodel = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))\nmodel.fit(x_train, y_train)\nmodel.score(x_test,y_test)","66813968":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nmodel = RandomForestClassifier()\ntrain_x = x_train\ntrain_y = y_train\ntest_x = x_test\ntest_y = y_test\n# fit the model with the training data\nmodel.fit(train_x,train_y)\n\n# number of trees used\nprint('Number of Trees used : ', model.n_estimators)\n\n# predict the target on the train dataset\npredict_train = model.predict(train_x)\nprint('\\nTarget on train data',predict_train) \n\n# Accuray Score on train dataset\naccuracy_train = accuracy_score(train_y,predict_train)\nprint('\\naccuracy_score on train dataset : ', accuracy_train)\n\n# predict the target on the test dataset\npredict_test = model.predict(test_x)\nprint('\\nTarget on test data',predict_test) \n\n# Accuracy Score on test dataset\naccuracy_test = accuracy_score(test_y,predict_test)\nprint('\\naccuracy_score on test dataset : ', accuracy_test)","299426bd":"### a) Linear classifier","67474ef9":"# Conclusion \n## WE tried different technnique for classification and finally saw that Random Forest gives best results from all of them.","16db1d7e":"### (i) Bagging meta-estimator","05d062d5":"## 4. Trying different Classification Techniques\n### a) Linear classifier\n### b) Support Vector Machine\n### c) Kernel Estimation\n### d) Bagging","047ea4a1":"### b) Support Vector Machine - SVM is a supervised machine learning algorithm which can be used for classification or regression problems. It uses a technique called the kernel trick to transform your data and then based on these transformations it finds an optimal boundary between the possible outputs.","02c61bd8":"## 2. Load DATA","168c1727":"## 1. Import Libraries","6af2c3ee":"### (ii) Random Forest","4317f833":"### (ii) DECISION TREE CLASSIFIER - A decision tree is a map of the possible outcomes of a series of related choices.","75756e77":"**Feature Selection**","086d4570":"## 3. Feature Engineering","246d8263":"### (i) Logistic Regression - Logit is a function through which the binary distribution is associated with the linear equation.","32e90207":"### c) Kernel estimation -> k-nearest neighbor - K nearest neighbors is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure. ","57a772e6":"### d) Bagging - Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set)."}}