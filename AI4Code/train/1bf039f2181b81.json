{"cell_type":{"b9594e4c":"code","1eecc933":"code","30f187d6":"code","8572bc4e":"code","6e4f19c4":"code","6253ca56":"code","8ee92956":"markdown","1e733551":"markdown","b8f0121f":"markdown"},"source":{"b9594e4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1eecc933":"from nltk.corpus import brown\nimport operator\nimport random","30f187d6":"class Vocabulary:\n    def __init__(self, name):\n        self.corpus = brown.sents()\n        self.name = name\n        self.smoothing = 0.1\n        self.n_vocab=10000\n        self.get_sentences_with_word2idx_limit_vocab()\n        self.get_bigram_probs()\n\n    \n    def get_sentences_with_word2idx_limit_vocab(self,):\n      self.indexed_sentences = []\n    \n      i = 2\n      self.word2idx = {'START': 0, 'END': 1}\n      self.idx2word = ['START', 'END']\n      self.start_idx = self.word2idx['START']\n      self.end_idx = self.word2idx['END']\n    \n      word_idx_count = { 0: float('inf'),1: float('inf'),}\n    \n      for sentence in self.corpus:\n        indexed_sentence = []\n        for token in sentence:\n          token = token.lower()\n          if token not in self.word2idx:\n            self.idx2word.append(token)\n            self.word2idx[token] = i\n            i += 1\n    \n          # keep track of counts for later sorting\n          idx = self.word2idx[token]\n          word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n          indexed_sentence.append(idx)\n        self.indexed_sentences.append(indexed_sentence)\n    \n      # restrict vocab size\n      self.sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n      self.word2idx_small = {}\n      new_idx = 0\n      idx_new_idx_map = {}\n      for idx, count in self.sorted_word_idx_count[:self.n_vocab]:\n        word = self.idx2word[idx]\n        #print(word, count)\n        self.word2idx_small[word] = new_idx\n        idx_new_idx_map[idx] = new_idx\n        new_idx += 1\n      # let 'unknown' be the last token\n      self.word2idx_small['UNKNOWN'] = new_idx \n      unknown = new_idx\n    \n      assert('START' in self.word2idx_small)\n      assert('END' in self.word2idx_small)\n    \n      # map old idx to new idx\n      self.sentences_small = []\n      for sentence in self.indexed_sentences:\n        if len(sentence) > 1:\n          new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n          self.sentences_small.append(new_sentence)\n\n    \n    def get_bigram_probs(self,):\n      # structure of bigram probability matrix will be:\n      # (last word, current word) --> probability\n      # we will use add-1 smoothing\n      # note: we'll always ignore this from the END token\n      V = len(self.word2idx_small)\n      self.bigram_probs = np.ones((V, V)) * self.smoothing\n      for sentence in self.sentences_small:\n        for i in range(len(sentence)):\n          if i == 0:\n            # beginning word\n            self.bigram_probs[self.start_idx, sentence[i]] += 1\n          else:\n            # middle word\n            self.bigram_probs[sentence[i-1], sentence[i]] += 1\n          # if we're at the final word\n          # we update the bigram for last -> current\n          # AND current -> END token\n          if i == len(sentence) - 1:\n            # final word\n            self.bigram_probs[sentence[i], self.end_idx] += 1\n    \n      # normalize the counts along the rows to get probabilities\n      self.bigram_probs \/= self.bigram_probs.sum(axis=1, keepdims=True)\n      \n        # a function to calculate normalized log prob score\n   # for a sentence\n    def get_score(self,sentence):\n        words=sentence.lower().split()\n        score = 0\n        for i in range(len(words)):\n          if words[i] not in self.word2idx:\n              print(\"Sorry, the word '\",words[i],\"' not in the vocabulary\")\n              pass\n          elif words[i] not in self.word2idx_small:\n              words[i] = 'UNKNOWN'\n          if i == 0:\n            # beginning word\n            score += np.log(self.bigram_probs[self.start_idx, self.word2idx_small[words[i]]])\n          else:\n            # middle word\n            score += np.log(self.bigram_probs[self.word2idx_small[words[i-1]], self.word2idx_small[words[i]]])\n        # final word\n        score += np.log(self.bigram_probs[self.word2idx_small[words[-1]], self.end_idx])\n        #print thennormalize the score\n        print(\"SCORE: \",score \/ (len(words) + 1) )\n","8572bc4e":"if __name__ == '__main__':\n    voc=Vocabulary('bigram')\n","6e4f19c4":"    from tqdm import tqdm\n    test_sent=[]\n    for sent in tqdm(voc.corpus):        \n        if set(sent).issubset(set(voc.word2idx_small.keys())) and len(sent)>10:\n            test_sent.append(sent)","6253ca56":"    # check real sentence from the corpus:\n    sent=' '.join(random.choice(test_sent))\n    print(\"real sentence from the corpus:\")\n    print(sent)\n    voc.get_score(sent)\n    \n   \n    # check fake sentence:\n    sent=random.choice(test_sent)\n    fake_sent = ' '.join(random.sample(sent, len(sent)))\n    print(\"fake sentence:\")\n    print(fake_sent)\n    voc.get_score(fake_sent)\n    ","8ee92956":"Define the vocabulary class","1e733551":"Fill the vocabulary and the bigram probabilities matrix","b8f0121f":"In this notebook i built a language model that giving scores to input sentences\u05e5\nThe score will be high for a coherent sentences. \nIn order to calculate the probability of each pair of words (bigrams) i used the markov chain and a simple counting of the bigram within the corpus."}}