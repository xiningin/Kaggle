{"cell_type":{"d177a046":"code","8306da49":"code","fcddbfd6":"code","c611f191":"code","6fe96109":"code","d0fcd55d":"code","5f717a68":"code","e4c9cd9b":"code","76bbdc78":"code","d9078ad3":"code","41b012be":"code","653a6b14":"code","bf7038c6":"code","38230893":"code","5a6aefdc":"code","05ef5c5b":"code","0a6ca836":"code","365ce5a0":"code","09010cee":"code","bb0e4b05":"code","72ce5bb7":"code","72d4bbc4":"code","b4dff7f4":"code","c0a575cc":"markdown","11f7338d":"markdown","126226fc":"markdown","e2f597bf":"markdown","b5f2e274":"markdown","62d93a8e":"markdown","be5fd2d8":"markdown","2022861a":"markdown","318c4854":"markdown","9b23eb08":"markdown","5b5586b5":"markdown","af80d246":"markdown","3c5cfd20":"markdown","fd05682a":"markdown"},"source":{"d177a046":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot \ninit_notebook_mode(connected=True)  \nimport cufflinks as cf  \ncf.go_offline() \nimport os\ndf = pd.read_csv('..\/input\/google_review_ratings.csv')\ndf.head()","8306da49":"df = df.loc[:, ~df.columns.str.contains('^Unnamed')]","fcddbfd6":"df.info()","c611f191":"Cols = [str(i) for i in range(1,25)]\nCols =['Category '+i for i in Cols]","6fe96109":"for i in Cols:\n    df[i] = pd.to_numeric(df[i],errors = 'coerce')","d0fcd55d":"df.info()","5f717a68":"df.isnull().sum()","e4c9cd9b":"df = df.fillna(df.mean())","76bbdc78":"df.isnull().sum()","d9078ad3":"df.describe()","41b012be":"New_cols = ['user_id', 'churches', 'resorts', 'beaches', 'parks', 'theatres', 'museums', 'malls', 'zoo', 'restaurants', 'pubs_bars', 'local_services', 'burger_pizza_shops', 'hotels_other_lodgings', 'juice_bars', 'art_galleries', 'dance_clubs', 'swimming_pools', 'gyms', 'bakeries', 'beauty_spas', 'cafes', 'view_points', 'monuments', 'gardens']\ndf.columns = New_cols","653a6b14":"x = df.copy()\nnew = x['user_id'].str.split(' ',n=2,expand=True)\nx['user'] = new[0]\nx['id'] = new[1]\nx = x.drop(['user_id','user'],axis=1)\nx.head()","bf7038c6":"AvgR = df[New_cols[1:]].mean()\nAvgR = AvgR.sort_values()\nplt.figure(figsize=(10,7))\nplt.barh(np.arange(len(New_cols[1:])), AvgR.values, align='center')\nplt.yticks(np.arange(len(New_cols[1:])), AvgR.index)\nplt.ylabel('Categories')\nplt.xlabel('Average Rating')\nplt.title('Average Rating for every Category')","38230893":"New_cols.remove('user_id')","5a6aefdc":"df[New_cols].iplot(kind='box')","05ef5c5b":"vals = df.iloc[ :, 1:].values\n\nfrom sklearn.cluster import KMeans\nwcss = []\nfor ii in range( 1, 30 ):\n    kmeans = KMeans(n_clusters=ii, init=\"k-means++\", n_init=10, max_iter=300) \n    kmeans.fit_predict( vals )\n    wcss.append( kmeans.inertia_ )\n    \nplt.plot( wcss, 'ro-', label=\"WCSS\")\nplt.title(\"Computing WCSS for KMeans++\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"WCSS\")\nplt.show()","0a6ca836":"X = df.drop(['user_id'],axis=1).values\nY = df['user_id'].values","365ce5a0":"km = KMeans(n_clusters=4, init=\"k-means++\", n_init=10, max_iter=500) \ny_pred = kmeans.fit_predict(X)","09010cee":"df[\"Cluster\"] = y_pred\ncols = list(df.columns)\ncols.remove(\"user_id\")\n\nsns.pairplot( df[cols], hue=\"Cluster\")","bb0e4b05":"import scipy.cluster.hierarchy as sch\nfrom sklearn.preprocessing import scale as s\nfrom scipy.cluster.hierarchy import dendrogram, linkage","72ce5bb7":"Z = sch.linkage(x,method='ward')\nden = sch.dendrogram(Z)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) \nplt.title('Hierarchical Clustering')","72d4bbc4":"def fd(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n\n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n        plt.xlabel('sample index or (cluster size)')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata","b4dff7f4":"Z = linkage(x,method='ward')\nfd(Z,leaf_rotation=90.,show_contracted=True,annotate_above=30000,max_d=80000)\nplt.tick_params(\n    axis='x',          \n    which='both',      \n    bottom=False,     \n    top=False,         \n    labelbottom=False) ","c0a575cc":"# Performing Data Cleaning\/Preprocessing Sequences\nThis involve steps like:-\n1. Removal of Unnamed Column(s)\n2. Column(s) Renaming\n2. Checking the correct data type(s) of the respective column(s)\n3. Finding empty instance(s) and filling them with suitable statistical aspect(s)","11f7338d":"**Graph Description:** By looking at the above dendrogram, we observe 3 distinct colors in the dendrogram, but this will not determine how many clusters are formed.","126226fc":"**Graph Description:** This is how clustering is done keeping each and every attribute\/column for the task","e2f597bf":"**Graph Description:** We observe from the above graph that, k's best value is 4 since the we observe the points where there is change in slope of the graph","b5f2e274":"# Importing the necessary Libraries","62d93a8e":"Here is the Horizontal Bar Graph representing the Mean\/Average aspect review of each Category","be5fd2d8":"**Graph Description:** Following the main critera of the cutting the dendrogram appropriatly we discover that there are basically 2 clusters, also observed from the above Graph. Observing the height of each  dendrogram division we decided to go with 80000 where the line would be drawn and 30000 to determine the dendrogram nodes","2022861a":"Here is the boxplot visualization of every Category","318c4854":"# EDA","9b23eb08":"Given below is special function made to serve the purpose of drawing the line which cuts the generated dendrogram to determine the number of clusters and the dendrogram node(s) which are below the cutting line ","5b5586b5":"We are saving a copy of the dataset for Hierarhical clustering","af80d246":"# Clustering\nFor the purpose of clustering we have decided to go with:-\n1. KMean Clustering \n2. Hierarhical Clustering\n\nObserve carefully how we carry the task, keeping every attribute(s) or feature(s) which gives us the best result","3c5cfd20":"# Hierarchical Clustering\nFor the purpose of Hierarchical Clustering we decided to with **Ward** Linkage.Ward's minimum variance criterion minimizes the total within-cluster variance. This who we proceed:-","fd05682a":"# KMean Clustering\nWe start with KMean Clustering, with determining the pefect number of clusters with the **Elbow Method** with Within-Cluster-Sum-of-Squares (WCSS), which is applied followes:-"}}