{"cell_type":{"34d77881":"code","51476ef4":"code","bcf59d01":"code","e8aea649":"code","1d35774b":"code","b1827e90":"code","f15d7d89":"code","47bb114d":"code","c35f6f0b":"code","0d9c5378":"code","fe4b9e31":"code","3877d147":"code","f6ada190":"code","e71970c2":"code","f6f61c33":"code","db957134":"code","3b6c0c59":"code","6682d4ba":"code","d41425d4":"code","b3e30285":"code","a9aad002":"code","29c01bfb":"code","854cc0a1":"code","4f678bc8":"code","27cafcbc":"code","ea27b58e":"markdown","af14898d":"markdown","f4490511":"markdown","16722367":"markdown","aee4fe4e":"markdown","4b559f89":"markdown","a8ac030d":"markdown","a7b009da":"markdown","4674b260":"markdown"},"source":{"34d77881":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","51476ef4":"DIR = '..\/input\/Street View Images\/'\nDIR2 = '..\/input\/RawAccel.csv'\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.transform import resize\nfrom skimage import data\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning) ","bcf59d01":"files = [os.path.join(DIR, fname)\n        for fname in os.listdir(DIR) if '._' not in fname]\nfiles = files[:]\nfiles = sorted(files)","e8aea649":"accelerometer = pd.DataFrame.from_csv('..\/input\/RawAccel.csv')\naccelerometer.describe()\n","1d35774b":"accelerometer.plot.line()","b1827e90":"imgs = [plt.imread(fname)[..., :3] for fname in files]\nimgs = [resize(img_i, (100, 100)) for img_i in imgs]\nimgs = np.array(imgs).astype(np.float32)\nprint(\"Number of images\", len(imgs))","f15d7d89":"gps_coords = pd.read_csv('..\/input\/StreetViewGPS.csv', error_bad_lines=False)\nprint(\"Number of GPS coords\", len(gps_coords))","47bb114d":"imgs = np.delete(imgs, (64, 65, 66, 67, 68, 69, 70, 75, 80), axis=0)","c35f6f0b":"print(\"Number of images\", len(imgs))\nprint(\"Number of GPS coords\", len(gps_coords))","0d9c5378":"print(\"GPS data and corresponding image:\\n\\n\", gps_coords.iloc[-1, :].T)\nplt.imshow(imgs[-1])","fe4b9e31":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.constraints import maxnorm\nfrom keras.optimizers import SGD\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.vis_utils import plot_model\nfrom mpl_toolkits.mplot3d import Axes3D","3877d147":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2))\n\nprint(model.summary())","f6ada190":"from keras import optimizers\nmodel.compile(loss='mean_squared_error', optimizer=optimizers.RMSprop(lr=1e-4))","e71970c2":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","f6f61c33":"targets = pd.concat([gps_coords.iloc[:,0], gps_coords.iloc[:,1]], axis=1)\ninputs = imgs","db957134":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = MinMaxScaler()\ntarget_scaler = scaler.fit(targets)\ntargets = target_scaler.transform(targets)\nx_train, x_test, y_train, y_test = train_test_split(inputs, targets, random_state=5,\n                                                   test_size=0.25)","3b6c0c59":"history = model.fit(x_train, y_train, verbose=0, epochs=50, batch_size=5,\n                   validation_data=(x_test, y_test))","6682d4ba":"print(history.history.keys())","d41425d4":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model accuracy')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","b3e30285":"from sklearn.metrics import mean_squared_error\ntarget_1 = targets[0].reshape(1,2)","a9aad002":"pred = model.predict(np.reshape(imgs[10], (1,100,100,3)))\npred = target_scaler.inverse_transform(pred)\ntarget_1 = target_scaler.inverse_transform(target_1)\nprint(\"MSE: \", (mean_squared_error(target_1, pred)))","29c01bfb":"comparison = np.concatenate((pred, target_1), axis=1)\ncomparison = pd.DataFrame(comparison)\ncomparison.columns = ['Predicted latitude', 'Predicted longitude', 'True latitude', 'True longitude']\ncomparison = pd.concat([comparison['Predicted latitude'], comparison['True latitude'], comparison['Predicted longitude'], comparison['True longitude']], axis=1)                     \ncomparison.head()","854cc0a1":"accelerometer","4f678bc8":"columnNames = list(accelerometer.head(0)) \nprint(columnNames)","27cafcbc":"accelerometer = pd.read_csv('..\/input\/RawAccel.csv')\nthreedee = plt.figure().gca(projection='3d')\nthreedee.scatter(accelerometer[' x'], accelerometer[' y'], accelerometer[' z'])\nthreedee.set_xlabel('X')\nthreedee.set_ylabel('Y')\nthreedee.set_zlabel('Z')\nplt.show()","ea27b58e":"## **Sensor Data used from the Zurich Urban Micro aerial vehicle dataset**","af14898d":"![](https:\/\/qulsar.com\/images\/Autonomous-car-sensor-fusion.png)","f4490511":"## **Need for Sensor fusion**\n\n* **Spatial coverage limits**: Each sensor may only cover a certain region of space.\nFor example, a dashboard camera will observe less surrounding region than a\ncamera with a wide-view lens.\n*  **Temporal coverage limits**: Each sensor may only provide updates over certain periods of time, which may cause uncertainty between updates.\n* **Imprecision**: Each sensor has limits to its sensing element.\n* **Uncertainty**: Unlike imprecision, uncertainty varies with the object being\nobserved rather than the device making the observation. Uncertainty may be\nintroduced by many environment factors or sensor defects in addition to time\ndelay.\n* **Deprivation of sensor**: Sensor element breakdown will cause loss of perception in environment","16722367":". Still some room for improvement, in real world terms the predictions are still far off, but is a good start. To try and improve results we can increase input image size or increase epochs.","aee4fe4e":"# **Sensor Fusion for an Autonomous System**","4b559f89":"* **GPS**: Global-positioning system (GPS) is a system of satellites and receivers used for global navigation of Earth designed by the U.S. military. GPS sends a signal to any GPS receiver with an unobstructed line of sight to four or more GPS satellites surrounding Earth. GPS is useful for finding the exact coordinates of a vehicle when it is in the line of sight of multiple satellites orbiting the Earth.\n    * **Fallbacks**: Expensive, subject to failure in bad weather conditions, subject to failure in distant locations where satellite coverage is blocked or unavailable, dependent on external data source, subject to hijacking and interference.\n    \n* **Camera**: A camera is an optical instrument that utilizes at least one converging or convex lens and a shutter to limit light intake into an enclosed housing for capturing images or recording image sequences [Kodak, 2017]. Video cameras work much like still-image cameras, but instead of simply capturing still images, they record a series of successive still images rapidly at a specific frame rate. A camera is useful for acquiring images or video sequences of object pixels in view of the lens in order to help detect, segment, and classify  objects based on perceivable object properties like location, color, shape, edges\nand corners.\n    * **Fallbacks**: Expensive, subject to failure in bad weather conditions,subject to failure in distant locations where satellite coverage is locked or unavailable, dependent on external data source, subject to hijacking and interference.","a8ac030d":"## Deep Learning for Geolocation Estimation\n\n#### We explore the extraction of geo-spatial information from purely visual data.\nThough there have been previous attempts to apply machine learning and computational algorithms to this fundamentally subtle task, to the best of our knowledge, there are no initiatives that have directly utilized Convolutional Neural Networks (CNNs) for the challenge of geo-localization. This project thus employs a methodology that explores CNNs in conjunction with an investigation of existing computer vision techniques to analyze images according to their geospatial attributes","a7b009da":"Getting the data ready. Need to match up the data from   '..\/input\/StreetViewGPS.csv'   with the images in the  \n  '..\/input\/Street View Images\/'  .","4674b260":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcREXNhuUksFOA-EFAq17v6nLllV6NWMRrW3ZwuCXpO5idPI9AGfgA)"}}