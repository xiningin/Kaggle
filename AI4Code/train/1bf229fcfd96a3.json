{"cell_type":{"047b21ec":"code","6ef2ba8e":"code","77aeef30":"code","f909ac4c":"code","cdb94fd0":"code","8bb3bc4b":"code","c4ec62f0":"code","f955ae98":"code","4fbcf6f2":"code","65c5057e":"code","9e1112a6":"code","2423551c":"code","651401eb":"code","e125e569":"code","eda1d449":"code","7f29b935":"code","e5677692":"code","7cf21988":"code","64fa38ba":"code","31afb50f":"code","12573d6a":"code","29e647f7":"code","e8372254":"code","d9c7a2a5":"code","a6932e3e":"code","f618e8cb":"code","d7d686c3":"code","fc18a2a9":"code","fc2e644e":"code","a738ed5a":"code","215f72ac":"code","e8783d64":"code","81350bad":"code","5bd409b4":"code","91bf1bca":"markdown","86d0d7a0":"markdown","2aa3c248":"markdown","b05ffbfa":"markdown","fbcdaf11":"markdown","82e586a3":"markdown","91b46212":"markdown","55483895":"markdown","b7ec3380":"markdown","4ec8af0b":"markdown","955aa8ad":"markdown","ddc37437":"markdown","8d05fc28":"markdown","eddd485b":"markdown","e2e61499":"markdown","859e96bd":"markdown","5ae8ed80":"markdown"},"source":{"047b21ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6ef2ba8e":"trainPD = pd.read_csv(r'\/kaggle\/input\/game-review-dataset\/train_gr\/train.csv')\ntestPD = pd.read_csv(r'\/kaggle\/input\/game-review-dataset\/test_gr\/test.csv')\ntrainPD.head()","77aeef30":"testPD.head()","f909ac4c":"df = trainPD.copy()\ndf[\"n_words\"] = df[\"user_review\"].apply(lambda s: len(s.split()))\ndf.head()                           ","cdb94fd0":"import matplotlib.pyplot as plt\n\npositiveDF = df[df[\"user_suggestion\"] == 1][\"n_words\"]\npositiveWordVals = positiveDF.values.tolist()\n\nnegativeDF = df[df[\"user_suggestion\"] == 0][\"n_words\"]\nnegativeWordVals = negativeDF.values.tolist()\n\nplt.figure(figsize=(20,10))\nplt.hist([positiveDF, negativeDF], bins=50, alpha=0.5, label=['positive reviews', 'negative reviews'],color=['g','r'])\nplt.title(\"Number of words per review\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Number of reviews\")\nplt.legend()\nplt.show()","8bb3bc4b":"reviewData = df[[\"title\", \"user_review\", \"user_suggestion\", \"n_words\"]].values.tolist()\nreviewData[0]","c4ec62f0":"#Removal of non-english reviews\n\n#Just checks whether at least 50% of the characters are within the ASCII code range (0 - 128)\ndef isEnglish(line):\n    totalChars = len(line)\n    nAsciiChars = 0\n    for c in list(line):\n        if ord(c) >= 0 and ord(c) <= 128:\n            nAsciiChars += 1\n    return (nAsciiChars \/ totalChars) >= 0.5\n\nenglishReviews = []\nnonEnglishReviews = []\n\nfor review in reviewData:\n    if isEnglish(review[1]):\n        englishReviews.append(review)\n    else:\n        nonEnglishReviews.append(review)\n\nprint(\"Sample of some removed reviews:\\n\")\nfor r in nonEnglishReviews[5:10]:\n    print(r)","f955ae98":"print(\"English Reviews Left:\\t{0}\".format(len(englishReviews)))\nprint(\"Discarded non-English reviews:\\t{0}\".format(len(nonEnglishReviews)))","4fbcf6f2":"#Remove some automatically inserted tags within the review\n# => (wasAltered:bool, correctedLine)\ndef removeStartingPhrase(line, phraseList):\n    for phrase in phraseList:\n        if line.startswith(phrase):\n            return (True, line[len(phrase):])\n    return (False, line)\n\nautomaticInsertions = [\"Early Access Review\", \"Product received for free\"]\n\noriginals = []\nfor review in englishReviews:\n    needsCorrection, correctedLine = removeStartingPhrase(review[1], automaticInsertions);\n    if needsCorrection:\n        originals.append(review[1])\n        review[1] = correctedLine","65c5057e":"print(\"Disclaimer Tags removed from {0} reviews\".format(len(originals)))","9e1112a6":"import string \nfrom collections import defaultdict\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\n\nsbStemmer = SnowballStemmer(\"english\")\n\n#Minor optimization, prevent the stemmer from re-running if the value was previously computed\npriorDict = {}\ndef memomizedStemmer(word): \n    if word in priorDict:\n        return priorDict[word]\n    else:\n        stemmed = sbStemmer.stem(word)\n        priorDict[word] = stemmed\n        return stemmed\n    \npunctuationRemover = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n\ndef countUniqueWords(lines):\n    wordCountDict = defaultdict(int)\n    for line in lines:\n        for word in line.lower().translate(punctuationRemover).split():\n            wordCountDict[memomizedStemmer(word)] += 1\n    return wordCountDict\n\ndef getDocumentFrequency(lines):\n    docFrequency = defaultdict(int)\n    for line in lines:\n        #Only count whether or not it appears in this document \n        wordSet = set(line.lower().translate(punctuationRemover).split())\n        for word in wordSet:\n            docFrequency[memomizedStemmer(word)] += 1\n    return docFrequency\n\npositiveReviews = [t[1] for t in englishReviews if t[2] == 1]\nnegativeReviews = [t[1] for t in englishReviews if t[2] == 0]\n                   \npositiveWordCounts = countUniqueWords(positiveReviews)\nnegativeWordCounts = countUniqueWords(negativeReviews)\n                   \npostiveWordDocFreq = getDocumentFrequency(positiveReviews)\nnegativeWordDocFreq = getDocumentFrequency(negativeReviews)\n\nfor stopword in stopwords.words('english'):\n    positiveWordCounts.pop(stopword, None)\n    negativeWordCounts.pop(stopword, None)\n\npositiveCols = [ (word, count, postiveWordDocFreq[word]) for word, count in positiveWordCounts.items()]\nnegativeCols = [ (word, count, negativeWordDocFreq[word]) for word, count in negativeWordCounts.items()]","2423551c":"positiveDF = pd.DataFrame(positiveCols, columns=[\"word\", \"count\", \"doc-freq\"]).sort_values(by=\"count\", ascending=False)\nnegativeDF = pd.DataFrame(negativeCols, columns=[\"word\", \"count\", \"doc-freq\"]).sort_values(by=\"count\", ascending=False)\n\npositiveDF['doc-frac'] = positiveDF['doc-freq'] \/ len(positiveCols)\nnegativeDF['doc-frac'] = negativeDF['doc-freq'] \/ len(negativeCols)","651401eb":"positiveDF.head()","e125e569":"negativeDF.head()","eda1d449":"negativeDFMerged = pd.merge(negativeDF, positiveDF, on=\"word\", how=\"left\", suffixes=[\"_neg\", \"_pos\"]).fillna(0)\nnegativeDFMerged[\"delta-frac\"] = negativeDFMerged[\"doc-frac_neg\"] - negativeDFMerged[\"doc-frac_pos\"]\nnegativeDFMerged[\"weighted-iou\"] = negativeDFMerged[\"delta-frac\"] * (negativeDFMerged[\"doc-frac_neg\"] \/ ( negativeDFMerged[\"doc-frac_neg\"] + negativeDFMerged[\"doc-frac_pos\"]))\nnegativeDFMerged[\"normalized-iou\"] = (negativeDFMerged[\"weighted-iou\"] - min(negativeDFMerged[\"weighted-iou\"])) \/ (max(negativeDFMerged[\"weighted-iou\"]) - min(negativeDFMerged[\"weighted-iou\"]))\nnegativeDFMerged = negativeDFMerged.sort_values(by=\"normalized-iou\", ascending=False)\n#negativeDFMerged = negativeDFMerged.sort_values(by=\"delta-frac\", ascending=True)\nnegativeDFMerged.head(25)","7f29b935":"positiveDFMerged = pd.merge(positiveDF, negativeDF,  on=\"word\", how=\"left\", suffixes=[\"_pos\", \"_neg\"]).fillna(0)\npositiveDFMerged[\"delta-frac\"] = positiveDFMerged[\"doc-frac_pos\"] - positiveDFMerged[\"doc-frac_neg\"]\npositiveDFMerged[\"weighted-iou\"] = positiveDFMerged[\"delta-frac\"] * (positiveDFMerged[\"doc-frac_pos\"] \/ ( positiveDFMerged[\"doc-frac_neg\"] +  positiveDFMerged[\"doc-frac_pos\"]))\npositiveDFMerged[\"normalized-iou\"] = (positiveDFMerged[\"weighted-iou\"] - min(positiveDFMerged[\"weighted-iou\"])) \/ (max(positiveDFMerged[\"weighted-iou\"]) - min(positiveDFMerged[\"weighted-iou\"]))\npositiveDFMerged = positiveDFMerged.sort_values(by=\"weighted-iou\", ascending=False)\n#positiveDFMerged = positiveDFMerged.sort_values(by=\"delta-frac\", ascending=True)\npositiveDFMerged.head(25)","e5677692":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nntlkStopWords = list(stopwords.words('english'))\n\nvectorizer = CountVectorizer(stop_words=ntlkStopWords, max_features=10000)\n#vectorizer = CountVectorizer(max_features=10000)\n#vectorizer = TfidfVectorizer(stop_words=ntlkStopWords, max_features=10000, ngram_range=(1,2), min_df = 3)\n\ndata_reviews_text = [review[1] for review in englishReviews]#df[\"user_review\"].values.tolist()\ndata_sentiment = [review[2] for review in englishReviews] #df[\"user_suggestion\"].values.tolist()\n\ndata_review_vec = vectorizer.fit_transform(data_reviews_text);","7cf21988":"from sklearn.naive_bayes import MultinomialNB\n\ntrain_x, test_x, train_y, test_y = train_test_split(data_review_vec, data_sentiment, test_size=0.2, random_state= 0xDEADBEEF)\n\nmnb = MultinomialNB()\nmnb.fit(train_x, train_y)\nprint(\"Multinomial Naive Bayes (No pre-processing): {0} %\".format(mnb.score(test_x, test_y) * 100))","64fa38ba":"negativeWordWeightDict = dict(zip(negativeDFMerged['word'],negativeDFMerged['normalized-iou'] ))\npositiveWordWeightDict = dict(zip(positiveDFMerged['word'],positiveDFMerged['normalized-iou'] ))\n\nvocabDict = vectorizer.vocabulary_\nwordVecLength = len(vocabDict)\n\nnegativeWeightVec = np.zeros(wordVecLength)\nnegMissing = []\nnegAdded = 0 \nfor word, idx in vocabDict.items():\n    if word in negativeWordWeightDict:\n        negAdded += 1\n        negativeWeightVec[idx] = negativeWordWeightDict[word]\n    else:\n        negMissing.append(word)\n\npositiveWeightVec = np.zeros(wordVecLength)\nposMissing = []\nposAdded = 0 \nfor word, idx in vocabDict.items():\n    if word in positiveWordWeightDict:\n        posAdded += 1\n        positiveWeightVec[idx] = positiveWordWeightDict[word]\n    else:\n        negMissing.append(word)\n\nmergedWeights = np.add(negativeWeightVec, positiveWeightVec)\nmergedWeights = np.square(mergedWeights)\nmergedWeights += 1 #smoothing\n    \nprint(\"word vocabulary length: {0}\".format(wordVecLength))\nprint(\"neg. word vocabulary: {0}\".format(len(negativeWordWeightDict)))\nprint(\"pos. word vocabulary: {0}\".format(len(positiveWordWeightDict)))\nprint(\"Matching negative words: {0}\".format(negAdded))\nprint(\"Matching positive words: {0}\".format(posAdded))","31afb50f":"data_review_vec_weighted = np.multiply(data_review_vec.toarray(), mergedWeights)","12573d6a":"train_weighted_x, test_weighted_x, train_weighted_y, test_weighted_y = train_test_split(data_review_vec_weighted, data_sentiment, test_size=0.2, random_state= 0xDEADBEEF)\n\nweighted_mnb = MultinomialNB()\nweighted_mnb.fit(train_weighted_x, train_weighted_y)\nprint(\"Multinomial Naive Bayes (After applying intersection-over-union weights): {0} %\".format(weighted_mnb.score(test_weighted_x, test_weighted_y) * 100))","29e647f7":"from sklearn.svm import LinearSVC\n\nsvClassifier = LinearSVC(max_iter=3000, tol=1e-5, dual=False, C=0.01)\n\nsvClassifier.fit(train_x, train_y)\nprint(\"Linear SVC, acc = {0} %\".format(svClassifier.score(test_x, test_y) * 100))","e8372254":"svWeightedClassifier = LinearSVC(max_iter=3000, tol=1e-5, dual=False, C=0.01)\n\nsvWeightedClassifier.fit(train_weighted_x, train_weighted_y)\nprint(\"Linear SVC (with intersection-over-union weights), acc = {0} %\".format(svWeightedClassifier.score(test_weighted_x, test_weighted_y) * 100))","d9c7a2a5":"#Append an index at the start pointing back to the original review\ndata_review_vec_labelled = [ [idx, review] for idx, review in enumerate(data_review_vec.toarray())]\ndata_sentiment_labelled = [ [idx, label] for idx, label in enumerate(data_sentiment)]\n\ntrain_x_lab, test_x_lab, train_y_lab, test_y_lab = train_test_split(data_review_vec_labelled, data_sentiment_labelled, test_size=0.2, random_state= 0xDEADBEEF)","a6932e3e":"#=> [originalreview, prediction, groundTruth][]\ndef GetMispredictions(indexedX, indexedY, spClassifier):\n    onlyX = [x for lbl, x in indexedX]\n    predictions = spClassifier.predict(onlyX)\n    incorrectPredictions = []\n    for idx, pred in enumerate(predictions):\n        groundTruth = indexedY[idx][1]\n        isCorrect = pred == groundTruth\n        if not isCorrect:\n            originalReviewIdx = indexedY[idx][0]\n            originalReviewText = data_reviews_text[originalReviewIdx]\n            incorrectPredictions.append([originalReviewText, pred, groundTruth])\n    return incorrectPredictions\n\nmispredictions = GetMispredictions(train_x_lab, train_y_lab, mnb)","f618e8cb":"actual_recommends = [lst for lst in mispredictions if lst[2] == 1]\nactual_disgruntled = [lst for lst in mispredictions if lst[2] == 0]\n\nprint(len(actual_recommends))\nprint(len(actual_disgruntled))","d7d686c3":"actual_recommends_df = pd.DataFrame.from_records(actual_recommends, columns=[\"review\",\"prediction\",\"ground-truth\"])\nactual_disgruntled_df = pd.DataFrame.from_records(actual_disgruntled, columns=[\"review\",\"prediction\",\"ground-truth\"])\n\npd.set_option('display.max_colwidth', 500) \nactual_recommends_df.head(20)","fc18a2a9":"actual_disgruntled_df.head(20)","fc2e644e":"import re\n\noverlapListRaw = [ \n    \"not\", \"would\",\"cant\",\"good\",\"but\",\"like\",\"get\",\"time\",\"only\",\"money\", \"pretty\", \n    \"free\", \"play\", \"great\", \"very\", \"fun\", \"best\", \"like\", \"lot\", \"friend\", \"enjoy\", \"learn\" , \"love\", \"good\", \"really\", \"well\", \"easy\", \"nice\", \"still\", \"help\",\n    \"money\",\"even\",\"waste\",\"tank\",\"bad\",\"pay\",\"fix\",\"kill\",\"grind\",\"worst\",\"because\",\"ruin\",\"tier\",\"remove\",\"match\",\"worse\",\"terrible\",\"nothing\",\"change\",\"make\",\"update\",\"uninstall\",\"want\"]\n\noverlapList = [memomizedStemmer(word) for word in overlapListRaw]\n\ngenBigramRegex = r\"\\b(?:(\\w+)\\s+)?(\" + \"|\".join(overlapList) + r\")\\s+(\\w+)\\b\"\nbigramReplacementRegex = r\"\\2 \\2\\3 \\1\\2 \\1\\2\\3\"\n\ndef processSentence(line):\n    #1.1) for the n't contraction, expand the \"not\" into a separate word\n    notAbbrSeparated = re.sub(r\"(\\w+)n't\", r\"\\1 not\", line.lower().replace(\"can't\", \"can not\").replace(\"won't\", \"will not\"))\n    #1.2) merge contractions (default tokenizer treats punctuation marks(even apostrophes ') as word delimiters))\n    mergedContractions = re.sub( r\"\\b(\\w+)'(\\w{1,2})\\b\",r'\\1\\2', notAbbrSeparated) #replace r'\\1\\2' with r'\\1_\\2' to observe the drop in accuracy\n    #2) Condensing long trails of letters into rep_<letter>\n    condensedRepeats = re.sub(r\"([a-zA-Z])\\1{2,}\", r\"rep_\\1\", mergedContractions)\n    #3) Word stemming\n    stemmed = \" \".join([memomizedStemmer(word) for word in condensedRepeats.split()])\n    #4) Selectively inserting \"Bigrams\" \/ \"Trigrams\"\n    withBigrams = re.sub(genBigramRegex, bigramReplacementRegex, stemmed)\n    #merge negation modifiers with the next word\n    return withBigrams\n\ndata_reviews_text_eng = [processSentence(review) for review in data_reviews_text]\ndata_sentiment_eng = data_sentiment[:]\n\n#overlapList\n#print(processSentence(\"I'll\"))","a738ed5a":"vectorizerEng = CountVectorizer(stop_words=ntlkStopWords, max_features=10000)\n#vectorizerEng = TfidfVectorizer(stop_words=ntlkStopWords, max_features=10000)\n\ndata_review_vec_eng = vectorizer.fit_transform(data_reviews_text_eng)\n\ntrain_x_eng, test_x_eng, train_y_eng, test_y_eng = train_test_split(data_review_vec_eng, data_sentiment_eng, test_size=0.2, random_state= 0xDEADBEEF)\n\nmnbEng = MultinomialNB()\nmnbEng.fit(train_x_eng, train_y_eng)\nprint(\"Multinomial Naive Bayes (after text processing) acc: {0} %\".format(mnbEng.score(test_x_eng, test_y_eng) * 100))","215f72ac":"from sklearn.svm import LinearSVC #Specialized Linear SVM solver (LibLinear backend) is supposedly much faster than the Linear Kernel within the SVM namespace (LibSVM)\nfrom sklearn.preprocessing import StandardScaler\n#Smaller Bag of Words for the SVM\nvectorizerEngSVM = CountVectorizer(stop_words=ntlkStopWords, max_features=10000)\n\ndata_review_vec_eng_svm = vectorizerEngSVM.fit_transform(data_reviews_text_eng)\n\n#scaler = StandardScaler()\n#data_review_vec_eng_svm_normalized = scaler.fit_transform(data_review_vec_eng_svm.toarray())\n\ntrain_svm_x_eng, test_svm_x_eng, train_svm_y_eng, test_svm_y_eng = train_test_split(data_review_vec_eng_svm, data_sentiment_eng, test_size=0.2, random_state= 0xDEADBEEF)\n\nsvEngClassifier = LinearSVC(max_iter=3000, tol=1e-5, dual=False, C=0.01)\n\nsvEngClassifier.fit(train_svm_x_eng, train_svm_y_eng)\nprint(\"Linear SVC (after text processing) acc: {0} %\".format(svEngClassifier.score(test_svm_x_eng, test_svm_y_eng) * 100))","e8783d64":"pigeonDatingSimulator = [ #Hatoful Boyfriend\n    [\"You cannot bang the birds.\", 0],\n    [\"My friend sent me this and I jokingly said I'll 100% it, but I honestly didn't think I would have gotten this invested into it but here we are.\", 1],\n    [\"It was great back in its day, I'm sure. Fans of modern visual novels might find themselves put off by this one. In particular, the pacing seems to drag on towards the end of the BBL path.\", 0],\n    [\"This game is at beak of it's performance. While other games casually fail to do one thing properly, Hatoful Boyfriend delivers on it's wings story of friendship, romance, comedy, mystery and horror all in one.\", 1],\n    [\"This is honestly the worst visual novel game I have ever played. The storywriting is SO bad. It is full of irrelevant subplots, and very little character development. I finished the entire game in less than an hour. BORING!\", 0],\n    [\"The endings never go where you expect. Also birds\", 1]\n]\n\nundesirableRodents = [ #Bad rats\n    [\"God has put me on this earth just to play Bat Rats\", 0],\n    [\"Wait till Garfield hears about this.\", 1],\n    [\"Bad Rats got me pregnant\", 0],\n    [\"bad rats very pog game, 10\/10. I loved the part when the ginger cat died, it was great. No complaints, very epic game.\",1],\n    [\"This game infects you with youtube poop disease. RUN!!! DONT GET CLOSER!!! LEAVE!!(Really though the game is a meh puzzle solving game at best. Even if its like $0.01 I wouldn't recommend getting it. If someone gifts you this game they really must hate you.)\", 0],\n    [\"I lost my lively hood due to this game. My son does not look at me the same as I look at him like he is a rat, and try to deal with him like he is a rat from the hip game \\\"bad rats\\\". Please honey, if you see this, i swear i've changed. You're more than a rat to me, you're my cat. All in All, bad rats very good\", 1]\n]","81350bad":"mergedData = [[\"Hatoful Boyfriend\", *item] for item in pigeonDatingSimulator] + [[\"Bad Rats\", *item] for item in undesirableRodents]\n\ndemo_x_text = [t[1] for t in mergedData]\ndemo_y = [t[2] for t in mergedData]\n\ndemo_x = vectorizerEngSVM.transform(demo_x_text)\n\ndemo_predictions = svEngClassifier.predict(demo_x)\n\nmergedDataWithPred = [[*t, pred] for t, pred in zip(mergedData, demo_predictions)]\n\ndemo_matches = [1 if (pred == groundTruth) else 0 for pred, groundTruth in zip(demo_predictions, demo_y)]\naccuracy = sum(demo_matches) \/ len(demo_y)\nprint(\"Demostration Accuracy: {0}%\".format(accuracy * 100))\nprint(demo_matches)","5bd409b4":"demoDf = pd.DataFrame.from_records(mergedDataWithPred, columns=[\"Game Title\",\"Review Text\",\"Actual Recommendation\",\"Predicted Recommendation\"])\ndemoDf.head(12)","91bf1bca":"**Potential Hurdles**\n\nGlancing through the misclassified reviews in the tables below highlights a subset of reviews which may prove especially problematic to these simple categorizers (SVM, Naive Bayes), namely:\n* Sarcastic Reviews - Look no further than row \\#10 or \\#0 in the dataframe below. When the tone mismatches the actual recommendation, inference is bound to be more difficult\n\nGetting a model to understand scarasm may be a bit of a stretch for SVMs or Naive Bayes, and hence they will not be tackled in this case. (LSTM neural networks may fair better, but the current dataset of 17K reviews may prove too small without any form of transfer learning)\n\nWhat is surprising though is how reviews with the phrase \"not recommend\" gets misclassified as a positive review (in the second dataframe below).\nOther shortcomings of the CountVectorizer are also highlighted in the reviews that are incorrectly labelled as positive (screams such as \"aahhh\" somehow labelled as positive). These shortcomings will be addressed by feeding the text through a rudimentary text processor.","86d0d7a0":"# Multinomial Naive Bayes with intersection-over-union Weights applied\n\nAfter getting the word vectors from the count vectorizer (Tf-Idf doesn't boost accuracy in this case btw), the words that are more likely to only appear in either the Positive or Negative classes will have their counts boosted by multipyling them with the \"*normalized-iou*\" value for positive and negative classes computed in the section above.\n\nThis actually results in a slight loss of accuracy (validation acc. drops to below 83%).","2aa3c248":"# Demostration\n\nAs I mentioned above, sarcastic reviews are expected to be the Achilles Heel of the model. I have specifically fished out reviews from 2 particular games that are bound to be flooded with these:\n1. Hatoful Boyfriend - A Pigoen dating simulator\n2. Bad Rats - A game whose reputation can largely be defined by the first word in its title\n\nThe small sample size yields poorer than expected accuracy - 66% (especially for reviews from *Bad Rats*).","b05ffbfa":"**Non-English Review Removal**\n\nNLTK and ScaPy probably have built in language detection, but for the sake of simplicity (and speed), English reviews will be treated as text which consist of ASCII characters for at least 50% of their length. As shown below, this also has the effect of removing reviews that are emoticon spam. In total, out of the ~17.5k reviews, only 91 will be discarded.","fbcdaf11":"# Review Length v.s. Sentiment\n\nThere appears to be correlation between review length (character count) and the sentiment of the review, with shorter reviews tending to be more positive. The trend is especially evident within the sub-100 characters range. This is unsurpisingly, as an impassioned hatred serves as an unexpectedly good motivator for devoting large amounts of time to a particular cause). For the longer reviews (> 400 chars), the gap in the number of positive and negative reviews narrows. The reversal in trend could confuse simpler classifiers (Naive Bayes, SVM), and thus character counts won't be used as a feature.","82e586a3":"**Disclaimer Tag Removal**\n\n*\"Early Access Review\"* and *\"Product received for free\"* disclaimers always occur at the start of the review sentences (if present). The disclaimer tags are present in 6000 of the ~17500 reviews, which can significantly skew word frequencies if left unchecked.","91b46212":"# Effects of applying Text-Processing\n\nRunning the processed text through the Multinomial Naive Bayes and Linear SVC both yield slight improvements, with the Linear SVC inching up from 83% to 85% validation accuracy.\n","55483895":"# Improving the model through Text Pre-Processing\n\nSimply using the CountVectorizer to get word embeddings is bound to result in the following issues:\n1. English contractions will be dropped entirely\n    * When splitting words, the count vectorizer uses the following Regex Pattern: r\u201d(?u)\\b\\w\\w+\\b\u201d. In short, it treats any consective sequence of alphanumeric characters (including the underscore _) of length 2 or more as separate words. For common contractions such as \"can't\", the 't' will be dropped entirely, resulting in only the token \"can\". Other egregious examples include \"won't\" $\\rightarrow$ \"won\" or \"don't\" $\\rightarrow$ \"don\".\n    * To resolve this, two strageties are used:\n        * \\[The \"n't\" contraction\\] - The character sequence \"n't\" will be replaced as a separate word \"not\". This is achieved using a combination of a simple Regex replacement pattern and string.replace() for the edge cases \"can't\" and \"won't\"\n        * \\[Other contractions (')\\] - The apostrophe is removed and the word is merged into a single token (i.e. \"I'm\" $\\rightarrow$ \"Im\")\n           * Note: Yes, this does lead to inaccurate contractions (i'll $\\rightarrow$ ill), but is in line with how most people ignore proper punctuation in informal messages. In fact, tests with replacing the apostrophe with an underscore (_) to preserve the distinction yields lower prediction accuracies (see comment in the source code below for details), which lends credence to this theory.\n2. Repeated characters used for emphasis will be treated as entirely different words\n    * Repeated characters such as in \"AAAHHHHHHHHHH\" may sometimes be used for emphasis. Problem is, the count vectorizer treats each instance as a completely different word as long as it differs by even one character. This adds noise to the word embeddings when passed to the Models for training\n    * Solution - Contract any sequence of 3 or more of the same character (characters don't repeat more than twice in English words <sub>AFAIK<\/sub>) into the phrase \"rep_\\<insert charcter here\\>\". (i.e. \"AAAAHHHHHH\" $\\rightarrow$ \"rep_Arep_H\")\n3. Different word inflections are treated differently\n    * Different grammatical inflections do convey different meanings, however, given that the statistical models look at each token individually, they are likely to be unable to make sense of the distinction. \n    * Solution - Use a word stemmer to reduce different inflections into a single form. \\[Note: ScaPy does have a lemmatizer, which is a more powerful tool to reduce words to their common form, but it runs far more slowly (especially since memoization cannot be used as an optimization technique as each Lemma depends on the entire sentence)\\]\n4. Modifier\/ Qualifier words are read in isolation without context\n    * These modifier\/qualifier words serve to add meaning to other words, and thus are less informative on their own.\n    * Solution - Insert a Bigram\/Trigram of the modifier word by appending the prior or next word to the modifier word to get a new token. (i.e. the phrase \"would *not* recommend\" produces the following list of tokens \\[\"not\", \"wouldnot\", \"notrecommend\", \"wouldnotrecommend\"\\])\n        * Note: A sane Software Engineer would likely use a dictionary and a rolling list of previously seen tokens to construct the Bigrams\/Trigrams. I have instead opted to use this opportunity as an excuse to write a horrendously long Regex.","b7ec3380":"# Some data cleanup\n\nThe dataset doesn't consist entirely of English Reviews, some Japanese and Russian reviews can be spotted in the mix. Additionally, the data-scraping process also includes the \"*Early Access Review*\" and \"*Product received for free*\" disclaimers that are automatically inserted at the top of each review. These should be counted as noise and will thus be removed in this section.","4ec8af0b":"# A Peek at our data\n\nThe data format is simple enough, user suggestions are marked as \"1\" for recommended and \"0\" otherwise under the \"user_suggestion\" column. There is a separate file containing the corresponding advertising blurp for the game's storefront, but that won't be used here.","955aa8ad":"# Finding the distribution of words in Positive & Negative reviews\n\nFor the Naive Bayes & SVM classifiers, the main feature that is being considered by model is word\/token frequency. As such, looking at the most commonly occuring words could yield some insights into potential roadblocks for the classifer models.","ddc37437":"# (Baseline \\#2) Linear SVM with no text pre-processing\n\nThe same datasets are run through a linear SVM, both show a slight improvement over the Naive Bayes model. And yet again, using the *intersection-over-union* weights results in a drop in accuracy. \n\n*P.S, don't make the grave mistake of using the linear SVM within the svm namespace, the specialized LinearSVC based on the LibLinear backend is significantly faster than the LibSVM variant*","8d05fc28":"**An Attempt to engineer more distinct features**\n\nInstead of taking a look at just the most common words in both the Negative & Positive Reviews, the words that are most likely to appear in only Positive or Negative reviews may prove more useful as input features.\n\nThe likelihood of appearing in only one class (Negative or Positive) is computed as the word frequency in that class, subtracted by the word frequency in the other class, i.e.\n\n$$F_\\text{delta(positive)} = \\frac{\\text{Number of times word}_i\\text{ occurs in the positive class}}{\\text{Total number of words in the positive class}} - \\frac{\\text{Number of times word}_i\\text{ occurs in the negative class}}{\\text{Total number of words in the negative class}}$$\n\nWhere higher values of $F_\\text{delta(positive)}$ implies that the word is more likely to only appear in one class. This value is called *\"delta-frac\"* in the dataframe below.\n\n$F_\\text{delta(positive)}$ is further normalized by multiplying it with the Intersection-Over-Union(IOU) of the fraction of word count $i$ over the total word count for class $c$, i.e.\n\n$$\\text{Weighted IOU}_{word_i, Class=Positive} = F_\\text{delta(positive)} \\times \\frac{ \\left( \\frac{N_{word_i}}{N_{total}} \\right)_{Positive} }{ \\left( \\frac{N_{word_i}}{N_{total}} \\right)_{Positive} + \\left( \\frac{N_{word_i}}{N_{total}} \\right)_{Negative}  } $$ \n\nOne further normalization step to scale it to 0-1 using its min & max values is performed to produce the column \"*normalized-iou*\" in the DF below (As with before, higher values imply it is more likely to only occur in that particular class).\n\nSorting by this metric results in more distinct word sets, and also reveals hints about data being skewed more towards certain games than others (i.e. negative reviews from *World of Tanks* inflating the word count for \"tank\" in the negative review set).\n\nAn attempt will be made to use the \"*normalized-iou*\" metric to boost classification accuracy later. (Spoiler, it doesn't work)\n\n*P.S. If you are wondering why some words appear truncated (i.e. \"wast\" instead of waste \/ wasted), that is the result of the Snowball Stemmer collapsing different word inflections into a single word stem.*","eddd485b":"# Introduction\n\nA Naive Bayes & Linear SVM are used to perform sentiment classification of the text reviews under 3 different settings\n1. With mimimal text processing \n2. Using a custom feature based on the uniqueness of a word in either the Positive and Negative classes\n3. Using text-preprocessing techniques to overcome the shortcomings of SciPy's built-in CountVectorizer\n\nThe last (3) setting yields a marginal improvement (2% for the Linear SVC) over the baseline (1), with a final validation test accuracy of ~85% using the Linear SVC model.\n\nState of the Art techniques (i.e LSTM neural networks) won't be considered due to the relatively small (by Deep Learning standards) dataset (~ 17.5k reviews)","e2e61499":"# Inspecting the wrongly classified reviews","859e96bd":"**Heavy overlap in the most common words for Negative & Positive Reviews**\n\nFrom the two tables below, *4 out of 5* of the most common words for negative and positive reviews overlap (\"game\" , \"play\", \"get\", \"like\"). Given the similarity of the distributions of the most common features, this could potentially lead to poor classification accuracy.","5ae8ed80":"# (Baseline) Multinomial Naive Bayes with no prior text preprocessing.\n\nThe following is the use of a Multinomial Naive-Bayes model to classify the reviews with no prior test pre-processing (aside from the text cleanup we did earlier).\nThis yields a baseline accuracy of 83%."}}