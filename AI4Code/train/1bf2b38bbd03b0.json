{"cell_type":{"73b4192f":"code","3bdab773":"code","e8ddd7ad":"code","e906cdc3":"code","f024a207":"code","5e79291b":"code","fb898f85":"code","118edec7":"code","b0b44a96":"code","c23ba9b8":"code","f21b6767":"code","b0dce8fe":"code","a3f3391c":"code","b4dbe813":"code","40ab9542":"code","21caddff":"code","f3b5011e":"code","3d5f555c":"code","418070a3":"code","9fdc5a5c":"code","2fc79bd5":"code","cc7acdb5":"code","3233b242":"code","3ce2871b":"code","fc84b42c":"code","6b5801ee":"code","98717343":"code","74dba858":"code","8afd6d56":"code","ac39a1be":"code","c1db229d":"code","13f0c276":"code","def30f74":"code","de715fbb":"code","b9751175":"code","13bc458d":"code","586de1be":"code","7e00106e":"code","ac4df9e7":"code","7b406252":"code","642730d2":"code","4f489a51":"code","70d34f27":"code","96ea956d":"code","c90240c6":"code","2dcf1e0b":"code","a16c9684":"code","44dc0b64":"code","1f335ef2":"code","3dc43eb3":"code","f5ab024f":"code","dce8947b":"code","9a1a8002":"code","ebc9b9ab":"code","769fff1e":"code","be88a67c":"code","9e82afd0":"code","affa9bf2":"code","8dff843c":"code","caa907b1":"code","bbefe177":"code","db64d8ed":"code","7d8949da":"code","0841eb6d":"code","b2d3475d":"markdown","bfd4a563":"markdown","82dfa6ec":"markdown","7ccb9a74":"markdown","caa3a197":"markdown","9778e799":"markdown","65d300f1":"markdown","5de6f0a6":"markdown","51055423":"markdown","8cdd8377":"markdown","7851518c":"markdown","cb52d544":"markdown","9ddc1cae":"markdown","21afe389":"markdown","7fe97b18":"markdown","2fc684cd":"markdown","97e4e946":"markdown","b2a3f984":"markdown","4805da4f":"markdown","bd5961f0":"markdown","4b36977d":"markdown","11e1dcb5":"markdown","af1724a7":"markdown","0e926571":"markdown","2ba9c33d":"markdown","66cbae3b":"markdown","86b7b9c2":"markdown","229084a6":"markdown","341f1640":"markdown","3f94d529":"markdown","e17836d8":"markdown"},"source":{"73b4192f":"import pandas as pd\nimport numpy as np\nimport os\nimport regex as re\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","3bdab773":"df = pd.read_csv('..\/input\/dbmisymptomsdiseasedata\/dataset_uncleaned.csv', engine='python')\ndf.count()","e8ddd7ad":"df.head()","e906cdc3":"df = df.fillna(0)","f024a207":"fill = df['Disease'].iloc[0]\nfor i in range(1,1867):\n    if df['Disease'].iloc[i] == 0:\n        df['Disease'].iloc[i] = fill\n    else:\n        fill = df['Disease'].iloc[i]\ndf['Disease']","5e79291b":"fill = df['Count of Disease Occurrence'].iloc[0]\nfor i in range(1,1867):\n    if df['Count of Disease Occurrence'].iloc[i] == 0.0:\n        df['Count of Disease Occurrence'].iloc[i] = fill\n    else:\n        fill = df['Count of Disease Occurrence'].iloc[i]\ndf['Count of Disease Occurrence']","fb898f85":"df = df[df.Symptom != 0]\ndf","118edec7":"df['Symptom'] = df['Symptom'].apply(lambda x: x.split('^'))\ndf['Symptom']","b0b44a96":"df = df.explode('Symptom').reset_index()","c23ba9b8":"df.Symptom = df.Symptom.apply(lambda x: x.split('_')[1])\ndf","f21b6767":"df['Disease'] = df['Disease'].apply(lambda x: x.split('^'))\ndf = df.explode('Disease').reset_index()\ndf.Disease = df.Disease.apply(lambda x: x.split('_')[1])\ndf","b0dce8fe":"df.drop(['index', 'level_0','Count of Disease Occurrence'], axis = 1, inplace = True)\ndf","a3f3391c":"df_sparse = pd.get_dummies(df, columns = ['Symptom']).drop('Symptom_', axis=1).drop_duplicates()\ndf_sparse.head()","b4dbe813":"df_sparse = df_sparse.groupby('Disease').sum().reset_index()\ndf_sparse.head()","40ab9542":"X = df_sparse[df_sparse.columns[1:]]\nY = df_sparse['Disease']","21caddff":"from sklearn.model_selection import train_test_split","f3b5011e":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","3d5f555c":"from sklearn.tree import DecisionTreeClassifier","418070a3":"from sklearn.ensemble import GradientBoostingClassifier\nxgb_clf = GradientBoostingClassifier()\nxgb_clf.fit(X, Y)\nscore = xgb_clf.score(X, Y)\nprint(score)","9fdc5a5c":"print (\"DecisionTree\")\nclf = DecisionTreeClassifier()\nmodel = clf.fit(X,Y)\nprint (\"Acurracy: \", model.score(X,Y))","2fc79bd5":"model.predict(x_test)","cc7acdb5":"model.score(x_test, y_test)","3233b242":"input_data = pd.read_csv('..\/input\/dbmisymptomsdiseasedata\/Training.csv')\ninput_data.head()","3ce2871b":"test_data = pd.read_csv('..\/input\/dbmisymptomsdiseasedata\/Testing.csv')\ntest_data.head()","fc84b42c":"#They are 4920 rows, 133 columns\ninput_data.shape","6b5801ee":"#seeing any null values are there with descending format\ninput_data.isnull().sum().sort_values(ascending=False)","98717343":"#looking how much percent each diseases having\ninput_data['prognosis'].value_counts(normalize = True)","74dba858":"#as we can see each no. diseases having the same percentage through bar chart\ninput_data['prognosis'].value_counts(normalize = True).plot.bar(color='red')\nplt.subplots_adjust(left = 0.9, right = 2 , top = 2, bottom = 1)","8afd6d56":"#checking the relationship between the variables by applying the correlation \ncorr = input_data.corr()\nmask = np.array(corr)\nmask[np.tril_indices_from(mask)] = False\nplt.subplots_adjust(left = 0.5, right = 16 , top = 20, bottom = 0.5)\nsns.heatmap(corr, mask=mask,vmax=.9, square=True,annot=True, cmap=\"YlGnBu\")","ac39a1be":"#took two high correlation variables and analysing if it is satisfying null hypothesis or alternate hypothesis\npd.crosstab(input_data['cold_hands_and_feets'],input_data['weight_gain'])","c1db229d":"#imported the chi square contingency\nfrom scipy.stats import chi2_contingency\n#as p value is  0.0  which is less than 0.05 then they are actually different from each other which satisfy the alternate hypothesis \nchi2_contingency(pd.crosstab(input_data['cold_hands_and_feets'],input_data['weight_gain']))","13f0c276":"x = input_data.drop(['prognosis'],axis =1)\ny = input_data['prognosis']","def30f74":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","de715fbb":"#imported naive_baye algorithm\nfrom sklearn.naive_bayes import MultinomialNB\n\n#fitted the model\nmnb = MultinomialNB()\nmnb = mnb.fit(x_train, y_train)\n\nscore = mnb.score(x_test, y_test)\nprint(\"Accuracy Score: \",score)","b9751175":"gbm_clf = GradientBoostingClassifier()\ngbm_clf.fit(x_train, y_train)\nscore = gbm_clf.score(x_train, y_train)\nprint(score)","13bc458d":"#by cross validating we got mean also 100%\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(mnb, x_test, y_test, cv=3)\nprint (scores)\nprint (scores.mean())","586de1be":"scores = cross_val_score(gbm_clf, x_test, y_test, cv=10)\nprint (scores)\nprint (scores.mean())","7e00106e":"real_diseases = y_test.values\ny_pred = gbm_clf.predict(x_test)\n#for the cross checking purpose i want to see if predicted values and actual values are same else it gives me worng prediction \nfor i in range(0, 20):\n    if y_pred[i] == real_diseases[i]:\n        print ('Pred: {0} Actual:{1}'.format(y_pred[i], real_diseases[i]))\n    else:\n        print('worng prediction')\n        print ('Pred: {0} Actual:{1}'.format(y_pred[i], real_diseases[i]))","ac4df9e7":"#imported Kfold\nfrom sklearn.model_selection import KFold\n\n## Function to run multiple algorithms with different K values of KFold.\ndef evaluate(train_data,kmax,algo):\n    test_scores = {}\n    train_scores = {}\n    for i in range(2,kmax,2):\n        kf = KFold(n_splits = i)\n        sum_train = 0\n        sum_test = 0\n        data = input_data\n        for train,test in kf.split(data):\n            train_data = data.iloc[train,:]\n            test_data = data.iloc[test,:]\n            x_train = train_data.drop([\"prognosis\"],axis=1)\n            y_train = train_data['prognosis']\n            x_test = test_data.drop([\"prognosis\"],axis=1)\n            y_test = test_data[\"prognosis\"]\n            algo_model = algo.fit(x_train,y_train)\n            sum_train += algo_model.score(x_train,y_train)\n            y_pred = algo_model.predict(x_test)\n            sum_test += accuracy_score(y_test,y_pred)\n        average_test = sum_test\/i\n        average_train = sum_train\/i\n        test_scores[i] = average_test\n        train_scores[i] = average_train\n        print(\"kvalue: \",i)\n    return(train_scores,test_scores)","7b406252":"from sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier()\nnb = MultinomialNB()\nfrom sklearn.linear_model import LogisticRegression\nlog = LogisticRegression()\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion='entropy',)\nfrom sklearn.ensemble import RandomForestClassifier\nran = RandomForestClassifier(n_estimators = 10)","642730d2":"algo_dict = {'l_o_g':log,'d_t':dt,'r_a_n':ran,'N_B' : nb, 'G_B' : gbm}\nalgo_train_scores={}\nalgo_test_scores={}","4f489a51":"max_kfold = 11\nfor algo_name in algo_dict.keys():\n    print(algo_name)\n    tr_score,tst_score = evaluate(input_data,max_kfold,algo_dict[algo_name])\n    algo_train_scores[algo_name] = tr_score\n    algo_test_scores[algo_name] = tst_score\nprint(algo_train_scores)\nprint(algo_test_scores)","70d34f27":"df_test = pd.DataFrame(algo_test_scores)\ndf_train = pd.DataFrame(algo_train_scores)\n\ndf_test.plot(grid = 1)\nplt.show()","96ea956d":"#building the model at k value 2 \ntest_scores={}\ntrain_scores={}\nfor i in range(2,4,2):\n    kf = KFold(n_splits = i)\n    sum_train = 0\n    sum_test = 0\n    data = input_data\n    for train,test in kf.split(data):\n        train_data = data.iloc[train,:]\n        test_data = data.iloc[test,:]\n        x_train = train_data.drop([\"prognosis\"],axis=1)\n        y_train = train_data['prognosis']\n        x_test = test_data.drop([\"prognosis\"],axis=1)\n        y_test = test_data[\"prognosis\"]\n        algo_model = gbm.fit(x_train,y_train)\n        sum_train += gbm.score(x_train,y_train)\n        y_pred = gbm.predict(x_test)\n        sum_test += accuracy_score(y_test,y_pred)\n    average_test = sum_test\/i\n    average_train = sum_train\/i\n    test_scores[i] = average_test\n    train_scores[i] = average_train\n    print(\"kvalue: \",i)","c90240c6":"print(train_scores)\nprint(test_scores)","2dcf1e0b":"importances = gbm.feature_importances_\nindices = np.argsort(importances)[::-1]","a16c9684":"features = input_data.columns[:-1]\nfor f in range(5):\n    print(\"%d. feature %d - %s (%f)\" % (f + 1, indices[f], features[indices[f]] ,importances[indices[f]]))","44dc0b64":"feature_dict = {}\nfor i,f in enumerate(features):\n    feature_dict[f] = i","1f335ef2":"feature_dict['redness_of_eyes'], feature_dict['cough']","3dc43eb3":"sample_x = [i\/52 if i ==52 else i\/24 if i==24 else i*0 for i in range(len(features))]\nlen(sample_x)","f5ab024f":"sample_x = np.array(sample_x).reshape(1,len(sample_x))\ngbm.predict(sample_x)","dce8947b":"gbm.predict_proba(sample_x)","9a1a8002":"gbm.__getstate__()","ebc9b9ab":"symptoms = x.columns","769fff1e":"regex = re.compile('_')","be88a67c":"symptoms = [i if regex.search(i) == None else i.replace('_', ' ') for i in symptoms ]","9e82afd0":"# Function to find all close matches of  \n# input string in given list of possible strings \nfrom difflib import get_close_matches  \ndef closeMatches(patterns, word): \n    print(get_close_matches(word, patterns, n=2, cutoff=0.7))","affa9bf2":"word = 'sivering'\ncloseMatches(symptoms, word)","8dff843c":"from flashtext import KeywordProcessor\nkeyword_processor = KeywordProcessor()\nkeyword_processor.add_keywords_from_list(symptoms)","caa907b1":"text = 'I have itching, joint pain and fatigue'\nkeyword_processor.extract_keywords(text)","bbefe177":"def predict_disease(query):\n    matched_keyword = keyword_processor.extract_keywords(query)\n    if len(matched_keyword) == 0:\n        print(\"No Matches\")\n    else:\n        regex = re.compile(' ')\n        processed_keywords = [i if regex.search(i) == None else i.replace(' ', '_') for i in matched_keyword]\n        print(processed_keywords)\n        coded_features = []\n        for keyword in processed_keywords:\n            coded_features.append(feature_dict[keyword])\n        #print(coded_features)\n        sample_x = []\n        for i in range(len(features)):\n            try:\n                sample_x.append(i\/coded_features[coded_features.index(i)])\n            except:\n                sample_x.append(i*0)\n        sample_x = np.array(sample_x).reshape(1,len(sample_x))\n        print('Predicted Disease: ',gbm.predict(sample_x)[0])\n                ","db64d8ed":"query = 'I have redness of eyes and cough'","7d8949da":"predict_disease(query)","0841eb6d":"symptoms[:20]","b2d3475d":"## 3.2 Cleaning our data","bfd4a563":"Writing our cleaned data","82dfa6ec":"### Trying out our classifier to learn diseases from the symptoms","7ccb9a74":"**Check for Alternate Hypothesis**","caa3a197":"**Check the relationship between the variables by applying the correlation **","9778e799":"Check for null or invalid entries","65d300f1":"## **2. Introduction**  \n**Due to big data progress in biomedical and healthcare communities, accurate study of medical data benefits early disease recognition, patient care and community services. When the quality of medical data is incomplete the exactness of study is reduced. Moreover, different regions exhibit unique appearances of certain regional diseases, which may results in weakening the prediction of disease outbreaks. In this project, it bid a Machine learning Decision tree map, Navie Bayes, Random forest algorithm by using structured and unstructured data from hospital. It also uses Machine learning algorithm for partitioning the data. To the highest of gen, none of the current work attentive on together data types in the zone of remedial big data analytics. Compared to several typical calculating algorithms, the scheming accuracy of our proposed algorithm reaches 98% with an regular speed which is quicker than that of the unimodal disease risk prediction algorithm and produces report.**","5de6f0a6":"**See the Target Variable Distribution**","51055423":"**-------------------------------**","8cdd8377":"**As p value is  0.0  which is less than 0.05 then they are actually different from each other which satisfy the alternate hypothesis **","7851518c":"**Symptoms Similarirty Matching [Future Scope]**","cb52d544":"**Cleaned dataset**","9ddc1cae":"## 3.3 Training our model for custom symptoms data","21afe389":"**Multinomial Naive Bayes**","7fe97b18":"## **1. Problem Statement**  \n**Understand Google\u2019s Dialogflow and use WebMD symptom checker data to build a conversational bot that can respond to users by asking step by step questions to find the symptom and its treatment details.**","2fc684cd":"**Take two symptom 'redness_of_eyes' and 'cough'**","97e4e946":"**Important Features**","b2a3f984":"Import some libraries","4805da4f":"WebMD doesn't provide sufficient symptoms data for disease prediction. So I used another data set provided by **Columbia University**.  \nThe following data set represents disease-symptom associations generated by an automated method based on information in textual discharge summaries of patients at New York Presbyterian Hospital admitted during 2004. The first column shows the disease, the second the number of discharge summaries containing a positive and current mention of the disease, and the associated symptom. Associations for the 150 most frequent diseases based on these notes were computed and the symptoms are shown ranked based on the strength of association.  The method used the MedLEE natural language processing system to obtain UMLS codes for diseases and symptoms from the notes; then statistical methods based on frequencies and co-occurrences were used to obtain the associations. A more detailed description of the automated method can be found in Wang X, Chused A, Elhadad N, Friedman C, Markatou M. Automated knowledge acquisition from clinical reports. AMIA Annu Symp Proc. 2008. p. 783-7. PMCID: PMC2656103.\n\n \n\nPlease contact friedman@dbmi.columbia.edu for any questions regarding the knowledge database.\nhttp:\/\/people.dbmi.columbia.edu\/~friedma\/Projects\/DiseaseSymptomKB\/index.html","bd5961f0":"**Test the model**","4b36977d":"# **Disease Prediction from Symptoms Data**","11e1dcb5":"## 3.1 Getting the data","af1724a7":"# 3. Methodology","0e926571":"**Train for Custom Training Data**","2ba9c33d":"**One Hot Encoding**","66cbae3b":"**So we got the result that it is a symptom of Common Cold**","86b7b9c2":"### Training a Classifier","229084a6":"Read the dataset of disease-symptom associations.","341f1640":"**Gradient Boosted Tree is the most efficient algorithm with K=2**","3f94d529":"**K-Fold Cross-Validation**","e17836d8":"**Check for any Null values**"}}