{"cell_type":{"2406f8a1":"code","1a5801ae":"code","e1243668":"code","17d4b486":"code","e59b78dd":"code","4c0cae7b":"code","bd7d6a54":"code","c09bc093":"code","23f757bc":"code","ec0b727f":"code","1523b298":"code","78f81d2b":"code","2643ee8c":"code","e1122a0f":"code","394cb253":"code","8e4ca4cc":"code","0dd5a4a7":"code","75e326b8":"code","d15d7d8a":"markdown","5819c882":"markdown","030a8d3a":"markdown","806c00bf":"markdown","90c894a5":"markdown","67f5f414":"markdown","05ccbe9a":"markdown","ed02efec":"markdown"},"source":{"2406f8a1":"import copy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.io.formats import style\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\nimport sklearn\nimport sklearn.preprocessing as sk_prep\nimport sklearn.model_selection as sk_ms\nimport sklearn.feature_selection as sk_fs\nimport sklearn.pipeline as sk_pipe\nimport sklearn.compose as sk_comp\nimport sklearn.base as sk_base\nimport sklearn.ensemble as sk_ens\nimport sklearn.metrics as sk_met\nimport sklearn.linear_model as sk_lm\nimport sklearn.tree as sk_tree\nimport sklearn.svm as sk_svm\nimport sklearn.decomposition as sk_de\nimport category_encoders as ce\n\nfrom scipy import stats\n\nimport lightgbm as lgbm","1a5801ae":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e1243668":"DATA_DIR = '\/kaggle\/input\/tabular-playground-series-jun-2021'\nRANDOM_STATE = 9003","17d4b486":"train_set = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\ntrain_data = train_set.iloc[:, 1:-1] # Feature columns\n\ntrain_y = train_set['target']\n\nprint(train_set.shape)\ntrain_set","e59b78dd":"# Fold preparation\n\ncv_folds = pd.Series(index=train_data.index, dtype='int64')\n\nkf = sk_ms.KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)","4c0cae7b":"loss_list_1 = []","bd7d6a54":"%%time\n\n# CV\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlr_cv_scores = sk_ms.cross_val_score(\n    model, train_data, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlr_loss = np.mean(lr_cv_scores)\nprint(f'Mean: {lr_loss} Folds: {lr_cv_scores}')","c09bc093":"%%time\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\n# CV\n\nlgbm_cv_scores = sk_ms.cross_val_score(\n    model, train_data, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlgbm_loss = np.mean(lgbm_cv_scores)\nprint(f'Mean: {lgbm_loss} Folds: {lgbm_cv_scores}')","23f757bc":"loss_list_1.append(['None', lr_loss, lgbm_loss])","ec0b727f":"# Create an array with True\/False for positive\/zero\ntr1_ar = train_data.values > 0\n# Convert data to 1\/0 and put in a DataFrame with the original column names\ntr1_X = pd.DataFrame(tr1_ar.astype('int32'), columns=train_data.columns, index=train_data.index)\n\ntr2_X, val2_X, tr2_y, val2_y = sk_ms.train_test_split(tr1_X, train_y, test_size=0.3, random_state=RANDOM_STATE)\n\ntr2_X","1523b298":"%%time\n\n# CV\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlr_cv_scores = sk_ms.cross_val_score(\n    model, tr1_X, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlr_loss = np.mean(lr_cv_scores)\nprint(f'Mean: {lr_loss} Folds: {lr_cv_scores}')","78f81d2b":"%%time\n\n# CV\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nlgbm_cv_scores = sk_ms.cross_val_score(\n    model, tr1_X, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlgbm_loss = np.mean(lgbm_cv_scores)\nprint(f'Mean: {lgbm_loss} Folds: {lgbm_cv_scores}')","2643ee8c":"loss_list_1.append(['PosEnc', lr_loss, lgbm_loss])","e1122a0f":"tr1_ar = train_data.values > 0\n# This time prepend \"pos_\" to the column names to distinguish them from the original columns\ntr1_X = pd.DataFrame(tr1_ar.astype('int32'), columns='pos_' + train_data.columns, index=train_data.index)\ntr3_X = pd.concat([train_data, tr1_X], axis=1)\n\ntr2_X, val2_X, tr2_y, val2_y = sk_ms.train_test_split(tr3_X, train_y, test_size=0.3, random_state=RANDOM_STATE)","394cb253":"%%time\n\n# CV\n\nmodel = sk_lm.LogisticRegression(solver='lbfgs', max_iter=1000)\n\nlr_cv_scores = sk_ms.cross_val_score(\n    model, tr3_X, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlr_loss = np.mean(lr_cv_scores)\nprint(f'Mean: {lr_loss} Folds: {lr_cv_scores}')","8e4ca4cc":"%%time\n\n# CV\n\nmodel = lgbm.LGBMClassifier(random_state=RANDOM_STATE)\n\nlgbm_cv_scores = sk_ms.cross_val_score(\n    model, tr3_X, train_y, \n    cv=kf,\n    scoring=sk_met.make_scorer(sk_met.log_loss, greater_is_better=False, needs_proba=True)\n) * -1\n\nlgbm_loss = np.mean(lgbm_cv_scores)\nprint(f'Mean: {lgbm_loss} Folds: {lgbm_cv_scores}')","0dd5a4a7":"loss_list_1.append(['PosEnc+Orig', lr_loss, lgbm_loss])","75e326b8":"loss_df = pd.DataFrame(loss_list_1, columns=['Method', 'LR_Loss', 'LGBM_Loss'])\n# cmap = matplotlib.colors.Colormap('viridis')\nstyle.Styler(loss_df, precision=4).background_gradient(cmap='viridis', vmin=1.70, vmax=1.85)","d15d7d8a":"We see from this table that positive encoding significantly improves logistic regression, but does little for LightGBM.  Including the original values so that no data is lost does not help much with either model.","5819c882":"# Setup","030a8d3a":"# Load Data","806c00bf":"# Synopsis\n\nThis shows the results of reducing all the features in this dataset to simply 0 or 1, where 1 replaces all positive values.  I have called this \"positive encoding.\"  We can see here that this feature engineering is useful in this dataset for linear regression.  This dataset is a special case, and there may be very few other datasets that would benefit from this type of encoding.\n\nBackground\n\nI had originally checked this just to see how much information was preserved with this simplification, and also as a preparation to using this in combination with the original features.  The reason for this combination would be to allow linear models to make a distinction between zeros and low positive numbers; that appeared to have some relevance in my original exploratory analysis.\n\nI was surprised to find that this one simple encoding that loses so much information allowed a basic linear regression to score as well as a basic LightGBM model.\n\nSince tree-based models like LightGBM are already free to handle any one value as special, it is no surprise that it does not gain from this encoding.  But it is unexpected that it loses very little in spite of having all positive values of each feature compressed together.","90c894a5":"# Models with no FE for Comparison","67f5f414":"# Positive Encoding Plus Original","05ccbe9a":"# Summary","ed02efec":"# Positive Encoding"}}