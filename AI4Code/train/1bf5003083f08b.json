{"cell_type":{"03e8d3e8":"code","5ddab58d":"code","f0325b34":"code","7f2a54b8":"code","ae6cdcee":"code","d906d57c":"code","9ae97895":"code","148aa800":"code","f4b4f671":"code","cbaef072":"code","03e9d6a1":"code","e963a2af":"code","3473a0bf":"code","d8af249f":"code","7279f859":"code","a9b3e284":"code","f4a319d3":"code","7826545b":"code","3440ca44":"code","fbbb761a":"code","f764948b":"code","21bf6588":"code","775c78a5":"code","0190ecac":"code","f93f66b0":"code","2ce9552e":"code","3528a2c0":"markdown","c80b530a":"markdown","85164d0a":"markdown","0541c189":"markdown","8b768210":"markdown","a8c2ad54":"markdown","e07fbe50":"markdown","aae96c21":"markdown","2d58fed6":"markdown"},"source":{"03e8d3e8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","5ddab58d":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import LeakyReLU\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nimport matplotlib.pyplot as plt ","f0325b34":"train = pd.read_csv('\/kaggle\/input\/Kannada-MNIST\/train.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/test.csv\")\nDig_MNIST = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/Dig-MNIST.csv\")","7f2a54b8":"sample_sub = pd.read_csv(\"\/kaggle\/input\/Kannada-MNIST\/sample_submission.csv\")","ae6cdcee":"print(\"Train set shape = \" +str(train.shape))\nprint(\"Test set shape = \" +str(test.shape))\nprint(\"Dif set shape = \" +str(Dig_MNIST.shape))","d906d57c":"train.head()","9ae97895":"X=train.iloc[:,1:].values \nY=train.iloc[:,0].values \nY[:10]","148aa800":"X = X.reshape(X.shape[0], 28, 28,1) \nprint(X.shape)\n","f4b4f671":"Y = keras.utils.to_categorical(Y, 10) \nprint(Y.shape)","cbaef072":"test.head()","03e9d6a1":"x_test=test.drop('id', axis=1).iloc[:,:].values\nx_test = x_test.reshape(x_test.shape[0], 28, 28,1)\nx_test.shape","e963a2af":"Dig_MNIST.head()","3473a0bf":"x_dig=Dig_MNIST.drop('label',axis=1).iloc[:,:].values\nprint(x_dig.shape)\nx_dig = x_dig.reshape(x_dig.shape[0], 28, 28,1)\nx_dig.shape","d8af249f":"y_dig=Dig_MNIST.label\ny_dig.shape","7279f859":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size = 0.10, random_state=42) ","a9b3e284":"train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 10,\n                                   width_shift_range = 0.25,\n                                   height_shift_range = 0.25,\n                                   shear_range = 0.1,\n                                   zoom_range = 0.25,\n                                   horizontal_flip = False)","f4a319d3":"valid_datagen = ImageDataGenerator(rescale=1.\/255) ","7826545b":"def lr_decay(epoch):#lrv\n    return initial_learningrate * 0.99 ** epoch","3440ca44":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), padding='same', input_shape=(28, 28, 1)),\n    tf.keras.layers.BatchNormalization(momentum=0.5, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(filters=64,  kernel_size=(3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.MaxPooling2D(2, 2),\n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),    \n    \n    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.2, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n    tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), padding='same'),\n    tf.keras.layers.BatchNormalization(momentum=0.1, epsilon=1e-5, gamma_initializer=\"uniform\"),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.2),\n    \n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256),\n    tf.keras.layers.LeakyReLU(alpha=0.1),\n \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nmodel.summary()\n\n","fbbb761a":"\nfrom keras.callbacks import ReduceLROnPlateau\n\ninitial_learningrate=2e-3\nbatch_size = 1024\nepochs = 30\ninput_shape = (28, 28, 1)\n\nmodel.compile(loss=\"categorical_crossentropy\",\n              optimizer=RMSprop(lr=initial_learningrate),\n              metrics=['accuracy'])\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='loss', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.2, \n                                            min_lr=0.00001)\n\nhistory = model.fit_generator(\n      train_datagen.flow(X_train,Y_train, batch_size=batch_size),\n      steps_per_epoch=100,\n      epochs=epochs,\n      callbacks=[learning_rate_reduction],\n      validation_data=valid_datagen.flow(X_valid,Y_valid),\n      validation_steps=50\n)","f764948b":"from sklearn import metrics\n\npreds_dig=model.predict_classes(x_dig\/255)\nmetrics.accuracy_score(preds_dig, y_dig)","21bf6588":"accuracy = history.history['acc']\nval_accuracy = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'r', label='Test accuracy')\nplt.title('Accuracy')\nplt.legend()\nplt.show()","775c78a5":"plt.figure()\nplt.plot(epochs, history.history['loss'], 'b', label='Training loss')\nplt.plot(epochs, history.history['val_loss'], 'r', label='Test loss')\nplt.title('Loss')\nplt.legend()\nplt.show()","0190ecac":"predictions = model.predict_classes(x_test\/255.)","f93f66b0":"predictions","2ce9552e":"submission = pd.read_csv('..\/input\/Kannada-MNIST\/sample_submission.csv')\nsubmission['label'] = predictions\nsubmission.to_csv(\"submission.csv\",index=False)","3528a2c0":"Let's fit the model on the whole training set.","c80b530a":"We slice the dataframes to define the features and the labels","85164d0a":"We split the data into training and validation set.","0541c189":"Now we must reshape the date to make it Keras friendly.","8b768210":"We use Keras ImageDataGenerator to artificially increase our training set.","a8c2ad54":"Now we convert the labels to categorical.","e07fbe50":"Let's load the data.","aae96c21":"The next function reduces the learning rate as the training advances.","2d58fed6":"I forked notebook from [here](https:\/\/www.kaggle.com\/bustam\/cnn-in-keras-for-kannada-digits) [bustam](http:\/\/https:\/\/www.kaggle.com\/bustam) and a small change in learning_rate_reduction gave me a little better score(0.99).\n\nThank you [bustam], you just made my first step of Kaggle and CNN journey more joyful. "}}