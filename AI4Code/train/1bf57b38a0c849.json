{"cell_type":{"85c92922":"code","701a4584":"code","7b30c1d1":"code","e9edae0b":"code","796da74f":"code","3a3fa89a":"code","96c3d26c":"code","4474f659":"code","e812d518":"code","4b4c1902":"code","a7aedd8c":"code","78fcaa82":"code","fe59d402":"code","daf35039":"code","c4ab324e":"code","3ecbd8d2":"code","3f586f9a":"code","89f0ceb3":"markdown","91154ac1":"markdown","69560c1c":"markdown","39906c1e":"markdown","3ddda1a3":"markdown","df67a192":"markdown","ef44956c":"markdown","09b36c41":"markdown","49f60fa9":"markdown","c4e875c8":"markdown","c291c381":"markdown","5c276dab":"markdown","02aa535c":"markdown"},"source":{"85c92922":"import numpy as np\nimport random\nimport matplotlib.pyplot as plt","701a4584":"class ContextBandit:\n    def __init__(self, arms=10):\n        self.arms = arms\n        self.init_distribution(arms)\n        self.update_state()\n        \n    def init_distribution(self, arms):\n        self.bandit_matrix = np.random.rand(arms,arms)\n        \n    def reward(self, prob):\n        reward = 0\n        for i in range(self.arms):\n            if random.random() < prob:\n                reward += 1\n        return reward\n        \n    def get_state(self):\n        return self.state\n\n    def update_state(self):\n        self.state = np.random.randint(0,self.arms)\n        \n    def get_reward(self,arm):\n        return self.reward(self.bandit_matrix[self.get_state()][arm])\n        \n    def choose_arm(self, arm):\n        reward = self.get_reward(arm)\n        self.update_state()\n        return reward","7b30c1d1":"env = ContextBandit(arms=10)\nstate = env.get_state()\nreward = env.choose_arm(1)\nprint('State ',state)\nprint('Reward ',reward)","e9edae0b":"arms = 10\nN, D_in, H, D_out = 1, arms, 100, arms","796da74f":"def relu(x):\n    return x * (x > 0)\n\ndef drelu(x):\n    return 1. * (x > 0)","3a3fa89a":"def mse(y, y_hat):\n    return np.mean((y - y_hat) ** 2)\n\ndef dmse(y, y_hat):\n    return np.mean(y - y_hat)","96c3d26c":"def softmax(av, tau):\n    return np.exp(av \/ tau) \/ np.sum( np.exp(av \/ tau) )","4474f659":"def one_hot(N, pos, val=1):\n    one_hot_vec = np.zeros(N)\n    one_hot_vec[pos] = val\n    return one_hot_vec","e812d518":"def forward(x, alpha):\n    wi, wo = alpha\n    \n    wil = x.reshape(1,10).dot(wi)\n    wilr = relu(wil)\n    \n    wol = wilr.dot(wo)\n    wolr = relu(wol)\n    return wil, wilr, wol, wolr","4b4c1902":"def backward(x, pred, y, alpha, params):\n    wil, wilr, wol, wolr = params\n    wi, wo = alpha\n    \n    pred = pred.reshape(10)\n    dloss = dmse(pred, y)\n    \n    # dloss * drelu(wol) * wilr\n    dwo = np.dot(wilr.T, dloss * drelu(wol))\n    # dloss * drelu(wol) * wo * drelu(wil) * x\n    dwi = np.dot(x.reshape(10,1), np.dot((dloss * drelu(wol)), (wo * drelu(wil).T).T))\n    \n    return dwi, dwo","a7aedd8c":"def SGE(alpha, grads, lr=1e-4):\n    wi, wo = alpha\n    dwi, dwo = grads\n    \n    wi += lr * dwi\n    wo += lr * dwo\n    \n    return wi, wo","78fcaa82":"wi = np.random.uniform(0, 0.5, size=(D_in, H))\nwo = np.random.uniform(0, 0.5, size=(H, D_out))\n\nalpha = wi, wo","fe59d402":"rewards = []\ntemp_rewards = []\nenv = ContextBandit(arms=10)\ncur_state = one_hot(arms, env.get_state())","daf35039":"def get_choice(params):\n    av_softmax = softmax(params[-1], tau=np.max(params[-1]))\n    av_softmax \/= av_softmax.sum()\n\n    return np.random.choice(arms, p=av_softmax.reshape(10))","c4ab324e":"for epoch in range(5000):\n    \n    params = forward(cur_state, alpha)\n\n    choice = get_choice(params)\n\n    cur_reward = env.choose_arm(choice)\n\n    one_hot_reward = params[-1].reshape(10).copy()\n    one_hot_reward[choice] = cur_reward\n\n    reward = one_hot_reward\n    temp_rewards.append(reward)\n\n    loss = mse(params[-1], reward)\n    \n    if(epoch % 1000 == 0):\n        mean_rewards = np.mean(temp_rewards)\n        print(\"Mean rewards :\", mean_rewards)\n        rewards.append(mean_rewards)\n        temp_rewards = []\n\n    dwi, dwo = backward(cur_state, params[-1], reward, alpha, params)\n    dwi = np.clip(dwi, -1, 1)\n    dwo = np.clip(dwo, -1, 1)\n    grads = dwi, dwo\n    \n    alpha = SGE(alpha, grads)\n    \n    cur_state = one_hot(arms, env.get_state())","3ecbd8d2":"plt.title('Mean rewards over epochs')\nplt.plot(rewards, 'bo')","3f586f9a":"cur_state = one_hot(arms, env.get_state())\nparams = forward(cur_state, alpha)\nchoice = get_choice(params)\ncur_reward = env.choose_arm(choice)\nprint('State: %d, Choice: %d, Reward: %d' % (np.argmax(cur_state), choice, cur_reward))","89f0ceb3":"<a id='game'><\/a>\n# Context Bandit Game Implementation","91154ac1":"## Content table\n\n1. [Description](#description)\n2. [Game implementation](#game)\n3. [Activation Functions](#activation)\n4. [Implementation](#implementation)\n5. [Results](#results)","69560c1c":"# Stochastic gradient descent","39906c1e":"<a id='activation'><\/a>\n# Activation functions","3ddda1a3":"<img align=left width=400 src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/82\/Las_Vegas_slot_machines.jpg\/800px-Las_Vegas_slot_machines.jpg\" \/>","df67a192":"<a id='implementation'><\/a>\n# Implementation","ef44956c":"## Forward and backward functions","09b36c41":"\n# Description\n\nIn probability theory, the multi-armed bandit problem, is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.<br>\n\nThis is a classic reinforcement learning problem that exemplifies the exploration\u2013exploitation tradeoff dilemma. The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine.<br>\n\nhttps:\/\/en.wikipedia.org\/wiki\/Multi-armed_bandit","49f60fa9":"# Credits\nContextBandit game code from<br>\nhttps:\/\/www.manning.com\/books\/deep-reinforcement-learning-in-action?query=deep%20reinforcement%20learning","c4e875c8":"# Context Bandit\nReinforcement Learning<br>\nFrom scratch<br>\nBy: Alin Cijov<br>","c291c381":"## One hot encoding for state","5c276dab":"## Training","02aa535c":"<a id='results'><\/a>\n# Results"}}