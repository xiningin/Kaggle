{"cell_type":{"ea05454a":"code","8a50fbdf":"code","9ed909b0":"code","6ca0fd9d":"markdown","0cd9a574":"markdown"},"source":{"ea05454a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8a50fbdf":"\nfrom sklearn.cluster import KMeans\n\n\n'''\nunsupervised model :  understand data from observation\n'''\n\nclass Gaussian_Mixture_Model(object):\n    def __init__(self,m,threshold=1e-4):\n        self.m=m\n        self.threshold=threshold #stop threshold\n\n    def __call__(self,n,space_num=10000):\n        ''':\n        after training : get coef,mu,cov\n        sampling : sampling n data based on gaussian mixture distribution\n        gaussian mixture distribtuion:sum over all k --- coef_k*N(xi|u_k,cov_k)\n        '''\n\n        min=self.x.min(axis=0)\n        max=self.x.max(axis=0)\n\n        #random choose a point in a specific dimension\n        space=np.array([np.random.choice(np.linspace(np.floor(min[d]), np.floor(max[d]), num=space_num),size=space_num)\\\n                        for d in range(self.x.shape[-1])]).T  #(space_num,dim)\n        prob=np.zeros((space_num))\n\n        for idx,xi in enumerate(space):\n            xi=xi[np.newaxis,:] #(1,d)\n            mixture_prob=np.sum(self.coef*self.multivariate_gaussian_pdf(xi,self.mu,self.cov))\n            prob[idx]=mixture_prob\n\n        prob=prob\/prob.sum()\n        sample_idx=np.random.choice(range(0,space_num),p=prob,size=n)\n\n        sample=space[sample_idx,:]\n\n        return sample\n\n\n    def init_parameters(self,x):\n        ''':parameter\n        initialize (aimed to speed up convergence):\n        coef: % of each cluster number\n        mu:midpoint of each cluster\n        covariance matrix sigma: covariance of each cluster\n        '''\n        try:\n            assert len(x.shape)==2\n        except:\n            raise ValueError(\"x dimension can't not equal to 2\")\n\n        n, dim = x.shape\n\n        kmeans=KMeans(n_clusters=self.m)\n        pred=kmeans.fit_predict(x)\n\n        #m mixture ceofficient\n        count=np.array([(pred==k).sum() for k in range(self.m)])\n        coef=count\/count.sum() #(m,)\n\n        #m parameters of gaussian distributions\n        mu=kmeans.cluster_centers_ #(m,d)\n        cov=np.array([np.cov(np.array([x[idx,:] for idx in np.where(pred==k)[0]]).T) for k in range(self.m)])\n\n        return coef,mu,cov\n\n\n    def multivariate_gaussian_pdf(self,x,mu,cov):\n        ''':parameter\n\n        intuition:xi\u5728\u67d0\u4e00\u7fa4\u7684\u6a5f\u7387\u662f\u591a\u5c11,\u5982\u679c\u96e2mu\u8d8a\u9060\u6a5f\u7387\u8d8a\u5c0f\n        x:(1,d)\n        mu:(m,d)\n        cov:(m,d,d)\n        '''\n        x=x[:,np.newaxis,:] #(1,1,d)\n        mu=mu[:,np.newaxis,:] #(m,1,d)\n        dim=x.shape[-1]\n        term1=np.power(2*np.pi,-0.5*dim)*np.power(np.linalg.det(cov),-0.5) #(m,)\n        term2=np.exp(-0.5*np.matmul(np.matmul((x-mu),np.linalg.inv(cov)),np.transpose(x-mu,axes=(0,2,1)))) #(m,1,1)\n\n        return term1*(term2.flatten()) #(m,)\n\n\n    def E_step(self,x,coef,mu,cov):\n        ''':parameter\n        x:(n,d)\n        coef:(m,)\n        mu:(m,d)\n        cov:(m,d,d)\n\n        return post prob\n        '''\n        for idx,xi in enumerate(x):#xi:(d,)\n            xi=xi[np.newaxis,:] #(1,d)\n            prob=self.multivariate_gaussian_pdf(xi,mu,cov) #(m,)\n            denominator=np.sum(coef*prob) #scaler\n            molecular=coef*prob #(m,)\n            wi=molecular\/denominator  #(m,)\n            if idx==0:\n                w=wi[np.newaxis,:] #(1,m)\n            else:\n                w=np.vstack((w,wi))\n        return w #(n,m)\n\n\n    def M_step(self,x,w):\n        ''':parameter\n        x:(n,d)\n        w:(n,m)\n        (x-mu) (x-mu).T\n        '''\n        n=w.shape[0]\n        sum_w=np.sum(w,axis=0) #(m,)\n        coef=sum_w\/n #(m,)\n\n\n        mu=np.matmul(w.T,x)\/sum_w[:,np.newaxis] #(m,d)\n\n\n        cov=[] #(m,d,d)\n        for k in range(self.m):\n            cov_k=0\n            sum_wi=0\n            for i in range(x.shape[0]):\n                coef_ik=w[i,k]\n                sum_wi+=coef_ik\n                residual=(x[i,:]-mu[k,:])[:,np.newaxis] #(d,1)\n                cov_k+=coef_ik*np.matmul(residual,residual.T) #(d,d)\n            cov.append(cov_k\/sum_wi)\n        cov=np.array(cov) #(m,d,d)\n\n        return coef,mu,cov\n\n\n    def fit_data_distribution(self,x):\n        '''\n        EM-Algorithm\n        '''\n        coef,mu,cov=self.init_parameters(x)\n\n        cov_prev=cov.copy()\n        i=0\n        while True:\n            w=self.E_step(x,coef,mu,cov) #expectation\n            coef, mu, cov=self.M_step(x,w)  #maximization\n\n            # convergence criterion\n            if np.mean(np.abs(cov-cov_prev))<self.threshold:\n                break\n            else:\n                cov_prev = cov.copy()\n        self.coef=coef #(m,)\n        self.mu=mu #(m,d)\n        self.cov=cov  #(m,d,d)\n        self.x=x #(n,d)\n\n\n\n\n\n\n","9ed909b0":"\nimport matplotlib.pyplot as plt\n\n\n\nn_samples = 50\n# generate random sample, two components\nnp.random.seed(0)\n# generate spherical data centered on (20, 20)\nshifted_gaussian = np.random.randn(n_samples, 2) + np.array([20, 20])\n# generate zero centered stretched Gaussian data\nC = np.array([[0., -0.7], [3.5, .7]])\nstretched_gaussian = np.dot(np.random.randn(n_samples, 2), C)\n# concatenate the two datasets into the final training set\nx = np.vstack([shifted_gaussian, stretched_gaussian])\n\nprint('fit data distribution...')\ngmm=Gaussian_Mixture_Model(m=2,threshold=1e-4)\ngmm.fit_data_distribution(x)\n\nprint('sampling...')\nsample=gmm(n=200,space_num=10000)\n\nplt.scatter(sample[:,0],sample[:,1],color='red',edgecolor='black',marker='s',s=40,label='generated data')\nplt.scatter(x[:,0],x[:,1],color='cyan',edgecolor='black',marker='s',s=40,label='real data')\nplt.legend()\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.show()","6ca0fd9d":"My very detail complete note about Gaussian Mixture Model + EM-Algorithm \n(sorry I use some chinese in my note :( ):  https:\/\/drive.google.com\/file\/d\/1uuB4A37MMh_QQPD1X8JDu8idCHPI552U\/view?usp=sharing","0cd9a574":"Test generate data from model "}}