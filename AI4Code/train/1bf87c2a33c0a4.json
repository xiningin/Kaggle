{"cell_type":{"9880411a":"code","a8abf10d":"code","cf39ab43":"code","e311dddc":"code","47949d65":"code","62493871":"code","8a301a26":"code","f9e3f445":"code","4b26b8da":"code","3c9998e0":"code","23ddb178":"code","cec4960b":"code","b749140d":"code","f47cc8f6":"code","ee12d70c":"code","0a18d6dc":"code","f8683dfd":"code","ddb456be":"code","309598bb":"code","0e7d7a00":"code","5b2569a6":"code","833ff2bb":"code","f33fc4b5":"code","9e62f71d":"code","3bb8c76d":"code","1dac6a25":"code","c466bfb0":"code","c182cb79":"code","a678ad55":"code","4219fa23":"code","5b7cd5fc":"code","a0902c95":"code","e4ad60c2":"code","f7f17e76":"code","a036b32b":"code","e28ce438":"code","fa7f02e4":"code","c53fbb04":"code","0d8ca615":"code","a8f0f8b3":"code","47759b55":"code","a0bce5aa":"code","836d6615":"code","1cc95d9d":"code","c0705536":"code","fbc53a8c":"code","ef4f666e":"code","98c064a3":"code","838dc2ac":"code","e86544af":"markdown","96f4d739":"markdown","69da956f":"markdown","b4effb65":"markdown","3783ac71":"markdown","01ca78f9":"markdown","c753ee8e":"markdown","a47c35fd":"markdown","3d86704f":"markdown","d0502a41":"markdown","30d0ffb5":"markdown","9ed1f934":"markdown","34434aee":"markdown","7222663d":"markdown","7caab970":"markdown","823b2dd4":"markdown","804b7a78":"markdown","f042e10b":"markdown","9883cb77":"markdown","cbad4a9e":"markdown","9005db6e":"markdown","1b12d91e":"markdown","2546729b":"markdown","63d354bc":"markdown","5ba2dc8f":"markdown","1087d77a":"markdown","576954e1":"markdown","76185f60":"markdown","8ab7184f":"markdown","939ca541":"markdown","8d70c4bd":"markdown"},"source":{"9880411a":"__seed = 222\n__n_folds = 10\n\n__data_path =\"..\/input\/actuarial-loss-estimation\"\n\nimport numpy as np \n\nimport pandas as pd \npd.set_option('max_columns', 100)\npd.set_option('max_rows', 200)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\n# To be able to create my own scikit transformer\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\nfrom sklearn.metrics import mean_squared_error\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred, squared = False)\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfolds = StratifiedKFold(n_splits=__n_folds, random_state = __seed, shuffle = True)","a8abf10d":"train_set = pd.read_csv(f\"{__data_path}\/train.csv\", index_col = \"ClaimNumber\")\ntest_set = pd.read_csv(f\"{__data_path}\/test.csv\", index_col = \"ClaimNumber\")\n\nprint(\"N rows in train set : {:,.0f} - in test set : {:,.0f}\".format(train_set.shape[0], test_set.shape[0]))\nprint(f\"N columns in train set : {train_set.shape[1]}\")\nprint(f\"N columns in test set : {test_set.shape[1]}\")\nprint(\"\\nColumns in train set are : {}\".format(train_set.columns))\n\nprint(\"\\nDtypes of each columns : \\n{}\".format(train_set.dtypes))\n\ntrain_set.head()","cf39ab43":"nrows, ncols = 3, 2\nfig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize=(15, 4.5*nrows))\n\nplt.subplots_adjust(hspace = 0.3, wspace = 0.5)\n    \nax = plt.subplot(nrows, ncols, 1)\nsns.distplot(train_set[\"UltimateIncurredClaimCost\"], color=\"green\", label=\"train\", ax= ax)\nax.set_title(\"TRAIN - Ultimate Incurred Claim Cost\");\n\nax = plt.subplot(nrows, ncols, 2)\nsns.distplot(train_set.loc[train_set[\"UltimateIncurredClaimCost\"]<30000, \"UltimateIncurredClaimCost\"], color=\"green\", label=\"train\", ax=ax)\nax.set_title(\"TRAIN - Ultimate Incurred Claim Cost (without extrem values > 30 000)\");\n\nax = plt.subplot(nrows, ncols, 3)\nsns.distplot(train_set[\"InitialIncurredCalimsCost\"], color=\"green\", label=\"train\", ax= ax)\nax.set_title(\"TRAIN - Initial Incurred Claim Cost\");\n\nax = plt.subplot(nrows, ncols, 4)\nsns.distplot(train_set.loc[train_set[\"InitialIncurredCalimsCost\"]<30000, \"InitialIncurredCalimsCost\"], color=\"green\", label=\"test\", ax=ax)\nax.set_title(\"TRAIN - Initial Incurred Claim Cost (without extrem values > 30 000)\");\n\nax = plt.subplot(nrows, ncols, 5)\nsns.distplot(test_set[\"InitialIncurredCalimsCost\"], color=\"blue\", label=\"train\", ax= ax)\nax.set_title(\"TEST - Initial Incurred Claim Cost\");\n\nax = plt.subplot(nrows, ncols, 6)\nsns.distplot(test_set.loc[test_set[\"InitialIncurredCalimsCost\"]<30000, \"InitialIncurredCalimsCost\"], color=\"blue\", label=\"test\", ax=ax)\nax.set_title(\"TEST - Initial Incurred Claim Cost (without extrem values > 30 000)\");","e311dddc":"print(\"RMSE with InitialIncurredCalimsCost : {:,.0f}\".\\\n    format(rmse(train_set[\"UltimateIncurredClaimCost\"], train_set[\"InitialIncurredCalimsCost\"])))","47949d65":"temp = train_set.loc[train_set[\"UltimateIncurredClaimCost\"]>30000, \"UltimateIncurredClaimCost\"]\ntemp = pd.cut(temp, 10)\nprint(temp.value_counts().sort_index())\n\ntrain_set.loc[train_set[\"UltimateIncurredClaimCost\"]>800000]","62493871":"df_nan_values = train_set.isna().sum(axis=0).to_frame(name=\"nan_in_train\")\ndf_nan_values[\"nan_in_test\"] = test_set.isna().sum(axis=0).astype(pd.Int8Dtype())\ndf_nan_values[(df_nan_values[\"nan_in_test\"]>0)|(df_nan_values[\"nan_in_train\"]>0)]","8a301a26":"for df in [train_set, test_set]:\n    df['YearOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).year\n    df['YearReported']  = pd.DatetimeIndex(df['DateReported']).year\n\nfeatures_to_check = ['YearOfAccident', \"YearReported\", \"MaritalStatus\", \"Gender\", \"PartTimeFullTime\"\n        , \"DaysWorkedPerWeek\", \"DependentChildren\", \"DependentsOther\", \"Age\"]\nnrows = len(features_to_check)\n\nhsize = [] ; tot_hsize = 0\nfor i, f in enumerate(features_to_check):\n    hsize.append(len(train_set[f].unique()))\n    tot_hsize += hsize[i]\n\nfig = plt.figure(figsize = (20, .5*tot_hsize))\nplt.subplots_adjust(hspace = 0.2, wspace = 0.1)\n\n# https:\/\/matplotlib.org\/3.3.3\/tutorials\/intermediate\/gridspec.html\nspec = fig.add_gridspec(nrows=nrows, ncols=2\n            , width_ratios=[1, 1], height_ratios=hsize)\n\nfor i, f in enumerate(features_to_check):\n    \n    ax = fig.add_subplot(spec[i, 0])\n    df_temp = train_set.groupby(f)[\"WeeklyWages\"].count().reset_index()\n    sns.barplot(x=\"WeeklyWages\", y=f, orient='h', data=df_temp, color=\"green\", ax=ax)\n    plt.xlabel('Count')\n    plt.title(f\"Number of claims by {f} in Train set\");\n    \n    ax = fig.add_subplot(spec[i, 1])\n    df_temp = test_set.groupby(f)[\"WeeklyWages\"].count().reset_index()\n    sns.barplot(x=\"WeeklyWages\", y=f, orient='h', data=df_temp, color=\"blue\", ax=ax)\n    plt.xlabel('Count')\n    plt.title(f\"Number of claims by {f} in Test set\");\n    \nfig.subplots_adjust(top=0.96)\nfig.suptitle(\"Train Set on the left     Test Set on the right\", fontsize=\"xx-large\");","f9e3f445":"from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\nfig, ax = plt.subplots(figsize=(15, 6))\n\nax = plt.subplot(1, 2, 1)\nvect = CountVectorizer(min_df = 5000, stop_words = \"english\").fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nstat_voc = pd.Series({f:n for f, n in zip(vect.get_feature_names(), claimdesc.sum(axis=0).tolist()[0])}).sort_values(ascending=False)\nsns.barplot(y=stat_voc.index[:10], x=stat_voc.values[:10], orient='h', color = \"green\", ax=ax)\nax.set_title(\"Most frequent words in Claim Desciption - Train set\");\n\nax = plt.subplot(1, 2, 2)\nvect = CountVectorizer(min_df = 2000, stop_words = \"english\").fit(test_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(test_set[\"ClaimDescription\"])\nstat_voc = pd.Series({f:n for f, n in zip(vect.get_feature_names(), claimdesc.sum(axis=0).tolist()[0])}).sort_values(ascending=False)\nsns.barplot(y=stat_voc.index[:10], x=stat_voc.values[:10], orient='h', color = \"blue\", ax=ax)\nax.set_title(\"Most frequent words in Claim Desciption - Test set\");","4b26b8da":"stop_words = ENGLISH_STOP_WORDS.union([\"right\", \"left\", \"strain\"])\n\nmost_frequent = 15\n\nfig, ax = plt.subplots(figsize=(15, 20))\nplt.subplots_adjust(hspace = 0.1, wspace = 0.3)\n\nax = plt.subplot(3, 2, 1)\nvect = CountVectorizer(min_df = 1000, stop_words = stop_words).fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nfeatures_names = vect.get_feature_names()\nstat_voc = pd.Series({f:n for f, n in zip(features_names, claimdesc.sum(axis=0).tolist()[0])})\nstat_voc.sort_values(ascending=False, inplace=True)\nsns.barplot(y=stat_voc.index[:most_frequent], x=stat_voc.values[:most_frequent], orient='h', ax=ax, color=\"green\")\nplt.title(\"Most frequent words (without ['right', 'left', 'strain'])\");\n\nax = plt.subplot(3, 2, 2)\nvect = CountVectorizer(ngram_range=(2, 2), min_df = 100, stop_words = stop_words).fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nfeatures_names = vect.get_feature_names()\nstat_voc = pd.Series({f:n for f, n in zip(features_names, claimdesc.sum(axis=0).tolist()[0])})\nstat_voc.sort_values(ascending=False, inplace=True)\nsns.barplot(y=stat_voc.index[:most_frequent], x=stat_voc.values[:most_frequent], orient='h', ax=ax, color=\"green\")\nplt.title(\"Most frequent bi-grams (without ['right', 'left', 'strain'])\");\n\nax = plt.subplot(3, 2, 3)\nvect = CountVectorizer(ngram_range=(3, 3), min_df = 100, stop_words = stop_words).fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nfeatures_names = vect.get_feature_names()\nstat_voc = pd.Series({f:n for f, n in zip(features_names, claimdesc.sum(axis=0).tolist()[0])})\nstat_voc.sort_values(ascending=False, inplace=True)\nsns.barplot(y=stat_voc.index[:most_frequent], x=stat_voc.values[:most_frequent], orient='h', ax=ax, color=\"green\")\nplt.title(\"Most frequent 3-grams (without ['right', 'left', 'strain'])\");\n\nax = plt.subplot(3, 2, 4)\nvect = CountVectorizer(ngram_range=(4, 4), min_df = 100, stop_words = stop_words).fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nfeatures_names = vect.get_feature_names()\nstat_voc = pd.Series({f:n for f, n in zip(features_names, claimdesc.sum(axis=0).tolist()[0])})\nstat_voc.sort_values(ascending=False, inplace=True)\nsns.barplot(y=stat_voc.index[:most_frequent], x=stat_voc.values[:most_frequent], orient='h', ax=ax, color=\"green\")\nplt.title(\"Most frequent 4-grams (without ['right', 'left', 'strain'])\")\n\nax = plt.subplot(3, 2, 5)\nvect = CountVectorizer(ngram_range=(5, 5), min_df = 100, stop_words = stop_words).fit(train_set[\"ClaimDescription\"])\nclaimdesc = vect.transform(train_set[\"ClaimDescription\"])\nfeatures_names = vect.get_feature_names()\nstat_voc = pd.Series({f:n for f, n in zip(features_names, claimdesc.sum(axis=0).tolist()[0])})\nstat_voc.sort_values(ascending=False, inplace=True)\nsns.barplot(y=stat_voc.index[:most_frequent], x=stat_voc.values[:most_frequent], orient='h', ax=ax, color=\"green\")\nplt.title(\"Most frequent 5-grams (without ['right', 'left', 'strain'])\");","3c9998e0":"for df in [train_set, test_set]:\n    \n    # Date Accident transformation\n    df['MonthOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).month\n    df['DayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day\n    df['WeekdayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day_name()\n    df['HourOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).hour\n        \n    # About reported data\n    df['WeekdayOfReported']  = pd.DatetimeIndex(df['DateReported']).day_name()\n        \n    # Reporting delay of report in days\n    df['DaysReportDelay'] = pd.DatetimeIndex(df['DateReported']).date - pd.DatetimeIndex(df['DateTimeOfAccident']).date\n    df['DaysReportDelay'] = (df['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\n    \n    # ReportedDate : in the same year than YearOfAccident or not ?\n    df[\"YearReportDelay\"] = df['YearReported'] - df['YearOfAccident']\n    df[\"YearReportDelay\"].clip(0, 1, inplace=True)\n    \n    # Clip extrems values\n    df[\"DependentsOther\"].clip(0, 1, inplace=True)\n    df[\"DependentChildren\"].clip(0, 3, inplace = True)\n    df[\"Age\"].clip(14, 75, inplace = True)\n    df[\"Gender\"].replace({\"U\":\"M\"}, inplace = True)\n    df[\"MaritalStatus\"].fillna(\"S\", inplace = True)\n#    df[\"YearReported\"].clip(1988, 2006, inplace = True)\n        \n    df.loc[df[\"HoursWorkedPerWeek\"] > 168, \"HoursWorkedPerWeek\"] = -1     # 84 = 7 * 12  # 168 = 7 * 24\n        \ntrain_set[[f for f in train_set.columns if f != \"ClaimDescription\"]].head()","23ddb178":"# For several columns\nclass LabelEncoders(TransformerMixin, BaseEstimator): \n    \n    def __init__(self, features=[]):\n        self.features_ = features\n\n    def fit(self, X, y=None):\n        \n        self.encoders_ = {}\n        for f in self.features_:\n            enc = LabelEncoder()\n            self.encoders_[f] = enc.fit(X[f])\n\n        return self\n    \n    def transform(self, X):\n        \n        df = X.copy()\n        \n        for f in self.features_:\n            df[f] = self.encoders_[f].transform(df[f])\n        \n        return df\n\nLE = LabelEncoders([\"MaritalStatus\", \"Gender\"])\nLE.fit(train_set)\nX_tr = LE.transform(train_set)\nX_tr[[\"MaritalStatus\", \"Gender\"]].head()","cec4960b":"class Log_StdScaler_ByGroup(TransformerMixin, BaseEstimator):\n    \n    '''\n    Transformer for continous feature\n    \n    Compute log of original feature\n    Compute mean and std for the feature for every group of another feature\n    Standardize the feature for every group\n    \n    The transformer will be fit on a train set\n    and it can transform another dataset \n    and it can niverse transform\n   \n    '''\n    \n    def __init__(self, groupby=\"YearOfAccident\", feature = 'UltimateIncurredClaimCost'\n            , stdscal = True, log = np.log1p, trunc_UltimateIncurredClaimCost = True\n            , verbose = False):\n        \n        self.groupby = groupby\n\n        self.feature = feature\n        self.log = log\n        self.stdscal = stdscal\n        self.trunc_UltimateIncurredClaimCost = trunc_UltimateIncurredClaimCost\n                \n        self.verbose = verbose\n        \n    def fit(self, X, y=None):\n        \n        df_temp = X[[self.groupby, self.feature]].copy()\n        if self.verbose: print(\"Initial situation : \\n\", X_tr[[self.groupby, self.feature]].head())\n            \n        if self.trunc_UltimateIncurredClaimCost and self.feature == 'UltimateIncurredClaimCost':\n            df_temp[self.feature].clip(0, 900000, inplace=True)\n        \n        if not self.log is None:\n            df_temp[self.feature] = self.log(df_temp[self.feature])\n            \n        self.df_stats = df_temp.groupby(self.groupby)[self.feature].agg([\"mean\", \"std\"]).reset_index()\n        if self.verbose: print(\"\\nFit :\\n\",self.df_stats)\n        \n        return self\n    \n    def transform(self, X):\n        \n        if self.verbose: print(\"Initial situation : \\n\", X[[self.groupby, self.feature]].head())\n\n        df = X.merge(self.df_stats, how=\"left\", on = self.groupby)\n            \n        if self.trunc_UltimateIncurredClaimCost and self.feature == 'UltimateIncurredClaimCost':\n            df[self.feature] = df[self.feature].clip(0, 900000)\n\n        if not self.log is None:\n            df[self.feature] = self.log(df[self.feature])\n            \n        if self.stdscal:\n            df[self.feature] = (df[self.feature] - df[\"mean\"]) \/ df[\"std\"]\n            \n        df.drop([\"mean\", \"std\"], axis=1, inplace = True)\n        \n        if self.verbose: print(\"\\nAfter transf :\\n\",df[[self.groupby, self.feature]].head())\n            \n        return df\n    \n    def inverse_transform(self, X):\n        \n        df = X.merge(self.df_stats, how=\"left\", on = self.groupby)\n        if self.stdscal:\n            df[self.feature] = df[self.feature] * df[\"std\"] + df[\"mean\"] \n        df.drop([\"mean\", \"std\"], axis=1, inplace = True)\n            \n        if self.log == np.log:\n            df[self.feature] = np.exp(df[self.feature])\n        if self.log == np.log1p:\n            df[self.feature] = np.exp(df[self.feature]) - 1\n\n        if self.verbose: print(\"\\nAfter inverse transf :\\n\",df[[self.groupby, self.feature]].head())\n        if self.verbose: print(df.columns)\n            \n        return df\n\nlogstd = Log_StdScaler_ByGroup(verbose=True, feature = \"InitialIncurredCalimsCost\")\nlogstd.fit(X_tr)\ndf_temp = logstd.transform(train_set)\ndf_temp = logstd.inverse_transform(df_temp)\n\ndel df_temp","b749140d":"def create_y_true(X_trn, X_val, y_param):\n\n    df_y_trn = X_trn[[y_param[\"groupby\"], \"UltimateIncurredClaimCost\"]].copy()\n    df_y_val = X_val[[y_param[\"groupby\"], \"UltimateIncurredClaimCost\"]].copy()\n        \n    y_logstd = Log_StdScaler_ByGroup(**y_param).fit(df_y_trn)\n    y_trn = y_logstd.transform(df_y_trn)[\"UltimateIncurredClaimCost\"].values\n    y_val = y_logstd.transform(df_y_val)[\"UltimateIncurredClaimCost\"].values\n        \n    for df in [df_y_trn, df_y_val]:\n        df.rename({\"UltimateIncurredClaimCost\":\"y_true\"}, inplace = True, axis = 1)\n            \n    return y_trn, y_val, df_y_trn, df_y_val, y_logstd\n            \ndef transform_y_pred(y_pred, df_y, y_logstd):\n    \n    df_y[\"UltimateIncurredClaimCost\"] = y_pred\n    df_y = y_logstd.inverse_transform(df_y)\n    \n    rmse_ = None\n    if \"y_true\" in df_y.columns:\n        rmse_ = rmse(df_y[\"y_true\"], df_y[\"UltimateIncurredClaimCost\"])\n    \n    return df_y[\"UltimateIncurredClaimCost\"].values, rmse_\n    \n        \ny_param = {\"groupby\" : \"YearOfAccident\", \"feature\" : 'UltimateIncurredClaimCost'\n            , \"stdscal\" : True, \"log\" : np.log, \"trunc_UltimateIncurredClaimCost\" : False\n            , \"verbose\" : False}\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_set, train_set[y_param[\"groupby\"]])):\n        \n    X_trn, X_val = train_set.iloc[trn_idx], train_set.iloc[val_idx]\n    y_trn, y_val, df_y_trn, df_y_val, y_logstd = create_y_true(X_trn, X_val, y_param)\n    \n#    print(df_y_trn.head())\n    \n    y_values, rmse_ = transform_y_pred(y_val, df_y_val, y_logstd)\n    print(f\"RMSE (pred = true) (orginal = inverse tranform ?): {rmse_:.3f}\")\n    \n    y_values, rmse_ = transform_y_pred(y_val + 1, df_y_val, y_logstd)\n    print(f\"RMSE (log pred = 1 + log true) : {rmse_:,.0f}\")","f47cc8f6":"from nltk.tokenize import RegexpTokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\nprint(tokenizer.tokenize(\"The Sky is blue, also the ocean is blue also Rainbow has a blue colour.\"))\n\nfrom nltk.corpus import stopwords \n__stop_words = stopwords.words('english') # Selecting english stopwords\n\n# Lemmatizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfrom sklearn.linear_model import Ridge","ee12d70c":"def drop_duplicates_token(doc):\n    ordered_tokens = set()\n    result = []\n    for word in doc:\n        if word not in ordered_tokens:\n            ordered_tokens.add(word)\n            result.append(word)\n    return result\ndoc = \"The Sky is blue, also the ocean is blue also Rainbow has a blue colour.\"\nprint(drop_duplicates_token(tokenizer.tokenize(doc)))","0a18d6dc":"%%time\nclass ClaimDesciptionScore(TransformerMixin, BaseEstimator): \n    def __init__(self, regressor, stop_words, lemmatizer = None, drop_duplicates_words = True\n                 , ngram_range=(1, 6), min_df=100, return_score = False, verbose = False):\n        \n        self.regressor = regressor\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.drop_duplicates_words = drop_duplicates_words\n        self.min_df = min_df\n        self.lemmatizer = lemmatizer\n        self.return_score = return_score\n        self.verbose = verbose\n        \n    def prepare_data(self, X):\n        \n        docs = X[\"ClaimDescription\"].copy()\n        \n        # Tokenize all docs\n        docs = [tokenizer.tokenize(doc.lower()) for doc in docs]\n        \n        # Remove words that are only one character.\n        docs = [[token for token in doc if len(token) > 1] for doc in docs]\n\n        # Remove stopwords\n        docs = [[token for token in doc if token not in self.stop_words] for doc in docs]\n\n        # Lemmatize the documents.\n        if self.lemmatizer == \"WordNet\":\n            docs = [[WordNetLemmatizer().lemmatize(token) for token in doc] for doc in docs]\n            \n        # Drop duplicates words\n        if self.drop_duplicates_words:\n            docs = [drop_duplicates_token(doc) for doc in docs]\n            \n        docs = [' '.join(doc) for doc in docs]\n        \n        return docs\n        \n\n    def fit(self, X, y=None):\n        \n        docs = self.prepare_data(X)\n        \n        self.vect = CountVectorizer(ngram_range=self.ngram_range, min_df = self.min_df).fit(docs)\n        X_trn = self.vect.transform(docs)\n        features_names = self.vect.get_feature_names()\n        \n        df_y_trn = X[[\"YearOfAccident\", \"UltimateIncurredClaimCost\"]].copy()\n        self.y_logstd = Log_StdScaler_ByGroup(groupby=\"YearOfAccident\", feature='UltimateIncurredClaimCost'\n            , stdscal=True, log=np.log1p, trunc_UltimateIncurredClaimCost=True\n            , verbose=False).fit(df_y_trn)\n        y_trn = self.y_logstd.transform(df_y_trn)[\"UltimateIncurredClaimCost\"].values\n        df_y_trn.rename({\"UltimateIncurredClaimCost\":\"y_true\"}, inplace = True, axis = 1)\n        \n        self.regressor.fit(X_trn, y_trn)\n        \n        if self.verbose:\n            print(self.regressor.coef_)\n            print(self.vect.get_feature_names())\n        \n        return self\n    \n    def transform(self, X):\n        \n        df = X.copy()\n        docs = self.prepare_data(X)\n        X_val = self.vect.transform(docs)\n        \n        df[\"ClaimDescriptionScore\"] = self.regressor.predict(X_val)\n        \n        rmse_ = None\n        if self.return_score and \"UltimateIncurredClaimCost\" in X.columns:\n            df_y_val = X[[\"YearOfAccident\", \"UltimateIncurredClaimCost\"]].copy()\n            y_val = self.y_logstd.transform(df_y_val)[\"UltimateIncurredClaimCost\"].values\n            df_y_val.rename({\"UltimateIncurredClaimCost\":\"y_true\"}, inplace = True, axis = 1)\n            \n            y_values, rmse_ = transform_y_pred(df[\"ClaimDescriptionScore\"].values, df_y_val, y_logstd)\n            if self.verbose: print(\"RMSE : {:,.0f}\".format(rmse_))\n        \n            return df, rmse_\n        else:\n            return df","f8683dfd":"import itertools\ndef product_dict(**kwargs):\n    keys = kwargs.keys()\n    vals = kwargs.values()\n    for instance in itertools.product(*vals):\n        yield dict(zip(keys, instance))","ddb456be":"%%time\ngrid_params={\"alpha\":[.001, .01, .1, 1], \"lemmatizer\":[None, \"WordNet\"]\n             , 'ngram_range':[(1, 4), (1, 5), (1, 6), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 6)]\n             , 'drop_duplicates_words':[True, False]}\n\n# Best params are :\ngrid_params={\"alpha\":[.1], \"lemmatizer\":[None]\n             , 'ngram_range':[(1, 5)]\n             , 'drop_duplicates_words':[True]}\n\nresults=[]\nfor i, p in enumerate(product_dict(**grid_params)):\n    \n    scores = []\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_set, train_set[\"YearOfAccident\"])):\n        ClaimDescScore = ClaimDesciptionScore(regressor = Ridge(alpha=p[\"alpha\"]), stop_words = __stop_words\n            , lemmatizer=p[\"lemmatizer\"], ngram_range=p[\"ngram_range\"]\n            , drop_duplicates_words=p[\"drop_duplicates_words\"], return_score = True)\n        _ = ClaimDescScore.fit(train_set.iloc[trn_idx])\n        _, rmse_ = ClaimDescScore.transform(train_set.iloc[val_idx])\n        scores.append(rmse_)\n        \n    p[\"rmse_\"] = np.mean(scores)\n    results.append(p)\nres = pd.DataFrame.from_dict(results)\n\nbest_params_ = res.loc[res[\"rmse_\"]==res[\"rmse_\"].min()].to_dict(orient=\"records\")\nprint(best_params_)\nres","309598bb":"import psutil\n__n_cores = psutil.cpu_count()     # Available CPU cores\nprint(f\"N Cores : {__n_cores}\")\nfrom multiprocessing import Pool   # Multiprocess Runs\n\ndef df_parallelize_run(func, t_split):\n    \n    num_cores = np.min([__n_cores, len(t_split)])\n    pool = Pool(num_cores)\n    df = pool.map(func, t_split)\n    pool.close()\n    pool.join()\n    \n    return df","0e7d7a00":"# Importing libraries\nimport gensim # open-source library for unsupervised topic modeling and NLP\nfrom gensim.utils import simple_preprocess\n\nimport gensim.corpora as corpora\nfrom pprint import pprint","5b2569a6":"class search_dominant_topic(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, num_topics=10, more_stop_words=[\"left\", \"right\", \"strain\"]\n                 , seed=0, verbose = False, n_jobs=4):\n        \n        self.num_topics = num_topics\n        self.verbose = verbose\n        self.seed = seed\n        \n        self.more_stop_words = more_stop_words\n        self.stop_words = stopwords.words('english')\n        self.stop_words.extend(['from', 'subject', 're', 'edu', 'use'] + self.more_stop_words) # adding new stopwords\n        \n        # Lists of new features by category\n        self.feats_categorical = [f\"word_{f}_in_description\" for f in self.more_stop_words] + [f\"max_topic_{i+1}\" for i in range(self.num_topics)]\n        self.feats_continous = [f\"topic_{i}\" for i in range(self.num_topics)]\n        \n        self.n_jobs=n_jobs\n        \n    def prepare_docs(self, X):\n        \n        docs = X[\"ClaimDescription\"].values.tolist()\n        \n        # Split the documents into tokens.\n        tokenizer = RegexpTokenizer(r'\\w+')\n        \n        # Convert to lowercase,  and split into words.\n        docs = [tokenizer.tokenize(doc.lower()) for doc in docs]\n        \n        # Remove words that are only one character.\n        docs = [[token for token in doc if len(token) > 1] for doc in docs]\n\n        # Remove stopwords\n        docs = [[token for token in doc if token not in stop_words] for doc in docs]\n\n        # Lemmatizer \n        lemmatizer = WordNetLemmatizer()\n        docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]\n        \n        return docs\n\n        \n    def fit(self, X, y=None):\n        \n        docs = self.prepare_docs(X)\n        \n        # Create a dictionary representation of the documents.\n#        dictionary = Dictionary(docs)\n        self.id2word = corpora.dictionary.Dictionary(docs)\n        \n        # Filter out words that occur less than 10 documents, or more than 30% of the documents.\n        self.id2word.filter_extremes(no_below=10, no_above=0.3)\n\n        # Bag-of-words representation of the documents.\n        corpus = [self.id2word.doc2bow(doc) for doc in docs]\n\n        self.lda_model = gensim.models.LdaMulticore(corpus = corpus,\n                id2word = self.id2word,\n                num_topics = self.num_topics,\n                random_state = self.seed)\n        \n        # Print the Keyword in the 10 topics\n        if self.verbose: pprint(lda_model.print_topics())\n            \n        return self\n    \n    def transform_a_part(self, X):\n    \n        docs = self.prepare_docs(X)\n        \n        # Term Document Frequency\n        # Converts a collection of words to a list of (word_id, word_frequency) 2-tuples.\n        corpus = [self.id2word.doc2bow(text) for text in docs]\n        \n        ll = list(self.lda_model[corpus])\n        for i, l in enumerate(ll):\n            for a in range(len(l), self.num_topics):\n                ll[i].append((-1, -1))\n        df = pd.DataFrame(ll)\n        \n        for i in range(self.num_topics):\n            df[[f\"_{i}\", f\"topic_{i}\"]] = pd.DataFrame(df[i].tolist())    \n        \n        cols = [f\"topic_{i}\" for i in range(self.num_topics)]\n        \n        df[\"sorted_topics\"] = df[cols].apply(lambda s: s.nlargest(2).index.tolist(), axis=1)\n        df[[f\"max_topic_{i+1}\" for i in range(2)]] = pd.DataFrame(df[\"sorted_topics\"].tolist()) \n        for i in range(2):\n            df[f\"max_topic_{i+1}\"] = df[f\"max_topic_{i+1}\"].str[6:].fillna(-1).astype(np.int8)\n        \n        df.fillna(-1, inplace=True)\n\n        cols = cols + [f\"max_topic_{i+1}\" for i in range(2)]\n        \n        df = pd.concat([X.reset_index(), df[cols]], axis=1).set_index(\"ClaimNumber\")\n        \n        for f in self.more_stop_words:\n        \n            df[f\"word_{f}_in_description\"] = df[\"ClaimDescription\"].str.lower()\n            df[f\"word_{f}_in_description\"] = df[f\"word_{f}_in_description\"].str.contains(f)\n        \n        return df\n    \n    def transform(self, X):\n        \n        N = X.shape[0]\n        \n        # Split X in p parts, p is the number of cores\n        df_temp = [X[int(N * p \/ self.n_jobs):int(N * (p + 1)\/self.n_jobs)] for p in range(self.n_jobs)]\n        \n        # Transform each part\n        return pd.concat(df_parallelize_run(self.transform_a_part, df_temp))","833ff2bb":"%%time\nres={}\n#for nt in [5]+list(range(10, 31)):\n#for nt in [5, 6, 10]:\nfor nt in [6]:\n    sdt = search_dominant_topic(num_topics = nt, seed=__seed)\n    _ = sdt.fit(train_set)\n    X_tr = sdt.transform(train_set)\n    \n    res[nt]=[]\n    a = X_tr[\"max_topic_1\"].value_counts()\n    res[nt].append(a[a.index!=-1].min())\n    \n    cols = [f\"topic_{i}\" for i in range(nt)]\n\n    X_tr[\"bis_sorted_topics\"] = X_tr[cols].apply(lambda s: s.nlargest(2).tolist(), axis=1)\n    val = abs(pd.DataFrame(X_tr[\"bis_sorted_topics\"].tolist())[0] - pd.DataFrame(X_tr[\"bis_sorted_topics\"].tolist())[1])\n#    res[nt][\"ecart<.1\"] = val[val<.10].count()\n    res[nt].append(val[val<.10].count())\n    \npd.DataFrame.from_dict(res, orient='index')","f33fc4b5":"%%time\nsdt = search_dominant_topic(seed=__seed)\n_ = sdt.fit(train_set)","9e62f71d":"%%time\nX_tr = sdt.transform(train_set)","3bb8c76d":"%%time\nX_te = sdt.transform(test_set)","1dac6a25":"class drop_features(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, drop_features=[]):\n        \n        self.drop_features = ['DateTimeOfAccident', 'DateReported', 'ClaimDescription',\n                             \"UltimateIncurredClaimCost\"] + drop_features\n        \n    def fit(self, X, y=None):\n        \n        return self\n    \n    def transform(self, X):\n        \n        df = X[[f for f in X.columns if f not in self.drop_features]].copy()\n            \n        return df","c466bfb0":"class RankFrequencyEncoder(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, feature, groupby=\"YearOfAccident\"\n                 , output_freq = False, output_rank = True\n                , verbose = False):\n        \n        self.groupby = groupby\n        self.feature = feature\n        self.verbose = verbose\n        self.output_freq = output_freq\n        self.output_rank = output_rank\n        \n        \n    def fit(self, X, y=None):\n        \n        df_temp = X[[self.groupby, self.feature]].copy()\n        df_temp = df_temp.groupby([self.groupby, self.feature]).size().reset_index()\n        df_temp.columns=[\"YearOfAccident\", 'InitialIncurredCalimsCost', f\"freq_{self.feature}\"]\n        df_temp.sort_values([\"YearOfAccident\", f\"freq_{self.feature}\"], ascending=[True, False], inplace=True)\n        df_temp[f\"rank_{self.feature}\"] = df_temp.groupby(\"YearOfAccident\")[f\"freq_{self.feature}\"].rank(\"dense\", ascending=False)\n        \n        self.df_encoder = df_temp.copy()\n        \n        if self.verbose:\n            print(self.df_encoder.loc[self.df_encoder[f\"rank_{self.feature}\"].isin([1, 2, 3])])\n        \n        return self\n    \n    def transform(self, X):\n        \n#        df_temp = X[[self.groupby, self.feature]].copy()\n        df = X.merge(self.df_encoder, how=\"left\", on = [self.groupby, self.feature])\n        \n        df[f\"freq_{self.feature}\"].fillna(-1, inplace=True)\n        df[f\"rank_{self.feature}\"].fillna(99, inplace=True)\n        df[f\"freq_{self.feature}\"] = df[f\"freq_{self.feature}\"].astype(np.int16)\n        df[f\"rank_{self.feature}\"] = df[f\"rank_{self.feature}\"].astype(np.int8)\n        if not self.output_freq:\n            df.drop(f\"freq_{self.feature}\", axis=1, inplace=True)\n        if not self.output_rank:\n            df.drop(f\"rank_{self.feature}\", axis=1, inplace=True)\n\n        return df","c182cb79":"FE = RankFrequencyEncoder(\"InitialIncurredCalimsCost\")\n_ = FE.fit(train_set)\ntemp = FE.transform(test_set)\nprint(temp.columns)\ntemp[\"rank_InitialIncurredCalimsCost\"].value_counts(dropna=False)[:10]","a678ad55":"nrows, ncols = 6, 2\nfig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize=(15, 4.5 * nrows))\nplt.subplots_adjust(hspace = 0.3, wspace = 0.5)\n    \nax = plt.subplot(nrows, ncols, 1)\nsns.scatterplot(x=train_set[\"InitialIncurredCalimsCost\"], y=train_set[\"UltimateIncurredClaimCost\"], ax=ax, color=\"green\")\nax.set_title(\"Train set - Cost : ultimate vs initial\");\n\nax = plt.subplot(nrows, ncols, 2)\ndf_temp = train_set.loc[(train_set[\"InitialIncurredCalimsCost\"]<1000000)&\n                       (train_set[\"UltimateIncurredClaimCost\"]<1000000)]\nsns.scatterplot(x=df_temp[\"InitialIncurredCalimsCost\"], y=df_temp[\"UltimateIncurredClaimCost\"], ax=ax, color=\"green\")\nax.set_title(\"Train set - Cost : ultimate vs initial without 2 extrems\");\n\nax = plt.subplot(nrows, ncols, 3)\ndf_temp = train_set.groupby(\"YearReported\")[\"UltimateIncurredClaimCost\"].mean()\ndf_temp.plot(ax=ax, color=\"green\")\nplt.ylabel('Mean of Ultimate Incurred Claim Cost')\nax.set_title(\"Train set - Ultimate Mean Cost per Year reported\");\n\nax = plt.subplot(nrows, ncols, 5)\ndf_temp = train_set.groupby(\"YearOfAccident\")[\"InitialIncurredCalimsCost\"].median()\ndf_temp.plot(ax=ax, color=\"green\")\nplt.ylabel('Median of Initial Incurred Claim Cost')\nax.set_title(\"Train set - Initial Median Cost per Year of Accident\");\n\nax = plt.subplot(nrows, ncols, 6)\ndf_temp = test_set.groupby(\"YearOfAccident\")[\"InitialIncurredCalimsCost\"].mean()\ndf_temp.plot(ax=ax, color=\"blue\")\nplt.ylabel('Mean of Initial Incurred Claim Cost')\nax.set_title(\"Test set - Initial Mean Cost per Year of Accident\");\n\nax = plt.subplot(nrows, ncols, 7)\ndf_temp = train_set.groupby(\"Age\")[\"UltimateIncurredClaimCost\"].mean()\ndf_temp.plot(ax=ax)\nplt.ylabel('Mean of Ultimate Incurred Claim Cost')\nax.set_title(\"Train set - Ultimate Mean cost per age\");\n\nax = plt.subplot(nrows, ncols, 8)\ndf_temp = train_set.groupby(\"DependentChildren\")[\"UltimateIncurredClaimCost\"].mean()\ndf_temp.plot(ax=ax)\nplt.ylabel('Mean of Ultimate Incurred Claim Cost')\nax.set_title(\"Train set - Ultimate Mean cost per Dependent Children\");\n\nax = plt.subplot(nrows, ncols, 9)\n#df_temp = tr.groupby(\"DependentsOther\")[\"UltimateIncurredClaimCost\"].mean().to_frame().reset_index()\ndf_temp = train_set.loc[(train_set[\"UltimateIncurredClaimCost\"]<50000)]\nsns.boxplot(y=\"UltimateIncurredClaimCost\", x=\"DependentsOther\", data=df_temp, ax=ax)\nax.set_title(\"Train set - Ultimate cost per other Dependents \");\n\nax = plt.subplot(nrows, ncols, 10)\ndf_temp = train_set.groupby(\"HoursWorkedPerWeek\")[\"UltimateIncurredClaimCost\"].mean()\ndf_temp.plot(ax=ax)\nplt.ylabel('Mean of Ultimate Incurred Claim Cost')\nax.set_title(\"Mean Initial cost per age\");\n\nax = plt.subplot(nrows, ncols, 11)\ndf_temp = train_set.groupby(\"HoursWorkedPerWeek\")[\"WeeklyWages\"].mean()\ndf_temp.plot(ax=ax)\nplt.ylabel('Weekly wages')\nplt.xlabel('Mean of Hours worked per week')\nax.set_title(\"Wages per hours worked\");\n","4219fa23":"categorical_features = ['Age', 'Gender', 'MaritalStatus','DependentChildren', 'DependentsOther'\n    , 'PartTimeFullTime', 'DaysWorkedPerWeek', 'MonthOfAccident', 'WeekdayOfAccident', 'HourOfAccident'\n    , 'WeekdayOfReported', \"YearOfAccident\", \"YearReportDelay\"\n    , \"word_left_in_description\", \"word_right_in_description\", \"word_strain_in_description\"] + \\\n    [f\"max_topic_{i+1}\" for i in range(2)]\n\nnrows = len(categorical_features)\n\nhsize = [] ; tot_hsize = 0\nfor i, f in enumerate(categorical_features):\n    hsize.append(len(X_tr[f].unique()))\n    tot_hsize += hsize[i]\n\nfig = plt.figure(figsize = (20, .5 * tot_hsize))\nplt.subplots_adjust(hspace = 0.5, wspace = 0.1)\n\n# https:\/\/matplotlib.org\/3.3.3\/tutorials\/intermediate\/gridspec.html\nspec = fig.add_gridspec(nrows=nrows, ncols=2\n            , width_ratios=[1, 1], height_ratios=hsize)\n\nfor i, f in enumerate(categorical_features):\n    \n    ax = fig.add_subplot(spec[i, 0])\n    df_temp = X_tr.groupby(f)[\"WeeklyWages\"].count().reset_index()\n    sns.barplot(x=\"WeeklyWages\", y=f, orient='h', data=df_temp, color=\"green\", ax=ax)\n    plt.xlabel('Count')\n    plt.title(f\"Claims number by {f} in Train set\")\n    \n    ax = fig.add_subplot(spec[i, 1])\n    df_temp = X_te.groupby(f)[\"WeeklyWages\"].count().reset_index()\n    sns.barplot(x=\"WeeklyWages\", y=f, orient='h', data=df_temp, color=\"blue\")\n    plt.xlabel('Count')\n    plt.title(f\"Claims number by {f} in Test set\");","5b7cd5fc":"continous_features = ['UltimateIncurredClaimCost', 'InitialIncurredCalimsCost', 'WeeklyWages'\n        , 'DaysReportDelay'] + [f\"topic_{i}\" for i in range(10)]\nnrows = len(continous_features)\n\nfig, ax = plt.subplots(figsize = (20, 8*nrows))\nplt.subplots_adjust(hspace = 0.3, wspace = 0.2)\n\nfor j, f in enumerate(continous_features):\n    plt.subplot(nrows, 2, j*2+1)\n    df_temp = X_tr.groupby(\"YearOfAccident\")[f].mean().reset_index()\n    sns.barplot(x=f, y=\"YearOfAccident\", orient='h', data=df_temp, color=\"green\")\n    plt.title(f\"{f} mean by Year of accident in Train set\")\n    \n    if f != 'UltimateIncurredClaimCost':\n        plt.subplot(nrows, 2, j*2+2)\n        df_temp = X_te.groupby(\"YearOfAccident\")[f].mean().reset_index()\n        sns.barplot(x=f, y=\"YearOfAccident\", orient='h', data=df_temp, color=\"blue\")\n        plt.title(f\"{f} mean by Year of accident in Test set\")","a0902c95":"years = sorted(list(X_tr[\"YearOfAccident\"].unique()))\nfig, ax = plt.subplots(nrows = len(years) + 1, ncols = 2, figsize=(17, 4.5*(len(years)+1)))\n\nlogstd = Log_StdScaler_ByGroup()\nlogstd.fit(X_tr)\ndf_temp = logstd.transform(X_tr)\n\nax = plt.subplot(len(years)+1, 2, 1)\nsns.boxplot(y=\"UltimateIncurredClaimCost\", x=\"YearOfAccident\", data=train_set.loc[train_set[\"UltimateIncurredClaimCost\"]<100000, [\"YearOfAccident\", \"UltimateIncurredClaimCost\"]], color=\"green\", ax=ax)\nax.set_title(\"TRAIN - Initial Costs < 100 000\");\n\nax = plt.subplot(len(years)+1, 2, 2)\nsns.boxplot(y=\"UltimateIncurredClaimCost\", x=\"YearOfAccident\", data=df_temp[[\"YearOfAccident\", \"UltimateIncurredClaimCost\"]], color=\"green\", ax=ax)\nax.set_title(\"TRAIN - Initial Costs after log and StdScal per Year\");\n\nfor i, year in enumerate(years):\n    \n    plt.subplots_adjust(hspace = 0.3, wspace = 0.5)\n    \n    ax = plt.subplot(len(years)+1, 2, 2 * i + 3)\n    sns.distplot(train_set.loc[train_set[\"YearOfAccident\"] == year, \"UltimateIncurredClaimCost\"], ax = ax)\n    ax.set_title(f\"UltimateIncurredClaimCost in {year}\");\n\n    ax = plt.subplot(len(years)+1, 2, 2 * i + 4)\n    sns.distplot(df_temp.loc[df_temp[\"YearOfAccident\"] == year, \"UltimateIncurredClaimCost\"], ax = ax)\n    ax.set_title(f\"Log of UltimateIncurredClaimCost in {year}\");\n\nfig.subplots_adjust(top=0.97)\nfig.suptitle(\"UltimateIncurredClaimCost on the left   \/   Log of UltimateIncurredClaimCost on the right\", fontsize=\"xx-large\");\n\ndel df_temp","e4ad60c2":"logstd = Log_StdScaler_ByGroup(feature = \"InitialIncurredCalimsCost\")\nlogstd.fit(X_tr)\ndf_temp1 = logstd.transform(X_tr)\ndf_temp2 = logstd.transform(X_te)\n\nfig, ax = plt.subplots(nrows = 2, ncols = 3, figsize=(17, 10))\n\nax = plt.subplot(2, 3, 1)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=train_set.loc[train_set[\"InitialIncurredCalimsCost\"]<10000, [\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"green\", ax=ax)\n#sns.boxplot(x=\"InitialIncurredCalimsCost\", y=\"YearOfAccident\", data=train_set[[\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], ax=ax, orient=\"h\")\nax.set_title(\"TRAIN - Initial Costs < 10 000\");\n\nax = plt.subplot(2, 3, 2)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=df_temp1[[\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"green\", ax=ax)\nax.set_title(\"TRAIN - Initial Costs after StdScal per Year\");\n\nax = plt.subplot(2, 3, 3)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=df_temp2[[\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"blue\", ax=ax)\nax.set_title(\"TEST - Initial Costs after StdScal per Year\");\n\nax = plt.subplot(2, 3, 4)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=train_set.loc[train_set[\"InitialIncurredCalimsCost\"]<2000, [\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"green\", ax=ax)\nax.set_title(\"TRAIN - Initial Costs < 2 000\");\n\nax = plt.subplot(2, 3, 5)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=df_temp1[[\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"green\", ax=ax)\nax.set_title(\"TRAIN - Initial Costs after StdScal per Year\");\n\nax = plt.subplot(2, 3, 6)\nsns.boxplot(y=\"InitialIncurredCalimsCost\", x=\"YearOfAccident\", data=df_temp2[[\"YearOfAccident\", \"InitialIncurredCalimsCost\"]], color=\"blue\", ax=ax)\nax.set_title(\"TEST - Initial Costs after StdScal per Year\");","f7f17e76":"# With Log transformation\n\nlogstd = Log_StdScaler_ByGroup(feature = \"WeeklyWages\")\nlogstd.fit(X_tr)\ndf_temp1 = logstd.transform(X_tr)\ndf_temp2 = logstd.transform(X_te)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(17, 10))\n\nax = plt.subplot(2, 3, 1)\nsns.boxplot(y=\"WeeklyWages\", x=\"YearOfAccident\", data=train_set.loc[train_set[\"WeeklyWages\"]<2000, [\"YearOfAccident\", \"WeeklyWages\"]], ax=ax, color=\"green\")\nax.set_title(\"TRAIN - WeeklyWages < 2000\");\n\nax = plt.subplot(2, 3, 2)\nsns.boxplot(y=\"WeeklyWages\", x=\"YearOfAccident\", data=df_temp1[[\"YearOfAccident\", \"WeeklyWages\"]], ax=ax, color=\"green\")\nax.set_title(\"TRAIN - WeeklyWages after StdScal per year\");\n\nax = plt.subplot(2, 3, 3)\nsns.boxplot(y=\"WeeklyWages\", x=\"YearOfAccident\", data=df_temp2[[\"YearOfAccident\", \"WeeklyWages\"]], ax=ax, color=\"blue\")\nax.set_title(\"TEST - WeeklyWages after StdScal per year\");","a036b32b":"from sklearn.ensemble import ExtraTreesRegressor","e28ce438":"%%time\ndef fit_extratree(X_train, y_param, pipe, X_test = None\n                      , n_estimators = 500, check_feature_importance = False\n                      , plots = False, verbose = False):\n\n    scores = []\n    y_test, y_valid = None, pd.Series(np.zeros(len(X_train)), index=X_train.index)\n    if not X_test is None: \n        y_test = pd.Series(np.zeros(len(X_test)), index=X_test.index)\n        df_y_test = X_test[[y_param[\"groupby\"]]].copy()\n    \n    if plots:\n        fig, ax = plt.subplots(nrows = folds.n_splits, ncols = 2, figsize=(15, 4.2 * folds.n_splits))\n        plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    \n    # Main loop on each fold\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, X_train[\"YearOfAccident\"])):\n        \n        # Samples selection on each fold\n        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n        y_trn, y_val, df_y_trn, df_y_val, y_logstd = create_y_true(X_trn, X_val, y_param)\n\n        # Fit encoders and transform data\n        _ = pipe.fit(X_trn)\n        X_trn = pipe.transform(X_trn) ; X_val = pipe.transform(X_val)\n    \n        # Fit Model\n        reg = ExtraTreesRegressor(n_estimators = n_estimators, random_state=__seed, n_jobs=-1).fit(X_trn, y_trn)\n        \n        # Prediction on valid samples\n        y_pred = reg.predict(X_val)\n        # Residual Plots \n        if plots:\n            sns.scatterplot(y_val, y_pred, ax=ax[fold_, 0])\n            ax[fold_, 0].set_title(f\"Predictions vs True value fold n\u00b0{fold_}\")\n            ax[fold_, 0].set_xlabel('True Ultimate Incurred Claim Cost')\n            ax[fold_, 0].set_ylabel('Predictions')\n            \n            sns.scatterplot(y_pred, y_val - y_pred, ax=ax[fold_, 1])\n            ax[fold_, 1].set_title(f\"Residuals vs Prediction fold n\u00b0{fold_}\")\n            ax[fold_, 1].set_xlabel('Predictions')\n            ax[fold_, 1].set_ylabel('Residuals');\n\n        # Prediction on Test samples\n        if not X_test is None: \n            X_te = pipe.transform(X_test)\n            y_te = reg.predict(X_te)\n            y_te, _ = transform_y_pred(y_te, df_y_test, y_logstd)\n            y_test += y_te \/ folds.n_splits\n\n        # Inverse transform and score of validation prediction\n        y_values, rmse = transform_y_pred(y_pred, df_y_val, y_logstd)\n        y_valid.iloc[val_idx] = y_values ; scores.append(rmse)\n        if verbose: print(\"MSE : {:,.0f}\".format(mse)) ; print(\"RMSE : {:,.0f}\".format(scores[fold_]))\n    # Folds loop END\n    \n    # Mean score\n    if verbose: print(\"MEAN RMSE : {:,.0f}\\n\".format(np.array(scores).mean()))\n    \n    return {\"rmse\":np.array(scores).mean(), \"y_test\":y_test, \"y_valid\":y_valid}\n# END    \n    \ny_param = {\"groupby\" : \"YearOfAccident\", \"feature\" : 'UltimateIncurredClaimCost'\n            , \"stdscal\" : False, \"log\" : None, \"trunc_UltimateIncurredClaimCost\" : False\n            , \"verbose\" : False}\n\nresults = {}\n#for num_topics in [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]:\n#    print(f\"Num_topics : {num_topics}\")\nfor num_topics in [12]:\n    \n    # Pipeline to transform data\n    pipe = make_pipeline(\n        search_dominant_topic(num_topics)\n        , ClaimDesciptionScore(regressor = Ridge(alpha=.1), stop_words = __stop_words\n            , lemmatizer=None, ngram_range=(1, 5)\n            , drop_duplicates_words=True, return_score = False)\n        , LabelEncoders([\"MaritalStatus\", \"Gender\", \"PartTimeFullTime\"\n                , \"word_left_in_description\", \"word_right_in_description\", \"word_strain_in_description\"\n                , \"WeekdayOfReported\", \"WeekdayOfAccident\"])\n        , RankFrequencyEncoder(\"InitialIncurredCalimsCost\", output_freq = True, output_rank = False)\n        , Log_StdScaler_ByGroup(feature = \"InitialIncurredCalimsCost\")\n        , Log_StdScaler_ByGroup(feature = \"WeeklyWages\")\n        , drop_features(['WeeksReportDelay', 'YearReported']))\n\n    et_result = fit_extratree(train_set, y_param, pipe, X_test = test_set, plots=True)\n    results[i] = et_result[\"rmse\"]\n    print(\"MEAN RMSE for num_topics {} : {:,.0f}\\n\".format(i, et_result[\"rmse\"]))\nresults","fa7f02e4":"from catboost import CatBoostRegressor\nfrom catboost import Pool as CatBoostPool # Careful because I use the name \"Pool\" from Multiprocessing in part 3 !!","c53fbb04":"%%time\ndef catboost_cv(X_train, pipe, y_param, params, X_test = None\n    , check_feature_importance = False, plots = False, verbose = False):\n\n    scores = [] ; feat_imp = {} ; best_iteration = 0\n    \n    y_test, y_valid = None, pd.Series(np.zeros(len(X_train)), index=X_train.index)\n    if not X_test is None: \n        y_test = pd.Series(np.zeros(len(X_test)), index=X_test.index)\n        df_y_test = X_test[[y_param[\"groupby\"]]].copy()\n    \n    if plots:\n        fig, ax = plt.subplots(nrows = folds.n_splits, ncols = 3, figsize=(15, 4.2 * folds.n_splits))\n        plt.subplots_adjust(hspace = 0.5, wspace = 0.4)\n    \n    # Main loop on each fold\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, X_train[y_param[\"groupby\"]])):\n        \n        # Samples selection on each fold\n        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n        y_trn, y_val, df_y_trn, df_y_val, y_logstd = create_y_true(X_trn, X_val, y_param)\n\n        # Fit encoders and transform data\n        _ = pipe.fit(X_trn)\n        X_trn = pipe.transform(X_trn) ; X_val = pipe.transform(X_val)\n        \n        train_dataset, eval_dataset = CatBoostPool(data=X_trn, label=y_trn), CatBoostPool(data=X_val, label=y_val)\n\n        model = CatBoostRegressor(**params, thread_count = -1)  \n\n        model.fit(train_dataset, use_best_model = True,\n          verbose = False, plot = False,\n          eval_set = eval_dataset, early_stopping_rounds = 50)\n        best_iteration += model.best_iteration_ \/ folds.n_splits\n        \n        y_pred = model.predict(CatBoostPool(data=X_val))\n\n        # Residual Plots \n        if plots:\n            training_curve_df = pd.DataFrame.from_dict(\n                {\"Train\" : model.evals_result_[\"learn\"][\"RMSE\"]\n                , \"Valid\" : model.evals_result_[\"validation\"][\"RMSE\"]})\n            training_curve_df.plot(ax = ax[fold_, 0])\n            ax[fold_, 0].set_title(f\"Training History - fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 0].set_xlabel('Iteration')\n            ax[fold_, 0].set_ylabel('RMSE')\n\n            sns.scatterplot(y_val, y_pred, ax=ax[fold_, 1])\n            ax[fold_, 1].set_title(f\"Predictions vs True value fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 1].set_xlabel('True Ultimate Incurred Claim Cost')\n            ax[fold_, 1].set_ylabel('Predictions')\n            \n            sns.scatterplot(y_pred, y_val - y_pred, ax=ax[fold_, 2])\n            ax[fold_, 2].set_title(f\"Residuals vs Prediction fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 2].set_xlabel('Predictions')\n            ax[fold_, 2].set_ylabel('Residuals');\n            \n        # Prediction on Test samples\n        if not X_test is None: \n            X_te = pipe.transform(X_test)\n            y_te = model.predict(CatBoostPool(data=X_te))\n            y_te, _= transform_y_pred(y_te, df_y_test, y_logstd)\n            y_test += y_te \/ folds.n_splits\n\n        # Inverse transform and score of validation prediction\n        y_values, rmse_ = transform_y_pred(y_pred, df_y_val, y_logstd)\n        \n        y_valid.iloc[val_idx] = y_values \n        scores.append(rmse_)\n        if verbose: print(\"RMSE on fold {} : {:,.0f}\".format(fold_ + 1, scores[fold_]))\n            \n        # For feature importance\n        if check_feature_importance:\n            \n            for feat in X_val.columns:\n                \n                if fold_ == 0: feat_imp[feat] = []\n                \n                # Values permutation\n                temp_df = pd.DataFrame(X_val.copy(), columns=X_val.columns)\n                temp_df[feat] = np.random.permutation(temp_df[feat])\n        \n                # Prediction and RMSE\n                y_temp = model.predict(temp_df)\n                _, rmse_ = transform_y_pred(y_temp, df_y_val, y_logstd)\n                \n                # Increase of RMSE after value permutation\n                feat_imp[feat].append(rmse_ - scores[fold_])\n                \n    # Folds loop END\n    \n    # Mean score\n    if verbose: \n        print(\"MEAN RMSE : {:,.0f}\\n\".format(np.array(scores).mean()))\n        print(\"RMSE on all validation samples : {:,.0f}\\n\".format(rmse(X_train[\"UltimateIncurredClaimCost\"], y_valid)))\n    \n    # Feature importance\n    feat_imp_df = None ; important_features = [] ; most_important_features= []\n    if check_feature_importance:\n        \n        feat_imp_df = pd.DataFrame(feat_imp).transpose()\n\n        scores_increase = [i for i in range(folds.n_splits)]\n        scores_increase_sign = [f\"sign_{i}\" for i in range(folds.n_splits)]\n\n        feat_imp_df[scores_increase_sign] = 0\n        for score_increase, score_increase_sign in zip(scores_increase, scores_increase_sign):\n            feat_imp_df.loc[feat_imp_df[score_increase]>0, score_increase_sign] = 1\n    \n        feat_imp_df[\"nb_folds\"] = feat_imp_df[scores_increase_sign].sum(axis=1)\n        most_important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] == folds.n_splits].index)\n        important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] >= 3 * folds.n_splits \/ 4].index)\n        \n        feat_imp_df[\"mean\"] = feat_imp_df[list(range(folds.n_splits))].mean(axis=1)\n        feat_imp_df.sort_values(\"mean\", ascending = False, inplace=True)\n        \n    return {\"rmse\":np.array(scores).mean(), \"y_test\":y_test, \"y_valid\":y_valid, \"feat_imp_df\":feat_imp_df \n            , \"most_important_features\":most_important_features\n            , \"important_features\":important_features}\n    # End\n \n\ny_param = {\"groupby\" : \"YearOfAccident\", \"feature\" : 'UltimateIncurredClaimCost'\n            , \"stdscal\" : False, \"log\" : None, \"trunc_UltimateIncurredClaimCost\" : False\n            , \"verbose\" : False}\n\n#for num_topic in [4, 5, 6, 7, 8, 10]:\nfor num_topic in [6]:\n    pipe = make_pipeline(\n        search_dominant_topic(num_topic)\n        , ClaimDesciptionScore(regressor = Ridge(alpha=.1), stop_words = __stop_words\n            , lemmatizer=None, ngram_range=(1,5)\n            , drop_duplicates_words=True, return_score = False)\n        , RankFrequencyEncoder(\"InitialIncurredCalimsCost\", output_freq = True, output_rank = False)\n        , LabelEncoders([\"MaritalStatus\", \"Gender\", \"PartTimeFullTime\"\n            , \"word_left_in_description\", \"word_right_in_description\", \"word_strain_in_description\"\n            , \"WeekdayOfReported\", \"WeekdayOfAccident\"])\n        , drop_features(['WeeksReportDelay', 'YearReported']))\n    \n    params = {'random_seed': __seed, 'eval_metric': 'RMSE', 'iterations': 1000, \"learning_rate\" : .1}\n\n    model_catboost = catboost_cv(train_set, pipe, y_param, params, X_test = test_set\n    , check_feature_importance = True, plots = True, verbose = True)","0d8ca615":"def plot_features_importance(df_feat_imp):\n    \n    fig = plt.figure(figsize = (15, 15), constrained_layout=False)\n    gs = fig.add_gridspec(nrows=20, ncols=5, left=0.05, right=0.95,\n                        wspace=0.1, hspace=.1)\n\n    ax1 = fig.add_subplot(gs[:, 0])\n    ax2 = fig.add_subplot(gs[:2, 2:])\n    ax3 = fig.add_subplot(gs[4:, 2:])\n\n    sns.barplot(x=\"nb_folds\",\n            y=df_feat_imp.index,\n            data=df_feat_imp, ax=ax1, color = \"green\")\n    ax1.set_title(\"N folds where feature is important\")\n    ax1.set_xlabel('Nb folds')\n\n    sns.barplot(x=\"mean\",\n            y=df_feat_imp.index[:3],\n            data=df_feat_imp[:3], ax=ax2, color = \"green\")\n    ax2.set_title(\"Biggest RMSE increase after values permutations\")\n    ax2.set_xlabel('RMSE gap after values permutations')\n\n    sns.barplot(x=\"mean\",\n            y=df_feat_imp.index[2:],\n            data=df_feat_imp[2:], ax=ax3, color = \"green\")\n    ax3.set_title(\"Other RMSE gaps after values permutations\")\n    ax3.set_xlabel('RMSE gap after values permutations');","a8f0f8b3":"plot_features_importance(model_catboost[\"feat_imp_df\"])","47759b55":"def columns_range_for_permutation(l_ohe_drop_first, l_ohe_handle, l_stdscal, l_pass, n_topics):\n\n    l_columns = {} ; n_columns = 0\n\n    for f in l_ohe_drop_first:\n        if f in [\"word_right_in_description\", \"word_left_in_description\", \"word_strain_in_description\"]:\n            l_columns[f]=[n_columns, n_columns+1]\n            n_columns+=1\n        else:\n            temp = OneHotEncoder(sparse=False, dtype='uint8', drop=\"first\")\n            temp.fit(train_set[[f]])\n            l_columns[f] = [n_columns, n_columns+len(temp.categories_[0]) - 1]\n            n_columns +=len(temp.categories_[0])-1\n    \n    for f in l_ohe_handle:\n        if f in ['max_topic_1', 'max_topic_2']:\n            l_columns[f]=[n_columns, n_columns + n_topics]\n            n_columns+=n_topics\n        else:\n            temp = OneHotEncoder(sparse=False, dtype='uint8', handle_unknown='ignore')\n            temp.fit(train_set[[f]])\n            l_columns[f] = [n_columns, n_columns+len(temp.categories_[0])]\n            n_columns +=len(temp.categories_[0])\n\n    for f in l_stdscal + l_pass:\n        if f in [f\"topic_{j}\" for j in range(n_topics)]:\n            l_columns[f]=[n_columns, n_columns+1]\n            n_columns+=1\n        else:\n            l_columns[f] = [n_columns, n_columns+1]\n            n_columns +=1\n    \n    return l_columns\n\nl_cols = columns_range_for_permutation(\n    l_ohe_drop_first = [\"MaritalStatus\", \"Gender\", \"PartTimeFullTime\", \"DependentsOther\"\n            , \"word_right_in_description\", \"word_left_in_description\", \"word_strain_in_description\"\n            , 'DependentChildren', \"WeekdayOfAccident\", \"WeekdayOfReported\", \"YearReportDelay\"], \n    l_ohe_handle = ['max_topic_1', 'max_topic_2', 'YearOfAccident', 'MonthOfAccident'], \n    l_stdscal = ['Age', 'HoursWorkedPerWeek', 'HourOfAccident'] + [f\"topic_{j}\" for j in range(12)],\n    l_pass = [\"InitialIncurredCalimsCost\", \"WeeklyWages\" , \"DaysReportDelay\"], \n    n_topics=12)\nprint(l_cols)","a0bce5aa":"from tensorflow import keras\nfrom tensorflow.random import set_seed\nfrom keras.constraints import maxnorm","836d6615":"def build_model(input_shape, dense_layers = [128, 128, 256], n_dropout_layer = 1, dropout_rate = .5\n               , rate_dropout_input_layer = None\n               , activation = \"relu\", seed = __seed):\n    \n    random.seed(seed) ; np.random.seed(seed) ; set_seed(seed)\n\n    net = keras.models.Sequential()\n    \n    # Input Layer\n    if rate_dropout_input_layer is None:\n        net.add(keras.layers.Input(shape = input_shape))\n    else:\n        net.add(keras.layers.Dropout(rate = rate_dropout_input_layer, input_shape=(input_shape,), seed=seed))\n    \n    # Dense Layers\n    n_dense_layer = len(dense_layers)\n    for i, nn in enumerate(dense_layers):\n        \n        # Put a dropout layer after the lasts dense layers\n        if n_dense_layer - i - 1 < n_dropout_layer:\n            net.add(keras.layers.Dense(nn, activation = activation\n                , kernel_initializer = keras.initializers.HeNormal(seed)\n                , kernel_constraint = maxnorm(3)))\n            net.add(keras.layers.Dropout(rate = dropout_rate, seed=seed))\n        else:\n            net.add(keras.layers.Dense(nn, activation = activation\n                , kernel_initializer = keras.initializers.HeNormal(seed)))\n            \n    # Output layer\n    net.add(keras.layers.Dense(1, activation=\"linear\"))\n    \n    return net\n\n\nnn_params = { \"dense_layers\":[155, 155, 155, 310], \"n_dropout_layer\":4, \"dropout_rate\":.05\n        , \"rate_dropout_input_layer\":None}\nnn_params[\"input_shape\"] = train_set.shape[1]\nnet = build_model(**nn_params)\nprint(net.summary())\n\ndel net","1cc95d9d":"%%time\ndef nn_cv(X_train, pipe, y_param, dict_cols_feats, nn_params, epochs, learning_rate, batch_size\n        , X_test = None, verbose = False, check_feature_importance = False\n        , plot_history=False, early_stopping = False):\n\n    scores = [] ; feat_imp = {} ; best_iteration_ = 0\n    \n    y_test, y_valid = None, pd.Series(np.zeros(len(X_train)), index=X_train.index)\n    if not X_test is None: \n        X_test[\"UltimateIncurredClaimCost\"] = 0 ; X_test = X_test[list(X_train.columns)]\n        y_test = pd.Series(np.zeros(len(X_test)), index=X_test.index)\n        df_y_test = X_test[[y_param[\"groupby\"]]].copy()\n    \n    if plot_history:\n        fig, ax = plt.subplots(nrows = folds.n_splits, ncols = 3, figsize=(15, 4.2 * folds.n_splits))\n        plt.subplots_adjust(hspace = 0.5, wspace = 0.5)\n    \n    # Main loop on each fold\n#    if 1==1:\n#        X_trn, X_val = train_test_split(X_train, test_size=.2, random_state=__seed, stratify=X_train[y_param[\"groupby\"]])\n        \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, X_train[y_param[\"groupby\"]])):\n        \n        # Samples selection on each fold\n        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n        y_trn, y_val, df_y_trn, df_y_val, y_logstd = create_y_true(X_trn, X_val, y_param)\n\n        # Fit encoders and transform data\n        _ = pipe.fit(X_trn)\n        X_trn = pipe.transform(X_trn) ; X_val = pipe.transform(X_val)\n    \n        nn_params[\"input_shape\"] = X_trn.shape[1]\n        net = build_model(**nn_params)\n#        if verbose: print(net.summary())\n            \n        checkpoint = keras.callbacks.ModelCheckpoint(\"nn.h5\", monitor = \"val_loss\"\n                    , save_best_only = True, mode = \"min\", verbose = 0)\n        lr_early_stopping = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True\n                    , monitor=\"val_loss\", mode=\"min\", verbose=0)\n        lr_callback = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 10, min_lr=learning_rate*.5**5)\n        if early_stopping:\n            callbacks = [checkpoint, lr_early_stopping, lr_callback]\n        else:\n#            lr_callback = keras.callbacks.ReduceLROnPlateau(factor = 0.5, patience = 10)\n            callbacks = [checkpoint, lr_callback]\n        \n        net.compile(loss = \"mean_squared_error\", optimizer = keras.optimizers.Nadam(learning_rate = learning_rate))\n        fitted = net.fit(x = X_trn, y = y_trn, batch_size = batch_size, \n            epochs = epochs, verbose = 0, callbacks = callbacks, validation_data = (X_val, y_val))\n\n        # Prediction on vlidation samples\n        net = keras.models.load_model(\"nn.h5\")\n        y_pred = net.predict(X_val)[:,0] ; y_valid.iloc[val_idx] = y_pred\n\n        # Some plots\n        if plot_history:\n            \n            temp = pd.DataFrame(fitted.history)[[\"loss\", \"val_loss\"]].rename({\"loss\":\"train_loss\"}, axis=1) \n            temp.plot(ax=ax[fold_, 0])\n            ax[fold_, 0].set_title(f\"NN fit History - fold n\u00b0{fold_+1}\")\n            ax[fold_, 0].set_xlabel('Epoch')\n            ax[fold_, 0].set_ylabel('Loss')\n            \n            sns.scatterplot(y_val, y_pred, ax=ax[fold_, 1], color=\"red\")\n            ax[fold_, 1].set_title(f\"Predictions vs Time to Eruption - fold n\u00b0{fold_+1}\")\n            ax[fold_, 1].set_xlabel('Ultimate Incurred Claim Cost')\n            ax[fold_, 1].set_ylabel('Predictions')\n\n            sns.scatterplot(y_pred, y_val - y_pred, ax=ax[fold_, 2], color=\"red\")\n            ax[fold_, 2].set_title(f\"Residuals vs Prediction fold n\u00b0{fold_+1}\")\n            ax[fold_, 2].set_xlabel('Predictions')\n            ax[fold_, 2].set_ylabel('Residuals');\n\n        # Prediction on Test samples\n        if not X_test is None: \n            X_te = pipe.transform(X_test)\n            y_te = net.predict(X_te)[:,0]\n            y_te, _ = transform_y_pred(y_te, df_y_test, y_logstd)\n            y_test += y_te \/ folds.n_splits\n            \n        # Inverse transform and score of validation prediction\n        y_values, rmse_ = transform_y_pred(y_pred, df_y_val, y_logstd)\n        y_valid.iloc[val_idx] = y_values ; scores.append(rmse_)\n        if verbose: print(\"RMSE on fold {} : {:,.0f}\".format(fold_, scores[fold_]))\n            \n        # Features importance, by value permutation\n        if check_feature_importance:\n            for feat, loc in dict_cols_feats.items():\n                if feat not in feat_imp.keys(): feat_imp[feat] = []\n                temp_df = X_val.copy()\n                temp_df[:,loc[0]:loc[1]] = np.random.permutation(temp_df[:,loc[0]:loc[1]])\n                \n                y_temp = net.predict(temp_df)[:,0]\n                y_values, rmse_ = transform_y_pred(y_temp, df_y_val, y_logstd)\n                # RMSE increase after value permutation\n                # the higher the RMSE grows, more important is the feature\n                feat_imp[feat].append(rmse_ - scores[fold_])\n\n    # End loop\n            \n    # Mean score\n    if verbose: print(\"MEAN RMSE : {:,.0f}\\n\".format(np.array(scores).mean()))\n    \n    # Feature importance\n    feat_imp_df = None ; important_features = [] ; most_important_features= []\n    if check_feature_importance:\n        \n        feat_imp_df = pd.DataFrame(feat_imp).transpose()\n\n        scores_increase = [i for i in range(folds.n_splits)]\n        scores_increase_sign = [f\"sign_{i}\" for i in range(folds.n_splits)]\n\n        feat_imp_df[scores_increase_sign] = 0\n        for score_increase, score_increase_sign in zip(scores_increase, scores_increase_sign):\n            feat_imp_df.loc[feat_imp_df[score_increase]>0, score_increase_sign] = 1\n    \n        feat_imp_df[\"nb_folds\"] = feat_imp_df[scores_increase_sign].sum(axis=1)\n        most_important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] == folds.n_splits].index)\n        important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] >= 3 * folds.n_splits \/ 4].index)\n        \n        feat_imp_df[\"mean\"] = feat_imp_df[list(range(folds.n_splits))].mean(axis=1)\n        feat_imp_df.sort_values(\"mean\", ascending = False, inplace=True)\n\n    # End\n    return {\"rmse\":np.array(scores).mean(), \"y_test\":y_test, \"y_valid\":y_valid\n           , \"feat_imp_df\":feat_imp_df}\n\n\n\n\n\n# Parameter for target transformation : log and stdscal\ny_param = {\"groupby\" : \"YearOfAccident\", \"feature\" : 'UltimateIncurredClaimCost'\n            , \"stdscal\" : True, \"log\" : np.log1p, \"trunc_UltimateIncurredClaimCost\" : True\n            , \"verbose\" : False}\n\n# NN parameters\nnn_params = { \"dense_layers\":[155, 155, 155, 310], \"n_dropout_layer\":4, \"dropout_rate\":.05\n        , \"rate_dropout_input_layer\":None}\n\nfor num_topics in [12]:\n    \n    l_ohe_drop_first = [\"MaritalStatus\", \"Gender\", \"PartTimeFullTime\", \"DependentsOther\"\n            , \"word_right_in_description\", \"word_left_in_description\", \"word_strain_in_description\"\n            , 'DependentChildren', \"WeekdayOfAccident\", \"WeekdayOfReported\", \"YearReportDelay\"]\n    l_ohe_handle = ['max_topic_1', 'max_topic_2', 'YearOfAccident', 'MonthOfAccident']\n    l_stdscal = ['Age', 'HoursWorkedPerWeek', 'HourOfAccident'] + [f\"topic_{j}\" for j in range(num_topics)]\n    l_pass = [\"InitialIncurredCalimsCost\", \"WeeklyWages\" , \"DaysReportDelay\"]\n    \n    dict_cols_feats = columns_range_for_permutation(\n        l_ohe_drop_first = l_ohe_drop_first, \n        l_ohe_handle = l_ohe_handle, \n        l_stdscal = l_stdscal,\n        l_pass = l_pass, \n        n_topics=num_topics)\n\n    pipe = make_pipeline(\n    search_dominant_topic(num_topics)\n    , Log_StdScaler_ByGroup(feature = \"DaysReportDelay\")\n    , Log_StdScaler_ByGroup(feature = \"InitialIncurredCalimsCost\")\n    , Log_StdScaler_ByGroup(feature = \"WeeklyWages\")\n    , make_column_transformer(\n        (OneHotEncoder(sparse=False, dtype='uint8', drop=\"first\"), l_ohe_drop_first)\n        , (OneHotEncoder(sparse=False, dtype='uint8', handle_unknown=\"ignore\"), l_ohe_handle)\n        , (StandardScaler(), l_stdscal)\n        , (\"passthrough\", l_pass)\n    , remainder=\"drop\"))\n\n    model_nn_AB = nn_cv(train_set, pipe, dict_cols_feats=dict_cols_feats, y_param=y_param\n        , nn_params=nn_params, epochs=30, batch_size = 64, learning_rate = .001\n        , X_test = test_set\n        , plot_history=True, check_feature_importance = True, early_stopping=True, verbose = True)\n    \n    print(f\"Mean RMSE : {model_nn_AB['rmse']:,.0f} for num_topics {num_topics}\")\n","c0705536":"plot_features_importance(model_nn_AB[\"feat_imp_df\"])","fbc53a8c":"import lightgbm as lgbm","ef4f666e":"%%time\ndef fit_lgbm_regressor(X_train, pipe, y_params, params, X_test = None\n    , early_stopping_rounds=50, n_estimators = 2000, check_feature_importance = False\n    , plots = False, verbose = True):\n    \n    if verbose:\n        lgbm_verbose=1000\n    else:\n        params[\"verbosity\"]=0 ; lgbm_verbose=0\n    \n    scores = [] ; feat_imp = {} ; best_iteration_ = 0\n    \n    y_test, y_valid = None, pd.Series(np.zeros(len(X_train)), index=X_train.index)\n    if not X_test is None: \n        y_test = pd.Series(np.zeros(len(X_test)), index=X_test.index)\n        df_y_test = X_test[[y_param[\"groupby\"]]].copy()\n    \n    if plots:\n        fig, ax = plt.subplots(nrows = folds.n_splits, ncols = 3, figsize=(15, 4.2 * folds.n_splits))\n        plt.subplots_adjust(hspace = 0.5, wspace = 0.4)\n    \n    # Main loop on each fold\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, X_train[y_params[\"groupby\"]])):\n        \n        # Samples selection on each fold\n        X_trn, X_val = X_train.iloc[trn_idx], X_train.iloc[val_idx]\n        y_trn, y_val, df_y_trn, df_y_val, y_logstd = create_y_true(X_trn, X_val, y_param)\n\n        # Fit encoders and transform data\n        _ = pipe.fit(X_trn)\n        X_trn = pipe.transform(X_trn) ; X_val = pipe.transform(X_val)\n    \n        # Fit model\n        model = lgbm.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n        model.fit(X_trn, y_trn, eval_set=[(X_trn, y_trn), (X_val, y_val)]\n            , eval_metric='rmse', early_stopping_rounds = early_stopping_rounds\n            , verbose=lgbm_verbose)\n\n        best_iteration = model.best_iteration_\n        best_iteration_ += best_iteration \/ folds.n_splits\n        \n        # Prediction on validation samples\n        y_pred = model.predict(X_val, num_iteration=best_iteration)\n\n        # Residual Plots \n        if plots:\n            training_curve_df = pd.DataFrame.from_dict(\n                {\"Train\" : model.evals_result_[\"training\"][\"rmse\"]\n                , \"Valid\" : model.evals_result_[\"valid_1\"][\"rmse\"]})\n            training_curve_df.plot(ax = ax[fold_, 0])\n            ax[fold_, 0].set_title(f\"Training History - fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 0].set_xlabel('Iteration')\n            ax[fold_, 0].set_ylabel('RMSE')\n\n            sns.scatterplot(y_val, y_pred, ax = ax[fold_, 1])\n            ax[fold_, 1].set_title(f\"Predictions vs True value fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 1].set_xlabel('True Ultimate Incurred Claim Cost')\n            ax[fold_, 1].set_ylabel('Predictions')\n            \n            sns.scatterplot(y_pred, y_val - y_pred, ax = ax[fold_, 2])\n            ax[fold_, 2].set_title(f\"Residuals vs Predictions fold n\u00b0{fold_ + 1}\")\n            ax[fold_, 2].set_xlabel('Predictions')\n            ax[fold_, 2].set_ylabel('Residuals');\n\n        # Prediction on Test samples\n        if not X_test is None: \n            X_te = pipe.transform(X_test)\n            y_te = model.predict(X_te, num_iteration=best_iteration)\n            y_te, _= transform_y_pred(y_te, df_y_test, y_logstd)\n            y_test += y_te \/ folds.n_splits\n\n        # Inverse transform and score of validation prediction\n        y_values, rmse_ = transform_y_pred(y_pred, df_y_val, y_logstd)\n        y_valid.iloc[val_idx] = y_values ; scores.append(rmse_)\n        if verbose: print(\"RMSE on fold {} : {:,.0f}\".format(fold_ + 1, scores[fold_]))\n            \n        # For feature importance\n        if check_feature_importance:\n            \n            # For each feature...\n            for feat in X_val.columns:\n                \n                if fold_ == 0: feat_imp[feat] = []\n                \n                # Permutation of values\n                temp_df = pd.DataFrame(X_val.copy(), columns=X_val.columns)\n                temp_df[feat] = np.random.permutation(temp_df[feat])\n        \n                # Prediction and RMSE after values permutation\n                y_temp = model.predict(temp_df, num_iteration=model.best_iteration_)\n                _, rmse_ = transform_y_pred(y_temp, df_y_val, y_logstd)\n                \n                # Increase of RMSE after value permutation\n                feat_imp[feat].append(rmse_ - scores[fold_])\n                \n    # Folds loop END\n    \n    # Mean score\n    if verbose: print(\"MEAN RMSE : {:,.0f}\\n\".format(np.array(scores).mean()))\n    \n    # Feature importance\n    feat_imp_df = None ; important_features = [] ; most_important_features= []\n    if check_feature_importance:\n        \n        feat_imp_df = pd.DataFrame(feat_imp).transpose()\n\n        scores_increase = [i for i in range(folds.n_splits)]\n        scores_increase_sign = [f\"sign_{i}\" for i in range(folds.n_splits)]\n\n        feat_imp_df[scores_increase_sign] = 0\n        for score_increase, score_increase_sign in zip(scores_increase, scores_increase_sign):\n            feat_imp_df.loc[feat_imp_df[score_increase]>0, score_increase_sign] = 1\n    \n        feat_imp_df[\"nb_folds\"] = feat_imp_df[scores_increase_sign].sum(axis=1)\n        most_important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] == folds.n_splits].index)\n        important_features = list(feat_imp_df[feat_imp_df[\"nb_folds\"] >= 3 * folds.n_splits \/ 4].index)\n        \n        feat_imp_df[\"mean\"] = feat_imp_df[list(range(folds.n_splits))].mean(axis=1)\n        feat_imp_df.sort_values(\"mean\", ascending = False, inplace=True)\n\n    return {\"rmse\":np.array(scores).mean(), \"y_test\":y_test, \"y_valid\":y_valid, \"feat_imp_df\":feat_imp_df \n            , \"most_important_features\":most_important_features\n            , \"important_features\":important_features}\n            \nparams = {'num_leaves': 20,\n            'min_data_in_leaf': 10, \n            'learning_rate': .01,\n            'max_bin': 255,            \n            \"feature_fraction\": .80,\n            \"bagging_freq\": 1,\n            \"bagging_fraction\": .80,\n            \"lambda_l2\": 0.05,\n            \"boosting\": \"gbdt\",\n            'objective':'regression',\n            \"metric\": 'rmse',\n            \"verbosity\": -1,\n            \"nthread\": -1,\n            'boost_from_average': True,\n            \"bagging_seed\": __seed, \n        }\n\ny_param = {\"groupby\" : \"YearOfAccident\", \"feature\" : 'UltimateIncurredClaimCost'\n            , \"stdscal\" : False, \"log\" : None, \"trunc_UltimateIncurredClaimCost\" : False\n            , \"verbose\" : False}\n\n\n#for i in range(5, 15):\nfor i in [6]:\n    pipe = make_pipeline(\n        search_dominant_topic(i)\n        , ClaimDesciptionScore(regressor = Ridge(alpha=.1), stop_words = __stop_words\n            , lemmatizer=None, ngram_range=(1,5)\n            , drop_duplicates_words=True, return_score = False)\n        , LabelEncoders([\"MaritalStatus\", \"Gender\", \"PartTimeFullTime\"\n            , \"word_left_in_description\", \"word_right_in_description\", \"word_strain_in_description\"\n            , \"WeekdayOfReported\", \"WeekdayOfAccident\"])\n        , RankFrequencyEncoder(\"InitialIncurredCalimsCost\", output_freq = True, output_rank = False)\n        , drop_features(['WeeksReportDelay', 'YearReported']))\n\n    lgbm_result = fit_lgbm_regressor(train_set, pipe, y_param, params, verbose=False, check_feature_importance = True\n                            , X_test=test_set, plots = True)\n    \n    print(f\"Mean RMSE for num_topics {i} : {lgbm_result['rmse']:,.0f}\")","98c064a3":"plot_features_importance(lgbm_result[\"feat_imp_df\"])","838dc2ac":"from sklearn.model_selection import KFold\nfolds = KFold(n_splits=5, random_state = __seed, shuffle = True)\n\ndef fit_blend(train_series, test_series, output_filename):\n    \n    # Predictions for several models on the train set, on valid samples only\n    df = pd.DataFrame(train_series).T\n    \n    # Predictions for several models on the test set\n    df_test = pd.DataFrame(test_series).T\n    \n    # \n    y_valid = pd.Series(np.zeros(len(train_set)), index=train_set.index)\n    y_valid.name = \"UltimateIncurredClaimCost\"\n    \n    y_test = pd.Series(np.zeros(len(test_set)), index=test_set.index)\n    y_test.name = \"UltimateIncurredClaimCost\"\n    \n    val_rmse = []\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_set)):\n        \n        X_trn, X_val = df.iloc[trn_idx], df.iloc[val_idx]\n        y_trn, y_val = train_set.iloc[trn_idx][\"UltimateIncurredClaimCost\"], train_set.iloc[val_idx][\"UltimateIncurredClaimCost\"]\n        \n        model = Ridge().fit(X_trn, y_trn)\n        \n        # Prediction and score on valid samples\n        y_valid.iloc[val_idx] = model.predict(X_val)\n        val_rmse.append(rmse(y_val, y_valid.iloc[val_idx]))\n        \n        # Prediction and test set\n        y_test += model.predict(df_test) \/ folds.n_splits\n\n        # Score with originals models\n        rmse_ = []\n        for f in df.columns:\n            rmse_.append(rmse(train_set.iloc[val_idx][\"UltimateIncurredClaimCost\"], df.iloc[val_idx][f]))\n        with np.printoptions(precision=3, suppress=True):\n            print(\"RMSE with blend {:,.0f} vs RMSE with best individual {:,.0f}. Blend coef. : {}\".format(val_rmse[fold_], np.min(rmse_), model.coef_))\n        \n    print(\"Mean RMSE : {:,.0f}\\n\".format(np.mean(val_rmse)))\n\n    \n    for f in df.columns:\n        print(\"RMSE with individual model n\u00b0{} an all valid samples : {:,.0f}\".format(f, rmse(train_set[\"UltimateIncurredClaimCost\"], df[f])))\n    print(\"RMSE with blend : {:,.0f}\".format(rmse(train_set[\"UltimateIncurredClaimCost\"], y_valid)))\n    sample_submission = y_test.to_frame().reset_index()\n    sample_submission.to_csv(output_filename, index=False)\n    \nfit_blend(\n    train_series = [et_result[\"y_valid\"], model_catboost[\"y_valid\"], model_nn_AB[\"y_valid\"], lgbm_result[\"y_valid\"]]\n    , test_series = [et_result[\"y_test\"], model_catboost[\"y_test\"], model_nn_AB[\"y_test\"], lgbm_result[\"y_test\"]]\n    , output_filename = \"submission.csv\")","e86544af":"## Train and test comparison\nThere is no difference :","96f4d739":"# Part 6 : CatBoost\nFor features importance, I use values permutation in valid samples after training. Look at :\nhttps:\/\/www.kaggle.com\/dansbecker\/permutation-importance","69da956f":"## Rank Frequency Encoder\nA king of frequency encoder, by Year Of Accident.\nFor Initial Claim Cost.","b4effb65":"## Label Encoder (for several columns at the same time)","3783ac71":"## Features with nan values","01ca78f9":"## Highest values of Incurred Claim Cost and Initial Claim Cost","c753ee8e":"## Count vectorizer - ClaimDescription\nFit a regressor with ClaimDescription only, a score.","a47c35fd":"# Part 3 : Feature Engineering \/ data preparation","3d86704f":"# Part 2 : Cleaning data","d0502a41":"## To drop useless columns","30d0ffb5":"# Part 5 : extratree","9ed1f934":"# Part 7 : Neural networks\nPipeline of data preparation is different than for tree methods ; instead of label encoders for categorical features, i will use one hot encoder. Standardize continous features is now very important...","34434aee":"## A tool to detect columns per feature in the matrix after one hot encoding and the others transformations\nThis part is not very clean, I need a tool to understand feature importance in the NN ; but because I used a rich pipeline of transformation with one hot encoders (which create several columns for one feaure), I need this DIY. ","7222663d":"## Check categories of several features","7caab970":"# Part 9 : Blend\nFit a blend of the four models, and check by CV on validation predictions that blend will improve RMSE.","823b2dd4":"# Part 1 : train set and test set\n## Read data","804b7a78":"## Initial Claim cost : before and after log transformation and per year of accident\nAfter standard scaler transformation, Initial Claim Cost become independant of year of accident.","f042e10b":"In this notebook, there will be :\n* some EDA\n* several regressors and several pipelines to transform data : \n    * Extratree \n    * Catboost\n    * a Neural Network with Keras\n    * Light GBM\n* and a blend of these regressors\n\nThere are several kind of features in this challenge : text, categorical, continous. Before each regression, we need to fit different transformers for every kind of features. There is an inflation problem with continous features and there are some extrem values => log transformation and standardize transformation by year of accident are usefull for several regression methods (ridge, NN). \n\nA score on **ClaimDescription** text will be used as a feature in those regressors.\nAnd there is a faster implementation of LDA than in https:\/\/www.kaggle.com\/tanguypledel\/topic-modeling-lda-implementation. Thank you Tanguy Pledel for your notebook.\n\nThe 27th january (6 weeks before the end of this competition), I had my first submission with a RMSE under 30000 on the public leaderboard with a notebook like this one.\n\nThe RMSE with this notebook can be lower with a higher number of folds : 20 for example.\n\nGood reading !\nAnd please share your improvements !","9883cb77":"## Weekly Wages : before and after standard scaler per year of accident\nAfter standard scaler transformation, weekly wages become independant of year of accident.","cbad4a9e":"# Part 8 : LGBM","9005db6e":"## Now, cleaning data\nAnd dates transformation","1b12d91e":"## Transformation of monetary features\nLog transformation, because of extrem values  \nStandardize transformation by year, because of inflation ","2546729b":"## RMSE with Initial Incurred Claims Cost","63d354bc":"## Topics in ClaimDescription : LDA_model with gensin\nhttps:\/\/www.kaggle.com\/tanguypledel\/topic-modeling-lda-implementation\/data  \nTanguy Pledel\n\nI add parallelize run to go faster during transformation. And I've written a transformer.","5ba2dc8f":"## About Ultimate Incurred Claim Cost (target) and Initial Claim Cost","1087d77a":"## Ultimate Incurred Claim Cost : Before and after log\/standard scaler encoder","576954e1":"On the left size, number of folds where RMSE is increasing after values permutation (the feature is important in those folds). If RMSE is decresing after values permutation, the feature is useless on validation data.\n\nOn the right size, mean RMSE increase after value permutation, on all folds.","76185f60":"## Constants & libraries","8ab7184f":"## NN","939ca541":"# Part 4 : Data Description and check of some feature transformers","8d70c4bd":"## About log transformation of target during CV"}}