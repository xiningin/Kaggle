{"cell_type":{"17c9316e":"code","11ea7b20":"code","4899b36b":"code","8d0530d4":"code","2bf01499":"code","ea815b9e":"code","36a2aef9":"code","04f6d1b0":"code","50c54da2":"code","8f1aab6f":"code","cbbf4e98":"code","7217b622":"code","039bd766":"code","9202f4c0":"code","3a14ec15":"code","2ac03b55":"code","9fd6ca25":"markdown","ad96ac8c":"markdown","6a792151":"markdown","b4c85ab3":"markdown","ac934bd8":"markdown","283ae098":"markdown","6c9b8663":"markdown","db526366":"markdown","fe13bf85":"markdown","69735495":"markdown","ea294311":"markdown","87583afa":"markdown","f1a4224c":"markdown","2236dc89":"markdown","26886694":"markdown","a0d578ad":"markdown","0659f1aa":"markdown","f1af1d7f":"markdown","a393c052":"markdown","eef70331":"markdown"},"source":{"17c9316e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os # to use operating system dependent functionality\nimport librosa # to extract speech features\nimport wave # read and write WAV files\nimport matplotlib.pyplot as plt # to generate the visualizations\n\n# MLP Classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\n# LSTM Classifier\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import *\nfrom keras.optimizers import rmsprop","11ea7b20":"def extract_mfcc(wav_file_name):\n    #This function extracts mfcc features and obtain the mean of each dimension\n    #Input : path_to_wav_file\n    #Output: mfcc_features'''\n    y, sr = librosa.load(wav_file_name)\n    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0)\n    \n    return mfccs","4899b36b":"##### load radvess speech dataset #####\nradvess_speech_labels = [] # to save extracted label\/file\nravdess_speech_data = [] # to save extracted features\/file\nsubset_dirs_list = ['Actor_01', 'Actor_02', 'Actor_03','Actor_04', 'Actor_05', 'Actor_06','Actor_07', 'Actor_08']\n\nfor dirname, dirs, filenames in os.walk('\/kaggle\/input\/ravdess-emotional-speech-audio\/'):\n    dirs[:] = [d for d in dirs if d in subset_dirs_list] # you can remove it to train the model over the entire dataset \n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        radvess_speech_labels.append(int(filename[7:8]) - 1) # the index 7 and 8 of the file name represent the emotion label\n        wav_file_name = os.path.join(dirname, filename)\n        ravdess_speech_data.append(extract_mfcc(wav_file_name)) # extract MFCC features\/file\n        \nprint(\"Finish Loading the Dataset\")","8d0530d4":"#### convert data and label to array\nravdess_speech_data_array = np.asarray(ravdess_speech_data) # convert the input to an array\nravdess_speech_label_array = np.array(radvess_speech_labels)\nravdess_speech_label_array.shape # get tuple of array dimensions\n\n#### make categorical labels\nlabels_categorical = to_categorical(ravdess_speech_label_array) # converts a class vector (integers) to binary class matrix\nravdess_speech_data_array.shape\nlabels_categorical.shape\nprint(ravdess_speech_label_array.shape, ravdess_speech_data_array.shape, labels_categorical.shape)","2bf01499":"x_train,x_test,y_train,y_test= train_test_split(np.array(ravdess_speech_data_array),labels_categorical, test_size=0.20, random_state=9)","ea815b9e":"# Initialize the Multi Layer Perceptron Classifier\nmodel=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)","36a2aef9":"# Train the model\nmodel.fit(x_train,y_train)","04f6d1b0":"# Predict for the test set\ny_pred=model.predict(x_test)","50c54da2":"# Calculate the accuracy of our model\naccuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n# Print the accuracy\nprint(\"Accuracy: {:.2f}%\".format(accuracy*100))","8f1aab6f":"# Split the training, validating, and testing sets\nnumber_of_samples = ravdess_speech_data_array.shape[0]\ntraining_samples = int(number_of_samples * 0.8)\nvalidation_samples = int(number_of_samples * 0.1)\ntest_samples = int(number_of_samples * 0.1)","cbbf4e98":"# Define the LSTM model\ndef create_model_LSTM():\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=False, input_shape=(40, 1)))\n    model.add(Dense(64))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(32))\n    model.add(Dropout(0.4))\n    model.add(Activation('relu'))\n    model.add(Dense(8))\n    model.add(Activation('softmax'))\n    \n    # Configures the model for training\n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    return model","7217b622":"### train using LSTM model\nmodel_A = create_model_LSTM()\nhistory = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1), labels_categorical[training_samples:training_samples+validation_samples]), epochs=100, shuffle=True)","039bd766":"### loss plots using LSTM model\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\n\nplt.plot(epochs, loss, 'ro', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","9202f4c0":"### accuracy plots using LSTM model\nplt.clf()                                                \n\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nplt.plot(epochs, acc, 'ro', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.show()","3a14ec15":"### evaluate using model A\nmodel_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:], -1), labels_categorical[training_samples + validation_samples:])","2ac03b55":"model_A.save_weights(\"Model_LSTM.h5\")","9fd6ca25":"*  Split the dataset into training and testing sets: (training set) a subset to train a model, (test set) a subset to test the trained model.\n*  **Note:**(train_test_split) function will split arrays or matrices into random train and test subsets, in our example, training set 80% and testing set 20% ","ad96ac8c":"*  **Generates output predictions for the input samples.**","6a792151":"* **Trains the model for a fixed number of epochs (iterations on a dataset).**","b4c85ab3":"## C. Train and Test MLP Classifier","ac934bd8":"**Preparing the dataset for machine learning models**","283ae098":"*  **(evaluate function): Returns the loss value & metrics values for the model in test mode.**","6c9b8663":"We have obtained **09.38% accuracy** (depend on the execution) by using MLP as classifier, but this is can be improved by adjusting the training hyperparameter, increasing the dataset, or trying other machine learning models. ","db526366":"## E. Credits and Further Reading\n1. Audio and music processing in Python, https:\/\/librosa.org\n2. ML\/DL and audio analysis, https:\/\/towardsdatascience.com\/how-to-apply-machine-learning-and-deep-learning-methods-to-audio-analysis-615e286fcbbc\n3. Sound classification, https:\/\/medium.com\/@mikesmales\/sound-classification-using-deep-learning-8bc2aa1990b7","fe13bf85":"* **Save the weights of the model as a HDF5 file**","69735495":"* **What is A multilayer perceptron (MLP)?**\n* A multilayer perceptron (MLP) is a deep, artificial neural network. It is composed of more than one perceptron. They are composed of an input layer to receive the signal, an output layer that makes a decision or prediction about the input, and in between those two, an arbitrary number of hidden layers that are the true computational engine of the MLP. **More Details:** (https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)","ea294311":"**RAVDESS Dataset Description**\n\nWe will use Speech audio-only files (16bit, 48kHz .wav) from the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset. This portion of the RAVDESS contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. **for More details:** (https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)","87583afa":"*  **(accuracy)**: is a function that is used to judge the performance of your model.","f1a4224c":"* Define MLP and adjust its **hyperparameter** (a parameter whose value is set before the learning process begins.)","2236dc89":"## B. Reading the Dataset & Feature Extraction","26886694":"**What is Mel-frequency Cepstrum Coefficients (MFCC)?** \n> The sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. **More Details: (https:\/\/tinyurl.com\/o88j3um)**\n![Mel-frequency Cepstrum Coefficients](https:\/\/miro.medium.com\/max\/3430\/1*pzE4i1TXaLCmzTXgdxFZjQ.jpeg)","a0d578ad":"## A. Importing the required Libraries","0659f1aa":"**Note:** The execution time of the following code may take few minutes to finish loading and preprocessing a subset of the dataset, and you can use the entire dataset to enhance the models accuracy.","f1af1d7f":"# Speech Emotion Recognition (SER)\n\n<br>\n\n* This notebook serves as an introduction to process audio data to predict user emotions.\n> \"Speech Emotion Recognition (SER) is one of the most challenging tasks in speech signal analysis domain, it is a research area problem which tries to infer the emotion from the speech signals. The importance of emotion recognition is getting popular with improving user experience and the engagement of Voice User Interfaces (VUIs) (e.g., smart speaker assistance). For example, customer services, recommender systems, and healthcare applications.\"\n\n<div><center><img src=\"https:\/\/camo.githubusercontent.com\/d7ecf631b87e28e81820007e46b77650b51e2f756ab90849312b4fb3510371d5\/68747470733a2f2f692e696d6775722e636f6d2f663154717669542e6a706567\" width=\"600px\"\/>\nTonal Emotions(source:MevonAI,https:\/\/tinyurl.com\/ze94b7ud)<\/center><\/div>\n\n","a393c052":"## D. Train and Test LSTM Classifier\n\n> In this example, we will use another model to improve the accuracy which is LSTM model.","eef70331":"* **What is Long short-term memory (LSTM)?**\n\nRecurrent neural networks, of which LSTMs are the most powerful and well known subset, are a type of artificial neural network designed to recognize patterns in sequences of data, such as text, genomes, handwriting and the spoken word. **More Details:** (https:\/\/keras.io\/layers\/recurrent\/#lstm) (Understanding LSTM Networks:https:\/\/tinyurl.com\/pbamomv)"}}