{"cell_type":{"3b2f2495":"code","e50eca82":"code","527bdcce":"code","72cb5a98":"code","bf77d8d8":"code","1edfe7d1":"code","78e5bef0":"code","4293bd30":"markdown","246e81e0":"markdown","2458b718":"markdown","8cd5671d":"markdown","4e9f4f8f":"markdown","b9899791":"markdown","eafc6f6a":"markdown","edbb3f07":"markdown"},"source":{"3b2f2495":"%matplotlib inline","e50eca82":"import pandas as pd\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \n\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n\nmpl.rcParams[\"figure.figsize\"] = (9,6)","527bdcce":"# Generate synthetic dataset with 8 blobs\nX, y = make_blobs(n_samples=1000, n_features=12, centers=8, shuffle=True, random_state=42)","72cb5a98":"# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(4,12))\n\nvisualizer.fit(X)    # Fit the data to the visualizer\nvisualizer.poof()    # Draw\/show\/poof the data","bf77d8d8":"# Instantiate the clustering model and visualizer \nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(4,12), metric='calinski_harabaz', timings=False)\n\nvisualizer.fit(X)    # Fit the data to the visualizer\nvisualizer.poof()    # Draw\/show\/poof the data","1edfe7d1":"# Instantiate the clustering model and visualizer \nmodel = KMeans(8)\nvisualizer = SilhouetteVisualizer(model)\n\nvisualizer.fit(X)    # Fit the data to the visualizer\nvisualizer.poof()    # Draw\/show\/poof the data","78e5bef0":"# Instantiate the clustering model and visualizer \nmodel = KMeans(6)\nvisualizer = SilhouetteVisualizer(model)\n\nvisualizer.fit(X)    # Fit the data to the visualizer\nvisualizer.poof()    # Draw\/show\/poof the data","4293bd30":"## Elbow Method\n\nK-Means is a simple unsupervised machine learning algorithm that groups data into the number $K$ of clusters specified by the user, even if it is not the optimal number of clusters for the dataset.\n\nYellowbrick's `KElbowVisualizer` implements the \u201celbow\u201d method of selecting the optimal number of clusters by fitting the K-Means model with a range of values for $K$. If the line chart looks like an arm, then the \u201celbow\u201d (the point of inflection on the curve) is a good indication that the underlying model fits best at that point.\n\nIn the following example, the `KElbowVisualizer` fits the model for a range of $K$ values from 4 to 11, which is set by the parameter `k=(4,12)`. When the model is fit with 8 clusters we can see an \"elbow\" in the graph, which in this case we know to be the optimal number since we created our synthetic dataset with 8 clusters of points.","246e81e0":"It is important to remember that the Elbow method does not work well if the data is not very clustered. In this case, you might see a smooth curve and the optimal value of $K$ will be unclear.\n\nYou can learn more about the Elbow method at Robert Grove's [Blocks](https:\/\/bl.ocks.org\/rpgove\/0060ff3b656618e9136b).","2458b718":"### Load the Data\n\nFor the following examples, we'll use scikit-learn's `make_blobs()` function to create a sample two-dimensional dataset with 8 random clusters of points.","8cd5671d":"## Silhouette Visualizer\n\nSilhouette analysis can be used to evaluate the density and separation between clusters. The score is calculated by averaging the silhouette coefficient for each sample, which is computed as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, normalized by the maximum value. This produces a score between -1 and +1, where scores near +1 indicate high separation and scores near -1 indicate that the samples may have been assigned to the wrong cluster.\n\nThe `SilhouetteVisualizer` displays the silhouette coefficient for each sample on a per-cluster basis, allowing users to visualize the density and separation of the clusters. This is particularly useful for determining cluster imbalance or for selecting a value for $K$ by comparing multiple visualizers.\n\nSince we created the sample dataset for these examples, we already know that the data points are grouped into 8 clusters. So for the first `SilhouetteVisualizer` example, we'll set $K$ to 8 in order to show how the plot looks when using the optimal value of $K$.\n\nNotice that graph contains homogeneous and long silhouettes. In addition, the vertical red-dotted line on the plot indicates the average silhouette score for all observations.","4e9f4f8f":"By default, the scoring parameter metric is set to distortion, which computes the sum of squared distances from each point to its assigned center. However, two other metrics can also be used with the `KElbowVisualizer`\u2014`silhouette` and `calinski_harabaz`. The `silhouette` score is the mean silhouette coefficient for all samples, while the `calinski_harabaz` score computes the ratio of dispersion between and within clusters.\n\nThe `KElbowVisualizer` also displays the amount of time to fit the model per $K$, which can be hidden by setting `timings=False`. In the following example, we'll use the `calinski_harabaz` score and hide the time to fit the model.","b9899791":"Yellowbrick also offers Classification, Feature Analysis, Model Selection, Regression, and Text Modeling Visualizers. You can learn more about them by reading their [documentation](http:\/\/www.scikit-yb.org\/en\/latest\/) and checking out my other Yellowbrick notebooks for more examples and customization tips.","eafc6f6a":"For the next example, let's see what happens when using a non-optimal value for $K$, in this case, 6.\n\nNow we see that the width of clusters 1 and 2 have both increased and their silhouette coefficient scores have dropped. This occurs because the width of each silhouette is proportional to the number of samples assigned to the cluster. The model is trying to fit our data into a smaller than optimal number of clusters, making two of the clusters larger (wider) but much less cohesive (as we can see from their below-average scores).","edbb3f07":"# Yellowbrick \u2014 Clustering Evaluation Examples\n\nThe Yellowbrick library is a diagnostic visualization platform for machine learning that allows data scientists to steer the model selection process. It extends the scikit-learn API with a new core object: the `Visualizer`. Visualizers allow models to be fit and transformed as part of the scikit-learn pipeline process, providing visual diagnostics throughout the transformation of high-dimensional data.\n\nIn machine learning, clustering models are unsupervised methods that attempt to detect patterns in unlabeled data. There are two primary classes of clustering algorithms: *agglomerative* clustering which links similar data points together, and *centroidal* clustering which attempts to find centers or partitions in the data.\n\nCurrently, Yellowbrick provides two visualizers to evaluate *centroidal* mechanisms, particularly K-Means clustering, that help users discover an optimal $K$ parameter in the clustering metric:\n\n- `KElbowVisualizer` visualizes the clusters according to a scoring function, looking for an \"elbow\" in the curve.\n- `SilhouetteVisualizer` visualizes the silhouette scores of each cluster in a single model."}}