{"cell_type":{"1542ffef":"code","31832e9a":"code","f454a26e":"code","7dae26ed":"code","28373f69":"code","e8e22f57":"code","0b75f81d":"code","5ef8d3d4":"code","a813ca88":"code","2728d0f5":"code","f7cfe32b":"code","d26cb6d6":"code","2113abcb":"code","8f44832d":"code","94ae483e":"code","c15a4d85":"code","559cec60":"code","f63ef2dc":"code","dce4080b":"code","92bef208":"code","3284c888":"code","d909d664":"code","257f2bda":"code","7180798f":"code","f8f6f585":"code","1208ddfa":"code","82476461":"code","327829bc":"code","d16acf1a":"code","097e0092":"code","882950c3":"code","07082e82":"code","6511140b":"code","ef49875a":"code","02483f20":"code","6be35d97":"code","b949879d":"code","564b018c":"code","f1d91afd":"code","843a7b58":"code","d1252727":"code","8f0168af":"code","80bca673":"code","3498efde":"code","c8868244":"code","9a2ebeba":"code","03b18ed8":"code","0ce66dca":"code","ef67cd4d":"code","85767b8c":"code","2b32d46f":"code","683c7366":"code","b6075374":"code","c53f9d63":"code","e7e3ed81":"code","dc7be426":"code","ef533479":"code","183bcae9":"code","51a44f9a":"code","1615d80d":"code","dd96af4d":"code","d5061d5b":"code","301c5189":"markdown","b6b0daab":"markdown"},"source":{"1542ffef":"# !pip list","31832e9a":"import numpy as np \nimport pandas as pd\nimport os\nfrom tqdm import tqdm ","f454a26e":"import tensorflow as tf\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Model, load_model\nfrom keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\nfrom keras.layers import concatenate, BatchNormalization, Input\nfrom keras.layers.merge import add\nfrom keras.utils import to_categorical\n# from keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport cv2\nimport string\nimport time\nimport sys\nprint(\"Running.....\")","7dae26ed":"os.listdir('\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/')","28373f69":"token_path = '\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\ntext = open(token_path, 'r', encoding = 'utf-8').read()\n# print(text[:500])","e8e22f57":"len(text.split('\\n'))\/\/5 ","0b75f81d":"# print(text[:1000])\n# print(type(text))\n# print(text.split('\\n')[0].split('\\t')[1])","5ef8d3d4":"def load_description(text):\n    mapping = dict()\n    for line in text.split(\"\\n\"):\n        token = line.split(\"\\t\")\n        if len(line) < 2:\n            continue\n        img_id = token[0].split('.')[0]\n        img_des = token[1]\n        if img_id not in mapping:\n            mapping[img_id] = list()\n        mapping[img_id].append(img_des)\n    return mapping\n\ndescriptions = load_description(text)\nprint(\"Number of items: \" + str(len(descriptions)))","a813ca88":"# descriptions","2728d0f5":"def clean_description(desc):\n    for key, des_list in desc.items():\n        for i in range(len(des_list)):\n            caption = des_list[i]\n            caption = [ch for ch in caption if ch not in string.punctuation]\n            caption = ''.join(caption)\n            caption = caption.split(' ')\n            caption = [word.lower() for word in caption if len(word)>1 and word.isalpha()]\n            caption = ' '.join(caption)\n            des_list[i] = caption\n\nclean_description(descriptions)\ndescriptions['1000268201_693b08cb0e']","f7cfe32b":"def to_vocab(desc):\n    words = set()\n    for key in desc.keys():\n        for line in desc[key]:\n            words.update(line.split())\n    return words\nvocab = to_vocab(descriptions)\nlen(vocab)","d26cb6d6":"import glob\n#same output as os.listdir\nimages = '\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/Images\/'\n# Create a list of all image names in the directory\nimg = glob.glob(images + '*.jpg')\nlen(img)","2113abcb":"i = plt.imread(img[99])\nplt.imshow(i)\ndel i","8f44832d":"from tqdm import tqdm\n\ntrain_path = '\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\ntrain_images = open(train_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\ntrain_img = []\n\nfor im in tqdm(img):\n    if(im[54:] in train_images):\n        train_img.append(im)","94ae483e":"test_path = '\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\ntest_images = open(test_path, 'r', encoding = 'utf-8').read().split(\"\\n\")\ntest_img = []\n\nfor im in tqdm(img):\n    if(im[len(images): ] in test_images):\n        test_img.append(im)\n","c15a4d85":"# train_img","559cec60":"# test_img","f63ef2dc":"len(train_img), len(test_img)","dce4080b":"#load descriptions of train and test set separately\ndef load_clean_descriptions(des, dataset):\n    dataset_des = dict()\n    for key, des_list in des.items():\n        if key+'.jpg' in dataset:\n            if key not in dataset_des:\n                dataset_des[key] = list()\n            for line in des_list:\n                desc = 'startseq ' + line + ' endseq'\n                dataset_des[key].append(desc)\n    return dataset_des\n\ntrain_descriptions = load_clean_descriptions(descriptions, train_images)\nprint('Descriptions: train=%d' % len(train_descriptions))","92bef208":"test_descriptions = load_clean_descriptions(descriptions, test_images)\nprint('Descriptions: test=%d' % len(test_descriptions))","3284c888":"# train_features","d909d664":"# train_descriptions","257f2bda":"# test_descriptions ","7180798f":"p = '\/kaggle\/input\/flickr8k\/flickr_data\/Flickr_Data\/Images\/1107246521_d16a476380.jpg'\nimg = plt.imread(p)\nplt.imshow(img)","f8f6f585":"base_model = VGG16(weights = 'imagenet')\n","1208ddfa":"base_model.layers[-2].output\n\nmodel_1 = Model(base_model.input, base_model.layers[-2].output)\n# base_model.summary()","82476461":"# model_1.save('vgg16_encodings.h5')","327829bc":"from keras.preprocessing.image import load_img, img_to_array\ndef preprocess_img(img_path):\n    #inception v3 accepts img in 299*299\n    img = load_img(img_path, target_size = (224, 224))\n    x = img_to_array(img)\n    # Add one more dimension\n    x = np.expand_dims(x, axis = 0)\n#     x = preprocess_input(x)\n    return x","d16acf1a":"#function to encode an image into a vector using inception v3\ndef encode(image):\n    image = preprocess_img(image)\n    vec = model_1.predict(image)\n    vec = np.reshape(vec, (vec.shape[1]))\n    return vec","097e0092":"from tqdm import tqdm\n#each image will be encoded to 4096 dimension vector(as per last layer of model)\n\n#run the encode function on all train images\n\n\n#--------ENCODING---------\n\n# start = time.time()\n# encoding_train = {}\n# for img in tqdm(train_img):\n#     encoding_train[img[len(images):]] = encode(img)\n# print(\"Time Taken is: \" + str(time.time() - start))\n\n# #Encode all the test images\n# start = time.time()\n# encoding_test = {}\n# for img in tqdm(test_img):\n#     encoding_test[img[len(images):]] = encode(img)\n\n# print(\"Time taken is: \" + str(time.time() - start))","882950c3":"import pickle\n#exporting image encodings\n# with open('encoding_train.pickle', 'wb') as handle:\n#     pickle.dump(encoding_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n    \n    \n# with open('encoding_test.pickle', 'wb') as handle:\n#     pickle.dump(encoding_test, handle, protocol=pickle.HIGHEST_PROTOCOL)    ","07082e82":"#importing image encodings\nwith open('\/kaggle\/input\/image-encodings\/image_encoding\/image_encoding\/encoding_test.pickle', 'rb') as handle:\n    encoding_test = pickle.load(handle)\n    \nwith open('\/kaggle\/input\/image-encodings\/image_encoding\/image_encoding\/encoding_train.pickle', 'rb') as handle:\n    encoding_train = pickle.load(handle)    ","6511140b":"encoding_train = dict(sorted(encoding_train.items()))\nencoding_test = dict(sorted(encoding_test.items()))\n\ntrain_img = sorted(train_img)\ntest_img = sorted(test_img)","ef49875a":"type(encoding_test), type(encoding_train)","02483f20":"train_features = encoding_train\ntest_features = encoding_test\n\nprint(\"Train image encodings: \" + str(len(train_features)))\nprint(\"Test image encodings: \" + str(len(test_features)))","6be35d97":"#list of all training captions\nall_train_captions = []\nfor key, val in train_descriptions.items():\n    for caption in val:\n        all_train_captions.append(caption)\nlen(all_train_captions)","b949879d":"#onsider only words which occur atleast 10 times\nvocabulary = vocab\nthreshold = 10\nword_counts = {}\nfor cap in all_train_captions:\n    for word in cap.split(' '):\n        word_counts[word] = word_counts.get(word, 0) + 1\n                \nvocab = [word for word in word_counts if word_counts[word] >= threshold]\nprint(\"Unique words: \" + str(len(word_counts)))\nprint(\"our Vocabulary: \" + str(len(vocab)))","564b018c":"#word mapping to integers\nixtoword = {}\nwordtoix = {}\n\nix = 1\nfor word in vocab:\n    wordtoix[word] = ix\n    ixtoword[ix] = word\n    ix += 1","f1d91afd":"vocab_size = len(ixtoword) + 1  #1 for appended zeros\nvocab_size","843a7b58":"#find the maximum length of a description in a dataset\n\n#max_length of val_captin is 31, so considering 34\nmax_length = max(len(des.split()) for des in all_train_captions)\nmax_length","d1252727":"len(train_descriptions)","8f0168af":"#since there are almost 30000 descriptions to process we will use datagenerator\nX1, X2, y = list(), list(), list()\nfor key, des_list in train_descriptions.items():\n    pic = train_features[key + '.jpg']\n    for cap in des_list:\n        seq = [wordtoix[word] for word in cap.split(' ') if word in wordtoix]\n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen = max_length)[0]\n            out_seq = to_categorical([out_seq], num_classes = vocab_size)[0]\n            #store\n            X1.append(pic)\n            X2.append(in_seq)\n            y.append(out_seq)\n\nX1 = np.array(X1)\nX2 = np.array(X2)\ny = np.array(y)\n","80bca673":"print(X1.shape) #image encodings\nprint(X2.shape) #image's caption with padding, maxlength\nprint(y.shape)  #categorical classes == vocab size","3498efde":"#each line from glove txt will be in form\n#word (at 0th index) and that particular word's 200d embedding(in the next indexes) \n\n#load glove vectors for embedding layer\nembeddings_index = {}\nglove = open('\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt', 'r', encoding = 'utf-8').read()\nfor line in glove.split(\"\\n\"):\n    values = line.split(\" \")\n    word = values[0]\n    indices = np.asarray(values[1: ], dtype = 'float32')\n    embeddings_index[word] = indices\nprint('Total word vectors: ' + str(len(embeddings_index)))","c8868244":"embeddings_index['plastic'].shape","9a2ebeba":"emb_dim = 200\nemb_matrix = np.zeros((vocab_size, emb_dim))\nfor word, i in wordtoix.items():\n    emb_vec = embeddings_index.get(word)\n    if emb_vec is not None:\n        emb_matrix[i] = emb_vec\nemb_matrix.shape","03b18ed8":"# test_descriptions","0ce66dca":"new_descriptions_test = []\nnew_descriptions_train = []\n\nfor key, values in test_descriptions.items():\n    v = [i.split()[1:-1] for i in values]\n    new_descriptions_test.append(v)\n    \nfor key, values in train_descriptions.items():\n    v = [i.split()[1:-1] for i in values]\n    new_descriptions_train.append(v)","ef67cd4d":"# new_descriptions_train","85767b8c":"# new_descriptions_test","2b32d46f":"def greedy_search(pic):\n    start = 'startseq'\n    for i in range(max_length):\n        seq = [wordtoix[word] for word in start.split() if word in wordtoix]\n#         print('for i',i)\n#         print('seq1',seq)\n        seq = pad_sequences([seq], maxlen = max_length)\n        yhat = model.predict([pic, seq])\n        yhat = np.argmax(yhat)\n#         print('yhat',yhat)\n        word = ixtoword[yhat]\n        start += ' ' + word\n        if word == 'endseq':\n            break\n    final = start.split()\n    final = final[1:-1]\n    final = ' '.join(final)\n#     print(seq)\n    return final  #returns predicted caption","683c7366":"# model = load_model('\/kaggle\/input\/image-encodings\/imgcap_model_7epochs.h5')\n# model.summary()","b6075374":"# define the model\nip1 = Input(shape = (4096, )) #for image,(input as image embedding)\nfe1 = Dropout(0.2)(ip1)\nfe2 = Dense(256, activation = 'relu')(fe1)\n\nip2 = Input(shape = (max_length, ))#for sequences (padded. max length)\nse1 = Embedding(vocab_size, emb_dim, mask_zero = True)(ip2)\nse2 = Dropout(0.2)(se1)\nse3 = LSTM(256)(se2)\n\ndecoder1 = add([fe2, se3])\ndecoder2 = Dense(256, activation = 'relu')(decoder1)\noutputs = Dense(vocab_size, activation = 'softmax')(decoder2)\nmodel = Model(inputs = [ip1, ip2], outputs = outputs)\nmodel.summary()\n\nmodel.layers[2].set_weights([emb_matrix])\nmodel.layers[2].trainable = False\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n# plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True)\n\n","c53f9d63":"bleu_score = []","e7e3ed81":"from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nchencherry = SmoothingFunction().method1\nsmoothie = SmoothingFunction().method4\n\n\ndef finding_bleu_score_train():\n    global hypothesis_test,hypothesis_train\n    hypothesis_test,hypothesis_train = [], []\n\n    for key, values in tqdm(encoding_train.items()):\n        temp = greedy_search(values.reshape(1, 4096))\n        temp = temp.split()\n#         hypothesis_test.append(temp)\n        hypothesis_train.append(temp)\n\n    score = corpus_bleu(new_descriptions_train, hypothesis_train,\n                        weights=(0.5,0.5), smoothing_function=chencherry)\n    print(score)\n    return score\n\ndef finding_bleu_score_test():\n    global hypothesis_test,hypothesis_train\n    hypothesis_test,hypothesis_train = [], []\n\n    for key, values in tqdm(encoding_test.items()):\n        temp = greedy_search(values.reshape(1, 4096))\n        temp = temp.split()\n#         hypothesis_test.append(temp)\n        hypothesis_test.append(temp)\n\n    score = corpus_bleu(new_descriptions_test, hypothesis_test,\n                        weights=(0.5, 0.5), smoothing_function=chencherry)\n    print(score)\n    return score","dc7be426":"epoch = 5\nfor i in range(1,epoch+1):\n    model.fit([X1, X2], y, epochs = 1, batch_size = 256)\n#     bleu_score_test.append(finding_bleu_score_test())","ef533479":"model.save('imgcap_model_7epochs.h5')","183bcae9":"#train bleu_score\nbleu_score.append(finding_bleu_score_train())","51a44f9a":"#test bleu_score\nbleu_score.append(finding_bleu_score_test())","1615d80d":"#Visualizing train data\n\nplt.figure(figsize=(16, 12))\n\nfor num, val in enumerate(train_img[:6]):\n    encoded = encode(val)\n    pred_cap = greedy_search(encoded.reshape(1,4096))\n    img = plt.imread(val)\n    plt.subplot(5,2,num+1)\n    plt.title(pred_cap)\n    plt.axis('off')\n    plt.imshow(img)\n","dd96af4d":"#Visualizing test data\n\nplt.figure(figsize=(16, 12))\n\nfor num, val in enumerate(test_img[10:16]):\n    encoded = encode(val)\n    pred_cap = greedy_search(encoded.reshape(1,4096))\n    img = plt.imread(val)\n    plt.subplot(5,2,num+1)\n    plt.title(pred_cap)\n    plt.axis('off')\n    plt.imshow(img)","d5061d5b":"#Visualizing random raw images\n\npath = '..\/input\/testing-captions3\/testing_captioning\/'\nimgs = os.listdir(path)\n\nplt.figure(figsize=(16, 12))\nfor num, x in enumerate(imgs):\n    encoded = encode(path+imgs[num])\n    pred_cap = greedy_search(encoded.reshape(1,4096))\n    img = plt.imread(path+x)\n    plt.subplot(5,2,num+1)\n    plt.title(pred_cap)\n    plt.axis('off')\n    plt.imshow(img)","301c5189":"####  VGG -> 4096 dim., 138 million paramemters, input img_size=(224, 224)\n####  InceptionV3 -> 2048dim., 23 million parameters, input img_size=(299,299)","b6b0daab":"BLEU-2 (2-grams) score observation.\n\nVGG16 model, for 5 epochs\n200d test- 0.28, train- 0.39\n50d  test- 0.29, train- 0.41\n100d test- 0.30 , train- 0.43\n\nInceptionV3 for 5 epochs\n200d test-0.17, train-0.17\n50d  test-0.17, train- 0.17\n100d test- 0.14 , train- 0.14\n\nBleu score drops after 5 epochs"}}