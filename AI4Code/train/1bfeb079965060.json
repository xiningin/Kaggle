{"cell_type":{"6c27766b":"code","5cc79b5a":"code","64d8852a":"code","2a3e2163":"code","bbf8ae43":"code","c884ee6f":"code","850ada34":"code","0b3604d5":"code","855e3adc":"code","0d501dd2":"code","658e56f5":"code","74eab1e9":"code","6cb5ecf6":"code","14349929":"code","f80f6bda":"code","995de0d7":"code","c480fa19":"code","36fdc1c7":"code","8830118d":"code","c74053ff":"code","e152647e":"code","e89e7e89":"code","29d9f90d":"code","58dc516f":"code","f2c2529c":"code","03bba9ff":"code","e0884432":"code","77cf2a43":"code","5ebc03f6":"code","5c98a6b7":"code","78db1768":"code","4fa3d2c5":"markdown","704430f3":"markdown","eba57593":"markdown","03fb245b":"markdown","e9793625":"markdown","96a18a64":"markdown","58afd8ad":"markdown","8525dc6f":"markdown","7c19736b":"markdown","7a4e84f5":"markdown"},"source":{"6c27766b":"#Inspiration: https:\/\/www.kaggle.com\/iandiaz97\/image-classification","5cc79b5a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import Sequential\n\nfrom os import listdir\nfrom os.path import isfile, join\n\nimport cv2\nfrom tqdm.notebook import tqdm\nfrom scipy import ndimage","64d8852a":"path_train = '..\/input\/ustensils\/train_dl'\npath_couteau = '\/couteau_train\/'\npath_fourchettes = '\/fourchettes_train\/'\npath_spoon = '\/spoon_train\/'\npath_test = '..\/input\/ustensils\/test_dl'","2a3e2163":"class_label = {0:'couteau', 1:'fourchette', 2:'cuillere et autre'}","bbf8ae43":"paths = [path_couteau,path_fourchettes,path_spoon]\npaths","c884ee6f":"data = [] # contiendra les photos\nlabels = [] # contiendra le label correct associ\u00e9s aux photos\nIMG_SIZE = 224 # est la taille de la photo qui sera impos\u00e9e. Si la taille de la photo est \n# diff\u00e9rente, elle sera r\u00e9adapter avec le 'resize' du package cv2.\nshapes = [] # permet d'avoir une trace de la vrai taille des photos\n\naugmentation = 5 # Si on voudra utiliser des techniques de data augmentation de type rotation\n\n\nfor i, path in enumerate(paths):\n    img_urls = listdir(path_train+path)\n    print(f\"Importing {class_label[i]}\")\n    for img_name in tqdm(img_urls):\n        img = cv2.imread(path_train+path+img_name)\n        shapes.append(img.shape)\n#        print(img_name)\n        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        im = np.array(img)\n        data.append(im)\n        labels.append(i)\n        for u in np.linspace(-15,15,num=augmentation):\n            im = ndimage.rotate(img, u)\n            im = cv2.resize(im,(IMG_SIZE,IMG_SIZE))\n            data.append(im)\n            labels.append(i)\n#            plt.imshow(im)\n#            plt.show()\n        im = np.array(cv2.flip(img,1))\n        data.append(im)\n        labels.append(i)","850ada34":"shapes[:5]\n# Voici les tailles r\u00e9elles des 5 premi\u00e8res photos:","0b3604d5":"import random\nnumber = random.randint(0,len(data))\nplt.imshow(data[number])\nplt.show()\nprint(class_label[labels[number]])","855e3adc":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6,2.3))\nax1.imshow(data[0])\nplt.imshow(data[6])\nplt.show()\nprint(class_label[0])","0d501dd2":"fig, (ax1, ax2,ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(6,2.3))\nax1.imshow(data[0])\nax2.imshow(data[1])\nax3.imshow(data[2])\nax4.imshow(data[3])\nplt.imshow(data[4])\nplt.show()\nprint(class_label[0])","658e56f5":"data = np.array(data)\ndata.shape\n# Il y a 3 classes, les images sont au format 225x225 et on compte 620 photos \n# (320 photos et leurs flips)","74eab1e9":"labels = np.array(labels)\nlabels.shape\n# Il y a 620 classes cibles associ\u00e9es aux 620 photos","6cb5ecf6":"path_couteau = '\/couteau_test\/'\npath_fourchettes = '\/fourchettes_test\/'\npath_spoon = '\/spoon_test\/'\n\npaths = [path_couteau,path_fourchettes,path_spoon]\n\nx_test = []\ny_test = []\n\nfor i, path in enumerate(paths):\n    img_urls = listdir(path_test+path)\n    print(f\"Importing {class_label[i]}\")\n    for img_name in tqdm(img_urls):\n        img = cv2.imread(path_test+path+img_name)\n        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n        im = np.array(img)\n        x_test.append(im)\n        y_test.append(i)","14349929":"x_test = np.array(x_test)\nx_test.shape\ny_test = np.array(y_test)\ny_test.shape","f80f6bda":"np.random.seed(1)\ntf.random.set_seed(2)","995de0d7":"ini = tf.keras.initializers.RandomNormal(\n    mean=0.0, stddev=1e-2, seed=None)\n# RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n\nmodel = Sequential()\nmodel.add(Conv2D(32,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                 input_shape=(IMG_SIZE,IMG_SIZE,3)\n                )\n         )\n\nmodel.add(Conv2D(32,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                )\n         )\nmodel.add(MaxPool2D((2,2)))\n\nmodel.add(Conv2D(64,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini, \n                )\n         )\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64,\n                 (3,3),\n                 activation='relu',\n                 kernel_initializer = ini,\n                )\n         )\n\nmodel.add(MaxPool2D((2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64,activation='relu'))\n\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(6, activation= 'sigmoid'))\n\n\noptim = tf.keras.optimizers.Adam(\n    learning_rate=5e-5, beta_1=0.9, beta_2=0.999, epsilon=1e-07\n)\n\nmodel.compile(\n    optimizer=optim, loss='sparse_categorical_crossentropy', metrics=['accuracy']\n)\n\nmodel.summary()","c480fa19":"tf.keras.utils.plot_model(\n    model, to_file='model.png', show_shapes=True, show_dtype=False,\n    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=50\n)","36fdc1c7":"from sklearn.model_selection import train_test_split\nx_test_2, x_valid_2, y_test_2, y_valid_2 = train_test_split(x_test,y_test, test_size = 0.8, shuffle= True)","8830118d":"early_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=5, restore_best_weights=True)\n\n\nepochs = 100\nbatch_size = 64\n\nhistory = model.fit(\n    data,labels, batch_size=batch_size, epochs=epochs,\n    #callbacks=[early_stop], \n    validation_data = (x_test, y_test), shuffle=True)","c74053ff":"plt.figure(figsize= (10,5))\nplt.plot(range(len(history.history['loss'])),history.history['loss'], label= 'Loss')\nplt.plot(range(len(history.history['val_loss'])),history.history['val_loss'], label= 'Val_loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss vs Epoch')\nplt.legend()\nplt.grid()\nplt.show()","e152647e":"plt.figure(figsize= (10,5))\nplt.plot(range(len(history.history['accuracy'])),history.history['accuracy'], label= 'Accuracy')\nplt.plot(range(len(history.history['val_accuracy'])),history.history['val_accuracy'], label= 'Val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Acuracy vs Epoch')\nplt.legend()\nplt.grid()\nplt.show()\n\nmax_ac = np.max(history.history['val_accuracy'])\nprint(f'Max Accuracy was: {max_ac}')","e89e7e89":"num_samples = 10\nfor i in range(num_samples):\n    test_num = random.randint(0,len(x_valid))\n    pred = model.predict(np.array([x_valid[test_num]]))\n    \n    plt.imshow(x_valid[test_num])\n    plt.show()\n    print(f'Pr\u00e9diction: {class_label[np.argmax(pred)]} \\nVrai classe: {class_label[y_valid[test_num]]}')\n    print(f'Num\u00e9ro de test: {test_num}')","29d9f90d":"model.evaluate(x_test,y_test)","58dc516f":"num_samples = 10\nfor i in range(num_samples):\n    test_num = random.randint(0,len(test_data))\n    pred = model.predict(np.array([test_data[test_num]]))\n    \n    plt.imshow(test_data[test_num])\n    plt.show()\n    print(f'Pr\u00e9diction: {class_label[np.argmax(pred)]} \\nVrai classe: {class_label[test_labels[test_num]]}')\n    print(f'Numero de test: {test_num}')","f2c2529c":"from keras.applications.vgg16 import VGG16\nvgg_model = VGG16()\n# summarize the model\nvgg_model.summary()","03bba9ff":"import keras \nfrom keras.applications import VGG16\nimport numpy as np\nfrom keras import Input\nfrom keras.layers import SimpleRNN, Dense\n\nbase_model = VGG16(weights='imagenet',input_shape=(224, 224, 3),include_top=False)  # Do not include the ImageNet classifier at the top.","e0884432":"base_model.trainable = False","77cf2a43":"inputs = keras.Input(shape=(224, 224, 3))\n# We make sure that the base_model is running in inference mode here,\n# by passing `training=False`. This is important for fine-tuning, as you will\n# learn in a few paragraphs.\nx = base_model(inputs, training=False)\n# Convert features of shape `base_model.output_shape[1:]` to vectors\nx = keras.layers.GlobalAveragePooling2D()(x)\n# A Dense classifier with a single unit (binary classification)\noutputs = keras.layers.Dense(1)(x)\ntr_model = keras.Model(inputs, outputs)","5ebc03f6":"tr_model.compile(optimizer=keras.optimizers.Adam(),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","5c98a6b7":"tr_model.fit(data, labels, epochs=20, validation_data = (x_test, y_test), shuffle=True)","78db1768":"tr_model.evaluate(x_test,y_test)","4fa3d2c5":"Exemple de flip","704430f3":"Nos donn\u00e9es sont organis\u00e9s en fichiers qui contiennent les photos des diff\u00e9rents \u00e9l\u00e9ments de couverts de table. D'apr\u00e8s le nom du fichier, on peut aussi identifier si ces photos sont d\u00e9di\u00e9s \u00e0 l'entrainement ou au test. C'est ce que nous allons utiliser pour mettre en input du mod\u00e8le les photos.","eba57593":"Importation des donnees test","03fb245b":"## I. Pr\u00e9paration des donn\u00e9es","e9793625":"## III. Entrainement","96a18a64":"## II. Le mod\u00e8le","58afd8ad":"# Application de mod\u00e8les d'apprentissage profonds pour la pr\u00e9diction d'ustensiles de cuisines: cuill\u00e8res, couteaux et fourchettes","8525dc6f":"## IV. Modele de tranfert learning","7c19736b":"Visualisation d'une photo al\u00e9atoire","7a4e84f5":"Exemple de rotations"}}