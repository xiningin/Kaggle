{"cell_type":{"be35faa0":"code","b682867a":"code","7b74ad34":"code","4d48340d":"code","5d02e85c":"code","bbedfa6b":"code","903c58a7":"code","d2faa838":"code","4ce84ded":"code","83a4c786":"code","791df0da":"code","f22e550e":"code","6e7fb966":"code","5e0de6a1":"code","6c852fe2":"code","efe2dc38":"code","ccda2e52":"code","83f42ca8":"code","c33e19a7":"code","e7e450ac":"code","384b2d4b":"code","9827151d":"code","0ebba8b1":"code","ad70dd25":"markdown","1897678e":"markdown","01306f30":"markdown","da8218d3":"markdown","2ab05856":"markdown","c75419dc":"markdown","60c1e0d6":"markdown","00632c63":"markdown","6f664995":"markdown","cbb173ec":"markdown"},"source":{"be35faa0":"!pip install pytorch_lightning","b682867a":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport gc\nimport random\nimport sys\nimport time\nimport feather\nimport numpy as np\nimport pandas as pd\nimport logging\nfrom contextlib import contextmanager\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\nimport tensorflow as tf\nimport torch\nimport torch.optim as optim\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import datasets, models\n\nimport pytorch_lightning as pl","7b74ad34":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nkaeru_seed = 1337\nseed_everything(seed=kaeru_seed)\n\nDATA_DIR = '..\/input\/titanic\/'","4d48340d":"train = pd.read_csv(DATA_DIR  + 'train.csv')\ntest = pd.read_csv(DATA_DIR  + 'test.csv')\ntarget = train[\"Survived\"]\n\nn_train = len(train)\ntrain = pd.concat([train, test], sort=True)\n\nprint(train.shape)\n# train.head()","5d02e85c":"categorical_features = []\nfor col in tqdm(train.columns):\n    if train[col].dtype == 'object':\n        categorical_features.append(col)\nprint(len(categorical_features))","bbedfa6b":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = {}\nfor cat_col in categorical_features:\n    label_encoders[cat_col] = LabelEncoder()\n    # fill missing values with <Miising> tokens\n    train[cat_col] = train[cat_col].fillna('<Missing>')\n    train[cat_col] = label_encoders[cat_col].fit_transform(train[cat_col])","903c58a7":"cat_dims = [int(train[col].nunique()) for col in categorical_features]\nemb_dims = [(x, min(50, (x + 1) \/\/ 2)) for x in cat_dims]","d2faa838":"from sklearn.preprocessing import StandardScaler\n\nnumerical_features = list(set(train.columns) - set(categorical_features) - set(['Survived']))\nprint(len(numerical_features))\n    \ntrain[numerical_features] = train[numerical_features].fillna(-999)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\ntrain[numerical_features] = scaler.fit_transform(train[numerical_features]) ","4ce84ded":"data = train[:n_train]\ntest = train[n_train:]\ndel test['Survived']\ngc.collect()\n\nprint(data.shape, test.shape)","83a4c786":"from sklearn.model_selection import train_test_split\n\ntrain_idx, valid_idx = train_test_split(list(data.index),test_size=0.2, random_state=kaeru_seed, stratify=target)\noutput_feature = \"Survived\"","791df0da":"class TitanicDataset(Dataset):\n  def __init__(self, data, cat_cols=None, output_col=None):\n\n    self.n = data.shape[0]\n\n    if output_col:\n      self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n    else:\n      self.y =  np.zeros((self.n, 1))\n\n    self.cat_cols = cat_cols if cat_cols else []\n    self.cont_cols = [col for col in data.columns\n                      if col not in self.cat_cols + [output_col]]\n\n    if self.cont_cols:\n      self.cont_X = data[self.cont_cols].astype(np.float32).values\n    else:\n      self.cont_X = np.zeros((self.n, 1))\n\n    if self.cat_cols:\n      self.cat_X = data[cat_cols].astype(np.int64).values\n    else:\n      self.cat_X =  np.zeros((self.n, 1))\n\n  def __len__(self):\n\n    return self.n\n\n  def __getitem__(self, idx):\n\n    return [self.y[idx], self.cont_X[idx], self.cat_X[idx]]","f22e550e":"class RMSELoss(torch.nn.Module):\n    def __init__(self):\n        super(RMSELoss,self).__init__()\n\n    def forward(self,x,y):\n        criterion = nn.MSELoss()\n        loss = torch.sqrt(criterion(x, y))\n        return loss\n      \ncriterion = RMSELoss()","6e7fb966":"class CoolSystem(pl.LightningModule):\n\n    def __init__(self, emb_dims, no_of_cont, lin_layer_sizes,\n               output_size, emb_dropout, lin_layer_dropouts):\n        super(CoolSystem, self).__init__()\n        \n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y)\n                                     for x, y in emb_dims])\n\n        no_of_embs = sum([y for x, y in emb_dims])\n        self.no_of_embs = no_of_embs\n        self.no_of_cont = no_of_cont\n\n        # Linear Layers\n        first_lin_layer = nn.Linear(self.no_of_embs + self.no_of_cont,\n                                lin_layer_sizes[0])\n\n        self.lin_layers =\\\n            nn.ModuleList([first_lin_layer] +\\\n            [nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n            for i in range(len(lin_layer_sizes) - 1)])\n    \n        for lin_layer in self.lin_layers:\n            nn.init.kaiming_normal_(lin_layer.weight.data)\n\n        # Output Layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1],\n                                  output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n\n        # Batch Norm Layers\n        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size)\n                                    for size in lin_layer_sizes])\n\n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.droput_layers = nn.ModuleList([nn.Dropout(size)\n                                  for size in lin_layer_dropouts])\n\n    def forward(self, cont_data, cat_data):\n      \n        if self.no_of_embs != 0:\n              x = [emb_layer(cat_data[:, i])\n                  for i,emb_layer in enumerate(self.emb_layers)]\n              x = torch.cat(x, 1)\n              x = self.emb_dropout_layer(x)\n\n        if self.no_of_cont != 0:\n            normalized_cont_data = self.first_bn_layer(cont_data)\n\n        if self.no_of_embs != 0:\n            x = torch.cat([x, normalized_cont_data], 1) \n        else:\n            x = normalized_cont_data\n\n        for lin_layer, dropout_layer, bn_layer in\\\n            zip(self.lin_layers, self.droput_layers, self.bn_layers):\n      \n            x = F.relu(lin_layer(x))\n            x = bn_layer(x)\n            x = dropout_layer(x)\n\n        x = self.output_layer(x)\n\n        return x\n\n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        y, cont_x, cat_x = batch\n        y_hat = self.forward(cont_x, cat_x)\n        return {'loss': criterion(y_hat, y)}\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        y, cont_x, cat_x = batch\n        y_hat = self.forward(cont_x, cat_x)\n        return {'val_loss': criterion(y_hat, y)}\n\n    def validation_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        return {'avg_val_loss': avg_loss}\n\n    def configure_optimizers(self):\n        # REQUIRED\n        # can return multiple optimizers and learning_rate schedulers\n        return torch.optim.Adam(self.parameters(), lr=0.02)\n\n    @pl.data_loader\n    def tng_dataloader(self):\n        # REQUIRED\n        return DataLoader(TitanicDataset(data=data.loc[train_idx], cat_cols=categorical_features, output_col=output_feature), batch_size=256)\n\n    @pl.data_loader\n    def val_dataloader(self):\n        # OPTIONAL\n        return DataLoader(TitanicDataset(data=data.loc[valid_idx], cat_cols=categorical_features, output_col=output_feature), batch_size=256)\n\n    @pl.data_loader\n    def test_dataloader(self):\n        # OPTIONAL\n        return DataLoader(TitanicDataset(data=test, cat_cols=categorical_features, output_col=None), batch_size=256)","5e0de6a1":"from pytorch_lightning.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(\n    filepath='..\/input\/weights.ckpt',\n    save_best_only=True,\n    verbose=True,\n    monitor='avg_val_loss',\n    mode='min'\n)","6c852fe2":"from pytorch_lightning import Trainer\nfrom test_tube import Experiment\n\nmodel = CoolSystem(emb_dims, no_of_cont=len(numerical_features), \n                          lin_layer_sizes=[50, 100],\n                          output_size=1, emb_dropout=0.04,\n                          lin_layer_dropouts=[0.001,0.01])\n\nexp = Experiment(save_dir='..\/input\/')\n\n# most basic trainer, uses good defaults\ntrainer = Trainer(experiment=exp, max_nb_epochs=5, train_percent_check=0.1, checkpoint_callback=checkpoint_callback) \ntrainer.fit(model)","efe2dc38":"# !tensorboard --logdir ..\/input\/","ccda2e52":"!ls ..\/input\/weights.ckpt\/","83f42ca8":"!ls ..\/input\/default\/version_0\/","c33e19a7":"res = pd.read_csv('..\/input\/default\/version_0\/metrics.csv')\nres","e7e450ac":"device = 'cpu'\ncheckpoint = torch.load('..\/input\/weights.ckpt\/_ckpt_epoch_5.ckpt')\n\ndef predict(fold, model, tk0, TEST_BATCH_SZ):    \n    model.load_state_dict(checkpoint['state_dict'])\n    for param in model.parameters():\n        param.requires_grad = False    \n    model.eval()\n    for i, (y, cont_x, cat_x) in enumerate(tk0):\n        with torch.no_grad():\n            cat_x = cat_x.to(device)\n            cont_x = cont_x.to(device)\n            preds = model(cont_x, cat_x)\n            test_preds[i * TEST_BATCH_SZ:(i + 1) * TEST_BATCH_SZ] = preds.detach().cpu().squeeze().numpy().ravel().reshape(-1, 1)               \n    return test_preds","384b2d4b":"test_dataset = TitanicDataset(data=test, cat_cols=categorical_features, output_col=None)\nTEST_BATCH_SZ = 256\nall_test_preds = []\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SZ, shuffle=False, num_workers=4, drop_last=True)\ntest_preds = np.zeros((len(test_dataset), 1))\ntk0 = tqdm(test_data_loader)\ntest_preds = predict(0, model, tk0, TEST_BATCH_SZ)\nall_test_preds.append(test_preds)\n\ntest_preds = np.mean(all_test_preds, 0) \nprint(test_preds.shape)","9827151d":"# define threshold\ndelta = 0.5\nsub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\nsub['Survived'] = (test_preds > delta).astype(int)\nsub.to_csv('submission.csv', index=False)","0ebba8b1":"sub.head()","ad70dd25":"### Preprocessing","1897678e":"### Save model settings","01306f30":"## This notebook show you how to use PyTorch lightening on Tabular dataset with Titanic\n\n#### references\n\n+ [Official GitHub](https:\/\/github.com\/williamFalcon\/pytorch-lightning)\n\n+ [Docs](https:\/\/williamfalcon.github.io\/pytorch-lightning\/Trainer\/Checkpointing\/#model-saving)\n\n+ [My last post showing how to use pytorch NN with tatanic](https:\/\/www.kaggle.com\/kaerunantoka\/titanic-pytorch-nn-tutorial)\n\n+ [yashuseth's blog post : 'A Neural Network in PyTorch for Tabular Data with Categorical Embeddings'](https:\/\/yashuseth.blog\/2018\/07\/22\/pytorch-neural-network-for-tabular-data-with-categorical-embeddings\/)","da8218d3":"### Define LightningModule which contains model, dataloader, and so on...","2ab05856":"### Training","c75419dc":"### Library","60c1e0d6":"### Check outputs","00632c63":"### First install pytorch_lightning","6f664995":"### Thank you for Reading ;)","cbb173ec":"### Predict"}}