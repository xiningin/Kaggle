{"cell_type":{"b482714a":"code","b0fe1d52":"code","2f70481f":"code","1ae58e6d":"code","a2ef3ecd":"code","94ba9844":"code","5f13b4bf":"code","528afd20":"code","86657fa4":"code","af9368d3":"code","3c1f9301":"code","44d0f7b2":"code","06391d11":"code","7bb75cfe":"code","7425b57d":"code","4e31619f":"code","a9d5db6c":"code","193dd7eb":"code","7d62c813":"code","aeeea0ca":"code","465979b0":"code","ede07952":"code","395e6c78":"code","c7f25d23":"code","a90f9d9a":"code","8bc3e9a9":"code","25c8b7de":"code","0fe88536":"code","0aa2487c":"code","26af57d7":"code","a6ada3cb":"code","d88850ad":"code","3f3d3f8b":"code","b2c94d09":"code","c87042eb":"code","c86770e2":"code","bcbb5e74":"code","b9ba057d":"code","03871aae":"code","d3badef3":"code","89446f10":"code","e6885145":"code","3a043d44":"code","a77564fe":"code","4ad967fd":"code","f3dc915c":"code","8efc3dc2":"code","44998f4c":"code","1dfde8d2":"code","ee7f0527":"code","365a91a5":"code","e3844993":"code","c4f48e85":"code","2ab2d2b3":"code","6be2ebc2":"code","9b59d681":"code","299351c5":"code","40049259":"code","aaead3c8":"code","83f47068":"code","cc849ea3":"code","e1b06fd6":"code","9d4be555":"code","193366ff":"code","d3f7ca88":"code","a42b5f6b":"code","d589635c":"code","b0a55719":"code","d3bca122":"code","176f774a":"code","fa9fd290":"code","6251230f":"code","7f3b9b29":"code","33986556":"code","719a77aa":"code","2962fdb8":"code","a6dfabfb":"code","6d3845b1":"code","2a0e7098":"code","ab25214b":"code","462e245e":"code","1e73dd2f":"code","feef4242":"code","bdb366df":"code","c32cc7fe":"code","5ba74c77":"code","e19a993d":"code","ee3f1fe4":"code","86daffbf":"code","d5711a53":"code","c1044c6f":"code","59151784":"code","f379b418":"code","2c5f691e":"code","a76a72f5":"code","d2a36454":"code","879648d4":"code","74012a2b":"code","e4c2659e":"code","c6ad3ba9":"code","a0479231":"code","61a06629":"code","2fe07dfe":"code","ed6277df":"code","18837a6a":"code","60a1e9cf":"code","0b1ae6fd":"code","484bd694":"code","cbdd3cf0":"code","d21b352b":"code","ffdb67f4":"code","1c2744b2":"code","633e78ce":"code","13c36723":"code","5201e3f6":"code","fa2a227f":"code","7854e530":"code","e292e6b3":"code","3166660d":"code","54803878":"code","8faa3964":"code","de71c50d":"code","a67a835f":"code","a17fc626":"code","b72f928a":"code","6f21f8ea":"code","c328804c":"code","bfe3e0da":"code","5316d7e1":"code","8cf62a03":"code","4b6ce2be":"code","0fbead3d":"code","43104eaa":"code","80cf97c7":"code","1d2d3c76":"code","f2fea465":"code","bbff0909":"code","a4f23296":"code","3c9f4d3f":"code","76e57e81":"code","1b7899ce":"code","e04cbdbc":"code","624fa403":"code","6bf579e0":"code","ff80e173":"code","8b60ba8c":"code","33f70e68":"code","3b07fd36":"code","1565b32b":"code","98e09150":"code","a632c36f":"code","7ed8f779":"code","7a8a4de7":"code","ebfcadb5":"code","3ae207ae":"code","ad3efac1":"code","b3181a07":"code","7ee5c3ac":"code","1636c821":"code","9de76a1a":"code","fa17596f":"code","c64fd5b0":"code","cc597cd1":"code","2ed43ade":"code","cb90eb56":"code","47cb2006":"code","645ed18d":"code","43fd1994":"code","8fdefbe8":"code","1a216039":"code","aba3504c":"code","5a72a6af":"code","e86a4afc":"code","8a56319c":"code","0f26fa67":"code","c4859c64":"code","f1935123":"code","1870917f":"code","20b44a6e":"code","4825f23c":"code","9fdc0372":"code","7a79864f":"code","dcaa82f6":"code","939e4cca":"code","92f8a458":"code","27c00458":"code","e29d2ba0":"code","5b75568c":"code","25436429":"code","49c98731":"code","87024927":"code","d525f88d":"code","f175ac90":"code","adde00fa":"code","41996272":"code","3bd8e679":"code","7a7059cc":"code","c534cd8c":"code","0e226856":"code","112fe78b":"code","8fe01487":"code","cc8c475e":"markdown","95028478":"markdown","2752e15a":"markdown","41463c79":"markdown","a5976e41":"markdown","df6e196a":"markdown","2c20e879":"markdown","57c17e01":"markdown","317f14ac":"markdown","84cdc92d":"markdown","8a09fa1b":"markdown","c56a2d21":"markdown","1e3da25e":"markdown","59514c0f":"markdown","1ce70b36":"markdown","674fdd73":"markdown","84cffa23":"markdown","fd194271":"markdown","fca9ecdd":"markdown","fc37eb5b":"markdown","ca04e783":"markdown","49da6c60":"markdown","a8278410":"markdown","67c63f54":"markdown","4c96ff7c":"markdown","013c28da":"markdown","5cdb1e0c":"markdown","d636d4ed":"markdown","6bb63738":"markdown","1b7b9d9a":"markdown","a4a06dc3":"markdown","f6ca6a13":"markdown","cf358a87":"markdown","987ee0fb":"markdown","28769c3c":"markdown","bf0d3757":"markdown","7acf2e5e":"markdown","4cd47832":"markdown","db289da7":"markdown","96359448":"markdown","c835ee6b":"markdown"},"source":{"b482714a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport warnings\nwarnings.simplefilter('ignore')\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nimport matplotlib as p\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport copy\nimport math\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, confusion_matrix","b0fe1d52":"pharma_data_original = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/pharma_data\/Training_set_advc.csv')\npharma_data_original.head()","2f70481f":"test_new = pd.read_csv('https:\/\/raw.githubusercontent.com\/dphi-official\/Datasets\/master\/pharma_data\/Testing_set_advc.csv')\ntest_new.head()","1ae58e6d":"pharma_data_original.isna().sum()","a2ef3ecd":"test_new.isna().sum()","94ba9844":"# Checking % of NA On train data - column wise\n# pd.set_option('display.max_rows', 500)\nNA_col_train = pd.DataFrame(pharma_data_original.isna().sum(), columns = ['NA_Count'])\nNA_col_train['% of NA'] = (NA_col_train.NA_Count\/len(pharma_data_original))*100\nNA_col_train.sort_values(by = ['% of NA'], ascending = False, na_position = 'first').head(10)","5f13b4bf":"# Extracting only the null value records to column 'A' and checking the counts and turned out to be the same\nnacheck = pharma_data_original[pharma_data_original['A'].isnull()]\nnacheck.isna().sum()","528afd20":"cols = pharma_data_original.columns\nnum_cols = pharma_data_original._get_numeric_data().columns\ncat_cols = list(set(cols) - set(num_cols))","86657fa4":"plt.figure(figsize = (14,10))\nsns.heatmap(pharma_data_original[num_cols].corr(), vmin=pharma_data_original[num_cols].values.min(), vmax=1, \n            annot=True, annot_kws={\"size\":10}, square = False)\nplt.show()","af9368d3":"print('Train - Unique values before cleaning: ', pharma_data_original['Treated_with_drugs'].nunique())\npharma_data_original['Treated_with_drugs'] = pharma_data_original['Treated_with_drugs'].str.strip()\npharma_data_original['Treated_with_drugs'] = pharma_data_original['Treated_with_drugs'].str.upper()\nprint('Train - Unique values after cleaning: ', pharma_data_original['Treated_with_drugs'].nunique())","3c1f9301":"print('Test - Unique values before cleaning: ', test_new['Treated_with_drugs'].nunique())\ntest_new['Treated_with_drugs'] = test_new['Treated_with_drugs'].str.strip()\ntest_new['Treated_with_drugs'] = test_new['Treated_with_drugs'].str.upper()\ntest_new['Treated_with_drugs'].nunique()\nprint('Test - Unique values after cleaning: ', test_new['Treated_with_drugs'].nunique())","44d0f7b2":"pharma_data_original['Patient_Smoker'].unique()","06391d11":"print('Train - Unique values before cleaning: ', pharma_data_original['Patient_Smoker'].unique())\npharma_data_original['Patient_Smoker'] = pharma_data_original['Patient_Smoker'].str.strip()\npharma_data_original['Patient_Smoker'] = pharma_data_original['Patient_Smoker'].str.upper()\npharma_data_original['Patient_Smoker'] = pharma_data_original['Patient_Smoker'].replace({'YESS': 'YES'})\nprint('Train - Unique values after cleaning: ', pharma_data_original['Patient_Smoker'].unique())","7bb75cfe":"test_new['Patient_Smoker'].unique()","7425b57d":"print('Test - Unique values before cleaning: ', pharma_data_original['Patient_Smoker'].unique())\ntest_new['Patient_Smoker'] = test_new['Patient_Smoker'].str.strip()\ntest_new['Patient_Smoker'] = test_new['Patient_Smoker'].str.upper()\ntest_new['Patient_Smoker'] = test_new['Patient_Smoker'].replace({'YESS': 'YES'})\ntest_new['Patient_Smoker'].unique()\nprint('Test - Unique values after cleaning: ', test_new['Patient_Smoker'].unique())","4e31619f":"pharma_data_original['New_ID'] = pharma_data_original['ID_Patient_Care_Situation'].groupby(pharma_data_original['Patient_ID']).transform('count')\npharma_data_original.head()","a9d5db6c":"test_new['New_ID'] = test_new['ID_Patient_Care_Situation'].groupby(test_new['Patient_ID']).transform('count')\ntest_new.head()","193dd7eb":"for i in pharma_data_original.columns:\n    if pharma_data_original[i].nunique() == 1:\n        print('With only 1 unique value: ', i)\n    if pharma_data_original[i].nunique() == pharma_data_original.shape[0]:\n        print('With all unique value: ', i)","7d62c813":"for i in test_new.columns:\n    if test_new[i].nunique() == 1:\n        print('With only 1 unique value: ', i)\n    if test_new[i].nunique() == test_new.shape[0]:\n        print('With all unique value: ', i)","aeeea0ca":"print('Train shape: ', pharma_data_original.shape)\nprint('Test shape: ', test_new.shape)","465979b0":"pharma_data_original = pharma_data_original.drop_duplicates()\ntest_new = test_new.drop_duplicates()\nprint('Train shape: ', pharma_data_original.shape)\nprint('Test shape: ', test_new.shape)","ede07952":"sns.catplot('Patient_Age', data= pharma_data_original, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = pharma_data_original['Patient_Age'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Patient_Age for train', fontsize = 20, color = 'black')\nplt.show()","395e6c78":"sns.catplot('Patient_Age', data= test_new, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test_new['Patient_Age'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Patient_Age for test', fontsize = 20, color = 'black')\nplt.show()","c7f25d23":"fig, (ax1,ax2) = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n\nlabel1 = ['Patient_Age']\n\n# Box plot for Patient_Age for train\nbplot1 = ax1.boxplot(pharma_data_original['Patient_Age'],\n                     vert=True,  # vertical box alignment\n                     patch_artist=True,  # fill with color\n                     labels = label1)  # will be used to label x-ticks\nax1.set_title('Box plot for Patient_Age for train')\n\n# Box plot for Patient_Age for test\nbplot2 = ax2.boxplot(test_new['Patient_Age'],\n                     vert=True,  # vertical box alignment\n                     patch_artist=True,  # fill with color\n                     labels = label1)  # will be used to label x-ticks\nax1.set_title('Box plot for Patient_Age for train')","a90f9d9a":"pharma_data_original = pharma_data_original[pharma_data_original['Patient_Age']<100]","8bc3e9a9":"fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(9,8))\n\nlabel1 = ['Patient_Age']\n\n# Box plot for Patient_Age for train\nbplot1 = ax1.boxplot(pharma_data_original['Patient_Age'],\n                     vert=True,  # vertical box alignment\n                     patch_artist=True,  # fill with color\n                     labels = label1)  # will be used to label x-ticks\nax1.set_title('Box plot for Patient_Age for train')","25c8b7de":"bin_labels = [1,2,3,4,5,6,7,8,9,10]\npharma_data_original['Age_band'] = pd.qcut(pharma_data_original['Patient_Age'], q=10, labels = bin_labels)\ntest_new['Age_band'] = pd.qcut(test_new['Patient_Age'], q=10, labels = bin_labels)","0fe88536":"sns.catplot('Age_band', data= pharma_data_original, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = pharma_data_original['Age_band'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Age_band for train', fontsize = 20, color = 'black')\nplt.show()","0aa2487c":"sns.catplot('Age_band', data= test_new, kind='count', alpha=0.7, height=4, aspect= 6)\n\n# Get current axis on current figure\nax = plt.gca()\n\n# Max value to be set\ny_max = test_new['Age_band'].value_counts().max() \n\n# Iterate through the list of axes' patches\nfor p in ax.patches:\n    ax.text(p.get_x() + p.get_width()\/5., p.get_height(),'%d' % int(p.get_height()),\n            fontsize=13, color='blue', ha='center', va='bottom')\nplt.title('Frequency plot of Age_band for test', fontsize = 20, color = 'black')\nplt.show()","26af57d7":"# Imputing 'Treated_with_drugs' with mode\n\ndef impute_with_mode(x):\n    max_x = x.value_counts()\n    mode = max_x[max_x == max_x.max()].index[0]\n    x[x.isna()] = mode\n    return x\n\npharma_data_original['Treated_with_drugs'] = pharma_data_original[['Treated_with_drugs']].apply(lambda x: impute_with_mode(x))","a6ada3cb":"pharma_data = pharma_data_original.copy()\ntest = test_new.copy()","d88850ad":"from fancyimpute import IterativeImputer\n\n# Initialize IterativeImputer\nmice_imputer = IterativeImputer()\n\n# Impute using fit_tranform\npharma_data[['A','B','C','D','E','F','Z']] = mice_imputer.fit_transform(pharma_data[['A','B','C','D','E','F','Z']])\npharma_data['Number_of_prev_cond'] = pharma_data.apply(lambda row: (row['A']+row['B']+row['C']+row['D']+row['D']+row['D']+row['Z'])\n                                                       if np.isnan(row['Number_of_prev_cond']) else row['Number_of_prev_cond'], axis=1)\npharma_data[['A','B','C','D','E','F','Z','Number_of_prev_cond']]=pharma_data[['A','B','C','D','E','F','Z','Number_of_prev_cond']].round()","3f3d3f8b":"pharma_data.isna().sum()","b2c94d09":"pharma_data2 = pharma_data_original[pharma_data_original['A'].notna()].copy()\ntest2 = test_new.copy()","c87042eb":"pharma_data2.isna().sum()","c86770e2":"print('Dataset1: ', pharma_data.shape)\nprint('Dataset2: ', pharma_data2.shape)","bcbb5e74":"for i in pharma_data_original.columns:\n    print(i)\n    print(pharma_data_original[i].nunique())","b9ba057d":"for i in test_new.columns:\n    print(i)\n    print(test_new[i].nunique())","03871aae":"col_train = pharma_data_original.columns\ncol_test = test_new.columns","d3badef3":"# Taking 32 as the threshold to include Treated_with_drugs column as well\n# Another way would be to use any threshold below that and add the colum later\nl1 = []\nfor i in col_train:\n    if pharma_data_original[i].nunique() <= 32:\n        l1.append(i)\n\nl1.remove('Survived_1_year')","89446f10":"l2 = []\nfor i in col_test:\n    if test_new[i].nunique() <= 32:\n        l2.append(i)","e6885145":"# Checking the columns in train and test are same or not\ndf = pd.DataFrame(l1, columns = ['train'])\ndf['test'] = pd.DataFrame(l2)\ndf","3a043d44":"# Dataset1\npharma_data[l1] = pharma_data[l1].apply(lambda x: x.astype('category'), axis=0)\ntest[l1] = test[l1].apply(lambda x: x.astype('category'), axis=0)\nprint('Dataset1: train dtypes:')\nprint('======================================')\nprint(pharma_data[l1].dtypes)\nprint('train shape: ', pharma_data.shape)\nprint('======================================')\nprint('test dtypes:')\nprint(test[l1].dtypes)\nprint('test shape: ', test.shape)","a77564fe":"# Dataset1\npharma_data2[l1] = pharma_data2[l1].apply(lambda x: x.astype('category'), axis=0)\ntest2[l1] = test2[l1].apply(lambda x: x.astype('category'), axis=0)\nprint('Dataset2: train dtypes:')\nprint('======================================')\nprint(pharma_data2[l1].dtypes)\nprint('train shape: ', pharma_data2.shape)\nprint('======================================')\nprint('test dtypes:')\nprint(test2[l1].dtypes)\nprint('test shape: ', test2.shape)","4ad967fd":"l1","f3dc915c":"cols_to_drop = ['Patient_ID', 'ID_Patient_Care_Situation', 'Patient_mental_condition', 'Patient_Age']","8efc3dc2":"pharma_data = pharma_data.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)\npharma_data2 = pharma_data2.drop(cols_to_drop, axis=1)\ntest2 = test2.drop(cols_to_drop, axis=1)\nprint('Dataset1: ', pharma_data.shape, ' ', test.shape)\nprint('Dataset2: ', pharma_data2.shape, ' ', test2.shape)","44998f4c":"# This code to be used if any columns to be dropped is in l1\ncat_columns = list(set(l1) - set(cols_to_drop))\ncat_columns","1dfde8d2":"# For MICE imputed datasets\nX = pharma_data.drop(['Survived_1_year'], axis = 1)\ny = pharma_data['Survived_1_year']\n\nX_num = len(X)\ncombined_dataset = pd.concat(objs=[X, test], axis=0)\ncombined_dataset = pd.get_dummies(combined_dataset, columns=cat_columns, drop_first=True)\nX = copy.copy(combined_dataset[:X_num])\ntest = copy.copy(combined_dataset[X_num:])","ee7f0527":"# For datasets with null value rows dropped\nX2 = pharma_data2.drop(['Survived_1_year'], axis = 1)\ny2 = pharma_data2['Survived_1_year']\n\nX_num = len(X2)\ncombined_dataset = pd.concat(objs=[X2, test2], axis=0)\ncombined_dataset = pd.get_dummies(combined_dataset, columns=cat_columns, drop_first=True)\nX2 = copy.copy(combined_dataset[:X_num])\ntest2 = copy.copy(combined_dataset[X_num:])","365a91a5":"pharma_data.to_csv(\"train_with_mice_imputation\", index=False)\ntest.to_csv(\"test_cleaned\", index=False)\npharma_data2.to_csv(\"train_with_null_rows_dropped\", index=False)\ntest2.to_csv(\"test_cleaned2\", index=False)","e3844993":"# For MICE imputed data\nX_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, random_state = 50)\n# For null rows dropped data\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X2,y2,test_size=0.3, random_state = 50)","c4f48e85":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]} # l1 Lasso l2 Ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=5)\nlogreg_cv.fit(X_train,y_train)\nlogreg_cv.best_params_","2ab2d2b3":"# Predict (train)\ny_train_pred = logreg_cv.predict(X_train)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: Logreg - train')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","6be2ebc2":"# Predict (val)\ny_val_pred = logreg_cv.predict(X_val)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: Logreg - val')\nprint('-----------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","9b59d681":"submission_lrcv = pd.DataFrame()\nsubmission_lrcv['Survived_1_year'] = logreg_cv.predict(test)\nsubmission_lrcv.head()","299351c5":"submission_lrcv.to_csv(\"submission_lrcv.csv\",index=False)","40049259":"logreg2=LogisticRegression()\nlogreg_cv2=GridSearchCV(logreg2,grid,cv=5)\nlogreg_cv2.fit(X_train2,y_train2)\nlogreg_cv2.best_params_","aaead3c8":"# Predict (train)\ny_train_pred = logreg_cv2.predict(X_train2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: Logreg - train')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","83f47068":"# Predict (val)\ny_val_pred = logreg_cv2.predict(X_val2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: Logreg - val')\nprint('-----------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","cc849ea3":"submission_lrcv2 = pd.DataFrame()\nsubmission_lrcv2['Survived_1_year'] = logreg_cv2.predict(test2)\nsubmission_lrcv2.head()","e1b06fd6":"submission_lrcv2.to_csv(\"submission_lrcv2.csv\",index=False)","9d4be555":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=50)\ndt.fit(X_train, y_train)","193366ff":"# Predict (train)\ny_train_pred = dt.predict(X_train)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: DT - train')\nprint('---------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","d3f7ca88":"# Predict (val)\ny_val_pred = dt.predict(X_val)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: DT - val')\nprint('-------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","a42b5f6b":"submission_dt = pd.DataFrame()\nsubmission_dt['Survived_1_year'] = dt.predict(test)\nsubmission_dt.head()","d589635c":"submission_dt.to_csv(\"submission_dt.csv\",index=False)","b0a55719":"dt2 = DecisionTreeClassifier(random_state=50)\ndt2.fit(X_train2, y_train2)","d3bca122":"# Predict (train)\ny_train_pred = dt2.predict(X_train2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: DT - train')\nprint('---------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","176f774a":"# Predict (val)\ny_val_pred = dt2.predict(X_val2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: DT - val')\nprint('-------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","fa9fd290":"submission_dt2 = pd.DataFrame()\nsubmission_dt2['Survived_1_year'] = dt2.predict(test2)\nsubmission_dt2.head()","6251230f":"submission_dt2.to_csv(\"submission_dt2.csv\",index=False)","7f3b9b29":"from sklearn.model_selection import GridSearchCV\nparams = {'max_leaf_nodes': list(range(2, 200,10)), \n          'min_samples_split': [3, 4, 5],\n          'max_depth': list(range(4,10)),\n          'criterion' :['gini', 'entropy']}","33986556":"%%time\ndtgs = DecisionTreeClassifier(random_state=50)\ncv_dt = GridSearchCV(estimator=dtgs, param_grid=params, scoring = 'f1', cv= 5)\ncv_dt.fit(X_train, y_train)\ncv_dt.best_params_","719a77aa":"# Predict (train)\ny_train_pred = cv_dt.predict(X_train)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv DT - train')\nprint('--------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","2962fdb8":"# Predict (val)\ny_val_pred = cv_dt.predict(X_val)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv DT - val')\nprint('------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","a6dfabfb":"submission_dtgscv = pd.DataFrame()\nsubmission_dtgscv['Survived_1_year'] = cv_dt.predict(test)\nsubmission_dtgscv.head()","6d3845b1":"submission_dtgscv.to_csv(\"submission_dtgscv.csv\",index=False)","2a0e7098":"%%time\ndtgs2 = DecisionTreeClassifier(random_state=50)\ncv_dt2 = GridSearchCV(estimator=dtgs2, param_grid=params, scoring = 'f1', cv= 5)\ncv_dt2.fit(X_train2, y_train2)\ncv_dt2.best_params_","ab25214b":"# Predict (train)\ny_train_pred = cv_dt2.predict(X_train2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: gscv DT - train')\nprint('--------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","462e245e":"# Predict (val)\ny_val_pred = cv_dt2.predict(X_val2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: gscv DT - val')\nprint('------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","1e73dd2f":"submission_dtgscv2 = pd.DataFrame()\nsubmission_dtgscv2['Survived_1_year'] = cv_dt2.predict(test2)\nsubmission_dtgscv2.head()","feef4242":"submission_dtgscv2.to_csv(\"submission_dtgscv2.csv\",index=False)","bdb366df":"from sklearn.ensemble import RandomForestClassifier\nrf0 = RandomForestClassifier()","c32cc7fe":"param_grid = { \n     'n_estimators': [55,60,65,70,80,100],\n     'max_features': ['auto', 'sqrt', 'log2'],\n     'max_depth' : [12,13,14,15,16],\n     'min_samples_leaf': [1,2,3,4],\n     'criterion' :['gini', 'entropy']\n}","5ba74c77":"%%time\ncv_rf = GridSearchCV(estimator=rf0, param_grid=param_grid, scoring='f1', cv= 5, verbose = 200)\ncv_rf.fit(X_train, y_train)","e19a993d":"cv_rf.best_params_","ee3f1fe4":"# Predict (train)\ny_train_pred = cv_rf.predict(X_train)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv RF - train')\nprint('--------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","86daffbf":"# Predict (val)\ny_val_pred = cv_rf.predict(X_val)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv RF - val')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","d5711a53":"submission_rfgscv = pd.DataFrame()\nsubmission_rfgscv['Survived_1_year'] = cv_rf.predict(test)\nsubmission_rfgscv.head()","c1044c6f":"submission_rfgscv.to_csv(\"submission_rfgscv.csv\",index=False)","59151784":"%%time\ncv_rf2 = GridSearchCV(estimator=rf0, param_grid=param_grid, scoring='f1', cv= 5, verbose = 200)\ncv_rf2.fit(X_train2, y_train2)","f379b418":"cv_rf2.best_params_ ","2c5f691e":"# Predict (train)\ny_train_pred = cv_rf2.predict(X_train2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: gscv RF - train')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","a76a72f5":"# Predict (val)\ny_val_pred = cv_rf2.predict(X_val2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: gscv RF - val')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","d2a36454":"submission_rfgscv2 = pd.DataFrame()\nsubmission_rfgscv2['Survived_1_year'] = cv_rf2.predict(test2)\nsubmission_rfgscv2.head()","879648d4":"submission_rfgscv2.to_csv(\"submission_rfgscv2.csv\",index=False)","74012a2b":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","e4c2659e":"param_test1 = {'learning_rate': [0.09, 0.1, 0.2, 0.3, 0.4, 0.5], 'max_depth': [4,5,6,8], 'min_child_weight':[1,2], 'n_estimators': [30,40,45,50], 'subsample':[0.6, 0.8]}","c6ad3ba9":"from sklearn.model_selection import GridSearchCV\nxgb1 = XGBClassifier(seed=27)\nxgb_gs1 = GridSearchCV(estimator = xgb1, param_grid = param_test1, scoring='f1',n_jobs=-1,cv=5)","a0479231":"%%time\neval_set = [(X_train, y_train)]\nxgb_gs1.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","61a06629":"xgb_gs1.best_params_","2fe07dfe":"# Predict (train)\ny_train_pred = xgb_gs1.predict(X_train)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv XGB - train')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","ed6277df":"# Predict (val)\ny_val_pred = xgb_gs1.predict(X_val)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv XGB - val')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","18837a6a":"submission_xgbgscv = pd.DataFrame()\nsubmission_xgbgscv['Survived_1_year'] = xgb_gs1.predict(test)\nsubmission_xgbgscv.head()","60a1e9cf":"submission_xgbgscv.to_csv(\"submission_xgbgscv.csv\",index=False)","0b1ae6fd":"xgb_clf = xgb.XGBClassifier(**xgb_gs1.best_params_, seed = 10)\n\n# Learn the model with training data\nxgb_clf.fit(X_train, y_train)\n\neval_set = [(X_train, y_train)]\nxgb_clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","484bd694":"# Plot the top 100 important features\nimp_feat_xgb=pd.Series(xgb_clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\nimp_feat_xgb[:100].plot(kind='bar',title='Top 50 Important features as per XGBoost', figsize=(20,10))\nplt.ylabel('Feature Importance Score')\nplt.subplots_adjust(bottom=0.25)\nplt.savefig('FeatureImportance.png')\nplt.show()","cbdd3cf0":"# Selecting features with importance based on the threshold of 0.01\nfrom sklearn.feature_selection import SelectFromModel\n\nthreshold = 0.01\n\nsfm = SelectFromModel(xgb_clf, threshold=threshold)\nsfm.fit(X_train, y_train)\nX_train_t = sfm.transform(X_train)","d21b352b":"X_val_t = sfm.transform(X_val)\ntest_t = sfm.transform(test)\nprint('Train shape: ', X_train_t.shape)\nprint('Val shape: ', X_val_t.shape)\nprint('Test shape: ', test_t.shape)","ffdb67f4":"xgb_clf2 = xgb.XGBClassifier(**xgb_gs1.best_params_, seed = 10)\n\n# Learn the model on transformed data\nxgb_clf2.fit(X_train_t, y_train)\n\neval_set = [(X_train_t, y_train)]\nxgb_clf2.fit(X_train_t, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","1c2744b2":"# Predict (train)\ny_train_pred = xgb_clf2.predict(X_train_t)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv-fea.imp. XGB - train')\nprint('-----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","633e78ce":"# Predict (val)\ny_val_pred = xgb_clf2.predict(X_val_t)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv-fea.imp. XGB - val')\nprint('----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","13c36723":"submission_xgbgscvfi = pd.DataFrame()\nsubmission_xgbgscvfi['Survived_1_year'] = xgb_clf2.predict(test_t)\nsubmission_xgbgscvfi.head()","5201e3f6":"submission_xgbgscvfi.to_csv(\"submission_xgbgscvfi.csv\",index=False)","fa2a227f":"threshold = 0.015\n\nsfm = SelectFromModel(xgb_clf, threshold=threshold)\nsfm.fit(X_train, y_train)\nX_train_t = sfm.transform(X_train)","7854e530":"X_val_t = sfm.transform(X_val)\ntest_t = sfm.transform(test)\nprint('Train shape: ', X_train_t.shape)\nprint('Val shape: ', X_val_t.shape)\nprint('Test shape: ', test_t.shape)","e292e6b3":"xgb_clf2 = xgb.XGBClassifier(**xgb_gs1.best_params_, seed = 10)\n\n# Learn the model on transformed data\nxgb_clf2.fit(X_train_t, y_train)\n\neval_set = [(X_train_t, y_train)]\nxgb_clf2.fit(X_train_t, y_train, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","3166660d":"# Predict (train)\ny_train_pred = xgb_clf2.predict(X_train_t)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv-fea.imp. XGB - train')\nprint('-----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","54803878":"# Predict (val)\ny_val_pred = xgb_clf2.predict(X_val_t)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv-fea.imp. XGB - val')\nprint('----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","8faa3964":"submission_xgbgscvfi015 = pd.DataFrame()\nsubmission_xgbgscvfi015['Survived_1_year'] = xgb_clf2.predict(test_t)\nsubmission_xgbgscvfi015.head()","de71c50d":"submission_xgbgscvfi015.to_csv(\"submission_xgbgscvfi015.csv\",index=False)","a67a835f":"xgb2 = XGBClassifier(seed=27)\nxgb_gs2 = GridSearchCV(estimator = xgb2, param_grid = param_test1, scoring='f1',n_jobs=-1,cv=5)","a17fc626":"%%time\neval_set = [(X_train2, y_train2)]\nxgb_gs2.fit(X_train2, y_train2, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","b72f928a":"xgb_gs2.best_params_","6f21f8ea":"# Predict (train)\ny_train_pred = xgb_gs2.predict(X_train2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: gscv XGB - train')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","c328804c":"# Predict (val)\ny_val_pred = xgb_gs2.predict(X_val2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: gscv XGB - val')\nprint('-------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","bfe3e0da":"submission_xgbgscv2 = pd.DataFrame()\nsubmission_xgbgscv2['Survived_1_year'] = xgb_gs2.predict(test2)\nsubmission_xgbgscv2.head()","5316d7e1":"submission_xgbgscv2.to_csv(\"submission_xgbgscv2.csv\",index=False)","8cf62a03":"xgb_clf3 = xgb.XGBClassifier(**xgb_gs2.best_params_, seed = 10)\n\n# Learn the model with training data\nxgb_clf3.fit(X_train2, y_train2)\n\neval_set = [(X_train2, y_train2)]\nxgb_clf3.fit(X_train2, y_train2, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","4b6ce2be":"# Plot the top 100 important features\nimp_feat_xgb=pd.Series(xgb_clf3.feature_importances_, index=X_train2.columns).sort_values(ascending=False)\nimp_feat_xgb[:100].plot(kind='bar',title='Dataset2: Top 50 Important features as per XGBoost', figsize=(20,10))\nplt.ylabel('Feature Importance Score')\nplt.subplots_adjust(bottom=0.25)\nplt.savefig('FeatureImportance.png')\nplt.show()","0fbead3d":"# Selecting features with importance based on the threshold of 0.01\nfrom sklearn.feature_selection import SelectFromModel\n\nthreshold = 0.01\n\nsfm3 = SelectFromModel(xgb_clf3, threshold=threshold)\nsfm3.fit(X_train2, y_train2)\nX_train_t2 = sfm3.transform(X_train2)","43104eaa":"X_val_t2 = sfm3.transform(X_val2)\ntest_t2 = sfm3.transform(test2)\nprint('Train shape: ', X_train_t2.shape)\nprint('Val shape: ', X_val_t2.shape)\nprint('Test shape: ', test_t2.shape)","80cf97c7":"xgb_clf4 = xgb.XGBClassifier(**xgb_gs2.best_params_, seed = 10)\n\n# Learn the model on transformed data\nxgb_clf4.fit(X_train_t2, y_train2)\n\neval_set = [(X_train_t2, y_train2)]\nxgb_clf4.fit(X_train_t2, y_train2, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","1d2d3c76":"# Predict (train)\ny_train_pred = xgb_clf4.predict(X_train_t2)\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: gscv-fea.imp. XGB - train')\nprint('-----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","f2fea465":"# Predict (val)\ny_val_pred = xgb_clf4.predict(X_val_t2)\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: gscv-fea.imp. XGB - val')\nprint('----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","bbff0909":"submission_xgbgscvfi2 = pd.DataFrame()\nsubmission_xgbgscvfi2['Survived_1_year'] = xgb_clf4.predict(test_t2)\nsubmission_xgbgscvfi2.head()","a4f23296":"submission_xgbgscvfi2.to_csv(\"submission_xgbgscvfi2.csv\",index=False)","3c9f4d3f":"import lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nclf = lgb.LGBMClassifier(silent=True, random_state = 123, early_stopping_rounds=5, metric='f1', n_jobs=4)","76e57e81":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparams ={'cat_smooth' : sp_randint(1, 100), 'min_data_per_group': sp_randint(1,1000), 'max_cat_threshold': sp_randint(1,100),\n        'learning_rate': [0.07,0.08,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89, 0.09,0.1], 'num_iterations': sp_randint(1000,3000),\n        'scale_pos_weight': sp_randint(1,15),'colsample_bytree': sp_uniform(loc=0.4, scale=0.6), 'num_leaves': sp_randint(500, 5000),  \n        'min_child_samples': sp_randint(100,500), 'min_child_weight': [1e-2, 1e-1, 1, 1e1], 'max_bin': sp_randint(100, 1500), 'max_depth': sp_randint(1, 15), \n        'min_data_in_leaf': sp_randint(500,3500), 'reg_lambda': sp_randint(1, 30), 'boosting': ['goss', 'dart']}","1b7899ce":"fit_params={\"early_stopping_rounds\":5, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train, y_train),(X_val,y_val)],\n            'eval_names': ['train','valid'],\n            'verbose': 300,\n            'categorical_feature': 'auto'}","e04cbdbc":"gs = RandomizedSearchCV( estimator=clf, param_distributions=params, scoring='f1',cv=5, refit=True,random_state=135,verbose=True)","624fa403":"gs.fit(X_train, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","6bf579e0":"chk1_params = {**gs.best_params_, 'scoring':'f1'}\nchk1_params","ff80e173":"lgbm_train1 = lgb.Dataset(X_train, y_train, categorical_feature=cat_columns)\nlgbm_val1 = lgb.Dataset(X_val, y_val, reference = lgbm_train1)","8b60ba8c":"model_lgbm_chk1 = lgb.train(chk1_params,lgbm_train1,\n                num_boost_round=1000,\n                valid_sets=[lgbm_train1, lgbm_val1],\n                feature_name=['f' + str(i + 1) for i in range(X_train.shape[-1])],\n                categorical_feature= [150], \n                verbose_eval=100)","33f70e68":"# Predict (train)\ny_train_pred = model_lgbm_chk1.predict(X_train, type = 'response')\ny_train_pred = np.absolute(y_train_pred)\ny_train_pred = y_train_pred.round()\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv LGBM1 - train')\nprint('-----------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","3b07fd36":"# Predict (val)\ny_val_pred = model_lgbm_chk1.predict(X_val)\ny_val_pred = np.absolute(y_val_pred)\ny_val_pred = y_val_pred.round()\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv LGBM1 - val')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","1565b32b":"submission_cvlgbm1 = pd.DataFrame()\nsubmission_cvlgbm1['Survived_1_year'] = model_lgbm_chk1.predict(test)\nsubmission_cvlgbm1['Survived_1_year'] = submission_cvlgbm1['Survived_1_year'].round().astype(int)\nsubmission_cvlgbm1.head()","98e09150":"submission_cvlgbm1.to_csv(\"submission_cvlgbm1.csv\",index=False) ","a632c36f":"clfnew = lgb.LGBMClassifier(silent=True, random_state = 135, early_stopping_rounds=5, metric='f1', n_jobs=4)","7ed8f779":"fit_params={\"early_stopping_rounds\":5, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train_t, y_train),(X_val_t,y_val)],\n            'eval_names': ['train','valid'],\n            'verbose': 300,\n            'categorical_feature': 'auto'}","7a8a4de7":"gs = RandomizedSearchCV( estimator=clfnew, param_distributions=params, scoring='f1',cv=5, refit=True,random_state=135,verbose=True)","ebfcadb5":"gs.fit(X_train_t, y_train, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","3ae207ae":"chk2_params = {**gs.best_params_, 'scoring':'f1'}\nchk2_params","ad3efac1":"lgbm_train2 = lgb.Dataset(X_train_t, y_train, categorical_feature=cat_columns)\nlgbm_val2 = lgb.Dataset(X_val_t, y_val, reference = lgbm_train2)","b3181a07":"model_lgbm_chk2 = lgb.train(chk2_params,\n                lgbm_train2,\n                num_boost_round=1000,\n                valid_sets=[lgbm_train2, lgbm_val2],\n                feature_name=['f' + str(i + 1) for i in range(X_train_t.shape[-1])],\n                categorical_feature= [150],\n                verbose_eval=100)","7ee5c3ac":"# Predict (train)\ny_train_pred = model_lgbm_chk2.predict(X_train_t, pred_contrib=False)\ny_train_pred = np.absolute(y_train_pred.round())\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: gscv LGBM2 - train')\nprint('-----------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","1636c821":"# Predict (val)\ny_val_pred = model_lgbm_chk2.predict(X_val_t)\ny_val_pred = np.absolute(y_val_pred.round())\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: gscv LGBM2 - val')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","9de76a1a":"submission_cvlgbm2 = pd.DataFrame()\nsubmission_cvlgbm2['Survived_1_year'] = model_lgbm_chk2.predict(test_t)\nsubmission_cvlgbm2['Survived_1_year'] = submission_cvlgbm2['Survived_1_year'].round().astype(int)\nsubmission_cvlgbm2.head()","fa17596f":"submission_cvlgbm2.to_csv(\"submission_cvlgbm2.csv\",index=False) ","c64fd5b0":"import lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV\nclf2 = lgb.LGBMClassifier(silent=True, random_state = 123, early_stopping_rounds=5, metric='f1', n_jobs=4)","cc597cd1":"params2 ={'cat_smooth' : sp_randint(1, 100), 'min_data_per_group': sp_randint(1,1000), 'max_cat_threshold': sp_randint(1,100),\n        'learning_rate': [0.07,0.08,0.81,0.82,0.83,0.84,0.85,0.86,0.87,0.88,0.89, 0.09,0.1], 'num_iterations': sp_randint(1000,3000),\n        'scale_pos_weight': sp_randint(1,15),'colsample_bytree': sp_uniform(loc=0.4, scale=0.6), 'num_leaves': sp_randint(500, 5000),  \n        'min_child_samples': sp_randint(100,500), 'min_child_weight': [1e-2, 1e-1, 1, 1e1], 'max_bin': sp_randint(100, 1500), 'max_depth': sp_randint(1, 15), \n        'min_data_in_leaf': sp_randint(500,3500), 'reg_lambda': sp_randint(1, 30), 'boosting': ['goss', 'dart']}","2ed43ade":"fit_params={\"early_stopping_rounds\":5, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train2, y_train2),(X_val2,y_val2)],\n            'eval_names': ['train','valid'],\n            'verbose': 300,\n            'categorical_feature': 'auto'}","cb90eb56":"gs2 = RandomizedSearchCV( estimator=clf2, param_distributions=params2, scoring='f1',cv=5, refit=True,random_state=123,verbose=True)","47cb2006":"gs2.fit(X_train2, y_train2, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs2.best_score_, gs2.best_params_))","645ed18d":"chk1_params = {**gs2.best_params_, 'scoring':'f1'}\nchk1_params","43fd1994":"lgbm_train3 = lgb.Dataset(X_train2, y_train2, categorical_feature=cat_columns)\nlgbm_val3 = lgb.Dataset(X_val2, y_val2, reference = lgbm_train3)","8fdefbe8":"model_lgbm_chk3 = lgb.train(chk1_params,lgbm_train3,\n                num_boost_round=1000,\n                valid_sets=[lgbm_train3, lgbm_val3],\n                feature_name=['f' + str(i + 1) for i in range(X_train2.shape[-1])],\n                categorical_feature= [150], \n                verbose_eval=100)","1a216039":"# Predict (train)\ny_train_pred = model_lgbm_chk3.predict(X_train2, type = 'response')\ny_train_pred = np.absolute(y_train_pred)\ny_train_pred = y_train_pred.round()\n\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred)\nacc = accuracy_score(y_train2, y_train_pred)\ncm = confusion_matrix(y_train2, y_train_pred)\nprint('Dataset2: gscv LGBM3 - train')\nprint('-----------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","aba3504c":"# Predict (val)\ny_val_pred = model_lgbm_chk3.predict(X_val2)\ny_val_pred = np.absolute(y_val_pred)\ny_val_pred = y_val_pred.round()\n\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred)\nacc = accuracy_score(y_val2, y_val_pred)\ncm = confusion_matrix(y_val2, y_val_pred)\nprint('Dataset2: gscv LGBM3 - val')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","5a72a6af":"submission_cvlgbm3 = pd.DataFrame()\nsubmission_cvlgbm3['Survived_1_year'] = model_lgbm_chk3.predict(test2)\nsubmission_cvlgbm3['Survived_1_year'] = submission_cvlgbm3['Survived_1_year'].round().astype(int)\nsubmission_cvlgbm3.head()","e86a4afc":"submission_cvlgbm3.to_csv(\"submission_cvlgbm3.csv\",index=False) ","8a56319c":"clf2new = lgb.LGBMClassifier(silent=True, random_state = 123, early_stopping_rounds=5, metric='f1', n_jobs=4)","0f26fa67":"fit_params={\"early_stopping_rounds\":5, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_train_t2, y_train2),(X_val_t2,y_val2)],\n            'eval_names': ['train','valid'],\n            'verbose': 300,\n            'categorical_feature': 'auto'}","c4859c64":"gs2 = RandomizedSearchCV( estimator=clf2new, param_distributions=params2, scoring='f1',cv=5, refit=True,random_state=123,verbose=True)","f1935123":"gs2.fit(X_train_t2, y_train2, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs2.best_score_, gs2.best_params_))","1870917f":"chk2_params = {**gs2.best_params_, 'scoring':'f1'}\nchk2_params","20b44a6e":"lgbm_train4 = lgb.Dataset(X_train_t2, y_train2, categorical_feature=cat_columns)\nlgbm_val4 = lgb.Dataset(X_val_t2, y_val2, reference = lgbm_train4)","4825f23c":"model_lgbm_chk4 = lgb.train(chk2_params,\n                lgbm_train4,\n                num_boost_round=1000,\n                valid_sets=[lgbm_train4, lgbm_val4],\n                feature_name=['f' + str(i + 1) for i in range(X_train_t2.shape[-1])],\n                categorical_feature= [150],\n                verbose_eval=100)","9fdc0372":"# Predict (train)\ny_train_pred = model_lgbm_chk4.predict(X_train_t2, pred_contrib=False)\ny_train_pred = np.absolute(y_train_pred.round())\n# Model evaluation (train)\nf1 = f1_score(y_train2, y_train_pred.round())\nacc = accuracy_score(y_train2, y_train_pred.round())\ncm = confusion_matrix(y_train2, y_train_pred.round())\nprint('Dataset1: gscv LGBM4 - train')\nprint('-----------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","7a79864f":"# Predict (val)\ny_val_pred = model_lgbm_chk4.predict(X_val_t2)\ny_val_pred = np.absolute(y_val_pred.round())\n# Model evaluation (train)\nf1 = f1_score(y_val2, y_val_pred.round())\nacc = accuracy_score(y_val2, y_val_pred.round())\ncm = confusion_matrix(y_val2, y_val_pred.round())\nprint('Dataset1: gscv LGBM4 - val')\nprint('---------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","dcaa82f6":"submission_cvlgbm4 = pd.DataFrame()\nsubmission_cvlgbm4['Survived_1_year'] = model_lgbm_chk4.predict(test_t2)\nsubmission_cvlgbm4['Survived_1_year'] = submission_cvlgbm4['Survived_1_year'].round().astype(int)\nsubmission_cvlgbm4.head()","939e4cca":"submission_cvlgbm4.to_csv(\"submission_cvlgbm4.csv\",index=False) ","92f8a458":"from sklearn.feature_selection import RFE #importing RFE class from sklearn library\n\nrfe_xgb1= RFE(estimator= xgb_clf2 , step = 1) # with Random Forest\n\n# Fit the function for ranking the features\nfit = rfe_xgb1.fit(X_train_t, y_train)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)","27c00458":"# Transforming the data\nX_train_t_rfe = rfe_xgb1.transform(X_train_t)\nX_val_t_rfe = rfe_xgb1.transform(X_val_t)\ntest_t_rfe = rfe_xgb1.transform(test_t)\n# Fitting our baseline model with the transformed data\nrfe_xgb_model = xgb_clf2.fit(X_train_t_rfe, y_train)","e29d2ba0":"# Predict (train)\ny_train_pred = rfe_xgb_model.predict(X_train_t_rfe)\n\n# Model evaluation (train)\nf1 = f1_score(y_train, y_train_pred)\nacc = accuracy_score(y_train, y_train_pred)\ncm = confusion_matrix(y_train, y_train_pred)\nprint('Dataset1: RFE on xgb_clf2 - train')\nprint('----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","5b75568c":"# Predict (val)\ny_val_pred = rfe_xgb_model.predict(X_val_t_rfe)\n\n# Model evaluation (train)\nf1 = f1_score(y_val, y_val_pred)\nacc = accuracy_score(y_val, y_val_pred)\ncm = confusion_matrix(y_val, y_val_pred)\nprint('Dataset1: RFE on xgb_cl2 - val')\nprint('-------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","25436429":"submission_rfexgb = pd.DataFrame()\nsubmission_rfexgb['Survived_1_year'] = rfe_xgb_model.predict(test_t_rfe)\nsubmission_rfexgb['Survived_1_year'] = submission_rfexgb['Survived_1_year'].round().astype(int)\nsubmission_rfexgb.head()","49c98731":"submission_rfexgb.to_csv(\"submission_rfexgb.csv\",index=False) ","87024927":"X.head()","d525f88d":"from sklearn import preprocessing\nXscaled = X.copy()\nXscaled[['Patient_Body_Mass_Index']] = preprocessing.scale(Xscaled[['Patient_Body_Mass_Index']])\nXscaled.head()","f175ac90":"testscaled = test.copy()\ntestscaled[['Patient_Body_Mass_Index']] = preprocessing.scale(testscaled[['Patient_Body_Mass_Index']])\ntestscaled.head()","adde00fa":"# For scaled data\nX_trains, X_vals, y_trains, y_vals = train_test_split(Xscaled,y,test_size=0.3, random_state = 50)","41996272":"xgbs = XGBClassifier(seed=27)\nxgbs_gs = GridSearchCV(estimator = xgbs, param_grid = param_test1, scoring='f1',n_jobs=-1,cv=5)","3bd8e679":"%%time\neval_set = [(X_trains, y_trains)]\nxgbs_gs.fit(X_trains, y_trains, early_stopping_rounds=10, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)","7a7059cc":"xgbs_gs.best_params_","c534cd8c":"# Predict (train)\ny_train_pred = xgbs_gs.predict(X_trains)\n\n# Model evaluation (train)\nf1 = f1_score(y_trains, y_train_pred)\nacc = accuracy_score(y_trains, y_train_pred)\ncm = confusion_matrix(y_trains, y_train_pred)\nprint('Dataset1: gscv XGB - train-scaled')\nprint('----------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm) ","0e226856":"# Predict (val)\ny_val_pred = xgbs_gs.predict(X_vals)\n\n# Model evaluation (train)\nf1 = f1_score(y_vals, y_val_pred)\nacc = accuracy_score(y_vals, y_val_pred)\ncm = confusion_matrix(y_vals, y_val_pred)\nprint('Dataset2: gscv XGB - val=scaled')\nprint('--------------------------------')\nprint('f1_score: ', f1)\nprint('accuracy_score: ', acc)\nprint('Confusion Matrix: ')\nprint(cm)","112fe78b":"submission_xgbs = pd.DataFrame()\nsubmission_xgbs['Survived_1_year'] = xgbs_gs.predict(testscaled)\nsubmission_xgbs['Survived_1_year'] = submission_xgbs['Survived_1_year'].round().astype(int)\nsubmission_xgbs.head()","8fe01487":"submission_xgbs.to_csv(\"submission_xgbs.csv\",index=False) ","cc8c475e":"### **2.8 Dummies of Categorical Variables** <a id = 'dum' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","95028478":"1. [**Libraries, Analyzing the Data, Data Preparation**](#libraries) <a href = '#libraries'><\/a>\n    - [**1.1 Checking NULL Values**](#null) <a href = '#null'><\/a>\n    - [**1.2 Correlation Plot**](#cor) <a href = '#cor'><\/a>\n2. [**Data Preparation**](#dp) <a href = '#dp'><\/a>   \n    - [**2.1 Checking Treated_with_drugs column**](#twd) <a href = '#twd'><\/a>\n    - [**2.2 Checking Patient_Smoker column**](#ps) <a href = '#ps'><\/a>\n    - [**2.3 Patient_ID and ID_Patient_Care_Situation**](#id) <a href = '#ps'><\/a>    \n    - [**2.4 Checking for Columns with all unique values or single value to drop**](#cc) <a href = '#cc'><\/a>    \n    - [**2.5 Patient_Age**](#age) <a href = '#age'><\/a>\n    - [**2.6 Creating Data Sets and Imputation**](#impcat) <a href = '#impcat'><\/a>\n    - [**2.7 Checking columns to convert to categorical**](#cnvrt) <a href = '#cnvrt'><\/a>\n    - [**2.8 Dummies of Categorical Variables**](#dum) <a href = '#dum'><\/a>\n3. [**Writing Data Sets**](#wds) <a href = '#wds'><\/a>   \n4. [**Models**](#mod) <a href = '#mod'><\/a>\n    - [**A.Logistic Regression**](#lr) <a href = '#lr'><\/a> \n        - Dataset1: **80.946209**\n        - Dataset2: **80.765755**\n    - [**B.Decision Tree**](#dt) <a href = '#dt'><\/a> \n        - Dataset1: **80.714652** \n        - Dataset2: **81.002231**\n        - [**Grid Search**](#dtgs) <a href = '#dtgs'><\/a>\n            - Dataset1: **84.811297** \n            - Dataset2: **83.697159**\n    - [**C.Random Forest GSCV**](#rf) <a href = '#rf'><\/a>  \n        - Dataset1: **85.380875**\n        - Dataset2:: **85.112291**\n    - [**D.XG Boost**](#xgb) <a href = '#xgb'><\/a> \n        - Dataset1: Without feature selection: **86.289245** \n        - Dataset1: With feature selection threshold 0.010: **86.344203** \n        - Dataset1: With feature selection threshold 0.015: **85.791246**\n        - Dataset2: Without feature selection: **86.230287** \n        - Dataset2: With feature selection: **86.270856** \n    - [**E.LGBM**](#lgbm) <a href = '#lgbm'><\/a> \n        - Dataset1: **85.575798**  \n        - Dataset1: With feature selection: **85.460637**\n        - Dataset2: **85.578876**  \n        - Dataset2: With feature selection: **85.513266**\n    - [**F.RFE**](#rfexgb) <a href = '#rfexgb'><\/a> \n        - Dataset1: With feature selection threshold 0.010: **85.885531**       \n    - [**G. XGB on Scaled Dataset1**](#xgbs) <a href = '#xgbs'><\/a> \n        - Dataset1: **86.079973**          \n        ","2752e15a":"#### For threshold = 0.015","41463c79":"### **2.4 Checking for Columns with all unique values or single value to drop** <a id = 'cc' ><\/a>\n    - Patient_mental_condition has just 1 value in train and test and will be dropped\n    - ID_Patient_Care_Situation has all unique values only in test set; aborbed in 2.3\n    - Although column Z has 1 value only in the test set we will still retain it initially for analysis\n\n[Home](#home) <a href = '#home'><\/a>","a5976e41":"#### Dataset2: Feature Importance","df6e196a":"#### Dataset2\n[Home](#home) <a href = '#home'><\/a>","2c20e879":"#### Dataset2\n[Home](#home) <a href = '#home'><\/a>","57c17e01":"### 2.1 Checking Treated_with_drugs column <a id = 'twd' ><\/a>","317f14ac":"### B. Decision Tree <a id = 'dt' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","84cdc92d":"Check 1","8a09fa1b":"Check 1","c56a2d21":"### F. RFE <a id = 'rfexgb' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","1e3da25e":"### E. LGBM <a id = 'lgbm' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","59514c0f":"## 2. Data Preparation <a id = 'dp' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","1ce70b36":"### 1.2 Correlation Plot <a id = 'cor' ><\/a>\n    - We do not see any specific high correlation\/s between variables \n[Home](#home) <a href = '#home'><\/a>","674fdd73":"**Dataset2**","84cffa23":"#### Feature Importance","fd194271":"#### Datasets with null value rows dropped\n[Home](#home) <a href = '#home'><\/a>","fca9ecdd":"### 2.2 Checking Patient_Smoker column <a id = 'ps' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","fc37eb5b":"### C. Random Forest GSCV <a id = 'rf' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","ca04e783":"## 1. Libraries, Analyzing the Data, Data Preparation <a id = 'libraries' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","49da6c60":"## 4. Models <a id = 'mod' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","a8278410":"#### xgb_clf2: XG Boost on transformed Dataset1 with threshold = 0.010","67c63f54":"### **2.6 Creating Datasets and Imputation** <a id = 'impcat' ><\/a>\n    - Imputing 'Treated_with_drugs' with mode\n    - Datasets 1: pharma_data, test\n        - MICE imputation on 'A','B','C','D','E','F','Z'\n        - Number_of_prev_cond = SUM('A','B','C','D','E','F','Z')\n    - Datasets 2: pharma_data2, test2\n        - Drop the rows containing null values\n[Home](#home) <a href = '#home'><\/a>","4c96ff7c":"## **3 Writing the Datasets** <a id = 'wds' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","013c28da":"Since the NA Counts are the same for columns A to F, Z, and Number_of_prev_cond, checking if the are of the same set of rows\n\n        1. Since they turned out to be the same data set to be prepared dropping these rows\n        2. MICE Imputation method to be explored","5cdb1e0c":"**Dataset2**","d636d4ed":"### A. Logistic Regression <a id = 'lr' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","6bb63738":"#### Grid Search <a id = 'dtgs' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","1b7b9d9a":"### D. XG Boost <a id = 'xgb' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","a4a06dc3":"### **2.7 Checking columns to convert to categorical** <a id = 'cnvrt' ><\/a>\n    - 'Patient_Smoker',  'Patient_Rural_Urban', 'Patient_mental_condition', 'A', 'B', 'C', 'D', 'E', 'F', 'Z', 'Number_of_prev_cond', 'New_ID', 'Age_band'\n[Home](#home) <a href = '#home'><\/a>","f6ca6a13":"<a id = 'home' ><\/a>","cf358a87":"### 2.3 Patient_ID and ID_Patient_Care_Situation <a id = 'id' ><\/a>\n    - Creating a new column that gives the count of ID_Patient_Care_Situation per Patient_ID\n    - Then we will drop Patient_ID and ID_Patient_Care_Situation\n[Home](#home) <a href = '#home'><\/a>","987ee0fb":"### 1.1 Checking NULL Values <a id = 'null' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","28769c3c":"#### Check-2: On transformed data","bf0d3757":"### G. XGB on Scaled Dataset1 <a id = 'xgbs' ><\/a>\n[Home](#home) <a href = '#home'><\/a>","7acf2e5e":"#### Check-2: On transformed data","4cd47832":"### **2.5 Patient_Age** <a id = 'age' ><\/a>\n    - Outliers of Patient_Age >100 observed in train set; these points will be dropped\n    - After that we create 10 bins for age and drop Patient_Age for initial analysis\n[Home](#home) <a href = '#home'><\/a>","db289da7":"#### Dropping duplicate rows","96359448":"#### Datasets for MICE Imputation","c835ee6b":"# Pharma Data Analysis\n*     Here we are analyzing on two sets of data\n            - Dataset1: Here the numerical null values are imputed using MICE technique\n            - Dataset2: Here the common null value rows are dropped\n            - Rest of the operations remain common for both the datasets\n            \n*     What Next?\n            - Further analysis on Diagnosed_Condition variable\n            - Treat A to F, Z, Number_of_prev_cond columns as Numerical and analyze\n            - Explore phase wise hyperparamter tuning "}}