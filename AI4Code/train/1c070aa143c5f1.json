{"cell_type":{"d09f9644":"code","5fde7808":"code","83f7e735":"code","18df3365":"code","ed650e97":"code","392facc1":"code","a8761b53":"code","c3346db9":"code","e7daac00":"code","1de0db37":"code","8e433bbc":"code","3441f6bb":"code","11157de1":"code","cc4e099b":"code","e57230fa":"code","317ba1fb":"code","47385319":"code","9c8b3c30":"code","e6370421":"code","eea964f3":"code","94e74acd":"code","b2f8be84":"code","30f67829":"code","13aaee7a":"code","2750e47b":"code","9ea6a015":"code","305c3e15":"code","e574c2af":"code","ffc6efc7":"code","2227bb43":"code","241a2e07":"code","9ae23d81":"code","67760116":"code","80db83e9":"markdown"},"source":{"d09f9644":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fde7808":"df = pd.read_csv('..\/input\/fake-news\/train.csv')\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')","83f7e735":"X=df.drop('label',axis=1)\ny=df['label']","18df3365":"df_train=df.copy()","ed650e97":"df.isnull().sum()","392facc1":"test.isnull().sum()","a8761b53":"#filling NULL values with empty string\ndf=df.fillna('')\ntest=test.fillna('')\n","c3346db9":"# We will be only using title and author name for prediction\n# Creating new coolumn total concatenating title and author\ndf['total'] = df['title']+' '+df['author']\ntest['total']=test['title']+' '+test['author']\n","e7daac00":"X = df.drop('label',axis=1)\ny=df['label']\nprint(X.shape)\nprint(y.shape)\n","1de0db37":"#Downloading stopwords \nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","8e433bbc":"#Lemmatizing map words to their root forms\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\nwordnet = WordNetLemmatizer()\nstop_words = stopwords.words('english')","3441f6bb":"#Applying stemming and some preprocessing\ndef clean_text(text):\n    text = text.lower() # lowering\n    text = text.encode(\"ascii\", \"ignore\").decode() # non ascii chars\n    text = re.sub(r'\\n',' ', text) # remove new-line characters\n    text = re.sub(r'\\W', ' ', text) # special chars\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # single characters\n    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) # single char at first\n    text = re.sub(r'[0-9]', ' ', text) # digits\n    text = re.sub(r'\\s+', ' ', text, flags=re.I) # multiple spaces\n    text=text.split()\n    return ' '.join([wordnet.lemmatize(word) for word in text if word not in stop_words])","11157de1":"df['total']=df['total'].apply(clean_text)","cc4e099b":"X=df['total']\ny=df['label']","e57230fa":"test['total']=test['total'].apply(clean_text)","317ba1fb":"from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\n","47385319":"test_X=test['total']","9c8b3c30":"#Choosing vocabulary size to be 5000\nvoc_size=5000","e6370421":"# Converting to one hot representation\nonehot_rep = [one_hot(words,voc_size)for words in X]\n# Converting to one hot representation for test set\nonehot_rep_test = [one_hot(words,voc_size)for words in test_X]\n","eea964f3":"#Padding Sentences to make them of same size\nembedded_docs = pad_sequences(onehot_rep,padding='pre',maxlen=25)\n#Padding Sentences to make them of same size\nembedded_docs_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=25)\n","94e74acd":"#We have used embedding layers with LSTM\nmodel = Sequential()\nmodel.add(Embedding(voc_size,40,input_length=25))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())\n","b2f8be84":"#Converting into numpy array\nX_final = np.array(embedded_docs)\ny_final = np.array(y)\ntest_final = np.array(embedded_docs_test)","30f67829":"X_final.shape,y_final.shape,test_final.shape","13aaee7a":"#training model\nmodel.fit(X_final,y_final,epochs=20,batch_size=64)\n","2750e47b":"y_pred = model.predict(test_final)","9ea6a015":"y_pred=np.round(y_pred)","305c3e15":"y_pred.astype('int64')","e574c2af":"final_sub = pd.DataFrame()\nfinal_sub['id']=test['id']\nfinal_sub['label'] = y_pred\nfinal_sub.to_csv('final_sub.csv',index=False)\n","ffc6efc7":"final_sub.head()","2227bb43":"final_sub.info()","241a2e07":"final_sub['label']=final_sub['label'].astype('int64')","9ae23d81":"final_sub.head()","67760116":"final_sub.to_csv('submission.csv',index=False)","80db83e9":"### Creating and training model"}}