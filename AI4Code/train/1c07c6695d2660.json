{"cell_type":{"478999b2":"code","3f0c39a3":"code","240bef21":"code","fb90f270":"code","6f7d3fbd":"code","9e30bc5b":"code","57dd7973":"code","bca8ec1f":"code","89312a1b":"code","f85546d9":"code","bf395fc2":"code","9deb1074":"code","3f86d0cc":"code","f7b1d1ed":"code","8e23d31f":"code","b1eaedac":"code","ce7f03aa":"code","3f2f06c2":"code","85a2afdb":"code","f424d4ee":"code","f001cf2f":"code","ea159aca":"code","ae24b282":"code","fe02a9ff":"code","2a6d20fa":"code","db876ccd":"code","6e62de63":"code","8398f628":"code","5c572b60":"code","3fa9a563":"code","ba4b9995":"code","54578211":"code","59faa89c":"code","ce99c6a2":"code","2860660c":"markdown","57d99c9a":"markdown","79ad1711":"markdown","484cb8f6":"markdown","aa76425a":"markdown","4975fc6a":"markdown","20ebd631":"markdown","e736d03b":"markdown","9b747ac7":"markdown","0e0b7a5f":"markdown","666c6eb8":"markdown","c20fb06b":"markdown","fa7acd32":"markdown","0e51ca04":"markdown","edf1eac0":"markdown","ed2e567f":"markdown","c3c09012":"markdown","762b250c":"markdown","6c03898f":"markdown","71261e07":"markdown","dbbf8713":"markdown","0eeaf7f5":"markdown","3c715282":"markdown","ae93877c":"markdown","f69cac83":"markdown","a2f18033":"markdown"},"source":{"478999b2":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom matplotlib.colors import ListedColormap\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nimport os\nfrom scipy.stats import zscore\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom keras.layers.core import Dense, Activation\nimport tensorflow as tf\nfrom tensorflow import keras\nimport keras\nfrom sklearn.datasets import make_moons\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import models\nfrom keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_classification\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\nimport gc\nimport itertools\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier","3f0c39a3":"IS_LOCAL = False\nif (IS_LOCAL):\n    location = \"..\/input\/dont-overfit\/\"\nelse:\n    location = \"..\/input\/\"\nos.listdir(location)","240bef21":"%%time\ntrain = pd.read_csv(os.path.join(location, 'train.csv'))\ntest = pd.read_csv(os.path.join(location, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(location, 'sample_submission.csv'))","fb90f270":"print(\"train: {}\\ntest: {}\".format(train.shape, test.shape))","6f7d3fbd":"def show_head(data):\n    return(data.head())","9e30bc5b":"show_head(train)","57dd7973":"show_head(test)","bca8ec1f":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (total\/data.isnull().count()*100)\n    miss_column = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    miss_column['Types'] = types\n    return(np.transpose(miss_column)) ","89312a1b":"missing_data(train)","f85546d9":"missing_data(test)","bf395fc2":"def describe_data(data):\n    return(data.describe())","9deb1074":"describe_data(train)","3f86d0cc":"describe_data(test)","f7b1d1ed":"label = train['target']\ntrain = train.drop(['id', 'target'], axis=1)\ntest = test.drop(['id'], axis=1)","8e23d31f":"label_count = label.value_counts().reset_index().rename(columns = {'index' : 'Labels'})\nlabel_count","b1eaedac":"show_head(train)","ce7f03aa":"show_head(label)","3f2f06c2":"sc = StandardScaler()\nxtrain = sc.fit_transform(train)\nxtest = sc.transform(test)\n\npca = PCA().fit(xtrain)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","85a2afdb":"print(\"xtrain: {}\\nxtest: {}\\nlabel: {}\".format(xtrain.shape, xtest.shape, label.shape))","f424d4ee":"missing_data(train)","f001cf2f":"from sklearn.metrics import classification_report as c_report\nfrom sklearn.metrics import confusion_matrix as c_matrix\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=4422)\nforcast_rfc2 = np.zeros(len(xtest))\nvalidation_pred_rfc2 = np.zeros(len(xtrain))\nscores_rfc2 = []\nvalid_rfc2 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_rfc2 = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\n    clf_rfc2.fit(x_train, y_train)\n    pred_valid = clf_rfc2.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_rfc2.predict_proba(xtest)[:, 1]\n    \n    validation_pred_rfc2[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_rfc2.append(accuracy_score(y_valid, pred_valid))\n    forcast_rfc2 += Pred_Real\n    valid_rfc2[xvad_indx] += (y_valid)\n\nprint(c_report(valid_rfc2[xvad_indx], validation_pred_rfc2[xvad_indx]))\nprint(c_matrix(valid_rfc2[xvad_indx], validation_pred_rfc2[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_rfc2), np.std(scores_rfc2)))","ea159aca":"from sklearn.tree import DecisionTreeClassifier\nforcast_dtc2 = np.zeros(len(xtest))\nvalidation_pred_dtc2 = np.zeros(len(xtrain))\nscores_dtc2 = []\nvalid_dtc2 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_dtc2 = DecisionTreeClassifier()\n    clf_dtc2.fit(x_train, y_train)\n    pred_valid = clf_dtc2.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_dtc2.predict_proba(xtest)[:, 1]\n    \n    validation_pred_dtc2[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_dtc2.append(accuracy_score(y_valid, pred_valid))\n    forcast_dtc2 += Pred_Real\n    valid_dtc2[xvad_indx] += (y_valid)\n\nprint(c_report(valid_dtc2[xvad_indx], validation_pred_dtc2[xvad_indx]))\nprint(c_matrix(valid_dtc2[xvad_indx], validation_pred_dtc2[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_dtc2), np.std(scores_dtc2)))","ae24b282":"from sklearn import svm\nforcast_svm2 = np.zeros(len(xtest))\nvalidation_pred_svm2 = np.zeros(len(xtrain))\nscores_svm2 = []\nvalid_svm2 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_svm2 = svm.SVC(kernel='linear', gamma=1)\n    clf_svm2.fit(x_train, y_train)\n    pred_valid = clf_svm2.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_svm2.predict(xtest)\n    \n    validation_pred_svm2[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_svm2.append(accuracy_score(y_valid, pred_valid))\n    forcast_svm2 += Pred_Real\n    valid_svm2[xvad_indx] += (y_valid)\n\nprint(c_report(valid_svm2[xvad_indx], validation_pred_svm2[xvad_indx]))\nprint(c_matrix(valid_svm2[xvad_indx], validation_pred_svm2[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_svm2), np.std(scores_svm2)))","fe02a9ff":"forcast_lr2 = np.zeros(len(xtest))\nvalidation_pred_lr2 = np.zeros(len(xtrain))\nscores_lr2 = []\nvalid_lr2 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_lr2 = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n    clf_lr2.fit(x_train, y_train)\n    pred_valid = clf_lr2.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_lr2.predict_proba(xtest)[:, 1]\n    \n    validation_pred_lr2[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_lr2.append(accuracy_score(y_valid, pred_valid))\n    forcast_lr2 += Pred_Real\n    valid_lr2[xvad_indx] += (y_valid)\n\nprint(c_report(valid_lr2[xvad_indx], validation_pred_lr2[xvad_indx]))\nprint(c_matrix(valid_lr2[xvad_indx], validation_pred_lr2[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_lr2), np.std(scores_lr2)))","2a6d20fa":"perm = PermutationImportance(clf_lr2, random_state=1).fit(xtrain, label)\neli5.show_weights(perm, top=50)","db876ccd":"(clf_lr2.coef_ != 0).sum()","6e62de63":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(clf_lr2).feature if 'BIAS' not in i]\nxtrain_lr_elif = train[top_features]\nxtest_lr_elif = test[top_features]\nscaler = StandardScaler()\nxtrain_lr_elif = scaler.fit_transform(xtrain_lr_elif)\nxtest_lr_elif = scaler.transform(xtest_lr_elif)\n\nforcast_lr4 = np.zeros(len(xtest_lr_elif))\nvalidation_pred_lr4 = np.zeros(len(xtrain_lr_elif))\nscores_lr4 = []\nvalid_lr4 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain_lr_elif, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain_lr_elif[xtrn_indx], xtrain_lr_elif[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_lr4 = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n    clf_lr4.fit(x_train, y_train)\n    pred_valid = clf_lr4.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_lr4.predict_proba(xtest_lr_elif)[:, 1]\n    \n    validation_pred_lr4[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_lr4.append(accuracy_score(y_valid, pred_valid))\n    forcast_lr4 += Pred_Real\n    valid_lr4[xvad_indx] += (y_valid)\n\nprint(c_report(valid_lr4[xvad_indx], validation_pred_lr4[xvad_indx]))\nprint(c_matrix(valid_lr4[xvad_indx], validation_pred_lr4[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_lr4), np.std(scores_lr4)))","8398f628":"perm = PermutationImportance(clf_lr2, random_state=1).fit(xtrain, label)\neli5.show_weights(perm, top=50)","5c572b60":"top_features = [i[1:] for i in eli5.formatters.as_dataframe.explain_weights_df(perm).feature if 'BIAS' not in i]\nxtrain_lr_perm = train[top_features]\nxtest_lr_perm = test[top_features]\nscaler = StandardScaler()\nxtrain_lr_perm = scaler.fit_transform(xtrain_lr_perm)\nxtest_lr_perm = scaler.transform(xtest_lr_perm)\n\nforcast_lr3 = np.zeros(len(xtest_lr_perm))\nvalidation_pred_lr3 = np.zeros(len(xtrain_lr_perm))\nscores_lr3 = []\nvalid_lr3 = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain_lr_perm, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain_lr_perm[xtrn_indx], xtrain_lr_perm[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_lr3 = linear_model.LogisticRegression(class_weight='balanced', penalty='l1', C=0.1, solver='liblinear')\n    clf_lr3.fit(x_train, y_train)\n    pred_valid = clf_lr3.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_lr3.predict_proba(xtest_lr_perm)[:, 1]\n    \n    validation_pred_lr3[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_lr3.append(accuracy_score(y_valid, pred_valid))\n    forcast_lr3 += Pred_Real\n    valid_lr3[xvad_indx] += (y_valid)\n\nprint(c_report(valid_lr3[xvad_indx], validation_pred_lr3[xvad_indx]))\nprint(c_matrix(valid_lr3[xvad_indx], validation_pred_lr3[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_lr3), np.std(scores_lr3)))","3fa9a563":"from sklearn.ensemble import BaggingClassifier\nforcast_bc = np.zeros(len(xtest))\nvalidation_pred_bc = np.zeros(len(xtrain))\nscores_bc = []\nvalid_bc = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_bc = BaggingClassifier()\n    clf_bc.fit(x_train, y_train)\n    pred_valid = clf_bc.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_bc.predict_proba(xtest)[:, 1]\n    \n    validation_pred_bc[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_bc.append(accuracy_score(y_valid, pred_valid))\n    forcast_bc += Pred_Real\n    valid_bc[xvad_indx] += (y_valid)\n\nprint(c_report(valid_bc[xvad_indx], validation_pred_bc[xvad_indx]))\nprint(c_matrix(valid_bc[xvad_indx], validation_pred_bc[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_bc), np.std(scores_bc)))","ba4b9995":"from sklearn.ensemble import AdaBoostClassifier\nforcast_adac = np.zeros(len(xtest))\nvalidation_pred_adac = np.zeros(len(xtrain))\nscores_adac = []\nvalid_adac = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_adac = AdaBoostClassifier()\n    clf_adac.fit(x_train, y_train)\n    pred_valid = clf_adac.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_adac.predict_proba(xtest)[:, 1]\n    \n    validation_pred_adac[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_adac.append(accuracy_score(y_valid, pred_valid))\n    forcast_adac += Pred_Real\n    valid_adac[xvad_indx] += (y_valid)\n\nprint(c_report(valid_adac[xvad_indx], validation_pred_adac[xvad_indx]))\nprint(c_matrix(valid_adac[xvad_indx], validation_pred_adac[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_adac), np.std(scores_adac)))","54578211":"from sklearn.ensemble import GradientBoostingClassifier\nforcast_gbc = np.zeros(len(xtest))\nvalidation_pred_gbc = np.zeros(len(xtrain))\nscores_gbc = []\nvalid_gbc = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    print('Fold {}, started at {}'.format(fold_, time.ctime()))\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_gbc = GradientBoostingClassifier()\n    clf_gbc.fit(x_train, y_train)\n    pred_valid = clf_gbc.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_gbc.predict_proba(xtest)[:, 1]\n    \n    validation_pred_gbc[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_gbc.append(accuracy_score(y_valid, pred_valid))\n    forcast_gbc += Pred_Real\n    valid_gbc[xvad_indx] += (y_valid)\n\nprint(c_report(valid_gbc[xvad_indx], validation_pred_gbc[xvad_indx]))\nprint(c_matrix(valid_gbc[xvad_indx], validation_pred_gbc[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_gbc), np.std(scores_gbc)))","59faa89c":"cat_params = {'learning_rate': 0.02,\n              'depth': 5,\n              'l2_leaf_reg': 10,\n              'bootstrap_type': 'Bernoulli',\n              'od_type': 'Iter',\n              'od_wait': 50,\n              'random_seed': 11,\n              'allow_writing_files': False}\nforcast_catc = np.zeros(len(xtest))\nvalidation_pred_catc = np.zeros(len(xtrain))\nscores_catc = []\nvalid_catc = np.zeros(len(label))\nfor fold_, (xtrn_indx, xvad_indx) in enumerate(folds.split(xtrain, label)):\n    x_train, x_valid = xtrain[xtrn_indx], xtrain[xvad_indx]\n    y_train, y_valid = label[xtrn_indx], label[xvad_indx]\n    \n    clf_catc = CatBoostClassifier(iterations=400, **cat_params)\n    clf_catc.fit(x_train, y_train)\n    pred_valid = clf_catc.predict(x_valid).reshape(-1,)\n    score = accuracy_score(y_valid, pred_valid)\n    Pred_Real = clf_catc.predict_proba(xtest)[:, 1]\n    \n    validation_pred_catc[xvad_indx] += (pred_valid).reshape(-1,)\n    scores_catc.append(accuracy_score(y_valid, pred_valid))\n    forcast_catc += Pred_Real\n    valid_catc[xvad_indx] += (y_valid)\n\nprint(c_report(valid_catc[xvad_indx], validation_pred_catc[xvad_indx]))\nprint(c_matrix(valid_catc[xvad_indx], validation_pred_catc[xvad_indx]))\nprint('accuracy is: {}, std: {}.'.format(np.mean(scores_catc), np.std(scores_catc)))","ce99c6a2":"sample_submission['target'] = forcast_lr4\nsample_submission.to_csv('Forcasting.csv', index=False)\nsample_submission.head(20)","2860660c":"Using ELI5 and permutation importance helps us to know how weights are been ascribed to each features in this Lr model.\nThe code below shows the feature importance of the model.\n\nThe first is using elif while the second is using permutation importance.","57d99c9a":"We quickly checked the features with non zero weight. 38 features has weight from all the features.","79ad1711":"I will now seperate the Label data and find the type of classification we are dealing with.\n\nFrom the code below, we noticed that the target is a binary target with some level of unbalance. The columns are more similar","484cb8f6":"**AdaBoostClassifier**","aa76425a":"I will like to reduce the dimmension of my data to ease visualization. I will be using principal component anaalysis to achieve this. PCA is essentially a method that reduces the dimention of the feature space in such a way that new variables are orthogonal to each other.\n\nTo use PCA, we need to apply scale standard to unitarilize our units of dimention.\n\nI will first plot a PCA graph to determine the number of components we need in apply PCA. This plot tells us that selecting 205 components we will preserve something around 96 percent of the total variance of the data. Not using all the component  makes us to use only the principle ones. I will now go forward by using the components suggested for me by PCA. ","4975fc6a":"I will briefly show the density plot of the columns to have a better understanding on how the columns are distributed.\n\nAll the columns are normally distributed with a shape peak.","20ebd631":"Now we will use Kfold cross validation to compare with our leave one out cross validation","e736d03b":"I will create a function to check missing variables and also to show the type of variables. I noticed that theirs is no missing values and the variables are all floats.","9b747ac7":"This reduces the score. I guess it does not work for us.","0e0b7a5f":"**INTRODUCTION**\n\nThe concept of overfitting can be equalized to a porpular phenomenon of our domestic Tailors or cloth designers. One day as it may, a customer who initially asked his tailor for a special suit design visited the said Tailor for his ceremonial suit. In pursuit of customer certisfaction in perfecting the sizing, the Tailor in the cause of measuring the customer, he added 2inches each to the size of the customer. The customer in his curiosity ask the reason for the strange action. The Tailor replied in a relaxed voice; if it is your exact size, the suit will too fit(Overfit) and if shorter the suit will not fit (Underfit). If at a certian range, the suit will be moderate (Fit).\n\nThe margin between Overfit and Underfit is a narrow path through the needles hole. Only the sage and the experienced lives their. Overfiting is when model learns and memories both details and noise during training. It is the perfectly fit illustrated in our story. The main effect of this is that it has a negative effect on a new data.\n\nSome of the few methods in solving this methods are:\n\n1. Training with more data\n\n2. Cross-Validation\n\n3. Removal of irrelevant features\n\n4. Early stopping\n\n5. Ensembling\n\n6. Regularization\n\nIn this kernel, I will be treating the data provided some of this solving skills. Four models will be practiced and prediction will be made. I will also be during my first NN model here.\n\nI just hope this kernel serves you well and solve some questions in your mind.\n\nNow lets get the ball rolling..............","666c6eb8":"to be continued...........","c20fb06b":"I will like to print the shape of the files to know the topography of my data. The train data has 250 rows with 302 features while test data has 19750 rows with 301 features. Their is unbalance and the train data is too small. Let us dig deep into the data.","fa7acd32":"**GradientBoostingClassifier**","0e51ca04":"**svm**","edf1eac0":"**RandomForestClassifier with KFold cross validation**","ed2e567f":"**DecisionTreeClassifier**","c3c09012":"**CatBoostingClassifier**","762b250c":"wow we have an increase in the score of the model from 0.708 to 0.74 by using the model to explain the weight......this is a good improvement but yet we need to check on the leaderboard.\n\nNow let us try permutation importance to explain the weight.","6c03898f":"We will now use the top features with great importance to train our model and also to predict to see the diffence with the former moddel.","71261e07":"**Preparing Data for Analysis**\n\nLoading Packages","dbbf8713":"**Data Exploration**\n\nNow we can see that the train set has id and target while the test set has the id row. If this two are removed the the data will be equal in features(columns). Also the data are all in variables with no alphabets. We need to go further in checking the missing values and seperating the target as label.","0eeaf7f5":"**Loading Data**\n\nWe will check the data files that are available and also print its formats. This helps us to know the real data we are working with. The nature of the data and how the data was collated. This is very crucial in for data cleansing and visualisation. The first code below is to access the location of the file and to check for the available file.\n\nThe data contains 20000 rows of continues variable and mere handful of training sample (250). This is a problem as it sounds. We will be faced with small sample data that can already cause overfit.","3c715282":"This shows that we are dealing with an unbalance data set. The data for training is samll and unbalance.","ae93877c":"**LogisticRegression**","f69cac83":" I will now creat a function called describe_data, this will describe our data. We will know waht exactly the problem is in this function. this function will show us the mean, std, counts and max and min of each measurement. it seems the measurement are categorised into series.","a2f18033":"**BaggingClassifier**"}}