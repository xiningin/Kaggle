{"cell_type":{"27099b94":"code","fc156ee5":"code","302f6c93":"code","5a6a2c7b":"code","04f34bfd":"code","1302242c":"code","3974172c":"code","a998e1f9":"code","b45c8486":"code","b2271c17":"code","97506f56":"code","1e3c2559":"code","4b5074e7":"code","8e2c045e":"markdown","d6e58537":"markdown","71beb73f":"markdown","f1168ca8":"markdown","505556b4":"markdown"},"source":{"27099b94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc156ee5":"df = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv').drop('Serial No.', axis=1)\ndf.head()","302f6c93":"df.isna().sum()","5a6a2c7b":"df=df.dropna()","04f34bfd":"train_data = df.sample(frac=0.8, random_state=0)\ntest_data = df.drop(train_data.index)","1302242c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig =plt.figure(figsize=[12,12])\nsns.heatmap(df.corr(), annot=True)","3974172c":"train_scores = train_data[df.columns[:-1]]\ntrain_chances = train_data[df.columns[-1]]\n\ntest_scores = test_data[df.columns[:-1]]\ntest_chances = test_data[df.columns[-1]]\ntrain_scores.head()","a998e1f9":"import tensorflow as tf\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing","b45c8486":"normalizer = preprocessing.Normalization()\nnormalizer.adapt(np.array(train_scores))\nprint(normalizer.mean.numpy())","b2271c17":"model = tf.keras.Sequential([\n    normalizer,\n    layers.Dense(64, activation='relu'),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\nmodel.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n             loss='mean_absolute_error')\nmodel.summary()","97506f56":"history = model.fit(train_scores, train_chances,\n                   epochs=20,\n                    validation_data=(test_scores, test_chances),\n                   verbose=1)","1e3c2559":"loss=history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs=range(len(loss))\n\nplt.plot(epochs, loss, 'r', label=\"Traing loss\")\nplt.plot(epochs, val_loss, 'b', label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","4b5074e7":"test_predictions = model.predict(test_scores).flatten()\na = plt.axes(aspect='equal')\nplt.scatter(test_chances, test_predictions)\nplt.xlabel('True Values [Chance of Admit]')\nplt.ylabel('Predictions [Chance of Admit]')\nlims = [0, 1]\nplt.xlim(lims)\nplt.ylim(lims)\n_ = plt.plot(lims, lims)","8e2c045e":"From the scatter plot, we can see the the predictions are quite close to the true values as the dots are close to the line x=y","d6e58537":"# Build a dnn_model with tensorflow","71beb73f":"# Get the data","f1168ca8":"# Split the data to train set and test set to evaluate","505556b4":"### Making prediction"}}