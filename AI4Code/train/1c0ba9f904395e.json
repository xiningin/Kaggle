{"cell_type":{"dbcec89c":"code","1b6564d2":"code","a1978b40":"code","32354f65":"code","b45b93d6":"code","8dcfbcd0":"code","6b8b74fd":"code","8f86806e":"code","633e35e4":"code","f82ba089":"code","fdac5605":"code","36ff0990":"code","167a0010":"code","e1cd70a3":"code","ed5d6e52":"code","0c6f74d1":"code","ab05b803":"code","32698871":"code","6c505e26":"code","ff2b057a":"code","1eaf48b2":"code","a660596f":"code","561b35e4":"code","34e4e774":"code","52804107":"code","f7d75787":"code","dc2c7595":"code","003fb60a":"code","b461f59a":"code","b0da20e3":"code","b28db524":"code","d5034b4d":"code","0d3d1cd0":"code","3863d7f2":"code","05b513d9":"code","7dcd1fd2":"code","6e196d72":"code","87c8ea2c":"code","fb7ece1b":"code","1260706d":"code","6866e8de":"code","0f93bb77":"code","1dbf3bdc":"code","8e89531a":"code","edfafd53":"markdown","68b34ec2":"markdown"},"source":{"dbcec89c":"!pip install dataprep by","1b6564d2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\nimport seaborn as sns\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a1978b40":"df=pd.read_csv('..\/input\/glass\/glass.csv')\ndf.head()","32354f65":"df.info()","b45b93d6":"df.describe()","8dcfbcd0":"print(df.shape)","6b8b74fd":"df.columns.tolist()","8f86806e":"df","633e35e4":"df.isnull().sum()","f82ba089":"df['Type'].value_counts()","fdac5605":"df['Type'].value_counts().sort_index(ascending=True)","36ff0990":"plt.figure(figsize=(10,10))\nsns.countplot(x='Type', data=df, order=df['Type'].value_counts().index);","167a0010":"#outlier\nplt.figure(figsize=(10,10))\nsns.boxplot(data=df, orient=\"h\");\n","e1cd70a3":"# Pearson Correlation\nplt.figure(figsize=(18,10))\nsns.heatmap(df.corr(method='pearson'), cbar=False, annot=True, fmt='.1f', linewidth=0.2, cmap='coolwarm');","ed5d6e52":"plot(df)","0c6f74d1":"plot(df, 'RI')","ab05b803":"plot(df, 'Na')","32698871":"plot(df, 'Mg')","6c505e26":"plot(df, 'Al')","ff2b057a":"plot(df, 'Si')","1eaf48b2":"plot(df, 'K')","a660596f":"plot(df, 'Ca')","561b35e4":"plot(df, 'Ca')","34e4e774":"plot(df, 'Ba')","52804107":"plot(df, 'Fe')","f7d75787":"plot(df, 'Type')","dc2c7595":"# Missing values\nplot_missing(df)","003fb60a":"create_report(df)","b461f59a":"df['Type'].value_counts()\n\ndf['Type'].value_counts() * 100 \/ len(df)\n\n\nsns.countplot(x='Type', data=df, palette='viridis')","b0da20e3":"x = df.loc[:,[\"RI\",\"Na\",\"Mg\",\"Al\",\"Si\",\"K\",\"Ca\",\"Ba\",\"Fe\"]]\ny = df.loc[:,[\"Type\"]]\nprint(x,y)","b28db524":"from imblearn.over_sampling import SMOTE\n\nx = df.drop('Type', axis=1)\ny = df['Type']\n\n# setting up testing and training sets\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.25, random_state=27)\n\n","d5034b4d":"X_train","0d3d1cd0":"print(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","3863d7f2":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","05b513d9":"# I uesd PCA But not work good result \n#from sklearn.decomposition import PCA\n#pca = PCA(n_components=2)\n#pca.fit(X_train)\n#X_train = pca.transform(X_train)\n#X_test = pca.transform(X_test)","7dcd1fd2":"def models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n    \n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n \n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n  forest.fit(X_train, Y_train)\n  \n    \n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n    \n    \n    \n    \n #Using xgboostClassifier of tree class to use Decision Tree Algorithm\n  from xgboost import XGBClassifier \n  xgboost = XGBClassifier(max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005)\n  xgboost.fit(X_train, Y_train)\n    \n    \n    \n #Using  SGDClassifierr of tree class to use Decision Tree Algorithm    \n  from sklearn.linear_model import SGDClassifier\n  SGD = SGDClassifier()\n  SGD.fit(X_train, Y_train)\n    \n    \n  #Using  AdaBoostClassifier of tree class to use Decision Tree Algorithm    \n  from sklearn.ensemble import AdaBoostClassifier\n  Ada = AdaBoostClassifier(n_estimators=2000, random_state = 0)\n  Ada.fit(X_train, Y_train)\n   \n\n\n\n  #Using  GradientBoostingClassifier of tree class to use Decision Tree Algorithm    \n  from sklearn.ensemble import GradientBoostingClassifier\n  clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1)\n  clf.fit(X_train, Y_train) \n\n#####\n  \n\n #Using Quadratic Discriminant Analysis of tree class to use Decision Tree Algorithm    \n  from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis \n  QDA = QuadraticDiscriminantAnalysis ()\n  QDA.fit(X_train, Y_train)\n    \n\n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train)*100)\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train)*100)\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train)*100)\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train)*100)\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train)*100)\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train)*100)\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train)*100)\n  print('[7]Xgboost Classifier Training Accuracy:', xgboost.score(X_train, Y_train)*100)\n  print('[8]SGD Classifier Training Accuracy:', SGD.score(X_train, Y_train)*100)\n  print('[9]AdaBoost Classifier Training Accuracy:', Ada.score(X_train, Y_train)*100)\n  print('[10]GradientBoosting Classifier Training Accuracy:', clf.score(X_train, Y_train)*100)\n  print('[11]Quadratic Discriminant AnalysisTraining Accuracy:', QDA.score(X_train, Y_train)*100)\n\n  return log, knn, svc_lin, svc_rbf, gauss,tree,forest,xgboost,SGD,Ada,clf,QDA\n\nmodel = models(X_train,Y_train)","6e196d72":"#Show the confusion matrix and accuracy for all of the models on the test data\n#Classification accuracy is the ratio of correct predictions to total predictions made.\nfrom sklearn.metrics import confusion_matrix\nfor i in range(len(model)):\n  cm = confusion_matrix(Y_test, model[i].predict(X_test))\n  TN = cm[0][0]\n  TP = cm[1][1]\n  FN = cm[1][0]\n  FP = cm[0][1]\n  print(cm)\n  print('Model[{}] Testing Accuracy = \"{}!\"'.format(i,  (TP + TN) \/ (TP + TN + FN + FP)))\n  print()# Print a new line\n\n#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print( accuracy_score(Y_test, model[i].predict(X_test)))\n  print()#Print a new line","87c8ea2c":"from sklearn.metrics import max_error\nfor i in range(len(model)):\n  print('Model ',i)\n\n  print( max_error(Y_test, model[i].predict(X_test)))","fb7ece1b":"from sklearn.metrics import mean_absolute_error\n\nfor i in range(len(model)):\n  print('Model ',i)\n\n  print( mean_absolute_error(Y_test, model[i].predict(X_test)))\n","1260706d":"from sklearn.metrics import mean_squared_log_error\n\nfor i in range(len(model)):\n  print('Model ',i)\n\n  print( mean_squared_log_error(Y_test, model[i].predict(X_test)))","6866e8de":"from sklearn.metrics import multilabel_confusion_matrix\n\n\nfor i in range(len(model)):\n  print('Model ',i)\n\n  print( multilabel_confusion_matrix(Y_test, model[i].predict(X_test)))\n\n","0f93bb77":"from sklearn.metrics import confusion_matrix\n\nfor i in range(len(model)):\n  print('Model ',i)\n\n  print( confusion_matrix(Y_test, model[i].predict(X_test)))","1dbf3bdc":"for i in range(len(model)):\n  print('Model ',i)\n\n  print( confusion_matrix(Y_test, model[i].predict(X_test), normalize='all'))\n","8e89531a":"acc_1 = 0.703703*100 \nacc_2 = 0.685185*100\nacc_3 = 0.666666*100\nacc_4 = 0.759259*100\nacc_5 = 0.537037*100\nacc_6 = 0.648148*100\nacc_7 = 0.796296*100\nacc_8 = 0.814814*100\nacc_9 = 0.592592*100\nacc_10 = 0.61111*100\nacc_11 = 0.81481*100\nacc_12 = 0.64814*100\nresults = pd.DataFrame([[\"Logistic Regression\",acc_1],[\"Nearest Neighbor\",acc_2],[\"Support Vector Machine (Linear Classifier)\",acc_3],\n                       [\"Support Vector Machine (RBF Classifier)\",acc_4],[\"Gaussian Naive Bayes\",acc_5],[\"Decision Tree Classifier\",acc_6],\n                       [\"Random Forest Classifier\",acc_7],[\"Xgboost Classifier\",acc_8],[\"SGD Classifier \",acc_9],[\"AdaBoost Classifier\",acc_10],\n                        [\"GradientBoosting Classifier\",acc_11],[\"Quadratic Discriminant Analysis\",acc_12],\n                       ],columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\nresults.style.background_gradient(cmap='Blues')","edfafd53":"### Description\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/738\/1370\/0a1267f68de353bec843f056c8343009\/dataset-card.jpg\" width=\"600px\">\n\n\nAttribute Information:\n\n1. Id number: 1 to 214 (removed from CSV file)\n2. RI: refractive index\n3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n4. Mg: Magnesium\n5. Al: Aluminum\n6. Si: Silicon\n7. K: Potassium\n8. Ca: Calcium\n9. Ba: Barium\n10. Fe: Iron\n11. Type of glass: (class attribute):\n* 1 buildingwindowsfloatprocessed \n* 2 buildingwindowsnonfloatprocessed \n* 3 vehiclewindowsfloatprocessed\n* 4 vehiclewindowsnonfloatprocessed (none in this database)\n* 5 containers\n* 6 tableware\n* 7 headlamps\n\n\n#### Dataset Link \n\n##### [Here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Glass+Identification)","68b34ec2":"<h1 style='background-color:#87CEEB; font-family:newtimeroman; font-size:210%; text-align:center; border-radius: 15px 50px;' > Glass Classification EDA+12 Algorithms  <\/h1>\n\n### We used 12 algorithms Classifier\n\n* Logistic Regression\n* K Nearest Neighbor Classifier\n* Support Vector Machine (Linear Classifier)\n* Support Vector Machine (RBF Classifier)\n* Gaussian Naive Bayes Classifier\n* Decision Tree Classifier\n* Random Forest Classifier\n* Xgboost Classifier\n* SGD Classifier \n* AdaBoost Classifier\n* Gradient Boosting Classifier\n* Quadratic Discriminant Analysis\n\n\n\n\n<img src=\"http:\/\/static1.squarespace.com\/static\/56f31e1745bf21a9d5220d2e\/583c89ec6a49631ee35b5a01\/5b0f50ef575d1f1f43aba45c\/1527730690236\/machine+learning+hub101.jpeg?format=1500w\" width=\"600px\">\n\n\n"}}