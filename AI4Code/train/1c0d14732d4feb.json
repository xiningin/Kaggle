{"cell_type":{"dc47d96d":"code","2b46b96e":"code","55a45333":"code","2a45b806":"code","33503b8f":"code","eab0e869":"code","f8a0955d":"code","7939de44":"code","7edc4130":"code","92a092f1":"code","518b2f3e":"code","2cd51004":"code","50faceb6":"code","11356f60":"code","85ec6197":"code","ce3e01ce":"code","640ef91d":"code","629d80a6":"code","3c24ba5a":"code","63b3668b":"code","48f10083":"markdown","ddd84ceb":"markdown","00038578":"markdown","9ce50e78":"markdown","17018d73":"markdown","5c15eb71":"markdown","f324ec7f":"markdown","183ca165":"markdown","debfd3a1":"markdown","a6117e81":"markdown","9c74d506":"markdown","43c1e317":"markdown","a209a637":"markdown","88dcb4a2":"markdown","36753ab8":"markdown","942c2a3b":"markdown","90adb1f7":"markdown","6f946c36":"markdown","33bf18e8":"markdown","132db753":"markdown","e8a72d9b":"markdown"},"source":{"dc47d96d":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nimport graphviz \nimport numpy as np","2b46b96e":"dataset = pd.read_csv(\"..\/input\/StudentsPerformance.csv\")\ndataset.head()","55a45333":"ax = sns.countplot(x=\"gender\", data=dataset)","2a45b806":"ax = sns.countplot(x=\"race\/ethnicity\", data=dataset)","33503b8f":"ax = sns.countplot(y=\"parental level of education\", data=dataset)","eab0e869":"ax = sns.countplot(x=\"lunch\", data=dataset)","f8a0955d":"ax = sns.countplot(x=\"test preparation course\", data=dataset)","7939de44":"ax = sns.distplot(dataset[\"math score\"]);","7edc4130":"dataset[\"math grade\"] = \"\"\ndataset.loc[(dataset[\"math score\"] >= 60), \"math grade\"] = \"Pass\"\ndataset.loc[(dataset[\"math score\"] < 60), \"math grade\"] = \"Fail\"\ndataset.drop(columns=['math score', 'reading score', 'writing score'], inplace=True)\ndataset.head()","92a092f1":"one_hot = pd.get_dummies(dataset['gender'], prefix='gender', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['race\/ethnicity'], prefix='race\/ethnicity', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['parental level of education'], prefix='parental level of education', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['lunch'], prefix='lunch', drop_first=True)\ndataset = dataset.join(one_hot)\none_hot = pd.get_dummies(dataset['test preparation course'], prefix='test preparation course', drop_first=True)\ndataset = dataset.join(one_hot)\ndataset.head()","518b2f3e":"data_train, data_test = train_test_split(dataset, test_size=0.20, random_state=21)","2cd51004":"columns_move = [\"gender\", \"race\/ethnicity\", \"parental level of education\", \"lunch\", \"test preparation course\", \"gender_male\", \"race\/ethnicity_group B\", \"race\/ethnicity_group C\", \"race\/ethnicity_group D\", \"race\/ethnicity_group E\", \"parental level of education_bachelor's degree\", \"parental level of education_high school\", \"parental level of education_master's degree\", \"parental level of education_some college\", \"parental level of education_some high school\", \"lunch_standard\", \"test preparation course_none\"]","50faceb6":"y_train = data_train[\"math grade\"].values\nX_train = data_train[columns_move].values\ny_test = data_test[\"math grade\"].values\nX_test = data_test[columns_move].values","11356f60":"model = DecisionTreeClassifier(criterion='gini', splitter='best', \n                               max_depth=None, min_samples_split=2, \n                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                               max_features=None, random_state=None, \n                               max_leaf_nodes=None, min_impurity_decrease=0.0, \n                               min_impurity_split=None, class_weight=None, \n                               presort=False)","85ec6197":"model.fit(X_train[:,5:], y_train)","ce3e01ce":"y_pred = model.predict(X_test[:,5:])\nprint(\"Model Accuracy (%):\", accuracy_score(y_test,y_pred)*100)","640ef91d":"dot_data = tree.export_graphviz(model, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"tree_structure\") ","629d80a6":"columns_move.append(\"math grade test\")\ncolumns_move.append(\"math grade pred\")","3c24ba5a":"y_pred = y_pred.reshape(len(y_pred),1)\ny_test = y_test.reshape(len(y_test),1)\nresultarray = np.append(X_test, y_test, axis=1)\nresultarray = np.append(resultarray, y_pred, axis=1)\nresultdf = pd.DataFrame(resultarray, columns=columns_move)","63b3668b":"resultdf.drop(columns=[\"gender_male\", \"race\/ethnicity_group B\", \"race\/ethnicity_group C\", \"race\/ethnicity_group D\", \"race\/ethnicity_group E\", \"parental level of education_bachelor's degree\", \"parental level of education_high school\", \"parental level of education_master's degree\", \"parental level of education_some college\", \"parental level of education_some high school\", \"lunch_standard\", \"test preparation course_none\"], inplace=True)\nresultdf.head(20)","48f10083":"### Create Numerical Arrays for the Model","ddd84ceb":"### Visualizations","00038578":"### Predictions","9ce50e78":"There is no tuning for the model. It is created with the default values of the DecisionTreeClassifier. That is why it will be very very big overfitting decision tree :).","17018d73":"Lets create a new feature, named as math grade. Math grade is Pass if math score is above 60, math grade Fail if math score is below 60. It is just an assumption. After creating the new column, I have removed the unneccasry columns and check the latest form of dataset again.\n\n#### *Hint: it is a good example for changing some rows of a column according to some conditional expressions on values of other columns. ","5c15eb71":"### Training the Model","f324ec7f":"### One Hot Encoding","183ca165":"### Combined Result Data Frame with Predictions","debfd3a1":"It is the first notebook of a series about decision trees. The followings are included in this first one:\n\n- Basic visualization of data\n\n- One Hot Encoding\n\n- Modelling a decision tree with default parameters (without any tuning)\n\n- Creating a pdf file to present the decision tree\n\n- Small hints (e.g. why drop_first should be applied after one hot encoding?, why one hot encoding is required for a decision tree?)\n\nIn later notebooks following topics will be explained in detail:\n\n- Mechanism of decision tree models\n\n- Parameter explanation and tuning of a decision tree\n\n- Pruning details for a decision tree\n\n- Applying Random Forest for the same data\n\n- Applying XGBoost for the same data\n\n- Applying Light GBM for the same data\n\n- Comparison of decision tree models applied\n","a6117e81":"### Train - Test Split","9c74d506":"### Data Processing","43c1e317":"### Explanation of the Study","a209a637":"A classification decision tree is modelled to predict the success in math of a student depending on the features (gender, race\/ethnicity, parental level of education, lunch, test preparation course). \n\nModel is established by using just default parameters that is why the tree is very big and most probably overfitting and accuracy rate is 43%.\n\nTuning is the topic of other notebooks.","88dcb4a2":"#### *Hint: One hot encoding is necessary if you use scikit-learn library for modelling. \n\nScikit-learn uses only numerical features and these numerical features are considered as continuos numeric values. In our case race feature includes group A, group B, group C, group D, group E and it will be encoded as 1,2,3,4,5 under one column if you use just label encoding not one hot encoding. Till now everything is fine but problem starts here. Since model assumes that all numerical values are continuous, you will see such internal nodes (splitting point): \"race < 4 which means group A race < group B race\" which is very weird. Weird because there is no continouity among race values. It would be the same if our feature was color. It may be acceptable for parental level of education if you set the order correctly because there is a continuity. Of course master's degree is higher than bachelor's degree and it is higher than some college etc. That is why we have to apply one hot encoding if we use scikit-learn. \n\n#### *Hint: Dropping one of the one hot encoded columns are a good approach otherwise weigths and effects of features over the model may be higher or lower. \n\nAssume that in our dataset we have only age and gender features. Since age is continous numeric variable that is good for scikit-learn but we have to apply one hot encoding for gender. After applying it we have 3 features, age, gender_male, gender_female. Lets give an example, lets say our person is male so as values gender_male is 1 and gender_female is 0 and lets say age is 20. That sounds ok but not actually. Lets say you are talking with the model and you check age feature and tell \"this person is 20 years old\". And then you check gender_male feature and tell \"this person is male\". And then you check gender_female column and tell \"this person is not a female (which means person is a male)\". Just to tell the model once about the person's gender, you have to drop one of the one hot encoded columns and drop_first=True makes it for us.\n","36753ab8":"### Abstract","942c2a3b":"Splitting the dataset into two datasets as train and test datasets. The ratio is 80\/20. \n\n#### *Hint: random_state parameter is used if you want to have exactly the same train and test datasets and if you don't want them to be different in every splitting when you run your notebook from the beginning. It doesn't matter which number, you can set 5, 77, 100 whatever. The only important point is to set the same number. For me it is 21 in this case.","90adb1f7":"### Graph of Decision Tree","6f946c36":"## Decision Tree Adventures 1 - Simplest Case","33bf18e8":"### Loading and Displaying the Dataset","132db753":"### Create the Model","e8a72d9b":"### Importing Libraries"}}