{"cell_type":{"d4f73afd":"code","fa0b980d":"code","807222c8":"code","34df5213":"code","659eaafa":"code","69cf0957":"code","f7eac57d":"code","84edaf75":"code","8ba22e1f":"code","061b0963":"code","c60f5e9e":"code","aad207d2":"code","dbaa90a5":"code","864cd5c5":"code","2dfff0b0":"code","c4592584":"code","4d5aabd9":"code","004ce1f5":"code","7349c396":"code","182ae46a":"code","b85a3f31":"code","70e44930":"code","8c516716":"code","f173e685":"code","b33d2eaa":"code","7edb935f":"code","788193a4":"code","b14679d1":"code","1bd3d953":"code","6e5cdb86":"code","577e6739":"code","8c7779d0":"code","bc5ec885":"code","6b00c7a1":"code","bd624a8f":"code","3bc4b43e":"code","89bc9e8e":"code","1f3d0927":"code","fea8cf96":"code","5cb7ae5b":"code","f6bf1ccd":"code","982a8ca6":"code","d5b4488f":"code","50aea599":"code","7c9ac0ec":"code","67ea3386":"code","f816ec2b":"code","8bb4b298":"code","aa0d9632":"code","e1d1187d":"code","386117be":"code","49f83e34":"code","9d7a330a":"code","cf4b2b12":"code","f3ac27f4":"code","de291049":"code","717e8d19":"code","a34764e3":"code","5162704c":"code","791ba13b":"code","b2c63447":"code","d6a2c226":"code","85c12de5":"code","9d0f41d1":"code","dedd47f0":"code","8eac46b9":"code","afd536ab":"code","ee9760a4":"code","97c833ab":"code","2501e938":"code","09321b5e":"code","235e8f66":"code","ff7320dd":"code","a9b6f628":"code","3f10db7e":"code","06cbae94":"code","73fd79fa":"code","d7a9ef25":"code","6ba7e68e":"code","49d0429c":"code","d8b2ceac":"code","c1d47e2d":"code","c0e996a6":"code","044015f1":"code","0396b392":"markdown","5ebcfed5":"markdown","3ee36a24":"markdown","4792b744":"markdown","ca494f46":"markdown","b34e067d":"markdown","b88dc673":"markdown","0748d780":"markdown","50644ad7":"markdown","621a997c":"markdown","deeab2c4":"markdown"},"source":{"d4f73afd":"import numpy as np\nimport pandas as pd\nimport json\nfrom tqdm import tqdm\nfrom transformers import pipeline\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom flashtext import KeywordProcessor\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\nfrom nltk import tokenize\nfrom transformers import AdamW\nfrom collections import Counter\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import AutoTokenizer, AutoConfig, TrainingArguments, Trainer, EvalPrediction\nimport sys\nimport time\nimport gc\nimport pickle\nimport re","fa0b980d":"# !pip install GPUtil\n# import torch\n# from GPUtil import showUtilization as gpu_usage\n# from numba import cuda\n\n# def free_gpu_cache():\n#     print(\"Initial GPU Usage\")\n#     gpu_usage()                             \n\n#     torch.cuda.empty_cache()\n\n#     cuda.select_device(0)\n#     cuda.close()\n#     cuda.select_device(0)\n\n#     print(\"GPU Usage after emptying the cache\")\n#     gpu_usage()","807222c8":"!nvidia-smi ","34df5213":"train_data = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsubmission_file = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')","659eaafa":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","69cf0957":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","f7eac57d":"print('Train data shape:', train_data.shape)\ntrain_data.head()","84edaf75":"file_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/0008656f-0ba2-4632-8602-3017b44c2e90.json'\nfile_id = file_path.split('\/')[-1].split('.json')[0]\nwith open(file_path) as json_file:\n    data = json.load(json_file)","8ba22e1f":"print('Number of elements:', len(data))\ncombined_text = ' '.join([x['text'] for x in data])","061b0963":"len(combined_text), combined_text[0:1000]","c60f5e9e":"print('Dataset label:', train_data[train_data['Id']==file_id]['dataset_label'].iloc[0],'\\n')\nstart_index = combined_text.find(train_data[train_data['Id']==file_id]['dataset_label'].iloc[0])\nprint('Text for the dataset label\\n',combined_text[start_index-100:start_index+100])","aad207d2":"train_data[train_data['Id']==file_id]['pub_title'].iloc[0],train_data[train_data['Id']==file_id]['dataset_title'].iloc[0],train_data[train_data['Id']==file_id]['dataset_label'].iloc[0],train_data[train_data['Id']==file_id]['cleaned_label'].iloc[0]","dbaa90a5":"print('Train data shape:', train_data.shape)\nprint('Number of unique IDs in train data:', train_data['Id'].nunique())\nprint('Number of unique pub_titles:', train_data['pub_title'].nunique())\nprint('Number of unique dataset titles:', train_data['dataset_title'].nunique())\nprint('Number of unique dataset labels:', train_data['dataset_label'].nunique())\nprint('Number of unique cleaned labels:', train_data['cleaned_label'].nunique())","864cd5c5":"print(submission_file.shape,submission_file['Id'].nunique())\nsubmission_file","2dfff0b0":"# Check if problem is NER (named entity recognition) - dataset label should be somewhere in text for each row\n\nnumber_of_elements_in_text = pd.DataFrame(data = None, columns = ['Train Id','Number of elements','Total document length (words)','Total document length (char)','start_index in text for dataset label','start_index in section_title for dataset label'])\nall_train_ids = train_data['Id'].unique().tolist()\ncount = 0\nfor i in tqdm(all_train_ids):    \n    file_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/\/' + i + '.json'\n    file_id = file_path.split('\/')[-1].split('.json')[0]\n    with open(file_path) as json_file:\n        data = json.load(json_file)\n        \n    combined_section_title = ' '.join([x['section_title'].strip().lower() for x in data])\n    combined_text = ' '.join([x['text'].strip().lower() for x in data])\n    \n    try:\n        start_index_text = combined_text.find((train_data[train_data['Id']==file_id]['dataset_label'].iloc[0]).lower().strip())\n    except:\n        start_index_text = np.NaN\n        \n    try:\n        start_index_section_title = combined_section_title.find((train_data[train_data['Id']==file_id]['dataset_label'].iloc[0]).lower().strip())\n    except:\n        start_index_section_title = np.NaN\n    \n    \n    number_of_elements_in_text.loc[count] = i, len(data), len(combined_text.strip().split(' ')), len(combined_text.strip()), start_index_text, start_index_section_title\n    count = count + 1    ","c4592584":"len(all_train_ids)","4d5aabd9":"print(number_of_elements_in_text.shape)\nnumber_of_elements_in_text.head()","004ce1f5":"print('Percentage of documents with dataset label in text:', number_of_elements_in_text[(number_of_elements_in_text['start_index in text for dataset label']!=-1)].shape[0]\/number_of_elements_in_text.shape[0] * 100,'%')","7349c396":"number_of_elements_in_text[(number_of_elements_in_text['start_index in text for dataset label']==-1)].shape","182ae46a":"# All dataset labels occur in the text\nnumber_of_elements_in_text[(number_of_elements_in_text['start_index in text for dataset label']==-1) & (number_of_elements_in_text['start_index in section_title for dataset label']!=-1)].shape","b85a3f31":"# Very few dataset labels occur in section_title\nnumber_of_elements_in_text[(number_of_elements_in_text['start_index in section_title for dataset label']!=-1)].shape","70e44930":"number_of_elements_in_text[number_of_elements_in_text['start_index in text for dataset label']==-1].head()","8c516716":"file_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/c9050bc3-2551-4f41-9f40-2851fc705c3c.json'\nfile_id = file_path.split('\/')[-1].split('.json')[0]\nwith open(file_path) as json_file:\n    data = json.load(json_file)","f173e685":"# Full forms and rare dataset names do not occur in text nor in section_title\ntrain_data[train_data['Id']==file_id]","b33d2eaa":"combined_section_title = ' '.join([x['section_title'].strip().lower() for x in data])\ncombined_text = ' '.join([x['text'].strip().lower() for x in data])","7edb935f":"(train_data[train_data['Id']==file_id]['dataset_label'].iloc[0]).strip().lower() in combined_section_title,(train_data[train_data['Id']==file_id]['dataset_label'].iloc[0]).strip().lower() in combined_text","788193a4":"train_data[train_data['Id'].isin(number_of_elements_in_text[number_of_elements_in_text['start_index in text for dataset label']==-1]['Train Id'].unique().tolist())]['dataset_label'].unique()","b14679d1":"df = number_of_elements_in_text[['Train Id','Number of elements']].drop_duplicates()\nprint(df.shape)\nfig = px.histogram(df, x='Number of elements',nbins = 200)\nfig.show()","1bd3d953":"df = number_of_elements_in_text[['Train Id','Total document length (words)']].drop_duplicates()\nprint(df.shape)\nprint('Maximum length:',df['Total document length (words)'].max())\nfig = px.histogram(df, x='Total document length (words)',nbins = 1000)\nfig.show()","6e5cdb86":"train_data_cl_label_summary = train_data.groupby(['cleaned_label']).agg({'Id':'nunique'}).reset_index().rename(columns = {'Id':'Number of documents'}).sort_values(by = 'Number of documents', ascending = False)\ntrain_data_cl_label_summary.head(10)","577e6739":"data_for_plot = train_data_cl_label_summary.head(20).sort_values(by = 'Number of documents')\nplt.barh(data_for_plot.head(20)['cleaned_label'],data_for_plot.head(20)['Number of documents'])","8c7779d0":"train_data['cleaned_label_length'] = train_data['cleaned_label'].apply(lambda x: len(x.strip().split(' ')))","bc5ec885":"data = train_data[['cleaned_label']].drop_duplicates().reset_index(drop = True)\ndata['cleaned_label_length'] = data['cleaned_label'].apply(lambda x: len(x.strip().split(' ')))\ntrain_data_cl_label_len_summary = data.groupby(['cleaned_label_length']).agg({'cleaned_label':'nunique'}).reset_index().rename(columns = {'cleaned_label':'Number of cleaned labels'}).sort_values(by = 'Number of cleaned labels', ascending = False)\ntrain_data_cl_label_len_summary.head(10)","6b00c7a1":"data_for_plot = train_data_cl_label_len_summary.sort_values(by = 'Number of cleaned labels')\nplt.barh(data_for_plot['cleaned_label_length'],data_for_plot['Number of cleaned labels'])","bd624a8f":"print('Minimum length:', train_data_cl_label_len_summary['cleaned_label_length'].min())\nprint('Maximum length:', train_data_cl_label_len_summary['cleaned_label_length'].max())","3bc4b43e":"train_data.head()","89bc9e8e":"labels_for_model = train_data['dataset_label'].str.strip().str.lower().unique().tolist()","1f3d0927":"keywordprocessor = KeywordProcessor()\nkeywordprocessor.add_keywords_from_list(keyword_list=labels_for_model)","fea8cf96":"def tokenize_sentence(x):\n    \"takes in a string and returns tokenized list after special character padded\"\n\n    return [x for x in x.strip().lower().split(\" \") if len(x) > 0]","5cb7ae5b":"def get_tags(sent, ep):\n    '''\n    Input: sent as a sentence tokenized as list of tokens, ep is list of eparker strings (not tokenized)\n    output: tags \n    '''\n    sent = [x.lower() for x in sent]\n    ep_non_nan = False\n    if isinstance(ep, list):\n        ep_non_nan = True\n        ep = [tokenize_sentence(x.lower()) for x in ep]\n    i = 0\n    tag = []\n    if(ep_non_nan):\n        while (i < len(sent)) and (len(ep) > 0):\n            if (len(ep[0]) == 1) and (ep[0][0] == sent[i]):\n                tag.append(\"B\")\n                i = i + 1\n                ep = ep[1:]\n\n            elif (len(ep[0]) > 1) and (ep[0] == sent[i:i + len(ep[0])]):\n                tag = tag + ['B'] + ['I'] * (len(ep[0]) - 1)\n                i = i + len(ep[0])\n                ep = ep[1:]\n\n            else:\n                tag.append(\"O\")\n                i = i + 1\n\n    tag = tag + ['O'] * (len(sent) - len(tag))\n\n    return tag","f6bf1ccd":"def vocab_sent_tokenize_label(sent_tokenized, token_tag):\n    try:\n        vocab_sent_token = []\n        sent_input_ids = []\n        vocab_token_tag = []\n        token_tag_ids = []\n        for sent_token_, tag_ in zip(sent_tokenized, token_tag):\n            _vocab_sent_token = tokenizer.tokenize(sent_token_)\n            _sent_input_ids = [\n                tokenizer.convert_tokens_to_ids(x) for x in _vocab_sent_token\n            ]\n            _vocab_token_tag = [tag_] * len(_vocab_sent_token)\n            _token_tag_ids = [tag2idx[x] for x in _vocab_token_tag]\n\n            vocab_sent_token.extend(_vocab_sent_token)\n            sent_input_ids.extend(_sent_input_ids)\n            vocab_token_tag.extend(_vocab_token_tag)\n            token_tag_ids.extend(_token_tag_ids)\n        return vocab_sent_token, sent_input_ids, vocab_token_tag, token_tag_ids\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","982a8ca6":"def sent_tag_tokenization(data):\n    try:\n        model_data_preprocessing = data.copy(deep=True)\n        model_data_preprocessing['sent_tokenized'] = model_data_preprocessing[\n            'sentence'].progress_apply(tokenize_sentence)\n\n        model_data_preprocessing[\n            'token_tag'] = model_data_preprocessing.progress_apply(\n                lambda x: get_tags(sent=x['sent_tokenized'], ep=x['dataset_label'])\n                if (isinstance(x['sent_tokenized'], list)) else np.nan, axis=1)\n\n        model_data_preprocessing['vocab_sent_tokenized'], model_data_preprocessing[\n            'sent_input_ids'], model_data_preprocessing[\n                'vocab_token_tag'], model_data_preprocessing['token_tag_ids'] = zip(\n                    *model_data_preprocessing.\n                    progress_apply(lambda x: vocab_sent_tokenize_label(\n                        sent_tokenized=x['sent_tokenized'], token_tag=x['token_tag'])\n                                   if isinstance(x['token_tag'], list) else np.nan,\n                                   axis=1))\n        return model_data_preprocessing\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","d5b4488f":"def pad_data(input_ids, token_ids):\n    try:\n        max_token_length = 512\n        attention_mask = []\n        for input_ in tqdm(input_ids):\n            attention_mask.append(torch.ones(len(input_[:max_token_length])))\n\n        padded_attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask,\n                                                                batch_first=True,\n                                                                padding_value=0.0)\n\n        padded_input_ids = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(input_[:max_token_length]) for input_ in input_ids],\n            batch_first=True,\n            padding_value=0.0)\n\n        padded_tags = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(tag_[:max_token_length]) for tag_ in token_ids],\n            batch_first=True,\n            padding_value=0.0)\n        return padded_input_ids, padded_attention_mask, padded_tags\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","50aea599":"def create_dataloader(token_ids, masks, tags, batch_size=16, val=False):\n    try:\n        # wrap tensors\n        data = TensorDataset(token_ids, masks, tags)\n\n        if val:\n            # sampler for sampling the data during training\n            sampler = SequentialSampler(data)\n            \n            # dataLoader for validation set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        else:    \n            # sampler for sampling the data during training\n            sampler = RandomSampler(data)\n            # dataLoader for train set\n            dataloader = DataLoader(data,\n                                    sampler=sampler,\n                                    batch_size=batch_size)\n        return dataloader\n    except Exception as e:\n        print(f\"Error in line no: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","7c9ac0ec":"# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/coleridge-model-data')\ntag2idx = {\n    'O': 0,\n    'B': 1,\n    'I': 2,\n}\n\nidx2tag = {\n    \"0\": \"O\",\n    \"1\": \"B\",\n    \"2\": \"I\"\n}","67ea3386":"training_data_v1 = train_data.groupby(['Id']).agg({'dataset_label':'|'.join}).reset_index()\ntraining_data_v1['dataset_label'] = training_data_v1['dataset_label'].apply(lambda x: x.split('|'))","f816ec2b":"pd.set_option('display.max_colwidth',500)\ntraining_data_v1.head()","8bb4b298":"training_data_v1['sentence'] = np.NaN\nfor i in tqdm(range(0, len(training_data_v1))):\n    text = training_data_v1['Id'].iloc[i]\n    file_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train\/\/' + text + '.json'\n    file_id = file_path.split('\/')[-1].split('.json')[0]\n    with open(file_path) as json_file:\n        data = json.load(json_file)\n\n    training_data_v1['sentence'].iloc[i] = ' '.join([x['text'].strip().lower() for x in data])","aa0d9632":"pd.set_option('display.max_colwidth',500)\ntraining_data_v1.head()","e1d1187d":"# Max token length for BERT is 512. Hence, we need to break down the sentences into smaller groups\n\ncreate_train_data = False # Make this true to create data\n\nif create_train_data:\n    training_data_for_model = pd.DataFrame(data = None, columns = ['Id','sentence'])\n    count = 0\n    for j in tqdm(range(0, len(training_data_v1))):\n        text_from_doc = tokenize_sentence(training_data_v1['sentence'].iloc[j])\n        parts = [' '.join(text_from_doc[i:i+512]) for i in range(0, len(text_from_doc), 500)]\n        for k in parts:\n            training_data_for_model.loc[count] = training_data_v1['Id'].iloc[j], k\n            count = count + 1\n            \n    pd.set_option('display.max_colwidth',500)\n    print(training_data_for_model.shape)\n    training_data_for_model.head()\n\n    print(training_data_for_model.shape)\n    training_data_for_model = training_data_for_model.merge(training_data_v1[['Id','dataset_label']], how = 'left', on = 'Id')\n    print(training_data_for_model.shape)\n\n    pd.set_option('display.max_colwidth',500)\n    print(training_data_for_model.shape)\n    training_data_for_model.head()\n\n    training_data_for_model['flag'] = training_data_for_model[['dataset_label','sentence']].apply(lambda x: ([1 if k.strip().lower() in x['sentence'].lower().strip() else 0 for k in x['dataset_label']]), axis = 1)\n\n    training_data_for_model['flag_sum'] = training_data_for_model['flag'].apply(lambda x: sum(x)) \n    training_data_for_model['flag_sum'] = np.where(training_data_for_model['flag_sum']>0,1,0)\n\n    training_data_for_model[training_data_for_model['Id']=='000efc17-13d8-433d-8f62-a3932fe4f3b8']\n\n    print('Sentences without labels:', training_data_for_model[training_data_for_model['flag_sum']==0].shape[0]\/training_data_for_model.shape[0] * 100,'%')\n    print('Sentences with labels:', training_data_for_model[training_data_for_model['flag_sum']==1].shape[0]\/training_data_for_model.shape[0] * 100,'%')\n\n    training_data_for_model_v1 = training_data_for_model[training_data_for_model['flag_sum']==1].reset_index(drop = True)\n\n    print(training_data_for_model_v1.shape)\n    training_data_for_model_v1.head()\n    \n    with open('training_data_for_model_v1.pkl', 'wb') as file:\n        pickle.dump(training_data_for_model_v1, file)","386117be":"with open('..\/input\/coleridgetrainingdata\/training_data_for_model_v1.pkl', 'rb') as file:\n    training_data_for_model_v1 = pickle.load(file)","49f83e34":"# For train\nbatch_size=10\n\ncreate_train_data = False # Put true for training\n\nif create_train_data:\n    processed_sentence_tag_full_data = sent_tag_tokenization(data=training_data_for_model_v1)\n    processed_sentence_tag = processed_sentence_tag_full_data[['sent_input_ids', 'token_tag_ids']]\n\n    input_ids = processed_sentence_tag['sent_input_ids'].tolist()\n    token_ids = processed_sentence_tag['token_tag_ids'].tolist()\n\n    padded_input_ids, padded_attention_mask, padded_tags = pad_data(\n        input_ids=input_ids, \n        token_ids=token_ids\n    )\n\n    train_dataloader = create_dataloader(\n        token_ids=padded_input_ids, \n        masks=padded_attention_mask, \n        tags=padded_tags, \n        batch_size=batch_size, \n        val=False\n    )\n\n    with open('train_dataloader.pkl', 'wb') as file:\n        pickle.dump(train_dataloader, file)","9d7a330a":"with open('..\/input\/coleridgetrainingdata\/train_dataloader.pkl', 'rb') as file:\n    train_dataloader = pickle.load(file)","cf4b2b12":"# del train_data, train_data_cl_label_len_summary, train_data_cl_label_summary, training_data_for_model, training_data_for_model_v1, training_data_v1\ngc.collect()\ntorch.cuda.empty_cache()","f3ac27f4":"# config = AutoConfig.from_pretrained(\n#     'bert-base-uncased', \n#     num_labels=len(tag2idx),\n#     id2label=idx2tag,\n#     label2id=tag2idx\n# )\n# model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', config=config)","de291049":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nprint(device)\n\n# model = model.to(device)\n\n# optimizer = AdamW(model.parameters(),\n#                   lr = 1e-5) # learning rate\n\ncriterion = nn.CrossEntropyLoss()","717e8d19":"def do_train(model, optimizer, loss_criteria, train_dataloader):\n    try:\n        model.train()\n\n        total_loss = 0\n        total_logits = []\n\n        # iterate over batches\n        for step, batch in enumerate(train_dataloader):\n\n            # progress update after every 50 batches.\n            if step % 50 == 0 and not step == 0:\n                print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n            sent_id, mask, labels = batch\n            \n            gc.collect()\n            torch.cuda.empty_cache()\n            # clear previously calculated gradients\n            model.zero_grad()\n\n            # get model predictions for the current batch\n            logits = model(sent_id.to(device), mask.to(device))\n\n            # compute the loss between actual and predicted values\n            loss = loss_criteria(logits.logits.permute(0, 2, 1), labels)\n\n            # add on to the total loss\n            total_loss = total_loss + loss.item()\n\n            # backward pass to calculate the gradients\n            loss.backward()\n\n            # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # update parameters\n            optimizer.step()\n\n            # model predictions are stored on GPU. So, push it to CPU\n            logits = logits.logits.detach().cpu().numpy()\n\n            # append the model predictions\n            total_logits.append(logits)\n\n        # compute the training loss of the epoch\n        avg_loss = total_loss \/ len(train_dataloader)\n\n        total_logits = np.concatenate(total_logits, axis=0)\n\n\n        return avg_loss, total_logits\n    except Exception as e:\n        print(f\"Error during training the model on line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)\n\n\n# function for evaluating the model\ndef do_evaluate(model, val_dataloader, loss_criteria):\n    print(\"\\nEvaluating...\")\n    \n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_logits = []\n\n    # iterate over batches\n    for step, batch in enumerate(val_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            #             elapsed = format_time(time.time() - t0)\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            logits = model(sent_id.to(device), mask.to(device))\n\n            # compute the validation loss between actual and predicted values\n            loss = loss_criteria(logits.logits.permute(0, 2, 1), labels)\n\n            total_loss = total_loss + loss.item()\n\n            logits = logits.logits.detach().cpu().numpy()\n\n            total_logits.append(logits)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(val_dataloader)\n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_logits = np.concatenate(total_logits, axis=0)\n\n    return avg_loss, total_logits","a34764e3":"train_flag = False\n\nif train_flag:\n    %%time\n    epochs=1 ## Need to increase this and see better performance\n\n    # empty lists to store training and validation loss of each epoch\n    train_losses=[]\n\n    #for each epoch\n\n    for epoch in range(epochs):\n        start = time.time()\n        print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n\n        #train model\n        train_loss, _ = do_train(\n            model = model,\n            optimizer = optimizer, \n            loss_criteria = criterion, \n            train_dataloader = train_dataloader \n\n        )\n\n        model.save_pretrained('model_file')\n        tokenizer.save_pretrained('tokenizer_file')\n\n        # append training and validation loss\n        train_losses.append(train_loss)\n\n        print(f\"Time taken: {time.time() - start}\")\n        print(f'\\nTraining Loss: {train_loss:.3f}')","5162704c":"# Load fine-tuned model\nmodel = AutoModelForTokenClassification.from_pretrained('..\/input\/coleridge-model-data')\nmodel = model.to(device)","791ba13b":"def get_prediction_from_logits(logits):\n    try:\n        tag_prob = nn.Softmax(dim=2)(logits)\n        tag_prediction = torch.argmax(tag_prob, dim=2).detach().cpu().numpy()\n        return tag_prediction\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)\n        \ndef classification_result(tag2idx, c_tag_id):\n    try:\n        prediction_result = []\n        for sent_ in c_tag_id:\n            prediction_result.append(\n                list(map(lambda x: list(tag2idx.keys())[list(tag2idx.values()).index(x)], sent_))\n            )\n            \n        tagged_entity = np.concatenate(prediction_result, axis=0)\n        return tagged_entity\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e) ","b2c63447":"submission_file","d6a2c226":"# For test data\ntest_data_v1 = submission_file[['Id']]\ntest_data_v1['sentence'] = np.NaN\nfor i in tqdm(range(0, len(submission_file))):\n    text = submission_file['Id'].iloc[i]\n    file_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test\/\/' + text + '.json'\n    file_id = file_path.split('\/')[-1].split('.json')[0]\n    with open(file_path) as json_file:\n        data = json.load(json_file)\n\n    test_data_v1['sentence'].iloc[i] = ' '.join([x['text'].strip().lower() for x in data])\n    \n# Max token length for BERT is 512. Hence, we need to break down the sentences into smaller groups\ntest_data_for_model = pd.DataFrame(data = None, columns = ['Id','sentence'])\ncount = 0\nfor j in tqdm(range(0, len(test_data_v1))):\n    text_from_doc = tokenize_sentence(test_data_v1['sentence'].iloc[j])\n    parts = [' '.join(text_from_doc[i:i+512]) for i in range(0, len(text_from_doc), 500)]\n    for k in parts:\n        test_data_for_model.loc[count] = test_data_v1['Id'].iloc[j], k\n        count = count + 1","85c12de5":"test_data_for_model['dataset_label'] = [[''] for x in range(0,len(test_data_for_model))]\nprint(test_data_for_model.shape)\ntest_data_for_model.head()","9d0f41d1":"# For test data - tokenization + attention masks\nprocessed_sentence_tag_full_data = sent_tag_tokenization(data=test_data_for_model)\nprocessed_sentence_tag = processed_sentence_tag_full_data[['sent_input_ids', 'token_tag_ids']]\n\ninput_ids = processed_sentence_tag['sent_input_ids'].tolist()\ntoken_ids = processed_sentence_tag['token_tag_ids'].tolist()\n\ntest_padded_input_ids, test_padded_attention_mask, test_padded_tags = pad_data(\n    input_ids=input_ids, \n    token_ids=token_ids\n)","dedd47f0":"# get predictions for test data\nfor i in range(0,10):\n    with torch.no_grad():\n        logits = model(test_padded_input_ids[i:i+1].to(device), test_padded_attention_mask[i:i+1].to(device))\n        preds = get_prediction_from_logits(logits=logits['logits'])\n    print(sum(preds[0]))","8eac46b9":"def get_predicted_labels(sent_padded_input_ids, sent_padded_attention_mask):\n    try:\n        with torch.no_grad():\n            logits = model(sent_padded_input_ids.to(device), sent_padded_attention_mask.to(device))\n            c_tag_id = get_prediction_from_logits(logits=logits['logits'])        \n        test_ids = np.squeeze(sent_padded_input_ids.reshape(1, -1)).detach().cpu().numpy()\n        preds = classification_result(\n            tag2idx = tag2idx, \n            c_tag_id = c_tag_id\n        )        \n        test_tokens = [tokenizer.convert_ids_to_tokens(int(x)) for x in test_ids]\n        \n        final_out_ls = []\n        #final_out_ls1 = []\n        temp = []\n        for _idx, _tag in enumerate(preds):\n            if _tag in ['B', 'I']:\n                temp.append(test_tokens[_idx])\n            else:\n                if len(temp)>0:\n                    e = ''\n                    for t2 in temp:\n                        if t2.startswith('##'): e = e+t2.lstrip('##')\n                        else: e = e + ' ' +t2\n                    # final_out_ls1.append(temp)\n                    final_out_ls.append(e.strip())\n                temp = []        \n        \n        return final_out_ls\n    except Exception as e:\n        print(f\"Error in line: {sys.exc_info()[2].tb_lineno}\")\n        print(e)","afd536ab":"predicted_labels = []\nfor i in tqdm(range(test_data_for_model.shape[0])):\n    predicted_labels.append(get_predicted_labels(test_padded_input_ids[i:i+1].to(device), test_padded_attention_mask[i:i+1].to(device)))","ee9760a4":"predicted_labels","97c833ab":"test_data_for_model['predicted_label'] = predicted_labels","2501e938":"test_data_for_model['PredictionString'] = test_data_for_model['predicted_label'].apply(lambda x: ' '.join(x))\ntest_data_for_model['PredictionString_clean'] = test_data_for_model['PredictionString'].apply(lambda x: clean_text(x))","09321b5e":"final_test_results_ner = test_data_for_model[test_data_for_model['PredictionString_clean']!=''].groupby('Id').agg({'PredictionString_clean':' | '.join}).reset_index().rename(columns = {'PredictionString_clean':'PredictionString'})","235e8f66":"final_test_results_ner","ff7320dd":"test_data_for_model['Predicted_labels - Lookup approach'] = test_data_for_model['sentence'].apply(lambda x: keywordprocessor.extract_keywords(x))","a9b6f628":"test_data_for_model[test_data_for_model['Predicted_labels - Lookup approach'].str.len() > 0].head()","3f10db7e":"test_data_for_model['Predicted_labels - Lookup approach'] = test_data_for_model['Predicted_labels - Lookup approach'].apply(lambda x: ' | '.join(x))\ntest_data_for_model['Predicted_labels - Lookup approach'] = test_data_for_model['Predicted_labels - Lookup approach'].apply(lambda x: clean_text(x))\nfinal_test_results_lookup = test_data_for_model[test_data_for_model['Predicted_labels - Lookup approach'].str.len() > 0].groupby('Id').agg({'Predicted_labels - Lookup approach':' | '.join}).reset_index().rename(columns = {'Predicted_labels - Lookup approach':'PredictionString'})","06cbae94":"final_test_results_lookup","73fd79fa":"final_test_results = submission_file[['Id']].merge(final_test_results_lookup, how = 'left', on = 'Id')\nfinal_test_results = final_test_results.merge(final_test_results_ner, how = 'left', on = 'Id')\nfinal_test_results['PredictionString'] = final_test_results['PredictionString_x'].fillna('') + ' | ' + final_test_results['PredictionString_y'].fillna('')","d7a9ef25":"final_test_results","6ba7e68e":"manual_noise_list_identified = ['international of']","49d0429c":"def remove_noise(text):\n    tokens = text.split(' | ')\n    tokens = list(set(tokens))\n    tokens = [x for x in tokens if (len(x)>=5) & (x not in manual_noise_list_identified)]\n    result = ' | '.join(tokens)\n    return result","d8b2ceac":"# Remove noise\nfinal_test_results['PredictionString'] = final_test_results['PredictionString'].apply(lambda x: remove_noise(x))","c1d47e2d":"final_test_results = submission_file[['Id']].merge(final_test_results, on = 'Id', how = 'left')[['Id','PredictionString']]","c0e996a6":"final_test_results","044015f1":"final_test_results.to_csv(f'submission.csv', index=False)","0396b392":"## Import packages","5ebcfed5":"## NER Model","3ee36a24":"## Load input files","4792b744":"### Number of elements in each document","ca494f46":"### Identifying problem as NER","b34e067d":"### Number of documents of cleaned labels","b88dc673":"### Length of documents","0748d780":"### Train data samples","50644ad7":"## EDA","621a997c":"## Lookup approach","deeab2c4":"### Length of cleaned labels"}}