{"cell_type":{"37017439":"code","4a2f7d56":"code","60186ac0":"code","a89fb3b1":"code","49f5f7d9":"code","0da3d55e":"code","cb3f4f72":"code","9f93b06b":"code","14ca3564":"code","3cce5b3f":"markdown","4131857d":"markdown","fdd75adb":"markdown","0fa6337e":"markdown","24f46582":"markdown"},"source":{"37017439":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4a2f7d56":"df = pd.read_csv(\"..\/input\/email-spam-classification-dataset-csv\/emails.csv\")\ndf.head(20)","60186ac0":"df.isnull().sum()","a89fb3b1":"df.describe()","49f5f7d9":"X = df.iloc[:,1:3001]\nX","0da3d55e":"Y = df.iloc[:,-1].values\nY","cb3f4f72":"train_x,test_x,train_y,test_y = train_test_split(X,Y,test_size = 0.22)","9f93b06b":"mnb = MultinomialNB(alpha=1.9)         # alpha by default is 1. alpha must always be > 0. \n# alpha is the '1' in the formula for Laplace Smoothing (P(words))\nmnb.fit(train_x,train_y)\ny_pred1 = mnb.predict(test_x)\nprint(\"Accuracy Score for Naive Bayes : \", accuracy_score(y_pred1,test_y))","14ca3564":"rfc = RandomForestClassifier(n_estimators=100,criterion='gini')\n# n_estimators = No. of trees in the forest\n# criterion = basis of making the decision tree split, either on gini impurity('gini'), or on infromation gain('entropy')\nrfc.fit(train_x,train_y)\ny_pred3 = rfc.predict(test_x)\nprint(\"Accuracy Score of Random Forest Classifier : \", accuracy_score(y_pred3,test_y))","3cce5b3f":"As expected, Random Forest Classifier performs the best among the three. Decision tree classifiers are excellent classifiers. Random forest is a popular ensemble model that uses a forest of decision trees. So, obviously, combibining the accuracy of 100 trees (as n_estimators=100 here), will create a powerful model.","4131857d":"# Creating the NB Model\n\nIn this project we are clasifying mails typed in by the user as either 'Spam' or 'Not Spam'. Our original dataset was a folder of 5172 text files containing the emails.\n\nNow let us understand why we have separated the words from the mails. This is because, this is a text-classification problem. When a spam classifier looks at a mail, it searches for potential words that it has seen in the previous spam emails. If it finds a majority of those words, then it labels it as 'Spam'. **Why did I say majority ? -->**\n\n*CASE 1* : suppose let's take a word 'Greetings'. Say, it is present in both 'Spam' and 'Not Spam' mails.\n\n*CASE 2* : Let's consider a word 'lottery'.Say, it is present in only 'Spam' mails.\n\n*CASE 3* : Let's consider a word 'cheap'. Say, it is present only in spam.\n\nIf now we get a test email, and it contains all the three words metioned above, there's high probability that it is a 'Spam' mail.\n\nThe most effective algorithm for text-classification problems is the Naive Bayes algorithm, that works on the classic Bayes' theorem. This theorem works on every individual word in the test data to make predictions(the conditional probability with higher probability is the predicted result). \n\n________________________________________________________________________________________________________________________\n\nSay, our test email(S)is,*\"You have won a lottery\"*\n\n**HOW NAIVE BAYES WORKS ON THIS DATA -->**\n\nP(S) = P('You') * P('have') * P('won') * P('a') * P('lottery') ____ 1\n\nTherefore, P(S|Spam) = P('You'|Spam) * P('have'|Spam) * P('won'|Spam) * P('a'|Spam) * P('lottery'|Spam) ____ 2 \n\nSame calculation for P(S|Not_Spam) ____ 3\n\nIf 2 > 3, then 'Spam' Else, 'Not_Spam'.\n\n**WHAT IF THE PROBABILITY IS ZERO ?** Here comes the concept of Laplace Smoothing, where P(words) = (word_count + 1)\/(total_no_of_words + no_of_unique_words)\n\n________________________________________________________________________________________________________________________\n\nHere, we'll work on the existing Multinomial Naive Bayes classifier (under scikit-learn). To further understand how well Naive Bayes works for text-classification, we'll use another standard classifier, SVC, to see how the two models perform.\n\n**WILL ENSEMBLE MODELS WORKS BETTER ?** Let us see. We will use Random Forests to compare.","fdd75adb":"# Random Forests (Bagging)\n\nEnsemble methods turn any feeble model into a highly powerful one. Let us see if ensemble model can perform better than Naive Bayes","0fa6337e":"# Naive Bayes","24f46582":"Hey Folks, if you like my work please hit up \n\nThank you:)"}}