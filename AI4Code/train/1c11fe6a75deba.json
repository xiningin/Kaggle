{"cell_type":{"23538835":"code","97be2f1f":"code","50d245ea":"code","387d090c":"code","b8cb61e2":"code","d8a246f7":"code","ea282219":"code","cdf2b84b":"code","43f98011":"markdown","643f5097":"markdown"},"source":{"23538835":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","97be2f1f":"class Logistic_Regression:\n    \n    \n    def __init__(self,learning_rate=0.1,epochs=500):\n        # Learning rate(alpha) is a hyper-parameter to determine the value of update of weights and bias on gradient descent\n        # Too small Learning rate could make convergence too slow\n        # Too big learning rate cause the descent to jumps the minima\n        self.learning_rate=learning_rate\n        # Epochs is the number to time the descent will take place\n        self.epochs=epochs\n        # Initialising weights and bias to None\n        # Weights and bias is tweaked by gradient to find the line of best fit.\n        # Weight is defined as how much of what features combined to give the Dependent Variable\n        self.weights=None\n        self.bias=None\n        \n    def sigmoid(self,x):\n        # sigmoid function for logistic regression\n        return 1\/(1+np.exp(-x))\n        \n        \n    def predicted_val(self,Xi,W,b):\n        # Xi=(num_of_samples,num_of_features);W=(num_of_features,1);b=(num_of_samples,1)\n        # Dimension of ypred --> (num_of_samples,1)\n        linearPred=np.dot(Xi,W) + b\n        # Passing the prediction through sigmoid\n        # To simply explain, is to bring down value between 0 and 1\n        ypred=self.sigmoid(linearPred) \n        return ypred\n        \n    def fit(self,X,y):\n        num_of_samples,num_of_features=X.shape\n        # reshape y because if y is a rank matrix -->(num_of_samples,)\n        # Then there will be a issues in dimension\n        # For better understanding. visit this link: https:\/\/www.kaggle.com\/getting-started\/176510\n        y=y.reshape(num_of_samples,1)\n        # weight vector is of dimension (num_of_features,1)\n        self.weights= np.random.randn(num_of_features,1)\n        # Bias is (1,1)\n        self.bias=np.random.randn(1,1)\n        # Looping over number of epochs\n        for i in range(self.epochs):\n            # dW is the derivative of the cost function \n            # Cost function is the binary or sigmoid cross-entropy \u2212(ylog(p)+(1\u2212y)log(1\u2212p))\n            # p=predicted\n            dW=np.dot(X.T,(self.predicted_val(X,self.weights,self.bias)-y))\/(num_of_samples)\n            db=np.sum(self.predicted_val(X,self.weights,self.bias)-y)\/(num_of_samples)\n            \n            # Updating weights and bias\n            self.weights=self.weights-self.learning_rate*dW\n            self.bias=self.bias-self.learning_rate*db\n            \n    def predict(self,X):\n        lin_pred_y=np.dot(X,self.weights) + self.bias\n        y_pred_sig=self.sigmoid(lin_pred_y) \n        # if any value greater than 0.5 in y_pred_sig then 1 else 0 \n        y_predicted = [1 if i > 0.5 else 0 for i in y_pred_sig]\n        return np.array(y_predicted)\n    \n    # Checking accuracy\n    def accuracy(self,y,ypredi):\n        return np.mean(y==ypredi)\n            ","50d245ea":"# Classification data\nfrom sklearn import datasets\nX,y=datasets.make_classification(n_samples=1000,n_features=6,n_classes=2,random_state=42)","387d090c":"# Splitting test,train split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.1, random_state=42)","b8cb61e2":"logreg=Logistic_Regression()","d8a246f7":"logreg.fit(X_train,y_train)","ea282219":"p=logreg.predict(X_test)","cdf2b84b":"logreg.accuracy(y_test,p)","43f98011":"# Logistic Regression","643f5097":"# If this Notebook helped, An upvote will boost my motivation. Thank you"}}