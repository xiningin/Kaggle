{"cell_type":{"a623997f":"code","e8cb6668":"code","f49d0786":"code","3e4c763f":"code","d77be351":"code","00bb14cb":"code","76c9934c":"code","5bd136cb":"code","8b9c29b7":"code","3dc3747c":"code","1482936a":"code","9485e71a":"code","b50fe3d1":"code","479ecbd9":"code","dffba626":"code","4b61592d":"code","64a41910":"code","20990eab":"code","7c07d4b4":"code","62ba2d95":"code","5451e3d6":"code","04948694":"code","b0c81889":"code","029e90cd":"code","ab1c6ce5":"code","f862b4e5":"code","67519680":"code","1e3ff296":"code","c81b3d41":"code","fcdd673b":"code","dce7a1bd":"code","66dacf84":"code","0cb05c81":"code","753f2583":"code","41cb44e7":"code","19debe0b":"code","ff4741ac":"markdown","49911392":"markdown","43163c23":"markdown","2adcc8cd":"markdown","fd5639c0":"markdown","6eada7ef":"markdown","6b648194":"markdown","35130214":"markdown","30666e3e":"markdown","8835d7ca":"markdown","2831fbf7":"markdown","6f7652d0":"markdown","f7e45d68":"markdown","a85dbb15":"markdown","7b468111":"markdown"},"source":{"a623997f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8cb6668":"# importing libraries\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f49d0786":"# Load the datasets:-\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ng_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","3e4c763f":"g_sub","d77be351":"# View raw data:-\ntrain.head()","00bb14cb":"# Dimensions of the dataset:-\ntrain.shape","76c9934c":"# Information of the dataset:-\ntrain.info()","5bd136cb":"# Checking Null values using heatmap:-\nsns.heatmap(train.isnull())","8b9c29b7":"# No. of null values in the dataset:-\ntrain.isnull().sum()","3dc3747c":"# Drop null values :-\ntrain.dropna(inplace = True)","1482936a":"# Summarizing the data\ntrain.describe()","9485e71a":"# Dimension of the dataset after removal of null values:-\ntrain.shape","b50fe3d1":"# Finding correlation:-\ntrain_corr = train.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(train_corr,annot = True);","479ecbd9":"# Total male and female present on the ship\np = train.Sex.value_counts()\np","dffba626":"p.plot.pie();","4b61592d":"# How many passengers survived and how many dies.\nsurvive = train.Survived.value_counts()\nsurvive","64a41910":"survive.plot.pie();","20990eab":"# Gender that survives the most\ng_survive = pd.crosstab(index = train['Survived'],columns = train['Sex'])\ng_survive.plot.bar();","7c07d4b4":"# AGE that survives the most\nagesurvives = pd.crosstab(index = train['Age'],columns = train['Survived'])\nagesurvives.plot.line();\nagesurvives.plot.hist();","62ba2d95":"# Embarked that survives the most\nembark = pd.crosstab(index = train['Embarked'],columns = train['Survived'])\nembark.plot.bar();","5451e3d6":"# Gender with embark that survives the most\nembark_gen = pd.crosstab(index = train['Embarked'],columns = train['Sex'])\nembark_gen.plot.bar();","04948694":"train.replace(('male', 'female'), (1, 0), inplace=True)\ntest.replace(('male', 'female'), (1, 0), inplace=True)","b0c81889":"# Getting Features\nfeatures = ['Sex','Pclass','SibSp','Parch','Embarked']\nx = pd.get_dummies(train[features])\n\n# Predicting value\ny = train['Survived']","029e90cd":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 1\/8, random_state = 52)","ab1c6ce5":"test_x = test[features]","f862b4e5":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.fit_transform(x_test)","67519680":"#import Library for Accuracy Score\nfrom sklearn.metrics import accuracy_score\n\n#import Library for Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n#Initialize the Logistic Regression Classifier\nlogisreg = LogisticRegression()\n\n#Train the model using Training Dataset\nlogisreg.fit(x_train,y_train)\n\n# Prediction using test data\ny_pred = logisreg.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and y_pred\nacc_logisreg = accuracy_score(y_test, y_pred)\nprint( 'Accuracy of Logistic Regression model : ', acc_logisreg )","1e3ff296":"#Import Library for Linear Discriminant Analysis\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\n\n\n#Initialize the Linear Discriminant Analysis Classifier\nlda = LinearDiscriminantAnalysis()\n\n#Train the model using Training Dataset\nlda.fit(x_train,y_train)\n\n# Prediction using test data\nlda_pred = lda.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and lda_pred\nacc_lda = accuracy_score(y_test, lda_pred)\nprint( 'Accuracy of Linear Discriminant Analysis Classifier: ', acc_lda )","c81b3d41":"#Import Library for Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\n#Initialize the Gaussian Naive Bayes Classifier\ngnb = GaussianNB()\n\n#Train the model using Training Dataset\ngnb.fit(x_train,y_train)\n\n# Prediction using test data\ngnb_pred = gnb.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and gnb_pred\nacc_ganb = accuracy_score(y_test, gnb_pred)\nprint( 'Accuracy of Gaussian Naive Bayes : ', acc_ganb )","fcdd673b":"#Import Library for Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#Initialize the Decision Tree Classifier\ndtc = DecisionTreeClassifier()\n\n#Train the model using Training Dataset\ndtc.fit(x_train,y_train)\n\n# Prediction using test data\ndtc_pred = dtc.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and dtc_pred\nacc_dtree = accuracy_score(y_test, dtc_pred)\nprint( 'Accuracy of  Decision Tree Classifier : ', acc_dtree )","dce7a1bd":"#Import Library for Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Initialize the Random Forest\nrfc = RandomForestClassifier()\n\n#Train the model using Training Dataset\nrfc.fit(x_train,y_train)\n\n# Prediction using test data\nrfc_pred = rfc.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and rfc_pred\nacc_rf = accuracy_score(y_test, rfc_pred)\nprint( 'Accuracy of  Random Forest : ', acc_rf )","66dacf84":"#Import Library for Support Vector Machine\nfrom sklearn import svm\n\n#Initialize the Support Vector Classifier\nsvmodel = svm.SVC()\n\n#Train the model using Training Dataset\nsvmodel.fit(x_train,y_train)\n\n# Prediction using test data\nsvm_pred = svmodel.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and svm_pred\nacc_svc = accuracy_score(y_test, svm_pred)\nprint( 'Accuracy of Support Vector Classifier: ', acc_svc )","0cb05c81":"#Import Library for K Nearest Neighbour Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Initialize the K Nearest Neighbour Model with Default Value of K=5\nknnmodel = KNeighborsClassifier()\n\n#Train the model using Training Dataset\nknnmodel.fit(x_train,y_train)\n\n# Prediction using test data\nknn_pred = knnmodel.predict(x_test)\n\n# Calculate Model accuracy by comparing y_test and knn_pred\nacc_knn = accuracy_score(y_test, knn_pred)\nprint( 'Accuracy of KNN Classifier: ', acc_knn )","753f2583":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','Linear Discriminant Analysis','Naive Bayes', 'Decision Tree', 'Random Forest', 'Support Vector Machines', \n              'K - Nearest Neighbors'],\n    'Score': [acc_logisreg, acc_lda, acc_ganb, acc_dtree, acc_rf, acc_svc, acc_knn]})\n\nmodels.sort_values(by='Score', ascending=False)","41cb44e7":"#Import Library for Accuracy Score\nfrom sklearn.metrics import accuracy_score\n\n#Import Library for Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\n#Initialize the Logistic Regression Classifier\nlogisreg = LogisticRegression()\n\n#Train the model using Training Dataset\nlogisreg.fit(x_train,y_train)\n\n# Prediction using test data\ny_pred = logisreg.predict(pd.get_dummies(test_x))","19debe0b":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_pred.astype(int)})\noutput.to_csv('submission.csv', index=False)","ff4741ac":"Now, the no. of Rows in Dataset is 183 and No. of Columns in Dataset is 12.","49911392":"# Titanic - Machine Learning from Disaster","43163c23":"No. of Rows in Dataset is 891 and No. of Columns in Dataset is 12.","2adcc8cd":"***5. Random Forest Classifier:-***","fd5639c0":"***7. KNN Classifier:-***","6eada7ef":"***6. Support Vector Machine:-***","6b648194":"***4. Decision Tree Classifier:-***\n","35130214":"***Analysis on Train Dataset:-***","30666e3e":"***1. Logistic Regression:-***","8835d7ca":"***3. Gaussian Naive Bayes:-***","2831fbf7":"> Now, as we can see from above Logistic Regression has highest accuracy i.e. 91.3%. So, we will predict our data using Logistic Regression Model.","6f7652d0":"# Model Selection:-","f7e45d68":"***Thank you...***\n\n***Please Upvote.....***","a85dbb15":"***2. Linear DIscriminent Analysis:-***","7b468111":"# Data Modelling:-"}}