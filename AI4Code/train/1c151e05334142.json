{"cell_type":{"01375f54":"code","edf9344a":"code","6493c5ff":"code","a12ae9cc":"code","5e6de5e7":"code","5610846c":"code","993aec43":"code","39a67a22":"code","fc09e967":"code","e84c930b":"code","4da3e742":"code","d5b2f879":"code","11c0e1bd":"code","14690f4c":"code","faa5d4f8":"code","9be9a446":"code","cfb21442":"code","6d7a9689":"code","e6341fd9":"code","ba14c2dc":"code","d324821a":"code","457db542":"code","8f9489b2":"code","ee027763":"code","19bad544":"code","e20c3740":"code","05a02ce5":"code","3b0c2759":"code","01d641b6":"code","492713b0":"code","5cc3b567":"code","c340a151":"code","5c73d2ac":"code","947edead":"markdown","e0d453c1":"markdown","8d382656":"markdown","37c39c7f":"markdown","4251b14f":"markdown","3f4d84e6":"markdown","a148ab75":"markdown","d05147ed":"markdown","acacaf7f":"markdown"},"source":{"01375f54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","edf9344a":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing  import LabelEncoder\nle = LabelEncoder()\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport missingno as miss\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.offline import iplot\nimport cufflinks as cf\ncf.go_offline()\nfrom tqdm import tqdm\n\nfrom nltk.corpus import stopwords    \nfrom nltk.tokenize import word_tokenize\nfrom textblob import TextBlob\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, MaxPooling1D, Conv1D, Concatenate, Bidirectional, GlobalMaxPool1D, ActivityRegularization, BatchNormalization\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score,confusion_matrix","6493c5ff":"df = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-for-financial-news\/all-data.csv\", encoding='latin-1', header = None)\ndf.columns = [\"Sentiment\", \"News Headline\"]\ndf.head()","a12ae9cc":"df.shape","5e6de5e7":"df.info()","5610846c":"df.describe(include = \"all\")","993aec43":"df.isna().sum()","39a67a22":"miss.bar(df)\nplt.show()","fc09e967":"#check for duplicates\n\nlen(df[df.duplicated()])","e84c930b":"df = df.drop_duplicates()\nprint(df.head())\nprint(df.shape)","4da3e742":"df['nr_of_char'] = df['News Headline'].str.len()\ndf['nr_of_char'] = df['nr_of_char'] \/ df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_char']].pivot(columns = 'Sentiment', values = 'nr_of_char').iplot(kind = 'box')","d5b2f879":"df['nr_of_words'] = df['News Headline'].str.split().str.len()\ndf['nr_of_words'] = df['nr_of_words'] \/ df['nr_of_char'].max()\ndf[['Sentiment', 'nr_of_words']].pivot(columns = 'Sentiment', values = 'nr_of_words').iplot(kind = 'box')","11c0e1bd":"df['nr_of_unique_words'] = df['News Headline'].apply(lambda x: len(set(x.split())))\ndf['nr_of_unique_words'] = df['nr_of_unique_words'] \/ df['nr_of_unique_words'].max()\ndf[['Sentiment', 'nr_of_unique_words']].pivot(columns = 'Sentiment', values = 'nr_of_unique_words').iplot(kind='box')","14690f4c":"df['nr_of_punctuation'] = df['News Headline'].str.split(r\"\\?|,|\\.|\\!|\\\"|'\").str.len()\ndf['nr_of_punctuation'] = df['nr_of_punctuation'] \/ df['nr_of_punctuation'].max()\ndf[['Sentiment', 'nr_of_punctuation']].pivot(columns = 'Sentiment', values = 'nr_of_punctuation').iplot(kind = 'box')","faa5d4f8":"stop_words = set(stopwords.words('english'))\ndf['nr_of_stopwords'] = df['News Headline'].str.split().apply(lambda x: len(set(x) & stop_words))\ndf['nr_of_stopwords'] = df['nr_of_stopwords'] \/ df['nr_of_stopwords'].max()\ndf[['Sentiment', 'nr_of_stopwords']].pivot(columns = 'Sentiment', values = 'nr_of_stopwords').iplot(kind = 'box')","9be9a446":"df.corr().iplot(kind='heatmap',colorscale=\"YlGnBu\",title=\"Feature Correlation Matrix\")","cfb21442":"df.insert(0, 'Id', range(1, 1 + len(df))) #defining custom Id column\ndef show_donut_plot(col): #donut plot function\n    \n    rating_data = df.groupby(col)[['Id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['Id']], autopct = '%1.0f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: Reviews \\n', loc='center')\n    plt.show()","6d7a9689":"show_donut_plot('Sentiment')","e6341fd9":"import re\nimport spacy\nnlp = spacy.load('en')\n\ndef normalize(msg):\n    \n    msg = re.sub('[^A-Za-z]+', ' ', msg) #remove special character and intergers\n    doc = nlp(msg)\n    res=[]\n    for token in doc:\n        if(token.is_stop or token.is_punct or token.is_currency or token.is_space or len(token.text) <= 2): #word filteration\n            pass\n        else:\n            res.append(token.lemma_.lower())\n    return res","ba14c2dc":"df[\"News Headline\"] = df[\"News Headline\"].apply(normalize)","d324821a":"df.head()","457db542":"words_collection = Counter([item for sublist in df['News Headline'] for item in sublist])\nfreq_word_df = pd.DataFrame(words_collection.most_common(20))\nfreq_word_df.columns = ['frequently_used_word','count']","8f9489b2":"fig = px.scatter(freq_word_df, x=\"frequently_used_word\", y=\"count\", color=\"count\", title = 'Frequently used words - Scatter plot')\nfig.show()","ee027763":"df[\"News Headline\"] = df[\"News Headline\"].apply(lambda x : \" \".join(x))\ndf = df[[\"News Headline\", \"Sentiment\"]]\ndf[\"Sentiment\"] = le.fit_transform(df[\"Sentiment\"])\ndf.head()","19bad544":"rename = {\"News Headline\": \"text\", \"Sentiment\": \"labels\"}\ndf.rename(columns = rename, inplace=True)","e20c3740":"!pip install transformers\n!pip install simpletransformers","05a02ce5":"train_x_y = df.sample(frac = 0.75, random_state = 42)\ntest_x_y = pd.concat([df, train_x_y]).drop_duplicates(keep=False)","3b0c2759":"print(train_x_y.shape)\nprint(test_x_y.shape)","01d641b6":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\n\n\nmodel_args = ClassificationArgs()\nmodel_args.train_batch_size = 2\nmodel_args.gradient_accumulation_steps = 8\nmodel_args.learning_rate = 3e-5\nmodel_args.num_train_epochs = 1\n\nmodel_bert = ClassificationModel(\"bert\", \"bert-base-uncased\", num_labels=3, args=model_args, use_cuda=False)","492713b0":"model_bert.train_model(train_x_y)","5cc3b567":"pred_bert, out_bert = model_bert.predict(test_x_y['text'].values)\n\nacc_bert = accuracy_score(test_x_y['labels'].to_numpy(), pred_bert)\nf1_bert = f1_score(test_x_y['labels'].to_numpy(), pred_bert, average='micro')\n\nprint(\"Accuracy score -->\", acc_bert)\nprint(\"F1 score -->\", f1_bert)","c340a151":"#graph with confusion matrix\n\ncm = confusion_matrix(pred_bert, test_x_y['labels'].to_numpy())\n#group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\ngroup_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()\/np.sum(cm)]\nlabels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(3,3)\nsns.heatmap(cm, annot=labels, fmt=\"\", cmap='Blues')\nplt.show()","5c73d2ac":"fig,ax=plt.subplots(figsize=(10,5))\nsns.regplot(x=pred_bert, y=test_x_y['labels'].to_numpy(),marker=\"*\")\nplt.show()","947edead":"## Number of punctuation marks","e0d453c1":"## Number of stopwords","8d382656":"## Number of unique words","37c39c7f":"## Feature correlation matrix","4251b14f":"## Let's start with BERT","3f4d84e6":"## Number of Characters","a148ab75":"## Target Distribution","d05147ed":"## Cleaning the text column","acacaf7f":"## Number of words"}}