{"cell_type":{"78b9dfda":"code","c6be6b83":"code","2b902995":"code","4cc7e325":"code","7eb099fb":"code","4ba9e933":"code","b5ef963e":"code","e3ed4971":"code","c489aacf":"code","52889a78":"code","bb77d4a6":"code","986ccf47":"code","c31a6f06":"code","729ba48e":"code","10afa2ac":"code","bb2f56df":"code","5b025ebf":"code","90a3a88c":"code","eae8b7e1":"markdown","76ed9e60":"markdown"},"source":{"78b9dfda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c6be6b83":"from sklearn.model_selection import train_test_split,StratifiedKFold\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2b902995":"data = pd.read_csv('..\/input\/creditcard.csv')","4cc7e325":"data.shape","7eb099fb":"data.describe()","4ba9e933":"data.head()","b5ef963e":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total_null', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))\n\nmissing_data(data)","e3ed4971":"data_feature = data.drop(['Class'],axis = 1)\ndata_label = data['Class']\ndata_feature = data_feature.values\ndata_label = data_label.values","c489aacf":"from sklearn.preprocessing import MinMaxScaler\ndata_feature = MinMaxScaler().fit_transform(data_feature)\n\ndef ewm(data):\n    shape_x,shape_y = data.shape\n    \n    k = 1 \/ np.log(shape_y)\n    y = data.sum(axis=0)\n    p = data \/ y\n    #\u8ba1\u7b97pij\n    test=p*np.log(p)\n    test=np.nan_to_num(test)\n    #\u8ba1\u7b97\u6bcf\u79cd\u6307\u6807\u7684\u4fe1\u606f\u71b5\n    ej=-k*(test.sum(axis=0))\n    #\u8ba1\u7b97\u6bcf\u79cd\u6307\u6807\u7684\u6743\u91cd\n    wi=(1-ej)\/np.sum(1-ej)\n    \n    return wi\nw = ewm(data_feature)\nw\n","52889a78":"features = [col for col in data.columns if col not in['Class']]\nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = features\nfold_importance_df[\"importance\"] = w\n\nplt.figure(figsize=(10,5))\nsns.barplot(y=\"importance\", x=\"feature\", data=fold_importance_df.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance ')\nplt.tight_layout()","bb77d4a6":"data_feature = data.drop(['Class'],axis = 1)\ndata_label = data['Class']\ndata_feature = data_feature.values\ndata_label = data_label.values","986ccf47":"# Train_test split\nrandom_state = 42\nX_train, X_test, y_train, y_test = train_test_split(data_feature, data_label, test_size = 0.20, random_state = random_state, stratify = data_label)","c31a6f06":"lgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"l2\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 6,  #13\n    \"learning_rate\" : 0.01,\n#     \"bagging_freq\": 5,\n#     \"bagging_fraction\" : 0.5,#0.4\n#     \"feature_fraction\" : 0.041, #0.05\n#     \"min_data_in_leaf\": 100,  #80\n#     \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    #\"lambda_l1\" : 5,\n    #\"lambda_l2\" : 5,\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}\n","729ba48e":"trn_data = lgb.Dataset(X_train, label=y_train)\nval_data = lgb.Dataset(X_test, label=y_test)\nevals_result = {}\nlgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,\n                        valid_sets = [trn_data, val_data],\n                        early_stopping_rounds=3000,\n                        verbose_eval = 1000,\n                        evals_result=evals_result\n                       )","10afa2ac":"features = [col for col in data.columns if col not in['Class']]\nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = features\nfold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n\nplt.figure(figsize=(10,5))\nsns.barplot(y=\"importance\", x=\"feature\", data=fold_importance_df.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance ')\nplt.tight_layout()","bb2f56df":"g = lgb.plot_tree(lgb_clf, tree_index=0, figsize=(40, 20), show_info=['split_gain'])\nplt.show()","5b025ebf":"import shap\n\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP values\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\nexplainer = shap.TreeExplainer(lgb_clf)\nshap_values = explainer.shap_values(X_train)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_train[0,:])","90a3a88c":"# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n\n# for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n#     features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\n\n#     X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n#     X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n            \n#     trn_data = lgb.Dataset(X_t, label=y_t)\n#     val_data = lgb.Dataset(X_valid, label=y_valid)\n#     evals_result = {}\n#     lgb_clf = lgb.train(lgb_params,\n#                         trn_data,\n#                         100000,\n#                         valid_sets = [trn_data, val_data],\n#                         early_stopping_rounds=3000,\n#                         verbose_eval = 5000,\n#                         evals_result=evals_result\n#                        )\n#     p_valid += lgb_clf.predict(X_valid)\n#     yp += lgb_clf.predict(X_test)\n# #     fold_importance_df = pd.DataFrame()\n# #     fold_importance_df[\"feature\"] = features\n# #     fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n# #     fold_importance_df[\"fold\"] = fold + 1\n# #     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n#     oof['predict'][val_idx] = p_valid\/N\n#     val_score = roc_auc_score(y_valid, p_valid)\n#     val_aucs.append(val_score)\n    \n#     predictions['fold{}'.format(fold+1)] = yp\/N","eae8b7e1":"<a>LGBM<\/a>","76ed9e60":"**\u71b5\u6743\u6cd5**entropy weight method(EWM) "}}