{"cell_type":{"44fc5ad3":"code","3b5b13f2":"code","37f5bc55":"code","23d8fca9":"code","d72a7f82":"code","d44f340e":"code","1cfe97fb":"code","6401f29d":"code","4644a220":"code","666dd759":"code","ba5fb855":"code","a4306584":"code","758c95ee":"code","16ad7cbd":"code","b9f1972e":"markdown","a62d50cd":"markdown","b5aa9c87":"markdown","2a76e564":"markdown","1405887b":"markdown","d8930e6c":"markdown","5f7512a8":"markdown","6891b660":"markdown","8922cf0e":"markdown","8c6329b4":"markdown"},"source":{"44fc5ad3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom fastai.vision.all import *\nimport albumentations\n\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b5b13f2":"#Code by ilovescience https:\/\/www.kaggle.com\/tanlikesmath\/cassava-classification-eda-fastai-starter\/notebook\n\nset_seed(999,reproducible=True)","37f5bc55":"dataset_path = Path('..\/input\/image-preprocessing-for-model-training')\nos.listdir(dataset_path)","23d8fca9":"train_df = pd.read_csv(dataset_path\/'paths_and_labels.csv')","d72a7f82":"train_df.head()","d44f340e":"#Code by ilovescience https:\/\/www.kaggle.com\/tanlikesmath\/cassava-classification-eda-fastai-starter\/notebook\n\ntrain_df['path'] = train_df['img_path'].map(lambda x:dataset_path\/'images'\/x)\ntrain_df = train_df.drop(columns=['img_path'])\ntrain_df = train_df.sample(frac=1).reset_index(drop=True) #shuffle dataframe\ntrain_df.head(10)","1cfe97fb":"len_df = len(train_df)\nprint(f\"There are {len_df} images\")","6401f29d":"import matplotlib.pyplot as plt\n\ntrain_df['class'].hist(figsize = (10, 5))\nplt.title('Breast Density')\nplt.xticks(rotation=90);","4644a220":"from PIL import Image\n\nim = Image.open(\"..\/input\/image-preprocessing-for-model-training\/mdb004.pgm.png\")\nwidth, height = im.size\nprint(width,height)","666dd759":"im","ba5fb855":"path = Path(\"\/kaggle\/input\/image-preprocessing-for-model-training\")\npath.ls()","a4306584":"#To avoid Error Dataloader does Not contain any batches we should reduce number of bs (batches\/ Bitches)\n#In the original bs was 32. Then I changed to 16\n\nnp.random.seed(42)\ndata = ImageDataLoaders.from_folder(path, train=\".\", valid_pct=0.2, item_tfms=RandomResizedCrop(512, min_scale=0.75),\n                                    bs=16,batch_tfms=[*aug_transforms(size=256, max_warp=0), Normalize.from_stats(*imagenet_stats)],num_workers=0)","758c95ee":"#To avoid clumsy subtitles (legendas) I changed figsize. The original was (7,8)\n\ndata.show_batch(nrows=3, figsize=(14,12))","16ad7cbd":"data.show_batch(nrows=2, figsize=(18,16))","b9f1972e":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: black;\"><b style=\"color:#03e8fc;\">Mammography Views<\/b><\/h1><\/center>\n\nAuthor: Dr Corey Thompson and Radswiki et al.\n\n\"Standard views are bilateral craniocaudal (CC) and mediolateral oblique (MLO) views, which comprise routine screening mammography. The views are usually used for all routine screening clients. That is, unless there is a contraindication, screening mammograms consist of these 4 views.\"\n\n\n<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">MLO versus ML<\/span><\/h1><br>\n\n\"The reason is that a mammogram is a two dimensional representation of a 3 dimensional structure; by the same token a map is not an accurate representation of the earth's actual geography. The ML view loses significant tissue volume in the upper outer quadrant of the breast where statistically the most breast cancers are found. By doing an MLO view you get extra tissue without extra exposure. The downside of the MLO view is it is not 90 degrees to the cc view so localization of a lesion requires some thought. The two views are not orthogonal.\"\n\ncraniocaudal view (CC view)\n\nmediolateral oblique view (MLO view)\n\nhttps:\/\/radiopaedia.org\/articles\/mammography-views","a62d50cd":"![](https:\/\/slideplayer.com\/17140014\/99\/images\/slide_1.jpg)slideplayer.com","b5aa9c87":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Dense Breast Tissue<\/span><\/h1><br>\n\nWhat is dense breast tissue?\n\n\"Dense breast tissue refers to the appearance of breast tissue on a mammogram. It's a normal and common finding.\"\n\n\"Breast tissue is composed of milk glands, milk ducts and supportive tissue (dense breast tissue), and fatty tissue (nondense breast tissue). When viewed on a mammogram, women with dense breasts have more dense tissue than fatty tissue.\"\n\n\"On a mammogram, nondense breast tissue appears dark and transparent. Dense breast tissue appears as a solid white area on a mammogram, which makes it difficult to see through.\"\n\nhttps:\/\/www.mayoclinic.org\/tests-procedures\/mammogram\/in-depth\/dense-breast-tissue\/art-20123968","2a76e564":"Predicting Breast Cancer by Applying Deep Learning to Linked Health Records and Mammograms\n\nAuthors:Ayelet Akselrod-Ballin*, Michal Chorev* , Yoel Shoshan, Adam Spiro, Alon Hazan, Roie Melamed, Ella Barkan, Esma Herzel, Shaked Naor, Ehud Karavani, Gideon Koren, Yaara Goldschmidt, Varda Shalev, Michal Rosen-Zvi, Michal Guindy\n\nJun 18 2019https:\/\/doi.org\/10.1148\/radiol.2019182622\n\n![](https:\/\/pubs.rsna.org\/cms\/10.1148\/radiol.2019182622\/asset\/images\/medium\/radiol.2019182622.fig2.gif)pubs.rsna.org","1405887b":"#Distribution of the different classes","d8930e6c":"That's all for now. How I applied a Cassava\/Helmet code to Mammogram.","5f7512a8":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Mammography Additional (supplementary) views<\/span><\/h1><br>\n\nThese views are used in diagnostic breast workups in addition to the standard views.\n\ntrue lateral view - 90\u00ba view \n\nmediolateral view - ML view\n\nlateromedial view - LM view\n\nlateromedial oblique view - LMO view\n\nlate mediolateral view - late ML view\n\nstep oblique views\n\nspot view - spot compression view\n\ndouble spot compression view\n\nmagnification view(s)\n\nexaggerated craniocaudal views - exaggerated CC views\n\nXCCL view\n\nXCCM view\n\naxillary view - axillary tail view\n\ncleavage view - valley view\n\ntangential views\n\ncaudocranial view - reversed CC view - 180\u00b0 CC view\n\nbullseye CC view\n\nrolled CC view\n\nelevated craniocaudal projection\n\ncaudal cranial projection\n\n20\u00b0 oblique projection \n\ninferomedial superolateral oblique projection\n\nEklund technique\n\nhttps:\/\/radiopaedia.org\/articles\/mammography-views","6891b660":"\"A deep learning algorithm predicted breast malignancy detected within 12 months from the index examination (area under the receiver operating curve AUC, 0.91; specificity of 77.3% at a sensitivity of 87%).\n\n\u25a0 \"The algorithm identified breast cancer in 34 of 71 (48%) women in whom the initial radiologist interpretation was negative for cancer but in whom cancer was detected within a year.\"\n\n\u25a0 \"The deep learning algorithm improved risk prediction over the Gail model (AUC, 0.78 vs 0.54, respectively; P < .004).\"\n\n\u25a0 \"The deep learning algorithm identified white blood cell profiles and thyroid function tests as associated with breast cancer despite that these factors are not currently integrated in published risk scores.\"\n\nhttps:\/\/pubs.rsna.org\/doi\/full\/10.1148\/radiol.2019182622","8922cf0e":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Mammograms: Applying DL to Predict Breast Cancer<\/span><\/h1><br>\n\nPredicting Breast Cancer by Applying Deep Learning to Linked Health Records and Mammograms\n\nAuthors:Ayelet Akselrod-Ballin*, Michal Chorev* , Yoel Shoshan, Adam Spiro, Alon Hazan, Roie Melamed, Ella Barkan, Esma Herzel, Shaked Naor, Ehud Karavani, Gideon Koren, Yaara Goldschmidt, Varda Shalev, Michal Rosen-Zvi, Michal Guindy\n\nJun 18 2019https:\/\/doi.org\/10.1148\/radiol.2019182622\n\n\"To evaluate the accuracy and efficiency of a combined machine and deep learning approach for early breast cancer detection applied to a linked set of digital mammography images and electronic health records.\"\n\n\"The algorithm was trained on 9611 mammograms and health records of women to make two breast cancer predictions: to predict biopsy malignancy and to differentiate normal from abnormal screening examinations. The study estimated the association of features with outcomes by using t test and Fisher exact test. The model comparisons were performed with a 95% confidence interval (CI) or by using the DeLong test.\"\n\n\"The resulting algorithm was validated in 1055 women and tested in 2548 women (mean age, 55 years \u00b1 10 ;standard deviation). In the test set, the algorithm identified 34 of 71 (48%) false-negative findings on mammograms. For the malignancy prediction objective, the algorithm obtained an area under the receiver operating characteristic curve (AUC) of 0.91 (95% CI: 0.89, 0.93), with specificity of 77.3% (95% CI: 69.2%, 85.4%) at a sensitivity of 87%. When trained on clinical data alone, the model performed significantly better than the Gail model (AUC, 0.78 vs 0.54, respectively; P < .004).\"\n\n\"The algorithm, which combined machine-learning and deep-learning approaches, can be applied to assess breast cancer at a level comparable to radiologists and has the potential to substantially reduce missed diagnoses of breast cancer.\"\n\nhttps:\/\/pubs.rsna.org\/doi\/full\/10.1148\/radiol.2019182622","8c6329b4":"<h1><span class=\"label label-default\" style=\"background-color:black;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:#03e8fc; padding:10px\">Density Levels<\/span><\/h1><br>\n\nA: Almost entirely fatty indicates that the breasts are almost entirely composed of fat. About 1 in 10 women has this result.\n\nB: Scattered areas of fibroglandular density indicates there are some scattered areas of density, but the majority of the breast tissue is nondense. About 4 in 10 women have this result.\n\nC: Heterogeneously dense indicates that there are some areas of nondense tissue, but that the majority of the breast tissue is dense. About 4 in 10 women have this result.\n\nD: Extremely dense indicates that nearly all of the breast tissue is dense. About 1 in 10 women has this result.\n\nhttps:\/\/www.mayoclinic.org\/tests-procedures\/mammogram\/in-depth\/dense-breast-tissue\/art-20123968"}}