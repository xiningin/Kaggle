{"cell_type":{"4247a146":"code","e9657257":"code","4a852b01":"code","e0d6bb82":"code","d7add1af":"code","3cdd6cff":"code","953ef990":"code","47e344bb":"code","095208b5":"code","6b389ec9":"code","c42498eb":"markdown","e0e97360":"markdown","800b27a1":"markdown","6f3e83f7":"markdown"},"source":{"4247a146":"! pip install sklearn","e9657257":"import os\nimport pandas as pd","4a852b01":"DATASET_DIR = \"..\/input\/automated-essay-scoring-dataset\/\"\nGLOVE_DIR = '.\/glove.6B\/'\nSAVE_DIR = '.\/'\n","e0d6bb82":"X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\ny = X['domain1_score']\nX = X.dropna(axis=1)\nX = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n\nX.head()","d7add1af":"minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\nmaximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]","3cdd6cff":"\nimport numpy as np\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\n\ndef essay_to_wordlist(essay_v, remove_stopwords):\n    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n    words = essay_v.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return (words)\n\ndef essay_to_sentences(essay_v, remove_stopwords):\n    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    raw_sentences = tokenizer.tokenize(essay_v.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0.\n    index2word_set = set(model.wv.index2word)\n    for word in words:\n        if word in index2word_set:\n            num_words += 1\n            featureVec = np.add(featureVec,model[word])        \n    featureVec = np.divide(featureVec,num_words)\n    return featureVec\n\ndef getAvgFeatureVecs(essays, model, num_features):\n    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n    counter = 0\n    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n    for essay in essays:\n        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n        counter = counter + 1\n    return essayFeatureVecs","953ef990":"\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom tensorflow.keras.models import Sequential, load_model, model_from_config\nimport tensorflow.keras.backend as K\n\ndef get_model():\n    \"\"\"Define the model.\"\"\"\n    model = Sequential()\n    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n    model.add(LSTM(64, recurrent_dropout=0.4))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='relu'))\n\n    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n    model.summary()\n\n    return model","47e344bb":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score\n\ncv = KFold(n_splits=5, shuffle=True)\nresults = []\ny_pred_list = []\n\ncount = 1\nfor traincv, testcv in cv.split(X):\n    \n    print(\"\\n--------Fold {}--------\\n\".format(count))\n    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n    \n    train_essays = X_train['essay']\n    test_essays = X_test['essay']\n    \n    sentences = []\n    \n    for essay in train_essays:\n        # Obtaining all sentences from the training essays.\n        sentences += essay_to_sentences(essay, remove_stopwords = True)\n    \n    # Initializing variables for word2vec model.\n    num_features = 300 \n    min_word_count = 40\n    num_workers = 4\n    context = 10\n    downsampling = 1e-3\n\n    print(\"Training Word2Vec Model...\")\n    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n\n    model.init_sims(replace=True)\n    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n    \n    # Generate training and testing data word vectors.\n    clean_train_essays = []\n    for essay_v in train_essays:\n        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n    \n    clean_test_essays = []\n    for essay_v in test_essays:\n        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n    \n    trainDataVecs = np.array(trainDataVecs)\n    testDataVecs = np.array(testDataVecs)\n    \n    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n    \n    lstm_model = get_model()\n    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n    y_pred = lstm_model.predict(testDataVecs)\n    \n    # Save any one of the 8 models.\n    if count == 5:\n         lstm_model.save('.\/final_lstm.h5')\n            \n    # Round y_pred to the nearest integer.\n    y_pred = np.around(y_pred)\n    \n    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n    print(\"Kappa Score: {}\".format(result))\n    results.append(result)\n\n    count += 1","095208b5":"print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))","6b389ec9":"    from gensim.test.utils import datapath\n    \n    content = \"He is very bad student.\"    \n    \n    if len(content) > 20:\n        num_features = 300\n        model = Word2Vec.load('.\/word2vecmodel.bin')\n        #model = Word2vec.KeyedVectors.load_word2vec_format(os.path.join(current_path, \".\/word2vec.bin\"), binary=True)\n        clean_test_essays = []\n        clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n        testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n        testDataVecs = np.array(testDataVecs)\n        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n\n        lstm_model = get_model()\n        lstm_model.load_weights(os.path.join(current_path, \".\/final_lstm.h5\"))\n        preds = lstm_model.predict(testDataVecs)\n\n        if math.isnan(preds):\n            preds = 0\n        else:\n            preds = np.around(preds)\n\n        if preds < 0:\n            preds = 0\n        if preds > question.max_score:\n            preds = question.max_score\n    else:\n        preds = 0\n        \n    print(preds)","c42498eb":"# Defining the Model","e0e97360":"# Preprocessing the data","800b27a1":"# Importing the data","6f3e83f7":"# Predict the Essay"}}