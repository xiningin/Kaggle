{"cell_type":{"b72dedcf":"code","126f9ca9":"code","0cc673c4":"code","25347826":"code","988d059c":"code","a27b2296":"code","7fa14473":"code","8bed8e10":"code","dfb1a02b":"code","434646be":"code","d9630b3b":"code","ecac4757":"code","6761d240":"code","2540ffa5":"code","84f08cab":"code","ce28b05a":"code","bad5918e":"code","46716a33":"code","85326ddb":"code","1262339f":"code","90099062":"code","629de914":"code","f8b5c7e2":"code","722ecdf4":"code","685c3757":"code","c4869037":"code","fde2d239":"code","ca9b4da5":"code","e492b8b5":"code","674d56f2":"code","8763dd50":"code","958a25c6":"code","ec972328":"code","cf1afb37":"code","e2360774":"code","e8cb9521":"code","59156df9":"code","e16a320e":"code","fd54045e":"code","dcd4e9d1":"code","d87a4c6c":"code","ef3069ce":"code","8f1e192e":"code","7ca3aa71":"code","c4a3d19a":"code","d2ac4c42":"code","66840918":"code","29285ddc":"code","ebf8ea36":"code","9c6debbc":"code","c3653bf8":"code","c08d291a":"code","5f041acb":"code","87042f6a":"code","caa47048":"code","dcd93396":"code","1e68d2a2":"code","38b757bc":"code","5f8c9ffc":"code","8c3c5ee6":"code","5c5b99d4":"code","c411543d":"code","3d2e5c25":"code","f9b03ae3":"code","ef8bedc4":"code","67d7003c":"markdown","28b926c7":"markdown","b5fc0251":"markdown","ba427911":"markdown","e895545d":"markdown","765ad276":"markdown","51cb02b2":"markdown","67adabf5":"markdown","ad527b96":"markdown","e98376b9":"markdown","037af51e":"markdown","cefcfc2e":"markdown","b4ab9a44":"markdown","499d9ac1":"markdown","a176f5fd":"markdown","12525799":"markdown","2dfe0dc5":"markdown","31b71017":"markdown","10150f9c":"markdown","bbbcdb20":"markdown","e3977141":"markdown","3bda1afd":"markdown","aeb78242":"markdown","c5ed32c8":"markdown","ab8ab5c8":"markdown","bcf90228":"markdown","cc8dac63":"markdown","e5e4b33e":"markdown","3af77f4c":"markdown","c2b6d2cc":"markdown","49bea7b7":"markdown","5fd8cee6":"markdown","c77d27be":"markdown","cb869821":"markdown","2ea236d1":"markdown","4bb4de8b":"markdown","4fc80335":"markdown","6547261a":"markdown","49ebeac2":"markdown","53a4d0c0":"markdown","b39629ea":"markdown","bcf5cb39":"markdown","2ff45fdd":"markdown","602a7f43":"markdown","df2f0cec":"markdown","18d8b422":"markdown","8ccfc45f":"markdown","afe56c51":"markdown","9777b241":"markdown","9927d6f1":"markdown","a2cb66f7":"markdown","2577fde6":"markdown","1e397b1a":"markdown","eb07c91f":"markdown","10bcf182":"markdown","b9f261f5":"markdown","54c4e158":"markdown","1adc0e06":"markdown"},"source":{"b72dedcf":"#import some necessary librairies\n\nimport numpy as np \nimport pandas as pd \n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\nimport missingno as missing\n\npd.set_option('display.float_format', lambda x: '{:.4f}'.format(x)) #Limiting floats output to 3 decimal points\n\nprint(\"All imported OK\")","126f9ca9":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","0cc673c4":"##display the first five rows of the train dataset.\ntrain.head(5)","25347826":"##display the first five rows of the test dataset.\ntest.head(5)\n","988d059c":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","a27b2296":"fig, ax= plt.subplots()\nax.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.xlabel('Living area (sq. feet)', fontsize=15)\nplt.ylabel('Sale Price ($)', fontsize=15)\n","7fa14473":"#The dropping is tricky. Always train.drop(train.isnull()), so train.drop(train.something)\n\ntrain=train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\nfig, ax= plt.subplots()\nax.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.xlabel('Living area (sq. feet)', fontsize=15)\nplt.ylabel('Sale Price ($)', fontsize=15)","8bed8e10":"#save the price\ny=train[\"SalePrice\"]\n#Double sum to avoid one value for each column\ntest.isnull().sum().sum()\n#ok, there are a lot. So imputing will be necessary.\ntest.isnull().sum()","dfb1a02b":"train_price=train.SalePrice.values\nall_data=pd.concat([train,test]).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data.shape","434646be":"#Use missingno to check the values\n\nmissing.bar(all_data);#Black is present data, missing values are removed. Like Alley\nmissing.heatmap(all_data);","d9630b3b":"#Now let's see where the missing values are\n\nall_data_nan=(all_data.isnull().sum())\/(len(all_data))\nall_data_nan=all_data_nan.drop(all_data_nan[all_data_nan==0].index).sort_values(ascending=False)\n#in the [] it's all_data_nan==0, not all_data_nan.isnull()==0, because I'm dropping the ones without\n#nulls, which I already counted.\n\nmiss_data=pd.DataFrame({'Missing proportion':all_data_nan})\nmiss_data.head()\n","ecac4757":"#Now I make a bar plot, missingno it's not great for this. I'll make my own also, I don't have much experience with seaborn\n\nplt.subplots(figsize=(13,13))\nplt.xticks(rotation=90)\nax = sns.barplot(x=miss_data.index, y=miss_data['Missing proportion'])\nplt.xlabel('Features', size=15)\nplt.ylabel('Missing proportion', size=15)\n","6761d240":"all_data[\"PoolQC\"]=all_data[\"PoolQC\"].fillna(\"None\")","2540ffa5":"all_data[\"MiscFeature\"]=all_data[\"MiscFeature\"].fillna(\"None\")","84f08cab":"all_data[\"Alley\"]=all_data[\"Alley\"].fillna(\"None\")","ce28b05a":"all_data[\"Fence\"]=all_data[\"Fence\"].fillna(\"None\")","bad5918e":"all_data[\"FireplaceQu\"]=all_data[\"FireplaceQu\"].fillna(\"None\")","46716a33":"#This can be done with a lambda function. I don't like lambda function because they don't help to read the code but I don't see an easy way of doing it without using a lambda.\n#They hurt readability, but they have a reason to exist. I don't like them but I use them if I need to.\n\n#First, group by neighborhood. Then, take the \"LotFrontage\" column and fill NaN with the median of each neighborhood.\n\nall_data[\"LotFrontage\"]=all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","85326ddb":"for col in (\"GarageQual\", \"GarageType\", \"GarageCond\", \"GarageFinish\"):\n    all_data[col]=all_data[col].fillna(\"None\")","1262339f":"#0, not \"0\" because I want a number, not a string!!\n#I can also copy paste\nfor col in (\"GarageYrBlt\", \"GarageArea\", \"GarageCars\"):\n    all_data[col]=all_data[col].fillna(0)","90099062":"for col in (\"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"):\n    all_data[col]=all_data[col].fillna(0)","629de914":"for col in (\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\"):\n    all_data[col]=all_data[col].fillna(\"None\")","f8b5c7e2":"mszo=all_data[\"MSZoning\"].value_counts()\nmszo_ratio=mszo\/(mszo.sum())\nprint(\"The number of values are: \",mszo)\nprint(\"The ratio of values are: \",mszo_ratio)\n","722ecdf4":"all_data[\"MSZoning\"]=all_data[\"MSZoning\"].fillna(\"RL\")","685c3757":"uti=all_data[\"Utilities\"].value_counts()\nuti_ratio=uti\/(uti.sum())\nprint(\"The number of values are: \",uti)\nprint(\"The ratio of values are: \",uti_ratio)","c4869037":"all_data[\"Utilities\"]=all_data[\"Utilities\"].fillna(\"AllPub\")","fde2d239":"all_data[\"Functional\"]=all_data[\"Functional\"].fillna(\"Typ\")","ca9b4da5":"ele=all_data[\"Electrical\"].value_counts()\nele_ratio=ele\/(ele.sum())\nprint(\"The number of values are: \",ele)\nprint(\"The ratio of values are: \",ele_ratio)","e492b8b5":"all_data[\"Electrical\"]=all_data[\"Electrical\"].fillna(\"SBrkr\")","674d56f2":"kit=all_data[\"KitchenQual\"].value_counts()\nkit_ratio=kit\/(kit.sum())\nprint(\"The number of values are: \",kit)\nprint(\"The ratio of values are: \",kit_ratio)","8763dd50":"all_data[\"KitchenQual\"]=all_data[\"KitchenQual\"].fillna(\"TA\")","958a25c6":"e1=all_data[\"Exterior1st\"].value_counts()\ne1_ratio=e1\/(e1.sum())\nprint(\"The number of values are: \",e1)\nprint(\"The ratio of values are: \",e1_ratio)","ec972328":"e2=all_data[\"Exterior2nd\"].value_counts()\ne2_ratio=e2\/(e2.sum())\nprint(\"The number of values are: \",e2)\nprint(\"The ratio of values are: \",e2_ratio)","cf1afb37":"all_data[\"Exterior1st\"]=all_data[\"Exterior1st\"].fillna(\"VinylSd\")\nall_data[\"Exterior2nd\"]=all_data[\"Exterior2nd\"].fillna(\"VinylSd\")","e2360774":"e2=all_data[\"SaleType\"].value_counts()\ne2_ratio=e2\/(e2.sum())\nprint(\"The number of values are: \",e2)\nprint(\"The ratio of values are: \",e2_ratio)","e8cb9521":"all_data[\"SaleType\"]=all_data[\"SaleType\"].fillna(\"WD\")","59156df9":"all_data[\"MasVnrType\"]=all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"]=all_data[\"MasVnrArea\"].fillna(0)","e16a320e":"e2=all_data[\"MSSubClass\"].value_counts()\ne2_ratio=e2\/(e2.sum())\nprint(\"The number of values are: \",e2)\nprint(\"The ratio of values are: \",e2_ratio)","fd54045e":"all_data[\"MSSubClass\"]=all_data[\"MSSubClass\"].fillna(\"None\")","dcd4e9d1":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","d87a4c6c":"#missing.bar(all_data);\ncheck=all_data.isnull().sum().sum()\nprint(\"Total number of NaN values is: \", check)\nprint(all_data.shape)","ef3069ce":"#I do this with the TRAIN set, not the test, which obviously doesn't have a price column\n\nsns.distplot(train[\"SalePrice\"], fit=norm);\n#Get mu and sigma used by the distribution\n\n[mu,sigma]=norm.fit(train[\"SalePrice\"])\nprint(\"mu = {:.2f} and sigma = {:.2f}\".format(mu, sigma))\n\n#Now make the plot a bit more decent\n\nplt.legend([\"Normal dist $\\mu$={:.2f}, $\\sigma$={:.2f}\".format(mu,sigma)])\n\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Sale price ($)\")\nplt.title(\"Sale price distribution\")\n","8f1e192e":"fig=plt.figure()\nqq=stats.probplot(train[\"SalePrice\"], plot=plt);\nplt.show()\n","7ca3aa71":"#y=np.log1p(y)\ntrain[\"SalePrice\"]=np.log1p(train[\"SalePrice\"])\ny=train[\"SalePrice\"]\n#y","c4a3d19a":"#And now let's check again that the transformation is useful\nsns.distplot(train[\"SalePrice\"], fit=norm);\n#Get mu and sigma used by the distribution\n\n[mu,sigma]=norm.fit(train[\"SalePrice\"])\nprint(\"mu = {:.2f} and sigma = {:.2f}\".format(mu, sigma))\n\n#Now make the plot a bit more decent\n\nplt.legend([\"Normal dist $\\mu$={:.2f}, $\\sigma$={:.2f}\".format(mu,sigma)])\n\nplt.ylabel(\"Frequency\")\nplt.xlabel(\"Sale price ($)\")\nplt.title(\"Sale price distribution\")\n\n#And QQ' plot to save space\nfig=plt.figure()\nqq=stats.probplot(train[\"SalePrice\"], plot=plt);\nplt.show()","d2ac4c42":"#first, calculate skewness\n\nnumeric=all_data.dtypes[all_data.dtypes!=\"object\"].index\nskewed_f=all_data[numeric].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed=pd.DataFrame({\"Skewness\" : skewed_f})\nskewed.head()","66840918":"#box cox transform, without knowing lambda. I know the skewness of a normal distribution is 3, so I will set a limit on 1\n\nskewed = skewed[abs(skewed)>0.5]\nskewed.dropna(inplace=True)\nprint(\"The number of skewed features is: \",skewed.shape[0])","29285ddc":"from scipy.special import boxcox1p, inv_boxcox1p\nfeat=skewed.index\n\nlmbda=0.25 #Arbitrary value. boxcox from scipy.stats has an optimizer, to find lambda. 0.25 is relatively high, but should work\n\nfor col in feat:\n    all_data[col]=boxcox1p(all_data[col], lmbda)\n#for col in feat:\n#    all_data[col]=inv_boxcox1p(all_data[col],lmbda)","ebf8ea36":"#check skewness\n\nnumeric2=all_data.dtypes[all_data.dtypes!=\"object\"].index\nskewed_2=all_data[numeric2].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewed2=pd.DataFrame({\"Skewness\" : skewed_2})\nskewed2.head()","9c6debbc":"print(\"DF shape before encoding is: \",all_data.shape)\n\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)\n\nprint(\"DF shape after encoding is: \",all_data.shape)","c3653bf8":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import r2_score\n\nfrom xgboost.sklearn import XGBRegressor\nimport lightgbm as LGB","c08d291a":"\ntrain = all_data[:len(train.index)]\ntest = all_data[len(train.index):]\n#train.head()\n#trial\n#y=log1pp of train[\"SalePrice\"], saved from before\nX=train\n\ntrain_X,val_X,train_y,val_y=train_test_split(X,y,test_size=0.2,shuffle=True,random_state=13)","5f041acb":"#This algorithm is sensitive to outliers. That's why I imported RobustScaler()\nLasso_reg=Lasso()\nparam_alpha={\"alpha\":[1e-6,1e-4, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 5, 10, 20]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_Lasso=GridSearchCV(estimator=Lasso_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1, refit=True)\n#I can directly fit and predict with the best param!!\n\nCV_Lasso_fit=CV_Lasso.fit(train_X,train_y)\npred_Lasso=CV_Lasso.predict(val_X)\npred_Lasso=np.expm1(pred_Lasso)\nval_y_u=np.expm1(val_y)\n\nalpha_good=CV_Lasso.best_params_\nprint(\"The best parameter alpha for the Lasso regressor is:\",CV_Lasso.best_params_)\nprint(\"The best score (NegMAE) for the Lasso regressor is:\",-CV_Lasso.best_score_)\n\nMAE_Lasso=MAE(val_y_u,pred_Lasso)\nr2_Lasso=r2_score(val_y_u,pred_Lasso)\n\nprint(\"The MAE of the prediction is:\", MAE_Lasso)\nprint(\"The R^2 of the prediction is:\", r2_Lasso)\n","87042f6a":"#This algorithm is sensitive to outliers. That's why I imported RobustScaler()\nRidge_reg=Ridge()\nparam_alpha={\"alpha\":[1e-6,1e-4, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 5, 10, 20]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_Ridge=GridSearchCV(estimator=Ridge_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_Ridge=CV_Ridge.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_Ridge=CV_Ridge.predict(val_X)\npred_Ridge=np.expm1(pred_Ridge)\n\nprint(\"The best parameter alpha for the Ridge regressor is:\",CV_Ridge.best_params_)\nprint(\"The best score (NegMAE) for the Ridge regressor is:\",-CV_Ridge.best_score_)\n\nMAE_Ridge=MAE(val_y_u,pred_Ridge)\nr2_Ridge=r2_score(val_y_u,pred_Ridge)\n\nprint(\"The MAE of the prediction is:\", MAE_Ridge)\nprint(\"The R^2 of the prediction is:\", r2_Ridge)","caa47048":"#This will take longer, it iterates over a much bigger grid\nENet_reg=ElasticNet()\nparam_alpha={\"alpha\":[1e-6,1e-4, 0.01, 0.1, 0.25, 0.5, 0.75, 1, 2, 5, 10, 20],\n            \"l1_ratio\":[1e-6,1e-4, 0.01, 0.1, 0.25, 0.5, 0.75, 1]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_ENet=GridSearchCV(estimator=ENet_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_ENet=CV_ENet.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_ENet=CV_ENet.predict(val_X)\npred_ENet=np.expm1(pred_ENet)\n\nprint(\"The best parameter alpha for the Elastic Net regressor is:\",CV_ENet.best_params_)\nprint(\"The best score (NegMAE) for the Elastic Net regressor is:\",-CV_ENet.best_score_)\n\nMAE_ENet=MAE(val_y_u,pred_ENet)\nr2_ENet=r2_score(val_y_u,pred_ENet)\n\nprint(\"The MAE of the prediction is:\", MAE_ENet)\nprint(\"The R^2 of the prediction is:\", r2_ENet)","dcd93396":"Lasso_AIC=LassoLarsIC(criterion=\"aic\")\nLasso_AIC.fit(train_X,train_y)\nalpha_aic_ = Lasso_AIC.alpha_\npred_AIC = Lasso_AIC.predict(val_X)\npred_AIC = np.expm1(pred_AIC)\n\nMAE_AIC=MAE(val_y_u,pred_AIC)\nr2_AIC=r2_score(val_y_u,pred_AIC)\n\nprint(\"The alpha value using AIC is:\",alpha_aic_)\nprint(\"The MAE of the prediction using AIC is:\", MAE_AIC)\nprint(\"The R^2 of the prediction using AIC is:\", r2_AIC)\n\nLasso_BIC=LassoLarsIC(criterion=\"bic\")\nLasso_BIC.fit(train_X,train_y)\nalpha_bic_ = Lasso_BIC.alpha_\npred_BIC = Lasso_BIC.predict(val_X)\npred_BIC = np.expm1(pred_BIC)\n\nMAE_BIC=MAE(val_y_u,pred_BIC)\nr2_BIC=r2_score(val_y_u,pred_BIC)\n\nprint(\"The alpha value using AIC is:\",alpha_bic_)\nprint(\"The MAE of the prediction using BIC is:\", MAE_BIC)\nprint(\"The R^2 of the prediction using BIC is:\", r2_BIC)\n\n","1e68d2a2":"KRidge_reg=KernelRidge()\nparam_alpha={\"kernel\":[\"linear\",\"poly\",\"rbf\"],\n            \"alpha\":[1e-4, 1e-2, 1, 2, 5, 10, 20],\n            \"gamma\":[1e-4, 1e-2, 1, 2, 5, 10, 20]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_KerRidge=GridSearchCV(estimator=KRidge_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_KerRidge=CV_KerRidge.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_KerRidge=CV_KerRidge.predict(val_X)\npred_KerRidge=np.expm1(pred_KerRidge)\n\nprint(\"The best parameter alpha for the Kernel Ridge regressor is:\",CV_KerRidge.best_params_)\nprint(\"The best score (NegMAE) for the Kernel Ridge regressor is:\",-CV_KerRidge.best_score_)\n\nMAE_KerRidge=MAE(val_y_u,pred_KerRidge)\nr2_KerRidge=r2_score(val_y_u,pred_KerRidge)\n\nprint(\"The MAE of the prediction is:\", MAE_KerRidge)\nprint(\"The R^2 of the prediction is:\", r2_KerRidge)","38b757bc":"%%time\n#RF will fit by a brute force approach, generating many trees. Should be faster because trees are fast.\nRF_reg=RandomForestRegressor()\nparam_alpha={\"n_estimators\":[100,200,500],\n            \"max_depth\":[3,5,10,None], #More than 3 may overfit unnecessarily. Small gamma is also tricky\n            \"max_features\":[10, \"sqrt\", \"log2\", None]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_RF=GridSearchCV(estimator=RF_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_RF=CV_RF.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_RF=CV_RF.predict(val_X)\npred_RF=np.expm1(pred_RF)\n\nprint(\"The best parameter alpha for the RF regressor is:\",CV_RF.best_params_)\nprint(\"The best score (NegMAE) for the RF regressor is:\",-CV_RF.best_score_)\n\nMAE_RF=MAE(val_y_u,pred_RF)\nr2_RF=r2_score(val_y_u,pred_RF)\n\nprint(\"The MAE of the prediction is:\", MAE_RF)\nprint(\"The R^2 of the prediction is:\", r2_RF)","5f8c9ffc":"%%time\n#For comparison later.\n#GB is sequential\nGB_reg=GradientBoostingRegressor()\nparam_alpha={\"loss\":[\"huber\"], #Don't increase the grid too much, loss comparison is a bit unnecessary\n            \"learning_rate\":[1e-2, 5e-2, 1e-1, 2e-1],\n            \"n_estimators\":[300, 500],#Remove max_depth as a parameter, the best was None\n            \"max_features\":[10, \"sqrt\", \"log2\", None]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_GB=GridSearchCV(estimator=GB_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_GB=CV_GB.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_GB=CV_GB.predict(val_X)\npred_GB=np.expm1(pred_GB)\n\nprint(\"The best parameter alpha for the GB regressor is:\",CV_GB.best_params_)\nprint(\"The best score (NegMAE) for the GB regressor is:\",-CV_GB.best_score_)\n\nMAE_GB=MAE(val_y_u,pred_GB)\nr2_GB=r2_score(val_y_u,pred_GB)\n\nprint(\"The MAE of the prediction is:\", MAE_GB)\nprint(\"The R^2 of the prediction is:\", r2_GB)","8c3c5ee6":"%%time\n#For comparison later.\n#GB is sequential\nXGB_reg=XGBRegressor()\nparam_alpha={\"eta\":[1e-6,1e-4,1e-2,5e-2,1e-1,2.5e-1], #Don't increase the grid too much, loss comparison is a bit unnecessary\n            \"gamma\":[1e-3,1e-1,1,2,5,10,20],\n             \"n_estimators\":[300,500],\n            \"max_depth\":[5,7,10]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_XGB=GridSearchCV(estimator=XGB_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_XGB=CV_XGB.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_XGB=CV_XGB.predict(val_X)\npred_XGB=np.expm1(pred_XGB)\n\nprint(\"The best parameter alpha for the GB regressor is:\",CV_XGB.best_params_)\nprint(\"The best score (NegMAE) for the GB regressor is:\",-CV_XGB.best_score_)\n\nMAE_XGB=MAE(val_y_u,pred_XGB)\nr2_XGB=r2_score(val_y_u,pred_XGB)\n\nprint(\"The MAE of the prediction is:\", MAE_XGB)\nprint(\"The R^2 of the prediction is:\", r2_XGB)","5c5b99d4":"%%time\n#For comparison later.\n#GB is sequential\nLGB_reg=LGB.LGBMRegressor()\nparam_alpha={\"boosting_type\":[\"gbdt\"],\n            \"learning_rate\":[1e-2, 5e-2, 1e-1, 2e-1, 3e-1],\n            \"num_leaves\":[10, 15, 20],\n            \"n_estimators\":[300,500],\n            \"stratified\":[False],\n            \"max_depth\":[5,7,10]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\nCV_LGB=GridSearchCV(estimator=LGB_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\nCV_LGB=CV_LGB.fit(train_X,train_y)\n#I can directly predict with the best param!!\npred_LGB=CV_LGB.predict(val_X)\npred_LGB=np.expm1(pred_LGB)\n\nprint(\"The best parameter alpha for the LGB regressor is:\",CV_LGB.best_params_)\nprint(\"The best score (NegMAE) for the LGB regressor is:\",-CV_LGB.best_score_)\n\nMAE_LGB=MAE(val_y_u,pred_LGB)\nr2_LGB=r2_score(val_y_u,pred_LGB)\n\nprint(\"The MAE of the prediction is:\", MAE_LGB)\nprint(\"The R^2 of the prediction is:\", r2_LGB)","c411543d":"%%time\n#COMMENTED FOR SPEED!! UNCOMMENT IF TIME IS NOT CRITICAL ~tens of minutes\n#SVR are slow, especially with large datasets (not this case), also, a 3D grid will increase time significantly.\n#SVR_reg=SVR()\n#param_alpha={\"kernel\":[\"linear\",\"poly\",\"rbf\"], #poly and rbf are not a good idea, this is linearized\n#            \"degree\":[2,3], #More than 3 may overfit unnecessarily. Small gamma is also tricky\n#            \"gamma\":[1e-3, 1e-2, 1e-1, 1, 2, 5]} #I think this values are enough\n#La_train_X=RobustScaler().fit(train_X)\n#CV_SVR=GridSearchCV(estimator=SVR_reg,param_grid=param_alpha,scoring=\"neg_mean_absolute_error\",cv=5,n_jobs=-1)\n#CV_SVR=CV_SVR.fit(train_X,train_y)\n#I can directly predict with the best param!!\n#pred_SVR=CV_SVR.predict(val_X)\n#pred_SVR=np.expm1(pred_SVR)\n\n#print(\"The best parameter alpha for the SVR regressor is:\",CV_SVR.best_params_)\n#print(\"The best score (NegMAE) for the SVR regressor is:\",-CV_SVR.best_score_)\n\n#MAE_SVR=MAE(val_y_u,pred_SVR)\n#r2_SVR=r2_score(val_y_u,pred_SVR)\n\n#print(\"The MAE of the prediction is:\", MAE_SVR)\n#print(\"The R^2 of the prediction is:\", r2_SVR)","3d2e5c25":"models=[\"Lasso\", \"Ridge\", \"Elastic Net\",\"Lasso AIC\", \"Lasso BIC\", \"Kernel Ridge\",\"Random Forest\", \"GBDT\", \"XGBoost\",\"LightBM\"]\nmaes=[MAE_Lasso, MAE_Ridge, MAE_ENet, MAE_AIC, MAE_BIC, MAE_KerRidge, MAE_RF, MAE_GB, MAE_XGB, MAE_LGB]\nr2s=[r2_Lasso, r2_Ridge, r2_ENet, r2_AIC, r2_BIC, r2_KerRidge, r2_RF, r2_GB, r2_XGB, r2_LGB]\n\n#Add SVR at the end if uncommented\n","f9b03ae3":"plt.subplots(figsize=(8,5))\nplt.xticks(rotation=90)\nax = sns.barplot(x=models, y=maes)\nsns.plt.ylim(14e3, 18e3)\nplt.xlabel('Model', size=15)\nplt.ylabel('MAE ($)', size=15)","ef8bedc4":"plt.subplots(figsize=(8,5))\nplt.xticks(rotation=90)\nax = sns.barplot(x=models, y=r2s)\nsns.plt.ylim(0.8, 1)\nplt.xlabel('Model', size=15)\nplt.ylabel('R^2 of every model (-)', size=15)","67d7003c":"Interestingly enough, a GradientBoostingRegressor is faster, although it's probably due to the differences in the GridSearchCV grid. However, they both select the same parameters for the trees (max features, n_estimators). GBT is significantly better by my metrics.","28b926c7":"Very nice! Now the distribution looks much better, like the linear models like. NOW LET'S LOOK AT THE SKEWNESS OF THE FEATURES (Not the price, the features!!) Remember, garbage in, garbage out. If my input is very skewed, my predictions will also be skewed. For reference, it's been a while since I studied this. Skewness means the tail on one side is longer than it should be (left is -, right is +).\nSkewness is the 3rd standardized moment, kurtosis is the 4th. Skewness shows if one tail dominates while kurtosis shows how much the tails dominate with respect to the total distribution. It may be more or less open (serr\u00e9e).\n[https:\/\/en.wikipedia.org\/wiki\/Skewness](http:\/\/)\n\n[https:\/\/en.wikipedia.org\/wiki\/Kurtosis](http:\/\/)\n\n","b5fc0251":"So Fill with SBrkr","ba427911":"Somehow the elastic net regression converges to the Lasso. Seems that in these three models I'm off by 14k. Not too bad when you consider that a house is in the order of 200k. I have ~7% error.\nNow let's try the LassoLarsIC, as a comparison, using both criteria.","e895545d":"Now I have a lot of Basement features. Seeing the description, they are 0 if they show NA.","765ad276":"Now all the data is cleaned\/transformed\/encoded, the libraries are all imported and I kept some of the training set as validation to check. My strategy is going to be the following. Find the best parameters for every algorithm with a simple grid search, and then keep the best and do cross validation. Finally, compare all the results.","51cb02b2":"I guess NaN means none then. Rich people don't need any loan. However, there are not too many, which make sense because rich people are few.","67adabf5":"Now I have the price \"cleaned\". Let\u00b4s see if there are any NaNs or something.","ad527b96":"MiscFeature is any additional features the house may have. Tennis court, 2nd garage, etc. NA mins none. Let's do the same as with the pool.","e98376b9":"Ok, with these values, let's fill with AllPub.","037af51e":"And this should finish all the imputations. Let's see if I missed something.","cefcfc2e":"Take the train data and split in train and validation. Let's keep 20% for validation.","b4ab9a44":"So now let's see what each variable does, and then we look at the data. Some people transform it to make it more normal or linear and then clean it, but I think that way may be dangerous. First I clean and then I transform. By cleaning I may not keep track of the changes I make to the transformed data. Start looking in the description file and hope I find the meaning of every feature.","499d9ac1":"PoolQC is the pool quality, but many houses don't have a pool. Change the missing value to None","a176f5fd":"77% of the data are RL. I'll use that to fill the NaN because they are few and they won't change the result too much.","12525799":"Ok, the price is reasonably linear with the size, except for the two bottom right houses. In this case, being only two, I think I can drop them and go on. Another option is to make my model resistant to outliers, but being two only, I don't see the point. If the test files has outliers, it should be done. Besides, since there are too many variables, I can't check that there are not any outliers in other columns, so I'll drop these two and try to normalize or transform the rest.","2dfe0dc5":"Not too normal, let's see the QQ' plot. I'll probably need to \"linearize\" the data and the QQ' plot may give me an insight on how to do it.","31b71017":"Functional. Description file says to assume typical (\"Typ\"). Easy.","10150f9c":"Ok, take the middle street and then fill with the most common value.","bbbcdb20":"Dwelling consists on using a property as a warrant of a loan. This is not a mortgage loan, but a loan especially used to buy a house to live there. Apparently there is a difference. In this case, my problem is the same as before.\nSee: [https:\/\/forum.wordreference.com\/threads\/first-lien-dwelling-secured-loan.3469532\/](http:\/\/)","e3977141":"#Data Processing","3bda1afd":"MSZoning is the zoning clasification. Let's see what they are, because NaN doesn't have an obvious meaning. An easy approach, group by neighborhood and then fill with the mode, but the amount of missing values is very small. THE TOTAL NUMBER OF ROWS IS ~2960. KEEP IT IN MIND FOR COMPARISON.","aeb78242":"So, after all this looooong imputation, let's have a look at the data. How is the sale price distribution and the QQ' plot, it's one of my favourites. I use sns.distplot. Common in the physicists' presentations: [https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html](http:\/\/)","c5ed32c8":"Now check the skewness again","ab8ab5c8":"Good, the most common again! It's almost WD40 XD","bcf90228":"Now with these results I can choose a model (or a few) and combine them to get better predictions. Besides, I can optimize the whole model by optimizing each individual one and then I combine (for example average, or weighte av. or whatever) them. I won't do this, but I can. The kernel that I quote at the beginning does that.","cc8dac63":"So fill with TA","e5e4b33e":"There are so many categories, so let's sort them and plot only the amount of values missing in the relevant columns. I don't care about the complete columns.","3af77f4c":"##Outliers","c2b6d2cc":"The higher end prices are further from the ideal distribution than the lower ones. A good transform may be the log1p, it penalizes the higher values. Don't forget to apply inverse transform later!!","49bea7b7":"At least I chose proper dimensions for the plots.","5fd8cee6":"Utilities, it is electricity, gas, etc. Let's repeat the same approach. If it works it works.","c77d27be":"LotFrontage: Linear feet of street connected to property. I guess it's like the houses in Sueca or in the Netherlands. Let's fill with the median of the neighborhood. Average may be skewed, and values from another neighborhood won't be too representative.","cb869821":"Re-split the data to train and test datasets","2ea236d1":"Import all the models I'm going to use, and then I'll compare them. I'll check them with MSE, I think it's a reasonable metric.","4bb4de8b":"Ok, not everything has the same skewness. That means my skew calculation must be correct.","4fc80335":"Add the total surface as a feature. That is quite important.","6547261a":"Ridge and Lasso predictions are aligned. I include the R^2 as an additional measure of the goodness of the fit. Now try elastic net, which attempts to correct all the pitfalls of Lasso.","49ebeac2":"Different Regressions to predict House Prices \n\nThis is inspired on the notebook: [https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard](http:\/\/)\n\nThings I do in this notebook\n\n- **Imputing missing values**  by proceeding sequentially through the data\n\n- **Transforming** some numerical variables that seem really categorical\n\n- **Label Encoding** some categorical variables that may contain information in their ordering set One-hot encoding.\n\n-  [**Box Cox Transformation**] and log-transformation. Compare.\n\n- **Compare different regression models**\n","53a4d0c0":"Fence availability. Very american, to park your track in your front yard .","b39629ea":"I leave SVR for the last one. Previously it was buried somewhere.\nThe SVR takes very long to run. In any case, I shouldn't try to fit the linearized version of my data with a non-linear model. Obviously, but still. If I can leave it overnight I can run this, otherwise I comment it.","bcf5cb39":"SaleType, same approach.","2ff45fdd":"There are a lot of data missing. Let's combine the two dataframes in 1 and fix the data","602a7f43":"Exterior1st and Exterior2nd, let's see again.","df2f0cec":"FireplaceQu is the fireplace quality. Not many people have a fireplace. Again, fill with None","18d8b422":"Very sparse dataset now. Anyway, pandas and numpy are good for this.","8ccfc45f":"Then I have categorical basement columns. NaN means None.","afe56c51":"Alley is type of access to property. I guess it's the walkway. Again, fill with None","9777b241":"Electrical. Description doesn't help much. They still use fuseboxes, like in Pays de Gex. Repeat the same approach as before to see if I get somewhere.","9927d6f1":"It improved a little bit. Probably with an automatic search I could do it better.\nNow, let's encode the categorical features. I use one-hot encoding, I don't think that LabelEncoder is necessary because I don't see why these features may contain information on their ordering. This may be wrong, I'm not an expert in housing.","a2cb66f7":"KitchenQual. It can't be none because most of the houses have a kitchen. Some people advertising houses in CERN market or Geneva facebook groups disagree, but let's be honest.","2577fde6":"Not a bad model. At the beginning I thought I may be overfitting because I forgot to untransform the prices. Let's check a couple more regression models. It's a good way to see if one particular model is too good.\nNow let's check ridge. There is a built in CV implementation of the model, but if I use the GridSearchCV function I can copy and paste many things","1e397b1a":"Plot the data, I'm sure there will be some outliers. If there are a lot, then it's bad. The obvious problem is to check the price and the size of the house. I could check also with the location if I knew the place, but I don't. The name of the column for the size is \"GrLivArea\" according to the documentation. It's in sq. feet, the sizes are not that huge! (divide by 10 and more or less you have the size in m^2, which are reasonable units).","eb07c91f":"Interestingly enough, using AIC I get a significantly better prediction than if I use BIC. This may be due to the max_iter or some of these parameters. The alpha values seem to converge to a similar value.","10bcf182":"MasVnrType and Area, they say that there is a label called none. I say this is the one.","b9f261f5":"GarageQual, type cond and finish, are Nan if they don't have garage. Replace with None.","54c4e158":"LASSO REGRESSION\n[https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/complete-tutorial-ridge-lasso-regression-python\/](http:\/\/)\n","1adc0e06":"GarageYrBlt, GarageArea, GarageCars will have the same meaning, no garage, no year built, no cars, but they are numeric. Fill with 0"}}