{"cell_type":{"f6f4c59e":"code","31113ada":"code","fda63bf8":"code","9a4b930c":"code","95b95d1e":"code","91d1a96f":"code","9d0d726c":"code","ef23008c":"code","d84662a9":"code","7f514c10":"code","a4ba7d58":"code","2d8e570b":"code","d556bf07":"code","c2230afe":"code","0f228cbc":"code","50d27a4a":"code","f9748840":"code","fea1c635":"code","b8dc85e8":"code","ddfd1752":"code","e182b5ac":"code","b79e383f":"code","46536b3b":"code","f5487223":"code","0c47691c":"code","9a175fd0":"code","39ebe219":"code","7a3553c6":"code","9fb564b2":"code","396b8073":"code","0a5e2e2f":"code","666a756f":"code","e80a80c0":"code","7ac620f1":"code","a65e3aa0":"code","3f5d201c":"code","e5b3d764":"code","fcc02918":"code","47a716a2":"code","41a8d796":"code","08d09b6a":"code","f9021bb3":"markdown","b1f990b9":"markdown","350ff5b2":"markdown","d4912d23":"markdown"},"source":{"f6f4c59e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31113ada":"df=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","fda63bf8":"df.head(2)","9a4b930c":"df.info()","95b95d1e":"df.describe().T","91d1a96f":"# Analyzing Quantitative variables.\nplt.figure(figsize=(30, 30))\nsns.set(font_scale=1.5)\nind = 1\n\nfor col in df.columns:\n    plt.subplot(4, 3, ind)\n    sns.boxplot(x=df[col])\n    ind += 1","9d0d726c":"sns.countplot(x=df.quality)","ef23008c":"y=df.quality.replace({3:0, 4:0, 5:0, 6:0, 7:1, 8:1})","d84662a9":"y.value_counts()","7f514c10":"fig, axes = plt.subplots(4, 3, figsize = (15,15))\naxes = axes.flatten()\n\nfor i in range(0,len(df.columns)-1):\n    sns.barplot(x=y, y=df.iloc[:,i], data=df, orient='v', ax=axes[i])\n\nplt.tight_layout()\nplt.show()","a4ba7d58":"df.quality=df.quality.replace({3:0, 4:0, 5:0, 6:0, 7:1, 8:1})","2d8e570b":"sns.pairplot(data=df, hue='quality')","d556bf07":"# Data Pre-processing with different normalization options.\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef data_preprocess(X,y,std_scale=False,minmax_scale=False):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n    \n    if std_scale or minmax_scale:\n        if std_scale:\n            scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n        else:\n            scaler = MinMaxScaler(copy=True,feature_range=(0,1))\n            \n        scaler.fit(X_train)\n\n        train_scaled = scaler.transform(X_train)\n        test_scaled = scaler.transform(X_test)\n    else:\n        train_scaled, test_scaled = X_train, X_test\n    \n    return(train_scaled, test_scaled, y_train, y_test)","c2230afe":"from sklearn.linear_model import LogisticRegression\n\ndef logistic_regression(X_train,X_test,y_train,y_test,cls_weight=None):\n    logreg = LogisticRegression(class_weight=cls_weight).fit(X_train, y_train)\n    print(\"Training set score: {:.3f}\".format(logreg.score(X_train,y_train)))\n    print(\"Test set score: {:.3f}\".format(logreg.score(X_test,y_test)))\n    return(logreg)","0f228cbc":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, matthews_corrcoef\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\")\n        print(\"_______________________________________________\")\n        print(f\"MCC: {matthews_corrcoef(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\")\n        print(\"_______________________________________________\")\n        print(f\"MCC: {matthews_corrcoef(y_test, pred)}\")","50d27a4a":"# Heatmap\nplt.figure(figsize=(len(df.columns), len(df.columns)-7))\nsns.heatmap(df.corr(), annot=True, cmap=\"RdYlGn\", annot_kws={\"size\":15})","f9748840":"X=df.drop('quality',axis=1)","fea1c635":"X_train,X_test,y_train,y_test=data_preprocess(X,y,std_scale=True)","b8dc85e8":"log_reg=logistic_regression(X_train,X_test,y_train,y_test)","ddfd1752":"logit_model = sm.Logit(y_train, X_train)\nresult = logit_model.fit()\nresult.summary2()","e182b5ac":"X=df.drop(labels=['citric acid','chlorides','free sulfur dioxide', 'total sulfur dioxide','pH','quality'],axis=1)","b79e383f":"X_train,X_test,y_train,y_test=data_preprocess(X,y,std_scale=True)","46536b3b":"log_reg=logistic_regression(X_train,X_test,y_train,y_test)","f5487223":"logit_model = sm.Logit(y_train, X_train)\nresult = logit_model.fit()\nresult.summary2()","0c47691c":"y_pred = log_reg.predict(X_test)","9a175fd0":"print_score(log_reg,X_train,y_train,X_test,y_test,train=True)\nprint_score(log_reg,X_train,y_train,X_test,y_test,train=False)","39ebe219":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n# predict probabilities\nlr_probs = log_reg.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nlr_probs = lr_probs[:, 1]\n# calculate scores\nlr_auc = roc_auc_score(y_test, lr_probs)\nprint('Logistic: ROC AUC=%.3f' % (lr_auc))\n# calculate roc curves\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n# plot the roc curve for the model\nplt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n# axis labels\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","7a3553c6":"X=df.drop('quality',axis=1)","9fb564b2":"X_train,X_test,y_train,y_test=data_preprocess(X,y)","396b8073":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(random_state=0)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","0a5e2e2f":"path = tree_clf.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\nprint(ccp_alphas)","666a756f":"fig, ax = plt.subplots(figsize=(15,5))\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","e80a80c0":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    clf.fit(X_train, y_train)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n      clfs[-1].tree_.node_count, ccp_alphas[-1]))","7ac620f1":"clfs = clfs[:-1]\nccp_alphas = ccp_alphas[:-1]\n\nnode_counts = [tree_clf.tree_.node_count for tree_clf in clfs]\ndepth = [tree_clf.tree_.max_depth for tree_clf in clfs]\nfig, ax = plt.subplots(1, 2,figsize=(20,8))\nax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle=\"steps-post\")\nax[0].set_xlabel(\"alpha\")\nax[0].set_ylabel(\"number of nodes\")\n\nax[1].plot(ccp_alphas, depth, marker='o', drawstyle=\"steps-post\")\nax[1].set_xlabel(\"alpha\")\nax[1].set_ylabel(\"depth of tree\")","a65e3aa0":"train_scores = [tree_clf.score(X_train, y_train) for tree_clf in clfs]\ntest_scores = [tree_clf.score(X_test, y_test) for tree_clf in clfs]\n\nfig, ax = plt.subplots(figsize=(15,5))\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","3f5d201c":"tree_clf = DecisionTreeClassifier(random_state=0,ccp_alpha=0.006)\ntree_clf.fit(X_train, y_train)","e5b3d764":"print_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","fcc02918":"# Feature Importance.\nfeat_importances = pd.Series(tree_clf.feature_importances_,index=X.columns)\nfeat_importances[feat_importances.values>0].sort_values(ascending=False).plot(kind='bar')\nplt.show()\n\nprint(feat_importances[feat_importances.values>0].sort_values(ascending=False))","47a716a2":"tree_clf = DecisionTreeClassifier(random_state=0,ccp_alpha=0.006)\ntree_clf.fit(X_train[['volatile acidity','sulphates','alcohol']], y_train)","41a8d796":"print_score(tree_clf, X_train[['volatile acidity','sulphates','alcohol']], y_train, \n            X_test[['volatile acidity','sulphates','alcohol']], y_test, train=True)\nprint_score(tree_clf, X_train[['volatile acidity','sulphates','alcohol']], y_train, \n            X_test[['volatile acidity','sulphates','alcohol']], y_test, train=False)","08d09b6a":"tree_clf.predict([np.array([0.88, 0.56, 9.4])])[0]","f9021bb3":"# Logistic regression","b1f990b9":"# Decision tree with post pruning gave the best possible result","350ff5b2":"# Logistic Regression - Iteration 2","d4912d23":"# DT"}}