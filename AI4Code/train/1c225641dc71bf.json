{"cell_type":{"2b2a8a63":"code","fbf42e0d":"code","0c077ead":"code","c08a34c8":"code","078959fe":"code","b07e760f":"code","0307e384":"code","163ebc18":"code","69c014c9":"code","c6f4e25f":"code","b98efa86":"code","70f838b7":"code","f67ee72e":"code","897220b7":"code","c54771c9":"code","53fe8f92":"code","83f791e0":"code","988577e2":"code","033dd098":"code","78520d14":"code","bfb0c97f":"code","50ad806c":"code","5ba74bee":"code","f457d34c":"code","f1450ca4":"code","cb2f21bf":"code","3b011c57":"markdown","f233dd8a":"markdown","53d5e2ff":"markdown","cb3524b9":"markdown","6f0f0916":"markdown","73a2618e":"markdown","1b4f283a":"markdown"},"source":{"2b2a8a63":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fbf42e0d":"# import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\n\nfrom nltk.corpus import brown\nfrom nltk.corpus import treebank\nfrom nltk.corpus import conll2000\n\nimport seaborn as sns\n\nfrom gensim.models import KeyedVectors\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Input\nfrom keras.layers import TimeDistributed\nfrom keras.layers import LSTM, GRU, Bidirectional, SimpleRNN, RNN\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","0c077ead":"conll_corpus = conll2000.tagged_sents(tagset='universal')","c08a34c8":"conll_corpus[0]","078959fe":"X = []\ny = []\n\nfor sentences in conll_corpus:\n    X_sentence = []\n    y_sentence = []\n    for words in sentences:\n        X_sentence.append(words[0])\n        y_sentence.append(words[1])\n     \n    X.append(X_sentence)\n    y.append(y_sentence)","b07e760f":"## Counting number of unique words and unique sentences\nnum_words = len(set([word.lower() for sentence in X for word in sentence]))\nnum_tags   = len(set([word.lower() for sentence in y for word in sentence]))\n\nprint(\"Total number of tagged sentences: {}\".format(len(X)))\nprint(\"Vocabulary size: {}\".format(num_words))\nprint(\"Total number of tags: {}\".format(num_tags))","0307e384":"# Looking at input to build the model\nprint('sample X: ', X[0], '\\n')\nprint('sample Y: ', y[0], '\\n')","163ebc18":"## Defininf tokenizer\ntokenizer = Tokenizer()\n## training and fitting the tokenizer\ntokenizer.fit_on_texts(X)\nX_vectorized = tokenizer.texts_to_sequences(X)","69c014c9":"## Converting y to vectors\ntag_tokenizer = Tokenizer()\ntag_tokenizer.fit_on_texts(y)\ny_encoded = tag_tokenizer.texts_to_sequences(y)","c6f4e25f":"# Priniting X, y in original and encoded part\n\nprint(\"** Original Data **\",\"\\n\")\nprint('X: ', X[0], '\\n')\nprint('Y: ', y[0], '\\n')\nprint()\nprint(\"** Encoded data **\",\"\\n\")\nprint('X: ', X_vectorized[0], '\\n')\nprint('Y: ', y_encoded[0], '\\n')","b98efa86":"## Checking length of the sequence for Padding\nlengths = [len(seq) for seq in X_vectorized]\nprint(\"Length of longest sentence: {}\".format(max(lengths)))","70f838b7":"MAX_SEQ_LENGTH = 78\n\nEMBEDDING_SIZE  = 300 \nVOCABULARY_SIZE = len(tokenizer.word_index) + 1\n\nX_padded = pad_sequences(X_vectorized, maxlen=MAX_SEQ_LENGTH, padding=\"post\")\nY_padded = pad_sequences(y_encoded, maxlen=MAX_SEQ_LENGTH, padding=\"post\")","f67ee72e":"y = to_categorical(Y_padded)\n\nX_train, X_test, Y_train, Y_test = train_test_split(X_padded, y, test_size = 0.15)\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = 0.15)\n\nNUM_CLASSES = y.shape[2]","897220b7":"# print number of samples in each set\nprint(\"TRAINING DATA\")\nprint('Shape of input sequences: {}'.format(X_train.shape))\nprint('Shape of output sequences: {}'.format(Y_train.shape))\nprint()\nprint(\"VALIDATION DATA\")\nprint('Shape of input sequences: {}'.format(X_validation.shape))\nprint('Shape of output sequences: {}'.format(Y_validation.shape))\nprint()\nprint(\"TESTING DATA\")\nprint('Shape of input sequences: {}'.format(X_test.shape))\nprint('Shape of output sequences: {}'.format(Y_test.shape))","c54771c9":"X_train[0]","53fe8f92":"Y_train[0]","83f791e0":"model = Sequential()\nmodel.add(Embedding(input_dim = VOCABULARY_SIZE, output_dim = EMBEDDING_SIZE, input_length  =  MAX_SEQ_LENGTH, trainable = True))\nmodel.add(SimpleRNN(128, return_sequences=True, activation = \"relu\"))\nmodel.add(LSTM(128, return_sequences=True, activation = \"relu\"))\nmodel.add(TimeDistributed(Dense(NUM_CLASSES, activation='softmax')))","988577e2":"model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['acc'])","033dd098":"model.summary()","78520d14":"model_training = model.fit(X_train, Y_train, batch_size=128, epochs=35, validation_data=(X_validation, Y_validation))","bfb0c97f":"# visualise training history\nplt.plot(model_training.history['acc'])\nplt.plot(model_training.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc=\"lower right\")\nplt.show()","50ad806c":"model.get_layer(\"embedding_3\").get_weights()[0].shape","5ba74bee":"model.get_layer(\"simple_rnn_3\").get_weights()[0].shape","f457d34c":"model.get_layer(\"lstm_3\").get_weights()[0].shape","f1450ca4":"model.get_layer(\"time_distributed\").get_weights()[0].shape","cb2f21bf":"model.predict([  71, 2283, 3816,   52,  340,    1,   33,   62, 9138,   24,  347,\n         24,    2, 4112,  238,    3,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          0])","3b011c57":"## Training the model","f233dd8a":"#### This notebook is for quiz 5 q2 for class CS6120 - Natural Language Processing\n\n<br>\n<br>\n\n#### References:\n1. https:\/\/www.kaggle.com\/tanyadayanand\/pos-tagging-using-rnn\/notebook\n2. https:\/\/towardsdatascience.com\/pos-tagging-using-rnn-7f08a522f849","53d5e2ff":"# Getting X and y ","cb3524b9":"# 1. Cleaning and Processing Data","6f0f0916":"# Converting words to vectors","73a2618e":"## Split data in training, validation and tesing sets","1b4f283a":"## Padding sequences"}}