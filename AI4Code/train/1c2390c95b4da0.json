{"cell_type":{"595f4026":"code","c501209d":"code","722c4f20":"code","f4f9ca58":"code","6f640515":"code","6baee34c":"code","be0e68f5":"code","bff627e1":"code","54f30168":"code","b8681b71":"code","254b56ac":"code","20a32ab8":"code","c5938ec6":"code","2f2be43c":"code","9dfec631":"code","072dad29":"code","2db6c444":"code","df7cc8a9":"code","b84c037f":"code","52f87e67":"markdown","67593818":"markdown","a5d88f61":"markdown","ceab24a5":"markdown","04d5db0e":"markdown","cedec78a":"markdown","6a931ec1":"markdown"},"source":{"595f4026":"import scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nsns.set_palette('husl')\n\nfrom sklearn.pipeline import Pipeline\n\n# Pre-processing\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, Normalizer, \n                                      MaxAbsScaler, RobustScaler, PowerTransformer)\nfrom sklearn.preprocessing import PolynomialFeatures  # , Imputer \nfrom sklearn.experimental import enable_iterative_imputer  # to import IterativeImputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.utils import resample\n\n# Dimensionallity Reduction and Feature Extraction\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA, NMF, SparsePCA, FastICA, FactorAnalysis\nfrom sklearn.manifold import TSNE, Isomap\nfrom hdbscan import HDBSCAN\nfrom umap import UMAP\n\n# Clustering\nfrom sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\n\n# Classifiers\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\n\n# Post-analysis\nfrom sklearn.metrics import (classification_report, confusion_matrix, plot_confusion_matrix,\n                                accuracy_score, roc_curve, roc_auc_score, classification_report)\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold","c501209d":"predict_titanic_df = pd.read_csv('test.csv')\ntitanic_df = pd.read_csv('train.csv')\ntitanic_df.head()","722c4f20":"# Separate majority and minority classes\ndf_majority = titanic_df[titanic_df.Survived==0].copy()\ndf_minority = titanic_df[titanic_df.Survived==1].copy()\n \n# Upsample minority class\ndf_minority_upsampled = resample(df_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=df_majority.shape[0],    # to match majority class\n                                 random_state=42) # reproducible results\n \n# Combine majority class with upsampled minority class\nresampled_titanic_df = pd.concat([df_majority, df_minority_upsampled])\n\n\ndata_columns = resampled_titanic_df.columns.drop(['Survived', 'PassengerId'])\nX_df = resampled_titanic_df[data_columns].copy()\npredict_X_df = predict_titanic_df[data_columns].copy()\ny_df = resampled_titanic_df['Survived'].copy()\nid_df = resampled_titanic_df['PassengerId'].copy()","f4f9ca58":"y = y_df.values\n\nX_df.drop(['Name', 'Ticket', 'Cabin'], axis='columns', inplace=True)\nX_df = pd.get_dummies(X_df)\nX = X_df.values\npredict_X_df.drop(['Name', 'Ticket', 'Cabin'], axis='columns', inplace=True)\npredict_X_df = pd.get_dummies(predict_X_df)\npredict_X = predict_X_df.values\n\n\n#imputer = SimpleImputer()\nimputer = IterativeImputer()\nX = imputer.fit_transform(X)\npredict_X = imputer.fit_transform(predict_X)\n\nscaler = StandardScaler()\n#scaler = RobustScaler()\n#scaler = Normalizer()\n#scaler = MinMaxScaler()\n#scaler = PowerTransformer()\nX = scaler.fit_transform(X)\npredict_X = scaler.fit_transform(predict_X)\n\nX_df = pd.DataFrame(data=X, columns=X_df.columns)\npredict_X_df = pd.DataFrame(data=predict_X, columns=predict_X_df.columns)\nX_df.head()","6f640515":"X_df.info(), predict_X_df.info()","6baee34c":"model = RandomForestRegressor()\nmodel.fit(X, y)\n\nnum_features = 10\nfeatures = X_df.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[-num_features:]  # top \"num_features\" number of features\nextracted_features = [features[i] for i in indices]\n\nfig, ax = plt.subplots(figsize=(8, 6))\nplt.title(f'Feature Importances', fontsize=18)\nplt.bar(range(len(indices)), importances[indices], color='b', align='center')\nplt.xticks(range(len(indices)), extracted_features)\nplt.ylabel('Relative Importance', fontsize=16)\nplt.show()","be0e68f5":"reducer = PCA(n_components=4)\nX_reduced = reducer.fit_transform(X)","bff627e1":"labels = y\nreducer_name = str(reducer).split('(')[0]\n\nexp_var = np.var(X_reduced, axis=0)\/np.sum(np.var(X_reduced, axis=0))\nprint(f'Explained Variance Fraction: {exp_var}')\n\nfig, ax = plt.subplots(figsize=(8, 8))\nscatter = ax.scatter(X_reduced[:, 0], X_reduced[:, 1], c=labels, cmap=plt.cm.Set1, edgecolor='k')\nax.set_title(f\"Dimensionality Reduction {reducer_name}\", fontsize=18)\nax.set_xlabel(f\"Eigen-vector 1\", fontsize=16)\nax.set_ylabel(f\"Eigen-vector 2\", fontsize=16)\nlegend = ax.legend(*scatter.legend_elements(), ncol=2)\nax.add_artist(legend)\nplt.show()","54f30168":"X_vals = X#_reduced\n\nX_train, X_test, y_train, y_test = train_test_split(X_vals, y, test_size=0.2)\nclf = RandomForestClassifier()\n#clf = XGBClassifier()\n#clf = DecisionTreeClassifier()\n#clf = GaussianNB()\n\nclf.fit(X_train, y_train)\n\nlabels = y\nclf_name = str(clf).split('(')[0]\n\nscores = cross_val_score(clf, X_vals, y, cv=10)\nscore = np.round(scores.mean(), 3)\nstd = np.round(scores.std(), 3)\nprint(f'Score: {score} +\/- {std}')\n        \nfig, ax = plt.subplots(figsize=(8, 8))\nplot_confusion_matrix(clf, X_test, y_test, display_labels=labels, \n                          cmap=plt.cm.Blues, normalize='true', ax=ax)\nplt.title(f'Classifier {clf_name}', fontsize=18)\nplt.xticks(rotation='vertical')\nax.xaxis.label.set_size(16)\nax.yaxis.label.set_size(16)\nplt.margins(0.2)\nplt.subplots_adjust(bottom=0.15)\nplt.show()","b8681b71":"predict_y = clf.predict(predict_X)","254b56ac":"submission_df = predict_titanic_df.copy()\nsubmission_df['Survived'] = predict_y\nsubmission_df = submission_df[['PassengerId', 'Survived']]\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","20a32ab8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import resample\n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Lambda, Flatten, Activation, Reshape\nfrom keras.optimizers import Adam, RMSprop, SGD\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils import np_utils\nfrom keras import  backend as K\n\n# CNN\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, UpSampling2D\n\nfrom keras.datasets import mnist\n\n#from keras.layers.core import Dense\n\nimport theano ","c5938ec6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nnp.random.seed(42) \n\nmodel = Sequential()\nmodel.add(Dense(60, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","2f2be43c":"# Compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train, epochs=100, \n                      batch_size=32, verbose=1, \n                      validation_split=0.2,\n                      workers=4, use_multiprocessing=True)","9dfec631":"loss_train, acc_train  = model.evaluate(X_train, y_train, verbose=False)\nloss_test, acc_test  = model.evaluate(X_test, y_test, verbose=False)\nprint(f'Train acc\/loss: {acc_train:.3}, {loss_train:.3}')\nprint(f'Test acc\/loss: {acc_test:.3}, {loss_test:.3}')","072dad29":"y_pred_train = model.predict(X_train, verbose=True)\ny_pred_test = model.predict(X_test,verbose=True)","2db6c444":"# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1, 2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1, 2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","df7cc8a9":"predict_y = model.predict(predict_X)\npredict_y = np.round(predict_y.flatten(), 0).astype(int)","b84c037f":"submission_df = predict_titanic_df.copy()\nsubmission_df['Survived'] = predict_y\nsubmission_df = submission_df[['PassengerId', 'Survived']]\nsubmission_df.to_csv('submission_nn.csv', index=False)\nsubmission_df.head()","52f87e67":"### Component Analysis (optional)","67593818":"### Preprocess data","a5d88f61":"### Feature Extraction","ceab24a5":"___","04d5db0e":"### Classify","cedec78a":"### Prepare Submission","6a931ec1":"There are several things I can notice from looking at the data:\n\n  * The label to predict is 'Survived'.\n  * 'Name' doesn't seem to add any important information\n  * 'Titcket' doesn't seem to add any important information either\n"}}