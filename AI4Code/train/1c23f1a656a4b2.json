{"cell_type":{"df3c5129":"code","5f1cc6ac":"code","72639bf4":"code","45e460c2":"code","2f0c56ac":"code","f427cf0a":"code","026f2cda":"code","d4fc6ba6":"code","0f7bdae7":"code","60b93ac4":"code","0a919570":"code","fd0cba5a":"code","85f3cc72":"code","0b5cb587":"code","8845b0e9":"code","29388bad":"markdown","ebbec457":"markdown","a52372fb":"markdown","be8ffd13":"markdown","bdd41955":"markdown","2a2710ca":"markdown","1527b111":"markdown","7aa213df":"markdown"},"source":{"df3c5129":"import seaborn as sns","5f1cc6ac":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\npd.options.mode.chained_assignment = None\n","72639bf4":"train_df = pd.read_csv('..\/input\/application_train.csv')\ntest_df = pd.read_csv('..\/input\/application_test.csv')\nsample_subm = pd.read_csv('..\/input\/sample_submission.csv')","45e460c2":"train_df_ltd = train_df[['AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]\ntest_df_ltd = test_df[['AMT_INCOME_TOTAL', 'AMT_ANNUITY', 'AMT_CREDIT', 'AMT_GOODS_PRICE', 'EXT_SOURCE_1', 'EXT_SOURCE_2','EXT_SOURCE_3']]\n\ny = train_df_ltd['TARGET']\ndel train_df_ltd['TARGET']","2f0c56ac":"#let's add one feature that we know performs very well\ntrain_df_ltd['ann_nr'] = train_df_ltd['AMT_CREDIT']\/train_df_ltd['AMT_ANNUITY']\ntest_df_ltd['ann_nr'] = test_df_ltd['AMT_CREDIT']\/test_df_ltd['AMT_ANNUITY']","f427cf0a":"pca = PCA()\nstandard_scaler = StandardScaler() \ntrain_scaled = standard_scaler.fit_transform(train_df_ltd.fillna(-1))\npca.fit(train_scaled)","026f2cda":"pca.explained_variance_","d4fc6ba6":"transformed = pca.transform(train_scaled)\nans = pd.DataFrame(transformed, columns = ['pca_1','pca_2','pca_3','pca_4','pca_5','pca_6', 'pca_7', 'pca_8'])\nans['target'] = y\nprint('the application train subset projected into the calculated components has the shape of: {}'.format(transformed.shape))","0f7bdae7":"lm = sns.lmplot(x='pca_1', y='pca_2',col='target', data=ans, fit_reg=False)\nax = lm.axes      \nax[0,0].set_ylim(-7,7)         \nax[0,0].set_xlim(-5,15)         \n#xlim=(-5,15), ylim=(-7,7))\nfig = plt.gcf()\nfig.set_size_inches(20, 14)","60b93ac4":"from scipy.stats import ttest_ind\ndefault_pca_1 = ans.loc[ans['target']==1, ['pca_1']]\nnot_default_pca_1 = ans.loc[ans['target']==0, ['pca_1']]\n\ndefault_pca_2 = ans.loc[ans['target']==1, ['pca_2']]\nnot_default_pca_2 = ans.loc[ans['target']==0, ['pca_2']]\n\n\nprint(ttest_ind(default_pca_1, not_default_pca_1))\nprint(ttest_ind(default_pca_2, not_default_pca_2))","0a919570":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold","fd0cba5a":"knn_scores = []\nfor n in np.linspace(10,150,8, dtype=int):\n    skf = StratifiedKFold(n_splits=4, random_state=0)\n    fold_scores = []\n    for train_index, test_index in skf.split(ans.drop(['target'], axis=1), y):\n        y_train = y[train_index]\n        y_test = y[test_index]\n        X_train = ans.drop(['target'], axis=1).iloc[train_index]\n        X_test = ans.drop(['target'], axis=1).iloc[test_index]\n        clf = KNeighborsRegressor(n_neighbors=n, weights='distance')\n        clf.fit(X_train, y_train)\n        fold_scores.append(roc_auc_score(y_test, clf.predict(X_test)))\n    knn_scores.append(np.mean(fold_scores))","85f3cc72":"plt.plot(np.linspace(10, 150, 8, dtype=int), knn_scores)","0b5cb587":"test_scaled = standard_scaler.transform(test_df_ltd.fillna(-1))\ntest_pca = pca.transform(test_scaled)\nclf = KNeighborsRegressor(n_neighbors=150, weights='distance')\nclf.fit(ans.drop(['target'], axis=1), y)","8845b0e9":"sample_subm['TARGET'] =  clf.predict(test_pca)\nsample_subm.to_csv('knn_subm.csv', index=False)","29388bad":"It turns out that the first 6 components explain a huge majority of the variance within the limited dataset.","ebbec457":"## Visualising the first principal components between target groups","a52372fb":"It seems that the more neighbours, the better score of a kNN model. Feel free to experiment with adding even more neighbours. <br>\nLast but not least, let's generate the predictions of such a model with 150 neighbours for the test set.","be8ffd13":"## Projecting the feature subset into the principal components","bdd41955":"In this kernel, the PCA of several basic features available in application_train dataset will be fitted. Then, the kNN algorithm will be used to generate predictions only based on these components. \nSuch predictions can be used for stacking \/ blending because they shouldn't be similar to the other available results. Also, the principal components could be added as inputs for the second layer models in stacking.","2a2710ca":"The means of both first and second components are significantly different (which is not surprising for such big samples). It gives the hope that even such a basic classification algorithm\nlike kNN could be able to at least partially differentiate between the two classes","1527b111":"> ## kNN cross-validation and submission","7aa213df":"Looking only at the first two PCA components, the distributions of observations with the target '1' and '0' are similar. Let's just quickly compare these coordinates:"}}