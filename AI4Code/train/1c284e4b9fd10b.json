{"cell_type":{"4c254d15":"code","69ed98e0":"code","6b7b3afe":"code","f82f9533":"code","1a69e0d4":"code","cee781a7":"code","0a1b53ab":"code","4ee4c039":"code","919d65ea":"code","680831be":"code","a578230d":"code","29ee955c":"code","2fd72dad":"code","b764862a":"code","5dfc5ebc":"code","787ae955":"code","63d11af7":"code","08201ce8":"code","7bbb5657":"code","1c6a6bbc":"code","b2d928d9":"code","e1a9f0f5":"code","7bdfb242":"code","295f330e":"code","1dd56b05":"code","bf4ed942":"code","083d2252":"code","e4bbd643":"code","b77c35db":"code","b91e8ee4":"code","1fbde13f":"code","87e85977":"code","bf6eb537":"code","8f4bc3be":"code","f60a6763":"code","5e4140e2":"code","4198cc95":"code","39f5f82e":"code","b39f911d":"code","02f15fd7":"code","e3d4d792":"code","5618cd96":"code","6c860692":"code","e5afc795":"code","ccbdb27d":"code","8fc778ec":"code","deffeb3a":"code","f11d908e":"code","65033f16":"code","e0a6a892":"code","2c4dec1e":"code","ddb1bc66":"code","65dd420a":"code","8e48849c":"code","72e566f7":"code","eb388a33":"code","a8001bab":"code","8d064e86":"code","60415fe0":"code","a2f24ade":"code","7bfa794d":"code","cbaa3425":"code","2471d32b":"code","89689b26":"code","04b42fa7":"code","8be8995e":"code","6b235667":"code","40aecfb1":"code","9c841d58":"code","dbb8d565":"code","ae7f9c5f":"code","8da1fd6b":"code","91cb944d":"code","b09926ec":"code","c47b3bc2":"code","7c368314":"code","6faa7810":"code","7d12fee3":"code","8fca0da1":"markdown","58df6b34":"markdown","2b7d8cf1":"markdown","28792cf0":"markdown","fa9d3ab6":"markdown","185314a4":"markdown","d80a48f3":"markdown","05970691":"markdown","efe3d110":"markdown","b268f4b1":"markdown","29f99ef0":"markdown","b227b21d":"markdown","5fd2c5fc":"markdown","30076f1f":"markdown","7ba5c930":"markdown","28ab64d2":"markdown","c1f81705":"markdown","d8cacbee":"markdown","4dcdeaee":"markdown","085acdaf":"markdown","ff6312dc":"markdown","04e59973":"markdown","71edc6fe":"markdown","36c1954e":"markdown","85840c76":"markdown","8516bace":"markdown","024077af":"markdown","c2cb7bad":"markdown","84fabb62":"markdown","af2cd7c9":"markdown","3a97e86b":"markdown","151eb780":"markdown","09ffa51d":"markdown","8a4a8214":"markdown","d520111c":"markdown","198e48ce":"markdown","a10bd022":"markdown","3a4e5042":"markdown"},"source":{"4c254d15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plt\n%matplotlib inline","69ed98e0":"train_source = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain_source.drop(['Id'], axis=1, inplace=True)","6b7b3afe":"# The origina data shape is ( 1460, 80 )\ntrain_source.shape","f82f9533":"train_source.tail(1)","1a69e0d4":"train_source['SalePrice']","cee781a7":"import seaborn as sns\nsns.set()\n\nsns.distplot(train_source[\"SalePrice\"])","0a1b53ab":"\ntrain_source[\"LotFrontage\"] = train_source[\"LotFrontage\"].fillna(train_source[\"LotFrontage\"].mean())","4ee4c039":"train_source.isnull().sum()","919d65ea":"train_source[\"LotFrontage\"].mean()","680831be":"train_source.shape","a578230d":"drop_col_list = []\n\nfor col in train_source.columns:\n    if train_source[col].isna().sum() > 900:\n        drop_col_list.append(col)\n        \nprint(f'{drop_col_list} is\/are going to be drop')\n\ntrain_source.drop(drop_col_list, axis=1, inplace=True)","29ee955c":"# The origina data shape is ( 1460, 80 ) and after dropping NA values is (1460, 80 )\n#  ['Alley', 'PoolQC', 'Fence', 'MiscFeature'] is\/are going to be drop  then the new shape will be  ( 1460, 76)\ntrain_source.shape","2fd72dad":"train_source.isna().sum()","b764862a":"index = train_source.loc[train_source['LotArea']>40000].index\ntrain_source.drop(index, axis=0, inplace=True)","5dfc5ebc":"train_source.shape","787ae955":"#train_source[\"LotFrontage\"] = train_source[\"LotFrontage\"].fillna(0)\ntrain_source = train_source.fillna(0)","63d11af7":"#  I have to be careful with this sentence for it removes all invalid\ntrain_source.dropna(axis = 1, inplace=True)\n#ts = train_source.dropna(axis = 1)\n","08201ce8":"train_source.head()","7bbb5657":"train_source.shape","1c6a6bbc":"train_source.isna().sum()","b2d928d9":"train_source.shape","e1a9f0f5":"# select the list of columns we will use to train the model\nusing_col_list = list(train_source.columns)\nusing_col_list.remove('SalePrice')\n   \nprint(len(using_col_list))\n","7bdfb242":"# It will be better to change variable Year from numeric to string \nyear_col = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\ntrain_source = train_source.astype({'YearBuilt':'str', 'YearRemodAdd':'str', 'GarageYrBlt':'str', 'YrSold':'str'})\ntrain_source.shape","295f330e":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ncorr = train_source.corr()\nmax_correlate = corr.index[abs(corr[\"SalePrice\"])>0.4]\n\nplt.figure(figsize=(10,10))\ngraph=sns.heatmap(train_source[max_correlate].corr(),annot=True,cmap=\"YlGnBu\")","1dd56b05":"norm_train_label = train_source\nlabel_mean, label_std = train_source['SalePrice'].mean(), train_source['SalePrice'].std()\nnorm_train_label['SalePrice'] = (train_source['SalePrice']-label_mean)\/label_std\nnorm_train_label['SalePrice']\n","bf4ed942":"norm_train_label.head(2)","083d2252":"##  ***Important *** after one-hot encoding the shape of norm_train_labels changed from (1446,76) to (1446, 503)\nnorm_train_label.shape\n","e4bbd643":"norm_train_label['SalePrice']","b77c35db":"norm_train_label","b91e8ee4":"\ndata_x = norm_train_label.drop('SalePrice', axis=1)\ndata_y = norm_train_label['SalePrice']","1fbde13f":"data_x","87e85977":"#  let's see what happens if I hot encoded the whole data before the split... \ndata_x_one_hot = pd.get_dummies(data_x, drop_first=True)\ndata_x_one_hot.shape, data_y.shape","bf6eb537":"#  now I will split the already one_hot encoded data between train_data 80%  and test_data 20%\n\ntf.random.set_seed(42)\nfrom sklearn.model_selection import train_test_split\n#train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2, random_state=2)\ntrain_x, test_x ,train_y, test_y = train_test_split(data_x_one_hot, data_y, test_size=0.2, random_state=42)","8f4bc3be":"# OK I notice that both train data and test data have equivalent shapes \n\ndata_x_one_hot.shape, train_x.shape,  test_x.shape","f60a6763":"train_x.shape   # the shape of training data","5e4140e2":"test_x.shape  #  the shape of testing data","4198cc95":"train_x.columns","39f5f82e":"train_x.head(2)","b39f911d":"from sklearn import linear_model\n\n\nmodel_regression = linear_model.Lasso(alpha=25, max_iter=50,tol=0.2)  # you can modify these global parameters and the look at the scores\nmodel_regression.fit(train_x,train_y)","02f15fd7":"train_x.shape, train_y.shape","e3d4d792":"model_regression.score(train_x, tf.squeeze(train_y))\n# 0.7500271092207966\n# Linear regression model gave me prediction value of 0.7362906288285491  which is good, however, ridge regression's prediction value is much better.","5618cd96":"model_regression.score(test_x, tf.squeeze(test_y))\n# Linear regression model gave me testing value of 0.7885353538647556  which is good but weird, however, ridge regression's prediction value is much better.","6c860692":"test_x.shape, test_y.shape","e5afc795":"from sklearn.linear_model  import Ridge\nmodel_ridge_regression = Ridge(alpha=25, max_iter=50,tol=0.2)\nmodel_ridge_regression.fit(train_x,train_y)\n","ccbdb27d":"model_ridge_regression.score(train_x, train_y)","8fc778ec":"test_x.shape, test_y.shape","deffeb3a":"model_ridge_regression.score(test_x, test_y)\n","f11d908e":"tf.random.set_seed(42)","65033f16":"test_source = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nID = test_source.pop('Id')\n","e0a6a892":"test_source","2c4dec1e":"test_source.shape","ddb1bc66":"#  Fix null values of LotFrontage\ntest_source[\"LotFrontage\"] = test_source[\"LotFrontage\"].fillna(test_source[\"LotFrontage\"].mean())","65dd420a":"drop_col_list = []\n\nfor col in test_source.columns:\n    if test_source[col].isna().sum() > 900:\n        drop_col_list.append(col)\n        \nprint(f'{drop_col_list} is\/are going to be drop')\n\ntest_source.drop(drop_col_list, axis=1, inplace=True)","8e48849c":"test_source.shape","72e566f7":"test_source.isna().sum()","eb388a33":"test_source[\"LotArea\"].mean()","a8001bab":"test_source.shape","8d064e86":"test_source = test_source.fillna(0)","60415fe0":"test_source.shape","a2f24ade":"test_source.dropna(axis=1, inplace=True)","7bfa794d":"# It will be better to change variable Year from numeric to string \nyear_col = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt','YrSold']\ntest_sourcex = test_source.astype({'YearBuilt':'str', 'YearRemodAdd':'str', 'GarageYrBlt':'str', 'YrSold':'str'})\ntest_sourcex.shape\n\n","cbaa3425":"test_one_hot = pd.get_dummies(test_source, drop_first=True)\n","2471d32b":"test_one_hot.head(5)","89689b26":"test_one_hot.shape","04b42fa7":"# This way you can get one row of your data .. I will see if this is helpful  later\nrow = next(test_one_hot.iterrows())[1]\nrow","8be8995e":"val_cols = test_one_hot.columns\ntrain_cols = data_x_one_hot.columns\nprint(val_cols), train_cols\n","6b235667":"cols_to_drop=[]\nprint(len(val_cols), len(train_cols))\ntot=0\nfor i in  val_cols:\n   if i not in train_cols:\n    tot=tot+1\n    cols_to_drop.append(i)\n    #test_one_hot.drop([test_one_hot], axis=1)\nprint(\" Columns in val_columns that are not in train_cols and must be dropped  : \",tot)\nprint(cols_to_drop)\n","40aecfb1":"test_one_hot.columns","9c841d58":"# This way I drop columns in validation data that are not present in training data, however, this is not the best way .. I will fing it later\ntest_xx =pd.DataFrame(test_one_hot)\ntest_xx.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'YrSold', 'MSZoning_C (all)', 'Utilities_AllPub', 'Exterior1st_AsbShng', 'Exterior2nd_AsbShng', 'KitchenQual_Ex', 'Functional_Maj1', 'SaleType_COD'], axis=1, inplace=True)\n#test_xx.drop([cols_to_drop], axis=1, inplace=True)\n","dbb8d565":"cols_to_add =[]\nprint(len(val_cols), len(train_cols))\ntot=0\nfor i in  train_cols:\n   if i not in val_cols:\n    tot=tot+1\n    cols_to_add.append(i)\n    #np.delete(test_one_hot, test_one_hot[i], axis=1)\nprint(\" Columns in train_cols that are not in val_cols : \",tot)\n","ae7f9c5f":"test_xx[cols_to_add] = 0\ntest_xx.shape","8da1fd6b":"from pandas import Series, DataFrame\nyhat=model_ridge_regression.predict(test_xx)\n#yhat=model_ridge_regression.predict(norm_predict)","91cb944d":"yhat","b09926ec":"label_mean, label_std","c47b3bc2":"\nSalePrice = yhat*label_std+label_mean\nSalePrice, SalePrice.shape","7c368314":"submission_model = pd.DataFrame({'Id':ID, 'SalePrice':SalePrice})\nsubmission_model.head()","6faa7810":"submission_model.to_csv('.\/submission.csv', index=False)","7d12fee3":"#submission = pd.DataFrame([i for i in SalePrice],index=ID, columns=['SalePrice'])\n#submission.to_csv(\"submission.csv\", index=False)\n#submission.head(10)","8fca0da1":"## calculate SalePrice from nomalized format  back to normal value ","58df6b34":"## Preparing submission file","2b7d8cf1":"### Good job !!!, after dropping extra columns and adding the missing one I finally go the validation data shape I was looking for  ( 561, 455 ) ,\nso let's make the predictions using validation data","28792cf0":"## Looking for missing column values","fa9d3ab6":"## Correlation Graph","185314a4":"### I have to make sure the number of columns is the same for both train_x and test_one_hot...\n* Initially both shapes are different  train_x ( 460, 422 )  and test_one_hot ( 561, 432)\n*  If the number of rows is different is OK, it's normal that means different number of records, however, the number of columns must match before input in to the model\n","d80a48f3":"## Cleaning Data :\n## Dropping columns 'NA' values","05970691":"## Second option using Ridge Regression \n## This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).","efe3d110":"### Making sure LotFrontage has no null or nan values..\n* I am going to replace null or invalid values with the mean fo all values","b268f4b1":"## OK, I find the columns missing, so now I have to insert\/add them in to validation data","29f99ef0":"### It works!!  see below the prediction data, because SalePrice was normalized, the final result is also normalized\n** I have to find the way to reverse the nirmalization process to see the final prediction in regular format","b227b21d":"## Validating Ridge Regression Model","5fd2c5fc":"## Using train_test_split to split data between train and test : test is 20% and the rest will be train","30076f1f":"## Loading Data source","7ba5c930":"## Normalize label ( SalePrice )  to make computation easy","28ab64d2":"##  Let's find the columns in train_cols ( training data ) that are not in val_cols ( validation data )\n* I will save the list of these columns because I have to add them to validation date..\n*  Remember the idea is make sure the columns in validation data and train data match otherwise the model won't work","c1f81705":"### ummmm no too bad let's to improve these numbers","d8cacbee":"## Scoring the Model","4dcdeaee":"## Dropping missing values","085acdaf":"##  Dropping no normal values of LotArea .. ","ff6312dc":"### Let's one_hot_encoding the data before input in to the model","04e59973":"## Running Predictions...","71edc6fe":"## OK I find these columns, now I have to delete\/drop them from the validation data .. ","36c1954e":"##  Data Analisys.\n## The data includes a large number or columns, some of them with invalid or incomplete values","85840c76":"### Building and Fitting\/Training  the Model","8516bace":"## Dropping missing values","024077af":"## Dropping columns with NA  values","c2cb7bad":"## SalePrice is the column we are going to predict","84fabb62":"##  Let's find the columns in val_cols ( validation data ) that are not in train_cols ( Training data )\n* I will save the list of these columns because I have to drop them from validation date..\n*  Remember the idea is make sure the columns in validation data and train data match otherwise the model won't work","af2cd7c9":"## # The origina data shape is ( 1460, 80 ) and after dropping NA values is (1460, 76 )\n","3a97e86b":"## Linear Regression Lasso Model.. \n*  ***Data Cleaning ***\n* Dropping columns 'NA' values\n* Looking for missing column values\n* Dropping mission values ;; missing values, NA Values will make the model fail\n* One-hot encoded is done after spliting the data ***ERRR because the dimensions of train_x and test_x will be different and the testing won't work***\n* Doing one_hot_encoding before splitting the data will keep the dimensions of both  train_x and test_x  equivalentes","151eb780":"## Scoring Ridge Regression Model","09ffa51d":"###  Bingo!!!! I got much better results with Ridge Regression","8a4a8214":"## Evaluating the Model","d520111c":"## Generating One Hot Encoding","198e48ce":"## Looking at SalesPrice  after normalizing the price","a10bd022":"## Conclusion:\n### After testing with other regression models we found that Ridge Regression produce the best predictions in this case.\n### I spent lot of good time cleaning, scaling, normalizing and processing data to make sure the shape of training , testing and validation data is correct","3a4e5042":"## Preparing  data for training and testing"}}