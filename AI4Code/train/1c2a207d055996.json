{"cell_type":{"6ea040cb":"code","da3b62b2":"code","96d4187d":"code","66d9afac":"code","ffd4bdcc":"code","a8b184ab":"code","88550381":"code","4d353bf3":"code","c6dfa044":"code","4aeade0a":"code","5959dccb":"code","18ddf464":"code","300cb1a9":"code","61a83428":"code","d8011584":"code","5bd1c2e7":"code","c4b3dbff":"code","4855699d":"markdown","ddd3fc40":"markdown","0274e8a4":"markdown","f0ceab60":"markdown","1e95fa19":"markdown"},"source":{"6ea040cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da3b62b2":"#import the depedencies\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline","96d4187d":"#Load the CSV File\nSEED = 42\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata.head()","66d9afac":"#Divide the dataset into label and features\nX,y = data.drop(['label'], axis = 1), data['label']","ffd4bdcc":"#Split the dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = SEED)","a8b184ab":"#The the custom dataset loader\ntransform = transforms.ToTensor()\nclass MNISTDataset(Dataset):\n    def __init__(self,images,labels=None,transforms=None):\n        self.X = images\n        self.y = labels\n        self.transform = transforms\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self,i):\n        data = self.X.iloc[i,:]\n        data = np.asarray(data).astype(np.uint8).reshape(28,28,1)\n        \n        if self.transform:\n            data = self.transform(data)\n\n            \n        if self.y is not None:\n            return (data,self.y.iloc[i])\n        \n        else:\n            return data","88550381":"#Loading the data into the dataloader\ntrain_data = MNISTDataset(X_train,y_train,transform)\ntest_data = MNISTDataset(X_test,y_test,transform)\n\ntrainloader = DataLoader(train_data,batch_size=64,shuffle=True)\ntestloader = DataLoader(test_data,batch_size=64,shuffle=True)","4d353bf3":"dataiter = iter(trainloader)\nimages,labels = dataiter.next()\nimages = images.numpy()\n\nfig = plt.figure(figsize=(25,4))\nfor i in range(20):\n    ax = fig.add_subplot(2,20\/2,i+1,xticks=[],yticks=[])\n    ax.imshow(np.squeeze(images[i]), cmap='gray')\n    \n    ax.set_title(str(labels[i].item()))","c6dfa044":"#Architecture of the Model\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784,512)\n        self.fc2 = nn.Linear(512,256)\n        self.fc3 = nn.Linear(256,128)\n        self.fc4 = nn.Linear(128,10)\n        \n    def forward(self,x):\n        x = x.view(-1,28*28)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        x = self.fc4(x)\n        \n        return x","4aeade0a":"model = Net()\nprint(model)","5959dccb":"#Specify Loss and the optimizer\nfrom torch import optim\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(),lr=0.003)","18ddf464":"#Training the model\nepochs = 60\ntraining_loss = []\nfor epoch in range(epochs):\n    train_loss = 0.0\n    for images, labels in trainloader:\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output,labels)\n        loss.backward()\n        optimizer.step()\n        train_loss +=loss.item()*images.size(0)\n        \n    print('Epoch {}\/{}...'.format(epoch+1,epochs),\n         'Training Loss: {:.3f}'.format(train_loss))","300cb1a9":"#tesing the model\ntest_loss = 0.0\nclass_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nbatch_size = 10\n\nmodel.eval()\nfor images, labels in testloader:\n    output = model(images)\n    loss = criterion(output,labels)\n    test_loss += loss.item()*images.size(0)\n    _, pred = torch.max(output,1)\n    correct = np.squeeze(pred.eq(labels.data.view_as(pred)))\n    \n    for i in range(batch_size):\n        label = labels.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] +=1\n        \ntest_loss = test_loss\/len(testloader)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\nfor i in range(10):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            str(i), 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","61a83428":"\ndf_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\ntest_data = MNISTDataset(df_test,transforms=transform)\ntestloader = DataLoader(test_data,batch_size=64,shuffle=False)\n","d8011584":"from torch.autograd import Variable\ndef prediction(testloader):\n    model.eval()\n    test_pred = torch.LongTensor()\n    \n    with torch.no_grad():\n        for images in testloader:\n            output = model(images)\n            pred = output.data.max(1,keepdim=True)[1]\n            test_pred = torch.cat((test_pred,pred),dim=0)\n        \n    return test_pred\n\ntest_pred = prediction(testloader)","5bd1c2e7":"out_df = pd.DataFrame(np.c_[np.arange(1, len(test_data)+1)[:,None], test_pred.numpy()], \n                      columns=['ImageId', 'Label'])\nout_df.shape\nout_df.to_csv('submission.csv', index=False)","c4b3dbff":"out_df.shape","4855699d":"## Submission","ddd3fc40":"## Testing the Model\n\nModel.eval() will configure all layers to be ready for assessment. This affects layers such as dropout layers that switch 'off' nodes with some probability during training, but should allow each node to be 'on' for assessment.","0274e8a4":"## Train the Network\n\nThe steps for training\/learning from a batch of data are described as follows:\n\n1. Clear the gradients of all optimized variables\n2. Forward pass : compute predicted outputs by passing inputs to the model\n3. Calculate the loss\n4. Backward Pass: compute gradient of the loss with respect to the model parameters\n5. Perform a single optimization step(parameter update)\n6. Update the average training loss","f0ceab60":"### Workflow of the Notebook\n\n1. Import all the depedencies\n2. Load the data\n3. Visualize the data\n4. Model Architecture\n5. Train the model\n6. Test the model\n7. Submission\n","1e95fa19":"## About the Dataset\n\nThe MNIST is a well-known handwritten digit classification assignment dataset. It comprises images with a gray scale of 28x28 pixels, each of which has a mark between 0 and 9, indicating the corresponding digit.\n\n"}}