{"cell_type":{"a679f269":"code","0687a9ef":"code","14ccd4ee":"code","44cef4b8":"code","3b610972":"code","7061b8df":"code","77215780":"code","be0a75e0":"code","df4a8ec6":"code","ec26ffaf":"code","879e215c":"code","f53a9fbb":"code","72ba37db":"code","dc7df732":"code","258a9001":"code","b1a37be3":"markdown","19837812":"markdown","f080eafb":"markdown","7e5b45d1":"markdown","8a4d1c6a":"markdown","2e92d6a9":"markdown","a5578ec9":"markdown","f27a032e":"markdown"},"source":{"a679f269":"!pip3 install pyspark","0687a9ef":"from pyspark.sql import SparkSession  # required to created a dataframe\nspark=SparkSession.builder.appName(\"EDA_PySpark\").getOrCreate() \n\nimport pyspark.sql.functions\n\nfrom pyspark.sql.types import DoubleType, IntegerType","14ccd4ee":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","44cef4b8":"X = spark.read.csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", header=True)","3b610972":"X.printSchema()","7061b8df":"X = X.withColumn(\"id\", X[\"id\"].cast(IntegerType()))\nX = X.withColumn(\"claim\", X[\"claim\"].cast(IntegerType()))\nfor i in range(1, 119):\n    X = X.withColumn(\"f\"+str(i), X[\"f\"+str(i)].cast(DoubleType()))","77215780":"X.printSchema()","be0a75e0":"for i in range(12):\n    X.select(X.columns[(i*10):(i*10+10)]).show(5, truncate=0)","df4a8ec6":"(X.count(), len(X.columns))","ec26ffaf":"for i in range(24):\n    X.select(X.columns[(i*5):(i*5+5)]).describe().show(truncate=0)","879e215c":"for i in range(24):\n    X.where(X[\"claim\"] == 0)[X.columns[(i*5):(i*5+5)]].describe().show(truncate=0)","f53a9fbb":"for i in range(24):\n    X.where(X[\"claim\"] == 1)[X.columns[(i*5):(i*5+5)]].describe().show(truncate=0)","72ba37db":"for i in range(120):\n    means = X.groupBy(\"claim\").agg({X.columns[i]:\"mean\"}).collect()\n    stDev = X.groupBy(\"claim\").agg({X.columns[i]:\"stddev\"}).collect()\n    xbar = means[1][\"avg(\"+X.columns[i]+\")\"]\n    mu = means[0][\"avg(\"+X.columns[i]+\")\"]\n    sigma = stDev[0][\"stddev(\"+X.columns[i]+\")\"]\n    print(X.columns[i], \": \", (xbar - mu) \/ sigma)","dc7df732":"view = X.createOrReplaceTempView(\"playground\")","258a9001":"spark.sql(\"SELECT claim, AVG(f34), STD(f34) FROM playground GROUP BY claim ORDER BY claim\").show(truncate=0)","b1a37be3":"<a id=\"load\"><\/a>\n## II. Loading the dataset\n\nWe load the training set to perform the EDA. We aren't supposed to know the content of the test set to avoid overfitting it.\n\nPySpark reads features as a string by default, it's able to infer column types but a good habit is to set them manually (this avoids unpleasant surprises). We print the schema to verify that it matches our [EDA with Pandas](https:\/\/www.kaggle.com\/cmarquay\/eda-skewness).\n\nWe start by displaying the first few lines and some basic information. So we see that the training set contains 957,919 rows and 120 columns. The id column is actually the index, and the claim column is our y target: both are of type integer. Finally, we have 118 features of type double which constitute our X.","19837812":"<a id=\"spark\"><\/a>\n## I. Setting up spark\n\nWe start by setting up a Spark context.","f080eafb":"<a id=\"sql\"><\/a>\n## VI. Using SparkSQL\n\nThe createOrReplaceTempView() method loads the Spark dataframe for use with SQL queries.\n\nThe feature with the largest data gap when claim is 0 or 1 is f34 with a z-score of 0.04313979653511135. We note that the means of f34 are however very close to each other, their distance is much lower than their standard deviations.","7e5b45d1":"<a id=\"filters\"><\/a>\n## IV. Filtering based on claim values\n\nThe where() and filter() methods are identical, they allow us to filter the values of the Spark dataframe according to a logical condition.\n\nThe claim column is a column containing binary values 0 or 1. Here, we therefore seek to know the statistics of the columns according to the value of the claim column. We first display column statistics for rows where claim is 0, then we display column statistics for rows where claim is 1.","8a4d1c6a":"<a id=\"groupagg\"><\/a>\n## V. Grouping and aggregating data\n\nWe display a lot of stats, but we may also want to use them. Spark dataframes can be grouped by column values to obtain statistics through aggregations. With min, max, and skewness, we can find the minimum, the maximum, and the skewness of a column. With mean and stddev, we can standardize the data.\n\nHere, we try to compare the statistics obtained just above to see if there is a difference in the distribution of the features when claim is equal to 0 and when claim is equal to 1. We therefore calculate the z-score to know if claim==0 is significantly different from a population where claim==1. A z-score is the distance in standard deviations of a value from the mean. 67% of the data in a distribution have a z-score between -1 and 1. 95% of the data in a distribution have a z-score between -2 and 2. 97.5% of the data in a distribution have a z-score between -3 and 3. This is why we consider z-scores less than -3 or greater than 3 as outliers.\n\nWe find that no z-score exceeds 0.05 and that the two distributions are therefore very similar. The objective of this contest is to differentiate claim==0 and claim==1 while they're statistically identical.","2e92d6a9":"<a id=\"stats\"><\/a>\n## III. Summary statistics\n\nWe display some statistics concerning the 120 columns.","a5578ec9":"Feel free to comment. I'll take into account the advice and comments to improve this notebook.\n\nFor more information on EDA including visualizations with Pandas and Seaborn, see as well: [EDA skewness](https:\/\/www.kaggle.com\/cmarquay\/eda-skewness)\n\nFor more information on a tutorial and manipulations with PySpark: [try this video](https:\/\/www.youtube.com\/watch?v=3-pnWVWyH-s)","f27a032e":"# EDA with PySpark\n\nThe training set contains a large amount of data with 957,919 rows and 120 columns. This is the opportunity to use PySpark to query data efficiently. PySpark is a language for practicing EDA in particular, which uses Python to perform parallel tasks on the data. The data is loaded into a Spark dataframe which looks like what is known with R and Pandas but which is a distributed table. Spark dataframes thus offer a wider range of transformation than with Pandas dataframes. We'll finally see that Spark dataframes offer the possibility of using SQL for queries.\n\nIf you're browsing this notebook, feel free to comment. I'll take into account the advice and comments to improve it.\n\n[I. Setting up spark](#spark)\n\n[II. Loading the dataset](#load)\n\n[III. Summary statistics](#stats)\n\n[IV. Filtering based on claim values](#filters)\n\n[V. Grouping and aggregating data](#groupagg)\n\n[VI. Using SparkSQL](#sql)\n\nFor more information on EDA including visualizations with Pandas and Seaborn, see as well: [EDA skewness](https:\/\/www.kaggle.com\/cmarquay\/eda-skewness)\n\nFor more information on a tutorial and manipulations with PySpark: [try this video](https:\/\/www.youtube.com\/watch?v=3-pnWVWyH-s)"}}