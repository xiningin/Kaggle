{"cell_type":{"51c53ce6":"code","d56c5419":"code","6fd1c99f":"code","da2839f5":"code","1bd2da08":"code","a6dcf918":"code","b0ebdfb4":"code","473db904":"code","8371dc80":"code","7d632a36":"code","3d48e9f7":"code","b058ca43":"code","49b14b94":"code","92c8d0ef":"code","bc2e8873":"code","ec4e45fe":"code","788e1715":"code","74d8405b":"code","539dc990":"code","4b356bd1":"code","fe53553b":"markdown","8e192ead":"markdown","c48ff929":"markdown","a6f5db62":"markdown","e5cf468f":"markdown","199bc3aa":"markdown","23f3edc9":"markdown","19bea6ab":"markdown"},"source":{"51c53ce6":"# ====================================================\n# Library\n# ====================================================\nimport sys\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\n\nimport os\nimport math\nimport time\nimport random\nimport shutil\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nfrom matplotlib import pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom albumentations import (\n    Compose, Normalize, Resize\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\n\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","d56c5419":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='resnet200d_320'\n    size=512\n    batch_size=128\n    seed=416\n    target_size=11\n    target_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n                 'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n                 'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n                 'Swan Ganz Catheter Present']\n    n_fold=5\n    trn_fold=[0] # [0, 1, 2, 3, 4]\n    load_embed=True\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","6fd1c99f":"OUTPUT_DIR = '.\/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nTRAIN_PATH = '..\/input\/ranzcr-clip-catheter-line-classification\/train'\nTEST_PATH = '..\/input\/ranzcr-clip-catheter-line-classification\/test'\ntrain = pd.read_csv('..\/input\/ranzcr-clip-catheter-line-classification\/train.csv')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","da2839f5":"%%time\n\nNIH_XRAYS_DIRS = [f'..\/input\/data\/images_0{str(i).zfill(2)}' for i in range(1, 13)]\nNIH_XRAYS = []\nfor imdir in NIH_XRAYS_DIRS:\n    impaths = [imdir + '\/images\/' + os.listdir(imdir + '\/images')[i] for i in range(len(os.listdir(imdir + '\/images')))]\n    NIH_XRAYS.append(impaths)\n    \nNIH_XRAYS = np.concatenate(NIH_XRAYS)\nprint(NIH_XRAYS.shape)","1bd2da08":"# ====================================================\n# Dataset\n# ====================================================\nclass NIHDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['StudyInstanceUID'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image\n    \nclass TrainDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['StudyInstanceUID'].values\n        self.labels = df[CFG.target_cols].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{TRAIN_PATH}\/{file_name}.jpg'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        label = torch.tensor(self.labels[idx]).float()\n        return image","a6dcf918":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])","b0ebdfb4":"nih_df = pd.DataFrame(columns=['StudyInstanceUID'],\n                    data=NIH_XRAYS)","473db904":"import cuml","8371dc80":"class FeatureExtractor(nn.Module):\n    def __init__(self, model_name='resnet200d_320', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity()\n        self.model.fc = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(n_features, CFG.target_size)\n\n    def forward(self, x):\n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return pooled_features, features\n    \ndef extract(model, test_loader, device):\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    feats = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        with torch.no_grad():\n            pooled_feats, features = model(images)\n        feats.append(pooled_feats.to('cpu').numpy())\n    feats = np.concatenate(feats)\n    return feats","7d632a36":"if not CFG.load_embed:      \n    train_dataset = TrainDataset(train, transform=get_transforms(data='valid'))\n    nih_dataset = NIHDataset(nih_df, transform=get_transforms(data='valid'))\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=False, \n                             num_workers=CFG.num_workers, pin_memory=True)\n    nih_loader = DataLoader(nih_dataset, batch_size=CFG.batch_size, shuffle=False, \n                             num_workers=CFG.num_workers, pin_memory=True)","3d48e9f7":"if not CFG.load_embed:    \n    extractor = FeatureExtractor(CFG.model_name, pretrained=True).to(device)\n    embed_train = extract(extractor, train_loader, device)\n    np.save(f'embed_train', embed_train.astype('float32'))\n    embed_nih = extract(extractor, nih_loader, device)\n    np.save('embed_nih', embed_nih.astype('float32'))","b058ca43":"if CFG.load_embed:   \n    embed_train = np.load('..\/input\/ranzcrnihoverlap\/embed_train.npy')\n    embed_nih = np.load('..\/input\/ranzcrnihoverlap\/embed_nih_full.npy')","49b14b94":"print(embed_train.shape, embed_nih.shape)","92c8d0ef":"KNN = 3\nmodel = cuml.neighbors.NearestNeighbors(n_neighbors=KNN)\nmodel.fit(embed_nih)\ndistances, indices = model.kneighbors(embed_train)\nmm = np.min(distances,axis=1)","bc2e8873":"CUTOFF = 5\nidx = np.where( (mm<CUTOFF) )[0]\nprint(f'There are {len(idx)} potential duplicate images that have distance < {CUTOFF}')","ec4e45fe":"for i, k in enumerate(idx):\n    \n    if i == 16: break\n    \n    plt.figure(figsize=(10,5))\n    \n    plt.subplot(1,2,1)\n    img = cv2.imread(TRAIN_PATH + '\/' + train.loc[k]['StudyInstanceUID'] + '.jpg')\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f\"{train.loc[k]['StudyInstanceUID'] + '.jpg'}\", fontsize=8)\n    plt.imshow(img)\n\n\n    plt.subplot(1,2,2)\n    img = cv2.imread(NIH_XRAYS[indices[k, 0]])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f\"{'\/'.join([_ for _ in NIH_XRAYS[indices[k, 0]].split('\/')[3:]])}\", fontsize=8)\n    plt.imshow(img)\n\n    plt.show()","788e1715":"#check borderline cases\nborderline_idx = np.where( (mm<CUTOFF+1) & (mm>CUTOFF))[0]\nprint(f'There are {len(borderline_idx)} borderline cases that have distance between {CUTOFF} and {CUTOFF+1}')","74d8405b":"#check borderline cases\nborderline_idx = np.where( (mm<CUTOFF+1) & (mm>CUTOFF))[0]\n\nfor i, k in enumerate(borderline_idx):\n    \n    if i == 16: break\n    \n    plt.figure(figsize=(10,5))\n    \n    plt.subplot(1,2,1)\n    img = cv2.imread(TRAIN_PATH + '\/' + train.loc[k]['StudyInstanceUID'] + '.jpg')\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f\"{train.loc[k]['StudyInstanceUID'] + '.jpg'}\", fontsize=8)\n    plt.imshow(img)\n\n\n    plt.subplot(1,2,2)\n    img = cv2.imread(NIH_XRAYS[indices[k, 0]])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f\"{'\/'.join([_ for _ in NIH_XRAYS[indices[k, 0]].split('\/')[3:]])}\", fontsize=8)\n    plt.imshow(img)\n\n    plt.show()","539dc990":"dupes = pd.DataFrame(columns=['ranzcr_path', 'nih_path'])\ndupes['ranzcr_path'] = train.loc[idx]['StudyInstanceUID'].values\ndupes['nih_path'] = nih_df.loc[indices[idx, 0]]['StudyInstanceUID'].values\ndupes.head()","4b356bd1":"dupes.to_csv('duplicated_paths.csv', index=False)","fe53553b":"# Datasets","8e192ead":"# Library","c48ff929":"# Transforms","a6f5db62":"# Find Duplicates with RAPIDS","e5cf468f":"# CFG","199bc3aa":"# Data Loading","23f3edc9":" **It seems that this approach is not as accurate as I hoped. This is most likely because the images in the NIH XRay dataset are cropped differently than those in our training images. Please comment below if you see a mistake (I rushed the creation of this kernel).**","19bea6ab":"# NIH Chest XRay Overlaps\n\n**In this notebook, I show how to use RAPIDS to find duplicate images between the [RANZCR CLiP competition](https:\/\/www.kaggle.com\/c\/ranzcr-clip-catheter-line-classification) training dataset and the [NIH Chest XRays dataset](https:\/\/www.kaggle.com\/nih-chest-xrays\/data).** \n\n**We simply take a ResNet200D pretrained on ImageNet and create CNN embeddings for the RANZCR train dataset and for the NIH Chest XRay Dataset.**\n\n**These embeddings are then used to train a RAPIDS cuML NearestNeighbors model so that we can manually inspect images whose embeddings are similar between the NIH XRay dataset and the RANZCR CLiP training dataset.**\n\n**The motivation for this notebook comes from [@chrisdeotte](https:\/\/www.kaggle.com\/cdeotte)'s excellent [kernel](https:\/\/www.kaggle.com\/cdeotte\/rapids-cuml-knn-find-duplicates) from the [Melanoma competition](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification). Please upvote it before upvoting this one (if you decide to). Much of the code is repurposed from [@yasufuminakama](https:\/\/www.kaggle.com\/yasufuminakama) and his incredible [notebooks](https:\/\/www.kaggle.com\/yasufuminakama\/ranzcr-resnet200d-3-stage-training-step1). Check out his work as well, as he is a very talented coder.** \n\n**Note that we can do the same procedure to find potential duplicates between the public test images and the NIH XRay dataset. I may do this in a future commit of the notebook.**"}}