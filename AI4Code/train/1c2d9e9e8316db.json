{"cell_type":{"27ba84eb":"code","58e63645":"code","f75e826f":"code","a9b10745":"code","06bcf860":"code","e9e78d09":"code","6826d8c4":"code","8d191684":"code","e2d1ebab":"code","1582b28a":"code","fe9d2061":"code","3ebf978d":"code","6e429850":"code","8a975354":"code","07388475":"code","2a089a81":"code","1bc7aef0":"code","c36c56e7":"markdown","b04653aa":"markdown","c1b2c707":"markdown","47351b10":"markdown","e8416c1a":"markdown","c6c26d80":"markdown","b6b37550":"markdown","005d1203":"markdown","59d2bd18":"markdown","1cc7e5dd":"markdown","f1f07451":"markdown","560f1986":"markdown","4a961720":"markdown"},"source":{"27ba84eb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","58e63645":"df = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf = df.drop(columns=['Id'])\nprint(df.shape)\ndf.head(3)","f75e826f":"#droping the target column\ntarget = df['Species']\ndata_x = df.iloc[:,:-1]","a9b10745":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\ndata = scale.fit_transform(data_x.values)\ndata.shape","06bcf860":"df_matrix = np.asmatrix(data)\nprint(df_matrix.shape)","e9e78d09":"covar_matrix = np.cov(df_matrix.T)\nprint(covar_matrix.shape)","6826d8c4":"eigvalues, eigvectors = np.linalg.eig(covar_matrix)\n","8d191684":"print(eigvalues)","e2d1ebab":"print(eigvectors)","1582b28a":"top2eig = eigvectors[:,0:2]\ntop2eig","fe9d2061":"new_data = data_x.dot(top2eig)\n#creating a new dataframe including target\nnew_df = pd.DataFrame(np.hstack((new_data,np.array(target).reshape(-1,1))),columns=['1st_component','2nd_component','Species'])\n\nnew_df.head(2)","3ebf978d":"#plotting data\nsns.scatterplot(new_df['1st_component'],new_df['2nd_component'],hue=new_df['Species'])\nplt.title('Scatter-plot')\nplt.show()\n","6e429850":"from sklearn.decomposition import PCA\npca  = PCA(n_components=2)\n#here data is scaled data that we did earlier using standard scalar\npca_components = pca.fit_transform(data)\nprint(pca_components.shape)","8a975354":"#creating a new dataframe including target\nnew_df_pca = pd.DataFrame(np.hstack((pca_components,np.array(target).reshape(-1,1))),columns=['1st_component','2nd_component','Species'])\nnew_df_pca.head()\n","07388475":"#plotting data\nsns.scatterplot(new_df_pca['1st_component'],new_df_pca['2nd_component'],hue=new_df_pca['Species'])\nplt.title('Scatter-plot')\nplt.show()","2a089a81":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2,perplexity=20,n_iter=1000)\ntsne_result = tsne.fit_transform(data)\n\n\n\n\n#creating a new dataframe including target\nnew_df_tsne = pd.DataFrame(np.hstack((tsne_result,np.array(target).reshape(-1,1))),columns=['1st_component','2nd_component','Species'])\nnew_df_tsne.head()","1bc7aef0":"#plotting data\nsns.scatterplot(new_df_tsne['1st_component'],new_df_tsne['2nd_component'],hue=new_df_tsne['Species'])\nplt.title('Scatter-plot')\nplt.show()","c36c56e7":"**Step2 : Generate a Covariance matrix**","b04653aa":"# 2. TSNE","c1b2c707":"We got a covariance matrix of shape 4x4","47351b10":"## PCA with sklearn","e8416c1a":"# 1. Principle Component Analysis\n\n## PCA From Scratch\n\n**Step 1: Normalize the data so that whole data will be in single scale**","c6c26d80":"* TSNE is an iterative algorithm. You have to run TSNE at different perplexity and itertations and see when stable result comes.","b6b37550":"**<font color=\"blue\">Principal Component analysis (PCA): <\/font>** PCA is an unsupervised linear dimensionality reduction and data visualization technique for very high dimensional data. As having high dimensional data is very hard to gain insights from adding to that, it is very computationally intensive. The main idea behind this technique is to reduce the dimensionality of data that is highly correlated by transforming the original set of vectors to a new set which is known as Principal component. PCA tries to preserve the Global Structure of data i.e when converting d-dimensional data to d\u2019-dimensional data then it tries to map all the clusters as a whole due to which local structures might get lost. Application of this technique includes Noise filtering, feature extractions, stock market predictions, and gene data analysis.\n\n**<font color=\"blue\">T-distributed stochastic neighbourhood embedding (t-SNE):<\/font>** t-SNE is also a unsupervised non-linear dimensionality reduction and data visualization technique. The math behind t-SNE is quite complex but the idea is simple. It embeds the points from a higher dimension to a lower dimension trying to preserve the neighborhood of that point.This technique finds application in computer security research, music analysis, cancer research, bioinformatics, and biomedical signal processing.\n\n![image.png](attachment:image.png)\n\n","005d1203":"## TSNE using sklearn","59d2bd18":"Here pca_components will give the result obtained by dot product of our data and matrix of top 2 eigen vectors","1cc7e5dd":"**Step3 : Compute eigen values and eigen vectors** <br>\nNow, let us find the eigen values and eigen vectors of the covariance matrix . This is also called an eigen decomposition. The eigen values tell us the variance in the data set and eigen vectors tell us the corresponding direction of the variance.","f1f07451":"**Step5: Transforming data**","560f1986":"![image.png](attachment:image.png)\n\n\n\n\n\nThis kernel aims at following implementations:\n\n* PCA from scratch\n* PCA using Sklearn\n* TSNE using Sklean\n\nThis will be helpful for beginners ti understand dimensionality reduction techniques.","4a961720":"**Step4: Select top k eigen values and corresponding eigen vectors**\n\nSuppose we want top 2 features.As we have the top 2 eigen vectors and the original matrix with us, its time to form the new data set with reduced k-dimensions (here k=2). PCA being a linear technique, it forms linear equation between the old data set and new data set.\n\nnew data set =dot product([old data set],[eigen vector] )\n"}}