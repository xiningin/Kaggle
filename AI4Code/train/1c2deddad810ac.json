{"cell_type":{"d573a036":"code","d96661b5":"code","f726513a":"code","1f3dec02":"code","a4525da0":"code","b598e1a6":"code","87cfaa96":"code","ee4379a9":"code","8c7ccd98":"code","909cbc0d":"code","b7512307":"code","689241c9":"code","cbaae828":"code","0da50d1b":"code","9799fa2d":"code","9b343b4b":"code","adb7d5e2":"code","5ff67e80":"code","75a407d3":"code","de7e4aa7":"code","6b6d8aea":"code","f6e39416":"code","9241c6a6":"code","1139b2bd":"code","66254313":"markdown","00a8d5b0":"markdown","78fe6b90":"markdown","16be7a44":"markdown","02743526":"markdown"},"source":{"d573a036":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport dask.dataframe as dd\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport dask_xgboost as xgb\nimport dask.dataframe as dd\nfrom sklearn import preprocessing, metrics\nimport gc\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d96661b5":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n\n# function to read the data and merge it (ignoring some columns, this is a very fst model)\n\n\ndef read_data():\n    print('Reading files...')\n    calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n    calendar = reduce_mem_usage(calendar)\n    print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n    sell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\n    sell_prices = reduce_mem_usage(sell_prices)\n    print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n    sales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\n    print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n    return calendar, sell_prices, sales_train_validation, submission\n\n\ndef melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 55000000, merge = False):\n    \n    # melt sales data, get it ready for training\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    \n    # seperate test dataframes\n    test1_rows = [row for row in submission['id'] if 'validation' in row]\n    test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test1 = submission[submission['id'].isin(test1_rows)]\n    test2 = submission[submission['id'].isin(test2_rows)]\n    \n    # change column names\n    test1.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\n    test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n                      'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n    \n    # get product table\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # merge with product table\n    test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n    test1 = test1.merge(product, how = 'inner', on = 'id')\n    test2 = test2.merge(product, how = 'inner', on = 'id')\n    test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n    \n    # \n    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\n    \n    sales_train_validation['part'] = 'train'\n    test1['part'] = 'test1'\n    test2['part'] = 'test2'\n    \n    # add ignore index\n    data = pd.concat([sales_train_validation, test1, test2], axis = 0, ignore_index=True)\n    \n    del sales_train_validation, test1, test2\n    \n    # get only a sample for fst training\n    \n    # reset index after narrowing down to just one store\n    # data = data.reset_index()\n    data = data.loc[nrows:]\n\n    \n    # drop some calendar features\n    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    \n    # delete test2 for now\n    data = data[data['part'] != 'test2']\n    \n    if merge:\n        # notebook crash with the entire dataset (maybee use tensorflow, dask, pyspark xD)\n        data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n        data.drop(['d', 'day'], inplace = True, axis = 1)\n        # get the sell price data (this feature should be very important)\n        data = data.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n        print('Our final dataset to train has {} rows and {} columns'.format(data.shape[0], data.shape[1]))\n    else: \n        pass\n    \n    gc.collect()\n    \n    return data\n        \ncalendar, sell_prices, sales_train_validation, submission = read_data()\n#data = melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 27500000, merge = True)\n\n# changed left to inner","f726513a":"# narrow down to ones store to make faster\n\n# use .loc because dataframe\none_store_sales = sales_train_validation.loc[sales_train_validation['store_id'] =='CA_1']\none_store_sell_prices =  sell_prices.loc[sell_prices['store_id'] =='CA_1']\n \ndata = melt_and_merge(calendar, one_store_sell_prices, one_store_sales, submission, nrows = 50000, merge = True)","1f3dec02":"data.shape","a4525da0":"def transform(data):\n    \n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n        \n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = preprocessing.LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data\n\ndef simple_fe(data):\n    \n    # rolling demand features\n    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    \n    \n    # price features\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) \/ (data['lag_price_t1'])\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) \/ (data['rolling_price_max_t365'])\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    # time features\n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    \n    \n    return data\n\n","b598e1a6":"# transform the data\n\ndata = transform(data)\ndata = simple_fe(data)\ndata.shape","87cfaa96":"data.info()","ee4379a9":"# create copy of data so don't have to rerun later\ndata2 = data\n\n# create year\/month variable so can group by month\ndata2[\"yrmth\"] = data2[\"year\"].astype(str)+data2[\"month\"].astype(str)\n\n# calculate total demand for each item in each year\/month\ndata_agg_demand = data2.groupby([\"item_id\", \"yrmth\"]).agg(\"sum\")[\"demand\"]\n\n# get the median monthly demand for each item\ndata_agg_demand = data_agg_demand.groupby([\"item_id\"]).median()\n\n# get the median sales price for each item\ndata_agg_sales = data2.groupby([\"item_id\"]).agg(\"median\")[\"sell_price\"]\n\n","8c7ccd98":"# result: median demand per item\ndata_agg_demand\n\n","909cbc0d":"# result: median sales price per item\ndata_agg_sales","b7512307":"# put median demand\/sales into original data2 data\ndata_append = data2.merge(data_agg_demand, on = ['item_id'], how = 'inner')\ndata_append = data_append.merge(data_agg_sales, on = ['item_id'], how = 'inner')\n\n# rename new columns\ndata_append = data_append.rename(columns={\"demand_y\":\"med_mth_demand\"})\ndata_append = data_append.rename(columns={\"sell_price_y\":\"med_price\"})\n\n# just keep item, dept, cat, and median demand\ndata_append = data_append[[\"item_id\", \"dept_id\", \"cat_id\", \"med_mth_demand\", \"med_price\"]]\n# keep only one row per item\ndata_append = data_append.drop_duplicates()\n\n# show data\ndata_append\n\n","689241c9":"# create dummy variables for department and category\ndata_input = pd.get_dummies(data = data_append, columns = [\"dept_id\", \"cat_id\"])\ndata_input","cbaae828":"# histogram of demand\ndata_input['med_mth_demand'].hist(bins = 15)\n\n# extremely skewed data\n# don't use standard sacalar because data does not have normal distribution\n# will use min max scalar instead","0da50d1b":"# normalize demand and price column\nfrom sklearn.preprocessing import MinMaxScaler\ndata_input[[\"med_mth_demand\", \"med_price\"]] = MinMaxScaler().fit_transform(data_input[[\"med_mth_demand\", \"med_price\"]])\ndata_input","9799fa2d":"import statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.cluster import KMeans\n\n# select just the features\nfeature_input = data_input.iloc[:, 1:13]\nfeature_input","9b343b4b":"# determining k\nlimit = 15\nwcss=[]\nfor k in range(1,limit):\n    kmeans = KMeans(k)\n    kmeans.fit(feature_input)\n    wcss_iter = kmeans.inertia_\n    wcss.append(wcss_iter)\n\nnumber_clusters = range(1,limit)\nplt.plot(number_clusters,wcss)\nplt.title('The Elbow title')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\n\n# based on the plot we want 7 clusters","adb7d5e2":"# clustering\n# set random state to prevent it from changing on each run\nk = 7\nkmeans = KMeans(k, random_state=0)\nkmeans.fit(feature_input)\nidentified_clusters = kmeans.fit_predict(feature_input)\n\ndata_cluster = data_input\ndata_cluster[\"cluster\"] = identified_clusters\n\nprint(data_cluster.groupby([\"cluster\"]).size())\n\n# plotting by median monthly demand (after removing outliers so can see better)\ndata_reduce =  data_cluster.loc[data_cluster[\"med_mth_demand\"] <.20]\ndata_reduce.pivot(columns=\"cluster\", values=\"med_mth_demand\").plot.hist(bins = 50)\n\n\ndata_cluster.groupby('cluster').mean().sort_values(by = \"med_mth_demand\")\n\n# looks like cluster 5 has very low demand, so does cluster 0 (as evidenced by summary table)\n# cluster 1 has the highest demand by far\n# can also see most common department and category for each group","5ff67e80":"data_cluster","75a407d3":"# get a fresh version of the original dataset\n\ndata3 = data\ndata3.shape\n\n# get dataframe to merge (only cluster column)\ndata_cluster = data_cluster[[\"item_id\", \"cluster\"]]\n\n# add new cluster classification\ndata_apply = data3.merge(data_cluster, on = [\"item_id\"], how = 'inner')\ndata_apply","de7e4aa7":"# pull out 7 dataframes, one for each cluster\n\nclust_0 = data_apply.loc[data_apply[\"cluster\"] == 0]\nclust_1 = data_apply.loc[data_apply[\"cluster\"] == 1]\nclust_2 = data_apply.loc[data_apply[\"cluster\"] == 2]\nclust_3 = data_apply.loc[data_apply[\"cluster\"] == 3]\nclust_4 = data_apply.loc[data_apply[\"cluster\"] == 4]\nclust_5 = data_apply.loc[data_apply[\"cluster\"] == 5]\nclust_6 = data_apply.loc[data_apply[\"cluster\"] == 6]\n\n\n# verify lengths match\ntotal_count = len(clust_0)+len(clust_1)+len(clust_2)+len(clust_3)+len(clust_4)+len(clust_5)+len(clust_6)\nif total_count == len(data_apply):\n    print(\"length matches\")\nelse:\n    print(\"length does not match: error\")\n","6b6d8aea":"def run_lgb(data):\n    \n    # going to evaluate with the last 28 days\n    x_train = data[data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    y_val = x_val['demand']\n    test = data[(data['date'] > '2016-04-24')]\n    del data\n    gc.collect()\n\n    # define random hyperparammeters\n    params = {\n        'boosting_type': 'gbdt',\n        'metric': 'rmse',\n        'objective': 'regression',\n        'n_jobs': -1,\n        'seed': 236,\n        'learning_rate': 0.1,\n        'bagging_fraction': 0.75,\n        'bagging_freq': 10, \n        'colsample_bytree': 0.75}\n\n    train_set = lgb.Dataset(x_train[features], y_train)\n    val_set = lgb.Dataset(x_val[features], y_val)\n    \n    del x_train, y_train\n\n    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, valid_sets = [train_set, val_set], verbose_eval = 100)\n    val_pred = model.predict(x_val[features])\n    val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n    print(f'Our val rmse score is {val_score}')\n    y_pred = model.predict(test[features])\n    test['demand'] = y_pred\n    return test\n\ndef predict(test, submission):\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    final.to_csv('submission.csv', index = False)\n    \n\n# define list of features\nfeatures = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30']\n\n\ndef transform_train_and_eval(data):\n    data = transform(data)\n    data = simple_fe(data)\n    # reduce memory for new features so we can train\n    data = reduce_mem_usage(data)\n    test = run_lgb(data)\n    predict(test, submission)\n    \n#transform_train_and_eval(data)","f6e39416":"# run the model on each cluster\n# (running models using just the one store for speed, but ideally will apply the models to entire dataset)\n\nc0 = transform_train_and_eval(clust_0)\nc0_sub = pd.read_csv('submission.csv')\n\nc1 = transform_train_and_eval(clust_1)\nc1_sub = pd.read_csv('submission.csv')\n\nc2 = transform_train_and_eval(clust_2)\nc2_sub = pd.read_csv('submission.csv')\n\nc3 = transform_train_and_eval(clust_3)\nc3_sub = pd.read_csv('submission.csv')\n\nc4 = transform_train_and_eval(clust_4)\nc4_sub = pd.read_csv('submission.csv')\n\nc5 = transform_train_and_eval(clust_5)\nc5_sub = pd.read_csv('submission.csv')\n\nc6 = transform_train_and_eval(clust_6)\nc6_sub = pd.read_csv('submission.csv')\n","9241c6a6":"# combine submission files, just keep validation for CA_1\ncombo = pd.concat([c0_sub, c1_sub, c2_sub, c3_sub, c4_sub, c5_sub, c6_sub], axis=0)\ncombo = combo[(combo[\"F1\"])>0]\ncombo.to_csv('submission.csv', index = False)\n\n# predictions for 28 days in just the CA_1 store\ncombo.shape\ncombo","1139b2bd":"##### Ideal path to submit to competition, but could not get entire dataset to load without crashing\n\n# get entire dataset > melt merge\n# add cluster id to entire data set (based on item_id)\n# filter data into 7 parts (7 clusters)\n# run model on each of the 7 parts\n\n# example code below:\n\n#calendar, sell_prices, sales_train_validation, submission = read_data()\n#data4 = melt_and_merge(calendar, sell_prices, sales_train_validation, submission, nrows = 27500000, merge = True)\n#data_apply = data4.merge(data_cluster, on = [\"item_id\"], how = 'inner')\n\n\n#clust_0 = data_apply.loc[data_apply[\"cluster\"] == 0]    \n#c0 = transform_train_and_eval(clust_0)\n#c0_sub = pd.read_csv('submission.csv')     ## etc for each one\n\n\n#combo = pd.concat([c0_sub, c1_sub, etc.], axis=0)","66254313":"**Jessica Mohr**","00a8d5b0":"* We have the data to build our first model, let's build a baseline and predict the validation data (in our case is test1)","78fe6b90":"# Objective\n\n* Make a baseline model that predict the validation (28 days). \n* This competition has 2 stages, so the main objective is to make a model that can predict the demand for the next 28 days","16be7a44":"The code had an issue where validation data was filling in within the submission file but evaluation data was not.\n\nAlso I was unable to run the predictions on the entire dataset because the session kept getting cancled due to lack of memory avaliable.","02743526":"**Clustering results:**\n\n7 clusters\nOne with extremely high demand, meaning fast moving (1).\nFour with medium demand (4, 2, 3, 6), and two slow moving clusters with very low demand (0, 5).\n\nThe slowest moving cluster also had the lowest prices (0).\nThe most expensive was one of the medium demand products (4).\n\nThe two fastest moving products also had some of the lowest prices (6, 1).\n\nThe three fastest moving groups are most often food products.\nThe four slower moving groups are often hobby or household.\n\nTwo of the more expensive groups are most often household items, but the most expensive group is most often hobby items.\n  "}}