{"cell_type":{"1db4fee7":"code","192d139b":"code","3865de5f":"code","aa769368":"code","7140663a":"code","33544426":"code","23c1d55a":"code","ec42deef":"code","11a912d6":"code","4612e896":"code","5bbe2ee9":"code","273de79e":"code","6a0bc291":"code","4228d0d9":"code","832c18a5":"code","7e8db3fc":"code","189c2142":"code","d95fdf02":"code","3fc0b740":"code","9b0c3d51":"code","acf166b4":"code","ca91bb2b":"code","62dd0ddc":"code","a1dee9e9":"code","9b513b75":"code","08b1fa30":"code","f977154c":"code","824e58b5":"code","94475c53":"code","8922841a":"code","7d686561":"code","78130157":"code","a321d558":"code","8015c3bd":"code","7726ee51":"code","24f16609":"code","fa311717":"markdown","a1b69889":"markdown","1e9c20c9":"markdown","fc2bdacc":"markdown","8034ec5a":"markdown","a878e349":"markdown","28039621":"markdown","ae320483":"markdown","4e9118ef":"markdown","a1bd8d4c":"markdown","d8c2540c":"markdown","929a0f99":"markdown","5d17e6dd":"markdown","98e7e57d":"markdown","9e66ab93":"markdown","808fcf79":"markdown","2a92b0ce":"markdown","357b5fa1":"markdown","096d74d3":"markdown","11812bec":"markdown","346e4aca":"markdown","9cade0f1":"markdown","a839faa6":"markdown","627796b7":"markdown","50931e97":"markdown","23b7df34":"markdown","5867e3d4":"markdown","715b455f":"markdown","c3d3508a":"markdown","106bd81a":"markdown","a16f20d0":"markdown","54d84bf0":"markdown","6616fa31":"markdown","7a229684":"markdown","04275d4b":"markdown","633b1173":"markdown","96d92fd1":"markdown","e815490f":"markdown","c5d331a0":"markdown","0211c0b1":"markdown","b34aa341":"markdown","36e01ebb":"markdown","29468999":"markdown","3b250d01":"markdown","b822f59f":"markdown","b8b5909c":"markdown","9714cd7a":"markdown","19cb1f4d":"markdown","68e120ee":"markdown","04981143":"markdown","83938bd9":"markdown","ea4b65d2":"markdown","ddd2ad86":"markdown","8d873f6c":"markdown","1ab8ad28":"markdown","8347c79f":"markdown","34d2f718":"markdown","6fce34c4":"markdown","41eb5f22":"markdown","290bb5f1":"markdown","46dbc5cd":"markdown","26c071d1":"markdown","22d3b9cd":"markdown","7f47eb90":"markdown","451a86b1":"markdown","1d9b8069":"markdown","0f514858":"markdown","bea78cd8":"markdown","b3dbcd82":"markdown","02c92e1a":"markdown","9b6a115c":"markdown","dadb1c38":"markdown","53da259d":"markdown","f237eec5":"markdown","0f721e28":"markdown","4f2e859f":"markdown","755a469c":"markdown","e7907dec":"markdown","0b8c7ba3":"markdown","0694ea85":"markdown","c89818aa":"markdown","a3ca7f6a":"markdown","384d6fe0":"markdown","d93b746b":"markdown","cfcd3706":"markdown","b8d50067":"markdown","ca8b7440":"markdown","ce398c04":"markdown","d4894514":"markdown","3453fb54":"markdown","5a6d572e":"markdown"},"source":{"1db4fee7":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Input\nfrom keras.optimizers import Adam, SGD, Adadelta, Adagrad\n\n","192d139b":"##To read directly from Drive in Google Colab use:\n# from google.colab import drive\n# drive.mount('\/content\/drive')\n##input path: \".\/drive\/MyDrive\/botnet_dataset_DL.csv\"\n\n##To read in Kaggle, upload file in Kaggle \n##input path: \"\/kaggle\/input\/botnetdl\/botnet_dataset_DL.csv\"","3865de5f":"df = pd.read_csv(\"\/kaggle\/input\/botnet-dataset-dlcsv\/botnet_dataset_DL.csv\")","aa769368":"print(\"N\u00famero de filas: \"+str(df.shape[0]))\nprint(\"N\u00famero de columnas: \"+str(df.shape[1]))\n\nclases=pd.DataFrame(df['type'].value_counts())\nprint(\"Tenemos \"+str(len(clases))+ \" clases.\")\nprint(\"Su total y proporci\u00f3n es: \")\nclases[\"proporcion\"] = round(clases[\"type\"]\/df.shape[0],2)\nclases.rename(columns={\"type\": \"total\"}, inplace=True)\nprint(clases)\n\n## para ver como funciona la estandarizaci\u00f3n y comprobar que la realizo correctamente despu\u00e9s, realizo el histograma de la variable MI_dir_L5_weight\n## esto sirve como exploraci\u00f3n de los datos, pero debido a la gran cantidad de columnas que tenemos, y a que en una red neuronal no estudiamos\n## que variables justifican la respuesta del modelo, solo estudio una variable. \n\nplt.figure(figsize=(15,8))\nplt.title('Histogram of ' + df.columns[0] )\nplt.hist(df.iloc[:,0], facecolor='blue')\nplt.show()\n\n## estudia el rango de las variables\nrango = list()\nfor col in range(0, len(df.columns)-1):\n    rango.append(max(df.iloc[:,col])-min(df.iloc[:,col]))\n\nplt.figure(figsize=(8,12))\nplt.title('Violin plot of range of variables' )\nplt.violinplot(rango,showmedians=True, quantiles=[0.25,0.75])\nplt.show()","7140663a":"#Changing pandas dataframe to numpy array\nfeatures = df.iloc[:,:115].values\nlabels = df.iloc[:,115:116].values","33544426":"features=(features - np.mean(features, axis=0)) \/ np.std(features, axis=0)\n\n# muestro el histograma de la primera variable, para demostrar que se ha realizado la estandarizaci\u00f3n. \nplt.figure(figsize=(15,8))\nplt.title('Histogram of ' + df.columns[0] + ' standardized' )\nplt.hist(features[:,0], facecolor='blue')\nplt.show()\n\n## estudia el rango de las variables una vez estandarizadas\nrango = list()\nfor col in range(0, features.shape[1]):\n    rango.append(max(features[:,col])-min(features[:,col]))\n\nplt.figure(figsize=(8,12))\nplt.title('Violin plot of range of variables' )\nplt.violinplot(rango, showmedians=True, quantiles=[0.25,0.75])\nplt.show()\n\n\n","23c1d55a":"\n# binarizaci\u00f3n de las categor\u00edas\nlabels_onehot = pd.get_dummies(pd.DataFrame(labels))\nlabels_names=labels_onehot.columns\nlabels_onehot=labels_onehot.values\n\nlabels_names=[label.replace(\"0_\", \"\") for label in labels_names]\n\nlabels, labels_onehot, labels_names","ec42deef":"train_x, test_x, train_y, test_y = train_test_split(features, labels_onehot, test_size=0.2)","11a912d6":"model = Sequential()\nmodel.add(Dense(64, input_shape=(115,), activation=\"relu\"))\nmodel.add(Dense(32, activation=\"relu\"))\nmodel.add(Dense(10, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(6, activation=\"softmax\"))\nmodel.summary()","4612e896":"model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy',metrics=['accuracy'])\n","5bbe2ee9":"\n\nhistory =model.fit(x=train_x, y=train_y,\n                epochs=20,\n                validation_data=(test_x, test_y))\n\n# Visualizamos la evoluci\u00f3n de la accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n\n# Visualizamos la evoluci\u00f3n del error cometido por la red\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","273de79e":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nf, ax = plt.subplots(1, 1, figsize = (12, 8))\n\ncm=confusion_matrix( np.argmax(test_y, axis=-1),np.argmax(model.predict(test_x), axis=-1))\ndisp=ConfusionMatrixDisplay(cm, display_labels=labels_names)\ndisp.plot(ax=ax, values_format='.5g') \n","6a0bc291":"\nimport timeit\n\n\ndef crea_entrena_y_cronometra_modelo_nn(capas_ocultas):\n    num_capas_ocultas = len(capas_ocultas)\n    model = Sequential()\n    model.add(Dense(capas_ocultas[0], input_shape=(115,), activation=\"relu\"))\n    for capa in range(1,num_capas_ocultas):\n        model.add(Dense(capas_ocultas[capa], activation=\"relu\"))\n\n    model.add(Dense(6, activation=\"softmax\"))\n    model.compile(optimizer=Adam(lr=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n\n    start = timeit.default_timer()\n    history= model.fit(x=train_x, y=train_y, epochs=20,validation_data=(test_x, test_y), verbose=0)\n    tiempo_train = timeit.default_timer() - start\n    return num_capas_ocultas, tiempo_train, model, history\n\n\n    ","4228d0d9":"\nlista_todas_capas = [[10,8],[32,10,8],[64,32,10,8],[96,64,32,10,8]]\n\nresultados_tiempo = pd.DataFrame(columns=['num_capas','tiempo_train', 'capas'])\nfor lista in lista_todas_capas:\n    num_capas_ocultas, tiempo_train, model, history= crea_entrena_y_cronometra_modelo_nn(capas_ocultas = lista)\n    resultados_tiempo.loc[len(resultados_tiempo)] = [int(num_capas_ocultas),round(tiempo_train, 2), lista]\n","832c18a5":"# Visualizamos la evoluci\u00f3n del error cometido por la red\nresultados_tiempo=resultados_tiempo.set_index('num_capas')\nplt.plot(resultados_tiempo['tiempo_train'],color='blue', marker='o')\nplt.title('tiempo de entrenamiento respecto al n\u00famero de capas ocultas')\nplt.ylabel('tiempo de entrenamiento')\nplt.xlabel('capas ocultas')\nplt.legend(['tiempo de entrenamiento'], loc='upper right')\nplt.show()","7e8db3fc":"# elegir mejor arquitectura anterior\nmejor_arquitectura=resultados_tiempo.sort_values(by='tiempo_train', ascending=True).iloc[0]['capas']","189c2142":"\n# pasar el optimizador ya creado\ndef crea_entrena_y_cronometra_modelo_nn_v2(capas_ocultas, optimizador):\n    num_capas_ocultas = len(capas_ocultas)\n    model = Sequential()\n    model.add(Dense(capas_ocultas[0], input_shape=(115,), activation=\"relu\"))\n    for capa in range(1,num_capas_ocultas):\n        model.add(Dense(capas_ocultas[capa], activation=\"relu\"))\n\n    model.add(Dense(6, activation=\"softmax\"))\n    \n    model.compile(optimizer=optimizador, loss='categorical_crossentropy', metrics=['accuracy'])\n\n    start = timeit.default_timer()\n    history= model.fit(x=train_x, y=train_y, epochs=20,validation_data=(test_x, test_y), verbose=0)\n    tiempo_train = timeit.default_timer() - start\n    \n    loss, accuracy = model.evaluate(test_x,test_y, verbose=0)\n    return num_capas_ocultas, tiempo_train, accuracy, loss, model, history\n","d95fdf02":"resultados_opts = pd.DataFrame(columns=['opt','lr', 'tiempo', 'accuracy', 'modelo','modelo_entrenado'])\n\noptimizador = [Adam, SGD, Adadelta, Adagrad]\nlearning_rate = [0.001, 0.01, 0.05, 0.1]\nfor opt in optimizador:\n    for lr in learning_rate:\n        num_capas_ocultas, tiempo_train, accuracy, loss, modelo,history = crea_entrena_y_cronometra_modelo_nn_v2(mejor_arquitectura,opt(learning_rate=lr))\n        resultados_opts.loc[len(resultados_opts)] = [str(opt).split(\".\")[-1].split(\"'\")[0], lr, round(tiempo_train,2),round(accuracy,4), modelo,history]\n        \n","3fc0b740":"\n# Visualizamos la evoluci\u00f3n de la accuracy respecto al learning rate y para cada modelo\nplt.figure(figsize=(15,8))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adam']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adam']['accuracy'], linestyle = '-')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adam']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adam']['accuracy']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='SGD']['lr'],resultados_opts.loc[resultados_opts['opt']=='SGD']['accuracy'], linestyle = '--')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='SGD']['lr'],resultados_opts.loc[resultados_opts['opt']=='SGD']['accuracy']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adadelta']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adadelta']['accuracy'], linestyle = '-.')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adadelta']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adadelta']['accuracy']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adagrad']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adagrad']['accuracy'], linestyle = ':')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adagrad']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adagrad']['accuracy']): \n    plt.text(a, b, str(b))\nplt.title('accuracy by optimizer and changing by learning rate')\nplt.ylabel('accuracy')\nplt.xlabel('learning rate')\nplt.xticks(learning_rate)\nplt.legend(['Adam', 'SGD', 'Adadelta', 'Adagrad'])\nplt.show()\n\n# Visualizamos la evoluci\u00f3n del tiempo de ejecuci\u00f3n respecto al learning rate y para cada modelo\nplt.figure(figsize=(15,8))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adam']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adam']['tiempo'], linestyle = '-')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adam']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adam']['tiempo']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='SGD']['lr'],resultados_opts.loc[resultados_opts['opt']=='SGD']['tiempo'], linestyle = '--')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='SGD']['lr'],resultados_opts.loc[resultados_opts['opt']=='SGD']['tiempo']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adadelta']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adadelta']['tiempo'], linestyle = '-.')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adadelta']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adadelta']['tiempo']): \n    plt.text(a, b, str(b))\nplt.plot(resultados_opts.loc[resultados_opts['opt']=='Adagrad']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adagrad']['tiempo'], linestyle = ':')\nfor a,b in zip(resultados_opts.loc[resultados_opts['opt']=='Adagrad']['lr'],resultados_opts.loc[resultados_opts['opt']=='Adagrad']['tiempo']): \n    plt.text(a, b, str(b))\nplt.title('time by optimizer and changing by learning rate')\nplt.ylabel('time')\nplt.xlabel('learning rate')\nplt.xticks(learning_rate)\nplt.legend(['Adam', 'SGD', 'Adadelta', 'Adagrad'])\nplt.show()","9b0c3d51":"mejor_modelo_hist=resultados_opts.loc[(resultados_opts['opt']=='SGD') & (resultados_opts['lr'] == 0.1)]['modelo_entrenado'].iloc[0]\nmejor_modelo=resultados_opts.loc[(resultados_opts['opt']=='SGD') & (resultados_opts['lr'] == 0.1)]['modelo'].iloc[0]\n\n# Visualizamos la evoluci\u00f3n de la accuracy\nplt.plot(mejor_modelo_hist.history['accuracy'])\nplt.plot(mejor_modelo_hist.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')\nplt.show()\n\n# Visualizamos la evoluci\u00f3n del error cometido por la red\nplt.plot(mejor_modelo_hist.history['loss'])\nplt.plot(mejor_modelo_hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\n\nf, ax = plt.subplots(1, 1, figsize = (12, 8))\n\ncm=confusion_matrix(np.argmax(test_y, axis=-1), np.argmax(mejor_modelo.predict(test_x), axis=-1))\ndisp=ConfusionMatrixDisplay(cm, display_labels=labels_names)\ndisp.plot(ax=ax, values_format='.5g') \n","acf166b4":"labels = np.where(labels=='benign',0,1)\npd.DataFrame(labels).value_counts()","ca91bb2b":"data = np.hstack((features, labels))\nbenignos = data[:,115]==0\ndata_benignos = data[benignos]\ndata_malignos = data[~benignos]\n\ndata_benignos.shape, data_malignos.shape","62dd0ddc":"# separo el 20% de benignos para el test\ntrain_and_valid_benign,test_benign = train_test_split(data_benignos, test_size=0.2)\n# separo el 80% de datos benignos en 40% para el entrenamiento y el otro 40% para la validaci\u00f3n. \ntrain, valid=train_test_split(train_and_valid_benign, test_size=0.5)\n# escojo de todos los malignos, solo el mismo tama\u00f1o de datos que de benignos que hay en test\ntest_malign=data_malignos[np.random.choice(data_malignos.shape[0], test_benign.shape[0], replace=False)]\n# agrupo los datos de test, juntando el 20% de los benignos y el mismo n\u00famero de malignos\ntest=np.vstack((test_benign,test_malign))","a1dee9e9":"train.shape, valid.shape, test.shape","9b513b75":"# capa de entrada\ncapa_entrada = Input(shape=(115,))\n# capas ocultas\nencoder = Dense(90, activation='tanh')(capa_entrada)\nencoder = Dense(60, activation='tanh')(encoder)\nencoder = Dense(40, activation='tanh')(encoder)\n# representaci\u00f3n codificada (encoded)\nencoder = Dense(30, activation='tanh')(encoder)\n\nencoder =Model(capa_entrada,encoder)\nencoder.summary()","08b1fa30":"# capa de entrada, en este caso de la dimensi\u00f3n de la representaci\u00f3n\ncapa_representacion_in = Input(shape=(30,))\n# capas ocultas\ndecoder = Dense(40, activation='tanh')(capa_representacion_in)\ndecoder = Dense(60, activation='tanh')(decoder)\ndecoder = Dense(90, activation='tanh')(decoder)\n# representaci\u00f3n codificada (encoded)\ndecoder = Dense(115)(decoder)\n\ndecoder =Model(capa_representacion_in,decoder)\ndecoder.summary()","f977154c":"autoencoder=Model(capa_entrada, decoder(encoder(capa_entrada)))\nprint(\"Primero vemos la estructura del autoencoder.\")\nautoencoder.summary()\nprint(\"A continuaci\u00f3n se muestra la estructura del segundo y tercer componente del modelo, que son el encoder y decoder.\")\nautoencoder.layers[1].summary()\nautoencoder.layers[2].summary()","824e58b5":"train_sin_label=train[:,0:115]\nvalid_sin_label=valid[:,0:115]\ntest_sin_label = test[:,0:115]","94475c53":"from tensorflow.keras.callbacks import EarlyStopping\n\nEA = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nautoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n\nautoencoder_history=autoencoder.fit(train_sin_label, train_sin_label,\n                epochs=100,\n                batch_size=32,\n                validation_data=(valid_sin_label, valid_sin_label), callbacks=[EA])","8922841a":"# Visualizamos la evoluci\u00f3n del error cometido por la red\nplt.plot(range(1,len(autoencoder_history.history['loss'])+1),autoencoder_history.history['loss'])\nplt.plot( range(1,len(autoencoder_history.history['val_loss'])+1),autoencoder_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","7d686561":"predicciones_valid = autoencoder.predict(valid_sin_label)\nerror_reconstruccion_valid = keras.losses.mean_squared_error(valid_sin_label, predicciones_valid)\nplt.figure(figsize=(15,8))\nplt.hist(error_reconstruccion_valid.numpy(),bins = np.arange(min(error_reconstruccion_valid.numpy()), max(error_reconstruccion_valid.numpy()) + 1,1), facecolor='blue')\nplt.xticks(np.arange(round(min(error_reconstruccion_valid.numpy())), round(max(error_reconstruccion_valid.numpy())+1,-1), 5))\nplt.show()","78130157":"plt.figure(figsize=(15,8))\nplt.hist(error_reconstruccion_valid.numpy(),bins = np.arange(min(error_reconstruccion_valid.numpy()), max(error_reconstruccion_valid.numpy()) + 1,1), facecolor='blue')\nplt.ylim(0, 1000)\nplt.xticks(np.arange(round(min(error_reconstruccion_valid.numpy())), round(max(error_reconstruccion_valid.numpy())+1,-1), 5))\nplt.show()","a321d558":"predicciones_test = autoencoder.predict(test_sin_label)\nerror_reconstruccion_test = keras.losses.mean_squared_error(test_sin_label, predicciones_test)\nplt.figure(figsize=(15,8))\nplt.hist(error_reconstruccion_test.numpy(),bins = np.arange(min(error_reconstruccion_test.numpy()), max(error_reconstruccion_test.numpy()) + 1,1), facecolor='blue')\nplt.xticks(np.arange(round(min(error_reconstruccion_test.numpy())), round(max(error_reconstruccion_test.numpy())+1,-1), 5))\nplt.show()","8015c3bd":"plt.figure(figsize=(15,8))\nplt.hist(error_reconstruccion_test.numpy(),bins = np.arange(min(error_reconstruccion_test.numpy()), max(error_reconstruccion_test.numpy()) + 1,1), facecolor='blue')\nplt.ylim(0, 1000)\nplt.xticks(np.arange(round(min(error_reconstruccion_test.numpy())), round(max(error_reconstruccion_test.numpy())+1,-1), 5))\nplt.show()","7726ee51":"from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\nresultados_umbral = pd.DataFrame(columns=['N','umbral', 'accuracy_val', 'accuracy_test', 'precision','recall', 'especificidad', 'FPR'])\nfor n in range(1,11):\n    umbral = np.mean(error_reconstruccion_valid)+(n*np.std(error_reconstruccion_valid))\n    prediccion_valid = np.where(error_reconstruccion_valid > umbral, 1, 0)\n    accuracy_valid = accuracy_score(valid[:,115],prediccion_valid)\n    prediccion_test = np.where(error_reconstruccion_test > umbral, 1, 0)\n    accuracy_test = accuracy_score(test[:,115],prediccion_test)\n    precision_test = precision_score(test[:,115], prediccion_test)\n    recall_test = recall_score(test[:,115], prediccion_test)\n    tn, fp, fn, tp = confusion_matrix(test[:,115], prediccion_test).ravel()\n    specificity = tn \/ (tn+fp)\n    fpr = fp \/ (fp+tn)\n    resultados_umbral.loc[n]=[int(n), umbral, accuracy_valid, accuracy_test,precision_test,recall_test,specificity,fpr]\n    \ndisplay(resultados_umbral)\n    \n","24f16609":"umbral = np.mean(error_reconstruccion_valid)+(1*np.std(error_reconstruccion_valid))\nprediccion_valid = np.where(error_reconstruccion_valid > umbral, 1, 0)\naccuracy_valid = accuracy_score(valid[:,115],prediccion_valid)\nprediccion_test = np.where(error_reconstruccion_test > umbral, 1, 0)\naccuracy_test = accuracy_score(test[:,115],prediccion_test)\nprecision_test = precision_score(test[:,115], prediccion_test)\nrecall_test = recall_score(test[:,115], prediccion_test)\ntn, fp, fn, tp = confusion_matrix(test[:,115], prediccion_test).ravel()\nspecificity = tn \/ (tn+fp)\nfpr = fp \/ (fp+tn)\nprint(\"accuracy test:\"+ str(accuracy_test))\nprint(\"recall test:\"+ str(recall_test))\nprint(\"precision test:\"+ str(precision_test))\n\n\nf, ax = plt.subplots(1, 1, figsize = (12, 8))\ncm=confusion_matrix(test[:,115], prediccion_test)\ndisp=ConfusionMatrixDisplay(cm, display_labels=[\"benigno\", \"maligno\"])\ndisp.plot(ax=ax, values_format='.5g') \n","fa311717":"Finalmente, unificamos ambos bloques en uno solo para definir el Autoencoder.","a1b69889":"**Explicaci\u00f3n:** En la gr\u00e1fica de evoluci\u00f3n de accuracy, vemos como el modelo comenz\u00f3 en su primera epoch bastante acertado con m\u00e1s de un 97% de acierto en train y test. A partir de aqu\u00ed, el modelo ascendi\u00f3 hasta estar cerca del 1 de accuracy en todas las epochs, salvo en la tercera, que disminuyo su rendimiendo en ambos conjuntos. A partir de la cuarta, el modelo dio unos peque\u00f1os cambios que le hicieron empeorar un poco en algunas epochs, pero se corrigi\u00f3 en la siguiente, permitiendo el accuracy final sea muy alto y la loss final sea de casi 0. \n\nEn la matriz de confusi\u00f3n, vemos como el modelo efectivamente est\u00e1 correctamente, entrenado, es capaz de predecir correctamente la gran mayor\u00eda de instancias en su clase correcta, salvo unas poas instancias, de nuevo ocurre lo mismo que anteriormente, ya que la mayor\u00eda de errores son de predecir como benign instancias del resto de clases. \n\nPodemos concluir que el modelo de red neuronal que he desarrollado a partir de 2 capas ocultas, entrenado con un optimizador SGD y un learning rate de 0.1 es un modelo muy bueno prediciendo estos ataques. Adem\u00e1s, de todos los que hemos entrenado es el modelo m\u00e1s eficiente, ya que consigue el mismo resultado que otros modelos en menos tiempo. ","1e9c20c9":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Define el primer bloque del Autoencoder, el codificador (o <i>encoder<\/i>), junto con la capa de entrada. Todas las capas ocultas deber\u00e1n tener funci\u00f3n de activacion <code>tanh<\/code>\n<\/div>","fc2bdacc":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Cuesti\u00f3n:<\/strong> \u00bfQu\u00e9 diferencia hay entre los Autoencoders cl\u00e1sicos y los Variational Autoencoders? \u00bfQu\u00e9 utilidad tendr\u00edan los Variational Autoencoders en el problema tratado en la pr\u00e1ctica?\n<\/div>","8034ec5a":"Actualmente existen muchos dispositivos pertenecientes al Internet de las Cosas (*Internet of Things*, IoT). Su uso se extiende a muchos campos, como neveras inteligentes, asistentes virtuales de voz (Google Home, Alexa,...), c\u00e1maras de seguridad, detectores, etc. Este crecimiento tambi\u00e9n ha conllevado un aumento de ataques de botnets, con el fin de tomar control de los dispositivos de forma remota y poder as\u00ed enviar spam, denegar servicios, generar beneficios con fraude publicitario, robo de bitcoins, etc. ","a878e349":"Cargamos el conjunto de datos:","28039621":"**Explicaci\u00f3n:** Primero vemos la estructura del autoencoder formado a partir del encoder y decoder, vemos como est\u00e1 compuesto por una entrada, el modelo encoder y el modelo decoder. Por lo tanto el encoder y el decoder est\u00e1n conectados tal como se espera dentro del autoencoder, pero podemos acceder a cada uno de manera separada y extraerlos uno vez est\u00e9n entrenados.","ae320483":"### 3.3 Entrenamiento del modelo","4e9118ef":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Construye el Autoencoder con el codificador y decodificador dise\u00f1ados y muestra la estructura de capas del modelo\n<\/div>","a1bd8d4c":"A continuaci\u00f3n compilaremos y entrenaremos el modelo con la funci\u00f3n de p\u00e9rdida m\u00e1s adecuada.","d8c2540c":"Al acercar la gr\u00e1fica en el eje y, podemos ver que hay much\u00edsimas instancias de m\u00e1s de 1 de mse, lo cual posiblemente sean instancias malignas junto con algunas de las pocas benignas que nuestro autoencoder no puede codificar correctamente. Vemos que hay muchas instancias con mse de m\u00e1s de 10, incluso alguna de m\u00e1s de 85.","929a0f99":"Finalmente, dividimos el conjunto de datos en 80% para entrenamiento y 20% para test.","5d17e6dd":"**Explicaci\u00f3n:** Al ver el conjunto de test, vemos tambi\u00e9n que hay much\u00edsimas instancias con un mse de entre 0 y 1, pero claramente hay m\u00e1s instancias con mse mayores, esto se debe a que las instancias malignas no se pueden reconstruir correctamente por el autoencoder y obtienen un mse alto. ","98e7e57d":"Vemos que el modelo final, es capaz de identificar si un ataque es maligno o benigno con un 0.9223 de acierto, destaca sobretodo que es muy bueno acertando cuando se predice un ataque maligno, ya que su precision es de 0.9921 lo que significa que cuando predice que una instancia es maligna m\u00e1s de un 99% de las veces esta es maligna, lo cual se ve reflejado en la matriz de confusi\u00f3n, porque vemos que apenas hay 133 beningos que se han predicho como malignos pero se han acertado muchisimos malignos. El modelo es un poco peor a la hora de identificar que todos los ataques malignos son malignos, ya que hay bastantes ataques malignos que confunde con benignos, lo que hace que su recall sea  de 0.8520 y que veamos que hay 2916 malignos predichos como benignos. En general, vemos que el autoencoder con su umbral es un modelo un poco conservador, que se asegura de no hacer falsos positivos, lo que hace que le cueste m\u00e1s detectar los positivos reales. \n\nEl modelo es mejorable, sobretodo respecto a la diferencia de accuracy entre el acierto en validaci\u00f3n y el acierto en test, esto se debe sobretodo a que el modelo es capaz de comprender muy bien como se codifica y decodifica casi todas las instancias benignas, pero hay instancias benignas que son algo diferentes y por tanto, la red neuronal no las decodifica correctamente, su error de mse es mayor y el umbral que se define es algo superior de lo que deber\u00eda ser, haciendo que hayan varias instancias malignas en el conjunto de test que se clasifican como benignas para clasificar la mayor\u00eda de instancias benignas correctamente. Esto se ve reflejado, como ya se ha comentado en el recall del modelo, porque hay varios falsos negativos, es decir instancias malignas clasificadas como benignas. \n**Una posibilidad de soluci\u00f3n para esto, ser\u00eda aumentar la arquitectura de la red, a\u00f1adiendo m\u00e1s capas ocultas con m\u00e1s neuronas**, de esta manera es posible que el autoencoder sea capaz de codificar correctamente estas instancias benignas en el conjunto de entrenamiento para descodificarlas correctamente. A\u00f1adiendo este poder a la red es posible que a la hora de definir un umbral este sea m\u00e1s bajo y aumente las instancias malignas correctamente identificadas en el test puesto que estas seguiran teniendo un mse muy grande, mientras que las benignas del test tendr\u00e1n todas un mse peque\u00f1o puesto que la red es mejor codificando y decodificando estas. \n\nOtro aspecto interensante que se observa ocurre en el entrenamiento del modelo, al ver la gr\u00e1fica de loss del modelo por epoch para train y valid, vemos como el modelo disminuye su loss rapidamente, y despu\u00e9s comienza a dar bandazos sin conseguir disminuir la loss. Gracias a que utilizamos el early stopping, en cuanto el modelo comienza a dar estos bandazos, nos quedamos con la mejor iteraci\u00f3n hasta el momento, pero puede ser interesante controlar la velocidad de aprendizaje del modelo, **definiendo una funci\u00f3n de aprendizaje variable que se reduzca cuando se tiene una loss peque\u00f1a**, con el objetivo de que el modelo no de bandazos y continue aprendiendo un poco m\u00e1s para codificar y decodificar mejor las instancias benignas.\n\nPor otra parte, se podr\u00eda interpretar esta diferencia entre el accuracy de validation y test como debido a un sobreespecializaci\u00f3n de la red sobre algunas de las instancias benignas, haciendo que otras no se codifiquen correctamente y al decodificarlas no se obtenga el valor de sus variables correctamente, haciendo que tengan un mse muy alto y por tanto supongan un umbral mayor debido a que no se comprende correctamente como codificar y decodificar de manera general todas las instancias benignas. **Una manera de acabar con esta sobrespecializaci\u00f3n posible sobre algunas instancias es aplicar la regularizaci\u00f3n L2,** esta consiste en a\u00f1adir un t\u00e9rmino extra a la funci\u00f3n de coste haciendo que los pesos de la red se modifiquen de manera gradual, y encontrando un equilibrio entre pesos peque\u00f1os y la funci\u00f3n de coste. **Otra posible soluci\u00f3n es realizar un dropout**, este consiste en que cada neurona de las capas de entrada y ocultas tienen una posibilidad de ser eliminadas en una etapa de entrenamiento, por lo que no se utiliza para esa etapa de entrenamiento. De esta manera el autoencoder generalizar\u00e1 mejor con todas las entradas benignas con las que se entrene y es posible que sepa reconstruir mejor todas las instancias. ","9e66ab93":"**Explicaci\u00f3n:** En la matriz de confusi\u00f3n vemos como la grand\u00edsima mayor\u00eda de instancias de test se han predicho correctamente, lo m\u00e1s destacable parece ser que de todos los tipos de ataques que no son benign se malclasifican unos pocos como benign, pero que se predicen los benign reales correctamente siempre, ya que todos los benign reales se han predicho como benign. ","808fcf79":"Empezaremos preparando el conjunto de datos tal y como se ha explicado.","2a92b0ce":"**Explicaci\u00f3n:** Los datos se encontraban asociados a su label para poder hacer la separaci\u00f3n en los 3 conjuntos, pero ahora mantenemos solo las features sin las etiquetas para entrenar el modelo y luego para validarlo o testearlo. ","357b5fa1":"**Explicaci\u00f3n:** Vemos que el modelo se ha entrenado correctamente a lo largo de las 20 epochs, tambi\u00e9n podemos ver que en la primera epoch comenz\u00f3 con una loss muy baja y un accuracy bastante alto. Estas m\u00e9tricas ha ido dando bandazos al pasar las epochs debido a que el learning rate es demasiado alto para las etapas de entrenamiento por mini-lotes que se realizan. No obstante, vemos que por suerte, el modelo consigue acabar con un accuracy muy bueno y una loss muy baja. ","096d74d3":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Cuesti\u00f3n:<\/strong> Tanto los modelos Variational Autoencoders (VAE) como las Generative Adversarial Networks (GAN) son modelos generativos. Explica en un m\u00e1ximo de 1000 caracteres cu\u00e1l es la diferencia entre ambos modelos.\n<\/div>","11812bec":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.5 ptos):<\/strong> Define y construye los conjuntos de entrenamiento, validaci\u00f3n y test (con 20% de datos benignos) adecuados para el Autoencoder. \n    \nIndica el n\u00famero de instancias final de cada conjunto.\n    \n \n-----------------------------------------------------------------------------------------------------------\n<i>Recordatorio<\/i>: el conjunto de test deber\u00e1 tener un 20% de datos de la clase benigna, y una proporci\u00f3n igual de clase maligna. Los conjuntos de entrenamiento y validaci\u00f3n deber\u00e1n incluir el resto de datos benignos con la misma proporci\u00f3n.\n<\/div>","346e4aca":"**Explicaci\u00f3n:** Al ver el sumario del modelo, vemos que se ha generado tal y como se especifica en el enunciado, est\u00e1 compuesto por 4 capas ocultas, y una de salida. El n\u00famero de capas de la capa de salida se asigna a 6 porque es el n\u00famero de clases que tenemos en el ejercicio, debido a que tenemos que diferenciar entre las 5 clases de ataques malignos y la clase de ataque benigno. \n\nOtra informaci\u00f3n que nos proporciona el sumario es del n\u00famero de par\u00e1metros que se entrenar\u00e1n en la red, este es mayor cuanto m\u00e1s conexiones de neuronas hay entre una capa y otra, por ejemplo, entre la capa de salida y la \u00faltima capa oculta se entrenar\u00e1n 54 par\u00e1metros, que se componen de los 8 pesos para cada neurona de la capa de salida respecto a la salida de las neuronas de la capa oculta (8x6=48), m\u00e1s los 8 pesos de bias para las neuronas de las capas de salida. (48+6=54). ","9cade0f1":"## 3. Detecci\u00f3n de anomal\u00edas con Autoencoders (5 puntos)","a839faa6":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio:<\/strong> Explorar los datos: n\u00famero de caracter\u00edsticas (<i>features<\/i>), n\u00famero de clases del conjunto, su proporci\u00f3n, etc. \n<\/div>","627796b7":"A continuaci\u00f3n compilaremos y entrenaremos el modelo, aplicando la t\u00e9cnica de *Early Stopping* para evitar que el error en los datos de validaci\u00f3n aumente y haya sobreentrenamiento. En este caso no especificaremos ninguna m\u00e9trica en la compilaci\u00f3n, dado que la m\u00e9trica *accuracy* utilizada en el ejercicio 2 es propia de los problemas de clasificaci\u00f3n.","50931e97":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Unifica las clases pertenecientes a ataques de botnet en una \u00fanica clase, de modo que el dataset conste ahora de dos clases: benigno y malicioso en formato binario (0 y 1 respectivamente)\n<\/div>","23b7df34":"## 4. Variational Autoencoders (1 punto)","5867e3d4":"Antes de dise\u00f1ar una red neuronal es importante tener claros el tipo de datos que se tienen y el objetivo final. Usar unos par\u00e1metros inadecuados al conjunto de datos puede llevar a resultados incorrectos y poco fiables.","715b455f":"#### 2.3.1 N\u00famero de capas ocultas","c3d3508a":"## 1. Pre-procesado (0.25 puntos)","106bd81a":"#### 2.3.2 Velocidad de aprendizaje y Optimizador","a16f20d0":"**Explicaci\u00f3n:** He unido las features estandarizadas con las etiquetas, para separar tods los datos en funci\u00f3n de si son malignos o benignos.","54d84bf0":"El primer modelo que dise\u00f1aremos ser\u00e1 un modelo que nos permita clasificar autom\u00e1ticamente todas las clases del conjunto de datos. Utilizando las funcionalidades de Keras, nos basaremos en un modelo secuencial de capas completamente conectadas. ","6616fa31":"**Explicaci\u00f3n:** He definido el early stopping del modelo tal y como se indica en el enunciado, de tal manera que cuando lleve 5 epochs sin mejorar el resultado de loss, se volvera a la mejor epoch anterior. Adem\u00e1s compilo el modelo con un optimizador Adam con learning rate de 0.001 y la funci\u00f3n de perdida indicada. El autoencoder lo entreno, 100 epochs, con una batch_size de 32 y el early stopping, de manera que es posible que pare antes de las 100 epochs. ","7a229684":"**Respuesta**: La loss function a utilizar tiene que ser **CategoricalCrossentropy**. Se tiene que usar una loss function de entrop\u00eda cruzada para evitar la posible saturaci\u00f3n que se puede producir en las neuronas de la capa de salida si se encuentran en valores extremos. Adem\u00e1s, el utilizar esta funci\u00f3n acelera el entrenamiento generalmente aunque no se produzca esta saturaci\u00f3n. \nEn este caso, usamos en concreto la Categorical porque la salida de nuestra red neuronal tiene 6 etiquetas de clase. Si tuvieramos dos, usariamos la BinaryCrossentropy. ","04275d4b":"El siguiente c\u00f3digo carga los paquetes necesarios para la pr\u00e1ctica:","633b1173":"A continuaci\u00f3n, seleccionaremos el modelo con el que se haya obtenido un tiempo de entrenamiento menor en el ejercicio anterior, y compararemos su rendimiento en funci\u00f3n de la velocidad de aprendizaje y de otros optimizadores. ","96d92fd1":"**Respuesta:** La funci\u00f3n de activaci\u00f3n para la capa de salida ser\u00e1 la **funci\u00f3n softmax**, el valor tr\u00e1s la aplicaci\u00f3n de la funci\u00f3n de activaci\u00f3n se corresponde con el exponencial del valor obtenido de la funci\u00f3n de entrada para esta neurona de salida entre el sumatorio del exponencial del valor obtenido de la funci\u00f3n de entrada de todas las neuronas de la capa de salida. \n\nLa funci\u00f3n softmax tiene dos propiedas muy interesantes: (1) los valores de activaci\u00f3n de salida son todos positivos y (2) el conjunto de valores de salida siempre suma 1 entre todas las neuronas de salida. Estas propiedades justifican muy bien la elecci\u00f3n de la funci\u00f3n softmax para nuestro problema, ya que tenemos 6 clases y **los 6 valores de neuronas de salida se pueden interpretrar como una distribuci\u00f3n de probabilidades para identificar a que clase pertenece la instancia que se predice.**\n\nEsto quiere decir que cuando se predice una clase con un valor muy cercano a 1, esque la red est\u00e1 bastante segura que la clase correcta es esa, mientras que cuando las probabilidades est\u00e1n distribuidas entre las clases, aunque se predice la clase mayor, la red no est\u00e1 segura de esta predicci\u00f3n. ","e815490f":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.5 ptos):<\/strong> Compila y ejecuta el modelo con:\n <ul>    \n    <li>Optimizador Adam<\/li>\n     <li>Funci\u00f3n de p\u00e9rdida de la media del error cuadr\u00e1tico (<i>Mean Squared Error<\/i>)<\/li>\n    <li>Velocidad de aprendizaje: 0.001<\/li>\n    <li>Tama\u00f1o del <i>batch<\/i>: 32<\/li>\n    <li>\u00c9pocas: 100<\/li>\n<\/ul>\n    \nIncluye la t\u00e9cnica de <i>EarlyStopping<\/i> para monitorizar la p\u00e9rdida durante la validaci\u00f3n cada 5 \u00e9pocas, y de modo que el modelo recupere los pesos de la \u00e9poca con mejor valor de esta p\u00e9rdida\n<\/div>","c5d331a0":"**Respuesta:** Si estudiamos las dos gr\u00e1ficas obtenidas, podemos ver que los diferentes modelos con diferentes optimizadores consiguen llegar a un 100% de accuracy, solo que var\u00eda el learning rate con el que lo consiguen. El optimizador Adam comienza con un accuracy de 0.9998 en un learning rate de 0.001 y 0.01 pero al aumentar, este accuracy disminuye, teniendo menos de 0.5 en un learning rate de 0.1. Los otros 3 optimizadores tienen una mejora de su accuracy respecto a aumentar el learning rate, el SGD y Adagrad consiguen el accuracy de 0.9998 en un learning rate de 0.01 que mantienen a learning rate mayores y el Adadelta va mejorando su accuracy hasta que con un learning rate de 0.1 consigue un 1 de accuracy. Vemos que destaca el SGD porque incluso con su peor learning rate (0.001) tiene un accuracy de 0.9927. \nAl estudiar el tiempo de ejecuci\u00f3n en funci\u00f3n del learning rate para cada optimizador no parece existir ning\u00fan patron, simplemente vemos que hay optimizadores m\u00e1s r\u00e1pidos y optimizadores m\u00e1s lentos, siendo el SGD el optimizador m\u00e1s r\u00e1pido. \n\nSabiendo todo esto, lo l\u00f3gico es escoger un modelo entrenado con SGD, ya que es el optimizador con mejor accuracy de manera general, y adem\u00e1s el que m\u00e1s r\u00e1pido ejecuta. En concreto, elijo el modelo de SGD con learning rate de 0.1 porque es el que menos tiempo tarda de todos aquellos que tienen un accuracy de 0.9998, siendo el m\u00e1s eficiente. ","0211c0b1":"**Explicaci\u00f3n:** El decoder tambi\u00e9n lo defino de manera separada y lo construyo, de esta manera, tiene una entrada de 30 neuronas, que recibir\u00e1 la salida del encoder. Adem\u00e1s, podemos ver como las capas ocultas van aumentando en n\u00famero de neuronas correspondiente a las capas de neuronas del encoder. ","b34aa341":"### 2.2 Entrenamiento modelo","36e01ebb":"### 3.2 Definici\u00f3n del modelo","29468999":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.2 ptos):<\/strong> Indica qu\u00e9 funci\u00f3n de p\u00e9rdida ser\u00e1 la id\u00f3nea y justifica brevemente la elecci\u00f3n.\n<\/div>","3b250d01":"**Explicaci\u00f3n:** Vemos como el loss del autoencoder ha ido disminuyendo poco a poco, hasta que se estabiliza un poco por debajo de 0.05 para ambos conjuntos, en la epoch 13 el modelo tiene la loss m\u00e1s baja para el conjunto de validaci\u00f3n, y a partir de aqu\u00ed la loss nunca es inferior a este valor. Por tanto, cuando se llega a la epoch 18 y no se obtiene una loss menor, el entrenamiento para y se selecciona el modelo de la epoch 13, que obtuvo la loss inferior hasta ese momento, todo esto gracias al EarlyStopping. \n\nAdem\u00e1s, vemos como a partir de la epoch 13, la loss del conjunto de validaci\u00f3n comienza a separarse y diferenciarse de la loss del conjunto de entrenamiento, por lo que el EarlyStopping nos est\u00e1 permitiendo evitar claramente el sobreentrenamiento, no solo escoger el mejor modelo hasta ahora. ","b822f59f":"<div style=\"width: 100%; clear: both;\">\n<div style=\"float: left; width: 50%;\">\n<img src=\"http:\/\/www.uoc.edu\/portal\/_resources\/common\/imatges\/marca_UOC\/UOC_Masterbrand.jpg\", align=\"left\">\n<\/div>\n<div style=\"float: right; width: 50%;\">\n<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.875 \u00b7 Deep Learning \u00b7 PEC1<\/p>\n<p style=\"margin: 0; text-align:right;\">2020-2 \u00b7 M\u00e1ster universitario en Ciencia de datos (Data science)<\/p>\n<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Inform\u00e1tica, Multimedia y Telecomunicaci\u00f3n<\/p>\n<\/div>\n<\/div>\n<div style=\"width:100%;\">&nbsp;<\/div>\n\n\n# PEC 1: Redes neuronales completamente conectadas\n\nEn esta pr\u00e1ctica implementaremos dos tipos de red neuronal:\n\n   - Red completamente conectada para un problema de clasificaci\u00f3n\n   - Autoencoder para la detecci\u00f3n de anomal\u00edas   \n\n\n**Importante: La entrega debe hacerse en formato notebook y en formato html donde se vea el c\u00f3digo y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el men\u00fa File $\\to$ Download as $\\to$ HTML.**","b8b5909c":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.75 ptos):<\/strong> Implementa una funci\u00f3n para entrenar los modelos indicados, entrena los modelos, y representa una gr\u00e1fica con el n\u00famero de capas ocultas por modelo en el eje de las <i>x<\/i>, y el tiempo de entrenamiento en el eje de las <i>y<\/i>. \n\nComenta brevemente el resultado obtenido.\n    \n-----------------------------------------------------------------------------------------------------------\n<b>Importante<\/b>: para la entrega de la PEC, fijar el par\u00e1metro <code>verbose = 0<\/code> en el entrenamiento de los modelos. La soluci\u00f3n \u00fanicamente debe mostrar las gr\u00e1ficas resultantes.\n\n<b>Nota<\/b>: este ejercicio puede tardar unas horas en ejecutarse\n\n<\/div>","9714cd7a":"### 3.5 Predicci\u00f3n de anomal\u00edas","19cb1f4d":"**Explicaci\u00f3n:** Para las labels vemos como al binarizar, se ha generado una columna para cada categor\u00eda y cada fila solo tiene un 1 entre todas las columnas, lo que indica que la fila pertenece a esa categor\u00eda. Al comparar el nombre de etiquetas con la columna binarizada, vemos como el ataque *syn* se corresponde con la columna cuarta, o el ataque *ack* con la columna 1, por lo que las filas 2 y 3 tienen un 1 en la primera columnas. ","68e120ee":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n    <strong>Ejercicio (0.2 ptos):<\/strong> Compila el modelo con la funci\u00f3n de p\u00e9rdida elegida, una funci\u00f3n de optimizaci\u00f3n Adam, y con una velocidad de aprendizaje de 0.01, y m\u00e9trica <i>accuracy<\/i>\n<\/div>","04981143":"**Explicaci\u00f3n:** Al ver la gr\u00e1fica con el eje y reducido, podemos ver que no solo hay un poco menos de 100 instancias con un mse de entre 1 y 2, sino que hay tambien varias con mse superiores, incluso hay alguna de m\u00e1s de 45 de mse. Estas instancias son aquellas que el autoencoder no puede codificar y decodificar correctamente, por lo que su decodificaci\u00f3n es bastante diferente que los valores originales. ","83938bd9":"**Explicaci\u00f3n:** Defino el encoder por s\u00ed solo, con una capa de entrada y como se indica con el resto de capas ocultas con funci\u00f3n de activaci\u00f3n tanh. La representaci\u00f3n interna del autoencoder es la salida del encoder, que tiene una salida de 30 neuronas y su output ser\u00e1 la entrada del decoder. De esta manera, una vez entrenado el autoencoder, podemos separarlo para otras funcionalidades de todo el autoencoder. ","ea4b65d2":"Primeramente analizaremos qu\u00e9 ocurre cuando modificamos la profundidad de la red neuronal. Para ello usaremos los mismos hiperpar\u00e1metros que en el modelo anterior, y compararemos ese modelo con tres arquitecturas m\u00e1s de distinto n\u00famero de capas ocultas:\n\n   - 2 capas ocultas de 10 y 8 neuronas\n   - 3 capas ocultas de 32, 10 y 8 neuronas\n   - 4 capas ocultas de 64, 32, 10 y 8 neuronas [modelo del apartado anterior]\n   - 5 capas ocultas de 96, 64, 32, 10 y 8 neuronas","ddd2ad86":"## 0. Contexto y carga de librer\u00edas","8d873f6c":"Las redes neuronales artificiales, como el resto de modelos de aprendizaje autom\u00e1tico, requieren que los datos de entrada est\u00e9n en un formato espec\u00edfico para poder trabajar de forma correcta. Concretamente estos deben estar en formato num\u00e9rico y se recomienda que est\u00e9n estandarizados. Adem\u00e1s, en caso de haber m\u00e1s de dos clases ser\u00e1 recomendable tambi\u00e9n binarizarlas.","1ab8ad28":"**Respuesta:** Los variational autoencoder siguen la misma estructura de la red que un autoencoder normal, salvo en la generaci\u00f3n de la representaci\u00f3n de codificaci\u00f3n. Un autoencoder genera una \u00fanica codificaci\u00f3n para una instancia, mientras que **la diferencia principal de un variational autoencoder es que genera una codificaci\u00f3n media y una desviaci\u00f3n estandar. De esta manera, la codificaci\u00f3n se genera a partir de la distribuci\u00f3n gaussiana con la media y desviaci\u00f3n estandar obtenidas. La decodificaci\u00f3n parte de esta codificaci\u00f3n generada.** \n\nEsta diferencia principal de funcionamiento y estructura de un Variational Autoencoder, hace que presente otras diferencias respecto a un Autoencoder normal (1) que es un modelo probabil\u00edstico, siendo su salida estoc\u00e1stica siempre, ya que a partir de una misma instancia de entrenamiento se generar\u00e1n vectores de codificaci\u00f3n diferentes (cada vez que se entrena con esa instancia) que el decoder tendr\u00e1 que interpretar correctamente y (2) que es un modelo generativo, ya que puede generar nuevas instancias de datos similares a los datos de entrenamiento. \n\nEn esta pr\u00e1ctica nos vendr\u00eda muy bien la naturaleza estoc\u00e1stica, ya que es posible que esto facilitara la comprensi\u00f3n general de las instancias benignas porque la desviaci\u00f3n estandar aplicada a la media har\u00eda que el decoder tenga que aprender muy bien la decodificaci\u00f3n para TODAS las instancias benignas, evitando que algunas de estas instancias benignas se reconstruyan mal y al generar un umbral este sea grande e incluya instancias malignas. Tambi\u00e9n es muy beneficiosa la naturaleza de que es un modelo generativo, ya que permite que generemos nuevas instancias benignas para los conjuntos de datos de entrenamiento, validaci\u00f3n y test, aumentando as\u00ed las instancias benignas pudiendo igualarlas en n\u00famero con las instancias malignas y entrenar los modelos con mayor n\u00famero, haciendo que se comprenda mejor la codificaci\u00f3n de una instancia benigna. ","8347c79f":"**Explicaci\u00f3n:** Vemos que todos los conjuntos de datos tienen el mismo n\u00famero, por lo que se ha hecho bien la partici\u00f3n de los datos para el modelo de autoencoder. El train tiene una instancia menos porque hab\u00eda un n\u00famero de instancias no divisible por 5, pero no influye en nada. ","34d2f718":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>An\u00e1lisis (0.5 ptos):<\/strong> Justifica, de todas las configuraciones analizadas, qu\u00e9 modelo y con qu\u00e9 hiper-par\u00e1metros es m\u00e1s eficiente la clasificaci\u00f3n de ataques de botnet. Muestra gr\u00e1ficamente la evoluci\u00f3n del <i>accuracy<\/i> y de la p\u00e9rdida de los conjuntos de validaci\u00f3n y test, y la matriz de confusi\u00f3n para el modelo final elegido y comenta los resultados.\n<\/div>","6fce34c4":"Una vez entrenado el modelo, podemos obtener la predicci\u00f3n para los datos de validaci\u00f3n y para los datos de test separadamente, y calcular el error de reconstrucci\u00f3n en cada caso. El error en la validaci\u00f3n ser\u00e1 el que nos permita definir m\u00e1s adelante el umbral a partir del cual consideramos que los datos son an\u00f3malos o maliciosos.","41eb5f22":"**Explicaci\u00f3n:** Al ver la gr\u00e1fica de tiempo de entrenamiento respecto al n\u00famero de capas ocultas podemos concluir claramente que el n\u00famero de capas influye en aumentar el tiempo de entrenamiento, y vemos como el modelo con menos capas (2) es el modelo con menor tiempo de entrenamiento. Por tanto, el modelo que seleccionamos para el siguiente ejercicio es el que tiene 2 capas ocultas con 10 y 8 neuronas. ","290bb5f1":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.5 ptos):<\/strong> Calcula el valor del umbral y la predicci\u00f3n para la N seleccionada y determina el <i>accuracy<\/i>, el <i>recall<\/i> y la precisi\u00f3n (rendimiento relacionado con las tasas de verdaderos positivos y negativos) del clasificador cuando el error de reconstrucci\u00f3n del conjunto de test es superior al umbral calculado.\n    \nGenera la matriz de confusi\u00f3n correspondiente.\n\n-----------------------------------------------------------------------------------------------------------\n<i>Nota<\/i>: se puede hacer uso de las funciones proporcionadas en la librer\u00eda <code>sklearn.metrics<\/code>\n<\/div>","46dbc5cd":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>An\u00e1lisis (0.25 ptos):<\/strong> Comenta los resultados obtenidos. \u00bfEs mejorable el modelo? Si es el caso, \u00bfc\u00f3mo se podr\u00eda mejorar, o qu\u00e9 par\u00e1metros podr\u00edan influir m\u00e1s?\n<\/div>","26c071d1":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Calcula la predicci\u00f3n del modelo autoencoder para el conjunto de validaci\u00f3n y calcula el error de reconstrucci\u00f3n cometido durante la validaci\u00f3n del modelo.\n    \n Representa en un histograma el error cometido para los datos del conjunto de validaci\u00f3n.\n<\/div>","22d3b9cd":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.4 ptos):<\/strong> Entrena el modelo con 20 \u00e9pocas y representa dos gr\u00e1ficas:\n   <ul>\n       <li> <i>Accuracy<\/i> del conjunto de train y del de test<\/li>\n       <li> P\u00e9rdida (<i>loss<\/i>) del conjunto de train y del de test<\/li>\n    <\/ul>\n<\/div>","7f47eb90":"**Si solo nos interesa identificar el mayor n\u00famero de anomal\u00edas mejor, independientemente de cuanto nos suponga fallar ante ataques benignos, nos tenemos que fijar en la m\u00e9trica recall y el mejor modelo es el modelo 1. El recall de este modelo indica que de todos los ataques malignos, se ha identificado correctamente un 85.20%. Este valor se produce  debido al valor del umbral, si consiguieramos un modelo mejor codificando las instancias benignas, el umbral ser\u00eda inferior y podr\u00edamos detectar m\u00e1s ataques malignos. Por esto mismo los otros modelos son peores identificando las anomal\u00edas, porque su umbral es mayor, lo que hace que su recall sea inferior.** Es importante tambi\u00e9n comentar del modelo que su FPR es el mayor porque al tener el umbral m\u00e1s bajo, hay un mayor n\u00famero de instancias benignas que se clasifican como malignas. \n\nTambi\u00e9n vemos como el modelo 1 es el que peor precisi\u00f3n y especificidad tiene, porque se predicen m\u00e1s ataques malignos (gracias a tener un menor umbral) para asegurar que se encuentran la mayor\u00eda de malignos posible aumentando as\u00ed el n\u00famero de falsos positivos. Es importante destacar que el accuracy en test es el mejor, pero el peor en validation, aunque en test el acierto empeora mucho m\u00e1s al aumentar el umbral de lo que mejora en validation. \n\nSi nos interesara detectar informaci\u00f3n maligna, pero disminuyendo el n\u00famero de falsos positivos en los ataques benignos, tendr\u00edamos que elegir un modelo con un umbral mayor pero que todav\u00eda mantenga un buen valor de recall. Por ejemplo, elegir el modelo 2, har\u00eda que se falle un poco m\u00e1s en la detecci\u00f3n de malignos, teniendo un recall del 74.64%, pero a cambio se reduce la tasa de falsos positivos en un 0.23% y por tanto hay muchos m\u00e1s ataques benignos identificados correctamente sin ser consideraros malignos. Esto se aprecia tambi\u00e9n al ver que aumentan la especificidad y la precision, adem\u00e1s del acierto de validaci\u00f3n, que solo contiene ataques benignos. \n\n**A\u00fan as\u00ed la disminuci\u00f3n en recall y en accuracy que tenemos al elegir el modelo 2 es muy grande para lo poco que se mejora en detectar benignos, esto sumado a que el objetivo es detectar la mayor\u00eda de malignos posible, es indiscutible que se tiene que elegir aquel modelo con menor umbral y mayor recall que es el modelo 1.** ","451a86b1":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Calcula la predicci\u00f3n del modelo autoencoder para el conjunto de test y calcula el error de reconstrucci\u00f3n cometido durante la fase de test del modelo.\n    \nRepresenta en un histograma el error cometido para los datos del conjunto de test.\n<\/div>","1d9b8069":"**Explicaci\u00f3n:** Vemos como la variable MI_dir_L5_weight se ha estandarizado correctamente y se encuentra centrada en torno al valor 0 y los valores extremos est\u00e1n menos separados en rango. Por lo que la estandarizaci\u00f3n ha sido efectiva en esta variable.\n\nRespecto al rango de todas las variables, vemos que se reducido mucho el n\u00famero de variables con un rango enorme, y ahora incluso podemos visualizar la mediana y el IQR entre 5 y 15 aproximadamente. Por tanto, la estandarizaci\u00f3n ha acercado el rango de todas las variables, que ahora facilitar\u00e1n el mejor entrenamiento de la red neuronal y el mejor rendimiento. ","0f514858":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>An\u00e1lisis (0.5 ptos):<\/strong> Seg\u00fan los resultados obtenidos en la tabla anterior, \u00bfqu\u00e9 valor de N es el m\u00e1s adecuado para la detecci\u00f3n de informaci\u00f3n maligna (anomal\u00edas)? Justifica la elecci\u00f3n.\n<\/div>","bea78cd8":"### 2.3 An\u00e1lisis de hiperpar\u00e1metros","b3dbcd82":"**Explicaci\u00f3n:** Al ver la gr\u00e1fica del error de reconstrucci\u00f3n para el conjunto de validaci\u00f3n, vemos como la grand\u00edsima mayor\u00eda de instancias tienen un mse de entre 0 y 1, lo cual es bueno porque significa que el autoencoder es capaz de codificarlas y decodificarlas pareciendose mucho a la instancia benigna original. Tambieen hay algunas pocas por encima del 1, por tanto acercamos el gr\u00e1fico en el eje y para que sea posible apreciar mejor las instancias de m\u00e1s de 0 de mse que hay. ","02c92e1a":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (1 pto):<\/strong> Implementa una funci\u00f3n para entrenar la arquitectura elegida del ejercicio anterior para 4 optimizadores diferentes: Adam, SGD, Adadelta, Adagrad, y 4 velocidades de aprendizaje: 0.001, 0.01, 0.05, 0.1. \nEntrena los modelos, y representa dos gr\u00e1ficas con la velocidad de aprendizaje en el eje de las <i>x<\/i>, y en el eje de las <i>y<\/i>:\n <ul>\n     <li> el Tiempo de entrenamiento<\/li>\n     <li> el <i>Accuracy<\/i><\/li>\n<\/ul>   \n    \nCada gr\u00e1fica debe incluir la evoluci\u00f3n obtenida por los cuatro optimizadores para poderlos comparar mejor.\n    \n-----------------------------------------------------------------------------------------------------------\n<b>Importante<\/b>: para la entrega de la PEC, fijar el par\u00e1metro <code>verbose = 0<\/code> en el entrenamiento de los modelos. La soluci\u00f3n \u00fanicamente debe mostrar las gr\u00e1ficas resultantes.\n    \n<b>Nota<\/b>: este ejercicio puede tardar unas horas en ejecutarse\n\n<\/div>","9b6a115c":"En el curso hemos visto que hay distintas variantes de los Autoencoders. Un tipo concreto son los llamados Variational Autoencoders (VAE).","dadb1c38":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Define el segundo bloque del Autoencoder, el decodificador (o <i>decoder<\/i>), con las capas correspondientes y funci\u00f3n de activaci\u00f3n <code>tanh<\/code>, y sin ninguna funci\u00f3n de activaci\u00f3n espec\u00edfica a la salida.\n<\/div>","53da259d":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n    <strong>An\u00e1lisis (0.2 ptos):<\/strong> Representa la matriz de confusi\u00f3n y comenta los resultados del entrenamiento anterior y de la clasificaci\u00f3n obtenida\n    \n-----------------------------------------------------------------------------------------------------------\n<b>Nota<\/b>: se pueden usar funciones de la librer\u00eda <code>sklearn.metrics<\/code>\n\n<\/div>","f237eec5":"Los Autoencoders son un tipo de red neuronal completamente conectada que tiene por objetivo reproducir los datos de entrada. Normalmente se divide en dos componentes: el codificador (*encoder*), el cual mapea el espacio de entrada a un espacio de menor dimensi\u00f3n (tambi\u00e9n llamado en ingl\u00e9s *latent space*), y el decodificador (*decoder*), el cual mapea desde este espacio m\u00e1s peque\u00f1o a uno igual al espacio de entrada (tambi\u00e9n llamado en ingl\u00e9s *reconstruction space*). \n\nAunque en general los Autoencoders se utilizan mucho con im\u00e1genes (*denoising*, aumento de datos, etc), son tambi\u00e9n un m\u00e9todo com\u00fan para la detecci\u00f3n de anomal\u00edas en un conjunto de datos: el modelo aprende a reconstruir los datos de entrada, por lo que podemos hacer que aprenda a reconstruir \u00fanicamente datos 'buenos' o normales. El modelo nos proporcionar\u00e1 un \"error de reconstrucci\u00f3n\", basado en la media de la diferencia quadr\u00e0tica entre los datos de entrada (X) y la reconstrucci\u00f3n hecha o predicci\u00f3n (X'). \n\n$$\\textrm{mse}(X, X') = \\frac{1}{n} \\sum^{n}_{1=1} (X_i - X'_i)^2$$\n\nCon este error, podremos asignar un umbral (*threshold*) a partir del cual el modelo considerar\u00e1 que la reconstrucci\u00f3n es buena o no. As\u00ed, usando los datos de test, si el error de reconstrucci\u00f3n es superior al umbral fijado, se considerar\u00e1 ese dato como an\u00f3malo o malicioso (i.e. un ataque de botnet).\n\nPara ello es necesario que tanto los datos de entrenamiento como los de validaci\u00f3n incluyan \u00fanicamente datos 'buenos' (clase benigna) y que la proporci\u00f3n entre ambos conjuntos sea del 50%. El conjunto de test, por otro lado, deber\u00e1 incluir una parte del conjunto de datos 'buenos', y una parte equivalente en tama\u00f1o de datos 'an\u00f3malos' (clase maligna). Como se puede observar, s\u00f3lo hablamos de dos clases, por lo que todas las clases de ataques de botnet de nuestro conjunto deber\u00e1n unificarse en una \u00fanica clase.","0f721e28":"### 3.1 Preparaci\u00f3n conjunto de datos","4f2e859f":"## 2. Clasificaci\u00f3n (3.75 puntos)","755a469c":"**Explicaci\u00f3n:** Vemos que el n\u00famero de filas son 440617 y que hay 116 columnas, siendo la clase la \u00faltima de clase. Adem\u00e1s vemos que hay 6 clases siendo la clase con m\u00e1s instancias la clase benigna pero que apenas tiene una proporci\u00f3n del 22%. Aunque hay desbalanceo de clases, este no es muy grave puesto que que la clase con menos instancias tiene un 12% de proporci\u00f3n, y por tanto no tenemos que preocuparnos mucho sobre un posible overfitting derivado de que la clases est\u00e9n desbalanceadas. \n\nAdem\u00e1s vemos el histograma de la primera variable, vemos que su distribuci\u00f3n no es nada normalizada y que su valor m\u00ednimo est\u00e1 en 0 y tiene muchos valores bajos, mientras que su valor m\u00e1ximo est\u00e1 pasado los 350 y hay pocos valores por encima del valor 150. Aplicaremos la estandarizaci\u00f3n para que la variable se reduzca en su rango y los valores se centren en torno al valor 0, pero que se mantengan los valores extremos aunque con un rango menor en funci\u00f3n de la desviaci\u00f3n estandar.\n\nViendo la distribuci\u00f3n del rango de las variables, vemos que hay dos extremos, hay much\u00edsimas variables con un rango peque\u00f1o, pero tambi\u00e9n hay muchas con un rango extremo del rango de 6e-17. De hecho, hay tantas variables con un rango peque\u00f1o que la mediana y el IQR no se aprecian en la gr\u00e1fica. Es importante que el rango de las variables se mantenga similar entre todas para que se mejore el entrenamiento y rendimiento del modelo sin que los atributos con rango enorme afecten al entrenamiento. ","e7907dec":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio:<\/strong> Estandardizar y binarizar debidamente los datos. \n<\/div>","0b8c7ba3":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.3 ptos):<\/strong> Implementa la arquitectura para el modelo de clasificaci\u00f3n con la funci\u00f3n de activaci\u00f3n elegida para la capa de salida y con las siguientes caracter\u00edsticas:\n    \n   - 4 capas ocultas con 64,32,10,8 neuronas sucesivamente\n   - Funci\u00f3n de activaci\u00f3n de las capas intermedias: ReLU\n<\/div>","0694ea85":"**Explicaci\u00f3n:** Vemos como el unificar todos los ataques en una \u00fanica clase, hace que tengamos una gran mayor\u00eda de ataques malignos y pocos benignos. ","c89818aa":"Mientras que un VAE, como ya se ha comentado, est\u00e1 compuesto de un encoder y un decoder que trabajan en conjunto para codificar y decodificar las instancias de datos, teniendo en cuenta que el vector de codificaci\u00f3n parte de unas medias y desviaciones est\u00e1ndar. En una GAN una red generativa y una red discriminativa compiten entre ellas en un juego de suma 0, donde las ganancias de una red, suponen las perdidas de la otra. La red generativa tiene como objetivo generar instancias de los datos que confundan a la red discriminativa a pensar que esa instancia pertenece al conjunto real, partiendo de un vector latente que suele seguir una distribuci\u00f3n normal, y la red discriminativa se encarga de predecir si los datos que recibe son reales o generados por la red generativa. \n\nAs\u00ed, una GAN genera una red generativa que es capaz de enga\u00f1ar a un discriminador, mientras que el VAE genera un encoder generador que es capaz de ser interpretado por un decoder. Pero ambas generan nuevos datos. ","a3ca7f6a":"Cada clase cuenta con informaci\u00f3n estad\u00edstica como el peso, n\u00famero de paquetes enviados, tama\u00f1o medio del paquete, varianza del tama\u00f1o del paquete, desviaciones est\u00e1ndar, etc. Estas estad\u00edsticas se muestran agregadas por varios aspectos: MI (*Source MAC-IP*), H (*Source Host IP*) HH (*Source and destination host IP*), HpHp (*Source and destination host and port*), HH_jit (*Source and destination host traffic jitter*). Todas ellas calculadas para distintos tama\u00f1os de ventana: 100ms, 500ms, 1.5s, 10s.","384d6fe0":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.25 ptos):<\/strong> Representa gr\u00e1ficamente la p\u00e9rdida durante el entrenamiento y la validaci\u00f3n. Comenta el resultado.\n<\/div>","d93b746b":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (1 pto):<\/strong> Calcula el umbral para valores de N del 1 al 10, y muestra en una \u00fanica tabla los siguientes par\u00e1metros:\n<ul>\n    <li> N<\/li>\n    <li> umbral de anomal\u00eda (o <i>threshold<\/i>)<\/li>\n    <li> <i>accuracy<\/i> de la validaci\u00f3n<\/li>\n    <li> <i>accuracy<\/i> del test<\/li>\n    <li> precisi\u00f3n<\/li>\n    <li> recall<\/li>\n    <li> especificidad<\/li>\n    <li> tasa de falsos positivos (FPR)<\/li>\n<\/ul>\n    \n-----------------------------------------------------------------------------------------------------------\n<i>Nota<\/i>: se puede hacer uso de las funciones proporcionadas en la librer\u00eda <code>sklearn.metrics<\/code>\n<\/div>","cfcd3706":"Tal y como hemos visto en la teor\u00eda, existen diversas t\u00e9cnicas de optimizaci\u00f3n para mejorar el rendimiento de una red neuronal, acelerar el aprendizaje o evitar\/reducir el sobreajuste. En este apartado analizaremos el efecto que causa, en t\u00e9rminos de tiempo de entrenamiento y de *accuracy*, un cambio en la arquitectura de la red, en la velocidad de aprendizaje y en el optimizador.","b8d50067":"Para detectar las anomal\u00edas necesitamos fijar el umbral a partir del cual consideramos que el dato es una anomal\u00eda. El umbral (o *threshold*) se define generalmente como una desviaci\u00f3n est\u00e1ndar por encima de la mediana del error de reconstrucci\u00f3n. Generalizando:\n\n$$\\textrm{threshold} = \\overline{\\textrm{mse}(X_{\\textrm{val}} - X'_{\\textrm{val}})} + N * \\sigma_{\\textrm{mse}(X_{\\textrm{val}})}$$\n\nDonde N es un valor entero que se puede modificar para ajustar mejor el umbral seg\u00fan el objetivo del problema.","ca8b7440":"### 3.4 Error de reconstrucci\u00f3n","ce398c04":"En esta pr\u00e1ctica trataremos de clasificar y detectar distintos tipos de ataques a una c\u00e1mara de seguridad Provision PT-838. El conjunto de datos proporcionado (adaptado [UCI Machine Learning Repository][1]) contiene informaci\u00f3n de datos benignos y de cinco tipos de ataque de la familia de botnets *Mirai*:\n\n   - *scan*: escaneo autom\u00e1tico de dispositivos vulnerables\n   - *Ack flooding*: ataque de denegaci\u00f3n de servicio (DDoS) con el fin de saturar el sistema de destino con datos maliciosos\n   - *Syn flooding*: similar y ligado al *Ack*\n   - *UDP flooding*: ataque de denegaci\u00f3n de servicio (DDoS) con el fin de saturar el ancho de banda en la red\n   - *UDPplain flooding*: similar al UDP pero con menos opciones, optimizado para mayor PPS\n\n[1]: http:\/\/archive.ics.uci.edu\/ml\/index.php","d4894514":"### 2.1 Definici\u00f3n modelo","3453fb54":"El modelo estar\u00e1 formado por el codificador y el decodificador. Dado que se trata de una reconstrucci\u00f3n, las capas ocultas del codificador tendr\u00e1n n\u00famero decreciente de neuronas hasta la dimensi\u00f3n m\u00ednima, y el codificador reconstruir\u00e1 de nuevo esas capas en valor ascendente. En el modelo a definir en este apartado, el autoencoder utilizar\u00e1 internamente 7 capas ocultas con 90, 60, 40, 30, 40, 60, 90 neuronas, adem\u00e1s de las capas de entrada y salida.","5a6d572e":"<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n<strong>Ejercicio (0.2 ptos):<\/strong> Indica qu\u00e9 funci\u00f3n de activaci\u00f3n ser\u00e1 la id\u00f3nea para la capa de salida y justifica brevemente la elecci\u00f3n\n<\/div>"}}