{"cell_type":{"de1037b0":"code","6006053e":"code","41d74247":"code","d1c3e881":"code","8ba16d7c":"code","4c89c323":"code","a6769c51":"code","09148ddd":"code","18adb859":"code","329efc0a":"code","5e21a895":"code","40e32848":"code","568d970d":"code","23e59389":"code","a05fea4f":"code","bcf1b27c":"code","30f23a34":"code","7a06c593":"code","d8c61798":"code","ed4dd610":"code","343520ca":"code","e6ef9a33":"code","7d93fe43":"code","d8535634":"code","ecd22a45":"code","877d4343":"code","4ad2d1e0":"code","540ba45f":"code","0edbcd01":"code","9af5537b":"code","17b25c3b":"code","0633426e":"code","a652689a":"code","11e7b44a":"code","3c83c90c":"code","a2fe7459":"code","a5b52e09":"code","61b6305f":"code","7553c46b":"code","f3312a01":"code","2788dba4":"code","fcd1b1e5":"code","8d5a27db":"code","771ae716":"code","5ebd7574":"code","00e0bed1":"code","7ce3d8e6":"code","35767c13":"code","5dd5874a":"code","4aaeec3d":"code","ed33dabe":"code","d79e81b0":"markdown","88b53519":"markdown","88314614":"markdown","9ed13e2c":"markdown","d998ad67":"markdown","33fc0b8f":"markdown","26e72039":"markdown","160dff86":"markdown","6b040d47":"markdown","58635a22":"markdown","75f21d32":"markdown","5cfca90f":"markdown","78378b31":"markdown","9825598d":"markdown","201a14ea":"markdown","fccda825":"markdown","5b868b4d":"markdown","9216b176":"markdown","d2666644":"markdown","ed90d082":"markdown","15ebfe11":"markdown","221c77a8":"markdown","35c4b0bf":"markdown","1d05a73b":"markdown","ea6308a1":"markdown","442031d5":"markdown","32d08485":"markdown","daa1c775":"markdown","3907a5c7":"markdown","b2f35932":"markdown","a3b8d2bb":"markdown","3898db0e":"markdown"},"source":{"de1037b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pylab as pylab\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': '18',\n         'axes.titlesize':'24',\n         'xtick.labelsize':'12',\n         'ytick.labelsize':'12'}\npylab.rcParams.update(params)\n\n# Models from Scikit-Learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Model evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/customer-personality-analysis'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6006053e":"data = pd.read_csv(os.path.join(dirname, filename), sep='\\t', parse_dates=['Dt_Customer'])","41d74247":"data.info()","d1c3e881":"df_copy = data.copy()","8ba16d7c":"def campaign_acceptance(row):\n    toReturn = row\n    campaigns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n    toReturn['accepted'] = 0\n    if row[campaigns].any() == 1:\n        toReturn['accepted'] = 1\n    \n    return pd.Series(toReturn, index=list(toReturn.keys()))","4c89c323":"# There are too much redundant value for Education\ndf_copy['Education'].replace({'Basic': 'Undergraduate', '2n Cycle': 'Undergraduate'}, inplace=True)\n\n# The same for Marital Status\ndf_copy['Marital_Status'].replace({'Absurd': 'Single', 'YOLO': 'Single',\n                                    'Alone': 'Single', 'Widow': 'Single', 'Together': 'Couple'}, inplace=True)\n\n# Let's see the customers fidelity years and their age\ndf_copy['fidelity_years'] = df_copy['Dt_Customer'].dt.year - df_copy['Year_Birth']\ndf_copy['age'] = datetime.now().year - df_copy['Year_Birth']\n\n# sum up the kid and teen as children\ndf_copy['children'] = df_copy['Kidhome'] + df_copy['Teenhome']\ndf_copy['has_child'] = df_copy['children'] > 0\n\n# Calculate total amount of expenses in different kind of products\ndf_copy['total_spend'] = df_copy['MntWines'] + df_copy['MntFruits'] + df_copy['MntMeatProducts'] +\\\n                        df_copy['MntFishProducts'] + df_copy['MntSweetProducts'] + df_copy['MntGoldProds']\n\n# Let's aggregate the acceptance of compaigns in one feature\ndf_copy = df_copy.apply(campaign_acceptance, axis=1)\n\n# Let's convert the strings in categories\nfor label, content in df_copy.items():\n    if pd.api.types.is_string_dtype(content):\n        df_copy[label] = content.astype('category').cat.as_ordered()\n        \n# Remove the NaN values from Income\ndf_copy['Income'].dropna(inplace=True)","a6769c51":"df_copy = df_copy.drop(['Z_Revenue', 'Z_CostContact'], axis=True)","09148ddd":"df_copy.columns","18adb859":"df_copy['accepted'].value_counts()","329efc0a":"fig, (ax1, ax2) = plt.subplots(figsize=(20, 10), ncols=2)\ngrouped = df_copy.groupby('Education').count().reset_index()\n\nsns.countplot(ax=ax1, x=\"Education\", data=df_copy, palette= sns.color_palette(\"hls\", 4))\n\nsns.countplot(ax=ax2, x='Marital_Status', data=df_copy, palette= sns.color_palette(\"hls\", 4))\n\nax2.set_xlabel('Marital Status')\nax2.set_title('Civil Status of Customers')\nax1.set_title('Education status of customers')\nax1.set_ylabel('Count');\nax2.set_ylabel('Count');","5e21a895":"has_child_perc = df_copy[df_copy['has_child'] == True].shape[0] \/ df_copy.shape[0]\nhas_nochild_perc = df_copy[df_copy['has_child'] == False].shape[0] \/ df_copy.shape[0]\nfig, ax = plt.subplots(figsize=(10, 10))\n# Anche qui vedere la percentuale di quanti hanno i figli\n\nsns.barplot(x=['Has Child', 'No Child'], y= [has_child_perc, has_nochild_perc], \n            palette = sns.color_palette('hls', 2));\nax.set_ylabel('Percentage of customers')\nax.set_title(f'The {round(has_child_perc, 2) * 100}% of customers has children');","40e32848":"# Complain in the last 2 years\ncomplain = df_copy[df_copy['Complain'] == 1]\n\nfig, (ax1, ax2) = plt.subplots(figsize=(20, 10), ncols=2)\n\ned_compl_perc = complain.groupby('Education').count()['ID'] \/ df_copy.shape[0] * 100\nmarital_compl_perc = complain.groupby('Marital_Status').count()['ID'] \/ df_copy.shape[0] * 100\n\nsns.barplot(ax=ax1, x = ed_compl_perc.index, y = ed_compl_perc.values, palette = sns.color_palette('hls', 4))\nsns.barplot(ax=ax2, x = marital_compl_perc.index, y = marital_compl_perc.values, \n            palette = sns.color_palette('hls', 4));\n\nax2.set_xlabel('Marital Status')\nax2.set_title('Civil Status of Customers')\nax1.set_title('Education status of customers')\nax1.set_ylabel('Percentage of complain customers');","568d970d":"print(f'Percentage of complaining cutomers: {round((complain.shape[0] \/ df_copy.shape[0]) * 100, 2)}%')","23e59389":"campaigns = {\n    'Campaign': ['First', 'Second', 'Third', 'Fourth', 'Fifth', 'Last'],\n    'Acceptance Rate': [(df_copy[df_copy['AcceptedCmp1'] == 1].shape[0] \/ df_copy.shape[0]) * 100,\n                       (df_copy[df_copy['AcceptedCmp2'] == 1].shape[0] \/ df_copy.shape[0]) * 100,\n                       (df_copy[df_copy['AcceptedCmp3'] == 1].shape[0] \/ df_copy.shape[0]) * 100,\n                       (df_copy[df_copy['AcceptedCmp4'] == 1].shape[0] \/ df_copy.shape[0]) * 100,\n                       (df_copy[df_copy['AcceptedCmp5'] == 1].shape[0] \/ df_copy.shape[0]) * 100,\n                       (df_copy[df_copy['Response'] == 1].shape[0] \/ df_copy.shape[0]) * 100]\n}","a05fea4f":"fig, (ax1, ax2) = plt.subplots(figsize=(20, 10), ncols=2, sharex=True)\n\noffers = np.array([1, 2, 3, 4, 5])\nsns.barplot(ax=ax1, x = campaigns['Campaign'], y = campaigns['Acceptance Rate'], palette = sns.color_palette('hls', 6))\n#for offer in offers:\n#    sns.barplot(x = offer, y = (df_copy[df_copy['AcceptedCmp' + str(offer)] == 1].shape[0] \/ df_copy.shape[0]) * 100)\nsns.lineplot(ax=ax2, x = campaigns['Campaign'], y= campaigns['Acceptance Rate'], markers=True, dashes=False)    \n\nfig.suptitle('Campaigns Acceptance Rate', fontsize=24)\nax1.set_ylabel('Acceptance rate')\nax1.set_xlabel('Campaigns')\nax2.set_xlabel('Campaigns');","bcf1b27c":"fig, ax = plt.subplots(figsize=(7, 7))\n#define data\nslices = [df_copy[df_copy['accepted'] == 1].shape[0], \n          df_copy[df_copy['accepted'] == 0].shape[0]]\n\nlabels = ['Accepted', 'Rejected']\n\n#define Seaborn color palette to use\ncolors = sns.color_palette('pastel')[0:2]\n\n#create pie chart\nax.pie(slices, labels = labels, colors = colors, autopct='%.0f%%');\nax.set_title('Campaign Joined People');","30f23a34":"df_copy['Recency'].describe()","7a06c593":"fig, ax = plt.subplots(figsize=(10, 10))\n\nlabels = ['Meat', 'Fish', 'Sweet', 'Gold']\nvalues = [df_copy['MntMeatProducts'].mean(), df_copy['MntFishProducts'].mean(),\n         df_copy['MntSweetProducts'].mean(), df_copy['MntGoldProds'].mean()]\n\nsns.barplot(x = labels, y=values, palette = sns.color_palette('hls', 4))\n\nax.set_xlabel('Products')\nax.set_ylabel('Average Expenditure ($)')\nax.set_title('Average expenditure by products');","d8c61798":"df_copy = df_copy[df_copy['Income'] < 120000]\ndf_copy['Income'].describe()","ed4dd610":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.histplot(df_copy['Income'], stat='density');","343520ca":"df_copy['total_spend'].describe()","e6ef9a33":"fig, ax = plt.subplots(figsize=(15, 10))\nsns.histplot(df_copy['total_spend'], stat='density')\n\nax.set_xlabel('Total Expenditures($)');","7d93fe43":"ed_group = df_copy.groupby('Education').sum().reset_index()","d8535634":"def compute_percentage_purchases(row, group):\n    toReturn = row\n    toReturn['WebPerc'] = 0\n    toReturn['StorePerc'] = 0\n    \n    # Get Total web and store purchases\n    total_web_purchases = group['NumWebPurchases'].sum()\n    total_store_purchases = group['NumStorePurchases'].sum()\n    \n    if row['Education'] == 'Graduation':\n        # Divide the customer store and web purchases by the total\n        toReturn['WebPerc'] = row['NumWebPurchases'] \/ total_web_purchases\n        toReturn['StorePerc'] = row['NumStorePurchases'] \/ total_store_purchases\n    \n    if row['Education'] == 'Undergraduate':\n        toReturn['WebPerc'] = row['NumWebPurchases'] \/ total_web_purchases\n        toReturn['StorePerc'] = row['NumStorePurchases'] \/total_store_purchases\n    \n    if row['Education'] == 'PhD':\n        toReturn['WebPerc'] = row['NumWebPurchases'] \/ total_web_purchases\n        toReturn['StorePerc'] = row['NumStorePurchases'] \/ total_store_purchases\n        \n    if row['Education'] == 'Master':\n        toReturn['WebPerc'] = row['NumWebPurchases'] \/total_web_purchases\n        toReturn['StorePerc'] = row['NumStorePurchases'] \/ total_store_purchases\n    \n    return pd.Series(toReturn, index = list(toReturn.keys()))","ecd22a45":"df_copy = df_copy.apply(compute_percentage_purchases, group = ed_group, axis=1)\n#df_copy['NumStorePurchasesPerc'] = df_copy['NumStorePurchases'] \/ ed_group['NumWebPurchases']","877d4343":"# Let'se the mean of the computed value\ned_group = df_copy.groupby('Education').mean().reset_index()","4ad2d1e0":"fig, (ax1, ax2) = plt.subplots(figsize=(20, 10), ncols = 2)\n\nsns.barplot(ax=ax1, x = ed_group['Education'], y = ed_group['StorePerc'], palette = sns.color_palette('hls', 4));\nsns.barplot(ax=ax2, x = ed_group['Education'], y = ed_group['WebPerc'], palette = sns.color_palette('hls', 4));\n\nax1.set_ylabel('Purchases Percentage in Store')\nax1.set_title('Store')\nax2.set_ylabel('Purchases Percentage on Web')\nax2.set_title('Web')\nplt.suptitle('Percentage Purchases by Education level', fontsize=24);","540ba45f":"df_copy.dropna(inplace=True)","0edbcd01":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n# define the attribute to cluster ['age', 'children', 'total_spend', 'Income']\ncluster_features = ['age', 'children', 'total_spend', 'Income'] #['children', 'total_spend', 'Income', 'WebPerc', 'StorePerc']\ntoCluster = df_copy[cluster_features]\n# Normalization\nscaler = MinMaxScaler()\ntoCluster[cluster_features] = scaler.fit_transform(toCluster)\n# Elbow method to select the right number of cluster\nK = range(1,10)\ndistortions = []\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(toCluster)\n    distortions.append(kmeanModel.inertia_)","9af5537b":"plt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","17b25c3b":"clusters = KMeans(n_clusters = 4, random_state=42)","0633426e":"labels = clusters.fit_predict(toCluster)\ndf_copy['cluster'] = labels","a652689a":"df_copy['cluster'].value_counts()","11e7b44a":"df_copy.groupby('cluster').mean()","3c83c90c":"fig, ax = plt.subplots(figsize=(15, 10))\n\ndf_copy = df_copy[df_copy['Income'] < 200000]\n\n\nx_axis = 'Income'\ny_axis = 'total_spend'\nsns.scatterplot(x = df_copy[df_copy['cluster'] == 0][x_axis],\n                y = df_copy[df_copy['cluster'] == 0][y_axis], \n            palette='salmon', legend = 'auto')\n\nsns.scatterplot(x= df_copy[df_copy['cluster'] == 1][x_axis], \n                y = df_copy[df_copy['cluster'] == 1][y_axis], \n            palette='blue', legend = 'auto')\n\nsns.scatterplot(x= df_copy[df_copy['cluster'] == 2][x_axis], \n                y = df_copy[df_copy['cluster'] == 2][y_axis], \n            palette='green', legend = 'auto')\n\nsns.scatterplot(x= df_copy[df_copy['cluster'] == 3][x_axis], \n                y = df_copy[df_copy['cluster'] == 3][y_axis], \n            palette='red', legend = 'auto')\n\nlinedf = pd.DataFrame({'x': df_copy['Income'].values,'y': np.full(df_copy.shape[0], 1400)})\nplt.plot(linedf['x'], linedf['y'], color='blue', linestyle='dashed')\n#sns.lineplot(x = 'x', y = 'y', data = linedf, palette = 'salmon')\n\nlinedf2 = pd.DataFrame({'x': df_copy['Income'].values,'y': np.full(df_copy.shape[0], 800)})\nplt.plot(linedf2['x'], linedf2['y'], color='salmon', linestyle='dashed')\n#sns.lineplot(x = 'x', y = 'y', data = linedf2, palette = 'red')\n\nax.set_title(\"Income vs Total amount spent\")\nax.set_xlabel('Income')\nax.set_ylabel('Total Spent')\nax.legend(['Rich Class', 'Middle Class', '0', '1', '2', '3']);","a2fe7459":"cluster1 = df_copy[df_copy['cluster'] == 3]\ncluster1 = cluster1[cluster1['age'] < 120]\ncluster2 = df_copy[df_copy['cluster'] == 1]\ncluster2 = cluster2[cluster2['age'] < 120]\n\nfig, (ax1, ax2) = plt.subplots(figsize=(15, 20), nrows=2)\n\nsns.regplot(ax=ax1, x = cluster1['age'], y = cluster1['total_spend'])\nsns.regplot(ax=ax2, x = cluster2['age'], y = cluster2['total_spend'])\n\nfrom scipy import stats\n        \nslope1, intercept2, r_value2, pv2, se2= stats.linregress(cluster1['age'], cluster1['total_spend'])\nslope2, intercept2, r_value2, pv2, se2 = stats.linregress(cluster2['age'], cluster2['total_spend']) \nprint(slope1, slope2)","a5b52e09":"#Create Age segment\ncut_labels_Age = ['Young', 'Adult', 'Mature', 'Senior']\ncut_bins = [0, 30, 45, 65, 120]\ndf_copy['age_group'] = pd.cut(df_copy['age'], bins=cut_bins, labels=cut_labels_Age)\n\ncut_labels_Age = ['0-30', '30-45', '45-65', '65+']\ncut_bins = [0, 30, 45, 65, 120]\ndf_copy['age_range'] = pd.cut(df_copy['age'], bins=cut_bins, labels=cut_labels_Age)\n\n#Create Income segment\ncut_labels_Income = ['Low income', 'Low to medium income', 'Medium to high income', 'High income']\ndf_copy['Income_group'] = pd.qcut(df_copy['Income'], q=4, labels=cut_labels_Income)\n\n#Create Fidelity segment. The time the customer is client\ncut_labels_Seniority = ['New customers', 'Discovering customers', 'Experienced customers', 'Old customers']\ndf_copy['Fidelity_group'] = pd.qcut(df_copy['fidelity_years'], q=4, labels=cut_labels_Seniority)\n\n# Create Recency segment. Recency is some sort of purchases frequency\ncut_labels_Recency = ['High Frequency', 'Normal Frequency', 'Low Frequency', 'Occasional']\ndf_copy['Recency_group'] = pd.qcut(df_copy['Recency'], q=4, labels=cut_labels_Recency)\n#data=data.drop(columns=['Age','Income','Seniority'])","61b6305f":"fig, ax = plt.subplots(figsize=(20, 20), nrows=2, ncols=2)\nax1 = ax[0][0]\nax2 = ax[0][1]\nax3 = ax[1][0]\nax4 = ax[1][1]\n\nsns.countplot(ax = ax1, x = df_copy['age_group'], palette = sns.color_palette('hls', 4))\nax1.set_xlabel('Age Group')\n\nsns.countplot(ax = ax2, x = df_copy['Income_group'], palette = sns.color_palette('hls', 4))\nax2.set_xlabel('Income Group')\n\nsns.countplot(ax = ax3, x = df_copy['Fidelity_group'], palette = sns.color_palette('hls', 4))\nax3.set_xlabel('Fidelity Group')\n\nsns.countplot(ax = ax4, x = df_copy['Recency_group'], palette = sns.color_palette('hls', 4))\nax4.set_xlabel('Recency Group');","7553c46b":"income_age = pd.crosstab(df_copy['age_group'], df_copy['Income_group']).reset_index()\nincome_age['total'] = income_age['Low income'] + income_age['Low to medium income'] + income_age['Medium to high income'] + income_age['High income']\nincome_age['low-perc'] = round(income_age['Low income'] \/ income_age['total'], 2)\nincome_age['lowmed-perc'] = round(income_age['Low to medium income'] \/ income_age['total'], 2)\nincome_age['medhigh-perc'] = round(income_age['Medium to high income'] \/ income_age['total'], 2)\nincome_age['high-perc'] = round(income_age['High income'] \/ income_age['total'], 2)","f3312a01":"income_young_dict = {\n    'age_group': 'Young',\n    'income': ['low-income', 'low-med-income', 'med-high-income', 'high-income'],\n    'percentage': [income_age[income_age['age_group'] == 'Young']['low-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Young']['lowmed-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Young']['medhigh-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Young']['high-perc'].values[0],]\n}\n\nincome_adult_dict = {\n    'age_group': 'Adult',\n    'income': ['low-income', 'low-med-income', 'med-high-income', 'high-income'],\n    'percentage': [income_age[income_age['age_group'] == 'Adult']['low-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Adult']['lowmed-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Adult']['medhigh-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Adult']['high-perc'].values[0],]\n}\n\nincome_mature_dict = {\n    'age_group': 'Mature',\n    'income': ['low-income', 'low-med-income', 'med-high-income', 'high-income'],\n    'percentage': [income_age[income_age['age_group'] == 'Mature']['low-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Mature']['lowmed-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Mature']['medhigh-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Mature']['high-perc'].values[0],]\n}\n\nincome_senior_dict = {\n    'age_group': 'Senior',\n    'income': ['low-income', 'low-med-income', 'med-high-income', 'high-income'],\n    'percentage': [income_age[income_age['age_group'] == 'Senior']['low-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Senior']['lowmed-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Senior']['medhigh-perc'].values[0],\n                  income_age[income_age['age_group'] == 'Senior']['high-perc'].values[0],]\n}\n\nfactor_df = pd.DataFrame(income_young_dict).append(pd.DataFrame(income_adult_dict), ignore_index=True)\\\n            .append(pd.DataFrame(income_mature_dict), ignore_index=True).append(pd.DataFrame(income_senior_dict), ignore_index=True)\nsns.catplot(x='age_group', y='percentage', hue='income', data=factor_df, kind='bar', palette=sns.color_palette('hls', 4),\n           height=8, aspect=1.5);","2788dba4":"cluster = df_copy[df_copy['cluster'] == 0]\n\ncluster['age_group'].mode().values[0], cluster['age_range'].mode().values[0], cluster['Income_group'].mode().values[0], cluster['Fidelity_group'].mode().values[0], cluster['Recency_group'].mode().values[0]","fcd1b1e5":"cluster = df_copy[df_copy['cluster'] == 1]\n\ncluster['age_group'].mode().values[0], cluster['age_range'].mode().values[0], cluster['Income_group'].mode().values[0], cluster['Fidelity_group'].mode().values[0], cluster['Recency_group'].mode().values[0]","8d5a27db":"cluster = df_copy[df_copy['cluster'] == 2]\n\ncluster['age_group'].mode().values[0], cluster['age_range'].mode().values[0], cluster['Income_group'].mode().values[0], cluster['Fidelity_group'].mode().values[0], cluster['Recency_group'].mode().values[0]","771ae716":"cluster = df_copy[df_copy['cluster'] == 3]\n\ncluster['age_group'].mode().values[0], cluster['age_range'].mode().values[0], cluster['Income_group'].mode().values[0], cluster['Fidelity_group'].mode().values[0], cluster['Recency_group'].mode().values[0]","5ebd7574":"df_copy.head().T","00e0bed1":"df_copy = df_copy.drop(['ID', 'Year_Birth', 'Kidhome',\n                        'Teenhome', 'Dt_Customer'], axis=1)","7ce3d8e6":"# This will turn all the strings value into category values\ndf_copy['has_child'] = df_copy['has_child'].astype('category').cat.as_ordered()\nfor label, content in df_copy.items():\n    if pd.api.types.is_string_dtype(content):\n        df_copy[label] = content.astype('category').cat.as_ordered()\n\n# Turn categorical variables into numbers\nfor label, content in df_copy.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Turn categories into numbers and add + 1\n        df_copy[label] = pd.Categorical(content).codes + 1","35767c13":"toScale = ['Income', 'Recency', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds',\n          'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth',\n          'fidelity_years', 'age', 'children', 'total_spend', 'WebPerc', 'StorePerc']\n\nscaler = MinMaxScaler()\ndf_copy[toScale] = scaler.fit_transform(df_copy[toScale])","5dd5874a":"np.random.seed(42)\n\nX = df_copy.drop('cluster', axis=1)\ny = df_copy['cluster']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n\nlr_model = LogisticRegression(solver='liblinear')\nknn_model = KNeighborsClassifier()\nrf_model = RandomForestClassifier()\n\n\nmodels = {\n    'LogisticRegression': lr_model,\n    'KNeighborsClassifier': knn_model,\n    'RandomForestClassifier': rf_model\n}\n\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n    print(name, ', score:', model.score(X_test, y_test))","4aaeec3d":"# Cross validation on the best model\nfrom sklearn.model_selection import cross_val_score\n\nbest_clf = models['RandomForestClassifier']\n\ncv_acc = cross_val_score(best_clf, X, y, cv=5, scoring='accuracy')\n\ncv_acc","ed33dabe":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({'features': columns, \n                        'feature_importances': importances})\n          .sort_values('feature_importances', ascending=False)\n          .reset_index(drop=True))\n    \n    fig, ax = plt.subplots(figsize=(10, 10))\n    \n    ax.barh(df['features'][:n], df['feature_importances'][:n])\n    ax.set_ylabel('Features')\n    ax.set_xlabel('Feature importance')\n    ax.invert_yaxis()\n    \nplot_features(X.columns, best_clf.feature_importances_)","d79e81b0":"There aren't very difference between the percentage of web and store purchases by education level, for example Graduation category has the similar percentage both for store and web. Maybe this problem is due to the unbalanced level in Education categories.\n\n**Question for readers:** Can be sampling a good solution in this case?","88b53519":"Customers focus their purchases mainly on meat products","88314614":"### Feature Engineering\nWhat we've done:\n* Aggregated some Education categories\n* Aggregated some Marital Status categories\n* Computed a new feature: the fidelity years (customer's loyalty years)\n* Computed a new feature: the age of the customers\n* Computed a new feature: Sum up the kid and teen of customers as unique feature Children\n* Computed a new feature: If a customer has or not a child\n* Computed a new feature: Sum of all expenditures in different categories of products\n","9ed13e2c":"Since our Random Forest model provides the best scores so far with default parameters, we can proceed towars the cross-validation. From results the model doesn't seem to soffer the overfitting.","d998ad67":"Customers are centered around 50K dollars of annual Income. I guess that customers are mostly middle class people.","33fc0b8f":"### Let's add more features to our customers\n\nI want to segment the customer by different features as age, fidelity years, income and recency days. I borrowed from the linked notebook: *[Customer Personality Analysis with Python](https:\/\/thecleverprogrammer.com\/2021\/02\/08\/customer-personality-analysis-with-python\/)*.","26e72039":"customers purchases frequency seems to be high, on average 49 days pass from one purchase to the next one.","160dff86":"## Preparing the tools\n\nLet's import the main libraries","6b040d47":"Thus, the company have four kind of customers:\n* Mature custmomers (45 - 65 years old) middle class, old customer with a normal frequency of purchases (Cash Cow)\n* Adult customers (30 - 45 years old) low income, new customer with a high level of frequency purchases (Opportunity)\n* Mature customers similar to the first one except for the annual income, that in this case is low to medium\n* Mature customers with high income but low frequency of purchases. They are very experienced customers.","58635a22":"Let's suppose the company is promoting discounts campaign. The below chart show us possible target that could partecipate to campaign, in particular is possible to focus more effort on young and adult people with low income.","75f21d32":"## Conclusion\nI propose again the main goals of this analysis\n> Need to perform clustering to summarize customer segments\n\nThe customer segments defined are the following:\n* Mature custmomers (45 - 65 years old) middle class, experienced customer with a normal frequency of purchases, we can consider this kind of customer as a **Cash Cow**, due to their loyalty and constant expenditures\n* Adult customers (30 - 45 years old) low income, new customer with a high level of frequency purchases, we can consider them as an opportunity to grow the company income making the right campaign for them.\n* Mature customers similar to the first one except for the annual income, that in this case is low to medium\n* Mature customers with high income but low frequency of purchases. They are very experienced customers, they tends to save their money it could be difficult to get more from them.\n\n> Given the customer routine spending, can we predict at which segment a customer can be associated?\n\nIt is possible to predict the customer segment given its expenditure routine. In this way, it is possible to run more specific campaigns for each kind of customers.","5cfca90f":"Less than 1% of customers complain about company services, but we can see that in this little pool of customer graduate and in-couple people tend to complain more.","78378b31":"## Prepare data for modelling","9825598d":"Let's try to compute the purchases made in a store or on the web in percentage.","201a14ea":"Some columns could be converted in categorical type and we can add more information to this data. Let's work with features","fccda825":"Moreover it seems there are columns without a description and with only one value, we are going to remove those columns","5b868b4d":"We are going to work more on this feature in the next chunks. ","9216b176":"## Cluster\n\nWe are going to clusterize our customers as goal of the notebook and to discover other insights","d2666644":"Now we can see some outliers from the point of view of Income, so we are going to remove them","ed90d082":"The total spend of rich people has a decreasing trend as well as the age increase with a negative slope of -0.09, instead the poor people have a slightly opposite behaviour. This means rich people at age increasing tend to save their money, meanwhile poor people increase their expenditures.\n\n**Note:** Of course, it is a my opinion, I'm really far to offend someone. It is just what I'm reading from analyzed data and it is just an attempt to show a my intuition in light of the above chart. Is it my reasoning linear and according with the shown charts?\n","15ebfe11":"It is interesting to note how rich people (people with high annual income) tends to overlap with middle class people and in few cases with poor class. Indeed, it is possible to note different samples with high annual income and low total expenditure in products and, on the other side, samples with low annual income with high total spending. In few words, here we have a depiction of different behaviors and habits between rich and poor people, where true rich people tend to handle and hold their money for investments (we can ipotize) and poor people that are not able to manage money and tend to spend more to show off their no-poor condition. A classic example is Warren Buffet, he is a multi-bilionare but he lives in the same house bought in 1950s years and he don't spend its money if it not necessary, instead in a lot of cases poorer class tend to live beyond their means. We can have more evidence by the following charts, where are compared the cluster 3 containing people with a highest mean of annual income and cluster 1 containing people with the lowest annual income in mean.","221c77a8":"# Data Exploration\n\nIn this section we are going to analyze our data in order to visualize and compare features and their values, try to look at the creation of new features if necessary and look for other kind of pre-processing needs as normalization, transformation and so on.\n\nThe following questions can guide our EDA:\n\n* Which kind of customer tends to accept promotions at first offer? A the second \/ third and so on...?\n* Which kind of customer make more web visit of the web?\n* Which kind of customer make more purchases with discounts?\n* What is the best place (web\/store) in which customer spends more?\n* What is the average age of the enrolled customer?\n* Who complain more? Customer with High Education or Low Education? Aged people or young?\n\nIn update...\n\nAdd description of the features added, preprocessing and so on...","35c4b0bf":"So, we can predict the customer segment with high accuracy given its purchases behaviors. Moreover, from the above chart we can see the best attributes impacting the prediction","1d05a73b":"## Let's start\n\nLet's discover the dataset structure and compute some new feature","ea6308a1":"## Problem Definition \/ Goals\nOur goals in this analysis are:\n\n> Need to perform clustering to summarize customer segments\n\n> Given the customer routine spending, can we predict at which segment a customer can be associated?","442031d5":"Total expenditure is concentrated around an average value of 600+ dollars with a high level of standard deviation. Thus, we can ipotize the presence of very loyal customers and just occasional customers.","32d08485":"## 3. Features\n\nThe features of this dataset are divided in three macro categories:\n\n**People**\n\n* ID: Customer's unique identifier\n* Year_Birth: Customer's birth year\n* Education: Customer's education level\n* Marital_Status: Customer's marital status\n* Income: Customer's yearly household income\n* Kidhome: Number of children in customer's household\n* Teenhome: Number of teenagers in customer's household\n* Dt_Customer: Date of customer's enrollment with the company\n* Recency: Number of days since customer's last purchase\n* Complain: 1 if customer complained in the last 2 years, 0 otherwise\n\n**Products**\n\n* MntWines: Amount spent on wine in last 2 years\n* MntFruits: Amount spent on fruits in last 2 years\n* MntMeatProducts: Amount spent on meat in last 2 years\n* MntFishProducts: Amount spent on fish in last 2 years\n* MntSweetProducts: Amount spent on sweets in last 2 years\n* MntGoldProds: Amount spent on gold in last 2 years\n\n\n**Promotion**\n\n* NumDealsPurchases: Number of purchases made with a discount\n* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n**Place**\n\n* NumWebPurchases: Number of purchases made through the company\u2019s web site\n* NumCatalogPurchases: Number of purchases made using a catalogue\n* NumStorePurchases: Number of purchases made directly in stores\n* NumWebVisitsMonth: Number of visits to company\u2019s web site in the last month","daa1c775":"Also the acceptance rate seems to be very low, but even though the acceptance rate is very low for all campaigns, it seems that people \"gave up\" at the last campaign. In few words, it seems that the last campaign was more effective.","3907a5c7":"The company has a lot of Graduate customers follower by PhD people. From the point of view of civil status they are mostly in couple (married and together)","b2f35932":"We can drop some columns because they were used to compute other columns and are redundant","a3b8d2bb":"### Discovering Insights","3898db0e":"In general only the 27% of customers joined to the promoted campaigns"}}