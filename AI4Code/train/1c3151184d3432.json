{"cell_type":{"52e0ee6d":"code","0e0091df":"code","91be66a7":"code","45da32fa":"code","6995ac28":"code","22a3c35b":"code","93a7ad9f":"code","7fc261dd":"code","aca3b3e5":"code","9dae96a1":"code","83f40462":"code","a93e98bc":"code","f0547e76":"code","9441488a":"code","fc4ee48b":"code","d3d81a43":"code","951062e0":"code","ee847e2b":"code","14a8268c":"code","7df6336d":"code","d1e460e9":"code","4590ed03":"code","c15cdd64":"code","67b84c94":"code","e710b4ca":"markdown","91194c7a":"markdown","482d18d5":"markdown","48fd2c49":"markdown","40f26cdb":"markdown"},"source":{"52e0ee6d":"from typing import List, Tuple\nimport random\nimport html\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold, KFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport os\nfrom scipy.stats import spearmanr\nfrom scipy.optimize import minimize\nfrom math import floor, ceil\nfrom transformers import *\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef tqdm(it, *args, **kwargs):\n    return it\n\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nseed_everything()\nnp.set_printoptions(suppress=True)\n\nprint(tf.__version__)\nprint(torch.__version__)","0e0091df":"PATH = '..\/input\/google-quest-challenge\/'\n\nBERT_PATH = '..\/input\/bertpretrained\/uncased_L-12_H-768_A-12\/uncased_L-12_H-768_A-12\/'\ntokenizer = BertTokenizer.from_pretrained(BERT_PATH)\n\nMAX_SEQUENCE_LENGTH = 512\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\nsub = pd.read_csv(PATH+'sample_submission.csv')\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)\n\noutput_categories = list(df_train.columns[11:])\ninput_categories = list(df_train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","91be66a7":"df_train.question_body = df_train.question_body.apply(html.unescape)\ndf_train.question_title = df_train.question_title.apply(html.unescape)\ndf_train.answer = df_train.answer.apply(html.unescape)\n\ndf_test.question_body = df_test.question_body.apply(html.unescape)\ndf_test.question_title = df_test.question_title.apply(html.unescape)\ndf_test.answer = df_test.answer.apply(html.unescape)","45da32fa":"def _preprocess_text(s: str) -> str:\n    return s\n\n\ndef _trim_input(question_tokens: List[str], answer_tokens: List[str], max_sequence_length: int, q_max_len: int, a_max_len: int) -> Tuple[List[str], List[str]]:\n    q_len = len(question_tokens)\n    a_len = len(answer_tokens)\n    if q_len + a_len + 3 > max_sequence_length:\n        if a_max_len <= a_len and q_max_len <= q_len:\n            ## Answer \u3082 Question \u3082\u9577\u904e\u304e\u308b\u5834\u5408\u3001\u3069\u3061\u3089\u3082\u9650\u754c\u307e\u3067\u5207\u308a\u8a70\u3081\u308b\u3057\u304b\u306a\u3044\n            q_new_len_head = floor((q_max_len - q_max_len\/2))\n            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n            a_new_len_head = floor((a_max_len - a_max_len\/2))\n            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n        elif q_len <= a_len and q_len < q_max_len:\n            ## Answer \u306e\u307b\u3046\u304c\u9577\u304f\u3001Question \u304c\u5341\u5206\u77ed\u3044\u306a\u3089\u3001\u305d\u306e\u5206 Answer \u306b\u307e\u308f\u3059\n            a_max_len = a_max_len + (q_max_len - q_len - 1)\n            a_new_len_head = floor((a_max_len - a_max_len\/2))\n            answer_tokens = answer_tokens[:a_new_len_head] + answer_tokens[a_new_len_head - a_max_len:]\n        elif a_len < q_len:\n            assert a_len <= a_max_len\n            q_max_len = q_max_len + (a_max_len - a_len - 1)\n            q_new_len_head = floor((q_max_len - q_max_len\/2))\n            question_tokens = question_tokens[:q_new_len_head] + question_tokens[q_new_len_head - q_max_len:]\n        else:\n            raise ValueError(\"unreachable: q_len: {}, a_len: {}, q_max_len: {}, a_max_len: {}\".format(q_len, a_len, q_max_len, a_max_len))\n    return question_tokens, answer_tokens\n\n\ndef _convert_to_transformer_inputs(title: str, question: str, answer: str, tokenizer: BertTokenizer, question_only=False):\n    title = _preprocess_text(title)\n    question = _preprocess_text(question)\n    answer = _preprocess_text(answer)\n    question = \"{} [SEP] {}\".format(title, question)\n    question_tokens = tokenizer.tokenize(question)\n    if question_only:\n        answer_tokens = []\n    else:\n        answer_tokens = tokenizer.tokenize(answer)\n    question_tokens, answer_tokens = _trim_input(question_tokens, answer_tokens, MAX_SEQUENCE_LENGTH, (MAX_SEQUENCE_LENGTH - 3) \/\/ 2, (MAX_SEQUENCE_LENGTH - 3) \/\/ 2)\n    ids = tokenizer.convert_tokens_to_ids([\"[CLS]\"] + question_tokens + [\"[SEP]\"] + answer_tokens + [\"[SEP]\"])\n    padded_ids = ids + [tokenizer.pad_token_id] * (MAX_SEQUENCE_LENGTH - len(ids))\n    token_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(answer_tokens) + 1) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n    attention_mask = [1] * len(ids) + [0] * (MAX_SEQUENCE_LENGTH - len(ids))\n    return padded_ids, token_type_ids, attention_mask\n\nsample_args = df_train[\"question_title\"].values[0], df_train[\"question_body\"].values[0], df_train[\"answer\"].values[0]\nsample_ids = _convert_to_transformer_inputs(*sample_args, tokenizer, question_only=True)\nprint(sample_ids)\nprint(tokenizer.convert_ids_to_tokens(sample_ids[0]))","6995ac28":"def compute_input_arrays(df, question_only=False):\n    input_ids, input_token_type_ids, input_attention_masks = [], [], []\n    for title, body, answer in zip(df[\"question_title\"].values, df[\"question_body\"].values, df[\"answer\"].values):\n        ids, type_ids, mask = _convert_to_transformer_inputs(title, body, answer, tokenizer, question_only=question_only)\n        input_ids.append(ids)\n        input_token_type_ids.append(type_ids)\n        input_attention_masks.append(mask)\n    return (\n        np.asarray(input_ids, dtype=np.int32),\n        np.asarray(input_token_type_ids, dtype=np.int32),\n        np.asarray(input_attention_masks, dtype=np.int32),\n    )\n\n\ndef compute_output_arrays(df):\n    return np.asarray(df[output_categories])","22a3c35b":"class Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        config = BertConfig.from_json_file(BERT_PATH + \"\/bert_config.json\")\n        config.output_hidden_states = True\n        self.bert = BertForPreTraining.from_pretrained(BERT_PATH + \"\/bert_model.ckpt.index\", from_tf=True, config=config).bert\n        self.cls_token_head = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.qa_sep_token_head = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.linear = nn.Sequential(\n            nn.Dropout(0.1),\n            nn.Linear(768 * 2, 30),\n        )\n        \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_mask, -1) - 1)\n\n#         p_question_answer_dropout = 0.2\n#         if self.training and random.random() < p_question_answer_dropout:\n#             if random.random() < 0.5:\n#                 # mask question\n#                 attention_mask = attention_mask * (token_type_ids == 1)\n#             else:\n#                 # mask answer\n#                 attention_mask = attention_mask * (token_type_ids == 0)\n        \n        _, _, hidden_states = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n        x_cls = self.cls_token_head(x)\n        \n        # Gather [SEP] hidden states\n        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n        \n        x_qa_sep = self.qa_sep_token_head(x)\n        x = torch.cat([x_cls, x_qa_sep], -1)\n        x = self.linear(x)\n        return x","93a7ad9f":"outputs = torch.tensor(compute_output_arrays(df_train), dtype=torch.float)\ninputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train)]\nquestion_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_train, question_only=True)]\ntest_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test)]\ntest_question_only_inputs = [torch.tensor(x, dtype=torch.long) for x in compute_input_arrays(df_test, question_only=True)]","7fc261dd":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","aca3b3e5":"for n, _ in Model().named_parameters():\n    print(n)","9dae96a1":"LABEL_WEIGHTS = torch.tensor(1.0 \/ df_train[output_categories].std().values, dtype=torch.float32).to(device)\nLABEL_WEIGHTS = LABEL_WEIGHTS \/ LABEL_WEIGHTS.sum() * 30\nfor name, weight in zip(output_categories, LABEL_WEIGHTS.cpu().numpy()):\n    print(name, \"\\t\", weight)","83f40462":"BEST_BINS = [400, 400, 15, 100, 400, 7, 1600, 100, 100, 400, 100, 9, 8, 50, 9, 8, 15, 400, 400, 5, 400, 400, 800, 50, 200, 1600, 20, 200, 1600, 1600]\n\ndef binning_output(preds, n_bins=BEST_BINS):\n    preds = preds.copy()\n    for i in range(preds.shape[-1]):\n        n = n_bins[i]\n        binned = (preds[:, i] * n).astype(np.int32).astype(np.float32) \/ n\n        unique_values, unique_counts = np.unique(binned, return_counts=True)\n        # \u591a\u6570\u6d3e\u4ee5\u5916\u304c 0.5 % \u3092\u4e0b\u56de\u3063\u305f\u3089 binning \u3092\u3084\u3081\u308b\n        minor_value_ratio = (unique_counts.sum() - unique_counts.max()) \/ unique_counts.sum()\n        if minor_value_ratio < 0.005:\n            keep = np.argsort(preds[:, i])[::-1][:int(len(preds) * 0.005) + 1]\n            binned[keep] = preds[keep, i]\n        preds[:, i] = binned\n    return preds\n\n\ndef compute_spearmanr(trues, preds, n_bins=None):\n    rhos = []\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        if len(np.unique(col_pred)) == 1:\n            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n        rhos.append(spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos)\n\n\n\ndef compute_loss(outputs, targets, alpha=0.5, margin=0.1, question_only=False):\n    if question_only:\n        outputs = outputs[:, :21]\n        targets = targets[:, :21]\n    bce = F.binary_cross_entropy_with_logits(outputs, targets, reduction=\"none\")\n    bce = (bce * LABEL_WEIGHTS[:bce.size(-1)]).mean()\n    \n    batch_size = outputs.size(0)\n    if batch_size % 2 == 0:\n        outputs1, outputs2 = outputs.sigmoid().contiguous().view(2, batch_size \/\/ 2, outputs.size(-1))\n        targets1, targets2 = targets.contiguous().view(2, batch_size \/\/ 2, outputs.size(-1))\n        # 1 if first ones are larger, -1 if second ones are larger, and 0 if equals.\n        ordering = (targets1 > targets2).float() - (targets1 < targets2).float()\n        margin_rank_loss = (-ordering * (outputs1 - outputs2) + margin).clamp(min=0.0)\n        margin_rank_loss = (margin_rank_loss * LABEL_WEIGHTS[:outputs.size(-1)]).mean()\n    else:\n        # batch size is not even number, so we can't devide them into pairs.\n        margin_rank_loss = 0.0\n\n    return alpha * bce + (1 - alpha) * margin_rank_loss\n\n\ndef train_and_predict(train_data, valid_data, test_data, q_train_data, q_valid_data, q_test_data, q_epochs, epochs, batch_size, fold):\n    dataloader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n    valid_dataloader = torch.utils.data.DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n    test_dataloader = torch.utils.data.DataLoader(test_data, shuffle=False, batch_size=batch_size)\n    q_dataloader = torch.utils.data.DataLoader(q_train_data, shuffle=True, batch_size=batch_size)\n    q_valid_dataloader = torch.utils.data.DataLoader(q_valid_data, shuffle=False, batch_size=batch_size)\n    q_test_dataloader = torch.utils.data.DataLoader(q_test_data, shuffle=False, batch_size=batch_size)\n\n    model = Model().to(device)\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n            \"weight_decay\": 0.0,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-4\n            \n        }\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(len(dataloader) * (q_epochs) * 0.05),\n        num_training_steps=len(dataloader) * (q_epochs)\n    )\n    \n    test_predictions = []\n    valid_predictions = []\n\n    ## Question Only\n    for epoch in range(q_epochs): \n        import time\n        start = time.time()\n        model.train()\n        train_losses = []\n        train_preds = []\n        train_targets = []\n        for input_ids, token_type_ids, attention_mask, targets in tqdm(q_dataloader, total=len(q_dataloader)):\n            input_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n            train_targets.extend(targets.detach().cpu().numpy())\n            loss = compute_loss(outputs, targets, question_only=True)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses.append(loss.detach().cpu().item())\n        model.eval()\n        valid_losses = []\n        valid_preds = []\n        valid_targets = []\n        with torch.no_grad():\n            for input_ids, token_type_ids, attention_mask, targets in tqdm(q_valid_dataloader, total=len(q_valid_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                targets = targets.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                prob = outputs.sigmoid()\n                prob[:, 21:] = 0.0\n                valid_preds.extend(prob.cpu().numpy())\n                valid_targets.extend(targets.cpu().numpy())\n                loss = compute_loss(outputs, targets, question_only=True)\n                valid_losses.append(loss.detach().cpu().item())\n            valid_predictions.append(np.stack(valid_preds))\n            test_preds = []\n            for input_ids, token_type_ids, attention_mask in tqdm(q_test_dataloader, total=len(q_test_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                prob = outputs.sigmoid()\n                prob[:, 21:] = 0.0\n                test_preds.extend(prob.cpu().numpy())\n            test_predictions.append(np.stack(test_preds))\n            print()\n        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) \/ len(valid_predictions)),\n            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n        ))\n        print(\"\\t elapsed: {}s\".format(time.time() - start))\n\n    ## Q and A\n    model = Model().to(device)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay) and \"bert\" in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if  p.requires_grad and any(nd in n for nd in no_decay) and \"bert\" in n], \n            \"weight_decay\": 0.0,\n            \"lr\": 5e-5\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if p.requires_grad and \"bert\" not in n],\n            \"weight_decay\": 1e-2,\n            \"lr\": 5e-4\n            \n        }\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(len(dataloader) * (epochs) * 0.05),\n        num_training_steps=len(dataloader) * (epochs)\n    )\n\n    for epoch in range(epochs): \n        import time\n        start = time.time()\n        model.train()\n        train_losses = []\n        train_preds = []\n        train_targets = []\n        for input_ids, token_type_ids, attention_mask, targets in tqdm(dataloader, total=len(dataloader)):\n            input_ids = input_ids.to(device)\n            token_type_ids = token_type_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            targets = targets.to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            train_preds.extend(outputs.detach().sigmoid().cpu().numpy())\n            train_targets.extend(targets.detach().cpu().numpy())\n            loss = compute_loss(outputs, targets)\n            model.zero_grad()\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            train_losses.append(loss.detach().cpu().item())\n        model.eval()\n        valid_losses = []\n        valid_preds = []\n        valid_targets = []\n        with torch.no_grad():\n            for input_ids, token_type_ids, attention_mask, targets in tqdm(valid_dataloader, total=len(valid_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                targets = targets.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                valid_preds.extend(outputs.sigmoid().cpu().numpy())\n                valid_targets.extend(targets.cpu().numpy())\n                loss = compute_loss(outputs, targets)\n                valid_losses.append(loss.detach().cpu().item())\n            valid_predictions.append(np.stack(valid_preds))\n            test_preds = []\n            for input_ids, token_type_ids, attention_mask in tqdm(test_dataloader, total=len(test_dataloader)):\n                input_ids = input_ids.to(device)\n                token_type_ids = token_type_ids.to(device)\n                attention_mask = attention_mask.to(device)\n                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n                test_preds.extend(outputs.sigmoid().cpu().numpy())\n            test_predictions.append(np.stack(test_preds))\n            print()\n        print(\"Epoch {}: Train Loss {}, Valid Loss {}\".format(epoch + 1, np.mean(train_losses), np.mean(valid_losses)))\n        print(\"\\t Train Spearmanr {:.4f}, Valid Spearmanr (avg) {:.4f}, Valid Spearmanr (last) {:.4f}\".format(\n            compute_spearmanr(np.stack(train_targets), np.stack(train_preds)),\n            compute_spearmanr(np.stack(valid_targets), sum(valid_predictions) \/ len(valid_predictions)),\n            compute_spearmanr(np.stack(valid_targets), valid_predictions[-1])\n        ))\n        print(\"\\t elapsed: {}s\".format(time.time() - start))\n\n    return valid_predictions, test_predictions","a93e98bc":"class Fold(object):\n    def __init__(self, n_splits=5, shuffle=True, random_state=71):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_groupkfold(self, train, group_name):\n        group = train[group_name]\n        unique_group = group.unique()\n\n        kf = KFold(\n            n_splits=self.n_splits,\n            shuffle=self.shuffle,\n            random_state=self.random_state\n        )\n        folds_ids = []\n        for trn_group_idx, val_group_idx in kf.split(unique_group):\n            trn_group = unique_group[trn_group_idx]\n            val_group = unique_group[val_group_idx]\n            is_trn = group.isin(trn_group)\n            is_val = group.isin(val_group)\n            trn_idx = train[is_trn].index\n            val_idx = train[is_val].index\n            folds_ids.append((trn_idx, val_idx))\n\n        return folds_ids","f0547e76":"gkf = Fold(n_splits=3, shuffle=True, random_state=71)\nfold_ids = gkf.get_groupkfold(df_train, group_name=\"url\")\n\nfor train_idx, valid_idx in fold_ids:\n    print((df_train.loc[train_idx, \"question_type_spelling\"] > 0).sum())\n    print((df_train.loc[valid_idx, \"question_type_spelling\"] > 0).sum())","9441488a":"histories = []\ntest_dataset = torch.utils.data.TensorDataset(*test_inputs)\nq_test_dataset = torch.utils.data.TensorDataset(*test_question_only_inputs)\n\nfor fold, (train_idx, valid_idx) in enumerate(fold_ids):\n    import gc\n    gc.collect()\n\n    train_inputs = [inputs[i][train_idx] for i in range(3)]\n    q_train_inputs = [question_only_inputs[i][train_idx] for i in range(3)]\n    train_outputs = outputs[train_idx]\n    train_dataset = torch.utils.data.TensorDataset(*train_inputs, train_outputs)\n    q_train_dataset = torch.utils.data.TensorDataset(*q_train_inputs, train_outputs)\n\n    valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n    q_valid_inputs = [question_only_inputs[i][valid_idx] for i in range(3)]\n    valid_outputs = outputs[valid_idx]\n    valid_dataset = torch.utils.data.TensorDataset(*valid_inputs, valid_outputs)\n    q_valid_dataset = torch.utils.data.TensorDataset(*q_valid_inputs, valid_outputs)\n\n    history = train_and_predict(\n        train_data=train_dataset, \n        valid_data=valid_dataset,\n        test_data=test_dataset, \n        q_train_data=q_train_dataset, \n        q_valid_data=q_valid_dataset,\n        q_test_data=q_test_dataset, \n        q_epochs=3, epochs=3, batch_size=8, fold=fold\n        )\n\n    histories.append(history)","fc4ee48b":"# get val preds per each epochs\nval_preds_list = []\nn_epochs = len(histories[0][0])\n\nfor epoch in range(n_epochs):\n    val_preds_one_epoch = np.zeros([len(df_train), 30])    \n\n    for fold, (train_idx, valid_idx) in enumerate(fold_ids):\n        val_pred = histories[fold][0][epoch]\n        val_preds_one_epoch[valid_idx, :] += val_pred\n\n    val_preds_list.append(val_preds_one_epoch)","d3d81a43":"oof_predictions = np.zeros((n_epochs, len(df_train), len(output_categories)), dtype=np.float32)\n\nfor j, name in enumerate(output_categories):\n    for epoch in range(n_epochs):\n        col = \"{}_{}\".format(epoch, name)\n        oof_predictions[epoch, :, j] = val_preds_list[epoch][:, j]\n\noof_predictions.shape","951062e0":"# get test preds per each epochs\ntest_preds_list = []\n\nfor epoch in range(n_epochs):\n    test_preds_one_epoch = 0\n\n    for fold in range(len(fold_ids)):\n        test_preds = histories[fold][1][epoch]\n        test_preds_one_epoch += test_preds\n\n    test_preds_one_epoch = test_preds_one_epoch \/ len(fold_ids)\n    test_preds_list.append(test_preds_one_epoch)","ee847e2b":"test_predictions = np.zeros((n_epochs, len(df_test), len(output_categories)), dtype=np.float32)\n\nfor j, name in enumerate(output_categories):\n    for epoch in range(n_epochs):\n        col = \"{}_{}\".format(epoch, name)\n        test_predictions[epoch, :, j] = test_preds_list[epoch][:, j]\n\ntest_predictions.shape","14a8268c":"import numpy as np\nimport pandas as pd\nfrom abc import abstractmethod\nfrom sklearn.metrics import roc_auc_score\n\n\nclass Base_Model(object):\n    @abstractmethod\n    def fit(self, x_train, y_train, x_valid, y_valid, config):\n        raise NotImplementedError\n    \n    @abstractmethod\n    def get_best_iteration(self, model):\n        raise NotImplementedError\n\n    @abstractmethod\n    def predict(self, model, features):\n        raise NotImplementedError\n        \n    @abstractmethod\n    def get_feature_importance(self, model):\n        raise NotImplementedError      \n        \n\n    def cv(self, y_train, train_features, test_features, feature_name, folds_ids, config):\n        # initialize\n        test_preds = np.zeros(len(test_features))\n        oof_preds = np.zeros(len(train_features))\n        importances = pd.DataFrame(index=feature_name)\n        best_iteration = 0\n        cv_score_list = []\n        models = []\n\n        for i_fold, (trn_idx, val_idx) in enumerate(folds_ids):\n            # get train data and valid data\n            x_trn = train_features.iloc[trn_idx]\n            y_trn = y_train[trn_idx]\n            x_val = train_features.iloc[val_idx]\n            y_val = y_train[val_idx]\n            \n            # train model\n            model, best_score = self.fit(x_trn, y_trn, x_val, y_val, config)\n            cv_score_list.append(best_score)\n            models.append(model)\n            best_iteration += self.get_best_iteration(model) \/ len(folds_ids)\n    \n            # predict out-of-fold and test\n            oof_preds[val_idx] = self.predict(model, x_val)\n            test_preds += self.predict(model, test_features) \/ len(folds_ids)\n\n            # get feature importances\n            importances_tmp = pd.DataFrame(\n                self.get_feature_importance(model),\n                columns=[f'gain_{i_fold+1}'],\n                index=feature_name\n            )\n            importances = importances.join(importances_tmp, how='inner')\n\n        # summary of feature importance\n        feature_importance = importances.mean(axis=1)\n\n        # full train\n        # model, best_score = self.full_train(train_features, y_train, config, best_iteration * 1.5)\n        # oof_preds = self.predict(model, train_features)\n        # test_preds = self.predict(model, test_features)\n    \n        evals_results = {\"evals_result\": {\n            \"cv_score\": {f\"cv{i+1}\": cv_score for i, cv_score in enumerate(cv_score_list)},\n            \"n_data\": len(train_features),\n            \"best_iteration\": best_iteration,\n            \"n_features\": len(train_features.columns),\n            \"feature_importance\": feature_importance.sort_values(ascending=False).to_dict()\n        }}\n\n        return models, oof_preds, test_preds, feature_importance, evals_results","7df6336d":"def lgb_compute_spearmanr(preds, trues):\n    rhos = spearmanr(trues.get_label(), preds).correlation\n    return \"spearmanr\", rhos, True\n\n\ndef compute_spearmanr_each_col(trues, preds, n_bins=None):\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    rhos = spearmanr(trues, preds).correlation\n    return rhos","d1e460e9":"import lightgbm as lgb\nfrom pathlib import Path\n\n\nclass LightGBM(Base_Model):\n    def fit(self, x_train, y_train, x_valid, y_valid, config):\n        d_train = lgb.Dataset(x_train, label=y_train)\n        d_valid = lgb.Dataset(x_valid, label=y_valid)\n        lgb_model_params = config[\"model\"][\"model_params\"]\n        lgb_train_params = config[\"model\"][\"train_params\"]\n        model = lgb.train(\n            params=lgb_model_params,\n            train_set=d_train,\n            valid_sets=[d_valid],\n            valid_names=['valid'],\n            feval=lgb_compute_spearmanr,\n            **lgb_train_params\n        )\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def full_train(self, x_train, y_train, config, iteration):\n        d_train = lgb.Dataset(x_train, label=y_train)\n        lgb_model_params = config[\"model\"][\"model_params\"]\n        model = lgb.train(\n            params=lgb_model_params,\n            train_set=d_train,\n            feval=lgb_compute_spearmanr,\n            num_boost_round=int(iteration)\n        )\n        best_score = dict(model.best_score)\n        return model, best_score\n\n    def get_best_iteration(self, model):\n        return model.best_iteration\n    \n    def predict(self, model, features):\n        return model.predict(features)\n        \n    def get_feature_importance(self, model):\n        return model.feature_importance(importance_type='gain')","4590ed03":"config = {\n    \"model\": {\n        \"name\": \"lightgbm\",\n        \"model_params\": {\n            \"boosting_type\": \"gbdt\",\n            \"objective\": \"rmse\",\n            \"tree_learner\": \"serial\",\n            \"learning_rate\": 0.1,\n            \"max_depth\": 1,\n            \"seed\": 71,\n            \"bagging_seed\": 71,\n            \"feature_fraction_seed\": 71,\n            \"drop_seed\": 71,\n            \"verbose\": -1\n        },\n        \"train_params\": {\n            \"num_boost_round\": 5000,\n            \"early_stopping_rounds\": 200,\n            \"verbose_eval\": 500\n        }\n    }\n}\n\n\noutputs = compute_output_arrays(df_train)\noof_preds_list = []\ntest_preds_list = []\n\nfor i_col in range(len(output_categories)):\n    y_train = outputs[:, i_col]\n    #x_train = pd.DataFrame(oof_predictions[:, :, 2].T)\n    x_train = pd.DataFrame(np.concatenate([oof_predictions[:, :, i].T for i in range(30)], axis=1))\n    x_test = pd.DataFrame(np.concatenate([test_predictions[:, :, i].T for i in range(30)], axis=1))\n    feature_name = x_train.columns\n\n    model = LightGBM()\n    models, oof_preds, test_preds, feature_importance, evals_results = model.cv(\n            y_train, x_train, x_test, feature_name, fold_ids, config\n    )\n    oof_preds_list.append(oof_preds.reshape(-1, 1))\n    test_preds_list.append(test_preds.reshape(-1, 1))\n\n    print(i_col, output_categories[i_col])\n    print(compute_spearmanr_each_col(oof_preds, y_train))\n    print(len(oof_preds), len(np.unique(oof_preds)))\n    print(len(test_preds), len(np.unique(test_preds)))","c15cdd64":"def compute_spearmanr(trues, preds, n_bins=None):\n    rhos = []\n    if n_bins:\n        preds = binning_output(preds, n_bins)\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        if len(np.unique(col_pred)) == 1:\n            col_pred[np.random.randint(0, len(col_pred) - 1)] = col_pred.max() + 1\n        rhos.append(spearmanr(col_trues, col_pred).correlation)\n    return np.mean(rhos)\n\n\noof_preds_fi = np.concatenate(oof_preds_list, axis=1)\nprint(compute_spearmanr(outputs, oof_preds_fi))","67b84c94":"test_preds_fi = np.concatenate(test_preds_list, axis=1)\nsub.iloc[:, 1:] = test_preds_fi\nsub.to_csv('submission.csv', index=False)","e710b4ca":"## 5. Submit","91194c7a":"## 3. Modeling","482d18d5":"## 2. Preprocessing","48fd2c49":"## 4. Training","40f26cdb":"## 1. Read data and tokenizer"}}