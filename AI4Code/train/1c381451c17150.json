{"cell_type":{"c96f3cdb":"code","0b29e8dd":"code","436b4c6e":"code","5661b0ce":"code","38f78860":"code","ef5db133":"code","3c4ba2c9":"code","929eb449":"code","a371da0d":"code","ccb2ac01":"code","1b64e5b8":"code","690d46ac":"code","00e340ff":"code","dd00ca20":"code","80a4ce38":"code","e08dd730":"markdown","85a41da1":"markdown","f9939e7e":"markdown","72601bc4":"markdown","db8f64d5":"markdown","59d2b7b3":"markdown","842d9a1e":"markdown","62c4e249":"markdown","495488c4":"markdown","0deba539":"markdown","a3c965dc":"markdown","1c48529a":"markdown","f0be9d93":"markdown"},"source":{"c96f3cdb":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))","0b29e8dd":"conn = sqlite3.connect('..\/input\/database.sqlite')\ndf = pd.read_sql(con=conn, sql='select * from scripts')\n\ndf.character = df.character.astype(str)\ndf.actor = df.actor.astype(str)\ndf.segment = df.segment.astype(str)\ndf[:10]","436b4c6e":"%%time\nAll_MP_Scripts = ''\nlast_seg = ''\n\nfor index, line in df.iterrows():\n    seg = line[3].lower()\n    type_of_line = line[4]\n    actor = line[5].lower()\n    character = line[6].lower()\n    detail = line[7].lower()\n    Script = ''\n    if seg != last_seg:\n        Script += '<scene '\n        if seg != 'none':\n            Script += seg\n        Script += '>\\n\\n'\n    if type_of_line != 'Direction':\n        Script += character+': '+ detail+' \\n\\n'\n    last_seg = seg\n    All_MP_Scripts += Script\n\nprint(All_MP_Scripts[:1000])","5661b0ce":"text_file = open(\"All_MP_Scripts.txt\", \"w\")\ntext_file.write(All_MP_Scripts)\ntext_file.close()","38f78860":"Text_Data = All_MP_Scripts.lower()\n\ncharindex = list(set(Text_Data))\ncharindex.sort() \nprint(charindex)\n\nnp.save(\"charindex.npy\", charindex)","ef5db133":"%%time\nCHARS_SIZE = len(charindex)\nSEQUENCE_LENGTH = 75\nX_train = []\nY_train = []\nfor i in range(0, len(Text_Data)-SEQUENCE_LENGTH, 1 ): \n    X = All_MP_Scripts[i:i + SEQUENCE_LENGTH]\n    Y = All_MP_Scripts[i + SEQUENCE_LENGTH]\n    X_train.append([charindex.index(x) for x in X])\n    Y_train.append(charindex.index(Y))\n\nX_train = np.reshape(X_train, (len(X_train), SEQUENCE_LENGTH))\n\nY_train = np_utils.to_categorical(Y_train)","3c4ba2c9":"def get_model():\n    model = Sequential()\n    inp = Input(shape=(SEQUENCE_LENGTH, ))\n    x = Embedding(CHARS_SIZE, 75, trainable=False)(inp)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(512,)(x)\n    x = Dense(256, activation=\"elu\")(x)\n    x = Dense(128, activation=\"elu\")(x)\n    outp = Dense(CHARS_SIZE, activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=Adam(lr=0.001),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","929eb449":"filepath=\"model_checkpoint.hdf5\"\n\ncheckpoint = ModelCheckpoint(filepath,\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=1)","a371da0d":"class TextSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        pattern = X_train[700]\n        outp = []\n        seed = [charindex[x] for x in pattern]\n        sample = 'TextSample:' +''.join(seed)+'|'\n        for t in range(100):\n          x = np.reshape(pattern, (1, len(pattern)))\n          pred = self.model.predict(x)\n          result = np.argmax(pred)\n          outp.append(result)\n          pattern = np.append(pattern,result)\n          pattern = pattern[1:len(pattern)]\n        outp = [charindex[x] for x in outp]\n        outp = ''.join(outp)\n        sample += outp\n        print(sample)\n\ntextsample = TextSample()","ccb2ac01":"# model = load_model(filepath)","1b64e5b8":"model_callbacks = [checkpoint, early, textsample]\nmodel.fit(X_train, Y_train,\n          batch_size=256,\n          epochs=15,\n          verbose=1,\n          callbacks = model_callbacks)","690d46ac":"# model = load_model(filepath)\nmodel.save_weights(\"full_train_weights.hdf5\")\nmodel.save(\"full_train_model.hdf5\")","00e340ff":"\n%%time\nTEXT_LENGTH  = 5000\nLOOPBREAKER = 3\n\n\nx = np.random.randint(0, len(X_train)-1)\npattern = X_train[x]\noutp = []\nfor t in range(TEXT_LENGTH):\n  if t % 500 == 0:\n    print(\"%\"+str((t\/TEXT_LENGTH)*100)+\" done\")\n  \n  x = np.reshape(pattern, (1, len(pattern)))\n  pred = model.predict(x, verbose=0)\n  result = np.argmax(pred)\n  outp.append(result)\n  pattern = np.append(pattern,result)\n  pattern = pattern[1:len(pattern)]\n  ####loopbreaker####\n  if t % LOOPBREAKER == 0:\n    pattern[np.random.randint(0, len(pattern)-10)] = np.random.randint(0, len(charindex)-1)","dd00ca20":"outp = [charindex[x] for x in outp]\noutp = ''.join(outp)\n\nprint(outp)","80a4ce38":"f =  open(\"output_text_sample.txt\",\"w\")\nf.write(outp)\nf.close()","e08dd730":"# Prep the Text for the RNN\nNext we will prepare an index of every unique character in our text. We are only getting rid of capitalization for simplicity, but still keeping all special characters. This will give us an output that retains the punctuation and format of the original. \n\nNote that if you want to replace the Monty Python scripts with some other text to duplicate, here would be the place to do it. Just replace the All_MP_Scripts with any other text file and the rest of the notebook will run the same. (the bigger the better, anything ~1MB+ is great) ","85a41da1":"# Save the Model\nTraining is done, save it. This is also a great place to load any pretrained models before generating new text.","f9939e7e":"# Pythonic Python Script for Making Monty Python Scripts\nThe python language was named after Monty Python's Flying Circus so why not use python to generate an unlimited supply of new Monty Python scripts? This notebook is a compressed version of my text generating AI. Text generator like this one require a lot of computational power so it only became really feasible to do them on Kaggle Kernels when they upgraded to have a GPU and a 6 hour computational limit. Even so, 6 hours is still kind of lean for an LSTM text generator but I can make it work quite well anyways. \n\nThe goal of this notebook is to serve as a introduction to text generation NLPs. These LSTM text generator are actually not that difficult to make. However, most tutorials on the topic are incomplete and\/or generate poor results. I'll try to talk about every step of the process thoroughly and clearly. Other than that, this notebook is pretty easy to adapt to any text generation you might want to do. Just pop in any sizeable txt file and the model will learn to make more text in that style. Things like Shakespeare are common and work well for this type of text generation. Make sure that GPU is enabled in settings. Now lets make an AI generate something completely different.\n\n## Imports\nAs always, a block of imports.\n","72601bc4":"# Let's See the Results\nAs you can see, the output is not bad. Text generators like this are pretty good on a line by line basis. Some of the lines and scene names seem really plausible as Monty Python dialogue. Plot and scene structure is off. Different characters show up talking about irrelevant things. In some ways that works wit Monty Python. Still, more AI structures are needed to keep track of the plot and such. Anyways, this is the extent of most AI text generation these days. This is why stuff like Shakespeare and poetry are popular by AI generation, abstract language makes imperfect text generation less detectable. \n\nA final thing I find interesting is that, while typos are generally few when trained well, the AI actually sometimes makes up new words in the same style. I guess this is from the AI making a typo but covering it with something that sounds right to it. So it is making some typos but they are just not overt and they seem like uncommon words. For example, once this script generated the phrase \"I say some perpilly cricket\". While that is a very british\/Monty Python sounding phrase, the word perpilly does not appear in the original Monty Python scripts anywhere and is not a word in english at all! Something to look for when generating your now unlimited supply of Monty Python scripts.\n","db8f64d5":"Now that we have our scripts, let's save it and move on to real work.","59d2b7b3":"\n# Load Model\nLoad models or weights here. For following up on partially trained models saved by checkpoint.","842d9a1e":"# Create the Model\nThe model uses 3 LSTMs stacked on top of each. Adding another LSTM layer and\/or running it a lot longer or in multiple session will give better results. However, the 3 LSTM should do fine in 6 hour and adding the loopbreaker to our code later will make even under trained models give good results. Also note that we are using CuDNNLSTMs. If you don't know what that is, it is a special LSTM layer specially made for NIVDA GPUs. These function the same as regular LSTM layers but are automatically optimised for the GPU. You lose some customization with these layers but they work roughly twice as fast as regular LSTMs layers if conditions are right.\n","62c4e249":"# Changing Database into Text\nThe first thing that needs to be done is change the Monty Python database into one nice long string of all the scripts. Originally, I tried putting both the directions and dialogue into the text. However, since there are so many directions this ends up making a script that is mostly stuff like \"cut to a picture of a man in the street.\" or \"cut to stock video of a train\" ect. We will just focus on the dialogue to make something more interesting to read. ","495488c4":"# Checkpoints and Custom Callback\nWe will use 3 callbacks. Checkpoint, EarlyStopping, and a custom TextSample callback. Text sample prints a sample line at the end of every epoch to see how the model is progressing each epoch. For Kaggle, this is less important as you have to commit your code to run this long enough to output results.","0deba539":"# Generating New Monty Python Scripts\nThis block generates new text in the style of the input text of TEXT_LENGTH size in characters. It takes a random seed pattern from the training set, predicts the next character, adds it to the end of the pattern, then drops the first character of the pattern and predicts on the new pattern and so forth.\n\nPretty much this text generator *tries* to accurately duplicate the Monty Python script but inevitably makes errors ,and those errors compound, but is still trained well enough that it ends up making Monty Python *like* scripts \n\n## The Loopbreaker\nThis is simple bit of I came up with while putting this together. Every so many character predictions, the program just changes one of the characters in the pattern to predict on (except the last few, to prevent spelling errors). This causes our model to perceive a slightly different text which causes it to change it's overall predictions slightly too. Without this, even a well trained model might start to repeat itself at some point and get caught in a loop. The loopbreaker can even prevent overfitting or allow under trained models to perform much better. Without a loopbreaker like this, models will need to be trained for many more hours before they can function without looping in on themselves.\n\nChanging this value up and down an interesting way to significantly change the output. Setting it high will have more repeated speech, slightly lower might get many line starting the same then vering off into different directions, really low will get lots of varied text but line structures and format might become unstable. Probably keep it somewhere between 1 and 10.\n","a3c965dc":"\n# Train Model\nEven with a GPU, this can take a while. As is, I'm setting this notebook to take almost the full 6 hour limit. I have played around with training these types of models for 12 or even 24 hours wit more layers.  However, usually if gotten to roughly around 1.0 loss the generator is good enough to go. Can train almost indefinitely on most models. We are not *really* worried about overfitting. Hypothetically, if the loss gets too low the text might become overfit, which in this case means just copying the text in the most inefficient way possible. However, it should take an unrealistically long time to get to that point (or just impossible).\n","1c48529a":"# Save Text Output","f0be9d93":"# Create Sequences\nIn a nutshell, this model will look at the last 75 characters in the script and attempt to predict the 76th. Our X variable will be a 75 character sequence and our Y variable will be the 76th character. This block chops the text data into such sequences of characters. \n\nNote that this part also tokenizes the characters, which is to say it replaces each character with a number that corresponds to it's index in charindex. This is why it is important to save a copy of the charindex with your model just in case. We will need it to decode our predictions later.\n"}}