{"cell_type":{"be51d06d":"code","a572346b":"code","c6a41df8":"code","be9bc38e":"code","550dac16":"code","0bda3d9a":"code","6e18f31e":"code","6333a66e":"code","cd05f90b":"code","4fccb77a":"code","932534f4":"code","09701b8e":"code","23d10d3d":"code","bb5ad590":"code","5152a999":"code","0d899020":"code","469cd03f":"code","20c8551a":"code","8644ec8e":"code","7a222c72":"code","86b047ad":"code","04417226":"code","f9dc5cf1":"code","820b42e3":"code","a3c277f1":"code","7e1088fa":"code","aa8d71b9":"code","638aa6d8":"code","3f7ac259":"code","4b75b633":"code","c668b297":"code","ed3b76c0":"code","0b1e3407":"code","57798b8a":"code","490a85eb":"code","b9d6da04":"code","068dab6a":"code","a83f2164":"code","7f7f9656":"code","e6f083a6":"code","62b04d60":"code","da10385f":"markdown","feeedb21":"markdown","dcdef92f":"markdown","c9696104":"markdown","4f2b9127":"markdown","8b4eb0e2":"markdown","a3e6310f":"markdown","11f8167b":"markdown","62622c18":"markdown","10970c05":"markdown","7f9c0a96":"markdown","3bd26ac2":"markdown","81324194":"markdown","aa80142a":"markdown","48b03416":"markdown","f3e6d2ee":"markdown","deb53545":"markdown","dc13cb1d":"markdown","d13db889":"markdown"},"source":{"be51d06d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSVfile I\/O (e.g. pd.read_csv)\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport seaborn as sns\nfrom plotly import tools\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\nfrom sklearn.ensemble import RandomForestRegressor\n# http:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-Intro.html\n# https:\/\/github.com\/Microsoft\/LightGBM\nimport lightgbm as lgb\n# Add evaluation metric to measure the model's performance\n# Regression metrics available:\n# http:\/\/scikit-learn.org\/stable\/modules\/classes.html#regression-metrics\n# http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#regression-metrics\n# http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#receiver-operating-characteristic-roc\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.r2_score.html\n# Cannot use sklearn.metrics.accuracy_score as it is a Classification metric\nfrom sklearn.metrics import make_scorer, r2_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom IPython.display import display # Allows the use of display() for DataFrames\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\nimport itertools\n\n#warnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)","a572346b":"# Support functions\ndef bar_hor(df, col, title, color, w=None, h=None, lm=0, limit=100, return_trace=False, rev=False, xlb = False):\n    cnt_srs = df[col].value_counts()\n    yy = cnt_srs.head(limit).index[::-1] \n    xx = cnt_srs.head(limit).values[::-1] \n    if rev:\n        yy = cnt_srs.tail(limit).index[::-1] \n        xx = cnt_srs.tail(limit).values[::-1] \n    if xlb:\n        trace = go.Bar(y=xlb, x=xx, orientation = 'h', marker=dict(color=color))\n    else:\n        trace = go.Bar(y=yy, x=xx, orientation = 'h', marker=dict(color=color))\n    if return_trace:\n        return trace \n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\ndef bar_hor_noagg(x, y, title, color, w=None, h=None, lm=0, limit=100, rt=False):\n    trace = go.Bar(y=x, x=y, orientation = 'h', marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n\n\ndef bar_ver_noagg(x, y, title, color, w=None, h=None, lm=0, rt = False):\n    trace = go.Bar(y=y, x=x, marker=dict(color=color))\n    if rt:\n        return trace\n    layout = dict(title=title, margin=dict(l=lm), width=w, height=h)\n    data = [trace]\n    fig = go.Figure(data=data, layout=layout)\n    iplot(fig)\n    \ndef gp(col, title):\n    df1 = app_train[app_train[\"TARGET\"] == 1]\n    df0 = app_train[app_train[\"TARGET\"] == 0]\n    a1 = df1[col].value_counts()\n    b1 = df0[col].value_counts()\n    \n    total = dict(app_train[col].value_counts())\n    x0 = a1.index\n    x1 = b1.index\n    \n    y0 = [float(x)*100 \/ total[x0[i]] for i,x in enumerate(a1.values)]\n    y1 = [float(x)*100 \/ total[x1[i]] for i,x in enumerate(b1.values)]\n\n    trace1 = go.Bar(x=a1.index, y=y0, name='Target : 1', marker=dict(color=\"#96D38C\"))\n    trace2 = go.Bar(x=b1.index, y=y1, name='Target : 0', marker=dict(color=\"#FEBFB3\"))\n    return trace1, trace2 ","c6a41df8":"# This implementation was copied from: https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","be9bc38e":"# List available data files\n#print(os.listdir(\"..\/input\"))\nprint(\"Loading data files...\")\n\nstart = time()\n# Load all the datasets and reduce the memory usage\nposc_bal = reduce_mem_usage(pd.read_csv(\"..\/input\/POS_CASH_balance.csv\"))\nbureau_bal = reduce_mem_usage(pd.read_csv(\"..\/input\/bureau_balance.csv\"))\napp_train = reduce_mem_usage(pd.read_csv(\"..\/input\/application_train.csv\"))\nprev_app = reduce_mem_usage(pd.read_csv(\"..\/input\/previous_application.csv\"))\ninst_pay = reduce_mem_usage(pd.read_csv(\"..\/input\/installments_payments.csv\"))\ncc_bal = reduce_mem_usage(pd.read_csv(\"..\/input\/credit_card_balance.csv\"))\napp_test = reduce_mem_usage(pd.read_csv(\"..\/input\/application_test.csv\"))\nbureau = reduce_mem_usage(pd.read_csv(\"..\/input\/bureau.csv\"))\nend = time()\n\nprint(\"Finished loading data files and running memory optimization in {} seconds.\".format(int(round(end - start))))","550dac16":"# Show first 5 rows of each dataset\nprint('Point of Sale Cash Balance')\ndisplay(posc_bal.head())\nprint('Buereau Balance')\ndisplay(bureau_bal.head())\nprint('Applications Train')\ndisplay(app_train.head())\nprint('Previous Applications')\ndisplay(prev_app.head())\nprint('Installment Payments')\ndisplay(inst_pay.head())\nprint('Credit Card Balance')\ndisplay(cc_bal.head())\nprint('Applications Test')\ndisplay(app_test.head())\nprint('Bureau')\ndisplay(bureau.head())","0bda3d9a":"# Show dataset descriptive statistics\nprint('Point of Sale Cash Balance')\ndisplay(posc_bal.describe(exclude=['category']))\nprint('Buereau Balance')\ndisplay(bureau_bal.describe(exclude=['category']))\nprint('Applications Train')\ndisplay(app_train.describe(exclude=['category']))\nprint('Previous Applications')\ndisplay(prev_app.describe(exclude=['category']))\nprint('Installment Payments')\ndisplay(inst_pay.describe(exclude=['category']))\nprint('Credit Card Balance')\ndisplay(cc_bal.describe(exclude=['category']))\nprint('Applications Test')\ndisplay(app_test.describe(exclude=['category']))\nprint('Bureau')\ndisplay(bureau.describe(exclude=['category']))","6e18f31e":"eda = pd.DataFrame(\n    [\n        ['Point of Sale Cash Balance', posc_bal.shape[0], posc_bal.shape[1] - 2, np.sum(posc_bal.dtypes=='category'), \n            np.sum(posc_bal.isnull().sum() > 0), posc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Bureau Balance', bureau_bal.shape[0], bureau_bal.shape[1] - 1, np.sum(bureau_bal.dtypes=='category'), \n            np.sum(bureau_bal.isnull().sum() > 0), bureau_bal.isnull().sum().sum()], # Features don't include SK_ID_BUREAU\n        ['Applications Train', app_train.shape[0], app_train.shape[1] - 2, np.sum(app_train.dtypes=='category'), \n            np.sum(app_train.isnull().sum() > 0), app_train.isnull().sum().sum()], # Features don't include SK_ID_CURR or TARGET\n        ['Previous Applications', prev_app.shape[0], prev_app.shape[1] - 2, np.sum(prev_app.dtypes=='category'), \n            np.sum(prev_app.isnull().sum() > 0), prev_app.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Installment Payments', inst_pay.shape[0], inst_pay.shape[1] - 2, np.sum(inst_pay.dtypes=='category'), \n            np.sum(inst_pay.isnull().sum() > 0), inst_pay.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Credit Card Balance', cc_bal.shape[0], cc_bal.shape[1] - 2, np.sum(cc_bal.dtypes=='category'), \n            np.sum(cc_bal.isnull().sum() > 0), cc_bal.isnull().sum().sum()], # Features don't include SK_ID_PREV and SK_ID_CURR\n        ['Applications Test', app_test.shape[0], app_test.shape[1] - 1, np.sum(app_test.dtypes=='category'), \n            np.sum(app_test.isnull().sum() > 0), app_test.isnull().sum().sum()], # Features don't include SK_ID_CURR\n        ['Bureau', bureau.shape[0], bureau.shape[1] - 2, np.sum(bureau.dtypes=='category'), \n            np.sum(bureau.isnull().sum() > 0), bureau.isnull().sum().sum()], # Features don't include SK_ID_CURR and SK_ID_BUREAU\n    ],\n    columns=['Dataset', 'samples', 'number_features', 'number_categorical_features', 'number_features_missing_values', \n                'total_number_missing_values']\n)\n\ndisplay(eda.head(8))","6333a66e":"plt.figure(figsize=(18,9))\nplt.subplot(121)\napp_train[\"TARGET\"].value_counts().plot(fontsize = 16,\n                                        kind = 'pie',\n                                        autopct = \"%1.0f%%\",\n                                        colors = sns.color_palette(\"prism\",8),\n                                        startangle = 90,\n                                        labels=[\"1 - Repayer\",\"0 - Defaulter\"],\n                                        explode=[.1,0],\n                                       )\nplt.title(\"Distribution of Target Label for Applications Train dataset\", fontsize=20)","cd05f90b":"# Implementation source: https:\/\/www.kaggleusercontent.com\/kf\/4442153\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Df_QZcauc2BVOOmdHRjw1Q.ruvOVG8p44cAqUgN2tZHTPK-y8DwzYtkIoGA39JWR938aOHRdCqQRYjQj1U8AAiXqRfoRScRMjXH_DrMDqBWO9JIBKjTxS7yQyC3ouVc-MuExzzH0lGZdfJT2HJGkjvqSVLm4gYg7ML3r_jmJ3dP--6dmgHGsW1TQ6D04GnZzk6xwZseKGjCzeIYavlz44Qj.WDYyfq5ILj9HsKasnQ37uA\/__results__.html#Comparing-summary-statistics-between-defaulters-and-non---defaulters-for-loan-amounts-.\ncols = [ 'AMT_INCOME_TOTAL', 'AMT_CREDIT','AMT_ANNUITY', 'AMT_GOODS_PRICE']\n\ndf = app_train.groupby(\"TARGET\")[cols].describe().transpose().reset_index()\ndisplay(df)\ndf = df[df[\"level_1\"].isin([ 'mean', 'std', 'min', 'max'])] \ndf_x = df[[\"level_0\",\"level_1\",0]]\ndf_y = df[[\"level_0\",\"level_1\",1]]\ndf_x = df_x.rename(columns={'level_0':\"amount_type\", 'level_1':\"statistic\", 0:\"amount\"})\ndf_x[\"type\"] = \"1 - Repayer\"\ndf_y = df_y.rename(columns={'level_0':\"amount_type\", 'level_1':\"statistic\", 1:\"amount\"})\ndf_y[\"type\"] = \"0 - Defaulter\"\ndf_new = pd.concat([df_x,df_y],axis = 0)\n\nstat = df_new[\"statistic\"].unique().tolist()\nlength = len(stat)\n\nplt.figure(figsize=(13,15))\n\nfor i,j in itertools.zip_longest(stat,range(length)):\n    plt.subplot(2,2,j+1)\n    fig = sns.barplot(df_new[df_new[\"statistic\"] == i][\"amount_type\"],df_new[df_new[\"statistic\"] == i][\"amount\"],\n                hue=df_new[df_new[\"statistic\"] == i][\"type\"],palette=[\"g\",\"r\"])\n    plt.title(i + \"--Defaulters vs Non defaulters\")\n    plt.subplots_adjust(hspace = .4)\n    fig.set_facecolor(\"lightgrey\")","4fccb77a":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_BIRTH\")\nax = sns.distplot(app_train[\"DAYS_BIRTH\"])","932534f4":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of AMT_CREDIT\")\nax = sns.distplot(app_train[\"AMT_CREDIT\"])","09701b8e":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_ID_PUBLISH\")\nax = sns.distplot(app_train[\"DAYS_ID_PUBLISH\"])","23d10d3d":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_REGISTRATION\")\nax = sns.distplot(app_train[\"DAYS_REGISTRATION\"])","bb5ad590":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(app_train[\"DAYS_EMPLOYED\"])","5152a999":"# Merge training and testing datasets - This will help in two ways:\n# When handling categorical variables it will ensure both datasets end up with the same features\n# When handling missing values, if we use the mean to fill in missing values, they will be more representative\napp_train['is_train'] = 1\napp_train['is_test'] = 0\napp_test['is_train'] = 0\napp_test['is_test'] = 1\nprint(\"\\nJoining the training(app_train) and testing(app_test) dataset for pre-processing into pandas DataFrame 'data'.\")\n\n# data = pd.concat([app_train, app_test], axis=0, sort=False)\n# ERROR: TypeError: concat() got an unexpected keyword argument 'sort'\ndata = pd.concat([app_train, app_test], axis=0)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' for app_train\n# And substract 3 for app_test, as it doesn't have a 'TARGET' column\nprint(\"app_train has {0:,} samples and {1} features.\".format(app_train.shape[0], app_train.shape[1]-4))\nprint(\"app_test has {0:,} samples and {1} features.\".format(app_test.shape[0], app_test.shape[1]-3))\nprint(\"data has {0:,} samples and {1} features BEFORE one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nassert(data.shape[0] == app_train.shape[0] + app_test.shape[0])\nassert(data.shape[1] >= max(app_train.shape[1], app_test.shape[1]))","0d899020":"# Handle Categorical variables - Turn categorical variables into numerical features using the one-hot encoding scheme\n# Support function for one-hot encoding\ndef _one_hot_encoding(data):\n    # http:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.get_dummies.html\n    return pd.get_dummies(data)\n\n# Handle categorical variables\nprint(\"\\nPerforming one-hot encoding on {} dataset.\".format('data'))\ndata = _one_hot_encoding(data)\n# Substract 4 from the features count for the columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test'\nprint(\"app has {0:,} samples and {1} features AFTER one-hot encoding.\".format(data.shape[0], data.shape[1]-4))\nposc_bal = _one_hot_encoding(posc_bal)\n#bureau_bal = _one_hot_encoding(bureau_bal)\nprev_app = _one_hot_encoding(prev_app)\ninst_pay = _one_hot_encoding(inst_pay)\ncc_bal = _one_hot_encoding(cc_bal)\nbureau = _one_hot_encoding(bureau)","469cd03f":"# Keep a copy of the application_train & application_test datasets without merging with the rest of the datasets\ndata_train_test = data.copy()","20c8551a":"# Merge Point of Sale Cash Balance dataset\nprint(\"Merge 'Point of Sale Cash Balance' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nposc_bal_count = posc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nposc_bal['POSC_BAL_COUNT'] = posc_bal['SK_ID_CURR'].map(posc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nposc_bal = posc_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nposc_bal_avg = posc_bal.groupby('SK_ID_CURR').mean()\nposc_bal_avg.columns = ['pcb_' + col for col in posc_bal_avg.columns]\ndata = data.merge(right=posc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')","8644ec8e":"'''\n# Merge Bureau Balance dataset\nprint(\"Merge 'Bureau Balance' dataset.\")\n#'SK_ID_BUREAU'\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\nbureau_bal_count = bureau_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nbureau_bal['bureau_bal_COUNT'] = bureau_bal['SK_ID_CURR'].map(bureau_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nbureau_bal = bureau_bal.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nbureau_bal_avg = bureau_bal.groupby('SK_ID_CURR').mean()\nbureau_bal_avg.columns = ['posc_' + col for col in bureau_bal_avg.columns]\ndata_train = data_train.merge(right=bureau_bal_avg.reset_index(), how='left', on='SK_ID_CURR')\n'''","7a222c72":"# Merge Previous Applications dataset\nprint(\"Merge 'Previous Applications' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR'\nprev_app_count = prev_app[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprev_app['PREV_COUNT'] = prev_app['SK_ID_CURR'].map(prev_app_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\nprev_app = prev_app.drop(['SK_ID_PREV'], axis=1)\n\n# Average values for all other features in previous applications\nprev_app_avg = prev_app.groupby('SK_ID_CURR').mean()\nprev_app_avg.columns = ['pa_' + col for col in prev_app_avg.columns]\ndata = data.merge(right=prev_app_avg.reset_index(), how='left', on='SK_ID_CURR')","86b047ad":"# Merge Installments Payments dataset\nprint(\"Merge 'Installments Payments' dataset.\")\n# Count the number of installments payments for a given 'SK_ID_CURR', and create a new feature\ninst_pay_count = inst_pay[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninst_pay['INST_PAY_COUNT'] = inst_pay['SK_ID_CURR'].map(inst_pay_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ninst_pay = inst_pay.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ninst_pay_avg = inst_pay.groupby('SK_ID_CURR').mean()\ninst_pay_avg.columns = ['ip_' + col for col in inst_pay_avg.columns]\ndata = data.merge(right=inst_pay_avg.reset_index(), how='left', on='SK_ID_CURR')","04417226":"# Merge Credit Card Balance dataset\nprint(\"Merge 'Credit Card Balance' dataset.\")\n# Count the number of previous applications for a given 'SK_ID_CURR', and create a new feature\ncc_bal_count = cc_bal[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncc_bal['CC_BAL_COUNT'] = cc_bal['SK_ID_CURR'].map(cc_bal_count['SK_ID_PREV'])\n# Remove the 'SK_ID_PREV' column from the dataset as it doesn't add value\ncc_bal = cc_bal.drop(['SK_ID_PREV'], axis=1)\n\n## Average values for all other features in previous applications\ncc_bal_avg = cc_bal.groupby('SK_ID_CURR').mean()\ncc_bal_avg.columns = ['ccb_' + col for col in cc_bal_avg.columns]\ndata = data.merge(right=cc_bal_avg.reset_index(), how='left', on='SK_ID_CURR')","f9dc5cf1":"# Merge Bureau dataset\nprint(\"Merge 'Bureau' dataset.\")\n# Count the number of credits registered in the bureau for a given 'SK_ID_CURR', and create a new feature\nbureau_count = bureau[['SK_ID_CURR', 'SK_ID_BUREAU']].groupby('SK_ID_CURR').count()\nbureau['BUREAU_COUNT'] = bureau['SK_ID_CURR'].map(bureau_count['SK_ID_BUREAU'])\n# Remove the 'SK_ID_BUREAU' column from the dataset as it doesn't add value\nbureau = bureau.drop(['SK_ID_BUREAU'], axis=1)\n\n## Average values for all other features in previous applications\nbureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg.columns = ['b_' + col for col in bureau_avg.columns]\ndata = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')","820b42e3":"# Transforming skewed continuous features\n#skewed = ['DAYS_EMPLOYED']\n#data[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n# I need to handle negative numbers, if x = -1 then it will throw an error; log(0) = Inf","a3c277f1":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of DAYS_EMPLOYED\")\nax = sns.distplot(data[\"DAYS_EMPLOYED\"])","7e1088fa":"# Normalizing numerical features\nfrom sklearn.preprocessing import MinMaxScaler\n#app_train_copy = app_train.copy()\n\nscaler = MinMaxScaler()\n# Full list of top ten features, discounting EXT_SOURCE_X becuase they are already normalizaed:\n# ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH', 'pcb_CNT_INSTALMENT_FUTURE', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED']\n\n# numerical = ['DAYS_BIRTH', 'AMT_ANNUITY', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']\n# 12 entries in 'AMT_ANNUITY' are NaN - I need to fix that first before Normalizing\n\n# 'pcb_CNT_INSTALMENT_FUTURE' belongs to a different dataset\n\n# numerical = ['DAYS_BIRTH', 'AMT_CREDIT', 'DAYS_ID_PUBLISH']\n\nnumerical = ['DAYS_BIRTH', 'AMT_CREDIT', 'DAYS_ID_PUBLISH', 'DAYS_REGISTRATION', 'DAYS_EMPLOYED']\ndata[numerical] = scaler.fit_transform(data[numerical])","aa8d71b9":"data_to_use = 'ALL' # 'ALL' or 'data_train_test'\nif data_to_use == 'data_train_test':\n    data = data_train_test.copy()","638aa6d8":"# Handle missing data\n# https:\/\/pandas.pydata.org\/pandas-docs\/stable\/missing_data.html#filling-with-a-pandasobject\n# https:\/\/www.kaggle.com\/dansbecker\/handling-missing-values\n# http:\/\/scikit-learn.org\/dev\/modules\/generated\/sklearn.impute.SimpleImputer.html\nprint(\"\\nFilling NaN values in the dataset using pandas.fillna() using the column mean() value.\")\nprint(\"Number of NaN values in the dataset BEFORE running pandas.fillna(): {:,}\".format(data.isnull().sum().sum()))\ndata = data.fillna(data.mean())\nnan_after = data.isnull().sum().sum()\nprint(\"Number of NaN values in the dataset AFTER running pandas.fillna(): {:,}\".format(nan_after))\nassert(nan_after == 0)","3f7ac259":"import gc\n# Clean variables that are no longer needed\n# Not used yet: bureau_bal_count, bureau_bal_avg\ndel posc_bal, posc_bal_count, posc_bal_avg, bureau_bal, app_train, app_test\ndel prev_app, prev_app_count, prev_app_avg, inst_pay, inst_pay_count, inst_pay_avg, cc_bal, cc_bal_count, cc_bal_avg\ndel bureau, bureau_count, bureau_avg, data_train_test\ngc.collect()","4b75b633":"# Separate the data into the original test and training datasets\n# Remove columns 'TARGET', 'SK_ID_CURR', 'is_train', 'is_test' as they are not features\nprint(\"\\nSeparating the training and testing dataset after completing pre-processing.\")\ntrain = data[data['is_train'] == 1]\n\n# Separate the 'target label' from the training dataset\ntarget = train['TARGET']\ntrain = train.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\ntest = data[data['is_test'] == 1]\n\n# To be used when preparing the submission\ntest_id = test['SK_ID_CURR']\ntest = test.drop(['TARGET', 'SK_ID_CURR', 'is_test', 'is_train'], axis=1)\nprint(\"train has {:,} samples and {} features.\".format(train.shape[0], train.shape[1]))\nprint(\"test has {:,} samples and {} features.\".format(test.shape[0], test.shape[1]))","c668b297":"# Split 'features' and 'target label' data into training and validation data using train_test_split\n# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\nprint(\"\\nSplitting the training dataset into actual training and validation datasets\")\nX_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=42)\nassert(train.shape[0] == X_train.shape[0] + X_val.shape[0])\nassert(X_train.shape[1] == train.shape[1])\nassert(X_val.shape[1] == train.shape[1])\nassert(target.shape[0] == y_train.shape[0] + y_val.shape[0])\nprint(\"training dataset has {0:,} samples and {1} features.\".format(X_train.shape[0], X_train.shape[1]))\nprint(\"validating dataset has {0:,} samples and {1} features.\".format(X_val.shape[0], X_val.shape[1]))","ed3b76c0":"# Run GridSearchCV or fully train an estimator\n# 'grid_search_RFR', 'grid_search_LGBM', 'train_estimators', 'train_estimator_LGBM', 'train_estimator_RFR', 'LGBM_KFold'\nrun_mode = 'LGBM_KFold'","0b1e3407":"# Run GridSearchCV on LGBM\nif run_mode == 'grid_search_LGBM':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    X_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    y_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    X_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    y_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    \n    estimator = lgb.LGBMClassifier(\n          objective='binary',\n          metric='auc',\n          num_iteration=5000, # num_boost_round=5000,\n          verbose=1,\n          silent=False,\n          colsample_bytree=.8,\n          subsample=.9,\n          reg_alpha=.1,\n          reg_lambda=.1,\n          min_split_gain=.01,\n          min_child_weight=1,\n          # early_stopping_rounds=100\n          # ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n    )\n    \n    '''\n    parameters = {\n          'task': ['train'],\n          'boosting_type': ['gbdt'],\n          'objective': ['binary'],\n          'metric': ['auc'],\n          'learning_rate': [0.01],\n          'num_leaves': [48],\n          'num_iteration': [5000],\n          'verbose': 0,\n          'colsample_bytree': [.8],\n          'subsample': [.9],\n          'max_depth': [7],\n          'reg_alpha': [.1],\n          'reg_lambda': [.1],\n          'min_split_gain': [.01],\n          'min_child_weight': [1]\n        }\n    '''\n    parameters = {\n          'boosting_type': ['gbdt'], # 'dart'\n          'num_leaves': [35, 48, 80],\n          'min_data_in_leaf': [20], # [15, 20, 25],\n          'learning_rate': [0.005],\n          'max_depth': [7], # [6, 7, 8],\n        }\n    \n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(X_train_small, y_train_small)\n    end = time()\n    grid_fit_time = (end - start) \/ 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) \/ 60))\n\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    print(\"\\nParameters used for tunning: \\n{}\".format(parameters))\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    #pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    #print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(X_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val_small, best_pred_val)))\n","57798b8a":"# Run GridSearchCV\nif run_mode == 'grid_search_RFR':\n    perc_samples = 0.15\n    print(\"\\nPreparing to run Hyperparameters tunning with GridSearchCV using {0:.2f}% of the training samples\".format(perc_samples * 100))\n    features_train_small = X_train[:int(perc_samples * X_train.shape[0])]\n    target_train_small = y_train[:int(perc_samples * y_train.shape[0])]\n    features_val_small = X_val[:int(perc_samples * X_val.shape[0])]\n    target_val_small = y_val[:int(perc_samples * y_val.shape[0])]\n    #features_test_small = features_test[:int(perc_samples * features_test.shape[0])]\n\n    # Initialize the Estimator (Learner or Regression Model)\n    estimator = RandomForestRegressor(n_jobs=-1,\n                                      random_state=42,\n                                      verbose=0)\n\n    # Determine which Parameters to tune\n    '''\n    Tested so far:\n    parameters = {\n        'n_estimators': [9, 10, 11, 12, 13, 14, 15],\n        'criterion': ['mse', 'mae'],\n        'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7],\n        'max_features': [0.01, 0.1, 0.25, 0.45, 0.5, 0.55, 0.6, 0.75],\n        'min_samples_split': [2, 3, 4, 5],\n        'warm_start': [False, True]\n    }\n    '''\n    parameters = {\n        'n_estimators': [130, 135, 145],\n        'min_samples_leaf': [55, 62, 75],\n        'max_features': [0.2], # [0.18, 0.2, 0.23]\n        'min_samples_split': [2], # [2, 3]\n    }\n\n    # Create a scorer to measure hyperparameters performance\n    scorer = make_scorer(roc_auc_score)\n\n    # Create GridSearchCV grid object\n    grid_obj = GridSearchCV(estimator=estimator, \n                            param_grid=parameters, \n                            scoring=scorer)\n\n    # Fit the GridSearchCV grid object with the reduced training dataset and find the best hyperparameters\n    start = time()\n    grid_fit = grid_obj.fit(features_train_small, target_train_small)\n    end = time()\n    grid_fit_time = (end - start) \/ 60 # Ellapsed time in minutes\n    print(\"\\nGridSearchCV estimator fit time: {0:.2f} minutes\".format((end - start) \/ 60))\n\n    # Get the best estimator\n    best_est = grid_obj.best_estimator_\n    print(\"\\nBest Estimator: \\n{}\\n\".format(best_est))\n\n    # Get the best score\n    best_score = grid_obj.best_score_\n    print(\"\\nBest Estimator Score: {}\\n\".format(best_score))\n\n    # Get the best parameters\n    best_params = grid_obj.best_params_\n    print(\"\\nBest Hyperparameters that yield the best score: \\n{}\\n\".format(best_params))\n\n    # Make predictions with unoptimized estimator on the validation set\n    pred_val = (estimator.fit(features_train_small, target_train_small)).predict(features_val_small)\n    print(\"\\nUnoptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, pred_val)))\n\n    # Predict with the best estimator on the validation set\n    best_pred_val = best_est.predict(features_val_small)\n    print(\"\\nOptimized Estimator prediction score on Validation set: \\t{}\".format(roc_auc_score(target_val_small, best_pred_val)))\n\n    # Predict with the best estimator on the testing set\n    #pred_test = best_est.predict(features_test)","490a85eb":"# Train estimator LGBM\nif run_mode == 'train_estimator_LGBM':\n    params = {\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': 0.1,\n        'min_data_in_leaf': 30,\n        'num_leaves': 31,\n        'max_depth': -1,\n        'feature_fraction': 0.5,\n        'scale_pos_weight': 2,\n        'drop_rate': 0.02,\n        'metric': 'auc',\n        'num_boost_round': 200,\n    }\n\n    data_split = 'kfold' # Possible values: 'kfold' or 'train_test_split'\n    if data_split == 'train_test_split':\n        # Using split merged datasets with train_test_split\n        lgb_train = lgb.Dataset(data=X_train, label=y_train)\n        lgb_eval = lgb.Dataset(data=X_val, label=y_val)\n        start = time()\n        estimator = lgb.train(\n            params = params,\n            train_set = lgb_train,\n            valid_sets = lgb_eval,\n            early_stopping_rounds = 350,\n            verbose_eval = 200\n        )\n        end = time()\n    elif data_split == 'kfold':   \n        # Using KFolds to split the merged dataset for cross-validation\n        lgb_train_cv = lgb.Dataset(data=train, label=target)\n        start = time()\n        cv_results = lgb.cv(\n            params = params,\n            train_set = lgb_train_cv,\n            nfold = 2,\n            early_stopping_rounds = 50,\n            stratified = True,\n            verbose_eval = 50\n        )\n        optimum_boost_rounds = np.argmax(cv_results['auc-mean'])\n        print('Optimum boost rounds = {}'.format(optimum_boost_rounds))\n        print('Best LGBM CV result = {}'.format(np.max(cv_results['auc-mean'])))\n        estimator = lgb.train(\n            params = params,\n            train_set = lgb_train_cv,\n            num_boost_round = optimum_boost_rounds,\n            verbose_eval = 50\n        )\n        end = time()\n    \n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n\n    lgb.plot_importance(estimator, figsize=(12, 12), max_num_features=30);","b9d6da04":"print(estimator)","068dab6a":"# Parameters from Aguiar https:\/\/www.kaggle.com\/jsaguiar\/updated-0-792-lb-lightgbm-with-simple-features\/code\n# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    from sklearn.model_selection import KFold, StratifiedKFold\n\n    folds = KFold(n_splits=10, shuffle=True, random_state=1024)\n\n    oof_preds = np.zeros(train.shape[0])\n    sub_preds = np.zeros(test.shape[0])\n    feature_importance = pd.DataFrame()\n    feats = train.columns\n\n    start = time()\n    for n_fold, (train_index, valid_index) in enumerate(folds.split(train, target)):\n        train_x, train_y = train.iloc[train_index], target.iloc[train_index]\n        valid_x, valid_y = train.iloc[valid_index], target.iloc[valid_index]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = lgb.LGBMClassifier(\n            nthread=4,\n            n_estimators=10000,\n            learning_rate=0.02,\n            num_leaves=34,\n            colsample_bytree=0.9497036,\n            subsample=0.8715623,\n            max_depth=8,\n            reg_alpha=0.041545473,\n            reg_lambda=0.0735294,\n            min_split_gain=0.0222415,\n            min_child_weight=39.3259775,\n            silent=-1,\n            verbose=-1, \n        )\n\n        clf.fit(\n            train_x,\n            train_y,\n            eval_set = [(valid_x, valid_y)],\n            eval_metric = 'auc',\n            verbose = 200,\n            early_stopping_rounds = 500,\n        )\n\n        oof_preds[valid_index] = clf.predict_proba(valid_x, num_iterations=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test, num_iterations=clf.best_iteration_)[:, 1] \/ folds.n_splits\n\n        fold_importance = pd.DataFrame()\n        fold_importance['feature'] = feats\n        fold_importance['importance'] = clf.feature_importances_\n        fold_importance['fold'] = n_fold + 1\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n        print('Fold {:02d} AUC: {:.6f}'.format(n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_index])))\n\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n    print('Full AUC score: {:.6f}'.format(roc_auc_score(target, oof_preds)))","a83f2164":"# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    # Display feature importance\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')","7f7f9656":"# Train estimator LGBM with KFold Cross-validation\nif run_mode == 'LGBM_KFold':\n    # Prepare submission file\n    submission = pd.DataFrame()\n    submission['SK_ID_CURR'] = test_id\n    submission['TARGET'] = sub_preds\n    submission.to_csv('LGBM_SKFold.csv', index=False)","e6f083a6":"# Train estimator RandonForrestRegressor\nif run_mode == 'train_estimator_RFR':\n    # Initialize the Estimator (Learner or Regression Model) with the best hyperparameters\n    # Alternative: n_estimators=135, max_features=0.2, min_samples_split=2, min_samples_leaf=62\n    # Alternative2: criterion='mae', # default='mse', VERY SLOW\n    estimator = RandomForestRegressor(n_estimators=125, # default=10\n                                      max_features=0.2, # default='auto'\n                                      min_samples_split=2, # default=2\n                                      min_samples_leaf=75, # default=1\n                                      n_jobs=-1, # default=1\n                                      random_state=42, # default=None\n                                      verbose=0) # default=0\n    print(\"\\nPreparing to train the following estimator: \\n{}\".format(estimator))\n\n    # Fit the estimator with the training dataset\n    start = time()\n    estimator.fit(X_train, y_train)\n    end = time()\n    print(\"\\nEstimator fit time: {} seconds\".format(int(round(end - start))))\n\n    # Predict with the validation dataset\n    pred_val = estimator.predict(X_val)\n    print(\"\\nEstimator prediction score on Validation set: \\t{}\".format(roc_auc_score(y_val, pred_val)))\n    \n    # Determine the feature importance\n    fi = pd.DataFrame()\n    fi['feature'] = X_train.columns\n    fi['importance'] = estimator.feature_importances_\n    display(fi.sort_values(by=['importance'], ascending=False).head(10))\n\n    # TODO: GRAPH THE FEATURE IMPORTANCE","62b04d60":"if 'train_estimator_' in run_mode:\n    # Predict using the 'test' dataset for submission\n    pred_test = estimator.predict(test)\n    \n    # Prepare prediction for submission\n    print(\"\\nPreparing prediction for submission.\")\n    submission = pd.DataFrame()\n    submission['SK_ID_CURR'] = test_id\n    # Replace any negative number with zero, required for https:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm\/code\n    # pred_test[pred_test < 0] = 0\n    submission['TARGET'] = pred_test\n    submission.head()\n    file_name = run_mode.split('train_estimator_')[1] + '.csv'\n    submission.to_csv(file_name, index=False)","da10385f":"## <a id=\"eda_ds_desc\">3.2 Datasets numerical statistics<\/a>","feeedb21":"# **Sections:**\n[1. Import libraries & support functions](#import)  \n[2. Dataset preparation](#data_import)  \n[3. Exploratory Data Analysis (EDA)](#eda)  \n&nbsp; [3.1 Datasets samples](#eda_ds_samples)  \n&nbsp; [3.2 Datasets numerical statistics](#eda_ds_desc)  \n&nbsp; [3.3 Datasets comparisons](#eda_ds_comparison)  \n&nbsp; [3.4 Target Label](#eda_app_train_target)  \n&nbsp; [3.5 Amounts comparison](#eda_amts)  \n&nbsp; [3.6 Distribution of DAYS_BIRTH](#eda_days_birth)  \n&nbsp; [3.7 Distribution of AMT_CREDIT](#eda_amt_credit)  \n&nbsp; [3.8 Distribution of DAYS_ID_PUBLISH](#eda_days_id_publish)  \n&nbsp; [3.9 Distribution of DAYS_REGISTRATION](#eda_days_registration)  \n&nbsp; [3.10 Distribution of DAYS_EMPLOYED](#eda_days_employed)  \n[4. Data Preprocessing](#4)  \n[5. Split Data into Training and Validation](#5)  \n[6. Hyperparameter Tuning](#6)  \n[7. Model Fitting & Prediction](#7)  ","dcdef92f":"## <a id=\"eda_days_registration\">3.9 Distribution of DAYS_REGISTRATION<\/a>","c9696104":"## <a id=\"eda_app_train_target\">3.4 Target Label<\/a>","4f2b9127":"# <a id=\"7\">7 Model Fitting & Prediction<\/a>","8b4eb0e2":"## <a id=\"eda_days_birth\">3.6 Distribution of DAYS_BIRTH<\/a>","a3e6310f":"## <a id=\"eda_days_id_publish\">3.8 Distribution of DAYS_ID_PUBLISH<\/a>","11f8167b":"## <a id=\"eda_amts\">3.5 Amounts comparison<\/a>","62622c18":"## <a id=\"eda_days_employed\">3.10 Distribution of DAYS_EMPLOYED<\/a>","10970c05":"# <a id=\"import\">1 Import Libraries and create support functions<\/a>","7f9c0a96":"## <a id=\"eda_amt_credit\">3.7 Distribution of AMT_CREDIT<\/a>","3bd26ac2":"# <a id=\"eda\">3 Exploratory Data Analysis (EDA)<\/a>","81324194":"# <a id=\"data_import\">2 Dataset Import<\/a>","aa80142a":"# <a id=\"4\">4 Data Preprocessing<\/a>","48b03416":"## <a id=\"eda_ds_samples\">3.1 Datasets samples<\/a>","f3e6d2ee":"Acknowledgements:\n- Dataset flattening, feature engineering, LGBM parameters: https:\/\/www.kaggle.com\/shep312\/lightgbm-harder-better-slower\n- Dataset flattening, LGBM model starting point: https:\/\/www.kaggle.com\/shivamb\/homecreditrisk-extensive-eda-baseline-0-772\n- General ideas: https:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm\/code\n- Reducing memory footprint: https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage","deb53545":"## <a id=\"eda_ds_comparison\">3.3 Datasets comparisons<\/a>","dc13cb1d":"# <a id=\"5\">5 Split Data into Training and Validation<\/a>","d13db889":"# <a id=\"6\">6 Hyperparameter Tuning<\/a>"}}