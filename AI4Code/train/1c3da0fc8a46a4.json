{"cell_type":{"0e733d0d":"code","836bc50e":"code","9dee8d32":"code","6b41f027":"code","176c8993":"code","3df62517":"code","3430a121":"code","1133f8af":"code","c913217e":"code","e68f09ec":"code","03fa0cd9":"code","66ccc077":"code","eee43c09":"code","cf8d0eff":"code","b34c6941":"code","358137b2":"code","55e3de06":"code","679e699d":"code","9eab1691":"code","f4f5cf42":"code","daaea9fd":"code","dc7c491f":"code","abfd0d2e":"code","ac255f80":"code","8ddc2791":"code","50803ce0":"code","f55b3ce3":"code","f0d00f19":"code","d91b72dd":"code","9a9b450a":"code","99f50129":"code","985c0917":"code","083e4d7b":"code","97fe1385":"markdown","05d6e75f":"markdown","ab666568":"markdown","86d4170d":"markdown","3ba80758":"markdown","9df32f2e":"markdown","922ee8e8":"markdown","a62cb8a0":"markdown","fda5c352":"markdown","ddfabf96":"markdown","608ae2c1":"markdown","bcc9f91a":"markdown","cb4819bd":"markdown","b141b573":"markdown","bc1f36b9":"markdown","bdcf9a6f":"markdown","42e749b0":"markdown","a166a3d1":"markdown","7dfed5d3":"markdown","4d2ff0f2":"markdown","a488d3a8":"markdown","25b7983b":"markdown","d86c2ab3":"markdown"},"source":{"0e733d0d":"# Import liberaries\nimport pandas as pd\npd.set_option('display.max_column' , 500)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#Read the data \ntrain  = pd.read_parquet('\/kaggle\/input\/kaggle-pog-series-s01e01\/train.parquet')\ntest = pd.read_parquet('\/kaggle\/input\/kaggle-pog-series-s01e01\/test.parquet')\nsample_submission = pd.read_csv('\/kaggle\/input\/kaggle-pog-series-s01e01\/sample_submission.csv')\n\n#Concat the two dataframes\ndf= pd.concat([train, test]).reset_index(drop = True)\n\nprint ('\\nColumns in train set not found in the test set :' ,set(train.columns)-set(test.columns) )\nprint ('_'*50)\nprint (\"\\n\")\n\n#Drop Un-needed columns\ndf.drop(['video_id' , 'channelId' , 'view_count' ,  'dislikes', 'likes',\n         'comment_count' , 'thumbnail_link' , 'ratings_disabled' ,\n         'comments_disabled' , 'id' , 'isTest']  , \n        axis = 1 , inplace =True)\n\ndf.head(2)","836bc50e":"#fill nan values in the description column  with str \ndf.description.fillna('not provided', inplace = True)\n\n\n\n#remove the outliers \ndf.duration_seconds = df.duration_seconds.apply(lambda x : np.nan if x> 70000 else x)\n#Normalize transformation\ndf.duration_seconds = np.log1p(df.duration_seconds)\n\n\n\n#fill the Nan values using pandas groupby method - will try channel title\nmaper = df.groupby('channelTitle')['duration_seconds'].mean().to_dict()\nnew_column = df['channelTitle'].map(maper)\ndf.duration_seconds = df.duration_seconds.fillna(new_column)\n#Some channel titel has duration_seconds with nan values\n#will try to fill it using the categoryId\nmaper = df.groupby('categoryId')['duration_seconds'].mean().to_dict()\nnew_column = df['categoryId'].map(maper)\ndf.duration_seconds = df.duration_seconds.fillna(new_column)","9dee8d32":"df.info()","6b41f027":"#convert trending date to datetime column\ndf.trending_date = pd.to_datetime(df.trending_date)\n#Remove the time zone from the published date\ndf.publishedAt = df.publishedAt.dt.tz_localize(None)\n\n# create a column with timedelta as total hours, as a float type\ndf['Delta_in_hours'] = (df.trending_date - df.publishedAt) \/ pd.Timedelta(hours=1)\n\n#Check the distribution of the new feature\nfig , ax = plt.subplots()\ndf['Delta_in_hours'].hist(ax = ax)\nax.set_title('Distribution of Delta Duarion in Hours from published date to trending date');","176c8993":"#Remove the negative values\ndf['Delta_in_hours'] = df['Delta_in_hours'].apply(lambda x: np.nan if x<=0 else x )\n\n#fill the Nan values with groupby method ( will use the channeltitle first then the categoryId )\n#channel title\nmaper = df.groupby(\"channelTitle\")[\"Delta_in_hours\"].mean().to_dict()\nseries = df['channelTitle'].map(maper)\ndf['Delta_in_hours'] = df['Delta_in_hours'].fillna(series)\n#categoryId\nmaper = df.groupby(\"categoryId\")[\"Delta_in_hours\"].mean().to_dict()\nseries = df['categoryId'].map(maper)\ndf['Delta_in_hours'] = df['Delta_in_hours'].fillna(series)\n\n#Now let's normalize the distribution\ndf['log_Delta_in_hours'] = np.log1p(df['Delta_in_hours'])\n#Check the distribution one more time\nfig , ax = plt.subplots()\ndf['log_Delta_in_hours'].hist(ax = ax)\nax.set_title('Distribution of log Delta Duarion in Hours ');","3df62517":"\"\"\"\n\n## (2) So what was the rate of views per time  ( view count \/ detla duration )\n#Calculate the average views frequency\n\n\ndf['Views_Per_hours'] = df.view_count \/ df.Delta_in_hours\ndf['Views_Per_minutes'] = df.view_count \/  df.Delta_in_minutes\ndf.head(2)\n\n#handle the inf values\n\n#let's handel the Inf values \ndf.loc[~np.isfinite(df['Views_Per_hours']), 'Views_Per_hour']= np.nan\ndf.loc[~np.isfinite(df['Views_Per_minutes']), 'Views_Per_minutes']= np.nan\n\n#fill the nan values with the mode\ndf['Views_Per_hours'].fillna(df['Views_Per_hours'].mode()[0] , inplace = True)\ndf['Views_Per_minutes'].fillna(df['Views_Per_minutes'].mode()[0] , inplace = True)\n\n\nfig , ax = plt.subplots(2,1)\ndf['Views_Per_hours'].hist(ax = ax[0])\ndf['Views_Per_minutes'].hist(ax = ax[1])\nax[0].set_title('Distribution of Views_Per_hour')\nax[1].set_title('Distribution of Views_Per_minutes')\nplt.tight_layout()\n\n\n#Normalize the distribution using power transfromation\nfrom sklearn.preprocessing import PowerTransformer\nPT = PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n\ndf['PT_Views_Per_hours'] = PT.fit_transform(df['Views_Per_hours'].values.reshape(-1, 1))\ndf['PT_Views_Per_minutes'] = PT.fit_transform(df['Views_Per_minutes'].values.reshape(-1, 1))\n\nfig , ax = plt.subplots(2,1)\ndf['PT_Views_Per_hours'].hist(ax = ax[0])\ndf['PT_Views_Per_hours'].hist(ax = ax[1])\nax[0].set_title('Distribution of PT_Views_Per_hours')\nax[1].set_title('Distribution of PT_Views_Per_hours')\nplt.tight_layout()\n\n\"\"\"\nprint ('skip')","3430a121":"# let's do that for published date\ndf['published_year'] = df.publishedAt.dt.year\ndf['published_month'] = df.publishedAt.dt.month\ndf['published_day'] = df.publishedAt.dt.dayofweek\ndf['published_Quarter'] =df.publishedAt.dt.quarter\n#and now for trending date\ndf['trending_date_year'] = df.trending_date.dt.year\ndf['trending_date_month'] = df.trending_date.dt.month\ndf['trending_date_day'] = df.trending_date.dt.dayofweek\ndf['trending_date_Quarter'] = df.trending_date.dt.quarter\ndf.head(2)","1133f8af":"df.groupby('published_day')['target'].mean()","c913217e":"def encode_the_numbers (column):\n    \"\"\"\n    function to encode the pandas column depend on thier average target from low to high\n    \"\"\"\n    helper_df = df.groupby(column)['target'].mean().sort_values(ascending = False).reset_index().reset_index()\n    maper = helper_df.groupby(column)[\"index\"].mean().to_dict()\n    df[column] = df[column].map(maper)","e68f09ec":"columns_to_encode = ['published_year' , 'published_month' , 'published_day' ,'published_Quarter' , \n                    'trending_date_year' ,'trending_date_month' , 'trending_date_day' , 'trending_date_Quarter']\n#encode the columns\nfor column in columns_to_encode:\n    encode_the_numbers (column)\n","03fa0cd9":"#let's check it out\nfig , ax = plt.subplots (3,1)\ndf.groupby('published_day')['target'].mean().plot(kind = 'barh' , ax = ax[0])\ndf.groupby('trending_date_day')['target'].mean().plot(kind = 'barh' , ax = ax[1])\ndf.groupby('published_Quarter')['target'].mean().plot(kind = 'barh' , ax = ax[2])\nplt.tight_layout()","66ccc077":"encode_the_numbers ('categoryId')\nfig , ax = plt.subplots ()\ndf.groupby('categoryId')['target'].mean().plot(kind = 'barh' , ax = ax)","eee43c09":"\ndf['number_of_words_in_title'] = df.title.str.count(' ').add(1)\ndf['number_of_letters_in_title'] = df.title.str.len()\ndf['title_uppercases'] = df['title'].str.findall(r'[A-Z]').str.len()\/df['number_of_letters_in_title']\ndf['title_lowercases'] = df['title'].str.findall(r'[a-z]').str.len()\/df['number_of_letters_in_title']\n#thanks to \n#https:\/\/www.kaggle.com\/michau96\/pogmodel-lightgbm-fe-explanation\n#He add also df[['title_sentiment_polarity', 'title_sentiment_subjectivity']] but i am not famillier with this liberiry","cf8d0eff":"df['number_of_links_in_discribtion'] = df.description.str.count('https:\/\/')\ndf['number_of_hashtags_in_discribtion'] = df.description.str.count('#')\ndf['number_of_exlimination_in_discribtion'] = df.description.str.count('!')\ndf['number_of_words_in_discribtion'] = df.description.str.count(' ').add(1)\ndf['number_of_letters_in_discribtion'] = df.description.str.len()\ndf['description_uppercases'] = df['title'].str.findall(r'[A-Z]').str.len()\/df['number_of_letters_in_discribtion']\ndf['description_lowercases'] = df['title'].str.findall(r'[a-z]').str.len()\/df['number_of_letters_in_discribtion']\n\n","b34c6941":"from sklearn.preprocessing import PowerTransformer\nPT = PowerTransformer()\ncolumns_to_normalize =['number_of_links_in_discribtion' , 'number_of_hashtags_in_discribtion' ,\n                      'number_of_exlimination_in_discribtion' , 'number_of_words_in_discribtion' , \n                      'number_of_letters_in_discribtion' ,'description_uppercases' , \n                      'description_lowercases']\n#normalize the columns\nfor column in columns_to_normalize:\n    df[column] = PT.fit_transform(df[column].values.reshape(-1, 1))\n","358137b2":"#encode\nfrom sklearn.preprocessing import LabelEncoder \nencoder = LabelEncoder()\ndf['has_thumbnail'] = encoder.fit_transform(df['has_thumbnail'])\n","55e3de06":"#find the intersection channels\ntrain_channelTitle_set = set(train.channelTitle.unique().tolist())\ntest_channelTitle_set = set(test.channelTitle.unique().tolist())\nintersection_channel = list(test_channelTitle_set.intersection(train_channelTitle_set))\n\n#loop for every channel\n#slice the dataframe of the channel and calculate the average of the target\naverage_target = []\n\nfor channel in intersection_channel:\n    train_slicer = train [train.channelTitle == channel]\n    average_target.append(train_slicer.target.mean())\n\n    \n#create a new dataframe and sort based on the average target\n#reindex the dataframe and use the new index as the channel label encoder \nchannel_Data_frame = pd.DataFrame({'channel_title':intersection_channel , \n                                  \"average_target\": average_target , })\n\nchannel_Data_frame = channel_Data_frame.sort_values(by = 'average_target' , ascending = False).reset_index(drop = True).reset_index().rename(columns ={'index':'label'})\n#Create a dic with the sorted channels and thier index number \nmap1 = channel_Data_frame.groupby(\"channel_title\")[\"label\"].mean().to_dict()\n#map the new column\ndf['channel_encodding'] = df['channelTitle'].map(map1)\n#fill the remaning channels with -99\ndf['channel_encodding'].fillna(-99 , inplace = True)","679e699d":"df['number_of_tags'] = df.tags.str.count(\"|\").add(1)","9eab1691":"#Find the count of each tag in the test set\n\nfrom collections import Counter\n#construct the counter\ntags = Counter()\n\n#loop on each tag in each row and add it to the counter\nfor row in range(test.shape[0]):\n    for tag in test.tags[row].split('|'):\n        tags[tag]+=1\n        \n#create dataframe from the dict and sort it according to the frequency\ntags_df = pd.DataFrame.from_dict(tags, orient='index').reset_index()\ntags_df = tags_df.rename(columns = {'index': 'Tag' ,0: 'Frequency'})\ntags_df = tags_df.sort_values(by = 'Frequency' , ascending = False)\ntags_df.head(15)","f4f5cf42":"#Create a new dummy columns for the top 20 tags found in the test data set\nnew_dummy_columns = tags_df.head(20).Tag.tolist()\n\nfor column in new_dummy_columns:\n    #check if the column name exsist in the tag column\n    df[column] = df.tags.str.count(column)\n    #encode binary 1: exsist , 0:not found\n    df[column] = df[column].apply(lambda x :1 if x != 0 else x)\n","daaea9fd":"from sklearn.preprocessing import PowerTransformer\nPT_target = PowerTransformer()\ndf['target'] = PT_target.fit_transform(df.target.values.reshape(-1, 1))","dc7c491f":"df.target.hist()","abfd0d2e":"#from the perviouse cell we have the tags in the test dataset \n# that will take very long to go throw all the tags\n# I will slice the tags for that accours at least 25 times in the test set\ntags_in_test = tags_df[tags_df.Frequency>24].Tag.tolist()\n\n\nimport statistics\ndef find_the_mean_target_of_tag (word):\n    \"\"\"\n    a function to get the average target for a specific tag\n    input : word (str)\n    output : average target (float)\n    \"\"\"\n    target_tag = []\n    \n    for tag , target in zip (train.tags , train.target):\n        if word in tag.split('|'):\n            target_tag.append(target)\n    try:\n        #return the average target\n        return statistics.mean(target_tag)\n    except:\n        #if the test tag was not found in the train data set\n        return 0\n\n\n#now lets apply the function on all tags in dataset\ntags_targets = {}\nfor tag in tags_in_test:\n    tags_targets[tag] = find_the_mean_target_of_tag(tag)\n    ","ac255f80":"#Now we will use the tag_target dict to weight every tags combination in the concanated df\n\n#create a new list to add it to the data frame later\nwieghted_tags_combination = []\n#loop for each row in the dataframe\nfor tag in df.tags:\n    sum_T = 0\n    count = 0\n    #loop for each word in tags combination\n    for word in tag.split('|'):\n        if word in tags_targets.keys():\n            sum_T += tags_targets[word]\n            count +=1\n    try:\n        average = sum_T\/count\n    except:\n        #in case the word not in the tag\n        average = 0\n        \n    wieghted_tags_combination.append(average)\n\n#add the new column to the df    \ndf['tags_combination target'] = wieghted_tags_combination","8ddc2791":"corr_df = df.select_dtypes('number').drop('target', axis=1).corrwith(df.target).sort_values().reset_index().rename(columns = {'index':'feature' ,0:'correlation'})\n\nfig , ax = plt.subplots(figsize  = (5,20))\nax.barh(y =corr_df.feature , width = corr_df.correlation )\nax.set_title('correlation between featuer and target'.title() ,\n            fontsize = 16 , fontfamily = 'serif' , fontweight = 'bold')\nplt.show();","50803ce0":"columns_with_low_correlation = corr_df[(corr_df.correlation >-0.03) & (corr_df.correlation<0.03)].feature.tolist()\n\ndf.drop(columns_with_low_correlation  , axis = 1 , inplace = True)","f55b3ce3":"numerical_df = df.select_dtypes('number')\ntrain  = numerical_df[numerical_df.target.notnull()]\ntest = numerical_df[numerical_df.target.isnull()].drop('target' , axis = 1).values","f0d00f19":"X = train.drop('target' , axis = 1).values\ny = train.target.values\n","d91b72dd":"from sklearn.preprocessing import StandardScaler\nSC = StandardScaler ()\nX = SC.fit_transform(X)\ntest = SC.transform(test)","9a9b450a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)","99f50129":"\n\nfrom sklearn.linear_model import Ridge\nRidge=Ridge()\n\nfrom sklearn.neighbors import KNeighborsRegressor\nknn=KNeighborsRegressor()\n\nfrom sklearn.linear_model import BayesianRidge\nBayesian = BayesianRidge()\n\nfrom sklearn.tree import DecisionTreeRegressor\ndec_tree = DecisionTreeRegressor()\n\nfrom sklearn.svm import SVR\nSVR=SVR()\n\nfrom sklearn.ensemble import RandomForestRegressor\nRandomForestRegressor = RandomForestRegressor()\n\n\n#evaluation\nfrom sklearn.metrics import mean_absolute_error\n\"\"\"\n\n\nregs = [Ridge ,knn , Bayesian ,dec_tree ,SVR ,RandomForestRegressor]\nnames = ['Ridge' ,'knn' , 'Bayesian' ,'dec_tree' ,'SVR' ,'RandomForestRegressor']\n\"\"\"\n\nregs = [RandomForestRegressor]\nnames = ['RandomForestRegressor']\n\nfor reg , name in zip (regs , names):\n    \n    reg.fit(X_train, y_train)\n    predict_train = reg.predict(X_train)\n    predict_test = reg.predict(X_test)\n        \n        \n        \n    \n","985c0917":"predict_train = PT_target.inverse_transform(predict_train.reshape(-1, 1))\npredict_test = PT_target.inverse_transform(predict_test.reshape(-1, 1))\ny_train = PT_target.inverse_transform(y_train.reshape(-1, 1))\ny_test = PT_target.inverse_transform(y_test.reshape(-1, 1))\n        \n    \nprint (name)\nprint ('train MAE : ' , mean_absolute_error(y_train , predict_train))\nprint ('test MAE: ' , mean_absolute_error(y_test , predict_test))\nprint ('_________________________')","083e4d7b":"\n#predict\nsubmision_prediction = RandomForestRegressor.predict(test)\n#inverse_transfrom\nsubmision_prediction = PT_target.inverse_transform(submision_prediction.reshape(-1, 1))\n#get the id from the test set\ntest_set = pd.read_parquet('\/kaggle\/input\/kaggle-pog-series-s01e01\/test.parquet')\n\n#create a submission df\nsub = pd.DataFrame()\n\nsub['id'] = test_set.id.values\nsub['target'] = submision_prediction\n\nsub.to_csv('sub_v8.csv' , index=  False)\nsub.head()","97fe1385":"## (FE-3)  Encode the categoryId to be in linear shape\n\nusing the same previous function re-sort the categoryID based on thier average target ","05d6e75f":"## (FE-7) What we can do for the channel title\n\nStrategy:\nnot all the channels in the train set exist in the test set\n\n* we will find the intersection channells\n* sort the intersection channells according to the average target\n* encode the sorted channels as acontinues variabels\n* map every thing to the df","ab666568":"## Import the libraries and Read the data","86d4170d":"# Objective of this notebook\n\n* I aim from this notebook to provide simple guide on how to extract and engineer features from the dataset\n* Prepared from a Beginner to beginners","3ba80758":"## (FE-11) Weight the Tags combination with average traget\nstartegy:\n* go for the test set and find all tags in test.tags.split\n* for each tag we need to know the crosponding average target\n* retern back to the df and calculate the average target for each tag combination","9df32f2e":"## (FE-5) Break every thing in the descriotion column","922ee8e8":"# Feature selection","a62cb8a0":"##  (FE-1) Calculate the Time delta duration from publish date to trending date","fda5c352":"## (FE-8) Count the number of tages per video ","ddfabf96":"## (FE-10) Normalize the Target","608ae2c1":"## (FE-2) Let's extract the quarter , year , month and day from the date time columns ( publishedAt and trending date)","bcc9f91a":"## Submission","cb4819bd":"# Build the Model","b141b573":"# Now let's find the corrlation between the numerical columns and our target","bc1f36b9":"## (FE-9)  create a dummy columns for the most frequent tags in the test dataset (1  =  tag exsit , 0  = tag not exist)","bdcf9a6f":"we can see the higher target was at day 4 and the lowest was at day 6 <br>\nso we need to encode those number to be in ranked in linear mannar from high to low ","42e749b0":"we need to normalize the destribution of the recently generated columns","a166a3d1":"## (FE-4) What can we get from the title","7dfed5d3":"* we can found some negative values which is not logic\n* Also we need to normalize the distribution of the new column","4d2ff0f2":"# Feature Engineering ","a488d3a8":"# Data Cleaning\n\n* Remove outliers\n* Normalize Transformation\n* Fill the Nan's","25b7983b":"## Insights\n* [Youtube Prediction Feature Eng](https:\/\/www.kaggle.com\/satoshiss\/youtube-prediction-feature-eng#Popular-Tag?)\n* [PogModel LightGBM + FE + Explanation](https:\/\/www.kaggle.com\/michau96\/pogmodel-lightgbm-fe-explanation)\n\n","d86c2ab3":"## (FE-6) encode the has_thumbnail column"}}