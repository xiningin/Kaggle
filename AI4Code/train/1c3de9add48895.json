{"cell_type":{"2d58f30e":"code","e6c14cf1":"code","7bc2dc55":"code","e0a7a273":"code","b7cfa5cb":"code","7edf90e4":"code","b229d7cd":"code","df865fd1":"code","ba89edc5":"code","7d4a4f5a":"code","8f484bfd":"code","2a4dea0c":"code","1d24a631":"code","4788baf7":"code","f5266bf3":"code","c3c8a964":"code","d2053291":"code","543175c6":"markdown","17decd2a":"markdown","d609a75c":"markdown","b2457c00":"markdown","64c34d0a":"markdown","68d4c929":"markdown","4ec0ad39":"markdown"},"source":{"2d58f30e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport warnings; warnings.filterwarnings(action='once')\nimport seaborn as sns\n# from pywaffle import Waffle\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.optimizers import RMSprop\nfrom keras.utils import to_categorical\nimport keras\n\n\n# Keras version.\nprint('Using Keras version', keras.__version__)\n\nsns.set(style=\"darkgrid\")\n\nRANDOM_SEED = 42\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e6c14cf1":"# Read data.\ndata_path = '\/kaggle\/input\/mushroom-classification\/mushrooms.csv'\ndf = pd.read_csv(data_path)\ndf.head()","7bc2dc55":"df.info()","e0a7a273":"df.describe()","b7cfa5cb":"# Print unique values for columns.\ncolumns = df.columns\nfor col in columns:\n    print('{feat_name}: {feat_values}'.format(feat_name=col, feat_values=df[col].unique()))","7edf90e4":"sns.countplot(x='class', data=df)","b229d7cd":"# First, create the list of labels as our y values.\nle = preprocessing.LabelEncoder()\ny = le.fit_transform(df['class'])\nprint(y)","df865fd1":"# Drop the labels from the dataframe, encode all features.\nX = df.drop('class', axis=1)\ncolumns = X.columns\nfor i in range(len(X.columns)):\n    le = preprocessing.LabelEncoder()\n    X[columns[i]] = le.fit_transform(X[columns[i]])\n    \nX.head()","ba89edc5":"# Inspect unique values again.\nfor col in columns:\n    print('{}: {}'.format(col, X[col].unique()))","7d4a4f5a":"# We need to know the maximum number of possible values for the embedding layer.\n# If we were using text, this would be the size of the vocabulary.\n# Find number of uniabsque values for each feature.\nfor col in columns:\n    print('{}: {}'.format(col, X[col].nunique()))","8f484bfd":"# Convert features to sequences.\nsequences = []\n# seq = '{}' * 22\ncolumns = X.columns\nfor idx, row in X.iterrows():\n    sequence = []\n    for i in range(len(columns)):\n        sequence.append(row[columns[i]])\n    sequences.append(sequence)\n    \n# Print first example and label, length of example sequence.\nprint('{sequence}: {label}'.format(sequence=sequences[0], label=y[0]))\nprint('len of sequences:', len(sequences[0]))","2a4dea0c":"# Build train\/test sets.\nx_train, x_test, y_train, y_test = train_test_split(sequences, y,\n                                                    test_size=0.1,\n                                                    random_state=RANDOM_SEED)\n# Convert to numpy arrays.\nx_train = np.array(x_train)\nx_test = np.array(x_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nprint(x_train)\nprint(y_train)","1d24a631":"def build_model():\n    embeddings_dims = 300\n    max_seq_length = len(sequences[0])\n    max_features = 12\n    filters = 250\n    kernel_size = 3\n    hidden_dims = 250\n    \n    using_pretrained_emb = False #@param {type:\"boolean\"}\n\n    # CNN via Keras.\n    model = Sequential()\n\n    if using_pretrained_emb:\n      model.add(layers.Embedding(max_features,\n                                 embeddings_dims,\n                                 embeddings_initializer=Constant(vocab),\n                                 input_length=max_seq_length,\n                                 trainable=False))\n    else:\n      model.add(layers.Embedding(max_features,\n                                 embeddings_dims,\n                                 input_length=max_seq_length))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.MaxPooling1D())\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.MaxPooling1D())\n    model.add(layers.Conv1D(filters,\n                            kernel_size,\n                            padding='valid',\n                            activation='relu',\n                            strides=1))\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(hidden_dims))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Activation('relu'))\n    model.add(layers.Dense(1))\n    model.add(layers.Activation('sigmoid'))\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adadelta',\n                  metrics=['accuracy'])\n    \n    return model","4788baf7":"# Build the model, check out the summary.\nmodel = build_model()\nmodel.summary()","f5266bf3":"# Train.\nhistory = model.fit(x_train, y_train,\n                    epochs=10,\n                    verbose=True,\n                    validation_data=(x_test, y_test),\n                    batch_size=16)","c3c8a964":"# Get predictions for test set.\npreds_test = model.predict_classes(x_test)\n\ncnn_metrics = {'acc': metrics.accuracy_score(y_test, preds_test)}\ncnn_metrics['prec'] = metrics.precision_score(y_test, preds_test)\ncnn_metrics['rec'] = metrics.recall_score(y_test, preds_test)\ncnn_metrics['f1'] = metrics.f1_score(y_test, preds_test)\ncnn_metrics['f1_macro'] = metrics.f1_score(y_test, preds_test,\n                                           average='macro')\ncnn_metrics['auc'] = metrics.roc_auc_score(y_test, preds_test)\n\nfor metric in cnn_metrics:\n  print('{metric_name}: {metric_value}'.format(metric_name=metric, metric_value=cnn_metrics[metric]))\n\n# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Get training and test accuracy history.\ntraining_acc = history.history['acc']\ntest_acc = history.history['val_acc']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show();\n\n# Visualize acc history\nplt.plot(epoch_count, training_acc, 'r--')\nplt.plot(epoch_count, test_acc, 'b-')\nplt.legend(['Training Acc', 'Test Acc'])\nplt.xlabel('Epoch')\nplt.ylabel('Acc')\nplt.show();","d2053291":"# Using the Model class from keras, rebuild the model using the layer outputs and model inputs.\nfrom keras.models import Model\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = Model(inputs=model.input, outputs=layer_outputs)\n\n# Run prediction for a single example to get activations.\nactivations = activation_model.predict(x_train[1].reshape(1, 22))\n\n# Plot them (switching to a heatmap once the shape of the data requires it).\nfor layer_num, act in enumerate(activations):\n    if len(act.shape) > 2:\n        plt.rcParams[\"axes.grid\"] = False\n        plt.matshow(act[0, :, :], cmap='viridis')\n    else:\n        plt.figure(figsize = (16,1))\n        sns.heatmap(act, cbar=False, cmap='viridis')","543175c6":"## Configure the Environment ","17decd2a":"## Build and Train the Model\n\n**Define a function to build the CNN model.[](http:\/\/)**","d609a75c":"## Read \/ Explore the Data","b2457c00":"## Visualize Activations\n**Convolutional networks often produce interesting patterns in the activations of their layers, so let's take a took at them.**","64c34d0a":"## Metrics\n\n**The metrics class of scikit-learn provides an easy way to produce a variety of metrics for the performance of the model.**","68d4c929":"**Plots are minimally helpful since the data are all categorical.**","4ec0ad39":"**The way these examples have been recorded, however, make them ideal to be formed as sequences. We encode the categorical data, then convert them to sequences.**"}}