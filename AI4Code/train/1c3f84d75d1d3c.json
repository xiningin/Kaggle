{"cell_type":{"504be700":"code","4288b959":"code","9c86595e":"code","90dada89":"code","361758d9":"code","cfa9f554":"code","7bdf601b":"code","52256f6d":"code","afa1cc82":"code","dce48122":"code","f48ae8ef":"code","0f12f713":"code","7c37778b":"code","2466a8cf":"code","12cd6547":"code","9c7ae607":"code","4cfaa7ba":"code","a8742135":"code","b17a908a":"code","8bb4867a":"code","b9a2f834":"code","4f0b6c40":"code","2fffa871":"markdown","2ab3dfbb":"markdown","0571cfc7":"markdown","fbdd18ad":"markdown","e69fa1f9":"markdown","51ca2fce":"markdown","0018d291":"markdown","1da17ac7":"markdown","6b583c25":"markdown","c85bca8d":"markdown","c01978ad":"markdown","81739b89":"markdown","4fb4d329":"markdown","438fc92b":"markdown","e9693ff0":"markdown","05862488":"markdown"},"source":{"504be700":"from scipy import signal\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport random\nimport numpy as np\nimport pandas as pd\nimport time\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten\nfrom tensorflow.keras.callbacks import Callback, LearningRateScheduler\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras import losses, models, optimizers\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\n!pip install catboost==0.23.1 -q\nseed = 1337\n\nrandom.seed(seed)\nnp.random.seed(seed)","4288b959":"train = pd.read_csv('..\/input\/data-without-drift\/train_clean.csv')\ntarget = train['open_channels']\n\ntrain.head()","9c86595e":"def calc_gradients(s, n_grads=4):\n\n    grads = pd.DataFrame()\n    \n    g = s.values\n    for i in range(n_grads):\n        g = np.gradient(g)\n        grads['grad_' + str(i+1)] = g\n        \n    return grads","90dada89":"def calc_low_pass(s, n_filts=10):\n\n    wns = np.logspace(-2, -0.3, n_filts)\n    low_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='low')\n        zi = signal.lfilter_zi(b, a)\n        low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return low_pass","361758d9":"def calc_high_pass(s, n_filts=10):\n\n    wns = np.logspace(-2, -0.3, n_filts)\n    high_pass = pd.DataFrame()\n    x = s.values\n    for wn in wns:\n        b, a = signal.butter(1, Wn=wn, btype='high')\n        zi = signal.lfilter_zi(b, a)\n        high_pass['highpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, x, zi=zi*x[0])[0]\n        high_pass['highpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, x)\n        \n    return high_pass","cfa9f554":"def calc_roll_stats(s, windows=[3, 10, 50, 100, 500]):\n\n    roll_stats = pd.DataFrame()\n    \n    for w in windows:\n        roll_stats['roll_mean_' + str(w)] = s.rolling(window=w, min_periods=1).mean()\n        roll_stats['roll_std_' + str(w)] = s.rolling(window=w, min_periods=1).std()\n        roll_stats['roll_min_' + str(w)] = s.rolling(window=w, min_periods=1).min()\n        roll_stats['roll_max_' + str(w)] = s.rolling(window=w, min_periods=1).max()\n        roll_stats['roll_range_' + str(w)] = roll_stats['roll_max_' + str(w)] - roll_stats['roll_min_' + str(w)]\n        roll_stats['roll_mean_s_' + str(w)] = s.rolling(window=w, min_periods=1).mean().shift(-w)\n        roll_stats['roll_std_s_' + str(w)] = s.rolling(window=w, min_periods=1).std().shift(-w)\n        roll_stats['roll_min_s_' + str(w)] = s.rolling(window=w, min_periods=1).min().shift(-w)\n        roll_stats['roll_max_s_' + str(w)] = s.rolling(window=w, min_periods=1).max().shift(-w)\n        roll_stats['roll_range_s_' + str(w)] = roll_stats['roll_max_s_' + str(w)] - roll_stats['roll_min_s_' + str(w)]\n        roll_stats['roll_q25_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.25)\n        roll_stats['roll_q75_' + str(w)] = s.rolling(window=w, min_periods=1).quantile(0.75)\n        roll_stats['kurtosis_' + str(w)] = roll_stats.apply(lambda x: pd.Series(x).kurtosis())\n\n    roll_stats = roll_stats.fillna(value=0)\n             \n    return roll_stats","7bdf601b":"def create_shift(s, windows=[1, 2, 3]):\n    \n    cf = pd.DataFrame()\n\n    for w in windows:    \n        cf['signal_shift_+' + str(w)] = s.shift(w)\n        cf['signal_shift_-' + str(w)] = s.shift(-1*w)\n\n    cf['signal_power'] = s ** 2\n    cf = cf.fillna(value=0) \n    \n    return cf ","52256f6d":"def calc_ewm(s, windows=[50, 500, 1000]):\n\n    ewm = pd.DataFrame()\n    for w in windows:\n        ewm['ewm_mean_' + str(w)] = s.ewm(span=w, min_periods=1).mean()\n        ewm['ewm_std_' + str(w)] = s.ewm(span=w, min_periods=1).std()\n\n    ewm = ewm.fillna(value=0)\n        \n    return ewm","afa1cc82":"def add_features(s):\n\n    gradients = calc_gradients(s)\n    low_pass = calc_low_pass(s)\n    high_pass = calc_high_pass(s)\n    roll_stats = calc_roll_stats(s)\n    ewm = calc_ewm(s)\n    shift = create_shift(s)\n    \n    return pd.concat([s, gradients, low_pass, high_pass, roll_stats, ewm, shift], axis=1)","dce48122":"def divide_and_add_features(s, signal_size=500000):\n\n    s = s\/max(s.max(),-s.min())\n    ls = []\n    for i in tqdm(range(int(s.shape[0]\/signal_size))):\n        sig = s[i*signal_size:(i+1)*signal_size].copy().reset_index(drop=True)\n        sig_featured = add_features(sig)\n        ls.append(sig_featured)\n\n    df = pd.concat(ls, axis=0)\n\n    return df","f48ae8ef":"def reduce_mem_usage(df, verbose=True):\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in tqdm(df.columns):\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","0f12f713":"train = divide_and_add_features(train['signal'])\ntrain = reduce_mem_usage(train)","7c37778b":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.2, random_state=seed)","2466a8cf":"def plot_cm(y_true, y_pred, title):\n    size = 13\n    figsize=(size,size)\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","12cd6547":"def get_class_weight(classes, exp=1):\n    hist, _ = np.histogram(classes, bins=np.arange(12)-0.5)\n    class_weight = hist.sum()\/np.power(hist, exp)\n    return class_weight\n\nclass_weight = get_class_weight(y_train)\nencoder = LabelEncoder()\nencoder = encoder.fit(y_train)\ny_train = encoder.transform(y_train)\nX_test = X_test.to_numpy()\nX_train = X_train.to_numpy()\n    \nmodel = Sequential()\nmodel.add(Dense(77, activation=\"relu\"))\nmodel.add(Dense(8, activation=\"relu\"))\nmodel.add(Dense(102, activation=\"relu\"))\nmodel.add(Dense(174, activation=\"relu\"))\nmodel.add(Dense(18, activation=\"relu\"))\nmodel.add(Dense(30, activation=\"relu\"))\nmodel.add(Dense(11, activation=\"softmax\"))\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_crossentropy'])\nmodel.fit(X_train, y_train, batch_size=1024, epochs=35, verbose=1, class_weight=class_weight)\n\npreds=model.predict(X_test)\n\ncnn_preds=[]\nfor j in range(preds.shape[0]):\n    cnn_preds.append(np.argmax(preds[j]))","9c7ae607":"plot_cm(y_test, cnn_preds, \"NN\")","4cfaa7ba":"catb = CatBoostClassifier(random_seed=seed, verbose=0,loss_function='MultiClass', iterations=3000, learning_rate=0.07, task_type=\"GPU\")\ncatb.fit(X_train, y_train)\npreds_class = catb.predict(X_test, prediction_type='Class')\npreds_real = np.apply_along_axis(lambda x: x[0], 1, preds_class)","a8742135":"plot_cm(y_test, preds_real, \"Catboost\")","b17a908a":"params = {'num_leaves': 475, \n               'max_depth': 91, \n               'max_bin': 230, \n               'min_data_in_leaf': 215, \n               'min_data_in_bin': 173, \n               'min_gain_to_split': 3.99, \n               'lamda_l1': 0.0004829498900910097, \n               'lamda_l2': 1.3150995937706052e-06, \n               'learning_rate': 0.08575524059926458, \n               'metric': 'RMSE', \n               'bagging_fraction': 0.46527611789576895, \n               'feature_fraction': 0.39877736376091427, \n               'bagging_freq': 3}\n\ndataset = lgb.Dataset(X_train,label=y_train)\ngbc = lgb.train(params, dataset)\ngbc_pred = gbc.predict(X_test).tolist()\ngbc_pred = list(map(lambda x: round(x),gbc_pred))","8bb4867a":"plot_cm(y_test, gbc_pred, \"LGBM\")","b9a2f834":"catb = CatBoostClassifier(random_seed=seed, verbose=0,loss_function='MultiClass', iterations=3200, learning_rate=0.02,task_type=\"GPU\")\ncatb.fit(X_train, y_train)\npreds_class = catb.predict(X_test, prediction_type='Class')\npreds_real = np.apply_along_axis(lambda x: x[0], 1, preds_class)","4f0b6c40":"plot_cm(y_test, preds_real, \"Catboost high\")","2fffa871":"# Preprocessing\/Feature Engineering\n<b><font size=\"4\">Importing libraries<\/font><\/b>","2ab3dfbb":"<b><font size=\"4\">Catboost<\/font><n><\/n><\/b>\n\nIn my opinion, it's the most powerful part of this models stack. We couldn't add more parameters to Catboost cause we used GPU to spend less time while training.","0571cfc7":"![lgbm_cr.png](attachment:lgbm_cr.png)","fbdd18ad":"Then let's add some statistical and physical functions:\n1. **calc_gradients**: This function calculate gradients for a pandas series. Returns the same number of samples\n2. **calc_low_pass**: applies low pass filters to the signal. Left delayed and no delayed. \n3. **calc_high_pass**: applies high pass filters to the signal. Left delayed and no delayed\n4. **calc_roll_stats**: calculates rolling stats like mean, std, min, max, quantiles... The number of windows, and specifically the functions themselves were chosen empirically to have gain the best performance.\n6. **create_shift**: creates shifts, we choosed only +-1,2,3 because others did not give a sufficient increase in performance.\n7. **calc_ewm**: calculates exponential weighted functions.\n8. **add_features**: concatenates all calculations together\n9. **divide_and_add_features**: dividing the signal in bags of \"signal_size\".Then normalize the data dividing it by 15.0.\n10. **reduce_mem_usage**: apply this function and save quite a lot of memory.","e69fa1f9":"![nn_cr.png](attachment:nn_cr.png)","51ca2fce":"![catb_cr.png](attachment:catb_cr.png)","0018d291":"<b><font size=\"4\">Stacking: Part 2<\/font><n><\/n><\/b>\n\nWhile I was tuning Catboost I've found my model has got slightly better performance on the high labels(6+). So I've decided make \"Smart Stacking\" on high predictions.\n<n> <\/n>\"Smart Stacking\" alhoritm:\n1. Stack our base models, in my case I averaged them\n2. Create a mask on target \"target[target>5]\" to select only 6+ labels\n3. Apply this mask also to train set\n4. Predict our masked train set with model for high predictions \n5. Concatenate our masked predictions with old predictions","1da17ac7":"<b><font size=\"4\">Reading Train Data<\/font><n><\/n><\/b>\n\nIn this kernel I'm using already cleaned data without drift, so I won't pay attention on the preprocessing. You can check it in discussions.","6b583c25":"<b><font size=\"4\">NN<\/font><n><\/n><\/b>\n\nWe also choose construction of this NN by ourselves, but to improve it, you can try to use more epochs and play with lr and etc.","c85bca8d":"<b><font size=\"4\">Summary<\/font><n><\/n><\/b>\n\nIn this kernel main idea was to show my own method of stacking, very powerful tool as Catboost (cause I really can't understand why people doesn't use it as often as XGBoost or LGBM). Also sometimes it's useful to make EDA, which shows you not only plots and will give you clues to the for better solution.\nSo if this kernel brings you some new information please upvote it! Write your comments because I'm only studying and it'd be pleasant for me to hear some advices from you.","c01978ad":"<b><font size=\"5\">Intro<\/font><n><\/n><\/b>\n\nIn this notebook I will describe my solution of \"University of Liverpool - Ion Switching\"\nThis notebook was made by me and my mate - [Andriy Samoshin](https:\/\/www.kaggle.com\/mrmorj****). We're students of 2nd year KPI, IASA (located in Kyiv, Ukraine)","81739b89":"# Modeling\nThis is the most interesting part as for me. Here I'll show Double Blending, and some interesting insights with Confussion Matrix.\nBecause of some lack of time we used train test split, I know, it would be much better to use CV, but we did't want to spend ~1 hour for one CV, so we choosed optimal X_train and X_test to have got the same distribution.","4fb4d329":"<b><font size=\"4\">LGBM<\/font><n><\/n><\/b>\n\nAnd the last is LGBM, we din't choose XGBoost, because LGBM is quite fast and hasn't got much difference in comparison with XGB (in performance). Also LGBM was tuned with Optuna. I'll show params of tuned LGBM","438fc92b":"After we've trained these model, we just averaged all results and then round them. It's quite easy method, works as Voting Classifier.","e9693ff0":"<b><font size=\"4\">Stacking: Part 1<\/font><n><\/n><\/b>\n\nWe used 3 models to make a stack with them: LGBM, Catboost, NN. I will show their structure and Confussion Matrix, but to save time, I will not run them in this kernel.","05862488":"![catb_high_cr.png](attachment:catb_high_cr.png)"}}