{"cell_type":{"5cb4e7a3":"code","8eaa71af":"code","7bc8727e":"code","b50bd88b":"code","9d1d2109":"code","25f9c877":"code","b3ba078a":"code","beaf90e8":"code","5870a2e8":"code","9fdac247":"code","dd73456b":"code","24b06995":"code","aaafbe01":"code","af629bb6":"code","68916f18":"code","60873d11":"code","8d5a2830":"code","fc9a996d":"code","ac20261b":"code","3bea81d5":"code","904d25a5":"code","e6c9f15e":"code","612d9c78":"code","3c653b3c":"code","e9eec12e":"code","a85f6b2d":"code","29f27b1d":"code","b2307f82":"code","0db8591c":"code","e5ea5de0":"code","5d415ff6":"code","c66d5c25":"code","e30cc660":"code","10a73fcd":"markdown","f7377e70":"markdown","30906143":"markdown","5518dc56":"markdown","39621c74":"markdown","501c397d":"markdown","0010ba70":"markdown","9924b161":"markdown","7619e6da":"markdown","1b8f856a":"markdown","678f091e":"markdown"},"source":{"5cb4e7a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8eaa71af":"data = pd.read_csv('\/kaggle\/input\/nba2k20-player-dataset\/nba2k20-full.csv')\ndata.head()","7bc8727e":"# convert b_day column to get just year\ndata['b_day'] = pd.to_datetime(data['b_day']).dt.year\n\n# get height in meters\ndata['height'] = pd.to_numeric(data['height'].apply(lambda x: x.split('\/')[1]))\n\n# get weight in kgs\ndata['weight'] = pd.to_numeric(data['weight'].apply(lambda x: x.split('\/')[1].split('kg')[0]))","b50bd88b":"data.info()","9d1d2109":"# convert salary to int64 by removing $ sign\ndata['salary'] = pd.to_numeric(data['salary'].apply(lambda x: x[1:]))\n\n# convert draft_peak to int64 by replacing 'Undrafted' to 0\ndata['draft_peak'] = pd.to_numeric(data['draft_peak'].replace('Undrafted', 0))\n\n# update position column (determined in EDA)\ndata['position'] = data['position'].map({'F':'F', 'F-G': 'F-G', 'G':'G', 'F-C':'F-C', 'C':'C', 'G-F':'F-G', 'C-F':'F-C'})","25f9c877":"# check distribution of target\nsns.histplot(data['salary'])","b3ba078a":"def plot_numerical(feature):\n    sns.lmplot(x=feature, y='salary', data=data)\n    plt.show()\n    \ndef plot_categorical(feature, figsize=None):\n    df = data.groupby([feature])['salary'].describe()[['mean', '50%', 'min', 'count']]\n\n    labels = df.index.values\n    x = np.arange(len(labels))\n    width = 0.9\n    fig, ax1 = plt.subplots(figsize=(18, 5))\n\n    # plot bars for min, median and mean salary\n    rects1 = ax1.bar(x-width\/2, df['50%'], width\/3, label='median')\n    rects2 = ax1.bar(x-width\/6, df['mean'], width\/3, label='mean')\n    rects3 = ax1.bar(x+width\/6, df['min'], width\/3, label='min')\n\n    ax1.set_ylabel('Salary', fontsize=12)\n    ax1.set_title(feature, fontsize=15)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=90)\n    ax1.legend()\n\n    # plot counts of data points\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Counts', fontsize=12)\n    ax2.plot(x-width\/2, df['count'], color='red', linestyle='dashed')\n\n    # annotate counts of data points\n    for i, rect in enumerate(rects2):\n        height = int(round(rect.get_height()))\n        ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                     xy=(rect.get_x() + rect.get_width()\/2, height),\n                     xytext=(0, 3), textcoords=\"offset points\",\n                     ha='center', va='bottom', color='red')\n    plt.show()","beaf90e8":"for feature in ['rating']:\n    plot_numerical(feature)","5870a2e8":"for feature in ['jersey', 'team', 'college', 'country']:\n    plot_categorical(feature)","9fdac247":"for feature in ['position']:\n    plot_categorical(feature)","dd73456b":"for feature in ['b_day', 'height', 'weight']:\n    plot_numerical(feature)","24b06995":"for feature in ['draft_year', 'draft_peak']:\n    plot_numerical(feature)","aaafbe01":"plot_categorical('draft_round')","af629bb6":"categorical_features = ['jersey', 'team', 'position', 'country', 'draft_round', 'college']\nnumerical_features = ['rating', 'b_day', 'height', 'weight', 'draft_year', 'draft_peak']\nto_drop = ['full_name'] # contain all unique values","68916f18":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","60873d11":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","8d5a2830":"# Bivariate Analysis Correlation plot with the Numeric variables\nplt.figure(figsize=(5, 5))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","fc9a996d":"# Bivariate Analysis Correlation plot with the Categorical variables\nplt.figure(figsize=(10, 10))\nsns.heatmap(round(df[categorical_features+numerical_features+['salary']].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()","ac20261b":"from statsmodels.stats.outliers_influence import variance_inflation_factor","3bea81d5":"# Calculating VIF\nvif = pd.DataFrame()\ntemp = df.dropna()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['team', 'college', 'draft_year', \n                                                                                                      'height', \n                                                                                                      'weight', 'b_day', 'country']]\nvif[\"VIF\"] = [variance_inflation_factor(temp[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","904d25a5":"missingValueFeatures = pd.DataFrame({'missing %': data.isnull().sum()*100\/len(data)})\nmissingValueFeatures[missingValueFeatures['missing %']>0]","e6c9f15e":"# update categorical features to use only quality features using vif and correlation observations\n# jersey is ignored as it is very sparsed data and also due to lack of good no. of data points\ncategorical_features = ['position', 'draft_round']","612d9c78":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, \n                    open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","3c653b3c":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['salary']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","e9eec12e":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor","a85f6b2d":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['b_day', 'salary', 'height', 'weight',  'draft_peak'])]\n\n''' Rescaling to [0,1] '''\nscaler = StandardScaler()\nscaler.fit(train_data[feature_cols])\ntrain_data[feature_cols] = scaler.transform(train_data[feature_cols])","29f27b1d":"X = train_data[feature_cols]\ny = train_data['salary']\n\nvalidation_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, \n                                                    random_state=0)","b2307f82":"model = LinearRegression()\nmodel.fit(X_train, y_train)","0db8591c":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('Accuracy: ', round(model.score(X_train, y_train)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Accuracy: ', round(model.score(X_test, y_test)*100, 2))","e5ea5de0":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred))), y=y_pred,\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test))), y=y_test,\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","5d415ff6":"model = XGBRegressor( \n    n_estimators = 300,\n    learning_rate=0.2, \n    min_child_weight=3,\n    max_depth = 2,\n    subsample = 0.75,\n    seed=0)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    early_stopping_rounds=20,\n    eval_set=[(X_test,y_test)],\n    verbose=False)","c66d5c25":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('Accuracy: ', round(model.score(X_train, y_train)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Accuracy: ', round(model.score(X_test, y_test)*100, 2))","e30cc660":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred))), y=y_pred,\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test))), y=y_test,\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","10a73fcd":"# Label encoding categorical features for correlation","f7377e70":"# Removing features using VIF","30906143":"# Handling Categorical Features (Label Encoding & One Hot Encoding)","5518dc56":"# CORRELATION","39621c74":"# Training Model","501c397d":"# Handling Missing Values","0010ba70":"**Observations:**\n* b_day - draft_year\n* height - weight\n* position - height and weight (-ve)","9924b161":"team and college are not correlated to any other feature and are highly uncorrelated to each other as well. As the colleg or team are not affecting the salary much, we may drop them while training model","7619e6da":"# Model 1: Linear Regression","1b8f856a":"# EDA","678f091e":"**Columns that are contributing towards high Salary-**\n* rating - +ve\n* position - 'C' position gets the highest salary\n* b_day - aged players are getting higher salary\n* draft_year - newer the player lesser is the salary\n* draft_round - players drafted in 1st round get higher salary while Undrafted get the least\n\n\n\n"}}