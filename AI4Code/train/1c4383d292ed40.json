{"cell_type":{"d195779b":"code","1b0fa69e":"code","448743b3":"code","22357f9c":"code","b214aef1":"code","df321bc7":"code","90e1950a":"code","08b9c14f":"code","2822f461":"code","d7b825c0":"code","64c345bf":"code","a6f6dab2":"code","7e8a6172":"code","112b332f":"code","6590455e":"code","d37f79e4":"code","0fd3447f":"code","0c5b8c51":"code","a8d59983":"code","73de32d6":"code","bd34bae9":"code","e111dccc":"code","b8aa4fb4":"code","1d1b93d5":"code","017e732f":"code","5d54ee36":"code","71d2e36f":"code","ae01b96a":"code","5c59df27":"code","4698965b":"code","eeb099e3":"code","fd16abf3":"code","76061944":"code","a5f1562e":"code","e51d3384":"code","39312f29":"code","ea1cdba7":"code","62994036":"code","5989cd00":"code","2a0ee356":"code","409b8f66":"code","0b8a39a0":"code","1d0f6a10":"code","baf2e6c5":"code","2acad67b":"code","75011d23":"markdown","f3e738e2":"markdown","c196e8fd":"markdown","3766eea1":"markdown","f20a27a5":"markdown","017808d9":"markdown","ae2b5349":"markdown","03c08acc":"markdown","ef837146":"markdown","a2397cf2":"markdown","e3b40d7d":"markdown","ef4d2cb9":"markdown","cd5d1d8c":"markdown","820cc2bc":"markdown","27d426e6":"markdown","e10476db":"markdown","08b5078e":"markdown","5fe2ba9c":"markdown"},"source":{"d195779b":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix , classification_report\nfrom sklearn.metrics import roc_curve , auc\nfrom sklearn.svm import SVC\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1b0fa69e":"# Load Data\ndf = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","448743b3":"df.head()","22357f9c":"df.info()","b214aef1":"df.SeniorCitizen.unique()","df321bc7":"#Convert to Categorical variable\ndf.SeniorCitizen= df.SeniorCitizen.apply(lambda x : 'No' if x == 0 else 'Yes')\n","90e1950a":"#Check Type after conversion\ndf.SeniorCitizen.unique()","08b9c14f":"df['TotalCharges_new']= pd.to_numeric(df.TotalCharges,errors='coerce')","2822f461":"#Check NULL values after the conversion\ndf.loc[pd.isna(df.TotalCharges_new),'TotalCharges']","d7b825c0":"#Fill 11 Missing values from the original column\nTotalCharges_Missing=[488,753,936,1082,1340,3331,3826,4380,5218,6670,6754]\ndf.loc[pd.isnull(df.TotalCharges_new),'TotalCharges_new']=TotalCharges_Missing\n","64c345bf":"#We are good to replace old columns with the new numerical column\ndf.TotalCharges=df.TotalCharges_new\ndf.drop(['customerID','TotalCharges_new'],axis=1,inplace=True)\ndf.info()","a6f6dab2":"df.dtypes=='object'\ncategorical_var=[i for i in df.columns if df[i].dtypes=='object']\nfor z in categorical_var:\n    print(df[z].name,':',df[z].unique())","7e8a6172":"Dual_features= ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\nfor i in Dual_features:\n    df[i]=df[i].apply(lambda x: 'No' if x=='No internet service' else x)\n#Remove No Phones Service that equivilent to No for MultipleLines\ndf.MultipleLines=df.MultipleLines.apply(lambda x: 'No' if x=='No phone service' else x)\n","112b332f":"#Check levels or all Categorical Variables\nfor z in [i for i in df.columns if df[i].dtypes=='object']:\n    print(df[z].name,':',df[z].unique())","6590455e":"continues_var=[i for i in df.columns if df[i].dtypes !='object']\nfig , ax = plt.subplots(1,3,figsize=(15,5))\nfor i , x in enumerate(continues_var):\n    ax[i].hist(df[x][df.Churn=='No'],label='Churn=0',bins=30)\n    ax[i].hist(df[x][df.Churn=='Yes'],label='Churn=1',bins=30)\n    ax[i].set(xlabel=x,ylabel='count')\n    ax[i].legend()","d37f79e4":"fig , ax = plt.subplots(1,3,figsize=(15,5))\nfor i , xi in enumerate(continues_var):\n    sns.boxplot(x=df.Churn,y=df[xi],ax=ax[i],hue=df.gender)\n    ax[i].set(xlabel='Churn',ylabel=xi)\n    ax[i].legend()\n","0fd3447f":"#Remove Churn Variable for Analysis\ncategorical_var_NoChurn= categorical_var[:-1]","0c5b8c51":"#Count Plot all Categorical Variables with Hue Churn\nfig , ax = plt.subplots(4,4,figsize=(20,20))\nfor axi , var in zip(ax.flat,categorical_var_NoChurn):\n    sns.countplot(x=df.Churn,hue=df[var],ax=axi)","a8d59983":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder=LabelEncoder()\nfor x in [i for i in df.columns if len(df[i].unique())==2]:\n    print(x, df[x].unique())\n    df[x]= label_encoder.fit_transform(df[x])","73de32d6":"#Check Variables after Encoding\n[[x, df[x].unique()] for x in [i for i in df.columns if len(df[i].unique())<10]]","bd34bae9":"#Encode Variables with more than 2 Classes\ndf= pd.get_dummies(df, columns= [i for i in df.columns if df[i].dtypes=='object'],drop_first=True)\n  ","e111dccc":"#Check Variables after Encoding\n[[x, df[x].unique()] for x in [i for i in df.columns if len(df[i].unique())<10]]","b8aa4fb4":"#Create Features DataFrame\nX=df.drop('Churn',axis=1)\n#Create Target Series\ny=df['Churn']\n#Split Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","1d1b93d5":"#Scale Data\nsc= StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_train=pd.DataFrame(X_train,columns=X.columns)\nX_test=sc.transform(X_test)","017e732f":"#Check Data after Scaling\nX_train.head()","5d54ee36":"#Apply RandomForest Algorethm\nrandom_classifier= RandomForestClassifier()\nrandom_classifier.fit(X_train,y_train)","71d2e36f":"y_pred= random_classifier.predict(X_test)","ae01b96a":"#Classification Report\nprint(classification_report(y_test,y_pred))","5c59df27":"#Confusion Matrix\nmat = confusion_matrix(y_test, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","4698965b":"#get features Importances\nxx= pd.Series(random_classifier.feature_importances_,index=X.columns)\nxx.sort_values(ascending=False)","eeb099e3":"y_pred_proba=random_classifier.predict_proba(X_test)[:,1]","fd16abf3":"fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\nroc_auc=auc(fpr,tpr)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('Random Forest ROC curve')\nplt.legend(loc='best')\n","76061944":"svm_classifier= SVC(probability=True)\nsvm_classifier.fit(X_train,y_train)","a5f1562e":"#Predict\ny_pred_svm= svm_classifier.predict(X_test)\n#Classification Report\nprint(classification_report(y_test,y_pred_svm))","e51d3384":"#Confusion Matrix\nmat_svm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(mat_svm.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","39312f29":"y_pred_svm_proba=svm_classifier.predict_proba(X_test)[:,1]\n#ROC Curve\nfpr_svm, tpr_svm, _svm = roc_curve(y_test, y_pred_svm_proba)\nroc_auc=auc(fpr_svm,tpr_svm)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('SVM ROC curve')\nplt.legend(loc='best')\n","ea1cdba7":"#Initiate ANN Classifier\nann_classifier= Sequential()\nX.shape","62994036":"#Adding Hidden Layer1\nann_classifier.add(Dense(12,activation='relu',kernel_initializer='uniform',input_dim=23))\n#Adding Hidden Layer2\nann_classifier.add(Dense(12,activation='relu',kernel_initializer='uniform'))\n#Adding output Layer\nann_classifier.add(Dense(1,activation='sigmoid',kernel_initializer='uniform'))\n#Compile them Model\nann_classifier.compile(optimizer='adam',loss='binary_crossentropy', metrics = ['accuracy'])","5989cd00":"ann_classifier.summary()","2a0ee356":"%time ann_classifier.fit(X_train,y_train,batch_size=10,epochs=100)","409b8f66":"#Get Prediction Proba\ny_pred_ann_proba= ann_classifier.predict(X_test)","0b8a39a0":"#Convert Prediction to Int\ny_pred_ann= (y_pred_ann_proba>.5).astype('int')","1d0f6a10":"#Priint Classification Report\nprint(classification_report(y_test,y_pred_ann))","baf2e6c5":"#Confusion Matrix\nmat_ann = confusion_matrix(y_test, y_pred_ann)\nsns.heatmap(mat_ann.T, square=True, annot=True, fmt='d', cbar=False,\n          xticklabels=['No','Yes'],\n          yticklabels=['No','Yes'] )\nplt.xlabel('true label')\nplt.ylabel('predicted label')","2acad67b":"#Roc Curve\nfpr_ann,tpr_ann,_ann=roc_curve(y_test,y_pred_ann_proba)\nroc_auc=auc(fpr_ann,tpr_ann)\n#Now Draw ROC using fpr , tpr\nplt.plot([0, 1], [0, 1], 'k--',label='Random')\nplt.plot(fpr,tpr,label='ROC curve (area = %0.2f)' %roc_auc)\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')","75011d23":"We observe SeniorCitizen Should be categorical variables, but comes as int64. Will convert it back to categorical","f3e738e2":"## SVM Model\n\nNow will run SVM model and compare","c196e8fd":"There are some variables has value 'No Internet Service' that equivalent to 'No'. Will merge both values","3766eea1":"Variables Looks good now and we are ready for data splitting and scaling\n\n# Data Scaling and Splitting","f20a27a5":"SVM results is a little better than Random Forest. But not a huge improvement\n\nFinally will draw ROC Curve for this model\n","017808d9":"# Applying ML Models\n\n## Random Forest Model\n\nWill start by train Random forest model using default parameters and all variables and get initial results","ae2b5349":"Result are not bad as a start. Recall, and Precision of Churn='Yes' is not that good. We need to check features importance for the next tuning","03c08acc":"## ANN Model\n\nLastly, will build Artificial Neural Network (ANN) Model which theoretically should bring best result.\nWill build 2 Hidden Layers with 12 Nodes , Using Variate Function , and Output layer with one Node using Sigmoid Function. Will not run Cross Validation as a first run and will use Adam as optimizer with 100 epochs\n","ef837146":"We can see a real impact of all continues variables on Churn specially Tenue(Loyal Customers Stay)\n\nWill Check now Box Plot for more explorations","a2397cf2":"We need to use this list in next tuning of the model\n\nFinally will draw ROC curve for the model","e3b40d7d":"- We cannot see a real Impact of gender\n- Seniors are less loyalty\n- Partners are more loyal\n- Dependents are more loyal\n- Customers does not have multiplelines are more loyal\n- Customer are not happy with Optical Fiber and Leaving with rate of other internet services\n- Customers with month-to-month contract are more willing to leave than people with contracts\n- Paperless customers are more willing to leave that paper billing\n- Customer pay using electronic check is more willing to leave\n\nI Can conclude that mostly customers are suffering from the services , and specially advances customers who are using paperless billing and electronic payment. Some variables has no real impact of Churn but as a first trial for the model i will include all variables, should remove variables in the tuning phase","ef4d2cb9":"We can see a big improvement with ANN comparing with other models.\n\n## Next Step\n\nI will review deeply all variables, and start tune ANN models for better results\n\n\nI hope this Kernel is useful. Happy to receive your comments, questions, and advises\n\n","cd5d1d8c":"Now it is more clear the impact of Continues Variables on Churn , We can see minimal impact of Gender\n\nNow will convert to check regarding Categorical Variables","820cc2bc":"Another Observation that TotalCharges is continues variables and comes as object. Will convert to numeric format","27d426e6":"# Data Analysis and Visualizations\n\nFirst will analyze continues variables against Churn variable","e10476db":"Now will check all categorical variables levels","08b5078e":"# Objectives\n\nI am trying to build a churn model using three ML algorithms (Random Forest, SVM , and ANN). I am building as initial trial all models with default parameters without any tuning or Cross Validation. Will Check which is the best model for next steps of tuning\n","5fe2ba9c":"## Categorical Variables Encoding\n\nFor logistics variables(2 classes) will encode using Label Encoder , For Variables has more than 2 classes will use get_dummies function "}}