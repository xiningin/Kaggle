{"cell_type":{"c2e05dfd":"code","fdb4afd5":"code","dc00c961":"code","81b282bd":"code","ffde9088":"code","d4b21420":"code","6e7fae79":"code","b1821900":"code","b55d2f50":"code","cf359b23":"code","d868f246":"code","41355cab":"code","e6a5f2d0":"code","51d628c4":"code","e041266b":"code","4aee245f":"code","951cd644":"code","fec550b0":"code","e3a9bc2f":"code","6731c3b0":"code","f2a564b7":"code","8fb0e5ef":"code","d7659a53":"code","8fafdbf8":"code","c0e90ccc":"markdown","debea978":"markdown","2c95ec0a":"markdown","3467feb4":"markdown","002e0a2d":"markdown","a5d81791":"markdown","484962e2":"markdown","140569e4":"markdown","da8ce1da":"markdown","e256824f":"markdown","96e03793":"markdown","defb70c9":"markdown","d0fcca8a":"markdown","7806b62c":"markdown","f52d1206":"markdown","79a47766":"markdown","ee5ef935":"markdown","525c6a08":"markdown","55df3a18":"markdown","8b499d4e":"markdown","fa60dc67":"markdown","34b33206":"markdown","8207c45e":"markdown","6ef1a9bb":"markdown","131522c3":"markdown"},"source":{"c2e05dfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, download_plotlyjs, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint()\nprint(\"The files in the dataset are:-\")\nfrom subprocess import check_output\nprint(check_output(['ls','..\/input']).decode('utf'))\n\n# Any results you write to the current directory are saved as output.\n","fdb4afd5":"#Importing the datasets\ndf_train = pd.read_csv('..\/input\/Train.csv')\ndf_test = pd.read_csv('..\/input\/Test.csv')","dc00c961":"df_train.head()","81b282bd":"df_test.head()","ffde9088":"df_train.shape","d4b21420":"df_test.shape","6e7fae79":"try:\n    df_train.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\n    df_test.drop(labels=['Item_Identifier', 'Outlet_Identifier', 'Outlet_Establishment_Year'], axis=1, inplace=True)\nexcept Exception as e:\n    pass","b1821900":"df_train.head()","b55d2f50":"temp_df = df_train.isnull().sum().reset_index()\ntemp_df['Percentage'] = (temp_df[0]\/len(df_train))*100\ntemp_df.columns = ['Column Name', 'Number of null values', 'Null values in percentage']\nprint(f\"The length of dataset is \\t {len(df_train)}\")\ntemp_df","cf359b23":"def convert(x):\n    if x in ['low fat', 'LF']: \n        return 'Low Fat'\n    elif x=='reg':\n        return 'Regular'\n    else:\n        return x\n\ndf_train['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\ndf_test['Item_Fat_Content'] = df_train['Item_Fat_Content'].apply(convert)\n\nprint(f\"Now Unique values in this column in Train Set are\\t  {df_train['Item_Fat_Content'].unique()} \")\nprint(f\"Now Unique values in this column in Test Set are\\t  {df_test['Item_Fat_Content'].unique()} \")","d868f246":"# Counting the values\ncount = df_train['Outlet_Size'].value_counts().reset_index()\ncount.iplot(kind='bar', color='deepskyblue', x='index', y='Outlet_Size', title='High VS Mediun VS Small',\n           xTitle='Size', yTitle='Frequency')\n","41355cab":"df_train['Outlet_Size'].fillna(value='Medium', inplace= True)\ndf_test['Outlet_Size'].fillna(value='Medium', inplace= True)","e6a5f2d0":"# Let us Import the Important Libraries  to train our Model for Machine Learning \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder # To deal with Categorical Data in Target Vector.\nfrom sklearn.model_selection import train_test_split  # To Split the dataset into training data and testing data.\nfrom sklearn.model_selection import cross_val_score   # To check the accuracy of the model.\nfrom sklearn.preprocessing import Imputer   # To deal with the missing values\nfrom sklearn.preprocessing import StandardScaler   # To appy scaling on the dataset.","51d628c4":"# Let us create feature matrix and Target Vector.\nx_train = df_train.iloc[:, :-1].values    # Features Matrix\ny_train = df_train.iloc[:,-1].values   # Target Vector\nx_test = df_test.values    # Features Matrix","e041266b":"df_train.head()","4aee245f":"imputer = Imputer()\nx_train[:,[0]] = imputer.fit_transform(x_train[:,[0]])\nx_test[:,[0]] = imputer.fit_transform(x_test[:,[0]])\n","951cd644":"labelencoder_x = LabelEncoder()\nx_train[:, 1 ] = labelencoder_x.fit_transform(x_train[:,1 ])\nx_train[:, 3 ] = labelencoder_x.fit_transform(x_train[:,3 ])\nx_train[:, 5 ] = labelencoder_x.fit_transform(x_train[:,5 ])\nx_train[:, 6 ] = labelencoder_x.fit_transform(x_train[:,6 ])\nx_train[:, 7 ] = labelencoder_x.fit_transform(x_train[:,7 ])\n\n\n#this is need to done when we have more than two categorical values.\nonehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \nx_train = onehotencoder_x.fit_transform(x_train).toarray()\n\n# Let's apply same concept on test set.\nx_test[:, 1 ] = labelencoder_x.fit_transform(x_test[:,1 ])\nx_test[:, 3 ] = labelencoder_x.fit_transform(x_test[:,3 ])\nx_test[:, 5 ] = labelencoder_x.fit_transform(x_test[:,5 ])\nx_test[:, 6 ] = labelencoder_x.fit_transform(x_test[:,6 ])\nx_test[:, 7 ] = labelencoder_x.fit_transform(x_test[:,7 ])\n\n\n#this is need to done when we have more than two categorical values.\nonehotencoder_x = OneHotEncoder(categorical_features=[3,5,6,7]) \nx_test = onehotencoder_x.fit_transform(x_test).toarray()","fec550b0":"sc_X=StandardScaler()\nx_train=sc_X.fit_transform(x_train)\nx_test = sc_X.fit_transform(x_test)","e3a9bc2f":"from sklearn.decomposition import PCA\npca = PCA(n_components=None)\nx_train = pca.fit_transform(x_train)\nx_test = pca.fit_transform(x_test)\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","6731c3b0":"pca = PCA(n_components=25)\nx_train = pca.fit_transform(x_train)\nx_test = pca.fit_transform(x_test)","f2a564b7":"# Multi-linear regression Model.\nregressor_multi = LinearRegression()\nregressor_multi.fit(x_train,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_multi, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Multi-linear Regressor Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\")","8fb0e5ef":"\"\"\"# Random Forest Model.\nregressor_random = RandomForestRegressor(n_estimators=100,)\nregressor_random.fit(x_train,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_random, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Random Forest Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\") \"\"\"\n\nprint(\"Here accuray is 53% with deviation of 3%.\")","d7659a53":"\"\"\"\n# Fitting polynomial regression to dataset\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg=PolynomialFeatures(degree=4) #These 3 steps are to convert X matrix into X polynomial\nx_poly=poly_reg.fit_transform(x_train) #matrix. \nregressor_poly=LinearRegression()\nregressor_poly.fit(x_poly,y_train)\n\n# Let us check the accuray\naccuracy = cross_val_score(estimator=regressor_poly, X=x_train, y=y_train,cv=10)\nprint(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\nprint(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n\"\"\"\nprint(\"Here accuracy is 55% with deviation of 2%\")","8fafdbf8":"y_pred = regressor_multi.predict(x_test)\n\ny_pred[:5]","c0e90ccc":"#### 1). Removing Unwanted Columns\/Features.","debea978":"#### observation:-\n* As the accuracy of Multi-linear regression Model is the best one.\n* Multi-linear Regression Model takes less time as compare to Random forest and Polynomial regression Models.\n* We will choose Multi-linear regression Model.\n* Here we are getting the accuracy of 55% and deviation of 2%, means in future if we mak eprediction on new values then we will get the accuracy in range 53% to 57%.\n* We are getting low accuracy due to less quantity of data.","2c95ec0a":"### 2). Dealing With the Categorical Values in Features\/Columns.","3467feb4":"#### In this Notebook we will Learn:-\n* Basic EDA.\n* Feature Engineering\n* Dealing with missing values.\n* Aplly Scaling on Feature matrix.\n* Dealing with Categorical Dataset.\n* Dimensionality Reduction (PCA) .\n* K-Cross validation to check accuracy.\n* Multi-linear Regression\n* Random Forest Regressor\n* Polynomial Regression\n* Prediction on new Values.","002e0a2d":"# BASIC ANALYSIS AND FEATURES ENGINEERING","a5d81791":"# IF THIS KERNEL IS HELPFUL, THEN PLEASE UPVOTE.\n<img src='https:\/\/drive.google.com\/uc?id=1LBdaJj2pTM0cq9PY6k70RaGfUFDakUzG' width=500 >","484962e2":"# PREDICTION WITH REGRESSION MODELS.","140569e4":"* We will remove the missing values from 'Medium' in both Training set and Test set.","da8ce1da":"### ===============================================================================","e256824f":"* So it is clear that we do not have to remove null values, as they 28% and 17% in the Outlet_Size and Item_Weight Columns respectively.\n* Null values are in less quantity.\n* We will replace them later with thier mean or mode values.","96e03793":"#### 4). Dealing with the Missing Values in Categorical type column i.e. 'Outlet_Size'","defb70c9":"### Let us make Prediction on test set","d0fcca8a":"### 5). Apply Multi-Linear Regression Model, Polynomial Regression and Random Forest Model and compare thier accuracy and pick the best one.","7806b62c":"<img src='https:\/\/drive.google.com\/uc?id=1-6z0sZc9YrK_czjy8mBQuxBj3wdD01-V' width=800 >","f52d1206":"### ============================================================================\n### ============================================================================\n### ============================================================================\n### ============================================================================","79a47766":"#### 2). Getting Information about Null values,","ee5ef935":"### 1). Dealing with Missing data.","525c6a08":"#### Multi-Linear Regression","55df3a18":"#### Random Forest Model","8b499d4e":"# REGRESSION From Scratch With SALES PREDICTION","fa60dc67":"### 3). Now time to Apply Feature Scaling on Feature matrix .","34b33206":"### 4). DIMENSIONALITY REDUCTION\n* We are doing this to reduce the number of dimensions\/features in the dataset.\n* The features which have less effect on the prediction , we will remove those features.\n* It also boosts the process.\n* It saves time.\n* Here we will use Principal Component Analysis (PCA) with 'rbf' kernel.","8207c45e":"#### 3). Making Correction in 'Item_Fat_Content' column.","6ef1a9bb":"* Here we will take n_component = 24.","131522c3":"#### Polynomial regression"}}