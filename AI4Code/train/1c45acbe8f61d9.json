{"cell_type":{"6d97a408":"code","301e355a":"code","44e86290":"code","10fef2d8":"code","6a17988b":"code","fd43b564":"code","6a55b4e9":"code","5b855f87":"code","32b7b5e1":"code","6245b09f":"code","c2be3785":"code","be1ef23c":"code","ccc3cf81":"code","28de131b":"code","6661dc16":"code","50366281":"code","265bce26":"code","38f52077":"code","cb6cf7d3":"code","0b396f28":"code","68292a2c":"code","a7abca6a":"code","2fccd2e8":"code","c3f58ed6":"code","f5ef86b8":"code","fc16fa0e":"code","a3f45b2f":"code","f6e9f6b8":"code","93035054":"markdown","322ab7e4":"markdown","f442df49":"markdown","cfef56b9":"markdown","3252bb9c":"markdown"},"source":{"6d97a408":"import numpy as np \nimport pandas as pd \nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\n%matplotlib notebook\n\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nfrom lime.lime_text import LimeTextExplainer\nfrom tqdm import tqdm\nimport string\nimport random\nimport operator\nimport seaborn as sns\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom statistics import *\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\nimport warnings\nimport nltk\n\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\n\n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n\n# set seeds for reproducability\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(2)\nseed(1)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","301e355a":"#list of data that we have in the workspace\n\nprint(os.listdir(\"..\/input\"))","44e86290":"# countries that use English as an official language\nbritish_youtube = pd.read_csv(\"..\/input\/GBvideos.csv\")\ncanadian_youtube = pd.read_csv(\"..\/input\/CAvideos.csv\")\nus_youtube = pd.read_csv(\"..\/input\/USvideos.csv\")\n","10fef2d8":"#combine tables\nthree_countries=pd.concat([canadian_youtube, british_youtube,us_youtube])\nthree_countries.shape\n","6a17988b":"#remove duplicate\nthree_countries= three_countries.drop_duplicates(['video_id'], keep='first')\n","fd43b564":"#need to be decoded \nthree_countries.category_id.head()","6a55b4e9":"import json\n\ndef category_name(path):\n    with open(path) as json_file:  \n        data = json.load(json_file)\n    category_info_list=[]\n    for row in data['items']:\n        id_info=row['id']\n        category_name=row['snippet']['title']\n        categoty_info=(id_info ,category_name)\n        category_info_list.append(categoty_info)\n    return(dict(category_info_list))\n        \n    ","5b855f87":"category_name(\"..\/input\/CA_category_id.json\")","32b7b5e1":"category_list=category_name(\"..\/input\/CA_category_id.json\")\ncategory_names=[]\nfor i in three_countries.category_id:\n    category_name=category_list.get(str(i))\n    category_names.append(category_name)\n\nthree_countries['category_names']=category_names","6245b09f":"#now, we have category name :)\nthree_countries['category_names'].head()","c2be3785":"three_countries.info()","be1ef23c":"entertainment_title= three_countries[\"title\"][(three_countries['category_names'] == 'Entertainment')] \nnews_politics_title= three_countries[\"title\"][(three_countries['category_names'] == 'News & Politics')] \npeople_title= three_countries[\"title\"][(three_countries['category_names'] == 'People & Blogs')] \nmusic_title= three_countries[\"title\"][(three_countries['category_names'] == 'Music')] \nsports_title= three_countries[\"title\"][(three_countries['category_names'] == 'Sports')] \ncomedy_title= three_countries[\"title\"][(three_countries['category_names'] == 'Comedy')] \n\ntitles_to_include = three_countries[\"title\"][(three_countries['category_names'] != 'Music')]\ntitles_to_include.head()\ntitles_to_include.count()","ccc3cf81":"\nQ1 = three_countries.views.quantile(0.25)\nQ2 = three_countries.views.quantile(0.5)\nQ3 = three_countries.views.quantile(0.75)\nIQR = Q3 - Q1\nprint(Q1)\nprint(Q2)\nprint(Q3)\n\n#popular_videos=three_countries.loc[three_countries.views > (Q3 + 1.5 * IQR)]\npopular_videos=three_countries.loc[three_countries.views > (Q2)]\n\nthree_countries['popular']=0\n#three_countries.loc[three_countries.views > (Q3 + 1.5 * IQR),'popular']=1\nthree_countries.loc[three_countries.views > (Q2),'popular']=1\n\nthree_countries['popular'].value_counts()","28de131b":"#make a variable that tells ratio of like and dislike\nthree_countries['like_percentage']=(three_countries['likes']\/(three_countries['likes']+three_countries['dislikes'])*100)\n#date column as datatime datatype\nthree_countries[\"publish_time\"] = pd.to_datetime(three_countries[\"publish_time\"])","6661dc16":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nparser = English()\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\n\n#normal = three_countries[\"title\"][three_countries[\"popular\"] == 0].progress_apply(spacy_tokenizer)\npopular = three_countries[\"title\"][three_countries[\"popular\"] == 1].apply(spacy_tokenizer)\n\n","50366281":"#tokenize words by popularity \n\ndef word_generator(text):\n    word = list(text.split())\n    return word\ndef bigram_generator(text):\n    bgram = list(nltk.bigrams(text.split()))\n    bgram = [' '.join((a, b)) for (a, b) in bgram]\n    return bgram\ndef trigram_generator(text):\n    tgram = list(nltk.trigrams(text.split()))\n    tgram = [' '.join((a, b, c)) for (a, b, c) in tgram]\n    return tgram\n\n\n#normal_words = normal.apply(word_generator)\npopular_words = popular.apply(word_generator)\n#normal_bigrams = normal.apply(bigram_generator)\npopular_bigrams = popular.apply(bigram_generator)\n#normal_trigrams = normal.apply(trigram_generator)\npopular_trigrams = popular.apply(trigram_generator)","265bce26":"#function that makes a pretty word frequency plot\n\ndef word_plot(words,my_color):\n    slist =[]\n    for x in words:\n        slist.extend(x)\n    fig = plt.figure(figsize=(15, 10))\n    pd.Series(slist).value_counts()[:20].sort_values(ascending=True).plot(kind='barh',fontsize=20, color=my_color)\n    plt.show()\n","38f52077":"# word_plot(popular_words,'blue')\n","cb6cf7d3":"word_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 1),\n    max_features=10000)\nword_vectorizer.fit(three_countries.title)\nword_features = word_vectorizer.transform(three_countries.title)\n\nclassifier_popular = LogisticRegression(C=0.1, solver='sag')\nclassifier_popular.fit(word_features ,three_countries.popular)\n","0b396f28":"names=['normal','popular']","68292a2c":"c_tf = make_pipeline( word_vectorizer,classifier_popular)\nexplainer_tf = LimeTextExplainer(class_names=names)\n\nexp = explainer_tf.explain_instance(three_countries.title.iloc[10], c_tf.predict_proba, num_features=4, top_labels=1)\nexp.show_in_notebook(text=three_countries.title.iloc[10])\n","a7abca6a":"entertainment_title= three_countries[\"title\"][(three_countries['category_names'] == 'Entertainment')] \nnews_politics_title= three_countries[\"title\"][(three_countries['category_names'] == 'News & Politics')] \npeople_title= three_countries[\"title\"][(three_countries['category_names'] == 'People & Blogs')] \nmusic_title= three_countries[\"title\"][(three_countries['category_names'] == 'Music')] \nsports_title= three_countries[\"title\"][(three_countries['category_names'] == 'Sports')] \ncomedy_title= three_countries[\"title\"][(three_countries['category_names'] == 'Comedy')] ","2fccd2e8":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(popular)\ninp_sequences[:10]","c3f58ed6":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","f5ef86b8":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    \n    # Add Hidden Layer 1 - LSTM Layer\n    model.add(LSTM(100))\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","fc16fa0e":"model.fit(predictors, label, epochs=5, verbose=5)\n","a3f45b2f":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","f6e9f6b8":"print (generate_text(\"Drake\", 7, model, max_sequence_len))\n#print (generate_text(\"united states\", 5, model, max_sequence_len))\n# print (generate_text(\"Bangtan\", 4, model, max_sequence_len))\n# print (generate_text(\"Fergie\", 4, model, max_sequence_len))\n# print (generate_text(\"korea\", 4, model, max_sequence_len))\n# print (generate_text(\"Minnesota\", 4, model, max_sequence_len))","93035054":"# Generating titles by lstm\n\n> Language Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word based on the previous sequence of words used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. In this notebook, I will explain how to create a language model for generating natural language text by implement and training state-of-the-art Recurrent Neural Network. ([source](https:\/\/medium.com\/phrasee\/neural-text-generation-generating-text-using-conditional-language-models-a37b69c7cd4b))\n<br> <br>\nLanguage modelling requires a sequence input data, as given a sequence (of words\/tokens) the aim is the predict next word.  \n\n","322ab7e4":"![](http:\/\/www.shivambansal.com\/blog\/text-lstm\/2.png)\n\nUnlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a \u2018memory state\u2019 of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n\nThe memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for the network to learn and tune the parameters of the earlier layers. To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed.\n\nLSTMs have an additional state called \u2018cell state\u2019 through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively. To learn more about LSTMs, here is a great post. Lets architecture a LSTM model in our code. I have added total three layers in the model.\n\nInput Layer : Takes the sequence of words as input\nLSTM Layer : Computes the output using LSTM units. I have added 100 units in the layer, but this number can be fine tuned later.\nDropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the LSTM layer. It helps in preventing over fitting. (Optional Layer)\nOutput Layer : Computes the probability of the best possible next word as output\nWe will run this model for total 50 epoochs but it can be experimented further.\n\n[source](http:\/\/www.shivambansal.com\/blog\/text-lstm\/2.png)","f442df49":"# **Finding Outlier Youtube Video**","cfef56b9":"# **EDA\/Data Cleaning**","3252bb9c":"### TfidfVectorizer\n\nTf-idf analyzes the impact of tokens (words) throughout the whole documents. For example, the more times a word appears in a document (each title), the more weight it will have. However, the more documents (titles) the word appears in, it is 'penalized' and the weight is diminished because it is empirically less informative than features that occur in a small fraction of the training corpus ([source](https:\/\/www.kaggle.com\/adamschroeder\/countvectorizer-tfidfvectorizer-predict-comments))\n\n* tf(t)= the term frequency is the number of times the term appears in the document\n* idf(d, t) = the document frequency is the number of documents 'd' that contain term 't'"}}