{"cell_type":{"c405c073":"code","a04c18b0":"code","4f8fb135":"code","ef81eb29":"code","ad9b114a":"code","b6a37116":"code","3b22cbcd":"code","74985344":"code","32344551":"code","6f8dcc6d":"code","9bbaf640":"code","1919a72d":"code","df603a8c":"code","8f3603fb":"code","30bd0674":"code","404940ba":"code","d9bddf68":"code","219a8825":"code","9157fd21":"code","7c0c9e1d":"code","989aef28":"code","0d03fd45":"code","40c51225":"code","351f0f63":"code","fd069a31":"code","f94df915":"code","fbb267cd":"code","bb3e001c":"code","1692b2b7":"code","88535cd9":"code","69072848":"code","31486cdb":"code","e06af301":"code","4e66660b":"code","81913873":"code","33484dd0":"code","e614f3e0":"code","6a38338f":"code","5c14faa3":"code","eed54335":"code","c5206943":"code","4d0d7e77":"code","06e2d53a":"code","44cc47d6":"code","1d89d37a":"code","1ae55191":"markdown","4f4093ed":"markdown","822c8a8f":"markdown","9144aef6":"markdown","411970ff":"markdown","358e005f":"markdown","b77d82d6":"markdown","eea3ce6a":"markdown","0968746a":"markdown","7a5874c7":"markdown","6912217e":"markdown","2bd3478c":"markdown","1728707a":"markdown"},"source":{"c405c073":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom sklearn.pipeline import Pipeline\n\n#to data preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n#NLP tools\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#train split and fit models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom nltk.tokenize import TweetTokenizer\n\n#model selection\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a04c18b0":"dataset = pd.read_csv('..\/input\/hate-speech-and-offensive-language-dataset\/labeled_data.csv')\ndataset.head()","4f8fb135":"dataset.info()","ef81eb29":"dataset.describe().T","ad9b114a":"dt_transformed = dataset[['class', 'tweet']]\ny = (dt_transformed.iloc[:, :-1].values).ravel()\ndt_transformed","b6a37116":"# Dividindo o df em treino e teste\ndf_train, df_test = train_test_split(dt_transformed, test_size = 0.10, random_state = 42, stratify=dt_transformed['class'])\ndf_train.shape, df_test.shape","3b22cbcd":"df_train['class'].value_counts().plot(kind='bar')","74985344":"def preprocessing(data):\n    stemmer = nltk.stem.RSLPStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    corpus = []\n    for tweet in data:\n      review = re.sub(r\"@[A-Za-z0-9_]+\", \" \", tweet)\n      review = re.sub('RT', ' ', review)\n      review = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", \" \", review)\n      review = re.sub(r\"https?\", \" \", review)\n      review = re.sub('[^a-zA-Z]', ' ', review)\n      review = review.lower()\n      review = review.split()\n      ps = PorterStemmer()\n      review = [ps.stem(word) for word in review if not word in set(all_stopwords) if len(word) > 2]\n      review = ' '.join(review)\n      corpus.append(review)\n\n    return np.array(corpus)","32344551":"corpus = preprocessing(df_train['tweet'].values)\ncorpus.shape","6f8dcc6d":"# treino e valida\u00e7\u00e3o do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus, df_train['class'], test_size = 0.10, random_state = 42, stratify=df_train['class'])\nc_train.shape, c_vad.shape","9bbaf640":"def tokenize(c_train, c_vad):\n    tweet_tokenizer = TweetTokenizer() \n    vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, max_features = 1010)\n    X_train = vectorizer.fit_transform(c_train).toarray()\n    X_vad = vectorizer.transform(c_vad).toarray()\n    return X_train, X_vad","1919a72d":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","df603a8c":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","8f3603fb":"def set_confusion_matrix(clf, X, y, title):\n    plot_confusion_matrix(clf, X, y)\n    plt.title(title)\n    plt.show()","30bd0674":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","404940ba":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","d9bddf68":"conjunto = c_train\nhate_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 0]\noff_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 1]\nnone_tweets = [sentence for sentence, label in zip(conjunto, y) if label == 2]","219a8825":"hate_words = ' '.join(hate_tweets)\noff_words = ' '.join(off_tweets)\nnone_words = ' '.join(none_tweets)","9157fd21":"def get_wordcloud(text):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.show()","7c0c9e1d":"get_wordcloud(hate_words)\nget_wordcloud(off_words)\nget_wordcloud(none_words)","989aef28":"def wordListToFreqDict(wordlist):\n    wordfreq = [(wordlist.count(p))\/len(wordlist) for p in wordlist]\n    return dict(list(zip(wordlist,wordfreq)))","0d03fd45":"def sortFreqDict(freqdict):\n    aux = [(freqdict[key], key) for key in freqdict]\n    aux.sort()\n    aux.reverse()\n    return aux","40c51225":"hate_dict = sortFreqDict(wordListToFreqDict(hate_words.split()))\noff_dict = sortFreqDict(wordListToFreqDict(off_words.split()))\nnone_dict = sortFreqDict(wordListToFreqDict(none_words.split()))","351f0f63":"len(hate_dict), len(off_dict), len(none_dict)","fd069a31":"def get_common(wordlist, n):\n    return ([w[1] for w in wordlist])[:n]\n\ncommon_words = list()\ncommon_words.append(get_common(hate_dict, 2000))\ncommon_words.append(get_common(off_dict, 1000))\ncommon_words.append(get_common(none_dict, 1000))\ncommon_words = np.unique(np.hstack(common_words))","f94df915":"common_words_dict = ({i:j for i, j in zip(common_words, range(len(common_words)))})","fbb267cd":"vectorizer = CountVectorizer(analyzer=\"word\", tokenizer=tweet_tokenizer.tokenize, vocabulary=common_words_dict)\nX_train = vectorizer.fit_transform(c_train).toarray()\nX_vad = vectorizer.transform(c_vad).toarray()\nX_train.shape, X_vad.shape","bb3e001c":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","1692b2b7":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","88535cd9":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","69072848":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","31486cdb":"df_hate = df_train[df_train['class'] == 0]\ndf_off = df_train[df_train['class'] == 1]\ndf_none = df_train[df_train['class'] == 2]","e06af301":"df_off_under = df_off.sample(n_hate, random_state=0)\ndf_none_under = df_none.sample(n_hate, random_state=0)\n\ndf_under = pd.concat([df_hate, df_off_under, df_none_under], axis=0)\nprint(df_under['class'].value_counts())","4e66660b":"corpus_under = preprocessing(df_under['tweet'].values)","81913873":"# treino e valida\u00e7\u00e3o do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus_under, df_under['class'], test_size = 0.10, random_state = 42, stratify=df_under['class'])\nc_train.shape, c_vad.shape","33484dd0":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","e614f3e0":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","6a38338f":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)","5c14faa3":"target_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","eed54335":"n_off, n_none, n_hate = df_train['class'].value_counts()\nn_hate, n_off, n_none ","c5206943":"df_hate_over = df_hate.sample(n_off, replace=True, random_state=0)\ndf_none_over = df_none.sample(n_off, replace=True, random_state=0)\ndf_over = pd.concat([df_off, df_hate_over, df_none_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_over['class'].value_counts())","4d0d7e77":"corpus_over = preprocessing(df_over['tweet'].values)\n# treino e valida\u00e7\u00e3o do corpus\nc_train, c_vad, y_train, y_vad = train_test_split(corpus_over, df_over['class'], test_size = 0.10, random_state = 42, stratify=df_over['class'])\nc_train.shape, c_vad.shape","06e2d53a":"X_train, X_vad = tokenize(c_train, c_vad)\nX_train.shape, X_vad.shape","44cc47d6":"# Logistic Regression\nmodel = LogisticRegression(multi_class='ovr', solver='liblinear', random_state = 0)\nmodel.fit(X_train, y_train.ravel())\ny_pred = model.predict(X_vad)","1d89d37a":"set_confusion_matrix(model, X_vad, y_vad, type(model).__name__)\ntarget_names = ['class 0', 'class 1', 'class 2']\nprint(classification_report(y_vad, y_pred, target_names=target_names))","1ae55191":"0 - Discurso de \u00f3dio\n\n1 - Linguagem ofensiva\n\n2 - nenhum dos dois","4f4093ed":"Pegando as palavras que mais aparecem em cada classe","822c8a8f":"# Making the Confusion Matrix","9144aef6":"## Cleaning the texts","411970ff":"## Analisando melhor as palavras de cada classe","358e005f":"# Importing the libraries","b77d82d6":"Aumentar as amostras para que ficassem balanceadas resultou em resultados melhores, mas como os dados est\u00e3o repetidos, n\u00e3o sei se o modelo generaliza bem o problema. O ideal seria aumentar as amostras com certa vari\u00e2ncia entre elas.","eea3ce6a":"# Importing the dataset","0968746a":"## Extraindo as features utilizando tokeniza\u00e7\u00e3o","7a5874c7":"## Treinando Logistic Regression","6912217e":"> ### UNDERSAMPLING\n\nVemos que os algoritmos ainda continuam confundindo bastante hate speech (0) com offensive language(1). Vamos tentar melhorar o problema de balanceamento desses dados.","2bd3478c":"### OVERSAMPLING","1728707a":"Treinando agora os modelos com estes dados:"}}