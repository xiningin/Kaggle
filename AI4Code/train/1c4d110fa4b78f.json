{"cell_type":{"f64876ab":"code","25e01a38":"code","d5aaeefb":"code","eb4dbde2":"code","21152b90":"code","5052b4cf":"code","a9f4eef3":"code","45846669":"markdown","9c99d55d":"markdown","e8b2a98b":"markdown","52ebea82":"markdown","74a33357":"markdown","14844d74":"markdown","a5142c59":"markdown","1b7f62ac":"markdown","0e547712":"markdown"},"source":{"f64876ab":"\nimport numpy as np\nimport pandas as pd\n\nN = 20\nts = pd.Series(\n    np.arange(N),\n    index=pd.period_range(start='2010', freq='A', periods=N, name='Year'),\n    dtype=pd.Int8Dtype,\n)\n\n# Lag features\nX = pd.DataFrame({\n    'y_lag_2': ts.shift(2),\n    'y_lag_3': ts.shift(3),\n    'y_lag_4': ts.shift(4),\n    'y_lag_5': ts.shift(5),\n    'y_lag_6': ts.shift(6),    \n})\n\n# Multistep targets\ny = pd.DataFrame({\n    'y_step_3': ts.shift(-2),\n    'y_step_2': ts.shift(-1),\n    'y_step_1': ts,\n})\n\ndata = pd.concat({'Targets': y, 'Features': X}, axis=1)\n\ndata.head(10).style.set_properties(['Targets'], **{'background-color': 'LavenderBlush'}) \\\n                   .set_properties(['Features'], **{'background-color': 'Lavender'})","25e01a38":"\nfrom pathlib import Path\nfrom warnings import simplefilter\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\n\nsimplefilter(\"ignore\")\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True, figsize=(11, 4))\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=16,\n    titlepad=10,\n)\nplot_params = dict(\n    color=\"0.75\",\n    style=\".-\",\n    markeredgecolor=\"0.25\",\n    markerfacecolor=\"0.25\",\n)\n%config InlineBackend.figure_format = 'retina'\n\n\ndef plot_multistep(y, every=1, ax=None, palette_kwargs=None):\n    palette_kwargs_ = dict(palette='husl', n_colors=16, desat=None)\n    if palette_kwargs is not None:\n        palette_kwargs_.update(palette_kwargs)\n    palette = sns.color_palette(**palette_kwargs_)\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.set_prop_cycle(plt.cycler('color', palette))\n    for date, preds in y[::every].iterrows():\n        preds.index = pd.period_range(start=date, periods=len(preds))\n        preds.plot(ax=ax)\n    return ax\n\n\ndata_dir = Path(\"..\/input\/ts-course-data\")\nflu_trends = pd.read_csv(data_dir \/ \"flu-trends.csv\")\nflu_trends.set_index(\n    pd.PeriodIndex(flu_trends.Week, freq=\"W\"),\n    inplace=True,\n)\nflu_trends.drop(\"Week\", axis=1, inplace=True)","d5aaeefb":"def make_lags(ts, lags, lead_time=1):\n    return pd.concat(\n        {\n            f'y_lag_{i}': ts.shift(i)\n            for i in range(lead_time, lags + lead_time)\n        },\n        axis=1)\n\n\n# Four weeks of lag features\ny = flu_trends.FluVisits.copy()\nX = make_lags(y, lags=4).fillna(0.0)\n\n\ndef make_multistep_target(ts, steps):\n    return pd.concat(\n        {f'y_step_{i + 1}': ts.shift(-i)\n         for i in range(steps)},\n        axis=1)\n\n\n# Eight-week forecast\ny = make_multistep_target(y, steps=8).dropna()\n\n# Shifting has created indexes that don't match. Only keep times for\n# which we have both targets and features.\ny, X = y.align(X, join='inner', axis=0)","eb4dbde2":"\n# Create splits\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)","21152b90":"\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])","5052b4cf":"from sklearn.multioutput import MultiOutputRegressor\n\nmodel = MultiOutputRegressor(XGBRegressor())\nmodel.fit(X_train, y_train)\n\ny_fit = pd.DataFrame(model.predict(X_train), index=X_train.index, columns=y.columns)\ny_pred = pd.DataFrame(model.predict(X_test), index=X_test.index, columns=y.columns)","a9f4eef3":"\ntrain_rmse = mean_squared_error(y_train, y_fit, squared=False)\ntest_rmse = mean_squared_error(y_test, y_pred, squared=False)\nprint((f\"Train RMSE: {train_rmse:.2f}\\n\" f\"Test RMSE: {test_rmse:.2f}\"))\n\npalette = dict(palette='husl', n_colors=64)\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize=(11, 6))\nax1 = flu_trends.FluVisits[y_fit.index].plot(**plot_params, ax=ax1)\nax1 = plot_multistep(y_fit, ax=ax1, palette_kwargs=palette)\n_ = ax1.legend(['FluVisits (train)', 'Forecast'])\nax2 = flu_trends.FluVisits[y_pred.index].plot(**plot_params, ax=ax2)\nax2 = plot_multistep(y_pred, ax=ax2, palette_kwargs=palette)\n_ = ax2.legend(['FluVisits (test)', 'Forecast'])","45846669":"### Multioutput model\n\nWe'll use linear regression as a MultiOutput strategy. Once we have our data prepared for multiple outputs, training and prediction is the same as always.","9c99d55d":"To use the DirRec strategy, you would only need to replace `MultiOutputRegressor` with another scikit-learn wrapper, `RegressorChain`. The Recursive strategy we would need to code ourselves.\n\n# Your Turn #\n\n[**Create a forecasting dataset**](https:\/\/www.kaggle.com\/kernels\/fork\/20667477) for *Store Sales* and apply the DirRec strategy.","e8b2a98b":"The above illustrates how a dataset would be prepared similar to the *Defining a Forecast* figure: a three-step forecasting task with a two-step lead time using five lag features. The original time series is `y_step_1`. The missing values we could either fill-in or drop.\n\n# Multistep Forecasting Strategies #\n\nThere are a number of strategies for producing the multiple target steps required for a forecast. We'll outline four common strategies, each with strengths and weaknesses.\n\n### Multioutput model\n\nUse a model that produces multiple outputs naturally. Linear regression and neural networks can both produce multiple outputs. This strategy is simple and efficient, but not possible for every algorithm you might want to use. XGBoost can't do this, for instance.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/uFsHiqr.png\" width=300, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>\n<\/center><\/figcaption>\n<\/figure>\n\n### Direct strategy\n\nTrain a separate model for each step in the horizon: one model forecasts 1-step ahead, another 2-steps ahead, and so on. Forecasting 1-step ahead is a different problem than 2-steps ahead (and so on), so it can help to have a different model make forecasts for each step. The downside is that training lots of models can be computationally expensive.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/HkolNMV.png\" width=900, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>\n<\/center><\/figcaption>\n<\/figure>\n\n### Recursive strategy\n\nTrain a single one-step model and use its forecasts to update the lag features for the next step. With the recursive method, we feed a model's 1-step forecast back in to that same model to use as a lag feature for the next forecasting step. We only need to train one model, but since errors will propagate from step to step, forecasts can be inaccurate for long horizons.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/sqkSFDn.png\" width=300, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>\n<\/center><\/figcaption>\n<\/figure>\n\n### DirRec strategy\n\nA combination of the direct and recursive strategies: train a model for each step and use forecasts from previous steps as *new* lag features. Step by step, each model gets an additional lag input. Since each model always has an up-to-date set of lag features, the DirRec strategy can capture serial dependence better than Direct, but it can also suffer from error propagation like Recursive.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/B7KAvAO.png\" width=900, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>\n<\/center><\/figcaption>\n<\/figure>\n\n# Example - Flu Trends #\n\nIn this example we'll apply the MultiOutput and Direct strategies to the *Flu Trends* data from Lesson 4, this time making true forecasts for multiple weeks beyond the training period.\n\nWe'll define our forecasting task to have an 8-week horizon with a 1-week lead time. In other words, we'll be forecasting eight weeks of flu cases starting with the following week.\n\nThe hidden cell sets up the example and defines a helper function `plot_multistep`.","52ebea82":"Remember that a multistep model will produce a complete forecast for each instance used as input. There are 269 weeks in the training set and 90 weeks in the test set, and we now have an 8-step forecast for each of these weeks.","74a33357":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https:\/\/www.kaggle.com\/learn\/time-series\/discussion) to chat with other learners.*","14844d74":"XGBoost here is clearly overfitting on the training set. But on the test set it seems it was able to capture some of the dynamics of the flu season better than the linear regression model. It would likely do even better with some hyperparameter tuning.","a5142c59":"# Introduction #\n\nIn Lessons 2 and 3, we treated forecasting as a simple regression problem with all of our features derived from a single input, the time index. We could easily create forecasts for any time in the future by just generating our desired trend and seasonal features.\n\nWhen we added lag features in Lesson 4, however, the nature of the problem changed. Lag features require that the lagged target value is known at the time being forecast. A lag 1 feature shifts the time series forward 1 step, which means you could forecast 1 step into the future but not 2 steps.\n\nIn Lesson 4, we just assumed that we could always generate lags up to the period we wanted to forecast (every prediction was for just one step forward, in other words). Real-world forecasting typically demands more than this, so in this lesson we'll learn how to make forecasts for a variety of situations.\n\n# Defining the Forecasting Task #\n\nThere are two things to establish before designing a forecasting model:\n- what information is available at the time a forecast is made (features), and,\n- the time period during which you require forecasted values (target).\n\nThe **forecast origin** is time at which you are making a forecast. Practically, you might consider the forecast origin to be the last time for which you have training data for the time being predicted. Everything up to he origin can be used to create features.\n\nThe **forecast horizon** is the time for which you are making a forecast. We often describe a forecast by the number of time steps in its horizon: a \"1-step\" forecast or \"5-step\" forecast, say. The forecast horizon describes the target.\n\n<figure style=\"padding: 1em;\">\n<img src=\"https:\/\/i.imgur.com\/xwEgcOk.png\" width=500, alt=\"\">\n<figcaption style=\"textalign: center; font-style: italic\"><center>A three-step forecast horizon with a two-step lead time, using four lag features. The figure represents what would be a single row of training data -- data for a single prediction, in other words.\n<\/center><\/figcaption>\n<\/figure>\n\nThe time between the origin and the horizon is the **lead time** (or sometimes *latency*) of the forecast. A forecast's lead time is described by the number of steps from origin to horizon: a \"1-step ahead\" or \"3-step ahead\" forecast, say. In practice, it may be necessary for a forecast to begin multiple steps ahead of the origin because of delays in data acquisition or processing.\n\n# Preparing Data for Forecasting #\n\nIn order to forecast time series with ML algorithms, we need to transform the series into a dataframe we can use with those algorithms. (Unless, of course, you are only using deterministic features like trend and seasonality.)\n\nWe saw the first half of this process in Lesson 4 when we created a feature set out of lags. The second half is preparing the target. How we do this depends on the forecasting task.\n\nEach row in a dataframe represents a single forecast. The time index of the row is the first time in the forecast horizon, but we arrange values for the entire horizon in the same row. For multistep forecasts, this means we are requiring a model to produce multiple outputs, one for each step.","1b7f62ac":"### Direct strategy\n\nXGBoost can't produce multiple outputs for regression tasks. But by applying the Direct reduction strategy, we can still use it to produce multi-step forecasts. This is as easy as wrapping it with scikit-learn's `MultiOutputRegressor`.","0e547712":"First we'll prepare our target series (weekly office visits for the flu) for multistep forecasting. Once this is done, training and prediction will be very straightfoward."}}