{"cell_type":{"56272176":"code","c1d89bd9":"code","d240007a":"code","7e01fe85":"code","6123f4e1":"code","1f08f6d0":"code","17448211":"code","c288b378":"code","f4d9e4ba":"code","be6b7274":"code","bfd760e8":"code","e1caf8a3":"code","872e7840":"markdown"},"source":{"56272176":"# Import the necessary packages\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","c1d89bd9":"# Import and read dataset\n\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","d240007a":"df = df[df['ejection_fraction']<70]\n\ninp_data = df.drop(data[['DEATH_EVENT']], axis=1)\nout_data = df[['DEATH_EVENT']]\n\nscaler = StandardScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","7e01fe85":"y_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","6123f4e1":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","1f08f6d0":"def weightInitialization(n_features):\n    w = np.zeros((1, n_features))\n    b = 0\n    return w,b","17448211":"def sigmoid_activation(result):\n    final_result = 1\/(1+np.exp(-result))\n    return final_result","c288b378":"def model_optimize(w,b,X,Y):\n    m = X.shape[0]\n    \n    # prediction\n    final_result = sigmoid_activation(m)\n    Y_T = Y.T\n    cost = (-1\/m)*(np.sum((Y_T*np.log(final_result)) + ((1-Y_T)*(np.log(1-final_result)))))\n    \n    # gradient calculation\n    dw = (1\/m)*(np.dot(X.T, (final_result-Y.T).T))\n    db = (1\/m)*(np.sum(final_result-Y.T))\n    \n    grads = {\n        \"dw\": dw,\n        \"db\": db\n    }\n    \n    return grads, cost","f4d9e4ba":"def model_predict(w, b, X, Y, learning_rate, no_iterations):\n    costs = []\n    for i in range(no_iterations):\n        grads, cost = model_optimize(w,b,X,Y)\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        #weight update\n        w = w - (learning_rate * (dw.T))\n        b = b - (learning_rate * db)\n        \n        if (i % 100 == 0):\n            costs.append(cost)\n    \n    #final parameters\n    coeff = {\"w\": w, \"b\": b}\n    gradient = {\"dw\": dw, \"db\": db}\n    \n    return coeff, gradient, costs","be6b7274":"def predict(final_pred, m):\n    y_pred = np.zeros((1,m))\n    for i in range(final_pred.shape[1]):\n        if final_pred[0][i] > 0.5:\n            y_pred[0][i] = 1\n    return y_pred","bfd760e8":"#Get number of features\nn_features = X_train.shape[1]\nprint('Number of Features', n_features)\nw, b = weightInitialization(n_features)\n#Gradient Descent\ncoeff, gradient, costs = model_predict(w, b, X_train, y_train, learning_rate=0.0001,no_iterations=4500)\n#Final prediction\nw = coeff[\"w\"]\nb = coeff[\"b\"]\nprint('Optimized weights', w)\nprint('Optimized intercept',b)\n#\nfinal_train_pred = sigmoid_activation(np.dot(w,X_train.T)+b)\nfinal_test_pred = sigmoid_activation(np.dot(w,X_test.T)+b)\n#\nm_tr =  X_train.shape[0]\nm_ts =  X_test.shape[0]\n#\ny_tr_pred = predict(final_train_pred, m_tr)\nprint('Training Accuracy',accuracy_score(y_tr_pred.T, y_train))\n#\ny_ts_pred = predict(final_test_pred, m_ts)\nprint('Test Accuracy',accuracy_score(y_ts_pred.T, y_test))","e1caf8a3":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\n\nclf.fit(X_train, y_train)\n\nprint (clf.intercept_, clf.coef_)\n\npred = clf.predict(X_test)\n\nprint ('Accuracy from sk-learn: {0}'.format(clf.score(X_test, y_test)))","872e7840":"---"}}