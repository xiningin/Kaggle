{"cell_type":{"cb1fd7b1":"code","3a88c0eb":"code","1f77c501":"code","58b3c55d":"code","7d3b8845":"code","9eae82e9":"code","570a99be":"code","42969015":"code","57ef6348":"code","94994d88":"code","f885f210":"code","1f27f575":"code","96d56a78":"code","58235c0e":"code","20e2197d":"markdown"},"source":{"cb1fd7b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OrdinalEncoder\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3a88c0eb":"#Input and test data\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","1f77c501":"#feature importance identification for continuous and categorical variables\n#using mutual_information and chi-square test and crammer's v\n\n\nfrom sklearn import feature_selection\nimport math\nfrom scipy import stats\nfrom collections import defaultdict\ndef feature_importance_classification(features, target, cont_cols, cat_cols, n_neighbors=3,\n                                      random_state=None):\n    '''chisquare test'''\n    cont = features[cont_cols]\n    disc = features[cat_cols]\n\n    cont_imp = pd.DataFrame(index=cont.columns)\n    disc_imp = pd.DataFrame(index=disc.columns)\n\n    # Continuous features\n    if cont_imp.index.size > 0:\n        # F-test\n        f_test = feature_selection.f_classif(cont, target)\n        cont_imp['f_statistic'] = f_test[0]\n        cont_imp['f_p_value'] = f_test[1]\n\n        # Mutual information\n        mut_inf = feature_selection.mutual_info_classif(cont, target, discrete_features=False,\n                                                        n_neighbors=n_neighbors,\n                                                        random_state=random_state)\n        cont_imp['mutual_information'] = mut_inf\n\n    # Discrete features\n    if disc_imp.index.size > 0:\n\n        # Chi\u00b2-test\n        chi2_tests = defaultdict(dict)\n\n        for feature in disc.columns:\n            #             if disc[feature].dtype != np.int32 and disc[feature].dtype != np.int64:\n            #                 disc[feature] = disc[feature].apply(lambda x: x.lower())\n            cont = pd.crosstab(disc[feature], target)\n            statistic, p_value, _, _ = stats.chi2_contingency(cont)\n            chi2_tests[feature]['chi2_statistic'] = statistic\n            chi2_tests[feature]['chi2_p_value'] = p_value\n\n        chi2_tests_df = pd.DataFrame.from_dict(chi2_tests, orient='index')\n        disc_imp['chi2_statistic'] = chi2_tests_df['chi2_statistic']\n        disc_imp['chi2_p_value'] = chi2_tests_df['chi2_p_value']\n\n        # Cram\u00e9r's V (corrected)\n        disc_imp['cramers_v'] = [\n            cramers_vcorrected_stat(pd.crosstab(feature, target).values)\n            for obj_first, feature in disc.iteritems()\n        ]\n\n    return cont_imp, disc_imp\n\ndef cramers_vcorrected_stat(confusion_matrix):\n    \"\"\"Calculate Cram\u00e9rs V statistic for categorial-categorial association.\n\n    Uses correction from Bergsma and Wicher, Journal of the Korean Statistical\n    Society 42 (2013): 323-328.\n    \"\"\"\n    chi2 = stats.chi2_contingency(confusion_matrix)[0]\n    number_of_rows = confusion_matrix.sum()\n    phi2 = chi2 \/ number_of_rows\n    row, key = confusion_matrix.shape\n    phi2_corr = max(0, phi2 - ((key - 1) * (row - 1)) \/ (number_of_rows - 1))\n    r_corr = row - ((row - 1) ** 2) \/ (number_of_rows - 1)\n    k_corr = key - ((key - 1) ** 2) \/ (number_of_rows - 1)\n    return math.sqrt(phi2_corr \/ min((r_corr - 1), (k_corr - 1)))","58b3c55d":"#target variable set\ntarget = \"Survived\"\ntarget_arr = [\"Survived\"]\n\n#manually separating columns based on their data\ncat_cols = [\"Pclass\",\"Sex\",\"Embarked\",\"Cabin\"]\ncont_cols = [\"SibSp\",\"Parch\",\"Fare\",\"Age\"]","7d3b8845":"print(df_train[cat_cols+cont_cols+target_arr].shape)\ndf_final = df_train[cat_cols+cont_cols+target_arr]\ndf_final[cat_cols] = df_final[cat_cols].fillna(df_final[cat_cols].mode().iloc[0])\ndf_final[cont_cols] = df_final[cont_cols].fillna(df_final[cont_cols].mean())","9eae82e9":"feature_importance_classification(df_final,df_final[target],cont_cols,cat_cols)","570a99be":"#After running the feature selection we will select the following\nfinal_cat = [\"Pclass\",\"Sex\",\"Cabin\"]\nfinal_cont = [\"Fare\"]","42969015":"from sklearn.naive_bayes import GaussianNB,CategoricalNB\ngc = GaussianNB()\ncont_classifier = gc.fit(df_final[cont_cols],df_final[target])\ncont_classifier.class_prior_","57ef6348":"Cc = CategoricalNB()\noenc = OrdinalEncoder()\nout_df = pd.DataFrame(data=oenc.fit_transform(df_final[cat_cols]),columns=cat_cols)\nmod = Cc.fit(out_df,df_final[target])\nnp.exp(mod.class_log_prior_)","94994d88":"print(df_test[cat_cols+cont_cols].shape)\n# test_final = df_test[cat_cols+cont_cols].dropna()\ndf_test[cat_cols] = df_test[cat_cols].fillna(df_test[cat_cols].mode().iloc[0])\ndf_test[cont_cols] = df_test[cont_cols].fillna(df_test[cont_cols].mean())\ntest_final = df_test[cat_cols+cont_cols]\nprint(test_final.shape)\ntest_out_df = oenc.fit_transform(test_final[cat_cols])\npred2 = cont_classifier.predict_log_proba(test_final[cont_cols])\npred1 = mod.predict_log_proba(test_out_df)\nn = len(cat_cols)+len(cont_cols)","f885f210":"#jll - sum of log probability of continuous and categorical data\n\nlog_prior = mod.class_log_prior_\njlls = []\njlls.append(cont_classifier._joint_log_likelihood(test_final[cont_cols]))\njlls.append(mod._joint_log_likelihood(np.array(test_out_df.astype(int))))\n\njlls = np.hstack([jlls])\n\njlls = jlls - log_prior\njll = jlls.sum(axis=0) + log_prior","1f27f575":"#Standardising the results\nfo = np.exp(jll)\nsumso = np.sum(fo, axis = 1, keepdims = True) \nresult = fo\/sumso","96d56a78":"#Output calculation\ny_pred = np.argmax(result,axis=1)","58235c0e":"zz = zip(df_test[\"PassengerId\"],y_pred)\nout = [[a[0],a[1]] for a in zz]\nmy_submission = pd.DataFrame(out,columns=[\"PassengerId\",\"Survived\"])\nmy_submission.to_csv('submission_1.csv', index=False)\n\n# df_test[\"PassengerId\"].shape\n# y_pred.shape","20e2197d":"we can take prior probability of the classes from cont_classifier as cont_classifier.class_prior_ or from CategoricalNB as np.exp(mod.class_log_prior_)"}}