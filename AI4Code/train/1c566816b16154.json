{"cell_type":{"d594a9e3":"code","696062a4":"code","b145eaa5":"code","c81384c8":"code","83222ac9":"code","0248be6f":"code","4417d849":"code","4197758a":"code","73597309":"code","5c49eaac":"code","a4d92409":"code","a41aa6da":"code","16f4b771":"code","d35b37d0":"code","40cde442":"code","b6609306":"code","28949e44":"code","1e2d4525":"code","fea7ba48":"code","937384f8":"code","e68b26ec":"code","2c2e8ea9":"code","c47ba2a0":"code","8efab603":"code","73203c1c":"code","5e87076d":"code","be60e791":"code","c591d06b":"code","24349f2b":"code","53d417dc":"code","dabea3f1":"code","4cdde134":"code","a5449883":"code","a73dad16":"code","70134fd9":"code","0ca08857":"markdown","796aaa78":"markdown","60ed335f":"markdown","6d284d4c":"markdown","aef73d02":"markdown","b3310002":"markdown","77000433":"markdown","2617f224":"markdown","8ce7e684":"markdown","b9cf4599":"markdown","621eaf1e":"markdown","fbc8fe24":"markdown","3ba6cf79":"markdown","0be760e4":"markdown","3e771c41":"markdown","a75210f5":"markdown","57fc0241":"markdown","7c3a1c16":"markdown","54a63770":"markdown","6d82859c":"markdown","8ca02bc4":"markdown","c279d7f9":"markdown","b4fd105f":"markdown","c7121a8f":"markdown","5737ec31":"markdown","6849505b":"markdown","f680ebb9":"markdown","6f2ef5b2":"markdown","5537bdfa":"markdown","61e53455":"markdown","604e2f2b":"markdown","82078c18":"markdown"},"source":{"d594a9e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","696062a4":"#import packages\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\n%matplotlib inline","b145eaa5":"#load train dataset\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","c81384c8":"df.info(), df.describe(), df.head(10)","83222ac9":"df.duplicated().sum()","0248be6f":"df.drop(columns = ['Name', 'Ticket', 'Cabin'], axis =1, inplace = True)","4417d849":"plt.subplot(2,1,1)\nbins = np.arange(0,80,3)\nplt.hist(df.Age, bins = bins)\nplt.subplot(2,1,2)\nsb.boxplot(data=df , x = 'Age')\n;","4197758a":"# fill Age nulls with the average\ndf.Age = df.Age.fillna(df.Age.median())","73597309":"df.tail()","5c49eaac":"# dropping nulls in the `embarked` variable\ndf.Embarked.fillna(df.Embarked.mode(), inplace = True)","a4d92409":"df.info()","a41aa6da":"sb.pairplot(df);","16f4b771":"corr = df.corr() # correlation map\nfig, ax = plt.subplots(figsize=(20,20))\nax = sb.heatmap(corr, annot=True, cmap= 'vlag_r', center = 0)","d35b37d0":"plt.figure(figsize=[4,4])\nsorted_label = df.Survived.value_counts()\nplt.pie(sorted_label, labels = ['Dead', 'Survived'], startangle=90, counterclock=False, autopct='%1.1f%%');","40cde442":"plt.figure(figsize=[4,4])\nsorted_label = df.Sex.value_counts()\nplt.pie(sorted_label, labels = sorted_label.index, startangle=90, counterclock=False, autopct='%1.1f%%');","b6609306":"plt.subplot(2,1,1)\nbins = np.arange(0,80+1,1)\nplt.hist(df.Age, bins = bins)\nplt.subplot(2,1,2)\nsb.boxplot(data=df , x = 'Age')\n;","28949e44":"base_color = sb.color_palette()[0]\nfig, ax = plt.subplots(nrows= 4, figsize = [8,15])\nsb.countplot(data= df, x= 'SibSp', color= base_color, ax = ax[0])\nsb.countplot(data= df, x= 'Parch', color= base_color, ax = ax[1])\nsb.countplot(data= df, x= 'Embarked', color= base_color, ax = ax[2])\nsb.countplot(data= df, x= 'Pclass', color= base_color, ax = ax[3])\n;","1e2d4525":"bins = np.arange(0,500+10,10)\nplt.hist(df.Fare, bins = bins);","fea7ba48":"log_binsize = 0.05\nbins = 10 ** np.arange(1, np.log10(df['Fare'].max())+log_binsize, log_binsize)\nplt.hist(df.Fare, bins = bins)\nplt.xscale('log')","937384f8":"plt.figure(figsize = [12,15])\nplt.subplot(4,2,1)\nsb.countplot(data= df, x= 'Survived', hue = 'Sex')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('')\nplt.subplot(4,2,2)\nsb.barplot(data= df, x= 'Survived', y = 'Age')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('')\nplt.subplot(4,2,3)\nsb.barplot(data= df, x= 'Survived', y = 'Fare')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('')\nplt.subplot(4,2,4)\nsb.countplot(data= df, x='Survived', hue = 'Embarked')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('')\nplt.subplot(4,2,5)\nsb.barplot(data= df, y = 'Survived', x = 'SibSp')\nplt.subplot(4,2,6)\nsb.barplot(data= df, y = 'Survived', x = 'Parch')\nplt.subplot(4,2,7)\nsb.countplot(data= df, x='Survived', hue = 'Pclass')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('');","e68b26ec":"sb.barplot(data= df, x='Survived', y = 'Fare', hue = 'Pclass')\nplt.xticks([0,1], ['Dead', 'Survived'])\nplt.xlabel('');","2c2e8ea9":"sb.barplot(data= df, x='Sex', y='Survived', hue = 'Embarked');","c47ba2a0":"sb.barplot(data= df, x='Sex', y='Survived', hue = 'Pclass');","8efab603":"g = sb.FacetGrid(data= df, col= 'Sex', row = 'Embarked',\n                margin_titles= True)\ng.map(sb.barplot, 'Pclass', 'Survived');","73203c1c":"dummies_Sex = pd.get_dummies(df['Sex'], drop_first=True)\ndummies_Embarked = pd.get_dummies(df['Embarked'], drop_first= True)\ndf_new = df.join(dummies_Sex).join(dummies_Embarked)\ndf_new['family_size'] = df_new.SibSp + df_new.Parch + 1\ndf_new","5e87076d":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfeatures = ['male', 'Pclass', 'Q', 'S']\nX = df_new[features]\ny = df_new['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nclf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:{:.6f}\".format(metrics.accuracy_score(y_test, y_pred)))\nprint(\"Precision:{:.6f}\".format(metrics.precision_score(y_test, y_pred)))\nprint(\"Recall:{:.6f}\".format(metrics.recall_score(y_test, y_pred)))","be60e791":"feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp","c591d06b":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nseed = 0\nkfold = model_selection.KFold(n_splits=50, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC(probability= True)\nestimators.append(('svm', model3))\nmodel4 = RandomForestClassifier()\nestimators.append(('rf', model4))\n# create the ensemble model\nensemble = VotingClassifier(estimators, voting= 'soft')\nensemble.fit(X_train, y_train)\ny_pred = ensemble.predict(X_test) \n  \n# using accuracy_score \nscore = metrics.accuracy_score(y_test, y_pred) \nscore","24349f2b":"#load test dataset\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","53d417dc":"# fill Embarked nulls with the median and mode\ndf_test.Embarked.fillna(df.Embarked.mode(), inplace = True)\ndummies_Sex = pd.get_dummies(df_test['Sex'], drop_first=True)\ndummies_Embarked = pd.get_dummies(df_test['Embarked'], drop_first=True)\ndf_test = df_test.join(dummies_Sex).join(dummies_Embarked)\ndf_test.drop(['Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked','Age'], axis = 1, inplace = True)","dabea3f1":"df_test.info()","4cdde134":"X_test = df_test[features]\ndf_test['Survived'] = clf.predict(X_test)","a5449883":"df_test.drop(features, axis = 1, inplace = True)","a73dad16":"df_test","70134fd9":"df_test.to_csv('submission.csv', index=False)","0ca08857":"### Exploring `Age`","796aaa78":"## Univariate Exploration","60ed335f":"First Lets take a bird's eye view over the dataset","6d284d4c":"# Predictions","aef73d02":"> Being in Pclass 3 always lower chance of survival specially in females  \n> Best chance of survival will be for females in Pclass 1&2","b3310002":"# Building ML Model","77000433":"# Data Wrangling ","2617f224":"## Dropping columns and Dealing with Nulls","8ce7e684":"## Multivariate Exploration","b9cf4599":"## Sci-Kit Model","621eaf1e":"### Survived with Fare and Pclass","fbc8fe24":"> It is noticed that the relation between survived passenger and sex is strong (females were more likely to survive)  \n> It seems that no relation between age and surviving rate  \n> The majority of survivials were with averaged higher fare\n> It is noted that embarked from S are likely to die with 2:1 chance.  \n> SibSp and Parch don't seem to be strongly related to survival rate.  \n> There is a much higher chance if a passenger is Pclass 3 that he will die!","3ba6cf79":"## Filling Nulls for `Age` by medien ","0be760e4":"> from above plot and refering to heatmap in the univariate section, it is evident that Pclass and Fare are colinear  ","3e771c41":"### Survived with Sex and Embarked","a75210f5":"## What are the main features in the data?  \n> * `Survived` is the dependant variable we wish to predict  \n> * `Age`, `Sex` expected to be our main predictors  \n> * Further exploration of the features will be done in the next section before attempting building our prediction model.","57fc0241":"> Best chance of survival will be for females in Pclass 1,2 and Embarked from 'C' and 'Q'","7c3a1c16":"### Survived with Sex","54a63770":"## Bivariate Exploration","6d82859c":"Looking at the `age variable to check outliers","8ca02bc4":"# Models Ensemble","c279d7f9":"### Exploring `SibSp`, `Pclass` , `Parch` and `Embarked`","b4fd105f":"## Checking Dublicates","c7121a8f":"## Encoding Categorical Variables","5737ec31":"From previous section of EDA, I will try my first model with predictors `Sex`, `Pclass` and `Embarked`","6849505b":"### Survived with Sex and Pclass","f680ebb9":"### From bivariate section we conclude that the best predictors for survival rate will be:  \n> Sex, Fare, Embarked and Pclass  \n  \n  We will explore in the next section if multicolinearity exists between some of those predictors","6f2ef5b2":"### Main Feature `Survived`","5537bdfa":"### Exploring `Sex`","61e53455":"> from the above plot we can see that the 'Q' station have very low survival rate in the male section, this is what caused the overall rate to drop among population in the pervious section.  \n> We can see that the 'C' station had higher rate of living chances in both genders.","604e2f2b":"> Dropping `cabin`, `name` and `ticket`","82078c18":"# EDA"}}