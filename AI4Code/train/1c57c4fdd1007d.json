{"cell_type":{"b59fcc9f":"code","0e3260c0":"code","8881ea3f":"code","98e2ecc0":"code","dde0cd93":"code","2fa1f378":"code","dd87df81":"code","de01e7ef":"code","0c7c419d":"code","a21d31e0":"code","81f5ebc7":"code","1ef31088":"code","7b900574":"code","6d17452a":"code","cbe0c5a4":"code","93957465":"code","cc98f896":"code","3f7ed900":"markdown","5ee2cfb7":"markdown","8a4e5543":"markdown","9e3d886b":"markdown","d9233cae":"markdown","4f2c7f79":"markdown","d541350a":"markdown","8fecd8f2":"markdown","16836e5a":"markdown","98e70575":"markdown","337f2273":"markdown","7abc652d":"markdown","6bcd0d77":"markdown","56d7b6c0":"markdown","f11bc8e0":"markdown","d7ab0bc1":"markdown","389821b6":"markdown","79f53cb2":"markdown","d3d72540":"markdown","e5c881fb":"markdown","c05bab84":"markdown"},"source":{"b59fcc9f":"import pandas as pd\nimport re\nfrom collections import defaultdict\nimport random\n\ndf = pd.read_csv('\/kaggle\/input\/covid19-tweets\/covid19_tweets.csv')","0e3260c0":"df.head()","8881ea3f":"def preprocess_text(text: str):\n    \"\"\"\n    \n    :param text: - tweet text\n    :returns: - list of preprocessed sentences of a tweet.\n    \"\"\"\n    \n    # lower capital letters\n    text = text.lower()\n    \n    # delete links\n    text = re.sub(r'https.+?', '', text)\n\n    \n    # delete everything except punctuation marks\n    text = re.sub(r'[^a-z !?.\\n]', '', text)\n    \n    # remove whitespace before punctuation mark\/whitespace\/end of line\n    text = re.sub(r' (\\?|\\!|\\.|\\n| |$)', r'\\1', text)\n\n    \n    # remove whitespace at the begining of the line or after punctuation mark\n    text = re.sub(r'(\\?|\\!|\\.|\\n|^) ', r'\\1', text)\n    \n    # spliting by puncuation mark or new line\n    texts = re.split('\\?|\\.|\\!|\\n', text)\n    \n    # deleting empty sentences\n    texts = [t for t in texts if t]\n    \n\n    return texts","98e2ecc0":"test_tweet = 'BiG, brother is 100% watching  you .Are You scared???'\npreprocess_text(test_tweet)","dde0cd93":"corpus = []\nfor t in df['text']:\n    corpus.extend(preprocess_text(t))","2fa1f378":"for c in corpus[:10]:\n    print(c)","dd87df81":"def create_ngrams(N: int, min_count: int = 5, min_tokens: int = 5):\n    \n    \"\"\"\n    :param N: ngram size\n    :param min_count: minimum acceptable count of ngram founds\n    :param min_tokens: minimum acceptable count of tokens in sentence\n    \n    :returns: Dict[Tuple[str], int] \n    \"\"\"\n    ngram = defaultdict(int)\n    \n    for line in corpus:\n        tokens = line.split()\n        \n        if len(tokens) > min_tokens:\n            for i in range(len(tokens) - N + 1):\n                ngram[tuple(tokens[i : i + N])] += 1\n                \n            ngram[tuple(['^'] + tokens[0 : N - 1])] += 1\n            \n            ngram[tuple(tokens[-N:-1] + ['$'])] += 1\n            \n    ngram = {key: value for key, value in ngram.items() if value > min_count}\n    \n    return ngram\n    ","de01e7ef":"trigrams = create_ngrams(3)","0c7c419d":"len(trigrams)","a21d31e0":"list(trigrams.items())[:10]","81f5ebc7":"def get_next_token(previous_tokens: tuple, ngrams: dict, method: str = 'random'):\n    \"\"\"\n    Returns the next token if found one\n    \n    :param previous_tokens: previous tokens\n    :param ngrams: ngrams - ngrams, where to search for the next token\n    :param method: method of search. \n        'random' - searches for the random token. \n        'weighted' - searches for random token, but token wich was found more times will have more probability to be searched\n        'most_common' - searches for the most common token\n        \n    :returns: str\n    \"\"\"\n    matching_ngrams = {key: value for key, value in ngrams.items() if key[:2] == previous_tokens}\n    \n    if matching_ngrams:\n        \n        if method == 'random':\n            return random.choice(list(matching_ngrams.keys()))[-1]\n        \n        elif method == 'weighted':\n            return random.choices(list(matching_ngrams.keys()), weights=list(matching_ngrams.values()))[0][-1]\n        \n        elif method == 'most_common':\n            return sorted(matching_ngrams.items(), key=lambda x: x[1])[-1][0][-1]","1ef31088":"previous_tokens = ('in', 'the')\n\nprint('random:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='random'))\n    \nprint('weighted:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='weighted'))\n\nprint('most_common:')\nfor _ in range(3):\n    print('\\t', get_next_token(previous_tokens, trigrams, method='most_common'))","7b900574":"def get_starter(ngrams, method='random'):\n    starters = {key: value for key, value in ngrams.items() if key[0] == '^'}\n\n    if method == 'random':\n        return random.choice(list(starters.keys()))\n    \n    elif method == 'weighted':\n        return random.choices(list(starters.keys()), weights=list(starters.values()))[0]\n    \n    elif method == 'most_common':\n        return sorted(starters.items(), key=lambda x: x[1])[-1][0]","6d17452a":"print('random:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='random'))\n    \nprint('weighted:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='weighted'))\n\nprint('most_common:')\nfor _ in range(3):\n    print('\\t', get_starter(trigrams, method='most_common'))","cbe0c5a4":"def generate_tweet(ngrams, min_length: int = 10, starter_method='random', next_method='most_common'):\n    \n    N = len(list(ngrams.keys())[0])\n    \n    starter = get_starter(ngrams, starter_method)\n    tokens = list(starter)[1:]\n    \n    next_token = get_next_token(tuple(list(starter)[1:]), ngrams, next_method)\n    while next_token:\n        tokens.append(next_token)\n        \n        last_tokens = tuple(tokens[-N+1:])\n        next_token = get_next_token(last_tokens, ngrams, next_method)\n    \n    if len(tokens) < min_length or tokens[-1] != '$':\n        tweet = generate_tweet(ngrams, min_length, starter_method, next_method)\n    else:\n        tweet = ' '.join(tokens)\n    return tweet ","93957465":"random.seed(43)\ntweets = [generate_tweet(trigrams) for _ in range(10)]\nfor tweet in tweets:\n    print('*', tweet)","cc98f896":"for tweet in tweets:\n    print(tweet)\n    for text in df['text']:\n        if tweet[:-1] in text:\n            print('\\t', text.replace('\\n', ' ').replace('\\t', ''))\n            \n    else:\n        print('\\t--No matches')\n    print()","3f7ed900":"Here are some generated tweets:","5ee2cfb7":"# Content\n1. [Intoduction](#Intoduction)\n1. [Initializing](#Initializing)\n1. [Describing the data](#Describing-the-data)\n1. [Preprocessing the text](#Preprocessing-the-text)\n1. [Making ngram model](Making-ngram-model)\n1. [Making the generator](Making-the-generator)\n1. [Testing the generator](#Testing-the-generator)\n1. [Conclusion](#Conclusion)\n","8a4e5543":"## Starting the tweet\nNow, let's create function, which will start the tweet. It will search for ngrams starting with \"^\" symbol and return the ngram due to \"method\"","9e3d886b":"# Describing the data\nThe dataset has information about tweets and their authors. It's author [Gabriel Preda](https:\/\/www.kaggle.com\/gpreda) searched tweets with hashtags refering to covid.\n\nTo generate tweets we will need only \"text\" column.","d9233cae":"# Making the generator","4f2c7f79":"### Lets create a trigram model","d541350a":"### Test it","8fecd8f2":"Let's look if your generated tweets have matches","16836e5a":"## Creating a tweet\nNow, let's write a function, which will create a tweet. If the length of created tweet is less than \"min_length\", it regeneretes it untill everything is ok.","98e70575":"# Making ngram model\nSo, here we will make ngram model using default dict. It's a normal dict, but if you try to call undefined key from it, it defines it for you with the default value.\n\nThe keys of our ngram dict will be a tuple of tokens - list of N words, which go in their order.\n\nAlso i whanted to set some parameters:\n1. **N** - This will create ngrams of N size\n1. **min_count** - it will exclude such ngrams, which were found in text less than than min_count value\n1. **min_tokens** - it will add ngram only if there are more than *\"min_tokens\"* count of words in sentence","337f2273":"# Preprocessing the text","7abc652d":"# Initializing\nIn this project we will need\n* **pandas** - to read the data\n* **re** - to preprocess the text \n* **defaultdict** from collections - to create ngram models \n* **random** - to generate tweets\n\nLet's import packages and read csv:","6bcd0d77":"## Creating text corpus","56d7b6c0":"### Test of preprocessing:","f11bc8e0":"# Conclusion\nWe have created a text generator working with ngram models and trained it on covid tweets.\nThe result is not so overwhelming, there is a lack of sence in generated tweets. I think, the dataset is too small for this task and ngram model is not the thing we need.\nBut we had fun :)","d7ab0bc1":"# Intoduction\nHello, here I whant to show you my attempt to generate covid tweets using **ngram models**, trained on [COVID19 Tweets](https:\/\/www.kaggle.com\/gpreda\/covid19-tweets) dataset.","389821b6":"### Test it","79f53cb2":"You might can see special symbols:\n* **^** - is used to mark the begining of a sentence \n* **\\$** - is used to mark the end of a sentence ","d3d72540":"## Helper function\nTo create ngram models we need to clean the sentences. Let's create function, which has **\"text\"** parameter and wich returns list of clean sentences.\n\nIt wil:\n1. lower the case\n1. delete links\n1. delete special symbols, e.g. %, ^, &\n1. delete line breaks\n1. remove unnecessary whitespaces\n1. split tweet text by punctuation marks","e5c881fb":"# Testing the generator","c05bab84":"Now let's create function, which will give us the nex token (word) by previous_tokens."}}