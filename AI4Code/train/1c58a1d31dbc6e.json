{"cell_type":{"e3ed8b5b":"code","e479b2a4":"code","a2f5d743":"code","3308dba4":"code","ac92d004":"code","c8919fc2":"code","901cab65":"code","663f66e0":"code","57c19721":"code","1c31fc90":"code","ab9ccda0":"code","c2a1c9c9":"code","d8f9aa90":"code","c561ea58":"code","c9354026":"code","920b550a":"code","85f35500":"code","08c07609":"code","5a91b0fa":"code","9c6369f2":"code","72c2cef2":"code","4e76a523":"code","99edebef":"code","81adeefd":"code","5954fd5b":"code","a5cdac62":"code","1563bff3":"code","f507ad62":"code","eeccf166":"code","cec948f0":"code","b4c65d91":"code","c41aafaf":"code","a962edbc":"code","1b1f5736":"code","adcaf051":"code","4cd3705f":"code","d30eab1d":"code","39e4cf73":"code","e4990803":"code","5e971d59":"code","3396dc61":"code","8a40b642":"markdown","5169e056":"markdown","3ee9f192":"markdown","a73af326":"markdown","a1db9405":"markdown","dd14f4ef":"markdown","8aa3e428":"markdown","d1a18e13":"markdown","d1a1633a":"markdown","1c6876d3":"markdown","98a56c48":"markdown","db21c7c4":"markdown","4546d5d8":"markdown","cf8c8882":"markdown","5bc37798":"markdown","e099e9ce":"markdown","9ba916de":"markdown","ca5f5e93":"markdown","97a6aa5d":"markdown","f747ac32":"markdown","a4e5cb79":"markdown","f240eafa":"markdown","9ddb893d":"markdown","9ce8b069":"markdown","f126928b":"markdown","744bd6c9":"markdown","c381fccc":"markdown","498cf135":"markdown","0d077992":"markdown"},"source":{"e3ed8b5b":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd","e479b2a4":"day = pd.read_csv(\"..\/input\/boom-bike-dataset\/bike_sharing_data.csv\")","a2f5d743":"day.head()","3308dba4":"day.shape","ac92d004":"day.info()","c8919fc2":"day.describe()","901cab65":"import matplotlib.pyplot as plt\nimport seaborn as sns","663f66e0":"sns.pairplot(day)\nplt.show()","57c19721":"plt.figure(figsize=(20, 12))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = day)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'season', y = 'cnt', data = day)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'mnth', y = 'cnt', data = day)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = day)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = day)\nplt.show()","1c31fc90":"day.drop(['instant','dteday', 'casual', 'registered'], axis = 1, inplace = True)","ab9ccda0":"day.head()","c2a1c9c9":"# Let's drop the first column from season using 'drop_first = True'\nseason_status = pd.get_dummies(day['season'], drop_first = True)\n\nseason_status = season_status.rename(columns ={ 1:'spring',2:'summer',\n                                                3:'fall',\n                                                4:'winter'})\n# Add the results to the original housing dataframe\nday = pd.concat([day, season_status], axis = 1)\n\n# Now let's see the head of our dataframe.\nday.head()\n","d8f9aa90":"# Dropping 'season' as we have created the dummies for it\nday.drop(['season'], axis = 1, inplace = True)\n\nday.head()","c561ea58":"# Let's drop the first column from month using 'drop_first = True'\nmonth = pd.get_dummies(day['mnth'], drop_first = True)\n\nmonth = month.rename(columns ={ 1:'January',2:'February',3:'March',4:'April',5:'May',6:'June',7:'July',8:'August',9:'September',10:'October',11:'November',12:'December'})\n# Add the results to the original housing dataframe\nday = pd.concat([day, month], axis = 1)\n\n# Dropping 'mnth' as we have created the dummies for it\nday.drop(['mnth'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","c9354026":"# Let's drop the first column from weekday using 'drop_first = True'\nnew_weekday = pd.get_dummies(day['weekday'], drop_first = True)\n\nnew_weekday = new_weekday.rename(columns ={ 0:'Sunday',1:'Monday',2:'Tuesday',3:'Wednesday',4:'Thursday',5:'Friday',6:'Saturday'})\n\n# Add the results to the original housing dataframe\nday = pd.concat([day, new_weekday], axis = 1)\n\n# Dropping 'weekday' as we have created the dummies for it\nday.drop(['weekday'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","920b550a":"# Let's drop the first column from weekday using 'drop_first = True'\nweathersituation = pd.get_dummies(day['weathersit'], drop_first = True)\n\nweathersituation = weathersituation.rename(columns ={ 1:'Clear',2:'Mist',3:'Light Snow',4:'Heavy Rain'})\n\n# Add the results to the original housing dataframe\nday = pd.concat([day, weathersituation], axis = 1)\n\n# Dropping 'weekday' as we have created the dummies for it\nday.drop(['weathersit'], axis = 1, inplace = True)\n\n# Now let's see the head of our dataframe.\nday.head()","85f35500":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(0)\ndf_train, df_test = train_test_split(day, train_size = 0.7, test_size = 0.3, random_state = 100)","08c07609":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","5a91b0fa":"# Applying scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = ['temp','atemp', 'hum', 'windspeed', 'cnt' ]\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\n\ndf_train.head()","9c6369f2":"df_train.describe","72c2cef2":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (26, 15))\nsns.heatmap(df_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","4e76a523":"plt.figure(figsize=[6,6])\nplt.scatter(df_train.atemp, df_train.cnt)\nplt.show()","99edebef":"y_train = df_train.pop('cnt')\nX_train = df_train","81adeefd":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","5954fd5b":"# Running RFE with output number of variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train,y_train)\n\nrfe = RFE(lm,10)    # running RFE\nrfe = rfe.fit(X_train,y_train)","a5cdac62":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","1563bff3":"col = X_train.columns[rfe.support_]\ncol","f507ad62":"X_train.columns[~rfe.support_]","eeccf166":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","cec948f0":"# Adding a constant variable\nimport statsmodels.api as sm\nX_train_rfe = sm.add_constant(X_train_rfe)","b4c65d91":"lm = sm.OLS(y_train,X_train_rfe).fit()  # Running the Linear Model","c41aafaf":"# Summary of the Linear Model\nprint(lm.summary())","a962edbc":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif = vif.sort_values(by=\"VIF\", ascending = False)\nvif\n\n    ","1b1f5736":"y_train_cnt = lm.predict(X_train_rfe)","adcaf051":"# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4cd3705f":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_cnt), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","d30eab1d":"num_vars = ['temp','atemp','hum', 'windspeed', 'cnt' ]\ndf_test[num_vars] = scaler.transform(df_test[num_vars])","39e4cf73":"y_test = df_test.pop('cnt')\nX_test = df_test","e4990803":"# Now let's use our model to make predictions.\n\n\n# Adding a constant variable \nX_test_new = sm.add_constant(X_test)\n\n\n# Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test_new[X_train_rfe.columns]\n\n","5e971d59":"# Making predictions\ny_pred = lm.predict(X_test_new)\n","3396dc61":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      ","8a40b642":"## Step 6 - Making Predictions","5169e056":"## Step 7: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","3ee9f192":"'temp','yr','winter' are some of the  variables which help to increse the count significantly.","a73af326":"#### Visualising Numeric Variables\n\nLet's make a pairplot of all the numeric variables","a1db9405":"As noticed here, 'atemp' seems to be correlated to 'cnt' the most. Let's see a pairplot for 'atemp' vs 'cnt'.","dd14f4ef":"So, we pick 'atemp' as the first variable and we'll try to fit a regression line to that.","8aa3e428":"The variable 'weekday' has 7 levels from 0 to 6\n\n- 0 will correspond to Sunday \n- 1 will correspond to Monday\n.\n.\n.\n- 6 will correspond to Saturday\n","d1a18e13":"## Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","d1a1633a":"### Dividing into X and Y sets for the model building\n","1c6876d3":"Inspect the various aspects of the housing dataframe","98a56c48":"## Step 4: Splitting the Data into Training and Testing Sets\n\nThe first basic step for regression is performing a train-test split.","db21c7c4":"The variable 'season', 'mnth', 'weekday', 'weathersit' has multiple levels.\n\nFor this, we will use dummy variables.","4546d5d8":"#### Visualising Categorical Variables\n\nAs you might have noticed, there are a few categorical variables as well. Let's make a boxplot for some of these variables.","cf8c8882":"#### Dividing into X_test and y_test","5bc37798":"\nWe can see that the equation of our best fitted line is:\n\n$ cnt = 0.2265  \\times  yr - 0.0897  \\times  holiday + 0.5666  \\times temp - 0.2864 \\times hum - 0.2014 \\times windspeed + 0.1002 \\times summer + 0.1521 \\times winter + 0.0494 \\times August + 0.1187 \\times September - 0.1917 \\times Light Snow $\n","e099e9ce":"#### Problem Statement:\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \nEssentially, the company wants \u2014\n\n\n- To identify the variables which are significant in predicting the demand for shared bikes.\n\n- To create a linear model that quantitatively relates how well those variables describe the bike demands\n\n- To know the accuracy of the model, i.e. how well these variables can predict bikes demand.\n\n**So interpretation is important!**","9ba916de":"## Step 1: Reading and Understanding the Data\n\nLet us first import NumPy and Pandas and read the housing dataset","ca5f5e93":"## Step 2: Visualising the Data\n","97a6aa5d":"The variable 'weathersit' has 4 levels\n\n- 1 will correspond to Clear, Few clouds, Partly cloudy, Partly cloudy \n- 2 will correspond to Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- 3 will correspond to Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n- 4 will correspond to Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\n","f747ac32":"## Step 3: Data Preparation","a4e5cb79":"Dropping unnecessary variables - 'instant', 'dteday', 'casual', 'registered'","f240eafa":"### Building model using statsmodel, for the detailed statistics","9ddb893d":"### Dummy Variables","9ce8b069":"#### Applying the scaling on the test sets","f126928b":"## Step 5: Building a model using RFE\n\nUsing the LinearRegression function from SciKit Learn for its compatibility with RFE(Recursive feature elimination)\n\n### RFE","744bd6c9":"The variable 'mnth' has 12 levels from 1 to 12\n\n- 1 will correspond to January .....\n- 12 will correspond to December\n","c381fccc":"## Bike Sharing Case Study","498cf135":"The variable 'season' has four levels\n- `1` will correspond to `spring`\n- `2` will correspond to `summer`\n- `3` will correspond to `fall`\n- `4` will correspond to `winter`","0d077992":"### Rescaling the Features\nWe will use MinMax scaling."}}