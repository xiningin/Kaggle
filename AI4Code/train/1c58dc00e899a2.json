{"cell_type":{"b649335e":"code","eddd5d1a":"code","100355d3":"code","c58e2c0e":"code","724101bb":"code","0ed2bbff":"code","80707bf8":"code","8a8e0afb":"code","cb86f3f2":"code","e16552b9":"code","242505db":"code","3d31ebbb":"code","312825ee":"code","6af0e0bc":"code","e7d4ad8a":"code","7d6e0d7d":"code","e33224b5":"code","3b879ff1":"code","43993b67":"code","bb542f08":"code","05f9ef8d":"code","2bb8c2e5":"code","016a0830":"code","91c854ac":"code","1c583c88":"code","2eeff1db":"code","ff248805":"code","18c13545":"code","b7fe2269":"code","1ae951e9":"code","787d7fdd":"code","58fe8e97":"code","3bc2f3bd":"code","802061c8":"code","4cb191d1":"code","bd6ed392":"code","85a19df8":"code","c3702359":"code","5e4771a7":"code","17f77fe9":"code","353f0141":"code","73987f80":"code","0966be58":"code","3fa2d429":"code","eecf7257":"code","c339d602":"code","3d91aad2":"code","6d005279":"code","b52dba9e":"code","3509ff8c":"code","c4f970e9":"code","c2b207da":"code","59362e97":"code","22bb5777":"code","a8e7fb30":"code","a872122d":"code","f8601237":"code","e81cee17":"code","240e6bc7":"code","b070e96c":"code","144f0e8f":"code","3f30f9de":"code","bd0b1835":"code","d04202a2":"code","0458733a":"code","d139dc80":"code","a2273591":"code","9cedb242":"code","3ca9d99b":"code","ddc4c467":"code","8db0af1d":"code","2f436b7f":"code","09a04281":"code","cb757a08":"code","e2c493ba":"code","3bf5eb8c":"code","14a3a0c6":"code","2903557f":"code","7f592967":"code","929d6ebb":"code","034ee091":"code","9c05a8e7":"code","8feaaf47":"code","e4d76152":"code","5db492d8":"code","d68dda49":"code","ed88cd00":"code","f22eea18":"code","091e2c52":"code","f6058e01":"code","14d491b7":"code","7c39bf22":"code","7fdbddb3":"code","04f2c61b":"code","208c9112":"code","2421f535":"code","0e157f4b":"code","75f19a10":"code","0359cbb5":"code","a3259341":"code","015b848d":"code","799ef775":"code","cc7c3b9f":"code","2eb875d3":"code","37c2db6f":"code","5507609d":"code","dd950b6f":"code","5413bc8d":"code","86e9639e":"code","6632bc0e":"markdown","4c60da6f":"markdown","6b460488":"markdown","f20cef5d":"markdown","e2851a90":"markdown","8c768869":"markdown","ab68e1c6":"markdown","4b7c160f":"markdown","ecc9f39b":"markdown","72e19da8":"markdown","9789459f":"markdown","6eac066a":"markdown","32c72a4d":"markdown","a96fd047":"markdown","e9f80d13":"markdown","6bbb0170":"markdown","15f07d91":"markdown","e6c91ae3":"markdown","c25a4c64":"markdown","45d9f8e1":"markdown","ae5ef8e6":"markdown","f9ee1856":"markdown","818ae3da":"markdown","5261d872":"markdown","86807671":"markdown","12f881a9":"markdown","200b6661":"markdown","b343131f":"markdown","3453cc55":"markdown","525ae3f5":"markdown","dbfb4a83":"markdown","3e66f9e8":"markdown","3ce6496a":"markdown","1d4a8fcb":"markdown","74b7b179":"markdown","3ad0731a":"markdown"},"source":{"b649335e":"# Fix random seeds for reproducibility\n\nGLOBAL_SEED = 87216\n\nfrom numpy.random import seed\nseed(GLOBAL_SEED)\n\nfrom tensorflow import set_random_seed\nset_random_seed(GLOBAL_SEED)\n\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\n\nimport random as rn\nrn.seed(GLOBAL_SEED)","eddd5d1a":"import os\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport seaborn as sns; sns.set()\n\nfrom pandas.api.types import CategoricalDtype\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\nfrom xgboost import DMatrix\nfrom xgboost import XGBRegressor\nfrom xgboost import cv\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n\npd.options.display.max_columns = None\npd.options.display.max_rows = None\n\nplt.rcParams['figure.figsize'] = [15, 5]","100355d3":"# Load train and test data\n# DataFrames train and test are to be transformed in place,\n# and ames I will keep mostly intact for future reference and EDA.\nraw_data_path = r'..\/input\/house-prices-advanced-regression-techniques'\nmetadata_path = r'..\/input\/house-prices-metadata'\names_dtypes = {'MSSubClass': 'str'}\names = pd.read_csv(os.path.join(raw_data_path, 'train.csv'), dtype=ames_dtypes)\ntrain = ames.copy()\ntarget = train['SalePrice'].copy()\ntrain.drop('SalePrice', axis=1, inplace=True)\ntest = pd.read_csv(os.path.join(raw_data_path, 'test.csv'), dtype=ames_dtypes)\n\n# Load metadata\nordinal_features_recoding = pd.read_excel(os.path.join(metadata_path, \n                                                       'ordinal_features_recoding.xlsx'))\nfeature_description=pd.read_excel(os.path.join(metadata_path,'feature_description.xlsx'))\nfeature_categories=pd.read_excel(os.path.join(metadata_path,'feature_categories.xlsx'))\nprint('Train shape:', train.shape)\nprint('Test shape:', test.shape)","c58e2c0e":"# Useful lists of feature names\nNUMERIC_FEATURE_NAMES = ['LotFrontage', 'LotArea', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea',\n                         'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF',\n                         '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath',\n                         'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd',\n                         'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF',\n                         'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n                         'MiscVal', 'MoSold', 'YrSold']\n\nORDINAL_FEATURE_NAMES = ['LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond',\n                         'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                         'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'Electrical', 'KitchenQual',\n                         'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond',\n                         'PavedDrive', 'PoolQC', 'Fence']\n\nNOMINAL_FEATURE_NAMES = ['Id', 'MSSubClass', 'MSZoning', 'Street', 'Alley', 'LandContour',\n                         'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n                         'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n                         'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'GarageType',\n                         'MiscFeature', 'SaleType', 'SaleCondition']\n\nFEATURES_WITH_MEANINGFUL_NAN = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                                'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish',\n                                'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n\n# Check if all feature names correspond with source data\nassert set(train.columns) - set(NUMERIC_FEATURE_NAMES) - \\\n    set(ORDINAL_FEATURE_NAMES) - set(NOMINAL_FEATURE_NAMES) == set()","724101bb":"def add_missing_0_to_mssubclass(df):\n    \"\"\"Zeros in 020-090 get cut off. This function prepends them back.\"\"\"\n    \n    df['MSSubClass'] = df['MSSubClass'].apply(\n        lambda x: '0' + str(x) if len(str(x)) == 2 else x)\n\n\nadd_missing_0_to_mssubclass(ames)\nadd_missing_0_to_mssubclass(train)\nadd_missing_0_to_mssubclass(test)","0ed2bbff":"def replace_meaningful_nan(df, feature_names):\n    \"\"\"Replaces nan where 'NA' is an existing category.\"\"\"\n    df.loc[:, feature_names] = df.loc[:, feature_names].fillna('Missing')\n\n\nreplace_meaningful_nan(ames, FEATURES_WITH_MEANINGFUL_NAN)","80707bf8":"train.head()","8a8e0afb":"test.head()","cb86f3f2":"ordinal_features_recoding.head()","e16552b9":"feature_description.head()","242505db":"feature_categories.head()","3d31ebbb":"fig, ax = plt.subplots(2, 1, figsize=[15, 8])\nsns.distplot(ames['SalePrice'], ax=ax[0])\nsns.distplot(np.log(ames['SalePrice']), ax=ax[1], axlabel='Log SalePrice')","312825ee":"fig, ax = plt.subplots(1, 2)\nstats.probplot(ames['SalePrice'], plot=ax[0])\nstats.probplot(ames['SalePrice'].apply(np.log), plot=ax[1])\n[a.get_lines()[0].set_markerfacecolor('C0') for a in ax]\nax[0].set_title('SalePrice')\nax[1].set_title('Log SalePrice')\nplt.show()","6af0e0bc":"fig = plt.figure(figsize=[15, 15])\nfeature_list = sorted(NUMERIC_FEATURE_NAMES)\nnum_subplots = len(feature_list)\nncols = 5\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    sns.distplot(ames[feature].dropna(), ax=ax, bins=10,\n                 kde=False, hist_kws={'alpha': 1, 'edgecolor': 'white'})\nfig.suptitle('Distribution plots of numeric features', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","e7d4ad8a":"fig = plt.figure(figsize=[15, 15])\nfeature_list = sorted(NUMERIC_FEATURE_NAMES)\nnum_subplots = len(feature_list)\nncols = 5\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    ax.scatter(x=ames[feature], y=ames['SalePrice'],\n               edgecolor='white', linewidth=0.4,)\n    plt.xlabel(feature)\n    ax.yaxis.set_major_formatter(ticker.FuncFormatter(\n        lambda y, pos: '%.0f' % (y * 1e-3)))\nfig.suptitle('Scatter plots of numeric features vs SalePrice in 000 USD', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","7d6e0d7d":"fig = plt.figure(figsize=[15, 20])\nfeature_list = sorted(ORDINAL_FEATURE_NAMES)\nnum_subplots = len(feature_list)\nncols = 4\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    if ames[feature].dtype == np.object:\n        order = ordinal_features_recoding.loc[ordinal_features_recoding['Name'] == feature, 'Code']\n    else: \n        order = None\n    sns.countplot(x=feature, data=ames, color='darkcyan', order=order, ax=ax)\n    ax.set_ylabel('')\nfig.suptitle('Count plots of ordinal features', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","e33224b5":"fig = plt.figure(figsize=[15, 15])\nfeature_list = sorted(ORDINAL_FEATURE_NAMES)\nnum_subplots = len(feature_list)\nncols = 5\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    if ames[feature].dtype == np.object:\n        order = ordinal_features_recoding.loc[ordinal_features_recoding['Name']\n                                              == feature, 'Code']\n    else:\n        order = None\n    sns.stripplot(x=ames[feature], y=ames['SalePrice'], color='darkcyan',\n                  edgecolor='whitesmoke', linewidth=0.4, size=7, jitter=True, order=order, ax=ax)\n    ax.yaxis.set_major_formatter(ticker.FuncFormatter(\n        lambda y, pos: '%.0f' % (y * 1e-3)))\n    plt.ylabel('')\nfig.suptitle('Stripplots of ordinal features vs SalePrice in 000 USD', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","3b879ff1":"fig = plt.figure(figsize=[15, 20])\nfeature_list = sorted(ORDINAL_FEATURE_NAMES)\nnum_subplots = len(feature_list)\nncols = 4\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    if ames[feature].dtype == np.object:\n        order = ordinal_features_recoding.loc[ordinal_features_recoding['Name']\n                                              == feature, 'Code']\n    else:\n        order = None\n    sns.boxplot(x=ames[feature], y=ames['SalePrice'], color='darkcyan', order=order, ax=ax)\n    ax.yaxis.set_major_formatter(ticker.FuncFormatter(\n        lambda y, pos: '%.0f' % (y * 1e-3)))\n    plt.ylabel('')\nfig.suptitle('Boxplots of ordinal features vs SalePrice in 000 USD', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","43993b67":"fig = plt.figure(figsize=[15, 20])\nfeature_list = sorted(NOMINAL_FEATURE_NAMES)\nfeature_list.remove('Id')\nnum_subplots = len(feature_list)\nncols = 4\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    sns.countplot(x=feature, data=ames, color='tomato', ax=ax)\n    plt.xticks(rotation=90)\n    ax.set_ylabel('')\nfig.suptitle('Count plots of nominal features', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","bb542f08":"fig = plt.figure(figsize=[15, 20])\nfeature_list = sorted(NOMINAL_FEATURE_NAMES)\nfeature_list.remove('Id')\nnum_subplots = len(feature_list)\nncols = 4\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    sns.stripplot(x=ames[feature], y=ames['SalePrice'], color='tomato',\n                  edgecolor='whitesmoke', linewidth=0.4, size=7, jitter=True, ax=ax)\n    plt.ylabel('')\n    plt.xticks(rotation=90)\n    ax.yaxis.set_major_formatter(ticker.FuncFormatter(\n        lambda y, pos: '%.0f' % (y * 1e-3)))\nfig.suptitle('Stripplots of nominal features vs SalePrice in 000 USD', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","05f9ef8d":"fig = plt.figure(figsize=[15, 25])\nfeature_list = sorted(NOMINAL_FEATURE_NAMES)\nfeature_list.remove('Id')\nnum_subplots = len(feature_list)\nncols = 4\nnrows = num_subplots \/\/ ncols + 1\nfor n, feature in enumerate(feature_list):\n    ax = fig.add_subplot(nrows, ncols, n+1)\n    sns.boxplot(x=ames[feature], y=ames['SalePrice'], color='tomato', ax=ax)\n    plt.ylabel('')\n    plt.xticks(rotation=90)\n    ax.yaxis.set_major_formatter(ticker.FuncFormatter(\n        lambda y, pos: '%.0f' % (y * 1e-3)))\nfig.suptitle('Stripplots of nominal features vs SalePrice in 000 USD', fontsize=18)\nfig.tight_layout(rect=[0, 0.03, 1, 0.96])","2bb8c2e5":"# Overall correlation matrix\ncorr_matrix_pearson = ames.corr()\nplt.figure(figsize=(11, 9))\nsns.heatmap(corr_matrix_pearson, square=True)\nplt.show()","016a0830":"# Features most correlated with SalePrice\ntop_correlated = corr_matrix_pearson.nlargest(10, 'SalePrice')[\n    'SalePrice'].index\ncorr_matrix_pearson_top_correlated = corr_matrix_pearson.loc[top_correlated, top_correlated]\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix_pearson_top_correlated, cbar=True,\n            annot=True, square=True, fmt='.2f', annot_kws={'size': 12})\nplt.show()","91c854ac":"def inspect_feature(feature, get_text=True, get_plots=True):\n    \"\"\"Prints general info on the selected feature (name, type, desctiprion) from ames,\n    as well as results of describe for numeric features and results of value_counts \n    for ordinal and nominal features. Generates separate sets of discriptive plots \n    for numeric (distplot, scatterplot vs SalePrice) and other features (countplot, \n    barplot of median SalePrice for each category, stripplots vs SalePrice and \n    log SalePrice).\n    \"\"\"\n\n    assert feature in ames.columns, \\\n    'feature must be name of an existing column in ames DataFrame!'\n    \n    f_type = feature_description.loc[feature_description['Name']\n                                      == feature, 'Type2'].values[0]\n    f_description = feature_description.loc[feature_description['Name']\n                                             == feature, 'Description'].values[0]\n    if get_text:\n\n        # Print general info\n        print('Name:', feature)\n        print('Type:', f_type)\n        print('Description:', f_description, '\\n')\n\n        # Print describe() or value_counts()\n        if f_type == 'Numeric':\n            f_stats = ames[feature].describe()\n            f_stats['nan'] = ames[feature].isnull().sum()\n            print('Stats:\\n', pd.DataFrame(f_stats), sep='')\n        else:\n            val_counts = pd.DataFrame(ames[feature].value_counts(dropna=False))\n            val_counts.rename({feature: 'Count'}, axis=1, inplace=True)\n            val_counts['Fraction'] = val_counts['Count'] \/ \\\n                val_counts['Count'].sum()\n            val_counts = pd.merge(val_counts, \n                                  (feature_categories\n                                   .loc[feature_categories['Name'] == feature, :]\n                                   .astype(str)),\n                                  how='left', \n                                  left_on=val_counts.index.astype(str),\n                                  right_on='Code')\n            val_counts = val_counts[['Code', 'Category', 'Count', 'Fraction']]\n            print('Value counts:\\n', val_counts, sep='')\n\n    if f_type == 'Numeric' and ames[feature].nunique() <= 10:\n        f_type = 'Pseudo Ordinal'\n\n    if get_plots:\n\n        # Variables for use in plotting\n        formatter000 = ticker.FuncFormatter(lambda y, pos: '%.0f' % (y * 1e-3))\n        log_saleprice = ames['SalePrice'].apply(np.log)\n        if ames[feature].dtype == np.object:\n            if f_type == 'Ordinal':\n                order = ordinal_features_recoding.loc[ordinal_features_recoding['Name']\n                                                      == feature, 'Code']\n            if f_type == 'Nominal':\n                order = feature_categories.loc[feature_categories['Name'] == feature, 'Code']\n        else:\n            order = None\n\n        # Generete plots for numeric features\n        if f_type == 'Numeric':\n            from warnings import filterwarnings\n            filterwarnings(\"ignore\", category=UserWarning)\n            rc_dict = {'axes.titlesize': 17, 'axes.labelsize': 14,\n                       'xtick.labelsize': 14, 'ytick.labelsize': 14}\n            with plt.rc_context(rc=rc_dict):\n                fig = plt.figure(figsize=[15, 4])\n                layout = (1, 2)\n                ax1 = plt.subplot2grid(layout, (0, 0), rowspan=1, colspan=1)\n                ax2 = plt.subplot2grid(layout, (0, 1), rowspan=1, colspan=1)\n                # Distplot\n                sns.distplot(ames[feature].dropna(), ax=ax1, bins=10,\n                             kde=False, hist_kws={'alpha': 1, 'edgecolor': 'white'})\n                ax1.set_title('Distplot')\n                # Scatter plot vs SalePrice\n                ax2.scatter(x=ames[feature], y=ames['SalePrice'], s=70,\n                            edgecolor='white', linewidth=0.5,)\n                ax2.set_xlabel(feature)\n                ax2.set_ylabel('SalePrice, 000 USD')\n                ax2.yaxis.set_major_formatter(formatter000)\n                ax2.set_title('Scatter plot vs SalePrice')\n                fig.tight_layout()\n\n        # Generete plots for ordinal and nominal features\n        else:\n            rc_dict = {'axes.titlesize': 17, 'axes.labelsize': 14,\n                       'xtick.labelsize': 13, 'ytick.labelsize': 14}\n            with plt.rc_context(rc=rc_dict):\n                fig = plt.figure(figsize=[15, 10])\n                layout = (2, 2)\n                ax1 = plt.subplot2grid(layout, (0, 0), rowspan=1, colspan=1)\n                ax2 = plt.subplot2grid(layout, (0, 1), rowspan=1, colspan=1)\n                ax3 = plt.subplot2grid(layout, (1, 0), rowspan=1, colspan=1)\n                ax4 = plt.subplot2grid(layout, (1, 1), rowspan=1, colspan=1)\n                # Countplot\n                sns.countplot(x=feature, data=ames,\n                              color='darkcyan', order=order, ax=ax1)\n                ax1.set_title('Countplot')\n                # Barplot of median SalePrice for each category\n                grouped = ames.groupby(feature)['SalePrice'].median()\n                sns.barplot(x=grouped.index, y=grouped,\n                            color='darkcyan', order=order, ax=ax2)\n                ax2.yaxis.set_major_formatter(formatter000)\n                ax2.set_title('Median SalePrice for each category')\n                # Stripplot vs SalePrice\n                sns.stripplot(x=ames[feature], y=ames['SalePrice'], color='darkcyan',\n                              edgecolor='white', linewidth=0.5, size=9, jitter=True,\n                              order=order, ax=ax3)\n                ax3.yaxis.set_major_formatter(formatter000)\n                ax3.set_title('Scatter plot vs SalePrice')\n                # Stripplot plot vs log SalePrice\n                sns.stripplot(x=ames[feature], y=log_saleprice, color='darkcyan',\n                              edgecolor='white', linewidth=0.5, size=9, jitter=True,\n                              order=order, ax=ax4)\n                ax4.set_title('Scatter plot vs log SalePrice')\n                for ax in fig.axes:\n                    for tick in ax.get_xticklabels():\n                        tick.set_rotation(90)\n                    ax.set_xlabel('')\n                    ax.set_ylabel('')\n                fig.tight_layout()","1c583c88":"def get_nan_counts(df):\n    \"\"\"Generates sorted nan counts and percentages for columns in a DataFrame.\"\"\"\n    nan_counts = pd.DataFrame(df.isnull().sum()).reset_index()\n    nan_counts.columns = ['feature', 'is nan']\n    nan_counts['is nan ratio'] = nan_counts['is nan'] \/ df.shape[0]\n    nan_counts = nan_counts.sort_values('is nan ratio', ascending=False)\n    return nan_counts\n    \n\ndef association_test(X, y, sort_column=2):\n    \"\"\"Creates a DataFrame with measures of association between\n    features in X, and y. Measures include 'Pearson's r', \n    'Spearman's rho', 'Root R^2', and 'p-value of F'.\n    \"\"\"\n    # Regression\n    output_regr = []\n    X = pd.DataFrame(X)\n    for feature in X.columns:\n        if X[feature].dtype == np.object:\n            dm = pd.get_dummies(X[feature])\n        else:\n            dm = X[feature]\n        dm = sm.add_constant(dm)\n        result = sm.OLS(y, dm.astype(float), missing='drop').fit()\n        output_regr.append({'Feature': feature, 'Root R^2': np.sqrt(\n            result.rsquared), 'p-value of F': result.f_pvalue})\n    output_regr = pd.DataFrame(output_regr).set_index('Feature')\n    output_regr.index.name = None\n    \n    # Correlation\n    X = X.select_dtypes(exclude=np.object)\n    pearson = X.apply(lambda col: col.corr(y, method='pearson'))\n    spearman = X.apply(lambda col: col.corr(y, method='spearman'))\n    output_correl = pd.concat([pearson, spearman], axis=1)\n    output_correl.columns = [\"Pearson's r\", \"Spearman's rho\"]\n    \n    # Combined output\n    sort = True if sort_column == 'index' else False\n    output = pd.concat([output_correl, output_regr], sort=sort, axis=1)\n    if sort_column != 'index':\n        output = output.sort_values(output.columns[sort_column], ascending=False)\n    return output","2eeff1db":"# Preliminaty regression modelling shows that this range contains two egregious outliers\noutliers_index = train.loc[(train['GrLivArea'] > 4000)\n                           & (target < 300000), :].index\ntrain.drop(outliers_index, inplace=True)\ntarget.drop(outliers_index, inplace=True)\n\n# Id feature is useless without geolocation\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)\n\n# Log-transform SalePrice in order to make its distribution closer to normal\ntarget = target.apply(np.log)","ff248805":"get_nan_counts(train).head(20)","18c13545":"get_nan_counts(test).head(20)","b7fe2269":"replace_meaningful_nan(train, FEATURES_WITH_MEANINGFUL_NAN)\nget_nan_counts(train).head(10)","1ae951e9":"replace_meaningful_nan(test, FEATURES_WITH_MEANINGFUL_NAN)\nget_nan_counts(test).head(20)","787d7fdd":"def impute_nan_manually(df):\n    \"\"\"Manual NaN imputation that does not cause data leakage.\n    This must be done before recoding ordinal features.\n    \"\"\"\n    # Correct typo\n    test['GarageYrBlt'].replace(2207, 2007, inplace=True)\n    # Replace wrong missing garage with nan\n    for col in ['GarageQual', 'GarageCond']:\n        wrong_missing_garage_filter = (\n            df['GarageType'] != 'Missing') & (df[col] == 'Missing')\n        df.loc[wrong_missing_garage_filter, col] = np.nan\n    # Set year to 1900 for missing garages\n    missing_garage_filter = df['GarageType'] == 'Missing'\n    df.loc[missing_garage_filter, 'GarageYrBlt'] = 1900\n    # Impute nan for MasVnr\n    both_vnr_nan_filter = df['MasVnrType'].isnull() & df['MasVnrArea'].isnull()\n    vnr_type_nan_filter = df['MasVnrType'].isnull() & (df['MasVnrArea'] > 0)\n    df.loc[both_vnr_nan_filter, 'MasVnrType'] = 'None'\n    df.loc[both_vnr_nan_filter, 'MasVnrArea'] = 0\n    df.loc[vnr_type_nan_filter, 'MasVnrType'] = 'BrkFace'\n    # Set nan to 0 for missing basements\n    missing_bsmt_filter = (df['BsmtQual'] == 'Missing') & (\n        df['BsmtCond'] == 'Missing')\n    df.loc[missing_bsmt_filter, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                          'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = 0","58fe8e97":"impute_nan_manually(train)\nimpute_nan_manually(test)","3bc2f3bd":"def recode_ordinal(df):\n    \"\"\"Replaces all string codes of ordinal features \n    with their numeric equivalents.\"\"\"\n\n    feature_names = set(ordinal_features_recoding['Name'])\n    for col in feature_names:\n        ser = ordinal_features_recoding.loc[ordinal_features_recoding['Name'] == col,\n                               ['Code', 'NumericCode_1']].set_index('Code')\n        ser = pd.Series(ser['NumericCode_1'])\n        if df[col].dtype == 'O':\n            df[col] = df[col].map(ser)","802061c8":"recode_ordinal(train)\ntrain[ORDINAL_FEATURE_NAMES].dtypes","4cb191d1":"recode_ordinal(test)\ntest[ORDINAL_FEATURE_NAMES].dtypes","bd6ed392":"area_names_filter = ['area', 'square']\narea_names = list(feature_description['Name'][feature_description['Description'].str.contains(\n    '|'.join(area_names_filter))])\nfeature_description.loc[feature_description['Name'].isin(area_names), ['Name', 'Description']]","85a19df8":"# Some area features are sums of other features:\n# TotalBsmtSF = BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF\n# GrLivArea = 1stFlrSF + 2ndFlrSF\n\nfloor_area_names = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                    'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea']\ntrain[floor_area_names].head(8)","c3702359":"association_test(train[area_names], target, sort_column=0)","5e4771a7":"def transform_area(df, names):\n    \n    # Porch features\n    df['TotalPorchArea'] = df['EnclosedPorch'] + \\\n        df['3SsnPorch'] + df['OpenPorchSF']\n    porch_type = df['TotalPorchArea'].apply(\n        lambda x: 'Missing' if x == 0 else 'Multiple')\n    porch_type[(df['TotalPorchArea'] == df['EnclosedPorch'])\n               & (df['EnclosedPorch'] > 0)] = 'Enclosed'\n    porch_type[(df['TotalPorchArea'] == df['3SsnPorch'])\n               & (df['3SsnPorch'] > 0)] = '3Ssn'\n    porch_type[(df['TotalPorchArea'] == df['OpenPorchSF'])\n               & (df['OpenPorchSF'] > 0)] = 'Open'\n    df['PorchType'] = porch_type\n    porch_names = ['EnclosedPorch', '3SsnPorch', 'OpenPorchSF']\n    df.drop(porch_names, axis=1, inplace=True)\n    \n    # Garage features\n    df['GarageAreaPerCar'] = df['GarageArea'] \/ df['GarageCars']\n    df.loc[~np.isfinite(df['GarageAreaPerCar']), 'GarageAreaPerCar'] = 0\n    df.drop(['GarageArea'], axis=1, inplace=True)\n    \n    # Bathrooms\n    df['TotalBath'] = df['BsmtFullBath'] + df['BsmtHalfBath'] + df['FullBath'] + df['HalfBath']\n    df.drop(['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath'], axis=1, inplace=True)\n    \n    # Express area features as relations to corresponding totals\n    df['TotalFloorArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n    by_tbsf_names = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF']\n    by_gla_names = ['1stFlrSF', '2ndFlrSF']\n    include = ['TotalPorchArea']\n    exclude = ['TotalFloorArea', 'GarageArea']\n    by_tfa_names = set(names + include) - \\\n        set(by_tbsf_names + by_gla_names + exclude + porch_names)\n    divisors = ['TotalBsmtSF', 'GrLivArea', 'TotalFloorArea']\n    tags = ['TBSF', 'GLA', 'TFA']\n    for feats, div, tag in zip([by_tbsf_names, by_gla_names, by_tfa_names], divisors, tags):\n        for f in feats:\n            new_name = f + '\/' + tag\n            df[new_name] = df[f] \/ df[div]\n#             df.drop(f, axis=1, inplace=True)\n            df.loc[~np.isfinite(df[new_name]), new_name] = 0\n    \n#     # Total floor area per room, car, fireplace\n#     tally_names = ['GarageCars', 'TotRmsAbvGrd', 'Fireplaces', 'BedroomAbvGr', 'KitchenAbvGr']\n#     for f in tally_names:\n#         new_name = 'TFA\/' + f\n#         df[new_name] = df['TotalFloorArea'] \/ df[f]\n#         df.loc[~np.isfinite(df[new_name]), new_name] = 0","17f77fe9":"transform_area(train, area_names)\ntransform_area(test, area_names)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","353f0141":"searchfor = ['\/TBSF', '\/GLA', '\/TFA', 'TotalFloorArea']\nnew_area_names = list(train.columns[train.columns.str.contains('|'.join(searchfor))])\nassociation_test(train[new_area_names], target, sort_column=0)","73987f80":"# searchfor = ['TFA\/']\n# new_tally_names = list(train.columns[train.columns.str.contains('|'.join(searchfor))])\n# association_test(train[new_tally_names], target, sort_column=0)","0966be58":"# plt.scatter(x=train['TFA\/KitchenAbvGr'], y=target)","3fa2d429":"bsmt_name_filter = ['Bsmt']\nbsmt_names = list(train.columns[train.columns.str.contains('|'.join(bsmt_name_filter))])\nfeature_description.loc[feature_description['Name'].isin(bsmt_names), ['Name', 'Description']]","eecf7257":"inspect_feature('BsmtFullBath')","c339d602":"# Try weighted basement finished area\n\nbsmt_fin_sf_wghtd = train['BsmtFinType1'] * train['BsmtFinSF1\/TBSF'] + \\\n    train['BsmtFinType2'] * train['BsmtFinSF2\/TBSF']\nplt.scatter(x=bsmt_fin_sf_wghtd, y=target, s=70,\n            edgecolor='white', linewidth=0.5,)\nplt.scatter(x=bsmt_fin_sf_wghtd[bsmt_fin_sf_wghtd < 0.4], \n            y=target[bsmt_fin_sf_wghtd < 0.4], s=70, c='red',\n            edgecolor='white', linewidth=0.5)","3d91aad2":"def transform_bsmt(df):\n    # Add weighted basement finished area, drop redundant area features\n    df['BsmtFinSFWghtd'] = df['BsmtFinType1'] * df['BsmtFinSF1\/TBSF'] + \\\n        df['BsmtFinType2'] * df['BsmtFinSF2\/TBSF']\n    to_drop = ['BsmtFinType1', 'BsmtFinSF1\/TBSF', 'BsmtFinType2', 'BsmtFinSF2\/TBSF']\n    df.drop(to_drop, axis=1, inplace=True)\n    \n    # Add indicator features\n    df['BsmtCond_TA'] = df['BsmtCond'].apply(lambda x: 1 if x == 4 else 0)\n    df['BsmtFinSFWghtd_lessthan0.4'] = df['BsmtFinSFWghtd'].apply(lambda x: 1 if x < 4 else 0)\n    \n    # Add polynomial features\n    df['BsmtQual_p2'] = df['BsmtQual'].apply(lambda x: np.power(x, 2))","6d005279":"transform_bsmt(train)\ntransform_bsmt(test)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","b52dba9e":"qual_name_filter = ['Qual', 'Qu', 'QC', 'Fence']\nqual_names = list(feature_description['Name'][feature_description['Name'].str.contains(\n    '|'.join(qual_name_filter))])\nqual_names.remove('LowQualFinSF')\nfeature_description.loc[feature_description['Name'].isin(qual_names), ['Name', 'Description']]","3509ff8c":"association_test(train[qual_names], target)","c4f970e9":"inspect_feature('GarageQual')","c2b207da":"def transform_qual(df):\n    # Absence of fence appear to add to SalePrice,\n    # which does not make sense.\n    df.drop(['Fence'], axis=1, inplace=True)\n    \n    # Aggregate all pool features into one\n    df['HasPool'] = df['PoolQC'].apply(lambda x: 0 if x == 1 else 1)\n    to_drop = ['PoolQC', 'PoolArea\/TFA']\n    df.drop(to_drop, axis=1, inplace=True)\n    \n    # Express all quality features as relation to OverallQual\n    divide_by_OQ = ['ExterQual', 'BsmtQual', 'HeatingQC',\n                    'KitchenQual', 'FireplaceQu', 'GarageQual']\n    for f in divide_by_OQ:\n        max_code = ordinal_features_recoding.loc[ordinal_features_recoding['Name'] == f, \n                                                 'NumericCode_1'].max()\n        rescaled = (df[f] \/ max_code)*10\n        new_name = f + '\/OQ'\n        # Dividing quality features by OverallQual produces new features\n        # that show weird \"sawtooth\" relationship pattern with SalePrice. \n        # Below is an attempt to make it more linear.\n        df[new_name] = (rescaled - df['OverallQual']) \/ df['OverallQual']\n        df[new_name] = df[new_name].apply(lambda x: x - x if x < 0 else x)\n        df.drop(f, axis=1, inplace=True)","59362e97":"transform_qual(train)\ntransform_qual(test)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","22bb5777":"searchfor = ['\/OQ']\nnew_qual_names = list(train.columns[train.columns.str.contains('|'.join(searchfor))])\nassociation_test(train[new_qual_names], target, sort_column=0)","a8e7fb30":"cond_names = ['OverallCond', 'ExterCond', 'BsmtCond', 'GarageCond']\nfeature_description.loc[feature_description['Name'].isin(cond_names), ['Name', 'Description']]","a872122d":"association_test(train[cond_names], target)","f8601237":"inspect_feature('OverallCond')","e81cee17":"def transform_cond(df):\n    # Add indicator features\n    df['OverallCond_5'] = df['OverallCond'].apply(lambda x: 1 if x == 5 else 0)\n    df['ExterCond_TA'] = df['ExterCond'].apply(lambda x: 1 if x == 3 else 0)\n    df['GarageCond_TA'] = df['GarageCond'].apply(lambda x: 1 if x == 4 else 0)","240e6bc7":"transform_cond(train)\ntransform_cond(test)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","b070e96c":"feature_description.loc[feature_description['Type2'] == 'Nominal', ['Name', 'Description']]","144f0e8f":"inspect_feature('MasVnrType', get_text=True)","3f30f9de":"association_test(train[list(set(NOMINAL_FEATURE_NAMES) & set(train.columns))], target)","bd0b1835":"def transfort_nominal(df):\n    # Add indicator features\n    df['Condition12_PosX'] = (df['Condition1'].isin(['PosN', 'PosA']) |\n                              df['Condition1'].isin(['PosN', 'PosA'])) * 1\n    df['HasShed'] = (df['MiscFeature'] == 'Shed') * 1\n    # Merge categories\n    df['BldgType'] = df['BldgType'].apply(\n        lambda x: 'Other' if x not in ['1Fam', 'TwnhsE'] else x)\n    df['RoofStyle'] = df['RoofStyle'].apply(\n        lambda x: 'Other' if x not in ['Gable', 'Hip'] else x)\n    df['RoofMatl'] = df['RoofMatl'].apply(lambda x: 'Other' if x not in [\n                                          'CompShg', 'Tar&Grv', 'WdShngl', 'WdShake'] else x)\n    df['Foundation'] = df['Foundation'].apply(\n        lambda x: 'StoneWood' if x in ['Stone', 'Wood'] else x)\n    df['Heating'] = df['Heating'].apply(\n        lambda x: 'Other' if x in ['Floor', 'Grav', 'OthW', 'Wall'] else x)\n    df['SaleType'] = df['SaleType'].apply(\n        lambda x: 'Other' if x not in ['WD', 'New', 'COD'] else x)\n    # Binarize\n    df['CentralAir'] = df['CentralAir'].apply(\n        lambda x: 1 if x == 'Y' else 0).astype(np.int)\n    # Drop useless or redundant\n    df.drop(['MiscFeature', 'MiscVal'], axis=1, inplace=True)","d04202a2":"transfort_nominal(train)\ntransfort_nominal(test)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","0458733a":"feature_description.loc[feature_description['Type2'] == 'Numeric', ['Name', 'Description']]","d139dc80":"inspect_feature('GarageYrBlt')","a2273591":"association_test(train[list(set(NUMERIC_FEATURE_NAMES) & set(train.columns))], target)","9cedb242":"def transform_age(df):\n    df['AgeBuilt'] = df['YrSold'] - df['YearBuilt']\n    df['AgeRemod'] = df['YrSold'] - df['YearRemodAdd']\n    df['AgeGarage'] = df['YrSold'] - df['GarageYrBlt']\n    df['EffectiveAge'] = (df['YearRemodAdd'] -\n                          df['YearBuilt']) * 0.6 + df['AgeRemod']\n    df.drop(['YearBuilt', 'YearRemodAdd', 'GarageYrBlt',\n             'AgeBuilt', 'AgeRemod'], axis=1, inplace=True)","3ca9d99b":"transform_age(train)\ntransform_age(test)\nprint('train.shape:', train.shape)\nprint('test.shape:', test.shape)","ddc4c467":"feature_description.loc[feature_description['Type2'] == 'Ordinal', ['Name', 'Description']]","8db0af1d":"association_test(train[list(set(ORDINAL_FEATURE_NAMES) & set(train.columns))], target)","2f436b7f":"inspect_feature('Functional')","09a04281":"# Top 30 features most correlated with SalePrice\npt_assoc_test = association_test(train, target)\npt_assoc_test.head(30)","cb757a08":"# Top 20 features with largest positive skew\nskewness = train.select_dtypes(include=np.number).apply(\n    lambda x: stats.skew(x, nan_policy='propagate'))\nskewness = pd.DataFrame(skewness.sort_values(ascending=False))\nskewness.columns = ['Skew']\nskewness.head(20)","e2c493ba":"# Show the effect of power transformation\ntransf = np.power(train['GarageCars'], 1)\nprint('Skew:', stats.skew(transf))\nprint('\\nAssociation test:\\n', association_test(transf, target))\nfig, ax = plt.subplots(1, 1, figsize=[15, 5])\nsns.distplot(transf, ax=ax)\nplt.show()","3bf5eb8c":"def add_power_transformed(df):\n    # Add root transformed features\n    df['AgeGarage_p1\/2'] = df['AgeGarage'].apply(lambda x: np.power(x, 1\/2))\n    df['TotRmsAbvGrd_p1\/4'] = df['TotRmsAbvGrd'].apply(lambda x: np.power(x, 1\/4))\n    df['TotalFloorArea_p1\/3'] = df['TotalFloorArea'].apply(lambda x: np.power(x, 1\/3))\n    df['GrLivArea_p1\/6'] = df['GrLivArea'].apply(lambda x: np.power(x, 1\/6))\n    df['ExterQual\/OQ_p1\/3'] = df['ExterQual\/OQ'].apply(lambda x: np.power(x, 1\/3))\n    df['BsmtQual\/OQ_p1\/2'] = df['BsmtQual\/OQ'].apply(lambda x: np.power(x, 1\/2))\n    df['KitchenQual\/OQ_p1\/2.5'] = df['KitchenQual\/OQ'].apply(lambda x: np.power(x, 1\/2.5))\n    df['LotArea\/TFA_p1\/7'] = df['LotArea\/TFA'].apply(lambda x: np.power(x, 1\/7))\n    df['LotArea_p1\/10'] = df['LotArea'].apply(lambda x: np.power(x, 1\/10))\n    df['KitchenAbvGr_p1\/3'] = df['KitchenAbvGr'].apply(lambda x: np.power(x, 1\/3))\n    # Add interactions\n    df['TotalFloorArea*OverallQual'] = df['TotalFloorArea'] * df['OverallQual']\n    df['GrLivArea*OverallQual'] = df['GrLivArea'] * df['OverallQual']\n    df['GarageCars*BsmtQual_p2'] = df['GarageCars'] * df['BsmtQual_p2']\n    df['TotalBsmtSF*TotalBath'] = df['TotalBsmtSF'] * df['TotalBath']\n    df['1stFlrSF*GarageFinish'] = df['1stFlrSF'] * df['GarageFinish']\n    df['TotalBsmtSF*BsmtQual_p2'] = df['TotalBsmtSF'] * df['BsmtQual_p2']\n    df['GarageCars*GarageFinish'] = df['GarageCars'] * df['GarageFinish']\n    \nadd_power_transformed(train)\nadd_power_transformed(test)","14a3a0c6":"modes = train.mode().iloc[0, :]\nmode_counts = (train.values == modes.values).sum(axis=0)\nmode_counts = pd.DataFrame(mode_counts, index=train.columns, columns=['Count'])\nmode_counts['Mode'] = modes\nmode_counts['Fraction'] = mode_counts['Count'] \/ train.shape[0]\nmode_counts.sort_values('Fraction', inplace=True, ascending=False)\nmode_counts = mode_counts[['Mode', 'Count', 'Fraction']]\nmode_counts.head(10)","2903557f":"train.drop(['Utilities'], axis=1, inplace=True)\ntest.drop(['Utilities'], axis=1, inplace=True)","7f592967":"def trainsform_object_to_categorical(train_df, test_df):\n    \"\"\"Assigns a common exhaustive set of categories for each\n    feature of type object in train and test. This elimates the\n    need to align training and validatio\/test datasets after\n    dummification.\n    \"\"\"\n    assert np.sum(train_df.columns != test_df.columns) == 0\n    obj_cols = list(train_df.select_dtypes(include=np.object).columns)\n    for df in [train_df, test_df]:\n        for col in obj_cols:\n            categories = (set(train_df[col].unique()) |\n                          set(test_df[col].unique())) - set([np.nan])\n            cat_type = CategoricalDtype(\n                categories=categories, ordered=None)\n            df[col] = df[col].astype(cat_type)","929d6ebb":"trainsform_object_to_categorical(train, test)","034ee091":"class MyImputer(BaseEstimator, TransformerMixin):\n    def __init__(self, to_mode=None):\n        self.to_mode = to_mode\n        self.X_ft = None\n\n    def fit(self, X, y=None):\n        assert isinstance(X, pd.DataFrame)\n        self.X_ft = X\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        X_tr = X.copy()\n        nan_cols = X_tr.columns[X_tr.isnull().any()]\n        \n        # Fill via groupby\n        to_groupby = ['MSZoning'] #['LotFrontage', 'MSZoning']\n#         if 'LotFrontage' in nan_cols:\n#             median_lotfront_per_hood = self.X_ft.groupby(\n#                 'Neighborhood')['LotFrontage'].agg(np.median)\n#             median_lotfront_mapped = X_tr['Neighborhood'].map(\n#                 median_lotfront_per_hood)\n#             X_tr['LotFrontage'].fillna(median_lotfront_mapped, inplace=True)\n        if 'MSZoning' in nan_cols:\n            mode_mszoning_per_hood = self.X_ft.groupby(\n                'Neighborhood')['MSZoning'].agg(pd.Series.mode)\n            mode_mszoning_mapped = X_tr['Neighborhood'].map(mode_mszoning_per_hood)\n            X_tr['MSZoning'].fillna(mode_mszoning_mapped, inplace=True)\n            \n        # Fill with mode\n        to_mode = list(set(self.to_mode) & set(nan_cols) - set(to_groupby))\n        mode_ft = self.X_ft[self.to_mode].mode().iloc[0, :]\n        X_tr.loc[:, self.to_mode] = X_tr.loc[:,\n                                             self.to_mode].fillna(mode_ft)\n        \n        # Fill with median\n        to_median = list(set(nan_cols) - set(self.to_mode) -\n                         set(to_groupby))\n        median_ft = self.X_ft[to_median].median()\n        X_tr.loc[:, to_median] = X_tr.loc[:, to_median].fillna(median_ft)\n        \n        return X_tr","9c05a8e7":"class MyDummifier(BaseEstimator, TransformerMixin):\n    def __init__(self, params={}):\n        self.params = params\n        self.cols = None\n\n    def fit(self, X, y=None):\n        assert isinstance(X, pd.DataFrame)\n        return self\n\n    def transform(self, X):\n        assert isinstance(X, pd.DataFrame)\n        X = X.copy()\n        X = pd.get_dummies(X, **self.params)\n        return X","8feaaf47":"# Standard corss-validation KFold object\nkfolds = KFold(n_splits=5, shuffle=True, random_state=238)","e4d76152":"def rmse(y, y_hat):\n    \"\"\"Calculate RMSE based on actual and predicted values of dependent variable\"\"\"\n    return np.sqrt(mean_squared_error(y, y_hat))\n\n\ndef rmse_cv(estimator, X, y, kf=None):\n    \"\"\"Calculate cross-validated RMSE when dependent variable is already log-transformed.\"\"\"\n    rmse_cv = np.sqrt(-cross_val_score(estimator, X, y,\n                                       scoring=\"neg_mean_squared_error\",\n                                       cv=kf))\n    return rmse_cv\n\n\ndef rmsle_cv(estimator, X, y, kf=None):\n    \"\"\"Calculate cross-validated RMSLE.\"\"\"\n    rmsle_cv = np.sqrt(np.log(-cross_val_score(estimator, X, y,\n                                               scoring='neg_mean_squared_error',\n                                               cv=kf)))\n    return rmsle_cv\n\n\ndef run_rmse_cv(estimator, X, y, kf=None, record_name=None, num_results_to_show=2):\n    \"\"\"Wrapper function that runs rmse_cv, writes the results of all \n    runs into a dict of lists, and prints the results of a specified number of runs.\n    \"\"\"\n    result = rmse_cv(estimator, X, y, kf).mean()\n    if record_name != None:\n        try:\n            global estimator_result_record\n            assert isinstance(estimator_result_record, dict)\n        except:\n            globals()['estimator_result_record'] = dict()\n        try:\n            estimator_result_record[record_name]\n        except:\n            estimator_result_record[record_name] = []\n        estimator_result_record[record_name].append(result)\n        num_results_recorded = len(estimator_result_record[record_name])\n        for n in reversed(range(num_results_to_show)):\n            try:\n                print('RMSLE for run {}:'.format(num_results_recorded - n),\n                      estimator_result_record[record_name][-(n+1)])\n            except:\n                continue\n    return result\n\n\ndef run_gridsearch(estimators, param_grid=None, cv=kfolds):\n    \"\"\"Run standard grid-search and show results.\"\"\"\n    pipe = make_pipeline(preprocessing_pipe, *estimators)\n    grid = GridSearchCV(pipe,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='neg_mean_squared_error')\n    grid.fit(train, target)\n    return grid.best_params_, np.sqrt(-grid.best_score_)","5db492d8":"# Standard preprocessing pipeline for all estimators\nto_mode = list(train.select_dtypes(include='category').columns) + ORDINAL_FEATURE_NAMES\nto_mode = list(set(to_mode) & set(train.columns))\npreprocessing_pipe = make_pipeline(MyImputer(to_mode=to_mode), MyDummifier())","d68dda49":"linreg_pipe = make_pipeline(preprocessing_pipe, LinearRegression())\nlinreg_rmse_cv = run_rmse_cv(linreg_pipe, train, target,\n                             kf=kfolds, record_name='linreg', num_results_to_show=4)","ed88cd00":"# # Grid search for alpha\n\n# param_grid = {'ridge__alpha': [0.0001, 0.0003, 0.0005, 0.0007, 0.0009,\n#                                0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50]}\n# run_gridsearch([RobustScaler(), Ridge()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'ridge__alpha': 15}, 0.11028230638314304)","f22eea18":"# Ridge with tweaked grid-searched alpha\nridge_pipe = make_pipeline(preprocessing_pipe, Ridge(**{'alpha': 17}))\nridge_rmse_cv = run_rmse_cv(ridge_pipe, train, target,\n                            kf=kfolds, record_name='ridge', num_results_to_show=4)","091e2c52":"# # Grid search for alpha\n\n# # param_grid = {'lasso__alpha': [0.0001, 0.0003, 0.0005, 0.0007, 0.0009,\n# #                                0.01, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 50]}\n# param_grid = {'lasso__alpha': [0.00047, 0.00048, 0.00049, 0.00050, 0.00051, \n#                                0.00052, 0.00053, 0.00054, 0.00055, 0.00056, 0.00057]}\n# run_gridsearch([RobustScaler(), Lasso()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'lasso__alpha': 0.00053}, 0.10711245295695188)","f6058e01":"# Lasso with tweaked grid-searched alpha\nlasso_pipe = make_pipeline(\n    preprocessing_pipe, RobustScaler(), Lasso(**{'alpha': 0.00055}))\nlasso_rmse_cv = run_rmse_cv(lasso_pipe, train, target,\n                            kf=kfolds, record_name='lasso', num_results_to_show=4)","14d491b7":"# # Grid search for parameters\n\n# param_grid = {'elasticnet__alpha': [0.0001, 0.0003, 0.0005, 0.0007, 0.0009,\n#                                     0.01, 0.05, 0.1],\n#               'elasticnet__l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]}\n# run_gridsearch([RobustScaler(), ElasticNet()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'elasticnet__alpha': 0.0005, 'elasticnet__l1_ratio': 1.0},\n# #  0.10712358737747567)","7c39bf22":"# ElasticNet with tweaked grid-searched parameters\nenet_pipe = make_pipeline(preprocessing_pipe,\n                          RobustScaler(),\n                          ElasticNet(**{'alpha': 0.00055,\n                                        'l1_ratio': 1}))\nenet_rmse_cv = run_rmse_cv(enet_pipe, train, target,\n                           kf=kfolds, record_name='enet', num_results_to_show=2)","7fdbddb3":"# # Grid search for parameters\n\n# param_grid = {'randomforestregressor__n_estimators': [5, 15, 30, 40, 100]\n#               , 'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']\n#               , 'randomforestregressor__min_samples_leaf': [1, 3, 4, 5]\n#               , 'randomforestregressor__min_samples_split': [2, 8, 10, 12]\n#              }\n# run_gridsearch([RandomForestRegressor()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'randomforestregressor__max_features': 'sqrt',\n# #   'randomforestregressor__min_samples_leaf': 1,\n# #   'randomforestregressor__min_samples_split': 2,\n# #   'randomforestregressor__n_estimators': 40},\n# #  0.1350395546020487)","04f2c61b":"# RandomForestRegressor with tweaked grid-searched parameters\n\nrf_pipe = make_pipeline(preprocessing_pipe,\n                        RandomForestRegressor(\n                            **{'max_features': 'auto',\n                               'min_samples_leaf': 1,\n                               'min_samples_split': 2,\n                               'n_estimators': 40}\n                        )\n                        )\nrf_rmse_cv = run_rmse_cv(rf_pipe, train, target, kf=kfolds,\n                         record_name='rf', num_results_to_show=2)","208c9112":"# # Choose a relatively high learning rate, \n# # and find the optimal number of estimators for it\n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [86, 88, 90, 92, 94]}\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'xgbregressor__learning_rate': 0.3, 'xgbregressor__n_estimators': 92},\n# #  0.12494039958515651)","2421f535":"# # # Tune max_depth and min_child_weight \n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [92],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [2.8, 3.0, 3.1]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # Results of the last run:\n# # ({'xgbregressor__learning_rate': 0.3,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 92},\n# #  0.12399834713926551)","0e157f4b":"# # Tune gamma\n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [92],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [3.1],\n#               'xgbregressor__gamma': [0.0, 0.1, 0.2, 0.3, 0.4]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # # Results of the last run:\n# # ({'xgbregressor__gamma': 0.0,\n# #   'xgbregressor__learning_rate': 0.3,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 92},\n# #  0.12399834713926551)","75f19a10":"# # Recalibrate n_estimators\n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [88, 90, 92, 94, 96, 100],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [3.1],\n#               'xgbregressor__gamma': [0]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # # Results of the last run:\n# # ({'xgbregressor__gamma': 0,\n# #   'xgbregressor__learning_rate': 0.3,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 96},\n# #  0.12391391755598129)","0359cbb5":"# # Tune subsample and colsample_bytree\n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [96],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [3.1],\n#               'xgbregressor__gamma': [0],\n#               'xgbregressor__subsample': [0.80, 0.85, 0.90, 1],\n#               'xgbregressor__colsample_bytree': [0.80, 0.85, 0.90, 1]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # # Results of the last run:\n# # ({'xgbregressor__colsample_bytree': 1,\n# #   'xgbregressor__gamma': 0,\n# #   'xgbregressor__learning_rate': 0.3,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 96,\n# #   'xgbregressor__subsample': 1},\n# #  0.12391391755598129)","a3259341":"# # Tune regularization parameters\n\n# param_grid = {'xgbregressor__learning_rate': [0.3],\n#               'xgbregressor__n_estimators': [96],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [3.1],\n#               'xgbregressor__gamma': [0],\n#               'xgbregressor__subsample': [1],\n#               'xgbregressor__colsample_bytree': [1],\n#               'xgbregressor__reg_alpha': [0.19, 0.2, 0.21]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # # Results of the last run:\n# # ({'xgbregressor__colsample_bytree': 1,\n# #   'xgbregressor__gamma': 0,\n# #   'xgbregressor__learning_rate': 0.3,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 96,\n# #   'xgbregressor__reg_alpha': 0.2,\n# #   'xgbregressor__subsample': 1},\n# #  0.12220976107917751)","015b848d":"# # Lower the learning rate and add more trees\n# # Recalibrating subsample and colsample_bytree also appear to be necessary.\n\n# param_grid = {'xgbregressor__learning_rate': [0.05],\n#               'xgbregressor__n_estimators': [480],\n#               'xgbregressor__max_depth': [3],\n#               'xgbregressor__min_child_weight': [3.1],\n#               'xgbregressor__gamma': [0],\n#               'xgbregressor__subsample': [0.68, 0.7, 0.72],\n#               'xgbregressor__colsample_bytree': [0.68, 0.7, 0.72],\n#               'xgbregressor__reg_alpha': [0.2]\n#              }\n\n# run_gridsearch([XGBRegressor()], param_grid=param_grid)\n\n# # # Results of the last run:\n# # ({'xgbregressor__colsample_bytree': 0.72,\n# #   'xgbregressor__gamma': 0,\n# #   'xgbregressor__learning_rate': 0.05,\n# #   'xgbregressor__max_depth': 3,\n# #   'xgbregressor__min_child_weight': 3.1,\n# #   'xgbregressor__n_estimators': 480,\n# #   'xgbregressor__reg_alpha': 0.2,\n# #   'xgbregressor__subsample': 0.68},\n# #  0.11328134672815252)","799ef775":"gridsearched_params = {'learning_rate': 0.05,\n                       'n_estimators': 600,\n                       'max_depth': 3,\n                       'min_child_weight': 3.1,\n                       'gamma': 0,\n                       'subsample': 0.68,\n                       'colsample_bytree': 0.72,\n                       'reg_alpha': 0.2}\nxgb_model = XGBRegressor(**gridsearched_params, nthread=1, n_jobs=1)\nxgb_pipe = make_pipeline(preprocessing_pipe, xgb_model)\nxgb_rmse_cv = run_rmse_cv(xgb_pipe, train, target,\n                          kf=kfolds, record_name='xgb', num_results_to_show=4)","cc7c3b9f":"def build_nn_model():\n    model = Sequential()\n    model.add(Dense(256, activation='relu', kernel_regularizer=l1(\n        0.006), bias_regularizer=l1(0.006)))\n    model.add(Dropout(0.2))\n    model.add(Dense(256, activation='relu', kernel_regularizer=l1(\n        0.006), bias_regularizer=l1(0.006)))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l1(\n        0.006), bias_regularizer=l1(0.006)))\n    model.add(Dropout(0.1))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\n\nnn_model = KerasRegressor(build_fn=build_nn_model,\n                          epochs=100, batch_size=200, verbose=0)\nnn_pipe = make_pipeline(preprocessing_pipe, StandardScaler(), nn_model)\nnn_rmse_cv = run_rmse_cv(nn_pipe, train, target.values, kf=kfolds,\n                     record_name='nn', num_results_to_show=4)","2eb875d3":"class BaggingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n\n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions, axis=1)","37c2db6f":"bag_model = BaggingRegressor([ridge_pipe, lasso_pipe, xgb_pipe])\nbag_rmse_cv = run_rmse_cv(bag_model, train, target, kf=kfolds,\n                      record_name='bag', num_results_to_show=4)","5507609d":"class StackingRegressor(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X.iloc[train_index, :], y.iloc[train_index])\n                y_pred = instance.predict(X.iloc[holdout_index, :])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        \n        return self\n\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X)\n                             for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_])\n        return self.meta_model_.predict(meta_features)","dd950b6f":"stack_model = StackingRegressor(base_models=(\n    ridge_pipe, lasso_pipe, xgb_pipe), meta_model=Lasso(alpha=0.0006))\nstack_rmse_cv = run_rmse_cv(stack_model, train, target, kf=kfolds,\n                     record_name='stack', num_results_to_show=4)","5413bc8d":"scores = [ridge_rmse_cv, lasso_rmse_cv,\n          enet_rmse_cv, xgb_rmse_cv, bag_rmse_cv, stack_rmse_cv]\nmodel_names = ['Ridge', 'Lasso', 'ElasticNet', 'XGBoost', 'Bagging', 'Stacking']\nfig = plt.figure(figsize=[15, 5])\nax = fig.add_subplot(111)\nsns.barplot(model_names, scores, orient='v', ax=ax)\nax.set_ylim(bottom=0.104, top=0.117)\nax.set_title('Cross-validated RMSLE of candidate models')\nfor p in ax.patches:\n    ax.annotate(\"%.6f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                ha='center', va='center', fontsize=13, color='k', \n                xytext=(0, 20), textcoords='offset points')\nplt.show()","86e9639e":"final_model = BaggingRegressor([ridge_pipe, lasso_pipe, xgb_pipe])\nfinal_model.fit(train, target)\nfinal_prediction = np.exp(final_model.predict(test))\nsubmission = pd.read_csv(os.path.join(raw_data_path, 'sample_submission.csv'))\nprint('submission.shape', submission.shape)\nprint('final_prediction.shape', final_prediction.shape)\nsubmission['SalePrice'] = final_prediction\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","6632bc0e":"### Nominal features","4c60da6f":"### Ordinal features","6b460488":"### Lasso","f20cef5d":"### LinearRegression","e2851a90":"StackingRegressor().predict uses pretrained models from fit method stored in self.meta_model_ in order to avoid fitting base models to the entire training set. Alternative stacking model that does use this additional fitting step produces marginally better score, but runs approximately 1.5 times slower.","8c768869":"# House Prices: Advanced Regression Techniques  \nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\nIn this notebook I want to accomplish the following:\n* Perform detailed manual feature engineering.\n* Set up a reliable cross-validation framework.\n* Use test set only for calculating submissions. Base all decisions on cross-validation using training set.\n* Push all preprocessing stages that leak data to each cross-validation split.\n* Use sklearn pipelines to encapsulate data leaking preprocessing.\n* Use custom sklearn transformer classes and make them work with pandas DataFrames.\n* Try at least linear models, gradient boosting, and neural nets.\n* Try bagging and stacking.\n\n## Imports","ab68e1c6":"### Basic recoding","4b7c160f":"## Exploratory data analysis\n\n### Target variable","ecc9f39b":"RandomForestRegressor performs consistently worse than linear models. Apparently random forests are not very good at modelling multivariate linear dependences. More details here:  \nhttps:\/\/stats.stackexchange.com\/questions\/174806\/linear-regression-performing-better-than-random-forest-in-caret","72e19da8":"## Modelling\n### Utitily functions","9789459f":"No matter what I do, I cannot make xgboost produce reproducible cross-validation results in Jupyter. RMSLE does stay the same when I rerun the notebook, but changes as soon as I restart the kernel. nthread and n_jobs are both set to 1, so it is not a multithreading problem. Default random_state is 0, thus it can't be because of non-fixed random seed. Tests show that subsample and colsample_bytree are a major contributing factor to non-reproducibility here.","6eac066a":"### Power transformations","32c72a4d":"### Neural network","a96fd047":"### Object to categorical","e9f80d13":"### Nominal features","6bbb0170":"### Condition features","15f07d91":"### RandomForestRegressor","e6c91ae3":"## Feature engineering\nThe idea is to do manual feature engineering this way:  \n* Inspect features grouped by topic (area, basement, quality, condition etc.) and perform transformations that spring to mind based on the inspection.\n* Further inspect features grouped by type (numeric, ordinal, nominal) and possibly perform additional transformations.\n* Try to normalize highly skewed features using log, simple power or Box-Cox transformations.\n* Possibly add interactions between features most correlated with SalePrice.\n* Do not drop any existing features unless they have very low variance, or it is blatantly obvious that they useless. Multicollinearity be damned.  \n\n### Utility functions","c25a4c64":"#### Model","45d9f8e1":"### ElasticNet","ae5ef8e6":"Neural net performs even worse than RandomForestRegressor. Dropout and regularization noticeably improve the score, but it is still too high compared to other models.","f9ee1856":"### Numeric features","818ae3da":"### Stacking","5261d872":"### Area features","86807671":"### Bagging","12f881a9":"### Quality features","200b6661":"### Ordinal features","b343131f":"### Correlation","3453cc55":"## Submission","525ae3f5":"### Low variance features","dbfb4a83":"### Basement features","3e66f9e8":"### XGBoost\n#### Grid searches","3ce6496a":"### Custom transformers","1d4a8fcb":"### Ridge","74b7b179":"## Load data","3ad0731a":"### Numeric features"}}