{"cell_type":{"f7b88440":"code","6633a4a8":"code","77145447":"code","b408630a":"code","ae1e0378":"code","07aef5b9":"code","3eceb537":"code","d2ed87a0":"code","ff2eba31":"code","78dc62b5":"code","2582b009":"code","a93bde63":"code","842f5017":"code","da07db67":"code","9a10dc2c":"code","e6e31a0d":"code","1a4f2c38":"markdown","031485e9":"markdown","993eb748":"markdown","0d75f662":"markdown","c36aefe7":"markdown","be954444":"markdown","be071347":"markdown"},"source":{"f7b88440":"import re\nimport string\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport transformers\nfrom transformers import BertTokenizer\nfrom transformers import TFAutoModel","6633a4a8":"print(tf.__version__)\nprint(transformers.__version__)","77145447":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","b408630a":"train.head()","ae1e0378":"test.head()","07aef5b9":"#Use regex to clean the data\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef decontraction(text):\n    text = re.sub(r\"won\\'t\", \" will not\", text)\n    text = re.sub(r\"won\\'t've\", \" will not have\", text)\n    text = re.sub(r\"can\\'t\", \" can not\", text)\n    text = re.sub(r\"don\\'t\", \" do not\", text)\n    \n    text = re.sub(r\"can\\'t've\", \" can not have\", text)\n    text = re.sub(r\"ma\\'am\", \" madam\", text)\n    text = re.sub(r\"let\\'s\", \" let us\", text)\n    text = re.sub(r\"ain\\'t\", \" am not\", text)\n    text = re.sub(r\"shan\\'t\", \" shall not\", text)\n    text = re.sub(r\"sha\\n't\", \" shall not\", text)\n    text = re.sub(r\"o\\'clock\", \" of the clock\", text)\n    text = re.sub(r\"y\\'all\", \" you all\", text)\n\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"n\\'t've\", \" not have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'d've\", \" would have\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ll've\", \" will have\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    return text \n\ndef seperate_alphanumeric(text):\n    words = text\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(text):\n    tchr = text.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, text):\n    substitute = re.sub(r'(\\w)\\1+', rep, text)\n    return substitute\n\ntrain['text'] = train['text'].apply(lambda x : remove_url(x))\ntrain['text'] = train['text'].apply(lambda x : remove_punct(x))\ntrain['text'] = train['text'].apply(lambda x : remove_emoji(x))\ntrain['text'] = train['text'].apply(lambda x : decontraction(x))\ntrain['text'] = train['text'].apply(lambda x : seperate_alphanumeric(x))\ntrain['text'] = train['text'].apply(lambda x : unique_char(cont_rep_char,x))\n\ntest['text'] = test['text'].apply(lambda x : remove_url(x))\ntest['text'] = test['text'].apply(lambda x : remove_punct(x))\ntest['text'] = test['text'].apply(lambda x : remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x : decontraction(x))\ntest['text'] = test['text'].apply(lambda x : seperate_alphanumeric(x))\ntest['text'] = test['text'].apply(lambda x : unique_char(cont_rep_char,x))","3eceb537":"seq_len = 256\nbatch_size = 16\nnum_samples = len(train)\nmodel_name = 'cardiffnlp\/twitter-roberta-base-sentiment'","d2ed87a0":"tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n\ntrain_tokens = tokenizer(train['text'].tolist(), max_length=seq_len, \n                         truncation=True, padding='max_length', \n                         add_special_tokens=True, return_tensors='np')\n\ny_train = train['target'].values\nlabels = np.zeros((num_samples, y_train.max() + 1))\nlabels[np.arange(num_samples), y_train] = 1\n\ndataset = tf.data.Dataset.from_tensor_slices((train_tokens['input_ids'], train_tokens['attention_mask'], labels))\n\ndef map_func(input_ids, masks, labels):\n    return {\n        'input_ids': input_ids,\n        'attention_mask': masks\n    }, labels\n\ndataset = dataset.map(map_func)\ndataset = dataset.shuffle(10000).batch(batch_size=batch_size, drop_remainder=True)\n\nsplit = 0.7\nsize = int((train_tokens['input_ids'].shape[0] \/\/ batch_size) * split)\n\ntrain_ds = dataset.take(size)\nval_ds = dataset.skip(size)","ff2eba31":"model = TFAutoModel.from_pretrained(model_name)\n\n# Two inputs\ninput_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(seq_len,), name='attention_mask', dtype='int32')\n\n# Transformer\n# embeddings = model.bert(input_ids, attention_mask=mask)[1]\nembeddings = model(input_ids, attention_mask=mask)[0]\nembeddings = embeddings[:, 0, :]\n# Classifier head\nx = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n# x = tf.keras.layers.Dropout(0.1)(x)\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n\nbert_model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n\n# freeze bert layers\n# bert_model.layers[2].trainable = False\n\noptimizer = tf.keras.optimizers.Adam(lr=1e-5)\nloss = tf.keras.losses.CategoricalCrossentropy()\nacc = tf.keras.metrics.BinaryAccuracy()\n\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n\nhistory = bert_model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=2,\n    batch_size=batch_size\n)","78dc62b5":"def plot_learning_evolution(r):\n    plt.figure(figsize=(12, 8))\n    \n    plt.subplot(2, 2, 1)\n    plt.plot(r.history['loss'], label='Loss')\n    plt.plot(r.history['val_loss'], label='val_Loss')\n    plt.title('Loss evolution during trainig')\n    plt.legend()\n\n    plt.subplot(2, 2, 2)\n    plt.plot(r.history['binary_accuracy'], label='binary_accuracy')\n    plt.plot(r.history['val_binary_accuracy'], label='val_binary_accuracy')\n    plt.title('Accuracy score evolution during trainig')\n    plt.legend();","2582b009":"plot_learning_evolution(history)","a93bde63":"bert_model.evaluate(val_ds)","842f5017":"def prep_data(text):\n    tokens = tokenizer(text, max_length=512, truncation=True, \n                       padding='max_length', \n                       add_special_tokens=True, \n                       return_tensors='tf')\n    return {'input_ids': tokens['input_ids'], \n            'attention_mask': tokens['attention_mask']}\n\ntest['target'] = None\n\nfor i, row in test.iterrows():\n    tokens = prep_data(row['text'])\n    probs = bert_model.predict(tokens)\n    pred = np.argmax(probs)\n    test.at[i, 'target'] = pred\n    \ntest['target'] = test['target'].astype(int)","da07db67":"test.head()","9a10dc2c":"sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(), 'target':test['target']})\nsub.to_csv('submission.csv', index=False)","e6e31a0d":"sub.head()","1a4f2c38":"# \ud83d\udcca Model Evaluation","031485e9":"# \ud83d\udcda Natural Language Processing with Disaster Tweets \ud83d\udcac\n \n>- \ud83c\udfaf Goal: Predict which Tweets are about real disasters and which ones are not","993eb748":"# \ud83d\udd28 Preprocessing","0d75f662":"# \u2702\ufe0f Tokenization","c36aefe7":"# \ud83d\udccb Loading the Data","be954444":"# \ud83e\udd16 Model Building","be071347":"# 5. Making submission"}}