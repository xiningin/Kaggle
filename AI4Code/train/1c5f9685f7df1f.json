{"cell_type":{"93fa19fc":"code","eea1f22c":"code","e3a3a6f3":"code","1129cbb0":"code","001920d0":"code","6e9d5a49":"code","049fa0f0":"code","3b26e021":"code","e52bf3b9":"code","8493b3db":"code","f8b78869":"code","ba544cb2":"code","22b17252":"code","d392b0a7":"code","e4c850e4":"code","8b257d8b":"code","276809b8":"code","85bd50b2":"code","6cc807f2":"code","dcdd7012":"code","59db22af":"code","7c43bb2e":"code","da905656":"code","ee598945":"code","c6cae414":"code","10a53f9e":"code","cbae9b67":"code","e5cb185f":"code","d5e38615":"code","3dd9e1be":"code","1661000b":"code","5abc7315":"code","d77e6e8d":"code","c089fecc":"code","df7bee28":"code","e74613b0":"code","0d3e2efc":"code","6d53421f":"code","de4ad908":"code","7d12ea0d":"code","c4d78cec":"markdown","9270e223":"markdown","db081b55":"markdown"},"source":{"93fa19fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import rcParams\nfrom matplotlib.pyplot import *\n\n\n\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","eea1f22c":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.head()","e3a3a6f3":"train.describe(include='all')","1129cbb0":"'''Pearson Correlation Matrix\n'''\n\n# colormap = plt.cm.RdBu\n# plt.figure(figsize=(14,12))\n# plt.title('Pearson Correlation of Features', y=1.05, size=15)\n# sns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n#             square=True, cmap=colormap, linecolor='white', annot=True)","001920d0":"print(\n    'Percentage of missing values:\\n\\n{}'.format(pd.isnull(train).sum() * 100 \/ len(train)))","6e9d5a49":"# fig, axs = plt.subplots(ncols=3)\n\n# sns.barplot(x='Sex', y='Survived', data=train, ax=axs[0]);\n# sns.barplot(x='Pclass', y='Survived', data=train, ax=axs[1])\n# sns.boxplot(x='SibSp',y='Survived', data=train, ax=axs[2])\n\n# subplots_adjust(left=0.125, bottom=0.9, right=0.15, top=0.9, wspace=0.2, hspace=0.2)","049fa0f0":"sns.barplot(x='Sex', y='Survived', data=train);\n\nprint(\"Percentage of females who survived: \",\n      train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived: \",\n      train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)\n","3b26e021":"sns.barplot(x='Pclass', y='Survived', data=train);\n\nprint(\"Percentage of females who survived: \",\n      train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived: \",\n      train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived: \",\n      train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)\n","e52bf3b9":"#draw a bar plot for SibSp vs. survival\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n#I won't be printing individual percent values for all of these.\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 3 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 4 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)","8493b3db":"sns.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","f8b78869":"#sort the ages into logical categories\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\n\nbins = [-1, 0, 5, 12, 18,\n        24, 35, 60, np.inf]\n\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager',\n          'Student', 'Young Adult', 'Adult', 'Senior']\n\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\n\nsns.pointplot(x=\"AgeGroup\", y=\"Survived\", data=train,\n             fig_size = (7,5))\n\nplt.show()","ba544cb2":"test.describe(include=\"all\")","22b17252":"#dropping uneecessary cols\ntest = test.drop([\"Cabin\", \"Ticket\"], axis=1)\ntrain = train.drop([\"Cabin\", \"Ticket\"], axis=1)\n\ntest.head()","d392b0a7":"print(\"Number of people embarked from Southampton:\" )\nS = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(S)\nprint(\"Number of people embarked from Cherbourg:\" )\nC = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(C)\nprint(\"Number of people embarked from Queenstown:\" )\nQ = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(Q)\n     ","e4c850e4":"#Replacing all missing Embarked values with the most frequent occured port\n\ntrain = train.fillna({\"Embarked\": \"S\"})","8b257d8b":"train[\"Embarked\"].unique()","276809b8":"combined = [train, test]\n\n# Feature engineering\n\n#Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in combined:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in combined:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# train.head()","85bd50b2":"for dataset in combined:\n    dataset['Title'] = dataset[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \npd.crosstab(train.Title, train.Sex)","6cc807f2":"for dataset in combined:\n    dataset[\"Title\"] = (dataset[\"Title\"]\n                                    .replace(['Lady', 'Capt', 'Col', 'Don', 'Dr',\n                                             'Major', 'Rev', 'Jonkheer', 'Dona'],\n                                           \"Rare\")\n                                   .replace(\"Mlle\", \"Miss\")\n                                   .replace(\"Ms\", \"Miss\")\n                                   .replace(\"Mme\", \"Mrs\")\n                                   .replace([\"Countess\", \"Lady\",\"Sir\"], \"Royal\")\n                       )\n    \ntrain[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=True).mean()","dcdd7012":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3,\n                 \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\n\nfor dataset in combined: \n    dataset[\"Title\"] = (dataset[\"Title\"]\n                            .map(title_mapping)\n                            .fillna(0))\n     \ntrain.head()","59db22af":"#age feature\n\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\n\n\nfor x in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][x] == \"Unknown\":\n        train[\"AgeGroup\"][x] = age_title_mapping[train[\"Title\"][x]]\n        \nfor x in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][x] == \"Unknown\":\n        test[\"AgeGroup\"][x] = age_title_mapping[test[\"Title\"][x]]\n        \n        \n\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\n\n\n\n#train = train.drop(['Age'], axis = 1)\n#test = test.drop(['Age'], axis = 1)","7c43bb2e":"#drop the name feature since it contains no more useful information.\ntrain = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)\n","da905656":"sex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","ee598945":"#map each Embarked value to a numerical value\n\nembarked_mapping = {\"S\": 1,\n                    \"C\": 2,\n                    \"Q\": 3 }\n\n\ntrain[\"Embarked\"] = train[\"Embarked\"].map(embarked_mapping)\ntest[\"Embarked\"] = test[\"Embarked\"].map(embarked_mapping)\n\nprint(\"Done\")","c6cae414":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\n#train = train.drop(['Fare'], axis = 1)\n#test = test.drop(['Fare'], axis = 1)","10a53f9e":"train.head()","cbae9b67":"test.head()","e5cb185f":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\n\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","d5e38615":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","3dd9e1be":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 3)\nprint(acc_logreg)","1661000b":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","5abc7315":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","d77e6e8d":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","c089fecc":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","df7bee28":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","e74613b0":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","0d3e2efc":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","6d53421f":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","de4ad908":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\n\nmodels.sort_values(by='Score', ascending=False)","7d12ea0d":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = randomforest.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","c4d78cec":"**Feature engineering**","9270e223":"Final Submission","db081b55":"Model testing"}}