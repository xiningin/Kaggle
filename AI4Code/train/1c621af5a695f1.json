{"cell_type":{"a2f9fa10":"code","b5a12e68":"code","8c8dd019":"code","ba91487a":"code","de71d305":"code","00a274c0":"code","85a7847e":"code","8dc1d455":"code","4c47fa8c":"code","82a0a541":"code","fff2514c":"code","26b0910d":"code","afddf6f2":"code","95fbb080":"code","7a67cac0":"code","4c629f59":"markdown","cafba852":"markdown","bb5d59ac":"markdown","40e41057":"markdown","7739a33c":"markdown","7eb73275":"markdown","11a5caff":"markdown","2f29c5fe":"markdown","075d8340":"markdown","197dd378":"markdown","d406cb28":"markdown","132ff178":"markdown","6b08b116":"markdown","47fc4747":"markdown","58cee5eb":"markdown","13479a38":"markdown","ff05b34b":"markdown","0fe0c667":"markdown","b15ee6e2":"markdown","17c962e8":"markdown","9556e7ec":"markdown","2d2f2377":"markdown","1f7e7570":"markdown","8e8904bd":"markdown","7e98b434":"markdown","63030a44":"markdown","9c6faa58":"markdown","370a2840":"markdown","d611fa9c":"markdown","1507685d":"markdown","572e7d44":"markdown","43920f15":"markdown","9720d653":"markdown","67456770":"markdown","f4059e34":"markdown","de3e7ca4":"markdown","64cc5538":"markdown","c54c23e4":"markdown"},"source":{"a2f9fa10":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","b5a12e68":"#Create independent and Dependent Features\ncolumns = df.columns.tolist()\n\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n\n# Store the variable we are predicting \ntarget = \"Class\"\n\n# Define a random state \nstate = np.random.RandomState(42)\n\nX = df[columns]\nY = df[target]\n\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","8c8dd019":"#Check for missing values:\ndf.isnull().values.any()","ba91487a":"LABELS = [\"Normal\", \"Fraud\"]\n\ncount_classes = pd.value_counts(df['Class'], sort = True)\n\ncount_classes.plot(kind = 'bar', rot=0)\n\nplt.title(\"Visualization of Class Variable\")\n\nplt.xticks(range(2), LABELS)\n\nplt.ylabel(\"Frequency\")","de71d305":"## Get the Fraud and the normal dataset \n\nfraud = df[df['Class']==1]\n\nnormal = df[df['Class']==0]","00a274c0":"print(\"BEFORE UNDER SAMPLING\")\nprint(\"---------------------\")\nprint(fraud.shape,normal.shape)","85a7847e":"from imblearn.under_sampling import RandomUnderSampler","8dc1d455":"rus = RandomUnderSampler(random_state=42, replacement=True)\nX_res, y_res = rus.fit_resample(X, Y)","4c47fa8c":"from collections import Counter\nprint('Original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(y_res)))","82a0a541":"from imblearn.over_sampling import RandomOverSampler\n\nros =  RandomOverSampler(random_state=42)\nX_ros, y_ros = ros.fit_resample(X, Y)\n","fff2514c":"print('Original dataset shape {}'.format(Counter(Y)))\nprint('Resampled dataset shape {}'.format(Counter(y_ros)))","26b0910d":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 33)","afddf6f2":"X_train_new, y_train_new = sm.fit_resample(X, Y)","95fbb080":"# observe that data has been balanced\npd.Series(y_train_new).value_counts().plot.bar()","7a67cac0":"from imblearn.under_sampling import ClusterCentroids\nsampler = ClusterCentroids()\nX_rs, y_rs = sampler.fit_resample(X, Y)\nprint('Cluster centriods undersampling {}'.format(Counter(y_rs)))","4c629f59":"<center><h1 style=\"font-size:200%; font-family:cursive; color:navy;\"><b>1. Use Right Evaluation Metrics <\/b><\/h1><\/center>","cafba852":"<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>4. Tomek Links \/ SMOTE-Tomek Links<\/b><\/h1><\/center>","bb5d59ac":"<p style=\"font-size:200%; font-family:cursive; background:skyblue; padding:25px; color:navy;\"><b>Problem with Imbalance Data ?<\/b><\/p>","40e41057":"<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>3. Synthetic Minority Over-Sampling Technique (SMOTE)<\/b><\/h1><\/center>","7739a33c":"<center><h2 style=\"font-size:200%; font-family:cursive; color:red;\">\u201cLet's Start the Implement of SMOTE using Python\u201d<\/h2><\/center>","7eb73275":"<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>7. Use of Ensemble Methods <\/b><\/h1><\/center>","11a5caff":"<center><h2 style=\"font-size:200%; font-family:cursive; color:red;\">\u201cLet's Start the Implement of Cluster Centroids using Python\u201d<\/h2><\/center>","2f29c5fe":"<p style=\"font-size:200%; font-family:cursive; background:skyblue; padding:25px; color:navy;\"><b>What is Imbalanced (\ud83d\udeab \u2696) & Balanced Data(\u2696) ?<\/b><\/p>\n\n<ul>\n    <li style=\"font-size:150%; font-family:verdana;\"><b>Balanced Data - <\/b>Let\u2019s Consider an example: if in our data set we have positive values which are approximately same as negative values. Then we can say our dataset in balanced.<\/li>\n    <center><img src=\"https:\/\/proavschool.com\/wp-content\/uploads\/2011\/11\/iStock_000016468646XSmall1.jpg\"><\/center>\n    <li style=\"font-size:150%; font-family:verdana;\"><b>Imbalanced Data - <\/b>If there's huge difference between the positive values and negative values. Then we can say our dataset in Imbalance Dataset.<\/li>\n    <center><img src=\"data:image\/jpeg;base64,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\"><\/center>\n    <br>\n<\/ul>","075d8340":"<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>9. Use of Multiple Classifiers<\/b><\/h1><\/center>","197dd378":"<center><h2 style=\"font-size:250%; font-family:cursive; background:skyblue; padding:30px;\">\u201cEveryone wants to be perfect. So why our dataset should not be perfect? Let\u2019s make it perfect\u201d<\/h2><\/center>","d406cb28":"<br>","132ff178":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Multiple Classification system is an approach where a classification system is built for imbalanced data based on the combination of several classifiers. It is a method for building multiple classifier systems in which each constituting classifier is trained on a subset of the majority class and on the whole minority class. The basis behind this method is the partition of the set of samples of the majority class in several subsets, each consisting of as many samples as the minority class.<\/li>\n<\/ul>","6b08b116":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Machine learning models are data-hungry. In most cases, researchers spend most of their time in tasks like data cleaning, analysing, visualising, among others during an end-to-end machine learning process and contribute less time in data collection. While all of these steps are important, often the collection of data gets limited to certain numbers. To avoid such circumstances, one must add more data into the dataset. Collecting more data with relevant examples of the undersampled class of the dataset will help to overcome the issue.<\/li>\n<\/ul>\n<center><img src=\"https:\/\/socialgamesnc.files.wordpress.com\/2013\/06\/creating-the-most-accurate-projections.jpg\" width=400px height=350px><\/center>","47fc4747":"<p style=\"font-size:170%; font-family:cursive;\">Just like the name suggests, the technique generates synthetic data for the minority class.<\/p>\n\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">SMOTE proceeds by joining the points of the minority class with line segments and then places artificial points on these lines.<\/li>\n    <center><img src=\"https:\/\/miro.medium.com\/max\/875\/1*nHwVl6AgcP_ym8RbXuKvSg.png\"><\/center>\n<\/ul>\n<br>\n<br>\n\n<h2 style=\"font-size:170%; font-family:cursive;\">The SMOTE algorithm works in 4 simple steps:<\/h2>\n<br>\n<ol>\n    <li style=\"font-size:150%; font-family:cursive;\">Choose a minority class input vector<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Repeat the steps until data is balanced<\/li>\n<\/ol>","58cee5eb":"<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>5. Under-sampling: Cluster Centroids<\/b><\/h1><\/center>","13479a38":"<p style=\"font-size:200%; font-family:cursive; color:navy;\"><b>Techniques to Handle Imbalance Data: <\/b><\/p>\n<ol>\n    <li style=\"font-size:170%; color:black;font-family:cursive;\">Use the right evaluation metrics<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Re-Sampling - (Undersampling & Oversampling)<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Synthetic Minority Over-Sampling Technique (SMOTE)<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Tomel \/ SMOTE + Tomek<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Under-sampling: Cluster Centroids<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Use K-fold Cross-Validation in the right way<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Use of Ensemble Methods<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Use of Cost-Sensitive Algorithms<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">Use of Multiple Classifiers<\/li>\n    <li style=\"font-size:170%;color:black;font-family:cursive;\">The More Data, The Better<\/li>\n<\/ol>\n    ","ff05b34b":"<p style=\"font-size:150%; font-family:cursive;\">A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and\/or adding more examples to the minority class (over-sampling).<\/p>\n<br>\n<center><img src=\"https:\/\/miro.medium.com\/max\/725\/1*H6XodlitlGDl9YdbwaZLMw.png\"><\/center>\n<br>\n<br>","0fe0c667":"<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>10. The More Data, The Better<\/b><\/h1><\/center>","b15ee6e2":"<h2 style=\"font-size:150%; font-family:cursive;\"><b>UNDER SAMPLING<\/b><\/h2>","17c962e8":"<center><h2 style=\"font-size:250%; font-family:cursive; background:skyblue; padding:30px;\">CONCLUSION<\/h2><\/center>","9556e7ec":"<h2 style=\"font-size:150%; font-family:cursive;\"><b>OVER SAMPLING<\/b><\/h2>","2d2f2377":"<p style=\"font-size:150%; font-family:cursive;\">This method undersamples the majority class by replacing a cluster of majority samples This method finds the clusters of majority class with K-mean algorithms. Then it keeps the cluster centroids of the N clusters as the new majority samples.<\/p>","1f7e7570":"<center><h1 style=\"color:red;\">Please give an upvote, if you like & find this kernel helpful. Thank You!!!!!<\/h1><\/center>","8e8904bd":"<center><h2 style=\"font-size:250%; font-family:cursive; background:skyblue; padding:30px;\">\u201cLet's Understand these Techniques using Credit Card Fraud Data\u201d<\/h2><\/center>","7e98b434":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Cost-Sensitive Learning is a type of learning that takes the misclassification or other types of costs into consideration. Cost-sensitive learning is a popular and common approach to solve the class imbalanced datasets. Popular machine learning libraries such as support vector machines (SVM), random forest, decision trees, logistic regression, among others, can be configured using the cost-sensitive training.<\/li>\n<\/ul>","63030a44":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">As with most things in data science and machine learning algorithms, there is no definitive right approach that works every time. Depending on the nature of your dataset, distribution of classes, predictors and model, some of the above-mentioned methods may work better. It is up to you to figure out the best combination.<\/li>\n<\/ul>\n<br>\n<p style=\"font-size:170%; font-family:cursive;\">A few pointers to keep in mind are:<\/p>\n\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Always do the train\/test split before creating synthetic\/augmented samples. You want to validate and test your model on original data observations.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Use the same metrics for comparison \u2014 every time you try something new, remember to compare them right. Don\u2019t look at only accuracy for one model and sensitivity for another.<\/li>\n<\/ul>\n\n<p style=\"font-size:170%; font-family:cursive;\">In addition to these steps, don\u2019t forget that you still have to do things such as data cleaning, feature selection and hyper-parameter tuning.<\/p>","9c6faa58":"<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>2. RE-SAMPLING TECHNIQUES <\/b><\/h1><\/center>","370a2840":"<h2 style=\"font-size:150%; font-family:cursive;\"><b>BASIC DATA EXPLORATION<\/b><\/h2>","d611fa9c":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Tomek Links is one of a modification from Condensed Nearest Neighbors (CNN, not to be confused with Convolutional Neural Network) undersampling technique that is developed by Tomek (1976).<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">This method combines the SMOTE ability to generate synthetic data for minority class and Tomek Links ability to remove the data that are identified as Tomek links from the majority class. <\/li>\n<\/ul>\n\n<br>\n<p style=\"font-size:170%; font-family:cursive;\">The process of SMOTE-Tomek Links is as follows.<\/p>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">(Start of SMOTE) Choose random data from the minority class.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Calculate the distance between the random data and its k nearest neighbors.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Multiply the difference with a random number between 0 and 1, then add the result to the minority class as a synthetic sample.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Repeat step number 2\u20133 until the desired proportion of minority class is met. (End of SMOTE)<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">(Start of Tomek Links) Choose random data from the majority class.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">If the random data\u2019s nearest neighbor is the data from the minority class (i.e. create the Tomek Link), then remove the Tomek Link.<\/li>\n    \n<center><img src=\"http:\/\/glemaitre.github.io\/imbalanced-learn\/_images\/sphx_glr_plot_smote_tomek_001.png\"><\/center>\n\n    ","1507685d":"<center><img src=\"https:\/\/miro.medium.com\/max\/12000\/0*IulFfGY3Qq_D1LsR\" height=200px width=1000px><\/center>","572e7d44":"<center><img src=\"https:\/\/miro.medium.com\/max\/875\/0*_mO22Txi4K3KvTTg.png\" height=300px width=700px><\/center>\n<p style=\"font-size:150%; font-family:cursive;\">One of the major issues that new developer users fall into when dealing with unbalanced datasets relates to the metrics used to evaluate their model. Using simpler metrics like <b>accuracy score<\/b> can be misleading. In a dataset with highly unbalanced classes, the classifier will always \u201cpredicts\u201d the most common class without performing any analysis of the features and it will have a high accuracy rate, obviously not the correct one.<\/p>\n\n<br>\n<h1 style=\"font-size:170%; font-family:cursive;\"><b>Metrics that can provide better insight include:<\/b><\/h1>\n\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">The <b>precision<\/b> of a class define how trustable is the result when the model answer that a point belongs to that class.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The <b>recall<\/b> of a class expresses how well the model is able to detect that class.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">The <b>F1 score<\/b> of a class is given by the harmonic mean of precision and recall<br>(2\u00d7precision\u00d7recall \/ (precision + recall)), it combines precision and recall of a class in one metric.<\/li>\n<\/ul>\n\n<br>\n<p style=\"font-size:170%; font-family:cursive;\">For a given class, the different combinations of recall and precision have the following meanings:<\/p>\n<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">high recall + high precision : the class is perfectly handled by the model.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">low recall + high precision : the model can\u2019t detect the class well but is highly trustable when it does.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">high recall + low precision : the class is well detected but the model also include points of other classes in it.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">low recall + low precision : the class is poorly handled by the model.<\/li>\n<\/ul>","43920f15":"<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>6. Use K-fold Cross-Validation in the right way<\/b><\/h1><\/center>","9720d653":"<br>","67456770":"<br>\n<center><h1 style=\"font-size:200%; font-family:cursive;color:navy;\"><b>8. Use of Cost-Sensitive Algorithms<\/b><\/h1><\/center>","f4059e34":"<center><h2 style=\"font-size:200%; font-family:cursive; color:red;\">\u201cLet's Start the Implement of UNDER SAMPLING & OVER SAMPLING\u201d<\/h2><\/center>","de3e7ca4":"<center><h2 style=\"font-size:250%; font-family:cursive;\">\u201cFeeding imbalanced data to your classifier can make it biased in favor of the majority class, simply because it did not have enough data to learn about the minority.\u201d<\/h2><\/center>","64cc5538":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">Use of ensemble methods is one of the ways to handle the class imbalance problems of the dataset. The learning algorithms construct a set of classifiers and then classify new data points by making a choice of their predictions known as Ensemble methods.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">It has been discovered that ensembles are often much more accurate than the individual classifiers which make them up. Some of the commonly used Ensemble techniques are Bagging or Bootstrap Aggregation, Boosting and Stacking.<\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2017\/03\/16142904\/ICP4.png\"><\/center>","c54c23e4":"<ul>\n    <li style=\"font-size:150%; font-family:cursive;\">It is noteworthy that cross-validation should be applied properly while using over-sampling method to address imbalance problems.<\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. <\/li>\n    <li style=\"font-size:150%; font-family:cursive;\">That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won\u2019t be an overfitting problem.<\/li>\n<\/ul>\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/825\/1*fb-rdCbeErLMzDXBa4IGDw.png\"><\/center>"}}