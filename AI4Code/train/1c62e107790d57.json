{"cell_type":{"a5c15ec9":"code","162ebab6":"code","3a25a7e6":"code","6a7c87bd":"code","f8dd5122":"code","ee359781":"code","956c86a1":"code","c207b113":"code","d1b4805c":"code","c7965a6f":"code","f0c894b4":"code","3c5fe25b":"code","04d96ffb":"code","64ab2195":"code","60ed766d":"code","d658e7b1":"code","1cf60712":"code","788463f6":"code","9b7ff965":"code","984bb7d5":"code","fb22d43f":"code","efa98f33":"code","4091a241":"code","c2c18959":"code","ea9ee124":"code","b0f66443":"code","f0f201c8":"code","e075e633":"code","9d36ac96":"markdown","96caecb4":"markdown","25717dd8":"markdown","02637ed4":"markdown","3e212466":"markdown","8d823478":"markdown","191e6336":"markdown","0a6aba3e":"markdown","21cec708":"markdown","e496960e":"markdown","9658caf3":"markdown","ae75446f":"markdown","3f02a3c5":"markdown","cb5b8628":"markdown","4b7903f8":"markdown","a06b5ade":"markdown","78b42406":"markdown","a37c1492":"markdown","4c9a1ac6":"markdown","388c5014":"markdown","fd52f3e2":"markdown"},"source":{"a5c15ec9":"# Base\nimport numpy as np\nimport pandas as pd\nimport missingno as msno\n\n# Plots\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n# Warnings \nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, validation_curve, train_test_split\n\n# Modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Utilities\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 100)\npd.set_option('display.max_rows', 20)\npd.set_option('display.float_format', lambda x: '%.3f' % x)","162ebab6":"data = pd.read_csv(\"\/kaggle\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = data.copy()","3a25a7e6":"def check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Types #####################\")\n    print(dataframe.info())\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #######################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.25, 0.50, 0.75, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","6a7c87bd":"df.groupby([\"League\", \"Division\"]).agg({\"Salary\":[\"mean\", \"median\"],\n                            \"Hits\":[\"mean\",\"median\"],\n                            \"Years\":[\"mean\", \"median\"],\n                            \"CHits\":[\"mean\", \"median\"],\n                            \"Assists\":[\"mean\", \"median\"],\n                            \"Errors\":[\"mean\", \"median\"]})","f8dd5122":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","ee359781":"def cat_summary(dataframe, col_name, plot=False):\n    plt.style.use('seaborn-darkgrid')\n    fig, ax = plt.subplots(1, 2)\n    ax = np.reshape(ax, (1, 2))\n    ax[0, 0] = sns.histplot(x=dataframe[col_name], color=\"green\", bins=10, ax=ax[0, 0])\n    ax[0, 0].set_ylabel('Frequency')\n        # ax[0, 0].set_title('Distribution')\n    ax[0, 1] = plt.pie(dataframe[col_name].value_counts().values, labels=dataframe[col_name].value_counts().keys(),\n                           colors=sns.color_palette('bright'), shadow=True, autopct='%.0f%%')\n    plt.title(\"Percent\")\n\n    fig.set_size_inches(10, 6)\n    fig.suptitle('Analysis of Categorical Variables', fontsize=13)\n    plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)","956c86a1":"def num_summary(dataframe, numerical_col):\n    # setup the plot grid\n    plt.style.use('seaborn-darkgrid')\n    fig, ax = plt.subplots(1, 2)\n    ax = np.reshape(ax, (1, 2))\n    ax[0, 0] = sns.histplot(x=dataframe[numerical_col], color=\"green\", bins=20, ax=ax[0, 0])\n    ax[0, 0].set_ylabel('Frequency')\n    ax[0, 0].set_title('Distribution')\n    ax[0, 1] = sns.boxplot(y=dataframe[numerical_col], color=\"purple\", ax=ax[0, 1])\n    ax[0, 1].set_title('Quantiles')\n\n    fig.set_size_inches(10, 6)\n    fig.suptitle('Analysis of Numerical Variables', fontsize=13)\n    plt.show()\n\nfor col in num_cols[0:10]:\n    num_summary(df, col)","c207b113":"def correlation_matrix(dataframe, cols):\n    fig = plt.gcf()\n    fig.set_size_inches(10, 8)\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    plt.title('Correlation Map', size=20)\n    fig = sns.heatmap(dataframe[cols].corr(), annot=True, linewidths=0.5, annot_kws={'size': 12}, linecolor='w', cmap='RdBu')\n    plt.show(block=True)\n\ncorrelation_matrix(df, num_cols)","d1b4805c":"def target_correlation_matrix(dataframe, corr_th=0.5, target=\"Salary\"):\n    corr = dataframe.corr()\n    corr_th = corr_th\n    try:\n        filter = np.abs(corr[target]) > corr_th\n        corr_features = corr.columns[filter].tolist()\n        sns.clustermap(dataframe[corr_features].corr(), annot=True, fmt=\".2f\")\n        plt.show()\n        return corr_features\n    except:\n        print(\"Y\u00fcksek threshold de\u011feri, corr_th de\u011ferinizi d\u00fc\u015f\u00fcr\u00fcn!\")\n        \ntarget_correlation_matrix(df, corr_th=0.5, target=\"Salary\")","c7965a6f":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n    \nmissing_values_table(df, na_name=True)","f0c894b4":"msno.bar(df)\nplt.show()","3c5fe25b":"msno.matrix(df)\nplt.show()","04d96ffb":"# Drop missing values\ndf.dropna(inplace=True)","64ab2195":"def outlier_thresholds(dataframe, col_name, q1=0.1, q3=0.9):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\nfor col in num_cols:\n    print(col, outlier_thresholds(df, col))","60ed766d":"def replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    replace_with_thresholds(df,col)","d658e7b1":"df.describe().T","1cf60712":"# New Features\ndf[\"Hits_Success\"] = (df[\"AtBat\"] - df[\"Hits\"])\ndf[\"NEW_ATBAT_CATBAT_RATE\"] = df[\"AtBat\"] \/ df[\"CAtBat\"]\ndf[\"NEW_HMRUN_RATE\"] = df[\"HmRun\"] \/ (df[\"CHmRun\"]+ 0.00001)\ndf[\"NEW_RUN_RATE\"] = df[\"Runs\"] \/ df[\"CRuns\"]\ndf[\"NEW_RBI_RATE\"] = df[\"RBI\"] \/ (df[\"CRBI\"] + 0.00001)\ndf[\"NEW_HITS_RATE\"] = df[\"Hits\"] \/ df[\"CHits\"]\ndf[\"NEW_WALKS_RATE\"] = df[\"Walks\"] \/ df[\"CWalks\"]\ndf[\"NEW_WALKS_RATE\"] = df[\"CHits\"] \/ df[\"CAtBat\"]\ndf[\"NEW_CRUNS_RATE\"] = df[\"CRuns\"] \/ df[\"Years\"]\ndf[\"NEW_CHITS_RATE\"] = df[\"CHits\"] \/ df[\"Years\"]\n\ndf[\"NEW_TOTAL_BASES\"] = ((df[\"CHits\"] * 2) + (4 * df[\"CHmRun\"]))\ndf[\"NEW_SLUGGIN_PERCENTAGE\"] = df[\"NEW_TOTAL_BASES\"] \/ df[\"CAtBat\"]\ndf[\"NEW_ISOLETED_POWER\"] = df[\"NEW_SLUGGIN_PERCENTAGE\"] - df[\"NEW_WALKS_RATE\"]\ndf[\"NEW_TRIPLW_CROWN\"] = (df[\"CHmRun\"] * 0.4) + (df[\"CRBI\"] * 0.25) + (df[\"NEW_WALKS_RATE\"] * 0.35)","788463f6":"df.head()","9b7ff965":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","984bb7d5":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\nbinary_cols = [col for col in df.columns if df[col].dtype not in [int, float] and df[col].nunique() == 2]\n\nfor col in binary_cols:\n    label_encoder(df, col)","fb22d43f":"df.isnull().sum().sum()","efa98f33":"y=df[\"Salary\"]\nX=df.drop(\"Salary\", axis=1)\n\nX_train, X_test, y_train, y_test= train_test_split(X, y,\n                                                   test_size=0.30,\n                                                   random_state=46)\n\nrf_model = RandomForestRegressor(random_state=17).fit(X_train, y_train)\n\n# Train Error\ny_pred = rf_model.predict(X_train)\nprint(\"Train RMSE:\", \"{:,.4f}\".format(np.sqrt(mean_squared_error(y_train, y_pred))), \"\\n\")\n\n\n# Test Error\ny_pred2 = rf_model.predict(X_test)\nprint(\"Test RMSE:\", \"{:,.4f}\".format(np.sqrt(mean_squared_error(y_test, y_pred2))))\n","4091a241":"rf_model = RandomForestRegressor(random_state=17)\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 8, 15, \"auto\"],\n             \"min_samples_split\": [8, 13, 15, 20],\n             \"n_estimators\": [100, 200, 250,]}\n\n\nrf_best_grid = GridSearchCV(rf_model,\n                            rf_params,\n                            cv=5,\n                            n_jobs=-1,\n                            verbose=True).fit(X, y)\n\nrf_final = rf_model.set_params(**rf_best_grid.best_params_,\n                               random_state=17).fit(X, y)\n\n\nrmse = np.mean(np.sqrt(-cross_val_score(rf_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\nprint(f\"RMSE: {round(rmse, 4)}\")","c2c18959":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n\nplot_importance(rf_final, X)","ea9ee124":"def base_models(X, y, scoring=\"RMSE\"):\n    print(\"Base Models....\")\n    models = [('LR', LinearRegression()),\n              (\"Ridge\", Ridge()),\n              (\"Lasso\", Lasso()),\n              (\"ElasticNet\", ElasticNet()),\n              ('KNN', KNeighborsRegressor()),\n              ('CART', DecisionTreeRegressor()),\n              ('RF', RandomForestRegressor()),\n              ('SVR', SVR()),\n              ('GBM', GradientBoostingRegressor()),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n              (\"LightGBM\", LGBMRegressor()),\n              # (\"CatBoost\", CatBoostRegressor(verbose=False))\n              ]\n\n    for name, regressor in models:\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"{scoring}: {round(rmse, 4)} ({name}) \")\n\nbase_models(X, y)\n","b0f66443":"cart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 8, 15, \"auto\"],\n             \"min_samples_split\": [8, 13, 15, 20],\n             \"n_estimators\": [100, 200, 250,]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n                   \"n_estimators\": [300, 500, 1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}","f0f201c8":"regressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n              (\"RF\", RandomForestRegressor(), rf_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","e075e633":"def voting_regressorr(best_models, X, y):\n    print(\"Voting Regressor\")\n    voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('LightGBM', best_models[\"LightGBM\"])])\n\n    cv_results = np.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {cv_results}\")\n\n    return voting_reg\n\nvoting_regressorr(best_models, X, y)","9d36ac96":"<p style=\"padding: 10px; background-color:#7FDBFF;color: Black;font-weight: bold;\n          text-align: center; font-size:250%;\">Salary Prediction<\/p>","96caecb4":"<a id = \"8\"><\/a><h1 id=\"Encoding\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Encoding<\/span><\/h1>","25717dd8":"Major League Baseball Data from the 1986 and 1987 seasons.\n\n**AtBat:** Number of times at bat in 1986\n\n**Hits:** Number of hits in 1986\n\n**HmRun:** Number of home runs in 1986\n\n**Runs:** Number of runs in 1986\n\n**RBI:** Number of runs batted in in 1986\n\n**Walks:** Number of walks in 1986\n\n**Years:** Number of years in the major leagues\n\n**CAtBat:** Number of times at bat during his career\n\n**CHits:** Number of hits during his career\n\n**CHmRun:** Number of home runs during his career\n\n**CRuns:** Number of runs during his career\n\n**CRBI:** Number of runs batted in during his career\n\n**CWalks:** Number of walks during his career\n\n**League:** A factor with levels A and N indicating player's league at the end of 1986\n\n**Division:** A factor with levels E and W indicating player's division at the end of 1986\n\n**PutOuts:**Number of put outs in 1986\n\n**Assists:** Number of assists in 1986\n\n**Errors:** Number of errors in 1986\n\n**Salary:** 1987 annual salary on opening day in thousands of dollars\n\n**NewLeague:** A factor with levels A and N indicating player's league at the beginning of 1987","02637ed4":"## Analysis of Numerical Variables","3e212466":"## Stacking & Ensemble Learning","8d823478":"## Outliers","191e6336":"<a id = \"9\"><\/a><h1 id=\"Model\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Model<\/span><\/h1>","0a6aba3e":"<a id = \"1\"><\/a><h1 id=\"Introduction\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Introduction<\/span><\/h1>","21cec708":"## Hyperparameter Tuning","e496960e":"## Analysis of Correlation","9658caf3":"![4f8efb84751fb.image.jpg](attachment:ba1d9422-980f-435b-8206-0993013c9a8d.jpg)","ae75446f":"<a id = \"5\"><\/a><h1 id=\"Exploratory Data Analysis\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Exploratory Data Analysis<\/span><\/h1>","3f02a3c5":"\n<a id = \"2\"><\/a><h1 id=\"Dataset Story\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Dataset Story<\/span><\/h1>","cb5b8628":"## Other Models","4b7903f8":"Salary information and career statistics for 1986 for shared baseball players' salary estimates We will carry out a machine learning project.","a06b5ade":"<a id = \"7\"><\/a><h1 id=\"Feature Extraction\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Feature Extraction<\/span><\/h1>","78b42406":"## Random Forest Model and Feature Importances","a37c1492":"<a id = \"6\"><\/a><h1 id=\"Data Preprocessing\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Data Preprocessing<\/span><\/h1>","4c9a1ac6":"<a id = \"2\"><\/a><h1 id=\"Importing Libraries and Utilities\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Importing Libraries and Utilities<\/span><\/h1>","388c5014":"<a id = \"4\"><\/a><h1 id=\"Load and Check Data\"><span class=\"label label-default\" style=\"background-color:#e28743; font-size:30px; color: Black; \">Load and Check Data<\/span><\/h1>","fd52f3e2":"## Analysis of Categorical Variables"}}