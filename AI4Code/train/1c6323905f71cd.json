{"cell_type":{"608de035":"code","d729e8c5":"code","502a83c0":"code","51d23633":"code","18132390":"code","8c4ee740":"code","b8b12bd5":"code","bfeba5b1":"code","862fac2a":"code","9ad1ec1a":"code","e980e592":"code","4433c66b":"code","d4d772ec":"code","30794330":"code","af8282c8":"code","86e3dc76":"code","6d080c2a":"code","75dc19ba":"code","a9dce02b":"code","00859bc5":"code","c7b603bb":"code","6bebd05f":"code","3d553508":"code","4b2c0dbc":"code","b15304e5":"code","a42c886c":"code","5562b41d":"code","a873c536":"code","02124242":"code","b8543fc4":"code","b5f937f3":"code","1d38f91a":"code","61e35904":"code","308ef23a":"code","3b017060":"code","7a810af7":"code","49979443":"code","c2321ac1":"code","784baac5":"code","0f0ced22":"code","58529f54":"code","e57e79b3":"code","f90f72d1":"code","8ea12f6e":"markdown","849bee48":"markdown","09216891":"markdown","aa9145f0":"markdown","7368c935":"markdown","17717c96":"markdown","0acbf5af":"markdown"},"source":{"608de035":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier","d729e8c5":"LOCAL_FLAG = False\n\nINPUT_PATH = '\/kaggle\/input\/tabular-playground-series-mar-2021\/'\nTRAIN_FILE = 'train.csv'\nTEST_FILE = 'test.csv'\nSUBMISSION_FILE = 'sample_submission.csv'\n\n\nEARLY_STOPPING_ROUNDS = 200\nNUM_FOLDS = 10\nSEED = 42","502a83c0":"train = pd.read_csv(INPUT_PATH + TRAIN_FILE, index_col='id')\ntest = pd.read_csv(INPUT_PATH + TEST_FILE, index_col='id')\nsubmission = pd.read_csv(INPUT_PATH + SUBMISSION_FILE, index_col='id')","51d23633":"train.head()","18132390":"train.describe()","8c4ee740":"print(train.shape)\nprint(train.info())","b8b12bd5":"print(train.nunique())","bfeba5b1":"test.head()","862fac2a":"test.describe()","9ad1ec1a":"print(test.shape)\nprint(test.info())","e980e592":"print(test.nunique())","4433c66b":"submission.head()","d4d772ec":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ntrain.head()","30794330":"target = train.pop('target')","af8282c8":"target = target.to_numpy()","86e3dc76":"target.shape","6d080c2a":"columns = test.columns","75dc19ba":"print(columns)","a9dce02b":"train = pd.read_csv(INPUT_PATH + TRAIN_FILE, index_col='id')\ntest = pd.read_csv(INPUT_PATH + TEST_FILE, index_col='id')\n\nfor c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n\n        \ntarget = train.pop('target')\ntarget = target.to_numpy()","00859bc5":"def cross_valid(model, train, target, num_folds=10, random_state=42):#, init_model=pretrain_lgbm):\n\n    train_oof = np.zeros((len(train)))\n    test_preds = 0\n\n    kf = StratifiedKFold(n_splits=num_folds, random_state=SEED, shuffle=True)\n    aucs=[]\n\n    for f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, target))):\n\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        \n        train_target, val_target = target[train_ind], target[val_ind]\n\n        model.fit(train_df, \n        train_target,\n        eval_set= [(train_df, train_target), (val_df, val_target)], \n        eval_metric='auc', \n        verbose=0,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        #init_model=pretrain_lgbm,\n        )\n        \n        \n        temp_oof = model.predict_proba(val_df)[:, 1]\n        temp_test = model.predict_proba(test[columns])[:, 1]\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/num_folds\n        \n        aucs.append(roc_auc_score(val_target, temp_oof))\n        \n        print(f'Fold {f}: {roc_auc_score(val_target, temp_oof)}')\n        \n    print(\"Mean ROC AUC Score: \", np.mean(aucs))\n    \n    return train_oof, test_preds, np.mean(aucs)","c7b603bb":"lgb_params={\n    'learning_rate': 0.01,\n    'metric': 'auc',\n    'n_estimators': 11000,\n    'num_leaves': 20067,\n    'max_depth': 27,\n    'reg_alpha': 9.630576598001266,\n    'reg_lambda': 2.346945113164939,\n    'colsample_bytree': 0.29858836720777177,\n    'subsample': 0.6267448547447422,\n    'min_child_samples': 61,\n    'subsample_freq': 2,\n    'subsample': 0.8329687190743886,\n    'max_bin': 899,\n    'min_data_per_group': 73,\n    'random_state': SEED,\n    'n_jobs': -1,\n    'bagging_seed': SEED,\n    'feature_fraction_seed': SEED\n}\n\n\nclf1 = LGBMClassifier(**lgb_params)\n\ntrain_oof_1, test_preds_1, score_oof_1 = cross_valid(clf1, train, target, num_folds=NUM_FOLDS, random_state=SEED)","6bebd05f":"print(f'Score oof: {score_oof_1:0.5f}') ","3d553508":"# np.save('train_oof_lgbm_1', train_oof_1)\n# np.save('test_preds_lgbm_1', test_preds_1)","4b2c0dbc":"clf2  = LGBMClassifier(\n        n_estimators=12000,\n        num_leaves=105,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01\n    )\n\ntrain_oof_2, test_preds_2, score_oof_2 = cross_valid(clf2, train, target, num_folds=NUM_FOLDS, random_state=SEED)","b15304e5":"print(f'Score oof: {score_oof_2:0.5f}') ","a42c886c":"clf3 = LGBMClassifier(\n        n_estimators=12000,\n        num_leaves=50,\n        colsample_bytree=.8,\n        subsample=.8,\n        max_depth=8,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01\n    )\n\ntrain_oof_3, test_preds_3, score_oof_3 = cross_valid(clf3, train, target, num_folds=NUM_FOLDS, random_state=SEED)","5562b41d":"print(f'Score oof: {score_oof_3:0.5f}') ","a873c536":"# clf1 = LGBMClassifier(\n#         n_estimators=10000,\n#         num_leaves=20,\n#         colsample_bytree=.8,\n#         subsample=.8,\n#         max_depth=8,\n#         reg_alpha=.1,\n#         reg_lambda=.1,\n#         min_split_gain=.01\n#     )\n\n# clf1.fit(X_train, \n#         y_train,\n#         eval_set= [(X_train, y_train), (X_test, y_test)], \n#         eval_metric='auc', \n#         verbose=0, \n#         early_stopping_rounds=50\n#        )\n\n# y_pred1 = clf1.predict_proba(X_test)[:, 1]\n# score1 = roc_auc_score(y_test, y_pred1)\n# print(f'{score1:0.5f}') ","02124242":"# score1 = roc_auc_score(y_test, y_pred1)\n# print(f'{score1:0.5f}') ","b8543fc4":"# clf2 = LGBMClassifier(\n#         n_estimators=20000,\n#         num_leaves=25,\n#         colsample_bytree=.8,\n#         subsample=.8,\n#         max_depth=7,\n#         reg_alpha=.1,\n#         reg_lambda=.1,\n#         min_split_gain=.01\n#     )#RandomForestClassifier(n_estimators=2000, max_depth=8, n_jobs=-1)\n\n# clf2.fit(X_train, \n#         y_train,\n#         eval_set= [(X_train, y_train), (X_test, y_test)], \n#         eval_metric='auc', \n#         verbose=0, \n#         early_stopping_rounds=40\n#        )\n\n\n# y_pred2 = clf2.predict_proba(X_test)[:, 1]","b5f937f3":"# score2 = roc_auc_score(y_test, y_pred2)\n# print(f'{score2:0.5f}') ","1d38f91a":"# clf3 = LGBMClassifier(\n#         n_estimators=20000,\n#         num_leaves=30,\n#         colsample_bytree=.8,\n#         subsample=.8,\n#         max_depth=8,\n#         reg_alpha=.1,\n#         reg_lambda=.1,\n#         min_split_gain=.01\n#     )#RandomForestClassifier(n_estimators=3000, max_depth=12, n_jobs=-1)\n# clf3.fit(X_train, \n#         y_train,\n#         eval_set= [(X_train, y_train), (X_test, y_test)], \n#         eval_metric='auc', \n#         verbose=0, \n#         early_stopping_rounds=30\n#        )\n\n# y_pred3 = clf3.predict_proba(X_test)[:, 1]","61e35904":"# score3 = roc_auc_score(y_test, y_pred3)\n# print(f'{score3:0.5f}')","308ef23a":"# from scipy.optimize import minimize\n\n# ### finding the optimum weights\n\n# predictions = []\n# predictions.append(test_preds_1)\n# predictions.append(test_preds_2)\n# predictions.append(test_preds_3)\n\n# def roc_auc_loss_func(weights):\n#     ''' scipy minimize will pass the weights as a numpy array '''\n#     final_prediction = 0\n#     for weight, prediction in zip(weights, predictions):\n#             final_prediction += weight*prediction\n\n#     return roc_auc_score(y_test, final_prediction)\n\n# # starting random value for each weight\n# starting_values = [0.5]*len(predictions) \n\n# cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n# # our weights are bound between 0 and 1\n# bounds = [(0,1)]*len(predictions) \n\n# res = minimize(roc_auc_loss_func, starting_values, method = 'Nelder-Mead', bounds=bounds, constraints=cons)\n\n# print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n# print('Best Weights: {weights}'.format(weights=res['x']))","3b017060":"# tt1 = clf1.predict_proba(test)[:, 1]\n# tt2 = clf2.predict_proba(test)[:, 1]\n# tt3 = clf3.predict_proba(test)[:, 1]","7a810af7":"# submission['target'] =  (res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3 #(test_preds_1 + test_preds_2 + test_preds_3)\/3 #(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\n# submission.to_csv('submission_nelder_mead.csv')\n# submission.head()","49979443":"# from scipy.optimize import minimize\n\n# ### finding the optimum weights\n\n# predictions = []\n# predictions.append(test_preds_1)\n# predictions.append(test_preds_2)\n# predictions.append(test_preds_3)\n\n# def roc_auc_loss_func(weights):\n#     ''' scipy minimize will pass the weights as a numpy array '''\n#     final_prediction = 0\n#     for weight, prediction in zip(weights, predictions):\n#             final_prediction += weight*prediction\n\n#     return roc_auc_score(y_test, final_prediction)\n\n# # starting random value for each weight\n# starting_values = [0.5]*len(predictions) \n\n# cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n# # our weights are bound between 0 and 1\n# bounds = [(0,1)]*len(predictions) \n\n# res = minimize(roc_auc_loss_func, starting_values, bounds=bounds, constraints=cons)\n\n# print('Ensamble Score: {best_score}'.format(best_score=res['fun']))\n# print('Best Weights: {weights}'.format(weights=res['x']))","c2321ac1":"tt1 = clf1.predict_proba(test)[:, 1]\ntt2 = clf2.predict_proba(test)[:, 1]\ntt3 = clf3.predict_proba(test)[:, 1]","784baac5":"# submission['target'] =  (0.33*test_preds_1 + 0.33*test_preds_2 + 0.34*test_preds_3)\/3 #(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\n# submission.to_csv('submission.csv')\n# submission.head()","0f0ced22":"submission['target'] = (test_preds_1 + test_preds_2 + test_preds_3)\/3 #(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\nsubmission.to_csv('submission.csv')\nsubmission.head()","58529f54":"submission['target'] =  (test_preds_1)#(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\nsubmission.to_csv('submission_model_1.csv')\nsubmission.head()","e57e79b3":"submission['target'] =  (test_preds_2) #(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\nsubmission.to_csv('submission_model_2.csv')\nsubmission.head()","f90f72d1":"submission['target'] =  (test_preds_3) #(res['x'][0]*tt1+res['x'][1]*tt2+res['x'][2]*tt3)\/3\nsubmission.to_csv('submission_model_3.csv')\nsubmission.head()","8ea12f6e":"Hello, kagglers! :D Nice to meet you. In this notebook, I am going to share some cool material that I have been learning and collecting from a many different places!","849bee48":"# Initial Exploration","09216891":"# Imports","aa9145f0":"# Label Encoding","7368c935":"# Parameters","17717c96":"## Simple Ensemble","0acbf5af":"## Work in progress :)"}}