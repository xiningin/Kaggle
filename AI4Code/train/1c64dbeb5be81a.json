{"cell_type":{"e3832158":"code","9100e793":"code","170b81c9":"code","17f45785":"code","ae5c051c":"code","6216d1b8":"code","f0e0d965":"code","5f86f662":"code","70444b24":"code","589b3bf9":"code","f315fcb1":"code","aa7c0185":"code","4521166d":"code","42095e55":"code","89257696":"code","e78c143e":"code","df353770":"code","89541a8d":"code","963a04d1":"code","6bef5cde":"code","7ac86ab0":"code","0be5c6dd":"code","7d247865":"code","4c8b546a":"code","2d6126d0":"code","cc093cb9":"code","7366a196":"code","1e8b32fb":"code","83cdc0de":"code","722c9bc4":"code","5c96de0c":"code","b9677dc3":"code","72baa0f0":"code","a15b2b3a":"code","b1422901":"code","9eb36519":"code","724ca826":"code","181ec3b3":"code","55f56069":"code","0369b359":"code","9d94bdab":"code","d214be0f":"code","d3990d3b":"code","549496c2":"markdown","f96b0ec9":"markdown","71fa95f8":"markdown","ddc9edf0":"markdown","13ff5546":"markdown","dc77006e":"markdown","adbce41f":"markdown","224f4caa":"markdown","9d987ed1":"markdown","aec49498":"markdown","48c606b4":"markdown","55b269fa":"markdown","e407cd21":"markdown","9aaf146f":"markdown","f063ee40":"markdown","eb51de72":"markdown","74e5a8ae":"markdown","3e4684d6":"markdown","dd1da574":"markdown","c50edfd7":"markdown","43846b5d":"markdown","deee738c":"markdown","a0baa086":"markdown","f7e5e567":"markdown","5aab1e51":"markdown","7b677fbc":"markdown","92659302":"markdown","2f99ee4e":"markdown","ff9eaca3":"markdown","8ace7caf":"markdown","d4df8300":"markdown","60e7bafc":"markdown","1bbb425d":"markdown","05b51f1e":"markdown","7f5a9b7a":"markdown","850c8244":"markdown","953603cd":"markdown","b82281c6":"markdown","f196cc8a":"markdown","39036e38":"markdown","7577c14e":"markdown","9f88520f":"markdown","ddbaae47":"markdown","dd5d8043":"markdown","2c0edab4":"markdown","72a8c6a7":"markdown","ef1bba70":"markdown","c5328437":"markdown","73943ab3":"markdown","5ba10bdc":"markdown","989d26ea":"markdown","756a8c07":"markdown","f5bf717b":"markdown","19d4eee0":"markdown","7d3df88a":"markdown"},"source":{"e3832158":"# Basic Pydata Libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt     \nimport seaborn as sns\n\n# Standard plotly imports\nimport plotly.offline as py \nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\ncufflinks.go_offline(connected=True)\n\n#nlp\nimport string\nimport re    #for regex\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\nfrom nltk import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\n# Tweet tokenizer does not split at apostophes which is what we want\nfrom nltk.tokenize import TweetTokenizer   \nfrom wordcloud import WordCloud, STOPWORDS\n\n## warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9100e793":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","170b81c9":"# Missing values\ndf_train.isnull().sum() \/ len(df_train) * 100","17f45785":"df_train.info()","ae5c051c":"df_train.shape","6216d1b8":"df_train.head()","f0e0d965":"count_target_zero = round(df_train[df_train['target'] == 0]['target'].count() \/ len(df_train['target']) * 100,2)\nprint(f'Total of zero values in Toxic rate: {count_target_zero}%')\n\nplt.figure(figsize=(13,6))\n\ng = sns.distplot(df_train[df_train['target'] > 0]['target'])\nplt.title('Toxic Distribuition', fontsize=22)\nplt.xlabel(\"Toxic Rate\", fontsize=18)\nplt.ylabel(\"Distribuition\", fontsize=18) \n\nplt.show()","5f86f662":"comment_adj = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\nplt.figure(figsize=(14,6))\n\nfor col in comment_adj[1:]:\n    g = sns.distplot(df_train[df_train[col] > 0][col], label=col, hist=False)\n    #plt.legend(f'{col} Distribuition', fontsize=22)\n    plt.xlabel(\"Rate\", fontsize=18)\n    plt.ylabel(\"Distribuition\", fontsize=18)\n    plt.legend(loc=1, prop={'size': 14})\n\nplt.show()","70444b24":"## Creating a flag to toxic and non-toxic comments\ndf_train['toxic'] = np.where(df_train['target'] >= .5, 'Toxic', 'Non-Toxic')","589b3bf9":"df_train['toxic'].value_counts().iplot(kind='bar', xTitle='Toxic or Non-Toxic', yTitle=\"Count\", \n                                       title='Distribuition of Toxicity of comments')","f315fcb1":"etnics = ['asian' , 'latino' , 'black', 'white', 'other_race_or_ethnicity']\n\nreligions = ['atheist', 'buddhist', 'hindu', 'jewish', 'muslim', 'christian', 'other_religion']\n\nsexual = ['female', 'male', 'other_gender'] \n\nsexual_orientation = ['heterosexual', 'bisexual', 'transgender', 'homosexual_gay_or_lesbian', 'other_sexual_orientation']\n\ndisabilities = ['intellectual_or_learning_disability', 'physical_disability', 'psychiatric_or_mental_illness', 'other_disability']\n\nreactions = ['funny', 'wow', 'sad', 'likes', 'disagree', 'sexual_explicit']","aa7c0185":"## The inspiration to this kernel is https:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw\n\netnics_dem = df_train.loc[:, ['target']+list(etnics + ['toxic'])].dropna()\ncount_etnics = etnics_dem.iloc[:, 1:][etnics_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nreligion_dem = df_train.loc[:, ['target']+list(religions  + ['toxic'])].dropna()\ncount_religions = religion_dem.iloc[:, 1:][religion_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nsexual_dem = df_train.loc[:, ['target']+list(sexual  + ['toxic'])].dropna()\ncount_sexual = sexual_dem.iloc[:, 1:][sexual_dem.iloc[:, 1:] > 0].groupby('toxic').count()\n\nsexual_orient = df_train.loc[:, ['target']+list(sexual_orientation  + ['toxic'])].dropna()\ncount_orient_sexual = sexual_orient.iloc[:, 1:][sexual_orient.iloc[:, 1:] > 0].groupby('toxic').count()\n\ndisabilities_dem = df_train.loc[:, ['target']+list(disabilities  + ['toxic'])].dropna()\ncount_desabilities = disabilities_dem.iloc[:, 1:][disabilities_dem.iloc[:, 1:] > 0].groupby('toxic').count()","4521166d":"list_groupbys = [count_etnics, count_religions, count_sexual, count_orient_sexual, count_desabilities]\nlist_names = ['Ethnics Comments by Toxic and Non-Toxic Classification',\n              'Religions Comments by Toxic and Non-Toxic Classification',\n              'Sexual Comments by Toxic and Non-Toxic Classification',\n              'Sexual Orientation Comments by Toxic and Non-Toxic Classification',\n              'Disabilities Comments by Toxic and Non-Toxic Classification', ]","42095e55":"for plot, text in zip(list_groupbys, list_names):\n    plot.T.iplot(kind='bar', xTitle='Demographic categories', yTitle='Count',\n                 title=text)","89257696":"df_train['rating'].value_counts().iplot(kind='bar', title='Rating of Comment', \n                                        xTitle='Rating', yTitle='Count')","e78c143e":"round((pd.crosstab(df_train['rating'], df_train['toxic'], \n            normalize='index') * 100),2).iplot(kind='bar', barmode='stack', \n                                               title= \"Rating Ratio by Toxic and Non-Toxic\",\n                                               xTitle=\"Rating Status\", yTitle='% of Toxic and Non-Toxic')\n","df353770":"# transforming to pandas \ndf_train['created_date'] = pd.to_datetime(df_train['created_date'], format='%Y-%m-%d %H:%M:%S')\n\ndf_train['month'] = df_train['created_date'].dt.month\ndf_train['weekday'] =  df_train['created_date'].dt.weekday_name\ndf_train['hour'] =  df_train['created_date'].dt.hour\n\n# df_train['created_date'] = pd.to_datetime(df_train['created_date'])","89541a8d":"# printing the first and last date\nprint(f'The first date is {df_train[\"created_date\"].dt.date.min()} and the last date is {df_train[\"created_date\"].dt.date.max()}')","963a04d1":"# I will filter by comments date higher than 2016-01-01\ntoxic_comment_dates = df_train[df_train['created_date'] >= '2016-01-01'].groupby([df_train['created_date'].dt.date,'toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_dates.iplot(kind='bar', barmode='stack', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )","6bef5cde":"# dates higher than 2016-01-01\ntoxic_comment_months = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['month','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_months.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Date', \n                   xTitle='Dates', yTitle='Comment Counts'\n                )","7ac86ab0":"# dates higher than 2016-01-01\ntoxic_comment_week = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['weekday','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0).sort_index()\n\ntoxic_comment_week.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Weekdays', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )","0be5c6dd":"# dates higher than 2016-01-01\ntoxic_comment_hour = df_train[df_train['created_date'] >= '2016-01-01'].groupby(['hour','toxic'])['id'].count().sort_index().unstack(\"toxic\").fillna(0)\n\ntoxic_comment_hour.iplot(kind='bar', barmode='group', \n                   title='Toxic and Non-Toxic Comment by Hours', \n                   xTitle='Weekday Names', yTitle='Comment Counts'\n                )","7d247865":"#Setting the stopwords\neng_stopwords = set(stopwords.words(\"english\"))\n\n#Word count in each comment:\ndf_train['count_word']= df_train[\"comment_text\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count\ndf_train['count_unique_word']=df_train[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count\ndf_train['count_letters']=df_train[\"comment_text\"].apply(lambda x: len(str(x)))\n\n#punctuation count\ndf_train[\"count_punctuations\"] = df_train[\"comment_text\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count\ndf_train[\"count_words_upper\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count\ndf_train[\"count_words_title\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords\ndf_train[\"count_stopwords\"] = df_train[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\n#Average length of the words\ndf_train[\"mean_word_len\"] = df_train[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","4c8b546a":"comments_counts = ['count_word', 'count_unique_word', 'count_letters', 'count_punctuations', \n                   'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len']\ndef quantiles(columns):\n    # To append the quantile outputs\n    quantile_data = []\n\n    # Looping for the created columns\n    for counts in columns:\n        # Quantiles from desired columns\n        quantiles = df_train[counts].quantile([.01,.25,.5,.75,.99])\n        # Store quantile DataFrame in list\n        quantile_data.append(quantiles)\n\n    # Now concat the data that you got the quantiles\n    return pd.concat(quantile_data, axis=1)","2d6126d0":"quantiles(comments_counts)","cc093cb9":"trace0 = go.Box(\n    x=df_train['toxic'].sample(20000),\n    y=df_train['count_word'].sample(20000),\n    name='Toxic', showlegend=False, jitter=0.2, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace2 = go.Box(\n    x=df_train[(df_train['count_unique_word'] <= 128)]['toxic'].sample(20000),\n    y=df_train[(df_train['count_unique_word'] <= 128)]['count_unique_word'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace4 = go.Box(\n    x=df_train[ (df_train['count_letters'] <= 999)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_letters'] <= 999)]['count_letters'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace6 = go.Box(\n    x=df_train[ (df_train['count_punctuations'] <= 45)]['toxic'].sample(20000),\n    y=df_train[  (df_train['count_punctuations'] <= 45)]['count_punctuations'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\n\ntrace8 = go.Box(\n    x=df_train[ (df_train['count_words_upper'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_upper'] <= 9)]['count_words_upper'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace10 = go.Box(\n    x=df_train[ (df_train['count_words_title'] <= 9)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_words_title'] <= 9)]['count_words_title'].sample(20000),\n    name='Toxic', showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace12 = go.Box(\n    x=df_train[ (df_train['count_stopwords'] <= 88)]['toxic'].sample(20000),\n    y=df_train[ (df_train['count_stopwords'] <= 88)]['count_stopwords'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n)\n\ntrace14 = go.Box(\n    x=df_train[  (df_train['mean_word_len'] <= 9.129411)]['toxic'].sample(20000),\n    y=df_train[ (df_train['mean_word_len'] <= 9.129411)]['mean_word_len'].sample(20000),\n    name='Toxic',  showlegend=False, marker = dict( color = 'rgb(0, 128, 128)')\n    \n)\n\n\ndata = [trace0, trace2, trace4, trace6,trace8, \n        trace10,trace12, trace14]\n\nfig = tls.make_subplots(rows=4, cols=2, specs=[[{}, {}], \n                                               [{}, {}], \n                                               [{}, {}], \n                                               [{}, {}]],\n                          subplot_titles=('Word Counts','Unique Words Count', 'Letters Count', 'Punctuation Count', \n                                          'Upper Case Count','Words Title Count', 'Stopwords Count', 'Mean Words Len'))\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace4, 2, 1)\nfig.append_trace(trace6, 2, 2)\nfig.append_trace(trace8, 3, 1)\nfig.append_trace(trace10, 3, 2)\nfig.append_trace(trace12, 4, 1)\nfig.append_trace(trace14, 4, 2)\n\nfig['layout'].update(title='Comment Metrics by Toxic and Non-Toxic', autosize=True, boxmode='group')\n\niplot(fig)","7366a196":"#testing a violin plot","1e8b32fb":"df_train['attacked_group'] = df_train[etnics+ sexual + sexual_orientation + religions + disabilities].replace(0, np.nan).idxmax(axis=1)\ndf_train['attacked_group'] = df_train['attacked_group'].fillna(\"No demo group detected\")","83cdc0de":"att_count = df_train['attacked_group'].value_counts()\nprint(f\"Total of No Demographic Group Detected {att_count[0]}\")\n\ndf_train[df_train['attacked_group'] != 'No demo group detected']['attacked_group'].value_counts().iplot(kind='bar', title='Count of Highest values in Attacked Groups',\n                                                xTitle='Demographic Group Name', yTitle='Count of highest \"Citations\"')","722c9bc4":"attacked_group = pd.crosstab(df_train['attacked_group'],df_train['toxic'], aggfunc='count', values=df_train['target']).apply(lambda r: r\/r.sum(), axis=1)\nattacked_group.iplot(kind='bar',barmode='stack',\n                     title='Percent of Toxic and Non-Toxic Comments for Attacked Groups',\n                     xTitle='Demographic Group Name', yTitle='Percent ratio of each Group')","5c96de0c":"## Again, calling the quantile function that we created before\nquantiles(reactions)\n\n\naggs = {\n    'sexual_explicit': ['sum', 'size'],\n    'likes': ['sum'],\n}\n\n# Previous applications categorical features\n\nprev_agg = df_train[df_train['attacked_group'] != 'No demo group detected'].groupby(['attacked_group','toxic']).agg({**aggs})\n\nprev_agg.columns = pd.Index(['Agg_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n\nprev_agg.rename(columns={'Agg_sexual_explicit_SUM':'Sexual bias sum',\n                         'Agg_likes_SUM':' Likes sum',\n                         'Agg_sexual_explicit_SIZE':'Total Comments'}, inplace=True)\nprev_agg.T","b9677dc3":"prev_agg.sort_index().unstack(\"toxic\").fillna(0).iplot(kind='bar', showlegend=False, \n                                                       title ='Demographic Groups by Sum of sexual explicit and Sum of Likes',\n                                                       xTitle='Demographic Groups', yTitle='Count')","72baa0f0":"color_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ndef PieChart(df_cat, df_value, title, limit=15):\n    \"\"\"\n    This function helps to investigate the proportion of metrics of toxicity and other values\n    \"\"\"\n\n    # count_trace = df_train[df_cat].value_counts()[:limit].to_frame().reset_index()\n    rev_trace = df_train[df_train['toxic'] == \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n    rev_trace_non = df_train[df_train['toxic'] != \"Toxic\"].sample(50000).groupby(df_cat)[df_value].mean().nlargest(limit).to_frame().reset_index()\n\n    trace1 = go.Pie(labels=rev_trace_non[df_cat], \n                    values=rev_trace_non[df_value], name= \"Non-Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=True,\n                    domain= {'x': [0, .48]})\n\n    trace2 = go.Pie(labels=rev_trace[df_cat], \n                    values=rev_trace[df_value], name=\"Toxic\", hole= .5, \n                    hoverinfo=\"label+percent+name+value\", showlegend=False, \n                    domain= {'x': [.52, 1]})\n\n    layout = dict(title= title, height=450, font=dict(size=15),\n                  annotations = [\n                      dict(\n                          x=.20, y=.5,\n                          text='Non-Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      ),\n                      dict(\n                          x=.80, y=.5,\n                          text='Toxic', \n                          showarrow=False,\n                          font=dict(size=20)\n                      )\n        ])\n\n    fig = dict(data=[trace1, trace2], layout=layout)\n    iplot(fig)","a15b2b3a":"PieChart(\"attacked_group\", 'sexual_explicit', \"Mean of sexual Explicit by categories\", limit=10)","b1422901":"PieChart(\"attacked_group\", 'likes', \"Mean of sexual Explicit by categories\", limit=10)","9eb36519":"PieChart(\"article_id\", 'likes', \"Mean Likes in Toxic and Non-Toxic comments by TOP 10 Articles\", limit = 10)","724ca826":"PieChart(\"article_id\", 'sexual_explicit', \"Mean Sexual Explicit in Comments by TOP 10 Articles\", limit = 10)","181ec3b3":"PieChart(\"publication_id\", 'target', \"Mean Target (Toxicity) in Comments by each Publication\", limit = 10) ","55f56069":"PieChart(\"publication_id\", 'sexual_explicit', \"Mean Sexual Explicit in comments by Publication\", limit = 10)","0369b359":"PieChart(\"publication_id\", 'likes', \"Mean Likes in Comments by each Publication\", limit = 10)","9d94bdab":"name = df_train['identity_annotator_count'].value_counts()[:8]\n\nfig = pd.crosstab(df_train[df_train['identity_annotator_count'].isin(name.index)]['identity_annotator_count'], \n                  df_train[df_train['identity_annotator_count'].isin(name.index)]['toxic'], \n                  normalize='index').iplot(kind='bar', barmode='stack', bargap=.2, asFigure=True,\n                                           title= \"TOP 8 Identity Annotator by Toxic and Non-Toxic\",\n                                           xTitle=\"Identity Annotator Count\", yTitle='Count')\nfig.layout.xaxis.type = 'category'\niplot(fig)","d214be0f":"\ntext_to_analize= df_train[df_train['toxic'] == \"Non-Toxic\"]['comment_text']\n\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1 \n\nplt.figure(figsize = (15,15))\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1000,\n                          max_font_size=120, \n                          random_state=42\n                         ).generate(str(text_to_analize))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - NON-TOXIC\")\nplt.axis('off')\nplt.show()","d3990d3b":"from wordcloud import WordCloud, STOPWORDS\n\ntext_to_analize= df_train[df_train['toxic'] == \"Toxic\"]['comment_text']\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1 \n\nplt.figure(figsize = (15,15))\n\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(\n                          background_color='black',\n                          stopwords=stopwords,\n                          max_words=1000,\n                          max_font_size=120, \n                          random_state=42\n                         ).generate(str(text_to_analize))\n\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"WORD CLOUD - TOXICS\")\nplt.axis('off')\nplt.show()","549496c2":"## Creating a flag to separe Toxic and Non-Toxic comments (Target >= 0.5)","f96b0ec9":"## Rating by Toxic and Non-Toxic comments","71fa95f8":"## Knowing identity_annotator_count","ddc9edf0":"## Grouping by toxic by each demographic ","13ff5546":"## Ploting the Distribuition of Categorical Toxic and Non-Toxic","dc77006e":"\nEnglish is not my first language, so sorry for any mistake. ","adbce41f":"99% of all data are in this top 8 Identitity annotators. <br>\nThe value of toxic on the value 0 is lowest in proportionality than Identity Annotator 4;<br>\nWhat it means? ","224f4caa":"## Ploting Target (Toxicity) distribuition","9d987ed1":"## Sexual Explicit by Publication Id","aec49498":"It's interesting. <br>\n5% of approved comments are Toxic; <br>\nAs we can see 66% of rejected comments aren't toxic.\n","48c606b4":"# NOTE: This Kernel is not finished. I'm working on it yet. \nVotes up the kernel and stay tuned.\n\n\n# Introduction\n\nIn this notebook I will create a baseline to understand the Jigsaw data and after it create my model to predict Toxic Comments.\n\n## Competition Description:\nCan you help detect toxic comments \u2015 and minimize unintended model bias? That's your challenge in this competition.\n\nThe Conversation AI team, a research initiative founded by Jigsaw and Google (both part of Alphabet), builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion.\n\nLast year, in the Toxic Comment Classification Challenge, you built multi-headed models to recognize toxicity and several subtypes of toxicity. This year's competition is a related challenge: building toxicity models that operate fairly across a diverse range of conversations.\n\nHere\u2019s the background: When the Conversation AI team first built toxicity models, they found that the models incorrectly learned to associate the names of frequently attacked identities with toxicity. Models predicted a high likelihood of toxicity for comments containing those identities (e.g. \"gay\"), even when those comments were not actually toxic (such as \"I am a gay woman\"). This happens because training data was pulled from available sources where unfortunately, certain identities are overwhelmingly referred to in offensive ways. Training a model from data with these imbalances risks simply mirroring those biases back to users.\n\nIn this competition, you're challenged to build a model that recognizes toxicity and minimizes this type of unintended bias with respect to mentions of identities. You'll be using a dataset labeled for identity mentions and optimizing a metric designed to measure unintended bias. Develop strategies to reduce unintended bias in machine learning models, and you'll help the Conversation AI team, and the entire industry, build models that work well for a wide range of conversations.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n","55b269fa":"## Toxicity Subtype attributes Distribution","e407cd21":"## Ploting the distribuitions of the data modelling","9aaf146f":"## WordCloud of Toxic Comments","f063ee40":"## Head - knowning the data","eb51de72":"## WordCloud of Non-Toxic Comments","74e5a8ae":"## HELP: How can I set manually the size of box?! \nWhy they're not not aligned?","3e4684d6":"It's a very informative visualization. <br>We can see that some categories, as blacks, hindu (indians?), and hetero(??) orientation  has a highest number of toxic comments against this group. ","dd1da574":"- I will try a way to best apresent this chart above\n\nIt's another interesting information about the data. Below, I will try to investigate it further;","c50edfd7":"## Distribuitions of the new features","43846b5d":"## Sexual Explicit distribution by Articles Id","deee738c":"## Defining some categories of comment and counting","a0baa086":"## Likes distribution by Publication Id","f7e5e567":"As we can see, the unique valid values are likes and sexual explicit. <br>\nLet's investigate it further","5aab1e51":"# Finding the highest values for each demographic group","7b677fbc":"## Importing the Data","92659302":"## Looking the distribution by week days;","2f99ee4e":"## Creating a list to plot","ff9eaca3":"## Likes by Attacked Group","8ace7caf":"We have almost 1.6M comments that are not related to demographic groups that are in the categories","d4df8300":"Wow. It's very interesting and curious. <br>\nIn Non-Toxic:<br>\nAs we can see, comments about Transgender has a value 3 times highest than the third position. <br>\n  \nIn Toxic:<br>\nIt's interesting that atheists has a very high mean in sexual explicit. I will investigate this category further in comments.","60e7bafc":"We can see that altough the number comments has increased a lot, the toxic comments are seemgly a constant value","1bbb425d":"## Looking the % ratio of Toxic and Non-Toxic for each demo group","05b51f1e":"## Understanding the \"Reaction\" metrics","7f5a9b7a":"## Ploting distribuition of text metrics","850c8244":"## Creating some features from comments ","953603cd":"## Toxicity mean by Publication","b82281c6":"## Let's invetigate some \"stealth\" columns by many metrics\n- Some tests using PieChart","f196cc8a":"Interesting distribuition. <br>\nFor default, in the competition description we will consider toxic when the target has values above 0.5","39036e38":"## Shape of our data","7577c14e":"## Import librarys","9f88520f":"Interesting that Saturday and Sunday are the day with smallest number of comments but \"respecting\" the Toxicity Ratio.","ddbaae47":"# Let's start the work","dd5d8043":"## Exploring some information about the data\n- nulls\n- data types\n- shape\n- First rows of dataset","2c0edab4":"## Some questions that I will try to answer: <br>\n- What is the distribuition of Toxicity?<br>\n- What is the most attacked group?<br>\n- We can see some difference between comments of toxic and non-toxic comments?<br>\n- Are the date of the year correlated with the toxicity comments?<br>\n- The toxicity ratio is equal to all articles?<br>\n- What's the groups with the highest toxicity, likes, disagrees.  \n- And much more.","72a8c6a7":"## Looking the distribution by Months;\nWe will consider the dates higher than 2016-01-01 so I have a small sample of months. <br>\nIt's just to explore and may find a interesting pattern.","ef1bba70":"## Rating distribution","c5328437":"## Sexual Explicit by Attacked Groups","73943ab3":"We can see and have insight about some of the toxicy comments. Further, I will explore the comments;","5ba10bdc":"## Likes distribution by Articles Id","989d26ea":"Ok, Now that we have some idea of how the dataset is, we can start exploring and understanding the dataset","756a8c07":"## Data info","f5bf717b":"## I will keep improving and Working on this Kernel. \n# If you liked this Kernel, please votes up the kernel \n\nSome fonts that I used to inspire and learn to build this kernel: <br>\nhttps:\/\/www.kaggle.com\/nz0722\/simple-eda-text-preprocessing-jigsaw <br>\nhttps:\/\/www.kaggle.com\/gpreda\/jigsaw-eda <br>\nand a lot of other that I forget to save the link","19d4eee0":"Very interesting distribuitions. <br>\nalso, we can see that the Toxicity ratio is very constant.","7d3df88a":"## Understanding Toxic and Non-Toxic Comments by Dates"}}