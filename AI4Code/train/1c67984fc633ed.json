{"cell_type":{"1e3adb23":"code","39130275":"code","f068512f":"code","4babf7d5":"code","f144b88b":"code","e7ebfe23":"code","d01370cf":"code","9af8f227":"code","41d7ef29":"code","2ae46c97":"code","b8ef63b3":"code","9073d9b0":"code","e9d4a997":"code","326f171e":"code","c58ccf44":"code","c7b4ecd0":"markdown","dd7b2d08":"markdown","6835b0e8":"markdown","f4e03e62":"markdown","ad453a90":"markdown","32668564":"markdown","028f9a38":"markdown","73820ecf":"markdown","56366e91":"markdown","59bf1085":"markdown","c46be9a1":"markdown","0e686543":"markdown","1cb0f5c6":"markdown","738cc305":"markdown","61ab49a6":"markdown"},"source":{"1e3adb23":"import numpy as np\nimport math","39130275":"data_path = \"..\/input\/mnist_seven.csv\"\ndata = np.genfromtxt(data_path, delimiter=\",\", dtype=\"uint8\")\ntrain, dev, test = data[:4000], data[4000:4500], data[4500:]","f068512f":"def normalize(dataset):\n    X = dataset[:, 1:] \/ 255.     # Normalize input features\n    Y = (dataset[:, 0] == 7) * 1  # Convert labels from 0-9 to Is7 (1) or IsNot7(0)\n    return X.T,Y.reshape(1, -1)","4babf7d5":"X_train, Y_train = normalize(train)\nprint(X_train.shape)\nprint(Y_train.shape)\n\nX_test, Y_test = normalize(test)\nprint(X_test.shape)\nprint(Y_test.shape)\n\n# shuffle the training data since we do SGD\n# we shuffle outside the training \n# since we want to compare unvectorized and vectorized versions\n# It doesn't affect to batch training later\nnp.random.seed(8888)     # Do not change those seedings to make our results comparable\nnp.random.shuffle(train) \n","f144b88b":"def train_unvectorized(X_train, Y_train, lr=0.2, lambdar=0.0001, epochs=5):\n    \n    n = X_train.shape[0]\n    m = X_train.shape[1]\n    \n    # Xavier Initialization\n    np.random.seed(1234)\n    w = np.random.randn(n) * (np.sqrt(2. \/ (n + 1)))\n    b = 0\n\n    for epoch in range(epochs):\n        L = 0\n        for j in range(m):   # Loop over every training instance\n            # Forward pass\n            z = 0 \n            r = 0 # calculate the regularizer\n            for i in range(n):\n                z += w[i] * X_train[i,j]\n                r += w[i]**2\n            z += b\n            o = 1. \/ (1 + math.exp(-z))\n\n            # Calculate the loss\n            c = (Y_train[:,j] - o)**2\n            l = c \/ 2 + (lambdar \/ 2) * r\n            L += l\n\n            # Backward pass and update the weights\/bias\n            for i in range(n):\n                w[i] -= lr * (o - Y_train[:,j]) * o * (1 - o) * X_train[i,j] + lambdar * w[i]\n            b -= lr * (o - Y_train[:,j]) * o * (1 - o)\n        L \/= m\n        print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L))\n    \n    return w, b\n        ","e7ebfe23":"def test_unvectorized(X_test, Y_test, w, b):\n    \n    n_test = X_test.shape[0]\n    m_test = X_test.shape[1]\n    corrects = 0\n    \n    for j in range(m_test):\n\n        # Forward pass\n        z = 0 \n        for i in range(n_test):\n            z += w[i] * X_test[i,j]\n        z += b\n        o = 1. \/ (1 + np.exp(-z))\n        # Evaluate the outputs\n        if ((o >= 0.5) and (Y_test[0, j] == 1)) \\\n        or ((o < 0.5) and (Y_test[0, j] == 0)):\n            corrects +=1\n    \n    print(\"Accuracy of our LLSR:\" + str((corrects * 100.) \/ m_test) + \"%\")\n    \n    return corrects\n","d01370cf":"w, b = train_unvectorized(X_train, Y_train)\n_ = test_unvectorized(X_test, Y_test, w, b)","9af8f227":"def train_vectorized(X_train, Y_train, lr=0.2, lambdar=0.0001, epochs=5):\n    \n    n = X_train.shape[0]\n    m = X_train.shape[1]\n    \n    # Xavier Initialization\n    np.random.seed(1234)\n    w = np.random.randn(n) * (np.sqrt(2. \/ (n + 1)))\n    b = 0\n\n    for epoch in range(epochs):\n        L = 0\n        for j in range(m):\n\n            # Forward pass\n            z = np.dot(w, X_train[:,j]) + b\n            r = np.sum(w**2)\n            o = 1. \/ (1 + np.exp(-z))\n\n            # Calculate the loss (for each instance - SGD) \n            c = (Y_train[:,j] - o)**2\n            l = c \/ 2 + (lambdar \/ 2) * r\n            L += l\n\n            # Backward pass and update the weights\/bias (for each instance - SGD) \n            w -= lr * (o - Y_train[:,j]) * o * (1 - o) * X_train[:,j] + lambdar * w \n            b -= lr * (o - Y_train[:,j]) * o * (1 - o)\n        L \/= m\n        print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L))\n    return w, b","41d7ef29":"def test_vectorized(X_test, Y_test, w, b):\n    \n    m_test = X_test.shape[1]\n    result = np.zeros((1, m_test))\n    \n    z = np.dot(w, X_test) + b\n    o = 1. \/ (1 + np.exp(-z))\n    result = (o > 0.5)\n    corrects = np.sum(result == Y_test)\n\n    print(\"Accuracy of our LLSR:\" + str((corrects * 100.) \/ m_test) + \"%\")\n    \n    return corrects\n","2ae46c97":"w, b = train_vectorized(X_train, Y_train)\n_ = test_vectorized(X_test, Y_test, w, b)","b8ef63b3":"def train_batch(X_train, Y_train, lr=0.1, lambdar=0.0001, epochs=50):\n    \n    n = X_train.shape[0]\n    m = X_train.shape[1]\n\n    # Xavier Initialization\n    np.random.seed(1234)\n    w = np.random.randn(1, n) * (np.sqrt(2. \/ (n + 1)))\n    b = 0\n\n    for epoch in range(epochs):\n\n        # Forward pass\n        z = np.dot(w, X_train) + b\n        o = 1. \/ (1 + np.exp(-z))\n        r = np.sum(w**2)    \n\n        # Calculate the loss \n        # (axis here makes the training more general \\\n        # if there are more output neurons than 1, \\\n        # we want to sum over the instances, so axis=1)\n        L = (np.sum((Y_train - o)**2, axis=1) + lambdar * r) \/ (2 * m)\n\n        # Backward pass and update the weights\/bias\n        w -= lr * (np.dot((o - Y_train) * o * (1 - o), X_train.T) + lambdar * w) \/ m\n        b -= lr * (np.sum((o - Y_train) * o * (1 - o))) \/ m\n\n        print(\"Error of the epoch {0}: {1}\".format(epoch + 1, L))\n        \n    return w, b\n        ","9073d9b0":"w_batch, b_batch = train_batch(X_train, Y_train, lr=2, lambdar=0.5, epochs=1001)\n_ = test_vectorized(X_test, Y_test, w_batch, b_batch)","e9d4a997":"w, b = train_vectorized(X_train, Y_train, epochs=1001)\n_ = test_vectorized(X_test, Y_test, w, b)","326f171e":"def train_minibatch(X_train, Y_train, batch_size=256, lr=0.1, lambdar=0.0001, epochs=50):\n    \n    n = X_train.shape[0]\n    \n    # Xavier Initialization\n    np.random.seed(1234)\n    w = np.random.randn(1, n) * (np.sqrt(2. \/ (n + 1)))\n    b = 0\n\n    for epoch in range(epochs):\n        \n        # Split into minibatches into a *list* of sub-arrays\n        # we want to split along the number of instances, so axis = 1\n        X_minibatch = np.array_split(X_train, batch_size, axis = 1)\n        Y_minibatch = np.array_split(Y_train, batch_size, axis = 1) \n        \n        # We shuffle the minibatches of X and Y in the same way\n        nmb = len(X_minibatch) # number of minibatches\n        np.random.seed(5678)\n        shuffled_index = np.random.permutation(range(nmb))\n                                               \n        # Now we can do the training, we cannot vectorize over different minibatches\n        # They are like our \"epochs\"\n        for i in range(nmb):\n            X_current = X_minibatch[shuffled_index[i]]\n            Y_current = Y_minibatch[shuffled_index[i]]\n            m = X_current.shape[1]\n\n            # Forward pass\n            z = np.dot(w, X_current) + b\n            o = 1. \/ (1 + np.exp(-z))\n            r = np.sum(w**2)    \n\n            # Calculate the loss \n            # (axis here makes the training more general \\\n            # if there are more output neurons than 1, \\\n            # we want to sum over the instances, so axis=1)\n            L = (np.sum((Y_current - o)**2, axis=1) + lambdar * r) \/ (2 * m)\n\n            # Backward pass and update the weights\/bias\n            w -= lr * (np.dot((o - Y_current) * o * (1 - o), X_current.T) + lambdar * w) \/ m\n            b -= lr * (np.sum((o - Y_current) * o * (1 - o))) \/ m\n\n            print(\"Error of the iteration {0}: {1}\".format(epoch * nmb + i + 1, L))\n\n    return w, b","c58ccf44":"# Do not run this for more than 100 epochs!!!!!!!!!\nw_minibatch, b_minibatch = train_minibatch(X_train, Y_train, batch_size=512, lr=0.001, lambdar=0.0001, epochs=35)\n_ = test_vectorized(X_test, Y_test, w_minibatch, b_minibatch)","c7b4ecd0":"The guideline is to avoid explicit for-loops. _Hint_: We cannot make the epoch loop disappears, but all other loops can be replaced by vectorization.","dd7b2d08":"First, import numpy and math:","6835b0e8":"# Vectorized Version of Minibatch Gradient Descent\n\nFinally, we can do minibatch training, it is the same as batch training (the formula) but one iteration runs over a subset of the whole dataset at a time, and those subsets (minibatches) are shuffled before training. Note how I did that minibatch splitting and shuffling using our $numpy.random$ ","f4e03e62":"And the vectorized inference (short, clear and fast):","ad453a90":"And the (unvectorized) inference:","32668564":"# Vectorized Version of Batch Gradient Descent\n\nHere is the fully vectorized version, batch training (vectorizing over training instances). The formula (you might be able to derive them after the next lecture):\n\n$$ z = w \\cdot X + b $$\n\n$$ o = \\sigma(z) $$\n\n$$ C = \\frac{1}{2m}\\sum_{j=1}^{m}(y^{(j)}-o^{(j)})^2 $$\n\n$$ R = \\frac{1}{2m}\\sum_{i=1}^{n}w_i^2 $$\n\n$$ L = C + \\lambda R $$\n\n$$ \\frac{\\partial C}{\\partial z^{(j)}} = \\frac{1}{m}(o^{(j)} - Y^{(j)}) * o^{(j)} * (1 - o^{(j)}) $$\n\n$$ \\frac{\\partial z^{(j)}}{\\partial w_i} = x_i $$\n\n$$ \\Rightarrow \\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z} \\cdot X^T $$\n\n$$ \\frac{\\partial R}{\\partial w} = \\frac{1}{m}w $$ \n\n$$ \\Rightarrow \\frac{\\partial L}{\\partial w} = \\frac{\\partial C}{\\partial w} + \\lambda\\frac{\\partial R}{\\partial w} $$\n\n$$ \\frac{\\partial z}{\\partial b} = 1 $$\n\n$$ \\Rightarrow \\frac{\\partial L}{\\partial b} = \\frac{\\partial C}{\\partial b} = \\sum_{j=1}^{m}(o^{(j)} - Y^{(j)}) * o^{(j)} * (1 - o^{(j)}) $$\n\n$$ w = w - \\eta * \\frac{\\partial L}{\\partial w} $$\n\n$$ b = b - \\eta *  \\frac{\\partial L}{\\partial b} $$","028f9a38":"Test on our test data. Please wait for some time. The accuracy should be better than 89.2%. This high score 89.2% is the baseline, achieved by do nothing rather than predicting all images are not a \"seven\" :p. ","73820ecf":"Minibatch Training for this LLSR is very sensitive to hyperparameter choosing. Should use with early stopping. Do not supprise if the accurary is bad. Shuffling the minibatch also takes time, so do not run this with large number of epochs.","56366e91":"\nWe will use LLSR for the MNIST_SEVEN task: predict a $128\\times 128$-pixel image of a handwritten digit whether it is a \"7\" or not. This is a binary classification task. I did the data reading for you. There is 5000 images, I split the first 4000 images for training, 500 images for tuning, 500 images for test. On this exercise we do not need to tune anything, so we'd leave the tuning (called the _dev set_) untouch. The first field is the label (\"0\"-\"9\") of the image, the rest are the grayscale value of each pixel. \n\n\nBefore running the below cell, you should change the `data_path` pointing to the correct location of your dataset csv file.","59bf1085":"# <center> KIT Praktikum NN: L2-regularized Logistic Least Squares Regression <\/center>\n\nOn this exercise, you are going to apply what you learn from the `numpy` tutorial in the implementation of L2-regularized Logistic Least Squares Regression (LLSR). I will provide you the formula by now (you can do it yourself after the next lecture!!!), first you should use pens and papers to vectorize them. Then you implement the full of the classifier based on your vectorized version.\n\n<center><img src=\"https:\/\/github.com\/thanhleha-kit\/PraktikumNeuronaleNetze\/blob\/master\/Images\/LogisticRegression.png?raw=true\" style=\"width:298px;height:275px\"><\/center>\n\n\nL2-regularized Logistic Least Squares Regression is similar to the standard Logistic Regression: It is a binary classifier containing only one layer, mapping the input features to only one output using sigmoid function. The differents here are two things: \n* Instead of the _binary crossentropy error_ for the loss, it uses the _squared error_.\n* It is applied the L2-regularization.\n\nNote that we will do an SGD training for this exercise. More specifically:\n* There are $m$ data instance on the training set, each has $n$ input features. \n* $x_{i}^{(j)}$ denotes the $i^{th}$ input feature of the $j^{th}$ data instance.\n* $y^{(j)}$ denotes the binary label ($0$ or $1$) of the $j^{th}$ data instance.\n* $w_{i}$ denotes the weight connecting the $i^{th}$ input feature to the output.\n* $b$ is the bias of the Logistic Least Squares Regression.\n\nSo the steps of an unvectorized version are:\n* The weights are initialized using Xavier Initialization, the bias can be initialized as 0.\n* Train over 5 epochs, each epoch we do those steps:\n  *  Loop over every data instance $x^{(j)}$:\n     * Calculate the output of the LLSR: $o^{(j)} = \\sigma(\\sum_{i=1}^{n} w_ix_i^{(j)} + b)$\n     * Calculate the cost: squared error $c^{(j)} = (y^{(j)} - o^{(j)})^2$\n     * The final loss function is L2-regularized: $l^{(j)} = \\frac{1}{2}c^{(j)} + \\frac{\\lambda}{2}\\sum_{i=1}^{n}w_i^2$. \n     * Update the weights: \n         * Loop over every weight $w_i$ and update once at a time: $w_i = w_i - \\eta((o^{(j)}-y^{(j)})o^{(j)}(1-o^{(j)})x_i^{(j)} + \\lambda w_i)$\n     * Update the bias: $b = b - \\eta (o^{(j)}-y^{(j)})o^{(j)}(1-o^{(j)})$\n  *  Calculate the total loss (of the epoch): $L = \\frac{1}{m}\\sum_{j=1}^{m}l^{(j)}$. Print it out. \n","c46be9a1":"Those following runs should return exact the same outputs like the (unvectorized) training and inference before but in less than a second. The vectorization should be more effective (much faster) if this is not an one-layer logistic regression but a deep network.","0e686543":"# Unvectorized Version of SGD\n\nFirst the unvectorized version of training:","1cb0f5c6":"Since it is a batch training and requires different hyperparameters, the result might not be comparable to the SGD trainings above. ","738cc305":"One thing to compare: the speed. Try to run the same number of epochs (1000) with SGD, vectorized training, you can see it still takes a long time to run compared to the fully batch training.","61ab49a6":"# Vectorized Version of Stochastic Gradient Descent\n\nNow we move to the vectorized version of training and inference, just replace for-loops and total-sums by $np.dot()$,  $np.sum()$ and the numpy pair-wise operations (you should do the vectorization using pens and papers first)."}}