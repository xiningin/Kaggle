{"cell_type":{"0a9530cc":"code","f6ad3e2b":"code","b22e2cdd":"code","abd5d4f2":"code","ed40a4ab":"code","8d9039c0":"code","ac759b41":"code","97b05013":"code","b6a67c5a":"code","a9bf2e7e":"code","7a357298":"code","02cfecca":"code","b222836c":"code","d9da788f":"code","79429bcf":"code","9da47c09":"code","e0c341c3":"code","ec8d8eff":"code","b2ab9fb3":"code","e9ab6ab3":"markdown","a999c09e":"markdown","ac48e464":"markdown","2b70b0c9":"markdown","9c2abfb0":"markdown","c6c7bb16":"markdown","436a6ad4":"markdown","59637210":"markdown","d26ac476":"markdown"},"source":{"0a9530cc":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, recall_score, f1_score, accuracy_score, precision_score\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.decomposition import PCA, IncrementalPCA, LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\nfrom tqdm.notebook import tqdm, trange\nfrom typing import NoReturn, Union, List\nimport tensorflow as tf\nfrom mlxtend.classifier import EnsembleVoteClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom umap import UMAP","f6ad3e2b":"df = pd.read_csv('C:\/Users\/Dino_P340\/Documents\/KaggleExp\/CreditCardFraud\/creditcard.csv')\ndf.head()","b22e2cdd":"# use a dataset sample for development purposes\ndev = False","abd5d4f2":"df.groupby(['Class']).Class.count().plot(kind='pie', title='Fraudulent VS Genuine Transactions')","ed40a4ab":"df_s = df.sample(2000)\nX = df_s[[_ for _ in df.columns if _ != 'Class']]\n\npca = PCA(n_components=2)\ntsne = TSNE(n_components=2)\nump = UMAP(n_components=2)\nipca = IncrementalPCA(n_components=2)\n\nx_pca = pca.fit_transform(X)\nx_tsne = tsne.fit_transform(X)\nx_umap = ump.fit_transform(X)","8d9039c0":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nsizes = pd.Series(df_s['Class']+1).pow(5) # represent fraud with bigger point\n\naxes[0].scatter(x_pca[:, 0], x_pca[:, 1], s=sizes, c=df_s['Class'].values)\naxes[1].scatter(x_tsne[:, 0], x_tsne[:, 1], s=sizes, c=df_s['Class'].values)\naxes[2].scatter(x_umap[:, 0], x_umap[:, 1], s=sizes, c=df_s['Class'].values)\n\naxes[0].set_title('PCA')\naxes[1].set_title('t-SNE')\naxes[2].set_title('UMAP')\n\nfig.tight_layout()\nplt.show()","ac759b41":"# time and amount scaling\ndf['Time'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))\ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\n\ndf_anom = df[df['Class'] == 1]\ndf_norm = df[df['Class'] == 0]\n\nif dev:\n    df_norm = df_norm.sample(5000, random_state=42)\ndf_test_norm = df_norm.sample(df_anom.shape[0])\ndf_test = pd.concat([\n    df_anom,\n    df_test_norm\n])\ndf_train = df_norm.drop(df_test_norm.index)\n\nfeature_cols = [_ for _ in df.columns if _ != 'Class']","97b05013":"X_train = df_train[feature_cols]\ny_train = df_train['Class'] # will not be used\nX_test = df_test[feature_cols]\ny_test = df_test['Class'] # for evaluation\nprint('''\ntrain: [{:>8} x {:<5}]\n test: [{:>8} x {:<5}]\n'''.format(*X_train.shape, *X_test.shape))","b6a67c5a":"def sensitivity_keras(y_true, y_pred):\n    \"\"\"credits: https:\/\/datascience.stackexchange.com\/a\/40746\/28592\n    \n    param:\n    y_pred - Predicted labels\n    y_true - True labels \n    Returns:\n    Specificity score\n    \"\"\"\n    neg_y_true = 1 - y_true\n    neg_y_pred = 1 - y_pred\n    fp = tf.keras.backend.sum(neg_y_true * y_pred)\n    tn = tf.keras.backend.sum(neg_y_true * neg_y_pred)\n    specificity = tn \/ (tn + fp + tf.keras.backend.epsilon())\n    return specificity","a9bf2e7e":"class Scaled_IsolationForest(IsolationForest):\n    \"\"\"The purpose of this sub-class is to transform prediction values from {-1, 1} to {1,0}\n    \"\"\"\n    def predict(self, X):\n        pred = super().predict(X)\n        scale_func = np.vectorize(lambda x: 1 if x == -1 else 0)\n        return scale_func(pred)\n\nclass Scaled_OneClass_SVM(OneClassSVM):\n    \"\"\"The purpose of this sub-class is to transform prediction values from {-1, 1} to {1,0}\n    \"\"\"\n    def predict(self, X):\n        return np.array([y==-1 for y in super().predict(X)])\n    \nclass NoveltyDetection_Sequential(tf.keras.models.Sequential):\n    \"\"\"This custom `tf.keras.models.Sequential` sub-class transforms autoencoder's output into {1,0}.\n    Output value is determined based on reproduction (decode) loss. If reproduction loss is more than a threashold then, the input sample is considered as anomaly (outlier).\n    Based on few experiments, 1.5 is a dissent threashold (don't as why :P). Future work: determine the threashold using a more sophisticated method.\n    \"\"\"\n    def predict(self, x, *args, **kwargs):\n        pred = super().predict(x, *args, **kwargs)\n        mse = np.mean(np.power(x - pred, 2), axis=1)\n        scale_func = np.vectorize(lambda x: 1 if x > 1.5 else 0)\n        return scale_func(mse)","7a357298":"# define early stop in order to prevent overfitting and useless training\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='mse',\n    patience=10,\n    verbose=1, \n    mode='min',\n    restore_best_weights=True,\n)\n\n# it's a common practice to store the best model\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath='autoenc.hdf5',\n    save_best_only=True,\n    monitor='val_loss',\n    mode='min',\n    verbose=0\n)\n\ndef get_autoencoder() -> tf.keras.models.Sequential:\n    \"\"\"Build an autoencoder\n    \"\"\"\n    model = NoveltyDetection_Sequential([\n        tf.keras.layers.Dense(X_train.shape[1], activation='relu', input_shape=(X_train.shape[1], )),\n        # add some noise to prevent overfitting\n        tf.keras.layers.GaussianNoise(0.05),\n        tf.keras.layers.Dense(2, activation='relu'),\n        tf.keras.layers.Dense(X_train.shape[1], activation='relu')\n    ])\n    model.compile(optimizer='adam', \n                        loss='mse',\n                        metrics=['acc', sensitivity_keras])\n    return model","02cfecca":"clfs = {\n    'isolation_forest': {\n        'label': 'Isolation Forest',\n        'clb': Scaled_IsolationForest,\n        'params': {\n            'contamination': 'auto',\n            'n_estimators': 300\n        },\n        'predictions': None,\n        'model': None\n    },\n    'ocsvm': {\n        'label': 'OneClass SVM',\n        'clb': Scaled_OneClass_SVM,\n        'params': {\n            'kernel': 'rbf',\n            'gamma': 0.3,\n            'nu': 0.01,\n        },\n        'prediction': None,\n        'model': None\n    },\n    'auto-encoder': {\n        'label': 'Autoncoder',\n        'clb': get_autoencoder,\n        'params': {},\n        'fit_params': {\n            'x': X_train, 'y': X_train,\n            'validation_split': 0.2,\n            'callbacks': [early_stop, checkpoint],\n            'epochs': 64,\n            'batch_size': 256,\n            'verbose': 0\n        },\n        'predictions': None,\n        'model': None\n    }\n}","b222836c":"%%time\n\nt = trange(len(clfs))\nfor name in clfs:\n    t.set_description(clfs[name]['label'])\n    clfs[name]['model'] = clfs[name]['clb'](**clfs[name]['params'])\n    if 'fit_params' in clfs[name]:\n        clfs[name]['model'].fit(**clfs[name].get('fit_params', {}))\n    else:\n        clfs[name]['model'].fit(X_train)\n    clfs[name]['predictions'] = clfs[name]['model'].predict(X_test)\n    t.update()\nt.close()","d9da788f":"def print_eval_metrics(y_true, y_pred, name='', header=True):\n    \"\"\"Function for printing purposes\n    \"\"\"\n    if header:\n        print('{:>20}\\t{:>10}\\t{:>10}\\t{:>8}\\t{:>5}'.format('Algorith', 'Accuracy', 'Recall', 'Precision', 'f1'))\n    acc = accuracy_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    prec = precision_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    print('{:>20}\\t{:>1.8f}\\t{:>1.8f}\\t{:>1.6f}\\t{:>1.3f}'.format(\n        name, acc, recall, prec, f1\n    ))","79429bcf":"y_preds = np.column_stack([clfs[_]['predictions'] for _ in clfs])\nenseble_preds = []","9da47c09":"hard_vot = EnsembleVoteClassifier([clfs[_]['model'] for _ in clfs], fit_base_estimators=False)\nhard_vot.fit(X_test, y_test)\nenseble_preds.append((hard_vot.predict(X_test), 'Hard Voting'))","e0c341c3":"wei_hard_vot = EnsembleVoteClassifier([clfs[_]['model'] for _ in clfs], weights=[\n        0.4,\n        0.1,\n        0.8\n    ], fit_base_estimators=False)\nwei_hard_vot.fit(X_test, y_test)\nenseble_preds.append((wei_hard_vot.predict(X_test), 'Weighted Hard Voting'))","ec8d8eff":"rf = RandomForestClassifier()\n\nx_tr_ens, x_ts_ens, y_tr_ens, y_ts_ens = train_test_split(y_preds, y_test, test_size=.5)\nrf.fit(x_tr_ens, y_tr_ens)","b2ab9fb3":"print_header = True\nfor k, v in clfs.items():\n    print_eval_metrics(y_test, v['predictions'], v['label'], print_header)\n    print_header = False\n\nprint('\\n')\n\nfor prds, l in enseble_preds:\n    print_eval_metrics(y_test, prds, l, print_header)\n    print_header = False\n\nprint('\\n')\n    \nprint_eval_metrics(\n    y_ts_ens,\n    rf.predict(x_ts_ens),\n    'Bleding using RF', False\n)","e9ab6ab3":"Enseble","a999c09e":"Evaluation\nIt's crusial to detect fraudulent transactions, therefor a significat evaluation metric could the simplicity. For every trained method and ensebling method the following evaluation metrics will be calculated:\n\nAccuracy\nRecall\nPrecision\nf1 Score","ac48e464":"Hard Voting\nThis is one of the simplest way to combine multiple models in order to generalize better and achive better performance.","2b70b0c9":"Dataset\nV{1-28}: PCA decompossition outcome\nClass label (0: normal, 1: fraudulent)\nTime: Number of seconds elapsed between this transaction and the first transaction in the dataset\nAmount: transaction amount","9c2abfb0":"Preprocessing\nTime and Amount fields should be scaled\n\nDuring the step of pre-processing, the dataset will be splited in two parts:\n\n~283K samples of genuine transactions (Training set)\nAll fraudulent samples and equal number of genuine samples (Test set)\nNovelty detection is concered as a semi-supervised task due to the fact that only the normal samples are used during the phase of training. During the phase of evaluation, a balanced subset of genuine and fraudulent samples will be used.","c6c7bb16":"Weighted Hard Voting\nUsing weighted hard voting you can take advantage of most high-performed models","436a6ad4":"Blending\nWe are going to use the the predicted values as input to another model","59637210":"Future Work\nFind the auto-encoding loss threashold using a more sophisticated way\nTest more models and different configurations\n\nhttps:\/\/www.kaggle.com\/sp1thas\/semi-supervised-fraud-detection\/notebook","d26ac476":"Training\nWe are going to define some wrappers, these classes will work as adapters in order to have an abstract implementation."}}