{"cell_type":{"e8d699ec":"code","d4d9a81b":"code","015c11b9":"code","e920b16d":"code","e9869c41":"code","636ea935":"code","3191d2bc":"code","fdca80c6":"code","8d15626d":"code","d954a743":"code","67723c33":"code","84aceff6":"code","7886f744":"code","6918a803":"code","3d030936":"code","c34c158e":"code","345329ea":"code","508fef36":"code","10aa92d4":"code","0de7ae33":"code","51a7cce8":"code","ba4556f8":"code","b55f600e":"code","5a929175":"code","66a7bc79":"code","87b3a27a":"code","4cb797ce":"code","7d3f177d":"code","3cf2ba7e":"code","f43569f8":"code","0f261582":"code","8142d2ac":"code","1e3c0604":"code","cad2cfe9":"code","3b7211c4":"code","c17ae11c":"code","c17f346c":"code","426532d8":"code","7ccd2742":"code","dfe12d2f":"code","1cd12ff5":"code","cb812094":"code","57e3ab80":"code","3d900ca7":"code","3960e2c9":"code","35e8ff23":"code","54feebf9":"code","2b7c05b4":"code","68786f70":"code","d03ac87c":"code","cd72e8d8":"code","d1f615db":"code","ec25eb49":"markdown"},"source":{"e8d699ec":"!pip install --no-index --find-links \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.0-py3-none-any.whl pytorch-tabnet","d4d9a81b":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn import functional as F","015c11b9":"x_train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\nx_test = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ny_train = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ny_train_non = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","e920b16d":"y_train_non.shape","e9869c41":"trt_cp_index=x_train[x_train['cp_type']=='trt_cp'].index","636ea935":"x_test_checkpoint=x_test.copy()","3191d2bc":"x_test_checkpoint.head()","fdca80c6":"x_train.head()","8d15626d":"x_train=x_train.drop(['sig_id'],1)\nx_test=x_test.drop(['sig_id'],1)\ny_train=y_train.drop(['sig_id'],1)\nx_train.head()","d954a743":"x_train=x_train.drop(['cp_type'],1)\nx_test=x_test.drop(['cp_type'],1)","67723c33":"x_train['cp_time'] = x_train['cp_time'].map({24:0,48:1,72:2})\nx_train.cp_time.unique()\nx_test['cp_time'] = x_test['cp_time'].map({24:0,48:1,72:2})\nx_test.cp_time.unique()","84aceff6":"x_train['cp_dose'] = x_train['cp_dose'].map({\"D1\":0,\"D2\":1})\nx_train.cp_dose.unique()\nx_test['cp_dose'] = x_test['cp_dose'].map({\"D1\":0,\"D2\":1})\nx_test.cp_dose.unique()","7886f744":"x_train","6918a803":"genes=[col for col in x_train.columns if col.startswith(\"g-\")]\ncells=[col for col in x_train.columns if col.startswith(\"c-\")]","3d030936":"a_train=x_train.copy()\na_test=x_test.copy()","c34c158e":"x_train=a_train.copy()\nx_test=a_test.copy()","345329ea":"scaler = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\nscaled_data=scaler.fit_transform(x_train[genes+cells])\nx_train=pd.concat([x_train.iloc[:,:2],pd.DataFrame(scaled_data,columns=genes+cells)],1)\n# scaled_data=scaler.transform(x_test[genes+cells])\n# x_test=pd.concat([x_test.iloc[:,:2],pd.DataFrame(scaled_data,columns=genes+cells)],1)\n\n\ntransformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\nscaled_data=scaler.fit_transform(x_test[genes+cells])\nx_test=pd.concat([x_test.iloc[:,:2],pd.DataFrame(scaled_data,columns=genes+cells)],1)","508fef36":"x_test.iloc[:,2:].sum().sum()","10aa92d4":"# genes = [cols for cols in x_train.columns if cols.startswith('c-')]\n# data=pd.concat([pd.DataFrame(x_train[genes]),pd.DataFrame(x_test[genes])])\n# g_data=PCA(0.95,random_state=42).fit_transform(data)","0de7ae33":"# g_data.sum()","51a7cce8":"data=pd.concat((x_train[genes],x_test[genes]))\ng_data=PCA(n_components=0.95,random_state=44).fit_transform(data)\n\ng_train=g_data[:x_train.shape[0]]\ng_test=g_data[x_train.shape[0]:]\n\ng_train_df=pd.DataFrame(g_train,columns=[f'pca_g-{i}' for i in range(g_data.shape[1])])\ng_test_df=pd.DataFrame(g_test,columns=[f'pca_g-{i}' for i in range(g_data.shape[1])])\n\n\ndata=pd.concat((x_train[cells],x_test[cells]))\nc_data=PCA(n_components=0.95,random_state=44).fit_transform(data)\n\nc_train=c_data[:x_train.shape[0]]\nc_test=c_data[x_train.shape[0]:]\n\nc_train_df=pd.DataFrame(c_train,columns=[f'pca_c-{i}' for i in range(c_data.shape[1])])\nc_test_df=pd.DataFrame(c_test,columns=[f'pca_c-{i}' for i in range(c_data.shape[1])])","ba4556f8":"X_train=pd.concat((x_train,g_train_df,c_train_df),axis=1)\nX_test=pd.concat((x_test,g_test_df,c_test_df),axis=1)","b55f600e":"from sklearn.feature_selection import VarianceThreshold\nvarthre = VarianceThreshold(0.8)\ntrain_varthre=varthre.fit_transform(X_train.iloc[:,2:])\ntest_varthre=varthre.transform(X_test.iloc[:,2:])","5a929175":"X_train=pd.concat([X_train.iloc[:,:2],pd.DataFrame(train_varthre,columns=X_train.columns[2:][varthre.get_support()])],1)\nX_test=pd.concat([X_test.iloc[:,:2],pd.DataFrame(test_varthre,columns=X_test.columns[2:][varthre.get_support()])],1)\n\nX_train.shape,X_test.shape,y_train.shape,submission.shape","66a7bc79":"display(X_train.head(3))\nprint(X_train.shape)\ndisplay(X_test.head(3))\nprint(X_test.shape)","87b3a27a":"genes=[col for col in X_train.columns if col.startswith(\"g-\")]\ncells=[col for col in X_train.columns if col.startswith(\"c-\")]","4cb797ce":"from sklearn.cluster import KMeans\n# kmeans=KMeans(n_clusters=40,random_state=982)\nkmeans=KMeans(n_clusters=35,random_state=982)\nkmeans.fit(pd.concat([X_train[genes],X_test[genes]]))\nX_train[\"cluster_g\"]=kmeans.labels_[:len(X_train)]\nX_test[\"cluster_g\"]=kmeans.labels_[len(X_train):]\n\nfrom sklearn.cluster import KMeans\n# kmeans=KMeans(n_clusters=8,random_state=982)\nkmeans=KMeans(n_clusters=5,random_state=982)\nkmeans.fit(pd.concat([X_train[cells],X_test[cells]]))\nX_train[\"cluster_c\"]=kmeans.labels_[:len(X_train)]\nX_test[\"cluster_c\"]=kmeans.labels_[len(X_train):]\n\nX_train=pd.get_dummies(X_train,columns=['cluster_g','cluster_c'])\nX_test=pd.get_dummies(X_test,columns=['cluster_g','cluster_c'])","7d3f177d":"gsquarecols=['g-574','g-211','g-216','g-0','g-255','g-577','g-153','g-389','g-60','g-370','g-248','g-167','g-203','g-177','g-301','g-332','g-517','g-6','g-744','g-224','g-162','g-3','g-736','g-486','g-283','g-22','g-359','g-361','g-440','g-335','g-106','g-307','g-745','g-146','g-416','g-298','g-666','g-91','g-17','g-549','g-145','g-157','g-768','g-568','g-396']","3cf2ba7e":"for df in X_train,X_test:\n    df['time_dose']=df['cp_time']+df['cp_dose']\n    df['g_median']=df[genes].median(axis=1)\n    df['c_median']=df[cells].median(axis=1)\n    df['gc_median']=df[genes+cells].median(axis=1)\n    df['g_max']=df[genes].max(axis=1)\n    df['c_max']=df[cells].max(axis=1)\n    df['gc_max']=df[genes+cells].max(axis=1)\n    df['g_min']=df[genes].min(axis=1)\n    df['c_min']=df[cells].min(axis=1)\n    df['gc_min']=df[genes+cells].min(axis=1)\n    df['g_sum']=df[genes].sum(axis=1)\n    df['c_sum']=df[cells].sum(axis=1)\n    df['gc_sum']=df[genes+cells].sum(axis=1)\n    df['g_mean']=df[genes].mean(axis=1)\n    df['c_mean']=df[cells].mean(axis=1)\n    df['gc_mean']=df[genes+cells].mean(axis=1)\n    df['g_std']=df[genes].std(axis=1)\n    df['c_std']=df[cells].std(axis=1)\n    df['gc_std']=df[genes+cells].std(axis=1)\n    df['g_skew']=df[genes].skew(axis=1)\n    df['c_skew']=df[cells].skew(axis=1)\n    df['gc_skew']=df[genes+cells].skew(axis=1)\n    df['g_kurtosis']=df[genes].kurtosis(axis=1)\n    df['c_kurtosis']=df[cells].kurtosis(axis=1)\n    df['gc_kurtosis']=df[genes+cells].kurtosis(axis=1)\n    df['g_q25']=df[genes].quantile(0.25,axis=1)\n    df['c_q25']=df[cells].quantile(0.25,axis=1)\n    df['gc_q25']=df[genes+cells].quantile(0.25,axis=1)\n    df['g_q75']=df[genes].quantile(0.75,axis=1)\n    df['c_q75']=df[cells].quantile(0.75,axis=1)\n    df['gc_q75']=df[genes+cells].quantile(0.75,axis=1)\n    \n    \n    df['c52_c42'] = df['c-52'] * df['c-42']\n    df['c13_c73'] = df['c-13'] * df['c-73']\n    df['c26_c13'] = df['c-23'] * df['c-13']\n    df['c33_c6'] = df['c-33'] * df['c-6']\n    df['c11_c55'] = df['c-11'] * df['c-55']\n    df['c38_c63'] = df['c-38'] * df['c-63']\n    df['c38_c94'] = df['c-38'] * df['c-94']\n    df['c13_c94'] = df['c-13'] * df['c-94']\n    df['c4_c52'] = df['c-4'] * df['c-52']\n    df['c4_c42'] = df['c-4'] * df['c-42']\n    df['c13_c38'] = df['c-13'] * df['c-38']\n    df['c55_c2'] = df['c-55'] * df['c-2']\n    df['c55_c4'] = df['c-55'] * df['c-4']\n    df['c4_c13'] = df['c-4'] * df['c-13']\n    df['c82_c42'] = df['c-82'] * df['c-42']\n    df['c66_c42'] = df['c-66'] * df['c-42']\n    df['c6_c38'] = df['c-6'] * df['c-38']\n    df['c2_c13'] = df['c-2'] * df['c-13']\n    df['c62_c42'] = df['c-62'] * df['c-42']\n    df['c90_c55'] = df['c-90'] * df['c-55']\n    \n    for feature in cells:\n             df[f'{feature}_squared'] = df[feature] ** 2     \n                \n    for feature in gsquarecols:\n        df[f'{feature}_squared'] = df[feature] ** 2","f43569f8":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","0f261582":"X_train=X_train.loc[trt_cp_index,:]\ny_train=y_train.loc[trt_cp_index,:]\ny_train_non=y_train_non.loc[trt_cp_index,:]","8142d2ac":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_train_non.shape)","1e3c0604":"# def getmodel(layers,dropouts,activations1,activations2,layer_sizes1,layer_sizes2,lr,wd):\n#     model=tf.keras.Sequential()\n#     model.add(tf.keras.Input(shape=(689,)))\n    \n    \n#     for i in range(layers):\n#         model.add(\n#             tf.keras.layers.BatchNormalization()\n#         )\n#         model.add(\n#             tf.keras.layers.Dropout(dropouts[i])\n#         )\n#         model.add(\n#             tfa.layers.WeightNormalization(tf.keras.layers.Dense(layer_sizes1[i],activation=activations1[i]))\n#         )\n#         model.add(\n#             tf.keras.layers.Dense(layer_sizes2[i],activation=activations2[i])\n#         )\n        \n        \n#     model.add(tf.keras.layers.Dense(206,activation='softmax'))\n    \n# #     model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 756),loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n#     model.compile(optimizer=tfa.optimizers.AdamW(lr = lr, weight_decay = wd, clipvalue = 756),loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n    \n#     return model\n\n\n# def get_model(layers,units,dropouts,activations):\n#     model=tf.keras.Sequential()\n#     model.add(tf.keras.Input(shape=(874,)))\n#     model.add(tf.keras.layers.BatchNormalization())\n        \n#     for i in range(layers):\n#         model.add(tf.keras.layers.Dense(units[i], activation=activations[i]))\n#         model.add(tf.keras.layers.Dropout(dropouts[i]))\n#         model.add(tf.keras.layers.BatchNormalization())\n            \n#     model.add(tf.keras.layers.Dense(206,activation='softmax'))\n    \n#     model.compile(optimizer = tf.keras.optimizers.Adam(1e-3),loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n#     return model\n\ndef create_model(num_cols, hid_layers, activations, dropout_rate, lr, num_cols_y):\n    \n    inp1 = tf.keras.layers.Input(shape = (num_cols, ))\n    x1 = tf.keras.layers.BatchNormalization()(inp1)\n\n    for i, units in enumerate(hid_layers):\n#         x1 = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units, activation=activations[i]))(x1)\n        x1 = tf.keras.layers.Dense(units, activation=activations[i])(x1)\n        x1 = tf.keras.layers.Dropout(dropout_rate[i])(x1)\n        x1 = tf.keras.layers.BatchNormalization()(x1)\n    \n    x1 = tf.keras.layers.Dense(num_cols_y,activation='sigmoid')(x1)\n    model = tf.keras.models.Model(inputs= inp1, outputs= x1)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr),loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.001), metrics=['accuracy'])\n#     model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 756),loss=tf.keras.losses.BinaryCrossentropy(),metrics=['accuracy'])\n    \n    return model","cad2cfe9":"hps={\n    30:[\n    {\n        \"units\":[1410,1246,512,512,512],\n        \"activations\":['selu', 'swish', 'elu','elu','elu'],\n        \"dropouts\":[0.49,0.51,0.0,0.0,0.0]\n    },\n    {\n        \"units\":[1127,1133, 1209, 512],\n        \"activations\":[ 'selu','swish','swish','elu'],\n        \"dropouts\":[0.32,0.02,0.26,0.0]\n    },\n    {\n        \"units\":[796,1995,573],\n        \"activations\":['selu','selu','swish'],\n        \"dropouts\":[0.22,0.21,0.09]\n    },\n    {\n        \"units\":[959,1557,1826],\n        \"activations\":['selu','selu','elu'],\n        \"dropouts\":[0.42,0.61,0.3]\n    },\n    {\n        \"units\":[1081,2033],\n        \"activations\":['elu','elu'],\n        \"dropouts\":[0.44,0.12]\n    }    \n    ],\n    58:[\n        {\n            \"units\":[561,882,2023],\n            \"activations\":['elu','elu','elu'],\n            \"dropouts\":[0.43,0.17,0.29]\n        },\n        {\n            \"units\":[687,1085,1365,1427,1325],\n            \"activations\":['selu','swish','swish','elu','selu'],\n            \"dropouts\":[0.59,0.15,0.31,0.08,0.41]\n        },\n        {\n            \"units\":[1324,1475,976,512],\n            \"activations\":['selu','selu','selu', 'elu'],\n            \"dropouts\":[0.39,0.38,0.19,0.0]\n        },\n        {\n            \"units\":[1410,1246,512,512,512],\n            \"activations\":['selu', 'swish', 'elu', 'elu', 'elu'],\n            \"dropouts\":[0.49,0.51,0.0,0.0,0.0]\n        },\n        {\n            \"units\":[750,1734,638,512],\n            \"activations\":['selu','swish','selu','elu'],\n            \"dropouts\":[0.42,0.5,0.42,0.0]\n        }\n    ],\n    3255:[\n        {\n            \"units\":[1444,629,512,512,512],\n            \"activations\":['selu', 'swish', 'elu', 'elu', 'elu'],\n            \"dropouts\":[0.62,0.4,0.0,0.0,0.0]\n        },\n        {\n            \"units\":[899,940,893,718],\n            \"activations\":['elu', 'elu', 'selu', 'selu'],\n            \"dropouts\":[0.33,0.28,0.29,0.17]\n        },\n        {\n            \"units\":[2030,1422],\n            \"activations\":['elu', 'selu'],\n            \"dropouts\":[0.53,0.37]\n        },\n        {\n            \"units\":[1194,1562],\n            \"activations\":[ 'swish','swish'],\n            \"dropouts\":[0.59,0.0]\n        },\n        {\n            \"units\":[1130,766,520,512],\n            \"activations\":[  'selu','elu','elu', 'elu'],\n            \"dropouts\":[0.29,0.14,0.27,0.0]\n        }\n    ]\n}\n","3b7211c4":"from pickle import load,dump","c17ae11c":"y_train_non=y_train_non.iloc[:,y_train_non.columns!='sig_id']\nfinal_y=pd.concat([y_train,y_train_non],1)\nfinal_pred=pd.DataFrame(np.zeros((submission.shape[0], final_y.shape[1])),columns=final_y.columns)\n\nfor seed in [30,58,3255]:\n    for n,(tr,te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(X_train,y_train)):\n        print(\"SEED:\"+str(seed)+\" Fold:\"+str(n))\n        \n\n        units=hps[seed][n]['units']\n        activations=hps[seed][n]['activations']\n        dropouts=hps[seed][n]['dropouts']\n        \n        model=create_model(X_train.shape[1], units, activations, dropouts, 1e-3, final_y.shape[1])\n        \n\n        model.fit(X_train.values[tr],\n                 final_y.values[tr],\n                 batch_size=128,\n                 epochs=150,\n                 validation_data=(X_train.values[te],final_y.values[te]),\n                 verbose=1,\n                  callbacks=[\n                      tf.keras.callbacks.ReduceLROnPlateau(\n                          monitor='val_loss', \n                          factor=0.1, \n                          patience=3,\n                          epsilon = 1e-4, \n                          mode = 'min',\n                          verbose=1\n                      )\n                      ,\n\n                      tf.keras.callbacks.EarlyStopping(\n                          monitor='val_loss',\n                          min_delta=0,\n                          patience=10,\n                          mode='auto',\n                          verbose=1,\n                          baseline=None,\n                          restore_best_weights=True\n                      )\n                  ]\n                 )\n        model.save(f'pSEED{seed}FOLD{n}.h5')\n        model=create_model(X_train.shape[1], units, activations, dropouts, 1e-3, final_y.shape[1])\n        model.load_weights(f'pSEED{seed}FOLD{n}.h5')\n        final_pred+=model.predict(X_test)","c17f346c":"y_pred=final_pred.loc[:,submission.columns[1:]]\/15","426532d8":"############################### HPS for transfer learning ##############################################\nhps={\n    30:[\n    {\n        \"units\":[512,1187, 2048, 1971],\n        \"activations\":['elu', 'swish', 'selu','swish'],\n        \"dropouts\":[0.28,0.26,0.47,0.6]\n    },\n    {\n        \"units\":[1127,532, 1488, 1219],\n        \"activations\":['swish', 'selu','elu','swish'],\n        \"dropouts\":[0.57,0.0,0.16,0.44]\n    },\n    {\n        \"units\":[1123,640, 512],\n        \"activations\":['elu','swish','elu'],\n        \"dropouts\":[0.14,0.64,0.0]\n    },\n    {\n        \"units\":[1110,1248,588, 971],\n        \"activations\":['selu','selu','elu','selu'],\n        \"dropouts\":[0.29,0.61,0.64,0.41]\n    },\n    {\n        \"units\":[1687,1494,1513],\n        \"activations\":['elu','swish','elu'],\n        \"dropouts\":[0.17,0.02,0.66]\n    }    \n    ],\n    58:[\n        {\n            \"units\":[1395,1309],\n            \"activations\":['selu','elu'],\n            \"dropouts\":[0.51,0.36]\n        },\n        {\n            \"units\":[897,1321],\n            \"activations\":['elu', 'swish'],\n            \"dropouts\":[0.59,0.63]\n        },\n        {\n            \"units\":[1235,841],\n            \"activations\":['selu', 'elu'],\n            \"dropouts\":[0.69,0.05]\n        },\n        {\n            \"units\":[1628,1272,512],\n            \"activations\":['elu', 'swish', 'elu'],\n            \"dropouts\":[0.63,0.44,0.0]\n        },\n        {\n            \"units\":[646,1056,1827,774,1463],\n            \"activations\":['selu','elu','swish','selu','selu'],\n            \"dropouts\":[0.51,0.62,0.47,0.35,0.65]\n        }\n    ],\n    3255:[\n        {\n            \"units\":[1148,1108,512,512],\n            \"activations\":['selu', 'swish', 'elu', 'elu'],\n            \"dropouts\":[0.28,0.69,0.0,0.0]\n        },\n        {\n            \"units\":[1060,778,1430],\n            \"activations\":['elu', 'selu', 'elu'],\n            \"dropouts\":[0.68,0.44,0.66]\n        },\n        {\n            \"units\":[1742,1453,1871,1655],\n            \"activations\":['selu', 'swish', 'swish', 'selu'],\n            \"dropouts\":[0.46,0.57,0.5,0.61]\n        },\n        {\n            \"units\":[1788,1817,512,512,512,512],\n            \"activations\":[ 'swish', 'elu', 'elu','elu','elu','elu'],\n            \"dropouts\":[0.68,0.54,0.0,0.0,0.0,0.0]\n        },\n        {\n            \"units\":[907,764],\n            \"activations\":[  'elu', 'elu'],\n            \"dropouts\":[0.47,0.38]\n        }\n    ]\n}","7ccd2742":"final_pred=submission.iloc[:,1:].copy()\nfinal_pred.loc[:,:]=0","dfe12d2f":"for seed in [30,58,3255]:\n    for n,(tr,te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(X_train,y_train)):\n        print(\"SEED:\"+str(seed)+\" Fold:\"+str(n))\n        \n        units=hps[seed][n]['units']\n        activations=hps[seed][n]['activations']\n        dropouts=hps[seed][n]['dropouts']\n        \n        \n        model2=create_model(X_train.shape[1], units, activations, dropouts, 1e-3,y_train_non.shape[1])\n        \n        model2.fit(X_train.values[tr],\n         y_train_non.values[tr],\n         batch_size=128,\n         epochs=150,\n         validation_data=(X_train.values[te],y_train_non.values[te]),\n         verbose=1,\n          callbacks=[\n              tf.keras.callbacks.ReduceLROnPlateau(\n                  monitor='val_loss', \n                  factor=0.1, \n                  patience=3,\n                  epsilon = 1e-4, \n                  mode = 'min',\n                  verbose=1\n              )\n              ,\n              \n              tf.keras.callbacks.EarlyStopping(\n                  monitor='val_loss',\n                  min_delta=0,\n                  patience=10,\n                  mode='auto',\n                  verbose=1,\n                  baseline=None,\n                  restore_best_weights=True\n              )\n          ]\n         )\n\n        model=create_model(X_train.shape[1], units, activations, dropouts, 1e-3, y_train.shape[1])\n        for i in range(len(model.layers)-1):\n            model.layers[i].set_weights(model2.layers[i].get_weights())\n\n        model.fit(X_train.values[tr],\n                 y_train.values[tr],\n                 batch_size=128,\n                 epochs=150,\n                 validation_data=(X_train.values[te],y_train.values[te]),\n                 verbose=1,\n                  callbacks=[\n                      tf.keras.callbacks.ReduceLROnPlateau(\n                          monitor='val_loss', \n                          factor=0.1, \n                          patience=3,\n                          epsilon = 1e-4, \n                          mode = 'min',\n                          verbose=1\n                      )\n                      ,\n\n                      tf.keras.callbacks.EarlyStopping(\n                          monitor='val_loss',\n                          min_delta=0,\n                          patience=10,\n                          mode='auto',\n                          verbose=1,\n                          baseline=None,\n                          restore_best_weights=True\n                      )\n                  ]\n                 )\n        model.save(f'tSEED{seed}FOLD{n}.h5')\n        model=create_model(X_train.shape[1], units, activations, dropouts, 1e-3, y_train.shape[1])\n        model.load_weights(f'tSEED{seed}FOLD{n}.h5')\n        \n        final_pred+=model.predict(X_test)","1cd12ff5":"y_pred_transfer=final_pred\/15\n# y_pred=y_pred\/2.35","cb812094":"from sklearn.metrics import log_loss\nfrom pytorch_tabnet.metrics import Metric\nfrom sklearn.metrics import roc_auc_score, log_loss\n\nclass LogitsLogLoss(Metric):\n    \"\"\"\n    LogLoss with sigmoid applied\n    \"\"\"\n\n    def __init__(self):\n        self._name = \"logits_ll\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_pred):\n        \"\"\"\n        Compute LogLoss of predictions.\n\n        Parameters\n        ----------\n        y_true: np.ndarray\n            Target matrix or vector\n        y_score: np.ndarray\n            Score matrix or vector\n\n        Returns\n        -------\n            float\n            LogLoss of predictions vs targets.\n        \"\"\"\n        logits = 1 \/ (1 + np.exp(-y_pred))\n        aux = (1-y_true)*np.log(1-logits+1e-15) + y_true*np.log(logits+1e-15)\n        return np.mean(-aux)","57e3ab80":"tabnet_params = dict(\n    n_d = 32,\n    n_a = 32,\n    n_steps = 1,\n    gamma = 1.3,\n    lambda_sparse = 0,\n    optimizer_fn = optim.Adam,\n    optimizer_params = dict(lr = 2e-2, weight_decay = 1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = 456,\n    verbose = 10\n)","3d900ca7":"final_pred=submission.iloc[:,1:].copy()\nfinal_pred.loc[:,:]=0\n\nfor seed in [30,58,3255]:\n    for n,(tr,te) in enumerate(MultilabelStratifiedKFold(n_splits=5, random_state=seed, shuffle=True).split(X_train,y_train)):\n        print(\"SEED:\"+str(seed)+\" Fold:\"+str(n))\n        \n        model = TabNetRegressor(**tabnet_params)\n        \n        model.fit(\n        X_train = X_train.values[tr],\n        y_train = y_train.values[tr],\n        eval_set = [(X_train.values[te],y_train.values[te])],\n        eval_name = [\"val\"],\n        eval_metric = [\"logits_ll\"],\n        max_epochs = 200,\n        patience = 20,\n        batch_size = 512, \n        virtual_batch_size = 32,\n        num_workers = 1,\n        drop_last = False,\n        # To use binary cross entropy because this is not a regression problem\n        loss_fn = F.binary_cross_entropy_with_logits)\n        \n \n        model.save_model(f\"tabSEED{seed}FOLD{n}\")\n        \n        final_pred+=1 \/ (1 + np.exp(-model.predict(X_test.values)))","3960e2c9":"y_pred_tabnet=final_pred\/15","35e8ff23":"y_pred_final=y_pred * 0.25 + y_pred_transfer * 0.25 + y_pred_tabnet * 0.5","54feebf9":"submission.head()","2b7c05b4":"submission.iloc[:,1:]=y_pred_final","68786f70":"submission.loc[x_test_checkpoint[\"cp_type\"]=='ctl_vehicle','5-alpha_reductase_inhibitor':]=0","d03ac87c":"submission.head()","cd72e8d8":"submission.sum(1)","d1f615db":"submission.to_csv('submission.csv',index=False)","ec25eb49":"<h1>TRANSFER LEARNING START<\/h1>"}}