{"cell_type":{"e9647fc9":"code","26c299bf":"code","a498cf6f":"code","86eb80ca":"code","3f0240e1":"code","520aea9d":"code","0d9b0324":"code","4abecf04":"code","6fee9008":"code","2115ff54":"code","afc4c7f6":"code","57890080":"code","e6915436":"code","033d6d3c":"code","3e0c590d":"code","2bc72531":"code","356a6e05":"code","635d357f":"code","6200d1b5":"code","38f4f620":"code","e3111e89":"code","3c630312":"code","8d5903dc":"code","0c4590ec":"code","f35f731f":"code","fdbcf77f":"code","418392ae":"code","caccf801":"code","ea878554":"code","489ff7ef":"markdown","4a2adaa9":"markdown","10004ea8":"markdown","d0b7cfd1":"markdown","ab24ddff":"markdown","c085e2c3":"markdown","3514cf55":"markdown","e7fd157f":"markdown","29a00d2b":"markdown","50a8441d":"markdown","2a0efa16":"markdown","8d58857a":"markdown"},"source":{"e9647fc9":"#Importing required libraries\n\n! pip install dice_ml\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.impute import SimpleImputer","26c299bf":"#Loading dataset\n\ntitanic = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_sub= pd.read_csv('..\/input\/titanic\/test.csv') #Submission dataset\n\nID=data_sub[\"PassengerId\"]\ntitanic.columns = titanic.columns.str.lower() # use all lower case column names\ndata_sub.columns = data_sub.columns.str.lower()\ntitanic.head()","a498cf6f":"titanic.describe()","86eb80ca":"#Removing columns with unique IDs\n\ntitanic.drop([\"passengerid\",\"name\",\"cabin\",\"ticket\"],inplace=True,axis=1) ##Remove unwanted\/Unique columns\ndata_sub.drop([\"passengerid\",\"name\",\"cabin\",\"ticket\"],inplace=True,axis=1)\ntitanic.drop_duplicates(inplace=True) #Remove duplicate values","3f0240e1":"#Functions for data Visualization\n\ndef pie_chart(values,label,t):\n    #print(values)\n    plt.figure(figsize=(6,6))\n    sns.set_style('ticks')\n    sns.set(font_scale = 1)\n    plt.title(t,fontsize=16)\n    plt.pie(values,labels=label,shadow = True,autopct='%1.1f%%',textprops={'fontsize': 14})\n    #plt.legend(loc='best')\n    plt.show()\n\ndef bar_plot(name):\n    plt.figure(figsize=(6,6))\n    sns.set_style('ticks')\n    sns.set(font_scale = 1.3)\n    plt.title(f\"{name} data distribution by decision  \",fontsize=16)\n    sns.histplot(data=titanic, x=name, hue=\"survived\",multiple=\"dodge\", shrink=.8)\n\n    plt.show()    \n    #print()\n    \n    \ndef histplot(name,t):\n    plt.figure(figsize=(6,6))\n    sns.set(font_scale = 1.3)\n    sns.displot(titanic[name], kde=True)\n    plt.title(t,fontsize=16)\n    plt.show()\n    \n    plt.figure(figsize=(6,6))\n    sns.set(font_scale = 1.3)\n    sns.boxplot(data=titanic, x='survived', y=name)\n    plt.title(t,fontsize=16)\n    plt.show()\n    #print()\n    \nnan_val=titanic.isnull().any(axis=1).sum()\ntotal_val=len(titanic)    \npie_chart([nan_val,total_val-nan_val],[\"Missing values\",\"Complete Records\"],\"Data Distribution\")","520aea9d":"#Missing Columns\n\nprint(f\"Missing Values in Training dataset \\n{data_sub.isnull().sum()}\\n\")\nprint(f\"Missing Values in Testing dataset \\n{titanic.isnull().sum()}\")","0d9b0324":"#Imputing\n\n#Imputing age values\nimputer = KNNImputer(n_neighbors=5, weights=\"uniform\")\ntitanic[\"age\"] = imputer.fit_transform(titanic['age'].values.reshape(-1,1))\ndata_sub[\"age\"] = imputer.fit_transform(data_sub['age'].values.reshape(-1,1))\n\n#Imputing fare values in Submission dataset\ndata_sub[\"fare\"] = imputer.fit_transform(data_sub['fare'].values.reshape(-1,1))\n\n#Imputing Embarked values in Submission dataset\nimp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\ntitanic[\"embarked\"] = imp.fit_transform(titanic['embarked'].values.reshape(-1,1))","4abecf04":"#Predicting Class data Distribution\n\nval=titanic[\"survived\"].value_counts() #Converting a nimerical variable into sum of the categorocal variable\nnames=val.index.tolist()\npie_chart(val,names,\"Target data distribution\")","6fee9008":"cat_columns=[\"sex\",\"embarked\",\"pclass\",\"sibsp\",\"parch\"]\nfor name in titanic.columns:\n    if (name in cat_columns ):\n        print(name)\n        val=titanic[name].value_counts() #Converting a nimerical variable into sum of the categorocal variable\n        val=val[0:5] #Up to the 10 labels\n        names=val.index.tolist()\n        pie_chart(val,names,f\"How the data distributed in {name} column\")\n        bar_plot(name)\n    if (name not in cat_columns and name != \"survived\"):\n        print(name)\n        histplot(name,f\"Histogram of {name} data\")      ","2115ff54":"#corelation between data columns\n\ncorr=titanic.corr().round(1)\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nabs(corr['survived']).sort_values()[:-1].plot.barh()\nplt.gca().set_facecolor('#FFFFFF')","afc4c7f6":"#Getting features and targets\n\ndata_features=titanic.iloc[:,1:]\ndata_targets=titanic[\"survived\"]","57890080":"#Cat boost classifier\n\nimport catboost\nprint('catboost version:', catboost.__version__)\nfrom catboost import CatBoostClassifier \n\n#Split data into training and testing\nx_train,x_test,y_train,y_test=train_test_split(data_features,data_targets,test_size=0.2)\n \ncat_columns=[\"sex\",\"embarked\",\"pclass\",\"sibsp\",\"parch\"]\nparams = {'iterations':5000,\n        'learning_rate':0.001,\n        'cat_features':cat_columns,\n        'depth':3,\n        'eval_metric':'AUC',\n        'verbose':200,\n        'od_type':\"Iter\", # overfit detector\n        'od_wait':500, # most recent best iteration to wait before stopping\n        'random_seed': 1\n          }\n\ncat_model = CatBoostClassifier(**params)\ncat_model.fit(x_train, y_train,   \n          eval_set=(x_test, y_test), \n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True);\n","e6915436":"#Evalurtaion\nmatplotlib.rc_file_defaults()\n\npred_cat_model=cat_model.predict(x_test)\nprint(\"Cat Boost Classifier -Training Accuracy score: \"+str(round(accuracy_score(y_train,cat_model.predict(x_train)),4)))\nprint(\"Cat Boost Classifier -Testing Accuracy score: \"+str(round(accuracy_score(y_test,pred_cat_model),4)))\n\nplot_confusion_matrix(cat_model,x_test,y_test)\nprint(classification_report(y_test,pred_cat_model,target_names=[\"Yes\",\"No\"]))","033d6d3c":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\n\n#Split the dataset\ntrain_x,test_x,train_y,test_y=train_test_split(data_features,data_targets,test_size=0.2)\n\n#Getting copy of the datasets \ntrain_scale=train_x.copy()\ntest_scale=test_x.copy()\ndata_cat=data_sub.copy()\n\n#Normalization of Numerical Columns\nscaler.fit(train_scale[[\"age\",\"fare\"]])\n\ntrain_scale[[\"age\",\"fare\"]]=scaler.transform(train_scale[[\"age\",\"fare\"]])\ntest_scale[[\"age\",\"fare\"]] =scaler.transform(test_scale[[\"age\",\"fare\"]])\ndata_sub[[\"age\",\"fare\"]] =scaler.transform(data_sub[[\"age\",\"fare\"]])","3e0c590d":"#Label Encording\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\ncat_columns=[\"sex\",\"embarked\",\"pclass\",\"sibsp\"]\n\nfor col_name in cat_columns:\n    labelencoder.fit(train_scale[col_name])\n    \n    train_scale[col_name]=labelencoder.transform(train_scale[col_name])\n    test_scale[col_name]=labelencoder.transform(test_scale[col_name])\n    data_sub[col_name]=labelencoder.transform(data_sub[col_name])","2bc72531":"cat_columns=[\"sex\",\"embarked\",\"pclass\",\"sibsp\",\"parch\"]\nparams = {'iterations':5000,\n        'learning_rate':0.001,\n        'cat_features':cat_columns,\n        'depth':3,\n        'eval_metric':'AUC',\n        'verbose':200,\n        'od_type':\"Iter\", # overfit detector\n        'od_wait':500, # most recent best iteration to wait before stopping\n        'random_seed': 1\n          }\n\ncat_model_2 = CatBoostClassifier(**params)\ncat_model_2.fit(train_scale, train_y,   \n          eval_set=(test_scale, test_y), \n          use_best_model=True, # True if we don't want to save trees created after iteration with the best validation score\n          plot=True);","356a6e05":"pred_cat_model=cat_model_2.predict(test_scale)\nprint(\"Cat Boost Classifier -Training Accuracy score: \"+str(round(accuracy_score(train_y,cat_model_2.predict(train_scale)),4)))\nprint(\"Cat Boost Classifier -Testing Accuracy score: \"+str(round(accuracy_score(test_y,pred_cat_model),4)))\n\nplot_confusion_matrix(cat_model_2,test_scale,test_y)\nprint(classification_report(test_y,pred_cat_model,target_names=[\"Yes\",\"No\"]))","635d357f":"import xgboost as xgb\nxgb_param={\n    \"max_depth\":3,\n    \"eta\":0.01,\n    \"silent\":0,\n    \"eval_metric\":\"auc\",\n    \"subsample\":0.8,\n    \"colsample_bytree\":0.8,\n    \"objective\":\"binary:logistic\",\n    \"seed\":0\n    }\n\ndTrain=xgb.DMatrix(train_scale,train_y,enable_categorical=True)\ndTest=xgb.DMatrix(test_scale,test_y,enable_categorical=True)\n\nEvals=[(dTrain,\"train\"),(dTest,\"dTest\")]\nmodel_xgb=xgb.train(params=xgb_param, dtrain=dTrain, num_boost_round=2000,\n              evals=Evals, maximize=True, \n              verbose_eval=50)","6200d1b5":"predictions_xgb_test=model_xgb.predict(dTest)\np_test=[0 if i<= 0.5 else 1 for i in predictions_xgb_test]\n\npredictions_xgb_train=model_xgb.predict(dTrain)\np_train=[0 if i<= 0.5 else 1 for i in predictions_xgb_train]\n\nprint(\"XGB-Training Accuracy score: \"+str(round(accuracy_score(train_y,p_train),4)))\nprint(\"XGB-Testing Accuracy score: \"+str(round(accuracy_score(test_y,p_test),4)))\n\n#plot_confusion_matrix(model_xgb,test_x,test_y)\nprint(classification_report(test_y,p_test,target_names=[\"Yes\",\"No\"]))","38f4f620":"#Feature importance according to XGBOOST Classifier\n\nxgb.plot_importance(model_xgb)\nplt.rcParams['figure.figsize'] = [5, 5]\nplt.show()","e3111e89":"#Shap Visualizations for catbooster model 1 (Model without Preprocessing)\n\nimport shap\nfrom catboost import Pool\nshap_values = cat_model.get_feature_importance(Pool(x_test, label=y_test,cat_features=cat_columns),\n                                               type=\"ShapValues\")\nexpected_value = shap_values[0,-1]\nshap_values = shap_values[:,:-1]\n\nindex=2\nshap.initjs()\nshap.force_plot(expected_value, shap_values[index,:], x_test.iloc[index,:])\n\n#Red- Not Survive\n#Blue- Survived","3c630312":"#Summary Plot\nshap.summary_plot(shap_values,x_test)","8d5903dc":"#How the data is distributed\n\nexplainer = shap.TreeExplainer(cat_model)\nshap_values = explainer.shap_values(x_train)\nshap.force_plot(explainer.expected_value, shap_values,x_train)","0c4590ec":"import dice_ml\n\ncontinous_col=[\"age\",\"fare\"]\ndice_data = dice_ml.Data(dataframe=titanic,continuous_features=continous_col,outcome_name='survived')\ndice_model= dice_ml.Model(model=cat_model, backend=\"sklearn\")\nexplainer = dice_ml.Dice(dice_data,dice_model, method=\"random\")\n\ne1 = explainer.generate_counterfactuals(x_test[1:2], total_CFs=2, desired_class=\"opposite\")\ne1.visualize_as_dataframe(show_only_changes=True)","f35f731f":"#%%Example 2\ne2 = explainer.generate_counterfactuals(x_test[4:5],\n                                  total_CFs=3,\n                                  desired_class=\"opposite\",\n                                  features_to_vary=[\"age\", \"sex\", \"pclass\"]\n                                  )\ne2.visualize_as_dataframe(show_only_changes=True)","fdbcf77f":"def make_file(ID,Val,name):\n    df=pd.DataFrame(list(zip(ID,Val)),columns=[\"PassengerId\",\"Survived\"])\n    df.to_csv(name,index=False)","418392ae":"##Getting predictions for the submission file (XGBOOST)\n\ndSubmit=xgb.DMatrix(data_sub,enable_categorical=True)\npredictions_xgb_submit=model_xgb.predict(dSubmit)\np_train=[0 if i<= 0.5 else 1 for i in predictions_xgb_submit]\nmake_file(list(ID),p_train,\"xgb.csv\")","caccf801":"##Getting predictions for the submission file (CATBOOST_2)\n\npredictions_cat2_submit=cat_model_2.predict(data_sub)\nmake_file(list(ID),list(predictions_cat2_submit),\"Cat2.csv\")","ea878554":"##Getting predictions for the submission file (CATBOOST_1)\n\npredictions_cat1_submit=cat_model.predict(data_cat)\nmake_file(list(ID),list(predictions_cat1_submit),\"Cat1.csv\")","489ff7ef":"# **Titanic Data** \n## 90+ Accuracy with Gradient Boosting Alorithms\n### With Explanations from Shap and Counterfactuals from LiME\n\n<img src =\"https:\/\/www.thoughtco.com\/thmb\/Heru39HLQLHqscAgjCVS4mL3NDI=\/1500x1027\/filters:fill(auto,1)\/GettyImages-517357578-5c4a27edc9e77c0001ccf77d.jpg\">","4a2adaa9":"## Data Visualization","10004ea8":"## Setting Up","d0b7cfd1":"### Preprocessing before training the model again","ab24ddff":"### (2) Catboost classifier (After scaling and Label encoding)","c085e2c3":"**Performance does not change significantly after Preprocessing**","3514cf55":"### (1) Catboost classifier (Without scaling and Label encoding)","e7fd157f":"## Data Preperation","29a00d2b":"## Counter factuals","50a8441d":"## Explaining ML models","2a0efa16":"## Training ","8d58857a":"### (3) XGBOOST classifier "}}