{"cell_type":{"289c679d":"code","b547a7e3":"code","90355900":"code","2aa177ea":"code","dd09a6c8":"code","462f310b":"code","d4a8bf51":"code","318941cb":"code","a109ad91":"code","45265134":"code","b4e35e72":"code","cd0acbbf":"code","bcfa4fce":"code","8b6704e1":"code","51f4ad0b":"code","259e4158":"code","80d3a2af":"code","b0e93d08":"code","ded0c6cd":"code","3928796d":"code","f9b3244a":"code","3961e3bd":"code","4ce020fa":"code","1f13a7f7":"code","e1f2d3db":"code","4e9076f3":"code","02c52f4f":"code","21de560a":"code","5ce8c17d":"code","725626bc":"code","8f443d0f":"code","48cf4695":"code","f4f7bc51":"code","ba109ac8":"code","dfe8ef31":"code","59b9687c":"code","4b390c9b":"code","28795dc8":"markdown","fa60c0e0":"markdown","b0d953d1":"markdown","56b3de53":"markdown","697aea37":"markdown","92547694":"markdown","9957bae5":"markdown","b0a1e4fa":"markdown","2e2aa55a":"markdown","6b48b14b":"markdown","ea63dee5":"markdown","a4db8a05":"markdown","2475ab28":"markdown","10fafd3c":"markdown","f3d84ee7":"markdown"},"source":{"289c679d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\npalette = plt.get_cmap('Set2')","b547a7e3":"net_df = pd.read_csv('..\/input\/netflix-shows\/netflix_titles.csv')\nnet_df.head()","90355900":"net_df.shape","2aa177ea":"net_df.info()","dd09a6c8":"# Missing Values\nmsno.bar(net_df, figsize=(20,10))\nplt.show()","462f310b":"# Amount of content by country\ncountries = net_df['country'].value_counts()[net_df['country'].value_counts(normalize=True)> 0.005]\nlist_countries = list(countries.index)\nplt.figure(figsize=(20,10))\nplt.title('Amount of content by country', fontsize=18)\nplt.tick_params(labelsize=14)\nsns.barplot(y=countries.index, x=countries.values, alpha=0.6)\nplt.show()","d4a8bf51":"# Movies and TV Shows count\nmovie_tvshows = net_df['type'].value_counts()\nplt.figure(figsize=(14, 7))\nplt.bar(movie_tvshows.index, movie_tvshows.values, alpha=0.8)\nplt.title('Amount of movies and TV shows', fontsize=18)\nplt.show()","318941cb":"TVshows = net_df[net_df['type'] == 'TV Show']\nMovie = net_df[net_df['type'] == 'Movie']\nTVshows_progress = TVshows['release_year'].value_counts().sort_index()\nMovie_progress = Movie['release_year'].value_counts().sort_index()\nplt.figure(figsize=(14, 7))\n\nplt.plot(TVshows_progress.index, TVshows_progress.values, label='TV shows')\nplt.plot(Movie_progress.index, Movie_progress.values, label='Movie')\n\nplt.axvline(2019, alpha=0.3, linestyle='--', color='r')\nplt.axvline(2021, alpha=0.3, linestyle='--', color='r')\nplt.axvspan(2019, 2021, alpha=0.2, color='r', label='Coronavirus')\n\nplt.xticks(list(range(1925, 2026, 5)), fontsize=12)\nplt.title('Content growth throughout history', fontsize=18)\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Amount of content', fontsize=14)\nplt.yticks(fontsize=12)\nplt.legend()\nplt.show()","a109ad91":"net_df.dropna(inplace=True)\nrating = net_df['rating'].value_counts()\nplt.figure(figsize=(14,7))\nplt.title('Content ratings of the shows', fontsize=18)\nplt.tick_params(labelsize=14)\nsns.barplot(y=rating.index, x=rating.values, alpha=0.6)\nplt.show()","45265134":"grp = net_df.groupby('type')\nmovie = grp.get_group('Movie')\nmovie['duration'] = [int(i.split(' ')[0]) for i in movie.duration.dropna()]\nplt.figure(figsize=(14, 7))\nsns.distplot(movie['duration'], bins=60).set(ylabel=None)\nplt.title('Length distribution of films', fontsize=18)\nplt.xlabel('Duration', fontsize=14)\nplt.show()","b4e35e72":"short = movie.sort_values('duration')[['title', 'duration']].iloc[:20]\nplt.figure(figsize=(14,7))\nplt.title('Top 20 shortest movies available on Netflix', fontsize=18)\nplt.tick_params(labelsize=14)\nsns.barplot(y=short['title'], x=short['duration'], alpha=0.6)\nplt.show()","cd0acbbf":"long = movie.sort_values('duration')[['title', 'duration']].iloc[-20:]\nplt.figure(figsize=(14,7))\nplt.title('Top 20 longest movies available on Netflix', fontsize=18)\nplt.tick_params(labelsize=14)\nsns.barplot(y=long['title'], x=long['duration'], alpha=0.6)\nplt.show()","bcfa4fce":"directors = net_df['director'].value_counts()\nplt.figure(figsize=(14,7))\nplt.title('Top 10 directors available on Netflix', fontsize=18)\nplt.tick_params(labelsize=14)\nsns.barplot(y=directors.index[:10], x=directors.values[:10], alpha=0.6)\nplt.show()","8b6704e1":"labels = np.hstack([np.array(i.split(', ')) for i in net_df.listed_in.dropna()])\nunique = np.unique(labels)\ndef generate_label(x):\n    genres = x.split(', ')\n    label = np.zeros(shape=unique.shape)\n    for i in genres:\n        for j in range(len(unique)):\n            if unique[j]==i:\n                label[j]=1\n    return label.astype(int)","51f4ad0b":"from tqdm.notebook import tqdm\ndata = pd.DataFrame()\ndata['text'] = net_df.dropna()['description']\ndata['title'] = net_df.dropna()['title']\ndata['label'] = [generate_label(x) for x in tqdm(net_df.dropna()['listed_in'])]\ndata[['text', 'label']].head()","259e4158":"# Let's begin by checking the word length of show descriptions so that \n# we can trim and pad the texts to the same length for easy processing in the \n#future steps.\ndef get_wordlen(x): \n    return len(x.split())\n\ndata['len'] = data.text.apply(get_wordlen)\ndata['len'].plot(kind='hist')\nplt.title('histogram of show description word length')\nplt.xlabel('word length')\nplt.show()\nfor i in np.arange(0.9,1,0.01):\n    p = data.len.quantile(i)\n    print(f'word length at {int(i*100)} percentile:',p)","80d3a2af":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data[['text', 'title']], data['label'], test_size=0.3, random_state=33)\nX_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=33)\ny_train.shape, y_val.shape, y_test.shape","b0e93d08":"!pip install bert-tensorflow","ded0c6cd":"# importing necessary libraries\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom transformers import BertTokenizer, TFBertModel\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Input\ntf.get_logger().setLevel('ERROR')","3928796d":"#Creating a transformer model from pretrained BERT_EN_UNCASED\ntf.keras.backend.clear_session()\nmax_seq_length = 31\ninput_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\" )\ninput_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=False, name='BERT')\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\nbert_model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=sequence_output)\nbert_model.summary(), bert_model.output, bert_model.input","f9b3244a":"tf.keras.utils.plot_model(bert_model, show_shapes=False, show_dtype=False,\n                          show_layer_names=True, rankdir='TB', \n                          expand_nested=False, dpi=96)","3961e3bd":"from bert import tokenization\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() \ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","4ce020fa":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ndef text_to_tokens(x):\n    t = np.asarray(tokenizer.tokenize(x))\n    if len(t)>max_seq_length-2:\n        t = t[:max_seq_length-2]\n    padding = np.asarray(['[PAD]']*(max_seq_length-t.shape[0]-2))\n    pre, post = np.asarray(['[CLS]']), np.asarray(['[SEP]'])\n    final = np.concatenate((pre,t,post,padding))\n    ids = np.asarray(tokenizer.convert_tokens_to_ids(final))\n    mask = (ids != 0)*1\n    segment = np.zeros_like(ids)\n    return ids, mask, segment","1f13a7f7":"from tqdm.notebook import tqdm\n# initializing lists to collect the generated tokens, masks and segments. \nX_train_tokens, X_val_tokens, X_test_tokens = [], [], []\nX_train_mask, X_val_mask, X_test_mask = [], [], []\nX_train_segment, X_val_segment, X_test_segment = [], [], []\n# Generating and storing tokens, masks, segments values for X_train texts\nfor i,x in tqdm(enumerate(X_train.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_train_tokens.append(t) \n    X_train_mask.append(m) \n    X_train_segment.append(s)\n    \n# Generating and storing tokens, masks, segments values for X_val texts\nfor i,x in tqdm(enumerate(X_val.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_val_tokens.append(t) \n    X_val_mask.append(m) \n    X_val_segment.append(s)\n    \n# Generating and storing tokens, masks, segments values for X_test texts\nfor i,x in tqdm(enumerate(X_test.text.values)): \n    t,m,s = text_to_tokens(x) \n    X_test_tokens.append(t) \n    X_test_mask.append(m) \n    X_test_segment.append(s)","e1f2d3db":"### Obtaining the numerical embeddings of text from BERT\n# converting the tokens lists to array type\nX_train_tokens = np.row_stack(X_train_tokens) \nX_val_tokens = np.row_stack(X_val_tokens) \nX_test_tokens = np.row_stack(X_test_tokens)\n# converting the masks lists to array type\nX_train_mask = np.row_stack(X_train_mask) \nX_val_mask = np.row_stack(X_val_mask) \nX_test_mask = np.row_stack(X_test_mask)\n# converting the segment lists to array type\nX_train_segment = np.row_stack(X_train_segment) \nX_val_segment = np.row_stack(X_val_segment) \nX_test_segment = np.row_stack(X_test_segment)","4e9076f3":"X_train_pooled_output = bert_model.predict([X_train_tokens, X_train_mask, X_train_segment])\nX_val_pooled_output = bert_model.predict([X_val_tokens, X_val_mask, X_val_segment])\nX_test_pooled_output = bert_model.predict([X_test_tokens, X_test_mask, X_test_segment])\nX_train_pooled_output.shape","02c52f4f":"tf.keras.backend.clear_session()\n\ninput_layer = Input((None, 768))\ngpa = GlobalAveragePooling1D()(input_layer)\n# x = Dense(units=64, activation='elu')(gpa)\n# x = Dense(units=64, activation='elu')(x)\noutput_layer = Dense(units=42, activation='sigmoid')(gpa)\n\nmlp = Model(input_layer, output_layer)\nmlp.summary()","21de560a":"tf.keras.utils.plot_model(mlp, show_shapes=False, show_dtype=False,\n                          show_layer_names=True, rankdir='TB', \n                          expand_nested=False, dpi=96)","5ce8c17d":"# Performance Metric : Accuracy Score\nfrom sklearn.metrics import accuracy_score\ndef get_accuracy(y, y_pred):\n    acc = []\n    for i,j in zip(y, y_pred):\n        acc.append(accuracy_score(i,j))\n    return np.mean(acc)\n\ndef accuracy(y, y_pred):\n    return tf.py_function(get_accuracy, (y, tf.cast((y_pred>0.5), tf.float32)), tf.double)","725626bc":"from tensorflow.keras import optimizers\nmetrics = [accuracy]\nmlp.compile(optimizer=optimizers.Adam(0.0001), loss='binary_crossentropy', metrics=metrics)","8f443d0f":"y_train_output = np.vstack(y_train.values)\ny_test_output = np.vstack(y_test.values)\ny_val_output = np.vstack(y_val.values)","48cf4695":"history = mlp.fit(X_train_pooled_output, y_train_output, epochs=40, \n                  validation_data=(X_val_pooled_output, y_val_output))","f4f7bc51":"# Performance Check\ndf_metric = pd.DataFrame()\ndf_metric['epoch'] = np.arange(len(history.history['loss']))\ndf_metric['loss'] = history.history['loss']\ndf_metric['val_loss'] = history.history['val_loss']\ndf_metric['accuracy'] = history.history['accuracy']\ndf_metric['val_accuracy'] = history.history['val_accuracy']","ba109ac8":"# Train-Val accuracy check\nfig = px.line(df_metric, x=\"epoch\", y=[\"accuracy\", 'val_accuracy'])\nfig.show()","dfe8ef31":"# Binary cross entropy loss\nfig = px.line(df_metric, x=\"epoch\", y=[\"loss\", 'val_loss'])\nfig.show()","59b9687c":"# Below are some samples from the test dataset which has not been seen by our \n# model till now. Let's see how the model performs on these samples.\nfrom sklearn.metrics import accuracy_score\ny_pred = (mlp.predict(X_test_pooled_output)>0.5)*1\nacc = [accuracy_score(i,j) for i,j in zip(y_pred, y_test)]\nidx = np.argsort(acc)[::-1]\ndef show(i):\n    print(f'movie: {X_test.title.values[i]}')\n    print(f'description: {X_test.text.values[i]}')\n    y_act_idx = unique[np.where(y_test_output[i]==1)]\n    y_pred_idx = unique[np.where(y_pred[i]==1)]\n    print(f'metric score: {acc[i]}')\n    print(f'actual genre: {y_act_idx}')\n    print(f'predicted genre: {y_pred_idx}')\n    print('\\n', '*'*50, '\\n')\n    \nfor i in idx[:10]:\n    show(i)","4b390c9b":"# histogram of accuracy scores achieved by the model on the test\/unseen dataset.\nfig = px.histogram(acc, nbins=20, labels={'value':'Accuracy score'})\nfig.show()","28795dc8":"#### Approach:\nWe'll be using transfer learning to get embeddings from pretrained BERT for each show's description. \nNext, we'll be training a MLP from the generated embeddings to predict the show genre.\n","fa60c0e0":"![493f5bba-81a4-11e9-bf79-066b49664af6_cm_1440w.png](attachment:a9e00be2-9bb6-424e-95ef-3b0ba6e5cd39.png)","b0d953d1":"<b>Netflix is a subscription-based streaming service<\/b> that allows our members to watch TV shows and movies without commercials on an internet-connected device. You can also download TV shows and movies to your iOS, Android, or Windows 10 device and watch without an internet connection.\n<br>\n<br>\nIn this notebook, first we will do some <b>Exploratory Data Analysis (EDA)<\/b> to know and describe the data in a better way through interactive graphs and visualizations. Then we will build a multilabel classifier using <b>BERT and MLP<\/b> to predict the Genre of a show using its description.","56b3de53":"<h2><center>Exploratory Data Analysis<\/center><\/h2>","697aea37":"### Using tokenization to convert the description text into a format understood by BERT\nWe'll be creating 3 type of inputs from a given test:\n\n* token_ids: The token embeddings are numerical representations of words in the input sentence.\n* token_masks: The mask tokens that help BERT to understand what all input words are relevant and what all are just there for padding.\n* token_segments: The segment embeddings are used to help BERT distinguish between the different sentences in a single input.","92547694":"#### Data\nThe input data here will be show description and the output labels will be the 42 Genres.\n<br>\n**Input data:** This will be the text describing a show\n<br>\n**Output labels:** This will be a vector of size 42 (since there are 42 genres) having values 0 or 1. 0 corresponding to a genre not present and 1 corresponding a genre present","9957bae5":"<h1><center>Netflix Movies and TV Shows<\/center><\/h1>\n<h3><center>Movies and TV Shows listings on Netflix<\/center><\/h3>","b0a1e4fa":"##### The content amount grew before and during the coronavirus period.","2e2aa55a":"#### Data Description\n##### The dataset has 7787 rows and 12 columns:\n* show_id: unique id of each show (not much of a use for us in this notebook)\n* type: The category of a show, can be either a Movie or a TV Show\n* title: Name of the show\n* director: Name of the director(s) of the show\n* cast: Name of actors and other cast of the show\n* country: Name of countries the show is available to watch on Netflix\n* date_added: Date when the show was added on Netflix\n* release_year: Release year of the show\n* rating: Show rating on netflix\n* duration: Time duration of the show\n* listed_in: Genre of the show\n* description: Some text describing the show\n\n##### Missing Values are found only in columns \"director\", \"cast\", \"country\"","6b48b14b":"##### The largest amount of content is provided by America, followed by India and UK.","ea63dee5":"<h2><center>Goal\/Aim<\/center><\/h2>","a4db8a05":"### Thank You for reading my Notebook. Please UPVOTE if you like it.\n### Suggest improvements in the comments.","2475ab28":"<h2><center>Model:BERT+MLP<\/center><\/h2>","10fafd3c":"#### Creating a MLP model which can take the BERT embeddings as input and generate predictions.\nSince the embeddings are of a dimension (31, 768), we will forst initiale an Input layer of the same dimension, next we'll add a GlobalAveragePooling1D layer to extract 768 embeddings from the input (average for each of the 31 rows) and finally we'll be initializing a Dense layer as the output layer with 42 units and sigmoid as the activation function.\nSigmoid is used because each of the 42 outputs could take a value between 0 and 1.\n\nFor training the model, we'll use binary_crossentropy as the loss function and Adaptive Momentum (ADAM) as the optimizer.","f3d84ee7":"##### Inference\nThe largest count of shows are made with the 'TV-MA' rating (2863 shows) \"TV-MA\": For mature audiences only.\n\nSecond largest is the 'TV-14' rating (1931 shows) \"TV-14\": May be inappropriate for children younger than 14 years of age.\n\nThird largest is the 'TV-PG' rating (806 shows) \"TV-PG\": Parental guidance suggested\n\nFourth largest is the very popular 'R' rating (665 shows) \"R\": May be unsuitable for children under the age of 17 (Under 17 requires accompanying parent or adult guardian\")"}}