{"cell_type":{"dfd42992":"code","5d5e7d59":"code","e5964bb2":"code","81c0c608":"code","fa9aa2e6":"code","f81b7b74":"code","c118d025":"code","0280deaf":"code","065210e7":"code","88eb4e30":"code","1c57ed73":"code","d94cd392":"code","bcf7690c":"code","d76ff7bd":"code","2bf74853":"code","876d6654":"code","9eafe823":"code","a9d33963":"code","715a4be5":"code","f919206f":"code","43a61303":"code","d0a4cc64":"code","e700fda3":"code","c70b6d47":"code","90e5357f":"code","1250c214":"code","361a8a3e":"code","f5072d81":"markdown","127b64ca":"markdown","bd08939c":"markdown","120d121b":"markdown","17baa2b6":"markdown","6d51f34d":"markdown","94f88732":"markdown","aff834ab":"markdown","6387c639":"markdown","16f40d10":"markdown","9442c9f5":"markdown","a790f7bd":"markdown","e67b8d77":"markdown","53d166be":"markdown","8412c3fe":"markdown","54c2bb08":"markdown","6483f6e5":"markdown","e7f8b25a":"markdown"},"source":{"dfd42992":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","5d5e7d59":"# print(os.listdir('..\/input\/'))\n# print(os.listdir('.\/'))\n\ntraining_data_set = pd.read_csv('..\/input\/train.csv')\ntesting_data_set = pd.read_csv('..\/input\/test.csv')\n\nprint(\"Training data set size is {} samples.\".format(len(training_data_set)))\nprint(\"Test data set size is {} samples.\".format(len(testing_data_set)))","e5964bb2":"training_data_set.head()","81c0c608":"interesting_datapoints = pd.DataFrame(training_data_set[training_data_set.budget == training_data_set.budget.max()])\ninteresting_datapoints = interesting_datapoints.append(training_data_set[training_data_set.budget == training_data_set.budget.min()][0:1])\ninteresting_datapoints = interesting_datapoints.append(training_data_set[training_data_set.revenue == training_data_set.revenue.max()])\ninteresting_datapoints = interesting_datapoints.append(training_data_set[training_data_set.revenue == training_data_set.revenue.min()][0:1])\ninteresting_datapoints = interesting_datapoints.append(training_data_set[training_data_set.revenue \/ training_data_set.budget == max(training_data_set.revenue \/ [budget_row if budget_row > 1000 else -1 for budget_row in training_data_set.budget])])\n\ninteresting_datapoints","fa9aa2e6":"subplot, plots = plt.subplots(3, 2)\nsubplot.set_figheight(50)\nsubplot.set_figwidth(44)\n\nplots[0][0].plot(training_data_set.revenue.sort_values().reset_index().revenue)\nplots[0][0].set_title('Revenue', fontsize=24)\n\nplots[0][1].plot(np.sqrt(np.sqrt(training_data_set.revenue.sort_values().reset_index().revenue)))\nplots[0][1].set_title('Revenue sqrt of sqrt', fontsize=24)\n\nplots[1][0].plot(training_data_set.budget.sort_values().reset_index().budget)\nplots[1][0].set_title('Budget', fontsize=24)\n\nplots[1][1].plot(np.log(training_data_set.budget.sort_values().reset_index().budget))\nplots[1][1].set_title('Budget Log', fontsize=24)\n\nplots[2][0].plot(training_data_set.popularity.sort_values().reset_index().popularity)\nplots[2][0].set_title('Popularity', fontsize=24)\n\nplots[2][1].plot(np.sqrt(np.sqrt(training_data_set.popularity.sort_values().reset_index().popularity)))\nplots[2][1].set_title('Popularity sqrt of sqrt', fontsize=24)\n\nplt.show()","f81b7b74":"print(\"Numerical columns: {}\\n\".format(training_data_set.select_dtypes(include=[np.number]).columns.tolist()))\nprint(\"Which columns have missing data: \\n{}\".format(training_data_set.isna().any()))","c118d025":"training_data_set['original_language'].unique()","0280deaf":"def get_release_month(data):\n    release_months = [\n        # Through out the whole data set (train + test) there's only one record\n        # without release_date, so hardcoding the most common value doesn't seem\n        # that outrageous \n        int(date.split('\/')[0]) if date is not None else 12 \n        for date in data['release_date']\n    ]\n    return release_months\n\ndef append_release_month_col(data, release_month):\n    modified_data = data.copy()\n    modified_data['release_month'] = release_month\n    return modified_data\n\nprint(\"Trainig set rows without date: {}\".format(training_data_set['release_date'].isna().any().sum()))\nprint(\"Testing set rows without date: {}\".format(testing_data_set['release_date'].isna().any().sum()))\n\nrelease_months = get_release_month(training_data_set)\n\nplt.hist(release_months)\nplt.show()\n\n","065210e7":"def get_dircetor(data):\n    import ast\n    director_list = []\n    \n    if data.columns.contains('crew'):\n        crew_data = data['crew'].copy()\n    else:\n        crew_data = data.copy()\n        \n    for crew_info in crew_data:\n        if type(crew_info) == type(''):\n            directors = [\n                [employee['id'], employee['name']]\n                 for employee in ast.literal_eval(crew_info) \n                 if employee['job'] == 'Director']\n            \n            if not directors:\n                director_list.append([[-1, 'None']])\n            else:\n                director_list.append(directors)\n                \n        else:\n            director_list.append([[-1, 'None']])\n        \n    return director_list \n\ndef append_director_col(data, director_list):\n    modified_data = data.copy()\n    modified_data['director'] = director_list\n    return modified_data\n\n\nunique_directors_train = pd.Series((director[0][0] for director in get_dircetor(training_data_set))).unique().size\nunique_directors_test = pd.Series((director[0][0] for director in get_dircetor(testing_data_set))).unique().size\n\nprint(\"Movies in training set: {}, unique directors: {}. One director on average directs {:.2f} movies.\"\n      .format(len(training_data_set), unique_directors_train, len(training_data_set) \/ unique_directors_train))\nprint(\"Movies in testing set: {}, unique directors: {}. One director on average directs {:.2f} movies.\"\n      .format(len(testing_data_set), unique_directors_test, len(testing_data_set) \/ unique_directors_test))","88eb4e30":"from sklearn.model_selection import train_test_split\n\ndef separate_only_numeric(data):\n    \"\"\" Returns DataFrame with only numeric columns\n    \n    Arguments:\n    data - DataFrame that will be used as source\n    \"\"\"\n    return data.select_dtypes(include=[np.number])\n\ndef exclude_na_rows(data):\n    \"\"\" Returns DataFrame where rows with na values have been removed\n    \n    Arguments:\n    data - DataFrame that will be used as source\n    \"\"\"\n    return data.dropna(axis='index')\n\ndef extract_label(data, columns=None, separate_label=True, label_name='revenue'):\n    \"\"\" Separates the label from the data if specified, and selects only specified columns\n    \n    Arguments:\n    data - array of data to be processed\n    columns - columns to be left after preprocessing\n    separate_label - whether to separate the label from the data and return it as a second parameter\n    label_name - the name of the label column\n    \n    Returns: data_X, data_y\n    \"\"\"\n    data_copy = data.copy()\n    if separate_label: \n        data_y = data_copy.pop(label_name)\n    else:\n        data_y = [0 for _ in range(len(data_copy))]\n    \n    if columns is None:\n        columns = data_copy.columns.difference(['id']).tolist()\n    data_X = data_copy[columns]\n    return data_X, data_y\n\ndef transform_dataset(data, test_size=0.3, only_numeric=True, delete_nan=True, separate_label=True):\n    \"\"\" Preprocesses the data\n    \n    Arguments:\n    data - array of data to be processed\n    train_size - percentage (0 to 1) of data to be used as training data\n    only_numeric - whether to use only numeric data\n    training_data - whether this data has labels\n    \n    Returns: train_X, test_X, train_y, test_y\n    \"\"\"\n    if only_numeric: \n        data = separate_only_numeric(data)\n    if delete_nan:\n        data = exclude_na_rows(data)\n    data_X, data_y  = extract_label(data, separate_label=separate_label)\n    return train_test_split(data_X, data_y, test_size=test_size, random_state=1)","1c57ed73":"from sklearn.base import TransformerMixin, BaseEstimator\n\nclass NumericDataSelector(TransformerMixin, BaseEstimator):\n        \n    def fit(self, data, labels=None):\n        return self\n        \n    def transform(self, data, labels=None):\n        return separate_only_numeric(data)\n    \nclass FeatureSelector(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, feature_names=None, remove_feature_names=None):\n        self._feature_names = feature_names \n        self._remove_feature_names = remove_feature_names \n        \n    def fit(self, data, labels=None):\n        return self\n        \n    def transform(self, data, labels=None):\n        if self._feature_names is None:\n            self._feature_names = data.columns.tolist()\n        \n        if self._remove_feature_names is not None:\n            self._feature_names = [feat for feat in self._feature_names if feat not in self._remove_feature_names]\n        \n        return data[self._feature_names]\n    \nclass RootScaler(TransformerMixin, BaseEstimator):\n    \n    def __init__(self, scale_features=None, root_pow=4):\n        self._scale_features = scale_features\n        self._root_pow = root_pow\n        \n    def fit(self, data, labels=None):\n        return self\n    \n    def transform(self, data, labels=None):\n        if self._scale_features is None:\n            self._scale_features = data.columns.tolist()\n        \n        scaled_data = data[self._scale_features]\n        scaled_labels = labels\n        \n        for _ in range(0, self._root_pow, 2):\n            scaled_data = np.sqrt(scaled_data)\n            if scaled_labels is not None:\n                scaled_labels = np.sqrt(scaled_labels)\n        \n        transformed_data = data.copy()\n        transformed_data[self._scale_features] = scaled_data\n        \n        return transformed_data if scaled_labels is None else (transformed_data, scaled_labels)\n   \nclass SimpleImputerWrapper(TransformerMixin):\n\n    def __init__(self):\n        from sklearn.impute import SimpleImputer\n        self._imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n    \n    def fit(self, data, labels=None):\n        self._imputer.fit(data)\n        return self\n    \n    def transform(self, data, labels=None):\n        import pandas as pd\n        imputed_data = self._imputer.transform(data)\n        return pd.DataFrame(imputed_data, columns= data.columns)\n        \nclass PrintData(TransformerMixin, BaseEstimator):\n    \n    def fit(self, data, labels=None):\n        return self\n    \n    def transform(self, data, labels=None):\n        print('Data length: {}, column count: {}'.format(len(data), len(data.columns)))\n        print('Data:')\n        print(data)\n        \n        if labels is not None:\n            print(\"Labels:\")\n            print(labels)\n            \n        return data if labels is None else (data, labels)\n        \nclass DateFeatureAdder(TransformerMixin, BaseEstimator):\n    \n    def fit(self, data, labels=None):\n        return self\n    \n    def transform(self, data, labels=None):\n        from sklearn.preprocessing import LabelBinarizer\n        \n        data_copy = data.copy()\n        date_feature = get_release_month(data_copy)\n        binarizer = LabelBinarizer()\n        date_feature_binary = pd.DataFrame(binarizer.fit_transform(date_feature), columns=binarizer.classes_)\n        \n        data_copy = data_copy.reset_index()\n        joint_columns = data_copy.columns.append(date_feature_binary.columns)\n        data_copy = pd.concat([data_copy, date_feature_binary], axis=1, ignore_index=True)\n        data_copy.columns = joint_columns\n        data_copy.set_index('index')\n        return data_copy","d94cd392":"data, labels = extract_label(training_data_set)\ndata_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.3, random_state=1)\n\nprint(\"Training data set samples: {}, testing: {}\".format(len(data_train), len(data_test)))\nprint(\"Full data set samples: {}.\".format(len(data)))","bcf7690c":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\ndef model_name(model):\n    \"\"\" From a given model type extracts its name.\n    \n    Arguments:\n    model -- the model object whose name is needed\n    \"\"\"\n    model_name = str(type(model)).split('.')[-1][:-2]\n    if model_name == 'Pipeline':\n        model_name = model.steps[-1][0]\n    return model_name\n\ndef model_rmsle(model, data, labels):\n    \"\"\" Evaluates regression model accuracy using RMSLE error function.\n    \n    Arguments:\n    model -- model to evaluate\n    X -- data set to evaluate on\n    y -- labels for evaluation\n    \"\"\"\n    return np.sqrt(metrics.mean_squared_log_error(labels, np.abs(model.predict(data))))\n\ndef model_cv(model, data, labels, scoring='r2'):\n    \"\"\" Calculates models cross-validationn performance with a given scoring method\n    \n    Arguments:\n    model - model, which performance will be checked\n    X - features\n    y - labels\n    scoring - scoring method (default r2)\n    \"\"\"\n    return np.average(cross_val_score(model, data, labels, cv=5, scoring=scoring))\n\ndef evaluate_models(models_list, data, labels, cross_val=False):\n    \"\"\" Evaluates model performance for all models in the list on a given data set (either a test set, or by cross val). \n    Returns a dictionary in such format {model_name: accuracy_score}\n    \n    Arguments:\n    models_list -- list of sklearn models to evaluate\n    data_X -- the data for making predictions\n    data_y -- labels\n    cross_val -- indicator whether to use cross validation\n    \"\"\"\n    evaluations = {}\n    for i, model in enumerate(models_list):\n        evaluations.update({model_name(model) + str(i) : model_cv(model, data, labels) if cross_val else model_rmsle(model, data, labels)})\n    \n    return evaluations\n\n","d76ff7bd":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import FunctionTransformer\n\nextract_numeric_rows = ('Extract numeric rows', NumericDataSelector())\nremove_id = ('Remove id column', FeatureSelector(remove_feature_names=['id']))\nimpute = ('Impute data', SimpleImputerWrapper())\nscale_by_pow_4 = ('Scale data by root of power 4', RootScaler(scale_features=['budget']))\nnormalize = ('Normalize data', Normalizer())\nprint_data = ('Print data', PrintData())\nmonth_feature_adder = ('Add binary release month', DateFeatureAdder())\n\nnumeric_only_steps = [\n    ('Extract numeric rows', NumericDataSelector()),\n    ('Remove id column', FeatureSelector(remove_feature_names=['id'])),\n    ('Impute data', SimpleImputer(missing_values=np.nan, strategy='median'))\n]\n\nnumeric_scaled_normalized_steps = [\n    ('Extract numeric rows', NumericDataSelector()),\n    ('Remove id column', FeatureSelector(remove_feature_names=['id'])),\n    ('Impute data', SimpleImputerWrapper()),\n    ('Scale data by root of power 4', RootScaler(scale_features=['budget'])),\n    ('Normalize data', Normalizer(norm='max'))\n]\n\nnumeric_and_month = [\n    month_feature_adder,\n    extract_numeric_rows,\n    remove_id,\n    impute,\n    scale_by_pow_4,\n    normalize\n]","2bf74853":"from sklearn.dummy import DummyRegressor\n\ndummy_regressor = DummyRegressor(strategy='median')\ndummy_steps = numeric_only_steps.copy()\ndummy_steps.append(('DummyRegressor', dummy_regressor))\n\ndummy_pipeline = Pipeline(steps=dummy_steps)\ndummy_pipeline.fit(data_train.copy(), labels_train.copy())\nmodels = [dummy_pipeline]\n\n","876d6654":"from sklearn.ensemble import RandomForestRegressor\n\nrandom_forest = RandomForestRegressor()\nrandom_forest_steps = numeric_only_steps.copy()\nrandom_forest_steps.append(('Random forest regressor', random_forest))\n\nrandom_forest_pipe = Pipeline(steps=random_forest_steps)\nrandom_forest_pipe.fit(data_train.copy(), labels_train.copy())\nmodels.append(random_forest_pipe)\n \n","9eafe823":"random_forest_scaled_steps = numeric_scaled_normalized_steps.copy()\nrandom_forest_scaled_steps.append(('Random forest scaled & normalized', random_forest))\n\nrandom_forest_scaled_pipe = Pipeline(steps=random_forest_scaled_steps)\nrandom_forest_scaled_pipe.fit(data_train.copy(), labels_train.copy())\nmodels.append(random_forest_scaled_pipe)\n\n","a9d33963":"from skopt import BayesSearchCV\n\nrandom_forest_hyper_search = BayesSearchCV(\n    RandomForestRegressor(), \n    {'n_estimators': (10, 100),\n     'min_samples_split': (2, 20)},\n    n_iter=32)\n\nrandom_forest_hyper_steps = numeric_scaled_normalized_steps.copy()\nrandom_forest_hyper_steps .append(('Random forest hyper tuning', random_forest_hyper_search))\n\nrandom_forest_hyper_pipe = Pipeline(steps=random_forest_hyper_steps)\n# random_forest_hyper_pipe.fit(data_train.copy(), labels_train.copy())\n# models.append(random_forest_hyper_pipe)\n\n","715a4be5":"random_forest_month_steps = numeric_scaled_normalized_steps.copy()\nrandom_forest_month_steps.append(('Random forest hyper with month', random_forest_hyper_search))\n\nrandom_forest_month_pipe = Pipeline(steps=random_forest_month_steps)\n# random_forest_month_pipe.fit(data_train.copy(), labels_train.copy())\n# models.append(random_forest_month_pipe)\n\n","f919206f":"from sklearn.svm import SVR\n\nsvr = SVR()\n\nsupport_vector_reg_steps = numeric_only_steps.copy()\nsupport_vector_reg_steps.append(('SupportVectorRegressor', svr))\n                       \nsupport_vector_reg_pipe = Pipeline(support_vector_reg_steps)\nsupport_vector_reg_pipe.fit(data_train.copy(), labels_train.copy())\nmodels.append(support_vector_reg_pipe)\n\n","43a61303":"svr_normalized_steps = numeric_scaled_normalized_steps.copy()\nsvr_normalized_steps.append(('SVR normalized', svr))\n\nsvr_normalized_pipe = Pipeline(svr_normalized_steps)\nsvr_normalized_pipe.fit(data_train.copy(), labels_train.copy())\nmodels.append(svr_normalized_pipe)\n\n","d0a4cc64":"from skopt import BayesSearchCV\n\nsvr_hyper_model = BayesSearchCV(\n    SVR(),\n    {\n        'C': (1e-6, 1e+6, 'log-uniform'),  \n        'gamma': (1e-6, 1e+1, 'log-uniform'),\n        'degree': (1, 8),\n        'kernel': ['linear', 'poly', 'rbf'],\n    },\n    n_iter=32\n)\n\nsvr_hyper_steps = numeric_scaled_normalized_steps.copy()\nsvr_hyper_steps.append(('SVR hyper tuning', svr_hyper_model))\n\nsvr_hyper_pipe = Pipeline(svr_hyper_steps)\n# svr_hyper_pipe.fit(data_train.copy(), labels_train.copy())\n# models.append(svr_hyper_pipe)\n\n","e700fda3":"svr_hyper_month_steps = numeric_and_month.copy()\nsvr_hyper_month_steps.append(('SVR hyper & month', svr_hyper_model))\n\nsvr_hyper_month_pipe = Pipeline(svr_hyper_month_steps)\n# svr_hyper_month_pipe.fit(data_train.copy(), labels_train.copy())\n# models.append(svr_hyper_month_pipe)\n","c70b6d47":"from xgboost import XGBRegressor\n\nxgbregressor = XGBRegressor()\n\nxgboost_steps = numeric_only_steps.copy()\nxgboost_steps.append(('XGBRegressor', xgbregressor))\n\nxgboost_pipe = Pipeline(xgboost_steps)\nxgboost_pipe.fit(data_train, labels_train)\nmodels.append(xgboost_pipe)\n\n","90e5357f":"xgboost_month_steps = numeric_and_month.copy()\nxgboost_month_steps.append(('XGBRegressor', xgbregressor))\n\nxgboost_month_pipe = Pipeline(xgboost_month_steps)\n# xgboost_month_pipe.fit(data_train, labels_train)\n# models.append(xgboost_month_pipe)\n\n","1250c214":"evaluations = evaluate_models(models, data_test, labels_test, cross_val=False)\n\nfor model, score in evaluations.items():\n    print(\"{} : {}\".format(model, score))\n    \n    ","361a8a3e":"final_model = models[np.argmin(list(evaluations.values()))]\nprint(\"Final chosen model is: {}\".format(model_name(final_model)))\nfinal_model.fit(data, labels)\n\npredictions = final_model.predict(testing_data_set)\npd.DataFrame(predictions, index=testing_data_set.id, columns=['revenue']).to_csv('.\/submission.csv')\n\n","f5072d81":"## 1.3 Visualising distributions","127b64ca":"## 1.1 Checking out the columns and types of data","bd08939c":"# Fin\nScore result history:\n1. 3.74493 (487th) - RandomForrestRegressor with only numeric data\n2. 3.19264 (468th) - RandomForrestRegressor with only numeric data and removed id column\n3. 2.77804 (467th) - XGBoost with only numeric and scaled\n","120d121b":"# 1. Exploring the data","17baa2b6":"#3 Support vector regressor\n\n","6d51f34d":"## 2.2 Defining pipeline classes\n\n","94f88732":"# 3. Model building\n## 3.1. Setting up evaluation methods\n\n","aff834ab":"## 2.3 Creating training and testing data sets\n\n","6387c639":"#5 XGBoost regressor\n\n","16f40d10":"# 2. Data preparation\nTODO:\n1. Implement pipeline system\n      Feature selection -> Imputing -> Normalization(scaling)\n2. Add feature scaling (possibly with logarithmic scaling)\n3. Add additional features\n\n## 2.1 Defining methods\n\n","9442c9f5":"# 4. Evaluation\n\n","a790f7bd":"## 1.2 Checking some interesting facts\n1. Movie with the biggest budget\n2. One of the movies with the smallest budgets\n3. Movie with the highest revenue\n4. One of the movies with the smallest revenue\n5. The movie with the highest ratio of revenue\/budget (not considering movies with budgets below a 1000)","e67b8d77":"## 1.4 More technical checks\n1. Which columns have numerical data\n2. Which columns contain missing values","53d166be":"## 3.2 Creating the pipelines\n\n","8412c3fe":"## 1.5 Categorical data analysis \n","54c2bb08":"## 3.3 Creating & fitting the models\n#1 The dummy regressor to be used as a sanity check for other model performance\n\n","6483f6e5":"# 5. Final predictions\n## 5.1 Training the final model with full training data set and exporting predictions\nCode bellow automatically uses the model with the highest score from models list\n\n","e7f8b25a":"#2 RandomForrestRegressor with only numeric data will be used as a baseline for other model performance\n\n"}}