{"cell_type":{"7f475f31":"code","f28a3623":"code","f3912bd2":"code","d4e7183b":"code","d0ebea95":"code","e3471181":"code","2f6a75ac":"code","5dfb2dae":"code","93cb58a8":"code","a5e7dd45":"code","5d84466e":"code","477f8d12":"code","b6ff996c":"code","ffe516e5":"code","1e340c6e":"code","6f69cd8c":"code","97a47d4f":"code","07b2acac":"code","a4fb00b7":"code","d7aa446a":"code","f89f3294":"code","a79b4538":"code","69d19eab":"code","6742af76":"code","37f3c009":"code","f50f916b":"code","1dcd9ac7":"code","85817f3c":"code","a8f40370":"markdown","0a11aae1":"markdown","16197549":"markdown","dce066ec":"markdown","a0126f31":"markdown","4b4f459f":"markdown","ebc11487":"markdown","88801483":"markdown","b56c61b2":"markdown","6dea6921":"markdown","469b63e1":"markdown","86ec87fc":"markdown"},"source":{"7f475f31":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","f28a3623":"# df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/nunnarilabs\/ml\/master\/heart.csv\")\n\n# intialise data of lists. \ndata = {'popcorn':[1,1,0,1,0,0], \n        'age':[12,87,44,19,32,14],\n       'color':[0,1,0,2,1,0],\n       'op':[1,1,0,0,1,1]} \n  \n# Create DataFrame \ndf = pd.DataFrame(data) ","f3912bd2":"df.head()","d4e7183b":"df.describe()","d0ebea95":"df.op.value_counts()","e3471181":"# initial prediction\nlog_of_odds = np.log(df.op.value_counts()[1]\/ df.op.value_counts()[0])\nprint(log_of_odds)","2f6a75ac":"probability = np.exp(log_of_odds)\/(1+np.exp(log_of_odds))\nprint(probability)","5dfb2dae":"y_hat = np.array([probability]*6)\nprint(y_hat)","93cb58a8":"df['initial_pred'] = y_hat","a5e7dd45":"df","5d84466e":"y = df.op\nresiduals = y - y_hat\nresiduals","477f8d12":"X = df.drop('op', axis=1)","b6ff996c":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nregressor = DecisionTreeRegressor(max_depth=2) \nregressor.fit(X, residuals)","ffe516e5":"text_representation = tree.export_text(regressor)\nprint(text_representation)","1e340c6e":"fig = plt.figure(figsize=(5,4))\n_ = tree.plot_tree(regressor, feature_names=df.columns, filled=True)","6f69cd8c":"reg_pred_tree1 = regressor.predict(X)\/(y_hat * (1 - y_hat))\nreg_pred_tree1","97a47d4f":"# Explanation for previous expression\nregressor.predict(X)","07b2acac":"# Explanation for previous expression\n(y_hat * (1 - y_hat))","a4fb00b7":"# usually learning rate will be 0.1\nlr = 0.8\nlog_of_odds_prediction = y_hat + 0.8 * reg_pred_tree1\nlog_of_odds_prediction","d7aa446a":"def prob(pred):\n    return np.exp(pred)\/ (1+ np.exp(pred))","f89f3294":"prob(1.8)","a79b4538":"vectorized_func = np.vectorize(prob)","69d19eab":"after_tree1 = vectorized_func(log_of_odds_prediction)\nafter_tree1","6742af76":"df['pred_prob'] = after_tree1","37f3c009":"df","f50f916b":"def gradient_boost_classification(X,y,M, lr):\n    log_of_odds = np.log(y.value_counts()[1]\/ y.value_counts()[0])\n    probability = np.exp(log_of_odds)\/(1+np.exp(log_of_odds))\n    y_hat = np.array([probability]*len(y))\n    \n    for i in range(M):\n        # calculate residuals\n        residuals = y - y_hat\n        # fit the tree for residuals\n        regressor = DecisionTreeRegressor(max_depth=2) \n        regressor.fit(X, residuals)\n        # Transformation\n        reg_pred = regressor.predict(X)\/(y_hat * (1 - y_hat))\n        # Add the result to previous tree op\n        log_of_odds_prediction = y_hat + lr * reg_pred\n        new_prob = vectorized_func(log_of_odds_prediction)\n        y_hat = new_prob\n        \n    return y_hat\n\n\n    ","1dcd9ac7":"gradient_boost_classification(X, df.op, 10, 0.8)","85817f3c":"gradient_boost_classification(X, df.op, 8, 0.8)","a8f40370":"This notebook is inspired from this video, all the data and steps followed in the turorial can be found here.\n\nhttps:\/\/www.youtube.com\/watch?v=jxuNLH5dXCs&ab_channel=StatQuestwithJoshStarmer","0a11aae1":"## Initial Prediction","16197549":"### Convert to Probability","dce066ec":"## Calculate Pseudo-Residuals\n\n![](https:\/\/blog.paperspace.com\/content\/images\/2019\/11\/image-9.png)","a0126f31":"Since the Probability of Loving Troll 2 is greater than 0.5, we can Classify everyone in the Training Dataset as someone who Loves Troll 2. \n\n(0.5 is a common threshold used for classification decisions made based on probability; note that the threshold can easily be taken as something else.)\n\n","4b4f459f":"## Build a tree to predict residuals\n\nWe will use this residual to get the next tree. It may seem absurd that we are considering the residual instead of the actual value, but you will se how it is done.","ebc11487":"The easiest way to use the log(odds) for classification is to convert it to a probability. To do so, we'll use this formula:\n\n![](https:\/\/blog.paperspace.com\/content\/images\/2019\/11\/image-8.png)","88801483":"When we use Gradient Boost for Classification, the most common transformation is the following formula.\n\n![](https:\/\/blog.paperspace.com\/content\/images\/2019\/11\/image-10.png)","b56c61b2":"*log(odds)*\n\n*log(yes\/no)*","6dea6921":"When we used Gradient Boost for Regression, a leaf with single Residual had an Output Value equal to that Residual.\n\nIn contrast, when we use Gradient Boost for Classification, the situation is a little more complex. This is because the predictions are in terms of **log(odds)**\n\n\n\n","469b63e1":"There is an improvement in the prediction.","86ec87fc":"Now that we have transformed it, we can add our initial lead with our new tree with a learning rate.\n\n![](https:\/\/blog.paperspace.com\/content\/images\/2019\/11\/image-14.png)"}}