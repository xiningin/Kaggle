{"cell_type":{"ea66e952":"code","b68aa9e4":"code","c061e82f":"code","63f37a30":"code","4a43167b":"code","786b8017":"code","a7b3d529":"code","92774c92":"code","f4d5576e":"code","dca8d0e5":"code","5a69ee13":"code","9f50fd80":"code","8584e13d":"code","e8d746a5":"code","374d580b":"code","232e4902":"code","cafd970e":"code","ce682917":"code","fca3aa5f":"code","c1afed1e":"code","0fc8cf8c":"code","b6e9ca72":"code","13ea1042":"code","80074c9b":"code","d0df15be":"code","95988983":"code","1ff8c235":"code","7fdf82fa":"code","01af7aa1":"code","a9d80a33":"code","739c7277":"code","d2e0976e":"code","1d5d60ee":"code","06546605":"code","baf8670b":"code","bfe42f46":"code","d1e03bac":"code","9b6ad304":"code","134fa903":"code","53639c1b":"code","25117429":"code","6c6b11dd":"code","0a33e340":"code","44545c87":"code","8663f428":"code","1a5af8cf":"code","14926bf8":"code","c38bb9cf":"code","337e4e09":"code","079ab7bc":"code","b863d89e":"code","5c32f170":"code","b33caa63":"code","560ac808":"code","abbceb38":"code","25195356":"code","19b0f9f9":"code","d786c695":"code","af89f877":"code","d62cbdc1":"code","3686abff":"code","8c6787c9":"code","c065eee7":"code","64333f6d":"code","07701340":"code","da450367":"code","710280a7":"code","2260528a":"code","c1f2fe45":"code","2627b913":"code","41c19a7d":"code","37753cc5":"code","a1065b4d":"code","8a38e9ac":"code","fd463be2":"code","63277346":"code","71795303":"code","25b994ec":"code","d9c970bb":"code","572a854f":"code","08b55c3a":"code","7b0e2784":"code","66b86d13":"code","14652820":"code","93971c81":"code","9a58f776":"code","10d11ba6":"code","9a047d10":"code","d3f349c2":"code","276abcd0":"code","a20843a1":"code","23369916":"code","b231e0bf":"code","64a8fef4":"code","c0497ee6":"code","3c50d7d9":"code","b9763a54":"code","5dfd7d03":"code","f57dba0a":"code","db433569":"code","24cca396":"code","b9adec3f":"code","cbf4cbbb":"code","fe740370":"code","3ccfe306":"code","6f15b5c0":"code","f3beaffd":"code","bbda77a3":"code","f5a00f5d":"code","b5737168":"code","1041b6a6":"code","b74306a9":"code","9dc005dd":"code","38c48fa7":"code","5fcd384e":"code","bfea0e32":"code","99d5356f":"code","dc5cfc99":"code","407ab721":"code","75de8b03":"code","e7c63c95":"code","3d91504e":"code","c0e4276f":"code","89ee4e40":"code","bf849084":"code","f1f1ef57":"markdown","2fcc0dd2":"markdown","956c4f9c":"markdown","fbaea0ed":"markdown","c4adcdfa":"markdown","3a264316":"markdown","853e8a13":"markdown","b88699bc":"markdown","883a579e":"markdown","7b1d7ce8":"markdown","0125aa09":"markdown","7a6b7283":"markdown","337bef20":"markdown","f6af5419":"markdown","5ce28d5a":"markdown","a761fb06":"markdown","d7d9db0f":"markdown","f5a7b160":"markdown","7053ccc1":"markdown","4ec0b318":"markdown","f91ca0e9":"markdown","b805733d":"markdown","6f565016":"markdown","8edff2e6":"markdown","5fccbe3b":"markdown","da915fc5":"markdown","63e21bb6":"markdown","d2cbfc6b":"markdown","0dc15a7a":"markdown","3ab250fb":"markdown","7e7a0eea":"markdown","60f768bd":"markdown","7bb0e38d":"markdown","bfc96859":"markdown","3f50fb32":"markdown","7a76d400":"markdown","e8929131":"markdown","df1a811e":"markdown"},"source":{"ea66e952":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b68aa9e4":"import functools\nfrom IPython.core.display import display, HTML\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', 1000)\n#Metadata_df=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv')\nMetadata_df = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","c061e82f":"print('shape' ,Metadata_df.shape)","63f37a30":"print('info',Metadata_df.info())","4a43167b":"\nprint('Top 10',Metadata_df.head())","786b8017":"print('Last 10',Metadata_df.tail())","a7b3d529":"missing_data=(Metadata_df.isnull().sum(axis=0)\/Metadata_df.shape[0])*100\nmissing_data","92774c92":"#I will Drop the Columns WHO #Covidence and Microsoft Academic Paper ID , the they have more than 96 % of Missing Data\nMetadata_df=Metadata_df.drop(['who_covidence_id', 'arxiv_id'], axis=1)","f4d5576e":"Metadata_df.nunique()","dca8d0e5":"Metadata_df=Metadata_df.dropna(subset =['sha'])","5a69ee13":"Metadata_df.shape","9f50fd80":"Metadata_df.info()","8584e13d":"Metadata_df.nunique()","e8d746a5":"Metadata_df=Metadata_df.reset_index(drop=True)","374d580b":"Metadata_df.tail()","232e4902":"#textprops={'color':\"b\"}\nimport matplotlib.pyplot as plt\nMetadata_df['journal'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10),autopct = '%.2f%%',\n                                                                           title = 'Top Ten Active Journals')\nplt.title(\"Top Ten Active Journals\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top ten Journals\"+\".png\", bbox_inches='tight')","cafd970e":"#Top 10 Authors\nMetadata_df['authors'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Ten Active authors')\nplt.title(\"Top Ten authors\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten authors\"+\".png\", bbox_inches='tight')","ce682917":"#Top 10 licens\nMetadata_df['license'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Ten license')\nplt.title(\"Top Ten license\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten license\"+\".png\", bbox_inches='tight')","fca3aa5f":"#Top 10 Time\nMetadata_df['publish_time'].value_counts().iloc[[0,1,2,3,4,5,6,7,8,9]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Ten publish time')\nplt.title(\"Top Ten publish time\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top Ten publish time\"+\".png\", bbox_inches='tight')","c1afed1e":"#Top 10 Time\nMetadata_df['source_x'].value_counts().iloc[[0,1,2,3]].plot.pie(figsize = (10,10), autopct = '%.2f%%',\n                                                                           title = 'Top Sources')\nplt.title(\"Top sources\", bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Top sources\"+\".png\", bbox_inches='tight')","0fc8cf8c":"Metadata_df['publish_time']= pd.to_datetime(Metadata_df['publish_time']) \nMetadata_df['year']=pd.DatetimeIndex(Metadata_df['publish_time']).year\n\nconfirmed_total_yearn=pd.DataFrame(Metadata_df['year'].value_counts().reset_index().values,columns=['year','count'])\nconfirmed_total_yearn_index = confirmed_total_yearn.sort_index(axis = 0, ascending=False)\n\nconfirmed_total_yearn_index = confirmed_total_yearn_index.sort_values(by ='year' )\nconfirmed_total_yearn_index=confirmed_total_yearn_index.reset_index(drop=True)\nconfirmed_total_yearn_index['cum_sum_year'] = confirmed_total_yearn_index['count'].cumsum()","b6e9ca72":"import matplotlib.path as mpath\nfig, ax = plt.subplots(figsize=(15, 15))\nstar = mpath.Path.unit_regular_star(6)\ncircle = mpath.Path.unit_circle()\nverts = np.concatenate([circle.vertices, star.vertices[::-1, ...]])\ncodes = np.concatenate([circle.codes, star.codes])\ncut_star = mpath.Path(verts, codes)\n\n# Add x-axis and y-axis\nax.plot(confirmed_total_yearn_index['year'],\n        confirmed_total_yearn_index['cum_sum_year'],'--r',marker=cut_star, markersize=15,\n        color='purple')\nplt.grid()\n# Set title and labels for axes\nax.set(xlabel=\"Year\",\n       ylabel=\"Number of Articles\",\n       title=\"Published Articles VS Years\")\nplt.show()","13ea1042":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)","80074c9b":"'''def load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n    \n    return raw_files\n\ndef generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in tqdm(all_files):\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df'''","d0df15be":"'''import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\npmc_dir = '\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/pdf_json\/'\npmc_files = load_files(pmc_dir)\npmc_df = generate_clean_df(pmc_files)\npmc1_dir = '\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/'\npmc1_files = load_files(pmc1_dir)\npmc1_df = generate_clean_df(pmc1_files)\npmc2_dir = '\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json\/'\npmc2_files = load_files(pmc2_dir)\npmc2_df = generate_clean_df(pmc2_files)\npmc3_dir = '\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json\/'\npmc3_files = load_files(pmc3_dir)\npmc3_df = generate_clean_df(pmc3_files)\n#pmc_df.to_csv('clean_pmc.csv', index=False)\n#pmc_df.head()\npmc_df.rename(columns={'paper_id':'doc_id'},inplace = True)\npmc1_df.rename(columns={'paper_id':'doc_id'},inplace = True)\npmc2_df.rename(columns={'paper_id':'doc_id'},inplace = True)\npmc3_df.rename(columns={'paper_id':'doc_id'},inplace = True)\nframes = [pmc_df, pmc1_df, pmc2_df,pmc3_df]\nresult = pd.concat(frames)'''","95988983":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n#result.to_csv('combinedjsonfiles.csv', index=False)","1ff8c235":"result = pd.read_csv('\/kaggle\/input\/combined-json-files\/combinedjsonfiles.csv')","7fdf82fa":"result.shape","01af7aa1":"result.head()","a9d80a33":"Combined_data = pd.merge(result,Metadata_df,left_on='doc_id',right_on='sha', how='left')\n#Combined_data1 =Combined_data.to_csv('Combined_data_processed.csv')","739c7277":"Combined_data.shape","d2e0976e":"Combined_data.info()","1d5d60ee":"Combined_data.head()","06546605":"Combined_data.shape","baf8670b":"Combined_data.tail()","bfe42f46":"Combined_data.nunique()","d1e03bac":"Combined_data.drop_duplicates(subset =\"doc_id\", \n                     keep = False, inplace = True)","9b6ad304":"Combined_data=Combined_data.reset_index(drop=True)","134fa903":"Combined_data.isnull().sum()","53639c1b":"'''Combined_data_processing=Combined_data\nCombined_data_processing.info()'''","25117429":"from gensim.parsing.preprocessing import remove_stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom collections import Counter \nimport time\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom gensim import corpora,models,similarities\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom gensim.summarization import summarize\nfrom gensim.summarization import keywords\nporter = PorterStemmer()","6c6b11dd":"# getting the lower case , removing stopwords and gettting the word origin\ndef lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\ndef text_preprocessing(text):\n    text= re.sub('[^a-zA-z0-9\\s]','',text)\n    text=lower_case(text)\n    text=remove_stopwords(text)\n    text=stemSentence(text)    \n    return text","0a33e340":"Combined_data_processing = pd.read_csv('\/kaggle\/input\/combined-data-cleaned\/Combined_data_processed.csv')\nCombined_data_processing['title_x'].fillna('missing', inplace=True)\nCombined_data_processing['abstract_x'].fillna('missing', inplace=True)\nCombined_data_processing['text'].fillna('missing', inplace=True)","44545c87":"'''Combined_data_processing['text']=Combined_data_processing['text'].apply(lambda x: text_preprocessing(x))\nCombined_data_processing['abstract_x']=Combined_data_processing['abstract_x'].apply(lambda x: text_preprocessing(x))\nCombined_data_processing['title_x']=Combined_data_processing['title_x'].apply(lambda x: text_preprocessing(x))'''","8663f428":"# saving the processed data to csv file \n#Combined_data_Processed =Combined_data_processing.to_csv('Combined_data_processed.csv')","1a5af8cf":"#Combined_data_processing['doc_id'][~Combined_data_processing['doc_id'].isin(Combined_data_processing2['doc_id'])]","14926bf8":"Combined_data_processing=Combined_data_processing.reset_index(drop=True)","c38bb9cf":"Combined_data_processing.info()","337e4e09":"Combined_data_processing.shape","079ab7bc":"Combined_data_processing[Combined_data_processing.text.isnull()]\n#Combined_data_processing.loc(Combined_data_processing['text_body']==)\n#data_docs['text_body'][data_docs.subsetdoc_id=='ed5d3f1db5936c2b86eb9ea46eab96a0b67a124e']","b863d89e":"#Combined_data_processing['doc_id'].nunique()","5c32f170":"Combined_data_processing.tail()","b33caa63":"Combined_data_processing.isnull().sum()","560ac808":"#removing Empty Texts\n#Combined_data_processing=Combined_data_processing.dropna(subset =['text']) \n#Combined_data_processing=Combined_data_processing.reset_index(drop=True)","abbceb38":"Combined_data_processing.nunique()","25195356":"\n#corpus_text = [] # intializing list of collection of  clean reviews\n\n#for i in range(0, 29323):\n #   for word in str(Combined_data_processing['text'][i]).split():\n  #      corpus_text.append(word)","19b0f9f9":"Combined_data.info()","d786c695":"KeyWords=[]\nfor i in range(0,40115):\n    KeyWords.append(remove_stopwords(str(keywords(Combined_data['text'][i]))))","af89f877":"#len(KeyWords)","d62cbdc1":"#KeyWords= pd.DataFrame(KeyWords,columns=['KeyWords'])\n#KeyWords.to_csv('keywords.csv',index = False)","3686abff":"#KeyWords.head()","8c6787c9":"'''corpus_text = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, 33384):\n    for word in str(KeyWords['KeyWords'][i]).split():\n        corpus_text.append(word)'''","c065eee7":"#len(corpus_text)","64333f6d":" #removing the words less than four chars \n#corpus_text_modified=[i for i in corpus_text if 5 <=  len(i)]","07701340":"#len(corpus_text_modified)","da450367":"'''unique_string=(\" \").join(corpus_text)\nwordcloud = WordCloud(width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"your_file_name\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","710280a7":"corpus_authors = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, 40128):\n    for word in str(Combined_data_processing['authors_y'][i]).split():\n        corpus_authors.append(word)","2260528a":"len(corpus_authors)","c1f2fe45":"removed_words= ['nan', 'missing','miss']  \nfor word in list(corpus_authors):  # iterating on a copy since removing will mess things up\n    if word in removed_words:\n        corpus_authors.remove(word)","2627b913":"corpus_authors=[i for i in corpus_authors if 3 <=  len(i)]","41c19a7d":"unique_string=(\" \").join(corpus_authors)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"Authors_Words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","37753cc5":"corpus_title = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, 40128):\n    for word in str(Combined_data_processing['title_x'][i]).split():\n        corpus_title.append(word)","a1065b4d":"len(corpus_title)","8a38e9ac":" \nremoved_words= ['nan', 'missing','miss']  \nfor word in list(corpus_title):  # iterating on a copy since removing will mess things up\n    if word in removed_words:\n        corpus_title.remove(word)","fd463be2":"unique_string=(\" \").join(corpus_title)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=10000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"Titles_Words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()","63277346":"'''corpus_abstract = [] # intializing list of collection of  clean reviews\n\nfor i in range(0, 40128):\n    for word in str(Combined_data_processing['abstract_x'][i]).split():\n        corpus_abstract.append(word)'''","71795303":"#len(corpus_abstract)","25b994ec":"removed_words= ['nan', 'missed','miss','missing','abstract']  \nfor word in list(corpus_abstract):  # iterating on a copy since removing will mess things up\n    if word in removed_words:\n        corpus_abstract.remove(word)","d9c970bb":"#corpus_abstract=[i for i in corpus_abstract if 4 <=  len(i)]","572a854f":"'''unique_string=(\" \").join(corpus_abstract)\nwordcloud = WordCloud(background_color='white',width = 1000, height = 500 ,max_words=30000).generate(unique_string)\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.savefig(\"abstract_words\"+\".png\", bbox_inches='tight')\nplt.show()\nplt.close()'''","08b55c3a":"title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\n\ncounter_text = Counter(corpus_text) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(counter_text.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Texts',**title_font)\nplt.xlabel('Number of Words',**axis_font)\nplt.ylabel('Top 100 Words in Texts',**axis_font)\n\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()","7b0e2784":"'''title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\ncorpus_title=[i for i in corpus_title if 4 <=  len(i)]\ncounter_title = Counter(corpus_title) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(counter_title.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Titles',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Titles',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"title most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()'''","66b86d13":"'''counter_abstract = Counter(corpus_abstract) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(counter_abstract.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Abstract',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Abstracts',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"abstract most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()'''","14652820":"'''title_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\n#corpus_authors=[i for i in corpus_authors if 2 <=  len(i)]\ncorpus_authors = Counter(corpus_authors) \n  \n# most_common() produces k frequently encountered \n# input values and their respective counts. \nmost_occur = dict(corpus_authors.most_common(100)) \n  \nprint(most_occur) \n\nlabels, values = zip(*most_occur.items())\n\n# sort your values in descending order\nindSort = np.argsort(values)[::-1]\n\n# rearrange your data\nlabels = np.array(labels)[indSort]\nvalues = np.array(values)[indSort]\n\nindexes = np.arange(len(labels))\nplt.figure(figsize=(20,40))\nplt.barh(labels, values)\nplt.title('The most frequent words in Articles Abstract',**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.xlabel('Number of Words',**axis_font,color='black')\nplt.ylabel('Top 100 Words in Abstracts',**axis_font,color='black')\nplt.xticks(color='black')\nplt.yticks(color='black')\nplt.savefig(\"abstract most frequent Words\"+\".png\", bbox_inches='tight')\n# add labels\n#plt.xticks(indexes + bar_width, labels)\nplt.show()'''","93971c81":"#Implementing TFIDF \nvectorizer = TfidfVectorizer(max_features=5000)\nX = vectorizer.fit_transform(Combined_data_processing['text'].values)\n\n#implementing K-Means\nk = 13\nkmeans = MiniBatchKMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=3)\npca_result = pca.fit_transform(X.toarray())\n            \nprint(pca.components_)\nprint(pca.explained_variance_)","9a58f776":"from sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=50, random_state=0)\nlda.fit(X)","10d11ba6":"Combined_data_processing['y'] = y_pred","9a047d10":"vectorizers = []\nfor ii in range(0, 13):\n    # Creating a vectorizer\n    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))","d3f349c2":"vectorized_data = []\n\nfor current_cluster, cvec in enumerate(vectorizers):\n    try:\n        vectorized_data.append(cvec.fit_transform(Combined_data_processing.loc[Combined_data_processing['y'] == current_cluster, 'text']))\n    except Exception as e:\n        print(\"Not enough instances in cluster: \" + str(current_cluster))\n        vectorized_data.append(None)","276abcd0":"len(vectorized_data)","a20843a1":"from sklearn.decomposition import LatentDirichletAllocation\n# number of topics per cluster\nNUM_TOPICS_PER_CLUSTER = 13\n\nlda_models = []\nfor ii in range(0, 13):\n    # Latent Dirichlet Allocation Model\n    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n    lda_models.append(lda)\n    \nlda_models[0]","23369916":"clusters_lda_data = []\n\nfor current_cluster, lda in enumerate(lda_models):\n    # print(\"Current Cluster: \" + str(current_cluster))\n    \n    if vectorized_data[current_cluster] != None:\n        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))","b231e0bf":"def selected_topics(model, vectorizer, top_n=3):\n    current_words = []\n    keywords = []\n    \n    for idx, topic in enumerate(model.components_):\n        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n        for word in words:\n            if word[0] not in current_words:\n                keywords.append(word)\n                current_words.append(word[0])\n                \n    keywords.sort(key = lambda x: x[1])  \n    keywords.reverse()\n    return_values = []\n    for ii in keywords:\n        return_values.append(ii[0])\n    return return_values","64a8fef4":"all_keywords = []\nfor current_vectorizer, lda in enumerate(lda_models):\n    # print(\"Current Cluster: \" + str(current_vectorizer))\n\n    if vectorized_data[current_vectorizer] != None:\n        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))","c0497ee6":"all_keywords[0][:10]","3c50d7d9":"len(all_keywords)","b9763a54":"f=open('topics.txt','w')\n\ncount = 0\n\nfor ii in all_keywords:\n\n    if vectorized_data[count] != None:\n        f.write(', '.join(ii) + \"\\n\")\n    else:\n        f.write(\"Not enough instances to be determined. \\n\")\n        f.write(', '.join(ii) + \"\\n\")\n    count += 1\n\nf.close()","5dfd7d03":"#implementing K-Means Clustering\ntitle_font = {'fontname':'Arial', 'size':'18', 'color':'black', 'weight':'normal',\n              'verticalalignment':'bottom'} # Bottom vertical alignment for more space\naxis_font = {'fontname':'Arial', 'size':'14'}\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.color_palette(\"muted\", len(set(y_pred)))\n\n# plot\nsns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y_pred,sizes =100, legend='full',palette=palette)\nplt.title(\"PCA - Clustered (K-Means)\",**title_font,bbox={'facecolor':'0.8', 'pad':5})\nplt.savefig(\"Kmeans\"+\".png\", bbox_inches='tight')\nplt.show()","f57dba0a":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nax = plt.figure(figsize=(16,10)).gca(projection='3d')\nax.scatter(\n    xs=pca_result[:,0], \n    ys=pca_result[:,1], \n    zs=pca_result[:,2], \n    c=y_pred, \n    cmap='tab10'\n)\nax.set_xlabel('pca-1')\nax.set_ylabel('pca-2')\nax.set_zlabel('pca-3')\n\nplt.title(\"PCA - Clustered (K-Means)-3D\",**title_font,bbox={'facecolor':'0.9', 'pad':2})\nplt.savefig(\"Kmeans_3D\"+\".png\", bbox_inches=\"tight\")\nplt.show()","db433569":"#Getting the best number of clusters using elbow method\nimport time\ndef elbow_plot(data, start_K, end_K, step):\n    '''\n    Generate an elbow plot to find optimal number of clusters\n    graphing K values from start_K to end_K every step value\n    \n    INPUT: \n        data: Demographics DataFrame\n        start_K: Inclusive starting value for cluster number\n        end_K: Exclusive stopping value for cluster number\n        step: Step value between start_K and end_K\n    OUTPUT: Trimmed and cleaned demographics DataFrame\n    '''\n    score_list = []\n\n    for i in range(start_K, end_K, step):\n        print(i)\n        start = time.time()\n        kmeans = MiniBatchKMeans(i)\n        model = kmeans.fit(data)\n        score = model.score(data)\n        score_list.append(abs(score))\n        end = time.time()\n        elapsed_time = end - start\n        print(elapsed_time)\n\n    plt.plot(range(start_K, end_K, step), \n    score_list, linestyle='--', marker='o', color='b');\n    plt.xlabel('# of clusters K');\n    plt.ylabel('Sum of squared errors');\n    plt.title('SSE vs. K');\n    plt.savefig('elbow_plot.png')\nelbow_plot(pca_result, 1, 20, 1)","24cca396":"def review_to_words(text):\n    words = word_tokenize(str(text)) # Split string into words\n    \n    return words","b9adec3f":"Combined_data_processing.info()","cbf4cbbb":"'''Combined_data_processing['text'] = Combined_data_processing['text'].apply(lambda x: review_to_words(x))\n\ndocs = [] # intializing list of collection of  clean reviews\nfor i in range(0, 40128):\n    docs.append(Combined_data_processing['text'][i])\n    \n\ndictionary = corpora.Dictionary(docs)\nbag_of_words = [dictionary.doc2bow(gen_doc) for gen_doc in docs]\n \n\ntf_idf = models.TfidfModel(bag_of_words)\n#for doc in tf_idf[bag_of_words]:\n #   print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\nsims = similarities.Similarity('sims',tf_idf[bag_of_words],\n                                        num_features=len(dictionary))\n\n\n# Save the Dict and Corpus\ndictionary.save('mydict.dict')  # save dict to disk\ncorpora.MmCorpus.serialize('bow_corpus.mm', bag_of_words)\nsims.save('model.sav')'''\n#print(bag_of_words[10000])","fe740370":"dictionary = corpora.Dictionary.load('\/kaggle\/input\/mydict\/mydict.dict')\nbag_of_words = corpora.MmCorpus('\/kaggle\/input\/bagofwords\/bow_corpus.mm')\n#sims = similarities.Similarity.load('\/kaggle\/input\/simsmodel\/model.sav')","3ccfe306":"tf_idf = models.TfidfModel(bag_of_words)\nsims = similarities.Similarity('sims',tf_idf[bag_of_words],\n                                        num_features=len(dictionary))","6f15b5c0":"tasks = [(\"\"\"What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery Prevalence of asymptomatic shedding and transmission (e.g., particularly children) Seasonality of transmission Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic),'\\ \n          'Natural history of the virus and shedding of it from an infected person,'\\ \n          'Implementation of diagnostics and products to improve clinical processes,'\\   \n          'Disease models, including animal models for infection, disease and transmission,'\\\n' Tools and studies to monitor phenotypic change and potential adaptation of the virus,'\\\n' Immune response and immunity,'\\\n' Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings,'\\\n' Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings,'\\\n' Role of the environment in transmission\"\"\"),\n         (\"\"\" What do we know about COVID-19 risk factors? What have we learned from epidemiological studies? ,'\\\n          'Data on potential risks factors,'\\\n          'Smoking, pre-existing pulmonary disease,'\\\n          'Co-infections (determine whether co-existing respiratory\/viral infections make the virus more transmissible or virulent) and other co-morbidities ,'\\\n          'Neonates and pregnant women ,'\\\n          'Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.,'\\\n          'Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors ,'\\\n          'Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups ,'\\\n          'Susceptibility of populations,'\\\n          'Public health mitigation measures that could be effective for control\"\"\"),\n         (\"\"\"What do we know about virus genetics, origin, and evolution? What do we know about the virus origin and management measures at the human-animal interface?,'\\\n          'Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the virus over time ,'\\\n          'Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences, and determine whether there is more than one strain in circulation. Multi-lateral agreements such as the Nagoya Protocol could be leveraged ,'\\\n          'Evidence that livestock could be infected (e.g., field surveillance, genetic sequencing, receptor binding) and serve as a reservoir after the epidemic appears to be over ,'\\\n          'Evidence of whether farmers are infected, and whether farmers could have played a role in the origin.,'\\\n' Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 and other coronaviruses in Southeast Asia.,'\\\n' Experimental infections to test host range for this pathogen ,'\\\n' Animal host(s) and any evidence of continued spill-over to humans ,'\\\n' Socioeconomic and behavioral risk factors for this spill-over ,'\\\n' Sustainable risk reduction strategies \"\"\"),\n         (\"\"\" What do we know about vaccines and therapeutics? What has been published concerning research and development and evaluation efforts of vaccines and therapeutics?,'\\\n          'Effectiveness of drugs being developed and tried to treat COVID-19 patients.,'\\\n          'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.,'\\\n          'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.,'\\\n          'Exploration of use of best animal models and their predictive value for a human vaccine ,'\\\n          'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.,'\\\n          'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.,'\\\n          'Efforts targeted at a universal coronavirus vaccine.,'\\ \n          'Efforts to develop animal models and standardize challenge studies,'\\\n          'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers,'\\\n          'Approaches to evaluate risk for enhanced disease after vaccination,'\\\n          'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\"\"\"),\n         (\"\"\"What do we know about the effectiveness of non-pharmaceutical interventions? What is known about equity and barriers to compliance for non-pharmaceutical interventions?,'\\\n          'Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases ,'\\\n          'Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.,'\\\n          'Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches.,'\\\n          'Methods to control the spread in communities, barriers to compliance and how these vary among different populations.,'\\\n          'Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status.,'\\\n          'Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.,'\\\n          'Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high).,'\\\n          'Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"\"\"),\n         (\"\"\"What do we know about diagnostics and surveillance? What has been published concerning systematic, holistic approach to diagnostics (from the public health surveillance perspective to being able to predict clinical outcomes)?,'\\\n          'How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).,'\\\n          'Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.,'\\\n          'Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.,'\\\n          'National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).,'\\\n          'Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.,'\\\n          'Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).,'\\\n          'Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.,'\\\n          'Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.,'\\\n          'Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.,'\\\n          'Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.,'\\\n          'Policies and protocols for screening and testing.,'\\\n          'Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.,'\\\n          'Technology roadmap for diagnostics.,'\\\n          'Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.,'\\\n          'New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.,'\\\n          'Coupling genomics and diagnostic testing on a large scale.,'\\\n          'Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.,'\\\n          'Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.,'\\\n          'One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"\"\"),\n         (\"\"\"What has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?,'\\\n          'Resources to support skilled nursing facilities and long term care facilities.,'\\ \n          'Mobilization of surge medical staff to address shortages in overwhelmed communities ,'\\\n          'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies,'\\\n          'Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients,'\\\n          'Outcomes data for COVID-19 after mechanical ventilation adjusted for age.,'\\\n          'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.,'\\\n          'Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.,'\\\n          'Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.,'\\\n          'Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.,'\\\n          'Guidance on the simple things people can do at home to take care of sick people and manage disease.,'\\\n          'Oral medications that might potentially work.,'\\\n          'Use of AI in real-time health care delivery to evaluate interventions, risk factors, and outcomes in a way that could not be done manually.,'\\\n          'Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcomes.,'\\\n          'Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials,'\\\n          'Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials ,'\\\n          'Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"\"\"),\n         (\"\"\" What has been published about information sharing and inter-sectoral collaboration? What has been published about data standards and nomenclature? What has been published about governmental public health? What do we know about risk communication? What has been published about communicating with high-risk populations? What has been published to clarify community measures? What has been published about equity considerations and problems of inequity?,'\\\n          'Methods for coordinating data-gathering with standardized nomenclature.,'\\\n          'Sharing response information among planners, providers, and others.,'\\\n          'Understanding and mitigating barriers to information-sharing.,'\\\n          'How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).,'\\\n          'Integration of federal\/state\/local public health surveillance systems.,'\\\n          'Value of investments in baseline public health response infrastructure preparedness ,'\\\n          'Modes of communicating with target high-risk populations (elderly, health care workers).,'\\\n          'Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).,'\\\n          'Communication that indicates potential risk of disease to all population groups.,'\\\n          'Misunderstanding around containment and mitigation.,'\\\n          'Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.,'\\\n          'Measures to reach marginalized and disadvantaged populations.,'\\\n          'Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.,'\\\n          'Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.,'\\\n          'Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"\"\"),\n         (\"\"\"What has been published concerning ethical considerations for research? What has been published concerning social sciences at the outbreak response?,'\\\n          'Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019 ,'\\\n          'Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight ,'\\\n          'Efforts to support sustained education, access, and capacity building in the area of ethics ,'\\\n          'Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.,'\\\n          'Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures),'\\\n          'Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.,'\\\n          'Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"\"\")\n]","f3beaffd":"\nfrom gensim.parsing.preprocessing import remove_stopwords\nporter = PorterStemmer()\ntasks_data=pd.DataFrame(tasks)\ntasks_data.columns=['task_description']\n\n\ntasks_data['task_description']=tasks_data['task_description'].apply(lambda x: text_preprocessing(x))\n\n\nimport re\ndef review_to_words(text):\n    words = word_tokenize(str(text)) # Split string into words\n    \n    return words\n\ntasks_data['task_description']=tasks_data['task_description'].apply(lambda x: review_to_words(x))\n\ndocs_tasks = [] # intializing list of collection of  clean reviews\nfor i in range(0, 9):\n    docs_tasks.append(tasks_data['task_description'][i])","bbda77a3":"len(docs_tasks)","f5a00f5d":"def relevant_articles(tasks):\n    tasks = [tasks] if type(tasks) is str else tasks \n    \n    tasks_vectorized = vectorizer.transform(tasks)\n    tasks_topic_dist = pd.DataFrame(lda.transform(tasks_vectorized))\n\n    for index, bullet in enumerate(tasks):\n        print(bullet)\n        recommended = get_k_nearest_docs(tasks_topic_dist.iloc[index], k, lower, upper, only_covid19)\n        recommended = Combined_data_processing.iloc[recommended]\n\n        h = '<br\/>'.join(['<a href=\"' + l + '\" target=\"_blank\">'+ n + '<\/a>' for l, n in recommended[['url','title_x']].values])\n        display(HTML(h))","b5737168":"def tasks_bags_words(docs_tasks):\n    query_doc_bow = dictionary.doc2bow(docs_tasks)    \n    query_doc_tf_idf = tf_idf[query_doc_bow]\n\n    comparing_results=sims[query_doc_tf_idf]\n    max_results=sorted(range(len(comparing_results)), key=lambda i: comparing_results[i], reverse=True)[:100]\n    return comparing_results,max_results\n    ","1041b6a6":"Combined_data['title_x'].fillna('missing_title', inplace=True)","b74306a9":"def relevant_articles(j):\n    count=1\n    Title= []\n    Publish_Time=[]\n    Author=[]\n    Document_ID=[]\n    Journal=[]\n    Similarity=[]\n    Summary=[]\n    KeyWords=[]\n    comparing_results,task_bag=tasks_bags_words(docs_tasks[j])\n    for i in range(0,100):\n        \n  #  print(f'[ #{count} ]')\n   # link=(Combined_data['doi'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)).strip()\n        link=(Combined_data['url'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False)).strip()\n        if (Combined_data['title_x'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False)=='  missing_title'):\n        \n            title = nltk.sent_tokenize(Combined_data['text'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False))[0]\n        else:\n            title=Combined_data['title_x'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False)\n    #linka='https:\/\/doi.org\/'+link\n        linka=link\n        linkb=title\n        final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n   # display(HTML(final_link))\n    \n        Similarity.append(comparing_results[task_bag[i]])\n        Document_ID.append(Combined_data['doc_id'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False))\n        Title.append(final_link)\n    #Title.append(Combined_data['url'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False))\n        Publish_Time.append(Combined_data['publish_time'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False))\n        Author.append(Combined_data['authors_y'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False))\n        Journal.append(Combined_data['journal'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False))\n        try:\n            Summary.append(remove_stopwords(str(summarize(Combined_data['text'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False),word_count=50,split=True))))\n        except:\n            Summary.append(remove_stopwords(str(keywords(Combined_data['text'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False),split=True))))\n        KeyWords.append(remove_stopwords(str(keywords(Combined_data['text'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][task_bag[i]]].to_string(index=False),split=True))))\n    #print(f\"{Fore.YELLOW}Document ID      : {Combined_data['doc_id'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)} \")\n    #print(f\"{Fore.WHITE}Title          : {Combined_data['title_x'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(f\"{Fore.RED}Authors          : {Combined_data['authors'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(f\"{Fore.WHITE}Abstract       : {Combined_data['abstract_x'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(f\"{Fore.YELLOW}Publish Time  : {Combined_data['publish_time'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(f\"{Fore.GREEN}Journal        : {Combined_data['journal'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(f\"{Fore.WHITE}Complete Text        : {Combined_data['text_body'].loc[Combined_data['doc_id'] == Combined_data_processing['doc_id'][max_results7[i]]].to_string(index=False)}\")\n    #print(\"_____________________________________________________________________________________________________________________________________________________________________________________________________\")\n        count+=1\n    Document_ID= pd.DataFrame(Document_ID,columns=['Document_ID'])    \n    Title= pd.DataFrame(Title,columns=['Title'])  \n    Publish_Time= pd.DataFrame(Publish_Time,columns=['Publish_Time'])  \n    Author= pd.DataFrame(Author,columns=['Author'])  \n    Journal= pd.DataFrame(Journal,columns=['Journal']) \n    Similarity=pd.DataFrame(Similarity,columns=['Similarity'])\n    Summary= pd.DataFrame(Summary,columns=['Summary'])\n    KeyWords= pd.DataFrame(KeyWords,columns=['KeyWords'])\n    Document_ID = pd.concat([Document_ID,Title,KeyWords,Publish_Time,Author,Journal,Summary,Similarity], axis=1)\n    Document_ID.to_csv('Task8.csv',index = False)\n    Document_ID = Document_ID.reset_index(drop=True)\n    display(HTML(Document_ID.to_html(escape=False,index=False)))\n    ","9dc005dd":"import ipywidgets as widgets\nfrom IPython.display import clear_output\nimport joblib\nfrom IPython.display import HTML, display\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\ndef relevant_articles_for_text():    \n \n    kWidget = widgets.IntSlider(value=9, description='Task Number', max=8, min=0, layout=Layout(width='25%'))\n\n    button = widgets.Button(description=\"Display\")\n\n    display(VBox([HBox([kWidget], layout=Layout(width='90%', justify_content='space-around')),\n         button], layout=Layout(align_items='center')))\n\n    def on_button_clicked(b):\n        clear_output()\n        display(VBox([HBox([kWidget], layout=Layout(width='90%', justify_content='space-around')),\n             button], layout=Layout(align_items='center')))        \n        relevant_articles(kWidget.value)\n\n    button.on_click(on_button_clicked)","38c48fa7":"relevant_articles_for_text()","5fcd384e":"Combined_data.info()","bfea0e32":"from sklearn.manifold import TSNE\ntsne = TSNE(verbose=1, perplexity=100, random_state=42)\nX_embedded = tsne.fit_transform(X.toarray())","99d5356f":"import pickle\n\n# save the final t-SNE\npickle.dump(X_embedded, open(\"X_embedded.p\", \"wb\" ))\n\n# save the labels generate with k-means(20)\npickle.dump(y_pred, open(\"y_pred.p\", \"wb\" ))","dc5cfc99":"Combined_data.columns","407ab721":"import os\n\n# change into lib directory to load plot python scripts\nmain_path = os.getcwd()\nlib_path = '\/kaggle\/input\/out-resources'\nos.chdir(lib_path)","75de8b03":"from call_backs import input_callback, selected_code\nimport bokeh\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap, transform\nfrom bokeh.io import output_file, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\nfrom bokeh.layouts import column, widgetbox, row, layout\nfrom bokeh.layouts import column\n# show on notebook\noutput_notebook()\n# target labels\ny_labels = y_pred\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= Combined_data['title_x'],\n    authors = Combined_data['authors_x'],\n    journal = Combined_data['journal'],\n    abstract = Combined_data['abstract_x'],\n    labels = [\"C-\" + str(x) for x in y_labels],\n    links = Combined_data['doc_id']\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors{safe}\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n    (\"Link\", \"@links\")\n],\npoint_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\nplot = figure(plot_width=1200, plot_height=850, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'tap'], \n           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\", \n           toolbar_location=\"above\")\n\n# plot settings\nplot.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\nplot.legend.background_fill_alpha = 0.6","e7c63c95":"Combined_data_processing.head()","3d91504e":"import os\nos.chdir(main_path)\ntopic_path = 'topics.txt'\nwith open(topic_path) as f:\n    topics = f.readlines()","c0e4276f":"# Keywords\ntext_banner = Paragraph(text= 'Keywords: Slide to specific cluster to see the keywords.', height=45)\ninput_callback_1 = input_callback(plot, source, text_banner, topics)\n\n# currently selected article\ndiv_curr = Div(text=\"\"\"Click on a plot to see the link to the article.\"\"\",height=150)\ncallback_selected = CustomJS(args=dict(source=source, current_selection=div_curr), code=selected_code())\ntaptool = plot.select(type=TapTool)\ntaptool.callback = callback_selected\n\n# WIDGETS\nslider = Slider(start=0, end=13, value=13, step=1, title=\"Cluster #\", callback=input_callback_1)\nkeyword = TextInput(title=\"Search:\", callback=input_callback_1)\n\n# pass call back arguments\ninput_callback_1.args[\"text\"] = keyword\ninput_callback_1.args[\"slider\"] = slider","89ee4e40":"# STYLE\nslider.sizing_mode = \"stretch_width\"\nslider.margin=15\n\nkeyword.sizing_mode = \"scale_both\"\nkeyword.margin=15\n\ndiv_curr.style={'color': '#BF0A30', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndiv_curr.sizing_mode = \"scale_both\"\ndiv_curr.margin = 20\n\ntext_banner.style={'color': '#0269A4', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ntext_banner.sizing_mode = \"scale_both\"\ntext_banner.margin = 20\n\nplot.sizing_mode = \"scale_both\"\nplot.margin = 5\n\nr = row(div_curr,text_banner)\nr.sizing_mode = \"stretch_width\"","bf849084":"# LAYOUT OF THE PAGE\nl = layout([\n    [slider, keyword],\n    [text_banner],\n    [div_curr],\n    [plot],\n])\nl.sizing_mode = \"scale_both\"\n\n# show\noutput_file('t-sne_covid-19_interactive.html')\nshow(l)","f1f1ef57":"### 4.2.1.1   Articles text body Word Cloud","2fcc0dd2":"## We are Working on Unsupervised learning peoblem . Developing text and data mining tools that can help the medical community develop answers to high priority scientific questions. The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date. That will allow us to apply text and data mining approaches to find answers to the below tasks:\n1. What is known about transmission, incubation, and environmental stability?\n1. What do we know about COVID-19 risk factors?\n1. What do we know about virus genetics, origin, and evolution?\n1. What do we know about vaccines and therapeutics?\n1. What do we know about non-pharmaceutical interventions?\n1. What do we know about diagnostics and surveillance?\n1. What has been published about medical care?\n1. What has been published about information sharing and inter-sectoral collaboration?\n1. What has been published about ethical and social science considerations?","956c4f9c":"## creating Bag of Words for each Task and checking for Similarity(Using Cosine Similarity)","fbaea0ed":"# 7.3 The Third Task : What do we know about virus genetics, origin, and evolution?","c4adcdfa":"# 7.6 The Sixth Task : What do we know about diagnostics and surveillance?","3a264316":"## 2.2  Fetching the Papers from Json Files :\n### i will follow the below algorithm in fetching those json files:\nhttps:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool","853e8a13":"# Table of Contents:\n![image.png](attachment:image.png)","b88699bc":"## 6. Model Evaluation","883a579e":"# 7.4 The Fourth Task : What do we know about vaccines and therapeutics?","7b1d7ce8":"# 7. Tasks Filtering (We will show the first 100 most coorelated  Articles for each task)\n### The Tables below showing the top most coorelated articles according to their Cosine Similarity beteen each task and the Whole articles , there is a column showing the cosine similarity , See how is the close coorelation between tasks and related articles.","0125aa09":"## Word Cloud for Words in Titles","7a6b7283":"# 2. Problem Solving\n## We Will Follow the below Flow Charts  in our Problem Solving:\n## 1st Flow chart for Unsupervised Machine Learning algorithm that we will apply to Articles Words to group them in Clusters depending on theier similarities , and reaching to the optimum number of Clusters. \n![image.png](attachment:image.png)\n\n## 2nd Flow Chart showing how to filter the all Articles that we have around (29,323) Article , and Filtering them according to our Tasks(9 Tasks) , by applying NLP Techniques as shown Below:","337bef20":"## 3.  Data Cleaning","f6af5419":"### We Will follow the below chart for our CombinedData and our tasks for text preprocessing and data visualization:\n#### 1-Articles , which we have collected from Json Files.\n#### 2-Tasks , Which we will Combine together as we will see below\n![image.png](attachment:image.png)","5ce28d5a":"# 7.7 The Seventh Task : What has been published about medical care?","a761fb06":"### Combining the MetaData and Docs Data","d7d9db0f":"## 4.2 Articles Data Visualization and analysis (Text Preprocessing)","f5a7b160":"# 1. Problem Statement","7053ccc1":"## 4.2.3 Creating Bag of Words \n## 4.2.4 Creating TF-IDF\n## 4.2.5 Creating Cosine Similarity","4ec0b318":"### We have more than 56,590,638 Words in our Articles","f91ca0e9":"### 4.2.1 Text Pre-Processing","b805733d":"![image.png](attachment:image.png)","6f565016":"### 4.2.1.3 Word Cloud for Words in abstract","8edff2e6":" # 7.1 The First Task: What is known about transmission, incubation, and environmental stability?","5fccbe3b":"##  5. Modelling","da915fc5":"### Creating Bag of Words for Tasks (The tasks discription as it is in COVID-19 Open Research Dataset Challenge (CORD-19))\n#### I have divided each task according to completed discription mentioned in Cord-19 Tasks ","63e21bb6":"### 4.2.2  Data Visualization for most common Words(Top 100) in Texts ,Titles and abstratct","d2cbfc6b":"### 4.2.1.2 Word Cloud for Words in Authors","0dc15a7a":"# 7.8 The Eighth Task : What has been published about information sharing and inter-sectoral collaboration?","3ab250fb":"#### 4.2.2.3  Most Frequent Words in Abstracts (Top 100)","7e7a0eea":"#### 4.2.2.1    Most Frequent Words in Texts (Top 100)","60f768bd":"#### 4.2.2.2  Most Frequent Words in Titles (Top 100)","7bb0e38d":"## 4.1 MetaData Visualization","bfc96859":"# 7.2 The Second Task : What do we know about COVID-19 risk factors?","3f50fb32":"# 7.9 The Nineth Task : What has been published about ethical and social science considerations?","7a76d400":"## Conclusion\n1. -This scatter plot is generated from Articles text , each article text is a feature. \n1. -usinig features vector TfidfVectorizer. \n1. -Dimensionality Reduction using PCA.\n1. -generating clustering using k-Means where k=15 (the best value as elbow plot).\n1. -Topic Modeling is done on each cluster to get the keywords per cluster.","e8929131":"# 7.5 The Fifth Task : What do we know about non-pharmaceutical interventions?","df1a811e":"##  Fetching Data and Data Cleaing"}}