{"cell_type":{"ff39ac82":"code","aa92f80d":"code","425e6ce0":"code","2bbef30a":"code","27c8ade8":"code","fbe01fe9":"code","3a672344":"code","8f5813e4":"code","9159ec0b":"code","f48cfd59":"code","f778c472":"code","42ce86ac":"code","1a224ac9":"code","2513ef89":"code","a89e6e17":"code","d4e03f6d":"code","719c8853":"code","c76b638a":"code","e8cc3c05":"code","33a02a73":"code","bbdff9d8":"code","f8dc82d9":"code","b31d14dc":"code","7ccd2ee1":"code","41b05471":"code","dc2d65c9":"code","31e209a3":"code","a06e6015":"code","c2bde838":"code","0d803410":"code","3759d007":"code","97c5beb4":"code","2c1ff69e":"code","6627db18":"code","2afab78d":"code","6c88de8c":"code","71307171":"code","30369856":"markdown","829640aa":"markdown","932f4512":"markdown","bd02e4bd":"markdown","f7a92abc":"markdown","b83f4ab3":"markdown","f5769dcf":"markdown","78f6979d":"markdown","e9c4cb43":"markdown","f9727c3b":"markdown","e5a4ae5c":"markdown","6c4b0dc8":"markdown","853131e3":"markdown","40fc7192":"markdown","dfc8030c":"markdown","a189678c":"markdown","b84c3eff":"markdown","9d28ec8d":"markdown","d0c0f3ec":"markdown"},"source":{"ff39ac82":"import numpy as np \nimport pandas as pd \n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv')\n\n# train = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\COVID19\\Data\\train.csv')\n# test = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\COVID19\\Data\\test.csv')\n# train = pd.read_csv('\/Users\/teyang\/OneDrive\/Work\/Kaggle\/COVID19\/Data\/train.csv')\n# test = pd.read_csv('\/Users\/teyang\/OneDrive\/Work\/Kaggle\/COVID19\/Data\/test.csv')","aa92f80d":"# rename columns\ntrain = train.rename(columns={'Province\/State': 'Province_State', 'Country\/Region': 'Country_Region'})\ntest = test.rename(columns={'Province\/State': 'Province_State', 'Country\/Region': 'Country_Region'})","425e6ce0":"train.head()","2bbef30a":"test.head()","27c8ade8":"train['Date'].max(), test['Date'].min()","fbe01fe9":"# Remove the overlapping train and test data\n\nvalid = train[train['Date'] >= test['Date'].min()] # set as validation data\ntrain = train[train['Date'] < test['Date'].min()]\ntrain.shape, valid.shape","3a672344":"# Standard plotly imports\n#import chart_studio.plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot, init_notebook_mode, plot\n# Using plotly + cufflinks in offline mode\nimport cufflinks\ncufflinks.go_offline(connected=True)\ninit_notebook_mode(connected=True)","8f5813e4":"train_total = train[['Country_Region','Province_State','ConfirmedCases','Fatalities']]\ntrain_total['Province_State'] = train_total['Province_State'].fillna(train_total['Country_Region']) # replace NaN States with country name\ntrain_total = train_total.groupby(['Country_Region','Province_State'],as_index=False).agg({'ConfirmedCases': 'max', 'Fatalities': 'max'})","9159ec0b":"# pio.renderers.default = 'vscode'\npio.renderers.default = 'kaggle'\n\nfig = px.treemap(train_total.sort_values(by='ConfirmedCases', ascending=False).reset_index(drop=True), \n                 path=[\"Country_Region\", \"Province_State\"], values=\"ConfirmedCases\", height=600, width=800,\n                 title='Number of Confirmed Cases',\n                 color_discrete_sequence = px.colors.qualitative.Prism)\nfig.data[0].textinfo = 'label+text+value'\nfig.show()\n\nfig = px.treemap(train_total.sort_values(by='Fatalities', ascending=False).reset_index(drop=True), \n                 path=[\"Country_Region\", \"Province_State\"], values=\"Fatalities\", height=600, width=800,\n                 title='Number of Deaths',\n                 color_discrete_sequence = px.colors.qualitative.Prism)\nfig.data[0].textinfo = 'label+text+value'\nfig.show()","f48cfd59":"# Sum countries with states, not dealing with states for now\ntrain_agg= train[['Country_Region','Date','ConfirmedCases','Fatalities']].groupby(['Country_Region','Date'],as_index=False).agg({'ConfirmedCases': 'sum', 'Fatalities': 'sum'})\n\n# change to datetime format\ntrain_agg['Date'] = pd.to_datetime(train_agg['Date'])\n","f778c472":"! pip install pycountry_convert\nimport pycountry_convert as pc\nimport pycountry\n# function for getting the iso code through fuzzy search\ndef do_fuzzy_search(country):\n    try:\n        result = pycountry.countries.search_fuzzy(country)\n    except Exception:\n        return np.nan\n    else:\n        return result[0].alpha_2\n\ntrain_continent = train_agg\n# manually change name of some countries\ntrain_continent.loc[train_continent['Country_Region'] == 'Korea, South', 'Country_Region'] = 'Korea, Republic of'\ntrain_continent.loc[train_continent['Country_Region'] == 'Taiwan*', 'Country_Region'] = 'Taiwan'\n# create iso mapping for countries in df\niso_map = {country: do_fuzzy_search(country) for country in train_continent['Country_Region'].unique()}\n# apply the mapping to df\ntrain_continent['iso'] = train_continent['Country_Region'].map(iso_map)\n#train_continent['Continent'] = [pc.country_alpha2_to_continent_code(iso) for iso in train_continent['iso']]\n","42ce86ac":"def alpha2_to_continent(iso):\n    try: cont = pc.country_alpha2_to_continent_code(iso)\n    except: cont = float('NaN')\n    return cont\n\ntrain_continent['Continent'] = train_continent['iso'].apply(alpha2_to_continent) # get continent code\ntrain_continent.loc[train_continent['iso'] == 'CN', 'Continent'] = 'CN' # Replace China's continent value as we want to keep it separate\n\ntrain_continent = train_continent[['Continent','Date','ConfirmedCases','Fatalities']].groupby(['Continent','Date'],as_index=False).agg({'ConfirmedCases':'sum','Fatalities':'sum'})\ntrain_continent['Continent'] = train_continent['Continent'].map({'AF':'Africa','AS':'Asia','CN':'China','EU':'Europe','NA':'North America','OC':'Oceania','SA':'South America'})","1a224ac9":"long = pd.melt(train_continent, id_vars=['Continent','Date'], value_vars=['ConfirmedCases','Fatalities'], var_name='Case', value_name='Count').sort_values(['Date','Count'])\nlong['Date'] = long['Date'].astype('str')","2513ef89":"pio.renderers.default = 'kaggle' # does not work on vscode\n\n# color palette\ncnf = '#393e46' # confirmed - grey\ndth = '#ff2e63' # death - red\n# rec = '#21bf73' # recovered - cyan\n# act = '#fe9801' # active case - yellow\n\nfig = px.bar(long, y='Continent', x='Count', color='Case', barmode='group', orientation='h', text='Count', title='Counts by Continent', animation_frame='Date',\n             color_discrete_sequence= [dth,cnf], range_x=[0, 100000])\nfig.update_traces(textposition='outside')","a89e6e17":"# Interactive time series plot of confirmed cases\nfig = px.line(train_agg, x='Date', y='ConfirmedCases', color=\"Country_Region\", hover_name=\"Country_Region\")\nfig.update_layout(autosize=False,width=1000,height=500,title='Confirmed Cases Over Time for Each Country')\nfig.show()","d4e03f6d":"# Interactive time series plot of fatalities\nfig = px.line(train_agg, x='Date', y='Fatalities', color=\"Country_Region\", hover_name=\"Country_Region\")\nfig.update_layout(autosize=False,width=1000,height=500,title='Fatalities Over Time for Each Country')\nfig.show()","719c8853":"## Load Natural Earth Map Data\n\nimport geopandas as gpd # for reading vector-based spatial data format\nshapefile = '\/kaggle\/input\/natural-earth-maps\/ne_110m_admin_0_countries.shp'\n#shapefile = r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\COVID19\\110m_cultural\\ne_110m_admin_0_countries.shp'\n\n# Read shapefile using Geopandas\n#gdf = gpd.read_file(shapefile)[['ADMIN', 'ADM0_A3', 'geometry']]\ngdf = gpd.read_file(shapefile)\n\n# Drop row corresponding to 'Antarctica'\ngdf = gdf.drop(gdf.index[159])","c76b638a":"## Get the ISO 3166-1 alpha-3 Country Codes\n\nimport pycountry\n# function for getting the iso code through fuzzy search\ndef do_fuzzy_search(country):\n    try:\n        result = pycountry.countries.search_fuzzy(country)\n    except Exception:\n        return np.nan\n    else:\n        return result[0].alpha_3\n\n# manually change name of some countries\ntrain_agg.loc[train_agg['Country_Region'] == 'Korea, South', 'Country_Region'] = 'Korea, Republic of'\ntrain_agg.loc[train_agg['Country_Region'] == 'Taiwan*', 'Country_Region'] = 'Taiwan'\n# create iso mapping for countries in df\niso_map = {country: do_fuzzy_search(country) for country in train_agg['Country_Region'].unique()}\n# apply the mapping to df\ntrain_agg['iso'] = train_agg['Country_Region'].map(iso_map)\n","e8cc3c05":"# # function for getting the better country name through fuzzy search\n# def do_fuzzy_search_country(country):\n#     try:\n#         result = pycountry.countries.search_fuzzy(country)\n#     except Exception:\n#         return np.nan\n#     else:\n#         return result[0].name\n\n# country_map = {country: do_fuzzy_search_country(country) for country in train_agg['Country_Region'].unique()}\n# # apply the mapping to df\n# train_agg['Country_Region'] = train_agg['Country_Region'].map(country_map)","33a02a73":"# countries with no iso\nnoiso = train_agg[train_agg['iso'].isna()]['Country_Region'].unique()\n# get other iso from natural earth data, create the mapping and add to our old mapping\notheriso = gdf[gdf['SOVEREIGNT'].isin(noiso)][['SOVEREIGNT','SOV_A3']]\notheriso = dict(zip(otheriso.SOVEREIGNT, otheriso.SOV_A3))\niso_map.update(otheriso)","bbdff9d8":"# apply mapping and find countries with no iso again\ntrain_agg['iso'] = train_agg['Country_Region'].map(iso_map)\ntrain_agg[train_agg['iso'].isna()]['Country_Region'].unique()","f8dc82d9":"# change date to string, not sure why plotly cannot accept datetime format\ntrain_agg['Date'] = train_agg['Date'].dt.strftime('%Y-%m-%d')\n","b31d14dc":"# apply log10 so that color changes are more prominent\nimport numpy as np\ntrain_agg['ConfirmedCases_log10'] = np.log10(train_agg['ConfirmedCases']).replace(-np.inf, 0) # log10 changes 0 to -inf so change back\n","7ccd2ee1":"# Interactive Map of Confirmed Cases Over Time\n\n#pio.renderers.default = 'browser' # does not work on vscode\npio.renderers.default = 'kaggle'\nfig = px.choropleth(train_agg, locations='iso', color='ConfirmedCases_log10', hover_name='Country_Region', animation_frame='Date', color_continuous_scale='reds')\nfig.show()\n","41b05471":"# load google trends data\n#cv = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\COVID19\\GoogleTrends\\coronavirus.csv', encoding = 'ISO-8859-1')\n#covid = pd.read_csv(r'C:\\Users\\TeYan\\OneDrive\\Work\\Kaggle\\COVID19\\GoogleTrends\\covid19.csv', encoding = 'ISO-8859-1')\n\ncv = pd.read_csv('\/kaggle\/input\/covid19-googletrends\/coronavirus.csv', encoding = 'ISO-8859-1')\ncovid = pd.read_csv('\/kaggle\/input\/covid19-googletrends\/covid19.csv', encoding = 'ISO-8859-1')","dc2d65c9":"cv = cv.merge(covid, left_on=['Country','iso','date'],right_on=['Country','iso','date'],suffixes=('_cv', '_covid')) # merging removes some small countries but that's alright\ncv['hits'] = cv[['hits_cv','hits_covid']].max(axis=1) # get whichever has a higher proportion between the 2 keywords\n\ncv['iso'] = [pycountry.countries.get(alpha_2=a).alpha_3 for a in cv['iso']] # get the alpha_3 codes from the alpha_2 to merge with confirmed cases df\ncc_google = train_agg.merge(cv, left_on=['iso','Date'], right_on=['iso','date']) # merge confirmed cases df with google trend df","31e209a3":"import seaborn as sns\n\nsns.regplot(x='hits',y='ConfirmedCases_log10',data=cc_google,scatter_kws={'s':25},fit_reg=True, line_kws={\"color\": \"black\"})\n\n# does not work on Kaggle\n# p = sns.jointplot(x=\"hits\", y=\"ConfirmedCases_log10\", data=cc_google, kind='reg',\n#                   joint_kws={'line_kws':{'color':'black'}}, bw=0.1)\n# p.fig.set_figwidth(10)                    ","a06e6015":"popCountries = cc_google[cc_google['Country_Region'].isin(['Singapore','US','Italy','Iran'])] # select the countries\n\n# separate confirmed cases (cc) and hits (h) columns to normalize them by group, then merge back the columns\npc_cc = popCountries[['Country_Region','Date','ConfirmedCases']] # popular countries confirmed cases\npc_f = popCountries[['Country_Region','Date','Fatalities']] # popular countries fatalities\npc_h = popCountries[['Country_Region','Date','hits']] # popular countries hits\n# min-max normalization\npc_cc=pc_cc.assign(ConfirmedCases=pc_cc.groupby('Country_Region').transform(lambda x: (x - x.min()) \/ (x.max() - x.min()))) \npc_f=pc_f.assign(Fatalities=pc_f.groupby('Country_Region').transform(lambda x: (x - x.min()) \/ (x.max() - x.min()))) \npc_h=pc_h.assign(hits=pc_h.groupby('Country_Region').transform(lambda x: (x - x.min()) \/ (x.max() - x.min())))\n# merge back the columns\npopCountries = pc_cc.merge(pc_h, left_on=['Country_Region','Date'], right_on=['Country_Region','Date'])\npopCountries = popCountries.merge(pc_f, left_on=['Country_Region','Date'], right_on=['Country_Region','Date'])\npopCountries = popCountries[['Country_Region','Date','ConfirmedCases','Fatalities','hits']]\npopCountries = popCountries.rename(columns={'ConfirmedCases':'val1','Fatalities':'val2','hits':'val3'})\n","c2bde838":"# convert to long format for plotting\nlong = pd.wide_to_long(popCountries, stubnames='val', i=['Country_Region','Date'], j='CC_F_Hits').reset_index()\n\n# Replace values with labels\ndef replaceVal(x):\n    if x == 1: val = 'Confirmed Cases'\n    elif x == 2: val = 'Fatalities'\n    else: val = 'Hits'\n    return val\n\nlong['CC_F_Hits'] = long['CC_F_Hits'].apply(replaceVal)","0d803410":"# plot facet line plots for each country\nimport matplotlib.pyplot as plt\n\ng = sns.relplot(x=\"Date\", y=\"val\",\n            hue=\"CC_F_Hits\", col=\"Country_Region\", col_wrap=2,\n            height=4, aspect=1.45, facet_kws=dict(sharex=False),\n            kind=\"line\", legend=\"full\", data=long)\ng.set_xticklabels(rotation=45,fontsize=5,horizontalalignment='right')\ng.fig.subplots_adjust(top=0.9)\ng.fig.suptitle('Confirmed Cases & Google Search Trajectories for 4 Countries', fontsize=16)\ng.fig.set_figheight(10)\ng.set_axis_labels(y_var=\"Normalized Confirmed Cases & Google Search Hits\")","3759d007":"# get Iran\nir = cc_google[cc_google['Country_Region'] == 'Iran'].reset_index()\nir = ir[['Date','ConfirmedCases','Fatalities','hits']]\nir.Date = pd.to_datetime(ir.Date)\nir.index = ir.Date  # reassign the index.","97c5beb4":"# correlation of confirmed cases and google trend\nimport scipy.stats\nsns.regplot(x='hits',y='ConfirmedCases',data=ir,scatter_kws={'s':25},fit_reg=True, line_kws={\"color\": \"black\"})\n\nscipy.stats.pearsonr(ir.ConfirmedCases, ir.hits)","2c1ff69e":"! pip install pmdarima","6627db18":"# auto-arima\nimport pmdarima as pm\n\nmodel = pm.auto_arima(ir[['ConfirmedCases']], exogenous=ir[['hits']], # include google trends data as external regressor\n                        start_p=1, start_q=1,\n                        test='adf',       # use adftest to find optimal 'd'\n                        max_p=3, max_q=3, # maximum p and q\n                        m=1,              # frequency of series\n                        d=None,           # let model determine 'd'\n                        seasonal=False,   # No Seasonality\n                        start_P=0, D=0, trace=True, \n                        error_action='ignore', suppress_warnings=True, \n                        stepwise=True)\nprint(model.summary())","2afab78d":"model.plot_diagnostics(figsize=(7,5))\nplt.show()","6c88de8c":"# get the exogeneous regressor values (google trend) for the forecasting period\nexo = cv\nexo = cv[(pd.to_datetime(cv['date']) > ir.Date.max()) & (cv['Country'] == 'Iran')]\nexo.index = pd.to_datetime(exo['date'])\n\n# get the validation confirmed cases for the forecasting period\nir_val = valid[valid.Country_Region == 'Iran']\nir_val.Date = pd.to_datetime(ir_val.Date)\nir_val.index = ir_val.Date","71307171":"from datetime import timedelta\n\n# Forecast\nn_periods = 7 # validation only has 7 days now\nfitted, confint = model.predict(n_periods=n_periods, \n                                  exogenous=np.tile(exo.hits, 1).reshape(-1,1), # predict using google trend\n                                  return_conf_int=True)\n\nindex_of_fc = pd.date_range(ir.index[-1] + timedelta(days=1), periods = n_periods, freq='D') # get the date index range of the forecasting period\n\n# make series for plotting purpose\nfitted_series = pd.Series(fitted, index=index_of_fc)\nlower_series = pd.Series(confint[:, 0], index=index_of_fc)\nupper_series = pd.Series(confint[:, 1], index=index_of_fc)\n\n# Plot\nplt.plot(ir.ConfirmedCases, label='Fitting')\nplt.plot(fitted_series, color='darkgreen', label='Predicted')\nplt.plot(ir_val.ConfirmedCases, color='red', label='Actual')\nplt.fill_between(lower_series.index, \n                 lower_series, \n                 upper_series, \n                 color='k', alpha=.15)\n\nplt.title(\"ARIMA Forecast of Confirmed Cases for Iran\")\nplt.xticks(rotation=45, horizontalalignment='right')\nplt.legend(loc=\"upper left\")\nplt.show()","30369856":"I hope that this notebook has been useful. I am still learning time series modelling and any advice is appreciated. \n\n**Please give this notebook an upvote if you like it!**\n\nStay safe everyone!","829640aa":"<a id='Data_loading_structure'><\/a>\n## **1. Data Loading and Cleaning** ##","932f4512":"The actual confirmed cases appear to be just around the upper bound of the 95% confidence interval, which is not good! I also need to update my google trend data, and perhaps get more data of other search queries with keywords related to the virus symptoms.","bd02e4bd":"<a id='Map_Data'><\/a>\n## **4. Interactive Time Series Map** ##","f7a92abc":"### Questions to ask\nHow long does it take from huge spike in search queries of symptoms keywords to huge increase in confirmed cases?\n\nCan we use confirmed cases in combination with google search queries to predict subsequent cases? Using ARIMA models?\n\nPerhaps fitting both a logistic\/sigmoid function to the data as well as an exponential function and choose the one that is the best for each country\n\nCan we predict the spread of the virus in the country\/city based on the characteristics of the country\/city, like its transport mobility system, cleaniness, population density etc?\n\nHow long does it take from first confirmed case to mass grocery panic?","b83f4ab3":"### Tracjectories of Google search queries and confirmed cases over time for 4 countries","f5769dcf":"### Auto-ARIMA on Iran Example","78f6979d":"We can see that for the 4 countries, google search queries for the keywords 'coronavirus' and 'COVID-19' tend to experience a sharp spike just before or during the start of the first few cases. This might happen because people generally only start to express interest or worry when they receive news that their region or neighbouring countries experience a sharp increase in confirmed cases. For example, in the case of Italy, although there was a brief moment of interest regarding the virus around the start of February, it died down quickly as there was no spike in confirmed cases in Europe. However, as more cases started to appear around the region, Google search queries to find out more about the virus increased quickly. We can also see that search queries will decrease after the virus has been around the country for a while, in the case of Singapore, and towards the end of the time series for Italy and Korea.\n\n**NOTE:** Singapore experienced its first fatality on March 21 and so the it is not being plotted on the line chart above \n\nI am currently working on using ARIMA models to predict future cases........","e9c4cb43":"### Relationship betwen Google search queries and Confirmed Cases","f9727c3b":"The model with (p,d,q) = **(2,2,0)** performed the best with the lowest Akaike Information Criterion (AIC) score, which is a measure of the goodness-of-fit of the model. We see that hits, which is the external regressor (google search query) is significant.","e5a4ae5c":"Originally, I used a jointplot to show the distribution of both features but that does not work on Kaggle because it cannot estimate the density, but somehow works on my VS Code. That plot shows lots of points that have 0 confirmed cases but with google search hits, which is difficult to see on the bottom portion of the scatter plot.","6c4b0dc8":"<a id='Line_Plots'><\/a>\n## **3. Time Series Plots Per Continent and Country** ##","853131e3":"### Time Series Bar Chart of Cases per Country ###","40fc7192":"We can see that the virus originated in China, and spread across neighbouring Asia and Oceania in the beginning, followed by Europe and the Americas. It would be good to have travel and flight data to visualize how that influences the spread of the virus. Some countries\/regions such as the Middle East were lagging perhaps due to the lack of proper virus detection measures. Much of Africa does not sufficient data at the moment.","dfc8030c":"# **Covid-19 & Google Trends Auto-ARIMA Forecasting ** <br>\nTeYang<br>\nCreated: 20\/3\/2020<br>\nLast update: 22\/3\/2020<br>\n\n<img src=\"https:\/\/www.tvw.org\/wp-content\/uploads\/2020\/03\/COVID-19_image.jpg\" width=\"1000\" height=\"300\" align=\"center\"\/>\n\nThis kernel provides some exploratory analysis of the trajectory of the Covid-19 spread throughout the world using interactive plots from Plotly. As of now, data is still limited with few features but hopefully, that will improve when more people and organizations share their data to help fight the pandemic. As of now, the plots ignore states in countries and just sum up the total cases.\n\nUpdate: I scraped data from Google Trends relating to serach queries of the virus 'Coronavirus' and 'COVID-19'. This is based on several research articles showing that google trends has the potential to help with the prediction and detection of disease outbreaks, such as the 2015 [Zika outbreak](https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0165085), [Influenza](https:\/\/www.pnas.org\/content\/112\/47\/14473) and [Dengue fever](https:\/\/journals.plos.org\/plosntds\/article?id=10.1371\/journal.pntd.0002713). Other search queries such as the symptoms of the virus can in turn be used to make predictions as they should be correlated with people feeling unwell and presenting with symptoms as well as physician visits. I have not yet scraped the data for search queries for symptoms keywords, but would highly encourage someone else to do it!\n\n\n### What's in this kernel:\n1. [Data Loading and Cleaning](#Data_loading_structure)\n2. [Confirmed Cases and Deaths Across Countries\/Cities](#Frequencies)\n3. [Time Series Plots Per Country](#Line_Plots)\n4. [Interactive Time Series Map](#Map_Data)\n5. [Google Trends Exploration](#Google_Trends)\n6. [Auto-ARIMA Modelling](#ARIMA)","a189678c":"### Time Series Bar Chart of Cases per Continent ###","b84c3eff":"<a id='Frequencies'><\/a>\n## **2. Confirmed Cases and Deaths Across Countries\/Cities** ##","9d28ec8d":"<a id='Google_Trends'><\/a>\n## **5. Google Trends Exploration** ##\n\n<img src=\"https:\/\/miro.medium.com\/max\/821\/1*Fi6masemXJT3Q8YWekQCDQ.png\" width=\"600\" height=\"300\" align=\"center\"\/>\n\n\n**Google Trends** presents a good opportunity to track the public's interest in a topic in real time and across time. It has been used in academic research to predict **Zika virus outbreak**, **Influenza** and **Dengue fever**. Here I will be using Google search queries of the keywords: **'coronavirus'** and **'COVID-19'** to explore the relationship between popularity of the virus in search queries and the actual confirmed cases for a few countries.\n\n**NOTE:** The google trends data is **normalized** by Google and I do not think there is a way to look at the *absolute* counts of the search queries. They are on a scale of **0-100**, larger representing a higher **proportion** search or popularity of the keyword in the country. That means that for a country (e.g., San Marino) with very little search queries but a high proportion of those are related to the virus, the score will be higher than a country (e.g., Singapore), which has a high amounts of search qeuries but a lower proportion of coronavirus related searches. So although San Marino has a higher score than Singapore, it does not mean that it has a higher coronavirus-related search frequency compared to Singapore. Therefore, when comparing between countries, we are limited to looking at the **correlation** of Google search queries proportion and confirmed cases\/fatalities rather than a direct comparison of the search queries counts.","d0c0f3ec":"<a id='ARIMA'><\/a>\n## **6. Auto-ARIMA Modelling** ##"}}