{"cell_type":{"0eac513f":"code","25d019c2":"code","6fbad440":"code","345bfb24":"code","3a451d88":"code","1dfd5fea":"code","2884cd80":"code","86b10c2f":"code","030febed":"code","0cbee133":"code","ca88d911":"code","9df9f755":"code","e2b2ab00":"code","135690fd":"code","144fcdce":"code","3929700a":"code","e817124b":"markdown","501db617":"markdown"},"source":{"0eac513f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport sys\nimport sklearn\n\nprint('Python: {}'.format(sys.version))\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25d019c2":"names = ['Class', 'id', 'Sequence']\ndata = pd.read_csv('..\/input\/promoters\/promoters.csv', names = names, skiprows=1)\ndata.describe()","6fbad440":"classes = data.loc[:, 'Class']\nsequences = list(data.loc[:, 'Sequence'])\ndataset = {}\n\nfor i, seq in enumerate(sequences):\n    nucleotides = list(seq)\n    nucleotides = [x for x in nucleotides if x != '\\t']\n    nucleotides.append(classes[i])\n    dataset[i] = nucleotides\n\n#print(dataset)    \ndframe = pd.DataFrame(dataset)\nprint(dframe)","345bfb24":"dframe = dframe.transpose()\ndframe.rename(columns = {57: 'Class'}, inplace = True) \nprint(dframe.iloc[:5])\n#print(dframe)\ndframe.describe()","3a451d88":"dframe.columns","1dfd5fea":"model_data = pd.get_dummies(dframe)\nmodel_data.iloc[:4]\nmodel_data = model_data.drop(columns=['Class_-'])\nmodel_data.rename(columns = {'Class_+':'Class'}, inplace=True)","2884cd80":"model_data","86b10c2f":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom sklearn import model_selection","030febed":"X = np.array(model_data.drop(['Class'], 1))\nY = np.array(model_data['Class'])\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=0.25, random_state=1)","0cbee133":"scoring = 'accuracy'\n\n# Models used\nnames = [\"Nearest Neighbors\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"SVM Linear\", \"SVM RBF\", \"SVM Sigmoid\"]\n\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 3),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10,  max_features=1),\n    MLPClassifier(alpha=1),\n    SVC(kernel = 'linear'), \n    SVC(kernel = 'rbf'),\n    SVC(kernel = 'sigmoid')\n]\n\nmodels = zip(names, classifiers)\n\nresults = []\nnames = []\n\nfor name, model in models:\n    # The object to use to fit the data.\n    # K-Folds cross-validator, Provides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default).\n    # Each fold is then used once as a validation while the k - 1 remaining folds form the training set.\n    kfold = model_selection.KFold(n_splits=10, random_state = None)\n    # Evaluate a score by cross-validation\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","ca88d911":"models = zip(names, classifiers)\nfor name, model in models:\n    model.fit(X_train, Y_train)\n    predictions = model.predict(X_test)\n    print(name)\n    print(accuracy_score(Y_test, predictions))\n    print(classification_report(Y_test, predictions))","9df9f755":"model = SVC(kernel = 'sigmoid')\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_test)\nprint(name)\nprint(accuracy_score(Y_test, predictions))\nprint(classification_report(Y_test, predictions))","e2b2ab00":"! pip install tensorflow_decision_forests","135690fd":"import tensorflow_decision_forests as tfdf\n# Check the version of TensorFlow Decision Forests\nprint(\"Found TensorFlow Decision Forests v\" + tfdf.__version__)","144fcdce":"# Split the dataset into a training and a testing dataset.\n\ndef split_dataset(dataset, test_ratio=0.30):\n  \"\"\"Splits a panda dataframe in two.\"\"\"\n  test_indices = np.random.rand(len(dataset)) < test_ratio\n  return dataset[~test_indices], dataset[test_indices]\n\n\n# Encode the categorical label into an integer.\n# Name of the label column.\nlabel = \"Class\"\n\ndframe.columns = [      '0',       '1',       '2',       '3',       '4',       '5',       '6',       '7',\n             '8',       '9',      '10',      '11',      '12',      '13',      '14',      '15',\n            '16',      '17',      '18',      '19',      '20',      '21',      '22',      '23',\n            '24',      '25',      '26',      '27',      '28',      '29',      '30',      '31',\n            '32',      '33',      '34',      '35',      '36',      '37',      '38',      '39',\n            '40',      '41',      '42',      '43',      '44',      '45',      '46',      '47',\n            '48',      '49',      '50',      '51',      '52',      '53',      '54',      '55',\n            '56', 'Class']\n\nclasses = dframe[label].unique().tolist()\nprint(f\"Label classes: {classes}\")\n\ndframe[label] = dframe[label].map(classes.index)\n\ntrain_ds_pd, test_ds_pd = split_dataset(dframe)\nprint(\"{} examples in training, {} examples for testing.\".format(\n    len(train_ds_pd), len(test_ds_pd)))\n\n# Convert the dataset into a TensorFlow dataset.\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_ds_pd, label=label)\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(test_ds_pd, label=label)\n\n# Train a Random Forest model.\nmodel = tfdf.keras.RandomForestModel()\nmodel.compile(metrics=[\"accuracy\"])\n\nmodel.fit(train_ds)\n\n# Summary of the model structure.\nmodel.summary()\n","3929700a":"evaluation = model.evaluate(test_ds, return_dict=True)\nprint()\n\nfor name, value in evaluation.items():\n  print(f\"{name}: {value:.4f}\")\n\n# Export the model to a SavedModel.\n#model.save(\"project\/model\")","e817124b":"# DNA Classification\n### Predict whether or not a short sequence of DNA(E.coli) was a promoter or a \n### non-promoter using algorithms :\n    1. Tensorflow decision forest\n    2. Nearest Neighbors\n    3. Decision Tree\n    4. Random Forest\n    5. Neural Net\n    6. SVM Linear\n    7. SVM RBF\n    8. SVM Sigmoid\n\nDataset : https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/molecular-biology\/promoter-gene-sequences\/","501db617":"## Classification using Tensorflow decision forest"}}