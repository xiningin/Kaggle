{"cell_type":{"c81b6666":"code","cd7d4bc8":"code","28592b0d":"code","3d9a5c13":"code","17d1bf73":"code","08ffe3d4":"code","7e49e8e6":"code","29063250":"code","2c1e29c0":"code","032fa089":"code","769ef6d8":"code","a53a3501":"code","345b3e1c":"code","673cd9ac":"code","2865e819":"code","8df79585":"code","bc4bcc68":"code","af321493":"code","2f10f73b":"code","e37e647f":"code","f5779fba":"code","1056d62a":"code","2688d6ef":"code","1aac0b98":"code","32581907":"code","c01c7d63":"code","74031766":"code","8b0262a6":"markdown","454af4c8":"markdown","3b71b0cb":"markdown","98830f54":"markdown","55911a19":"markdown","918b98cf":"markdown","0ce0ee18":"markdown","57e454db":"markdown","4c67f68e":"markdown","2ca99160":"markdown","f98c4ed4":"markdown","e9f95c15":"markdown","92fc4248":"markdown","7cd4216e":"markdown","0e932ff5":"markdown","a81be694":"markdown","ffc8342e":"markdown","4119b302":"markdown","d21b1775":"markdown"},"source":{"c81b6666":"from google.colab import drive\ndrive.mount('\/content\/drive')","cd7d4bc8":"!pip install -Uqq kaggle\n!pip install --upgrade --force-reinstall --no-deps kaggle","28592b0d":"!rm -rf ~\/.kaggle\n!mkdir ~\/.kaggle\n\n!cp \/content\/drive\/MyDrive\/kaggle\/kaggle.json ~\/.kaggle\n!chmod 600 ~\/.kaggle\/kaggle.json\n\n!kaggle competitions download -c Kannada-MNIST","3d9a5c13":"!mkdir Kannada-MNIST\n!unzip -oq Kannada-MNIST.zip -d Kannada-MNIST\/","17d1bf73":"!pip install timm\n\n!pip install torchcontrib\n\n!pip install albumentations\n!pip install --upgrade --force-reinstall --no-deps albumentations\n\n!pip install pydicom\n\n!pip install -U catalyst","08ffe3d4":"!kaggle datasets download -d yasufuminakama\/pytorch-image-models\n\n!mkdir \/content\/pytorch-image-models\n!unzip -oq \/content\/pytorch-image-models.zip -d \/content\/pytorch-image-models\/","7e49e8e6":"!kaggle datasets download -d khyeh0719\/image-fmix\n\n!mkdir \/content\/image-fmix\n!unzip -oq \/content\/image-fmix.zip -d \/content\/image-fmix\/","29063250":"package_paths = [\n    '\/content\/pytorch-image-models\/pytorch-image-models-master', #'..\/input\/efficientnet-pytorch-07\/efficientnet_pytorch-0.7.0'\n    '\/content\/image-fmix\/FMix-master'\n]\nimport sys; \n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \nfrom fmix import sample_mask, make_low_freq_image, binarise_mask","2c1e29c0":"import torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n# import torchcontrib\n# from torchcontrib.optim import SWA\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport random\nimport cv2\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport os\n\nimport timm\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n\nimport seaborn as sns\nsns.set(style=\"whitegrid\")","032fa089":"import datetime","769ef6d8":"CFG = {\n    'fold_num': 5,\n    'seed': 324,\n    'model_arch': 'SE-Net',#'tf_efficientnet_b4_ns', #'seresnext50_32x4d',\n    'img_size': 28,\n    'epochs': 10,\n    'train_bs': 16,\n    'valid_bs': 32,\n    'T_0': 10,\n    'lr': 1e-2,\n    'min_lr': 1e-6,\n    'weight_decay':1e-6,\n    'num_workers': 2,\n    'accum_iter': 4, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'alpha': 0.1,\n    'beta': 1\n}","a53a3501":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\ndef show_batch_imgs(class_name, examples, df, transforms=None):\n  imgs = df[df['label']==class_name].sample(frac=1)[:examples]\n  plt.figure(figsize=(8,8))\n  for i in range(examples):\n    img = imgs.iloc[i][1:].values.astype(np.uint8).reshape((28,28))\n    if transforms:\n      img = transforms(image=img)['image']\n      img = img.numpy().reshape((28,28))\n    # print(img.shape)\n    plt.subplot(1, examples, i%examples+1)\n    plt.axis('off')\n    plt.imshow(img)\n    plt.title(class_name)","345b3e1c":"train_data = pd.read_csv(\"\/content\/Kannada-MNIST\/train.csv\")\ntest_data = pd.read_csv(\"\/content\/Kannada-MNIST\/test.csv\")","673cd9ac":"print(\"train_data shape:\", train_data.shape)","2865e819":"for i in range(10):\n  show_batch_imgs(i, examples=4, df=train_data)","8df79585":"train_data['label'].unique()","bc4bcc68":"ax = plt.subplots(figsize=(18, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='label', data=train_data);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Class Name\", size=20);\nplt.xticks(rotation=90);","af321493":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            # RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            # Transpose(p=0.5), # not good and make confuse\n            # HorizontalFlip(p=0.5), # make confuse\n            # VerticalFlip(p=0.5), # should be avioded\n            ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=30, p=0.5), # should control the rotate limit\n            # RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5), # not help in grayscale\n            CoarseDropout(p=0.1),\n            Cutout(p=0.1),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            # CenterCrop(CFG['img_size'], CFG['img_size'], p=1.), # may lose parts of the character\n            # Resize(CFG['img_size'], CFG['img_size']),\n            ToTensorV2(p=1.0),\n        ], p=1.)","2f10f73b":"for i in range(10):\n  show_batch_imgs(i, examples=4, df=train_data, transforms=get_train_transforms())","e37e647f":"class KMnistDataset(Dataset):\n  def __init__(self, data, transforms=None, do_fmix=None, fmix_params=None, do_cutmix=None, cutmix_params={'alpha':1}):\n    self.data = data\n    self.labels = self.data['label'].values\n    self.transforms = transforms\n    self.do_fmix = do_fmix\n    self.fmix_params = fmix_params\n    self.do_cutmix = do_cutmix\n    self.cutmix_params = cutmix_params\n  \n  def __len__(self):\n    return len(self.data)\n  \n  def __getitem__(self, index):\n    # get labels\n    target = self.labels[index]\n    target = torch.as_tensor(target, dtype=torch.long) \n    # get images\n    img = self.data.iloc[index, 1:].values.astype(np.float32).reshape((28,28))\n\n    if self.transforms:\n      img = self.transforms(image=img)['image']\n\n    # if self.do_cutmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n    \n    # if self.do_fmix and np.random.uniform(0., 1., size=1)[0] > 0.5:\n\n    return img, target","f5779fba":"def prepare_dataloader(data, train_idx, val_idx):\n  __train = data.loc[train_idx, :].reset_index(drop=True)\n  __valid = data.loc[val_idx, :].reset_index(drop=True)\n\n  train_dataset = KMnistDataset(__train, transforms=get_train_transforms(), do_fmix=False, do_cutmix=False)\n  valid_dataset = KMnistDataset(__valid, transforms=get_valid_transforms(), do_fmix=False, do_cutmix=False)\n\n  train_dataloader = torch.utils.data.DataLoader(\n      train_dataset,\n      batch_size=CFG['train_bs'],\n      pin_memory=False,\n      drop_last=False,\n      shuffle=True,\n      num_workers=CFG['num_workers'],\n  )\n  valid_dataloader = torch.utils.data.DataLoader(\n      valid_dataset,\n      batch_size=CFG['valid_bs'],\n      pin_memory=False,\n      shuffle=True,\n      num_workers=CFG['num_workers'],\n  )\n  return train_dataloader, valid_dataloader","1056d62a":"def train_one_epoch(epoch, model, loss_fn, optimizer, train_loader, writer, device, scheduler=None, schd_batch_update=False):\n  model.train() \n\n  running_loss = None\n\n  pbar = tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True)\n  for step, (imgs, image_labels) in pbar:\n    imgs = imgs.to(device).float()\n    image_labels = image_labels.to(device).long()\n\n    with autocast():\n      image_preds = model(imgs)\n      loss = loss_fn(image_preds, image_labels)\n\n      scaler.scale(loss).backward()\n\n      # calculating the loss\n      if running_loss is None:\n        running_loss = loss.item()\n      else:\n        running_loss = running_loss * .99 + loss.item() * .01\n      \n      if writer:\n        writer.add_scalar('step training loss', running_loss, epoch*len(train_loader)+step)\n        if scheduler:\n          last_lr = scheduler.get_last_lr()[0] # get the learning rate\n          writer.add_scalar('learning rate', last_lr, epoch*len(train_loader)+step)\n        else:\n          for param_group in optimizer.param_groups:\n            writer.add_scalar('learning rate', param_group['lr'], epoch*len(train_loader)+step)\n      \n      # updating the loss\n      if ((step + 1) % CFG['accum_iter'] == 0) or ((step + 1) == len(train_loader)):\n        scaler.step(optimizer)\n        scaler.update()\n        optimizer.zero_grad() \n        if scheduler is not None and schd_batch_update:\n          scheduler.step()\n      # printing out on the screen\n      if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(train_loader)):\n        description = f'epoch {epoch} loss: {running_loss:.4f}'\n        pbar.set_description(description)\n  if scheduler is not None and not schd_batch_update: # adjust the learning rate\n    scheduler.step()","2688d6ef":"def valid_one_epoch(epoch, model, loss_fn, valid_loader, writer, device, scheduler=None, schd_loss_update=False):\n  model.eval()\n\n  loss_sum = 0\n  sample_num = 0\n  image_preds_all = []\n  image_targets_all = []\n\n  pbar = tqdm(enumerate(valid_loader), total=len(valid_loader), position=0, leave=True)\n  for step, (imgs, image_labels) in pbar:\n    imgs = imgs.to(device).float()\n    image_labels = image_labels.to(device).long()\n\n    image_preds = model(imgs)\n    image_preds_all += [torch.argmax(image_preds, 1).detach().cpu().numpy()]\n    image_targets_all += [image_labels.detach().cpu().numpy()]\n\n    loss = loss_fn(image_preds, image_labels)\n\n    loss_sum += loss.item()*image_labels.shape[0]\n    sample_num += image_labels.shape[0]\n\n    if writer:\n      writer.add_scalar('step valid loss', loss_sum\/sample_num, epoch*len(valid_loader)+step)\n\n    if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n      description = f'epoch {epoch} loss: {loss_sum\/sample_num:.4f}'\n      pbar.set_description(description)\n\n  image_preds_all = np.concatenate(image_preds_all)\n  image_targets_all = np.concatenate(image_targets_all)\n  print('validation multi-class accuracy = {:.4f}'.format((image_preds_all==image_targets_all).mean()))\n  if writer:\n    writer.add_scalar('validation accuracy', (image_preds_all==image_targets_all).mean(), epoch)\n  \n  if scheduler is not None:\n    if schd_loss_update:\n      scheduler.step(loss_sum\/sample_num)\n    else:\n      scheduler.step()","1aac0b98":"class Sq_Ex_Block(nn.Module):\n    def __init__(self, in_ch, r):\n        super(Sq_Ex_Block, self).__init__()\n        self.se = nn.Sequential(\n            GlobalAvgPool(),\n            nn.Linear(in_ch, in_ch\/\/r),\n            nn.ReLU(inplace=True),\n            nn.Linear(in_ch\/\/r, in_ch),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        se_weight = self.se(x).unsqueeze(-1).unsqueeze(-1)\n        return x.mul(se_weight)\n\nclass GlobalAvgPool(nn.Module):\n    def __init__(self):\n        super(GlobalAvgPool, self).__init__()\n    def forward(self, x):\n        return x.view(*(x.shape[:-2]),-1).mean(-1)\n\nclass SE_Net(nn.Module):\n    def __init__(self,in_channels):\n        super(SE_Net,self).__init__()\n        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n        #                dilation=1, groups=1, bias=True, padding_mode='zeros')\n        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=64,kernel_size=3,stride=1,padding=0)\n        self.bn1 = nn.BatchNorm2d(num_features=64,eps=1e-3,momentum=0.01)\n        self.c2 = nn.Conv2d(64,64,3,1,0)\n        self.bn2 = nn.BatchNorm2d(64,1e-3,0.01)\n        self.c3 = nn.Conv2d(64,64,5,1,2)\n        self.bn3 = nn.BatchNorm2d(64,1e-3,0.01)\n        self.m1 = nn.MaxPool2d(2)\n        self.d1 = nn.Dropout(0.4)\n        \n        self.c4 = nn.Conv2d(64,128,3,1,0)\n        self.bn4 = nn.BatchNorm2d(128,1e-3,0.01)\n        self.c5 = nn.Conv2d(128,128,3,1,0)\n        self.bn5 = nn.BatchNorm2d(128,1e-3,0.01)\n        self.c6 = nn.Conv2d(128,128,5,1,2)\n        self.bn6 = nn.BatchNorm2d(128,1e-3,0.01)\n        self.m2 = nn.MaxPool2d(2)\n        self.d2 = nn.Dropout(0.4)\n        \n        self.c7 = nn.Conv2d(128,256,3,1,0)\n        self.bn7 = nn.BatchNorm2d(256,1e-3,0.01)\n        self.se3 = Sq_Ex_Block(in_ch=256,r=8)\n        self.m3 = nn.MaxPool2d(2)\n        self.d3 = nn.Dropout(0.4)\n\n        self.fc1 = nn.Linear(256*1*1,256)\n        self.bn8 = nn.BatchNorm1d(256,1e-3,0.01)\n        \n        self.out = nn.Linear(256,10)\n        \n        self.init_linear_weights()\n        \n    def forward(self,x):\n        x = self.bn1(F.leaky_relu(self.c1(x),0.1))\n        x = self.bn2(F.leaky_relu(self.c2(x),0.1))\n        x = self.bn3(F.leaky_relu(self.c3(x),0.1))\n        x = self.d1(self.m1(x))\n        \n        x = self.bn4(F.leaky_relu(self.c4(x),0.1))\n        x = self.bn5(F.leaky_relu(self.c5(x),0.1))\n        x = self.bn6(F.leaky_relu(self.c6(x),0.1))\n        x = self.d2(self.m2(x))\n        \n        x = self.bn7(F.leaky_relu(self.c7(x),0.1))\n        x = self.se3(x)\n        x = self.d3(self.m3(x))\n        \n        x = x.view(-1, 256*1*1) #reshape\n        x = self.bn8(F.leaky_relu(self.fc1(x),0.1))\n        return self.out(x)\n    \n    def init_linear_weights(self):\n        nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in')  #default mode: fan_in\n        nn.init.kaiming_normal_(self.out.weight, mode='fan_in')","32581907":"seed_everything(CFG['seed'])","c01c7d63":"folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train_data.shape[0]), train_data.label.values)","74031766":"for fold, (train_idx, valid_idx) in enumerate(folds):\n  print('Training with {} started'.format(fold))\n\n  print(len(train_idx), len(valid_idx))\n  train_loader, valid_loader = prepare_dataloader(train_data, train_idx, valid_idx)\n\n  writer = Writers[fold]\n  device = torch.device(CFG['device'])\n  model = SE_Net(in_channels=1).to(device)\n  scaler = GradScaler()\n  optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n  scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n\n  loss_tr = nn.CrossEntropyLoss().to(device)\n  loss_fn = nn.CrossEntropyLoss().to(device)\n\n  for epoch in range(CFG['epochs']):\n    train_one_epoch(epoch, model, loss_tr, optimizer, train_loader, writer, device, scheduler=scheduler, schd_batch_update=False)\n    with torch.no_grad():\n      valid_one_epoch(epoch, model, loss_fn, valid_loader, writer, device, scheduler=None, schd_loss_update=False)\n    torch.save(model.state_dict(),'{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch))\n\n  del model, optimizer, train_loader, valid_loader, scaler, scheduler\n  torch.cuda.empty_cache()\n\n  writer.flush()","8b0262a6":"### Take a look of the data","454af4c8":"## Define Model","3b71b0cb":"# Model","98830f54":"### Take a look of the data after augmentation","55911a19":"# EESM 5720 final project\n\nStudy on Kannada MNIST dataset with CNN. This is my final project source code for the EESM 5720 pattern recognition course in HKUST.\n\nIf you like this notebook, please drop me a vote, and feel free to text me if you have questions.\n\nInference part is [here](https:\/\/www.kaggle.com\/dongjai04\/se-net-baseline-eesm5720-inf).\n\nThank you for the [notebook](https:\/\/www.kaggle.com\/khyeh0719\/pytorch-efficientnet-baseline-inference-tta) from khyeh0719 and the [notebook](https:\/\/www.kaggle.com\/ccchang801023\/se-net-my-top1-baseline-model-with-pytorch) from ccchang801023.","918b98cf":"# Data","0ce0ee18":"# EDA","57e454db":"## Helper functions","4c67f68e":"## Training API","2ca99160":"## Train loop","f98c4ed4":"### SE Net","e9f95c15":"## Data Augmentation","92fc4248":"### Statistics","7cd4216e":"## Preprepare of library","0e932ff5":"## Load data and Define the dataset\/dataloader","a81be694":"# Main Loop","ffc8342e":"# Prepare","4119b302":"## Load data","d21b1775":"## Preprepare of the system"}}