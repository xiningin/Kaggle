{"cell_type":{"f4a04ce4":"code","d92c4a11":"code","373df85c":"code","bbde3b12":"code","07738a92":"code","e84a1157":"code","9e01d925":"code","abb90cd1":"code","de52ce01":"code","52ce11a0":"code","dd21fffd":"code","472e042d":"code","ea87ae02":"code","58a8fd49":"code","2bd2af3e":"code","096c63f2":"code","5091421b":"code","55adb41b":"code","227e9279":"code","134caab2":"markdown","17092b2d":"markdown","9bcae01b":"markdown","bd7438f3":"markdown","2579ea90":"markdown","08e492c6":"markdown","c461fed4":"markdown","5c63e60e":"markdown","d2e1f21d":"markdown","7f7f7f43":"markdown","a1715b25":"markdown","40690d4b":"markdown","b2a7868c":"markdown","38a37ae2":"markdown","0af394af":"markdown","6431c931":"markdown","f4623a03":"markdown"},"source":{"f4a04ce4":"import tensorflow as tf \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \n%matplotlib inline","d92c4a11":"#Import the input files\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv') \nevaluation = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsample = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\nprint(f'train shape = {train.shape}', f'test shape = {evaluation.shape}', sep='\\n')","373df85c":"train.head()","bbde3b12":"print(train.isnull().any().sum())\nprint(evaluation.isnull().any().sum())","07738a92":"targets = train['label']\ntrain = train.drop('label',axis = 1)","e84a1157":"train.describe()","9e01d925":"train \/= 255\nevaluation \/= 255","abb90cd1":"train.describe()","de52ce01":"index = np.random.randint(0,42000)\ntest_image = train.values[index].reshape(28,28)\nplt.imshow(test_image, cmap = 'bone')\nplt.title(targets.values[index])\nplt.show()","52ce11a0":"train = train.values.reshape(-1,28,28,1)\nevaluation = evaluation.values.reshape(-1,28,28,1)\ntargets = targets.values.reshape(-1,1)","dd21fffd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, targets, stratify = targets, test_size = 0.1, random_state = 42)","472e042d":"from keras.models import Sequential\nfrom keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dense, Dropout, Flatten\nfrom keras.optimizers import Adam","ea87ae02":"model = Sequential()\n\nmodel.add(Conv2D(32, input_shape = (28,28,1), kernel_size = (3,3), activation = 'relu'))\nmodel.add(Conv2D(32, kernel_size = (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\nmodel.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\nmodel.add(Conv2D(64, kernel_size = (3,3), activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation = 'softmax'))\n\n","58a8fd49":"optimizer = Adam(lr=0.001)\nmodel.compile(optimizer = optimizer,\n             loss = 'sparse_categorical_crossentropy',\n             metrics = ['accuracy'])\n\nmodel.summary()","2bd2af3e":"EPOCHS = 15\nBATCH_SIZE = 256","096c63f2":"history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = EPOCHS, batch_size = BATCH_SIZE)","5091421b":"model.evaluate(X_test, y_test)","55adb41b":"plt.figure(figsize=(9,6))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend(['train','validation'])\nplt.show()","227e9279":"evaluation = evaluation.reshape(28000,28,28,1)\nresults = model.predict_classes(evaluation)\n\nresults = pd.Series(results, name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), results], axis = 1)\n\nsubmission.to_csv(\"submission.csv\", index=False)","134caab2":"# Data\n### Importing Data\nFirst thing to do is import all the libraries you're going to need.","17092b2d":"### Training the model.\n","9bcae01b":"Now, let's describe our dataset again and notice if the standard deviation (std) is any different","bd7438f3":"### Exploring Data\nAfter printing out the shapes of 'train' and 'evaluation', you can see that the train set contains 1 additional column, use .head() to have a look at the first 5 rows of the data.","2579ea90":"# CNN\n### Defining the model\nOur model will be 2 layers of (2x Conv2D, 1x Maxpooling, 1x BatchNorm, 1x Dropout), 2 Dense layers and 1 output layer. <br \/>\nYou can mess around with the number of layers and parameters of each layer and see how it affects your evaluation score.","08e492c6":"# Evaluation\n### Evaluating the model","c461fed4":"### Compiling the model with the right Optimizer, loss and metric.\nYou can try and use other optimizers such as SGD, Adam works very well.","5c63e60e":"As shown, the first row \"label\" is the class of each instance, which should be our output. <br \/>\nNow, let's see if our data contains any flaws <br \/>\nCheck if the dataset contains any null values.","d2e1f21d":"### Visualizing the model's performance","7f7f7f43":"### Thank you for reading!","a1715b25":"### Splitting\nSplit the data into train and test segments, a 0.1-0.2 test-train ratio is good for most cases.","40690d4b":"# Introduction\nCNNs, short for Convolutionary Neural Networks, are a class of neural networks mostly used for dealing with image or video data.<br \/>\nConvolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex.<br \/>\nCNNs apply filters to images in order to extract different features, small (edges, curvatures..etc) and big (whole shapes, patterns..etc)<br \/>","b2a7868c":"# Prediction and submission","38a37ae2":"The standard deviation is much lower now, good job! <br \/>\nLet's check out one random sample using the .imshow() function.","0af394af":"# Build your first Convolutional Neural Network using Keras\n### This notebook will be your first step towards CNNs and Deep Learning.\n* **1. Introduction**\n* **2. Data**\n    * 2.1 Importing Data\n    * 2.2 Exploring Data\n    * 2.3 Preprocessing\n    * 2.4 Splitting\n* **3. CNN**\n    * 3.1 Defining the model\n    * 3.2 Compiling the model with the right Optimizer, loss and metric.\n    * 3.3 Training the model\n* **4. Evaluation**\n    * 4.1 Evaluating the model\n    * 4.2 Visualizing the model's performance\n* **5. Prediction and submission**","6431c931":"### Preprocessing\nTake the 'labels' column out and use .describe() to have some insight on the data.","f4623a03":"As we know, the images come in a grayscale format where all the values are between (0-255), a good thing you should do is standarize the data, which makes it easier for the model to converge. <br \/>\nStandarization transforms the data in a way that scales all the values between (0-1), you can do this easily by dividing all values by 255 since our values come in a (0-255) range"}}