{"cell_type":{"60b0f55a":"code","4fe2e1ed":"code","53a23e76":"code","c815b5a0":"code","efce7479":"code","c627fca7":"code","ff40b51b":"code","7c68a9db":"code","9053bd38":"code","97ad8864":"code","4b6c9c59":"code","a5e2c3bf":"code","2a55267b":"code","99a34d6e":"code","f5ab177c":"code","a7b4cf78":"code","bcca4bab":"code","5e589fef":"code","9034705f":"code","afe6a848":"code","78561ab4":"code","c7dbf337":"code","b13fbd23":"code","99add48b":"code","57977ee1":"code","c9c14696":"code","95cce1b5":"code","634ff6dc":"code","f862e775":"code","59e173bb":"code","1a9f96c6":"markdown","d3c465f5":"markdown","dffebb9c":"markdown","aaa9b8f8":"markdown","1f9b57db":"markdown","0c5d4f05":"markdown","91badadd":"markdown","0da20a64":"markdown","d3d2c50a":"markdown","59930627":"markdown"},"source":{"60b0f55a":"import torch\nimport random\nimport numpy as np\n\ndef seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n\nseed_all(2020)","4fe2e1ed":"!pip install transformers","53a23e76":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","c815b5a0":"def tokenize_and_cut(sentence):\n    tokens = tokenizer.tokenize(sentence)\n    max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n    tokens = tokens[:max_input_length-2]\n    return tokens","efce7479":"from torchtext import data\n\nTEXT = data.Field(batch_first = True,\n                  use_vocab = False,\n                  tokenize = tokenize_and_cut,\n                  preprocessing = tokenizer.convert_tokens_to_ids,\n                  init_token = tokenizer.cls_token_id,\n                  eos_token = tokenizer.sep_token_id,\n                  pad_token = tokenizer.pad_token_id,\n                  unk_token = tokenizer.unk_token_id)\n\nLABEL = data.Field(sequential=False, use_vocab=False)","c627fca7":"!mkdir .\/data\n!unzip -d .\/data ..\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip\n!unzip -d .\/data ..\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip","ff40b51b":"from torchtext import datasets\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\ntrain = pd.read_csv('.\/data\/train.tsv', sep='\\t')\ntest = pd.read_csv('.\/data\/test.tsv', sep='\\t')\ntrain, valid = train_test_split(train, test_size=0.2)\ntrain.to_csv('.\/data\/train.csv', index=False)\nvalid.to_csv('.\/data\/validation.csv', index=False)\n\ntrain, valid = data.TabularDataset.splits(\n    path='.\/data', train='train.csv', validation='validation.csv', format='csv', skip_header=True,\n    fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT), ('Sentiment', LABEL)])\ntest = data.TabularDataset('.\/data\/test.tsv', format='tsv', skip_header=True,\n                           fields=[('PhraseId', None), ('SentenceId', None), ('Phrase', TEXT)])","7c68a9db":"print(f\"Number of training examples: {len(train)}\")\nprint(f\"Number of validation examples: {len(valid)}\")\nprint(f\"Number of testing examples: {len(test)}\")","9053bd38":"print(vars(train[6]))","97ad8864":"print(tokenizer.convert_ids_to_tokens(vars(train[6])['Phrase']))","4b6c9c59":"BATCH_SIZE = 256\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iter = data.BucketIterator(train, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\nvalid_iter = data.BucketIterator(valid, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\ntest_iter = data.Iterator(test, batch_size=BATCH_SIZE, train=False, sort=False, device=DEVICE)","a5e2c3bf":"batch = next(iter(train_iter))\nphrase = batch.Phrase\nsent = batch.Sentiment\nprint(phrase.shape)\nprint(phrase)\nprint(sent.shape)\nprint(sent)","2a55267b":"from transformers import BertModel\n\nbert = BertModel.from_pretrained('bert-base-uncased')","99a34d6e":"from torch import nn\n\nclass BERTGRUSentiment(nn.Module):\n    def __init__(self, bert, output_dim):\n        super().__init__()\n        self.bert = bert\n        self.embedding_dim = bert.config.to_dict()['hidden_size']\n        self.gru11 = nn.GRU(self.embedding_dim, 512, num_layers=1, batch_first=True)\n        self.gru12 = nn.GRU(512, 256, num_layers=1, batch_first=True)\n        self.gru13 = nn.GRU(256, 128, num_layers=1, batch_first=True)\n        self.gru21 = nn.GRU(self.embedding_dim, 512, num_layers=1, batch_first=True)\n        self.gru22 = nn.GRU(512, 256, num_layers=1, batch_first=True)\n        self.gru23 = nn.GRU(256, 128, num_layers=1, batch_first=True)\n\n        self.fc = nn.Linear(256, output_dim)\n        \n        self.dropout1 = nn.Dropout(0.5)\n        self.dropout2 = nn.Dropout(0.3)\n\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    def forward(self, text):\n        # text = [batch size, sent len]\n\n        with torch.no_grad():\n            embedding = self.bert(text)[0]  # embedding = [batch size, sent len, emb dim]\n\n        output1, _ = self.gru11(embedding)\n        output1 = self.dropout1(output1)  # output1 = [batch size, sent len, 512]\n        \n        output1, _ = self.gru12(output1)\n        output1 = self.dropout2(output1)  # output1 = [batch size, sent len, 256]\n        \n        _, hidden1 = self.gru13(output1)  # hidden1 = [1, batch size, 128]\n\n        reversed_embedding = torch.from_numpy(embedding.detach().cpu().numpy()[:, ::-1, :].copy()).to(self.DEVICE)\n        \n        output2, _ = self.gru21(reversed_embedding)\n        output2 = self.dropout1(output2)  # output2 = [batch size, sent len, 512]\n        \n        output2, _ = self.gru22(output2)\n        output2 = self.dropout2(output2)  # output1 = [batch size, sent len, 256]\n        \n        _, hidden2 = self.gru23(output2)  # hidden2 = [1, batch size, 128]\n        \n        hidden = self.dropout2(torch.cat((hidden1[-1, :, :], hidden2[-1, :, :]), dim=1))  # hidden = [batch size, 256]\n\n        output = self.fc(hidden)  # output = [batch size, out dim]\n\n        return output","f5ab177c":"OUTPUT_DIM = 5\n\nmodel = BERTGRUSentiment(bert, OUTPUT_DIM)","a7b4cf78":"# import torch.nn as nn\n\n# class BERTGRUSentiment(nn.Module):\n#     def __init__(self, bert, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n#         super().__init__()\n#         self.bert = bert\n#         embedding_dim = bert.config.to_dict()['hidden_size']\n#         self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional,\n#                           batch_first = True, dropout = 0 if n_layers < 2 else dropout)\n#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n#         self.dropout = nn.Dropout(dropout)\n        \n#     def forward(self, text):\n#         # text = [batch size, sent len]\n        \n#         with torch.no_grad():\n#             embedding = self.bert(text)[0]  # embedding = [batch size, sent len, emb dim]\n\n#         _, hidden = self.gru(embedding)  # hidden = [n layers * n directions, batch size, hid dim]\n        \n#         if self.gru.bidirectional:\n#             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n#         else:\n#             hidden = self.dropout(hidden[-1,:,:])\n                \n#         # hidden = [batch size, hid dim]\n        \n#         output = self.fc(hidden)\n        \n#         # output = [batch size, out dim]\n        \n#         return output\n\n# HIDDEN_DIM = 256\n# OUTPUT_DIM = 5\n# N_LAYERS = 2\n# BIDIRECTIONAL = True\n# DROPOUT = 0.3\n\n# model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)","bcca4bab":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","5e589fef":"for name, param in model.named_parameters():                \n    if name.startswith('bert'):\n        param.requires_grad = False\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","9034705f":"for name, param in model.named_parameters():                \n    if param.requires_grad:\n        print(name)","afe6a848":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())","78561ab4":"criterion = nn.CrossEntropyLoss()","c7dbf337":"model = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)","b13fbd23":"import numpy as np\n\ndef accuracy(prediction, label):\n    \"\"\"\n    Returns accuracy per batch, i.e. if you get 8\/10 right, this returns 0.8, NOT 8\n    \"\"\"\n    prediction = torch.argmax(nn.functional.softmax(prediction, dim=1), dim=1)\n    acc = torch.sum(prediction == label).float() \/ len(prediction == label)\n    return acc","99add48b":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n        \n        data = batch.Phrase\n        label = batch.Sentiment\n        \n        prediction = model(data)\n        \n        loss = criterion(prediction, label)\n        \n        acc = accuracy(prediction, label)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","57977ee1":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            data = batch.Phrase\n            label = batch.Sentiment\n            \n            prediction = model(data)\n            \n            loss = criterion(prediction, label)\n            \n            acc = accuracy(prediction, label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","c9c14696":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","95cce1b5":"N_EPOCHS = 10\n\nbest_epoch = 0\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iter, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if valid_loss < best_valid_loss:\n        best_epoch = epoch + 1\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","634ff6dc":"from torchtext import data\ntrain_full = data.TabularDataset('.\/data\/train.tsv', format='tsv', skip_header=True,\n                                 fields=[('PhraseId', None), ('SentenceId', None),\n                                         ('Phrase', TEXT), ('Sentiment', LABEL)])\n\ntrain_full_iter = data.BucketIterator(train_full, batch_size=BATCH_SIZE, shuffle=True, device=DEVICE)\n\nmodel = BERTGRUSentiment(bert, OUTPUT_DIM)\n\noptimizer = optim.Adam(model.parameters())\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = model.to(DEVICE)\ncriterion = criterion.to(DEVICE)\n\nfor epoch in range(best_epoch + 1):\n    \n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_full_iter, optimizer, criterion)\n        \n    end_time = time.time()\n        \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n        \n    if epoch == best_epoch:\n        torch.save(model.state_dict(), 'model_full.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')","f862e775":"def predict(model, iterator):\n    \n    model.eval()\n    \n    predictions = []\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            data = batch.Phrase\n            \n            prediction = model(data)\n            \n            prediction = torch.argmax(nn.functional.softmax(prediction, dim=1), dim=1)\n            \n            predictions.extend(prediction.tolist())\n        \n    return predictions","59e173bb":"predictions = predict(model, test_iter)\n\nsubmission = pd.read_csv('..\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv')\nsubmission['Sentiment'] = predictions\nsubmission.to_csv('submissionBERTGRU.csv', index=False)","1a9f96c6":"A single input and target pair is stored as a `torchtext.data.Example` object in the structured dataset. Check an example to ensure things go right.","d3c465f5":"Set `requires_grad = False` for all parameters in `bert` model, so no fine-tuning on `bert`.","dffebb9c":"## 2 Build the Model","aaa9b8f8":"To simplify the case, I drop attribute `PhraseId` and `SentenceId` in the dataset, which might contain useful information.","1f9b57db":"## 3 Train the Model","0c5d4f05":"## 1 Prepare Data","91badadd":"Have a look at a mini-batch.","0da20a64":"# BERT for Sentiment Analysis\nIn this notebook, I will use the [BERT](https:\/\/arxiv.org\/abs\/1810.04805) model to accomplish the classification task on movie reviews. The idea is pretty simple, to regard BERT as an embedding layer and then pass through the result to a 3-layer bidirectional GRU further processed by a linear layer and softmax function.\n\nBERT model are considerably large, so it is impossible to train a BERT from the very begining in this notebook. Fortunately, [transformers library](https:\/\/github.com\/huggingface\/transformers) offers a pre-trained BERT model with well-maintained documentation. We can take advantage of it.\n\nThe pipeline of building a BERT model in this notebook mainly follows a [tutorial](github.com\/bentrevett\/pytorch-sentiment-analysis\/blob\/master\/6%20-%20Transformers%20for%20Sentiment%20Analysis.ipynb) on pytorch and sentiment analysis. You can find more detials on how to build simple model based on BERT by clicking on the link above.","d3d2c50a":"None of Pytorch RNN modules support different hidden size across different layers, so I need to implement it by hand.","59930627":"Train the model on entire training set again before submission. Since the training set is larger, one more epoch is assigned."}}