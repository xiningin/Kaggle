{"cell_type":{"0010bc74":"code","50589a49":"code","c402338d":"code","b9d9935d":"code","3fbb1fa7":"code","6e65e06b":"code","52331a86":"code","39cd7406":"code","0c1061fd":"code","56cc1a66":"code","1d7b8e84":"code","67d10c3f":"code","03aedac4":"code","d1686d77":"code","b0842bc8":"code","bb974c8a":"code","f1de61e0":"code","fe7461ba":"code","d774f5d5":"code","d6d95f83":"code","a8f1a845":"code","174ae621":"code","192632ad":"markdown","f8db906b":"markdown","d2bc531c":"markdown","d0d597d7":"markdown","72ba332f":"markdown","8bbf47d7":"markdown","842bec37":"markdown","31e39782":"markdown","f88703f4":"markdown"},"source":{"0010bc74":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport optuna\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","50589a49":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","c402338d":"train.head()","b9d9935d":"print('Training data shape (rows, cols): ', train.shape)","3fbb1fa7":"train.info()","6e65e06b":"train.describe()","52331a86":"print('Total null count: ', train.isnull().sum().sum())","39cd7406":"train.isnull().sum().sort_values(ascending = False)","0c1061fd":"# Target Distribution (0 or 1)\ndist_class = train['claim'].value_counts()\nlabels = ['0', '1']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nsns.barplot(x=dist_class.index, y=dist_class, ax=ax1).set_title(\"Target Count\")\n\nax2.pie(dist_class,\n        labels=labels,\n        counterclock=False,\n        startangle=90,\n        autopct='%1.1f%%',\n        pctdistance=0.7)\nplt.title(\"Target Frequency Proportion\")\nplt.show","56cc1a66":"features = [x for x in train.columns.values if x[0]==\"f\"]","1d7b8e84":"train['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)\n\ntrain['abs_sum'] = train[features].abs().sum(axis=1)\ntest['abs_sum'] = test[features].abs().sum(axis=1)\n\ntrain['sem'] = train[features].sem(axis=1)\ntest['sem'] = test[features].sem(axis=1)\n\ntrain['std'] = train[features].std(axis=1)\ntest['std'] = test[features].std(axis=1)\n\ntrain['avg'] = train[features].mean(axis=1)\ntest['avg'] = test[features].mean(axis=1)\n\ntrain['max'] = train[features].max(axis=1)\ntest['max'] = test[features].min(axis=1)\n\ntrain['min'] = train[features].min(axis=1)\ntest['min'] = test[features].min(axis=1)","67d10c3f":"X = train.drop(['id', 'claim'], axis = 1)\ny = train['claim']","03aedac4":"# def objective(trial,data=X,target=y):\n    \n#     X_train, X_valid, y_train, y_valid = train_test_split(data, target, train_size=0.8, test_size=0.2,random_state=0)\n    \n#     imputer = SimpleImputer(strategy='median')\n#     X_train = imputer.fit_transform(X_train)\n#     X_valid = imputer.transform(X_valid)\n    \n#     scaler = RobustScaler()\n#     X_train = scaler.fit_transform(X_train)\n#     X_valid = scaler.transform(X_valid)\n    \n#     params = {\n#         'objective': 'binary',\n#         'metric': 'auc', \n#         'boosting_type': 'gbdt',\n#         'n_estimators': 1000,\n#         'random_state': 42,\n#         'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.008,0.01,0.014,0.017,0.02]),\n#         'subsample': trial.suggest_loguniform('subsample', 0.4, 1.0),\n#         'subsample_freq': trial.suggest_loguniform('subsample_freq', 0.4, 1.0),\n#         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.4, 1.0),\n#         'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n#         'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n#         'min_child_weight': trial.suggest_int('min_child_weight', 5, 256),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n#     }\n#     model = lgb.LGBMClassifier(**params) \n#     model.fit(X_train, y_train)\n    \n#     preds = model.predict(X_valid)\n#     auc = roc_auc_score(y_valid, preds)\n    \n#     return auc","d1686d77":"# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)","b0842bc8":"# study.best_trial.params","bb974c8a":"# lgb_params = {\n#     'objective': 'binary',\n#     'metric': 'auc', \n#     'boosting_type': 'gbdt',\n#     'n_estimators': 1000,\n#     'random_state': 42,\n#     'learning_rate': 0.02,\n#     'subsample': 0.8751761372035946,\n#     'subsample_freq': 0.43935171514346294,\n#     'colsample_bytree': 0.675151285253419,\n#     'reg_alpha': 5.536980361906913,\n#     'reg_lambda': 0.3142777516202206,\n#     'min_child_weight': 201,\n#     'min_child_samples': 9,\n#     'bagging_fraction': 0.9678154285091293,\n#     'bagging_freq': 6\n# }\n\n# based on study.best_trial.params...\nlgb_params = {\n    'objective': 'binary',\n    'metric': 'auc', \n    'boosting_type': 'gbdt',\n    'n_estimators': 1000,\n    'random_state': 42,\n    'learning_rate': 0.02,\n    'subsample': 0.46366427250815384,\n    'subsample_freq': 0.9961802289581205,\n    'colsample_bytree': 0.6157140152844784,\n    'reg_alpha': 2.657030645814501,\n    'reg_lambda': 0.015450253262708286,\n    'min_child_weight': 137,\n    'min_child_samples': 54,\n    'bagging_fraction': 0.4169496814206163,\n    'bagging_freq': 1\n}","f1de61e0":"model = lgb.LGBMClassifier(**lgb_params)\nmodel","fe7461ba":"X_test = test.drop(['id'], axis = 1)","d774f5d5":"splits = 5\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_auc = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    imputer = SimpleImputer(strategy='median')\n    X_train = imputer.fit_transform(X_train)\n    X_valid = imputer.transform(X_valid)\n    \n    scaler = RobustScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    \n    model.fit(X_train, y_train,\n              verbose=False,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"auc\",\n              early_stopping_rounds=300,\n              )\n    \n    X_test = imputer.transform(X_test)\n    X_test = scaler.transform(X_test)\n    \n    preds += model.predict_proba(X_test)[:, 1] \/ splits\n    model_fi += model.feature_importances_ \/ splits\n    \n    oof_preds[valid_idx] = model.predict_proba(X_valid)[:, 1]\n    \n    fold_auc = roc_auc_score(y_valid, oof_preds[valid_idx])\n    print(f\"Fold {num} ROC AUC: {fold_auc}\")\n\n    total_mean_auc += fold_auc \/ splits\n    \nprint(f\"\\nOverall ROC AUC: {total_mean_auc}\")","d6d95f83":"importance = pd.DataFrame(model.feature_importances_, index=X.columns, columns=['importance'])\nimportance = importance.sort_values('importance', ascending=False)\nimportance","a8f1a845":"submission.claim = preds\nsubmission.head()","174ae621":"submission.to_csv('submission.csv', index=False)","192632ad":"## Feature Importance","f8db906b":"## Exploratory Data Analysis (EDA)","d2bc531c":"## If you like this kernel, please upvote:)","d0d597d7":"## Model Training & Evaluation","72ba332f":"## Make Submission","8bbf47d7":"## Hyperparameter Tuning using Optuna\n\nHyperparameter tuning is time-consuming, so it is commented out...","842bec37":"## LGBMClassifier","31e39782":"## References\n- https:\/\/www.kaggle.com\/dwin183287\/tps-september-2021-eda\n- https:\/\/www.kaggle.com\/realtimshady\/lightgbm-2-0\n- https:\/\/www.kaggle.com\/mohammadkashifunique\/hyperparameter-tuning-lgbm-optuna\n- https:\/\/www.kaggle.com\/maximkazantsev\/tps-09-21-eda-lightgbm-with-folds","f88703f4":"## Feature Engineering (Data Pre-Processing?)"}}