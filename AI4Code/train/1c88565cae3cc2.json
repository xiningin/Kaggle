{"cell_type":{"32e1ae7a":"code","0455b6de":"code","f162a13b":"code","5c30f095":"code","3875b290":"code","455d421b":"code","9104f557":"code","5cf89966":"code","419e46b6":"code","08f5786e":"code","8e19f96b":"code","f1c871c4":"code","eb8a57f8":"code","f17ece20":"code","a91a2f16":"code","d0a9564f":"code","ce4fbf9a":"code","5673845d":"code","476383b9":"code","1c460877":"code","821ebc73":"code","5881426c":"code","5b9617eb":"code","516f87a3":"code","ac23bc08":"markdown","76e2b387":"markdown","6dcbff76":"markdown","c78a0d1d":"markdown","f9af5aee":"markdown","0eeed7cc":"markdown","e321f9f5":"markdown","9be423d5":"markdown","06d878a8":"markdown","00a5cf61":"markdown","02e6d6c2":"markdown","59c8818c":"markdown","6c194c0b":"markdown","feba14b8":"markdown","8c90dcd1":"markdown"},"source":{"32e1ae7a":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nimport lightgbm as lgbm\nimport catboost as cb\nfrom sklearn.ensemble import AdaBoostClassifier\n \n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nimport statsmodels.api as sm\nimport os\nimport random\n\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","0455b6de":"df = pd.read_csv('..\/input\/music-genre-classification\/dataset.csv')\ndf.head()","f162a13b":"df.info()","5c30f095":"for i in ['label']:\n    print(df[i].unique())","3875b290":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['label'] = le.fit_transform(df['label'])\ndf = df.drop('filename', axis = 1)\ndf.head()","455d421b":"### Thanks for the function https:\/\/www.kaggle.com\/ankitak46\n\ndef remove_outliers(data):\n    arr=[]\n    #print(max(list(data)))\n    q1=np.percentile(data,25)\n    q3=np.percentile(data,75)\n    iqr=q3-q1\n    mi=q1-(1.5*iqr)\n    ma=q3+(1.5*iqr)\n    #print(mi,ma)\n    for i in list(data):\n        if i<mi:\n            i=mi\n            arr.append(i)\n        elif i>ma:\n            i=ma\n            arr.append(i)\n        else:\n            arr.append(i)\n    #print(max(arr))\n    return arr","9104f557":"df['chroma_stft'] = remove_outliers(df['chroma_stft'])\ndf['rmse'] = remove_outliers(df['rmse'])\ndf['spectral_centroid'] = remove_outliers(df['spectral_centroid'])\ndf['spectral_bandwidth'] = remove_outliers(df['spectral_bandwidth'])\ndf['rolloff'] = remove_outliers(df['rolloff'])\ndf['zero_crossing_rate'] = remove_outliers(df['zero_crossing_rate'])\ndf['mfcc1'] = remove_outliers(df['mfcc1'])\ndf['mfcc2'] = remove_outliers(df['mfcc2'])\ndf['mfcc3'] = remove_outliers(df['mfcc3'])\ndf['mfcc4'] = remove_outliers(df['mfcc4'])\ndf['mfcc5'] = remove_outliers(df['mfcc5'])\ndf['mfcc6'] = remove_outliers(df['mfcc6'])\ndf['mfcc7'] = remove_outliers(df['mfcc7'])\ndf['mfcc8'] = remove_outliers(df['mfcc8'])\ndf['mfcc9'] = remove_outliers(df['mfcc9'])\ndf['mfcc10'] = remove_outliers(df['mfcc10'])\ndf['mfcc11'] = remove_outliers(df['mfcc11'])\ndf['mfcc12'] = remove_outliers(df['mfcc12'])\ndf['mfcc13'] = remove_outliers(df['mfcc13'])\ndf['mfcc14'] = remove_outliers(df['mfcc14'])\ndf['mfcc15'] = remove_outliers(df['mfcc15'])\ndf['mfcc16'] = remove_outliers(df['mfcc16'])\ndf['mfcc17'] = remove_outliers(df['mfcc17'])\ndf['mfcc18'] = remove_outliers(df['mfcc18'])\ndf['mfcc19'] = remove_outliers(df['mfcc19'])\ndf['mfcc20'] = remove_outliers(df['mfcc20'])\n\nprint('Outliers successfully removed')","5cf89966":"X = df.drop('label', axis = 1)\ny = df.label","419e46b6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","08f5786e":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","8e19f96b":"models = [LogisticRegression(),LinearSVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier(),GaussianNB()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(X_train,y_train)\n    pred=clf.predict(X_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd","f1c871c4":"acc_frame=pd.DataFrame(d)\nacc_frame.sort_values(by = 'Accuracy', ascending = False)","eb8a57f8":"sns.barplot(y='Modelling Algo',x='Accuracy',data=acc_frame.sort_values(by = 'Accuracy', ascending = False))\n","f17ece20":"sns.factorplot(x='Modelling Algo',y='Accuracy',data=acc_frame.sort_values(by = 'Accuracy', ascending = False),kind='point',size=4,aspect=3.5)\n","a91a2f16":"cross_valid_scores = {}","d0a9564f":"%%time\nparameters = {\n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_desicion_tree = DecisionTreeClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_desicion_tree = GridSearchCV(\n    model_desicion_tree, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_desicion_tree.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_desicion_tree.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \\\n    f'{model_desicion_tree.best_score_:.3f}'\n)\ncross_valid_scores['desicion_tree'] = model_desicion_tree.best_score_\nprint('-----')","ce4fbf9a":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25], \n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n}\n\nmodel_random_forest = RandomForestClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_random_forest = GridSearchCV(\n    model_random_forest, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_random_forest.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_random_forest.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_random_forest.best_score_:.3f}'\n)\ncross_valid_scores['random_forest'] = model_random_forest.best_score_\nprint('-----')","5673845d":"%%time\nparameters = {\n    \"n_estimators\": [5, 10, 15, 20, 25, 50, 75, 100], \n    \"learning_rate\": [0.001, 0.01, 0.1, 1.],\n}\n\nmodel_adaboost = AdaBoostClassifier(\n    random_state=42,\n)\n\nmodel_adaboost = GridSearchCV(\n    model_adaboost, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_adaboost.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_adaboost.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: '+ \\\n    f'{model_adaboost.best_score_:.3f}'\n)\ncross_valid_scores['ada_boost'] = model_adaboost.best_score_\nprint('-----')","476383b9":"%%time\nparameters = {\n    'max_depth': [3, 5, 7, 9], \n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1]\n}\n\nmodel_xgb = xgb.XGBClassifier(\n    random_state=42, verbosity = 0\n)\n\nmodel_xgb = GridSearchCV(\n    model_xgb, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_xgb.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_xgb.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_xgb.best_score_:.3f}'\n)\ncross_valid_scores['xgboost'] = model_xgb.best_score_\nprint('-----')","1c460877":"%%time\nparameters = {\n    'n_estimators': [5, 10, 15, 20, 25, 50, 100],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [7, 15, 31],\n}\n\nmodel_lgbm = lgbm.LGBMClassifier(\n    random_state=42,\n    class_weight='balanced',\n)\n\nmodel_lgbm = GridSearchCV(\n    model_lgbm, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_lgbm.fit(\n    X_train, \n    y_train,\n)\n\nprint('-----')\nprint(f'Best parameters {model_lgbm.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_lgbm.best_score_:.3f}'\n)\ncross_valid_scores['lightgbm'] = model_lgbm.best_score_\nprint('-----')\n","821ebc73":"%%time\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1.],\n    \"penalty\": [\"l1\", \"l2\"]\n}\n\nmodel_logistic_regression = LogisticRegression(\n    random_state=42,\n    class_weight=\"balanced\",\n    solver=\"liblinear\",\n)\n\nmodel_logistic_regression = GridSearchCV(\n    model_logistic_regression, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_logistic_regression.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_logistic_regression.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_logistic_regression.best_score_:.3f}'\n)\ncross_valid_scores['logistic_regression'] = model_logistic_regression.best_score_\nprint('-----')","5881426c":"%%time\nparameters = {\n    \"C\": [0.001, 0.01, 0.1, 1.],\n    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n    \"gamma\": [\"scale\", \"auto\"],\n}\n\nmodel_svc = SVC(\n    random_state=42,\n    class_weight=\"balanced\",\n    probability=True,\n)\n\nmodel_svc = GridSearchCV(\n    model_svc, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_svc.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_svc.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_svc.best_score_:.3f}'\n)\ncross_valid_scores['svc'] = model_svc.best_score_\nprint('-----')","5b9617eb":"%%time\nparameters = {\n    \"weights\": [\"uniform\", \"distance\"],\n}\n\nmodel_k_neighbors = KNeighborsClassifier(\n)\n\nmodel_k_neighbors = GridSearchCV(\n    model_k_neighbors, \n    parameters, \n    cv=5,\n    scoring='accuracy',\n)\n\nmodel_k_neighbors.fit(X_train, y_train)\n\nprint('-----')\nprint(f'Best parameters {model_k_neighbors.best_params_}')\nprint(\n    f'Mean cross-validated accuracy score of the best_estimator: ' + \n    f'{model_k_neighbors.best_score_:.3f}'\n)\ncross_valid_scores['k_neighbors'] = model_k_neighbors.best_score_\nprint('-----')","516f87a3":"submit = pd.DataFrame(cross_valid_scores, index=['cross_valid_score']).T\nround(submit.sort_values(by = 'cross_valid_score', ascending = False),3)","ac23bc08":"## XGBoost","76e2b387":"# GRID SEARCH","6dcbff76":"## Adaboost","c78a0d1d":"# REMOVE OUTLIERS","f9af5aee":"## SVC","0eeed7cc":"# IMPORT LIBS","e321f9f5":"## LightGBM","9be423d5":"## Logistic Regression","06d878a8":"# MODEL","00a5cf61":"## KNN","02e6d6c2":"## Decision Tree","59c8818c":"# UPDATE 14.03.21","6c194c0b":"# DATA LOADING AND OVERVIEW","feba14b8":"# Thanks for watching!\n\n# If you liked my fork then upvoted or write your opinion","8c90dcd1":"## Random Forest"}}