{"cell_type":{"85090447":"code","f9975ec8":"code","de38b552":"code","8a12897e":"code","fd52067b":"code","dbc4e9e2":"code","dbc6714e":"code","c2ca339b":"code","008ea3f5":"code","56a4057e":"code","151bf8e0":"code","ecf817ed":"code","166c0ecd":"code","2e911140":"code","2c1859ac":"code","f1326b96":"code","9842ac00":"code","0fe41ef4":"code","129a4071":"code","004d956f":"code","c40c8750":"code","e030aefb":"code","dba76d21":"code","5c9c6d23":"markdown","326665b4":"markdown","786c8dca":"markdown","146974b4":"markdown"},"source":{"85090447":"!pip install transformers==3.0.0","f9975ec8":"import torch\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, BertTokenizerFast","de38b552":"torch.cuda.is_available()","8a12897e":"df = pd.read_csv('..\/input\/chinese-official-daily-news-since-2016\/chinese_news.csv')\ndf['text'] = df['headline'] + '. '+ df['content']\ndf = df[['text','tag']]\ndf['tag'] = df['tag'].map({'\u56fd\u5185' : 0, '\u56fd\u9645' : 1, '\u8be6\u7ec6\u5168\u6587' : 2})\ndf.dropna()\ndf.head()","fd52067b":"x_train, x_test, y_train, y_test = train_test_split(df['text'], df['tag'],\n                                                    stratify=df['tag'])","dbc4e9e2":"bert = AutoModel.from_pretrained('bert-base-chinese')\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')","dbc6714e":"train_idx = x_train.dropna().index\ntest_idx = x_test.dropna().index\n\ntrain_tokens = tokenizer.batch_encode_plus(x_train[train_idx].to_list(),\n                                           max_length = 50,\n                                           pad_to_max_length = True,\n                                           truncation = True)\ntest_tokens = tokenizer.batch_encode_plus(x_test[test_idx].to_list(),\n                                           max_length = 50,\n                                           pad_to_max_length = True,\n                                           truncation = True)","c2ca339b":"train_seq = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\ntrain_y = torch.tensor(y_train[train_idx].to_list())\n\ntest_seq = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\ntest_y = torch.tensor(y_test[test_idx].to_list())","008ea3f5":"from torch.utils.data import TensorDataset, RandomSampler, DataLoader","56a4057e":"train_data = TensorDataset(train_seq, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrainloader = DataLoader(train_data, \n                         sampler = train_sampler,\n                         batch_size = 32)\n\ntest_data = TensorDataset(test_seq, test_mask, test_y)\ntest_sampler = RandomSampler(test_data)\ntestloader = DataLoader(test_data, \n                         sampler = test_sampler,\n                         batch_size = 32)","151bf8e0":"for param in bert.parameters():\n    param.requires_grad = False","ecf817ed":"from torch import nn\nfrom transformers import AdamW\nimport torch.nn.functional as F\nfrom sklearn.utils.class_weight import compute_class_weight","166c0ecd":"class BertClassifier(nn.Module):\n    def __init__(self, bert):\n        super().__init__()\n        self.bert = bert\n        self.fc1 = nn.Linear(768,3)\n    \n    def forward(self, sent_id, mask):\n        _ , cls_hs = self.bert(sent_id, attention_mask = mask)\n        return F.log_softmax(self.fc1(cls_hs), dim = 1)","2e911140":"model = BertClassifier(bert)\nmodel = model.cuda()","2c1859ac":"optimizer = AdamW(model.parameters(), lr = 1e-5)","f1326b96":"class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights","9842ac00":"weights = torch.tensor(class_weights, dtype = torch.float)\nweights = weights.cuda()\n\ncriterion = nn.NLLLoss(weight = weights)","0fe41ef4":"from tqdm.notebook import tqdm","129a4071":"epochs = 10\n\nfor e in range(epochs):   \n    train_loss = 0.0\n    for batch in tqdm(trainloader):\n        batch = [i.cuda() for i in batch]\n        sent_id, masks, labels = batch\n\n        optimizer.zero_grad()\n        preds = model(sent_id, masks)\n        loss = criterion(preds, labels)\n        train_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n    print(f'Epoch:{e+1}\\t\\tTraining Loss: {train_loss \/ len(trainloader)}')","004d956f":"pred_label = []\ntrue_label = []\nfor batch in tqdm(testloader):\n    batch = [i.cuda() for i in batch]\n    sent_id, masks, labels = batch\n\n    preds = model(sent_id, masks)\n    pred_label.extend(torch.argmax(preds, axis = 1).cpu())\n    true_label.extend(labels.cpu())","c40c8750":"from sklearn.metrics import confusion_matrix, classification_report","e030aefb":"confusion_matrix(true_label, pred_label)","dba76d21":"print(classification_report(true_label, pred_label))","5c9c6d23":"# Fine Tuning our model","326665b4":"# Making our Model","786c8dca":"# Extracting Data and combining headline and content","146974b4":"# Preparing Data for Model"}}