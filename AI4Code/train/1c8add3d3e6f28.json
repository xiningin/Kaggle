{"cell_type":{"de787c25":"code","fccb8109":"code","d4e64428":"code","743f946a":"code","f4f4e182":"code","0005b4cb":"code","c2460178":"code","af748171":"code","8c83b7ba":"code","ba592882":"code","67f2a612":"code","93c5f312":"code","c7558df8":"code","5a177bc0":"code","4955a8ec":"code","ac3c9103":"code","943b3ef0":"code","7cf03de4":"code","13278c45":"code","f82feb5e":"code","83e2b17a":"markdown","205f83d8":"markdown","78792d34":"markdown","65f918fd":"markdown","cb6c2bfc":"markdown","bc770d64":"markdown"},"source":{"de787c25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file \nimport matplotlib.pyplot as plt # visualizations\nimport seaborn as sns # visualizations\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nfrom tqdm.notebook import tqdm_notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport re\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline","fccb8109":"pd.get_option(\"display.max_columns\", 200)\npd.set_option(\"display.max_rows\", 200)","d4e64428":"transactions = pd.read_csv('..\/input\/gender-test-s\/transactions.csv')\ntr_types = pd.read_csv('..\/input\/gender-test-s\/tr_types.csv', sep=';')\ntr_mcc_codes = pd.read_csv('..\/input\/gender-test-s\/tr_mcc_codes.csv', sep=';')\ngender_train = pd.read_csv('..\/input\/gender-test-s\/gender_train.csv')\ngender_test = pd.read_csv('..\/input\/gender-test-s\/gender_test.csv')","743f946a":"# extracting day, hour, is night from transaction datetime\n\nfor df in [transactions]:\n    df['day'] = df['tr_datetime'].str.split().apply(lambda x: int(x[0]) % 7)\n    df['hour'] = df['tr_datetime'].apply(lambda x: re.search(' \\d*', x).group(0)).astype(int)\n    df['night'] = ~df['hour'].between(5, 23).astype(int)\n","f4f4e182":"# separating transaction amount into 2 groups - income if transaction is positive\n# and spent if transaction is negative\n\ntransactions.loc[transactions['amount'] > 0, 'income'] = transactions['amount']\ntransactions.loc[transactions['amount'] <= 0, 'spent'] = transactions['amount']","0005b4cb":"# here I found stat metrics for each customer on their income and spents\n# find std for income and spent for each customer\n\nstd_trans  = transactions.groupby(['customer_id']).std()\nstd_trans.rename(columns={'income': 'std_income', 'spent': 'std_spent'}, inplace=True)\n\n# find minimum income and maximum spent for each customer\n\nmin_trans = transactions.groupby(['customer_id']).min()\nmin_trans.rename(columns={'income': 'min_income', 'spent': 'max_spent'}, inplace=True)\n\n# find maximum income and minimum spent for each customer\n\nmax_trans = transactions.groupby(['customer_id']).max()\nmax_trans.rename(columns={'income': 'max_income', 'spent': 'min_spent'}, inplace=True)\n\n# find mean income and mean spent for each customer\n\nmean_trans = transactions.groupby(['customer_id']).mean()\nmean_trans.rename(columns={'income': 'mean_income', 'spent': 'mean_spent'}, inplace=True)\n\n# find median income and median spent for each customer\nmedian_trans = transactions.groupby(['customer_id']).median()\nmedian_trans.rename(columns={'income': 'median_income', 'spent': 'median_spent'}, inplace=True)\n\n# find sum income and sum spent for each customer\nsum_trans = transactions.groupby(['customer_id']).sum()\nsum_trans.rename(columns={'income': 'sum_income', 'spent': 'sum_spent'}, inplace=True)\n\n# find count income and count spent for each customer\ncount_trans = transactions.groupby(['customer_id']).count()\ncount_trans.rename(columns={'income': 'count_income', 'spent': 'count_spent'}, inplace=True)\n","c2460178":"res = min_trans[['min_income', 'max_spent']]\nres_2 = max_trans[['max_income', 'min_spent']]\nres_3 = mean_trans[['mean_income', 'mean_spent']]\nres_4 = std_trans[['std_income', 'std_spent']]\nres_5 = median_trans[['median_income', 'median_spent']]\nres_6 = sum_trans[['sum_income', 'sum_spent']]\nres_7 = count_trans[['count_income', 'count_spent']]\n\ntransactions = transactions.join(res, on='customer_id')\ntransactions = transactions.join(res_2, on='customer_id')\ntransactions = transactions.join(res_3, on='customer_id')\ntransactions = transactions.join(res_4, on='customer_id')\ntransactions = transactions.join(res_5, on='customer_id')\ntransactions = transactions.join(res_6, on='customer_id')\ntransactions = transactions.join(res_7, on='customer_id')","af748171":"transactions.sort_values(['customer_id']).head()","8c83b7ba":"def features_creation(x): \n    # create columns for each mcc, tr, day, hour and night\n    \n    features = []\n    features.append(pd.Series(x['mcc_code'].value_counts().add_prefix('mcc_')))\n    features.append(pd.Series(x['tr_type'].value_counts().add_prefix('tr_')))\n    features.append(pd.Series(x['day'].value_counts().add_prefix('day_')))\n    features.append(pd.Series(x['hour'].value_counts().add_prefix('h_')))\n    features.append(pd.Series(x['night'].value_counts().add_prefix('night_')))\n    \n    return pd.concat(features)","ba592882":"from tqdm.notebook import tqdm\ntqdm.pandas()\n\ndata = transactions.groupby(transactions['customer_id']).progress_apply(features_creation).unstack(-1)","67f2a612":"# dealing with missing values to further pass the data through minmax scaler\n\ndata.fillna(0, inplace=True)\ndata.head()","93c5f312":"data_train = pd.merge(data, gender_train, on='customer_id', how=\"right\")\ndata_test = pd.merge(data, gender_test, on='customer_id', how=\"right\")\nprint('Train data shape is {} rows and {} columns.'.format(data_train.shape[0], data_train.shape[1]))\nprint('Test data shape is {} rows and {} columns.'.format(data_test.shape[0], data_test.shape[1]))","c7558df8":"data_train.set_index('customer_id', inplace=True)\ndata_test.set_index('customer_id', inplace=True)","5a177bc0":"target = data_train['gender']\ndata_train.drop(columns='gender', inplace=True)","4955a8ec":"X = data_train.copy()\ny = target\ntest_df = data_test.copy()\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\ntest_df = scaler.fit_transform(test_df)\n\nprint(X.shape)\nprint(test_df.shape)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=42)","ac3c9103":"model = XGBClassifier(random_state=42) \nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nroc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n\n# auc_score is 0.8570479791604508\n# Kaggle score 0.81956","943b3ef0":"import scikitplot as skplt\nimport matplotlib.pyplot as plt\n\ny_true = y_test\ny_probas =  model.predict_proba(X_test)\nskplt.metrics.plot_roc_curve(y_true, y_probas)\nplt.show()","7cf03de4":"model = XGBClassifier( learning_rate =0.1,\n n_estimators=500,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\nroc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n\n# Auc score is 0.8695333478689955\n# Kaggle score is 0.83096","13278c45":"y_true = y_test\ny_probas =  model.predict_proba(X_test)\nskplt.metrics.plot_roc_curve(y_true, y_probas)\nplt.show()","f82feb5e":"model.fit(X, y)\ny_pred =model.predict_proba(test_df)\nres = [x[1] for x in y_pred]\n\n\nsubmission = pd.DataFrame(index=data_test.index)\nsubmission['probability'] = res\nsubmission.to_csv('submission.csv')\nsubmission.head()\n","83e2b17a":"# Submission","205f83d8":"# Features creation","78792d34":"# First model with default hyperparameters","65f918fd":"# Split data into labeled train\/unlabeled test","cb6c2bfc":"# Summing up\n The task was prediction of gender 1 according to the customer's card transactions.\n \n**1.EDA**\n\n* From the first glance at data it was obvious that transaction mcc code would be the most important feature to predict gender. According to EDA it was possible to determine let's call it gender-related mcc codes: thus women tend to buy at second shop, women clothes, sewing supplies and women accessories much often than men, who in contrary are interested in car rent, stocks, computer club, gambling (sadly), and warious construction\/auto service. Also it was possible to find out that women and men have common mcc transaction types with almost zero difference: grocery shops, alcohol, tobacco, cash withdraw and cash transfer, communal payments, sport goods. (I made an irrelevant to the main task side conclusion that the more emansipated people live in country the harder to predict their gender would be, probably, since there's no certain line between what men or women 'should' buy).\n* Transaction types are also have some gender disbalance such as women tend to fill claim reports, transfer cash without comission, return goods. Men transfer money with overdraft using mobile app, transfer money using ATM with comission, return goods abroad. \n* Amount feature was also useful. For example during EDA I found out that men tend to spend higher amount at fur atelier than women. Also the highest spent at this mcc code also belong to men. Thus I decided to extract some stats features for the customers. \n\n**2. Model**\n\nI've made 2 XGBoost models:\n* First one was with all default hyperparameters which gave me auc_score - 0.8570479791604508 and Kaggle score - 0.81956\n* Second had tuned hyperparameters with the following result auc_score - 0.8695333478689955 Kaggle score - 0.83096, not great auc score improvement, but +0.0114 so it was definitely worth trying.\n","bc770d64":"# Second model with tuned hyperparameters"}}