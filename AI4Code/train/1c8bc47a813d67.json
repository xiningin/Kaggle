{"cell_type":{"83404668":"code","506487a4":"code","a76438d7":"code","b32fd201":"code","6e80e47f":"code","c5f3bcf9":"code","59ae6a2d":"code","57f2d828":"code","6aba5bcb":"code","2add7681":"code","ef757a9e":"code","9ba051d5":"code","38d2e276":"code","227a599f":"code","f7f2d372":"code","aa3edaca":"code","01b62d1a":"code","cf775037":"code","4d9ca5ae":"code","86a59b07":"markdown","40512c5c":"markdown","247ef42a":"markdown","5a002613":"markdown","dc4a3a17":"markdown","edb80366":"markdown"},"source":{"83404668":"import sys\nsys.path.insert(0, \"\/kaggle\/input\/weightedboxesfusion\")\n\nimport ensemble_boxes\n\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport os, re\nimport gc\nimport random\n\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\nfrom torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom matplotlib import pyplot as plt","506487a4":"DATA_DIR = \"\/kaggle\/input\/global-wheat-detection\"\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","a76438d7":"test_df = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\ntest_df.shape","b32fd201":"class WheatDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __len__(self) -> int:\n        return len(self.image_ids)\n\n    def __getitem__(self, idx: int):\n        image_id = self.image_ids[idx]\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        records = self.df[self.df['image_id'] == image_id]\n    \n        if self.transforms:\n            sample = {\"image\": image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n\n        return image, image_id\n","6e80e47f":"def fasterrcnn_resnet50_fpn(path,pretrained_backbone=False):\n\n    backbone = resnet_fpn_backbone('resnet50', pretrained_backbone)\n    model = FasterRCNN(backbone, 2)\n    model.load_state_dict(torch.load(path))\n    model.to(DEVICE)\n    model.eval()\n    return model","c5f3bcf9":"def get_model_101( path,pretrained=False):\n    backbone = resnet_fpn_backbone('resnet101', pretrained=pretrained)\n    model = FasterRCNN(backbone, num_classes=2)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n    model.load_state_dict(torch.load(path))\n    model.to(DEVICE)\n    model.eval()\n    return model","59ae6a2d":"model1 =get_model_101(\"..\/input\/faster-cnn-final-model\/fastercnn_resnet_101_145.pth\")\nmodel2= get_model_101(\"..\/input\/fastercnn-155\/fastercnn_101_155.pth\")\nmodel3= get_model_101(\"..\/input\/faster-cnn-101\/fastercnn_resnet_101_155_WightDecay.pth\")\n\nfaster_CNN_50_90_2=fasterrcnn_resnet50_fpn(\"..\/input\/isfix-model\/fastercnn_50_fix.pth\")\n\nmodels=[model1,model3,model2,faster_CNN_50_90_2]\n\n","57f2d828":"from ensemble_boxes import *\n\ndef make_ensemble_predictions(images):\n    images = list(image.to(DEVICE) for image in images)    \n    result = []\n    for model in models:\n        with torch.no_grad():\n            outputs = model(images)\n            result.append(outputs)\n            del model\n            gc.collect()\n            torch.cuda.empty_cache()\n    return result\n\ndef run_wbf_ensemble(predictions, image_index, image_size=1024, iou_thr=0.34, skip_box_thr=0.43, weights=None):\n    boxes = [prediction[image_index]['boxes'].data.cpu().numpy()\/(image_size-1) for prediction in predictions]\n    scores = [prediction[image_index]['scores'].data.cpu().numpy() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]) for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","6aba5bcb":"def get_test_transforms():\n    return A.Compose([\n            ToTensorV2(p=1.0)\n        ], p=1.0)","2add7681":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dataset = WheatDataset(test_df, os.path.join(DATA_DIR, \"test\"), get_test_transforms())\n\ntest_data_loader = DataLoader(\n    test_dataset,\n    batch_size=4,\n    shuffle=False,\n    num_workers=1,\n    drop_last=False,\n    collate_fn=collate_fn\n)","ef757a9e":"def format_prediction_string(boxes, scores):\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","9ba051d5":"class BaseWheatTTA:\n    \"\"\" author: @shonenkov \"\"\"\n    image_size = 1024\n\nclass TTAHorizontalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n\n    def augment(self, image):\n        return image.flip(1)\n    \n    def batch_augment(self, images):\n        return images.flip(2)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [1,3]] = self.image_size - boxes[:, [3,1]]\n        return boxes\n\nclass TTAVerticalFlip(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return image.flip(2)\n    \n    def batch_augment(self, images):\n        return images.flip(3)\n    \n    def deaugment_boxes(self, boxes):\n        boxes[:, [0,2]] = self.image_size - boxes[:, [2,0]]\n        return boxes\n    \nclass TTARotate90(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    \n    def augment(self, image):\n        return torch.rot90(image, 1, (1, 2))\n\n    def batch_augment(self, images):\n        return torch.rot90(images, 1, (2, 3))\n    \n    def deaugment_boxes(self, boxes):\n        res_boxes = boxes.copy()\n        res_boxes[:, [0,2]] = self.image_size - boxes[:, [3,1]] \n        res_boxes[:, [1,3]] = boxes[:, [0,2]]\n        return res_boxes\n\nclass TTACompose(BaseWheatTTA):\n    \"\"\" author: @shonenkov \"\"\"\n    def __init__(self, transforms):\n        self.transforms = transforms\n        \n    def augment(self, image):\n        for transform in self.transforms:\n            image = transform.augment(image)\n        return image\n    \n    def batch_augment(self, images):\n        for transform in self.transforms:\n            images = transform.batch_augment(images)\n        return images\n    \n    def prepare_boxes(self, boxes):\n        result_boxes = boxes.copy()\n        result_boxes[:,0] = np.min(boxes[:, [0,2]], axis=1)\n        result_boxes[:,2] = np.max(boxes[:, [0,2]], axis=1)\n        result_boxes[:,1] = np.min(boxes[:, [1,3]], axis=1)\n        result_boxes[:,3] = np.max(boxes[:, [1,3]], axis=1)\n        return result_boxes\n    \n    def deaugment_boxes(self, boxes):\n        for transform in self.transforms[::-1]:\n            boxes = transform.deaugment_boxes(boxes)\n        return self.prepare_boxes(boxes)","38d2e276":"from itertools import product\n\ntta_transforms = []\nfor tta_combination in product([TTAHorizontalFlip(), None], \n                               [TTAVerticalFlip(), None],\n                               [TTARotate90(), None]):\n    tta_transforms.append(TTACompose([tta_transform for tta_transform in tta_combination if tta_transform]))","227a599f":"def make_tta_predictions(images, score_threshold=0.57):\n    with torch.no_grad():\n        images = torch.stack(images).float().to(DEVICE)\n        predictions = []\n        for tta_transform in tta_transforms:\n            result = []\n            #ensemble predict\n            outputs = make_ensemble_predictions(tta_transform.batch_augment(images.clone()))\n           # outputs = model(tta_transform.batch_augment(images.clone()))\n\n            for i, image in enumerate(images):\n                #chose the boxes and scores\n                boxes, scores, labels = run_wbf_ensemble(outputs, image_index=i)\n                #boxes = outputs[i]['boxes'].data.cpu().numpy()   \n                #scores = outputs[i]['scores'].data.cpu().numpy()\n                \n                indexes = np.where(scores > score_threshold)[0]\n                boxes = boxes[indexes]\n                boxes = tta_transform.deaugment_boxes(boxes.copy())\n                result.append({\n                    'boxes': boxes,\n                    'scores': scores[indexes],\n                })\n            predictions.append(result)\n    return predictions","f7f2d372":"def run_wbf(predictions, image_index, image_size=1024, iou_thr=0.6, skip_box_thr=0.43, weights=None):\n    boxes = [(prediction[image_index]['boxes']\/(image_size-1)).tolist() for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist() for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).astype(int).tolist() for prediction in predictions]\n    boxes, scores, labels = ensemble_boxes.ensemble_boxes_wbf.weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels","aa3edaca":"results = []\nlist_box=[]\nlist_score=[]\nlist_image_id=[]\nfor images, image_ids in test_data_loader:\n\n    predictions = make_tta_predictions(images)\n    for i, image in enumerate(images):\n        boxes, scores, labels = run_wbf(predictions, image_index=i)\n        boxes = boxes.round().astype(np.int32).clip(min=0, max=1023)\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n        list_score.append(scores)\n        list_box.append(boxes)\n        list_image_id.append(image_id)\n        \n        result = {\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n        results.append(result)","01b62d1a":"plt.figure(figsize=[30,30])\nfor i in range(len(list_box)):\n    image=plt.imread('..\/input\/global-wheat-detection\/test\/'+list_image_id[i]+'.jpg')\n  \n    for b,s in zip(list_box[i],list_score[i]):\n            image = cv2.rectangle(image, (b[0],b[1]), (b[0]+b[2],b[1]+b[3]), (255,0,0), 2) \n            image = cv2.putText(image, '{:.2}'.format(s), (b[0]+np.random.randint(3),b[1]), cv2.FONT_HERSHEY_SIMPLEX,  \n                           1, (0,255,0), 2, cv2.LINE_AA)\n    plt.subplot(4,3,i+1)\n    plt.imshow(image)\nplt.show()","cf775037":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df","4d9ca5ae":"test_df.to_csv('submission.csv', index=False)","86a59b07":"# Data Loader For Test","40512c5c":"# Load All Model","247ef42a":"# Ensemble Prediction","5a002613":"# Format For CSV File","dc4a3a17":"# TTA","edb80366":" Hello everyone, this is my first competition in object detection, in this competition I chose to use the FasterCNN model.\n\nThe competition is wheat detection.\n\nI used Ensemble and TTA to improve the prediction result.\nInitially the model result without Ensemble and TTA was 0.67.\nAfter using TTA I raised the result to 0.695. After using Ensemble + TTA I raised the result to 0.713"}}