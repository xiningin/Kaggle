{"cell_type":{"00153c30":"code","19bd60f3":"code","f08131a3":"code","a3aab856":"code","68edb9d8":"code","83228a36":"code","8b6eaa07":"code","0b24d732":"code","8b8abe30":"code","d86fcd20":"code","52517dd8":"code","5bfdeb5b":"code","f3fc830d":"code","a8a3cbc0":"code","59740c2e":"markdown","f409c281":"markdown","6f1bbaeb":"markdown","41b76d93":"markdown","73a9ea76":"markdown","0d59a995":"markdown","88e5563f":"markdown","8c6606d5":"markdown"},"source":{"00153c30":"import keras\nimport pandas as pd\nfrom tqdm import tqdm\nimport numpy as np\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.svm import SVR\nimport numpy as np","19bd60f3":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntrain.head()","f08131a3":"# load the tokenizer\nmodel_path = \"..\/input\/huggingface-bert\/bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(model_path)","a3aab856":"# tokenize the excerpt data\ntrain_data = []\nfor sent in tqdm(train['excerpt'].tolist()):\n    train_data.append(tokenizer(sent, padding=\"max_length\", truncation=True, return_tensors=\"tf\"))","68edb9d8":"# load the model\nBERTmodel = TFBertModel.from_pretrained(model_path)","83228a36":"# embed the training sentences\ntrain_sent_embedding = []\nfor x in tqdm(train_data):\n    train_sent_embedding.append(BERTmodel(x).last_hidden_state[0][0])","8b6eaa07":"# import\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\n\n# set params\nparameters = [{'kernel': ['poly', 'rbf', 'sigmoid'], 'gamma': [1e-4, 1e-3, 1e-2],\n               'C': [1, 10, 100], 'epsilon': [10, 1, 0.1, 0.01, 0.001]}]\n\n#\nscorer = make_scorer(mean_squared_error, greater_is_better=False)\nsvr_gs = GridSearchCV(SVR(), parameters, cv = 5, scoring=scorer, verbose=10, n_jobs=None)\nsvr_gs.fit(train_sent_embedding, train['target'])","0b24d732":"# Checking the score for all parameters\nparameter_result = []\nprint(\"Grid scores on training set:\")\nmeans = svr_gs.cv_results_['mean_test_score']\nstds = svr_gs.cv_results_['std_test_score']\nfor mean, std, params in zip(means, stds, svr_gs.cv_results_['params']):\n    print(\"%0.3f (+\/-%0.03f) for %r\"% (mean, std * 2, params))\n    parameter_result.append({'mean': abs(mean), 'std': std, **params})\n    \n# select the settings with smallest loss\nparameter_result = pd.DataFrame(parameter_result)\nparameter_result = parameter_result.sort_values(by=['mean'])\nbest_settings = parameter_result.head(1).to_dict(orient='records')[0]","8b8abe30":"model = SVR(C=best_settings['C'], \n            epsilon=best_settings['epsilon'], \n            gamma=best_settings['gamma'],\n            kernel= best_settings['kernel'])\nmodel.fit(train_sent_embedding, train['target'])","d86fcd20":"test_data = []\nfor sent in tqdm(test['excerpt'].tolist()):\n    test_data.append(tokenizer(sent, padding=\"max_length\", truncation=True, return_tensors=\"tf\"))","52517dd8":"# embed the training sentences\ntest_sent_embedding = []\nfor x in tqdm(test_data):\n    test_sent_embedding.append(BERTmodel(x).last_hidden_state[0][0])","5bfdeb5b":"# perform predictions\ny_pred = model.predict(np.array(test_sent_embedding))","f3fc830d":"sub = test[['id']].copy()","a8a3cbc0":"sub['target'] = y_pred\nsub.to_csv(\"submission.csv\", index=False)","59740c2e":"### Submission","f409c281":"### Load data","6f1bbaeb":"### Decoder pre-requisite: Hyperparameter tuning\n\n- Use SVR regression.\n- Tune the hyperparameter by 5 fold CV `GridSearch`","41b76d93":"### Decoder \n\n- Create SVR regression model with best parameters identifier in above hyperparameter tuning","73a9ea76":"## CommonLit | BERT + tuned SVR\n\n- Encoder: Pre-tuned BERT model (bert-base-uncased)\n- Decoder: SVR regression model (with hyperparameter tuned using 5 fold CV)","0d59a995":"### Test","88e5563f":"### Encoder (train data)","8c6606d5":"### Prepare train data (tokenize)"}}