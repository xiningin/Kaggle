{"cell_type":{"77de98bc":"code","509073aa":"code","85978e41":"code","765a403c":"code","4607662f":"code","cd65b5ef":"code","d5d0e7c5":"code","7c12f5d8":"code","bc19c034":"code","c862627f":"code","8fa220e1":"code","a4751599":"code","dae1ea5e":"code","fcdf2cac":"code","2c1be4c0":"code","efb8e6d4":"code","e82a9082":"code","ff3fde93":"code","33b7bf16":"code","175b59c4":"code","c032d680":"code","a9749210":"code","ddff81cd":"code","219a41b4":"code","bce66f5c":"code","57d35c0e":"code","5ed7d71b":"code","e8c8bcef":"code","093ba7ba":"code","4d6f43e9":"code","aa28d91e":"code","0e5cd1ce":"code","285f00ff":"code","f88f4a01":"code","4a272d73":"code","81de7a9d":"code","ffc10810":"code","97db375f":"code","369584a6":"code","a0c1ec92":"code","1119f878":"code","e1c38eba":"code","3435c80b":"code","ffeff164":"markdown","9011f9ed":"markdown","9c390c94":"markdown","19b89d9e":"markdown","84cfca1b":"markdown","cfd5f268":"markdown","7afe328a":"markdown","44c90b96":"markdown","12d7aa97":"markdown","5f2b765e":"markdown","61ff382b":"markdown","4900ff5d":"markdown","140a0c67":"markdown","e5b4054f":"markdown","3177428d":"markdown","6b096155":"markdown"},"source":{"77de98bc":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","509073aa":"#importing packages\nimport pandas as pd # for dataframe\nimport numpy as np  # for algebric \nimport nltk         #for nlp \nimport matplotlib.pyplot as plt # for visualization\nimport seaborn as sns           # for visualization","85978e41":"# Importing csv files\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain[:5]","765a403c":"test[:2]","4607662f":"# viewing each unique values in each columns\nfor i in train.columns:\n    print('Column Name  :',i)\n    print(train[i].unique())","cd65b5ef":"# id is not essential. so, i'm droping it\ndf_train = train.drop(['id'],axis=1)\ndf_test = test.drop(['id'],axis=1)","d5d0e7c5":"#Hence when it comes twitter, Hashtag is one of important thing. so, i'm created new column contains only hashtags\n# for this i used regrx package.\nimport re\ndf_train['Hashtag'] = df_train['text'].map(lambda x: re.findall(r'#(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_test['Hashtag'] = df_test['text'].map(lambda x: re.findall(r'#(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_train['@'] = df_train['text'].map(lambda x: re.findall(r'@(\\w+)',x)).apply(lambda x: \", \".join(x))\ndf_test['@'] = df_test['text'].map(lambda x: re.findall(r'@(\\w+)',x)).apply(lambda x: \", \".join(x))\n\n# data Cleaning\n# i'm defining function to clean data. it will make work easier\ndef remove_punctuation(txt):\n    import string\n    result = txt.translate(str.maketrans('','',string.punctuation))\n    return result\ndef lower_text(txt):\n    return txt.lower()\ndef remove_no(txt):\n    import re\n    return re.sub(r\"\\d+\",\"\",txt)\ndef remove_html_tags(text):\n    \"\"\"Remove html tags from a string\"\"\"\n    import re\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\ndef removeurl(txt):\n    import re\n    return re.sub(r'http?\\S+|www\\.\\S+', '',txt)\n\n# defining all function to one for this problem\ndef norm(txt):\n    x = remove_punctuation(txt)\n    x = lower_text(x)\n    x = remove_html_tags(x)\n    x = remove_no(x)\n    x = removeurl(x)\n    return x\n\n# applying data cleaning function for text\ndf_train['text'] = df_train['text'].map(lambda x:  norm(x))\ndf_test['text'] = df_test['text'].map(lambda x:  norm(x))\n\n#There are some magic words \"%20\" which indicates space in this data.so,Removing magic words\ndf_train['keyword'] = df_train['keyword'].map(lambda s: s.replace('%20',' ') if isinstance(s, str) else s)\ndf_test['keyword'] = df_test['keyword'].map(lambda s: s.replace('%20',' ') if isinstance(s, str) else s)","7c12f5d8":"# Cleaned Data\ndf_test\ndf_train[:5]","bc19c034":"#Seperating data to explore it\ntrain_yes = df_train[df_train[\"target\"]==1]\ntrain_no = df_train[df_train[\"target\"]==0]\ntrain_yes[:5]","c862627f":"# created a para from all text for each target\n# Sepereate Data to two \ntrain_txt_yes = \" \".join(str(i) for i in train_yes['text'])\ntrain_txt_no = \" \".join(str(i) for i in train_no['text'])\ntest_txt = \" \".join(str(i) for i in df_test['text'])\n#train_txt_yes","8fa220e1":"#replacing empty space with null values\ntrain_yes[\"Hashtag\"]= train_yes[\"Hashtag\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_no[\"Hashtag\"]= train_yes[\"Hashtag\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_yes[\"@\"]= train_yes[\"@\"].replace(r'^\\s*$', np.nan, regex=True)\ntrain_no[\"@\"]= train_yes[\"@\"].replace(r'^\\s*$', np.nan, regex=True)\n#train_yes[\"Hashtag\"].isnull().sum()","a4751599":"print(\"Total No. of disaster tweets                        :\",train_yes[\"text\"].count())\nprint(\"Total No. of hashtag present in disaster tweets     :\",train_yes[\"Hashtag\"].notnull().sum())\nprint(\"Total No. of non-disaster tweets                    :\",train_no[\"text\"].count())\nprint(\"Total No. of hashtag present in non-disaster tweets :\",train_no[\"Hashtag\"].notnull().sum(),\"\\n\")\n\nprint(\"Total No. of disaster tweets                  :\",train_yes[\"text\"].count())\nprint(\"Total No. of @ present in disaster tweets     :\",train_yes[\"@\"].notnull().sum())\nprint(\"Total No. of non-disaster tweets              :\",train_no[\"text\"].count())\nprint(\"Total No. of @ present in non-disaster tweets :\",train_no[\"@\"].notnull().sum())","dae1ea5e":"# deffining function for tokenization & lemmatize.\n# Tokenization function\ndef Token_and_removestopword(txt):\n    import nltk\n    from nltk.corpus import stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    words = nltk.word_tokenize(txt)\n    without_stop_words = []\n    for word in words:\n        if word not in stop_words:\n            without_stop_words.append(word)\n    return without_stop_words\n#lemmatizing tokenized words\ndef lemmatize_word(tokens,pos=\"v\"): \n    import nltk\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # provide context i.e. part-of-speech \n    lemmas = [lemmatizer.lemmatize(word, pos =pos) for word in tokens] \n    return lemmas","fcdf2cac":"tok_yes = Token_and_removestopword(train_txt_yes)\ntok_no = Token_and_removestopword(train_txt_no)\ntest_tok = Token_and_removestopword(test_txt)\ntok_yes[:5]","2c1be4c0":"#lem_wd = [lemmatizer.lemmatize(x, pos ='v') for x in tok]\nlem_tok_yes = lemmatize_word(tok_yes)\nlem_tok_no = lemmatize_word(tok_no)\nlem_test = lemmatize_word(test_tok)\nprint(len(tok_yes))\nprint(len(lem_tok_yes))","efb8e6d4":"def top_word_dis(lem_tok,TopN=10):\n    fq = nltk.FreqDist(lem_tok)\n    rslt = pd.DataFrame(fq.most_common(TopN),\n                        columns=['Word', 'Frequency']).set_index('Word')\n    plt.style.use('ggplot')\n    rslt.plot.bar()","e82a9082":"top_word_dis(lem_tok_yes)","ff3fde93":"top_word_dis(lem_tok_no)","33b7bf16":"print(lem_test[:10])","175b59c4":"#TF-IDF Vectorizer\ndef tfidf(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    #to convert token input into text\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = TfidfVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    # to get both transform for train & split\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n\"\"\"\nAbove step is not necessary. you can also use below step\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nvectorizer.fit(train_data)\nvectorizer.transform(train_data)  ---> output as array\nvectorizer.transform(test_data) ---> essential if you has test data & to change its shape for further process\n\"\"\"","c032d680":"def countvectorizer(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import CountVectorizer\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = CountVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n\"\"\"\nsame as for tdidf\n\"\"\"","a9749210":"#Dimentional Reduction\ndef PCA(X_train,Y_train=None,X_test=None,n=1000):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=n)\n    X = pca.fit(X_train,y_train)\n    train = pca.transform(X_train)\n    if X_test.any() == None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out","ddff81cd":"# ML Algorithms\ndef logreg(X_train, y_train,X_test,n=20):\n    from sklearn.linear_model import LogisticRegression\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\ndef naivebiase(X_train, y_train,X_test,n=20):\n    from sklearn.naive_bayes import GaussianNB\n    model = GaussianNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\ndef accuracy(y_true, y_pred):\n    from sklearn.metrics import accuracy_score\n    return accuracy_score(y_true, y_pred)","219a41b4":"# Cleaned Data\ndf_test\ndf_train[:5]","bce66f5c":"#apply tokenization & lemmatization to each text\ndf_train[\"tokenized_text\"] = df_train[\"text\"].apply(lambda x:lemmatize_word(Token_and_removestopword(x)))\ndf_test[\"tokenized_text\"] = df_test[\"text\"].apply(lambda x:lemmatize_word(Token_and_removestopword(x)))\ndf_train[:5]","57d35c0e":"\"\"\"\nfor vectorization in countervector & tfidf the input be in text formate.\nthat's why i join all text in list in above mentioned function.\n\"\"\"\ntfidf_train,tfidf_test = tfidf(df_train[\"tokenized_text\"],df_test[\"tokenized_text\"])\ncnt_train,cnt_test = countvectorizer(df_train[\"tokenized_text\"],df_test[\"tokenized_text\"])","5ed7d71b":"#check whether shape aligns or not\nprint(tfidf_train.shape)\nprint(cnt_train.shape)\nprint(df_train[\"target\"].shape)","e8c8bcef":"# test & train Split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX=tfidf_train\ny=df_train[\"target\"]\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.1,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","093ba7ba":"# dimentionality reduction ---> it is not necessary for this problem. because it is very small data\nx_train,x_test = PCA(x_train,y_train,x_test,n=500)","4d6f43e9":"# prediction using logistic regression model. it is defined as function above\npreds = logreg(x_train, y_train,x_test)\naccuracy(y_test,preds)","aa28d91e":"#prediction using naivebiase Gaussian model. it is defined as function above\npreds = naivebiase(x_train, y_train,x_test)\naccuracy(y_test,preds)","0e5cd1ce":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nX=cnt_train\ny=df_train[\"target\"]\nx_train,x_test,y_train,y_test = train_test_split(X, y, test_size=0.1,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","285f00ff":"x_train,x_test = PCA(x_train,y_train,x_test,n=500)","f88f4a01":"preds = logreg(x_train, y_train,x_test)\naccuracy(y_test,preds)","4a272d73":"preds = naivebiase(x_train, y_train,x_test)\naccuracy(y_test,preds)","81de7a9d":"x_train = cnt_train\ny_train = df_train[\"target\"] \nx_test = cnt_test","ffc10810":"# check wheather shape alligns or not\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)","97db375f":"#if shape is not aligned check wheather do you transform test data in vectorization part or not.\n#if test data is not transformed as per train data it may led to error due to mismatch in features.\nx_train,x_test = PCA(x_train,y_train,x_test,n=500)","369584a6":"preds = logreg(x_train,y_train,x_test)","a0c1ec92":"preds","1119f878":"submission = test[[\"id\"]]\nsubmission[\"target\"] = preds","e1c38eba":"submission[:5]","3435c80b":"submission.to_csv(\"submission.csv\",index=False)","ffeff164":"## Vectorization\nconverting into numeric formate\n* Tf-IDF\n* Countvectorizer","9011f9ed":"## Submmision","9c390c94":"### Insights:\nBoth Vectorizer provides same accuracy.\nhence logistic regression model provides high accuracy than naive bayes","19b89d9e":"## Data Visualizing","84cfca1b":"# TF-IDF","cfd5f268":"### Tokenization & Lemminization","7afe328a":"Twitts has Hash Tag.","44c90b96":"# ML Models - Test Train","12d7aa97":"# Defining Functions","5f2b765e":"# CounterVectorizer","61ff382b":"# Importing Essential Libraries","4900ff5d":"#### From this, we can't find disaster tweets. because hashtag & @ count is not present in non-disaster tweets.","140a0c67":"# Thanks for viewing this kernal.  Its my first kernal on NLP in Kaggle :)","e5b4054f":"# Test Data","3177428d":"# Data Cleaning","6b096155":"# Splitting data to view word distribution"}}