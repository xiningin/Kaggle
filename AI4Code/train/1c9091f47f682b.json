{"cell_type":{"23c20835":"code","79da4915":"code","fa729f5c":"code","ef180c48":"code","8df79a41":"code","376e2aa9":"code","390d1fbb":"code","c55beb90":"code","8185fe17":"code","b3b6f933":"code","40a0d725":"code","576b0277":"code","a7bdd81d":"code","c0fab2b7":"code","d07103cf":"code","9d5cffc9":"code","bbd234de":"code","aa35f905":"code","75c7ad8f":"code","fdf75933":"code","e718c3b0":"code","7ffb4aa3":"code","2cb2ee17":"code","9f437cc8":"code","e3aa4b13":"code","c9c317aa":"code","7157052a":"code","0e5210c6":"code","104c6524":"code","4a99cc18":"code","6248e47f":"code","ec3bb1de":"code","4c098401":"code","f6824e96":"code","c4f6f3d9":"code","ba98b089":"code","5d344cd5":"code","aeea83c2":"code","a5de68b4":"code","345f8115":"markdown","9d6e6794":"markdown","17c9f740":"markdown","e31d45b7":"markdown","461e002b":"markdown","34ccb4fa":"markdown","198b21d9":"markdown","280734bc":"markdown","53c8561a":"markdown","26a7642c":"markdown","6a84f641":"markdown","4cdf5ea9":"markdown","99b244e1":"markdown","1d9dd407":"markdown","0d523015":"markdown","26ff6780":"markdown","5f6a2d3a":"markdown","e322f007":"markdown","159b664b":"markdown","fa05ab55":"markdown","4a88534d":"markdown","8db5dcc1":"markdown","a207c46a":"markdown"},"source":{"23c20835":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79da4915":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport re\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report","fa729f5c":"data_set = pd.read_csv('\/kaggle\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')","ef180c48":"data_set.head(5)","8df79a41":"data_set.describe(include = 'all')","376e2aa9":"data_set.info()","390d1fbb":"data_set.isnull().sum()","c55beb90":"data_set.shape","8185fe17":"dataset = pd.DataFrame({\"user_created\": data_set['user_created'], \"user_followers\": data_set['user_followers'],\n                       \"user_friends\": data_set['user_friends'], \"user_favourites\": data_set['user_favourites'], \n                       \"user_verified\": data_set['user_verified'], \"date\": data_set['date'],\n                       \"text\": data_set['text'], \"source\": data_set['source'],\n                       \"retweets\": data_set['retweets'], \"favorites\": data_set['favorites'], \n                       \"text_length\": data_set['text'].apply(len)})\ndataset.head(5)","b3b6f933":"plt.figure(figsize = (12, 8))\nsns.countplot(y = dataset['source'], data = data_set)\nplt.title(\"Source vs Count\")\nplt.show()","40a0d725":"import datetime\ndataset['date'] = pd.to_datetime(dataset['date']).dt.date\n\nplt.figure(figsize = (12, 8))\ndataset.sort_values('date', inplace = True)\nsns.countplot(y = dataset['date'], data = dataset)\nplt.title(\"Tweets per day\")\nplt.show()","576b0277":"plt.figure(figsize = (12, 8))\ndataset_cor = dataset[['user_followers', 'user_friends', 'user_favourites', 'retweets', 'favorites', 'text_length']].corr()\naxes = sns.heatmap(dataset_cor, linecolor = 'white', linewidths = 1, cmap = 'coolwarm', annot = True)\naxes.set_title('Heatmap')\nplt.show()","a7bdd81d":"mention = []\nfor i in range(0, len(dataset)):\n    review1 = dataset['text'][i]\n    review1 = re.findall('@[a-zA-Z0-9_]+', review1)\n    for j in review1:\n        mention.append(j)\n        \nmention","c0fab2b7":"df = pd.DataFrame(mention)\ndf = df[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist = FreqDist()\n\nfor words in df:\n    freqdist[words] =+1 \n    \nfreqdist","d07103cf":"df = df[:20,]\nplt.figure(figsize = (12, 8))\nsns.barplot(df.values, df.index, alpha = 0.8)\nplt.title(\"Top @(mention)\")\nplt.ylabel(\"Account Name\")\nplt.xlabel(\"Count\")\nplt.show()","9d5cffc9":"hashtags = []\nfor i in range(0, len(dataset)):\n    review1 = dataset['text'][i]\n    review1 = re.findall('#[a-zA-Z0-9_]+', review1)\n    for j in review1:\n        hashtags.append(j)\n        \nhashtags","bbd234de":"df1 = pd.DataFrame(hashtags)\ndf1 = df1[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist1 = FreqDist()\n\nfor words in df1:\n    freqdist1[words] +=1\n\nfreqdist1    ","aa35f905":"df1 = df1[:20, ]\nplt.figure(figsize = (12, 8))\nsns.barplot(df1.values, df1.index, alpha = 0.8)\nplt.title(\"Top Hashtag Used\")\nplt.ylabel(\"Hashtags (#)\")\nplt.xlabel(\"Count\")\nplt.show()","75c7ad8f":"from textblob import TextBlob\n\ndataset['sentiment'] = ' '\ndataset['polarity'] = None\n\nfor i,j in enumerate(dataset.text):\n    blob = TextBlob(j)\n    dataset['polarity'][i] = blob.sentiment.polarity\n    if blob.sentiment.polarity >= 0 :\n        dataset['sentiment'][i] = 'positive'\n    else:\n        dataset['sentiment'][i] = 'negative'","fdf75933":"dataset.head(5)","e718c3b0":"dataset['sentiment'].value_counts()","7ffb4aa3":"plt.figure(figsize = (12, 8))\nsns.countplot(x = dataset.sentiment, data = dataset)\nplt.show()","2cb2ee17":"plt.figure(figsize = (12, 12))\nsns.countplot(y = dataset['date'], hue = dataset['sentiment'], data = dataset)\nplt.title(\"Tweets per Day based on the Sentiment\")\nplt.show()","9f437cc8":"senti = pd.get_dummies(dataset['sentiment'], drop_first = True)\ndataset = pd.concat([dataset, senti], axis = 1)\ndataset = dataset.drop('sentiment', axis = 1)\ndataset.head(5)","e3aa4b13":"data = pd.DataFrame({\"Text\": dataset['text'], \"Sentiment\": dataset['positive']})\ndata.head(5)","c9c317aa":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nwords = []\nfor i in range(0, len(data)):\n    review2 = data['Text'][i]\n    review2 = re.sub('\"\"', ' ', review2)\n    review2 = re.sub('https:\/\/[a-zA-Z0-9.\/]+', ' ',review2)\n    review2 = re.sub('@[a-zA-Z0-9._]+', ' ', review2)\n    review2 = re.sub('#[a-zA-Z0-9._]+', ' ', review2)\n    # review2 = re.sub('\\n', ' ', review2)\n    review2 = re.sub('[^a-zA-Z]', ' ', review2)\n    review2 = review2.lower()\n    review2 = review2.split()\n    review2 = [word for word in review2 if not word in stopwords.words('english')]\n    for j in review2:\n        words.append(j)\n    review2 = [ps.stem(word) for word in review2]\n    review2 = ' '.join(review2)\n    corpus.append(review2)","7157052a":"df2 = pd.DataFrame(words)\ndf2 = df2[0].value_counts()\n\nfrom nltk.probability import FreqDist\nfreqdist2 = FreqDist()\n\nfor words in df2:\n    freqdist2[words] +=1\n\nfreqdist2    ","0e5210c6":"df2 = df2[:20, ]\nplt.figure(figsize = (12, 8))\nsns.barplot(df2.values, df2.index, alpha = 0.8)\nplt.title(\"Top Words Used\", fontdict = {'fontsize' : 15})\nplt.ylabel(\"Words\")\nplt.xlabel(\"Count\")\nplt.show()","104c6524":"dataset_final = pd.DataFrame(corpus, columns = ['Content'])  # To convert a List into a DataFrame\ndataset_final = pd.concat([data['Sentiment'], dataset_final], axis = 1)\ndataset_final","4a99cc18":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dataset_final.Content, dataset_final.Sentiment, test_size = 0.25)\n\nX_1 = X_train.reset_index(drop = True)\nX_2 = X_test.reset_index(drop = True)\n\nY_1 = y_train.reset_index(drop = True)\nY_2 = y_test.reset_index(drop = True)","6248e47f":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\n\nX_train = vectorizer.fit_transform(X_1)\nX_test = vectorizer.transform(X_2)","ec3bb1de":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(max_iter = 300)\nlr.fit(X_train, y_train)\n\ny_pred_log = lr.predict(X_test)\n\nprint(\"Accuracy of Logistic Regression is: {}%\".format(accuracy_score(y_test, y_pred_log) * 100))\nprint(\"Confusion Matrix of Logistic Regression is: \\n{}\".format(confusion_matrix(y_test, y_pred_log)))\nprint(\"{}\".format(classification_report(y_test, y_pred_log)))","4c098401":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf')\nsvc.fit(X_train, y_train)\n\ny_pred_svc = svc.predict(X_test)\n\nprint(\"Accuracy of Support Vector Classifier is: {}%\".format(accuracy_score(y_test, y_pred_svc) * 100))\nprint(\"Confusion Matrix of Support Vector Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_svc)))\nprint(\"{}\".format(classification_report(y_test, y_pred_svc)))","f6824e96":"from sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(X_train, y_train)\n\ny_pred_mnb = mnb.predict(X_test)\n\nprint(\"Accuracy of MultinomialNB is: {}%\".format(accuracy_score(y_test, y_pred_mnb) * 100))\nprint(\"Confusion Matrix of MultinomialNB is: \\n{}\".format(confusion_matrix(y_test, y_pred_mnb)))\nprint(\"{}\".format(classification_report(y_test, y_pred_mnb)))","c4f6f3d9":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train, y_train)\n\ny_pred_dt = dt.predict(X_test)\n\nprint(\"Accuracy of Decision tree Classifier is: {}%\".format(accuracy_score(y_test, y_pred_dt) * 100))\nprint(\"Confusion Matrix of Decision tree Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_dt)))\nprint(\"{}\".format(classification_report(y_test, y_pred_dt)))","ba98b089":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 300, criterion = 'entropy')\nrf.fit(X_train, y_train)\n\ny_pred_rf = rf.predict(X_test)\n\nprint(\"Accuracy of Random Forest Classifier is: {}%\".format(accuracy_score(y_test, y_pred_rf) * 100))\nprint(\"Confusion Matrix of Random Forest Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_rf)))\nprint(\"{}\".format(classification_report(y_test, y_pred_rf)))","5d344cd5":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\n\ny_pred_sgd = sgd.predict(X_test)\n\nprint(\"Accuracy of Sochastic Gradient Descent Classifier is: {}%\".format(accuracy_score(y_test, y_pred_sgd) * 100))\nprint(\"Confusion Matrix of Sochastic Gradient Descent Classifier is: \\n{}\".format(confusion_matrix(y_test, y_pred_sgd)))\nprint(\"{}\".format(classification_report(y_test, y_pred_sgd)))","aeea83c2":"fav = dataset[['favorites','text']].sort_values('favorites',ascending = False)[:5].reset_index()\nprint(\"**Top 5 most Favourited tweets:**\\n\")\nfor j in range(0, 5):\n    print(j,'.', fav['text'][j],'\\n')","a5de68b4":"fav = dataset[['retweets','text']].sort_values('retweets',ascending = False)[:5].reset_index()\nprint(\"**Top 5 most Favourited tweets:**\\n\")\nfor j in range(0, 5):\n    print(j,'.', fav['text'][j],'\\n')","345f8115":"### Creating a Corpus of Words (Clean text)","9d6e6794":"### Heatmap (Correlations)","17c9f740":"### Tweets per day based on the Sentiments","e31d45b7":"## **Exploratory Data Analysis**","461e002b":"### Applying Tfidf Vectorizer","34ccb4fa":"## **Importing the Libraries**","198b21d9":"### Logistic Regression","280734bc":"### Top mentions (@)","53c8561a":"### Top Favorited Tweets","26a7642c":"### Most Retweeted Tweets","6a84f641":"### Top words used","4cdf5ea9":"### Source vs. Count","99b244e1":"### Splitting the Data into Training and Test Set","1d9dd407":"### SGD Classifier","0d523015":"### Support Vector Classifier (SVC)","26ff6780":"### Multinomial Naive Bayes","5f6a2d3a":"## **Data Wrangling**","e322f007":"### Decision Tree Classifier","159b664b":"### Top Hashtags used (#)","fa05ab55":"## **Data Cleaning**\n\n### Applying Sentiments on the tweets\n\nTextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more","4a88534d":"### Tweets per day","8db5dcc1":"### Random Forest Classifier","a207c46a":"## **Applying Various Classification Models**"}}