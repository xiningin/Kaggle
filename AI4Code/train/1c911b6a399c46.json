{"cell_type":{"4c244e24":"code","1258d08b":"code","9bfb2e93":"code","c14c9d6d":"code","98d98560":"code","58f77230":"code","843723cc":"code","ddc33862":"code","172db0bd":"code","15ca6f45":"code","d2e2fda7":"code","2a44fa01":"code","bf11f75f":"code","96076ce8":"code","bcab7d26":"code","d1df5525":"markdown","28406cdd":"markdown","d38b08c5":"markdown","5183b9a7":"markdown","9921a6d2":"markdown","993241e4":"markdown","e598670f":"markdown","d3661f98":"markdown","83d81337":"markdown","9ff5bbcd":"markdown","c31c32f1":"markdown","75c1f393":"markdown"},"source":{"4c244e24":"# importing the libraries\nfrom bs4 import BeautifulSoup\nimport requests\n# <tr> defines a row in HTML table\n# <td> defines a data cell ini HTML table: <th> header info; <td> data info\n# <a> defines a hyperlink (from one page to another), href attribute indicates links destination","1258d08b":"#### read news from FINVIZ\n# Make a GET request to fetch the raw HTML content (require enable javascript https:\/\/stackoverflow.com\/questions\/53918187\/python-requests-to-continue-your-browser-has-to-accept-cookies-and-has-to-hav)\nhtml_content = requests.get('https:\/\/finviz.com\/news.ashx', headers = {'User-Agent': 'Popular browser\\'s user-agent',}).text\n\n# Parse the html content\nsoup = BeautifulSoup(html_content, \"lxml\")\n#print(soup.prettify()) # print the parsed data of html\n\n# Get all table tagged in HTML with <tr> or table into table_data, note in this case only class with \"nn\"\ntable_data = soup.findAll('tr', class_ = \"nn\")\n# For each row in table, print hypter link and data (top 3)\nfor i, table_row in enumerate(table_data):\n    # Read the text of the element 'a' into 'link_text'\n    link_text = table_row.a.get_text()\n    # Read the text of the element 'td' into 'data_text'\n    data_text = table_row.findAll('td', class_ = \"nn-date\")[0].get_text()\n    # Print the count\n    print(f'File number {i+1}:')\n    # Print the contents of 'link_text' and 'data_text' \n    print(link_text)\n    print(data_text)\n    # The following exits the loop after four rows to prevent spamming the notebook, do not touch\n    if i == 3:\n        break","9bfb2e93":"# Hold the parsed news into a list\nall_news = []\n# Iterate through the news\nfor x in table_data:\n    # Read the text from the tr tag into text\n    text = x.a.get_text()\n    # Append ticker, date, time and headline as a list to the 'parsed_news' list\n    all_news.append(text)\n\nprint(all_news[:3])\nprint(len(all_news))","c14c9d6d":"# Text pre-processing\nimport re\n# Remove punctuation\nall_news_p = [re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in all_news]\n# Lowercasing words\nall_news_p = [x.lower() for x in all_news]\n# convert to list for toeknization\nall_news_p = ' '.join(all_news_p)\n\n# Tokenize all news\nfrom nltk.tokenize import word_tokenize\nall_news_t = word_tokenize(all_news_p)\n# Retain only alphabetic words with more than 2 letters\nall_news_t = [x for x in all_news_t if x.isalpha() and len(x)>2]\n# Remove stop words\nfrom wordcloud import STOPWORDS\nall_news_t = [x for x in all_news_t if x not in set(STOPWORDS)]\n# Lemmatize tokens (shorten to root)\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemma = WordNetLemmatizer()\nall_news_t = [wordnet_lemma.lemmatize(x) for x in all_news_t]","98d98560":"# word cloud visualize news\n\n# Import the wordcloud library\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Create a WordCloud object\nstopwords_a = set(STOPWORDS).union(['new', 'stock', 'market', 'trade', 'update', 'investor', 'link', 'sale', 'world', 'global', 'say', 'talk'])\nwordcloud = WordCloud(stopwords=stopwords_a)\n\n# Generate a word cloud\nwordcloud.generate(' '.join(all_news_t))\n\n# Visualize the word cloud\nwordcloud.to_image()","58f77230":"# extract only Entity with spaCy and word cloud visualize entities\nimport spacy\n#!pip install en_core_web_sm\nnlp = spacy.load(\"en_core_web_sm\")\nent_lst =[]\n# defin extract entity\ndef extract_entities(message):\n    doc=nlp(message)\n    for ent in doc.ents:\n        if ent.label_ == 'ORG':\n            ent_lst.append(ent.text)\n    return ent_lst\nall_news_l = ' '.join(all_news_t)\n# Stocks metioned in today's news\nprint(set(extract_entities(all_news_l)))\n\n# Generate a word cloud for entity only\nwordcloude = WordCloud()\nwordcloude.generate(' '.join(ent_lst))\n# Visualize the word cloud\nwordcloude.to_image()","843723cc":"#### look at sepecific stock McDonald (search for sepecific stock in main page not in news tab)\n# Make a GET request to fetch the raw HTML content\nhtml_content = requests.get('https:\/\/finviz.com\/quote.ashx?t=MCD', headers = {'User-Agent': 'Popular browser\\'s user-agent',}).text\n\n# Parse the html content\nsoup = BeautifulSoup(html_content, \"lxml\")\n#print(soup.prettify()) # print the parsed data of html\n\n# Get all table tagged in HTML with <tr> or table into table_data, note in this case only class with \"nn\"\ntable_data = soup.findAll(id='news-table')[0].findAll('tr')\n\n# Hold the parsed news into a list\nparsed_news = []\n# Iterate through the news\nfor x in table_data:\n    # Read the text from the tr tag into text\n    text = x.a.get_text()\n    # Split the text in the td tag into a list \n    date_scrape = x.td.text.split()\n    # If the length of 'date_scrape' is 1, load 'time' with the only element\n    # If not, load 'date' with the 1st element and 'time' with the second\n    if len(date_scrape) == 1:\n        time = date_scrape[0]\n    else:\n        date = date_scrape[0]\n        time = date_scrape[1]\n    # Append ticker, date, time and headline as a list to the 'parsed_news' list\n    parsed_news.append([\"MCD\", date, time, x.a.text])\n\nprint(parsed_news[:3])\nprint(len(parsed_news))","ddc33862":"# NLTK VADER for sentiment analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# New words and values (based on experience, if see those words stock may increase\/drop)\nnew_words = {\n    'crushes': 10,\n    'surge': 10,\n    'jump': 5,\n    'beats': 5,\n    'misses': -5,\n    'trouble': -10,\n    'falls': -100,\n}\n# Instantiate the sentiment intensity analyzer with the existing lexicon\nvader = SentimentIntensityAnalyzer()\n# Update the lexicon\nvader.lexicon.update(new_words)","172db0bd":"import pandas as pd\n# Use these column names\ncolumns = ['ticker', 'date', 'time', 'headline']\n# Convert the list of lists into a DataFrame\nscored_news = pd.DataFrame(parsed_news, columns=columns)\n\n# Iterate through the headlines and get the polarity scores\nscores = [vader.polarity_scores(headline) for headline in scored_news.headline]\n# Convert the list of dicts into a DataFrame\nscores_df = pd.DataFrame(scores)\nscored_news.columns = columns\n# Join the DataFrames\nscored_news = scored_news.join(scores_df)\n# Convert the date column from string to a date\nscored_news['date'] = pd.to_datetime(scored_news.date).dt.date\nscored_news.head()","15ca6f45":"import matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\n\n# Group by date and ticker columns from scored_news and calculate the mean\nmean_c = scored_news.groupby(['date', 'ticker']).mean()\n# Unstack the column ticker\nmean_c = mean_c.unstack('ticker')\n# Get the cross-section of compound in the 'columns' axis\nmean_c = mean_c.xs(\"compound\", axis=\"columns\")\n# Plot a bar chart with pandas\nmean_c.plot.bar(figsize = (10, 6));","d2e2fda7":"# scored_news[['date', 'ticker']].groupby(['date']).count()\n# Seemed MCD is not a popular stock along the time, rarely news mentioned it untill late summer as stock impacted by the economy and investors turn to defensive stocks.","2a44fa01":"print('Is there any duplicated news?', scored_news[['date', 'headline']].duplicated().any())\nscored_news[['date', 'headline']][scored_news[['date', 'headline']].duplicated(keep=False)]","bf11f75f":"# Count the number of headlines in scored_news (store as integer)\nnum_news_before = scored_news.headline.count()\n# Drop duplicates based on ticker and headline\nscored_news_clean = scored_news.drop_duplicates(subset=['headline', 'ticker'])\n# Count number of headlines after dropping duplicates (store as integer)\nnum_news_after = scored_news_clean.headline.count()\n# Print before and after numbers to get an idea of how we did \nf\"Before we had {num_news_before} headlines, now we have {num_news_after}\"","96076ce8":"# Set the index to ticker and date\nsingle_day = scored_news_clean.set_index(['ticker', 'date'])\n# Cross-section the fb row\nsingle_day = single_day.xs('MCD')\n#https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.xs.html \n# xs(index, index2) # get values at specific index(s) \/ or column, level and axis \n\n\n# Select the day with highest score and reasonable number of news\nsingle_day = single_day.loc['2021-07-28']\n# Convert the datetime string to just the time\nsingle_day['time'] = pd.to_datetime(single_day['time']).dt.time\n# Set the index to time and sort by it\nsingle_day = single_day.set_index('time')\n# Sort it\nsingle_day = single_day.sort_index()\n# Label it\nsingle_day['label'] = single_day['compound'].apply(lambda x: 1 if x>=0.05 else 0 if x>-0.05 else -1)\n#positive sentiment : (compound score >= 0.05)\n#neutral sentiment : (compound score > -0.05) and (compound score < 0.05)\n#negative sentiment : (compound score <= -0.05)\nsingle_day.head()","bcab7d26":"TITLE = \"Negative, neutral, and positive sentiment for MCD on July 28rd\"\nCOLORS = [\"red\",\"orange\", \"green\"]\n# Drop the columns that aren't useful for the plot\nplot_day = single_day.drop(['compound', 'headline', 'label'], 1)\n# Change the column names to 'negative', 'positive', and 'neutral'\nplot_day.columns = ['negative', 'neutral', 'positive']\n# Plot a stacked bar chart\nplot_day.plot.bar(stacked = True, figsize=(10, 6), title = TITLE, color = COLORS).legend(bbox_to_anchor=(1.2, 0.5))\nplt.ylabel(\"scores\")","d1df5525":"Maybe pay attention to Apple, Boeing, Mcdonald, CME. Also see what federal news has impact on stocks.\n### 1.2 Stock McDonald MCD's news","28406cdd":"Stock price is highly depend on macroeconomy, meaning press released by government, new discover across industry and all kinds of news and changes could impact stock price and contirbute to the volatility of stock market. \nLuckily, with the help of internet and searching engine, we can access financial news and hopefully extract stock sentiment from all those news. In this notebook, we will generate investing insight by applying sentiment analysis on financial news headlines. Using this natural language processing technique, we can understand the emotion behind the headlines and predict whether the market has positive or negative feeling about the stock. Thus make more appropriate guess on stock's performance and trade accordingly.","d38b08c5":"Overall MCD is not a popular stock along the time, news rarely mentioned it untill late summer as stock impacted by the economy and investors turn to defensive stocks.\nWeekends seemed to be lower than rest of the days. \nAlso some headlines from July 23rd are the same but from different outlet.\n\nWhile some headlines are the same news piece from different sources, the fact that they are written differently could provide different perspectives on the same story. Plus, when one piece of news is more important, it tends to get more headlines from multiple sources. What we want to get rid of is verbatim copied headlines, as these are very likely coming from the same journalist and are just being \"forwarded\" around, so to speak.","5183b9a7":"### 2. NLTK VADER, Rule\/lexion-based sentiment\n\n- Make an instance of SentimentIntensityAnalyzer using the existing lexicon.\n- Update the lexicon with new words.\n\n\n\nVADER ( Valence Aware Dictionary for Sentiment Reasoning) is a model used for text sentiment analysis that is sensitive to both polarity (positive\/negative) and intensity (strength) of emotion. It is available in the NLTK package and can be applied directly to unlabeled text data.\nVADER relies on a dictionary that maps lexical features to emotion intensities known as sentiment scores. The sentiment score of a sentence(text) can be obtained by summing up the intensity of each word in the text.\nhttp:\/\/www.nltk.org\/_modules\/nltk\/sentiment\/vader.html\n\nThe disadvantage of lexion-based sentiment is it estimate valence scores (which represent opinion) based on pre-defined dictionary, and different word may have different polarity in different context (stock increased\/ volatility increased)","9921a6d2":"### 5. Sentiment on single trading day and stock\n\nFocus on one trading day and one single stock. We will make an informative plot where we will see the smallest grain possible: headline and subscores\n\n- visualize the positive, negative and neutral scores for a single day of trading and a single stock","993241e4":"The Compound score is a metric that calculates the sum of all the lexicon ratings which have been normalized between -1(most extreme negative) and +1 (most extreme positive).\n\n- positive sentiment : (compound score >= 0.05) \n- neutral sentiment : (compound score > -0.05) and (compound score < 0.05) \n- negative sentiment : (compound score <= -0.05)\n\nSentence was rated % positive, % neutral, % negative; and overall rated as Pos\/Neg\/Neutral.","e598670f":"### 1. Extract news from webpages (extract data from HTML files)\n\n- headline: has similar length compared to whole news; contain summarized information with less text\n- FINVIZ.com: list of trusted websites instead of independent bloggers, also with consistent textual pattern which reduce the pre-processing time","d3661f98":"### 4. Plot sentiment score results","83d81337":"Most of the news headline is straight forward, either neutral (no positive\/negative scores), or either one of the positive\/negatvie is null while other shows number. \n\nFor the news on 4:52 McDonalds Weaker On  Covid Worries Despite Robust Growth. Note headline indicates both growth and concerns, look into news detail could help better understand the situation.","9ff5bbcd":"#### 1.1 Read all news and identify hottest topic","c31c32f1":"### 3. Predict sentiment ouf of news headlines\n\nVADER is very high level so, in this case, we will not adjust the model further other than the lexicon additions from before.\n\nVADER \"out-of-the-box\" with some extra lexicon would likely translate into heavy losses with real money. A real sentiment analysis tool with chances of being profitable will require a very extensive and dedicated to finance news lexicon. Furthermore, it might also not be enough using a pre-packaged model like VADER.","75c1f393":"Word cloud contains both stock names (entity lik boeing, nasdaq) as well as county (China, usa). \nSepcify word cloud with entity only to extract hot stock names."}}