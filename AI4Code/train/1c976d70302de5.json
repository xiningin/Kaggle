{"cell_type":{"23ea8df6":"code","28b57936":"code","2abd886d":"code","38e7852a":"code","1737726e":"code","0e06dae0":"code","4d216f85":"code","f30df831":"code","151704bb":"code","0e7c8956":"code","86fdd6c2":"code","f9aa477c":"code","ffe70e92":"code","a1295632":"code","6ef0a7d4":"code","a502d7c5":"code","eb9b7c7d":"code","5674c651":"code","67331b74":"code","0ee60fd2":"code","109ea23c":"code","99179422":"code","014b7d58":"code","a7953d46":"code","6cd2cb61":"code","8cbe805d":"code","58c73259":"code","3759ea51":"code","051feb3b":"code","d56a7981":"code","6da9ad23":"code","16eb0885":"code","b7690898":"code","ad5c123b":"code","a3ee5227":"code","4998e99f":"code","4465546e":"code","c3e4fddb":"code","91a7d7f6":"code","16999038":"code","348e8845":"code","dc038a76":"code","4325ece0":"code","e506ab97":"code","f160efc1":"code","db852a1c":"code","4774cc70":"code","c45f76c4":"code","a7ca36f0":"code","fec7bf30":"code","a543d581":"code","bdc56da7":"code","eba3d7f8":"code","b23bfeea":"code","fdfb53d1":"code","37d2a10e":"code","cd0ea262":"code","b40c91fd":"code","93d073e2":"code","0b534cb1":"code","f2613805":"code","ef5c8ea6":"code","e56998ec":"code","bc2a997d":"code","2106b2d6":"code","8c2771e0":"code","af3c2faa":"code","4256133c":"markdown","eab77c38":"markdown","8f6f47a7":"markdown","b003f38c":"markdown","8181b79b":"markdown","a71bc683":"markdown","0207d197":"markdown","27b9c7cf":"markdown","6740fcfe":"markdown","caa534f1":"markdown","92bdf8b4":"markdown","b69e8f6a":"markdown","34832e4e":"markdown","52894672":"markdown","ff8981e5":"markdown","527d054b":"markdown","921389db":"markdown","5ec221c0":"markdown","b9db349d":"markdown","a9b5c374":"markdown","af450469":"markdown","b6643ad9":"markdown","737719f2":"markdown","baa34692":"markdown","3430cb43":"markdown","ded1db4f":"markdown","34a27865":"markdown","a0fddf73":"markdown","cc88d823":"markdown","09728391":"markdown","199b3252":"markdown","c6a9c84c":"markdown","beca2756":"markdown","e51f876b":"markdown","e878df07":"markdown","118a83a3":"markdown","346c99c2":"markdown","44d8cd79":"markdown","13abb53d":"markdown","444e6344":"markdown","af4564ff":"markdown","860cf501":"markdown","a6a0adc5":"markdown","2a22649f":"markdown","8d70d504":"markdown","8d3f348f":"markdown","3b1157b1":"markdown","eee469a7":"markdown","a955d8bc":"markdown","8167b26f":"markdown","4b794614":"markdown","6d19e2c9":"markdown","da29765b":"markdown","e91319b7":"markdown","0bc924c5":"markdown","cf441ec9":"markdown","f8d6b414":"markdown","1d9a4ab3":"markdown","0da728c0":"markdown","a2f07a0e":"markdown","39e3195c":"markdown","6a7a4008":"markdown","5c7734e6":"markdown","460de32f":"markdown","528e8b7c":"markdown","b997d06f":"markdown","d82b828d":"markdown","8b533384":"markdown","771df0e1":"markdown","a55c888b":"markdown"},"source":{"23ea8df6":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns","28b57936":"dataset = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","2abd886d":"print(dataset.count().unique())\ndataset.head()","38e7852a":"dataset.isnull().sum()","1737726e":"del dataset['Unnamed: 32']","0e06dae0":"X = dataset.iloc[:, 2:].values\ny = dataset.iloc[:, 1].values","4d216f85":"X[0]","f30df831":"y[:20]","151704bb":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ny = labelencoder.fit_transform(y)","0e7c8956":"y[:20]","86fdd6c2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","f9aa477c":"print(len(X_train))\nX_train","ffe70e92":"dataset.describe()","a1295632":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6ef0a7d4":"from sklearn.linear_model import LogisticRegression\nLogisticRegressionModel = LogisticRegression(random_state = 0)","a502d7c5":"LogisticRegressionModel.fit(X_train, y_train)","eb9b7c7d":"y_pred_A1 = LogisticRegressionModel.predict(X_test)","5674c651":"from sklearn.metrics import confusion_matrix\ncm_A1 = confusion_matrix(y_test, y_pred_A1)\nprint('Confusion Matrix for Logistic Regression Model')\nsns.heatmap(cm_A1,annot=True)","67331b74":"print(\"Logistic Regression Model accuracy is {}%\".format(((cm_A1[0][0] + cm_A1[1][1])\/cm_A1.sum())*100))","0ee60fd2":"from sklearn.neighbors import KNeighborsClassifier\nKNeighborsModel = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n\nKNeighborsModel.fit(X_train, y_train)\n\ny_pred_A2 = KNeighborsModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A2 = confusion_matrix(y_test, y_pred_A2)\nprint('Confusion Matrix for KNeighbors Model')\nsns.heatmap(cm_A2,annot=True)","109ea23c":"print(\"KNeighbors Model accuracy is {}%\".format(((cm_A2[0][0] + cm_A2[1][1])\/cm_A2.sum())*100))","99179422":"from sklearn.svm import SVC\nSVCModel = SVC(kernel = 'linear', random_state=0)\n\nSVCModel.fit(X_train, y_train)\n\ny_pred_A3 = SVCModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A3 = confusion_matrix(y_test, y_pred_A3)\nprint('Confusion Matrix for SVC Model')\nsns.heatmap(cm_A3,annot=True)","014b7d58":"print(\"SVC Model accuracy is {}%\".format(((cm_A3[0][0] + cm_A3[1][1])\/cm_A3.sum())*100))","a7953d46":"from sklearn.svm import SVC\nSVCrModel = SVC(kernel = 'rbf', random_state = 0)\n\nSVCrModel.fit(X_train, y_train)\n\ny_pred_A4 = SVCrModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A4 = confusion_matrix(y_test, y_pred_A4)\nprint('Confusion Matrix for SVC Kernelized Model')\nsns.heatmap(cm_A4,annot=True)","6cd2cb61":"print(\"SVC Kernelized Model accuracy is {}%\".format(((cm_A4[0][0] + cm_A4[1][1])\/cm_A4.sum())*100))","8cbe805d":"from sklearn.naive_bayes import GaussianNB\nGaussianNBModel = GaussianNB()\n\nGaussianNBModel.fit(X_train, y_train)\n\ny_pred_A5 = GaussianNBModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A5 = confusion_matrix(y_test, y_pred_A5)\nprint('Confusion Matrix for Gaussian NB Model')\nsns.heatmap(cm_A5,annot=True)","58c73259":"print(\"Gaussian NB Model accuracy is {}%\".format(((cm_A5[0][0] + cm_A5[1][1])\/cm_A5.sum())*100))","3759ea51":"from sklearn.tree import DecisionTreeClassifier\nDecisionTreeModel = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n\nDecisionTreeModel.fit(X_train, y_train)\n\ny_pred_A6 = DecisionTreeModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A6 = confusion_matrix(y_test, y_pred_A6)\nprint('Confusion Matrix for Decision Tree Model')\nsns.heatmap(cm_A6,annot=True)","051feb3b":"print(\"Decision Tree Model accuracy is {}%\".format(((cm_A6[0][0] + cm_A6[1][1])\/cm_A6.sum())*100))","d56a7981":"from sklearn.ensemble import RandomForestClassifier\nRandomForestModel = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\nRandomForestModel.fit(X_train, y_train)\n\ny_pred_A7 = RandomForestModel.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm_A7 = confusion_matrix(y_test, y_pred_A7)\nprint('Confusion Matrix for Random Forest Model')\nsns.heatmap(cm_A7,annot=True)","6da9ad23":"print(\"Random Forest Model accuracy is {}%\".format(((cm_A7[0][0] + cm_A7[1][1])\/cm_A7.sum())*100))","16eb0885":"fuzzy_data = dataset.copy()","b7690898":"fuzzy_data['uniformity'] = fuzzy_data['radius_worst'] - fuzzy_data['radius_mean']\nfuzzy_data['homogeneity'] = fuzzy_data['symmetry_worst'] - fuzzy_data['symmetry_mean']","ad5c123b":"fuzzy_data.head()","a3ee5227":"fuzzy_data = fuzzy_data[['area_mean', 'perimeter_mean', 'uniformity', 'homogeneity', 'diagnosis']]\nfuzzy_data.head()","4998e99f":"!pip install scikit-fuzzy\nimport numpy as np\nimport skfuzzy as fuzz\nfrom skfuzzy import control as ctrl","4465546e":"fuzzy_data.describe()","c3e4fddb":"AREA = ctrl.Antecedent(np.arange(0, 2501.000000, 0.0001), 'AREA')\nPERIMETER = ctrl.Antecedent(np.arange(0, 188.500000, 0.0001), 'PERIMETER')\nUNIFORMITY = ctrl.Antecedent(np.arange(0, 11.760000, 0.0001), 'UNIFORMITY')\nHOMOGENEITY = ctrl.Antecedent(np.arange(0, 0.404100, 0.0001), 'HOMOGENEITY')","91a7d7f6":"DIAGNOSIS = ctrl.Consequent(np.arange(0, 1, 0.0001), 'DIAGNOSIS')","16999038":"AREA['Smaller'] = fuzz.trapmf(AREA.universe, [0, 0, 748.8,1000])\nAREA['Larger'] = fuzz.trapmf(AREA.universe, [508.1, 2194, 2501,2501])\nAREA.view()","348e8845":"PERIMETER['Smaller'] = fuzz.trapmf(PERIMETER.universe, [0, 0, 92.58,103])\nPERIMETER['Larger'] = fuzz.trapmf(PERIMETER.universe, [85.1, 159.8, 188.5,188.5])\nPERIMETER.view()","dc038a76":"UNIFORMITY['Smaller'] = fuzz.trapmf(UNIFORMITY.universe, [0, 0, 1.669,2.6])\nUNIFORMITY['Larger'] = fuzz.trapmf(UNIFORMITY.universe, [0.65, 6.205, 11.76,11.76])\nUNIFORMITY.view()","4325ece0":"HOMOGENEITY['Smaller'] = fuzz.trapmf(HOMOGENEITY.universe, [0, 0, 0.1232,.19])\nHOMOGENEITY['Larger'] = fuzz.trapmf(HOMOGENEITY.universe, [0.0295, 0.2168, 0.4041,0.4041])\nHOMOGENEITY.view()","e506ab97":"DIAGNOSIS['B'] = fuzz.trimf(DIAGNOSIS.universe, [0, 0, 1])\nDIAGNOSIS['M'] = fuzz.trimf(DIAGNOSIS.universe, [0, 1, 1])\nDIAGNOSIS.view()","f160efc1":"rule1 = ctrl.Rule(AREA['Smaller'] & PERIMETER['Smaller'] & UNIFORMITY['Smaller'] & HOMOGENEITY['Smaller'], DIAGNOSIS['B'])\nrule2 = ctrl.Rule(AREA['Larger'] & PERIMETER['Larger'] & UNIFORMITY['Larger'] & HOMOGENEITY['Larger'], DIAGNOSIS['M'])","db852a1c":"Diag_ctrl = ctrl.ControlSystem([rule1, rule2])","4774cc70":"Diag = ctrl.ControlSystemSimulation(Diag_ctrl)","c45f76c4":"# A list to store predections generated by the fuzzy system\nfuzzy_preds = []\n# A list to store the equivilant real label, that's because we're skipping some rows, we'll talk why\nfuzzy_real_vals = []\n\n# looping over the rows of the dataset\nfor index, row in fuzzy_data.iterrows():\n    \n    #assigning antecedents values\n    Diag.input['AREA'] = row['area_mean']\n    Diag.input['PERIMETER'] = row['perimeter_mean']\n    Diag.input['UNIFORMITY'] = row['uniformity']\n    Diag.input['HOMOGENEITY'] = row['homogeneity']\n    '''\n    here we'll try to compute the output of the fuzzy system\n    but why TRY ? why not compute it directly\n    do you remember when we said we have a total of 16 rules for our fuzzy system\n    and we're using only 2 of them becuase the rest outputs Undefined\n    these undefined values will cause some errors so we'll just ignore these rows\n    that why we're using Try Except, ok let's continue :D\n    '''\n    try:\n        Diag.compute()\n        \n        #as we said the fuzzy system outputs value in range [0:1]\n        #so here we discretize it and then store it\n        fuzzy_preds.append(Diag.output['DIAGNOSIS'] > 0.5)\n        \n        fuzzy_real_vals.append(y[index])\n    except:\n        pass","a7ca36f0":"print(len(y) - len(fuzzy_preds))","fec7bf30":"cm_fuzz = confusion_matrix(fuzzy_preds, fuzzy_real_vals)\nsns.heatmap(cm_fuzz,annot=True)","a543d581":"print(\"Fuzzy System accuracy is {}%\".format(((cm_fuzz[0][0] + cm_fuzz[1][1])\/cm_fuzz.sum())*100))","bdc56da7":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Input","eba3d7f8":"NNmodel = Sequential()","b23bfeea":"#We add the input layer\nNNmodel.add(Input(shape=(30,)))\n\n#we add our first hidden layer and a dropout to avoid overfitting\nNNmodel.add(Dense(30, activation='relu', kernel_initializer='uniform'))\nNNmodel.add(Dropout(0.1))\n\n\n#we add the second layer and a dropout to avoid overfitting\nNNmodel.add(Dense(16, activation='relu', kernel_initializer='uniform'))\nNNmodel.add(Dropout(0.1))\n\n\n#now we add the output layer with a sigmoid activation cuz it's a binary classification\nNNmodel.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))","fdfb53d1":"NNmodel.summary()","37d2a10e":"NNmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","cd0ea262":"from tensorflow.keras.callbacks import ModelCheckpoint\n#first we create a checkpointer to store best epochs\ncheckpointer = ModelCheckpoint(\n    filepath=\"NNweights\",\n    save_weights_only=True,\n    monitor='accuracy',\n    mode='max',\n    save_best_only=True)\n\n#we'll set epochs to 300, it won't take long\nhistory = NNmodel.fit(X_train, y_train, batch_size=100, epochs=300, callbacks=[checkpointer])","b40c91fd":"\n#first we load the best saved weights\nNNmodel.load_weights(\"NNweights\")\n\nnn_pred = NNmodel.predict(X_test)\nnn_pred = (nn_pred > 0.5)","93d073e2":"cm_nn = confusion_matrix(y_test, nn_pred)\nsns.heatmap(cm_nn,annot=True)","0b534cb1":"print(\"Neural Network accuracy is {}%\".format(((cm_nn[0][0] + cm_nn[1][1])\/cm_nn.sum())*100))","f2613805":"!pip install pygad\nimport tensorflow.keras\nimport pygad.kerasga\nimport numpy\nimport pygad","ef5c8ea6":"def fitness_func(solution, sol_idx):\n    model_weights_matrix = pygad.kerasga.model_weights_as_matrix(model=NNmodel,\n                                                                 weights_vector=solution)\n    NNmodel.set_weights(weights=model_weights_matrix)\n\n    predictions = NNmodel.predict(X_train)\n    \n    bce = tensorflow.keras.losses.BinaryCrossentropy()\n    solution_fitness = 1.0 \/ (bce(y_train, predictions).numpy() + 0.00000001)\n\n    return solution_fitness\n","e56998ec":"def callback_generation(ga_instance):\n    print(\"Generation = {generation}\".format(generation=ga_instance.generations_completed))\n    print(\"Fitness    = {fitness}\".format(fitness=ga_instance.best_solution()[1]))\n","bc2a997d":"#initialize the weights vector as a chromosome from the NN Model\nweights_vector = pygad.kerasga.model_weights_as_vector(model=NNmodel)\n\n#Create Genetic Population\nkeras_ga = pygad.kerasga.KerasGA(model=NNmodel,\n                                 num_solutions=100)\n\n\nnum_generations = 100\nnum_parents_mating = 50\ncrossover_type = \"single_point\"\nmutation_type = \"random\"\nmutation_percent_genes = 10\n\ninitial_population = keras_ga.population_weights\n\nga_instance = pygad.GA(num_generations=num_generations, \n                       num_parents_mating=num_parents_mating, \n                       initial_population=initial_population,\n                       fitness_func=fitness_func,\n                       on_generation=callback_generation,\n                       crossover_type=crossover_type,\n                       mutation_type=mutation_type,\n                       mutation_percent_genes=mutation_percent_genes)\nga_instance.run()\n\n# After the generations complete, some plots are showed that summarize how the outputs\/fitness values evolve over generations.\nga_instance.plot_result(title=\"PyGAD & Keras - Iteration vs. Fitness\", linewidth=4)\n\n# Returning the details of the best solution.\nsolution, solution_fitness, solution_idx = ga_instance.best_solution()\nprint(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\nprint(\"Index of the best solution : {solution_idx}\".format(solution_idx=solution_idx))\n\n","2106b2d6":"# Fetch the parameters of the best solution.\nbest_solution_weights = pygad.kerasga.model_weights_as_matrix(model=NNmodel,\n                                                              weights_vector=solution)\n# Set the Weights\nNNmodel.set_weights(best_solution_weights)\n\n#predict\npredictions = NNmodel.predict(X_test)\npredictions = (predictions > 0.5)","8c2771e0":"cm_ga = confusion_matrix(y_test, predictions)\nsns.heatmap(cm_ga,annot=True)","af3c2faa":"print(\"Hybrid Neural Network\/Genetic accuracy is {}%\".format(((cm_ga[0][0] + cm_ga[1][1])\/cm_ga.sum())*100))","4256133c":"#### Now let's see the result's of our fuzzy system, shall we!","eab77c38":"1. SVC Model accuracy is 98.24561403508771%\n2. SVC Kernelized Model accuracy is 98.24561403508771%\n3. Random Forest Model accuracy is 97.36842105263158%\n4. Logistic Regression Model accuracy is 96.49122807017544%\n5. KNeighbors Model accuracy is 95.6140350877193%\n6. Decision Tree Model accuracy is 92.98245614035088%\n7. Gaussian NB Model accuracy is 90.35087719298247%","8f6f47a7":"Now let's import our dataset","b003f38c":"Also our diagnosis will have a value of [0:1] which will then be mapped to Malignant and Bengin","8181b79b":"Now let's take a look at X_train for example","a71bc683":"## 1.F Decision Trees","0207d197":"Homogenity Universe","27b9c7cf":"Now let's take a look at our modified new dataset","6740fcfe":"Also we define some callbacks to show us some statistics while training","caa534f1":"Let's limit our dataset only to these values","92bdf8b4":"## 1. Machine Learning Approaches","b69e8f6a":"Now let's classify our test data and discretize the results","34832e4e":"Let's load the best chromosome of weights and classify the test data","52894672":"Well, Not Bad Right?","ff8981e5":"Uniformity Universe","527d054b":"Looking at the mean values of each feature we notice that our data have very different ranges, so we need to **Normalize** our data, or whats called **Feature Scalling** so let's do that","921389db":"Now let's the universe for our Antecedent, note that these values are based on a research by \"National Center for Biotechnology Information, U.S. National Library of Medicine\"","5ec221c0":"Now let's create a Confusion Matrix to see how our results look like","b9db349d":"let's see how many rows got skipped","a9b5c374":"First we need to split the dataset into **Features:X** and **Labels:y**","af450469":"Now with our first classifier which is Logistic Regression, Let's Create a Model","b6643ad9":"This is a Hybrid Approach, we'll be using the same previous Neural Network model, but we'll be training it using Genetic algorithms, instead of the usual Adam optimizer and fit function\n\nwe'll be using PyGAD library to do the genetic algorithms stuff","737719f2":"## The Approaches We're Going to Test Are\n\n1. Machine Learning Approaches :\n    1. Logistic Regression\n    2. K Neighbors Classifier\n    3. C-Support Vector Classification (SVC) With Linear Kernel\n    4. C-Support Vector Classification (SVC) With rbf Kernel\n    5. Gaussian Naive Bayes\n    6. Decision Trees\n    7. Random Forest\n\n2. A Fuzzy Logic Approach\n3. Neural Network Approach\n4. A Neural Network Trained With Genetic Algorithms (Hybrid Approach)","baa34692":"And finaly the Diagnosis Universe Which will not be trapezoidals but triangles","3430cb43":"Now we import the fuzzy libraries","ded1db4f":"## 1.G Random Forest","34a27865":"Now before we create our antecedents let's first check the ranges of values in our dataset so we can set the limits of our universe","a0fddf73":"Now let's train the Model using our train data","cc88d823":"Let's check for null values in the dataset","09728391":"Now let's see a confusion matrix and some accuracy","199b3252":"Perimeter Universe","c6a9c84c":"First we import some libraries to be used","beca2756":"First we create a sequential model to add layers to","e51f876b":"### After Cleaning Our Data We do The Necessary Splits","e878df07":"## 4. Neural Network Trained With Genetic Algorithms","118a83a3":"### Now Let's Rank These Machine Learning Approaches Before Moving Forward","346c99c2":"## 1.D C-Support Vector Classification (SVC) With rbf Kernel","44d8cd79":"Let's take a look at the first values of X and y","13abb53d":"## 3. Neural Network Approach","444e6344":"# Cancer Diagnosis : Approaches Comparison","af4564ff":"As we can see out **Dataframe** turned into a **Numpy Array** which is what we need for our models\n\nbut we can see that our labels:y is characters, we need to convert it to binary values, so let's do that using LabelEncoder","860cf501":"## 1.C C-Support Vector Classification (SVC) With Linear Kernel","a6a0adc5":"Let's take a look at our dataset","2a22649f":"## 1.E Gaussian Naive Bayes","8d70d504":"## 1.B K Neighbors Classifier","8d3f348f":"Now let's compile our model","3b1157b1":"Now We'll add our 3 layers to the model, the 3 layers will be Dense layers, the output layer will have only 1 unit becuase we're doing binary classification, also we add a dropout after each layer of the first 2 to avoid over fitting","eee469a7":"Great\nNow we need to split the data into **Train Data** and **Test Data**\nwe're going to set the ratio to 20% Test : 80% Train ","a955d8bc":"In this approach we'll use a simple Neural Network with only 3 layers using Keras library, so let's start","8167b26f":"Area Universe","4b794614":"We'll base our model on 4 features \n- AREA\n- PERIMETER\n- UNIFORMITY\n- HOMOGENEITY","6d19e2c9":"Let's Look at a summary of our model before compiling it","da29765b":"#### Now we need to define our rules, based on the research mentioned above out of 16 combination of rules we have, only 2 of them have an output and the other 14 have undefined output neither Malignant nor Bengin\n\nso we're only going to define these 2 rules","e91319b7":"## 1.A Logistic Regression","0bc924c5":"In this notebook we're going to use the **Breast Cancer Wisconsin (Diagnostic) Data Set** and we're going to apply different methods for classification\n\nThe problem in hand here is a binary classification problem, given a set of features we wanna know whether a specific tumor is Malignant or Benign ","cf441ec9":"## 2. A Fuzzy Logic Approach","f8d6b414":"Now we define the fitness function which is used to evaluate each population, since our problem is binary classification we'll use tensorflow.keras.losses.BinaryCrossentropy() as mesurment for the fitness","1d9a4ab3":"Great, now we have on last problem to solve in our dataset, let's take a look at some statistics form our original data","0da728c0":"Let's see a confusion matrix and some accuracy","a2f07a0e":"Now Let's create a Control Systems using these 2 rules","39e3195c":"We can see that there is a whole column of nulls in our dataset, so let's drop that column","6a7a4008":"As we can see we have 33 columns, 30 of them represents our features and 1 is the classification label and also we have 569 rows","5c7734e6":"To use fuzzy logic on our dataset we first we'll need to create 2 new features\n- uniformity : difference between the radius extreme value and the radius mean value\n- homogeneity : difference between the extreme value of symmetry and the mean value of symmetry\n\nso let's create these 2 columns in a new dataset copied from our cleaned data","460de32f":"## Data Preparation\n\nLet's start by loading some important libraries","528e8b7c":"### Now as we've created our Fuzzy System, Let's use it to classify our dataset","b997d06f":"First lets import some libraries","d82b828d":"Now let's set our limits based on these maximum values","8b533384":"Now we start classifying the test data","771df0e1":"And in order to simulate this control system, we will create a ControlSystemSimulation","a55c888b":"Now let's train it using our training data"}}