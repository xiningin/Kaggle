{"cell_type":{"ab9bf97e":"code","11dd106e":"code","5b9e5321":"code","41c98b83":"code","103ff2f8":"code","847253af":"code","02cbc798":"code","0974de09":"code","f39311ff":"code","4724f514":"code","51e114de":"code","7c2407eb":"code","e00b6d41":"code","cadca280":"code","8ba55ec8":"code","429ca43e":"code","ab86793a":"code","7ac3606b":"code","72d77124":"code","c4e4f4b7":"code","cca48805":"code","592d0745":"code","f37e29d4":"code","f9b906be":"code","8f214781":"code","67f9f121":"code","52d996ce":"code","33a96bfe":"code","572d9b42":"code","ef5e6bf0":"code","046d7ba6":"markdown","cfe95d34":"markdown","82c4cf02":"markdown","69dd147d":"markdown","d5d8a153":"markdown","f272f895":"markdown","817b689c":"markdown","8a246e9b":"markdown","0f059827":"markdown","91ffaaff":"markdown","46882ce7":"markdown","157996b0":"markdown","16929ce9":"markdown","d4a2eb5a":"markdown","a28dde97":"markdown","a488309d":"markdown","26425b30":"markdown","7e496d59":"markdown","90814fb5":"markdown","c951d907":"markdown","f57223ba":"markdown","4cc1ed28":"markdown"},"source":{"ab9bf97e":"import numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_moons, make_circles, make_blobs\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom catalyst import utils\n\nsns.set(style=\"darkgrid\", font_scale=1.4)\n%config InlineBackend.figure_format = 'retina'","11dd106e":"class LinearRegression(nn.Module):\n    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n        super().__init__()\n        self.weights = nn.Parameter(torch.randn(in_features))\n        self.bias = bias\n        if bias:\n            self.bias_term = nn.Parameter(torch.randn(out_features))\n\n    def forward(self, x):\n        x = x @ self.weights\n        if self.bias:\n            x += self.bias_term\n        return x","5b9e5321":"X, y = make_moons(n_samples=10000, random_state=42, noise=0.07)","41c98b83":"plt.figure(figsize=(16, 10))\nplt.title(\"Dataset\")\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\nplt.show()","103ff2f8":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)","847253af":"X_train_t = torch.from_numpy(X_train).to(torch.float32)\ny_train_t = torch.from_numpy(y_train).to(torch.float32)\nX_val_t = torch.from_numpy(X_val).to(torch.float32)\ny_val_t = torch.from_numpy(y_val).to(torch.float32)","02cbc798":"train_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset = TensorDataset(X_val_t, y_val_t)\ntrain_dataloader = DataLoader(train_dataset, batch_size=128)\nval_dataloader = DataLoader(val_dataset, batch_size=128)","0974de09":"model = nn.Sequential(\n    LinearRegression(2, 1),\n)","f39311ff":"loss_function = nn.BCEWithLogitsLoss()","4724f514":"optimizer = torch.optim.SGD(model.parameters(), lr=0.05)","51e114de":"def train(model, epochs):\n    losses = []\n    max_epochs = epochs\n    stop_it = False\n    for epoch in range(max_epochs):\n        utils.set_global_seed(42 + epoch)\n        for it, (X_batch, y_batch) in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            outp = model(X_batch)\n            loss = loss_function(outp.flatten(), y_batch)\n            loss.backward()\n            losses.append(loss.detach().flatten()[0])\n            optimizer.step()\n            probabilities = torch.sigmoid(outp)\n            preds = (probabilities>0.5).type(torch.long)\n    return model, losses","7c2407eb":"model, losses = train(model, 20)","e00b6d41":"def show_loss(losses):\n    plt.figure(figsize=(12, 8))\n    plt.plot(range(len(losses)), losses)\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Loss\")\n    plt.show()","cadca280":"show_loss(losses)","8ba55ec8":"def show_separation(model, save=False, name_to_save=\"\"):\n    sns.set(style=\"white\")\n\n    xx, yy = np.mgrid[-1.5:2.5:.01, -1.:1.5:.01]\n    grid = np.c_[xx.ravel(), yy.ravel()]\n    batch = torch.from_numpy(grid).type(torch.float32)\n    with torch.no_grad():\n        probs = torch.sigmoid(model(batch).reshape(xx.shape))\n        probs = probs.numpy().reshape(xx.shape)\n\n    f, ax = plt.subplots(figsize=(16, 10))\n    ax.set_title(\"Decision boundary\", fontsize=14)\n    contour = ax.contourf(xx, yy, probs, 25, cmap=\"RdBu\",\n                          vmin=0, vmax=1)\n    ax_c = f.colorbar(contour)\n    ax_c.set_label(\"$P(y = 1)$\")\n    ax_c.set_ticks([0, .25, .5, .75, 1])\n\n    ax.scatter(X[100:,0], X[100:, 1], c=y[100:], s=50,\n               cmap=\"RdBu\", vmin=-.2, vmax=1.2,\n               edgecolor=\"white\", linewidth=1)\n\n    ax.set(xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n    if save:\n        plt.savefig(name_to_save)\n    else:\n        plt.show()","429ca43e":"show_separation(model)","ab86793a":"@torch.no_grad()\ndef predict(dataloader, model):\n    model.eval()\n    predictions = np.array([])\n    for x_batch, _ in dataloader:\n        outp = model(x_batch)\n        probs = torch.sigmoid(outp)\n        preds = (probs > 0.5).type(torch.long)\n        predictions = np.hstack((predictions, preds.numpy().flatten()))\n    predictions = predictions\n    return predictions.flatten()","7ac3606b":"from sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_val, predict(val_dataloader, model))\naccuracy","72d77124":"model = nn.Sequential(\n    nn.Linear(2, 30),\n    nn.ReLU(),\n    nn.Linear(30, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n)\n\nloss_function = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)","c4e4f4b7":"model, losses = train(model, 30)\nshow_loss(losses)","cca48805":"show_separation(model)","592d0745":"accuracy = accuracy_score(y_val, predict(val_dataloader, model))\naccuracy","f37e29d4":"X, y = make_circles(n_samples=10000, random_state=42, noise=0.02)","f9b906be":"X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)","8f214781":"X_train_t = torch.from_numpy(X_train).to(torch.float32)\ny_train_t = torch.from_numpy(y_train).to(torch.float32)\nX_val_t = torch.from_numpy(X_val).to(torch.float32)\ny_val_t = torch.from_numpy(y_val).to(torch.float32)","67f9f121":"train_dataset = TensorDataset(X_train_t, y_train_t)\nval_dataset = TensorDataset(X_val_t, y_val_t)\ntrain_dataloader = DataLoader(train_dataset, batch_size=128)\nval_dataloader = DataLoader(val_dataset, batch_size=128)","52d996ce":"model = nn.Sequential(\n    nn.Linear(2, 30),\n    nn.ReLU(),\n    nn.Linear(30, 20),\n    nn.ReLU(),\n    nn.Linear(20, 1)\n)\n\nloss_function = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.05)","33a96bfe":"model, losses = train(model, 50)\nshow_loss(losses)","572d9b42":"show_separation(model)","ef5e6bf0":"accuracy = accuracy_score(y_val, predict(val_dataloader, model))\naccuracy","046d7ba6":"Let's take a look at what happens in logistic regression. At the input we have a matrix object-attribute X and a column-vector $y$ - labels from $\\{0, 1\\}$ for each object. We want to find a matrix of weights $W$ and a bias $b$ that our model $XW + b$ will somehow predict the class of the object. As you can see in the output, our model can produce a number in the range from $(-\\infty;\\infty)$. This output is commonly referred to as \"logits\". We need to translate it to the interval from $[0; 1]$ in order for it to give us the probability of the object belonging to the first class, it is also better for this function to be monotonic, quickly calculated, have a derivative and on $-\\infty$ have the value $0$, on $+\\infty$ - $1$. This class of functions is called sigmoid. Most often we take as a sigmoid\n$$\n\\sigma(x) = \\frac{1}{1 + e^{-x}}.\n$$\n\nI will write a PyTorch module that implements $logits = XW + b$, where $W$ and $b$ are [parameters](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.parameter.Parameter.html) (`nn.Parameter`) of the model. In other words, here I implement the [module `nn.Linear`](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Linear.html) with our own hands. I will initialize weights with normal distribution (`torch.randn`).","cfe95d34":"Train 30 epochs and show results:","82c4cf02":"### Train loop\n\nHere is some pseudo code to help you figure out what happens during training:\n\n```python\nfor epoch in range(max_epochs):  # <----------- iterate over the dataset several times\n    for x_batch, y_batch in dataset:  # <------ iterate over the dataset. Since we use SGD and not GD, we take batches of a given size\n        optimizer.zero_grad()  # <------------- zero out the gradients of the model\n        outp = model(x_batch)  # <------------- get \"logits\" from the model\n        loss = loss_func(outp, y_batch)  # <--- calculate \"loss\" for logistic regression\n        loss.backward()  # <------------------- calculate gradients\n        optimizer.step()  # <------------------ do a gradient descent step\n        if convergence:  # <------------------- in case of convergence, exit the cycle\n            break\n```\n\nOur actual training function will look like this:","69dd147d":"Convert to `torch.tensor`:","d5d8a153":"[Blog post](https:\/\/blog.neuralpony.com\/2019\/06\/17\/simple-sklearn-datasets\/)","f272f895":"In this notebook I will demonstarte how you can use barebones PyTorch to separate linearly inseparable toy **[sklearn.make_moons](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_moons.html)** and **[sklearn.make_circles](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_circles.html)** datasets.","817b689c":"This is much better. We can see the **accuracy is almost 100%**","8a246e9b":"**Accuracy**","0f059827":"### Make Moons dataset\n\nLet's initialize `make_moons` dataset:","91ffaaff":"First, let's make necessary imports:","46882ce7":"Now, as we prepared our data, we can define our model with `nn.Sequential`:","157996b0":"### Make Circles Dataset\n\nFor demonstrative purposes we can do the same for **`make_circles`** dataset","16929ce9":"We got what we expected - **two \"zones\"**. Logistic regression module did the best it could. We can measure the accuracy of the model, but first we have to define `predict` function:","d4a2eb5a":"## Logistic Regression","a28dde97":"Before we proceed we will split the dataset into **tain\/validation**:","a488309d":"We can look at it in this way:","26425b30":"### Simple Neural Network\n\nFor better result however we need something a little more complex. Again we will use `nn.Sequential` to make our model. This time it contains three `nn.Linear` layers and two activation functions. We will use the same loss function and optimizer:","7e496d59":"For optimizer we will use **Stochastic gradient descent** with learning rate = 0.05.","90814fb5":"And make `TensorDataset`'s from them:","c951d907":"For loss function we will use [**`nn.BCEWithLogitsLoss`**](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.BCEWithLogitsLoss.html) - it combines a Sigmoid layer and the BCELoss (binary cross-entropy loss) in one single class.","f57223ba":"### Visualizing\n\nWith neat playing with matplotlib we can visualize the separation between classes in two zones:","4cc1ed28":"### Loss\n\nNext important step is to **plot loss function**:"}}