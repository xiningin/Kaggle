{"cell_type":{"98263c6f":"code","40c7430d":"code","0c110654":"code","1ec6fa14":"code","c146618f":"code","0dbf000c":"code","43825be0":"code","58d18fe8":"code","daf89e75":"code","563bdf96":"code","710ede99":"code","d00d4d9c":"code","b21d664b":"code","de6da2d1":"code","2aa3ac1b":"code","724e5242":"code","87d8398e":"code","b99040d5":"code","6233fa64":"code","c31ff12d":"code","46d1ea6b":"code","1faec9d3":"code","2e826b6f":"code","437df4ad":"code","553d9c57":"code","1d46eba4":"code","351b52d9":"code","7c6cec26":"code","e634eee3":"code","bb2c7c97":"code","ee669cfe":"code","7ae82242":"code","ae8bc734":"code","d16c48fe":"code","ab18a0b1":"code","78c6001a":"code","018fad83":"code","08eff0f0":"code","50962dd3":"code","3fe34788":"code","3b9ceab8":"code","ef634608":"code","caba31b8":"code","c8a33179":"code","dbb56f92":"code","74cfcc57":"code","963fe197":"code","f6a8ba57":"code","38ca7de3":"code","c065cfb3":"code","862e4020":"code","064a54f8":"code","b8e2c448":"code","eb4f8589":"code","bf7d6a62":"code","0c39a0a8":"code","e5135774":"code","f9d3d961":"code","1554b34a":"code","4e43f201":"code","2d608c1a":"code","610df7a6":"markdown","36d5a7a7":"markdown","ecece00a":"markdown","2d143548":"markdown","6d887f87":"markdown","9406bd0c":"markdown","2d90572f":"markdown","87e2beee":"markdown","9cffcf24":"markdown","4d272c47":"markdown","79a019a9":"markdown","48153f1c":"markdown","ad010e22":"markdown","6d6927a8":"markdown","0fa940a8":"markdown","f43d965e":"markdown","ec48d444":"markdown","8c400e97":"markdown","071d9a04":"markdown","fcd1e2c6":"markdown","0f5ee04e":"markdown","bdb2b2c3":"markdown","7b653e9d":"markdown","cb5d33f3":"markdown","c12843aa":"markdown","0d487eea":"markdown","eb9c30d9":"markdown","86deb3be":"markdown","fb92b44e":"markdown"},"source":{"98263c6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\n\nwarnings.filterwarnings(action=\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nimport copy as cp\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n%pylab inline\n\nplt.style.use('seaborn-poster')\nsns.set(font_scale=2)\nrandom_seed = 123456789\nnp.random.seed(random_seed)\n\nfrom sklearn.metrics            import mean_absolute_error,median_absolute_error,\\\n                                       mean_squared_error,mean_squared_log_error,r2_score,make_scorer\n\nfrom sklearn.preprocessing      import StandardScaler,PolynomialFeatures\nfrom sklearn.feature_extraction import DictVectorizer\n\nfrom sklearn.pipeline           import make_pipeline\nfrom sklearn.linear_model       import LinearRegression,Ridge,RidgeCV,Lasso,LassoCV,BayesianRidge\nfrom sklearn.svm                import LinearSVR,SVR\n\nfrom sklearn.feature_selection  import RFE\n\nfrom sklearn.model_selection    import train_test_split\nfrom sklearn.model_selection    import ShuffleSplit,GridSearchCV\n\n\ndef getMetrics(val_reels, val_predites):\n    correlations = pd.DataFrame(list(zip(val_predites,val_reels)), columns=['predictions','valeurs'])\n    return sqrt(mean_squared_error(val_reels, val_predites)),sqrt(abs(r2_score(val_reels, val_predites))),correlations.astype(float).corr().valeurs[0]   \n\ndef affichageMetricsSansLog(val_reels, val_predites):\n    print(\"Mean absolute error                 MAE   : %.6f\" % mean_absolute_error(val_reels, val_predites))\n    print(\"Mean squared error                  MSE   : %.6f\" % mean_squared_error(val_reels, val_predites))\n    print(\"Root mean squared error             RMSE  : %.6f\" % sqrt(mean_squared_error(val_reels, val_predites)))\n    print(\"-\"*50,\"\\n\\tPour comparer voici \\n\\tla distribution des valeurs de la variable :\\n\",\"-\"*50,\"\\n\",pd.Series(val_reels).describe(),\"\\n\",\"-\"*50,)\n\n    correlations = pd.DataFrame(list(zip(val_predites,val_reels)), columns=['predictions','valeurs'])\n    sns.set(font_scale=2)\n    plt.figure(figsize=(8,8))\n    plt.title('Correlation Pearson des variables', y=1.05, size=24)\n    ax = sns.heatmap(correlations.astype(float).corr(),linewidths=0.3,vmax=1.0,fmt =\"1.4f\", \n                square=True, cmap='coolwarm', linecolor='white', annot=True)\n    ax.set_xlim(0,2);# bug Matplotlib 3.1.1\n    ax.set_ylim(2,0);\n    plt.show()    \n    \n    sns.set(font_scale=2)\n    sns.jointplot(x='valeurs',y='predictions',\n                  data=correlations, kind='reg', size=25)\n\nnoms = [\n        \"LinearRegressionRidge\",\n        \"BayesianRidge\",\n        \"SVR_linear\",\n       ]\nregresseurs = [\n    Ridge(),\n    BayesianRidge(),\n    SVR(kernel='linear', C=0.005),\n  ]\n\nregresseurs  = { nom:reg  for nom,reg in zip(noms,regresseurs)}\nmodelRFE     = {}    \n\ndef report(results, n_top=3):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")\n            \ndef choixModelsRegressions(regresseurs):\n    \"\"\"Le choix des regresseurs.\"\"\"\n    noms = [x for x in regresseurs]\n    print('Initialisation des models\\n\\n---Si vous sesisez un ou plusieurs carat\u00e8res le modele est effac\u00e9 !---\\n')\n\n    for model in noms:\n        if input('Model : '+model+' -- ') != '':\n            regresseurs.pop(model)\n            \n            \ndef calculRegressions(X_train, X_test, y_train, y_test):\n    rmse,r,p = dict(),dict(),dict()\n    \n    yRegressions = pd.DataFrame()\n    yRegressions['Observations']=y_test\n\n\n    for nom in regresseurs:\n        t1 = time.time()  \n        yRegressions[nom] = regresseurs[nom].fit(X_train, y_train).predict(X_test)\n        rmse[nom],r[nom],p[nom] = getMetrics(y_test, yRegressions[nom])\n        print('Ex\u00e9cution  '+nom.ljust(30)+(': %.2fs' % (time.time() - t1)).lstrip('0')+('  p = %.12f' % p[nom]).lstrip('0'))\n\n    resultats = pd.DataFrame(pd.Series(rmse),columns=[\"RMSE\"])\n    resultats[\"R\"] = pd.Series(r)\n    resultats[\"Pearson\"] = pd.Series(p)\n    resultats.sort_values(by='Pearson',ascending=False, inplace=True)\n    return resultats,yRegressions, regresseurs\n\ndef initRFE(X, y, regresseurs, modelRFE):\n    for nom in regresseurs:\n        t1 = time.time()  \n        modelRFE[nom] = RFE(regresseurs[nom])\n        modelRFE[nom].fit(X,y)\n    return modelRFE\n\ndef calculRegressionsRFE(X, y, regresseurs, modelRFE, modelOcSVM=None, taux = 0.3):\n    rmse,r,p = dict(),dict(),dict()\n\n    for nom in regresseurs:\n        t1 = time.time() \n        \n        X_train, X_test, y_train, y_test = train_test_split(\n                       X, \n                       y,\n                       test_size=taux, \n                       random_state=random_seed\n                   )\n        if modelOcSVM is not None :\n            newX = modelOcSVM.predict(X_train)\n            X_train, y_train = X_train[newX != -1], y_train[newX != -1]\n        else :\n            newX = np.ones(X_train.shape[0])\n        \n        X_train, X_test = X_train[X_train.columns[modelRFE[nom].support_]],X_test[X_test.columns[modelRFE[nom].support_]]\n        \n        yRegressions[nom] = regresseurs[nom].fit(X_train, y_train).predict(X_test)\n        rmse[nom],r[nom],p[nom] = getMetrics(y_test, yRegressions[nom])\n        print('Ex\u00e9cution  '+nom.ljust(30)+(': %.2fs' % (time.time() - t1)).lstrip('0')+('  p = %.12f' % p[nom]).lstrip('0'))\n\n    resultats = pd.DataFrame(pd.Series(rmse),columns=[\"RMSE\"])\n    resultats[\"R\"] = pd.Series(r)\n    resultats[\"Pearson\"] = pd.Series(p)\n    resultats.sort_values(by='Pearson',ascending=False, inplace=True)\n    \n    return resultats,yRegressions,regresseurs,modelRFE","40c7430d":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\ndonnees = pd.concat([test,train],sort=False)\nprint('Train size : ',train.shape,'\\nTest size :',test.shape)\ndonnees.tail()","0c110654":"Cible           = 'SalePrice'\ncolQualitatives = ['MSSubClass', 'OverallQual', 'OverallCond'] + list(donnees.dtypes[donnees.dtypes == 'object'].reset_index()['index'].values)\ncolQuantitatives = list(donnees.drop(columns=colQualitatives + [Cible]).columns) ","1ec6fa14":"# Processing missing data\u00b6\n## Processing of quantitative data","c146618f":"donnees[colQuantitatives].isna().sum()[\n              donnees[colQuantitatives].isna().sum() > 0\n             ].sort_values()","0dbf000c":"plt.figure(figsize=(70,70))\nplt.title('Correlation Pearson des variables', y=1.05, size=80)\nsns.set(font_scale=3)\n\nax = sns_plot = sns.heatmap(donnees[colQuantitatives].corr(), fmt= '.2f',linewidths=0.3,vmax=1.0, \n            square=True, cmap='coolwarm', linecolor='white', annot=True, center = 0);\ntaille = len(colQuantitatives)\nax.set_xticklabels(ax.get_xticklabels(), fontsize = 40)\nax.set_yticklabels(ax.get_yticklabels(), fontsize = 40)\nax.set_xlim(0,taille);# bug Matplotlib 3.1.1\nax.set_ylim(taille,0);\nsns.set(font_scale=2)","43825be0":"variables = donnees[colQuantitatives].isna().sum()[\n              (donnees[colQuantitatives].isna().sum() > 0)&\n              (donnees[colQuantitatives].isna().sum() < 24)\n             ].reset_index()['index']\n\nfor variable in variables:\n    donnees[variable].fillna(train[variable].mean(),inplace=True)","58d18fe8":"donnees[colQuantitatives].isna().sum()[\n              donnees[colQuantitatives].isna().sum() > 0\n             ].sort_values()","daf89e75":"dataGarageYrBlt =  ['GarageYrBlt','YearBuilt','YearRemodAdd',\n                    'TotalBsmtSF','FullBath', '1stFlrSF', \n                    'GrLivArea','GarageCars','GarageArea']\n\nplt.figure(figsize=(60,60))\nplt.title('Correlation Pearson des variables', y=1.05, size=80)\nsns.set(font_scale=3)\nax = sns_plot = sns.heatmap(donnees[dataGarageYrBlt].corr(), fmt= '.2f',linewidths=0.3,vmax=1.0, \n            square=True, cmap='coolwarm', linecolor='white', annot=True);\ntaille = donnees[dataGarageYrBlt].columns.size\nax.set_xticklabels(ax.get_xticklabels(), fontsize = 40)\nax.set_yticklabels(ax.get_yticklabels(), fontsize = 40)\nax.set_xlim(0,taille);# bug Matplotlib 3.1.1\nax.set_ylim(taille,0);\nsns.set(font_scale=2)","563bdf96":"fig, (ax0, ax1) = plt.subplots(1, 2)\ndonnees[~donnees.GarageYrBlt.isna() ].GarageYrBlt.hist(figsize=(16,10),bins=50, color='navy',edgecolor='white',ax=ax0);\ndonnees[~donnees.GarageYrBlt.isna()& (donnees.GarageYrBlt < 2010) ].GarageYrBlt.hist(figsize=(16,10),bins=50, color='navy',edgecolor='white',ax=ax1);","710ede99":"donnees[~donnees.GarageYrBlt.isna()].GarageYrBlt.describe()","d00d4d9c":"X = donnees[(~donnees.GarageYrBlt.isna()) & (donnees.GarageYrBlt < 2010) ][dataGarageYrBlt[1:]]\ny = donnees[~donnees.GarageYrBlt.isna() & (donnees.GarageYrBlt < 2010)  ].GarageYrBlt.ravel()\nmodelSS = StandardScaler()\nmodelRL = LinearRegression()\nmodelRL.fit(modelSS.fit_transform(X),np.log(y))\ncoef = pd.DataFrame({'Coef':modelRL.coef_})\ncoef['Label'] = X.columns\ncoef.sort_values(by='Coef')","b21d664b":"y_pred = np.exp(modelRL.predict(modelSS.fit_transform(X)))\naffichageMetricsSansLog(pd.Series(y), pd.Series(y_pred))","de6da2d1":"regresseur = make_pipeline(PolynomialFeatures(4), Ridge())\nregresseur.fit(modelSS.fit_transform(X),np.log(y))\ny_pred = np.exp(regresseur.predict(modelSS.fit_transform(X)))\naffichageMetricsSansLog(pd.Series(y), pd.Series(y_pred))","2aa3ac1b":"X = donnees[( donnees.GarageYrBlt.isna()) | (donnees.GarageYrBlt > 2010) ][dataGarageYrBlt[1:]]\ny = donnees[( donnees.GarageYrBlt.isna()) | (donnees.GarageYrBlt > 2010) ].GarageYrBlt\ny_pred = pd.Series(np.exp(regresseur.predict(modelSS.fit_transform(X))), index=y.index)\ndonnees.GarageYrBlt[( donnees.GarageYrBlt.isna()) | (donnees.GarageYrBlt > 2010) ]  = y_pred\ndonnees.GarageYrBlt = donnees.GarageYrBlt.astype('int16')","724e5242":"dataLotFrontage = ['LotArea','TotalBsmtSF', '1stFlrSF','GrLivArea',\n                   'BedroomAbvGr', 'TotRmsAbvGrd','Fireplaces',\n                   'GarageCars', 'GarageArea']\nplt.figure(figsize=(60,60))\nplt.title('Correlation Pearson des variables', y=1.05, size=80)\nsns.set(font_scale=3)\nax = sns_plot = sns.heatmap(donnees[dataLotFrontage].corr(), fmt= '.2f',linewidths=0.3,vmax=1.0, \n            square=True, cmap='coolwarm', linecolor='white', annot=True);\ntaille = donnees[dataLotFrontage].columns.size\nax.set_xticklabels(ax.get_xticklabels(), fontsize = 40)\nax.set_yticklabels(ax.get_yticklabels(), fontsize = 40)\nax.set_xlim(0,taille);# bug Matplotlib 3.1.1\nax.set_ylim(taille,0);\nsns.set(font_scale=2)","87d8398e":"fig, (ax0, ax1) = plt.subplots(1, 2)\ndonnees[~donnees.LotFrontage.isna()].LotFrontage.hist(figsize=(16,10),bins=50, color='navy',edgecolor='white',ax=ax0);\ndonnees[(~donnees.LotFrontage.isna()) & (donnees.LotFrontage < 175) ].LotFrontage.hist(figsize=(16,10),bins=50, color='navy',edgecolor='white',ax=ax1);","b99040d5":"donnees[~donnees.LotFrontage.isna()].LotFrontage.describe()","6233fa64":"X = donnees[(~donnees.LotFrontage.isna()) & (donnees.LotFrontage < 175) ][dataLotFrontage[1:]]\ny = donnees[(~donnees.LotFrontage.isna()) & (donnees.LotFrontage < 175) ].LotFrontage.ravel()\nmodelSS = StandardScaler()\nmodelRL = LinearRegression()\nmodelRL.fit(modelSS.fit_transform(X),np.log(y))\n\ncoef = pd.DataFrame({'Coef':modelRL.coef_})\ncoef['Label'] = X.columns\ncoef.sort_values(by='Coef')","c31ff12d":"y_pred = np.exp(modelRL.predict(modelSS.fit_transform(X)))\naffichageMetricsSansLog(pd.Series(y), pd.Series(y_pred))","46d1ea6b":"regresseur = make_pipeline(PolynomialFeatures(4), Ridge())\nregresseur.fit(modelSS.fit_transform(X),np.log(y))\ny_pred = np.exp(regresseur.predict(modelSS.fit_transform(X)))\naffichageMetricsSansLog(pd.Series(y), pd.Series(y_pred))","1faec9d3":"X = donnees[donnees.LotFrontage.isna()][dataLotFrontage[1:]]\ny = donnees[donnees.LotFrontage.isna()].LotFrontage\n\ny_pred = pd.Series(np.exp(regresseur.predict(modelSS.fit_transform(X))), index=y.index)\ndonnees.LotFrontage[ donnees.LotFrontage.isna() ]  = y_pred\ndonnees.LotFrontage = donnees.LotFrontage.astype('int16')","2e826b6f":"donnees[colQualitatives].isna().sum()[\n              donnees[colQualitatives].isna().sum() > 0\n             ].sort_values()","437df4ad":"for col in donnees[colQualitatives].isna().sum()[\n              donnees[colQualitatives].isna().sum() > 0\n             ].reset_index()['index']:\n    print( \"%12s\"%col,'-',\"%2d\"%len(donnees[col].unique()),' - ',donnees[col].unique().tolist()[:8])\ndonnees[colQualitatives] = donnees[colQualitatives].fillna('Non Affect\u00e9e')    ","553d9c57":"donnees.BsmtFullBath = donnees.BsmtFullBath.round(2).astype('str')\ncolQualitatives.append('BsmtFullBath')\ndonnees.BsmtHalfBath = donnees.BsmtHalfBath.round(2).astype('str')\ncolQualitatives.append('BsmtHalfBath')\ndonnees.GarageCars = donnees.GarageCars.round(2).astype('str')\ncolQualitatives.append('GarageCars')","1d46eba4":"donnees['PoolArea'] = pd.cut(donnees['PoolArea'],[-0.01, 0, 800])\ncolQualitatives.append('PoolArea')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='PoolArea',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","351b52d9":"donnees['ScreenPorch'] = pd.cut(donnees['ScreenPorch'],[-0.01, 0, 100, 200, 300, 576])\ncolQualitatives.append('ScreenPorch')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='ScreenPorch',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","7c6cec26":"donnees['3SsnPorch'] = pd.cut(donnees['3SsnPorch'],[-0.01, 0, 508])\ncolQualitatives.append('3SsnPorch')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='3SsnPorch',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","e634eee3":"donnees['EnclosedPorch'] = pd.cut(donnees.EnclosedPorch,[-0.01, 0, 100, 200, 300, 1012])\ncolQualitatives.append('EnclosedPorch')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='EnclosedPorch',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","bb2c7c97":"donnees['BsmtFinSF2'] = pd.cut(donnees.BsmtFinSF2,[-0.01, 0, 380, 760, 1526])\ncolQualitatives.append('BsmtFinSF2')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='BsmtFinSF2',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","ee669cfe":"donnees['LowQualFinSF'] = pd.cut(donnees.LowQualFinSF,[-0.01, 0, 1070])\ncolQualitatives.append('LowQualFinSF')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='LowQualFinSF',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=45, fontsize=16)\nplt.show()","7ae82242":"for col in ['YearBuilt','YearRemodAdd','GarageYrBlt','MiscVal','MoSold','YrSold']:\n    print( \"%12s\"%col,'-',\"%2d\"%len(donnees[col].unique()),' - ',donnees[col].unique().tolist()[:12])","ae8bc734":"donnees['YearBuilt'] = pd.cut(donnees.YearBuilt,[1800,1930,1950,1960,1970,1980,1990,2000,2003,2006,2010])\ncolQualitatives.append('YearBuilt')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='YearBuilt',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90, fontsize=16)\nplt.show()","d16c48fe":"donnees['YearRemodAdd'] = pd.cut(donnees.YearRemodAdd,[1940,1970,1980,1990,1995,2000,2003,2006,2010])\ncolQualitatives.append('YearRemodAdd')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='YearRemodAdd',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90, fontsize=16)\nplt.show()","ab18a0b1":"donnees['GarageYrBlt'] = pd.cut(donnees.GarageYrBlt,[1000,1900,1960,1970,1980,1990,1995,2000,2003,2006,2010,3000])\ncolQualitatives.append('GarageYrBlt')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='GarageYrBlt',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90, fontsize=16)\nplt.show()","78c6001a":"donnees['DateSold'] = (donnees.YrSold.astype('str')+donnees.MoSold.apply(lambda x : \"%02d\"%x))\ncolQualitatives.append('DateSold')\nplt.figure(figsize=(14,12))\nsns_plot = sns.countplot(x='DateSold',data=donnees);    \nsns_plot.set_xticklabels(sns_plot.get_xticklabels(), rotation=90, fontsize=16)\nplt.show()","018fad83":"donnees.drop(columns=['MoSold','YrSold'],inplace=True)","08eff0f0":"for col in ['FullBath','HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd','Fireplaces','MiscVal']:\n    colQualitatives.append(col)\n    print( \"%13s\"%col,'-',\"%3d\"%len(donnees[col].unique()),' - ',\n           donnees[col].sort_values().unique().tolist()[:15])","50962dd3":"for col in donnees.dtypes[donnees.dtypes == 'category'].reset_index()['index'] :\n    donnees[col] = donnees[col].astype('str')","3fe34788":"modelVect = DictVectorizer()#(separator='_')\nmodelVect.fit(donnees[colQualitatives].to_dict('records'))\n\n# DataFrame avec les variables cat\u00e9gorielles\ndonneesQualitatives = modelVect.transform(donnees[colQualitatives].to_dict('records'))\ndonneesQualitatives = pd.DataFrame(\n                                   donneesQualitatives.toarray(), \n                                   columns=modelVect.feature_names_,\n                                   index=donnees.index\n                                   )","3b9ceab8":"donneesQualitatives.head()","ef634608":"donneesQualitatives.shape,donnees[colQualitatives].shape","caba31b8":"colQuantitatives = list(donnees.drop(columns=colQualitatives + [Cible]).columns) \nmodelSS = StandardScaler()\ndonneesQuantitatives = pd.DataFrame(\n                                   modelSS.fit_transform(donnees[colQuantitatives]), \n                                   columns=donnees[colQuantitatives].columns,\n                                   index=donnees.index\n                                   )\ndonneesQuantitatives.head()","c8a33179":"plt.figure(figsize=(60,60))\nplt.title('Correlation Pearson des variables', y=1.05, size=80)\nsns.set(font_scale=3)\nax = sns_plot = sns.heatmap(donneesQuantitatives.corr(), fmt= '.2f',linewidths=0.3,vmax=1.0, \n            square=True, cmap='coolwarm', linecolor='white', annot=True);\ntaille = donneesQuantitatives.columns.size\nax.set_xticklabels(ax.get_xticklabels(), fontsize = 30)\nax.set_yticklabels(ax.get_yticklabels(), fontsize = 30)\nax.set_xlim(0,taille);# bug Matplotlib 3.1.1\nax.set_ylim(taille,0);\nsns.set(font_scale=2)","dbb56f92":"plt.figure(figsize=(24,20))\nfor i,col in enumerate(donneesQuantitatives.columns):\n    plt.subplot(3, 4, i+1)\n    plt.title('distribution %s' % col)\n    donneesQuantitatives[col].hist(bins=50, color='navy',edgecolor='white')\nplt.show()    ","74cfcc57":"from sklearn.preprocessing import PowerTransformer\nyeoJohn = PowerTransformer(method='yeo-johnson')\n\ndonneesQuantitatives = pd.DataFrame(\n                                   yeoJohn.fit_transform(donneesQuantitatives),\n                                   columns=donneesQuantitatives.columns,\n                                   index=donneesQuantitatives.index\n                                   )","963fe197":"plt.figure(figsize=(24,20))\nfor i,col in enumerate(donneesQuantitatives.columns):\n    plt.subplot(3, 4, i+1)\n    plt.title('distribution %s' % col)\n    donneesQuantitatives[col].hist(bins=50, color='navy',edgecolor='white')\nplt.show()    ","f6a8ba57":"fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(36, 12))\nsns.distplot(donnees.SalePrice[~donnees.SalePrice.isna()],ax=ax0);\nsns.distplot(np.log(donnees.SalePrice[~donnees.SalePrice.isna()]),ax=ax1);","38ca7de3":"donneesFinal = donneesQuantitatives.join(donneesQualitatives)\ndonneesFinal['SalePrice'] = donnees.SalePrice\ndonneesTrain = donneesFinal[~donneesFinal.SalePrice.isna()]\ndonneesTest  = donneesFinal[donneesFinal.SalePrice.isna()]\ndonneesTest.drop(columns='SalePrice', inplace=True)\ndonneesTrain.shape,donneesTest.shape","c065cfb3":"X_train, X_test, y_train, y_test = train_test_split(\n                                                     donneesTrain.drop(columns='SalePrice').values, \n                                                     np.log(donneesTrain['SalePrice'].values),\n                                                     test_size=0.3, \n                                                     random_state=random_seed\n                                                 )\nX_train.shape, X_test.shape","862e4020":"resultats,yRegressions,regresseurs = calculRegressions(X_train, X_test, y_train, y_test)\nresultats","064a54f8":"from sklearn.svm import OneClassSVM\nmodelOcSVM = OneClassSVM(nu=0.01, gamma=0.001)\nmodelOcSVM.fit(donneesTrain.drop(columns='SalePrice'))\ny_pred = modelOcSVM.predict(donneesTrain.drop(columns='SalePrice'))\nprint('ndividuals considered outlier :',sum(y_pred==-1))","b8e2c448":"newX = modelOcSVM.predict(X_train)\nresultats,yRegressions,regresseurs = calculRegressions(X_train[newX != -1], X_test, y_train[newX != -1], y_test)\nresultats","eb4f8589":"modelRFE = initRFE(\n                    donneesTrain.drop(columns='SalePrice'),\n                    donneesTrain.SalePrice, \n                    regresseurs,\n                    modelRFE\n                   )","bf7d6a62":"resultats,yRegressions,regresseurs,modelRFE = calculRegressionsRFE(donneesTrain.drop(columns='SalePrice'), \n                   np.log(donneesTrain['SalePrice'].values), regresseurs, modelRFE, taux = 0.3)\nresultats","0c39a0a8":"resultats,yRegressions,regresseurs,modelRFE = calculRegressionsRFE(donneesTrain.drop(columns='SalePrice'), \n                   np.log(donneesTrain['SalePrice'].values), regresseurs, modelRFE, modelOcSVM, taux = 0.3)\nresultats","e5135774":"yRegressions.head()","f9d3d961":"affichageMetricsSansLog(np.exp(yRegressions.Observations), np.exp(yRegressions.LinearRegressionRidge))","1554b34a":"donneesTest.head()","4e43f201":"yRegressions = np.zeros(donneesTest.shape[0])\nfor nom in regresseurs:\n\n    X = donneesTest\n    newX = modelOcSVM.predict(X)\n\n    X = X[X.columns[modelRFE[nom].support_]]\n\n    yRegressions += regresseurs[nom].predict(X)\nyRegressions \/= len(regresseurs)\nyRegressions = np.exp(yRegressions)","2d608c1a":"submission = pd.DataFrame(yRegressions,index = donneesTest.index).reset_index()\nsubmission.columns = ['Id','SalePrice']\nsubmission.to_csv('submission01.csv',index=False)","610df7a6":">> **For the two remaining variables, we use one of the regression models in relation to the most correlated variables.**","36d5a7a7":">> **Evaluation of results without outliers**","ecece00a":"## The processing of the qualitative data\u00b6\n\nFor the qualitative variables the processing of the unaffected values is easier one adds a modality of more `Not Affected`.<br>\nThe use of the polynomial model of degree four is closer to the initial data.","2d143548":"# Preparation of the qualitative variables for learning","6d887f87":"# Train Test Split","9406bd0c":"# The separation of the validation dataset","2d90572f":">> A first list of qualitative and quantitative variables","87e2beee":"# A first evaluation of the regression models","9cffcf24":">> **All three regressors are used to perform the prediction**","4d272c47":"<img src=\"http:\/\/www.dba-expert.fr\/images\/media_documents\/kaggle\/inception_resnet_v2\/house_price.jpg\" width=\"800\">","79a019a9":">> **The use of the four-level polynomial model is closer to the original data.**\n","48153f1c":">> **Unique values for qualitative variables with NaN values**","ad010e22":"# The target variable ","6d6927a8":"# Preparation of quantitative variables for learning\n","0fa940a8":"# Importing datasets","f43d965e":"## Creation of the DataFrame with categorical variables and transformations of modalities into columns","ec48d444":"# Submission","8c400e97":"# Feature selection with **Recursive feature elimination**","071d9a04":">> **For variables or missing data are may frequent is assigned the average**","fcd1e2c6":">> **The use of the four-level polynomial model is closer to the original data.**","0f5ee04e":">> **As you can see the performances are better with both treatments**","bdb2b2c3":"### Estimation of the **GarageYrBlt**","7b653e9d":"## Estimating the LotFrontage\u00b6","cb5d33f3":"## The treatment of the distribution of quantitative variables","c12843aa":">> **Evaluation of the results after the selection of features on all the data**","0d487eea":"# Treatment of outlier","eb9c30d9":"**MSSubClass**: Identifies the type of dwelling involved in the sale.\t<br>\n>>        20\t1-STORY 1946 & NEWER ALL STYLES<br>\n>>        30\t1-STORY 1945 & OLDER<br>\n>>        40\t1-STORY W\/FINISHED ATTIC ALL AGES<br>\n>>        45\t1-1\/2 STORY - UNFINISHED ALL AGES<br>\n>>        50\t1-1\/2 STORY FINISHED ALL AGES<br>\n>>        60\t2-STORY 1946 & NEWER<br>\n>>        70\t2-STORY 1945 & OLDER<br>\n>>        75\t2-1\/2 STORY ALL AGES<br>\n>>        80\tSPLIT OR MULTI-LEVEL<br>\n>>        85\tSPLIT FOYER<br>\n>>        90\tDUPLEX - ALL STYLES AND AGES<br>\n>>       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER<br>\n>>       150\t1-1\/2 STORY PUD - ALL AGES<br>\n>>       160\t2-STORY PUD - 1946 & NEWER<br>\n>>       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER<br>\n>>       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES<br><br>\n\n**OverallQual** : Rates the overall material and finish of the house<br>\n>>       10\tVery Excellent<br>\n>>       9\tExcellent<br>\n>>       8\tVery Good<br>\n>>       7\tGood<br>\n>>       6\tAbove Average<br>\n>>       5\tAverage<br>\n>>       4\tBelow Average<br>\n>>       3\tFair<br>\n>>       2\tPoor<br>\n>>       1\tVery Poor<br><br>\n\n**OverallCond**: Rates the overall condition of the house<br>\n>>       10\tVery Excellent<br>\n>>       9\tExcellent<br>\n>>       8\tVery Good<br>\n>>       7\tGood<br>\n>>       6\tAbove Average\t<br>\n>>       5\tAverage<br>\n>>       4\tBelow Average\t<br>\n>>       3\tFair<br>\n>>       2\tPoor<br>\n>>       1\tVery Poor<br>","86deb3be":"# The performance of the regressors","fb92b44e":">> **Evaluation of the results after the selection of features and drop the outliers**"}}