{"cell_type":{"dabe20b7":"code","478d7f2f":"code","fe9d7298":"code","c3a1b788":"code","80282306":"code","e03c7cd5":"code","fd2c8289":"code","8d1365af":"code","eb071c0a":"code","601cca9a":"code","990ec106":"code","805ff01c":"code","df391be6":"code","a44be6f5":"code","ac7f432d":"code","c34d93d9":"code","9b7f8d3c":"code","68f85196":"code","28acfb2e":"code","8eac7877":"code","86ff8e1d":"code","967ec952":"code","98294bba":"code","52d79a16":"code","2edc8b25":"code","e2c50e24":"code","74c6cfd5":"code","86f87c73":"code","56547e8f":"code","3a8fe2cc":"code","48274ef3":"code","0a64f461":"code","4303caa0":"code","38647572":"code","f5445913":"code","37dd8238":"code","2675e78e":"code","225bc651":"code","43b2323f":"code","be5a1a38":"code","52870cb8":"code","8e8e9efc":"code","3a516593":"code","9fe84178":"code","6c58e2ab":"code","5d82379d":"code","53ae46fd":"code","79f045a3":"code","37f3d98c":"code","3c8ba226":"code","d54fb2e0":"code","66561451":"code","19c245e1":"code","08dfc41b":"code","21e593b8":"code","e9ad8a36":"code","884a0168":"code","9295bf23":"code","d47e18b8":"markdown","9abd07a2":"markdown","8211717e":"markdown","48907a37":"markdown","57021930":"markdown","8489e098":"markdown","6e33f99b":"markdown","ddf1e1cd":"markdown","95d45536":"markdown","3a8554bd":"markdown","94ead19e":"markdown","dcc1a15f":"markdown","65569e3b":"markdown","32991610":"markdown","c1e53b0e":"markdown","fc676a55":"markdown","11912bd2":"markdown","00afac7a":"markdown","00ed0a97":"markdown","9593823c":"markdown","edc57979":"markdown","b6dfa443":"markdown"},"source":{"dabe20b7":"# Import all the tools we need\n\n# Regular EDA (exploratory data analysis) and plotting libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to plot graphs inline\n%matplotlib inline\n\n# models from sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#model evaluation\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n","478d7f2f":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.shape #(rows,columns)\n","fe9d7298":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"salmon\",\"lightblue\"])","c3a1b788":"df.info()","80282306":"#Are there any missing values?\ndf.isna().sum()\n","e03c7cd5":"df.describe()","fd2c8289":"df.sex.value_counts()","8d1365af":"#compare target columns with sex column\npd.crosstab(df.target, df.sex)","eb071c0a":"# Create a lot of crosstab\n\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                   figsize=(10,6),\n                                   color=[\"salmon\",\"lightblue\"])\n\nplt.title(\"Heart Disease frequency per sex\")\nplt.xlabel(\"0 = No Disease, 1= Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","601cca9a":"df.head()","990ec106":"plt.figure(figsize=(10,6))\n\n#scatter with positive examples\nplt.scatter(df.age[df.target==1],\n           df.thalach[df.target==1],\n           c=\"salmon\");\n\n#scatter with negative example\n\nplt.scatter(df.age[df.target==0],\n           df.thalach[df.target==0],\n           c=\"black\");\n\nplt.title(\"Heart Disease in function of age and thalach\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Thalache (max heart rate)\")\nplt.legend([\"Disease\",\"No Disease\"]);","805ff01c":"# check the dstribution of the age column with a histogram\n\ndf.age.plot.hist(figsize=(10,6));","df391be6":"pd.crosstab( df.cp,df.target)","a44be6f5":"#make thr cross tab visual\n\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", figsize=(10,6),\n                                  color=[\"darkblue\",\"lightblue\"])\n\nplt.title(\"Heart Disease frequency per chest pain type\")\nplt.xlabel(\"Chest Pain type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"No Disease\",\"Disease\"])\nplt.xticks(rotation=0);","ac7f432d":"# Make a corelation matrix\n\ndf.corr()","c34d93d9":"# Lets make our corelation matrix a bit prettier\n\ncorr_matrix= df.corr()\nfig, ax= plt.subplots(figsize=(10,6))\nax= sns.heatmap(corr_matrix,\n               annot= True,\n               linewidths=0.5,\n               fmt=\".2f\",\n            cmap=\"twilight_r\");","9b7f8d3c":"#split data into X and y\n\nX=df.drop(\"target\", axis=1)\ny=df[\"target\"]","68f85196":"#split data into training and test split\nnp.random.seed(42)\n\n#Split into trin and test set\n\nX_train, X_test, y_train,y_test= train_test_split(X,y,test_size=0.2)","28acfb2e":"len(X_train), len(y_train)","8eac7877":"# Put models in a dictionary\n\nmodels={\"Logistic Regression\": LogisticRegression(),\n       \"KNN\":KNeighborsClassifier(),\n       \"Random Forest\": RandomForestClassifier()}\n\n#create a function to fit and score models\ndef fit_and_score(models,X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given ML models.\n    models: a dict of diff SciKit learn ML models\n    X_test :  test set (no label)\n    X_train: training set (no label)\n    y_train : training labels\n    y_test : test labels\n    \"\"\"\n    \n    #set random seed\n    np.random.seed(42)\n    #Make dictionary to keep model scores\n    \n    model_scores={}\n    \n    #Loop through models\n    for name, model in models.items():\n        #fit model to data\n        model.fit(X_train, y_train)\n        #evaluate the model and store in score dict\n        model_scores[name]= model.score(X_test, y_test)\n    return model_scores","86ff8e1d":"model_scores= fit_and_score(models=models,\n                           X_train=X_train,\n                           X_test=X_test,\n                           y_train=y_train,\n                           y_test=y_test)\n\nmodel_scores","967ec952":"model_compare =pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","98294bba":"#lets tune KNN\ntrain_scores = []\ntest_scores =  []\n\n#Create a list of diff values of n_neighbours\nneighbors = range(1,21)\n\n#Setup KNN Instance\nknn = KNeighborsClassifier()\n\n#Loop thriugh diff n_neighbours\nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    \n    #Fit the algo\n    knn.fit(X_train, y_train)\n    \n    #Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    #Update the test scores\n    test_scores.append(knn.score(X_test, y_test))\n    ","52d79a16":"train_scores","2edc8b25":"test_scores","e2c50e24":"#Visualize\n\nplt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data :{max(test_scores)*100:.2f}%\")","74c6cfd5":"#  Create a hyperparameter grid for LogisticRegression\nlog_reg_grid = {\"C\" : np.logspace(-4, 4, 20),\n            \"solver\": [\"liblinear\"]}\n\n#Create a hyperparam grid for RandomForestClassifier\nrf_grid = {\"n_estimators\" : np.arange(10, 1000, 50),\n         \"max_depth\" : [None,3,5,10],\n         \"min_samples_split\" : np.arange(2, 20, 2),\n         \"min_samples_leaf\": np.arange(1, 20, 2)}","86f87c73":"# Tune LogisticRegression\n\nnp.random.seed(42)\n\n#Setup random hyperparams search for LogisticRegression\n\nrs_log_reg= RandomizedSearchCV(LogisticRegression(),\n                              param_distributions = log_reg_grid,\n                              cv=5,\n                              n_iter=20,\n                              verbose=True)\n\n#Fit random hyperparam search model for LogisticRegression\nrs_log_reg.fit(X_train, y_train)","56547e8f":"rs_log_reg.best_params_","3a8fe2cc":"rs_log_reg.score(X_test, y_test)","48274ef3":"# Now we have tuned for LogisticRegression\n# Tune RandomFOrest","0a64f461":"np.random.seed(42)\n\n#Setup random hyperparam search for RandomFOrestClassifier\nrs_ref = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions = rf_grid,\n                          cv=5,\n                          n_iter= 20,\n                          verbose=True)\n# Fit random hyperparam search model for RandomForestCLassifier\n\nrs_ref.fit(X_train, y_train)\n\n","4303caa0":"#Finding best params\nrs_ref.best_params_","38647572":"#Evaluate RandomSearchCV search on RandomForestClassifier model\nrs_ref.score(X_test, y_test)","f5445913":"# Different hyperparameters for LR Model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n               \"solver\": [\"liblinear\"]}\n\n#Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid= log_reg_grid,\n                         cv=5,\n                         verbose=True)\n#Fit grid hyperparam search model\n\ngs_log_reg.fit(X_train, y_train);","37dd8238":"gs_log_reg.best_params_","2675e78e":"# Evaluate GridSearchCV for LR model\ngs_log_reg.score(X_test, y_test)","225bc651":"model_scores","43b2323f":"# make predictions\ny_preds=gs_log_reg.predict(X_test)","be5a1a38":"y_preds","52870cb8":"# Import ROC curve fucntion but we have done this previously.\n#  roc curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test)","8e8e9efc":"# Confusion matrix\nsns","3a516593":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y__test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"Predicted label\") # predictions go on the x-axis\n    plt.ylabel(\"True label\") # true labels go on the y-axis \n    \nplot_conf_mat(y_test, y_preds)","9fe84178":"print(classification_report(y_test, y_preds))","6c58e2ab":"# check our best hyperparams\ngs_log_reg.best_params_","5d82379d":"# create a new classifier with best params\n\nclf= LogisticRegression(C=0.20433597178569418,\n                       solver=\"liblinear\")\n# Cross validated accuracy\ncv_acc= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"accuracy\")\ncv_acc","53ae46fd":"cv_acc=np.mean(cv_acc)\ncv_acc","79f045a3":"# Cross validated precision\ncv_precision= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"precision\")\n\ncv_precision=np.mean(cv_precision)\ncv_precision","37f3d98c":"# Cross validated recall\ncv_recall= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"recall\")\n\ncv_recall=np.mean(cv_recall)\ncv_recall","3c8ba226":"# Cross validated f1\ncv_f1= cross_val_score(clf,\n                       X,\n                       y,\n                       cv=5,\n                       scoring=\"f1\")\n\ncv_f1=np.mean(cv_f1)\ncv_f1","d54fb2e0":"# putting it in a graph visualize \ncv_metrics= pd.DataFrame({\"Accuracy\": cv_acc,\n                         \"Precision\": cv_precision,\n                         \"Recall\": cv_recall,\n                         \"f1\": cv_f1},\n                        index=[0])\n\ncv_metrics.T.plot.bar(title=\"Cross validated classification metrics\", legend=False)\n","66561451":"gs_log_reg.best_params_","19c245e1":"clf= LogisticRegression(C=0.20433597178569418,\n                       solver=\"liblinear\")\n\nclf.fit(X_train, y_train);","08dfc41b":"#check coef attribute to give how each parameter contributes to our target labels\nclf.coef_","21e593b8":"# Match the coef's of features to columns\n\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict\n","e9ad8a36":"# Visualize feature importance\n\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"Feature importance\", legend=False);","884a0168":"# if the value is -ve negative corelation\n\npd.crosstab(df.sex, df.target)","9295bf23":"pd.crosstab(df.slope, df.target)","d47e18b8":"# Lets use GridSearchCV to exhaustively over specifed params\nSince our LogisticRegression model offers the best accuracy scores so far, so we'll try to improve using GridSeacrhCV","9abd07a2":"## Hyperparameter tuning by RandomizedSearchCV\n\nWe are going to tune \n* LogisticRegresion model and \n* RandomForestClassifier \n\nusing RandomizedSearchCV","8211717e":"#Lest compare age, thalach(Max heart rate) and target\n\n","48907a37":"# Predicting heart disease using Machine Learning\n\nThis notebook looks into using various Python based ML and data science libraries in an attempt to build a model which is capable of predicting wether or not someone has heart-disease or not based on their medical attributes\n\nWe're going to take following approach\n1. Problem Definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n## 1. Problem Definition\n\nIn a statement,\n> Given clinical data for a person in the dataset, can we predict or not if they have heart disease?\n\n## 2. Data\n\nThe original data came from Cleaveland data from the UCI Machine Learning Repository.\nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease\n\nThere is also a version available on Kaggle. \nhttps:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n## 3. Evaluation\n\n> If we can reach 95% accuracy at predicting wether or not a patient has heart disease during the proof of concept, we'll puersue the project.\n\n## 4. Features\n\n**Create data dictionary**\n\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n 0: Typical angina: chest pain related decrease blood supply to the heart\n 1: Atypical angina: chest pain not related to heart\n 2: Non-anginal pain: typically esophageal spasms (non heart related)\n 3: Asymptomatic: chest pain not showing signs of disease\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n* chol - serum cholestoral in mg\/dl\n* serum = LDL + HDL + .2 * triglycerides\n* above 200 is cause for concern\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n*'>126' mg\/dL signals diabetes\n* restecg - resting electrocardiographic results\n * 0: Nothing to note\n * 1: ST-T Wave abnormality\n * can range from mild symptoms to severe problems\n * signals non-normal heart beat\n * 2: Possible or definite left ventricular hypertrophy\n * Enlarged heart's main pumping chamber\n* thalach - maximum heart rate achieved\n* xang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n* slope - the slope of the peak exercise ST segment\n * 0: Upsloping: better heart rate with excercise (uncommon)\n * 1: Flatsloping: minimal change (typical healthy heart)\n * 2: Downslopins: signs of unhealthy heart\n* ca - number of major vessels (0-3) colored by flourosopy\n* colored vessel means the doctor can see the blood passing through\nthe more blood movement the better (no clots)\nthal - thalium stress result\n1,3: normal\n6: fixed defect: used to be defect but ok now\n7: reversable defect: no proper blood movement when excercising\ntarget - have disease or not (1=yes, 0=no) (= the predicted attribute)\n\n## Preparing the tools\nWe're going to use pandas, Matplotlib and NumPy for data analysis and manipulation.","57021930":"## Load the data\n","8489e098":"### Now we've got our data in training and tests sets, its time to build a ML model using ML map and test, train and predict.\nhttps:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html\n\nWe are going to try 3 diff ML models\n1. Logistic Regression : https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n2. K-nearest neighbour classifier\n3. RandomForrestClassifier","6e33f99b":"## Evaluate our tuned ML classifier, beyond accuracy\n* ROC Curve\n* AUC Score\n* Confusion matrix\n* CLassification score\n* Precision\n* Recall\n* F1 score\n... and it would be great if CV was used wherever possible\nto make comparison and evaluate our trained model, we need to make predictions....","ddf1e1cd":"{'Logistic Regression': 0.8852459016393442, 'KNN': 0.6885245901639344, 'Random Forest': 0.8360655737704918}\n\nThis tuning has improved our score from 83.60 to 86.88 but our LogisticRegression has more accuracy","95d45536":"Now grid is setup for models, lets us tune them using RandomizedSearchCV\n","3a8554bd":"## Lets check out chest pain type distribution\ncp - chest pain type\n* 0: Typical angina: chest pain related decrease blood supply to the heart\n* 1: Atypical angina: chest pain not related to heart\n* 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* 3: Asymptomatic: chest pain not showing signs of disease","94ead19e":"## Data Exploration(exploratory Data Analysis, EDA)\n\nThe goal here is to find out more about the data and become subject matter expert on the dataset you're working with\n\n1. What questions you are trying to solve?\n2. what kind of data do we have and how do we treat diff types?\n3. What's missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data","dcc1a15f":"### Calculate evaluation matrics using CV\n\nprecision, recall and F1 score of our model using cross_val_score","65569e3b":"# based on our exisiting dataset 72 out of 96 have heart disease which is approx 75%\n# in male 93 out of 207 have heart disease; almost 50%\nTherefore on an avg target is 62.5% chances of having heart disease","32991610":"### Heart Disease Frequency according to sex\n","c1e53b0e":"### We have achieved an improvement in our KNN score as compared to 68% previously, now with hyperparameter tuning we \n### have got upto 75% when we use 11 neighbors.\nAlthough it is an improvement but it is not close to the the other two models which had scores of \n{'Logistic Regression': 0.8852459016393442,\n 'KNN': 0.6885245901639344,\n 'Random Forest': 0.8360655737704918}\n\nThus we can focus on other models and try tuning it to achive our goal which is 95% accuracy","fc676a55":"slope - the slope of the peak exercise ST segment\n* 0: Upsloping: better heart rate with excercise (uncommon)\n* 1: Flatsloping: minimal change (typical healthy heart)\n* 2: Downslopins: signs of unhealthy heart","11912bd2":"## 5. Modelling","00afac7a":"# Now we have got out ROC curve and AUC matrics and confusion matric, Lets get Classification report\nand cross validated precision, recall and F1 score","00ed0a97":"## 6. Experimentation\n\nIf you have not hit your evaluation metric yet, ask yourself:\n* Could you collect more data\/samples?\n* Could try a better model? Like Catboost or XGBoost?\n* Could you improve the current models?( beyond what we have done so far)\n* If your model is good enough (you have hit your evaluation metric) how would you export it and share with others?\n* Ask more questions, reachout, ask on stackoverflow","9593823c":"### Model comparison","edc57979":"## Feature importance\n\nIt is another way of asking which features contribute most to the outcome of the model and how did they contribute?\n\nFinding fearure importance is diff for each ML model. oneway to find features which are important is by googling \"Model name\" feature importance.\n\nLets find the features important or our LogisticRegression model","b6dfa443":"Now we have got a baseline model and we know a model's first prediction is not something we must count on.\nNow we've got a baseline model... and we know a model's first predictions aren't always what we should based our next steps off. What should we do?\n\nLet's look at the following:\n\n1. Hyperparameter tuning\n2. Feature importance\n3. Confusion matrix\n4. Cross-validation\n5. Precision\n6. Recall\n7. F1 score\n8. Classification report\n9. ROC curve\n10. Area under the curve (AUC)\n\n## Hyper Parameter tuning (By hand)"}}