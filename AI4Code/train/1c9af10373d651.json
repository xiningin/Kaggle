{"cell_type":{"239baecb":"code","c265d6fc":"code","4974253d":"code","8a77c572":"code","7ead9754":"code","2c6b3400":"code","deab66f6":"code","2b9e7f65":"code","df3857f0":"code","17020fb3":"code","bfa1c96d":"code","31362356":"code","aa7f5051":"code","0a5356a8":"code","46a02866":"code","abb2a9a7":"code","65403284":"code","8d99c644":"code","a51f4874":"code","b5cd2ba9":"code","5d39b2bb":"code","516425c4":"code","b5f0e7f7":"code","01a4eb79":"code","4f438f45":"code","736a30ad":"code","c79e4a4a":"code","cd67c682":"code","dd90ee0c":"code","069bb4b3":"code","5f6411b8":"code","4b902c6b":"code","f9b552a3":"code","e26250cc":"code","73dbcaf7":"code","51af5ae4":"code","d12e7d05":"code","310a02dc":"code","0f78e690":"code","f3128a54":"code","7b218047":"code","d5d0c0d8":"code","28238638":"code","efd949a7":"code","a36dbbcd":"code","a3962504":"code","217bd9ac":"code","2f148a46":"code","323740ac":"code","12b69318":"code","01a698a6":"code","b99b379b":"code","ca721f97":"code","1f08a6c7":"code","5ff83cb1":"code","c3c318c6":"code","c73b17a0":"code","f55eeee3":"code","31c5ef30":"code","3fd3957a":"code","4ff9f13c":"code","40755f32":"code","4554793c":"code","b9ee86d1":"code","9e267687":"code","aff86ac7":"code","ff65a07b":"code","12069944":"code","46729341":"code","c7261a5b":"markdown","d5b1afcd":"markdown","c04b40d7":"markdown","35c1fabc":"markdown","97f88c52":"markdown","3cf9a26c":"markdown","80cc2476":"markdown","67fb0802":"markdown","417f66b0":"markdown","9eaf7112":"markdown","b7af91fb":"markdown"},"source":{"239baecb":"import pandas as pd\nfrom datetime import datetime\nimport numpy as np","c265d6fc":"# Let's create a pandas series that logs time every hour from 1st Feb'20 to 7th Feb'20\ndf = pd.date_range(start='2\/01\/2020', end='2\/07\/2020', freq='H')\ndf","4974253d":"len(df)","8a77c572":"#Now let's turn our series into a dataframe\ndf = pd.DataFrame(df, columns=['date'])\n\n# And add a 'made up' column for sales data\ndf['sales'] = np.random.randint(0,1000,size=(len(df)))\ndf.head()","7ead9754":"# Set your date as the index \ndf = df.set_index('date')\ndf.head()","2c6b3400":"# Selecting using date - getting exact value for cell \ndf.loc['2020-02-01 03:00:00', 'sales']","deab66f6":"# Selecting using date to return the row corresponding to that date\ndf.loc['2020-02-01 03:00:00']","2b9e7f65":"# Selecting an entire day\ndf.loc['2020-02-01']\n","df3857f0":"# Selecting an entire month\ndf.loc['2020-02']","17020fb3":"# Selecting a range of dates\ndf.loc['2020-02-01':'2020-02-02']","bfa1c96d":"df.index","31362356":"df.resample('D').mean()","aa7f5051":"df.resample('D').sum()","0a5356a8":"df.resample('W').mean()","46a02866":"df = pd.DataFrame({'year': [2015, 2016],\n                   'month': [2, 3],\n                   'day': [4, 5]})\ndf","abb2a9a7":"df.info()","65403284":"pd.to_datetime(df)","8d99c644":"pd.to_datetime('2019-01-01', format='%Y-%m-%d', errors='ignore')","a51f4874":"import statsmodels.api as sm\nimport matplotlib.pyplot as plt\n%matplotlib inline","b5cd2ba9":"df = pd.date_range(start='2\/01\/2020', end='2\/07\/2020', freq='H')\ndf = pd.DataFrame(df, columns=['date'])\ndf['sales'] = np.random.randint(0,1000,size=(len(df)))\ndf = df.set_index('date')\ndf.plot()","5d39b2bb":"time_series=df['sales']\ntype(time_series)","516425c4":"time_series.plot()","b5f0e7f7":"#Determine rolling statistics\nrolmean = df['sales'].rolling(window=24).mean() #window size 24 denotes 24 hour, giving rolling mean at daily level\nrolstd = df['sales'].rolling(window=24).std()\nprint(rolmean,rolstd)","01a4eb79":"orig = plt.plot(df['sales'], color='blue', label='Original')\nmean = plt.plot(rolmean, color='orange', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label='Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False);","4f438f45":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomp= seasonal_decompose(time_series,freq=24)","736a30ad":"fig= decomp.plot()\nfig.set_size_inches(15,10)","c79e4a4a":"df.head()","cd67c682":"from statsmodels.tsa.stattools import adfuller","dd90ee0c":"def adf_check(time_series):\n    result = adfuller(time_series)\n    print(\"Augmented Dicky-Fuller Test\")\n    labels=['Adf Test Statistics', 'p-value', '# of lags', 'Num of Observations used']\n    \n    for value,label in zip(result,labels):\n        print(label+ \":\"+str(value))\n        \n    if result[1]<= 0.05:\n        print(\"Strong evidence against null hypothesis\")\n        print(\"reject null hypotesis\")\n        print(\"data has no unit root and is stationary\")\n    else:\n        print('weak evidence against null hypothesis')\n        print('Fail to reject null hypo')\n        print('Data has a unit root, it is a non-stationary')\n        \n        ","069bb4b3":"adf_check(df['sales'])","5f6411b8":"#Estimating trend\ndf_logScale = np.log(df)\nplt.plot(df_logScale)","4b902c6b":"movingAverage = df_logScale.rolling(window=24).mean()\nmovingSTD = df_logScale.rolling(window=24).std()\nplt.plot(df_logScale)\nplt.plot(movingAverage, color='red')","f9b552a3":"datasetLogScaleMinusMovingAverage = df_logScale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n\n#Remove NAN values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)\n","e26250cc":"adf_check(df_logScale['sales'])","73dbcaf7":"exponentialDecayWeightedAverage = df_logScale.ewm(halflife=12, min_periods=0, adjust=True).mean()\nplt.plot(df_logScale)\nplt.plot(exponentialDecayWeightedAverage, color='red')\n\n","51af5ae4":"datasetLogScaleMinusExponentialMovingAverage = df_logScale - exponentialDecayWeightedAverage\nadf_check(datasetLogScaleMinusExponentialMovingAverage)","d12e7d05":"df_shift= df['sales'] - df['sales'].shift(1)\ndf_shift.plot()","310a02dc":"df_shift.dropna(inplace=True)\nadf_check(df_shift)","0f78e690":"adf_check(df_shift.dropna())","f3128a54":"from statsmodels.graphics.tsaplots import plot_acf,plot_pacf","7b218047":"fig_first= plot_acf(df_shift.dropna())","d5d0c0d8":"from pandas.plotting import autocorrelation_plot","28238638":"autocorrelation_plot(df_shift.dropna())","efd949a7":"result = plot_pacf(df_shift.dropna())","a36dbbcd":"plot_acf(df_shift.dropna())\nplot_pacf(df_shift.dropna());","a3962504":"from statsmodels.tsa.arima_model import ARIMA","217bd9ac":"help(ARIMA)","2f148a46":"model = ARIMA(df_shift, order=(2,1,0))\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","323740ac":"df_shift","12b69318":"import matplotlib.pyplot as plt\nresiduals = pd.DataFrame(model_fit.resid)\nresiduals.plot();\nplt.show();\nresiduals.plot(kind='kde')\nplt.show();\nprint(residuals.describe())","01a698a6":"from sklearn.metrics import mean_squared_error\nsize = int(len(df_shift) * 0.66)\ntrain, test = df_shift[0:size], df_shift[size:len(df_shift)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\n","b99b379b":"data = pd.read_csv(\"..\/input\/AirPassengers.csv\")\ndata['Month'] = pd.to_datetime(data['Month'],infer_datetime_format=True)","ca721f97":"df = data.set_index('Month')\ndf.info()","1f08a6c7":"train = df.iloc[:130]\ntest = df.iloc[130:]","5ff83cb1":"sarima = sm.tsa.statespace.SARIMAX(train,order=(7,1,7),seasonal_order=(7,1,7,12),enforce_stationarity=False, enforce_invertibility=False).fit()\nsarima.summary()","c3c318c6":"res = sarima.resid\nfig,ax = plt.subplots(2,1,figsize=(15,8))\nfig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\nfig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\nplt.show()","c73b17a0":"len(test)","f55eeee3":"test","31c5ef30":"from sklearn.metrics import mean_squared_error\npred = sarima.predict(test.index[0],test.index[13])\nprint('SARIMA model MSE:{}'.format(mean_squared_error(test,pred)))","3fd3957a":"pred","4ff9f13c":"pd.DataFrame({'test':test['#Passengers'],'pred':pred}).plot();plt.show()","40755f32":"train['passenger_age_mean']= np.random.randint(30,60,size=(len(train)))\ntest['passenger_age_mean']= np.random.randint(30,60,size=(len(test)))\nexog_train = train.passenger_age_mean\nexog_test = test.passenger_age_mean\n\nexog_train","4554793c":"train","b9ee86d1":"\nsarimax = sm.tsa.statespace.SARIMAX(train['#Passengers'],order=(7,1,7),seasonal_order=(1,0,5,12),exog = exog_train,enforce_stationarity=False, enforce_invertibility=False).fit()\nsarimax.summary()","9e267687":"res = sarimax.resid\nfig,ax = plt.subplots(2,1,figsize=(15,10))\nfig = sm.graphics.tsa.plot_acf(res, lags=50, ax=ax[0])\nfig = sm.graphics.tsa.plot_pacf(res, lags=50, ax=ax[1])\nplt.show()","aff86ac7":"test","ff65a07b":"\npred = sarima.predict(test.index[0],test.index[13],exog = exog_test)\nprint('SARIMAX model MSE:{}'.format(mean_squared_error(test['#Passengers'],pred)))","12069944":"pred","46729341":"pd.DataFrame({'test':test['#Passengers'],'pred':pred}).plot();plt.show()","c7261a5b":"<a class=\"anchor\" id=\"1.\"><\/a> \n# 1.How to Work with Time Series Data with Pandas","d5b1afcd":"<a class=\"anchor\" id=\"3.\"><\/a> \n# 3.General Forecasting Models - ARIMA(Autoregressive Integrated Moving Average)","c04b40d7":"# Resampling\n\n**Summary States** - we can use Statistical methods over different time intervals\n- mean(), sum(), count(), min(), max()\n\n**Down-sampling**\n- reduce datetime rows to longer frequency\n\n**Up-sampling**\n- increase datetime rows to shorter frequency\n\n## Resampling frequencies\n\n- 'min', 'T' - minute\n- \u2018H\u2019 - hour\n- \u2018D\u2019 - day\n- \u2018B\u2019 - business day\n- \u2018W\u2019 - week\n- \u2018M\u2019 - month\n- \u2018Q\u2019 - quarter\n- \u2018A\u2019 - year","35c1fabc":"# **How to use the Python programming Language for Time Series Analysis!**\n\nThis work was prepared together with [Gul Bulut](https:\/\/www.kaggle.com\/gulyvz) and [Bulent Siyah](https:\/\/www.kaggle.com\/bulentsiyah\/). **The whole study consists of two parties**\n* [Time Series Forecasting and Analysis- Part 1](https:\/\/www.kaggle.com\/gulyvz\/time-series-forecasting-and-analysis-part-1)\n* [Time Series Forecasting and Analysis- Part 2](https:\/\/www.kaggle.com\/bulentsiyah\/time-series-forecasting-and-analysis-part-2)\n\nThis kernel will teach you everything you need to know to use Python for forecasting time series data to predict new future data points.\n\n![](https:\/\/iili.io\/JaZxFS.png)\n   \nwe&#39;ll learn about state of the art Deep Learning techniques with Recurrent Neural Networks that\nuse deep learning to forecast future data points on part 2.\n![](https:\/\/iili.io\/JaZCMl.png)\nPart 2 kernel even covers Facebook&#39;s Prophet library, a simple to use, yet powerful Python library\ndeveloped to forecast into the future with time series data.\n![](https:\/\/iili.io\/JaZnP2.png)\n# **Content Part 1** \n\n1. [How to Work with Time Series Data with Pandas](#1.)\n1. [Use Statsmodels to Analyze Time Series Data](#2.)\n1. [General Forecasting Models - ARIMA(Autoregressive Integrated Moving Average)](#3.)\n1. [General Forecasting Models - SARIMA(Seasonal Autoregressive Integrated Moving Average)](#4.)\n1. [General Forecasting Models - SARIMAX(Seasonal Autoregressive Integrated Moving Average with exogenous regressors)](#5.)\n\n# **Content Part 2**\n\n1. [Deep Learning for Time Series Forecasting - (RNN)](https:\/\/www.kaggle.com\/bulentsiyah\/time-series-forecasting-and-analysis-part-2#1.)\n1. [Multivariate Time Series with RNN](https:\/\/www.kaggle.com\/bulentsiyah\/time-series-forecasting-and-analysis-part-2#2.)\n1. [Use Facebook's Prophet Library for forecasting](https:\/\/www.kaggle.com\/bulentsiyah\/time-series-forecasting-and-analysis-part-2#3.)\n","97f88c52":"Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n\n\n    P: Seasonal autoregressive order.\n    D: Seasonal difference order.\n    Q: Seasonal moving average order.\n    m: The number of time steps for a single seasonal period.","3cf9a26c":"<a class=\"anchor\" id=\"4.\"><\/a> \n# 4.General Forecasting Models - SARIMA(Seasonal Autoregressive Integrated Moving Average)","80cc2476":"## Build Model","67fb0802":"<a class=\"anchor\" id=\"2.\"><\/a> \n# 2.Use Statsmodels to Analyze Time Series Data","417f66b0":"\n\nThe implementation is called SARIMAX instead of SARIMA because the \u201cX\u201d addition to the method name means that the implementation also supports exogenous variables. Exogenous variables are optional can be specified via the \u201cexog\u201d argument.","9eaf7112":"\n\n<a class=\"anchor\" id=\"5.\"><\/a> \n# 5.General Forecasting Models - SARIMAX(Seasonal Autoregressive Integrated Moving Average)","b7af91fb":"Autoregressive Integrated Moving Average, or ARIMA, is a forecasting method for univariate time series data.\n\n\n    p: Trend autoregression order.\n    d: Trend difference order.\n    q: Trend moving average order.\n"}}