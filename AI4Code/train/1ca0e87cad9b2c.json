{"cell_type":{"c5cf55a3":"code","e694c8ea":"code","69485f72":"code","1a7bbee8":"code","e55c6961":"code","c6e7e968":"code","ca4c3531":"code","ea17a875":"code","768ccfc8":"code","676ff5b9":"code","f342baaa":"code","6349c844":"code","15c2e5bb":"code","02941d12":"code","711511c9":"code","68fab9d5":"code","6cb454ea":"code","0165b7c5":"code","445f0497":"code","a4c6a60f":"code","58dd4f82":"code","a719dc8a":"code","ed070624":"code","220c152e":"code","e176b4ba":"markdown","4cc00f34":"markdown","14b47968":"markdown","3f4695af":"markdown","4fb38924":"markdown","5fce241e":"markdown","0bed958a":"markdown","40c271f5":"markdown","3d63a75f":"markdown","c8df85c7":"markdown","3054b0d8":"markdown","3cabe675":"markdown","ee9bc3e2":"markdown","ac6702b8":"markdown","98121ebc":"markdown","40117fd1":"markdown"},"source":{"c5cf55a3":"import os\nimport re\nimport json\nimport glob\n\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\n%matplotlib inline\n\n\nimport string\n\n\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport json\nimport glob\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data')","e694c8ea":"!pip install -q gensim\n!pip install -q pynndescent>=0.4\n!pip install -q umap-learn\n!pip install -q wordcloud\n!pip install -q torch\n!pip install -q sentence_transformers\n!pip install -q hnswlib\n!pip install -q joblib<1.0.0","69485f72":"pip install top2vec[sentence_transformers]\n","1a7bbee8":"pip install top2vec[sentence_encoders]\n","e55c6961":"\nfrom copy import deepcopy\nfrom top2vec import Top2Vec","c6e7e968":"\n#Path to Json Files\ntrain_corpus =  glob.glob(\"..\/input\/coleridgeinitiative-show-us-the-data\/train\/*.json\")\ntest_corpus = glob.glob(\"..\/input\/coleridgeinitiative-show-us-the-data\/test\/*.json\")\n\n","ca4c3531":"pd.set_option('display.max_colwidth', None)\n","ea17a875":"df_train = pd.DataFrame()\nfor count, x in enumerate(train_corpus, len(train_corpus)):\n    df_train = pd.concat([df_train, pd.read_json(x)])\n    \ndf_train.to_csv(\"df_train.csv\", index = False)\n\ndf_train.head(2)","768ccfc8":"df_test = pd.DataFrame()\nfor count, x in enumerate(test_corpus, len(test_corpus)):\n    df_test = pd.concat([df_test, pd.read_json(x)])\n    \ndf_test.to_csv(\"df_test.csv\", index = False)\n\ndf_test.head(2)","676ff5b9":"def cleantext(df): \n    \n    df_train['cleaned_text'] = df_train['text'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n    df_train['cleaned_text'] = df_train['cleaned_text'].replace(\"  \", \" \")\n    \n    # convert tweets to lowercase\n    df_train['cleaned_text'] = df_train['cleaned_text'].str.lower()\n    \n     #remove_symbols\n    df_train['cleaned_text']  = df_train['cleaned_text'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n    \n    #remove punctuations \n    df_train['cleaned_text'] = df_train['cleaned_text'].replace(r'[[]!\"#$%\\'()\\*+,-.\/:;<=>?^_`{|}]+',\"\", regex = True)\n    \n    #remove_URL(x):\n    df_train['cleaned_text']  = df_train['cleaned_text'].replace(r'https.*$', \"\", regex = True)\n    \n    #remove stopwords and words_to_remove\n    \n    mystopwords = set(stopwords.words('english'))\n    \n    df_train['fully_cleaned_text'] = df_train['cleaned_text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in mystopwords]))\n    \n\n    return df\n\ndf_train = cleantext(df_train)","f342baaa":"df_train.head(2)","6349c844":"docs = list(df_train.loc[:, \"fully_cleaned_text\"].values)\n","15c2e5bb":"docs[:5]\n","02941d12":"\nmodel = Top2Vec(docs, embedding_model='universal-sentence-encoder')\n","711511c9":"model.get_num_topics()\n","68fab9d5":"topic_words, word_scores, topic_nums = model.get_topics(25)\n","6cb454ea":"for topic in topic_nums:\n    model.generate_topic_wordcloud(topic)\n","0165b7c5":"documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=7, num_docs=5)\n","445f0497":"documents","a4c6a60f":"#Semantic Similarties\ndocument_scores","58dd4f82":"document_ids","a719dc8a":"documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=7, num_docs=5)\nfor doc, score, doc_id in zip(documents, document_scores, document_ids):\n    print(f\"Document: {doc_id}, Score: {score}\")\n    print(\"-----------\")\n    print(doc)\n    print(\"-----------\")\n    print()","ed070624":"documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=[\"statisticians\", \"classifications\"], num_docs=5)\nfor doc, score, doc_id in zip(documents, document_scores, document_ids):\n    print(f\"Document: {doc_id}, Score: {score}\")\n    print(\"-----------\")\n    print(doc)\n    print(\"-----------\")\n    print()","220c152e":"words, word_scores = model.similar_words(keywords=[\"pandemics\"], keywords_neg=[], num_words=20)\nfor word, score in zip(words, word_scores):\n    print(f\"{word} {score}\")","e176b4ba":"**Context**\n\nData, evidence, and science are critical if government is to address so many imminent threats: pandemics, climate change and coastal inundation, Alzheimer\u2019s disease, child hunger, biodiversity, among others. Yet much of the information about data needed to inform evidence and science is locked inside publications.\n\nThe Coleridge Initiative is launching a new competition, Show Us the Data, to spur the use of artificial intelligence (AI) to unlock this information. Westat is among the sponsors of this exciting competition. Participants are challenged to use natural language processing (NLP) to identify critical datasets used in scientific publications. This work will help researchers quickly find the data they need to do their work. The effort will help improve access and use of critical datasets.","4cc00f34":"# **Merge all the JSON files(Publications Corpus) to a Dataframe**","14b47968":"# work is in Progress, kindly upvote if you find it useful and comment for suggestions.","3f4695af":"Search for similar words to pandemics.\n\n","4fb38924":"# Search Documents by Topic","5fce241e":"# **Test Corpus**","0bed958a":"![image.png](attachment:image.png)","40c271f5":"**For each of the returned documents we are going to print its content, score and document number**","3d63a75f":"# Load the data","c8df85c7":"**Challenge** - Much of the information about data necessary to inform evidence and science is locked inside publications.\n\n**Approach** - lets leverage natural language processing find the hidden-in-plain-sight data citations.\n\nBasically In this competition, we will use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications.","3054b0d8":"# Similar Keywords","3cabe675":"# Semantic Search Documents by Keywords\n#Search documents for content semantically similar to statisticians and classifications","ee9bc3e2":"# Import Packages","ac6702b8":"# Top2Vec topic modeling with BERT\n","98121ebc":"# Top2Vec","40117fd1":"#  Train Corpus Preprocessing"}}