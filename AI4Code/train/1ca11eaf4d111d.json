{"cell_type":{"78bf9da8":"code","c12835aa":"code","c8fac6f2":"code","32cf9a54":"code","630d7f9d":"code","372145d4":"code","dad5f6ab":"code","8353e74d":"code","4b1214a2":"code","3d1f9353":"code","14efcc67":"code","a25a14db":"markdown","39504bc3":"markdown","e127b415":"markdown","ae58d8d0":"markdown","ae05638b":"markdown","38a6a4ed":"markdown"},"source":{"78bf9da8":"import re\nimport string\nimport pandas as pd\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nre_url = re.compile(r'(?:http|ftp|https):\/\/(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:\/~+#-]*[\\w@?^=%&\/~+#-])?')\nre_email = re.compile('(?:[a-z0-9!#$%&\\'*+\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&\\'*+\/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])')","c12835aa":"df = pd.read_csv('\/kaggle\/input\/20-newsgroup-preprocessed\/20newsgroup_preprocessed.csv', sep=';', usecols=['target', 'text'])","c8fac6f2":"df.head()","32cf9a54":"print(df['text'][3398])","630d7f9d":"%%time\ndef clean_header(text):\n    text = re.sub(r'(From:\\s+[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Subject:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(([\\sA-Za-z0-9\\-]+)?[A|a]rchive-name:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Last-modified:[^\\n]+\\n)', '', text)\n    text = re.sub(r'(Version:[^\\n]+\\n)', '', text)\n\n    return text\n\n\ndf['text_cleaned'] = df['text'].apply(clean_header)","372145d4":"print(df['text_cleaned'][3398])","dad5f6ab":"%%time\ndef clean_text(text):        \n    text = text.lower()\n    text = text.strip()\n    text = re.sub(re_url, '', text)\n    text = re.sub(re_email, '', text)\n    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n    text = re.sub(r'(\\d+)', ' ', text)\n    text = re.sub(r'(\\s+)', ' ', text)\n    \n    return text\n\ndf['text_cleaned'] = df['text_cleaned'].apply(clean_text)","8353e74d":"print(df['text_cleaned'][3398])","4b1214a2":"%%time\n\nstop_words = stopwords.words('english')\n\ndf['text_cleaned'] = df['text_cleaned'].str.split() \\\n    .apply(lambda x: ' '.join([word for word in x if word not in stop_words]))","3d1f9353":"print(df['text_cleaned'][3398])","14efcc67":"df.head()","a25a14db":"### Remove stop words\n\nRead more about this in:\n\n- https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/dropping-common-terms-stop-words-1.html","39504bc3":"### Original document","e127b415":"### Clean Header\n\nWe decided to remove the header of the document to avoid situations like our model predicting that one common name has a high influence to determine the document class. This might happen if our training data has a lot of people with the same common name writing this one topic.","ae58d8d0":"## Read dataset\n\nFor demonstration purposes, we're going to read the dataset generated with the same pre-processing showed here, removing the text_cleaned column, that will be generated at the end of this notebook. ","ae05638b":"### Clean text\n\nRead more about this in:\n\n\n- https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html\n- https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/normalization-equivalence-classing-of-terms-1.html","38a6a4ed":"### Next steps\n\nIn this notebook, we provide a simple approach that you might use for cleaning your text before machine learning NLP tasks. We know this is just the beginning to normalize your text documents, but we didn't want to apply more cleaning to the point you can't try different ones.\n\nIf you need some inspiration, read more about stemming, lemmatization, tf-idf, and word embedding."}}