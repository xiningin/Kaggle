{"cell_type":{"c0a6be1b":"code","c0d7e81a":"code","b5a67e65":"code","2a2fbbfb":"code","0c55baef":"code","d99cd745":"code","2bce5c34":"code","ffec52ef":"code","c84535ed":"code","18c05c79":"code","7259bd3d":"code","357ad681":"code","acc55401":"code","62d1c2d0":"code","ae6778da":"code","0cf66ea5":"code","49206922":"code","d4d0094a":"code","c6c40d13":"code","008c7f97":"code","9aded8e5":"code","5918ff18":"code","0ae4413f":"code","e1bc4420":"code","5f798ad2":"code","f9064e81":"code","5eb5a6cf":"code","0c4a7b71":"code","f2f15cfc":"code","63639e5e":"code","c3d583db":"code","67c62bd6":"code","9853e971":"code","780a167f":"code","7e51f7a8":"code","0e3f82d5":"code","9fbb172c":"code","06f9d1bf":"code","f2c47a13":"code","ef6aafdb":"code","e6e7b2c8":"code","66bf1b52":"code","f2a5ce81":"code","48705c37":"code","25227e97":"code","2a97a34c":"code","96959ed5":"code","3fdd5c18":"code","4bf3e5e1":"code","2d009db5":"code","a25e3fde":"code","48f3e40e":"code","9c3c4a45":"code","65c3965b":"code","cd4b3abf":"code","b143aafa":"code","aa0fb077":"code","d931197f":"code","34d52bcd":"code","ff3dbf1c":"code","c01e3818":"code","ddf7227d":"code","1f3583d5":"code","b0e103cd":"code","0ebd3f79":"code","21d768d4":"code","b0021841":"markdown","13aa2a6a":"markdown","1b98246f":"markdown","7073b9fd":"markdown","3651de5c":"markdown","0d075173":"markdown","b2fb9537":"markdown","dde8892f":"markdown","6d400ba1":"markdown","25e38d02":"markdown","6ce59aa1":"markdown","97c43773":"markdown","581cff2c":"markdown","1c675e10":"markdown","b487b90b":"markdown","d62803a9":"markdown","9df6aae5":"markdown","b8c1b3ea":"markdown","b47d89a5":"markdown","702fe81c":"markdown","0547c447":"markdown","180a06d9":"markdown","072aa712":"markdown","aac76cdd":"markdown","e5015ec7":"markdown","b2756ffc":"markdown","b3f14456":"markdown","8cc358da":"markdown","f342ce63":"markdown","693a2237":"markdown","c01bcb9e":"markdown","157eac3f":"markdown","76b31d95":"markdown","743c81fe":"markdown","80b4eae8":"markdown","0b0eb81f":"markdown","51c9b777":"markdown","224b663b":"markdown","d9e2662c":"markdown","75052d11":"markdown","089613d2":"markdown","3b2c4029":"markdown","dda2637a":"markdown","8e8f982d":"markdown","5372f023":"markdown","3259e0b3":"markdown","59172e33":"markdown","a9b71016":"markdown","85d7d821":"markdown","f5d608bd":"markdown","cdded379":"markdown","526592dd":"markdown","351abf1b":"markdown","6b54018e":"markdown","1d63085d":"markdown","5b62a750":"markdown"},"source":{"c0a6be1b":"import os\nimport re  \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re           \nfrom bs4 import BeautifulSoup \nfrom nltk.corpus import stopwords   \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom keras.models import Sequential, Model\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPool1D\nfrom gensim.models import KeyedVectors\nfrom keras.models import Sequential\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Embedding\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model","c0d7e81a":"# Import libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model,Sequential\nfrom keras.utils import to_categorical\n# Load the input features\npd.set_option('display.max_colwidth',None)\ntrain_df=pd.read_csv('..\/input\/clean-quora-train-data\/clean_lem_stemmed_train_data.csv') #cleaned data imported from previous kernel\ntrain_df=train_df.dropna()\ntrain_df.head()","b5a67e65":"X = train_df['question_text'] # input\ny = train_df['target'].values # target \/label\n\nsentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=11)\n\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(sentences_train)\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\n\n# Adding 1 because of  reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1 # (in case of pre-trained embeddings it's +2)                         \nmaxlen = 131 # sentence length\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n\n\nprint(\"Padded and Tokenized Training Sequence\".format(),X_train.shape)\nprint(\"Target Training Values Shape\".format(),y_train.shape)\nprint(\"_____________________________________________\")\nprint(\"Padded and Tokenized Validation Sequence\".format(),X_val.shape)\nprint(\"Target Validatation Values Shape\".format(),y_val.shape)","2a2fbbfb":"num_tokens=len(tokenizer.word_index)+2\nprint(\"Number of Features\/Tokens:\",num_tokens)","0c55baef":"del train_df\nimport gc\ngc.collect()","d99cd745":"from keras.wrappers.scikit_learn import KerasClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nseed = 1000\n\n# generic function to plot the train Vs validation loss\/accuracy:\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    plt.figure(figsize=(25,15))\n    ## Accuracy\n    plt.subplot(2,2,1)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n\n    plt.title('Training Accuracy Vs Validation Accuracy\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(2,2,2)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    \n    plt.title('Training Loss Vs Validation Loss\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","2bce5c34":"from sklearn import metrics\n\ndef conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.set_context(\"notebook\", font_scale=1.1)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()","ffec52ef":"# ConvNet model\n\nembedding_dim = 100\n# number_of_tokens=len(tokenizer.word_index)+1\n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\n\n# model = tf.keras.Sequential()\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nx = layers.Conv1D(256, 5, activation=\"relu\")(embedded_sequences)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(10,activation='relu')(x)\npreds = layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.Model(int_sequences_input, preds)\n\nmodel.summary()\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","c84535ed":"history=model.fit(X_train, y_train,epochs=5,validation_data=(X_val, y_val),batch_size=1024)","18c05c79":"plot_history(history)","7259bd3d":"# save the model\n# model.save('cnn_nlp_model.h5')\n# plotting the architecture\ndot_img_file = '\/tmp\/model_cnn.png'\ntf.keras.utils.plot_model(model, show_shapes=True,to_file=dot_img_file, rankdir=\"TB\")","357ad681":"del model,history,\nimport gc\ngc.collect()","acc55401":"# Basic RNN(LSTM) model without pretrained embeddings\n\nmodel_RNN = Sequential([layers.Embedding(vocab_size, embedding_dim, input_length=maxlen),\n                        LSTM(64),\n                        Dense(16,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_RNN.summary()\n# save the model\n# model_RNN.save('rnn_nlp_model.h5')","62d1c2d0":"# plot the architecure\ndot_img_file = '\/tmp\/model_RNN.png'\ntf.keras.utils.plot_model(model_RNN, to_file=dot_img_file, rankdir=\"LR\",show_shapes=True)","ae6778da":"history = model_RNN.fit(X_train,y_train,epochs=2,validation_data=(X_val, y_val),batch_size=2056)","0cf66ea5":"plot_history(history)","49206922":"del model_RNN,dot_img_file,history\nimport gc\ngc.collect()","d4d0094a":"# \n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\n\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\nz=Bidirectional(LSTM(32,return_sequences='True'))(embedded_sequences)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel_biRNN= Model(inputs=int_sequences_input,outputs=z)\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_biRNN.summary()\n# save the model\n#model_biRNN.save('biRNN_nlp_model.h5')","c6c40d13":"dot_img_file = '\/tmp\/model_birnn.png'\ntf.keras.utils.plot_model(model_biRNN, to_file=dot_img_file, rankdir=\"LR\",show_shapes=True)","008c7f97":"history = model_biRNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","9aded8e5":"del model_biRNN,dot_img_file,history\nimport gc\ngc.collect()","5918ff18":"from gensim.models import KeyedVectors\n\npath_to_glove_file = os.path.join('..\/input\/pretrained\/', \"glove.6B.100d.txt\")\n\n## make a dict mapping words (strings) to their NumPy vector representation:\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, dtype=float, sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","0ae4413f":"## prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\nword_index=tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+ 2\nembedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n\n\n#load the pre-trained word embeddings matrix into an Embedding layer.\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False\n)","e1bc4420":"print(\"embedding matrix shape:\",embedding_matrix.shape)","5f798ad2":"## Build the ConvNet Model - GloVe embeddings\n\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\n\nx = layers.Conv1D(256, 5, activation=\"relu\")(embedded_sequences)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.MaxPooling1D()(x)\nx = layers.Conv1D(128, 5, activation=\"relu\")(x)\nx = layers.GlobalMaxPooling1D()(x)\nx = layers.Dense(128, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu')(x)\npreds = layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.Model(int_sequences_input, preds)\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","f9064e81":"history = model.fit(X_train,y_train,epochs=2,validation_data=(X_val, y_val),batch_size=2056)","5eb5a6cf":"plot_history(history)","0c4a7b71":"del model,history\nimport gc\ngc.collect()","f2f15cfc":"# load the GloVe word embeddings matrix into an Embedding layer\n\nmodel_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False),\n                        LSTM(60),\n                        Dense(20,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_RNN.summary()","63639e5e":"history = model_RNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","c3d583db":"#load the pre-trained word embeddings matrix into an Embedding layer.\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False\n)\n\nembedding_layer = Embedding(vocab_size,embedding_dim,input_length=maxlen,trainable=True)\nint_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\nembedded_sequences = embedding_layer(int_sequences_input)\n\nz=Bidirectional(LSTM(32,return_sequences='True'))(embedded_sequences)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel_biRNN= Model(inputs=int_sequences_input,outputs=z)\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# summary of model\nmodel_biRNN.summary()","67c62bd6":"%%time\n\nhistory = model_biRNN.fit(X_train,y_train,epochs=1,validation_data=(X_val, y_val),batch_size=2056)","9853e971":"del embedding_matrix, model_RNN, model_biRNN,history\nimport gc\ngc.collect()","780a167f":"google_news_embed=\"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\nembeddings_index = KeyedVectors.load_word2vec_format(google_news_embed, binary=True)","7e51f7a8":"print(\"Found %s word vectors.\" % len(embeddings_index.vocab))","0e3f82d5":"word_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2\nnb_words   = min(num_tokens, len(word_index))\nembed_size = 300\n\nembedding_matrix = np.zeros((nb_words+2, embed_size))","9fbb172c":"## prepare a corresponding embedding matrix that used in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nhits = 0\nmisses = 0\n\n# embedding matrix\nfor word, i in word_index.items():\n    try:\n        embedding_vector = embeddings_index.get_vector(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n    \n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))","06f9d1bf":"print(\"Embedding Matrix Shape:\",embedding_matrix.shape)\nprint(\"Number of Tokens      :\",num_tokens)","f2c47a13":"## Build the ConvNet Model - GoogleNews embeddings\n\nembedding_dim=300\n\nmodel = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                    layers.Conv1D(256, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.GlobalMaxPooling1D(),\n                    layers.Dense(128, activation=\"relu\"),\n                    layers.Dropout(0.5),\n                    layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu'),\n                    layers.Dense(1, activation='sigmoid')])\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","ef6aafdb":"history = model.fit(X_train,y_train,batch_size=2056,epochs=1,validation_data=(X_val,y_val))","e6e7b2c8":"del model,embeddings_index,history\nimport gc\ngc.collect()","66bf1b52":"model_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                        LSTM(60),\n                        Dense(20,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","f2a5ce81":"BATCH_size = 2056\nhistory = model_RNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","48705c37":"del model_RNN,history\nimport gc\ngc.collect()","25227e97":"model_biRNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                          layers.Bidirectional(LSTM(32,return_sequences='True')),\n                          layers.GlobalMaxPool1D(),\n                          layers.Dense(16,activation='relu'),\n                          layers.Dense(1,activation='sigmoid')])\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","2a97a34c":"# plotting the architecture\ndot_img_file = '\/tmp\/model_birnn_google.png'\ntf.keras.utils.plot_model(model_biRNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","96959ed5":"BATCH_size = 2056\nhistory = model_biRNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","3fdd5c18":"del model_biRNN,history,embedding_matrix\nimport gc\ngc.collect()","4bf3e5e1":"%%time\n\n# Using the fasttext word embeddding from crawl\n\nfasttext_file= \"..\/input\/pretrained\/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\n\nword_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2","2d009db5":"print(\"Found %s word vectors.\" % len(fasttext_model.vocab))","a25e3fde":"## prepare a corresponding embedding matrix that used in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nembed_size = 300\nembedding_matrix = np.zeros((num_tokens, embed_size))\n\nhits = 0\nmisses = 0\n\n# embedding matrix\nfor word, i in word_index.items():\n    try:\n        embedding_vector = fasttext_model.get_vector(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n    \n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))","48f3e40e":"print(\"Embedding Matrix Shape:\",embedding_matrix.shape)\nprint(\"Number of Tokens      :\",num_tokens)","9c3c4a45":"## Build the ConvNet Model - asttext embeddings\n\nembedding_dim=300\n\nmodel = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                    layers.Conv1D(256, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.MaxPooling1D(),\n                    layers.Conv1D(128, 5, activation=\"relu\"),\n                    layers.GlobalMaxPooling1D(),\n                    layers.Dense(128, activation=\"relu\"),\n                    layers.Dropout(0.5),\n                    layers.Dense(10, kernel_regularizer=regularizers.l1(l1=1e-4),activation='relu'),\n                    layers.Dense(1, activation='sigmoid')])\n\nmodel.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])","65c3965b":"BATCH_size = 1024\nhistory = model.fit(X_train,y_train,batch_size=BATCH_size,epochs=2,validation_data=(X_val,y_val))","cd4b3abf":"plot_history(history)","b143aafa":"# plotting the architecture\ndot_img_file = '\/tmp\/model_cnn_fast.png'\ntf.keras.utils.plot_model(model, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","aa0fb077":"del history,model,dot_img_file\nimport gc\ngc.collect()","d931197f":"model_RNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                        LSTM(64),\n                        Dense(32,activation='relu'),\n                        Dense(1,activation='sigmoid')])\n\n# compile\nmodel_RNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n# fit the model\nBATCH_size = 1024\nhistory = model_RNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=1,validation_data=(X_val,y_val))","34d52bcd":"# plotting the architecture\ndot_img_file = '\/tmp\/model_rnn_fast.png'\ntf.keras.utils.plot_model(model_RNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","ff3dbf1c":"del history,model_RNN,dot_img_file\nimport gc\ngc.collect()","c01e3818":"embedding_dim=300\n\nmodel_biRNN = Sequential([layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],trainable=False),\n                          layers.Bidirectional(LSTM(64,return_sequences='True')),\n                          layers.GlobalMaxPool1D(),\n                          layers.Dense(128,activation='relu'),\n                          layers.Dense(64,activation='relu'),\n                          layers.Dense(16,activation='relu'),\n                          layers.Dense(1,activation='sigmoid')])","ddf7227d":"dot_img_file = '\/tmp\/model_birnn_fast.png'\ntf.keras.utils.plot_model(model_biRNN, show_shapes=True,to_file=dot_img_file, rankdir=\"LR\")","1f3583d5":"%%time\n\nfrom sklearn import metrics\n\nmodel_biRNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n\n## Log Directory-TensorBoard\n\n# root log directory - with logs and sub-directory of current data and time\nroot_logdir = os.path.join(\"os.curdir\",\"my_logs\")\n\ndef get_run_logdir():\n    import time\n    run_id = time.strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n    return os.path.join(root_logdir,run_id)\n\nrun_logdir = get_run_logdir()\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir,histogram_freq=1)\n\nepochs=2\nBATCH_size = 1024\n\nfor e in range(epochs):\n    model_biRNN.fit(X_train,y_train,batch_size=BATCH_size,epochs=3,validation_data=(X_val,y_val),callbacks=[tensorboard_cb])\n    pred_fast_val_y = model_biRNN.predict([X_val], batch_size=1024, verbose=1)\n    best_thresh = 0.6\n    best_score = 0.0\n    for thresh in np.arange(0.1, 0.601, 0.01):\n        thresh = np.round(thresh, 2)\n        score = metrics.f1_score(y_val, (pred_fast_val_y>thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    print(\"Val F1 Score: {:.4f}\".format(best_score))","b0e103cd":"from sklearn import metrics\nimport seaborn as sns\n\npred_y_val = (pred_fast_val_y>best_thresh).astype(int)\n\ndef conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.set_context(\"notebook\", font_scale=1.1)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()\n    \n\nconf_matrix(y_val,pred_y_val,'Bidirectional RNN Model with fasttext embeddings\\n')","0ebd3f79":"test_df = pd.read_csv(\"..\/input\/testdataquora\/test.csv\")\ntest_sentences = test_df['question_text']\n\ntokenizer = Tokenizer(num_words=30000)\ntokenizer.fit_on_texts(test_sentences)\nX_test = tokenizer.texts_to_sequences(test_sentences)\nX_test = pad_sequences(X_test, padding='post', maxlen=126)\n\npred_test_y = model_biRNN.predict([X_test], batch_size=1024, verbose=1)\n\n#submission file\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission_quora_birnn.csv\", index=False)","21d768d4":"print(\"Metrics\\n\")\nprint(metrics.classification_report(y_val,pred_y_val))","b0021841":"### Designing the Embedding matrix with Glove Embeddings","13aa2a6a":"> ## Fasttext Embeddings","1b98246f":"### Embedding Layer with GloVe Embeddings","7073b9fd":"## Basic RNN Neural Networks Model without pre-trained Embeddings\n\nIn this context, I am building a preliminary deep neural model with different variants of RNNs. I am also building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. \n\nI am not be using any pretrained static\/dynamic embeddings but will be using a simple Neural Network model of LSTM to create the network.\n\nThere are three built-in RNN layers in Keras:\n\n- keras.layers.SimpleRNN, a fully-connected RNN where the output from previous timestep is to be fed to next timestep.\n\n- keras.layers.GRU, first proposed in Cho et al., 2014.\n\n- keras.layers.LSTM, first proposed in Hochreiter & Schmidhuber, 1997.\n\n[Read Jason's Blog - Best Practices for Text Classification](https:\/\/machinelearningmastery.com\/best-practices-document-classification-deep-learning\/)","3651de5c":"<img src=\"https:\/\/media.giphy.com\/media\/l1UkRZuk6FPFYn0ewa\/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">","0d075173":"### Fit the ConvNet Model with GoogleNews Embeddings","b2fb9537":"# Neural Networks with Static Semantic Embeddings Baseline\n \nIn this context, I'd explore certain embeddings which may increase the performance of the model. Pre-trained embeddings provide a better representation of word vectors.\n\n> ## GloVe, GoogleNews and FastText embeddigns are my starting point!!","dde8892f":"### Designing GoogleNews Embedding Matrix","6d400ba1":"> ##  GoogleNews Embeddings","25e38d02":"### BiDirectional RNN model with GloVe Embeddings ","6ce59aa1":"### Plot the loss and accuracy of the model","97c43773":"### ConvNet Model with Fasttext Embeddings","581cff2c":"### BiDirectional RNN Model with Fasttext Embeddings","1c675e10":"### Architecture","b487b90b":"### RNN Model with GoogleNews Embeddings","d62803a9":"## Bi-directional RNN model without pre-trained Embeddings","9df6aae5":"### Plot the accuracy and loss ","b8c1b3ea":"### Confusion Matrix on Validation data","b47d89a5":"<img src=\"https:\/\/media.giphy.com\/media\/xT1R9M8505GD2mz2da\/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">","702fe81c":"### Generic function to plot the train\/validation loss and accuracy","0547c447":"# \ud83d\ude80 END-to-END-Natural Language Processing-3\n\n\n#### In this notebook, I am demonstrating how to build different 'ANN Language Models' with Semantic Embeddings on Keras-TensorFlow 2.0 framework.\n\nFirst, I am building the ConvNet Language Model without pre-trained embedding and using it as benchmark.\n\nSecondly, I'd design the embedding matrix with pre-trained word embeddings (**GloVe,GoogleNews,Fasttext**) to feed to the embedding layer of the neural networks.\n\nThe following Neural Networks are build with the appropriate word embeddings:\n\n* ConvNets\n* Recurrent Neural Networks\/ Long Short Term Memory Cells\n* Bidirectional LSTMs \/ Gated Recurrecnt Units (GRU)\n\n> ### So, let's get started!!!\n\n**As always, I hope you find this kernel useful and your [UPVOTES](https:\/\/www.kaggle.com\/rizdelhi\/quora-insincere-questions-part-3) would be highly appreciated.**","180a06d9":"## About RNNs\n### Recurrent Neural Network (RNN)\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling **sequence data** such as time series or natural language.\n\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n- Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n- Ease of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\nSome resources for understanding the derivatives and optimization inside the RNNs:\n\n[Maths PDF](https:\/\/www.cs.toronto.edu\/~tingwuwang\/rnn_tutorial.pdf)\n\n[Colah's Article on RNNs](https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/)\n\n[Recurrent Neural Networks (RNN) with Keras](https:\/\/www.tensorflow.org\/guide\/keras\/rnn)\n\n### Long Short Term Memory (LSTM)\n\nDrawbacks of RNNS: One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they\u2019d be extremely useful. But can they? It depends. Sometimes, we only need to look at recent information to perform the present task. \n\nFor example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in \u201cthe clouds are in the sky,\u201d we don\u2019t need any further context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it\u2019s needed is small, RNNs can learn to use the past information. But there are also cases where we need more context. \n\nConsider trying to predict the last word in the text \u201cI grew up in France\u2026 I speak fluent French.\u201d Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It\u2019s entirely possible for the gap between the relevant information and the point where it is needed to become very large. Unfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n\nIn theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult. Thankfully, LSTMs don\u2019t have this problem!\n\n\n[LSTM Video](https:\/\/www.youtube.com\/watch?v=WCUNPb-5EYI)\n\n[About LSTM - blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/fundamentals-of-deep-learning-introduction-to-lstm\/)\n\n\nThere are several Variants of LSTMs some of the most famous being Depth GRU \/Gated Recurrent Units.\n\n### Gated Recurrent Unit (GRU)\n\nGRU introduced by Cho, et al. (2014).It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n[Paper: Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43905.pdf)","072aa712":"### Print the model architecture","aac76cdd":"### Data loaded ","e5015ec7":"### Embedding Matrix with Fasttext Embeddings","b2756ffc":"### RNN model on Keras Sequential API with GloVe embeddings","b3f14456":"### Fit the model","8cc358da":"### Fit the model","f342ce63":"### Delete unused memory","693a2237":"### Plot the architecure","c01bcb9e":"<img src=\"https:\/\/media.giphy.com\/media\/3og0IMVPaqrnGfBnZm\/giphy.gif\" align ='left'>","157eac3f":"### Build the ConvNet Model - GoogleNews Embeddings","76b31d95":"### Fit the model","743c81fe":"### Fit the model","80b4eae8":"### Plotting the accuracy & loss ","0b0eb81f":"### Plot the accuracy and loss","51c9b777":"### RNN Model with Fasttext Embeddings","224b663b":"### Fit the model","d9e2662c":"# Basic Building Blocks of Artificial Neural Networks (ANN) Model with Keras-Tensorflow 2.0\n\nWe'll be following the below pipeline to create neural networks for text classification:\n\n1. Tokenize the input features- This implies converting the input data into tokens (by using one hot encoding\/tokenizing) \n2. Tokenize the targets- This can be done with the help of Label Encoder(sklearn) or using \"values()\" from python\n3. Padd the tokenized features- To ensure that the length of the tokenized feature is same across all the entries(post padding)\n4. Create a simple model: Build a Sequential Network with Keras 'Embedding layer' as the starting point - 'Keras Sequential API' \n5. Add Layers to the model: Add either Conv\/LSTM\/Bi-LSTM\/RNN\/GRU layers with different activations ('relu' is recommended)\n6. Add the Dense layer to the model- At the end , we have to add the Dense layer by flattening the output of the previous layer\n7. Add necessary activation functions- Sigmoid for Binary Classification , Softmax for Multi-class classification\n8. Print the model architecture using 'plot_model'\n9. Plot loss\/accuracy of the model with matplotlib\n\nOR\n\nLaunch Tensorboard to visualize the training parameters - loss,accuracy, etc.\n\n\nThis forms the fundamental steps to build a basic but fundamental pipeline for any language modelling task. \n\nSophistications include adding custom embeddings before the keras Embedding layer and then adding certain other layers(transformer architectures) before the LSTM.","75052d11":"## Dynamic Embeddings - ELMo (Embeddings from Language Models)\n\n<img src='https:\/\/images.squarespace-cdn.com\/content\/v1\/5208f2f8e4b0f3bf53b73293\/1486510537795-YWYY4NDK68CT5VBPZZR2\/ke17ZwdGBToddI8pDm48kDrMjE7hBq4fQV3wYHraitJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzj2bmKhA1a89vhGCTEuFcMrGIAhTIwGn2DOXg1A8iNSPxvh_zK_LmuDa3ZMbEzfBk\/Elmo_Emoji_animating_Jazz_Hands_JS_Y_v06.gif?format=2500w'>\n\n\nDeep contextual embeddings and sentence\/word vectors falls under dynamic embeddings. These embeddings are current SOTA implying that there is a need for robust Neural Network models.\n\n\n[\ud83d\udcd6 READ **Attention Is All You Need**- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhine](https:\/\/arxiv.org\/abs\/1706.03762)\n\n[\ud83d\udcd6 READ **Deep contextualized word representations** - Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer](https:\/\/arxiv.org\/abs\/1802.05365)\n\nBoth these papers are essentially important for their contributions to contextual deep embeddings.\n\nI  highly recommend to read these papers!!! These are really cool explanation of how ELMo was designed.\n\nELMo deep contextualized word embeddings (developed by AllenNLP) are helpful in achieving state-of-the-art (SOTA) results in several NLP tasks. \n\n![Lena Voita's Blog](https:\/\/lena-voita.github.io\/resources\/lectures\/transfer\/elmo\/training-min.png)\n\n### Under the hood:\n\nThe architecture above uses a character-level convolutional neural network (CNN) to represent words of a text string into raw word vectors\nThese raw word vectors act as inputs to the first layer of biLM\nThe forward pass contains information about a certain word and the context (other words) before that word\nThe backward pass contains information about the word and the context after it\nThis pair of information, from the forward and backward pass, forms the intermediate word vectors\nThese intermediate word vectors are fed into the next layer of biLM\nThe final representation (ELMo) is the weighted sum of the raw word vectors and the 2 intermediate word vectors\n\nAs the input to the biLM is computed from characters rather than words, it captures the inner structure of the word. For example, the biLM will be able to figure out that terms like beauty and beautiful are related at some level without even looking at the context they often appear in. Sounds incredible!\n\n\n[\ud83d\udcd6 READ Analytical Vidya](https:\/\/www.analyticsvidhya.com\/blog\/2019\/03\/learn-to-use-elmo-to-extract-features-from-text\/)\n\n<img src =\"https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/03\/output_YyJc8E.gif\">","089613d2":"### Build the ConvNet Model - GloVe embeddings","3b2c4029":"## Function to plot confusion matrix","dda2637a":"### BiDirectional RNN model with GoogleNews Embeddings","8e8f982d":"### Architecture of the ConvNet model ","5372f023":"**Previous Kernels**\n\n> [\u26a1END-to-END-Natural Language Processing-1\u26a1- Exploratory Data Analysis & Pre-trained Word Embedding Models](https:\/\/www.kaggle.com\/rizdelhi\/end-to-end-natural-language-processing-1) \n\n> [\u26a1END-to-END-Natural Language Processing-2\u26a1-Statistical Models and Ensemble Technique to Improvise Performance](https:\/\/www.kaggle.com\/rizdelhi\/end-to-end-natural-language-processing-2) ","3259e0b3":"### Fit the model","59172e33":"### Validation F1 Score and Accuracy on the BiRNN model","a9b71016":"### Fit the model","85d7d821":"### Print the Model Architecture","f5d608bd":"### Train data","cdded379":"> #### Visit **[END-to-END-Natural Language Processing-4](https:\/\/www.kaggle.com\/rizdelhi\/end-to-end-natural-language-processing-4)** to the explore implemenation of the following:\n\n- Sequence2Sequence models without\/with Attention Heads\n- Transformers\n- ELMo, DistilBERT embeddings\n- BERT\n- roBERTo\n- ALBERT\n\n\n<img src=\"https:\/\/media.giphy.com\/media\/10b7yI48cD31K0\/giphy.gif\" width=\"300\" height=\"100\" align=\"right\">","526592dd":"> ## GloVe Embeddings","351abf1b":"### Word2Vec Model with Fasttext Embeddings","6b54018e":"<img src=\"https:\/\/media.giphy.com\/media\/10LKovKon8DENq\/giphy.gif\" width=\"300\" height=\"100\" align=\"left\">","1d63085d":"## Basic CNN Model for NLP with Keras Tokenizer\n\nI am using Keras Tokenizer Class to get the embedding layer to build a simple Convolutional Neural Network model.\n\n[Read: Understanding CNN for NLP](http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/)\n\n![CNN for NLP](http:\/\/www.wildml.com\/wp-content\/uploads\/2015\/11\/Screen-Shot-2015-11-06-at-12.05.40-PM.png)","5b62a750":"## Deep ConvNet model without pre-trained embeddings\n\nI am using the 'Keras Embedding' layer and visualize the results before using the embedding models."}}