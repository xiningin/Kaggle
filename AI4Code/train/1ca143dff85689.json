{"cell_type":{"be37f46e":"code","e721a169":"code","5ba9c7a4":"code","66526852":"code","9c7a84e1":"code","68c5fb27":"code","5641e008":"code","4b08dd27":"code","3281e6cd":"code","2908d435":"code","c23ecd2e":"code","df7091b7":"code","47357c61":"code","93aa3c5b":"code","4791c513":"code","6e24db64":"code","08459420":"code","92a56deb":"code","75ec36df":"code","94a4e20b":"code","e9a19012":"code","d315cd75":"code","4b7c5544":"code","1718f091":"code","8b128b21":"code","77efec6a":"code","eefb7b4c":"code","4a3f3129":"code","0742a25c":"code","9e81d2e5":"code","bfae3376":"code","400aa6db":"code","3f2c5777":"code","61c20adf":"code","4c44fd23":"code","8efeda52":"code","a3a85bde":"code","98f351f5":"code","b569b09c":"code","537d2cc2":"code","7d6b4711":"code","cab5861b":"code","72aede23":"code","6d41acdf":"code","3aa61490":"code","a5242efd":"code","588cecb2":"code","6b75215f":"code","66318978":"code","43c7c39c":"code","24980eae":"code","37b419ab":"code","212fe82d":"code","22ab12de":"markdown","90d791d9":"markdown","66920a34":"markdown","d0acc0ae":"markdown","507d2f47":"markdown","f5870654":"markdown","549637be":"markdown","c82badfd":"markdown","15000803":"markdown","9164175b":"markdown","0f51da51":"markdown","72954c11":"markdown","c5da7d62":"markdown","cbee90cf":"markdown","f4b62102":"markdown","6bb46883":"markdown","d90fbc4b":"markdown","8a90af79":"markdown","aa739917":"markdown","043476f4":"markdown","7d79ff6a":"markdown","948479b0":"markdown","a963af53":"markdown","c2a77035":"markdown","22f8301b":"markdown","b90d7b35":"markdown","db8c4532":"markdown","f5ab9402":"markdown","28ec202e":"markdown","d65b3da8":"markdown","343d9ddf":"markdown","b1ad6925":"markdown","feb2091d":"markdown","8ac69b27":"markdown","1778023f":"markdown","d9061645":"markdown","45422d70":"markdown","09f21547":"markdown","5404cdfb":"markdown","ebf30c76":"markdown","5e9a3cb8":"markdown","a6d411bb":"markdown","1673f309":"markdown","a1664e61":"markdown","af594327":"markdown","a4e51c42":"markdown","88152851":"markdown","cb13e27d":"markdown","5a10972c":"markdown","bdea006d":"markdown","2df6df6d":"markdown"},"source":{"be37f46e":"import numpy as np\nimport pandas as pd\n\nimport sklearn \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib as plt\nimport seaborn as sns\n\nimport time","e721a169":"adult = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\",\n        na_values=\"?\")","5ba9c7a4":"adult.head(20)","66526852":"adult.info()","9c7a84e1":"adult.describe(percentiles=[.05, .25, .5, .75, .95], include=['int', 'float'], exclude=[np.object])","68c5fb27":"adult.describe(percentiles=None, include=[np.object])","5641e008":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['age'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (10, 100))\nadult.loc[adult['income'] == '<=50K']['age'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (10, 100))\nfig.subplots_adjust(bottom = -0.3)","4b08dd27":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['hours.per.week'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (0,100))\nadult.loc[adult['income'] == '<=50K']['hours.per.week'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (0,100))\nfig.subplots_adjust(bottom = -0.3)","3281e6cd":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['education.num'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (0,20))\nadult.loc[adult['income'] == '<=50K']['education.num'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (0,20))\nfig.subplots_adjust(bottom = -0.3)","2908d435":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['fnlwgt'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (0,10**6))\nadult.loc[adult['income'] == '<=50K']['fnlwgt'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (0,10**6))\nfig.subplots_adjust(bottom = -0.3)","c23ecd2e":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['capital.gain'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (0,10000))\nadult.loc[adult['income'] == '<=50K']['capital.gain'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (0,10000))\nfig.subplots_adjust(bottom = -0.3)","df7091b7":"fig, axes = plt.pyplot.subplots(nrows = 2, ncols = 1)\nadult.loc[adult['income'] == '>50K']['capital.loss'].plot(kind = 'density', title = 'More than 50K', ax = axes[0], color = 'black', xlim = (0,5000))\nadult.loc[adult['income'] == '<=50K']['capital.loss'].plot(kind = 'density', title = 'Less than 50K', ax = axes[1], color = 'red', xlim = (0,5000))\nfig.subplots_adjust(bottom = -0.3)","47357c61":"sns.heatmap(adult.corr(), annot=True, cmap=plt.cm.Greens)","93aa3c5b":"nadult = adult.dropna()","4791c513":"a1 = nadult.groupby(['sex', 'income']).size().unstack()\na1['sum'] = nadult.groupby('sex').size()\na1 = a1.sort_values('sum', ascending = False)[['<=50K', '>50K']]\na1.plot(kind = 'bar', stacked = True)","6e24db64":"a2 = nadult.groupby(['workclass', 'income']).size().unstack()\na2['sum'] = nadult.groupby('workclass').size()\na2 = a2.sort_values('workclass', ascending = False)[['<=50K', '>50K']]\na2.plot(kind = 'bar', stacked = True)","08459420":"a3 = nadult.groupby(['relationship', 'income']).size().unstack()\na3['sum'] = nadult.groupby('relationship').size()\na3 = a3.sort_values('relationship', ascending = False)[['<=50K', '>50K']]\na3.plot(kind = 'bar', stacked = True)","92a56deb":"a4 = nadult.groupby(['marital.status', 'income']).size().unstack()\na4['sum'] = nadult.groupby('marital.status').size()\na4 = a4.sort_values('marital.status', ascending = False)[['<=50K', '>50K']]\na4.plot(kind = 'bar', stacked = True)","75ec36df":"a5 = nadult.groupby(['occupation', 'income']).size().unstack()\na5['sum'] = nadult.groupby('occupation').size()\na5 = a5.sort_values('occupation', ascending = False)[['<=50K', '>50K']]\na5.plot(kind = 'bar', stacked = True)","94a4e20b":"a6 = nadult.groupby(['education', 'income']).size().unstack()\na6['sum'] = nadult.groupby('education').size()\na6 = a6.sort_values('education', ascending = False)[['<=50K', '>50K']]\na6.plot(kind = 'bar', stacked = True)","e9a19012":"nadult.loc[adult['income'] == '>50K'][\"race\"].value_counts(), nadult[\"race\"].value_counts()","d315cd75":"nadult.loc[adult['income'] == '>50K'][\"native.country\"].value_counts(), nadult[\"native.country\"].value_counts()","4b7c5544":"%%time\n\nXadult = nadult[[\"age\",\"education.num\",\"capital.gain\", \"capital.loss\", \"hours.per.week\"]]\nYadult = nadult[['income']]\nmetrics = ['euclidean', 'manhattan', 'chebyshev', 'minkowski']\nweights = ['uniform', 'distance']\nK_melhor = 0\nmedia_melhor = 0\n\nfor i in range (20, 35):\n    for j in range (2):\n        for k in range (4):\n            knn = KNeighborsClassifier(n_neighbors = i,  weights = weights[j], metric = metrics[k])\n            scores = cross_val_score(knn, Xadult, Yadult.values.ravel(), cv=10)\n            if scores.mean() > media_melhor:\n                media_melhor = scores.mean()\n                K_melhor = i\n                parametros = [weights[j], metrics[k]]\n                print(K_melhor, media_melhor, parametros)\n                \nprint(K_melhor, media_melhor, parametros)","1718f091":"Xadult = nadult[[\"age\",\"education.num\",\"capital.gain\", \"capital.loss\", \"hours.per.week\"]]\nYadult = nadult[[\"income\"]]\n\nknn = KNeighborsClassifier(n_neighbors=31, metric='chebyshev', weights='uniform')\nknn.fit(Xadult, Yadult.values.ravel())","8b128b21":"testAdult = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\", na_values = \"?\")\nnumXtestAdult = testAdult[[\"age\",\"education.num\",\"capital.gain\", \"capital.loss\", \"hours.per.week\"]]\nYtestPred = knn.predict(numXtestAdult)","77efec6a":"testAdult.shape","eefb7b4c":"pd.DataFrame({'Id' : list(range(len(YtestPred)))})\nresultado = pd.DataFrame({'income' : YtestPred})\nprint(resultado)","4a3f3129":"resultado.to_csv(\"submissao.csv\", index = True, index_label = 'Id')","0742a25c":"from sklearn.model_selection import train_test_split","9e81d2e5":"X_train, X_test, Y_train, Y_test = train_test_split(Xadult, Yadult.values.ravel(), test_size=0.2, random_state=42)","bfae3376":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold","400aa6db":"%%time\n\nbest_score_gnb_avg = 0\nbest_var = 0\nfor i in range(1, 101):\n    gnb = GaussianNB(var_smoothing = i\/10000000000).fit(X_train, Y_train)\n    kf = KFold(shuffle=True, n_splits=5)\n    score = cross_val_score(gnb, X_train, Y_train, cv=kf)\n    if best_score_gnb_avg < score.mean():\n        best_gnb = gnb\n        best_score_gnb_avg = score.mean()\n        print('Score:', best_score_gnb_avg, ', var_smoothing:', i\/10000000000)\n        best_var = i\/10000000000","3f2c5777":"from sklearn.naive_bayes import CategoricalNB","61c20adf":"%%time\n\nbest_score_cnb_avg = 0\nscores = []\nbest_alpha = 0\nfor i in range(1, 10001):\n    cnb = CategoricalNB(alpha = i\/10000).fit(X_train, Y_train)\n    scores.append(cnb.score(X_train, Y_train))\n    if best_score_cnb_avg < cnb.score(X_train, Y_train):\n        best_score_cnb_avg = cnb.score(X_train, Y_train)\n        best_alpha = i\/10000\n        best_cnb = cnb\n        print('Score:', best_score_cnb_avg, ', alpha:', best_alpha)\n    if(i>500):\n        num = 6\n        avg_now = 0\n        avg_before = 0\n        for j in range(1, num):\n            avg_now += scores[i-j]\n            avg_before += scores[i+j-501]\n        avg_now \/= num\n        avg_before \/= num \n        if(avg_now < avg_before):\n            print('Stopped at: i=', i)\n            break","4c44fd23":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import loguniform","8efeda52":"%%time\n\nnn_1l = MLPClassifier(random_state=42, early_stopping=True, solver='adam')\nhyperparams = {'hidden_layer_sizes': [i for i in np.arange(2, 100)],\n               'alpha': loguniform(0.000001, 0.01), 'beta_1': [0.895, 0.9, 0.905],\n               'beta_2': [0.99, 0.999, 0.9999]}\n\nnn_clf_1l = RandomizedSearchCV(nn_1l, hyperparams, scoring='accuracy', n_iter=10, cv=5, n_jobs=-1)\nsearch_nn_1l = nn_clf_1l.fit(X_train, Y_train)","a3a85bde":"print('Melhores hiperpar\u00e2metros:', search_nn_1l.best_params_,', Score:', search_nn_1l.best_score_)","98f351f5":"from sklearn.ensemble import RandomForestClassifier","b569b09c":"%%time\n\nrf = RandomForestClassifier(random_state=42)\nhyperparameters = {'n_estimators': [i for i in np.arange(2, 300)], 'criterion': ['gini', 'entropy'],\n                  'min_samples_split': [j for j in np.arange(2, 10)], 'max_features': ['auto', 'log2']}\n\nrf_rs_cv = RandomizedSearchCV(rf, hyperparameters, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\nrf_fit = rf_rs_cv.fit(X_train, Y_train)","537d2cc2":"print('Melhores hiperpar\u00e2metros:', rf_fit.best_params_,', Score:', rf_fit.best_score_)","7d6b4711":"from sklearn.ensemble import AdaBoostClassifier","cab5861b":"%%time\n\nabc = AdaBoostClassifier()\nhyperparameters = {'n_estimators': [i for i in np.arange(50, 300)], 'learning_rate': loguniform(0.9, 1.0),\n                  'algorithm': ['SAMME', 'SAMME.R']}\n\nabc_rs_cv = RandomizedSearchCV(abc, hyperparameters, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\nabc_fit = abc_rs_cv.fit(X_train, Y_train)","72aede23":"print('Melhores hiperpar\u00e2metros:', abc_fit.best_params_, \", Score:\", abc_fit.best_score_)","6d41acdf":"X_test_cnb = X_test.drop(X_test.index[5309])\nY_test_cnb = np.delete(Y_test, 5309)\nX_test_cnb.shape","3aa61490":"%%time\n\nY_pred_gnb = best_gnb.predict(X_test)\nY_pred_cnb = best_cnb.predict(X_test_cnb)\nY_pred_nn = search_nn_1l.predict(X_test)\nY_pred_rf = rf_fit.predict(X_test)\nY_pred_abc = abc_fit.predict(X_test)","a5242efd":"print(\"Accuracy scores:\")\nprint(\"GaussianNB = \", accuracy_score(Y_test, Y_pred_gnb))\nprint(\"CategoricalNB = \", accuracy_score(Y_test_cnb, Y_pred_cnb))\nprint(\"Rede neural = \", accuracy_score(Y_test, Y_pred_nn))\nprint(\"Random Forest = \", accuracy_score(Y_test, Y_pred_rf))\nprint(\"AdaBoost Classifier = \", accuracy_score(Y_test, Y_pred_abc))","588cecb2":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score","6b75215f":"def bin_class(yt, empty_yt): \n    for i in range(len(yt)):\n        if yt[i] == '>50K':\n            empty_yt.append(True)\n        else:\n            empty_yt.append(False)\n            \n    return empty_yt","66318978":"yt_bin = []\nyt_cnb_bin = []\nyt_pred_gnb_bin = []\nyt_pred_cnb_bin = []\nyt_pred_nn_bin = []\nyt_pred_rf_bin = []\nyt_pred_abc_bin = []\n\nyt_bin = bin_class(Y_test, yt_bin)\nyt_cnb_bin = bin_class(Y_test_cnb, yt_cnb_bin)\nyt_pred_gnb_bin = bin_class(Y_pred_gnb, yt_pred_gnb_bin)\nyt_pred_cnb_bin = bin_class(Y_pred_cnb, yt_pred_cnb_bin)\nyt_pred_nn_bin = bin_class(Y_pred_nn, yt_pred_nn_bin)\nyt_pred_rf_bin = bin_class(Y_pred_rf, yt_pred_rf_bin)\nyt_pred_abc_bin = bin_class(Y_pred_abc, yt_pred_abc_bin)","43c7c39c":"print(\"Recall scores:\")\nprint(\"GaussianNB = \", recall_score(yt_bin, yt_pred_gnb_bin))\nprint(\"CategoricalNB = \", recall_score(yt_cnb_bin, yt_pred_cnb_bin))\nprint(\"Rede neural = \", recall_score(yt_bin, yt_pred_nn_bin))\nprint(\"Random Forest = \", recall_score(yt_bin, yt_pred_rf_bin))\nprint(\"AdaBoost Classifier = \", recall_score(yt_bin, yt_pred_abc_bin))","24980eae":"print(\"Precision scores:\")\nprint(\"GaussianNB = \", precision_score(yt_bin, yt_pred_gnb_bin))\nprint(\"CategoricalNB = \", precision_score(yt_cnb_bin, yt_pred_cnb_bin))\nprint(\"Rede neural = \", precision_score(yt_bin, yt_pred_nn_bin))\nprint(\"Random Forest = \", precision_score(yt_bin, yt_pred_rf_bin))\nprint(\"AdaBoost Classifier = \", precision_score(yt_bin, yt_pred_abc_bin))","37b419ab":"print(\"ROC AUC scores:\")\nprint(\"GaussianNB = \", roc_auc_score(yt_bin, yt_pred_gnb_bin))\nprint(\"CategoricalNB = \", roc_auc_score(yt_cnb_bin, yt_pred_cnb_bin))\nprint(\"Rede neural = \", roc_auc_score(yt_bin, yt_pred_nn_bin))\nprint(\"Random Forest = \", roc_auc_score(yt_bin, yt_pred_rf_bin))\nprint(\"AdaBoost Classifier = \", roc_auc_score(yt_bin, yt_pred_abc_bin))","212fe82d":"labels = ['GaussianNB', 'CategoricalNB', 'Rede neural', 'Random Forest', 'AdaBoost']\nacc_scores = [accuracy_score(Y_test, Y_pred_gnb), accuracy_score(Y_test_cnb, Y_pred_cnb),\n              accuracy_score(Y_test, Y_pred_nn), accuracy_score(Y_test, Y_pred_rf), \n              accuracy_score(Y_test, Y_pred_abc)]\n\nrecall_scores = [recall_score(yt_bin, yt_pred_gnb_bin), recall_score(yt_cnb_bin, yt_pred_cnb_bin),\n              recall_score(yt_bin, yt_pred_nn_bin), recall_score(yt_bin, yt_pred_rf_bin), \n              recall_score(yt_bin, yt_pred_abc_bin)]\n\nprecision_scores = [precision_score(yt_bin, yt_pred_gnb_bin), precision_score(yt_cnb_bin, yt_pred_cnb_bin),\n              precision_score(yt_bin, yt_pred_nn_bin), precision_score(yt_bin, yt_pred_rf_bin), \n              precision_score(yt_bin, yt_pred_abc_bin)]\n\nroc_auc_scores = [roc_auc_score(yt_bin, yt_pred_gnb_bin), roc_auc_score(yt_cnb_bin, yt_pred_cnb_bin),\n              roc_auc_score(yt_bin, yt_pred_nn_bin), roc_auc_score(yt_bin, yt_pred_rf_bin), \n              roc_auc_score(yt_bin, yt_pred_abc_bin)]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.2  # the width of the bars\n\nfig, ax = plt.pyplot.subplots()\nrects1 = ax.bar(x - 1.5*width, acc_scores, width, label='Accuracy')\nrects2 = ax.bar(x - width\/2, recall_scores, width, label='Recall')\nrects3 = ax.bar(x + width\/2, precision_scores, width, label='Precision')\nrects4 = ax.bar(x + 1.5*width, roc_auc_scores, width, label='ROC AUC')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Scores')\nax.set_title('Scores por classificador')\nax.set_xticks(x)\nax.set_xticklabels(labels)\nax.legend()\n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{:1.4f}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\nautolabel(rects1)\nautolabel(rects2)\nautolabel(rects3)\nautolabel(rects4)\n\nfig.set_size_inches(18.5, 10.5)\n\nplt.pyplot.show()","22ab12de":"Identificando os tipos de dados recebidos:","90d791d9":"Ap\u00f3s encontrar os par\u00e2metros com melhor perfomance, os par\u00e2metros do algoritmo s\u00e3o ajustados","66920a34":"A varia\u00e7\u00e3o dos par\u00e2metros ser\u00e1 dada pelo n\u00famero de \u00e1rvores (entre 2 e 300), crit\u00e9rio de avalia\u00e7\u00e3o (Gini ou entropia), n\u00famero de ramos de cada n\u00f3 e *features* de avalia\u00e7\u00e3o. Os par\u00e2metros para o RandomizedSearchCV continuam os mesmos:","d0acc0ae":"Treinamento do modelo e resultados:","507d2f47":"# Conclus\u00f5es\n\nNo geral, **melhor classificador** obtido foi o baseado na fun\u00e7\u00e3o **CategoricalNB**, com os melhores valores para as m\u00e9tricas de acur\u00e1cia, precis\u00e3o e ROC AUC e segundo melhor valor de *recall* (um pouco atr\u00e1s da Random Forest). O **segundo melhor** foi o **AdaBoost**, com m\u00e9tricas bem pr\u00f3ximas \u00e0s do melhor classificador. O **terceiro melhor** foi o baseado no algoritmo de **Random Forest**, que mesmo que possua o melhor *recall* ainda tem valores de acur\u00e1cia e precis\u00e3o menores que os outros 2 (tamb\u00e9m menor que o de Rede Neural). O **quarto melhor** foi o baseado na rede neural, e o **pior**, tamb\u00e9m mais simples, baseado no algoritmo **GausianNB**.\n\nEnfim, para os 3 melhores n\u00e3o foram observadas grandes diferen\u00e7as para as m\u00e9tricas medidas, sendo tais diferen\u00e7as maiores a partir do quarto melhor. Tamb\u00e9m \u00e9 interessante notar que o melhor classificador nem sempre \u00e9 o mais complexo e demorado para se obter resultados, visto que o melhor obtido nesse caso foi baseado num dos algoritmos mais simples (NB) e r\u00e1pidos para fazer as classifica\u00e7\u00f5es.","f5870654":"Sal\u00e1rio e educa\u00e7\u00e3o (contabilizada de forma n\u00e3o num\u00e9rica): novamente, quanto maior o tempo de educa\u00e7\u00e3o, maior tende a ser o sal\u00e1rio","549637be":"**2.2 - Rede Neural**\n\nO **segundo classificador** a ser constru\u00eddo ser\u00e1 baseado na **rede neural com 1 camada oculta**, tal qual constru\u00eddo no terceiro exerc\u00edcio.\n\n*Imports*:","c82badfd":"Educa\u00e7\u00e3o - quanto maior o n\u00famero de anos recebidos em educa\u00e7\u00e3o, maior tende a ser o sal\u00e1rio","15000803":"Resultados para *scores* de ROC AUC:","9164175b":"Identifica\u00e7\u00e3o dos percentis dos dados, m\u00e9dia e desvio-padr\u00e3o tamb\u00e9m:","0f51da51":"Status de casamento: nitidamente, a maior propor\u00e7\u00e3o salarial \u00e9 para pessoas que estejam casadas","72954c11":"# **PARTE 2 - *Classification with the Adult dataset***\n\nA partir daqui ser\u00e3o constru\u00eddos os novos classificadores al\u00e9m do kNN criado no primeiro exerc\u00edcio, por\u00e9m utilizando os mesmos dados tratados anteriormente.\n\nPrimeiramente, como no *dataset* n\u00e3o h\u00e1 uma base de dados pr\u00f3prios para os testes, temos que dividir a base Adult em dados de treino e de testes (*random state* = 42, como usado no EP de redes neurais) :","c5da7d62":"No entanto, para avaliarmos tais m\u00e9tricas devemos transformar as classifica\u00e7\u00f5es originais de *strings* ('>50K', '<=50K') em formato bin\u00e1rio:","cbee90cf":"Verifica\u00e7\u00e3o do cabe\u00e7alho","f4b62102":"Tamb\u00e9m podemos avaliar os modelos a partir de outras m\u00e9tricas, como precis\u00e3o, *recall* e ROC AUC:","6bb46883":"Agora veremos os resultados para as predi\u00e7\u00f5es de cada modelo.\n\nInicialmente, notei que havia um *bug* com a linha 5309 resultante do modelo com o CategoricalNB, que impedia que a predi\u00e7\u00e3o fosse realizada (erro tal qual a imagem abaixo). O *bug* ocorria pois o valor para a coluna de ***capital loss*** estava muito acima dos outros, tornando-se fora do limite de valores suportados pelo classificador. Assim, optei por excluir esta linha para a predi\u00e7\u00e3o deste classificador (o que deixa sua base de testes com 1 linha a menos, totalizando 6032 linhas, e felizmente interferindo muito pouco no *score* da classifica\u00e7\u00e3o): ![image.png](attachment:image.png)","d90fbc4b":"Sal\u00e1rio e classe de trabalho: maiores propor\u00e7\u00f5es s\u00e3o observadas para trabalhadores dos setores privado, local-gov, self-empl-inc e self-empl-not-inc. O mais baixo s\u00e3o o que n\u00e3o recebem pagamento..","8a90af79":"Tamb\u00e9m h\u00e1 outro subclassificador melhor ainda que o GaussianNB para nossa an\u00e1lise, o **CategoricalNB**, pr\u00f3prio para trabalhar com dados categ\u00f3ricos tais quais nosso dataframe pois produz diferentes *folds* a cada itera\u00e7\u00e3o. O \u00fanico problema em utiliz\u00e1-lo \u00e9 que \u00e9 impr\u00f3prio para se fazer *cross validation* com a fun\u00e7\u00e3o cross_val_score devido ao formato que os dados s\u00e3o retornados ap\u00f3s o .fit(), tendo ent\u00e3o que usar o m\u00e9todo .score() da pr\u00f3prio fun\u00e7\u00e3o CategoricalNB (o que n\u00e3o h\u00e1 problema, pois por padr\u00e3o ambos *scores* s\u00e3o a *accuracy* do modelo). Ainda assim, podemos ver como \u00e9 melhor que, inclusive, o pr\u00f3prio kNN:","aa739917":"# **PARTE 1 - *kNN for Adult dataset***\n\nCome\u00e7ando pelos *imports* das bibliotecas:","043476f4":"Ganho de capital - maiores ganhos de capital s\u00e3o observados com maiores sal\u00e1rios","7d79ff6a":"Os hiperpar\u00e2metros a serem mudados ser\u00e3o o tamanho da camada oculta, o *alpha*, *beta_1* e o *beta_2*. Tamb\u00e9m ser\u00e1 medida a acur\u00e1cia, tal qual os modelos anteriores, com 10 itera\u00e7\u00f5es para os hiperpar\u00e2metros e com cv dividido em 5 *folds*:","948479b0":"Enfim, tamb\u00e9m podemos visualizar tais resultados de forma gr\u00e1fica (com formata\u00e7\u00e3o de d\u00edgitos para n\u00e3o prejudicar a visualiza\u00e7\u00e3o):","a963af53":"Plotagem de gr\u00e1ficos com dados num\u00e9ricos. Pode-se ver que a maioria das pessoas com sal\u00e1rio > 50K\/ano possuem idade pr\u00f3xima a 40 anos, e que as pessoas com sal\u00e1rios menores do que 50K\/ano s\u00e3o mais novas, com concentra\u00e7\u00e3o entre 20 e 30 anos","c2a77035":"Peso final (fnlwgt) por densidade - pela formas semelhantes dos gr\u00e1ficos n\u00e3o h\u00e1 diferen\u00e7as significativas","22f8301b":"Leitura dos dados de treinamento:","b90d7b35":"Correla\u00e7\u00e3o entre sal\u00e1rio e dados discretos (n\u00e3o num\u00e9ricos)\nSal\u00e1rio e sexo: pode-se notar que, no geral, homens tendem a ganhar sal\u00e1rios maiores","db8c4532":"Sal\u00e1rio e ocupa\u00e7\u00e3o:","f5ab9402":"**2.4 - AdaBoost**","28ec202e":"Remove valores n\u00e3o identificados da base de treinamento:","d65b3da8":"Aqui n\u00e3o foi plotado um gr\u00e1fico devida a falta de propor\u00e7\u00e3o absurda das colunas.\n\nSal\u00e1rio e etnia: no geral, asi\u00e1ticos e ilheus do pac\u00edfico tendem a receber proporcionalmente os maiores sal\u00e1rios, seguidos por brancos, negros, latinos e outros:","343d9ddf":"A primeira parte deste EP, feita em setembro deste ano, tinha como objetivo tratar os dados do *dataset Adult*, analisar suas correla\u00e7\u00f5es e criar um modelo de classificador kNN utilizando *cross-validation* para classificar uma base de testes. Como esta parte j\u00e1 foi completada e avaliada anteriormente, optei por mant\u00ea-la praticamente igual ao que estava, adicionando a segunda parte de criar novos classificadores na se\u00e7\u00e3o sinalizada com um *header* por **PARTE 2 - *Classification with the Adult dataset***","b1ad6925":"**2.3 - *Random Forest***\n\n","feb2091d":"Este classificador possui tamb\u00e9m um \u00fanico hiperpar\u00e2metro, o *alpha*. Desta forma, vamos ver qual *alpha* produz o melhor *score* m\u00e9dio (com um crit\u00e9rio de parada para ver se a acur\u00e1cia continua crescendo):","8ac69b27":"Resultados para *scores* de precis\u00e3o","1778023f":"Esta fun\u00e7\u00e3o possui apenas um hiperpar\u00e2metro a ser variado, o *var_smoothing* (*default*: 10e-9), que segundo a descri\u00e7\u00e3o da biblioteca \u00e9 a por\u00e7\u00e3o das vari\u00e2ncias de todas as *features* a serem adicionadas \u00e0 vari\u00e2ncia do modelo para o c\u00e1lculo da estabilidade. \n\nAssim, vamos avaliar o *alpha* entre o intervalo (10e-10, 10e-8) e analisar qual dos valores produziu o modelo com melhor *score* m\u00e9dio com cross-validation, utilizando K-Folding: ","d9061645":"Os hiperpar\u00e2metros a serem variados ser\u00e3o o n\u00famero de estimadores, a taxa de aprendizagem e o tipo de algoritmo:","45422d70":"Predi\u00e7\u00f5es:","09f21547":"Identifica\u00e7\u00e3o de dados n\u00e3o num\u00e9ricos:","5404cdfb":"Mapa de correla\u00e7\u00e3o:\n\nPode-se ver a correla\u00e7\u00e3o entre as vari\u00e1veis num\u00e9ricas. No geral, todas as vari\u00e1veis s\u00e3o correlacionadas de forma fraca. As mais fortemente correlacionadas s\u00e3o n\u00famero de anos de educa\u00e7\u00e3o com horas de trabalho por semana \/ ganho de capital\n","ebf30c76":"Enfim, o **quarto classificador** a ser criado ser\u00e1 feito a partir do **AdaBoost**:","5e9a3cb8":"**2.1 - Naive-Bayes**\n\nO **primeiro classificador** a ser analisado ser\u00e1 o classificador de **Naive-Bayes**. Dentre os diversos tipos de classificadores NB dispon\u00edveis na biblioteca SKLearn, vou utilizar o **GaussianNB**, pois os outros s\u00e3o para dados bin\u00e1rios (BernoulliNB) ou multinomiais (MultinomialNB)\n\n*Import* da fun\u00e7\u00e3o dispon\u00edvel na biblioteca SKLearn:","a6d411bb":"Pa\u00eds de origem: no geral, n\u00e3o d\u00e1 para tirar conclus\u00f5es devido a falta de dados dos outros pa\u00edses","1673f309":"**Resultados e compara\u00e7\u00f5es**","a1664e61":"Sal\u00e1rio e situa\u00e7\u00e3o de relacionamento: maridos e esposas recebem, proporcionalmente, os maiores sal\u00e1rios de todos. Tamb\u00e9m pessoas que n\u00e3o est\u00e3o numa fam\u00edlia tendem a receber alto sal\u00e1rio.","af594327":"O **terceiro classificador** a ser usado ser\u00e1 o **Random Forest**:","a4e51c42":"Perda de capital - apesar de n\u00e3o ser muito not\u00f3ria, a maior perda de capital tamb\u00e9m acompanha maiores sal\u00e1rios, no geral","88152851":"[](http:\/\/)","cb13e27d":"Horas de trabalho por densidade da concentra\u00e7\u00e3o do sal\u00e1rio - h\u00e1 correla\u00e7\u00e3o de que, quanto mais uma pessoa trabalha, maior \u00e9 o sal\u00e1rio (apesar desta correla\u00e7\u00e3o n\u00e3o ser muito alta)\n","5a10972c":"Resultados para *scores* de *recall*","bdea006d":"Encontrando o KNN por itera\u00e7\u00f5es:","2df6df6d":"Agora vejamos qual dos modelos alcan\u00e7ou o melhor *score* para acur\u00e1cia:"}}