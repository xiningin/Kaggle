{"cell_type":{"7abd1913":"code","cdd6c72f":"code","132aea6a":"code","e8386d16":"code","2dbcd10f":"code","a7b29f1e":"code","f3942757":"code","0fb1d23d":"code","efeb32f7":"code","310f191c":"code","300497aa":"code","d6dab17c":"code","f42ea16f":"code","385e5ec1":"code","aad095bf":"code","5ea2df77":"code","709831ce":"code","bb7058f3":"code","0b9fc8df":"code","0e618caf":"code","629020f6":"code","28a33d25":"code","ec1f2376":"markdown"},"source":{"7abd1913":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport keras\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras import optimizers\nfrom keras import backend as K\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.layers import Dense, Activation, Flatten, Dense,MaxPooling2D, Dropout\nfrom keras.layers import Conv2D,MaxPool2D, BatchNormalization\nfrom keras.layers import Input, GlobalAveragePooling2D\nfrom keras import models\nfrom keras.models import Model","cdd6c72f":"img_size = 124\ntrain_data = []\nval_data = []\ntest_data = []\ndirectory = \"..\/input\/flowers-recognition\/flowers\/flowers\"\nfor sub_directory in os.listdir(directory):\n    count = 0\n    inner_directory = os.path.join(directory,sub_directory)\n    test_limit = int(0.85*(len(os.listdir(inner_directory))))\n    val_limit = int(0.8*test_limit)\n    for i in os.listdir(inner_directory):\n        try:\n            count += 1\n            img = cv2.imread(os.path.join(inner_directory,i),1)\n            img = cv2.resize(img,(img_size,img_size))\n            if count < val_limit:\n                train_data.append([img,sub_directory])\n            elif val_limit <= count < test_limit:\n                val_data.append([img,sub_directory])\n            else:\n                test_data.append([img,sub_directory])\n        except:\n            pass","132aea6a":"random.shuffle(train_data)\nrandom.shuffle(val_data)\nrandom.shuffle(test_data)","e8386d16":"train_X = []\ntrain_Y = []\nfor features,label in train_data:\n    train_X.append(features)\n    train_Y.append(label)","2dbcd10f":"val_X = []\nval_Y = []\nfor features,label in val_data:\n    val_X.append(features)\n    val_Y.append(label)\n","a7b29f1e":"test_X = []\ntest_Y = []\nfor features,label in test_data:\n    test_X.append(features)\n    test_Y.append(label)","f3942757":"train_X = np.array(train_X)\/255.0\ntrain_X = train_X.reshape(-1,124,124,3)\ntrain_Y = np.array(train_Y)","0fb1d23d":"val_X = np.array(val_X)\/255.0\nval_X = val_X.reshape(-1,124,124,3)\nval_Y = np.array(val_Y)","efeb32f7":"test_X = np.array(test_X)\/255.0\ntest_X = test_X.reshape(-1,124,124,3)\ntest_Y = np.array(test_Y)","310f191c":"train_X.shape","300497aa":"w=10\nh=10\nfig=plt.figure(figsize=(12,12))\ncolumns = 5\nrows = 5\nfor i in range(1, columns*rows +1):\n    img = train_X[i]\n    fig.add_subplot(rows, columns, i)\n    plt.imshow(np.squeeze(img))\nplt.show()","d6dab17c":"LE = LabelEncoder()\ntrain_Y = LE.fit_transform(train_Y)\ntest_Y = LE.fit_transform(test_Y)\nval_Y = LE.fit_transform(val_Y)\ntrain_Y = np_utils.to_categorical(train_Y)\ntest_Y = np_utils.to_categorical(test_Y)\nval_Y = np_utils.to_categorical(val_Y)","f42ea16f":"train_Y.shape","385e5ec1":"train_Y[0]","aad095bf":"model = Sequential()\n\nmodel.add(Conv2D(64, (3, 3), activation='relu',padding = 'same', input_shape=(124,124,3)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(64, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(128, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(128, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(256, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(256, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(256, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(512, (3, 3),padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.3))\n\nmodel.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(5, activation='softmax'))","5ea2df77":"model.summary()","709831ce":"model.compile(loss='categorical_crossentropy', optimizer=\"adam\",metrics=['accuracy'])","bb7058f3":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,    \n        rotation_range=20,    \n        width_shift_range=0.1,\n        height_shift_range=0.1,  \n        horizontal_flip=False,  \n        vertical_flip=False)\ndatagen.fit(train_X)","0b9fc8df":"history = model.fit(datagen.flow(train_X,train_Y, batch_size = 64) ,epochs = 200 , validation_data = datagen.flow(val_X, val_Y))","0e618caf":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Training Accuracy vs Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","629020f6":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Training Loss vs Validation Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","28a33d25":"score = model.evaluate(test_X, test_Y, verbose=0)\nprint(\"Loss: \" + str(score[0]))\nprint(\"Accuracy: \" + str(score[1]*100) + \"%\")","ec1f2376":"If you like this notebook, then do **Upvote** as it will keep me motivated in creating such kernels ahead. **Thanks!!**"}}