{"cell_type":{"e0055007":"code","a6611f56":"code","46686a4a":"code","244fedcf":"code","a7d7efc1":"code","6950a393":"code","4dfda1c4":"code","99cd3a34":"code","c03211c7":"code","3f15e4d1":"code","18e7dc61":"code","ea77a5d7":"code","46a1150e":"code","69014f19":"code","ef5dda2e":"code","f9dda44a":"code","f123f360":"code","2d49bd82":"code","8f1f2738":"code","95cf0c69":"code","32445487":"code","6d11d01b":"code","8d3b5539":"code","438bda40":"code","1770ca6e":"code","9895aa30":"code","14fddd02":"code","09e0a8b3":"markdown","51c37077":"markdown","85ca3ddb":"markdown","c7b33c7c":"markdown","6b7e8829":"markdown","cb7d7256":"markdown","6df03dce":"markdown","54c55ee5":"markdown","03c8a865":"markdown","3bff1945":"markdown","93a4f71c":"markdown","f8c2cd83":"markdown","166d4075":"markdown","57e0ca6a":"markdown","a58aae0c":"markdown","adb88677":"markdown","7e9cb163":"markdown"},"source":{"e0055007":"import pandas as pd\n\ndata = pd.read_csv('..\/input\/257k-gaiadr2-sources-with-photometry.csv', dtype={'source_id': str})","a6611f56":"len(data)","46686a4a":"_seen = set()\nshould_remove_set = set()\nfor _source_id in data['source_id']:\n    if _source_id in _seen:\n        should_remove_set.add(_source_id)\n    else:\n        _seen.add(_source_id)","244fedcf":"data = data[~data['source_id'].isin(should_remove_set)].reset_index(drop=True)\nlen(data)","a7d7efc1":"assert len(data) == len(set(data['source_id']))","6950a393":"import inspect\n\npd_concat_argspec = inspect.getfullargspec(pd.concat)\npd_concat_has_sort = 'sort' in pd_concat_argspec.args\n\ndef pd_concat(frames):\n    # Due to Pandas versioning issue\n    new_frame = pd.concat(frames, sort=False) if pd_concat_has_sort else pd.concat(frames)\n    new_frame.reset_index(inplace=True, drop=True)\n    return new_frame","4dfda1c4":"import types\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \n\nnp.random.seed(201808011)\n\ndef model_results(data_frame, label_extractor, var_extractor, trainer_factory, id_column='source_id', n_splits=2, n_runs=3, scale=False, max_n_training=None):\n    '''\n    Returns a frame with source_id, response and residual columns, with the same ordering as data_frame.\n    '''\n    sum_series = pd.Series([0] * len(data_frame))\n    for r in range(n_runs):\n        shuffled_frame = data_frame.sample(frac=1)\n        shuffled_frame.reset_index(inplace=True, drop=True)\n        response_frame = pd.DataFrame(columns=[id_column, 'response'])\n        kf = KFold(n_splits=n_splits)\n        for train_idx, test_idx in kf.split(shuffled_frame):\n            train_frame = shuffled_frame.iloc[train_idx]\n            if max_n_training is not None:\n                train_frame = train_frame.sample(max_n_training)\n            test_frame = shuffled_frame.iloc[test_idx]\n            train_labels = label_extractor(train_frame) if isinstance(label_extractor, types.FunctionType) else train_frame[label_extractor]\n            train_vars = var_extractor(train_frame)\n            test_vars = var_extractor(test_frame)\n            if scale:\n                scaler = StandardScaler()  \n                scaler.fit(train_vars)\n                train_vars = scaler.transform(train_vars)  \n                test_vars = scaler.transform(test_vars) \n            trainer = trainer_factory()\n            fold_model = trainer.fit(train_vars, train_labels)\n            test_responses = fold_model.predict(test_vars)\n            test_id = test_frame[id_column]\n            assert len(test_id) == len(test_responses)\n            fold_frame = pd.DataFrame({id_column: test_id, 'response': test_responses})\n            response_frame = pd_concat([response_frame, fold_frame])\n        response_frame.sort_values(id_column, inplace=True)\n        response_frame.reset_index(inplace=True, drop=True)\n        assert len(response_frame) == len(data_frame), 'len(response_frame)=%d' % len(response_frame)\n        sum_series += response_frame['response']\n    cv_response = sum_series \/ n_runs\n    assert len(cv_response) == len(data_frame)\n    sorted_result = pd.DataFrame({\n        id_column: np.sort(data_frame[id_column].values), \n        'response': cv_response})\n    data_frame_partial = pd.DataFrame({id_column: data_frame[id_column]})\n    merged_frame = pd.merge(data_frame_partial, sorted_result, how='inner', on=id_column, sort=False)\n    data_frame_labels = label_extractor(data_frame) if isinstance(label_extractor, types.FunctionType) else data_frame[label_extractor]\n    merged_frame['residual'] = data_frame_labels - merged_frame['response']\n    assert len(merged_frame) == len(data_frame)\n    return merged_frame","99cd3a34":"import math\nimport scipy.stats as stats\n\ndef print_evaluation(data_frame, label_column, response_frame):\n    _response = response_frame['response']\n    _label = label_column(data_frame) if isinstance(label_column, types.FunctionType) else data_frame[label_column]\n    _error = _label - _response\n    assert sum(response_frame['residual'] == _error) == len(data_frame)\n    _rmse = math.sqrt(np.sum(_error ** 2) \/ len(data_frame))\n    _correl = stats.pearsonr(_response, _label)[0]\n    print('RMSE: %.4f | Correlation: %.4f' % (_rmse, _correl,), flush=True)","c03211c7":"def extract_mag_model_vars(data_frame):\n    distance = (1000.0 \/ data_frame['parallax']).values\n    log_distance = np.log(distance)\n\n    feature_list = [log_distance, distance]\n    feature_list.append(data_frame['phot_g_mean_mag'] - data_frame['phot_rp_mean_mag'])\n    feature_list.append(data_frame['phot_bp_mean_mag'] - data_frame['phot_g_mean_mag'])\n    \n    return np.transpose(feature_list)    ","3f15e4d1":"LABEL_COLUMN = 'phot_g_mean_mag'\nMAX_N_TRAINING = 40000","18e7dc61":"from sklearn.neural_network import MLPRegressor\n\ndef get_mag_trainer():\n    return MLPRegressor(hidden_layer_sizes=(50, 25), max_iter=500, alpha=0.1, random_state=np.random.randint(1, 10000))","ea77a5d7":"mag_results = model_results(data, LABEL_COLUMN, extract_mag_model_vars, get_mag_trainer, n_runs=2, scale=True, max_n_training=MAX_N_TRAINING)","46a1150e":"print_evaluation(data, LABEL_COLUMN, mag_results)","69014f19":"NUM_OUTLIERS = 1000","ef5dda2e":"def produce_outliers():\n    global data_dim_outliers\n    global data_bright_outliers\n    global idx_dim_outlier\n    global idx_bright_outlier\n    \n    idx_res_sort = np.argsort(mag_results['residual'].values)\n    idx_dim_outlier = idx_res_sort[-NUM_OUTLIERS:]\n    idx_bright_outlier = idx_res_sort[:NUM_OUTLIERS]\n    data_dim_outliers = data.iloc[idx_dim_outlier]\n    data_bright_outliers = data.iloc[idx_bright_outlier]","f9dda44a":"import matplotlib.pyplot as plt","f123f360":"def plot_outliers():\n    plt.rcParams['figure.figsize'] = (14, 7)\n    plt.scatter(data_dim_outliers['ra'], data_dim_outliers['dec'], color='blue', s=8)\n    plt.scatter(data_bright_outliers['ra'], data_bright_outliers['dec'], color='red', s=8)\n    plt.title('Dim (blue) and bright (red) outliers')\n    plt.xlabel('Right Ascension (degrees)')\n    plt.ylabel('Declination (degrees)')\n    plt.show()\n    plt.scatter(data_dim_outliers['l'], data_dim_outliers['b'], color='blue', s=8)\n    plt.scatter(data_bright_outliers['l'], data_bright_outliers['b'], color='red', s=8)\n    plt.title('Dim (blue) and bright (red) outliers')\n    plt.xlabel('Galactic longitude (degrees)')\n    plt.ylabel('Galactic latitude (degrees)')\n    plt.show()\n    \nproduce_outliers()    \nplot_outliers()","2d49bd82":"S_RA_MIN = 65\nS_RA_MAX = 115\nS_DEC_MIN = -68\nS_DEC_MAX = -64","8f1f2738":"data_ra = data['ra']\ndata_dec = data['dec']\ns_mask_bottom_left = (data_ra >= S_RA_MIN) & (data_ra <= S_RA_MAX) & (data_dec >= S_DEC_MIN) & (data_dec <= S_DEC_MAX)\ns_mask_top_right = (data_ra >= S_RA_MIN + 180) & (data_ra <= S_RA_MAX + 180) & (data_dec >= -S_DEC_MAX) & (data_dec <= -S_DEC_MIN)\ns_mask = s_mask_bottom_left | s_mask_top_right","95cf0c69":"should_remove_set = should_remove_set.union(set(data[s_mask]['source_id']))","32445487":"len(should_remove_set)","6d11d01b":"data = data[~s_mask]\ndata.reset_index(inplace=True, drop=True)\nmag_results = mag_results[~s_mask]\nmag_results.reset_index(inplace=True, drop=True)","8d3b5539":"len(data)","438bda40":"assert np.sum(mag_results['source_id'] == data['source_id']) == len(data)","1770ca6e":"produce_outliers()\nplot_outliers()","9895aa30":"should_remove_frame = pd.DataFrame({\n    'source_id': list(should_remove_set)\n})","14fddd02":"should_remove_frame.to_csv('257k-gaiadr2-should-remove.csv', index=False)","09e0a8b3":"## New charts\nWhat do outlier visualizations look like now?","51c37077":"## Acknowledgments\n\nThis work has made use of data from the European Space Agency (ESA) mission Gaia (https:\/\/www.cosmos.esa.int\/gaia), processed by the Gaia Data Processing and Analysis Consortium (DPAC, https:\/\/www.cosmos.esa.int\/web\/gaia\/dpac\/consortium). Funding for the DPAC has been provided by national institutions, in particular the institutions participating in the Gaia Multilateral Agreement.","85ca3ddb":"## Data\nWe will use a [dataset of 257K Gaia DR2 stars](https:\/\/www.kaggle.com\/solorzano\/257k-gaia-dr2-stars) in the Northern and Southern Hemispheres, with magnitude of at most 13.5 and parallax of at least 2.0 mas (500 parsecs).","c7b33c7c":"We will also define a function that lets us evaluate the responses of a model trained with the *model_results* function.","6b7e8829":"## Overview\nWhen you train a Gaia-only model of stellar magnitude, a couple of obvious artifacts show up in a chart of residual outlier positions, at opposite ends of the celestial sphere. A list of the problematic Gaia DR2 source_id's from the [input dataset](https:\/\/www.kaggle.com\/solorzano\/257k-gaia-dr2-stars) is made available in the output tab.","cb7d7256":"Seems alright.","6df03dce":"Let's also make sure we capture this set of stars in the global collection we will write to the output tab.","54c55ee5":"## Removal of duplicates\nThe dataset contains row pairs that have the same *source_id*. It appears that some Gaia DR2 sources match two Tycho2 sources. We'll simply remove the problematic pairs before proceeding.","03c8a865":"Now we can remove the problematic sources from the *data* frame and the frame containing the results of the regression model.","3bff1945":"## Output\nLet's dump the list of source_id's that should be removed from the original dataset. Note that they not only include sources with the positional systematic involving anomalously dim stars, but also those that have multiple Tycho2 matching sources.","93a4f71c":"The following function is used to train a regression model, averaging multiple runs of k-fold cross-validation. It can use an arbitrary sklearn-style regressor (one with a *fit* function.) ","f8c2cd83":"## Outlier positions\nLet's get 1000 outliers from each end of the model's residual distribution, and present them in positional scatter charts.","166d4075":"## Variables\nWe'll keep it simple: Model variables will only include distance, logarithm of distance, and a couple \"color index\" features from Gaia itself. We're neglecting extinction near the galactic plane, but that's OK for purposes of this kernel.","57e0ca6a":"## Removal of stars with apparent systematic issue\nThere is a blue horizontal line of abnormally dim stars at a declination of about -65\u00b0, with a right ascension near 90\u00b0. A very similar feature also appears at the exact opposite end of the celestial sphere.\n\nIt's not clear what causes this artifact. Removal will be ad-hoc. The following bounding box covers the artifact in the bottom-left of the RA-DEC chart.","a58aae0c":"## Modeling helper functions\nThis function is just boilerplate:","adb88677":"Let's define a boolean series (*s_mask*) that is *True* only for *data* rows that are either in the bounding box defined above, or an equivalent box at the opposite end of the celestial sphere.","7e9cb163":"## Magnitude model\nThe regression label will be the *g* magnitude column. We'll train the model with a Neural Network. A GBM, for example, would also do, but it seems that Neural Network results are clearer in the outlier visualization."}}