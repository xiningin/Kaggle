{"cell_type":{"243bf0c6":"code","45f8e7e4":"code","17b071bf":"code","3e5b56c0":"code","1d73c759":"code","490a793c":"code","d07bfef7":"code","4ef1ac8e":"code","4a114000":"code","c4cef2ec":"code","f29bcf80":"code","8ac261fd":"code","0ad5e94c":"code","eaa7a729":"code","8634d636":"code","46cddeef":"code","a9a2cc1a":"code","e40b374c":"code","d471882c":"code","ff89d461":"code","c047e428":"code","f6b4c48e":"code","159b7ea8":"code","163b2e8d":"code","7f4f4576":"code","6b43f660":"code","a113456a":"code","e7d955f0":"code","1798e05f":"code","b1ab82e7":"code","b47351dd":"code","63468696":"code","26b96f49":"code","3024651e":"code","6c9a2c4a":"code","b9ed3310":"code","780de874":"code","6e65f4ba":"code","dd9889a1":"code","b13ae856":"code","bb50dad9":"code","c2188f7b":"code","328d50c7":"code","794898f0":"code","b1065574":"code","b140407a":"code","1fa42089":"code","037c5969":"code","64a856c5":"code","c9f6ded7":"code","cf805196":"code","abea3798":"code","2b07e5f1":"code","1ebe3869":"code","0e8bc093":"code","24bfdd09":"code","aadbad3c":"code","99d735ea":"code","8ac6de29":"code","c90f2ba9":"code","d36bc00d":"code","3e15aae3":"code","61f5663c":"code","989661f1":"code","939c0f7c":"code","b86374cd":"code","f4bb453f":"code","57c49aac":"code","7369d273":"code","76cebd84":"code","f2f205ea":"code","7821b532":"code","1b9e7f8b":"markdown"},"source":{"243bf0c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current sessionprint(vectorizer.vocabulary_)","45f8e7e4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MaxAbsScaler","17b071bf":"train_data = pd.read_csv('..\/input\/iba-ml1-final-project\/train.csv')\ntest_data = pd.read_csv('..\/input\/iba-ml1-final-project\/test.csv')","3e5b56c0":"sc = MaxAbsScaler()\n[train_data['Age']] = sc.fit_transform([train_data['Age']])","1d73c759":"[test_data['Age']] = sc.fit_transform([test_data['Age']])","490a793c":"train_data","d07bfef7":"[train_data['Pos_Feedback_Cnt']] = sc.fit_transform([train_data['Pos_Feedback_Cnt']])\n[test_data['Pos_Feedback_Cnt']] = sc.fit_transform([test_data['Pos_Feedback_Cnt']])","4ef1ac8e":"train_data['REVIEW'] = train_data['Review_Title'] + ' ' + train_data['Review']","4a114000":"test_data['REVIEW'] = test_data['Review_Title'] + ' ' + test_data['Review']","c4cef2ec":"train_data.drop(columns = ['Review_Title','Review'],inplace=True)\ntest_data.drop(columns = ['Review_Title','Review'],inplace=True)","f29bcf80":"train_data = pd.concat([train_data,pd.get_dummies(train_data['Division']),\n                       pd.get_dummies(train_data['Department']),\n                       pd.get_dummies(train_data['Product_Category'])],axis=1).drop(columns=['Division','Department','Product_Category'])","8ac261fd":"test_data = pd.concat([test_data,pd.get_dummies(test_data['Division']),\n                       pd.get_dummies(test_data['Department']),\n                       pd.get_dummies(test_data['Product_Category'])],axis=1).drop(columns=['Division','Department','Product_Category'])","0ad5e94c":"train_data.fillna('0',inplace=True)","eaa7a729":"test_data.fillna('0',inplace=True)","8634d636":"train_data","46cddeef":"train_sentences = []\nfor i in train_data.REVIEW.values:\n    train_sentences.append(i)","a9a2cc1a":"test_data['Casual bottoms'] = np.zeros(len(test_data))\ntest_data['Chemises'] = np.zeros(len(test_data))","e40b374c":"test_sentences = []\nfor i in test_data.REVIEW.values:\n    test_sentences.append(i)","d471882c":"import string\ntrain_sentences = [doc.lower() for doc in train_sentences]\ntest_sentences = [doc.lower() for doc in test_sentences]","ff89d461":"import nltk","c047e428":"from nltk.tokenize import word_tokenize\ntrain_sentences = [word_tokenize(doc) for doc in train_sentences]","f6b4c48e":"test_sentences = [word_tokenize(doc) for doc in test_sentences]","159b7ea8":"# Removing punctuation\nimport re\nregex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http:\/\/docs.python.org\/2\/library\/string.html\n\ntokenized_train_no_punctuation = []\n\nfor review in train_sentences:\n    new_review = []\n    for token in review:\n        new_token = regex.sub(u'', token)\n        if not new_token == u'':\n            new_review.append(new_token)\n    \n    tokenized_train_no_punctuation.append(new_review)","163b2e8d":"\ntokenized_test_no_punctuation = []\n\nfor review in test_sentences:\n    new_review = []\n    for token in review:\n        new_token = regex.sub(u'', token)\n        if not new_token == u'':\n            new_review.append(new_token)\n    \n    tokenized_test_no_punctuation.append(new_review)","7f4f4576":"# Cleaning text of stopwords\nfrom nltk.corpus import stopwords\n\ntokenized_train_no_stopwords = []\n\nfor doc in tokenized_train_no_punctuation:\n    new_term_vector = []\n    for word in doc:\n        if not word in stopwords.words('english'):\n            new_term_vector.append(word)\n    \n    tokenized_train_no_stopwords.append(new_term_vector)","6b43f660":"tokenized_test_no_stopwords = []\n\nfor doc in tokenized_test_no_punctuation:\n    new_term_vector = []\n    for word in doc:\n        if not word in stopwords.words('english'):\n            new_term_vector.append(word)\n    \n    tokenized_test_no_stopwords.append(new_term_vector)","a113456a":"# Stemming and Lemmatization\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nporter = PorterStemmer()\nwordnet = WordNetLemmatizer()\n\npreprocessed_train = []\n\nfor doc in tokenized_train_no_stopwords:\n    final_doc = []\n    for word in doc:\n        #final_doc.append(porter.stem(word))\n        final_doc.append(wordnet.lemmatize(word))\n    \n    preprocessed_train.append(final_doc)","e7d955f0":"preprocessed_test = []\n\nfor doc in tokenized_test_no_stopwords:\n    final_doc = []\n    for word in doc:\n        #final_doc.append(porter.stem(word))\n        final_doc.append(wordnet.lemmatize(word))\n    \n    preprocessed_test.append(final_doc)","1798e05f":"train_sentences = []\nfor i in preprocessed_train:\n    train_sentences.append(' '.join(i))","b1ab82e7":"test_sentences = []\nfor i in preprocessed_test:\n    test_sentences.append(' '.join(i))","b47351dd":"train_data['REVIEW'] = train_sentences","63468696":"test_data['REVIEW'] = test_sentences","26b96f49":"train_data","3024651e":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(max_features = 2000)\n\nvectorizer.fit(train_sentences)","6c9a2c4a":"print(vectorizer.vocabulary_)","b9ed3310":"train_vec = vectorizer.transform(train_sentences)","780de874":"train_vec","6e65f4ba":"test_vec = vectorizer.transform(test_sentences)","dd9889a1":"train_data","b13ae856":"from imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\nover = SMOTE(sampling_strategy=0.5,random_state=10) \nunder = RandomUnderSampler(sampling_strategy=0.5)\nsteps = [('u', under)]\npipeline = Pipeline(steps=steps)\n","bb50dad9":"train_data","c2188f7b":"Y_train = train_data[['Recommended']]\ntrain_Data = np.hstack((train_data.drop(columns=['Id','REVIEW','Rating','Recommended']),train_vec.toarray(),train_data[['Rating']]))","328d50c7":"set(train_data.drop(columns=['Id','REVIEW'])) - set(test_data.drop(columns=['Id','REVIEW']))","794898f0":"X,y = train_Data,Y_train","b1065574":"test_Data = np.hstack((test_data.drop(columns=['Id','REVIEW']),test_vec.toarray()))","b140407a":"test_Data.shape","1fa42089":"X.shape","037c5969":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X[:,:-1],X[:,-1])","64a856c5":"x_train.shape","c9f6ded7":"from sklearn.utils import class_weight\nfrom xgboost import XGBClassifier\n\nxgb_classifier = XGBClassifier()\nclasses_weights = class_weight.compute_sample_weight(\n    class_weight='balanced',\n    y=y_train\n)\n\nxgb_classifier.fit(x_train, y_train)","cf805196":"from sklearn.metrics import accuracy_score","abea3798":"y_hat_rating = xgb_classifier.predict(x_test)","2b07e5f1":"y_hat_rating","1ebe3869":"counts = 0\nfor i in range(len(y_hat_rating)):\n    if(y_test[i] == y_hat_rating[i]):\n        counts+=1","0e8bc093":"counts \/ len(y_hat_rating)","24bfdd09":"rating_tests = xgb_classifier.predict(test_Data)","aadbad3c":"nx_train,nx_test,ny_train,ny_test = train_test_split(np.hstack((X[:,:-1],xgb_classifier.predict(X[:,:-1]).reshape(len(X),1))),Y_train)","99d735ea":"ny_train","8ac6de29":"xgb_classifier = XGBClassifier()\n\nxgb_classifier.fit(nx_train, ny_train)","c90f2ba9":"accuracy_score(xgb_classifier.predict(nx_test),ny_test)","d36bc00d":"recom_tests = xgb_classifier.predict(np.hstack((test_Data,rating_tests.reshape(len(test_Data),1))))","3e15aae3":"test_data","61f5663c":"df = pd.DataFrame(np.hstack((test_data.Id.values.reshape(len(test_data),1),rating_tests.reshape(len(test_data),1),recom_tests.reshape(len(test_data),1))),columns=['Id','Rating','Recommended'])","989661f1":"df = df.astype('int64')\ndf.to_csv('latest_sub.csv',index=False)","939c0f7c":"X.shape","b86374cd":"from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\n\ninp1 = Input(shape=(2000,))\nemb = Embedding(len(vectorizer.vocabulary_)+1, 32, input_length=2000)(inp1)\nlstm = Bidirectional(LSTM(50))(emb)\nf1 = Flatten()(lstm)\n\n\n\n# merge = concatenate([f1, f2])\nhidden1 = Dense(10, activation='relu')(f1)\noutput = Dense(1, activation='sigmoid')(hidden1)\nmodel = Model(inputs=inp1, outputs=output)","f4bb453f":"train_vec.toarray().shape","57c49aac":"train_data.drop(columns=['Id','Rating','Recommended','REVIEW'])","7369d273":"from keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom keras.layers.merge import concatenate\n\ninp1 = Input(shape=(31,))\nd1 = Dense(15,activation = 'relu')(inp1)\nd2 = Dense(5,activation = 'relu')(d1)\n\n# emb = Embedding(len(vectorizer.vocabulary_)+1, 32, input_length=2000)(inp1)\n# lstm = Bidirectional(LSTM(50))(emb)\n# f1 = Flatten()(lstm)\n\n\n\n# merge = concatenate([f1, f2])\n# hidden1 = Dense(10, activation='relu')(f1)\noutput = Dense(1, activation='sigmoid')(d2)\nmodel = Model(inputs=inp1, outputs=output)","76cebd84":"inp1 = Input(shape=(2000,))\nemb = Embedding(len(vectorizer.vocabulary_)+1, 32, input_length=2000)(inp1)\nlstm = Bidirectional(LSTM(50))(emb)\nf1 = Flatten()(lstm)\n\n\ninp2 = Input(shape=(31,))\nd1 = Dense(15,activation = 'relu')(inp2)\nd2 = Dense(5,activation = 'relu')(d1)\nf2 = Flatten()(d2)\n\nmerge = concatenate([f1, f2])\nhidden1 = Dense(10, activation='relu')(merge)\noutput = Dense(1, activation='sigmoid')(hidden1)\nmodel = Model(inputs=[inp1,inp2], outputs=output)\n\n","f2f205ea":"model.summary()","7821b532":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy','AUC'])\nhistory = model.fit([train_vec.toarray(),train_data.drop(columns=['Id','Rating','Recommended','REVIEW'])]\n                    , np.asarray(Y_train).astype(np.float32).reshape(14091,1),batch_size=256, epochs=10,validation_split = 0.2)","1b9e7f8b":"## Text Cleaning"}}