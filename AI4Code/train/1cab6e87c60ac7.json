{"cell_type":{"3eab0359":"code","3118ce7f":"code","2d77913b":"code","09caf2db":"code","81a10bc2":"code","495a5db7":"code","2489b925":"code","7e0b844a":"code","6ab0071f":"code","0c4015ff":"code","9adc8788":"code","dc0c0ac0":"code","a45a846e":"code","54ba51e1":"code","91eb917a":"code","6740d7b7":"code","c9091edc":"code","19ea704a":"code","58ea3623":"code","23f1fd09":"code","84c19cec":"code","50e61641":"code","2e42e22e":"code","c3387b3d":"code","9c27cea7":"code","6cc7fa5f":"code","cc5a2476":"code","9bdd2052":"code","4a5b119b":"code","67d9d353":"code","6c06ae99":"code","e50f98d3":"code","0fe0fd7c":"code","5e2ce76d":"markdown","581df739":"markdown","0cd3952c":"markdown","180b1ced":"markdown","cdd0a31d":"markdown","27c4345e":"markdown","c266e271":"markdown","25aeab10":"markdown","9cd5cfc2":"markdown","d9d485b0":"markdown","f2cddaf9":"markdown","86460ac8":"markdown","0b5141ba":"markdown","c2b50b26":"markdown","a67239d0":"markdown","1f9b5cc8":"markdown","bbbd592e":"markdown","5cdf3f89":"markdown","6f09f784":"markdown","ec2f1ba5":"markdown","315d53fe":"markdown","8c5aa0e0":"markdown","dccfe62d":"markdown","c4b2f530":"markdown","383eb688":"markdown","3c348c3d":"markdown","d530cae9":"markdown","3effb707":"markdown","cd724cdc":"markdown"},"source":{"3eab0359":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3118ce7f":"import tensorflow as tf\nimport datetime\nimport matplotlib\nmatplotlib.use('Agg') # enable matplotlib to save .png to disk\nimport matplotlib.pyplot as plt\nfrom functools import partial\nimport os\nfrom sklearn.metrics import confusion_matrix, classification_report\n%matplotlib inline","2d77913b":"batch_size = 64\nepochs = 5\nregularizer = 1e-3\ntotal_train_samples = 60000\ntotal_test_samples = 10000\nlr_decay_epochs = 1\noutput_folder = '.\/model_output'\n\nif not os.path.exists(output_folder):\n    os.makedirs(output_folder)\n    \nsave_format=\"hdf5\" # or saved_model\n\nif save_format == \"hdf5\":\n    save_path_models = os.path.join(output_folder,\"hdf5_models\")\n    if not os.path.exists(save_path_models):\n        os.makedirs(save_path_models)\n    save_path = os.path.join(save_path_models,\"ckpt_epoch{epoch:02d}_val_acc{val_acc:.2f}.hdf5\")\nelif save_format == \"saved_model\":\n    save_path_models = os.path.join(output_folder,\"saved_models\")\n    if not os.path.exists(save_path_models):\n        os.makedirs(save_path_models)\n    save_path = os.path.join(save_path_models,\"ckpt_epoch{epoch:02d}_val_acc{val_acc:.2f}.ckpt\")\n    \n# To save logs\nlog_dir = os.path.join(output_folder,'logs_{}'.format(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")))\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)","09caf2db":"physical_devices = tf.config.experimental.list_physical_devices('GPU') # List all the visable GPU\nprint(\"All the available GPUs:\\n\",physical_devices)\nif physical_devices:\n    gpu=physical_devices[0] # show the first GPU\n    tf.config.experimental.set_memory_growth(gpu, True) # Increase the memory automatically if needed\n    tf.config.experimental.set_visible_devices(gpu, 'GPU') # Only choose the first GPU","81a10bc2":"train = pd.read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv\")","495a5db7":"train_x = train.drop(['label'], axis=1).values.reshape([-1, 28, 28])\ntrain_y = train.label\ntest_x = test.drop(['label'], axis=1).values.reshape([-1, 28, 28])\ntest_y = test.label\n\ntrain_x,test_x = train_x[...,np.newaxis]\/255.0,test_x[...,np.newaxis]\/255.0\ntotal_train_sample = train_x.shape[0]\ntotal_test_sample = test_x.shape[0]\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","2489b925":"train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\ntest_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n \ntrain_ds=train_ds.shuffle(buffer_size=batch_size*10).batch(batch_size).prefetch(buffer_size = tf.data.experimental.AUTOTUNE).repeat()\ntest_ds = test_ds.batch(batch_size).prefetch(buffer_size = tf.data.experimental.AUTOTUNE) # without repeat\uff0conly execute once","7e0b844a":"l2 = tf.keras.regularizers.l2(regularizer) # define the regularizer\nini = tf.keras.initializers.he_normal() # define the initializer of params\nconv2d = partial(tf.keras.layers.Conv2D,activation='relu',padding='same',kernel_regularizer=l2,bias_regularizer=l2)\nfc = partial(tf.keras.layers.Dense,activation='relu',kernel_regularizer=l2,bias_regularizer=l2)\nmaxpool = tf.keras.layers.MaxPooling2D\ndropout = tf.keras.layers.Dropout","6ab0071f":"x_input = tf.keras.layers.Input(shape=(28,28,1),name='input_node')\nx = conv2d(128,(5,5))(x_input)\nx = maxpool((2,2))(x)\nx = conv2d(256,(5,5))(x)\nx = maxpool((2,2))(x)\nx = tf.keras.layers.Flatten()(x)\nx = fc(128)(x)\nx_output = fc(10,activation=None,name='output_node')(x)\nmodel = tf.keras.models.Model(inputs=x_input,outputs=x_output)                ","0c4015ff":"print(\"The model architure:\\n\")\nprint(model.summary())","9adc8788":"tf.keras.utils.plot_model(model, to_file=os.path.join(log_dir,'model.png'), show_shapes=True, show_layer_names=True)","dc0c0ac0":"# set the learning decay rate, use the exponential decay\ntrain_steps_per_epoch = int(total_train_samples \/\/ batch_size)\ninitial_learning_rate = 0.01\n\n# optimizer\noptimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.95)\n\n# loss func\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# Evaluation factors\nmetrics = ['acc', 'sparse_categorical_crossentropy']","a45a846e":"model.compile(optimizer=optimizer, loss=loss, metrics=metrics)","54ba51e1":"# default save type: saved_model\nckpt = tf.keras.callbacks.ModelCheckpoint(save_path, monitor='val_acc', verbose=1,\n                                          save_best_only=False, save_weight_only=False,\n                                          save_frequency=1)","91eb917a":"# Stop training if the imporvement of acc is less than 0.01% for 5 epochs\nearlystop = tf.keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=5, verbose=True)","6740d7b7":"class LearningRateExponentialDecay:\n    def __init__(self, initial_learning_rate, decay_epochs, decay_rate):\n        self.initial_learning_rate = initial_learning_rate\n        self.decay_epochs = decay_epochs\n        self.decay_rate = decay_rate\n    def __call__(self, epoch):\n        dtype = type(self.initial_learning_rate)\n        decay_epochs = np.array(self.decay_epochs).astype(dtype)\n        decay_rate = np.array(self.decay_rate).astype(dtype)\n        epoch = np.array(epoch).astype(dtype)\n        \n        p = epoch \/ decay_epochs\n        lr = self.initial_learning_rate * np.power(decay_rate, p)\n        return lr\n\nlr_schedule = LearningRateExponentialDecay(initial_learning_rate, lr_decay_epochs, 0.96)\nlr = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)","c9091edc":"# Add the use of tensorboard\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir)","19ea704a":"terminate = tf.keras.callbacks.TerminateOnNaN()","58ea3623":"# reduce the lr, that needs to get greater change and longer monitoring period than auto lr decay\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=0.0001, min_lr=0)","23f1fd09":"csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(log_dir, 'logs.log'), separator=',')","84c19cec":"\"\"\" Please refer to the documents about how to use these params\"\"\"\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n                                             histogram_freq=1, # plot histogram of params and activations, must has test dataset\n                                             write_graph=True, # diagram of model architecture\n                                             write_images=True, # save model params by images\n                                             update_freq='epoch', # epoch\/batch\/integer higher value will lead to low speed\n                                             profile_batch=2, # record the performance of the model\n                                             embeddings_freq=1,\n                                             embeddings_metadata=None # not very clear about how to use this\n                                            )","50e61641":"file_writer_cm = tf.summary.create_file_writer(log_dir, filename_suffix='cm')\ndef plot_to_image(figure, log_dir, epoch):\n    \"\"\"Converts the matplotlib plot specified by 'figure' to a PNG image and\n    returns it. The supplied figure is closed and inaccessible after this call.\"\"\"\n    \n    # Save the plot to a PNG in memory\n    buf = io.BytesIO()\n    plt.savefig(buf, format='png')\n    fig = figure\n    fig.savefig(os.path.join(log_dir, 'during_train_confusion_epoch_{}.png'.format(epoch)))\n    # Closing the figure prevents it from being displayed directly inside the notebook\n    plt.close(figure)\n    buf.seek(0)\n    # Convert PNG buffer to TF image\n    image = tf.image.decode_png(buf.getvalue(), channels=4)\n    # Add the batch dimension\n    image = tf.expend_dims(image, 0)\n    return image\n\ndef plot_confusion_matrix(cm, class_names):\n    \"\"\"\n    Returns a matplotlib figure containing the plotted confusion matrix.\n\n    Args:\n    cm (array, shape = [n, n]): a confusion matrix of integer classes\n    class_names (array, shape = [n]): String names of the integer classes\n    \"\"\"\n    figure = plt.figure(figsize=(8,8))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix')\n    plt.colorbar\n    tick_marks = np.array(len(class_names))\n    plt.xticks(tick_marks, class_names, rotation=45)\n    plt.yticks(tick_marks, class_names)\n    \n    # Normalize the confusion matrix\n    cm = np.around(cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis], decimals=2)\n    \n    # Use white text if squares are dark, otherwise black\n    threshold = cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        color = 'white' if cm[i, j] > threshold else 'black'\n        plt.text(j, i, cm[i, j], horizontalalignment='center', color=color)\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return figure\n\ndef log_confusion_matrix(epoch, logs):\n    # Use the model to predict the values fro the validation dataset.\n    test_pred_raw = model.predict(test_x)\n    test_pred = mp.argmax(test_pred_raw, axis=1)\n    \n    # Calculate the confusion matrix.\n    cm = confusion_matrix(test_y, test_pred)\n    # Log the confusion matrix as an image summary.\n    figure = plot_confusion_matrix(cm, class_names=class_names)\n    cm_image = plot_to_image(figure, log_dir, epoch)\n    \n    # Log the confusion matrix as an image summary\n    with file_writer_cm.as_default():\n        tf.summary.image('Confusion Matrix', cm_image, step=epoch)\n        \n# Define the per-epoch callback\ncm_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_confusion_matrix)","2e42e22e":"callbacks = [ckpt, earlystop, lr, tensorboard, terminate, reduce_lr, csv_logger]","c3387b3d":"train_steps_per_epoch = np.floor(total_train_sample \/ batch_size).astype(np.int32)\ntest_steps_per_epoch = np.ceil(total_test_sample \/ batch_size).astype(np.int32)\nhistory = model.fit(train_ds, epochs=epochs,\n                    steps_per_epoch=train_steps_per_epoch,\n                    validation_data=test_ds,\n                    validation_steps=test_steps_per_epoch,\n                    callbacks=callbacks, verbose=1)","9c27cea7":"def plot(lrs, title='learing rate schedule'):\n    # calculate the lr according to epoch\n    epochs = np.arange(len(lrs))\n    plt.figure()\n    plt.plot(epochs, lrs)\n    plt.xticks(epochs)\n    plt.scatter(epochs, lrs)\n    plt.title(title)\n    plt.xlabel('Epoch #')\n    plt.ylabel('Learning Rate')\n\nplot(history.history['lr'])\nplt.savefig(os.path.join(log_dir, 'learning_rate.png'))","6cc7fa5f":"N=np.arange(epochs)\nplt.figure()\nplt.plot(N, history.history['loss'], label='train_loss')\nplt.scatter(N, history.history['loss'])\nplt.plot(N, history.history['val_loss'], label='val_loss')\nplt.scatter(N, history.history['val_loss'])\nplt.plot(N, history.history['acc'], label='train_acc')\nplt.scatter(N, history.history['acc'])\nplt.plot(N, history.history['val_acc'], label='val_acc')\nplt.scatter(N, history.history['val_acc'])\nplt.title('Training Loss and Accuracy on Our_dataset')\nplt.xlabel('Epoch #')\nplt.ylabel('Loss\/Accuracy')\nplt.legend()\nplt.savefig(os.path.join(log_dir,'training.png'))","cc5a2476":"model_json = model.to_json()\nwith open(os.path.join(log_dir, 'model_json.json'), 'w') as json_file:\n    json_file.write(model_json)","9bdd2052":"metrics = model.evaluate(test_ds, verbose=1)\nprint('val_loss:', metrics[0], 'val_acc:', metrics[1])","4a5b119b":"predictions = model.predict(test_ds,verbose=1)","67d9d353":"def print_metrics(labels, predictions, target_names, save=False, save_path=None):\n    # Calculate confusion result\n    preds = np.argmax(predictions, axis=1)\n    confusion_result = confusion_matrix(labels, preds)\n    pd.set_option('display.max_rows', 500)\n    pd.set_option('display.max_columns', 500)\n    pd.set_option('display.width', 1500)\n    confusion_result = pd.DataFrame(confusion_result, index=target_names, columns=target_names)\n    # classification results\n    report = classification_report(labels, preds, target_names=target_names, digits=4)\n    result_report = 'Confise_matrix:\\n{}\\n\\nClassification_report:\\n{}\\n'.format(confusion_result, report)\n    print(result_report)\n    if save:\n        savepath = os.path.join(save_path, 'predited_result.txt')\n        print('The result saved in %s' % savepath)\n        \n        with open(savepath, 'w') as f:\n            f.write(result_report)","6c06ae99":"print_metrics(test_y, predictions, class_names, True, log_dir)","e50f98d3":"for dirname, _, filenames in os.walk('\/kaggle\/working\/model_output\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0fe0fd7c":"# %%bash\n# tree model_output","5e2ce76d":"**Define the optimizer and loss func**","581df739":"**Define the model by functional method**","0cd3952c":"**Print the model's architecture**","180b1ced":"# Train the model","cdd0a31d":"**Set Hyperparams**","27c4345e":"# Plot lr changing curve and save it to log","c266e271":"**Print and save the model architecture**","25aeab10":"**Prepare the dataset**","9cd5cfc2":"**Make train dataset and test dataset by tf.data**","d9d485b0":"profile_batch=2 will use the second iteration to analyze the model, so there will be a warning at the beginning of the training.","f2cddaf9":"# Save Model Architecture and Params","86460ac8":"* Stop training when nan or inf occurs in loss","0b5141ba":"# The first way is to use the fit func in keras, here make examples by fashin mnist\nThis artical includes a total process to build a deep learning model architecture. \n* Contains tf.data to read datasets.\n* Use keras callback to realize checkpoint, and decay the learning rate, early stop func, etc.\n* Finally, cooperate with scikit-learn, make a specification of the model's process of calculation.","c2b50b26":"* Summarize all the callbacks","a67239d0":"* Save all the Scalar indicators during training.","1f9b5cc8":"# Test model on test dataset","bbbd592e":"* Update callback : update tensorboard","5cdf3f89":"# Plot val loss and acc curve","6f09f784":"* Learning rate decay, and save the change of lr","ec2f1ba5":"* When the model loss keep stable for several epochs, sharply reduce the lr\n* this strategy is not usually used with lr decay schedule","315d53fe":"# Show the file structure","8c5aa0e0":"* Use tensorboard","dccfe62d":"**Compile the model**","c4b2f530":"* draw confusion matrix to log and tensorboard after every test","383eb688":"# Reasoning on the model and analyzing the reasoning results","3c348c3d":"**Prepare to define the model**","d530cae9":"**Define callbacks**\n* generalize ckpt, save a model after epoch","3effb707":"* Stop training while the acc is not increasing during a long period","cd724cdc":"**Select GPU and auto allocate memory**"}}