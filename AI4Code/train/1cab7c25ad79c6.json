{"cell_type":{"a3c3fbfa":"code","438ed756":"code","09404692":"code","d9f4d3f8":"code","6905802c":"code","1249f0e4":"code","fdfa4575":"code","46b29f5b":"markdown","ecc48a97":"markdown","1d145497":"markdown","bba19956":"markdown"},"source":{"a3c3fbfa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","438ed756":"# For ordinal encoding categorical variables or one_hot, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# For training XG Boost model\nfrom xgboost import XGBRegressor\n\n# For validation\nfrom sklearn.metrics import mean_squared_error\n\n# For Pipeline\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.compose import ColumnTransformer\n\n","09404692":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\nX = train.drop(['target'],axis = 1) \ny = train['target']\n# split\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,train_size=0.8, test_size=0.2,random_state = 0)\n# preview\nX_train.head()","d9f4d3f8":"categorical_cols = [col for col in X_train.columns if 'cat' in col]\n\n# ues one_hot\ncategorical_transformer1 = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for  categorical data\npreprocessor_one = ColumnTransformer(\n    transformers=[\n        ('cat1', categorical_transformer1, categorical_cols)\n    ])\n\nprint('Complete')","6905802c":"model = XGBRegressor(n_estimators=1000, learning_rate=0.01, random_state = 0,tree_method = 'gpu_hist')\n\n# Bundle preprocessing of one_hot and modeling code in a pipeline\nmy_pipeline_one = Pipeline(steps=[('preprocessor1', preprocessor_one),\n                              ('model1', model)\n                             ])\n\nprint('Complete')","1249f0e4":"# Preprocessing of training data, fit model \nmy_pipeline_one.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds_one = my_pipeline_one.predict(X_valid)\n\n# Evaluate\nprint('result' , mean_squared_error(y_valid, preds_one, squared=False))\n\n","fdfa4575":"predictions = my_pipeline_one.predict(test)\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","46b29f5b":"# Step 2: load data and Split","ecc48a97":"# Step 1: Import helpful libraries","1d145497":"# Step 4: Build XGBoost  ","bba19956":"# Step 3: Use Pipeline for Preprocessing"}}