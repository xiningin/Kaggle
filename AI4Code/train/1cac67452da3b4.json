{"cell_type":{"e892097f":"code","78a99d32":"code","911cae0e":"code","852fb907":"code","93d6a68b":"code","6c78d27e":"code","936d3891":"code","977bff6d":"code","9a7a2d0e":"code","74130714":"code","189f11cc":"code","14b61100":"code","76904b21":"code","89581189":"code","b8904836":"code","ca54231a":"code","fde164cd":"code","bac545a9":"code","5c7632e7":"code","0bf80677":"code","de82d4e8":"code","0b74f091":"code","a735abbd":"code","9c9d8c86":"code","6bb8c305":"code","56f62ed3":"code","63e2d296":"code","b3a9c94e":"code","8963ed82":"code","1e4b5f51":"code","b421c93e":"code","d738575c":"code","e0272508":"code","8199a544":"code","c4405a6c":"code","cf20e835":"code","ae733c46":"code","c370c0d3":"code","db086f83":"code","47e5a26f":"code","5e5e9d5d":"code","381580a7":"code","595f091f":"code","e6e7c068":"code","361c26b7":"code","6dcdf081":"code","3e1a7a88":"code","b2f8f8fa":"code","a70e963b":"code","212ff377":"code","8a993fd6":"code","751e5c09":"code","5137cff1":"code","2fb0fe3e":"code","52b34f69":"code","99b37802":"code","6acd194b":"code","0c08a386":"code","76928fa9":"code","642e0ddf":"code","71cdff32":"code","27e2a5ad":"code","3a0d423e":"code","9c048a11":"code","9fad9641":"code","f071f84b":"code","375f2e60":"code","973f846b":"markdown","2a071766":"markdown","cb3d1854":"markdown","d74247b6":"markdown","0f5d7412":"markdown","e3572a2c":"markdown","727ec882":"markdown","0b44ceb2":"markdown","172236d2":"markdown","037334c7":"markdown","0c2e444a":"markdown","c6ec5826":"markdown","66fe2c84":"markdown","0b9a978c":"markdown","4e578398":"markdown","3ebb6b88":"markdown","1d008f36":"markdown","8c82e0b2":"markdown","20fe1723":"markdown","0c944c71":"markdown","97ae9164":"markdown","eb8516f1":"markdown"},"source":{"e892097f":"import os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","78a99d32":"import pandas as pd\nsms_spam_collection = pd.read_csv('..\/input\/SMSSpamCollection', sep = '\\t', header=None, names = ['Label', 'SMS'])","911cae0e":"sms_spam_collection.head()","852fb907":"sms_spam_collection.shape","93d6a68b":"sms_spam_collection.tail()","6c78d27e":"sms_spam_collection['Label'].value_counts(normalize=True)","936d3891":"sms_data_randomized = sms_spam_collection.sample(frac=1, random_state=1)","977bff6d":"training_data_length = round(len(sms_spam_collection) * 0.80)","9a7a2d0e":"training_data_length","74130714":"training_sms_dataset = sms_data_randomized[:training_data_length]","189f11cc":"training_sms_dataset.shape","14b61100":"training_sms_dataset.head()","76904b21":"training_sms_dataset.reset_index(drop = True, inplace = True)","89581189":"training_sms_dataset.head()","b8904836":"testing_sms_dataset = sms_data_randomized[training_data_length:].reset_index(drop = True)","ca54231a":"testing_sms_dataset.shape","fde164cd":"testing_sms_dataset.head()","bac545a9":"testing_sms_dataset['Label'].value_counts(normalize = True)","5c7632e7":"training_sms_dataset['Label'].value_counts(normalize = True)","0bf80677":"training_sms_dataset['SMS'].head() ","de82d4e8":"training_sms_dataset['SMS'] = training_sms_dataset['SMS'].str.replace('\\W', ' ').str.lower()","0b74f091":"training_sms_dataset.head()","a735abbd":"test_training_dataset = training_sms_dataset.iloc[0:10]\n# Before we begin working with the actual dataset, it is always recommended to first test out your plan of action. \n# We have decided to build a custom function that will take in the series as input and return a set of all unique words in spam and ham messages.\n# Thus building our vocabulary.","9c9d8c86":"test_training_dataset","6bb8c305":"def creating_vocab(series):\n  vocab = []\n  for x in series:\n    x_list = x.split()\n    for i in x_list: \n      vocab.append(i)\n  return list(set(vocab))\n\n# The reason we are converting it back to a list is so that we can use that list as column labels later on. ","56f62ed3":"test_vocab = creating_vocab(test_training_dataset['SMS'].iloc[0:2])","63e2d296":"test_vocab","b3a9c94e":"training_sms_dataset['SMS'] = training_sms_dataset['SMS'].str.split()\n\nvocabulary = []\nfor sms in training_sms_dataset['SMS']:\n    for word in sms:\n        vocabulary.append(word)\n        \nvocabulary = list(set(vocabulary))","8963ed82":"len(vocabulary)","1e4b5f51":"word_counts_per_sms = {unique_word: [0] * len(training_sms_dataset['SMS']) for unique_word in vocabulary}\n\nfor index, sms in enumerate(training_sms_dataset['SMS']):\n    for word in sms:\n        word_counts_per_sms[word][index] += 1","b421c93e":"words_df = pd.DataFrame(word_counts_per_sms)","d738575c":"words_df.head()","e0272508":"training_dataset_joined = pd.concat([training_sms_dataset,words_df], axis = 1)","8199a544":"training_dataset_joined.head()","c4405a6c":"training_dataset_joined['welp'].head()","cf20e835":"training_dataset_joined['all'].head()","ae733c46":"training_dataset_joined['Label'].value_counts(normalize = True)","c370c0d3":"p_ham = 0.86541\np_spam = 0.13459","db086f83":"spam = training_dataset_joined[training_dataset_joined['Label'] == 'spam']","47e5a26f":"spam.head()","5e5e9d5d":"ham = training_dataset_joined[training_dataset_joined['Label'] == 'ham']","381580a7":"ham.head()","595f091f":"#calculating Nspam\nspam_words = [len(x) for x in spam['SMS'] ]","e6e7c068":"n_spam = sum(spam_words)","361c26b7":"n_spam","6dcdf081":"#calculating Nham\nham_words = [len(x) for x in ham['SMS']]","3e1a7a88":"n_ham = sum(ham_words)","b2f8f8fa":"n_ham","a70e963b":"#calculating Nvocab\nn_vocab = len(vocabulary)\nn_vocab","212ff377":"#introduce alpha for laplace smoothing \nalpha = 1 ","8a993fd6":"spam_words_prob = {} \nham_words_prob = {}","751e5c09":"for word in vocabulary: \n  spam_words_prob[word] = 0 \n  ham_words_prob[word] = 0 ","5137cff1":"len(spam_words_prob.keys())","2fb0fe3e":"len(ham_words_prob.keys())","52b34f69":"ham.shape[0]","99b37802":"spam.shape[0]","6acd194b":"#calculating P(Wi | ham)\nfor word in vocabulary:\n    numerator = ham[word].sum() + alpha\n    denominator = n_ham + (n_vocab * alpha)\n    prob = numerator \/ denominator\n    ham_words_prob[word] = prob\n    \n     \n    \n    ","0c08a386":"#calculating P(Wi | spam)\nfor word in vocabulary: \n  numerator = spam[word].sum() + alpha\n  denominator = n_spam + (n_vocab * alpha)\n  prob = numerator \/ denominator\n  spam_words_prob[word] = prob\n","76928fa9":"def classify_message(sms): \n  #the first step is to clean the data like we performed earlier. \n  import re \n  sms_message = re.sub('\\W', ' ', sms)\n  sms_message = sms_message.lower()\n  sms_message = sms_message.split()\n\n  #remember the formula. P(spam | wn) = P(spam) * (W1|spam).....(Wn|spam) where n is the number of words in the sms.\n  #also remember the constants we calculated. Since p(spam) and p(ham) are both constants that are a part of the formula, let's assign to the formula first.\n  p_ham_given_message = p_ham\n  p_spam_given_message = p_spam\n\n  #now let's start calculating the probability for each word in the sms.\n\n  for word in sms_message: \n    if word in ham_words_prob:\n      p_ham_given_message *= ham_words_prob[word]\n\n    if word in spam_words_prob: \n      p_spam_given_message *= spam_words_prob[word]\n\n    \n  print(\"P(ham | new message) :\", p_ham_given_message)\n  print(\"P(spam | new message) :\", p_spam_given_message)\n\n  if p_ham_given_message > p_spam_given_message: \n      print('Label : Ham. This is a non spam sms.')\n  elif p_spam_given_message > p_ham_given_message: \n      print('Label : Spam. This is a spam sms')\n  else: \n      print('We need a humam to classify this message due to equal probabilities.')\n    \n    \n\n","642e0ddf":"#Let's test out our function\nclassify_message('WINNER!! This is the secret code to unlock the money: C3421.')","71cdff32":"classify_message(\"Sounds good, Tom, then see u there\")","27e2a5ad":"#let's make changes to our function so this time it outputs a label instead of printing it out. \ndef classify_message(sms): \n  #the first step is to clean the data like we performed earlier. \n  import re \n  sms_message = re.sub('\\W', ' ', sms)\n  sms_message = sms_message.lower()\n  sms_message = sms_message.split()\n\n  #remember the formula. P(spam | wn) = P(spam) * (W1|spam).....(Wn|spam) where n is the number of words in the sms.\n  #also remember the constants we calculated. Since p(spam) and p(ham) are both constants that are a part of the formula, let's assign to the formula first.\n  p_ham_given_message = p_ham\n  p_spam_given_message = p_spam\n\n  #now let's start calculating the probability for each word in the sms.\n\n  for word in sms_message: \n    if word in ham_words_prob:\n      p_ham_given_message *= ham_words_prob[word]\n\n    if word in spam_words_prob: \n      p_spam_given_message *= spam_words_prob[word]\n\n    \n  if p_ham_given_message > p_spam_given_message: \n      return 'ham'\n  elif p_spam_given_message > p_ham_given_message: \n      return 'spam'\n  else: \n      return 'hcr'\n#hcr stands for human classification required","3a0d423e":"testing_sms_dataset.head()","9c048a11":"testing_sms_dataset['Generated_Label'] = testing_sms_dataset['SMS'].apply(classify_message)","9fad9641":"testing_sms_dataset.head()","f071f84b":"#let us now test the accuracy of our model. \n correct_labels = 0\n total_labels = testing_sms_dataset.shape[0]\n for x in testing_sms_dataset.itertuples():\n   if x[1] == x[3]: \n     correct_labels += 1 \n\n\n","375f2e60":"accuracy = (correct_labels \/ total_labels) * 100  \nprint(\"The accuracy our filter is :\", accuracy)  ","973f846b":"We have pretty much calculated the constants. You might be wondering why is p(spam) and P(ham) constant. Remember one of our objectives. We train the computer to classify. We train it based on the training dataset. So now that a new message comes in, the knowledge gained from the training is used. The new message is NOT ADDED to the training dataset. Hence the values remain constant. \n\nWe will now look at calculations for P(Wi | spam) and P(Wi | ham). It is important to take a moment and understand a few things here especially as to why we perfomed all the above and below tasks beforehand.. Let' say you recieved a message \"Secret santa is coming!\". Imagine if you didnt have an algorithm in place. You would have to calculate the probability of P('secret' | spam) P(\n  secret' | ham) and so on for every word right at the moment. Now imagine having 1,000,000 sms in the pipeline waiting to be classified. \n\nWhat we rather do is calculate the probability of each unqiue word given spam and ham in our training dataset beforehand. Then a new message comes in, which is 'Secret santa is coming!'. We pick up the word secret in the message and realise we already have the probability for secret | spam and secret | ham calculated. See how much faster now the process goes given we have some values calculated beforehand. \n\nThe only catch here is that \"as long as words in the new message are in the training dataset\". All of the above logic only holds if we don't make any changes to the training dataset. ","2a071766":"The next step is to create a vocabulary. As mentioned earlier, the vocabulary is the set of unique words in both spam and ham (non spam) messages. To understand more simply, we are trying to calculate every value that our formula (mentioned above) will need. ","cb3d1854":"That was fun!. It might be confusing at first. Take your time and carefully calculate each value. Name your variables accurately so you can call them in functions or the formula easier. \n\nThe hard part is almost done. Now all the remains is the filter. In essence, the filter will be a function that will take in the message as an argument and return whether the messge is spam or not based on probability calculations.\n\nKeep in mind we have only calculated the probability of the words given they are in spam or ham messages. We still have to calculate the P(spam | new message) and P(ham | new message). The multinomial bayes will be calculated in our function. \n\nThat being said, take a break. Stretch out your limbs. Drink a hot cup of coffee and then let's get back to coding !","d74247b6":"Not particularly relevant, but SMS at index 1 needs a lot more explaining ! ","0f5d7412":"An important point to note here and you might have wondered, why didn't we just take 4458 random samples from the dataset using dataframe.sample() or why did we use the training length as a starting point for our testing dataset. \n\nThe answer lies in one key detail. We are sampling without replacement. In simple words if an sms is in the training dataset, it cannot be in the testing dataset. Remember our presumption that we will be treating the testing dataset as entirely new messages.","e3572a2c":"We can safely say that our datasets are representative of the population. The two variables that we have are nominal type variables. If they were ratio or ordinal type variables, we could also used other ways to calculate representation. ","727ec882":"Before we begin though, we need to perform a few data cleaning steps. \n- We will be changing all the cases to lowercase, so that 'secret' and 'SECRET' are not counted separately. \n- Second remove any unnecessary punctuation or question marks etc.  ","0b44ceb2":"**We are hoping to reach an 80% accuracy when it comes to classification by our filter.**\n\nLet's begin dividing the dataset into training and testing sets. First we need to randomize it. ","172236d2":"Let's get back to calculating the probability of words. ","037334c7":"Let us now check if the percentages of spam and non spam in our testing and training datasets are similar to the original dataset. ","0c2e444a":"The final result should be a dataframe where the columns are all the unqiue words and their values are the frequency or the count of each of the unique word in our sms. So let's begin. The plan of action here is to : \n- Create a dictionary where the key is the unique value in vocab. \n- The key's value is the count of the word in each sms.\n- Loop over the sms column and populate the dictionary.  ","c6ec5826":"**Ham here means non spam**","66fe2c84":"Now that we have cleaned and formatted the data, let us move towards calculations. Let's recap on the formulas from earlier. \n  \n![alt text](https:\/\/render.githubusercontent.com\/render\/math?math=P%28Spam%20%7C%20w_1%2Cw_2%2C%20...%2C%20w_n%29%20%5Cpropto%20P%28Spam%29%20%5Ccdot%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7CSpam%29&mode=display)\n\n![alt text](https:\/\/render.githubusercontent.com\/render\/math?math=P%28Ham%20%7C%20w_1%2Cw_2%2C%20...%2C%20w_n%29%20%5Cpropto%20P%28Ham%29%20%5Ccdot%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7CHam%29&mode=display)\n\n![alt text](https:\/\/render.githubusercontent.com\/render\/math?math=P%28w_i%7CSpam%29%20%3D%20%5Cfrac%7BN_%7Bw_i%7CSpam%7D%20%2B%20%5Calpha%7D%7BN_%7BSpam%7D%20%2B%20%5Calpha%20%5Ccdot%20N_%7BVocabulary%7D%7D&mode=display)\n\n  ![alt text](https:\/\/render.githubusercontent.com\/render\/math?math=P%28w_i%7CHam%29%20%3D%20%5Cfrac%7BN_%7Bw_i%7CHam%7D%20%2B%20%5Calpha%7D%7BN_%7BHam%7D%20%2B%20%5Calpha%20%5Ccdot%20N_%7BVocabulary%7D%7D&mode=display)\n\nLet's begin by calculating the individual values and then plug them into the formula. We will calculate \n\n- P(spam) \n- P(ham) \n- Nspam\n- Nvocabulary ","0b9a978c":"<h2> Building a spam filter for sms using Multinomial Naive Bayes Algorithm : \n","4e578398":"What we did was an inefficient way of coding. What we should rather have done was method chaining. Let's see it in example on the testing dataset. ","3ebb6b88":"We see a slight issue with the indexes our training dataset. Let's fix that.","1d008f36":"In this notebook we will be creating a spam filter for SMS. We will be using Multinomial Naibe Bayes algorithm which relies on the principals of the Bayes Theorem.\n\nCreating a filter is based on three key concepts : \n\n1.  Human classification of spam and non spam messages. \n2.  The computer then uses that human knowledge to estimate probabilities for new messages i.e. P(spam | new message) and P(non spam | new message).\n3.  Based on the probability calculations, classifies it as spam or non spam. In simple words, if the probability of P(spam | new message) is higher than message is marked as spam and vice versa.\n\n*If the probabilities are equal, then we may need a human to classify the message.*\n\n---\n\n\nSo our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n\nThe dataset was put together by Tiago A. Almeida and Jos\u00e9 Mar\u00eda G\u00f3mez Hidalgo, and it can be downloaded from the [The UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/sms+spam+collection). You can also download the dataset directly from this [link](https:\/\/dq-content.s3.amazonaws.com\/433\/SMSSpamCollection).\n","8c82e0b2":"Before we begin designing out software i.e our spam filter, it is important we create a test first. If we create the software first, then it is tempting to come up with a biased test just to make sure the software passes it. \n\nWe can break up our dataset into two parts ; a training set and a testing set. We will use the training set (which can classify as 80% of the data) to train the computer to classify the messages. We will use the testing set to test how good the filter is at classifying new mesages. \n\nTo better understand the reasoning think of it this way. The testing set we keep aside. Keep in mind that the testing set has already been classified by a human. (therefore we have a reference point). Once the filter is created, we treat the testing set as new messages and have our filter classify them. Then we can compare the filtered results by the computer against the filtered results by a human to gauge how effective our filter is. ","20fe1723":"Now that we have our training and testing datasets, it is time to train the computer to classify spam and non spam messages. As we stated earlier our algorithm is based on Naive Bayes. It is called Naive because it works on the assumption on conditional independence.\n\nThe formula that we will be using to calculate the probabilities is :\n\n---\n\n\n\n**P(spam | w1 w2 w3...wn) = P(spam) * P(w1 | spam) * P( w2 | spam)......P(wn | spam)**\n\n**P(non spam | w1 w2 w3...wn) = P(non spam) * P(w1 | non spam) * P( w2 | non spam)......P(wn | non spam)**\n\n\n---\nTo calculate P(W n | spam) or P(W n | non spam) we will be using the formula below. We will be working on the premise of additive smoothing and will be using Laplace smoothing (where alpha = 1). You can read more on additive smoothing but in a nutshell additive smoothing is used to counter 0 values for word probabilities.  \n\n\n---\nP(W n | spam) = ( N wn |spam * alpha) \/ (N spam) * (N vocab * alpha)\n\n- N wn =  The number of times the word occurs in the spam message \n- alpha = 1 (smoothing parameter)\n- N spam = The total number of words in the spam messages \n- N vocab = The total number of words in the vocabulary\n\nA similar formula can be built for the P( Wn | non spam). You might be wondering formulas are either not correct or going against probability rules. Despite it being correct that some probability rules are broken, we have to keep in mind the objective. The objective of the filter is to not calculate probabilities but rather use probability calculations to classify between spam and non spam. Even though we haven't used the exact bayes theorem (denominator missing), our probability calculations will still be enough to differentiate. \n\n\n\n\n\n\n\n\n\n\n","0c944c71":"You would see above that we used a different approach to get the vocabulary. We show cased two different approaches here to show that there are multiple options to complete a task. The option you chose is entirely up to you as long as the results are accurate. \n\n---\n\n\nNow that we built our vocabulary, out next task is to populate the frequency of the words that occur in each of our sms. For example if our sms is \"Hello There\" and \"Hello Tom\", then the result should be\n\n       Label | 'Hello'  | 'There' | 'Tom\"\n    0  spam       1          1        0\n    1  ham        1          0        1\n       ","97ae9164":"Now that our filter is ready, let's use it to test our testing dataset that we created earlier. Remember we have not used any values from the testing dataset, hence each message in the testing dataset can be classified as a new message. \n\nThe idea here is to create a column in our testing dataset that has the labels from our function. We can then compare the actual labels vs generated labels to measure accuracy.","eb8516f1":"**--------------------------------------------------------------------------------------------------------------------------------------------------\nI appreciate the time you have taken to read this notebook. This is but a learner's attempt to step into the vast and beautiful field that is Data Science. I would highly appreciate any and all feedback on this notebook whether it be my fellow learners, my seniors or industry experts. **"}}