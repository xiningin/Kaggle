{"cell_type":{"cf6f7006":"code","31330665":"code","fc5db241":"code","c79cbe24":"code","1a21891a":"code","63780820":"code","bfe69982":"code","aac48995":"code","0b543f8d":"code","4974ca6e":"code","d0d8f019":"code","2e31fec6":"code","e0d45f66":"code","c68b53a7":"code","2cc79e9d":"code","b2b92c41":"code","b6b0a648":"code","71853286":"code","e59f7a8b":"code","7c33dd17":"code","a82e5625":"code","15e28510":"code","e103a072":"markdown"},"source":{"cf6f7006":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31330665":"import numpy as np\nimport pandas as pd\nimport math\nimport sklearn\nimport sklearn.preprocessing\nimport datetime\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport math\nfrom math import sqrt\nfrom sklearn.metrics import mean_squared_error","fc5db241":"price_adj_df = pd.read_csv(\"\/kaggle\/input\/nyse\/prices-split-adjusted.csv\", index_col=0)\nprice_adj_df","c79cbe24":"price_adj_df.head()","1a21891a":"price_adj_df.info()","63780820":"price_adj_df.describe()","bfe69982":"price_adj_df.head()","aac48995":"# visualize\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\nplt.plot(price_adj_df[price_adj_df.symbol == \"AAPL\"].open.values, color='red', label='open')\nplt.plot(price_adj_df[price_adj_df.symbol == \"AAPL\"].close.values, color='blue', label='close')\nplt.plot(price_adj_df[price_adj_df.symbol == \"AAPL\"].low.values, color='yellow', label='low')\nplt.plot(price_adj_df[price_adj_df.symbol == \"AAPL\"].high.values, color='green', label='high')\nplt.title('stock price of Apple')\nplt.xlabel('time [days]')\nplt.ylabel('price')\nplt.legend()\n\n#plt.subplot(1, 2, 2)\n#plt.plot()\n#plt.title('stock volume')","0b543f8d":"# function to create train, validation, test data given stck data and sequence length\ndef load_data(stock, seq_len):\n    data_raw = stock.to_numpy() # convert to numpy array\n    data = []\n    \n    # create all possible sequences of length seq_len\n    for index in range(len(data_raw) - seq_len):\n        data.append(data_raw[index: index + seq_len])\n        \n    data = np.array(data)\n    val_size = int(np.round(0.1*data.shape[0]))\n    test_size = int(np.round(0.2*data.shape[0]))\n    train_size = int(np.round(0.8*data.shape[0]))\n    \n    x_train = np.asarray(data[:train_size, :-1, :]).astype(np.float32)\n    y_train = np.asarray(data[:train_size, -1, :]).astype(np.float32)\n    \n    x_valid = np.asarray(data[train_size:train_size+val_size, :-1, :]).astype(np.float32)\n    y_valid = np.asarray(data[train_size:train_size+val_size, -1, :]).astype(np.float32)\n    \n    x_test = np.asarray(data[train_size+val_size:, :-1, :]).astype(np.float32)\n    y_test = np.asarray(data[train_size+val_size:, -1, :]).astype(np.float32)\n    \n    return (x_train, y_train, x_valid, y_valid, x_test, y_test)","4974ca6e":"# choose one stock\naapl_stock = price_adj_df[price_adj_df.symbol == 'AAPL'].copy()\naapl_stock.drop(['symbol'], axis=1, inplace=True)\naapl_stock.drop(['volume'], axis=1, inplace=True)\ncols = aapl_stock.columns.values\nprint(cols)","d0d8f019":"# normalize stock\naapl_stock_norm = aapl_stock.copy()\nmin_max_scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1))\naapl_stock_norm['open'] = min_max_scaler.fit_transform(aapl_stock_norm.open.values.reshape(-1, 1))\naapl_stock_norm['close'] = min_max_scaler.fit_transform(aapl_stock_norm.close.values.reshape(-1, 1))\naapl_stock_norm['high'] = min_max_scaler.fit_transform(aapl_stock_norm.high.values.reshape(-1, 1))\naapl_stock_norm['low'] = min_max_scaler.fit_transform(aapl_stock_norm.low.values.reshape(-1, 1))\n","2e31fec6":"# create train, test data\nseq_len = 15 # choose sequence length\nx_train, y_train, x_valid, y_valid, x_test, y_test = load_data(aapl_stock_norm, seq_len)\nprint('x_train.shape = ',x_train.shape)\nprint('y_train.shape = ', y_train.shape)\nprint('x_valid.shape = ',x_valid.shape)\nprint('y_valid.shape = ', y_valid.shape)\nprint('x_test.shape = ', x_test.shape)\nprint('y_test.shape = ',y_test.shape)","e0d45f66":"plt.figure(figsize=(15, 5))\nplt.plot(aapl_stock_norm.open.values, color='red', label='open')\nplt.plot(aapl_stock_norm.close.values, color='blue', label='close')\nplt.legend()\nplt.title('aapl stock')\nplt.ylabel('normalized price\/volume')\nplt.xlabel('time [days]')","c68b53a7":"# Basic Cell RNN in keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\n# build model\nrnn_model = keras.Sequential()\n\nrnn_model.add(layers.SimpleRNN(32, input_shape=(14, 4), return_sequences=True))\nrnn_model.add(layers.SimpleRNN(32, return_sequences=False))\nrnn_model.add(layers.Dense(4))\n#rnn_model.add(layers.Activation('softmax'))\n\nrnn_model.summary()","2cc79e9d":"adam = optimizers.Adam(lr = 0.001)\nrnn_model.compile(loss = 'mean_squared_error', optimizer=adam,\n             metrics = ['accuracy'])","b2b92c41":"# train network\nrnn_model.fit(x_train, y_train, batch_size = 32,\n         epochs=20, validation_data=(x_valid, y_valid))","b6b0a648":"# predict and plot\nrnn_preds = rnn_model.predict(x_test)\ncol_lst = list(aapl_stock_norm.columns)\nfigure, axes = plt.subplots(4,1,figsize=(10, 15))\n\nfor (i, col_name) in enumerate(col_lst):\n    rnn_preds_col = min_max_scaler.inverse_transform(rnn_preds[:,i].reshape(-1,1))\n    Ytest = min_max_scaler.inverse_transform(y_test[:,i].reshape(-1,1))\n\n    testScore = math.sqrt(mean_squared_error(Ytest, rnn_preds_col))\n    print(col_name, ' Test Score: %.2f RMSE' % (testScore))\n\n    data = aapl_stock_norm[col_name].values.reshape(-1,1)\n    data = np.reshape(data, (data.shape[0], 1))\n    testPredictPlot = np.empty_like(data)\n    testPredictPlot[:] = np.nan\n    testPredictPlot[len(aapl_stock_norm[col_name])-len(rnn_preds_col)-1:len(aapl_stock_norm['close'])-1] = rnn_preds_col\n    \n    axes[i].plot(min_max_scaler.inverse_transform(aapl_stock_norm[col_name].values.reshape(-1,1)))\n    axes[i].plot(testPredictPlot)\n    axes[i].set_title(col_name +' Prediction')\n    plt.tight_layout()\n    #plt.plot(rnn_preds)","71853286":"# LSTM in keras\nlstm_model = keras.Sequential()\nlstm_model.add(layers.LSTM(200, input_shape=(14, 4), return_sequences=False))\n#lstm_model.add(layers.LSTM(80))\nlstm_model.add(layers.Dense(4))\nlstm_model.summary()\n\nadam = optimizers.Adam(lr = 0.001)\nlstm_model.compile(loss='mean_squared_error', optimizer=adam,\n                  metrics=[\"accuracy\"])\n\nlstm_model.fit(x_train, y_train, epochs=30,\n              batch_size=32, verbose=1,\n              validation_data=(x_valid, y_valid))","e59f7a8b":"# predict and plot\nlstm_preds = lstm_model.predict(x_test)\n\ncol_lst = list(aapl_stock_norm.columns)\nfigure, axes = plt.subplots(4,1,figsize=(10, 15))\n\nfor (i, col_name) in enumerate(col_lst):\n    lstm_preds_col = min_max_scaler.inverse_transform(lstm_preds[:,i].reshape(-1,1))\n    Ytest = min_max_scaler.inverse_transform(y_test[:,i].reshape(-1,1))\n\n    testScore = math.sqrt(mean_squared_error(Ytest, lstm_preds_col))\n    print(col_name, ' Test Score: %.2f RMSE' % (testScore))\n\n    data = aapl_stock_norm[col_name].values.reshape(-1,1)\n    data = np.reshape(data, (data.shape[0], 1))\n    testPredictPlot = np.empty_like(data)\n    testPredictPlot[:] = np.nan\n    testPredictPlot[len(aapl_stock_norm[col_name])-len(lstm_preds_col)-1:len(aapl_stock_norm['close'])-1] = lstm_preds_col\n    \n    axes[i].plot(min_max_scaler.inverse_transform(aapl_stock_norm[col_name].values.reshape(-1,1)))\n    axes[i].plot(testPredictPlot)\n    axes[i].set_title(col_name +' Prediction')\n    plt.tight_layout()\n    #plt.plot(rnn_preds)","7c33dd17":"# cross validation with lstm\nlstm_model = keras.Sequential()\nlstm_model.add(layers.LSTM(200, input_shape=(19, 4)))\nlstm_model.add(layers.Dense(4))\nlstm_model.summary()\n\nadam = optimizers.Adam(lr = 0.001)\nlstm_model.compile(loss='mean_squared_error', optimizer=adam,\n                  metrics=[\"accuracy\"])\n\nlstm_model.fit(x_train, y_train, epochs=20,\n              batch_size=32, verbose=1,\n              validation_data=(x_valid, y_valid))","a82e5625":"## Basic Cell RNN in tensorflow\n\nindex_in_epoch = 0\nperm_array = np.arange(x_train.shape[0]) # 0\ubd80\ud130 x_train\uae38\uc774\uae4c\uc9c0\nnp.random.shuffle(perm_array)\n\n# function to get the next batch\ndef get_next_batch(batch_size):\n    global index_in_epoch, x_train, perm_array\n    start = index_in_epoch\n    index_in_epoch +=batch_size\n    \n    if index_in_epoch > x_train.shape[0]:\n        np.random.shuffle(perm_array) # shuffle permutation array\n        start = 0 # start next epoch\n        index_in_epoch = batch_size\n        \n    end = index_in_epoch\n    return x_train[perm_array[start:end]], y_train[perm_array[start:end]]\n\n# parameters\nn_steps = seq_len-1\nn_inputs = 4\nn_neurons = 200\nn_outputs = 4\nn_layers = 3\nlearning_rate = 0.001\nbatch_size = 64\nn_epochs = 100\ntrain_set_size = x_train.shape[0]\ntest_set_size = x_test.shape[0]\n\nX = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\ny = tf.placeholder(tf.float32, [None, n_outputs])\n\n# use Basic RNN Cell\nlayers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n                                     activation=tf.nn.elu)\n         for layer in range(n_layers)]\n\nmulti_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\nrnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell)\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        \n        self.i2h = nn.Linear(n_categories + input_size + hidden_size,\n                            hidden_size)\n        self.i2o\n","15e28510":"# LSTM in pytorch\nimport torch\nimport torch.nn as nn\n\nx_train = torch.FloatTensor(x_train).view(-1)","e103a072":"## \uae30\ud0c0"}}