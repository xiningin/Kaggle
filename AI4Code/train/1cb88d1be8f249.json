{"cell_type":{"a72ae892":"code","8b7926a2":"code","ac538f7e":"code","0744a09a":"code","0e25ee8b":"code","edb9a279":"code","f67d92c3":"code","9735fb47":"code","c04764be":"code","8d405105":"code","f95b1e56":"code","bda4c2f3":"code","a36a8fa7":"code","01b6ae5d":"code","fc181e40":"code","c48b6c6e":"code","acfaf040":"code","fb1212db":"code","30b2e4c5":"code","e783ca16":"code","798f70a8":"code","0cff7de5":"code","087ca1e6":"code","9a527c90":"code","db80f5b5":"code","a9d526fb":"code","b9cf08d4":"code","b3edc94a":"code","6f50acd2":"code","eaf343fc":"code","e2f54634":"code","6f9180f8":"code","2f4ec7bf":"code","2243e418":"code","2133ff98":"code","6c6016c2":"code","cfab736f":"code","ece15230":"code","a017a7ad":"code","eac3ed57":"code","eb837e1c":"code","669cd9aa":"code","5f851c63":"code","458da47a":"code","493b7904":"code","5de2a212":"code","7308bea7":"code","a7200f4d":"code","a9dfdcce":"code","9808e7a8":"code","b10bf0d4":"code","cb992392":"code","0a326c84":"code","c29da6fb":"code","08648382":"markdown","e6591d12":"markdown","9951bbfd":"markdown","aff08147":"markdown","964fb5e0":"markdown","2f127db2":"markdown","cdba0512":"markdown","bc2c8570":"markdown","d6261e6a":"markdown","8710a9ce":"markdown","e23b2f6b":"markdown","365d8595":"markdown","43ad3086":"markdown","267fa27b":"markdown","e50229d2":"markdown","3f10753b":"markdown","4e641c7b":"markdown","7dcf914a":"markdown","c34bc63b":"markdown","4cce0bb8":"markdown","1ee636e7":"markdown","c5e18597":"markdown","33ab27f9":"markdown","ed658524":"markdown"},"source":{"a72ae892":"#Import important libraries\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras import initializers\nfrom keras import regularizers\nfrom keras import constraints\nfrom keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\nfrom keras.layers.core import Activation, SpatialDropout2D\nfrom keras.layers.merge import concatenate,add\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add ,Flatten ,Dense\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nfrom tensorflow import keras\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)","8b7926a2":"# Read csv data files\ntrain_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ntest_data = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","ac538f7e":"train_data.shape #(60,000*785)\ntest_data.shape #(10000,785)\ntrain_X= np.array(train_data.iloc[:,1:])\ntest_X= np.array(test_data.iloc[:,1:])\ntrain_Y= np.array (train_data.iloc[:,0]) # (60000,)\ntest_Y = np.array(test_data.iloc[:,0]) #(10000,)","0744a09a":"train_data.head()\n","0e25ee8b":"train_X.shape, test_X.shape","edb9a279":"# Convert the images into 3 channels to fit in input for transfer models\ntrain_X=np.dstack([train_X] * 3)\ntest_X=np.dstack([test_X]*3)\ntrain_X.shape,test_X.shape","f67d92c3":"# Reshape images as per the tensor format required by tensorflow\ntrain_X = train_X.reshape(-1, 28,28,3)\ntest_X= test_X.reshape (-1,28,28,3)\ntrain_X.shape,test_X.shape","9735fb47":"# Resize the images 48*48 as required by transfer learning \nfrom keras.preprocessing.image import img_to_array, array_to_img\ntrain_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in train_X])\ntest_X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((48,48))) for im in test_X])\ntrain_X.shape, test_X.shape","c04764be":"#specify Labels\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']","8d405105":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.title(class_names[train_Y[i]])\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])    \n    plt.imshow(train_X[i])\nplt.show()","f95b1e56":"# Normalize the data and change data type\ntrain_X = train_X \/ 255.\ntest_X = test_X \/ 255.\n","bda4c2f3":"train_Y_one_hot = to_categorical(train_Y)\ntest_Y_one_hot = to_categorical(test_Y)","a36a8fa7":"\nprint(train_Y_one_hot)\nprint(test_Y_one_hot)","01b6ae5d":"train_X,valid_X,train_label,valid_label = train_test_split(train_X,\n                                                           train_Y_one_hot,\n                                                           test_size=0.05,\n                                                           random_state=13\n                                                           )\n","fc181e40":"imageSize=train_X[0].shape[0]\nChannels=3\nprint(\"imageSize : \",imageSize)","c48b6c6e":"# DenseNet169\nfrom tensorflow.keras.applications  import DenseNet169 \npreTrainedModelDenseNet169 = DenseNet169(input_shape = (imageSize, imageSize, Channels), \n                                include_top = False, \n                                weights=None)\n\npreTrainedModelDenseNet169.load_weights(\"..\/input\/densenet-keras\/DenseNet-BC-169-32-no-top.h5\")\nDenseNet169layers=preTrainedModelDenseNet169.layers\nprint(\"Number of layer DenseNet169 : \",len(DenseNet169layers))","acfaf040":"for layer  in range(len(DenseNet169layers)-250):\n    DenseNet169layers[layer].trainable = False \n   \n\n\npreTrainedModelDenseNet169.summary()","fb1212db":"# ResNet152 V2 \nfrom tensorflow.keras.applications   import ResNet152V2\npreTrainedModelResNet152V2  = ResNet152V2 (input_shape = (imageSize, imageSize, Channels), \n                                include_top = False, \n                               weights=\"imagenet\")\n\n\nResNet152V2layers=preTrainedModelResNet152V2.layers\nprint(\"Number of layer ResNet152V2 : \",len(ResNet152V2layers))","30b2e4c5":"for layer  in range(len(ResNet152V2layers)-64): \n    ResNet152V2layers[layer].trainable = False \n    \n    \npreTrainedModelResNet152V2.summary()","e783ca16":"# VGG-16\nfrom tensorflow.keras.applications.vgg16  import VGG16\npreTrainedModelVgg16 = VGG16(input_shape = (imageSize, imageSize, Channels), \n                                include_top = False, \n                                weights=None)\npreTrainedModelVgg16.load_weights(\"..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\nVGG16layers=preTrainedModelVgg16.layers\nprint(\"Number of layer Vgg16 : \",len(VGG16layers))","798f70a8":"for layer  in range(len(VGG16layers)-5): \n       VGG16layers[layer].trainable = False \n        \npreTrainedModelVgg16.summary()","0cff7de5":"#DenseNet169 Model\nx=Flatten()(preTrainedModelResNet50.output)\n\n#Fully Connection Layers\n\n# FC1\nx=Dense(1024, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.4)(x)\n\n# FC2\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC3\nx=Dense(1024, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n\n# FC4\nx=Dense(512, activation=\"relu\")(x)\n\n# FC5\nx=Dense(512, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n\n# FC6\nx=Dense(256, activation=\"relu\")(x)\n\n# FC7\nx=Dense(256, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n\n# FC8\nx=Dense(128, activation=\"relu\")(x)\n\n#output layer\nx=Dense(10,activation=\"softmax\")(x)","087ca1e6":"#concatenation layers\nmodelDenseNet169=Model(preTrainedModelDenseNet169.input,x)\nmodelDenseNet169.summary()","9a527c90":"#ResNet152V2 Model\n\nx=Flatten()(preTrainedModelResNet152V2.output)\n\n#Fully Connection Layers\n\n# FC1\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC2\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC3\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC4\nx=Dense(1024, activation=\"relu\")(x)\n\n# #Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n\n# FC5\nx=Dense(512, activation=\"relu\")(x)\n\n# FC6\nx=Dense(512, activation=\"relu\")(x)\n\n\n# FC7\nx=Dense(256, activation=\"relu\")(x)\n\n# FC8\nx=Dense(256, activation=\"relu\")(x)\n\n# #Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n","db80f5b5":"#output layer\nx=Dense(10,activation=\"softmax\")(x)\n\n#concatenation layers\nmodelResNet152V22=Model(preTrainedModelResNet152V2.input,x)\nmodelResNet152V2.summary()\n","a9d526fb":"#Vgg16 Model\nx=Flatten()(preTrainedModelVgg16.output)\n\n#Fully Connection Layer\n\n# FC1\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC2\nx=Dense(1024, activation=\"relu\")(x)\n\n# FC3\nx=Dense(1024, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.5)(x)\n\n# FC4\nx=Dense(512, activation=\"relu\")(x)\n\n# FC5\nx=Dense(512, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.4)(x)\n\n# FC6\nx=Dense(256, activation=\"relu\")(x)\n\n# FC7\nx=Dense(64, activation=\"relu\")(x)\n\n# FC8\nx=Dense(64, activation=\"relu\")(x)\n\n#Dropout to avoid overfitting effect\nx=Dropout(0.2)(x)\n\n#output layer\nx=Dense(10,activation=\"softmax\")(x)","b9cf08d4":"#concatenation layers\nmodelVgg16=Model(preTrainedModelVgg16.input,x)\nmodelVgg16.summary()","b3edc94a":"#RMSPorp Optimization\noptRMSProp=tf.keras.optimizers.RMSprop(\n    learning_rate=0.0001,\n    momentum=0.0001,\n    epsilon=1e-07,\n    name=\"RMSprop\",\n)\n","6f50acd2":"#compile DenseNet169  Model\nmodelDenseNet169.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=['accuracy'])","eaf343fc":"#compile ResNet152V2 Model\nmodelResNet152V2.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=['accuracy'])","e2f54634":"#compile Vgg16 Model\nmodelVgg16.compile(optimizer=optRMSProp, loss=\"categorical_crossentropy\",metrics=['accuracy'])","6f9180f8":"#Hyperparameters \nEpochs=100\nBatchSize=700","2f4ec7bf":"#fit MobileNetV2 Model\nhistoryResNet152V2Model=modelResNet152V2.fit(train_X,train_label,validation_data=(valid_X,valid_label) ,epochs=Epochs \n                                                     , batch_size =BatchSize ,verbose=1)  ","2243e418":"#fit Vgg16 Model\nhistoryVgg16Model=modelVgg16.fit(train_X,train_label,validation_data=(valid_X,valid_label) ,epochs=Epochs \n                                                    , batch_size =BatchSize   ,verbose=1)\n","2133ff98":"#fit DenseNet169 Model\n\nhistoryDenseNet169Model=modelDenseNet169.fit(train_X,train_label,validation_data=(valid_X,valid_label) ,epochs=Epochs \n                                                                , batch_size =BatchSize,verbose=1)\n","6c6016c2":"#Save ResNet50 Model\nmodelDanseNet196.save(\"WeightsForDanseNet196.h5\")\nprint(\"Done for DenseNet169\")\n#Save ResNet152V2 Model\nmodelResNet152V2.save(\"WeightsForResNet152V2.h5\")\nprint(\"Done for ResNet152V2\")\n#Save Vgg16 Model\nmodelVgg16.save(\"WeightsForVgg16.h5\")\nprint(\"Done for Vgg16\")\n","cfab736f":"#DenseNet169 Model\n\nprint(\"- the Accuracy and Loss for DenseNet169 Model With 100 Epochs\")\nplt.figure(figsize=(40,20))\n# summarize history for accuracy\nplt.subplot(5,5,1)\nplt.plot(historyDenseNet169Model.history['accuracy'])\nplt.plot(historyDenseNet169Model.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\n\n\n# summarize history for loss\nplt.subplot(5,5,2)\nplt.plot(historyDenseNet169Model.history['loss'])\nplt.plot(historyDenseNet169Model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()","ece15230":"#ResNet152V2 Model\n\nprint(\"- the Accuracy and Loss for ResNet152V2 Model With 100 Epochs\")\n\nplt.figure(figsize=(40,20))\n# summarize history for accuracy\nplt.subplot(5,5,1)\nplt.plot(historyResNet152V2Model.history['accuracy'])\nplt.plot(historyResNet152V2Model.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\n\n\n# summarize history for loss\nplt.subplot(5,5,2)\nplt.plot(historyResNet152V2Model.history['loss'])\nplt.plot(historyResNet152V2Model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()\n\n","a017a7ad":"#Vgg16 Model\n\nprint(\"- the Accuracy and Loss for Vgg16 Model With 100 Epochs\")\n\nplt.figure(figsize=(40,20))\n# summarize history for accuracy\nplt.subplot(5,5,1)\nplt.plot(historyVgg16Model.history['accuracy'])\nplt.plot(historyVgg16Model.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\n\n\n# summarize history for loss\nplt.subplot(5,5,2)\nplt.plot(historyVgg16Model.history['loss'])\nplt.plot(historyVgg16Model.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()","eac3ed57":"#Evaluate DenseNet169 Model\nprint(\"Evaluate DenseNet169  Model\")\nmodelDenseNet169.evaluate(test_X,test_Y_one_hot)","eb837e1c":"#Evaluate ResNet152V2 Model\nprint(\"Evaluate ResNet152V2 Model\")\nmodelResNet152V2.evaluate(test_X,test_Y_one_hot)","669cd9aa":"#Evaluate Vgg16 Model\nprint(\"Evaluate Vgg16 Model\")\nmodelVgg16.evaluate(test_X,test_Y_one_hot)","5f851c63":"historyMobileNetV2Model","458da47a":"#predict DenseNet169 Model\nPredmodelDenseNet169=modelDenseNet169.predict(test_X)\n\n#predict ResNet152V2 Model\nPredmodelResNet152V2=modelResNet152V2.predict(test_X)\n\n#predict Vgg16 Model\nPredmodelVgg16= modelVgg16.predict(test_X)","493b7904":"#Process on Prediction values for DenseNet169 Model\nPredmodelDenseNet169= np.argmax(PredmodelDenseNet169,axis=1)\nprint(\"Prediction values for DenseNet169 Model :\\n\",PredmodelDenseNet169)\n\n#Process on Prediction values for MobileNetV2 Model\nPredmodelResNet152V2= np.argmax(PredmodelResNet152V2,axis=1)\nprint(\"\\nPrediction values for ResNet152V2 Model :\\n\",PredmodelResNet152V2)\n\n#Process on Prediction values for Vgg16 Model\nPredmodelVgg16= np.argmax(PredmodelVgg16,axis=1)\nprint(\"\\nPrediction values for Vgg16 Model :\\n\",PredmodelVgg16)","5de2a212":"#DenseNet169 model\nplt.figure(figsize=(30,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.title(f\"Predication: {class_names[PredmodelResNet50[i]]} <==> Truth: {class_names[test_Y[i]]}\")\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])    \n    plt.imshow(test_X[i])\nplt.show()","7308bea7":"#ResNet152V2 model\nplt.figure(figsize=(30,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.title(f\"Predication: {class_names[PredmodelResNet152V2[i]]} <==> Truth: {class_names[test_Y[i]]}\")\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])    \n    plt.imshow(test_X[i])\nplt.show()","a7200f4d":"#Vgg16 model\nplt.figure(figsize=(30,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.title(f\"Predication: {class_names[PredmodelVgg16[i]]} <==> Truth: {class_names[test_Y[i]]}\")\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])    \n    plt.imshow(test_X[i])\nplt.show()","a9dfdcce":"#confusion matrix  for DenseNet169 Model\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(test_Y,PredmodelDenseNet169)\nprint(cm)\n#Visualizing confusion matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True)\nplt.show()","9808e7a8":"#confusion matrix  for ResNet152V2 Model\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(test_Y,PredmodelResNet152V2)\nprint(cm)\n#Visualizing confusion matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True)\nplt.show()","b10bf0d4":"#confusion matrix  for Vgg16 Model\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(test_Y,PredmodelVgg16)\nprint(cm)\n#Visualizing confusion matrix\nfig, ax = plot_confusion_matrix(conf_mat=cm,\n                                colorbar=True)\nplt.show()","cb992392":"#Classification Report for DenseNet169 Model\nfrom sklearn.metrics import classification_report\nClassificationReport = classification_report(test_Y,PredmodelDenseNet169)\nprint('Classification Report for DenseNet169 Model is : \\n ', ClassificationReport )","0a326c84":"#Classification Report for ResNet152V2 Model\nfrom sklearn.metrics import classification_report\nClassificationReport = classification_report(test_Y,PredmodelResNet152V2)\nprint('Classification Report for ResNet152V2 Model is : \\n ', ClassificationReport )","c29da6fb":"#Classification Report for Vgg16 Model\nfrom sklearn.metrics import classification_report\nClassificationReport = classification_report(test_Y,PredmodelVgg16)\nprint('Classification Report for Vgg16 Model is : \\n ', ClassificationReport )","08648382":"<h1> ","e6591d12":"<h1>Fashine Mnist Project ","9951bbfd":"<h1> ","aff08147":"<h1> Evaluate Predication with Grund Truth for DensnNet169","964fb5e0":"<h1>Build Models ","2f127db2":"<h1>Normalization for data","cdba0512":"<h1> Evaluate Predication with Grund Truth for ResNet152 V2","bc2c8570":"<h1> Convert labels to one-hot encoder (To Categorical)","d6261e6a":"<h1>Vgg16 Model","8710a9ce":"<h1>Classification Report","e23b2f6b":"<h1>Splitting train data as train and validation data","365d8595":"<h1>Import Transfer learning Models (Pre-training models)","43ad3086":"<h1>Fit Models","267fa27b":"<h1>DenseNet169 Model","e50229d2":"<h1>  Predict Models","3f10753b":"<h1>Build  Fine Tuning Layers","4e641c7b":"<h1>Compile Models","7dcf914a":"<h1>ResNet152V2 Model","c34bc63b":"<h1>Preperation and Preprocessing Data","4cce0bb8":"<h1> Do Testing on all Models","1ee636e7":"<h1> Visualizing Accuracy and loss \n","c5e18597":"<h1>confusion matrix to check on accuracy ","33ab27f9":"<h1> Evaluate Predication with Grund Truth for Vgg16","ed658524":"<h1>  Evaluate Models"}}