{"cell_type":{"caf61347":"code","2b4fbb0b":"code","026a284a":"code","04a0b033":"code","05083ea8":"code","6db571a3":"code","3e99d96e":"code","af4bbc2a":"code","e1805add":"code","32b698bb":"code","c3ce5cee":"code","134ec200":"code","a2ed2183":"code","0144566d":"code","ba470164":"code","798bd3e1":"code","95c2606a":"code","09f9b440":"code","00d9a48e":"code","1c112728":"code","9077426a":"code","a0e7a96f":"code","a9ee1ede":"code","a13a68d4":"code","477fc7e0":"code","bb86184c":"code","78973a24":"code","a7b60327":"code","dc3df300":"code","26a26610":"code","a9b5655c":"code","c903c83b":"code","2ef2921c":"code","d17a3104":"code","5d71d6a5":"code","d610304c":"code","d6e6f822":"code","c6614149":"code","9709bfd7":"code","be151533":"code","7e412ce9":"code","9667548c":"code","e699b38c":"code","f8248ee4":"code","093f4c2c":"code","492c30b4":"code","505ebd93":"code","8668442c":"code","39fd6468":"code","ac1cd3c5":"code","1282f176":"code","e8488048":"code","b6938b59":"code","6fc8c3b9":"code","44d8e125":"code","5f6bb2ed":"code","5976c986":"code","52f00f44":"code","456b8db7":"code","d0df03ff":"code","9ffd1120":"code","7a1fe872":"code","576578fd":"code","f772de51":"code","d0c9f7f9":"code","a62e9c2f":"code","de6039cc":"code","6bc02093":"code","c5bb5252":"code","e6be8c0a":"code","649c697b":"code","45a9a035":"code","29ee37e4":"code","a28e3d4d":"code","6da0409c":"code","d3b6f467":"code","9670ad2d":"code","72553d1a":"code","d184a447":"code","1d1cb909":"code","7014da86":"code","122b0fe5":"code","7c1fe5dd":"code","1992ca22":"code","4d6565b0":"code","77a85fe3":"code","3862de49":"code","9023735c":"code","92429d87":"code","12656683":"code","a6eb5f23":"code","d786867c":"code","1349c75b":"code","7acb8681":"code","999044ff":"code","b065d95d":"code","09e19a82":"markdown","764b7779":"markdown","0aec0982":"markdown","8b036b97":"markdown","9daca43c":"markdown","fe29340a":"markdown","b20af66b":"markdown","e280ee55":"markdown","5cc9703e":"markdown","0d2f885a":"markdown","28eb6a98":"markdown","62a8e57a":"markdown","9286f101":"markdown","5c62932f":"markdown","942bf72c":"markdown","38f5ff3f":"markdown","0b1b2f20":"markdown","30517683":"markdown","a80f9530":"markdown","cac7c193":"markdown","1be24bfe":"markdown"},"source":{"caf61347":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport requests\nfrom scipy import stats\nfrom functools import partial\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, LeaveOneGroupOut, LeavePGroupsOut\nimport lightgbm as lgb\nimport os\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_colwidth', 500)\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n\n\n# Any results you write to the current directory are saved as output.","2b4fbb0b":"# def is_interactive():\n#    return 'runtime' in get_ipython().config.IPKernelApp.connection_file\n\n# if is_interactive(): \n#     plt.style.use('dark_background')\n\n#     COLOR = 'white' # used 'white' when editing in interactive mode with dark theme ON\n#     import matplotlib\n#     matplotlib.rcParams['text.color'] = COLOR\n#     matplotlib.rcParams['axes.labelcolor'] = COLOR\n#     matplotlib.rcParams['xtick.color'] = COLOR\n#     matplotlib.rcParams['ytick.color'] = COLOR\n#     else: plt.style.use('ggplot')\n        \nplt.style.use('ggplot')","026a284a":"def create_axes_grid(numplots_y, numplots_x, plotsize_x=6, plotsize_y=3):\n    fig, axes = plt.subplots(numplots_y, numplots_x)\n    fig.set_size_inches(plotsize_x * numplots_x, plotsize_y * numplots_y)\n    fig.subplots_adjust(wspace=0.1, hspace=0.25)\n    return fig, axes\n\ndef set_axes(axes, use_grid=True):\n    axes.grid(use_grid)\n    axes.tick_params(which='both', direction='inout', top=True, right=True, labelbottom=True, labelleft=True)","04a0b033":"import warnings\n\nclass generalCallback():\n    def __init__(self, model_trainer, *args, **kwargs):\n        self.model_trainer = model_trainer\n        \n    def onTrainStart(self, *args, **kwargs):\n        pass\n    def onFoldStart(self, *args, **kwargs):\n        pass\n    def onFoldEnd(self, *args, **kwargs):\n        pass\n    def onTrainEnd(self, *args, **kwargs):\n        pass\n        \n        \nclass Base_Model(object):\n    \"\"\"\n    Parent model class, contains functions universal for all models used. Initial implementation credits to Bruno Aquino.\n    \"\"\"\n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True, target='accuracy_group', val_metric=metrics.r2_score, params=None, cb=None):\n        if not verbose: warnings.filterwarnings(\"ignore\")\n        self.val_metric = val_metric\n        if cb is None: \n            self.cb = generalCallback(self)\n        else:\n            self.cb = cb(self)\n        self.train_df = train_df\n        self.test_df = test_df\n        self.params = params\n        self.features = features\n        if verbose:print(f'Using {len(features)} features')\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = target\n        self.verbose = verbose\n        self.all_data = []\n        \n    def __call__(self):\n        self.params = self.get_params()\n        self.cv = self.get_cv()\n        self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = GroupKFold(5)\n        return cv.split(self.train_df, self.train_df[self.target], groups=self.train_df.area_code)\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n        self.val_preds = []\n        self.val_ys = []\n        self.model = []\n#         self.area_codes = []\n        self.cb.onTrainStart()\n        \n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n#             self.area_codes.append(self.train_df['area_code'].iloc[val_idx])\n            self.cb.onFoldStart(val_idx=val_idx, train_idx=train_idx)\n            \n#             if sum(self.train_df.iloc[train_idx]['area_code'].isin(self.train_df.iloc[val_idx]['area_code'])) > 0: raise Exception('Same area codes in trn and val sets, may be overfitting')\n            \n            \n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            \n            all_data_fold = {\n                'x_train': x_train,\n                'x_val': x_val,\n                'y_train': y_train,\n                'y_val': y_val\n            }\n            self.all_data.append(all_data_fold)\n            \n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            \n            model = self.train_model(train_set, val_set)\n            self.model.append(model)\n            conv_x_val = self.convert_x(x_val.reset_index(drop=True))\n            \n            preds_all = model.predict(conv_x_val)\n            self.val_preds.append(preds_all)\n            \n            oof_score = self.val_metric(y_val, np.array(preds_all)) if type(self.val_metric)!=list else [i(y_val, np.array(preds_all)) for i in self.val_metric]\n            if self.verbose: print(f'Partial score (all) of fold {fold} is: {oof_score}')\n\n            self.val_ys.append(y_val.reset_index(drop=True).values)\n            \n            self.cb.onFoldEnd()\n\n        self.val_ys = np.concatenate(self.val_ys)\n        self.val_preds = np.concatenate(self.val_preds)\n#         self.area_codes = np.concatenate(self.area_codes)\n        self.cb.onTrainEnd()\n        \n        self.score = self.val_metric(self.val_ys, self.val_preds) if type(self.val_metric)!=list else [i(self.val_ys, self.val_preds) for i in self.val_metric]\n\n        if self.verbose: print(f'Our oof rmse score (all) is: {self.score}')\n\nclass uk_hp_model_cb(generalCallback):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        \n    def onTrainStart(self, *args, **kwargs):\n        self.model_trainer.area_codes = []\n    \n    def onFoldStart(self, *args, **kwargs):\n        val_idx = kwargs['val_idx']\n        train_idx = kwargs['train_idx']\n        self.model_trainer.area_codes.append(self.model_trainer.train_df['area_code'].iloc[val_idx])\n        if sum(self.model_trainer.train_df.iloc[train_idx]['area_code'].isin(self.model_trainer.train_df.iloc[val_idx]['area_code'])) > 0: \n            raise Exception('Same area codes in trn and val sets, may be overfitting')\n        \n    def onFoldEnd(self, *args, **kwargs):\n        pass\n    \n    def onTrainEnd(self, *args, **kwargs):\n        self.model_trainer.area_codes = np.concatenate(self.model_trainer.area_codes)   \n    \n    \nclass Lgb_Model(Base_Model):\n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        return lgb.train(self.params, train_set, valid_sets=[train_set, val_set], verbose_eval=verbosity)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = lgb.Dataset(x_train, y_train, categorical_feature=self.categoricals)\n        val_set = lgb.Dataset(x_val, y_val, categorical_feature=self.categoricals)\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.75,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.8,\n                    'max_depth': 150, # was 15\n                  'num_leaves': 50,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 300,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 400\n                    } if not self.params else self.params\n        return params","05083ea8":"class check():\n    def __init__(self, model):\n        self.m = model\n    def test(self):\n        print(self.m.b)\n\nclass asd():\n    def __init__(self):\n        self.a = 123\n        self.cb = check(self)\n    \n    def addNewVar(self):\n        self.b = '1111111'\n        \n    def test(self):\n        self.cb.test()\n\ntobj = asd()\ntobj.addNewVar()\ntobj.test()","6db571a3":"def prep_features_list(train_data, exc_col, cor_trh):\n    train_features = [i for i in train_data.columns if i not in exc_col]\n    # train_features = important_features\n    counter = 0\n    to_remove = []\n    for feat_a in train_features:\n        for feat_b in train_features:\n            if (feat_a != feat_b) & (feat_a not in to_remove) & (feat_b not in to_remove):\n                c = np.corrcoef(train_data[feat_a], train_data[feat_b])[0][1]\n                if c > cor_trh:\n                    counter += 1\n                    to_remove.append(feat_a)\n                    print(f'{counter}: FEAT_A: {feat_a} ||| FEAT_B: {feat_b} ||| Correlation: {np.round(c,3)}')\n\n\n    train_features = [i for i in train_data.columns if i not in exc_col + to_remove]\n\n    for i in to_remove:print(i)\n    \n    return train_features\n\ndef adj_r2_score(truth, preds, n_predictors):\n    n = truth.shape[0]\n    r2 = metrics.r2_score(truth, preds)\n    res = 1-(1-r2)*(n-1)\/(n-n_predictors-1)\n    return res","3e99d96e":"hp_original = pd.read_csv('..\/input\/uk-covid-19-related-data\/indicators-DistrictUA.data.csv')\nhp = hp_original.copy()","af4bbc2a":"def prep_health_profiles(hp):\n    hp.rename({i:i.replace(' ','_').lower() for i in hp.columns}, axis=1,inplace=True)\n    \n    hp_cols_to_use = ['indicator_name','area_code','area_name','sex','age','category_type','category','time_period','value']#,'count','denominator']\n    hp = hp.loc[~hp.value.isna(),hp_cols_to_use]\n    \n    last_periods = hp.groupby('indicator_name', as_index=False).last()[['indicator_name','time_period']]\n    hp = last_periods.merge(hp, on=['indicator_name','time_period'], validate='one_to_many')\n    \n    hp = hp.groupby(['indicator_name', 'area_code'], as_index=False).mean()[['indicator_name', 'area_code','value']]#,'count','denominator']]\n    \n    hp = hp.pivot(columns='indicator_name',index='area_code',values='value')\n    \n    return hp","e1805add":"health_profiles = prep_health_profiles(hp)","32b698bb":"def prep_mortality_data(d):\n    cols = d.loc[2,:]\n    d = d.loc[3:,:]\n    d.columns = cols\n    \n    d.reset_index(drop=True,inplace=True)\n    d.index.name=None\n    d.columns.name=None\n    \n    d.rename({i:i.replace(' ','_').lower() for i in d.columns}, axis=1,inplace=True)\n    d.rename({'area_name_':'area_name'}, axis=1,inplace=True)\n    \n    d_weekly = d.groupby(['area_code', 'cause_of_death', 'week_number'], as_index=False).sum().drop(['place_of_death'], axis=1)\n    d_all = d.groupby(['area_code', 'cause_of_death'], as_index=False).sum().drop(['week_number', 'place_of_death'], axis=1)\n    \n    return d_weekly, d_all\n    \ndef get_mortality_ratios(d, USE_WEEK):\n    d.drop(['geography_type', 'area_name'], axis=1, inplace=True)\n    \n    d_all = d.loc[d.cause_of_death=='All causes',:].drop('cause_of_death', axis=1).rename({'number_of_deaths':'dnum_all'}, axis=1)\n    d_cv = d.loc[d.cause_of_death=='COVID 19',:].drop('cause_of_death', axis=1).rename({'number_of_deaths':'dnum_cv'}, axis=1)\n\n    d_prop = d_cv.merge(d_all, how='left', on=['area_code', 'week_number'], validate='one_to_one') if USE_WEEK else d_cv.merge(d_all, how='left', on=['area_code'], validate='one_to_one')\n    d_prop.loc[:,'ratio'] = d_prop.dnum_cv\/d_prop.dnum_all\n\n    d_prop.drop(['dnum_cv','dnum_all'], axis=1, inplace=True)\n    \n    d_prop = d_prop.loc[d_prop.ratio>0,:].reset_index(drop=True)\n    \n    return d_prop","c3ce5cee":"d_original = pd.read_excel('..\/input\/uk-covid-19-related-data\/lahbtablesweek22.xlsx', sheet_name='Registrations - All data')\nd = d_original.copy()","134ec200":"mortality_data_weekly, mortality_data_weekly_sum = prep_mortality_data(d)","a2ed2183":"weekly_ratios = get_mortality_ratios(mortality_data_weekly, USE_WEEK=True)\nsum_ratios = get_mortality_ratios(mortality_data_weekly_sum, USE_WEEK=False)","0144566d":"def get_population_data(cd):\n    cd.rename({i:i.replace(' ','_').lower() for i in cd.columns}, axis=1,inplace=True)\n    cd.loc[:,'internal_migration_ratio'] = cd.loc[:,'internal_migration_net']\/cd.loc[:,'estimated_population_2016.']\n    cd.loc[:,'international_migration_ratio'] = cd.loc[:,'international_migration_net']\/cd.loc[:,'estimated_population_2016.']\n    \n    cd.rename({'la_code':'area_code'}, axis=1, inplace=True)\n    cols_to_use = [i for i in cd.columns if i not in ['la_name','country_code','country_name','region_code','region_name','county_code','county_name','estimated_population_2015']]\n    \n    cd = cd[cols_to_use]\n    \n    return cd","ba470164":"# cd_original = pd.read_excel('..\/input\/uk-covid-19-related-data\/Analysis Tool mid-2016 UK.xlsx', sheet_name='Components of Change')\ncd_original = pd.read_csv('..\/input\/uk-covid-19-related-data\/Analysis Tool mid-2016 UK.csv')\ncd = cd_original.copy()","798bd3e1":"population_data = get_population_data(cd)","95c2606a":"density = pd.read_csv('..\/input\/uk-covid-19-related-data\/density.csv')","09f9b440":"density = density[['geography code', 'Area\/Population Density: Density (number of persons per hectare); measures: Value']]\ndensity = density.rename({\n    'Area\/Population Density: Density (number of persons per hectare); measures: Value':'density',\n    'geography code':'area_code'\n}, axis=1)","00d9a48e":"density.head()","1c112728":"def merge_all(mortality_ratios, health_profiles, population_data):\n    hp_and_d = mortality_ratios.merge(health_profiles, left_on='area_code', right_index=True, how='left', validate='many_to_one')\n\n    hp_and_d.rename({i:i.replace(' ','_').lower() for i in hp_and_d.columns}, axis=1,inplace=True)\n    hp_and_d.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in hp_and_d.columns]\n    \n    hp_and_d.replace([np.inf, -np.inf], np.nan, inplace=True)\n\n    hp_and_d.reset_index(drop=True,inplace=True)\n    \n    hp_d_cd = hp_and_d.merge(population_data, on='area_code', how='left')\n    \n    hp_d_cd = hp_d_cd.merge(density, on='area_code', how='left')\n    \n    return hp_d_cd","9077426a":"train_data_weekly = merge_all(weekly_ratios, health_profiles, population_data)\ntrain_data_sum = merge_all(sum_ratios, health_profiles, population_data)","a0e7a96f":"import geopandas as gpd\n\nukmap = gpd.read_file('..\/input\/uk-covid-19-related-data\/Local_Authority_Districts__April_2019__Boundaries_UK_BFE-shp\/Local_Authority_Districts__April_2019__Boundaries_UK_BFE.shp')","a9ee1ede":"for_map = ukmap.merge(mortality_data_weekly_sum, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w\/e 12-Jun.')\nfor_map.plot(column='number_of_deaths', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","a13a68d4":"for_map = ukmap.merge(population_data, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w\/e 12-Jun.')\nfor_map.plot(column='births', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","477fc7e0":"for_map = ukmap.merge(health_profiles, left_on='LAD19CD', right_index=True, how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity. Data as of w\/e 12-Jun.')\nfor_map.plot(column='Suicide rate', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","bb86184c":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\n\nfig, ax = plt.subplots(1, figsize=(20, 12))\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=0, vmax=1))\nsm._A = []\ncbar = fig.colorbar(sm)\nax.set_title('Ratio of covid-19 deaths to all deaths per UK local authrotity')\nfor_map.plot(column='ratio', cmap='plasma', linewidth=0.8, ax=ax, edgecolor='0.8')","78973a24":"plt.hist(np.log(train_data_sum.density))","a7b60327":"from textwrap import wrap","dc3df300":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'density'\nax[0].set_title('Population density (persons per hectare) (log scale)', pad=20)\nfor_map.loc[:, var] = np.log(for_map.loc[:, var])\n# for_map.loc[:,var] \/= for_map['estimated_population_2016.']\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'supporting_information_____population_from_ethnic_minorities'\nax[1].set_title('Ethnic minorities (%)', pad=20)\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","26a26610":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'tb_incidence__three_year_average_'\nax[0].set_title('Tuberculosis incidence per 100,000 (3-year average)', pad=20)\n# for_map.loc[:,var] \/= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'smoking_status_at_time_of_delivery'\nax[1].set_title('Smoking status at time of delivery (%)', pad=20)\n# for_map.loc[:,var] \/= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","a9b5655c":"train_data_sum.sort_values('births', ascending=False)[['area_code','births']]","c903c83b":"for_map = ukmap.merge(train_data_sum, left_on='LAD19CD', right_on='area_code', how='right')\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\n# for_map.loc[:,'supporting_information_____population_from_ethnic_minorities'] -= for_map.loc[:,'supporting_information_____population_from_ethnic_minorities']\n\nfig, ax = plt.subplots(1,2, figsize=(20, 12))\n\nvar = 'supporting_information_____population_aged_under_18'\nax[0].set_title('Population aged under 18 (%)', pad=20)\n# for_map.loc[:,var] \/= 100\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[0], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[0])\n\n# divider = make_axes_locatable(ax[0])\n# cax = divider.append_axes('right', size='5%', pad=0.05)\n# fig.colorbar(im1, cax=cax, orientation='vertical')\n\nvar = 'births'\nax[1].set_title('Births (log scale)', pad=20)\nfor_map.loc[:,var] = np.log(for_map.loc[:,var])\nfor_map.plot(column=var, cmap='plasma', linewidth=0.8, ax=ax[1], edgecolor='0.8')\n\nsm = plt.cm.ScalarMappable(cmap='plasma', norm=plt.Normalize(vmin=for_map[var].min(), vmax=for_map[var].max()))\nsm._A = []\ncbar = fig.colorbar(sm, ax=ax[1])","2ef2921c":"train_data_sum.head()","d17a3104":"os.listdir(\"..\/input\/uk-covid-19-related-data\")","5d71d6a5":"wdata = pd.read_csv('..\/input\/uk-covid-19-related-data\/owid-covid-data.csv', parse_dates=['date'])","d610304c":"wdata.location.unique()","d6e6f822":"wdata.head()","c6614149":"w = wdata.loc[(wdata.total_cases_per_million!=0) & (~wdata.total_cases_per_million.isna())]\ncount_by_location = w.groupby('location').count().sort_values('date')\nmode = count_by_location.date.mode().iloc[0]\nprint(mode)\ncountries_include = count_by_location.loc[count_by_location.date>=mode].index.values","9709bfd7":"w = w.loc[w.location.isin(countries_include)]\n\nall_ts = np.zeros((countries_include.shape[0], mode, 1))\nall_ts_scaled = np.zeros((countries_include.shape[0], mode, 1))\nall_labels = np.zeros(countries_include.shape[0], dtype=object)\n\nfor idx, c in enumerate(countries_include):\n    curr_country = w.loc[w.location==c]\n    values = curr_country.total_cases_per_million.iloc[0:mode].values\n    all_ts[idx, :, 0] = np.gradient(values)\n    \n    values -= np.min(values)\n    values \/= np.max(values)\n    all_ts_scaled[idx, :, 0] = values\n\n    all_labels[idx] = c","be151533":"!pip install tslearn","7e412ce9":"from tslearn.clustering import TimeSeriesKMeans","9667548c":"n_clusters = 6\nkm = TimeSeriesKMeans(n_clusters, n_jobs=-1).fit_predict(all_ts)\n\nfig, axes = create_axes_grid(n_clusters\/\/2,2,10,5)\ncol = 0\n\nfor row, cluster_id in zip(np.repeat(np.arange(n_clusters\/2, dtype='int'), 2), range(n_clusters)):\n    names = all_labels[km==cluster_id]\n    name = ', '.join([i for i in names[0:5]])\n    \n    cases = all_ts[km==cluster_id, :, 0]\n    \n    axes[row, col].set_title(f'C {cluster_id} | {len(names)} countries | {name}')\n    axes[row, col].set_ylabel('Total cases per million')\n    axes[row, col].set_xlabel('Days since case 1')\n    \n    days = np.arange(0, mode)\n    \n    for i in range(sum(km==cluster_id)):\n        axes[row, col].plot(days, cases[i,:], color='blue')\n        \n    col = 1 if col == 0 else 0","e699b38c":"n_clusters = 6\nkm = TimeSeriesKMeans(n_clusters, n_jobs=-1).fit_predict(all_ts_scaled)\n\nfig, axes = create_axes_grid(n_clusters\/\/2,2,10,5)\ncol = 0\n\nfor row, cluster_id in zip(np.repeat(np.arange(n_clusters\/2, dtype='int'), 2), range(n_clusters)):\n    names = all_labels[km==cluster_id]\n    name = ', '.join([i for i in names[0:5]])\n    \n    cases = all_ts_scaled[km==cluster_id, :, 0]\n    \n    axes[row, col].set_title(f'C {cluster_id} | {len(names)} countries | {name}')\n    axes[row, col].set_ylabel('Total cases per million')\n    axes[row, col].set_xlabel('Days since case 1')\n    \n    days = np.arange(0, mode)\n    \n    for i in range(sum(km==cluster_id)):\n        axes[row, col].plot(days, cases[i,:], color='blue')\n        \n    col = 1 if col == 0 else 0","f8248ee4":"for row, cluster_id in zip(np.repeat(np.arange(n_clusters\/2, dtype='int'), 2), range(n_clusters)):\n    cluster_countries = all_labels[km==cluster_id]\n    \n    wdata.loc[wdata.location.isin(cluster_countries), 'cluster_id'] = cluster_id\n    \n    names = sorted(cluster_countries)\n    name = [', '.join(names[i:i+10]) for i in range(0, len(names), 10)]\n\n    name = '\\n '.join([i for i in name])\n    \n    print(f'C {cluster_id} | {len(names)} countries | {name}  \\n')","093f4c2c":"cols_include = ['cluster_id','total_cases','total_cases_per_million','total_tests','total_tests_per_thousand','tests_units','population_density','median_age','aged_65_older','aged_70_older', 'gdp_per_capita', 'cvd_death_rate', 'diabetes_prevalence']\ncluster_details = wdata[cols_include+['location']].groupby(['cluster_id','location'], as_index=False).last()\ncluster_details","492c30b4":"cluster_details_mean = cluster_details[cols_include].groupby('cluster_id').median()\ncluster_details_mean","505ebd93":"train_data = cluster_details\n\ntrain_features = [i for i in cluster_details if i not in ['cluster_id','location','tests_units',]]#'total_cases', 'total_tests']]\n\ndef wrapper(y, yhat, func):\n    y_m = np.zeros((yhat.shape[0],yhat.shape[1]))\n    if type(y) == pd.Series:\n        y_m[np.arange(0,y_m.shape[0],dtype=int), y.values.astype('int')] = 1\n    else:\n        y_m[np.arange(0,y_m.shape[0],dtype=int), y.astype('int')] = 1\n    r = func(y_m, yhat)\n    return r\n\n# validation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\nvalidation_metrics = [partial(wrapper, func=metrics.roc_auc_score)]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'multiclass',\n                      'num_classes': n_clusters,\n                    'metric': 'multiclass',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\nimport types\n\nlgb_model = Lgb_Model(train_df=train_data, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='cluster_id', val_metric=validation_metrics, params=params, cb=None)\n# lgb_model.get_cv = types.MethodType( lambda x: KFold(2, shuffle=True, random_state=1997).split(x.train_df, x.train_df[x.target]), lgb_model )\nlgb_model.get_cv = types.MethodType( lambda x: StratifiedKFold(2).split(x.train_df, x.train_df[x.target]), lgb_model )\nlgb_model()","8668442c":"model = lgb_model.model[0]\nfig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","39fd6468":"fig, ax = plt.subplots(2,6,figsize=(33, 10))\n\nvar = 'total_cases'\nax[0, 0].set_title(var)\nbplot0 = ax[0, 0].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<1e6), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_tests'\nax[0, 1].set_title(var)\nbplot1 = ax[0, 1].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<0.6*1e7), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'cvd_death_rate'\nax[0, 2].set_title(var)\nbplot2 = ax[0, 2].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_cases_per_million'\nax[0, 3].set_title(var)\nbplot3 = ax[0, 3].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'median_age'\nax[0, 4].set_title(var)\nbplot4 = ax[0, 4].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'aged_65_older'\nax[0, 5].set_title(var)\nbplot5 = ax[0, 5].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'population_density'\nax[1, 0].set_title(var)\nbplot6 = ax[1, 0].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<750), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'diabetes_prevalence'\nax[1, 1].set_title(var)\nbplot7 = ax[1, 1].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<0.6*1e7), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'total_tests_per_thousand'\nax[1, 2].set_title(var)\nbplot8 = ax[1, 2].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'gdp_per_capita'\nax[1, 3].set_title(var)\nbplot9 = ax[1, 3].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\nvar = 'aged_70_older'\nax[1, 4].set_title(var)\nbplot10 = ax[1, 4].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6), patch_artist=True)\n\n# var = 'aged_65_older'\n# ax[1, 5].set_title(var)\n# ax[1, 5].boxplot([train_data.loc[(train_data.cluster_id==i) & (train_data[var]<np.inf), var] for i in range(6)], labels=range(6))\n\nfor bp in [bplot0, bplot1, bplot2,bplot3,bplot4,bplot5,bplot6,bplot7,bplot8,bplot9,bplot10]:\n    for b, c in zip(bp['boxes'], ['yellow','green','red','green','red','red']):\n        b.set_facecolor(c)\n\nplt.show()","ac1cd3c5":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","1282f176":"%time\n\nimport shap\nx_val = lgb_model.all_data[0]['x_val']\n\nshap_values = shap.TreeExplainer(lgb_model.model[0]).shap_values(x_val)\n\nshap.summary_plot(shap_values, x_val)","e8488048":"# for f in important_features[0:8]:\n#     shap.dependence_plot(f, shap_values[-1], x_val)","b6938b59":"fig, axes = create_axes_grid(1,2,10,5)\n\ny = 'total_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y], label=x);\naxes[0].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y], label=x2);\naxes[0].legend()\n\ny = 'total_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y], label=x);\naxes[1].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y], label=x2);\naxes[1].legend()","6fc8c3b9":"fig, axes = create_axes_grid(1,2,10,5)\n\ny = 'new_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y].rolling(7).mean(), label=x);\naxes[0].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y].rolling(7).mean(), label=x2);\naxes[0].legend()\n\ny = 'new_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].plot(wdata.loc[wdata.location==x, 'date'], wdata.loc[wdata.location==x, y].rolling(7).mean(), label=x);\naxes[1].plot(wdata.loc[wdata.location==x2, 'date'], wdata.loc[wdata.location==x2, y].rolling(7).mean(), label=x2);\naxes[1].legend()","44d8e125":"def norm(arr):\n    return (arr - np.mean(arr))\/ np.std(arr)\n\nfig, axes = create_axes_grid(1,2,10,5)\n\ny = 'total_cases_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[0].set_title(f'{y} in {x} &  {x2}')\naxes[0].hist(norm(wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values.reshape(-1,1)), label=x);\naxes[0].hist(norm(wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values.reshape(-1,1)), label=x2);\naxes[0].legend()\n\ny = 'total_deaths_per_million'\nx = 'United Kingdom'\nx2 = 'Sweden'\naxes[1].set_title(f'{y} in {x} &  {x2}')\naxes[1].hist(norm(wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values.reshape(-1,1)), label=x);\naxes[1].hist(norm(wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values.reshape(-1,1)), label=x2);\naxes[1].legend()","5f6bb2ed":"from scipy import stats\n\ny = 'total_cases_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","5976c986":"y = 'total_deaths_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","52f00f44":"y = 'new_cases_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","456b8db7":"y = 'new_deaths_per_million'\na = wdata.loc[(wdata.location==x) & (wdata[y]>0), y].values\nb = wdata.loc[(wdata.location==x2) & (wdata[y]>0), y].values\n# a = wdata.loc[(wdata.location==x), y].values\n# b = wdata.loc[(wdata.location==x2), y].values\nprint(np.std(a), np.std(b), a.shape, b.shape)\nstats.ttest_ind(a, b, equal_var=False), stats.mannwhitneyu(a, b)","d0df03ff":"train_data = wdata.loc[wdata.location.isin([x, x2]), ['location', 'date', 'total_cases_per_million', 'total_deaths_per_million']].reset_index(drop=True)\ntrain_data.loc[train_data.location==x, 'date'] = np.arange(0, train_data.loc[train_data.location==x, 'date'].shape[0])\ntrain_data.loc[train_data.location==x2, 'date'] = np.arange(0, train_data.loc[train_data.location==x2, 'date'].shape[0])\n\nnp.random.seed(123321)\nridx = np.random.permutation(train_data.shape[0])\ntrain_data = train_data.loc[ridx,:].reset_index(drop=True)\nval_cutoff = round(0.8*train_data.shape[0])\n\ncat_dict = {\n    'United Kingdom': 0,\n    'Sweden': 1\n}\ntrain_data.loc[:, 'location'] = train_data.loc[:, 'location'].map(cat_dict)\n\ntrain_features = ['date', 'total_cases_per_million', 'total_deaths_per_million']\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='location', val_metric=validation_metrics, params=params, cb=None)\nlgb_model.get_cv = lambda: [(np.arange(0, val_cutoff, dtype=int), np.arange(val_cutoff, train_data.shape[0], dtype=int))]\nlgb_model()","9ffd1120":"area_code_to_name = cd_original.groupby(['LA Code','LA Name'], as_index=False).first()[['LA Code','LA Name']]\narea_code_to_name.rename({'LA Code':'area_code','LA Name':'la_name'},  axis=1, inplace=True)\n\nnum_areas = 20\nhp_d_cd = train_data_weekly.sort_values('ratio')\nareas = hp_d_cd.area_code.unique()[0:num_areas]\nhp_d_cd = hp_d_cd.sort_values(['area_code','week_number'])\n\nfig, axes = create_axes_grid(num_areas\/\/2,2,10,5)\ncol = 0\n\nfor row, area in zip(np.repeat(np.arange(num_areas\/2, dtype='int'), 2), areas):\n    name = area_code_to_name.loc[area_code_to_name.area_code==area,'la_name'].values[0]\n    population = hp_d_cd.loc[hp_d_cd.area_code==area, 'estimated_population_2016.'].values[0]\n    axes[row, col].set_title(f'{name} - {area} | Population: {population}')\n    axes[row, col].set_ylabel('Cov deaths \/ all deaths ratio')\n    \n    weeks = hp_d_cd.loc[hp_d_cd.area_code==area, 'week_number']\n    ratios = hp_d_cd.loc[hp_d_cd.area_code==area, 'ratio']\n    \n    axes[row, col].plot(weeks, ratios, color='blue')\n    col = 1 if col == 0 else 0","7a1fe872":"original_train_data_weekly = train_data_weekly.copy()","576578fd":"train_data_weekly =original_train_data_weekly\n\nsmoking_related = [i for i in train_data_weekly.columns if 'smoking' in i]\n# train_data_weekly[smoking_related] -= train_data_weekly[smoking_related].min()\n# train_data_weekly[smoking_related] \/= train_data_weekly[smoking_related].max()\n# train_data_weekly.loc[:, 'smoking_related'] = train_data_weekly[smoking_related].mean(axis=1)\n# train_data_weekly.drop(smoking_related, axis=1, inplace=True)","f772de51":"[i for i in train_data_weekly.columns if 'smoking' in i]","d0c9f7f9":"train_features = prep_features_list(train_data_weekly, exc_col = ['area_code', 'geography_type', 'area_name', 'ratio'], cor_trh=0.95)\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data_weekly, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='ratio', val_metric=validation_metrics, params=params, cb=uk_hp_model_cb)","a62e9c2f":"lgb_model()","de6039cc":"'FINAL SCORE:', lgb_model.score","6bc02093":"fig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Truth vs preds')\naxes.set_ylabel('True ratio')\naxes.set_xlabel('Predicted ratio')\naxes.scatter(lgb_model.val_preds, lgb_model.val_ys, color='blue')","c5bb5252":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","e6be8c0a":"individual_weeks_fi = fi.copy()","649c697b":"fig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","45a9a035":"original_train_data_sum = train_data_sum.copy()","29ee37e4":"train_data_sum.area_code.nunique()","a28e3d4d":"train_data_sum = original_train_data_sum\n\nsmoking_related = [i for i in train_data_sum.columns if 'smoking' in i]\n# train_data_sum[smoking_related] -= train_data_sum[smoking_related].min()\n# train_data_sum[smoking_related] \/= train_data_sum[smoking_related].max()\n# train_data_sum.loc[:, 'smoking_related'] = train_data_sum[smoking_related].sum(axis=1)\n# train_data_sum.drop(smoking_related, axis=1, inplace=True)","6da0409c":"# train_data_sum.drop('smoking_status_at_time_of_delivery',axis=1,inplace=True)\n[i for i in train_data_sum.columns if 'smoking' in i]","d3b6f467":"# from itertools import combinations\n\n# for f1,f2 in combinations(smoking_related, 2):\n#     print(f1,f2,stats.pearsonr(train_data_sum[f1],train_data_sum[f2]))\n#     plt.scatter(train_data_sum[f1],train_data_sum[f2])\n#     plt.show()","9670ad2d":"[i for i in train_data_sum.columns if 'smoking' in i]","72553d1a":"train_features = prep_features_list(train_data_sum, exc_col = ['area_code', 'geography_type', 'area_name', 'ratio'], cor_trh=0.95)\nvalidation_metrics = [metrics.r2_score, partial(adj_r2_score, n_predictors=len(train_features)), lambda x,y: stats.pearsonr(x,y)[0]]\n\nparams = {'n_estimators':6000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'regression',\n                    'metric': 'rmse',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.01,\n                    'feature_fraction': 0.7,\n                    'max_depth': 10, # was 15\n                  'num_leaves': 5,\n                    'lambda_l1': 0.1,  \n                    'lambda_l2': 0.1,\n                    'early_stopping_rounds': 100,\n                  'min_data_in_leaf': 1,\n                          'min_gain_to_split': 0.01,\n                          'max_bin': 100\n                    }\n\n\nlgb_model = Lgb_Model(train_df=train_data_sum, test_df=None, features=train_features, categoricals=[], n_splits=5, verbose=True, target='ratio', val_metric=validation_metrics, params=params, cb=uk_hp_model_cb)","d184a447":"lgb_model()","1d1cb909":"'FINAL SCORE:', lgb_model.score","7014da86":"fig, axes = create_axes_grid(1,1,10,5)\naxes.set_title('Truth vs preds')\naxes.set_ylabel('True ratio')\naxes.set_xlabel('Predicted ratio')\naxes.scatter(lgb_model.val_preds, lgb_model.val_ys, color='blue')","122b0fe5":"model = lgb_model.model[0]\n\nfi = [(i,f) for i, f in zip(model.feature_name(), model.feature_importance())]\nfi = sorted(fi, key = lambda x: x[1], reverse=True)\ncutoff_trh = np.percentile([i[1] for i in fi], 10)\nprint(cutoff_trh)\nimportant_features = [i[0] for i in fi if i[1] > cutoff_trh]\nfor i,z in fi: print((i,z))","7c1fe5dd":"summed_weeks_fi = fi.copy()","1992ca22":"fig, ax = plt.subplots(1,1,figsize=(15, 10))\nlgb.plot_importance(model, max_num_features = 20, ax=ax)","4d6565b0":"fi_both = pd.DataFrame({'Feature': [i for i,z in summed_weeks_fi]})\nfi_both.loc[:,'Rank_summed'] = [i for i in range(1, fi_both.shape[0]+1)]\nfi_both.loc[:,'Rank_weekly'] = fi_both.loc[:,'Feature'].map({i:r for r, (i,z) in enumerate(individual_weeks_fi, 1)})\n\nfi_both.loc[:, 'Lowest_rank'] = fi_both.apply(lambda x: max([x['Rank_weekly'], x['Rank_summed']]), axis=1)\nfi_both.loc[:, 'Diff_in_rank'] = np.abs(fi_both['Rank_weekly'] - fi_both['Rank_summed'])\nfi_both = fi_both.sort_values('Lowest_rank')","77a85fe3":"fi_both","3862de49":"import re\ndef title_prettify(x):\n    x = re.sub('_+', ' ', x)\n    x = x[0].upper() + x[1:]\n    return  x\n\ntitle_prettify('under_18s_conception_rate___1_000')","9023735c":"specific_week = None\ntrain_data = train_data_sum\n\nnum_features = 6\nfig, axes = create_axes_grid(num_features\/\/2,2,10,5)\ncol = 0\n\ndo_log = ['Density','Births']\n\nfor row, f_idx in zip(np.repeat(np.arange(num_features\/2, dtype='int'), 2), range(num_features)):\n#     f = fi_both.iloc[f_idx]['Feature']\n    f = summed_weeks_fi[f_idx][0]\n#     axes[row, col].set_title(f'{f} (rank: {f_idx+1})')\n    title = title_prettify(f)\n    axes[row, col].set_title(title)\n    axes[row, col].set_xlabel(title)\n    axes[row, col].set_ylabel('Covid \/ all deaths ratio')\n    \n    if title not in do_log:\n        if specific_week:\n            axes[row, col].scatter(train_data.loc[train_data.week_number==specific_week, f], train_data.loc[train_data.week_number==specific_week, 'ratio'], color='blue')\n        else:\n            axes[row, col].scatter(train_data[f], train_data.ratio, color='blue')\n    else:\n        title = f\"{title} (log scale)\"\n        axes[row, col].set_title(title)\n        axes[row, col].set_xlabel(title)\n        if specific_week:\n            axes[row, col].scatter(np.log(train_data.loc[train_data.week_number==specific_week, f]), train_data.loc[train_data.week_number==specific_week, 'ratio'], color='blue')\n        else:\n            axes[row, col].scatter(np.log(train_data[f]), train_data.ratio, color='blue')\n    col = 1 if col == 0 else 0","92429d87":"import shap","12656683":"%time \nx_val = lgb_model.all_data[0]['x_val']\n\nshap_values = shap.TreeExplainer(lgb_model.model[0]).shap_values(x_val)","a6eb5f23":"shap.summary_plot(shap_values, x_val)","d786867c":"for f in fi_both.iloc[0:6]['Feature']:\n    shap.dependence_plot(f, shap_values, x_val)","1349c75b":"# top10_f = hp_and_d.sort_values('tb_incidence__three_year_average_',ascending=False)[['area_code','ratio','tb_incidence__three_year_average_']].iloc[0:10]\n# top10_f = top10_f.merge(hp_original[['Area Code', 'Area Name', 'Area Type']].drop_duplicates(),  left_on='area_code', right_on='Area Code', how='left')\n# top10_f","7acb8681":"errors = pd.DataFrame({'area_code':lgb_model.area_codes,\n                      'prediction':lgb_model.val_preds,\n                      'truth':lgb_model.val_ys})\n\nerrors.loc[:,'error'] = abs(errors.prediction - errors.truth)\n\nerrors = errors.merge(hp_original[['Area Code', 'Area Name', 'Area Type']].drop_duplicates(),  left_on='area_code', right_on='Area Code', how='left')\n\nerrors = errors.groupby('Area Name').mean()\n\nerrors = errors.sort_values('error',ascending=False)","999044ff":"errors.head(10)","b065d95d":"errors.tail(10)","09e19a82":"## Summed weeks","764b7779":"# Merging","0aec0982":"The data comes from UK's Office for National Statistics\n\nLink: https:\/\/www.ons.gov.uk\/peoplepopulationandcommunity\/healthandsocialcare\/causesofdeath\/datasets\/deathregistrationsandoccurrencesbylocalauthorityandhealthboard","8b036b97":"# World data","9daca43c":"# Modelling","fe29340a":"# Prediction errors per region","b20af66b":"The dataset contains a lot of different indicators. Some of them contain values for different age groups, different sexes and for different time periods. The values of indicators themselves also contain confidence intervals and comparisons to other UK statistics.\n\nNotes\n* Despite all that information, at the moment I only use value by indicator by group, everything else being summed up. \n* Summing indicator values up is appropriate for indicators measured in absolute numbers, but is not very appropriate for percentages.","e280ee55":"# Map","5cc9703e":"### Density","0d2f885a":"From https:\/\/www.nomisweb.co.uk\/census\/2011\/wd102ew","28eb6a98":"# Population data","62a8e57a":"# Get health data from PHE","9286f101":"From https:\/\/www.ons.gov.uk\/peoplepopulationandcommunity\/populationandmigration\/populationestimates\/datasets\/populationestimatesanalysistool","5c62932f":"# Descriptive plots","942bf72c":"The dataset contains registered deaths by UK regions, weeks, causes (covid vs all), place\n\nAgain, at the moment only used part of this data. Here using regions and weeks.","38f5ff3f":"## Feature importance across 2 models","0b1b2f20":"Using light gbm, one of the most mainstream and most powerful and easy-to-use models\n\nI did not adjust hyperparameters specifically, just used what I used in one of previous competitions I took part in.","30517683":"## Weekly","a80f9530":"I have downloaded UK data on various health profiles across UK regions\n\nLink - https:\/\/fingertips.phe.org.uk\/profile\/health-profiles\/data#page\/6\/gid\/8000073\/pat\/6\/par\/E12000004\/ati\/201\/are\/E07000032\/iid\/20201\/age\/1\/sex\/2\/cid\/4\/page-options\/map-ao-4_cin-ci-4_ovw-tdo-0","cac7c193":"# Get covid mortality data","1be24bfe":"# Sweden"}}