{"cell_type":{"577d22e4":"code","df7eddfd":"code","a5e2e20d":"code","e1cb84ee":"code","a029a73a":"code","c4066499":"code","4dd6de8d":"code","b5576fce":"code","4cca73aa":"code","a68f6227":"code","d3d3af08":"code","8a45ee19":"code","abad6fa1":"code","a692ad46":"code","16cfe603":"code","dadb0969":"code","c18dc1f0":"code","daadfc9f":"code","6e326528":"code","bdd78798":"code","1c2a3b85":"code","ca78ac24":"code","d24e31fa":"code","e60b7b5c":"code","a7a6aab2":"code","41edfab4":"code","45c1a8b6":"code","e6ed6283":"code","3f8bb353":"code","67ca815a":"code","a8b854ac":"code","eeb490c3":"code","605c222b":"markdown","c82c9489":"markdown","20159a91":"markdown","12ecc966":"markdown","696b455b":"markdown","3937134b":"markdown","b2aa190a":"markdown","d2f31968":"markdown","8fa81a95":"markdown","f7b93dbd":"markdown","17f9886e":"markdown","9f66c4fc":"markdown","d716a5ae":"markdown"},"source":{"577d22e4":"import os\nimport sys\nimport glob\nimport time\nimport cv2                                           # pip install opencv-python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport albumentations as A\nfrom tqdm.notebook import tqdm                       # pip install tqdm\nfrom PIL import Image, ImageOps\nfrom scipy.spatial import ConvexHull\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom albumentations.pytorch import ToTensorV2        # pip install albumentations\nfrom sklearn.model_selection import train_test_split","df7eddfd":"# \uc720\ub2c8\ucf54\ub4dc \uae68\uc9d0\ud604\uc0c1 \ud574\uacb0\nmpl.rcParams['axes.unicode_minus'] = False\n\n# \ub098\ub214\uace0\ub515 \ud3f0\ud2b8 \uc801\uc6a9\nplt.rcParams[\"font.family\"] = 'NanumGothic'","a5e2e20d":"data = pd.read_csv('.\/data\/train.csv',encoding='utf8')\ndata","e1cb84ee":"test = pd.read_csv('.\/data\/test.csv',encoding='utf8')\ntest","a029a73a":"submission = pd.read_csv('.\/data\/sample_submission.csv',encoding='utf8')\nsubmission","c4066499":"landmarks_class = [\n    '\uc11c\uc6b8\ud55c\uc591\ub3c4\uc131',\n    '\ucda9\ubb34\uacf5 \uc774\uc21c\uc2e0 \ub3d9\uc0c1',\n    '\uae40\ud3ec\uad6d\uc81c\uacf5\ud56d',\n    '\uc138\ube5b\uc12c',\n    '\uc815\ub989',\n    '\uc591\ud654\ub300\uad50',\n    '\uc11c\uc6b8\uad11\uc7a5',\n    '\ubd81\ucd0c \ud55c\uc625\ub9c8\uc744',\n    '\ub86f\ub370\uc6d4\ub4dc\ud0c0\uc6cc',\n    '\uacbd\ubcf5\uad81'\n]","4dd6de8d":"plt.figure(figsize=(12,8))\nfor n, (idx, row) in enumerate(data.sample(16).iterrows(), start=1):\n    plt.subplot(4,4, n)\n    img = plt.imread(row['path'])\n    plt.imshow(img)\n    plt.title(landmarks_class[row['class']])\n    plt.axis('off')\nplt.tight_layout()\nplt.show()","b5576fce":"train, valid = train_test_split(data,\n                                test_size=0.2,\n                                random_state=42)","4cca73aa":"train","a68f6227":"valid","d3d3af08":"class LandmarksDataset(Dataset):\n    def __init__(self, data, transforms):\n        self.data = data\n        self.transforms = transforms\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __getitem__(self, index: int):\n        assert index <= len(self), 'index range error'\n\n        image = plt.imread(self.data.iloc[index, ]['path'])\n        image = np.array(image)\n        \n        target = torch.as_tensor(self.data.iloc[index, ]['class'])\n\n        if self.transforms is not None:\n            image = self.transforms(image=image)['image']\n\n        image = image\/255.0\n\n        return image, target","8a45ee19":"transforms_train = A.Compose([\n    A.Resize(128, 128),\n    ToTensorV2(),\n])\n\ntransforms_valid = A.Compose([\n    A.Resize(128, 128),\n    ToTensorV2(),\n])","abad6fa1":"train_dataset = LandmarksDataset(data=train, transforms=transforms_train)\nvalid_dataset = LandmarksDataset(data=valid, transforms=transforms_valid)","a692ad46":"batch_size = 32","16cfe603":"train_dataloader = DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\nvalid_dataloader = DataLoader(\n    valid_dataset, batch_size=batch_size, shuffle=True, num_workers=4)","dadb0969":"images, targets = next(iter(train_dataloader))\nplt.imshow(images[0].numpy().transpose(1,2,0))\nplt.title(landmarks_class[targets[0]])\nplt.axis('off')\nplt.show()","c18dc1f0":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 32, 3, 1)\n        self.conv3 = nn.Conv2d(32, 32, 3, 1)\n        self.conv4 = nn.Conv2d(32, 32, 3, 1)\n        \n        self.fc1 = nn.Linear(26912, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = self.conv3(x)\n        x = F.relu(x)\n        x = self.conv4(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        \n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n\n        # Apply softmax to x\n        output = F.softmax(x, dim=1)\n        return output","daadfc9f":"model = SimpleCNN().to('cuda')\nprint(model)","6e326528":"sum([p.numel() for p in model.parameters()])","bdd78798":"# Equates to one random 28x28 image\nresult = model(torch.rand((1, 3, 128, 128)).to('cuda'))\nprint (result)","1c2a3b85":"optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-2)\nloss_fn = nn.CrossEntropyLoss()","ca78ac24":"import datetime","d24e31fa":"epochs = 30\ntrain_losses = []\ntrain_acces = []\nvalid_losses = []\nvalid_acces = []\n\nstart_time = time.time()\n\nfor epoch in range(epochs):\n    \n    #########\n    # Train #\n    #########\n    model.train()\n    train_loss = 0\n    train_acc = 0\n    for i, (images, targets) in enumerate(train_dataloader):\n        images = images.to('cuda')\n        targets = targets.to('cuda')\n\n        scores = model(images)\n        _, preds = scores.max(dim=1)\n        loss = loss_fn(scores, targets)\n\n        train_loss += loss\n        train_acc += (targets == preds).sum() \/ batch_size\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    ##############\n    # Validation #\n    ##############\n    model.eval()\n    valid_loss = 0\n    valid_acc = 0\n    with torch.no_grad():\n        for i, (images, targets) in enumerate(valid_dataloader):\n            images = images.to('cuda')\n            targets = targets.to('cuda')\n\n            scores = model(images)\n            _, preds = scores.max(dim=1)\n\n            valid_loss += loss_fn(scores, targets)\n            valid_acc += (targets == preds).sum() \/ batch_size\n            \n    #######\n    # Log #\n    #######\n    train_losses.append(train_loss.detach().cpu().numpy() \/ len(train_dataloader))\n    train_acces.append(train_acc.detach().cpu().numpy() \/ len(train_dataloader))\n    valid_losses.append(valid_loss.detach().cpu().numpy() \/ len(valid_dataloader))\n    valid_acces.append(valid_acc.detach().cpu().numpy() \/ len(valid_dataloader))\n        \n\n    elapsed_time = time.time() - start_time\n        \n    print(f\"[{time.strftime('%H:%M:%S', time.gmtime(elapsed_time))}] Epoch {epoch+1:2d} \\\n    Train Loss: {train_losses[-1]:6.4f} Train Acc: {train_acces[-1]:6.4f} \\\n    Valid Loss: {valid_losses[-1]:6.4f} Valid Acc: {valid_acces[-1]:6.4f}\")","e60b7b5c":"plt.figure(figsize=(16,6))\n\nplt.subplot(1,2,1)\nplt.plot(train_losses)\nplt.plot(valid_losses)\nplt.title('Loss')\nplt.legend(['Train','Valid'])\nplt.grid(True)\n\nplt.subplot(1,2,2)\nplt.plot(train_acces)\nplt.plot(valid_acces)\nplt.title('Accuracy')\nplt.legend(['Train','Valid'])\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()","a7a6aab2":"transforms_test = A.Compose([\n    A.Resize(128, 128),\n    ToTensorV2(),\n])","41edfab4":"test_dataset = LandmarksDataset(data=test, transforms=transforms_test)","45c1a8b6":"test_dataloader = DataLoader(\n    test_dataset, batch_size=1, shuffle=False, num_workers=8)","e6ed6283":"model.eval()\npreds = []\nwith torch.no_grad():\n    for i, (images, targets) in tqdm(enumerate(test_dataset)):\n        images = images.to('cuda')\n        scores = model(images.unsqueeze(0))\n        _, pred = scores.max(dim=1)\n        \n        preds.append(pred.item())","3f8bb353":"submission['class'] = preds","67ca815a":"submission","a8b854ac":"test_path, pred = submission.sample(1).values[0]\nplt.imshow(plt.imread(test_path))\nplt.title(landmarks_class[pred])\nplt.axis('off')\nplt.show()","eeb490c3":"submission.to_csv('.\/data\/submission.csv', encoding='utf8', index=None)","605c222b":"\ub098\ub214 \uae00\uaf34 \uc124\uce58  \napt-get \uba85\ub839\uc73c\ub85c \ub098\ub214\uae00\uaf34(fonts-nanum)\uc744 \uc124\uce58\ud558\uace0, fc-cache \uba85\ub839\uc73c\ub85c \ud3f0\ud2b8 \uce90\uc2dc \uc0ad\uc81c\n```bash\n$ sudo apt-get install fonts-nanum*\n$ sudo fc-cache -fv\n```\n\n\ub9cc\uc77c \ub2e4\ub978 ttf \ud3f0\ud2b8\ub97c \uac00\uc838\uc654\ub2e4\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 \ubcf5\uc0ac\ud558\uace0, fc-cache \uba85\ub839\uc73c\ub85c \ud3f0\ud2b8 \uce90\uc2dc \uc0ad\uc81c\n```bash\n$ sudo cp new_font.ttf \/usr\/share\/fonts\/\n$ sudo fc-cache -fv\n```\n\nmatplotlib \ub098\ub214 \uae00\uaf34\uc744 \ucd94\uac00\n\ub098\ub214 \uae00\uaf34\uc744 matplotlib \uc5d0 \ubcf5\uc0ac\ud558\uace0, matplotlib\uc758 \ud3f0\ud2b8 \uce90\uc2dc\ub97c \uc0ad\uc81c\n```bash\n$ sudo cp \/usr\/share\/fonts\/truetype\/nanum\/Nanum* \/usr\/local\/lib\/python3.4\/dist-packages\/matplotlib\/mpl-data\/fonts\/ttf\/\n$ rm -rf \/home\/ubuntu\/.cache\/matplotlib\/*\n```","c82c9489":"## \ucc38\uace0\uc790\ub8cc\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","20159a91":"<img src=\"https:\/\/i.imgur.com\/fVakQcm.png\">\n\n# 2021\ub144 \ub370\uc774\ud130 \ud06c\ub9ac\uc5d0\uc774\ud2f0\ube0c \ucea0\ud504 - \ubcf8\uc120 A\ud300\n\uc7a5\uc18c:   \n\ub0a0\uc9dc: 2021\ub144 11\uc6d4 20\uc77c (\ud1a0)  \n\n<br>\n<br>\n\n<div align='right'>\ucd9c\uc81c\uc790: <b>\uc131\ubbfc\uc11d<\/b><\/div>\n<div align='right'>minsuksung@korea.ac.kr<\/div>\n\n<br>\n<br>","12ecc966":"## \ud559\uc2b5 \ubc0f \ud3c9\uac00\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","696b455b":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#\ub300\ud68c-\uc18c\uac1c\" data-toc-modified-id=\"\ub300\ud68c-\uc18c\uac1c-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>\ub300\ud68c \uc18c\uac1c<\/a><\/span><\/li><li><span><a href=\"#\ub77c\uc774\ube0c\ub7ec\ub9ac-\ubc0f-\uc635\uc158\" data-toc-modified-id=\"\ub77c\uc774\ube0c\ub7ec\ub9ac-\ubc0f-\uc635\uc158-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>\ub77c\uc774\ube0c\ub7ec\ub9ac \ubc0f \uc635\uc158<\/a><\/span><\/li><li><span><a href=\"#\ub370\uc774\ud130-\ubd88\ub7ec\uc624\uae30\" data-toc-modified-id=\"\ub370\uc774\ud130-\ubd88\ub7ec\uc624\uae30-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>\ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30<\/a><\/span><\/li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>EDA<\/a><\/span><\/li><li><span><a href=\"#\ud559\uc2b5-\ubc0f-\uac80\uc99d-\ub370\uc774\ud130-\ubd84\ub9ac\" data-toc-modified-id=\"\ud559\uc2b5-\ubc0f-\uac80\uc99d-\ub370\uc774\ud130-\ubd84\ub9ac-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>\ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130 \ubd84\ub9ac<\/a><\/span><\/li><li><span><a href=\"#\ubaa8\ub378\ub9c1\" data-toc-modified-id=\"\ubaa8\ub378\ub9c1-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>\ubaa8\ub378\ub9c1<\/a><\/span><\/li><li><span><a href=\"#\ud559\uc2b5-\ubc0f-\ud3c9\uac00\" data-toc-modified-id=\"\ud559\uc2b5-\ubc0f-\ud3c9\uac00-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>\ud559\uc2b5 \ubc0f \ud3c9\uac00<\/a><\/span><\/li><li><span><a href=\"#\uacb0\uacfc-\uc81c\ucd9c\ud558\uae30\" data-toc-modified-id=\"\uacb0\uacfc-\uc81c\ucd9c\ud558\uae30-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>\uacb0\uacfc \uc81c\ucd9c\ud558\uae30<\/a><\/span><\/li><li><span><a href=\"#\ucc38\uace0\uc790\ub8cc\" data-toc-modified-id=\"\ucc38\uace0\uc790\ub8cc-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>\ucc38\uace0\uc790\ub8cc<\/a><\/span><\/li><\/ul><\/div>","3937134b":"## \uacb0\uacfc \uc81c\ucd9c\ud558\uae30\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","b2aa190a":"## \ub300\ud68c \uc18c\uac1c\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>\n\n<img src=\"https:\/\/cdn.pixabay.com\/photo\/2016\/11\/02\/14\/32\/lotte-world-tower-1791802_1280.jpg\" width=50%>\n\n\uad6d\ub0b4\uc5d0\ub294 \ub3c4\uc2dc\ubcc4\ub85c \ub2e4\uc591\ud55c \uc885\ub958\uc758 \ub79c\ub4dc\ub9c8\ud06c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uadf8 \uc911 \uc798 \uc124\uacc4\ud574\ub454 \ub79c\ub4dc\ub9c8\ud06c \ud558\ub098\ub294 \uc5f4 \uad00\uad11\uc9c0 \ubd80\ub7fd\uc9c0 \uc54a\uc740 \uacbd\uc81c\ud6a8\uacfc\ub97c \uc90d\ub2c8\ub2e4. \ub79c\ub4dc\ub9c8\ud06c\ub97c \ud1b5\ud55c \uad00\uad11 \uc218\uc785\uc740 \uc5f0\uac04 \uc218\uc870\uc5d0 \uc774\ub97c \ub9cc\ud07c \uad6d\uac00 \uacbd\uc81c\uc5d0 \ub9c9\ub300\ud55c \uc601\ud5a5\uc744 \ub07c\uce69\ub2c8\ub2e4. ","d2f31968":"## \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>\n\n\uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\ub294 391\uc885\uc758 \uc11c\uc6b8\uc2dc \ub79c\ub4dc\ub9c8\ud06c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uadf8 \uc911\uc5d0\uc11c \uc784\uc758\ub85c 30\uc885\uc758 \ub79c\ub4dc\ub9c8\ud06c\ub9cc \uc0d8\ud50c\ub9c1\ub418\uc5b4 \ub370\uc774\ud130\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4. \uac01 \ub79c\ub4dc\ub9c8\ud06c\ub2f9 100\uc5ec\uc7a5\uc758 \ub370\uc774\ud130\uac00 \uc81c\uacf5\ub429\ub2c8\ub2e4. \uadf8\ub807\uac8c \ucd1d 3,000\uc7a5\uc758 \ub370\uc774\ud130\uac00 \uc8fc\uc5b4\uc9d1\ub2c8\ub2e4.\n","8fa81a95":"## \ub77c\uc774\ube0c\ub7ec\ub9ac \ubc0f \uc635\uc158\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>\n\n","f7b93dbd":"## \ud559\uc2b5 \ubc0f \uac80\uc99d \ub370\uc774\ud130 \ubd84\ub9ac\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","17f9886e":"## EDA\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","9f66c4fc":"## \ubaa8\ub378\ub9c1\n<hr style=\"height:3px;border:none;color:#5F71F7;background-color:#203E60;\" \/>","d716a5ae":"-"}}