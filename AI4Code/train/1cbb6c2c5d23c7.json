{"cell_type":{"8b5f3b3c":"code","98c5b975":"code","4c682967":"code","7db8254f":"code","f5919c45":"code","a5afae26":"code","398a9e37":"code","e2c6af54":"code","3db9b0cd":"code","61624d8c":"code","d4b8be9d":"code","b9bb09f6":"code","453ee9ee":"code","39fa6999":"code","709f16b1":"code","eeb09f76":"code","7283ce74":"code","b4a0f37b":"code","52a7bc3d":"code","6dd5e73b":"code","335f64d6":"code","8ab19b80":"code","1428fb37":"code","5ae41f5e":"code","71d7641b":"code","b25ccd12":"code","3403167f":"code","2b11958a":"code","2c8881d3":"code","8a4ac5b5":"code","07eafa87":"code","0f47384a":"code","c619075c":"code","4652abcf":"code","6e5c027e":"code","7dd82313":"code","883e1039":"code","49e927b2":"code","3f3f04e9":"code","40a2e6b2":"code","3075b436":"code","d15fbc5e":"code","df69cc8c":"markdown","a4979382":"markdown","e85c4585":"markdown","8a12d280":"markdown","b75b7a7b":"markdown","67ccfcf8":"markdown","5752e198":"markdown","92035853":"markdown","1f6bba9a":"markdown","8364d475":"markdown","459db077":"markdown","6937f509":"markdown","4f7da5d4":"markdown","9ee708a2":"markdown","c8271cda":"markdown"},"source":{"8b5f3b3c":"%env SM_FRAMEWORK=tf.keras\n!pip install ..\/input\/segmentation-models-keras\/Keras_Applications-1.0.8-py3-none-any.whl --quiet\n!pip install ..\/input\/segmentation-models-keras\/image_classifiers-1.0.0-py3-none-any.whl --quiet\n!pip install ..\/input\/segmentation-models-keras\/efficientnet-1.0.0-py3-none-any.whl --quiet\n!pip install ..\/input\/segmentation-models-keras\/segmentation_models-1.0.1-py3-none-any.whl --quiet\n\nprint(\"Segmentation Models installed.\")","98c5b975":"DEBUG = True","4c682967":"# libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport io\nimport math\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport segmentation_models as sm\nfrom kaggle_datasets import KaggleDatasets\n\nprint(tf.__version__)","7db8254f":"enet_type = 'efficientnetb5'\nimage_size = 1024\nbatch_size = 16 # original is 4\ninit_lr = 1e-4\nwarmup_epo = 1\n# If DEBUG == True, only run 3 epochs per fold\ncosine_epo = 29 if not DEBUG else 2\nn_epochs = warmup_epo + cosine_epo\n\nVID = \"V12\"\nFOLD_I_LIST=[0]","f5919c45":"sm.set_framework('tf.keras')\ntf.keras.backend.set_image_data_format('channels_last')","a5afae26":"try: # detect TPUs\n    # NEW: in Tensorflow 2.4\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # otherwise detect GPUs\n    strategy = tf.distribute.MirroredStrategy() # single-GPU or multi-GPU\n    \nprint(f\"Running on {strategy.num_replicas_in_sync} replicas\")","398a9e37":"seg_masks = 'ranzcr-segmentation-masks'\nGCS_DS_PATH = KaggleDatasets().get_gcs_path(seg_masks)\n\nGCS_DS_PATH","e2c6af54":"def decode_image(image_bytes):\n    image = tf.image.decode_jpeg(image_bytes, channels=3)\n    return image\n\ndef decode_mask(mask_bytes):\n    mask = tf.io.decode_png(mask_bytes, channels=3)\n    return mask\n\ndef read_tfrecord(example):\n    TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'mask': tf.io.FixedLenFeature([], tf.string),\n        'fold': tf.io.FixedLenFeature([], tf.int64),\n    }\n    \n    example = tf.io.parse_single_example(example, TFREC_FORMAT)\n    image = decode_image(example['image'])\n    mask = decode_mask(example['mask'])\n    fold = example['fold']\n\n    image = tf.reshape(image, (image_size, image_size, 3))\n    mask = tf.reshape(mask, (image_size, image_size, 3))\n    return image, mask, fold\n\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=None)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=None)\n    return dataset","3db9b0cd":"tfrec_file_names = tf.io.gfile.glob(GCS_DS_PATH + '\/*.tfrec')\ntfrec_file_names = \\\n    [ tfrec_file_names[0] ] if DEBUG else tfrec_file_names\nraw_ds = load_dataset(tfrec_file_names)\n\nprint(raw_ds)","61624d8c":"folds_list = []\nfor _, _, fold_batch in raw_ds.batch(256):\n    print('.', end='', flush=True)\n    folds_list.append(fold_batch)\n\nfolds = np.concatenate(folds_list)\nfold, counts = np.unique(folds, return_counts=True)\nfold_count_dict = dict(zip(fold, counts))\n\nfold_count_dict","d4b8be9d":"def fold_train_count(fold_i):\n    counts = [ \n        count for fold, count in fold_count_dict.items() \\\n        if fold != fold_i ]\n    return sum(counts)\n\ndef fold_val_count(fold_i):\n    return fold_count_dict[fold_i]","b9bb09f6":"def scale_image_mask(image, mask):\n    image = tf.cast(image, dtype=tf.float32) \/ 255.0\n    # Value range of mask is 0..1, so type cast only.\n    mask = tf.cast(mask, dtype=tf.float32)\n    return image, mask","453ee9ee":"def check_aug(aug_fun, with_mask):\n    image, mask, _ = next(iter(raw_ds.take(1)))\n    image, mask = scale_image_mask(image, mask)\n    \n    plt.figure(figsize=(12, 4))\n    rows = 2\n    cols = 5\n    aug_masks = []\n    for p in range(rows*cols):\n        aug_image, aug_mask = aug_fun(image, mask)\n        aug_masks.append(aug_mask)\n        \n        plt.subplot(rows, cols, p+1)\n        plt.imshow(aug_image)\n        plt.axis(\"off\")\n    plt.tight_layout()\n    plt.show()        \n    \n    if with_mask:\n        plt.figure(figsize=(12, 4))\n        for p, aug_mask in enumerate(aug_masks):\n            plt.subplot(rows, cols, p+1)\n            plt.imshow(aug_mask)\n            plt.axis(\"off\")\n        plt.tight_layout()\n        plt.show()","39fa6999":"def random_float(minval=0.0, maxval=1.0):\n    rnd = tf.random.uniform(\n        [], minval=minval, maxval=maxval, dtype=tf.float32)\n    return rnd\n\ndef choice(p, image1, mask1, image2, mask2):\n    rnd = random_float()\n    image = tf.where(rnd <= p, image1, image2)\n    mask = tf.where(rnd <= p, mask1, mask2)\n    return image, mask","709f16b1":"def mirror_boundary(v, max_v):\n    # v % (max_v*2.0-2.0) ==> v % (512*2-2) ==> [0..1022]\n    # [0..1022] - (max_v-1.0) ==> [0..1022] - 511 ==> [-511..511]\n    # -1.0 * abs([-511..511]) ==> [-511..0]\n    # [-511..0] + max_v - 1.0 ==> [-511..0] + 511 ==> [0..511]\n    mirror_v = -1.0 * tf.math.abs(\n        v % (max_v*2.0-2.0) - (max_v-1.0)) + max_v-1.0\n    return mirror_v\n\ndef clip_boundary(v, max_v):\n    clip_v = tf.clip_by_value(v, 0.0, max_v-1.0)\n    return clip_v\n\ndef interpolate_bilinear(image, map_x, map_y):\n    def _gather(image, map_x, map_y):\n        map_stack = tf.stack([map_x, map_y]) # [ 2, height, width ]\n        map_indices = tf.transpose(\n            map_stack, perm=[1, 2, 0])       # [ height, width, 2 ]\n        map_indices = tf.cast(map_indices, dtype=tf.int32)\n        gather_image = tf.gather_nd(image, map_indices)\n        return gather_image\n    \n    ll = _gather(image, tf.math.floor(map_x), tf.math.floor(map_y))\n    lr = _gather(image, tf.math.ceil(map_x), tf.math.floor(map_y))\n    ul = _gather(image, tf.math.floor(map_x), tf.math.ceil(map_y))\n    ur = _gather(image, tf.math.ceil(map_x), tf.math.ceil(map_y))\n    \n    fraction_x = tf.expand_dims(map_x % 1.0, axis=-1) # [h, w, 1]\n    int_l = (lr - ll) * fraction_x + ll\n    int_u = (ur - ul) * fraction_x + ul\n    \n    fraction_y = tf.expand_dims(map_y % 1.0, axis=-1) # [h, w, 1]\n    interpolate_image = (int_u - int_l) * fraction_y + int_l\n    return interpolate_image\n\ndef remap(image, height, width, map_x, map_y, mode):\n    assert \\\n        mode in ('mirror', 'constant'), \\\n        \"mode is neither 'mirror' nor 'constant'\"\n\n    height_f = tf.cast(height, dtype=tf.float32)\n    width_f = tf.cast(width, dtype=tf.float32)\n    map_x = tf.reshape(map_x, shape=[height, width])\n    map_y = tf.reshape(map_y, shape=[height, width])\n    if mode == 'mirror':\n        b_map_x = mirror_boundary(map_x, width_f)\n        b_map_y = mirror_boundary(map_y, height_f)\n    else:\n        b_map_x = clip_boundary(map_x, width_f)\n        b_map_y = clip_boundary(map_y, height_f)\n        \n    image_remap = interpolate_bilinear(image, b_map_x, b_map_y)\n    \n    if mode == 'constant':\n        map_stack = tf.stack([map_x, map_y])\n        map_indices = tf.transpose(map_stack, perm=[1, 2, 0])\n        x_ge_0 = (0.0 <= map_indices[ : , : , 0])    # [h, w]\n        x_lt_w = (map_indices[ : , : , 0] < width_f)\n        y_ge_0 = (0.0 <= map_indices[ : , : , 1])\n        y_lt_h = (map_indices[ : , : , 1] < height_f)\n        inside_boundary = tf.math.reduce_all(\n            tf.stack([x_ge_0, x_lt_w, y_ge_0, y_lt_h]), axis=0) # [h, w]\n        inside_boundary = inside_boundary[ : , : , tf.newaxis]  # [h, w, 1]\n        image_remap = tf.where(inside_boundary, image_remap, 0.0)\n\n    return image_remap","eeb09f76":"def HorizontalFlip(p):\n    def _do_horizontal_flip(image, mask):\n        aug_image = tf.image.flip_left_right(image)\n        aug_mask = tf.image.flip_left_right(mask)\n        return choice(p, aug_image, aug_mask, image, mask)\n    return _do_horizontal_flip","7283ce74":"horizontal_flip = HorizontalFlip(p=0.5)\ncheck_aug(horizontal_flip, with_mask=True)","b4a0f37b":"def RandomBrightness(max_delta, p):\n    def _do_random_brightness(image, mask):\n        aug_image = tf.image.random_brightness(image, max_delta)\n        return choice(p, aug_image, mask, image, mask)\n    return _do_random_brightness","52a7bc3d":"random_brightness = RandomBrightness(max_delta=0.2, p=0.75)\ncheck_aug(random_brightness, with_mask=False)","6dd5e73b":"def affine_transform(height, width, tx, ty, z, theta):\n    cx = (width - 1.0) * 0.5\n    cy = (height - 1.0) * 0.5\n    \n    center_shift_mat = tf.convert_to_tensor([\n        [1.0, 0.0, -cx],\n        [0.0, 1.0, -cy],\n        [0.0, 0.0, 1.0]], dtype=tf.float32)\n    trans_mat = center_shift_mat\n    \n    rot_rad = -2.0 * math.pi * theta \/ 360.0\n    roration_mat = tf.convert_to_tensor([\n        [tf.math.cos(rot_rad), tf.math.sin(rot_rad), 0.0],\n        [-tf.math.sin(rot_rad), tf.math.cos(rot_rad), 0.0],\n        [0.0, 0.0, 1.0]], dtype=tf.float32)\n    trans_mat = tf.linalg.matmul(roration_mat, trans_mat)\n    \n    shift_mat = tf.convert_to_tensor([\n        [1.0, 0.0, cx - tx],\n        [0.0, 1.0, cy - ty],\n        [0.0, 0.0, 1.0]], dtype=tf.float32)\n    trans_mat = tf.linalg.matmul(shift_mat, trans_mat)\n\n    zoom_mat = tf.convert_to_tensor([\n        [1.0 \/ z, 0.0, 0.0],\n        [0.0, 1.0 \/ z, 0.0],\n        [0.0, 0.0, 1.0]], dtype=tf.float32)\n    trans_mat = tf.linalg.matmul(zoom_mat, trans_mat)\n    \n    h_rng = tf.range(height, dtype=tf.float32)\n    w_rng = tf.range(width, dtype=tf.float32)\n    y, x = tf.meshgrid(h_rng, w_rng)\n    x = tf.reshape(x, [-1])\n    y = tf.reshape(y, [-1])\n    ones = tf.ones_like(x)\n    coord_mat = tf.stack([x, y, ones])\n    \n    res_mat = tf.linalg.matmul(trans_mat, coord_mat)\n    map_x = res_mat[0]\n    map_y = res_mat[1]\n    return map_x, map_y","335f64d6":"def ShiftScaleRotate(\n        shift_limit, scale_limit, rotate_limit, p):\n    def _do_shift_scale_rotate(image, mask):\n        image_shape = tf.shape(image)\n        height_i = image_shape[0]\n        width_i = image_shape[1]\n        height_f = tf.cast(height_i, dtype=tf.float32)\n        width_f = tf.cast(width_i, dtype=tf.float32)\n        tx = width_f * random_float(-shift_limit, shift_limit)\n        ty = height_f * random_float(-shift_limit, shift_limit)\n        z = random_float(1.0 - scale_limit, 1.0 + scale_limit)\n        theta = random_float(-rotate_limit, rotate_limit)\n\n        map_x, map_y = affine_transform(\n            height_f, width_f, tx, ty, z, theta)\n        aug_image = remap(\n            image, height_i, width_i, map_x, map_y, mode='constant')\n        aug_mask = remap(\n            mask, height_i, width_i, map_x, map_y, mode='constant')\n        return choice(p, aug_image, aug_mask, image, mask)\n    return _do_shift_scale_rotate","8ab19b80":"shift_scale_rotate = ShiftScaleRotate(\n    shift_limit=0.2, scale_limit=0.3, rotate_limit=30, p=0.75)\ncheck_aug(shift_scale_rotate, with_mask=True)","1428fb37":"def randints(shape, minval, maxval):\n    # maxval+1 to include maxval for the result.\n    # generated range is [minval, maxval) (maxval is not included)\n    return tf.random.uniform(\n        shape=shape, minval=minval, maxval=maxval+1, dtype=tf.int32)\n\ndef make_range_masks(size, starts, ends):\n    indice = tf.range(size, dtype=tf.int32)\n    start_masks = (\n        starts[ : , tf.newaxis] <= indice[  tf.newaxis, : ])\n    end_masks = (\n        indice[ tf.newaxis, : ] <= ends[ : , tf.newaxis])\n    range_masks = start_masks & end_masks\n    return range_masks\n\ndef make_region_mask(tops, lefts, bottoms, rights):\n    row_masks = make_range_masks(image_size, tops, bottoms)\n    col_masks = make_range_masks(image_size, lefts, rights)\n    region_masks = \\\n        row_masks[ : , : , tf.newaxis ] & \\\n        col_masks[ : , tf.newaxis, : ]\n    region_mask = tf.math.reduce_any(region_masks, axis=0)\n    region_mask = region_mask[ : , : , tf.newaxis]\n    return region_mask\n\ndef Cutout(num_cuts, mask_factor, p):\n    def _do_cutout(image, mask):\n        image_shape = tf.shape(image)\n        height_i = image_shape[0]\n        width_i = image_shape[1]\n        height_f = tf.cast(height_i, dtype=tf.float32)\n        width_f = tf.cast(width_i, dtype=tf.float32)\n        cut_h = tf.cast(height_f * mask_factor, dtype=tf.int32)\n        cut_w = tf.cast(width_f * mask_factor, dtype=tf.int32)\n\n        y_centers = randints([num_cuts], 0, image_size - 1)\n        x_centers = randints([num_cuts], 0, image_size - 1)\n        tops = tf.math.maximum(y_centers - cut_h\/\/2, 0)\n        lefts = tf.math.maximum(x_centers - cut_w\/\/2, 0)\n        bottoms = tf.math.minimum(tops + cut_h, height_i - 1)\n        rights = tf.math.minimum(lefts + cut_w, width_i - 1)\n\n        cut_region = make_region_mask(tops, lefts, bottoms, rights)\n        mask_value = tf.constant(0.0, dtype=tf.float32)\n        aug_image = tf.where(cut_region, mask_value, image)\n        return choice(p, aug_image, mask, image, mask)\n    return _do_cutout","5ae41f5e":"cut_out = Cutout(num_cuts=1, mask_factor=0.3, p=0.75)\ncheck_aug(cut_out, with_mask=False)","71d7641b":"def do_augment(image, mask):\n    image, mask = horizontal_flip(image, mask)\n    image, mask = random_brightness(image, mask)\n    image, mask = shift_scale_rotate(image, mask)\n    image, mask = cut_out(image, mask)\n    return image, mask","b25ccd12":"def select_train(ds, fold):\n    ds = ds.filter(lambda im, ms, f: f != fold)\n    return ds\n    \ndef select_val(ds, fold):\n    ds = ds.filter(lambda im, ms, f: f == fold)\n    return ds","3403167f":"def drop_fold(image, mask ,fold):\n    return image, mask\n\n# Actual mask is 2 channel, the last one is added for encoding as PNG.\ndef drop_mask_channel(image, mask):\n    mask = mask[ : , : , :-1 ]\n    return image, mask","2b11958a":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef build_dataset(\n        dset, augment=True, repeat=True, shuffle=1024):\n    dset = dset.map(drop_fold, num_parallel_calls=AUTOTUNE)\n    dset = dset.repeat() if repeat else dset\n    dset = dset.map(scale_image_mask, num_parallel_calls=AUTOTUNE)\n    dset = dset.map(\n        do_augment, num_parallel_calls=AUTOTUNE) if augment else dset\n    dset = dset.map(drop_mask_channel, num_parallel_calls=AUTOTUNE)\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(batch_size)\n    dset = dset.prefetch(AUTOTUNE)\n    return dset","2c8881d3":"def make_datasets(fold_i):\n    train_ds = select_train(raw_ds, fold_i)\n    train_ds = build_dataset(\n        train_ds, augment=True, repeat=True, shuffle=1024)\n\n    val_ds = select_val(raw_ds, fold_i)\n    val_ds = build_dataset(\n        val_ds, augment=False, repeat=False, shuffle=None)\n\n    train_steps = fold_train_count(fold_i) \/\/ batch_size\n    val_steps = fold_val_count(fold_i) \/\/ batch_size\n\n    return train_ds, val_ds, train_steps, val_steps","8a4ac5b5":"train_ds, val_ds, train_steps, val_steps = make_datasets(0)","07eafa87":"from pylab import rcParams\n\ndef visualize_images(ds):\n    rcParams['figure.figsize'] = 20,10\n\n    f, axarr = plt.subplots(1,5)\n    masks = []\n    ds_iter = iter(ds.unbatch())\n    for p in range(5):\n        img, mask = next(ds_iter)\n        axarr[p].imshow(img)\n        masks.append(mask)\n\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        axarr[p].imshow(masks[p][ : , : , 0])\n\n    f, axarr = plt.subplots(1,5)\n    for p in range(5):\n        axarr[p].imshow(masks[p][ : , : , 1])","0f47384a":"visualize_images(train_ds)","c619075c":"visualize_images(val_ds)","4652abcf":"def make_model():\n    with strategy.scope(): \n        unet = sm.Unet(\n            enet_type, encoder_weights='imagenet',\n            classes=2, activation='sigmoid')\n        \n        inputs = tf.keras.Input(\n            shape=(image_size, image_size, 3), name=\"inputs\")\n        outputs = unet(inputs)\n        model = tf.keras.Model(\n            inputs=inputs, outputs=outputs, name=\"seg_model\")\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss='binary_crossentropy',\n        metrics=['accuracy'],\n        # overheads and allows the XLA compiler to unroll the loop on TPU\n        # and optimize hardware utilization.\n        steps_per_execution=8)\n    model.summary()\n\n    return model","6e5c027e":"model = make_model()","7dd82313":"from matplotlib import pyplot as plt\n\nLR_START = init_lr\nLR_MAX = 1e-3\nLR_MIN = 1e-5\nLR_RAMPUP_EPOCHS = warmup_epo\nLR_SUSTAIN_EPOCHS = 0\nEPOCHS = n_epochs\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        decay_total_epochs = EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n        decay_epoch_index = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        phase = math.pi * decay_epoch_index \/ decay_total_epochs\n        cosine_decay = 0.5 * (1 + math.cos(phase))\n        lr = (LR_MAX - LR_MIN) * cosine_decay + LR_MIN\n    return lr\n\nrng = [i for i in range(EPOCHS)]\nlr_y = [lrfn(x) for x in rng]\nplt.figure(figsize=(10, 4))\nplt.plot(rng, lr_y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\". \\\n      format(lr_y[0], max(lr_y), lr_y[-1]))","883e1039":"cb_monitor = 'val_accuracy'\n\nclass RestoreBestWeights(tf.keras.callbacks.Callback):\n    def __init__(self):\n        super(RestoreBestWeights, self).__init__()\n        self.best_monitor = -np.Inf\n        self.best_weights = None\n        self.best_epoch = None\n        \n    def on_epoch_end(self, epoch, logs=None):\n        current_monitor = logs.get(cb_monitor)\n        if current_monitor > self.best_monitor:\n            self.best_monitor = current_monitor\n            self.best_weights = self.model.get_weights()\n            self.best_epoch = epoch\n            \n    def on_train_end(self, logs=None):\n        print(\"Restoring best weights on epoch {0}, {1} was {2:.5f}\".format(\n            self.best_epoch + 1, cb_monitor, self.best_monitor))\n        self.model.set_weights(self.best_weights)","49e927b2":"def make_callbacks(fold_i):\n    best_model_file_name = \"seg_model_{0}_{1}.hdf5\".format(VID, fold_i)\n    cb_mode = 'max'\n    cb_min_delta = 1e-4\n    cb_verbose = 1\n\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        best_model_file_name, save_best_only=True,\n        save_weights_only=False, monitor=cb_monitor, mode=cb_mode,\n        verbose=cb_verbose)\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = False)\n    restore_best_weights = RestoreBestWeights()\n    \n    return checkpoint, lr_callback, restore_best_weights","3f3f04e9":"def fit_one_fold(fold_i):\n    train_dataset, val_dataset, train_steps, val_steps = make_datasets(fold_i)\n    checkpoint, lr_callback, restore_best_weights = make_callbacks(fold_i)\n\n    history = model.fit(\n        train_dataset, \n        epochs=EPOCHS,\n        verbose=1,\n        callbacks=[checkpoint, lr_callback, restore_best_weights],\n        steps_per_epoch=train_steps,\n        validation_data=val_dataset,\n        validation_steps=val_steps)\n    return history","40a2e6b2":"def plot_history(history, title, labels, subplot):\n    plt.subplot(*subplot)\n    plt.title(title)\n    for label in labels:\n        plt.plot(history.history[label], label=label)\n    plt.legend()","3075b436":"def plot_fit_result(history):\n    plt.figure(figsize=(12, 4))\n    plot_history(history, \"Loss\", ['loss', 'val_loss'], (1, 2, 1))\n    plot_history(history, \"Accuracy\", ['accuracy', 'val_accuracy'], (1, 2, 2))\n    plt.show()","d15fbc5e":"for fold_i in FOLD_I_LIST:\n    print(\"####################\")\n    print(\"# Fold {0}\".format(fold_i))\n    history = fit_one_fold(fold_i)\n    plot_fit_result(history)","df69cc8c":"### Cutout","a4979382":"## Define Dataset","e85c4585":"## Dataset 2","8a12d280":"### HorizontalFlip","b75b7a7b":"## Install Segmentation Models Locally","67ccfcf8":"## TPU","5752e198":"## Model","92035853":"Count the number of data in each folds for train and validation.","1f6bba9a":"This notebook is the second part of [RANZCR 1st Place Solution by TF](https:\/\/www.kaggle.com\/tt195361\/ranzcr-1st-place-solution-by-tf-1-make-masks), training of the segmentation model. This notebook is based on [RANZCR 1st Place Soluiton Seg Model (small ver.)](https:\/\/www.kaggle.com\/haqishen\/ranzcr-1st-place-soluiton-seg-model-small-ver).\n\nFor the segmentation model, the original notebook uses UnetPlusPlus in [segmentation_models_pytorch](https:\/\/github.com\/qubvel\/segmentation_models.pytorch). At first, I tried to convert it to Keras by using torch.onnx.export() and [onnx2keras](https:\/\/github.com\/nerox8664\/onnx2keras) in this [version](https:\/\/www.kaggle.com\/tt195361\/ranzcr-1st-place-solution-by-tf-2-seg-model?scriptVersionId=57986327). But the training of this model didn't improve accuracy. So, I changed to use Unet in [Segmentation Models](https:\/\/github.com\/qubvel\/segmentation_models).\n\nI selected EfficientNetB5 for the base of Unet. The original notebook uses EfficientNetB1.\n\nFor data augmentation, the original notebook uses methods in [Albumentations](https:\/\/github.com\/albumentations-team\/albumentations). I made similar one by Tensorflow.\n\nIt took about 4 hours and 30 minutes on TPU to run 1 fold. So some number of sessions are necessary to run a set of folds.\n\nThe plots below are sample training history of this model. They are from [Version 10](https:\/\/www.kaggle.com\/tt195361\/ranzcr-1st-place-solution-by-tf-2-seg-model?scriptVersionId=61479593). The accuracy grew up to around 80% gradually. CV result of classification model by using this result is about 0.964 ~ 0.967.\n\n![image.png](attachment:76bc47c5-209a-4058-a010-bd4ab405e866.png)\n\nIn some trainings, the accuracy became to almost 100% in a few epochs, then kept decreasing. It looks unusual. The plots below are the history of [Version 9](https:\/\/www.kaggle.com\/tt195361\/ranzcr-1st-place-solution-by-tf-2-seg-model?scriptVersionId=61377764). CV result from this result is about 0.948, worse than the above one.\n\n![image.png](attachment:4325b7b5-63a6-4ca5-9707-339ef46203c3.png)\n","8364d475":"[Tensorflow 2.4 for TPUs released](https:\/\/www.kaggle.com\/product-feedback\/216256)","459db077":"## Visualization","6937f509":"### RandomBrightness","4f7da5d4":"### ShiftScaleRotate","9ee708a2":"## Augumentations\n\n### Utilities","c8271cda":"## Config and Libraries"}}