{"cell_type":{"a9c33e01":"code","d453644f":"code","855ea810":"code","be50ca77":"code","7cc63957":"code","9203f3f2":"code","7a9b9679":"code","4802f122":"code","69c2aae9":"code","0bd980e9":"code","0c280324":"code","216f4881":"code","dd5e7aca":"code","d95016be":"code","ee1b999d":"code","673214e8":"code","88b8aa41":"code","fd78c8c7":"code","a37fc352":"code","27da23f1":"code","6e24f81b":"code","7d410556":"code","b3504483":"code","6478766d":"code","ee9aedb2":"code","828f50ea":"code","c63f9648":"code","c60a2320":"code","0d44c220":"code","1ab0c9e4":"code","24cc688c":"code","c73bda1e":"code","1f27589c":"code","eb923a6e":"code","681d8d73":"code","1c46d497":"code","9199bce8":"code","07b71ddd":"code","2254fa1d":"code","d76d489e":"code","aaa41c0d":"code","dd90d256":"code","fc2084ee":"code","e98fc47f":"markdown","2c15b015":"markdown","0b91471f":"markdown","de4c8b44":"markdown","9df368ce":"markdown","77818b48":"markdown","62141650":"markdown","952ad85a":"markdown","01bd1ae0":"markdown","aee1aead":"markdown","5f19eeb7":"markdown","ec71e3d3":"markdown","f893ee27":"markdown","a38deb7d":"markdown","6a38c8ef":"markdown","ff9e254b":"markdown","ab59c002":"markdown","19872b7e":"markdown","999a0e6d":"markdown","d8550772":"markdown","b67fdeb4":"markdown","7cb5f9cd":"markdown","712bfd1f":"markdown","a71d7dca":"markdown"},"source":{"a9c33e01":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d453644f":"# importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# removing warnings\nimport sys\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n# depicting tree predictions\n!pip install pydotplus\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\n\n# secrets\n# from kaggle_secrets import UserSecretsClient\n# secret_label = \"notebook_secret\"\n# secret_value = UserSecretsClient().get_secret()","855ea810":"import pandas as pd\ndf = pd.read_csv(\"..\/input\/avocado-prices\/avocado.csv\")\ndf.head()","be50ca77":"## selecting a subset of df using a column condition\nalbany_df = df[df['region'] == 'Albany'].copy()\nalbany_df.head()","7cc63957":"## We want to change the index column from 'Unnamed: 0' to 'Date'\nalbany_df.set_index('Date') # but we haven't changed the original dataset","9203f3f2":"albany_df.head()\n# date is not the index column","7a9b9679":"albany_df.set_index('Date', inplace=True) # to change data in original database ","4802f122":"# plotting all the columns in terms with the date\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (20,8))\nalbany_df.drop(['Unnamed: 0'], axis=1, inplace=True)\nalbany_df.plot(figsize=(20,8), legend=True) \n\n# problem is, pandas couldn't guessed that the index was of type date","69c2aae9":"# changing the datatype of index to datetime\nalbany_df.index = pd.to_datetime(albany_df.index)","0bd980e9":"albany_df['AveragePrice'].plot()","0c280324":"# rolling mean to smoothen the data\nalbany_df['AveragePrice'].rolling(25).mean().plot()","216f4881":"# sorting the index\nalbany_df.sort_index(inplace=True)\nalbany_df['AveragePrice'].rolling(25).mean().plot()\n# now we can see a clear perception of what is happing to price over time","dd5e7aca":"# So we will store the rolling average in a new column\nalbany_df['rolling_mean_25'] = albany_df['AveragePrice'].rolling(25).mean()\nalbany_df.head()","d95016be":"df['region'].unique()","ee1b999d":"# seperating out each region as dataframe\n\ngraph_df = pd.DataFrame()\n\nfor region in df['region'].unique()[:15]: # otherwise we will run out of memory\n    print(region)\n    \n    region_df = df.copy()[df['region'] == region] # those rows which belong to current region\n    region_df.set_index(\"Date\", inplace=True)\n    region_df.sort_index(inplace=True) # so that, the dates get sorted in porper order\n    region_df[f'{region}_price_25_MA'] = region_df['AveragePrice'].rolling(25).mean()\n    \n    if graph_df.empty:\n        graph_df = region_df[[f'{region}_price_25_MA']] # to return a dataframe we used 2 square brackets\n    else:\n        graph_df = graph_df.join(region_df[f'{region}_price_25_MA'])\n        \n# the reason we run out of memory is the presence of two type with same dates","673214e8":"graph_df.head()","88b8aa41":"df = pd.read_csv(\"..\/input\/avocado-prices\/avocado.csv\")\ndf = df.copy()[df['type'] == 'organic']\ndf['Date'] = pd.to_datetime(df['Date']) \n\ndf.sort_values(by=['Date'], ascending=True, inplace=True)\ndf","fd78c8c7":"# seperating out each region as dataframe\n# this time we just considered only one type, so no duplicate dates\n\ngraph_df = pd.DataFrame()\n\nfor region in df['region'].unique(): \n#     print(region)\n    \n    region_df = df.copy()[df['region'] == region] # those rows which belong to current region\n    region_df.set_index(\"Date\", inplace=True)\n    region_df.sort_index(inplace=True) # so that, the dates get sorted in porper order\n    region_df[f'{region}_price_25_MA'] = region_df['AveragePrice'].rolling(25).mean()\n    \n    if graph_df.empty:\n        graph_df = region_df[[f'{region}_price_25_MA']] # to return a dataframe we used 2 square brackets\n    else:\n        graph_df = graph_df.join(region_df[f'{region}_price_25_MA'])\n\ngraph_df","a37fc352":"# Now plotting the graph without matplotlib\ngraph_df.plot(figsize=(20,8), legend=False)","27da23f1":"round((df.isnull().sum() \/ df.shape[0])*100, 2)","6e24f81b":"# a bit dig into data\ndf = pd.read_csv(\"..\/input\/avocado-prices\/avocado.csv\")\n\ndef data_examination(data):\n    # basic examination\n    display(data.head())\n    display(data.info())\n    display(data.describe(include='all'))\n    display(data.columns)\n    \n    # duplicate data rows\n    duplicates = data.duplicated().sum() # to check if there is any duplicated data\n    if duplicates == 0:\n        print(\"\\nThere are no duplicated entries\\n\")\n    else:\n        print(f'\\nThere are {duplicates} duplicates.\\n')\n        \n    \n    ## missing data check\n    data_missing = pd.DataFrame(round((data.isnull().sum() \/ \n                                     data.shape[0])*100, 2))\n    \n    if data_missing[0].sum() > 0:\n        print(f'\\nMissing values are\\n')\n        data_missing.plot(kind='bar')\n    else:\n        print(f'\\nThere are no missing values in dataframe\\n')\n        \n        \n    ## unique values\n    for col in data.columns:\n        if data[col].dtype == 'object' or data[col].dtype == 'str':\n            print(data[col].unique())\n            \n\ndata_examination(df)  ","7d410556":"df.columns = df.columns.str.lower()\ndf.drop(['unnamed: 0'], axis=1, inplace=True)\ndf.columns","b3504483":"for col in df.columns:\n    # convert the date column to datetime datatype\n    if col == 'date':\n        df[col] = pd.to_datetime(df[col])\n        \n    elif df[col].dtype == 'object': # changing to category\n        df[col] = df[col].astype('category')\n        ","6478766d":"numeric_cols = [col for col in df.columns if df[col].dtype in ['int64', 'float64']]\nnumeric_cols.remove('year')\n\ncategorical_cols = ['region', 'type']\n\ndate_cols = ['date', 'year']\ndisplay(df.info())","ee9aedb2":"def dist_custom(data, column_list, rows, cols, suptitle):\n    fig, axs = plt.subplots(rows, cols, figsize=(16,16))\n    \n    fig.suptitle(suptitle, y = 0.92, size=16)\n    \n    axs = axs.flatten()\n    \n    for i, col in enumerate(column_list):\n        sns.distplot(data[col], ax=axs[i])\n        axs[i].set_title(col + ', skewness is ' + str(round(data[col].skew(axis=0, skipna = True), 2)))\n#         fig.tight_layout()\n# printing custom plots\ndist_custom(df, numeric_cols, 3, 3, 'Distribution of numericl variables') #3 * 3 = 9 numeric columns","828f50ea":"def boxplots_custom(data, col_list, rows, cols, suptitle):\n    fig, axs = plt.subplots(rows, cols, sharey = True, figsize = (16, 16))\n    fig.suptitle(suptitle, y=0.93, size=16)\n    \n    axs = axs.flatten()\n    \n    for i, col in enumerate(col_list):\n        if i % 3 == 0:\n            axs[i].set_ylabel('The number of entries')\n        \n        sns.boxplot(data = data[col], orient = 'h', ax = axs[i])\n        axs[i].set_title(col)\n        \nboxplots_custom(df, numeric_cols, 3, 3, \"Boxplots before deleting outliers\")","c63f9648":"df.shape","c60a2320":"q1 = df[numeric_cols].quantile(0.25)\nq3 = df[numeric_cols].quantile(0.75)\niqr = q3 - q1 # iqr for each numeric columns\n\n# seleting only those rows, value is neither less nor greater than the IQR proximity rule.\ndf_filtered = df[~((df[numeric_cols] < (q1 - 1.5 * iqr))\n                  |\n                  (df[numeric_cols] > (q3 + 1.5 * iqr))).any(axis=1)]\n\ndisplay(df_filtered.shape)","0d44c220":"18249 - 11538","1ab0c9e4":"boxplots_custom(df_filtered, numeric_cols, 3, 3, \"Boxplots before deleting outliers\")","24cc688c":"df_filtered['month'] = df_filtered['date'].astype('datetime64[M]')\ndf_filtered['week'] = df_filtered['date'].astype('datetime64[W]')","c73bda1e":"df_filtered.head()","1f27589c":"df_filtered['averageprice'].describe()","eb923a6e":"df_filtered['averageprice'].hist(bins=100)","681d8d73":"bins = [0.48, 1.52, 1.78, 1.9, 2.49] # low, ... , high\nlabels = ['low', 'medium', 'high', 'expensive']\n\ndf_filtered['price_tag'] = pd.cut(df_filtered['averageprice'], bins = bins, labels = labels)\ndf_filtered.head()","1c46d497":"df_filtered.head()","9199bce8":"# plotly plots\nfig = make_subplots(rows = 2, cols = 2, subplot_titles = (\"Daily avg prices\", \"Weekly avg prices\", \"Monthly avg prices\", \"Yearly avg prices\"))\n\ndatasets = []\n\nfor time_period in ['date', 'week', 'month', 'year']:\n    datasets.append(round(df_filtered.groupby(time_period)['averageprice'].mean().reset_index(), 3))\n    \nrow, col = 1, 1\n\nfor idx, data in enumerate(datasets):\n    fig.add_trace(go.Scatter(x = data.iloc[:,0], y = data['averageprice']), row = row, col = col)\n    \n    fig.update_xaxes(title_text = 'Per '+data.iloc[:,0].name, row = row, col = col)\n    \n    fig.update_yaxes(title_text = 'The sum of average price', row = row, col = col)\n    \n    if idx == 1:\n        row, col = 2, 1\n    else:\n        col += 1\n\n# update title and height\nfig.update_layout(showlegend = False, title_text = \"customizing subplot axes\", height=700)\nfig.show()","07b71ddd":"# Initialize figure with subplots\nfig = make_subplots(rows = 1, cols = 2, subplot_titles = (\"Daily avg prices\", \"Monthly avg prices\"))\n\ndatasets = []\n\nfor time_period in ['date', 'month']:\n    for types in ['conventional', 'organic']:\n        datasets.append(round(df_filtered.query('type == @types').groupby(time_period)['averageprice'].mean().reset_index(), 3))\n\nrow, col = 1, 1\nlegend_ = ['conventional', 'organic']\n        \nfor idx, data in enumerate(datasets):\n    fig.add_trace(go.Scatter(x = data.iloc[:, 0], y = data['averageprice'], \n                            name = legend_[idx % 2] + ' per ' + data.iloc[:, 0].name), row = row, col =col)\n    \n    fig.update_xaxes(title_text = 'Per '+ data.iloc[:,0].name, row=row, col=col)\n    \n    fig.update_yaxes(title_text = 'The sum of average price', row=row, col=col)\n    \n    \n    if idx == 1:\n        col += 1\n\n# updating plot title\nfig.update_layout(showlegend=True, title_text = \"Daily and monthly avg prices for conventional and organic types\", height = 700)\n\nfig.show()\n        \n        ","2254fa1d":"# Histograms\n\nhist_graphs = df_filtered.hist(numeric_cols, figsize=(16,10), bins = 20)\n\nplt.suptitle(\"Hists after removing outliers\", y=0.96, size = 16)\n\nfor axs in hist_graphs.flatten():\n    axs.set_ylabel('frequency')\nplt.show()","d76d489e":"regions = df_filtered.groupby(['region'])['total volume'].sum().sort_values(ascending=False).reset_index()\n\nfig = px.bar(regions, x='region', y='total volume', title='Regions with the greatest total volume over the time')\nfig.show()","aaa41c0d":"types = df_filtered.groupby(['price_tag', 'type'])['date'].count().reset_index()\n\nplt.figure(figsize=(10,5))\n\nsns.countplot(x = 'price_tag', alpha = 0.7, hue = 'type', data = df_filtered)\n\nplt.legend(bbox_to_anchor = (1.1, 1.1), loc = 'upper left')\nplt.xlabel('Price tag'),\nplt.ylabel('Amount')\nplt.title('Relation between type and average price', size=16, y=1.01)","dd90d256":"import seaborn as sns\n\ncols_considered = ['averageprice', 'total volume', '4046', '4225', '4770', 'type']\n\npair_plots = sns.pairplot(data = df_filtered[cols_considered], hue = 'type')\n\npair_plots.fig.suptitle(\"Distributions and scatter plots for each variable depending on type\", y=1.01, size=16)\n\nfor ax in pair_plots.axes.flat:\n    ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)\n\nplt.show()","fc2084ee":"# plotting correlation matrix\ncorr = df_filtered.corr()\n\n# lets just plot the lower triangle, as values are symmetric along the diagonal\nlow_part = np.triu(corr)\n\n# plotting\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, fmt = '.2g', annot = True, mask = low_part)\nplt.title(\"correlation matrix\", size=16)\nplt.show()","e98fc47f":"### EDA: Exploratory Data Analysis","2c15b015":"#### Adding columns (additional)\n* month \n* week","0b91471f":"### Reading dataframe again, this time fixing a type","de4c8b44":"#### Correlation among certain variables","9df368ce":"### Observations:\n* There are a total of 18,249 entries and 14 columns\n* No missing or duplicated values (rows) observed\n* Unique values seems appropriate","77818b48":"#### Detecting Outliers","62141650":"### Some relevant columns in the dataset:\n\n* Date - The date of the observation\n* AveragePrice - the average price of a single avocado\n* type - conventional or organic\n* year - the year\n* Region - the city or region of the observation\n* Total Volume - Total number of avocados sold\n* 4046 - Total number of avocados with PLU 4046 sold\n* 4225 - Total number of avocados with PLU 4225 sold\n* 4770 - Total number of avocados with PLU 4770 sold","952ad85a":"#### Removing outliers: using IQR we will remove outliers","01bd1ae0":"## Basic Analysis","aee1aead":"#### A dig dive EDA","5f19eeb7":"### Renaming columns and droping unnecesary ones","ec71e3d3":"#### Changing data types","f893ee27":"#### So far, we have\n* renamed columns and dropped unnecessary column ('Unnamed: 0')\n* changed dtypes\n* observed skewness for numeric columns\n* observed outliers using boxplots, and removed them\n* added few columns for EDA","a38deb7d":"##### Observations: all the numeric features are skewed to the right. Let's examine outliers with the use of boxplots. ","6a38c8ef":"#### Filtering averagePrice into 4 categories","ff9e254b":"##### Plotting data for conventional and organic data","ab59c002":"## Data Preprocessing","19872b7e":"#### Categorical variables","999a0e6d":"##### Observations: Eliminated 6711 rows, ","d8550772":"#### Checking the distributions","b67fdeb4":"### If you liked the EDA part, I would request you upvote [kslarwtf's work](https:\/\/www.kaggle.com\/kslarwtf\/avocado-eda\/notebook). I took a lot of help from her.","7cb5f9cd":"#### Observations:\n* we see a daily and monthly avg price decline duing jan 2016-2017","712bfd1f":"### Visualization","a71d7dca":"##### Analyzing weekly, monthly and yearly prices"}}