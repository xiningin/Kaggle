{"cell_type":{"6dc110aa":"code","ee3c3258":"code","6fb4f642":"code","8d2d0120":"code","c629a25b":"code","4392a1cc":"code","dc4ed151":"code","03fcee2a":"code","00f5e33a":"code","7d0a59f0":"code","83bee1fc":"code","63690cc3":"code","e1a51c22":"code","11550fdd":"code","66b5defe":"code","4cf980d0":"code","1e0ba73a":"code","e9df6f9d":"markdown","54d8d39d":"markdown","98ee6c03":"markdown","3b375e15":"markdown","61a28686":"markdown","02dc9297":"markdown","aab37444":"markdown","65fe27a6":"markdown","f82b4ec1":"markdown","fc43c30e":"markdown","5fea10ed":"markdown","98ac3a68":"markdown","27658420":"markdown","6b856fbb":"markdown"},"source":{"6dc110aa":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport sklearn.datasets","ee3c3258":"class TreeNode:\n    def __init__(self,id, parent=None,vLevel=None,vSplitFeature=None,\n                 vOp=None,vSplitValue=None,vSplitSign=None,vPredictedClass=None):\n        self.id = id\n        self.children=[]\n        self.parent=parent\n        self.vLevel=vLevel\n        self.vSplitFeature=vSplitFeature\n        self.vOp=vOp\n        self.vSplitValue=vSplitValue\n        self.vSplitSign=vSplitSign\n        self.vPredictedClass=vPredictedClass\n        if(parent!=None):\n            parent.children=parent.children+[self]\n","6fb4f642":"def load_dataset(dataset = \"noisy_moons\"):  \n    N = 300\n    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n    \n    \n    datasets = {\"noisy_circles\": noisy_circles,\"noisy_moons\": noisy_moons,\n                \"blobs\": blobs,\"gaussian_quantiles\": gaussian_quantiles}\n    \n    if (dataset==\"planar\"):\n        np.random.seed(1)\n        m = 400 # number of examples\n        N = int(m\/2) # number of points per class\n        D = 2 # dimensionality\n        X = np.zeros((m,D)) # data matrix where each row is a single example\n        Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n        a = 4 # maximum ray of the flower\n\n        for j in range(2):\n            ix = range(N*j,N*(j+1))\n            t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n            r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n            X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n            Y[ix] = j\n\n        X = X.reshape(Y.shape[0],2 )\n        Y = Y.reshape(Y.shape[0],1)\n    else:\n        X, Y = datasets[dataset]\n        X, Y = X.reshape(Y.shape[0],2), Y.reshape(Y.shape[0],1)\n\n    # make blobs binary\n    if dataset == \"blobs\":\n        Y = Y%2\n        \n    return  X, Y","8d2d0120":"np.random.seed(1) # set a seed so that the results are consistent\n#Optioins for dataset \"planar\",\"noisy_circles\",\"noisy_moons\",\"blobs\",\"gaussian_quantiles\"\nX, Y = load_dataset(\"blobs\") \nprint(Y.shape,X.shape)\nK=2\nprint(\"Number of classes=\",K)","c629a25b":"cmap = ListedColormap(['blue', 'red']) \n\nplt.scatter(X[:, 0].flatten(), X[:, 1].flatten(), c=Y[:, 0].flatten(), cmap=cmap);\nplt.show()","4392a1cc":"def getGiniScore(X,Y,ri,ci):\n    G=0\n    S=X[ri,ci]\n    Y0=Y[np.where(X[:,ci]<=S)]\n    Y1=Y[np.where(X[:,ci]>S)]\n    ep=0.00000000001\n    \n    for i in range(K):\n        P=len(Y0[np.where(Y0==i)])\/(len(Y0)+ep)\n        Q=len(Y1[np.where(Y1==i)])\/(len(Y1) +ep)\n        G=G+(len(Y0)\/len(Y))*P**2 +(len(Y1)\/len(Y)) *Q**2\n\n    return G","dc4ed151":"def getBestSplit(X,Y,ThresholdCount):\n    ri=0\n    ci=0  \n    for i in range(K):\n        if(len(Y[np.where(Y==i)])==len(Y)):\n            ri=-1\n            ci=-1 \n\n    if(X.shape[0]<=ThresholdCount):\n        ri=-1\n        ci=-1   \n\n    if(ri!=-1 and ci!=-1):\n        G=np.zeros((X.shape))\n        for ri in range(G.shape[0]):\n            for ci in range(G.shape[1]):   \n                G[ri,ci]=getGiniScore(X,Y,ri,ci)\n\n        ri=np.unravel_index(np.argmax(G, axis=None), G.shape)[0]\n        ci=np.unravel_index(np.argmax(G, axis=None), G.shape)[1]\n    \n    return ri,ci","03fcee2a":"def createTree(X, y,Level=1,Node=TreeNode(id=\"root\",vPredictedClass=-1),ThresholdCount=1):\n     \n    ri,ci=getBestSplit(X,y,ThresholdCount)\n  \n    if( ri!=-1 and     ci!=-1):\n        SplitFeature=ci\n        SplitValue=X[ri,ci]\n\n        #PlotTreeSplit(X,SplitFeature,SplitValue,Level)  #Plot While Training\n        \n        X0=X[np.where(X[:,SplitFeature]<=SplitValue)]\n        Y0=y[np.where(X[:,SplitFeature]<=SplitValue)]     \n       \n        X1=X[np.where(X[:,SplitFeature]>SplitValue)]\n        Y1=y[np.where(X[:,SplitFeature]>SplitValue)]\n       \n\n        s0 = TreeNode(id=\"Level_\"+str(Level)+\"_Left(\"+\"X\"+str(SplitFeature)+\"<\"+str(round(SplitValue,1))+\")\", parent=Node,vLevel=Level,vSplitFeature=SplitFeature,vOp=\"<\",vSplitValue=SplitValue,vSplitSign=-1,vPredictedClass=-1)\n        s1 = TreeNode(id=\"Level_\"+str(Level)+\"_Right(\"+\"X\"+str(SplitFeature)+\">\"+str(round(SplitValue,1))+\")\", parent=Node,vLevel=Level,vSplitFeature=SplitFeature,vOp=\">\",vSplitValue=SplitValue,vSplitSign=1,vPredictedClass=-1)\n        s0=createTree(X0,Y0,Level+1,s0,ThresholdCount=ThresholdCount)        \n        s1=createTree(X1,Y1,Level+1,s1,ThresholdCount=ThresholdCount)\n\n    else:\n        if len(y[np.where(y==0)])<= len(y[np.where(y==1)]):\n            Node.vPredictedClass=1\n        else:\n            Node.vPredictedClass=0\n      \n\n    return Node","00f5e33a":"Threshold=30\nprint(X.T.shape)\n#Training\nTrainedTree = createTree(X, Y,ThresholdCount=Threshold)","7d0a59f0":"def predictTree(X,y,Node):\n    if(len(Node.children)!=0):\n        SplitFeature=Node.children[0].vSplitFeature\n        SplitValue=Node.children[0].vSplitValue\n        X0=X[np.where(X[:,SplitFeature]<=SplitValue)]\n        Y0=y[np.where(X[:,SplitFeature]<=SplitValue)]             \n        X1=X[np.where(X[:,SplitFeature]>SplitValue)]\n        Y1=y[np.where(X[:,SplitFeature]>SplitValue)]\n        newX1,newY1=predictTree(X0,Y0,Node.children[0])\n        newX2,newY2=predictTree(X1,Y1,Node.children[1])\n        newX= np.concatenate((newX1,newX2),axis=0)\n        newY=np.concatenate((newY1,newY2),axis=0)\n    else:\n        newX=X\n        for i in range(len(y)):\n            y[i]=Node.vPredictedClass\n        newY=y\n    return newX,newY","83bee1fc":"def accurracy(Xy,NewXy):\n    Xy=np.sort(Xy,axis=0)\n    NewXy=np.sort(NewXy,axis=0)\n    Y1=Xy[:,-1]\n    Y2=NewXy[:,-1]\n    m=np.mean(np.where(Y1==Y2,1,0))    \n    return m*100","63690cc3":"newX,newY=predictTree(X,Y,TrainedTree)\nXy=np.column_stack((X,Y))                #Merge dataset to sort order again \nnewXy=np.column_stack((newX,newY)   )    #Compare requires sorting as Tree shuffled the data in leaf nodes\nAccuracy=accurracy(Xy,newXy)\nprint(\"Traning  accuracy(\",Accuracy,\"%).\")","e1a51c22":"def PlotTreeSplit(X,SplitFeature,SplitValue,Level): \n    x_min, x_max = X[:, 0].min() , X[:, 0].max() \n    y_min, y_max = X[:, 1].min() , X[:, 1].max()\n    u = np.linspace(x_min, x_max, 2) \n    v = np.linspace(y_min, y_max, 2)      \n    for i in range(len(v)): \n        if (SplitFeature==0):        \n            u[i] = SplitValue\n        else:\n            v[i] = SplitValue\n    plt.plot(u, v)\n    plt.text(u[0],v[0],Level,rotation=90*SplitFeature )\n    return\n\n\n####################################################################\ndef PlotTree(X,y,Node):\n    if(len(Node.children)!=0):\n        SplitFeature=Node.children[0].vSplitFeature\n        SplitValue=Node.children[0].vSplitValue\n        Level=Node.children[0].vLevel\n        X0=X[np.where(X[:,SplitFeature]<=SplitValue)]\n        Y0=y[np.where(X[:,SplitFeature]<=SplitValue)]     \n        X1=X[np.where(X[:,SplitFeature]>SplitValue)]\n        Y1=y[np.where(X[:,SplitFeature]>SplitValue)]\n        PlotTreeSplit(X,SplitFeature,SplitValue,Level)\n        PlotTree(X0,Y0,Node.children[0])\n        PlotTree(X1,Y1,Node.children[1])\n    else:\n        plt.scatter(X[np.where(y==1),0],X[np.where(y==1),1],marker=\"+\")\n        plt.scatter(X[np.where(y!=1),0],X[np.where(y!=1),1],marker=\"o\")\n    return\n\n####################################################################\ndef PlotPoints(X,y):\n    plt.scatter(X[np.where(y==1),0],X[np.where(y==1),1],marker=\"+\")\n    plt.scatter(X[np.where(y!=1),0],X[np.where(y!=1),1],marker=\"o\")\n    return\n\n","11550fdd":"#%matplotlib notebook\nplt.subplot(111)  \nplt.title(\"Training (Threshold=\"+str(Threshold)+\")\")   \nPlotTree(X,Y,TrainedTree)\n\n","66b5defe":"plt.subplot(111) \nplt.title(\"Prediction \"+str(Accuracy)+\"%\")     \nPlotTree(newX,newY,TrainedTree)\nplt.show()","4cf980d0":"def pruneTree(X,y,Node,ThresholdCount):\n    if(len(Node.children)!=0):\n        SplitFeature=Node.children[0].vSplitFeature\n        SplitValue=Node.children[0].vSplitValue\n        X0=X[np.where(X[:,SplitFeature]<=SplitValue)]\n        Y0=y[np.where(X[:,SplitFeature]<=SplitValue)]             \n        X1=X[np.where(X[:,SplitFeature]>SplitValue)]\n        Y1=y[np.where(X[:,SplitFeature]>SplitValue)]\n        if (X0.shape[0]<ThresholdCount or X1.shape[0]<ThresholdCount):\n            Node.children=[]\n            PredictedClass=0\n            PredictedClassLen=0\n            for i in range(int(y.max()+1)):\n                if (len(y[np.where(y==i)])>PredictedClassLen):\n                    PredictedClass=i\n                    PredictedClassLen=len(y[np.where(y==i)])\n            Node.vPredictedClass=PredictedClass\n        else:            \n            pruneTree(X0,Y0,Node.children[0],ThresholdCount)\n            pruneTree(X1,Y1,Node.children[1],ThresholdCount)\n                \n    return Node","1e0ba73a":"PrunedTree = pruneTree(X, Y, TrainedTree,ThresholdCount=Threshold)\nnewX,newY=predictTree(X,Y,PrunedTree)\nPlotTree(newX,newY,PrunedTree)\nplt.show()","e9df6f9d":"<h1> Prediction\/Accuracy Evaluation","54d8d39d":"# Data Generate","98ee6c03":"<h3>Predict using Tree Created","3b375e15":"<h5>Accurracy on Training Data","61a28686":"<h1>Tree Classification <\/h1>","02dc9297":"<h1>\n   Pruning a tree","aab37444":"<img src='data:image\/png;base64,iVBORw0KGgoAAAANSUhEUgAAAlAAAAFGCAMAAACSW1FsAAABpFBMVEX\/\/\/\/F4LT4y61BcZydw+Zbm9UAAAAAWoCNWgCQOgD\/\/9u2\/\/\/\/\/7YAAGZmAACN4LRvMgDF4JvFwWWp4LQAMmVmtv8AAEjFn0iMLQCQ2\/\/4y5RvwbT\/tmbb\/\/8AOpAAUnypfihOn7SxUgDVy63F4IAALWEAZrb\/25A4cpQAADr4r2Gxy63bkDr4kUVOAAAsfpv4y3y2ZgAsACg6kNsAAEWMr61jka0sAEjVcic6AABjAAAsAAA4AACdqIEAACg6ADokbsU4AEVYqOZOACgAACdYLACdw8Vww+adi1xjACedw6QAAFyGw+YALIEATqQkADRwTgAkAACGbjQ+AAA6AGZmADpvfkg4ACeN4Js+i+ZmtrYkAFw+ADSQkGZvwYAAADRjLWFOAEgsMigsMmVOMmVvMigsfmUsfoBvfihOn4CNwWVmOpC2\/9s4LSc4LWE4cnyMcieMckU6kLZjkXyxr2FmAGaQOjqQtpC2tmbbtmap4IC225AkLDQAOmYkLIE6OgAAPphmOgA6OpAkbqRYbjRYblxmZmY6kJBOm9VwqIGQ27bb\/7aTokjUAAAWHUlEQVRo3u2bh2PbRpbGKY4HVrMkW6Zt2bRVTRVThatYthTF62ycsim+rONkN9lLttfL9np1r9d\/+t6bwQAgRYIkiDIYfC+xUGYwAN785ntvhmSt5qSJTK0Gq5ZRn1\/K0oBU1Xi6lLWBKPAEomAJgboEoGClAwpEAShIFAxAwQAUDEABKBiAggEoWLWBeueD9wEUgIr2+1tvep733tu099o3aO\/ppXDv3Y88zxReuvShKnvxHEDB4oEiTj4kTN7xPmWUnkb3Lr32V18zFT\/8ByYLQMFGAIq4UFvae+\/tcK8bqH9787kCijVNlX3D+85\/MFDmBIACUBqotwgVH513\/\/pr4V43UE\/54MXztxRXmrt3P2KgWN9eXJAqAFXhHIok6bVvK41596NPw70eoC59+MH7L56rYuaOtxzyVB2FH4ACUEqhOLqNoFDE2NMXz1UofOvNT9VWAcUpvPcdAAWgwpD3VAuSypzCvV6g6Ny\/91Gob7+NpBxAdSflBM8L1hg1twv3eoGi+OjnUB+8T3JFlQgoP4kHUAAqAEqJ0jtB4Ar3eoAigp6r5SlOwSnUvfc\/\/+TP8jwk5QAKH73AABQMQMEAFICCASgYgIIBKAAFA1AwAAUDUAAKQAEoGIAqxjotoFMgUMIJoGZnGmZ3fqERXwFAVVygZmfoSxJX5kYFqnnvrt7SVY3+xKkfCQ1pEUQ5K1CzM8f0rzUqUJ1jtdk+bdTm\/+Zuf4Xa\/uoKNKqyAY+BqrVJd+Y\/\/px0ZfuUlWd25rMFb3eFwek6pEorUWYUSCRaYYWwULfHAshqZbaOEyWyxakMCZRWqGOOVdThrDptBoTooPR7fuGYT5nDMOKpM1Gg6LBtgPGB4vbUVYyr2bqP1EgmpRjfypJwU8bDdATp9vbLOQNKUzNiDjV5fuzji0KgGqF6BUA1\/F0qMNsKhsF+IEj6Xzr6woqRJsGhgWqrhDoASmtKAFRXdtTeXekGaqHRVnEtCtSpp3g12yomVvKCumiWpJtM+eGuoQFgSYpRqHarO1UfrlAv54zqVXY1QbJFoQo5cpGpboVigjqhQgU5VIQbrU7+RZQabZ+aHMqkSBGgTIyMxMqKEhVC1c2Qc0ipHMprmByq43n\/9Ye5MMadRlOlUGf4Kp14eVf+TED98U+n4SQuApRuXmVfeltloDRUFwCSzqZTQ62Nj10mJ6pfjYoihQ9Ykk3zhvJUbZmCJZenwUBVm6kyrbPZJE\/xQFU39OlRB6DGx2m416opUwAqSbQb1WkVZApAJZOnkZ1WNaQAVCJ5Gsdp1ZIpAJVInsZ0WoWQAlCJ5Glsn1VGpgBUAnkSiXxWDaYA1Pg4CfZaIp9VACkANWa0U6vAIrHPnJcpADWmPJnC5A27jJTQXsOHL+PIky6epG2HZUokmrJUVJ7SHHbOMiUEeBpVnlK+ibNIgae85clxmQJPBchThRYSIE\/5yFNEpkQFrYxs6B88242T8m0VvzlVBqjM73eiQG2fdkPVezxKtJMZerXSoc9ypPSvyCK\/7wqAanv8U0L1Nw6oAfIk8\/FnJb\/gabVMNfnnhNv\/evdiyIsANWYyzvEov+FZTaYsBkrHu\/mFe3+rYh8DRf++VMr1W\/X3WJ\/jCgwb\/fiVrDWQKFHLjifM+mwnin76rAIebz0GZjBQng6O\/s4goER2PMUoffVkytqwp\/g45s0xa0\/DB6orh\/KBaqkA2aQtydXdvkFPvabMbFCalvttGam4cue2VosUcUThjjL0GKD8eMgZejsQqG6iRIbxZ7j\/qiZT1hJFHB2PDpSStMi0MAQqS3ka0XsV+22Dhc\/UbimJavghb3clAlRLAdXqAartNfrP9EStcJ6qlqFbSFQ7yMWDnNsHqqlyq2aYlPtANbty8oAoYQlP1ZIpC4nq6ABGvHyuVgUCoHjJs6H\/Xgx5XtfylDAftEhbHFcZpCzOzKOfv8QSyBXbPRJVs4un6siUC0ApgbqQSGXJU0K3VYEpB4BSn\/71q5whT4nXa3ykHF6PcveLedLOYej+T7DAU94ucxspN4HKtM9ECo8nQRTkKVWHucuUAE8FOcxRpAR4KsxhTsqUAE9F+stBpgR4Gtdd6a7fhEy5sRXgqejx55ZMCfBkgbscQkqAJyvc5YxMuQNULj0iMn0BCaAqJU\/Zu8sBpAR4KtpdImpSZGkAyjKe0ncX9XI9N8sDKQGexnVXmus2eeKkmcI6lNMZQt48KaIcUSiRt5XAXfnz5AxQuWt7PROiyg9U1kTlBpQbrgNQlgBVhOey0CgAVWWg6gAKQAEoAFU1oNL8HVsRTnFiHQoKBYUCUAAKQAGowPbvbwKoKgDV9rxGX7fcvuWRnVx8jQ2DxkZ\/Rja8Hfr7eBVAVRGo+QXPaw0AirjYV3CMZxu\/eOUBgMpB3K0Equn9dubK3ECg6vsEB2sVM7J3rhRrY7X+2POePeQdU3T71u\/O+JQC6le3VjVQkQuf\/Ya8Yo5dByoncbcSqM7uSieIeX2AOjo7YXDqj+9vHp3t1I++u0mHe68rKjZWbyt0FHMaMHV6Z++bD9krppRbOjq7v2kaqgBQuYi7jetQ8wv3\/rupY54cBBTjUT\/6+sN9X1wIqPMTf4fJoiKqfBK8+MYOD0DyiilVWyo0DTm\/DjWSuJuy5OJuo0I1vWOCanclTqHYGewFM1DIGXRqh3f2dbg76QWKxIy8YkrVloHyG6qGQg0R94h+JxV3G4HqeMqO43Io3wf1UKECbw1SKKr76z4K9fqDaiTlo4h74J3k4m4hUDzHY7t3dwBQe+c7vnvYQWaY1bUsmRxKKXI3UKTQZpjp8bl3fn\/TNFQhoAaLe6DfycXdQqB0+hTEvIvrUOoV1B69McfzhyrS0fGq8srRmSq5ABTVXa0HpXThK7\/\/5WbQUDWAihf3QQo1hrhbCJQ\/wev4MQ8fvaQI1BBxD\/Q7ubjjs7zqADWCuBv9Ti7uAAor5fg+FL4Phc\/yoFBQKAAFoAAUgAJQAApAASgABaAAFIACUAAK61BYh4JC9foECuUyUMK1iAeginWXcE2gAFTB7spZozLnyWmgRBncJRKYpP+SWQ49XXOWKCHK665Y4+mUlDU7LTcPuZEtCEt4iszWqwpUTeRtWbkr23WcodtAm6Qs9DmKXYdyxQp3V1eok\/BQEm2Huwa6xMJMyl6gyFlw19AhZh1S1gIlrdQoYRlP9vnIUqCktHMACut4ss1HdgIlbR2Awj6eLPORjUB1DzmMv+F+sEikLARKWj7+ilnfkfHlEutQow82WcP4kwn8BoWyfyIjrBlltnrJLqCk7RMZYc8os9RLogQ4WSRSwmKe7EDKIqAkBH3yN5cAatTBJSsKlEzZjxUBSpZg8BXiLpnDFe4BNdqwsiRDyHVdJ8n6kpT5PV\/NynUoWY7BV4C7ZK6XOaJQ4wiPrBRQySW5ODEvHihZlrGXu7tkYReXGKixh1J1Uk6Zs2edAEqWZ+jl7S5pQQtlAyrZIJKVSDllYf4tL1CyVEMvX3fJIodsOYGa5F0LncPksZ4jazl9j6rmzDqUtGEA2zr+pCUDtzwKNfFbSoeBShsB6T5QsmReytVd0npCbQMqnfcrZLInyshTrsPP\/u9jWCVSopw85Tj8cgcqzTeT7gElS9hyoUDJMjopP3fJkgxlW4BK\/Z1kAUBluI4ja5m2LzNuP2+gshgiTq2yZP8usuQeyiXddAYoWdJBXQxQ2b2JdAOovLRWugGULPnIztxdKb6EiDcpRZpWCFAZjz4Hlu1kijhN5Wo9TImyy1O+RInRhWAsk+MKweAnnMrd8gYqj+RA5r5sl78Q2MpTN1Gi5oh6yNyAkgULQew6UCFA6UfLZx0qDelIK2okyTEHKFThQmBJAnXxyYT1uiFEPU+LR8p6oKacBkqWj6chRIki+22ENMoioETuZidP8UQJW\/rNeqAKCL4jIZU\/T0QUgJoYKGGrhNeLAEoAqFICNWUpUHUANSlQ9jrIQqBkwf6KW4ea\/MGmL8cU3ry+PNo6VEEOGk4UFCq5Qm0deGw3FmPouQiID9S0d0h\/1y6PAVQNQBUIVHIhGCfkXfvKnYQPNv19BhFAFQBUMUIwFlBXl9Tzbb3xiXfjWz\/64oD+LnlP7qinuLr0swO1f+1Vz1sPH+wnS5f1g\/kXc\/mTT+jBzDGAyk6hChCCsYDim61dX946oJtdXbqxSEys8wkNFNFE260fLptT6pJDvnaNy\/livuaQxo5fYa3nAQGUsv37m6kCpceu0oHFq91CEOhAWkIwDlDq79Ybd7YO1vk+6\/qGN6l1BdR6QPC1R4shUKyrVI\/P8cVqS\/VMY04AdfsWB5eTiwRsGDQ2+jOy4e3Q38er2QKlx+7fsQ70CoHRgdSEYCygXmW3PRkC1JqO2yFQWweHVO+mfpt1tWWg\/MYcAYq42FdwjGcbv3jlQeZA+WP3C+623n6L6kAqQjAWUI8W\/WwvBig+7H4wqvHTPg\/2aDGfpDybaUsfoOr7BAdrFTOyd64Ua2O1\/tjznj3kHVN2+9bvzviUAupXt1Y1UJELn\/2GgDLHE61DdQnBEKAmF4Ix1qH0g7HqjQTUdNeDcdQ10skJ2CG94fVl09gY61A2TVv6AXV0dqLAeXx\/8+hsp3703U063HtdUUHnbyt2Xnlw+5YPGJ\/e2fvmQwYqLKQLz+5vmoZSUSg9drdigUpLCMae5XnMRFzIm\/a8H3+v68EIoMsaB+4+QvzGz3+wbBobT6GsmbYMAIrxqB99\/eG+Ly4E1PmJD5RCi8pu3zoJotrGDmdXj8NCtaVC01AaQPljdzhQkwlBKT96MR4asHwRnbb4s5YL05ZgljLRtGUQUOesoc8emiyIOTrn3Ip2FGREUw9QJGaPw0K1ZaD8hlKb5XmUlMeGvJSEoJRADVy+iExbzKyld9piZi1Tk01bBuRQfoCrhwoVhsIBCkV1f91HoV5\/gA+H8wJq8PJFz7RFxfueaYvJCaYmm7b0AWrvfMcnixkyOZQqPQlzqPubvUBR+h0W8oV751xpB0DlBtTA5YsoUP6spXfaYmYtU6OuX4y8DqXik9ojWHiy9pAVap+OV7VUHZ2pol6gqG5YyBe+8vtfbgYNAagcgBq4fBEBysxaeqctFxUq0bQF3zZwCKjByxc9QE13K5SatphZy9Rk0xZ8Hyrv70MlfzA50iyv7\/JFNOT5s5YL0xYza5ls2gKFckOh8G0DAAWgABSAAlAACkABKABVHaAEgAJQKQJV\/C92AVR51qHE8HWogoCq4Xd5kylUkf1mX9ATFz0EoJwBKneihKgBqCE8DQeqCI+NxpNCKk\/r6yFrgSqEqJinErYIgbUGoBICVbAQ2M2TBcsXFhElRnAYzMYkc9Rpiy0JFIAaAyhh7awlQWiRMrvYIuLXgaq+FcUtX2SXFtCbSZn1EITZlWOKTHmqZYcUgKqYd0KQMkKqpD6bX\/C83RUAlUyeskQqxmezM\/zLnpaNfmnyk3lX5sDTBDz1Oc7WaxooG4kifaKn2n45B54m4ikbkRKDgaKg0vTu3bVQoLIWJyeBkvFJVV5AWahQ7TweyjWeBpKTNlJi0DqMDnn\/nGT9JuMtA5X5fRwDSiZhLd2h6OdQFQ15FeIp9excxIQ8oqphaVI+\/4c58JQOT2mLlCgZUBTzMl42qBpPaSMlYkJeHhOqBEEv04XNCvKUMlL9PjrSQFmYQmWePrnF0xicZI1UNa2a8pRBdg6iXBxY4xKSukhV+3tQzul0AjzSRarKMuXg2ydjI90JXzWhcvK1k4Mh0\/duxczFQSILYRHmqsmi5A0GnoAULI8sCEjBUiYBRMHSxQAiBUtZVoAUeLI1gMLAE5ACT2VqFlaV6R1ECpa1jgAp8ASkYHbnOUAKPE1g+NIH0vFUeZrK2UCU0+Eud55AlNs8FQDUFICyPi0RJRIoSFQZhn3SPioEKEiUu6MeQEGfUu0kAAWBSlWiABSASrWXABSAAlAACkABKAA1mk1fjim8eX0ZQFUWqK0Dj+3GYgw9FwHxgZr2Dunv2mUABaAidu0rdxIq1PT3GUQABaD6AXV1SQnV1hufeDe+9aMvDujvkvfkjqLn6tLPDtT+tVc9bz0E6idLlzVQ\/sVc\/uQTAsocA6jigIrNS9Id9n2B4vuvXV\/eOiAQri7dWCQm1vmEBopoou3WD5fNKXXJIV+7xuV8MV9zSEHUr7DW88gAKmkfZZCXZA+U+rv1xp2tg3UWnHX9DDfpJRRQ68EjXHu0GALFL0L1+BxfrLZUzzQGoNJSqLTzkhyAepUHwZMhQK3pgRICtXVwSPW4GldSWwbKbwxApQxUKnlJ+olJX6AeLfryGgMUH3YrFNX4aR+FerSIpDwLoNLIS6bST0z6AcV3GQmo6S6FYsrNs\/KLHhL+15dNYwAqXaBSyUum0k9MBs7yPGYi7lmnPe\/H3+sCigC6rNNGfhUS0xs\/\/8GyaQxApQxUGnnJVPqJCT56KStQaeQlU+knJgCqpEClkpdMpZ+YAKgyz\/ImzkvST0wAFD56wddXABSAAlAACkABKAAFoAAUDEABKAAFoAAUgIIBKAAFoMATgEqvmwSAAlCpApX74wrw5DJQjJTIkybwVBKgJuknkZsBD8ycYIUDhUQXVnKiEEucRypfg8dhMBgMBoPBYDAYDAaDwWAwGAwGg8FgMBgMBoPBYJlY88pc\/4KOt7syWQuw7G12RnXT\/MK9u+N0utcKrm\/lBNT2V32c2h5Za6QWOkNqwrIAil3eDVTbO46\/qvOfps8SAdVpJQAqON+mR51fOB7ewvwC36dzjF7OFajdv5BEjQfU9ssvZxoT3DQZUOYJGajYJkwLnbFUF5YSUFf+7N27q4HiCHFljkXL8xoEmcfhkJDrTV3aLf4\/hKNJ9XXnzf7xfxdUDOVGqF\/nP\/7T6e6KfzA78xkXqvZb+uafddWubZ96u5+rmvrYaCjth9GLgdo+bXSVBi2ZFpRANdC\/RQA11\/EaDJQGybvypQaKedJ89QI1S\/IUZDQEVLBf0\/S1+aKGKppfUL3dUhioQpYNIy\/mhKrQ4av0FeaCIKZSWUShvIDfmrlPtCV1z2jSBcsZqO3Te\/9HQG2fsqQskDapkKcSb2LN9OFxJOIxJMcBUJGERnWwkYY2K1\/D79n5j1dUIcejECh9QsUoqqoq0oG5IIhf3ExPyIsERsVqI7yyCaCKBYp4+YyA0lO3jgFKCYHXDyhd4vdvR6X0ZkYfAEWhx\/MMUKe6paFAqS1jcRreegBQJub59+lqqYmQVzBQ8wv\/+C8M1EWF6me6o8z49+FoaqJUx1KR6u8uhYrwM5pCRTPsgUCZ+\/RTqPjcH5YhUJxVBzkUdVc7NofS\/WVint9rPgMmh+KO5rxMAWWWFkKgohkQnVDz+w7XPq5x4I2uRZiy3pCnj8x9wpZ0C756tdQMYnYG072cgSKWyOfB9EuHMEVUH6B8hNq62+ioGcSncLZFTO5+7gOlG\/YzdQaK4lSrizC+ldYd78rfv5wzF\/hE6bK+Sbm5T7Rp1UIwBfQ4YwdQ5eUTeQsMQMEAFAwGg8FgMBgMBoPBuuz\/AYbdGOXmGaeCAAAAAElFTkSuQmCC'>","65fe27a6":"<h2>Create Tree","f82b4ec1":"<h1>Classification Tree<\/h1>\n\n\n<p>\n    Just as in the regression setting, we use recursive binary splitting to grow a classi\ufb01cation tree. In the classi\ufb01cation setting, RSS cannot be used as a criterion for making the binary splits. Following are the methods used as alternative to RSS.\n   <p><h3>Classi\ufb01cation error rate<\/h3>\n      <p>  A natural alternative to RSS is the classi\ufb01cation error rate. this is simply the fraction of the training observations in that region that do not belong to the most common class:\n$$E = 1 \u2212 max_k(\\hat{p_{mk}})$$\n\n<p>Here $\\hat{p_{mk}}$ represents the proportion of training observations in the $m_{th}$ region that are from the $k_{th}$ class. However classi\ufb01cation error is not su\ufb03ciently sensitive for tree-growing, and in practice two other measures are preferable.\n\n<p><h3>Gini index and Deviance<\/h3>\n<p>The Gini index is de\ufb01ned by\n    $$G=\\displaystyle \\sum _{k=1}^K \\hat{p_{mk}}(1 \u2212 \\hat{p_{mk}})$$\na measure of total variance across the K classes. The Gini index takes on a small value if all of the $\\hat{p_{mk}}$\u2019s are close to zero or one. For this reason the Gini index is referred to as a measure of node purity \u2014 a small value indicates that a node contains predominantly observations from a single class.\n\n<p>Other Gini formula using weighted sum of square of probability (in this case we pick max)\n$$G=\\displaystyle  \\sum _{k=1}^K \\dfrac{N_{m}}{n}(\\hat{p_{mk}})^2$$\n\n\n   <p><h3>Cross-entropy<\/h3>\n<p>  An alternative to the Gini index is cross-entropy, given by  \n    $$D=\\displaystyle -\\sum _{k=1}^K \\hat{p_{mk}} log(\\hat{p_{mk}})$$\n    \n<h2>Predictions<\/h2>\n    <p>\n\u2022 We predict the response for a given test observation using the highest no. of the training observations in the region to which that test observation belongs. \n        <p>    ","fc43c30e":"<h1>Regression Tree<\/h1>\n\n\n<p>\n\n\u2022 In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes, for simplicity and for ease of interpretation of the resulting predictive model. \u2022 The goal is to \ufb01nd boxes  $R_1,R_2,...,R_J$ that minimize the RSS, given by\n\n$$\\displaystyle \\sum _{j=1}^J \\sum _{i\\epsilon R_j}\\left ( {Y}_{i}- \\hat{Y_{R_j}} \\right)^2$$\n\nwhere $\\hat{Y_{R_j}}$ is the mean response for the training observations within the $j^{th}$ box.\n\n<p>\n\u2022 Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. \u2022 For this reason, we take a top-down, greedy approach that is known as <b>recursive binary splitting<\/b>. \u2022 The approach is top-down because it begins at the top of the tree and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. \n    <p>\n        \u2022 It is <b>greedy<\/b> because at each step of the tree-building process, the <b>best split<\/b> is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.\n<p>\n\u2022 We \ufb01rst select the predictor $X_j$ and the cutpoint s such that splitting the predictor space into the regions \n    {$X|X_j < s$} and {$X|X_j \u2265 s$} leads to the greatest possible reduction in RSS. \n <p>   \n \u2022 Next, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within each of the resulting regions. \n   <p>  \n     \u2022 However, this time, instead of splitting the entire predictor space, we split one of the two previously identi\ufb01ed regions. We now have three regions.\n       <p>\u2022 Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than \ufb01ve observations.\n\n<h5>Predictions<\/h5>\n    <p>\n\u2022 We predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs. \n        <p>\n\n","5fea10ed":"We divide the predictor space \u2014 that is, the set of possible values for $ X_1,X_2,...,X_p$ \u2014 into J distinct and non-overlapping regions, $R_1,R_2,...,R_J$. \n    <p>\nFor every observation that falls into the region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.","98ac3a68":"<h5>Types of Decision Trees<\/h5>\n<p>Types of decision tree is based on the type of target variable we have. It can be of two types:\n<p><b>Classification Tree:<\/b> Decision Tree which has categorical target variable then it called as Classification tree.\n<p><b>Regression Tree:<\/b> Decision Tree has continuous target variable then it is called as Regression Tree.","27658420":"<h1>Plotting Hypothesis","6b856fbb":"\n<p>\u2022 The process described above may produce good predictions on the training set, but is likely to over\ufb01t the data, leading to poor test set performance.\n<p>\u2022 A smaller tree with fewer splits (that is, fewer regions $R_1,...,R_J$) might lead to lower variance and better interpretation at the cost of a little bias.\n<p>\u2022 One possible alternative to the process described above is to grow the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold.\n<p>\u2022 A better strategy is to grow a very large tree, and then prune it back in order to obtain a subtree\n<p>\u2022 Cost complexity pruning \u2014 also known as weakest link pruning \u2014 is used to do this"}}