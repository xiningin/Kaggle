{"cell_type":{"9d40fba4":"code","f1ffb6c9":"code","05f7d039":"code","825eef4c":"code","c16af19f":"code","e60d9dc7":"code","8c8fd1ce":"code","980851da":"code","62d30ad8":"code","e6507a8a":"code","665ec0ae":"code","30b1c964":"code","0b30d879":"code","ac9d93eb":"code","911059b2":"code","64b4ba95":"code","4769a697":"code","a926b3ee":"code","0e80b8e8":"code","fa467d39":"code","a6fce4f1":"code","b30e64b9":"code","f5ec6044":"code","9862cc10":"code","fefde11d":"code","e81453db":"code","fcfd7295":"code","1b8e83f0":"code","db67fbf0":"code","0c911c99":"code","d97181db":"code","98220fd0":"code","28edfda8":"code","b8770d4a":"code","62b86442":"code","d1211da7":"code","71dd376a":"code","c4aca319":"code","436db68c":"code","dc3ea4b8":"code","64b58aa4":"code","d8db9317":"code","6e00ad64":"code","cd92293b":"code","f2352a6f":"code","074c4225":"code","d932ba32":"code","72b49bb2":"code","52ca9c50":"code","7ba6128f":"code","00296420":"code","7f030e2e":"code","d0eb7b31":"code","2a87315b":"code","cafbfe60":"code","4aa7d2ad":"code","c54306bb":"code","3faf9f66":"code","cdf28073":"code","dcacaf11":"code","1bb26296":"code","5f03a710":"code","32580799":"code","7bebbae4":"code","31f139fd":"code","578ee206":"code","79c24c5d":"code","2affda1d":"code","d302b22c":"code","6f6bb8f1":"code","6a3df98b":"code","871a5321":"code","7c2dde54":"code","9553f729":"code","ae5a8e94":"code","06333662":"code","4b1f82da":"code","853e9ff0":"code","033927b8":"code","61575774":"code","a976e34a":"code","758fa57f":"code","1a73a68a":"code","2b07c31e":"code","a9405986":"markdown","a5b18dd3":"markdown","835909c0":"markdown","948be649":"markdown","0d4afcdf":"markdown","073dcdf5":"markdown","9082554b":"markdown","b66808f3":"markdown","b43d9c81":"markdown","3a61e636":"markdown","65d5f914":"markdown","c71805ac":"markdown","0f13a89a":"markdown","f34067ea":"markdown","d6c12b86":"markdown","17693ef3":"markdown","b5973293":"markdown","d1a0c6be":"markdown","70d82626":"markdown","41a27067":"markdown","73e4442d":"markdown","cef799f4":"markdown","2f5b7f37":"markdown","18deff5e":"markdown","db1c4def":"markdown","39dbcd5b":"markdown","f20369e6":"markdown","e8a5b57c":"markdown","7a81c830":"markdown","b08ebc2b":"markdown","eeeca879":"markdown","8c954825":"markdown","6123d0e2":"markdown","69e8cfda":"markdown","d8f5565b":"markdown","5cf6d359":"markdown","09a23bf9":"markdown","da3fe6aa":"markdown","fada6d48":"markdown","69c9b19f":"markdown","26261aab":"markdown","36439505":"markdown","223df213":"markdown","ce0f4da5":"markdown","d4279bb4":"markdown","d8c8bf8e":"markdown","c4ce37a3":"markdown","8f564be6":"markdown","f43dff8a":"markdown","d276ef48":"markdown","9f4ff75b":"markdown","482e26ea":"markdown","e8363fba":"markdown","d52162a2":"markdown","273c4f28":"markdown","54298ebd":"markdown","aa6bd076":"markdown","f4be3c57":"markdown","510c4bea":"markdown","f3f45830":"markdown","4d1c848a":"markdown","7b158fe4":"markdown","e926b608":"markdown","c5e7e4db":"markdown","88abc12f":"markdown","a36e86fd":"markdown","761067a8":"markdown","510b269e":"markdown","2355d685":"markdown","5b04c42a":"markdown","70bbc375":"markdown","30507680":"markdown","00131bab":"markdown","90a27053":"markdown","639df121":"markdown","add262fe":"markdown","f310c13e":"markdown","281615cb":"markdown","f71b21af":"markdown","7ee3053d":"markdown","aadcc395":"markdown"},"source":{"9d40fba4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('darkgrid')","f1ffb6c9":"training_raw = pd.read_csv('..\/input\/titanic\/train.csv')\ntraining_raw","05f7d039":"test_raw = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_raw","825eef4c":"training = training_raw.copy()\ntest = test_raw.copy()","c16af19f":"training.info()","e60d9dc7":"training.describe()","8c8fd1ce":"test.info()","980851da":"test.describe()","62d30ad8":"sns.countplot(training['Survived'])\nplt.show()","e6507a8a":"sns.countplot(training['Sex'])\nplt.show()","665ec0ae":"sns.countplot(x = 'Sex',data = training, hue = 'Survived')\nplt.show()","30b1c964":"plt.figure(figsize=(10,5))\nsns.distplot(training['Age'], kde = False, bins = 30)\nplt.show()","0b30d879":"training['Age_groups'] = pd.cut(x = training['Age'], bins = [0,18,60,100], labels = ['Child','Adult','Elderly'])","ac9d93eb":"plt.figure(figsize = (10,5))\nsns.countplot(x = 'Age_groups', data = training, hue = 'Survived', palette = 'rainbow')\nplt.show()","911059b2":"plt.figure(figsize=(12,6))\nsns.violinplot(y = 'Fare', x = 'Survived' , data = training)\nplt.show()","64b4ba95":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Pclass', data = training, hue = 'Survived', palette = 'Blues')\nplt.show()","4769a697":"plt.figure(figsize=(12,8))\nsns.heatmap(training.corr(), cmap = 'Spectral', annot = True)\nplt.show()","a926b3ee":"plt.figure(figsize=(8,8))\ncorrelation = pd.DataFrame()\ncorrelation['correlation'] = training.corrwith(training['Survived']).sort_values(ascending = False)\nsns.heatmap(correlation, cmap = 'Blues', annot = True)\nplt.show()","0e80b8e8":"plt.figure(figsize=(12,5))\ntraining.corrwith(training['Survived']).sort_values().drop('Survived').plot(kind='bar', color = 'red')\nplt.title('Correlation with Survived Feature (bar plot) ', fontsize= 15)\nplt.show()","fa467d39":"training.isnull().sum()\n","a6fce4f1":"test.isnull().sum()","b30e64b9":"training = training.drop('Cabin', axis = 1)\ntest = test.drop('Cabin', axis = 1)","f5ec6044":"median = training['Age'].median()","9862cc10":"training['Age'] = training['Age'].fillna(median)","fefde11d":"training  = training.drop('Age_groups', axis = 1)","e81453db":"training.isnull().sum()","fcfd7295":"#Applying the same in test data\n\nmedian = test['Age'].median()\ntest['Age'] = test['Age'].fillna(median)","1b8e83f0":"test.isnull().sum()","db67fbf0":"test['Fare'] = test['Fare'].fillna(test['Fare'].median())","0c911c99":"training['Embarked'].value_counts()","d97181db":"training['Embarked'] = training['Embarked'].fillna('S')","98220fd0":"training.select_dtypes(include = [np.object]) #Viewing the categorical feautures","28edfda8":"training['Name']","b8770d4a":"\"\"\"\nFirst splitting the names with ',' and taking the secong part. (Eg. : ' Mr. Owen Harris')\nThen splitting the reamaining text with '.' and taking the first part(Eg. : ' Mr' )\nFinally removing any space from begining or end with .strip() method\n\n\"\"\"\ntitle = [x.split(',')[1].split('.')[0].strip() for x in training['Name']]  \ntraining['title'] = title\ntraining['title'].unique()","62b86442":"training['title'].value_counts()","d1211da7":"training['title'] = training['title'].map({'Mr':'Mr','Miss': 'Miss', 'Mrs': 'Mrs','Ms': 'Miss', 'Dr': 'Other', 'Rev': 'Other',\n                                           'Col': 'Other', 'Mlle': 'Other', 'Major': 'Other', 'the Countess' : 'Other', \n                                           'Mme' : 'Other', 'Lady' : 'Mrs', 'Jonkheer': 'Other', 'Sir': 'Mr', \n                                           'Capt': 'Other', 'Don':'Other', 'Master': 'Master' })","71dd376a":"training['title'].value_counts()","c4aca319":"dummies = pd.get_dummies(training['title'])\ntraining = pd.concat([dummies, training], axis = 1)\n\n#We won't need the 'Name' and 'title' columns anymore\n\ntraining = training.drop(['Name', 'title'], axis = 1)","436db68c":"\"\"\"\nFirst splitting the names with ',' and taking the secong part. (Eg. : ' Mr. Owen Harris')\nThen splitting the reamaining text with '.' and taking the first part(Eg. : ' Mr' )\nFinally removing any space from begining or end with .strip() method\n\n\"\"\"\ntitle = [x.split(',')[1].split('.')[0].strip() for x in test['Name']]  \ntest['title'] = title\ntest['title'].unique()","dc3ea4b8":"test['title'].value_counts()","64b58aa4":"test['title'] = test['title'].map({'Mr':'Mr','Miss': 'Miss', 'Mrs': 'Mrs','Master': 'Master', 'Ms': 'Miss',\n                                   'Dr': 'Other', 'Rev': 'Other', 'Col': 'Other', 'Dona':'Other' })","d8db9317":"test['title'].value_counts()","6e00ad64":"dummies = pd.get_dummies(test['title'])\ntest = pd.concat([dummies, test], axis = 1)\n\n#We won't need the 'Name' and 'title' columns anymore\n\ntest = test.drop(['Name', 'title'], axis = 1)\n","cd92293b":"training.select_dtypes(include = np.object)","f2352a6f":"training['Ticket'].head(20)","074c4225":"training = training.drop('Ticket', axis = 1)\ntest = test.drop('Ticket', axis = 1)","d932ba32":"training['Embarked'].value_counts()","72b49bb2":"dummies = pd.get_dummies(training['Embarked'])\ntraining = pd.concat([dummies,training], axis = 1)","52ca9c50":"#Same for the test data\n\ndummies = pd.get_dummies(test['Embarked'])\ntest = pd.concat([dummies,test], axis = 1)","7ba6128f":"#Dropping the original embarked column\n\ntraining = training.drop('Embarked', axis = 1)\ntest = test.drop('Embarked', axis = 1 )","00296420":"dummies = pd.get_dummies(training['Sex'])\ntraining = pd.concat([dummies,training], axis = 1)","7f030e2e":"#Same for the test data\n\ndummies = pd.get_dummies(test['Sex'])\ntest = pd.concat([dummies,test], axis = 1)","d0eb7b31":"#Dropping the original Sex column\n\ntraining = training.drop('Sex', axis = 1)\ntest = test.drop('Sex', axis = 1 )","2a87315b":"training","cafbfe60":"training['Pclass'] = training['Pclass'].map({1:'1st_class', 2:'2nd_class', 3:'3rd_class'})\n\ntraining['Pclass'].value_counts()","4aa7d2ad":"dummies = pd.get_dummies(training['Pclass'])\ntraining = pd.concat([dummies,training], axis = 1)","c54306bb":"# Same for test data\n\ntest['Pclass'] = test['Pclass'].map({1:'1st_class', 2:'2nd_class', 3:'3rd_class'})\n\ndummies = pd.get_dummies(test['Pclass'])\ntest = pd.concat([dummies,test], axis = 1)","3faf9f66":"#Dropping the original Pclass column\n\ntraining = training.drop('Pclass', axis = 1)\ntest = test.drop('Pclass', axis = 1 )","cdf28073":"training['Parch'].value_counts()","dcacaf11":"training['SibSp'].value_counts()","1bb26296":"#Seperating the two column values\n\nParch = training['Parch']\n\nSib = training['SibSp']","5f03a710":"#Creating a new variable where we store the addition of the above two variables, i.e one full family \nFamily = Parch + Sib","32580799":"training['Family'] = Family","7bebbae4":"#Same for test set\n\nParch = test['Parch']\n\nSib = test['SibSp']\n\ntest['Family'] = Parch + Sib","31f139fd":"#Dropping original Parch and SibSp columns from both\n\ntraining = training.drop(['Parch', 'SibSp'], axis = 1)\ntest = test.drop(['Parch', 'SibSp'], axis = 1 )","578ee206":"test_id = test.copy()\n\ntest_id.to_csv('Test_with_Id.csv')","79c24c5d":"training = training.drop('PassengerId', axis = 1)\ntest = test.drop('PassengerId', axis = 1)","2affda1d":"#Saving these datasets for final modeling\n\ntraining.to_csv('Training_final.csv')\ntest.to_csv('Test_final.csv')","d302b22c":"training_data = pd.read_csv('Training_final.csv')\ntesting_data = pd.read_csv('Test_final.csv')","6f6bb8f1":"#Assigning inputs and targets\n\nx_train = training_data.drop('Survived', axis = 1)\ny_train = training_data['Survived']\n\nx_test = testing_data","6a3df98b":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train = scaler.fit_transform(x_train)\n\nx_test = scaler.transform(x_test)","871a5321":"#Defining a method or function that will print the cross validation score for each model\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\ndef model_report(cl):\n    \n    cl.fit(x_train, y_train)\n\n    print('Cross Val Score: ',(cross_val_score(cl,x_train,y_train, cv=5).mean()*100).round(2))#using a 5-Fold cross validation\n","7c2dde54":"from sklearn.model_selection import GridSearchCV\n\n#Defining a function that will calculate the best parameters and accuracy of the model based on those parameters\n#Using GridSearchCV\n\ndef grid_search(classifier,parameters):\n    \n    grid = GridSearchCV(estimator = classifier,\n                        param_grid = parameters,\n                        scoring = 'accuracy',\n                        cv = 5,\n                        n_jobs = -1\n                        )\n    \n    grid.fit(x_train,y_train)\n\n    print('Best parameters: ', grid.best_params_) #Displaying the best parameters of the model\n\n    print(\"Accuracy: \", ((grid.best_score_)*100).round(2))#Accuracy of the model based on those parameters","9553f729":"from sklearn.ensemble import RandomForestClassifier\n\nparam_rf = {\n    'n_estimators': [10,50,100,500,1000],\n    'min_samples_leaf': [1,10,20,50]\n    }\nrf = RandomForestClassifier(random_state = 0)\ngrid_search(rf,param_rf)","ae5a8e94":"# Let's train our model using those parameters\n\nrf = RandomForestClassifier(min_samples_leaf = 10, n_estimators = 100)\nrf.fit(x_train, y_train)\n\nmodel_report(rf)","06333662":"from sklearn.neighbors import KNeighborsClassifier\n\nn_neighbors = list(range(5,10))#This is basically the value of k\n                   \nparam_knn = {\n    'n_neighbors' : n_neighbors,\n    'p' : [1,2]\n    \n    }\n\nknn = KNeighborsClassifier(algorithm ='auto', n_jobs = -1)\ngrid_search(knn,param_knn)","4b1f82da":"# Let's train our model using those parameters\n\nknn = KNeighborsClassifier(n_neighbors = 6, p = 2)\n\nknn.fit(x_train, y_train)\n\nmodel_report(knn)","853e9ff0":"from sklearn.svm import SVC\n\nparam_svc = {\n    'C': [0.1, 1, 10, 100],  \n    'gamma': [0.0001, 0.001, 0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9], \n    'kernel': ['linear','rbf']\n    }\nsvc = SVC()\n\ngrid_search(svc,param_svc)","033927b8":"#Let's train our model using these parameters\n\nsvc = SVC( C = 100, gamma = 0.001, kernel = 'rbf')\n\nsvc.fit(x_train, y_train)\n\nmodel_report(svc)","61575774":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier(random_state = 42, learning_rate = 0.01)\n\nmodel_report(gb)","a976e34a":"from xgboost import XGBClassifier\n\nparam_xg = {\n    'learning_rate' : [0.1,0.2,0.01,0.02],\n    'booster' : ['gbtree', 'gblinear'],\n    'min_child_weight' : [3,4,5,6,7],\n    'max_depth' : [3,4,5,6,7,8],\n    'gamma' : [0.1,0.2,0.01,0.02],\n    'subsample' : [0.5,0.6,0.7]\n}\n\nxg = XGBClassifier()\n\ngrid_search(xg,param_xg)","758fa57f":"\n\nxg = XGBClassifier(learning_rate = 0.1, booster = 'gbtree', \n                   min_child_weight = 3, gamma = 0.2, max_depth = 8, subsample = 0.7)\n\nmodel_report(xg)","1a73a68a":"y_pred = xg.predict(x_test)","2b07c31e":"submission = pd.DataFrame({'PassengerId': test_id['PassengerId'],'Survived':y_pred})\n\nsubmission.to_csv('xgboost_submission.csv', index=False)","a9405986":"# Data Collection","a5b18dd3":"######     ","835909c0":"##### Pclass has a highly negative correlation where as Fare has a highly positive correlation.\n##### This is expected as we have seen earlier that the higher the Pclass , the lower is the survival rate and vice versa. Similarly, The lower the Fare , the lower is the survival rate and vice versa.","948be649":"##  ","0d4afcdf":"# Data Description","073dcdf5":"###   ","9082554b":"#### Now, We will use 2 boosting algorithms","b66808f3":"####  I submitted the predictions of both support vector classifier and xgboost classifier. Though cross validation score of SVC is higher than XGBoost, I got a score of 0.79665 upon submission of the predictions from xgboost which turned out to be more that SVC. \n##### Placed in the top 7% of the participants in this competition.","b43d9c81":"#### 2. K-Nearest neighbors","3a61e636":"#### We will now explore the distribution of the age column.","65d5f914":"#   ","c71805ac":"#### Age feature","0f13a89a":"#### 4. Gradient Boost","f34067ea":"#### Let's explore the name feature","d6c12b86":"##  ","17693ef3":"# Importing Libraries","b5973293":"#### Parch Feature and SibSp feature","d1a0c6be":"#### 1. Random Forest ","70d82626":"#  ","41a27067":"The survival rate of children is almost 50% whereas among the adults , most of them did'nt survive. The elderly people were very less in numbers and most then could'nt make it.","73e4442d":"##### Let's explore the ticket column","cef799f4":"### Handling missing values","2f5b7f37":"#### 3. Support Vector Classifier","18deff5e":"**For the PClass, we know there are 3 main classes. Let's change their names and further create their dummies.**","db1c4def":"Most of the people in that ship were between 20 and 40 years old. Some childrens were there and a very few number of elderly people.","39dbcd5b":"## Correlation among numeric features","f20369e6":"#   ","e8a5b57c":"These are all the unique titles of the passengers. Let's see their count values","7a81c830":"######  ","b08ebc2b":"##### This is more or less a balanced dataset as there is a 45-55 split among the two classes.","eeeca879":"######  ","8c954825":"##### An interesting fact here is that though survival rate of passengers in first class is high , passengers in Class 3 tend to have an extremly high rate of demise. Passengers in Class 2 have more or less same rate of survival.","6123d0e2":"#  ","69e8cfda":"##### Let's explore the sex column and see how many males and females were on-board.","d8f5565b":"#  ","5cf6d359":"######  ","09a23bf9":"**There are many titles that are similar to each other. Let's map them together so that we get a few number of unique titles.**","da3fe6aa":"#  ","fada6d48":"#### Let's observe which class of passengers have a higher survival rate . Does 1st class passengers given priority over others?","69c9b19f":"## Data Preprocessing","26261aab":"#### There is an immense number of missing values in the Cabin feature . It is also not an important feature in our analysis and so we should just drop the column","36439505":"### Loading data","223df213":"Let's find out how many males and females survived.","ce0f4da5":"#   ","d4279bb4":"### ML models:\n\n**We will first use the following ML models:**\n\n    \n    1. Random Forest\n\n    2. K-Nearest Neighbors\n    \n    3. Support Vector Classifier\n\n    4. Gradient Boost\n\n    5. XGBoost","d8c8bf8e":"####  For this we will use GridSearchCV \n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.","c4ce37a3":"######  ","8f564be6":"We will Just convert them into dummies","f43dff8a":"######  ","d276ef48":"##### There is only one missing value in Fare column in test set. We will fill that with the median fare value as median is'nt affected by outliers","9f4ff75b":"**We need to do the same things for test data as well**","482e26ea":"#### Upvote if you find this notebook useful. It increases motivation! :)","e8363fba":"## Model building","d52162a2":"#### 5. XGBoost","273c4f28":"These are basically full names of passengers. We will extract only the title part","54298ebd":"##### We will fill the NAN values with the mean age","aa6bd076":"### Scaling the data","f4be3c57":"We can see that cabin feature has a lot of missing values.","510c4bea":"##### Since most people are from Southampton, we will fill those missing places with 'S'","f3f45830":"###### We won't be needing the PassengerId column as well from the training data as well as test data but we will preserve the test dataset for the Id's","4d1c848a":"These are practically some numbers with text data which have no hidden implementations.\nIt's better to drop this feature.","7b158fe4":"######  ","e926b608":"**Seems like these two features does'nt hold a good correlation with our survival rate. Instead we could sum them up as one 'Family' feature.**","c5e7e4db":"### Hyper Parameter tuning:","88abc12f":"##### We won't be needing that 'Age_groups' column so will just drop it.","a36e86fd":"######   ","761067a8":"**Note: Whatever data manupulation we do in the training data, we must do the same in our test data.**","510b269e":"#    ","2355d685":"**Embarked Feature**","5b04c42a":"#   ","70bbc375":"#### As we will be predicting the survival rate , let's see how balanced the data is.","30507680":"##### Majority of the people who paid a lower fare tend to not survive at all.","00131bab":"**For the sex column , we know there are only 2 categorical values male and female. Convert them to dummies.**","90a27053":"#     ","639df121":"**There are 2 missing values in Embarked column of the training data. Let's check which Embarked value occurs the most**","add262fe":"It is seen that females tend to have a higher survival rate than males . It makes sense as females were given the first priority while most of the males stayed behind to ensure most of the people get to safety.","f310c13e":"Looks like there are more males than females on the ship.","281615cb":"**We will convert these into dummy variables using get_dummies from pandas. This technique is known as one-hot encoding.**","f71b21af":"# Exploratory Data Analysis","7ee3053d":"##### Let's observe the survival rate of Children , adults and old aged people. ","aadcc395":"### Feature Engineering"}}