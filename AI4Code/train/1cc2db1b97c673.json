{"cell_type":{"ad8960b8":"code","3ec1154a":"code","6c4efc7a":"code","3b63c12a":"code","e5ff1ed3":"code","eaff8729":"code","cb26eb68":"code","dca67140":"code","31e79d69":"code","9dd9f8e5":"code","bf04c3d6":"code","33a47c1a":"code","5d636ce3":"code","991dc491":"code","befc16b9":"code","20a08b9c":"code","e21bc244":"code","3c0b689d":"code","4853f9dd":"code","2f3fe2b7":"code","d73bb476":"code","bf4e59c4":"code","fe18fa33":"code","23dbd65d":"code","c12a7c89":"code","bcb5508a":"code","2ae8eb27":"code","7415ec58":"code","52c2e786":"code","ec153976":"code","c88f8c57":"code","c0df75aa":"code","11e505f2":"code","74ba5e46":"code","ca463bc2":"code","2af0f3ca":"code","848b95d1":"code","3541ebc8":"code","7abcf1f1":"code","f4482d3e":"code","3af5e614":"code","6edd977f":"code","a8651f88":"code","75bf60fd":"code","47dd7c21":"code","b8d8535d":"code","de374870":"markdown","1af06d4b":"markdown","2855c5f0":"markdown","e478dceb":"markdown","5eb161d5":"markdown","9ada81d7":"markdown","ed5af25f":"markdown","8d9a9277":"markdown","6f952c86":"markdown","f5dc9f33":"markdown","635e120b":"markdown","5329caa6":"markdown","5b9c8990":"markdown","0aee9c46":"markdown","d9facb02":"markdown","79c3f94a":"markdown","a9698a79":"markdown","c939c603":"markdown","f5f0c887":"markdown","93695c18":"markdown","4a2f584e":"markdown","d9a919f2":"markdown","b4b090a1":"markdown","e6578f39":"markdown","08d7f9dd":"markdown","746947db":"markdown","fa57bcf8":"markdown","d8c7729f":"markdown","9376505b":"markdown","6822774b":"markdown","2a42fba5":"markdown","15146b2f":"markdown","0ee6314f":"markdown","fdf1bbec":"markdown"},"source":{"ad8960b8":"import requests\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import ShuffleSplit, cross_val_score, train_test_split, KFold, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor","3ec1154a":"dataset = pd.read_csv('..\/input\/life-expectancy-who\/Life Expectancy Data.csv')\ndataset","6c4efc7a":"list(dataset.columns)","3b63c12a":"def rename_col_names(x):\n    out={}\n    for i in x:\n        out[i] = i.rstrip()\n    return out","e5ff1ed3":"col_names = list(dataset.columns)\ndataset.rename(columns=rename_col_names(col_names), inplace=True)","eaff8729":"col_names = list(dataset.columns)\ncol_names","cb26eb68":"dataset","dca67140":"numeric_cols = list(dataset.select_dtypes(include=np.number).columns)\nnumeric_cols.remove('Life expectancy') #target column\ncnt_numeric_cols = len(numeric_cols)\nfig, axes = plt.subplots(nrows=cnt_numeric_cols, ncols=3, figsize=(25,150))\nfig.tight_layout(pad=3)\n\nfor i in range(cnt_numeric_cols):\n    col = numeric_cols[i]\n    axes[i,0].set_title('{} Distribution'.format(col))\n    axes[i,0].set_xlabel(col)\n    sns.histplot(ax=axes[i,0], x=dataset[col])\n      \n    axes[i,1].set_title('{} Boxplot'.format(col))\n    axes[i,1].set_xlabel(col)\n    sns.boxplot(ax=axes[i,1], x=dataset[col])\n        \n    axes[i,2].set_title('{} Scatterplot'.format(col))\n    axes[i,2].set_xlabel(col)\n    sns.scatterplot(ax=axes[i,2], data=dataset, x=col, y='Life expectancy')\nplt.show()\nfig.savefig('Numerical Data Visualisation.jpeg', pil_kwargs={'quality': 95})","31e79d69":"print('column \\t nunique \\t unique \\t % of nunique in column \\t % of nan')\nprint('-'*100)\nfor i in dataset[numeric_cols]:\n    print(i,':',dataset[i].nunique(),dataset[i].unique(),dataset[i].nunique()*100\/13320,dataset[i].isna().sum()*100\/13320,end='\\n\\n\\n')","9dd9f8e5":"categorical_cols = list(dataset.select_dtypes('object'))\nless_category_cols = dataset[categorical_cols].columns[dataset[categorical_cols].nunique() < 200]\ncnt_less_category_cols = len(less_category_cols)\nfig, axes = plt.subplots(nrows=cnt_less_category_cols, ncols=3, figsize=(35, 20))\nfig.tight_layout(pad=3)\n\nfor i in range(cnt_less_category_cols):\n    col = less_category_cols[i]\n      \n    axes[i,0].set_title('{} Bargraph'.format(col))\n    axes[i,0].set_xlabel(col)\n    sns.barplot(ax=axes[i,0], data=dataset, x=col, y='Life expectancy')\n    \n    axes[i,1].set_title('{} Box Plot'.format(col))\n    axes[i,1].set_xlabel(col)\n    sns.boxplot(ax=axes[i,1], x=dataset[col], y=dataset['Life expectancy'])\n        \n    axes[i,2].set_title('{} Scatter Plot'.format(col))\n    axes[i,2].set_xlabel(col)\n    sns.scatterplot(ax=axes[i,2], data=dataset, x=col, y='Life expectancy')\n    \nplt.show()\nfig.savefig('Categorical Data Visualisation.jpeg', pil_kwargs={'quality': 95})","bf04c3d6":"dataset = dataset[dataset['Life expectancy'].notna()].copy()\ndataset","33a47c1a":"dataset.isna().sum()","5d636ce3":"dataset.isna().sum() \/ dataset.shape[0] * 100","991dc491":"columns_with_null = list(dataset.columns[dataset.isna().any()])\ndataset[columns_with_null] = dataset.groupby('Country')[columns_with_null].transform(lambda x:x.fillna(x.mean()))\ndataset","befc16b9":"dataset.isna().sum()","20a08b9c":"dataset.isna().sum() \/ dataset.shape[0] * 100","e21bc244":"dataset.dropna(inplace=True)","3c0b689d":"dataset.isna().sum() \/ dataset.shape[0] * 100","4853f9dd":"dataset","2f3fe2b7":"def status_encode(x):\n    if(x=='Developed'):\n        return 1\n    else:\n        return 0","d73bb476":"dataset['Status'] = dataset['Status'].apply(status_encode)\ndataset","bf4e59c4":"encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\nencoder.fit(dataset[['Country']])","fe18fa33":"encoded_cols = list(encoder.get_feature_names())\nencoded_cols","23dbd65d":"dataset[encoded_cols] = encoder.transform(dataset[['Country']])\ndataset = dataset.drop('Country',axis=1)\ndataset","c12a7c89":"def identify_cols(dataset):\n    col_names = list(dataset.columns)\n    input_cols = col_names.copy()\n    input_cols.remove('Life expectancy')\n    target_col = 'Life expectancy'\n    #encoded_cols\n    X = dataset[input_cols]\n    Y = dataset[target_col]\n    return  X, Y","bcb5508a":"X, Y = identify_cols(dataset)","2ae8eb27":"model_df=pd.DataFrame()\ndef train_validate(X, Y,stri):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=0.2)\n    #X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n    model = LinearRegression()\n    model.fit(X_train, Y_train)\n    df = pd.DataFrame({'Train Accuracy':[model.score(X_train, Y_train)*100],'Test Accuracy':[model.score(X_test, Y_test)*100]},index=[stri])\n    return df","7415ec58":"model_df = pd.concat([model_df, train_validate(X,Y,'Base Model')])\nmodel_df","52c2e786":"base_acc = abs(model_df['Train Accuracy'][0]-model_df['Test Accuracy'][0])\nbase_acc","ec153976":"discrete_features = (X.dtypes == 'int64') #finding the discrete columns\ndef find_mi_scores(X, Y, discrete_features):\n    mi_scores = mutual_info_regression(X, Y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","c88f8c57":"mi_scores = find_mi_scores(X, Y, discrete_features)\nmi_scores","c0df75aa":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks=list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title('Mutual Information Scores')\nplt.figure(dpi=100, figsize=(8,5))\nplot_mi_scores(mi_scores[:20])","11e505f2":"X, Y = identify_cols(dataset.drop(encoded_cols, axis=1))\ntrain_validate(X, Y, 'Without Country')","74ba5e46":"model_df.sort_values(by=['Train Accuracy','Test Accuracy'], ascending=False)","ca463bc2":"col_names = list(mi_scores.index)\nfor i in range(len(col_names)-1,-1,-1):\n    col = col_names[i]\n    if(col not in encoded_cols):\n        val_col = dataset[col].copy()\n        dataset = dataset.drop(col, axis=1)\n        X, Y = identify_cols(dataset)\n        tr,te = train_validate(X,Y,'Without ' + col + ' :')['Train Accuracy'],train_validate(X,Y,'Without ' + col + ' :')['Test Accuracy']\n        err = abs(tr[0]-te[0])\n        if(base_acc <= err):\n            base_acc = err\n        else:\n            dataset[col] = val_col","2af0f3ca":"col_to_consider =[i for i in dataset.columns.to_list() if i not in encoded_cols]\nprint('Columns Which are to be considered (after Feature Engineering)\\n',col_to_consider)","848b95d1":"len(col_to_consider)","3541ebc8":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=42, test_size=0.2)\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape","7abcf1f1":"model = LinearRegression()\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :',model.score(X_train, Y_train)*100)\nprint('Test Accuracy :',model.score(X_test, Y_test)*100)","f4482d3e":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\nkf_sc = cross_val_score(LinearRegression(), X, Y, cv=kf)\nprint('Accuracy :',kf_sc.mean()*100)","3af5e614":"cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\ncv_sc = cross_val_score(LinearRegression(), X, Y, cv=cv)\nprint('Accuracy :',cv_sc.mean()*100)","6edd977f":"def find_best_model(X, Y):\n    algos={\n        'linear_reg':{'model':LinearRegression(), 'params':{'normalize':[True, False]}},\n        'lasso':{'model':Lasso(), 'params':{'alpha':[1,2], 'selection':['random', 'cyclic']}},\n        'decision_tree':{'model':DecisionTreeRegressor(), 'params':{'criterion':['mse','friedman_mse'],'splitter':['best','random']}},\n        'random_forest':{'model':RandomForestRegressor(), 'params':{'n_jobs':[-1], 'n_estimators':[10, 50, 100],'max_depth':[5,10,20], 'max_leaf_nodes':[50, 100]}},\n        'xgb':{'model':XGBRegressor(), 'params':{'n_jobs':[-1], 'n_estimators':[10,50,100],'max_depth':[5,10,20],'max_leaf_nodes':[50,100],'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.55],'booster':['gblinear']}}\n            }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, random_state=42, test_size=0.2)\n    for algo, config in algos.items():\n        gs = GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X, Y)\n        scores.append({\n            'model':algo,\n            'best_score': gs.best_score_,\n            'best_params':gs.best_params_\n        })\n    return pd.DataFrame(scores, columns=['model','best_score', 'best_params'])\nmodels_summary = find_best_model(X, Y)","a8651f88":"models_summary","75bf60fd":"model = LinearRegression(normalize=True)\nmodel.fit(X_train, Y_train)\nprint('Training Accuracy :',model.score(X_train, Y_train)*100)\nprint('Test Accuracy :',model.score(X_test, Y_test)*100)","47dd7c21":"Y_train.values,model.predict(X_train)","b8d8535d":"Y_test.values,model.predict(X_test)","de374870":"# Grid Search CV","1af06d4b":"### Categorical Columns\nLesser categorical valued columns only can be visualized properly.<br>\nSo visualize only those columns with lesser categories (but let's say threshold 200 here -explained in Note).<br>\n<b>Note:<\/b> This throws error when only one categorical column satisfy with threshold since it will be 1D which contradicts with the below code for 2D.<br>\nThe plots for the categorical column country is included below because there will be only one categorical column that satisfy lesser nuniques which throws error.","2855c5f0":"#### Null Values Tratement","e478dceb":"## MI Scores","5eb161d5":"```\nDATASET_URL = 'https:\/\/raw.githubusercontent.com\/avinash-218\/Life-Expectancy-WHO\/master\/Life_Expectancy_Data.csv'\nreq = requests.get('https:\/\/raw.githubusercontent.com\/avinash-218\/Life-Expectancy-WHO\/master\/Life_Expectancy_Data.csv')\nurl_content = req.content\ncsv_file = open('Life_Expectancy_Data.csv','wb')\ncsv_file.write(url_content)\ncsv_file.close()\n```","9ada81d7":"### Model without Numerical Columns (Auto)","ed5af25f":"# Import Dataset","8d9a9277":"# K-Fold Cross Validation","6f952c86":"# Cross Validation","f5dc9f33":"#### Encoding Status Column","635e120b":"#### Removing Trailing Spaces in Column Names\nSome column names in the dataset contains trailing space. So let's remove the trailing spaces","5329caa6":"Even now some data are NaN. This is because for some countries these columns were not measured. So just drop them","5b9c8990":"# Train Test Split","0aee9c46":"Let's Analyse and continue the preprocessing","d9facb02":"# Data Analysis","79c3f94a":"##### Base Model","a9698a79":"Percentage of null values in each columns","c939c603":"# Identifying Input & Target Column(s)","f5f0c887":"# Data Cleaning","93695c18":"### Model without Categorical Columns (manual)","4a2f584e":"<b> Note :<\/b><br>\nAxes, title might not be visible in the saved image if your windows is in dark mode and the image launcher is the default windows program. Try to open with paint if these are not visible.<bt>\nOpening in the notebook also helps.","d9a919f2":"# Predicting The Age of Death - Linear Regression\nDataset : https:\/\/www.kaggle.com\/kumarajarshi\/life-expectancy-who","b4b090a1":"### Numeric Columns","e6578f39":"Below Code displays nunique, unique, % of nunique in the column, % of nan for each columns.","08d7f9dd":"Fill missing values grouped by countries.<br>\nEg: Fill missing values in GDP based on the same country.<br>\nRemove the rows in which the target column is NaN","746947db":"### Feature Selection\nDiscard the features which are causing irrelevant contribution to the dataset.<br>\n(Negative Impact or no Impact)","fa57bcf8":"# Final Best Model","d8c7729f":"Accuray decreases so this column should be considered","9376505b":"From the summary above,<br>\n<b>Linear Regression<\/b> with parameter(s) : normalize:'False' gives the best result","6822774b":"```\ncol_names = list(mi_scores.index)\nfor i in range(len(col_names)-1,-1,-1):\n    col = col_names[i]\n    if(col not in encoded_cols):\n        X, Y = identify_cols(dataset.drop(col, axis=1))\n        model_df = pd.concat([model_df, train_validate(X,Y,'Without ' + col + ' :')])\n```","2a42fba5":"##### Model without Country Column(original + encoded)","15146b2f":"The bar graph is plotted in descending order.<br>\nSo, the rest of the encoded Country Columns are still lesser.<br>\nSo it is not necessary to visualize the MI Score of them too.","0ee6314f":"# Feature Engineering\nscikit-learn's mutual_info_regression and mutual_info_classif treat discrete and continuous values differently. So it is required to inform which are discrete columns.","fdf1bbec":"# Importing Libraries"}}