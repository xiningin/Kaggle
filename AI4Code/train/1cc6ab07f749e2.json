{"cell_type":{"6df85246":"code","ef62d3eb":"code","6e4b7ed1":"code","d3c9303f":"code","601f38fd":"code","37986e67":"code","fe7a7955":"code","30593aea":"code","fc6cdc79":"code","356a8818":"code","49d05523":"code","385cf76a":"code","43f97537":"code","11ecd93e":"code","3bc78728":"code","6a9c1fdc":"code","6d388bba":"code","daac4fad":"code","9651161c":"code","074cba5f":"code","485f37ff":"code","39b191e8":"code","f37ef4ee":"code","de728fc8":"code","d85c51fb":"code","5b478e50":"code","93102bb2":"code","8f067259":"code","373f5b4b":"code","fa35cf88":"code","02e27ee2":"code","fe6dc4c4":"code","aa5f2eff":"code","7620224b":"code","88d0f873":"code","115e9047":"code","75ee6afc":"code","e06315a4":"code","03d5c503":"code","8639d1b0":"code","2f5b8fb7":"code","8fc75f74":"markdown","e884feb3":"markdown","e57a2615":"markdown","f4147862":"markdown","cdd8db0c":"markdown","fb790a70":"markdown","7a4e24b7":"markdown","e8e2fb5f":"markdown","463548f2":"markdown","aaea5966":"markdown","2bfd2edc":"markdown","4b6362cc":"markdown","7bd692e7":"markdown","299c6137":"markdown","7d4e299f":"markdown","c17b2e6f":"markdown","e7f5bb21":"markdown","3c2eee22":"markdown","b4a2a915":"markdown","5371100d":"markdown","f358a929":"markdown","7a9c68e2":"markdown","2e15dcf6":"markdown","371a5d98":"markdown","4be27565":"markdown","cd82c059":"markdown","57376365":"markdown","dbcd3044":"markdown","2cb6eca9":"markdown","ba0606c5":"markdown","136ee743":"markdown","9d70f1ce":"markdown","63591017":"markdown","368ff12c":"markdown","bc450819":"markdown","8d3be657":"markdown","5512adfa":"markdown","5954e98c":"markdown","06d9cf93":"markdown","2561b8f8":"markdown","4e0d6698":"markdown","48cd8a73":"markdown","3a197290":"markdown","dde68123":"markdown"},"source":{"6df85246":"# iports that we need:\n# plotting:\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n# data analysis\nimport numpy as np\nimport pandas as pd\n\n#ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# so we can import data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# load the data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","ef62d3eb":"# first look:\ntrain.describe(include='all')","6e4b7ed1":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(train.corr(), annot=True, linewidth=0.5, fmt='.2f', ax=ax)\nax.set_ylim(7,0);","d3c9303f":"def make_plot(x, y='Survived', df=train):\n    sns.barplot(x=x, y=y, data=df)","601f38fd":"make_plot('Sex')","37986e67":"make_plot('Pclass')","fe7a7955":"make_plot('Parch')","30593aea":"make_plot('SibSp')","fc6cdc79":"# making the bins:\nbins = [-np.inf, 0, 5, 12, 18, 25, 35, 55, np.inf]\nlabels = ['unknown', 'infant', 'child', 'teenager', 'student', 'young_adult', 'adult', 'old']\n\nfor data in [train, test]: # note I do this to the test set too\n    data['AgeGrp'] = pd.cut(data['Age'], bins=bins, labels=labels)","356a8818":"fig, ax = plt.subplots(figsize=(10,4))\nsns.barplot(x=\"AgeGrp\", y='Survived', hue=\"Sex\", data=train, ax=ax);\n# survival accounting for gender","49d05523":"sns.barplot(x=\"AgeGrp\", y='Survived', data=train);\n# total survival","385cf76a":"pd.DataFrame(train.isnull().sum(), columns=['Train']).join(\n                pd.DataFrame(test.isnull().sum(), columns=['Test'])\n                )","43f97537":"test[test['Fare'].isnull()]","11ecd93e":"median_fare = test.groupby(['Pclass']).Fare.median()[3] # as he was in 3rd class\ntest['Fare'].fillna(median_fare, inplace=True)","3bc78728":"train[train['Embarked'].isnull()]","6a9c1fdc":"sns.countplot(x='Embarked', data=train);","6d388bba":"train['Embarked'] = train['Embarked'].fillna('S')","daac4fad":"train['Age'] = train.groupby(['Pclass', 'SibSp'])['Age'].apply(lambda x: x.fillna(x.median()))\ntrain[train['Age'].isnull()]","9651161c":"train['Age'].fillna(11, inplace=True)\ntrain['Age'].isnull().sum()","074cba5f":"test['Age'] = test.groupby(['Pclass', 'SibSp'])['Age'].apply(\n    lambda x: x.fillna(x.median()))\ntest['Age'].isnull().sum()","485f37ff":"for data in [train, test]:\n    for feature in ['PassengerId', 'Cabin', 'AgeGrp']:\n        data.drop(feature, inplace=True, axis=1)","39b191e8":"train.head()","f37ef4ee":"for dataset in [train, test]:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# code from https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions    \n\npd.crosstab(train['Title'], train['Sex'])","de728fc8":"# grouping the uncommon names:\ntrain['Title'] = train['Title'].replace(['Ms', 'Mlle'],'Miss')\ntrain['Title'] = train['Title'].replace(['Mme'],'Mrs')\ntrain['Title'] = train['Title'].replace(['Dr','Rev','the','Jonkheer','Lady','Sir', 'Don', 'Countess'],'Nobles')\ntrain['Title'] = train['Title'].replace(['Major','Col', 'Capt'],'Navy')\ntrain.Title.value_counts()","d85c51fb":"sns.barplot(x = 'Title', y = 'Survived', data=train);","5b478e50":"# and for the tesst data - not all are present:\n\ntest['Title'] = test['Title'].replace(['Ms','Dona'],'Miss')\ntest['Title'] = test['Title'].replace(['Dr','Rev'],'Nobles')\ntest['Title'] = test['Title'].replace(['Col'],'Navy')\ntest.Title.value_counts()","93102bb2":"categorical_features = [ 'Sex', 'Title', 'Embarked']\n\nfor feature in categorical_features:\n    dummies = pd.get_dummies(train[feature]).add_prefix(feature+'_')\n    train = train.join(dummies)\n\nfor feature in categorical_features:\n    dummies = pd.get_dummies(test[feature]).add_prefix(feature+'_')\n    test = test.join(dummies)","8f067259":"for data in [train, test]:\n    for feature in ['Name', 'Sex', 'Title', 'Embarked', 'Ticket']:\n        data.drop(feature, axis=1, inplace=True)","373f5b4b":"# independant and dependant variables:\nX = train.drop('Survived', axis=1)\ny = train['Survived']","fa35cf88":"# this will split the model when we want to check our model.\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.22, random_state = 0)","02e27ee2":"from sklearn.metrics import accuracy_score","fe6dc4c4":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_val = sc.transform(X_val)\ntest = sc.transform(test)","aa5f2eff":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\nround(accuracy_score(y_pred, y_val) * 100, 2)","7620224b":"# decision tree:\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, KFold\n\n\n#Here we will use gridsearchcv to find the best values for our hyperparameter\n# kfold is for its internal validation:\ncv = KFold(n_splits=10, shuffle=True, random_state=42)\n\nparams = dict(max_depth=range(1,10),\n              max_features=[2, 4, 6, 8],\n              criterion=['entropy', 'gini']\n             )\nDTGrid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nDTGrid.fit(X_train, y_train)\nDecTree = DTGrid.best_estimator_\nprint(DTGrid.best_params_)\nround(DecTree.score(X_val, y_val) * 100, 2)","88d0f873":"# random forest, using grid search as above:\nfrom sklearn.ensemble import RandomForestClassifier\n\ncv=KFold(n_splits=10, shuffle=True, random_state=42)\n\nparams = {'n_estimators': [80, 100, 120, 140],\n              'max_depth': range(2,7),\n              'criterion': ['gini', 'entropy']      \n        }\n\n\nRFGrid = GridSearchCV(RandomForestClassifier(random_state=42),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nRFGrid.fit(X_train, y_train)\nRandForest = RFGrid.best_estimator_\nprint(RFGrid.best_params_)\nround(RandForest.score(X_val, y_val) * 100, 2)","115e9047":"# k nearest neighbors:\nfrom sklearn.neighbors import KNeighborsClassifier\n\nparams = dict(n_neighbors=[3,6,8,10],\n              weights=['uniform', 'distance'],\n              metric=['euclidean', 'manhattan']\n              )\ncv=KFold(n_splits=10, shuffle=True, random_state=42)\n\nKNNGrid = GridSearchCV(KNeighborsClassifier(),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nKNNGrid.fit(X_train, y_train)\nKNN = KNNGrid.best_estimator_\nprint(KNNGrid.best_params_)\nround(KNN.score(X_val, y_val) * 100, 2)","75ee6afc":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nlogreg.score(X_val, y_val)","e06315a4":"# Support Vector Machines:\nfrom sklearn.svm import SVC\n\nparams = {'C':[100,500,1000],\n          'gamma':[0.1,0.001,0.0001],\n          'kernel':['linear','rbf']\n          }\ncv=KFold(n_splits=10, shuffle=True, random_state=42)\n\nSVMGrid = GridSearchCV(SVC(random_state=42),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nSVMGrid.fit(X_train, y_train)\nSVM = SVMGrid.best_estimator_\nprint(SVMGrid.best_params_)\nround(SVM.score(X_val, y_val) * 100, 2)","03d5c503":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nparams = {'loss':['hinge', 'perceptron'],\n          'alpha':[0.01, 0.001, 0.0001],\n          'penalty':['l2', 'l1']\n          }\ncv=KFold(n_splits=10, shuffle=True, random_state=42)\n\nSGDGrid = GridSearchCV(SGDClassifier(random_state=42),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nSGDGrid.fit(X_train, y_train)\nSGD = SGDGrid.best_estimator_\nprint(SGDGrid.best_params_)\nround(SGD.score(X_val, y_val) * 100, 2)","8639d1b0":"# gradient Boosting:\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nparams = {'loss':[ 'deviance', 'exponential'],\n          'learning_rate':[ 0.1, 0.01, 0.001],\n          'n_estimators':[100, 400, 700]\n          }\n\ncv=KFold(n_splits=10, shuffle=True, random_state=42)\n\nGBGrid = GridSearchCV(GradientBoostingClassifier(random_state=42),\n                    param_grid=params, verbose=False,\n                    cv=cv)\n\nGBGrid.fit(X_train, y_train)\nGB = GBGrid.best_estimator_\nprint(GBGrid.best_params_)\nround(GB.score(X_val, y_val) * 100, 2)","2f5b8fb7":"model = SGDClassifier(**SGDGrid.best_params_)\nmodel.fit(X, y)\n\ntest_id = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmission = pd.DataFrame({'PassengerId': test_id['PassengerId'], 'Survived': model.predict(test) })\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\nsubmission.to_csv('submission.csv', index=False)","8fc75f74":"We still have some NAN values, because there are no age values for all members of the group Pclass=3 & SibSp=8. We can fill these in manually:","e884feb3":"# Missing Values","e57a2615":"# Analysing the data:","f4147862":"Age is a bit more difficult, as this is a quasi-continous feature. One way is to make bins, i.e. group ages together.","cdd8db0c":"There are a lot of missing values here, so we can't do these by hand. For simplicity, setting each value to the median of their Pclass and SibSp<br> (from the heatmap above, these are the two most correlated numerical variables)","fb790a70":"From the heatmap at the start, fare correlates well with passenger class. So we'll make a guess at his fare, from the median fare for his passenger class:","7a4e24b7":"## Encoding the features:","e8e2fb5f":"Notice that there is a large amount of varience for larger numbers. I think this is bacause there is less data for these. Also, most groups wouldn't all be the same gender, making their chances of survival quite variable. Nevertheless, this does tell us information and there is a definite pattern.","463548f2":"### Pclass","aaea5966":"Most of the feature variables don't correlate much with each other, which is good. There are a few exceptions, noticably Pclass with survived and Fare, and Parch with SibSp. It might be worth combining some of these features.","2bfd2edc":"Some notes and predictions to start off: <br><br> There are 891 entries in our training set. Most features (columns) don't have any missing values, only Age, Embarked and Cabin. <br> Cabin only has 204 values, we might want to drop this later on. <br><br> My initial thought are that Sex, Pclass and Age will be the most important factors, with the following effects:<br> - Females are more likely to survive<br> - Higher 'Passenger class' (i.e. 1st class) passengers will be more likely to survive<br> - Young childern will have a very high rate of survival","4b6362cc":"This is a small function to give us the accuracy of a given model:","7bd692e7":"### Numerical Variables","299c6137":"Cabin has almost no values in it. I had a quick google to see why, and this is because the only cabin log retrieved that was legible had information on the more luxury cabins. This means that we could turn this into a boolean variable, but this would correlate a lot with Pclass so instead I will delete it. We can also delete PassengerId because this by definition has no correlaion with survival. Also, we can get rid of our Age Group column, which we used earlier to help with visualisation, as an actual age will have far more information.","7d4e299f":"### Missing value: fare","c17b2e6f":"### Missing Values: Cabin","e7f5bb21":"The best predictor was Stochastic Graidient Descent, so we use this to predict our values. Note we will fit the entire training set, to get the best predictions:","3c2eee22":"We can then drop the original features:","b4a2a915":"### Missing values: embarked","5371100d":"### Age","f358a929":"Cleaning the data, and removing \/ guessing at the missing values.","7a9c68e2":"### Parents and Children, and Siblings and Spouses","2e15dcf6":"We do the same for the test set:","371a5d98":"This has an accuracy score of 78.4%, which is not bad for a first attempt!","4be27565":"# Model Selection:","cd82c059":"It's more difficult to find correlation with categorical data.","57376365":"I am relatively new to data science, this is my workings for my first Kaggle submission. I hope that it is helpfull to other beginner, and of course any comments or improvements would be greatly appreciated.","dbcd3044":"## Categorical Features:","2cb6eca9":"# Data Science Introduction","ba0606c5":"### Sex","136ee743":"## Looking at the Name variable:","9d70f1ce":"We see that, as expected, females are more likely to survive. But it is (at least to me) slightly surprising how big this difference is. Sex will definitely be a very important variable to use","63591017":"First, with a heatmap we can see how all of the numerical variables correlate with each other.","368ff12c":"To use machine learning, we have to have numerical not categorical freaures. There are two ways to do this, either:<br> - Replace categories by numbers<br> - One-hot encoding<br><br>I will use one-hot encoding as none of the following should be 'ordered', in the sense that you can't say:<br>embarked at Southampton > embarked at Cherbourg","bc450819":"First, we will have a quick look at iour data, and make our first predictions.","8d3be657":"First, we will split the training data so we can check how our models do.","5512adfa":"From the graph we can see that most passengers embarked at Southampton.","5954e98c":"### Missing Values: Age","06d9cf93":"I haven't scaled the data yet, this is next up:","2561b8f8":"I would have dropped this variable, but reading the many notebooks there is information to be found here:","4e0d6698":"As predicted, the better the ticket the higher the chance of survival. Note this is a cetegorical feature disguising itself as a numerical feature - as there is an order, i.e. 1st class > 2nd class > 3rd class, this is acceptable","48cd8a73":"As expected, infants had by far the highest total survival rates, but the top graph tells an interesting story. The female survival rate stays almost the same, except for children. But the male survival rate is very high for infants, and decreases quickly as age increases. So the main factor in the variatoin of survival rate are the male pasengers.","3a197290":"Now, we will try several types of machine learning algorithm to try and find the best. I am using Grid Search to find which values to use for model hyperparameters (these are values that we need to input). Because of this, the grid search code below takes a long time to run, especially on kaggle.","dde68123":"First, I'll make a function that will plot our categorical features against a numerical feature, to make visualisation easier."}}