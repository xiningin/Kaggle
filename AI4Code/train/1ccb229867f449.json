{"cell_type":{"48b895f9":"code","829c753c":"code","df62a2ae":"code","21892909":"code","90cf5aef":"code","a6331824":"code","96c14840":"code","3f57efce":"code","4f8e6571":"code","6871862b":"code","48798ec0":"code","2079715b":"code","f4a9b0a5":"code","91b591cb":"code","45b8637c":"code","fadd4253":"code","546db0ba":"code","9f2b6b4b":"code","a8c0c3c2":"code","0ee859f5":"code","5f032824":"code","effce6a7":"code","4c89c76b":"markdown","fa5ba9c0":"markdown","546a89f4":"markdown","a3a17914":"markdown","2c8f2f22":"markdown","db5d0a50":"markdown","08a31541":"markdown","ed945a5c":"markdown","b7a99a76":"markdown","e1235bc8":"markdown","dd3a0718":"markdown","c56deecb":"markdown","46883434":"markdown","cd571ce3":"markdown","fbf851ce":"markdown","5ff3e4ff":"markdown","8ddad057":"markdown","afb6e2a5":"markdown"},"source":{"48b895f9":"import pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom pickle import dump, load\nreviews = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\nprint(reviews.shape)\nprint(reviews.head())\nprint(reviews.isnull().sum())","829c753c":"reviews = reviews.dropna()\nreviews = reviews.drop(['Id','ProductId','UserId','ProfileName','HelpfulnessNumerator','HelpfulnessDenominator', 'Score','Time'], 1)\nreviews = reviews.reset_index(drop=True)\nprint(reviews.shape)\nprint(reviews.head())\nfor i in range(5):\n    print(\"Review #\",i+1)\n    print(reviews.Summary[i])\n    print(reviews.Text[i])\n    print()","df62a2ae":"contractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how does\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","21892909":"def clean_text(text, remove_stopwords = True):\n    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n    \n    # Convert words to lower case\n    text = text.lower()\n    \n    # Replace contractions with their longer forms \n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)\n    \n    # Format words and remove unwanted characters\n    text = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\<a href', ' ', text)\n    text = re.sub(r'&amp;', '', text) \n    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]\/]', ' ', text)\n    text = re.sub(r'<br \/>', ' ', text)\n    text = re.sub(r'\\'', ' ', text)\n    \n    # Optionally, remove stop words\n    if remove_stopwords:\n        text = text.split()\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n        text = \" \".join(text)\n\n    return text","90cf5aef":"# Clean the summaries and texts\nclean_summaries = []\nfor summary in reviews.Summary:\n    clean_summaries.append(clean_text(summary, remove_stopwords=False))\nprint(\"Summaries are complete.\")\n\nclean_texts = []\nfor text in reviews.Text:\n    clean_texts.append(clean_text(text))\nprint(\"Texts are complete.\")","a6331824":"# Inspect the cleaned summaries and texts to ensure they have been cleaned well\nfor i in range(5):\n    print(\"Clean Review #\",i+1)\n    print(clean_summaries[i])\n    print(clean_texts[i])\n    print()","96c14840":"stories = list()\nfor i, text in enumerate(clean_texts):\n    stories.append({'story': text, 'highlights': clean_summaries[i]})\n# save to file\ndump(stories, open('review_dataset.pkl', 'wb'))","3f57efce":"stories = load(open('review_dataset.pkl', 'rb'))\n#print('Loaded Stories %d' % len(stories))\n#print(type(stories))","4f8e6571":"def count_words(count_dict, text):\n    '''Count the number of occurrences of each word in a set of text'''\n    for sentence in text:\n        for word in sentence.split():\n            if word not in count_dict:\n                count_dict[word] = 1\n            else:\n                count_dict[word] += 1","6871862b":"word_counts = {}\n\ncount_words(word_counts, clean_summaries)\ncount_words(word_counts, clean_texts)\n            \nprint(\"Size of Vocabulary:\", len(word_counts))","48798ec0":"embeddings_index = []\n\n# Find the number of words that are missing from CN, and are used more than our threshold.\nmissing_words = 0\nthreshold = 20\n\nfor word, count in word_counts.items():\n    if count > threshold:\n        if word not in embeddings_index:\n            missing_words += 1\n            \nmissing_ratio = round(missing_words\/len(word_counts),4)*100\n            \nprint(\"Number of words missing from CN:\", missing_words)\nprint(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))","2079715b":"# Limit the vocab that we will use to words that appear \u2265 threshold or are in GloVe\n\n#dictionary to convert words to integers\nvocab_to_int = {} \n\nvalue = 0\nfor word, count in word_counts.items():\n    if count >= threshold or word in embeddings_index:\n        vocab_to_int[word] = value\n        value += 1\n\n# Special tokens that will be added to our vocab\ncodes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n\n# Add codes to vocab\nfor code in codes:\n    vocab_to_int[code] = len(vocab_to_int)\n\n# Dictionary to convert integers to words\nint_to_vocab = {}\nfor word, value in vocab_to_int.items():\n    int_to_vocab[value] = word\n\nusage_ratio = round(len(vocab_to_int) \/ len(word_counts),4)*100\n\nprint(\"Total number of unique words:\", len(word_counts))\nprint(\"Number of words we will use:\", len(vocab_to_int))\nprint(\"Percent of words we will use: {}%\".format(usage_ratio))","f4a9b0a5":"import numpy as np\n# Need to use 300 for embedding dimensions to match CN's vectors.\nembedding_dim = 300\nnb_words = len(vocab_to_int)\n\n# Create matrix with default values of zero\nword_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\nfor word, i in vocab_to_int.items():\n    if word in embeddings_index:\n        word_embedding_matrix[i] = embeddings_index[word]\n    else:\n        # If word not in CN, create a random embedding for it\n        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n        #embeddings_index[word] = new_embedding\n        word_embedding_matrix[i] = new_embedding\n\n# Check if value matches len(vocab_to_int)\nprint(len(word_embedding_matrix))","91b591cb":"def convert_to_ints(text, word_count, unk_count, eos=False):\n    ints = []\n    for sentence in text:\n        sentence_ints = []\n        for word in sentence.split():\n            word_count += 1\n            if word in vocab_to_int:\n                sentence_ints.append(vocab_to_int[word])\n            else:\n                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n                unk_count += 1\n        if eos:\n            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n        ints.append(sentence_ints)\n    return ints, word_count, unk_count","45b8637c":"word_count = 0\nunk_count = 0\n\nint_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\nint_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n\nunk_percent = round(unk_count\/word_count,4)*100\n\nprint(\"Total number of words in headlines:\", word_count)\nprint(\"Total number of UNKs in headlines:\", unk_count)\nprint(\"Percent of words that are UNK: {}%\".format(unk_percent))","fadd4253":"def create_lengths(text):\n    '''Create a data frame of the sentence lengths from a text'''\n    lengths = []\n    for sentence in text:\n        lengths.append(len(sentence))\n    return pd.DataFrame(lengths, columns=['counts'])","546db0ba":"lengths_summaries = create_lengths(int_summaries)\nlengths_texts = create_lengths(int_texts)\n\nprint(\"Summaries:\")\nprint(lengths_summaries.describe())\nprint()\nprint(\"Texts:\")\nprint(lengths_texts.describe())","9f2b6b4b":"# Inspect the length of texts\nprint(np.percentile(lengths_texts.counts, 90))\nprint(np.percentile(lengths_texts.counts, 95))\nprint(np.percentile(lengths_texts.counts, 99))","a8c0c3c2":"# Inspect the length of summaries\nprint(np.percentile(lengths_summaries.counts, 90))\nprint(np.percentile(lengths_summaries.counts, 95))\nprint(np.percentile(lengths_summaries.counts, 99))","0ee859f5":"def unk_counter(sentence):\n    '''Counts the number of time UNK appears in a sentence.'''\n    unk_count = 0\n    for word in sentence:\n        if word == vocab_to_int[\"<UNK>\"]:\n            unk_count += 1\n    return unk_count","5f032824":"# Sort the summaries and texts by the length of the texts, shortest to longest\n# Limit the length of summaries and texts based on the min and max ranges.\n# Remove reviews that include too many UNKs\n\nsorted_summaries = []\nsorted_texts = []\nmax_text_length = 84\nmax_summary_length = 13\nmin_length = 2\nunk_text_limit = 100 # use 1\nunk_summary_limit = 100 # use 0\n\nfor length in range(min(lengths_texts.counts), max_text_length): \n    for count, words in enumerate(int_summaries):\n        if (len(int_summaries[count]) >= min_length and\n            len(int_summaries[count]) <= max_summary_length and\n            len(int_texts[count]) >= min_length and\n            unk_counter(int_summaries[count]) <= unk_summary_limit and\n            unk_counter(int_texts[count]) <= unk_text_limit and\n            length == len(int_texts[count])\n           ):\n            sorted_summaries.append(int_summaries[count])\n            sorted_texts.append(int_texts[count])\n        \n# Compare lengths to ensure they match\nprint(len(sorted_summaries))\nprint(len(sorted_texts))","effce6a7":"for i in range(20):\n    print(\"Review #\",i+1)\n    print(clean_texts[i])\n    print(\"Summary #\",clean_summaries[i])\n    print()","4c89c76b":"Apply convert_to_ints to clean_summaries and clean_texts","fa5ba9c0":"Finally Showing the Summery and review","546a89f4":"Inspect the length of text","a3a17914":"Inspect the length of summaries","2c8f2f22":"6. Clean the summaries as shown in the following snippet:","db5d0a50":"Sort the summaries and texts by the length of the texts, shortest to longest\nLimit the length of summaries and texts based on the min and max ranges.\nRemove reviews that include too many UNKs","08a31541":"3. Replace contractions with their longer forms, as shown here:","ed945a5c":"## \ud83c\udfac Introdruction\n\nThis kernel will show you how to work on the problem of text summarization to create relevant summaries for product reviews about fine food sold on the world\u2019s largest e-commerce platform, Amazon. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. The notebook is excluding any model. Just preprocess the data for fit in a model. As I am a newbie to the world of ML, So in future I'm looking forward to it to build a model to fit the work.\n\nFind the repository in [Github](https:\/\/github.com\/Sabbir1996\/text_summarization)\n \n This dataset include the following:\n\u00b7 568,454 reviews\n\u00b7 256,059 users\n\u00b7 74,258 products","b7a99a76":"7. Finally, save all the reviews into a pickle file. pickle serializes objects so they can be saved to a file and loaded in a program again later on:","e1235bc8":"2. Remove null values and unneeded features, as shown in the following snippet:","dd3a0718":"## \ud83d\udcda Data processing \n\nIt is crucial that you serve the right data as input to the neural architecture for training and validation. Make sure that data is on a useful scale and format, and that meaningful features are included. This will lead to better and more consistent results.\n\nEmploy the following workflow for data preprocessing:\n\n1. Load the dataset using pandas\n\n2. Split the dataset into input and output variables for machine learning\n\n3. Apply a preprocessing transform to the input variables\n\n4. Summarize the data to show the change","c56deecb":"# Conclusion\n\nI hope that you found this project to be rather interesting and informative. \n\nThanks for reading this notebook. If you have any suggestion feel free to reach me in the comment. And don't forget to upvote. Stay in touch for more update. Thank you.  \ud83d\udc4d","46883434":"## Now get started step by step:\n\n1. Get started by importing important packages and your dataset. Use the pandas library to load data and review the shape of your dataset\u2014it includes 10 features and 5 million data points:","cd571ce3":"Convert words in text to an integer. If word is not in vocab_to_int, use UNK's integer. Total the number of words and UNKs. Add EOS token to the end of texts'''","fbf851ce":"Next, load the review dataset from the pickle file:","5ff3e4ff":"For making sure our data preprocessed well, we will count the number of occurrence of each word in a set of text","8ddad057":"4. Clean the text documents by replacing contractions and removing stop words:","afb6e2a5":"Find the number of times each word was used and the size of the vocabulary"}}