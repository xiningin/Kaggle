{"cell_type":{"b410d8bf":"code","34299bf6":"code","7f0b2d05":"code","4fda0423":"code","721c4901":"code","2bf3acf9":"code","bce095b9":"code","4b2067ba":"code","0eae214e":"code","019b7f3b":"markdown","1ba90f2c":"markdown"},"source":{"b410d8bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","34299bf6":"# The idea here is to convert an image into a compressed 1D representation from a 2D image.\n# The input image is converted to a 512 element vector from which we could hope to use a similarity metric\n# to compare against other 2D images. Using Resnet to extract the compressed representation.\nmodel = models.resnet18(pretrained='imagenet')","7f0b2d05":"content_pd = pd.read_csv('..\/input\/traincsv\/train.csv')\ncontent_pd.head()","4fda0423":"#Resize the image to 224x224 px\nscaler = transforms.Scale((224, 224))\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\n\n# Use the model object to select the desired layer\nlayer = model._modules.get('avgpool')\n\ndef extract_feature_vector(img):\n    # 2. Create a PyTorch Variable with the transformed image\n    #Unsqueeze actually converts to a tensor by adding the number of images as another dimension.\n    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\n\n    # 3. Create a vector of zeros that will hold our feature vector\n    #    The 'avgpool' layer has an output size of 512\n    my_embedding = torch.zeros(1, 512, 1, 1)\n\n    # 4. Define a function that will copy the output of a layer\n    def copy_data(m, i, o):\n        my_embedding.copy_(o.data)\n\n    # 5. Attach that function to our selected layer\n    h = layer.register_forward_hook(copy_data)\n\n    # 6. Run the model on our transformed image\n    model(t_img)\n\n    # 7. Detach our copy function from the layer\n    h.remove()\n\n    # 8. Return the feature vector\n    return my_embedding.squeeze().numpy()","721c4901":"from IPython.display import Image\nfrom IPython.core.display import HTML \n\ndef display_category(urls, category_name):\n    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"\n    images_list = ''.join([f\"<img style='{img_style}' src='{u}' \/>\" for _, u in urls.head(12).iteritems()])\n\n    display(HTML(images_list))","2bf3acf9":"# display a few images in the url\nids =  content_pd.landmark_id.value_counts().keys()[25]\nurls = content_pd[content_pd.landmark_id == ids].url\ndisplay_category(urls, 'My Favourite')","bce095b9":"from PIL import Image\nimport requests\nfrom io import BytesIO\n\n#Download the image into memory given the URL.\ndef get_image_from_url(url):\n    response = requests.get(url)\n    return Image.open(BytesIO(response.content))","4b2067ba":"# Now that you have the means for extracting the 1D representation from the given input image, you can use the cosine_similarity metric to compute the distance between\n# the given image and all other images and sort them in the ascending order and return the results.\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\ndef get_cosine_distance(im_url1, im_url2):\n    im1, im2 = get_image_from_url(im_url1), get_image_from_url(im_url2)\n    image1 = extract_feature_vector(im1).reshape(1, -1)\n    image2 = extract_feature_vector(im2).reshape(1, -1)\n    return cosine_similarity(image1, image2)","0eae214e":"# Print the distance between any 2 landmark images\nids =  content_pd.landmark_id.value_counts().keys()[1]\nurls = content_pd[content_pd.landmark_id == ids].url\nurls = [i for i in urls]\nurl1, url2 = urls[0], urls[1]\nprint('Distance between 2 images :', get_cosine_distance(url1, url2))","019b7f3b":"Code here shows how one can tap on a specific layer on Resnet to extract the vectorized feature representation of an image. Once you manage to do this you will be able to use Cosine\/Euclidean distances to measure similarity between 2 images.","1ba90f2c":"* This notebook will show you how you can use Pytorch and a pretrained Resnet model to develop an algorithm that can help you compare 2 images.\n* Underlying concept is to convert a high dimensional image to a manageable representative set of features using a pretrained DNN. In this case I have chosen resnet18 (not resnet34\/50\/101\/152 - because of hardware limitations imposed by my laptop)\n* At work I had the opportunity to evalutate multiple different models - VGG16, VGG19 and InceptionV3, with respect to a retail use case where given a set of apparel data from one retailer, I had to find exact matches in another. Resnet50 gave me the best accuracy - consistently for multiple retailers. And I found it to be resilient to changes in image background, illumination etc which was great.\n* The idea here tries to exploit a vector space and plots each image in the high-dimensional vector space and use cosine distance to evaluate the distance between any 2 vectors."}}