{"cell_type":{"d855a682":"code","7ee7b5d0":"code","2ce61d77":"code","5f9254ca":"code","30302ea1":"code","1bc637d2":"code","073ec9d5":"code","63eb6197":"code","8fe8b18d":"code","828a2abc":"code","49486293":"code","250d1b6d":"code","5b1f60c8":"code","1be12fda":"code","f504057d":"code","0d89c38e":"code","4a276e33":"code","37611cdb":"code","49bcd1e1":"code","9a83e458":"code","d97cae94":"code","bfe5efad":"code","45d86ec5":"code","2ecccfd8":"code","fa5e6194":"code","0b24ae94":"code","a5b3b74d":"code","7fbedb61":"code","65ab3ddb":"code","405f0586":"code","230cc2ad":"code","b9eb5ec3":"code","1135f9da":"code","cdc57bdb":"code","de08512a":"code","53e2d752":"code","6c83e80d":"code","a38d78e6":"code","fdbe2a14":"code","accc6e96":"code","1ace5be8":"code","11125e0d":"code","d4a601ca":"code","722bdd4c":"code","76c9a579":"code","1cdad2fb":"code","dd5ca75d":"code","993cf6a8":"code","3b20e59a":"code","55b44802":"code","0f1b0faa":"code","be5a9fec":"code","c9f54a81":"code","7aa077ed":"code","e8bd5845":"code","64997070":"code","a5f00c1e":"code","6a53df91":"code","d72e1394":"code","a59d4558":"code","9ab03ed9":"code","059f3133":"code","9edf91c1":"code","de9f0b1f":"code","9ff16f03":"code","cbbf41bd":"code","a3c6d4d2":"code","37f7cb9c":"code","2e63e642":"code","ab4b0f10":"code","d02a4178":"code","27397ee6":"code","e11410d7":"code","376da541":"code","962ce833":"code","ca4f7cae":"code","ad863802":"code","f2241b29":"code","e054752c":"code","f09fc8a8":"code","a94ebeb1":"code","25320d17":"code","6fd36fba":"code","41937a57":"code","94cb479e":"code","fe0e9ef1":"code","c4151724":"code","6038d6e5":"code","150d90e1":"code","8dc8288c":"code","d798827c":"code","98ab4318":"code","26d4a035":"code","c22a907f":"code","632d8317":"code","b4f9a5c9":"code","0c00df0f":"code","30f413bb":"code","1fd3fa7c":"code","94ba7285":"code","b3bb60f4":"code","6b7ce30a":"code","9b3be7d7":"code","57afea11":"code","9ab82df9":"code","fc649ca6":"code","69467750":"code","9841e8f9":"code","8390e3a2":"code","088f08a2":"code","8fd309c1":"code","7f69b107":"code","316d4eea":"code","f661f302":"code","18ce7116":"code","122b0e9d":"code","7201f3e4":"code","030d51f7":"code","6c346569":"code","e3f1edaa":"code","1f135ec8":"code","59c8f2d1":"code","5465c828":"code","81343a6a":"code","5babd92d":"code","20bc7ed5":"code","d18961ed":"code","755be872":"code","2064e04e":"code","29539839":"code","b62f44c8":"code","a331045e":"code","4d4509c1":"code","83a8a463":"code","e3542de4":"code","0e0dea48":"code","3aaca72f":"code","7013a4e6":"code","31294c57":"code","9db7e4bc":"code","38a60f34":"code","997224ee":"code","ae6d70df":"code","d8bd8d75":"code","d77581e4":"code","4f7081a8":"code","e411667b":"code","6469c91d":"code","686ae433":"code","c9f27736":"code","e52c7e1f":"code","69d71785":"code","17097358":"code","fb0bc215":"code","af5e0256":"code","7e1728a8":"code","9795dde5":"code","70b786f2":"code","9b4c0dd6":"code","06dc78ea":"code","f471c778":"code","1ebcc889":"code","64d7bf9f":"code","a7d7a262":"code","098f652c":"code","f0a7c852":"code","128ce748":"code","2dba3c01":"code","3107f17f":"code","7524c827":"code","630554af":"code","768cf765":"code","29fc0ce3":"code","5bda1822":"code","7e880760":"code","78152025":"code","b3a3b365":"code","a88ab8b3":"code","8c47894b":"code","fc13a142":"code","3735c305":"code","4777528e":"code","8d113635":"code","0454d2a0":"code","75bc043c":"code","9f7f5229":"code","227bec5c":"code","fe5adb6f":"code","355fbc38":"code","328b6156":"code","12c5c11d":"code","57748867":"code","24d8deb8":"code","f4acb4f1":"code","67c60772":"code","695a3d0c":"code","175f7dfd":"code","a5a130cc":"code","55ed60e1":"code","fa97bcbb":"code","bab700b9":"code","84ca7965":"code","36492f05":"code","2d0bfc59":"code","37d0d4b9":"code","d25fdde0":"code","019252b3":"code","395bfc9c":"code","bdcc3637":"code","0f0d5029":"code","5a5cb8af":"code","fd82767e":"code","5ab996f2":"code","e9c50131":"code","115ae4f6":"code","a019ba6d":"code","901f714b":"code","b233c6fd":"code","63ee5577":"code","d3a19bce":"code","ab84608f":"code","7bb63a27":"code","6d6f17a6":"code","6877d753":"code","3ae252ce":"code","93cd75a0":"code","7c610fd8":"code","5bc120a8":"code","985bf34f":"code","7f95ed05":"code","7bee73cd":"code","2c828d62":"code","cba89ab0":"code","b8dddb7f":"code","53dafc22":"code","65e0884a":"code","bc7a53cd":"code","d156923e":"code","62656f54":"code","c04cc14d":"code","a76746ce":"code","1759ea83":"code","f626ae2a":"code","7fa0c5c2":"markdown","477f98ab":"markdown","bd29e8be":"markdown","55f8c6eb":"markdown","437273dc":"markdown","88c07670":"markdown","177a0ffc":"markdown","cdee382e":"markdown","6fd1c2a3":"markdown","06a38810":"markdown","d8c63611":"markdown","0fc4177c":"markdown","95636623":"markdown","820400e8":"markdown","9c3c6082":"markdown","c17b1f49":"markdown","73bbeff1":"markdown","7d882cab":"markdown","21aa8422":"markdown","614dd276":"markdown","d15b783b":"markdown","0230794c":"markdown","452b470e":"markdown","f42e9059":"markdown","9d2cf803":"markdown","0f512940":"markdown","bf597f6c":"markdown","06b73320":"markdown","f9b7b44a":"markdown","14bd4793":"markdown","90d32dc4":"markdown","bc3c3571":"markdown","4f1b722c":"markdown","5a50efc8":"markdown","3777df12":"markdown","ba2ee3c7":"markdown","a0cc46da":"markdown","bbf5765e":"markdown","f478caf4":"markdown","473b2011":"markdown","52fca86f":"markdown","8e5109ac":"markdown","14e5fad7":"markdown","acf8865b":"markdown","47915d3c":"markdown","3017ca11":"markdown","ef4a2fb5":"markdown","c6a76ecf":"markdown","1e2f9c15":"markdown","af646e84":"markdown","951ec6fd":"markdown","edb3a172":"markdown","e07c70dd":"markdown","a3720f1f":"markdown","90160a9b":"markdown","028d5adf":"markdown","5e4a7c6b":"markdown","296e3242":"markdown","4a134273":"markdown","d51b7f39":"markdown","279a42c9":"markdown","7f194793":"markdown","80e2aa9b":"markdown","e1ab6954":"markdown","2b373fd3":"markdown","0b59d3f8":"markdown","f32915dc":"markdown","f97d6f14":"markdown","bdc896be":"markdown","8c635d7c":"markdown","daa842d8":"markdown","0d9c48db":"markdown","234d3dcc":"markdown","31bbde9c":"markdown","bd09f457":"markdown","db575912":"markdown","87c1d5b8":"markdown","2127d1e5":"markdown","add878fe":"markdown","232b2e37":"markdown","e43db1d1":"markdown","2a4330db":"markdown","aa3f30c2":"markdown","3e422041":"markdown","7057ded8":"markdown","acdd87a7":"markdown","7bc4d55b":"markdown","05e4fef0":"markdown","2e47f482":"markdown","ff5b5cb9":"markdown","04ed2927":"markdown","43c7a63d":"markdown","81510295":"markdown","c704b0d1":"markdown","a0008aa4":"markdown","5e9c8e4e":"markdown","7412c928":"markdown","09a8015e":"markdown","34d3b574":"markdown","93b793fb":"markdown","9660f81a":"markdown","cc6b0ddb":"markdown","598a267e":"markdown","7a4aa486":"markdown","0dbb957d":"markdown"},"source":{"d855a682":"# if you need please install these libraries to import the following ones\n\n!pip install pyforest\n!pip install --user pycaret \n!pip install pandas_profiling\n!pip install squarify\n!pip install termcolor\n!pip install catboost\n!pip install pyclustertend\n\n# 1-Import Libraies\n\nimport pandas_profiling\nimport pyforest\n\nimport ipywidgets\nfrom ipywidgets import interact\n\nimport numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.ticker as mticker\nimport squarify as sq\n\n# Importing plotly and cufflinks in offline mode\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored\nfrom termcolor import cprint\n\nfrom wordcloud import WordCloud\n\nimport scipy.stats as stats\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nimport datetime as dt\nfrom datetime import datetime\n\nfrom pyclustertend import hopkins\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor \nfrom sklearn.ensemble import ExtraTreesRegressor, AdaBoostClassifier\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, f_regression, mutual_info_regression\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.metrics import plot_confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, classification_report \nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import make_scorer, precision_score, precision_recall_curve, plot_precision_recall_curve \nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score\nfrom sklearn.metrics import silhouette_samples,silhouette_score\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold, KFold, cross_val_predict, train_test_split\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, cross_validate\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import scale, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, PowerTransformer, LabelEncoder \nfrom sklearn.svm import SVR, SVC\nfrom sklearn.tree import plot_tree, DecisionTreeClassifier\nfrom yellowbrick.classifier import ClassificationReport\n\nfrom xgboost import XGBRegressor, XGBClassifier, plot_importance\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n# To suppress Pandas Future warning\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","7ee7b5d0":"## Some Useful Functions\n\n###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum() \/ df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number'] > 0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('*'*100, 'red', attrs = ['bold']),\n          colored(\"\\nInfo:\\n\", attrs = ['bold']), sep = '')\n    print(df.info(), '\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Number of Uniques:\\n\", attrs = ['bold']), df.nunique(),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"All Columns:\", attrs = ['bold']), list(df.columns),'\\n', \n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n\n    df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')  \n    print(colored(\"Columns after rename:\", attrs = ['bold']), list(df.columns),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '')\n    print(colored(\"Descriptive Statistics \\n\", attrs = ['bold']), df.describe().round(2),'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n    print(colored(\"Descriptive Statistics (Categorical Columns) \\n\", attrs = ['bold']), df.describe(include = object).T,'\\n',\n          colored('*'*100, 'red', attrs = ['bold']), sep = '') # Gives a statstical breakdown of the data.\n    \ndef multicolinearity_control(df):\n    feature = []\n    collinear = []\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i]) > .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs = ['bold']), df.shape,'\\n',\n                                  colored('*'*100, 'red', attrs = ['bold']), sep = '')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs = ['bold']), sep = '')\n    print(\"There are\", df.duplicated(subset = None, keep = 'first').sum(), \"duplicated observations in the dataset.\")\n    duplicate_values = df.duplicated(subset = None, keep = 'first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep = 'first', inplace = True)\n        print(duplicate_values, colored(\" Duplicates were dropped!\"),'\\n',\n              colored('*'*100, 'red', attrs = ['bold']), sep = '')\n#     else:\n#         print(colored(\"There are no duplicates\"),'\\n',\n#               colored('*'*100, 'red', attrs = ['bold']), sep = '')     \n        \n# def drop_columns(df, drop_columns):\n#     if drop_columns != []:\n#         df.drop(drop_columns, axis = 1, inplace = True)\n#         print(drop_columns, 'were dropped')\n#     else:\n#         print(colored('We will now check the missing values and if necessary, the related columns will be dropped!', attrs = ['bold']),'\\n',\n#               colored('*'*100, 'red', attrs = ['bold']), sep = '')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i] \/ df.shape[0]*100) > limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis = 1, inplace = True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n    \n###############################################################################\n\n# To view summary information about the columns\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"Per_of_Nulls   : \", \"%\", round(df[col].isnull().sum() \/ df.shape[0]*100, 2))\n    print(\"Num_of_Nulls   : \", df[col].isnull().sum())\n    print(\"Num_of_Uniques : \", df[col].nunique())\n    print(\"Duplicates     : \", df.duplicated(subset = None, keep = 'first').sum())\n    print(df[col].value_counts(dropna = False))\n    \n###############################################################################\n\ndef fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col] == group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna = False))\n    \n###############################################################################    \n# show values in bar graphic\n\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\") \n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)","2ce61d77":"df0 = pd.read_csv('..\/input\/hr-dataset\/HR_Dataset.csv')\ndf = df0.copy()\ndf.head(3) ","5f9254ca":"df.tail(3) ","30302ea1":"df.sample(3)","1bc637d2":"df.profile_report()","073ec9d5":"first_looking(df)\nduplicate_values(df)\nprint(colored(\"Shape:\", attrs = ['bold']), df.shape,'\\n', colored('*'*100, 'red', attrs = ['bold']))","63eb6197":"df.columns","8fe8b18d":"df.rename({'departments_': 'department'}, axis=1, inplace=True)\ndf.head(1)","828a2abc":"# Alternative code\n# last_column = df.pop('left')\n# df.insert(9, 'left', last_column)\n# df.head(1)\n\ndf = df[['satisfaction_level', 'last_evaluation', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'work_accident',\n       'promotion_last_5years', 'department', 'salary', 'left']]\ndf.head(1)","49486293":"cprint(\"Have a First Look to 'left' Column\",'green')\nfirst_look('left')","250d1b6d":"# df['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')","5b1f60c8":"fig = px.pie(df, values = df['left'].value_counts(), \n             names = (df['left'].value_counts()).index, \n             title = '\"left\" Column Distribution')\nfig.show()","1be12fda":"y = df['left']\nprint(f'Percentage of left-1: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} observations for left-1)\\nPercentage of left-0: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} observations for left-0)')","f504057d":"df.groupby('left').mean()","0d89c38e":"cprint('Dataset describe results according to the \"left==1\" condition','green')\ndf[df['left'] == 1].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","4a276e33":"cprint('Dataset describe results according to the \"left==0\" condition','green')\ndf[df['left'] == 0].describe().T.style.background_gradient(subset = ['mean','min','50%', 'max'], cmap = 'RdPu')","37611cdb":"cprint(\"Have a First Look to 'left' Column\",'green')\nfirst_look('satisfaction_level')","49bcd1e1":"df['satisfaction_level'].value_counts().iplot(kind=\"bar\", title = '\"satisfaction_level\" Column Distribution')","9a83e458":"px.histogram(df, x = df['satisfaction_level'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'satisfaction_level and left')","d97cae94":"pd.crosstab(df['satisfaction_level'], df['left']).iplot(kind='bar', title = 'satisfaction_level and left')","bfe5efad":"sns.scatterplot(data=df, x=\"satisfaction_level\", y=\"left\", hue=\"left\");","45d86ec5":"df.corr(method='pearson')[7:]","2ecccfd8":"cprint(\"Have a First Look to 'last_evaluation' Column\",'green')\nfirst_look('last_evaluation')","fa5e6194":"df['last_evaluation'].value_counts().iplot(kind=\"bar\", title = '\"last_evaluation\" Column Distribution')","0b24ae94":"px.histogram(df, x = df['last_evaluation'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'last_evaluation and left')","a5b3b74d":"pd.crosstab(df['last_evaluation'], df['left']).iplot(kind='bar', title = 'last_evaluation and left')","7fbedb61":"cprint(\"Have a First Look to 'number_project' Column\",'green')\nfirst_look('number_project')","65ab3ddb":"df['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')","405f0586":"fig = px.pie(df, values = df['number_project'].value_counts(), \n             names = (df['number_project'].value_counts()).index, \n             title = '\"number_project\" Column Distribution')\nfig.show()","230cc2ad":"px.histogram(df, x = df['number_project'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'number_project and left')","b9eb5ec3":"pd.crosstab(df['number_project'], df['left']).iplot(kind='bar', title = 'number_project and left')","1135f9da":"cprint(\"Have a First Look to 'average_montly_hours' Column\",'green')\nfirst_look('average_montly_hours')","cdc57bdb":"df['average_montly_hours'].value_counts().iplot(kind=\"bar\", title = '\"average_montly_hours\" Column Distribution')","de08512a":"px.histogram(df, x = df['average_montly_hours'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'average_montly_hours and left')","53e2d752":"pd.crosstab(df['average_montly_hours'], df['left']).iplot(kind='bar', title = 'average_montly_hours and left')","6c83e80d":"plt.figure(figsize=(15, 8))\nfig = sns.histplot(\n    df,\n    x=\"average_montly_hours\", hue=\"left\",\n    multiple=\"stack\",\n    palette=\"light:m_r\",\n    edgecolor=\".3\",\n    linewidth=.5\n)\nfig2 = fig.twinx()\n\nfig.yaxis.set_label_position('left')\nfig2.yaxis.set_label_position('right')\n\nfig2.set_ylabel('frequency [%]')\n\nfor p in fig.patches:\n    x=p.get_bbox().get_points()[:,0]\n    y=p.get_bbox().get_points()[1,1]\n    fig.annotate('{:.1f}%'.format(100.*y\/len(df)), (x.mean(), y), \n            ha='center', va='bottom') # set the alignment of the text\nfig2.grid(None)\nfig2.set_ylim(0,100)","a38d78e6":"cprint(\"Have a First Look to 'time_spend_company' Column\",'green')\nfirst_look('number_project')","fdbe2a14":"df['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')","accc6e96":"fig = px.pie(df, values = df['time_spend_company'].value_counts(), \n             names = (df['time_spend_company'].value_counts()).index, \n             title = '\"time_spend_company\" Column Distribution')\nfig.show()","1ace5be8":"px.histogram(df, x = df['time_spend_company'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'time_spend_company and left')","11125e0d":"pd.crosstab(df['time_spend_company'], df['left']).iplot(kind='bar', title = 'time_spend_company and left')","d4a601ca":"cprint(\"Have a First Look to 'work_accident' Column\",'green')\nfirst_look('work_accident')","722bdd4c":"df['work_accident'].value_counts().iplot(kind=\"bar\", title = '\"work_accident\" Column Distribution')","76c9a579":"fig = px.pie(df, values = df['work_accident'].value_counts(), \n             names = (df['work_accident'].value_counts()).index, \n             title = '\"work_accident\" Column Distribution')\nfig.show()","1cdad2fb":"px.histogram(df, x = df['work_accident'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'work_accident and left')","dd5ca75d":"pd.crosstab(df['work_accident'], df['left']).iplot(kind='bar', title = 'work_accident and left')","993cf6a8":"cprint(\"Have a First Look to 'promotion_last_5years' Column\",'green')\nfirst_look('promotion_last_5years')","3b20e59a":"df['promotion_last_5years'].value_counts().iplot(kind=\"bar\", title = '\"promotion_last_5years\" Column Distribution')","55b44802":"fig = px.pie(df, values = df['promotion_last_5years'].value_counts(), \n             names = (df['promotion_last_5years'].value_counts()).index, \n             title = '\"promotion_last_5years\" Column Distribution')\nfig.show()","0f1b0faa":"px.histogram(df, x = df['promotion_last_5years'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'promotion_last_5years and left')","be5a9fec":"pd.crosstab(df['promotion_last_5years'], df['left']).iplot(kind='bar', title = 'promotion_last_5years and left')","c9f54a81":"cprint(\"Have a First Look to 'department' Column\",'green')\nfirst_look('department')","7aa077ed":"df['department'].value_counts().iplot(kind=\"bar\", title = '\"department\" Column Distribution')","e8bd5845":"fig = px.pie(df, values = df['department'].value_counts(), \n             names = (df['department'].value_counts()).index, \n             title = '\"department\" Column Distribution')\nfig.show()","64997070":"px.histogram(df, x = df['department'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'department and left')","a5f00c1e":"pd.crosstab(df['department'], df['left']).iplot(kind='bar', title = 'department and left')","6a53df91":"cprint(\"Have a First Look to 'salary' Column\",'green')\nfirst_look('salary')","d72e1394":"df['salary'].value_counts().iplot(kind=\"bar\", title = '\"salary\" Column Distribution')","a59d4558":"fig = px.pie(df, values = df['salary'].value_counts(), \n             names = (df['salary'].value_counts()).index, \n             title = '\"salary\" Column Distribution')\nfig.show()","9ab03ed9":"px.histogram(df, x = df['salary'], color='left', marginal = \"box\", hover_data = df.columns, \n             title = 'salary and left')","059f3133":"pd.crosstab(df['salary'], df['left']).iplot(kind='bar', title = 'salary and left')","9edf91c1":"numerical= df.drop(['left'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(colored(\"Numerical Columns:\", attrs=['bold']), list(df[numerical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')\nprint(colored(\"Categorical Columns:\", attrs=['bold']), list(df[categorical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')","de9f0b1f":"df[categorical].head().T","9ff16f03":"df[categorical].describe().T.style.background_gradient(subset=['unique','freq','count'], cmap='RdPu')","cbbf41bd":"df[categorical].nunique()","a3c6d4d2":"for i, col in enumerate(df[categorical].columns):\n    fig = px.histogram(df[col], color=df[\"left\"], width=800, height=800, title=col, pattern_shape=df[\"salary\"], pattern_shape_sequence=[\"x\", \".\", \"+\"])\n    fig.show()","37f7cb9c":"sns.swarmplot(y=\"satisfaction_level\", x=\"salary\", hue=\"left\", data=df, palette=\"husl\");","2e63e642":"sns.swarmplot(y=\"average_montly_hours\", x=\"department\", hue=\"left\", data=df, palette=\"husl\");","ab4b0f10":"sns.swarmplot(y=\"number_project\", x=\"department\", hue=\"left\", data=df, palette=\"husl\");","d02a4178":"sns.swarmplot(y=\"time_spend_company\", x=\"department\", hue=\"left\", data=df, palette=\"husl\");","27397ee6":"for i, col in enumerate(df[categorical].columns):\n    xtab = pd.crosstab(df[col], df[\"left\"], normalize=True)\n    print(colored('-'*40, 'red', attrs=['bold']), sep='')\n    print(xtab*100)","e11410d7":"numerical= df.drop(['left'], axis = 1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint('---------------------')\nprint(f'Numerical Columns:  {df[numerical].columns}')\nprint(f'Categorical Columns: {df[categorical].columns}')\nprint('---------------------')","376da541":"df[numerical].describe().T.style.background_gradient(subset = ['mean','std','50%','count'], cmap = 'RdPu')","962ce833":"df[numerical].iplot(kind = 'histogram', subplots = True, bins = 50)","ca4f7cae":"for i in numerical:\n    df[i].iplot(kind = 'box', title = i, boxpoints = 'all')","ad863802":"sns.pairplot(df, hue = \"left\", corner = True);","f2241b29":"plt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(df.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(df.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","e054752c":"df.corr()['left'].sort_values().drop('left').iplot(kind = 'barh', colors='purple');","f09fc8a8":"df_temp = df.corr()\n\ncount = 'Done'\nfeature =[]\ncollinear= []\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i] > .9 and df_temp[col][i] < 1) or (df_temp[col][i] < -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f'\\033[1mmulticolinearity alert in between\\033[0m {col} - {i}')\n        else:\n            print(f'For {col} and {i}, there is NO multicollinearity problem') \n\nprint('\\033[1mThe number of strong corelated features:\\033[0m', count) ","a94ebeb1":"df.columns","25320d17":"cprint('\"left\" Column Distribution','green')\ndf.left.value_counts()","6fd36fba":"# with plotly\n\ndf['left'].value_counts().iplot(kind=\"bar\", title = '\"left\" Column Distribution')","41937a57":"# with seaborn\n\nplt.figure(figsize = (7,5))\nsns.countplot(data = df, x = 'left');\nfor index,value in enumerate(df.left.value_counts()):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13);","94cb479e":"cprint('\"left\" Column Distribution','green')\nfig = plt.figure(figsize = (11, 6))\nax = fig.add_axes([0, 0, 1, 1])\nax.bar(df.left.value_counts().index, df.left.value_counts().values, color = 'green')\nplt.title('\"left\" Column Distribution')   \nplt.xlabel('left') \nplt.ylabel('Number of Employees') \nfor index,value in enumerate(df.left.value_counts()):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","fe0e9ef1":"cprint('\"number_project\" Column Distribution','green')\ndf.number_project.value_counts()","c4151724":"# with plotly\n\ndf['number_project'].value_counts().iplot(kind=\"bar\", title = '\"number_project\" Column Distribution')","6038d6e5":"cprint('\"number_project\" Column Distribution','green')\nfig = plt.figure(figsize = (11,6))\nax = fig.add_axes([0,0,1,1])\n# x = df.number_project.value_counts().index\n# y = df.number_project.value_counts().values\ndf.number_project.value_counts().plot(kind = \"bar\", color = \"orange\")\nplt.title('\"number_project\" Column Distribution')   \nplt.xlabel('number_project') \nplt.ylabel('Number of Employees')\nplt.xticks(rotation = 0)\nfor index,value in enumerate(df.number_project.value_counts().sort_values(ascending=False)):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","150d90e1":"cprint('\"time_spend_company\" Column Distribution','green')\ndf.time_spend_company.value_counts()","8dc8288c":"# with plotly\n\ndf['time_spend_company'].value_counts().iplot(kind=\"bar\", title = '\"time_spend_company\" Column Distribution')","d798827c":"cprint('\"time_spend_company\" Column Distribution','green')\nfig = plt.figure(figsize = (11,6))\nax = fig.add_axes([0,0,1,1])\ndf.time_spend_company.value_counts().plot(kind = \"bar\", color = \"pink\")\nplt.title('\"time_spend_company\" Column Distribution')   \nplt.xlabel('time_spend_company') \nplt.ylabel('Number of Employees')\nplt.xticks(rotation = 0)\nfor index,value in enumerate(df.time_spend_company.value_counts().sort_values(ascending=False)):\n    plt.text(index, value, f'{value}', ha = 'center', va = 'bottom', fontsize = 13)\nplt.show()","98ab4318":"for i in df:\n    df[i].iplot(kind = 'histogram', subplots = True, bins = 50)","26d4a035":"df1 = df.drop('left', axis = 1)\ndf1.head(1)","c22a907f":"df1 = pd.get_dummies(df1, columns = ['department','salary'], drop_first = True)\ndf1.head(1)","632d8317":"df1.head()","b4f9a5c9":"scaler = MinMaxScaler()\nscaler.fit(df1)\n\n#Store it separately for clustering\ndf1_scaled= scaler.transform(df1)","0c00df0f":"df1_scaled","30f413bb":"from pyclustertend import hopkins\n\nhopkins(df1_scaled, df1.shape[0])","1fd3fa7c":"#First : Get the Best KMeans \nks = range(1,10)\ninertias=[]\nfor k in ks :\n    # Create a KMeans clusters\n    kc = KMeans(n_clusters=k,random_state=1)\n    kc.fit(df1_scaled)\n    inertias.append(kc.inertia_)\n\n# Plot ks vs inertias\nf, ax = plt.subplots(figsize=(8, 6))\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.style.use('ggplot')\nplt.title('What is the Best Number for KMeans ?')\nplt.show()","94ba7285":"from yellowbrick.cluster import KElbowVisualizer\n\nkmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k = (1,10))\nvisu.fit(df1_scaled)\nvisu.show();","b3bb60f4":"from sklearn.metrics import silhouette_samples,silhouette_score\n\nssd =[]\n\nK = range(2, 10)\n\nfor k in K:\n    model = KMeans(n_clusters=k)\n    model.fit(df1_scaled)\n    ssd.append(model.inertia_)\n    print(f'Silhouette Score for {k} clusters: {silhouette_score(df1_scaled, model.labels_)}')","6b7ce30a":"from sklearn.cluster import KMeans\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_4 = KMeans(n_clusters=4, random_state=101)\nvisualizer = SilhouetteVisualizer(model_4)\nvisualizer.fit(df1_scaled)    # Fit the data to the visualizer\nvisualizer.poof();","9b3be7d7":"from sklearn.cluster import KMeans\n\nfrom yellowbrick.cluster import SilhouetteVisualizer\n\nmodel_3 = KMeans(n_clusters=3, random_state=101)\nvisualizer = SilhouetteVisualizer(model_3)\nvisualizer.fit(df1_scaled)    # Fit the data to the visualizer\nvisualizer.poof();","57afea11":"k_means_model = KMeans(n_clusters = 3, random_state = 101)\nk_means_model.fit_predict(df1_scaled)\nlabels = k_means_model.labels_\nlabels","9ab82df9":"df['predicted_clusters'] = labels\ndf","fc649ca6":"df['predicted_clusters'].value_counts()","69467750":"fig = px.pie(df, values = df['predicted_clusters'].value_counts(), \n             names = (df['predicted_clusters'].value_counts()).index, \n             title = 'Predicted_Clusters Distribution')\nfig.show()","9841e8f9":"fig = px.pie(df, values = df[df['left']==0]['predicted_clusters'].value_counts(), \n             names = df[df['left']==0]['predicted_clusters'].value_counts().index, \n             title = 'Predicted_Clusters_Almost_Lost Distribution')\nfig.show()","8390e3a2":"fig = px.pie(df, values = df[df['left']==1]['predicted_clusters'].value_counts(), \n             names = df[df['left']==1]['predicted_clusters'].value_counts().index, \n             title = 'Predicted_Clusters_Almost_Lost Distribution')\nfig.show()","088f08a2":"pd.crosstab(df['left'], \n            df['predicted_clusters']).iplot(kind=\"bar\", title = 'Compare (left vs predicted-clusters)',\n            xTitle = 'left & clusters', yTitle = 'counts')","8fd309c1":"df.groupby('left').mean()","7f69b107":"# cprint('Dataset describe results according to the \"left==1\" condition','green')\n\ndf.groupby(['left', 'predicted_clusters']).mean()","316d4eea":"pd.crosstab(df['predicted_clusters'], \n            df['left']).iplot(kind=\"bar\", title = 'Compare (predicted-clusters vs left)',\n            xTitle = 'clusters & left', yTitle = 'counts')","f661f302":"df.left.value_counts()","18ce7116":"# cprint('Dataset describe results according to the \"left==1\" condition','green')\n\ndf.groupby('predicted_clusters').mean()","122b0e9d":"# cprint('Dataset describe results according to the \"left==1\" condition','green')\n\ndf.groupby(['predicted_clusters', 'left']).mean()","7201f3e4":"df2 = df.drop('predicted_clusters', axis = 1)\ndf2.head(1)","030d51f7":"df2 = pd.get_dummies(df2, columns = ['department','salary'], drop_first = True)\ndf2.head(1)","6c346569":"X = df2.drop('left', axis = 1)\ny = df2['left']","e3f1edaa":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.3, random_state = 101)","1f135ec8":"scaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","59c8f2d1":"def eval(model, X_train, X_test):\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    print(confusion_matrix(y_test, y_pred))\n    print(\"Test_Set\")\n    print(classification_report(y_test,y_pred))\n    print(\"Train_Set\")\n    print(classification_report(y_train,y_pred_train))\n    plot_confusion_matrix(model, X_test, y_test, cmap=\"plasma\")","5465c828":"def train_val(y_train, y_train_pred, y_test, y_pred):\n    \n    scores = {\"train_set\": {\"Accuracy\" : accuracy_score(y_train, y_train_pred),\n                            \"Precision\" : precision_score(y_train, y_train_pred),\n                            \"Recall\" : recall_score(y_train, y_train_pred),                          \n                            \"f1\" : f1_score(y_train, y_train_pred)},\n    \n              \"test_set\": {\"Accuracy\" : accuracy_score(y_test, y_pred),\n                           \"Precision\" : precision_score(y_test, y_pred),\n                           \"Recall\" : recall_score(y_test, y_pred),                          \n                           \"f1\" : f1_score(y_test, y_pred)}}\n    \n    return pd.DataFrame(scores)","81343a6a":"from pycaret.classification import *\n\nemployee_clf = setup(df2, target = 'left', \n                     session_id = 123, \n                     train_size=0.8, \n                     log_experiment=True, \n                     log_plots=True, \n                     experiment_name = 'employee_churn_model')","5babd92d":"best_model_scores = compare_models()","20bc7ed5":"GB_model = GradientBoostingClassifier(random_state = 101)\nGB_model.fit(X_train, y_train)\ny_pred = GB_model.predict(X_test)\ny_train_pred = GB_model.predict(X_train)\n\nGB_model_f1 = f1_score(y_test, y_pred)\nGB_model_acc = accuracy_score(y_test, y_pred)\nGB_model_recall = recall_score(y_test, y_pred)\nGB_model_auc = roc_auc_score(y_test, y_pred)","d18961ed":"print(\"GB_Model\")\nprint (\"------------------\")\neval(GB_model, X_train, X_test)","755be872":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","2064e04e":"plt.figure(figsize=(12, 8))\nvisualizer = ClassificationReport(GB_model, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()","29539839":"train_val(y_train, y_train_pred, y_test, y_pred)","b62f44c8":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(GB_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","a331045e":"GB_feature_imp = pd.DataFrame(index=X.columns, data = GB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nGB_feature_imp","4d4509c1":"plt.figure(figsize = (12,6))\nsns.barplot(data = GB_feature_imp.sort_values('Importance', ascending = False), x = GB_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\nplt.xticks(rotation = 75);","83a8a463":"GB_cv = GradientBoostingClassifier(random_state = 101)\n\nGB_cv_scores = cross_validate(GB_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nGB_cv_scores = pd.DataFrame(GB_cv_scores, index = range(1, 11))\n\nGB_cv_scores.mean()[2:]","e3542de4":"param_grid = {\"n_estimators\":[100, 200, 300],\n              \"subsample\":[0.5, 1], \n              \"max_features\" : [None, 2, 3, 4],\n              \"learning_rate\": [0.001, 0.01, 0.1], \n              'max_depth':[3, 4, 5, 6]} ","0e0dea48":"GB_grid = GradientBoostingClassifier(random_state = 101)\nGB_grid_model = GridSearchCV(GB_grid, param_grid, scoring = \"f1\", verbose = 0, n_jobs = -1).fit(X_train, y_train)","3aaca72f":"GB_grid_model.best_estimator_","7013a4e6":"print(colored('\\033[1mBest Parameters of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for Gradient Boosting Model:\\033[0m', 'blue'), colored(GB_grid_model.best_estimator_, 'cyan'))","31294c57":"GB_tuned = GradientBoostingClassifier(learning_rate = 0.01,\n                                      max_depth = 6, \n                                      n_estimators = 200,\n                                      subsample = 0.5,\n                                      random_state = 101).fit(X_train, y_train)","9db7e4bc":"y_pred = GB_tuned.predict(X_test)\ny_train_pred = GB_tuned.predict(X_train)\n\nGB_tuned_f1 = f1_score(y_test, y_pred)\nGB_tuned_acc = accuracy_score(y_test, y_pred)\nGB_tuned_recall = recall_score(y_test, y_pred)\nGB_tuned_auc = roc_auc_score(y_test, y_pred)","38a60f34":"print(\"GB_tuned\")\nprint (\"------------------\")\neval(GB_tuned, X_train, X_test)","997224ee":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","ae6d70df":"plt.figure(figsize=(12, 8))\nvisualizer = ClassificationReport(GB_tuned, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()","d8bd8d75":"train_val(y_train, y_train_pred, y_test, y_pred)","d77581e4":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(GB_tuned)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","4f7081a8":"plot_roc_curve(GB_model, X_test, y_test);","e411667b":"plot_precision_recall_curve(GB_model, X_test, y_test);","6469c91d":"GB_Pred = {\"Actual\": y_test, \"GB_Pred\":y_pred}\nGB_Pred = pd.DataFrame.from_dict(GB_Pred)\nGB_Pred.head(20)","686ae433":"Model_Preds = GB_Pred\nModel_Preds.head(1)","c9f27736":"KNN_model = KNeighborsClassifier(n_neighbors = 5)\nKNN_model.fit(X_train, y_train)\ny_pred = KNN_model.predict(X_test)\ny_train_pred = KNN_model.predict(X_train)\n\nKNN_model_f1 = f1_score(y_test, y_pred)\nKNN_model_acc = accuracy_score(y_test, y_pred)\nKNN_model_recall = recall_score(y_test, y_pred)\nKNN_model_auc = roc_auc_score(y_test, y_pred)","e52c7e1f":"print(\"KNN_Model\")\nprint (\"------------------\")\neval(KNN_model, X_train, X_test)","69d71785":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","17097358":"plt.figure(figsize=(12, 8))\nvisualizer = ClassificationReport(KNN_model, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()","fb0bc215":"train_val(y_train, y_train_pred, y_test, y_pred)","af5e0256":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(KNN_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","7e1728a8":"KNN_cv = KNeighborsClassifier(n_neighbors = 5)\n\nKNN_cv_scores = cross_validate(KNN_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nKNN_cv_scores = pd.DataFrame(KNN_cv_scores, index = range(1, 11))\n\nKNN_cv_scores.mean()[2:]","9795dde5":"test_error_rates = []\n\nfor k in range(1, 30):\n    KNN = KNeighborsClassifier(n_neighbors = k)\n    KNN.fit(X_train, y_train) \n   \n    y_pred = KNN.predict(X_test)\n    \n    test_error = 1 - accuracy_score(y_test, y_pred)\n    test_error_rates.append(test_error)\n    \nprint(test_error_rates)\n\nplt.figure(figsize = (15, 8))\nplt.plot(range(1, 30), test_error_rates, color = 'blue', linestyle = '--', marker = 'o',\n         markerfacecolor = 'red', markersize = 10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K_values')\nplt.ylabel('Error Rate')\n\nplt.hlines(y = 0.04307948860478039, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 2 Line\")\nplt.hlines(y = 0.04558087826570312, xmin = 0, xmax = 30, colors = 'r', linestyles = \"--\", label = \"K-Value = 4 Line\")\nplt.hlines(y = 0.055864369093941, xmin = 0, xmax = 30, colors = 'blue', linestyles = \"--\", label = \"Default K-Value = 5 Line\")\nplt.legend(prop = {\"size\":14});","70b786f2":"# FIRST A QUICK COMPARISON TO OUR DEFAULT K=5\n\nknn5 = KNeighborsClassifier(n_neighbors = 5)\n\nknn5.fit(X_train,y_train)\npred = knn5.predict(X_test)\n\nprint('WITH K=5')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","9b4c0dd6":"# NOW K=2\n\nknn2 = KNeighborsClassifier(n_neighbors = 2)\n\nknn2.fit(X_train,y_train)\npred = knn2.predict(X_test)\n\nprint('WITH K=2')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","06dc78ea":"# NOW K=4\n\nknn4 = KNeighborsClassifier(n_neighbors = 4)\n\nknn4.fit(X_train,y_train)\npred = knn4.predict(X_test)\n\nprint('WITH K=4')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","f471c778":"# NOW K=6\n\nknn6 = KNeighborsClassifier(n_neighbors = 6)\n\nknn6.fit(X_train,y_train)\npred = knn6.predict(X_test)\n\nprint('WITH K=6')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","1ebcc889":"# NOW K=8\n\nknn8 = KNeighborsClassifier(n_neighbors = 8)\n\nknn8.fit(X_train,y_train)\npred = knn8.predict(X_test)\n\nprint('WITH K=8')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","64d7bf9f":"# NOW K=10\n\nknn10 = KNeighborsClassifier(n_neighbors = 10)\n\nknn10.fit(X_train,y_train)\npred = knn10.predict(X_test)\n\nprint('WITH K=10')\nprint('---------------')\nprint(confusion_matrix(y_test, pred))\nprint('---------------')\nprint(classification_report(y_test, pred))","a7d7a262":"k_values = range(1, 30)\nparam_grid = {\"n_neighbors\": k_values, \"p\": [1, 2], \"weights\": ['uniform', \"distance\"]}","098f652c":"KNN_grid = KNeighborsClassifier()\nKNN_grid_model = GridSearchCV(KNN_grid, param_grid, cv = 10, scoring = 'recall')  \nKNN_grid_model.fit(X_train, y_train)","f0a7c852":"KNN_grid_model.best_estimator_","128ce748":"print(colored('\\033[1mBest Parameters of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for KNN Model:\\033[0m', 'blue'), colored(KNN_grid_model.best_estimator_, 'cyan'))","2dba3c01":"# NOW WITH K=3\n\nKNN_tuned3 = KNeighborsClassifier(n_neighbors = 3, p = 1)\nKNN_tuned3.fit(X_train, y_train)\ny_pred = KNN_tuned3.predict(X_test)\ny_train_pred = KNN_tuned3.predict(X_train)\n\nKNN_tuned3_f1 = f1_score(y_test, y_pred)\nKNN_tuned3_acc = accuracy_score(y_test, y_pred)\nKNN_tuned3_recall = recall_score(y_test, y_pred)\nKNN_tuned3_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"KNN_tuned (K=3)\")\nprint (\"------------------\")\neval(KNN_tuned3, X_train, X_test)\ntrain_val(y_train, y_train_pred, y_test, y_pred)","3107f17f":"# NOW WITH K=1\n\nKNN_tuned1 = KNeighborsClassifier(n_neighbors = 1, p = 1)\nKNN_tuned1.fit(X_train, y_train)\ny_pred = KNN_tuned1.predict(X_test)\ny_train_pred = KNN_tuned1.predict(X_train)\n\nKNN_tuned1_f1 = f1_score(y_test, y_pred)\nKNN_tuned1_acc = accuracy_score(y_test, y_pred)\nKNN_tuned1_recall = recall_score(y_test, y_pred)\nKNN_tuned1_auc = roc_auc_score(y_test, y_pred)\n\nprint(\"KNN_tuned (K=1)\")\nprint (\"------------------\")\neval(KNN_tuned1, X_train, X_test)","7524c827":"plot_roc_curve(KNN_model, X_test, y_test);","630554af":"plot_precision_recall_curve(KNN_model, X_test, y_test);","768cf765":"KNN_Pred = {\"Actual\": y_test, \"KNN_Pred\":y_pred}\nKNN_Pred = pd.DataFrame.from_dict(KNN_Pred)\nKNN_Pred.head(20)","29fc0ce3":"KNN_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, KNN_Pred, left_index = True, right_index = True)\nModel_Preds.head(20)","5bda1822":"RF_model = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\nRF_model.fit(X_train, y_train)\ny_pred = RF_model.predict(X_test)\ny_train_pred = RF_model.predict(X_train)\n\nRF_model_f1 = f1_score(y_test, y_pred)\nRF_model_acc = accuracy_score(y_test, y_pred)\nRF_model_recall = recall_score(y_test, y_pred)\nRF_model_auc = roc_auc_score(y_test, y_pred)","7e880760":"print(\"RF_Model\")\nprint (\"------------------\")\neval(RF_model, X_train, X_test)","78152025":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","b3a3b365":"plt.figure(figsize=(12, 8))\nvisualizer = ClassificationReport(RF_model, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()","a88ab8b3":"train_val(y_train, y_train_pred, y_test, y_pred)","8c47894b":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(RF_model)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","fc13a142":"RF_feature_imp = pd.DataFrame(index=X.columns, data = RF_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nRF_feature_imp","3735c305":"plt.figure(figsize = (12,6))\nsns.barplot(data = RF_feature_imp.sort_values('Importance', ascending = False), x = RF_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\nplt.xticks(rotation = 75);","4777528e":"RF_cv = RandomForestClassifier(class_weight = \"balanced\", random_state = 101)\n\nRF_cv_scores = cross_validate(RF_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nRF_cv_scores = pd.DataFrame(RF_cv_scores, index = range(1, 11))\n\nRF_cv_scores.mean()[2:]","8d113635":"param_grid = {'n_estimators' : [50, 100, 300],\n             'max_features' : [2, 3, 4],\n             'max_depth' : [3, 5, 7, 9],\n             'min_samples_split' : [2, 5, 8]}","0454d2a0":"RF_grid = RandomForestClassifier(class_weight = 'balanced', random_state = 101)\nRF_grid_model = GridSearchCV(estimator = RF_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 0)\nRF_grid_model.fit(X_train, y_train)","75bc043c":"RF_grid_model.best_estimator_","9f7f5229":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_params_, 'cyan'))\nprint(\"--------------------------------------------------------------------------------------------------------------------\")\nprint(colored('\\033[1mBest Estimator of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), colored(RF_grid_model.best_estimator_, 'cyan'))","227bec5c":"RF_tuned = RandomForestClassifier(class_weight = 'balanced',\n                                  max_depth = 3,\n                                  max_features = 4,\n                                  n_estimators = 300,\n                                  random_state = 101).fit(X_train, y_train)","fe5adb6f":"y_pred = RF_tuned.predict(X_test)\ny_train_pred = RF_tuned.predict(X_train)\n\nRF_tuned_f1 = f1_score(y_test, y_pred)\nRF_tuned_acc = accuracy_score(y_test, y_pred)\nRF_tuned_recall = recall_score(y_test, y_pred)\nRF_tuned_auc = roc_auc_score(y_test, y_pred)","355fbc38":"print(\"RF_tuned\")\nprint (\"------------------\")\neval(RF_tuned, X_train, X_test)","328b6156":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","12c5c11d":"plt.figure(figsize=(12, 8))\nvisualizer = ClassificationReport(RF_tuned, support=True)\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()","57748867":"train_val(y_train, y_train_pred, y_test, y_pred)","24d8deb8":"from yellowbrick.classifier import ClassPredictionError\n\nvisualizer = ClassPredictionError(RF_tuned)\n\n# Fit the training data to the visualizer\nvisualizer.fit(X_train, y_train)\n\n# Evaluate the model on the test data\nvisualizer.score(X_test, y_test)\n\n# Draw visualization\nvisualizer.poof();","f4acb4f1":"plot_roc_curve(RF_model, X_test, y_test);","67c60772":"plot_precision_recall_curve(RF_model, X_test, y_test);","695a3d0c":"RF_Pred = {\"Actual\": y_test, \"RF_Pred\":y_pred}\nRF_Pred = pd.DataFrame.from_dict(RF_Pred)\nRF_Pred.head(20)","175f7dfd":"RF_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, RF_Pred, left_index = True, right_index = True)\nModel_Preds.head(20)","a5a130cc":"Model_Preds.sample(10)","55ed60e1":"from catboost import CatBoostClassifier\n\nCB_model = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\nCB_model.fit(X_train, y_train)\ny_pred = CB_model.predict(X_test)\ny_train_pred = CB_model.predict(X_train)\n\nCB_model_f1 = f1_score(y_test, y_pred)\nCB_model_acc = accuracy_score(y_test, y_pred)\nCB_model_recall = recall_score(y_test, y_pred)\nCB_model_auc = roc_auc_score(y_test, y_pred)","fa97bcbb":"print(\"CB_Model\")\nprint (\"------------------\")\neval(CB_model, X_train, X_test)","bab700b9":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","84ca7965":"train_val(y_train, y_train_pred, y_test, y_pred)","36492f05":"CB_feature_imp = pd.DataFrame(index = X.columns, data = CB_model.feature_importances_, columns = ['Importance']).sort_values(\"Importance\", ascending = False)\nCB_feature_imp","2d0bfc59":"plt.figure(figsize = (12, 6))\nsns.barplot(data = CB_feature_imp.sort_values('Importance', ascending = False), x = CB_feature_imp.sort_values('Importance', ascending = False).index, y = 'Importance')\nplt.xticks(rotation = 75);","37d0d4b9":"CB_cv = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\n\nCB_cv_scores = cross_validate(CB_cv, X_train, y_train, \n                              scoring = ['accuracy', 'precision','recall', 'f1', 'roc_auc'], cv = 10)\nCB_cv_scores = pd.DataFrame(CB_cv_scores, index = range(1, 11))\n\nCB_cv_scores.mean()[2:]","d25fdde0":"CB_cm = confusion_matrix(y_test, y_pred)\nCB_cm","019252b3":"CB_cm_df = pd.DataFrame(CB_cm)\nCB_cm_df","395bfc9c":"CB_cm_df = CB_cm_df.rename(columns={0:\"Employee_Stayed\", 1:\"Employe_Left\"}, index={0:\"Employee_Stayed\", 1:\"Employe_Left\"})\nCB_cm_df","bdcc3637":"CB_cm_df[\"Total\"] = CB_cm_df[\"Employee_Stayed\"] + CB_cm_df[\"Employe_Left\"]\nCB_cm_df","0f0d5029":"fig = px.bar(CB_cm_df, x=\"Employee_Stayed\", y=\"Total\", color=\"Employe_Left\", title=\"CatBoost Confusion Matrix Distribution\")\n\nfig.update_layout(\n    xaxis_title=\"Employees-Left                      Employees_Stayed\",\n    yaxis_title=\"The Number of Employees\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","5a5cb8af":"param_grid = {'learning_rate': [0.01, 0.03, 0.1, 0.5],\n              'depth': [4, 6, 8, 10],\n              'l2_leaf_reg': [1, 3, 5, 7, 9]}","fd82767e":"CB_grid = CatBoostClassifier(verbose = False, scale_pos_weight = 4, random_state = 101)\nCB_grid_model = GridSearchCV(estimator = CB_grid, \n                             param_grid = param_grid, \n                             scoring = \"recall\", \n                             n_jobs = -1, verbose = 0)\nCB_grid_model.fit(X_train, y_train)","5ab996f2":"CB_grid_model.best_params_","e9c50131":"print(colored('\\033[1mBest Parameters of GridSearchCV for Random Forest Model:\\033[0m', 'blue'), \n      colored(CB_grid_model.best_params_, 'cyan'))","115ae4f6":"CB_tuned = CatBoostClassifier(verbose = False, \n                              scale_pos_weight = 4,\n                              depth = 4,\n                              l2_leaf_reg = 3,\n                              learning_rate = 0.01,\n                              random_state = 101).fit(X_train, y_train)    ","a019ba6d":"y_pred = CB_tuned.predict(X_test)\ny_train_pred = CB_tuned.predict(X_train)\n\nCB_tuned_f1 = f1_score(y_test, y_pred)\nCB_tuned_acc = accuracy_score(y_test, y_pred)\nCB_tuned_recall = recall_score(y_test, y_pred)\nCB_tuned_auc = roc_auc_score(y_test, y_pred)","901f714b":"print(\"CB_tuned\")\nprint (\"------------------\")\neval(CB_tuned, X_train, X_test)","b233c6fd":"cf_matrix = confusion_matrix(y_test, y_pred)\n\ngroup_names = [\"True Negatives (TN)\", \"False Positives (FP)\\n(Type I Error)\", \"False Negatives (FN)\\n(Type II Error)\", \"True Positives (TP)\"]\n\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names, group_counts, group_percentages)]\n\nlabels = np.asarray(labels).reshape(2, 2)\n\nax = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap=\"Blues\")\nax.set(xlabel=\"Predicted Class\", ylabel = \"Actual Claass\");","63ee5577":"train_val(y_train, y_train_pred, y_test, y_pred)","d3a19bce":"CBtune_cm = confusion_matrix(y_test, y_pred)\nCBtune_cm","ab84608f":"CBtune_cm_df = pd.DataFrame(CBtune_cm)\nCBtune_cm_df","7bb63a27":"CBtune_cm_df = CBtune_cm_df.rename(columns={0:\"Employee_Stayed\", 1:\"Employe_Left\"}, index={0:\"Employee_Stayed\", 1:\"Employe_Left\"})\nCBtune_cm_df","6d6f17a6":"CBtune_cm_df[\"Total\"] = CBtune_cm_df[\"Employee_Stayed\"] + CBtune_cm_df[\"Employe_Left\"]\nCBtune_cm_df","6877d753":"fig = px.bar(CBtune_cm_df, x=\"Employee_Stayed\", y=\"Total\", color=\"Employe_Left\", title=\"CatBoost-Tuned Confusion Matrix Distribution\")\n\nfig.update_layout(\n    xaxis_title=\"Employees-Left                      Employees_Stayed\",\n    yaxis_title=\"The Number of Employees\",\n    font=dict(\n        family=\"Courier New, monospace\",\n        size=14,\n        color=\"#7f7f7f\"\n    )\n)\n\nfig.show()","3ae252ce":"plot_roc_curve(CB_model, X_test, y_test);","93cd75a0":"plot_precision_recall_curve(CB_model, X_test, y_test);","7c610fd8":"CB_Pred = {\"Actual\": y_test, \"CB_Pred\":y_pred}\nCB_Pred = pd.DataFrame.from_dict(CB_Pred)\nCB_Pred.head()","5bc120a8":"CB_Pred.drop(\"Actual\", axis = 1, inplace = True)\nModel_Preds = pd.merge(Model_Preds, CB_Pred, left_index = True, right_index = True)\nModel_Preds.head()","985bf34f":"Model_Preds.sample(10)","7f95ed05":"Model_Preds.loc[7347]","7bee73cd":"df2.loc[7347]","2c828d62":"compare = pd.DataFrame({\"Model\": [\"GB_model\", \"GB_tuned\", \"KNN_Model\", \"KNN_tuned3\", \"RF_model\", \"RF_tuned\", \"CB_model\", \n                                  \"CB_tuned\"],\n                        \n                        \"F1_Score\": [GB_model_f1, GB_tuned_f1, KNN_model_f1, KNN_tuned3_f1, RF_model_f1, RF_tuned_f1, \n                                     CB_model_f1, CB_tuned_f1],\n                                                 \n                        \"Accuracy_Score\": [GB_model_acc, GB_tuned_acc, KNN_model_acc, KNN_tuned3_acc, RF_model_acc, \n                                           RF_tuned_acc, CB_model_acc, CB_tuned_acc],\n                        \n                        \"Recall_Score\": [GB_model_recall, GB_tuned_recall, KNN_model_recall, KNN_tuned3_recall, RF_model_recall, \n                                     RF_tuned_recall, CB_model_recall, CB_tuned_recall],\n                       \n                        \"ROC_AUC_Score\": [GB_model_auc, GB_tuned_auc, KNN_model_auc, KNN_tuned3_auc, RF_model_auc, \n                                          RF_tuned_auc, CB_model_auc, CB_tuned_auc]})\n\ncompare = compare.sort_values(by=\"Recall_Score\", ascending=True)\nfig = px.bar(compare, x = \"Recall_Score\", y = \"Model\", title = \"Recall_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"F1_Score\", ascending=True)\nfig = px.bar(compare, x = \"F1_Score\", y = \"Model\", title = \"F1_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"Accuracy_Score\", ascending=True)\nfig = px.bar(compare, x = \"Accuracy_Score\", y = \"Model\", title = \"Accuracy_Score\")\nfig.show()\n\ncompare = compare.sort_values(by=\"ROC_AUC_Score\", ascending=True)\nfig = px.bar(compare, x = \"ROC_AUC_Score\", y = \"Model\", title = \"ROC_AUC_Score\")\nfig.show()","cba89ab0":"gradient_boosting_classifier = pickle.dump(GB_tuned, open('gradient_boosting_model', 'wb'))","b8dddb7f":"kneighbors_classifier = pickle.dump(KNN_tuned3, open('kneighbors_model', 'wb'))","53dafc22":"random_forest_classifier = pickle.dump(RF_tuned, open('random_forest_model', 'wb'))","65e0884a":"catboost_classifier = pickle.dump(CB_tuned, open('catboost_model', 'wb'))","bc7a53cd":"df.head(3)","d156923e":"df1.head(3)","62656f54":"df2.head(3)","c04cc14d":"df.columns","a76746ce":"df1.columns","1759ea83":"df2.columns","f626ae2a":"col_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', \n           'work_accident', 'promotion_last_5years', 'department_RandD', 'department_accounting', 'department_hr', \n           'department_management', 'department_marketing', 'department_product_mng', 'department_sales', \n           'department_support', 'department_technical', 'salary_low', 'salary_medium']\n\nscaler = MinMaxScaler()\nscaler_fitted = scaler.fit(df2[col_lst])\nscaler_deploy = pickle.dump(scaler_fitted, open('scaler.sav', 'wb'))","7fa0c5c2":"<a id=\"8.4.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","477f98ab":"<a id=\"8.5.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.3 Feature Importance for Random Forest Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","bd29e8be":"<a id=\"8.6.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","55f8c6eb":"# <p style=\"background-color:#B61151;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">EMPLOYEE CHURN ANALYSIS PROJECT<\/p>\n\n![image.png](attachment:image.png)\n\n**Image credit:** [VantageCircle](https:\/\/blog.vantagecircle.com\/employee-attrition\/)","437273dc":"### numerical variables","88c07670":"<a id=\"8.3.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","177a0ffc":"**It looks like there has NOT been a high cardinality issue.**","cdee382e":"### 'time_spend_company' Column","6fd1c2a3":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Loading & Reading the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df: You can [Visit Here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","06a38810":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 - A General Look at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**Pandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Similarly, for EDA, profile_report() is One-Line Magical Code creating reports in the interactive HTML format which is quite easy to understand and analyze the data. In short, at the first hand, what pandas profiling does is to save us all the work of visualizing and understanding the distribution of each variable.**\n\n**For a better understanding and more information, please refer to [Source 1](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/) & [Source 2](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3)**","d8c63611":"### 'department' Column","0fc4177c":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.1 User Defined Functions<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**We have defined some useful user defined functions.**","95636623":"**- Before jumping into modelling, let's try to get some information by examining the columns in the dataset one by one.** ","820400e8":"**Here, Dataset is broken into two parts in ratio of 70:30. It means 70% data will used for model training and 30% for model testing.**","9c3c6082":"***Let's go on with the examination of numerical and categorical columns.***","c17b1f49":"**- Naturally as salary rises, churn rate decreeses**.<br>\n**- Even if it is small, there is an increase in the form of high-medium-low according to the salary status.** ","73bbeff1":"It can be stated that tendency to left company regarding work-load has five phases;\n   - **Below 125 hours\/month** there is **no churn**\n   - **125-160 hours\/month** churn rate **is highest**\n   - **160-220 hours\/month** churn rate **very low (neglectable)**\n   - **220-288 hours\/month** churn rate **high (has a bell shape)**\n   - **Over 288 hours\/month** churn rate **%100!!!**","7d882cab":"**- Yellowbrick:** Yellowbrick is a suite of visualization and diagnostic tools that will enable quicker model selection. It\u2019s a Python package that combines scikit-learn and matplotlib. Some of the more popular visualization tools include model selection, feature visualization, classification and regression visualization\n\n**For a better understanding and more information, please refer to [Yellowbrick](https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/yellowbrick-a-set-of-visualization-tools-to-accelerate-your-model-selection-process\/)** ","21aa8422":"<a id=\"8.4.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","614dd276":"<a id=\"8.4.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.6 KNeighbors Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","d15b783b":"### 'last_evaluation' Column","0230794c":"**- The target column, 'left', has binary type values.**<br>\n**- There has been an imbalanced data for the analysis.**<br>\n**- Almost 17% of the employees prefered NOT to continue with the company and left.**<br>\n**- 1991 employees left.**<br>\n**- Almost 83% of the employees prefered to continue with the company and stayed.**<br>\n**- 10000 employees stayed.**","452b470e":"**It's clear that the employees who made 2, 6 and 7 projects are more inclined to leave their works. It's normal to expect employees who made more projects but not getting more salary or feeling more satisfaction to leave their works;however, those who left even though making less projects should be scrutinized more meticulously.** ","f42e9059":"### 'promotion_last_5years' Column","9d2cf803":"<a id=\"8.4.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.3 KNeighbors Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","0f512940":"<a id=\"8.3.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.5 Gradient Boosting Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","bf597f6c":"<a id=\"8.5.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","06b73320":"**- 'work_accident' column has binary type values.**\n**- While the employees who don't get involved in accident are more likely to leave with a value of 22.84%, those  who get involved in accident are less likely to leave with a value of 6.02%.**  \n**- Altough it does not appear to be a determining factor and needs to be examined carefully by looking at the conditions at workplace, companies' policies and etc.; in fact, it could be said that the left rate of those who have got involved in a work accident is proportionally lower than those who did NOT.** ","f9b7b44a":"<a id=\"8.5\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5 - Random Forest Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","14bd4793":"<a id=\"8.6.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.4 CatBoost Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","90d32dc4":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [   PREFACE](#0)\n* [1) DATA](#1)\n    * [1.1 Context](#1.1)\n    * [1.2 About the Features](#1.2) \n    * [1.3 What the Problem is](#1.3) \n    * [1.4 Project Structure & Tasks](#1.4) \n* [2) LIBRARIES NEEDED IN THE STUDY](#2)\n    * [2.1 User Defined Functions](#2.1)\n* [3) ANALYSIS](#3)\n    * [3.1) Loading & Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 - A General Look at the Data](#4.1)\n    * [4.2 - The Examination of Features](#4.2)\n* [5) DATA VISUALIZATION](#5)          \n    * [5.1 - Employees_Left](#5.1)\n    * [5.2 - Number of Projects](#5.2)\n    * [5.3 - Time Spent in the Company](#5.3)\n    * [5.4 - Subplots of features](#5.4)\n* [6) DATA PRE-PROCESSING](#6)          \n    * [6.1 - Label Encoding](#6.1)\n    * [6.2 - Scalling](#6.2)\n* [7 - CLUSTER ANALYSIS](#7)\n* [8 - MODEL BUILDING](#8)\n    * [8.1 - Spliting Data as Train & Test](#8.1)\n    * [8.2 - Best Model Scores with PyCaret](#8.2)        \n    * [8.3 - Gradient Boosting Classifier](#8.3)\n        * [8.3.1 Model Building](#8.3.1)\n        * [8.3.2 Evaluating Model Performance](#8.3.2)\n        * [8.3.3 Feature Importance for Gradient Boosting Model](#8.3.3) \n        * [8.3.4 Gradient Boosting Classifier Cross Validation](#8.3.4)\n        * [8.3.5 Gradient Boosting Classifier GridSearchCV](#8.3.5)\n        * [8.3.6 Gradient Boosting Classifier ROC (Receiver Operating Curve) & AUC (Area Under Curve)](#8.3.6)            \n        * [8.3.7 Prediction](#8.3.7)        \n    * [8.4 - KNeighbors Classifier](#8.4) \n        * [8.4.1 Model Building](#8.4.1)\n        * [8.4.2 Evaluating Model Performance](#8.4.2)\n        * [8.4.3 KNeighbors Classifier Cross Validation](#8.4.3) \n        * [8.4.4 Elbow Method for Choosing Reasonable K Values](#8.4.4)\n        * [8.4.5 KNeighbors Classifier GridsearchCV for Choosing Reasonable K Values](#8.4.5)\n        * [8.4.6 KNeighbors Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.4.6)            \n        * [8.4.7 Prediction](#8.4.7)         \n    * [8.5 - Random Forest Classifier](#8.5)\n        * [8.5.1 Model Building](#8.5.1)\n        * [8.5.2 Evaluating Model Performance](#8.5.2)\n        * [8.5.3 Feature Importance for Random Forest Model](#8.5.3) \n        * [8.5.4 Random Forest Classifier Cross Validation](#8.5.4)\n        * [8.5.5 Random Forest Classifier GridSearchCV](#8.5.5)\n        * [8.5.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.5.6)            \n        * [8.5.7 Prediction](#8.5.7)     \n    * [8.6 - CatBoost Classifier](#8.6)\n        * [8.6.1 Model Building](#8.6.1)\n        * [8.6.2 Evaluating Model Performance](#8.6.2)\n        * [8.6.3 Feature Importance for CatBoost Model](#8.6.3) \n        * [8.6.4 CatBoost Classifier Cross Validation](#8.6.4)\n        * [8.6.5 CatBoost Classifier GridSearchCV](#8.6.5)\n        * [8.6.6 CatBoost Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)](#8.6.6)            \n        * [8.6.7 Prediction](#8.6.7)     \n* [9) THE COMPARISON OF MODELS](#9)       \n* [10) MODEL DEPLOYMENT](#10)\n    * [10.1 - Save and Export the Model as .pkl](#10.1)\n    * [10.2 - Save and Export Variables as .pkl](#10.2)<br>\n**Note: There may be some additional sub-tasks associated with each task, you will see them in order during the course of the work.*    \n* [11) REFERENCES](#11)\n* [12) FURTHER READINGS](#12)","bc3c3571":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 - Label Encoding<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Lots of machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column. In order to encode this data, you could map each value to a number. e.g. Salary column's value can be represented as low:0, medium:1, and high:2. This process is known as label encoding, and sklearn conveniently will do this for you using LabelEncoder.**","4f1b722c":"<a id=\"8.4.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.5 KNeighbors Classifier GridsearchCV for Choosing Reasonable K Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","5a50efc8":"### 'work_accident' Column","3777df12":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro","ba2ee3c7":"<a id=\"10.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.1 - Save and Export the Model as .pkl<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","a0cc46da":"<a id=\"8.6.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","bbf5765e":"**Let's first load the required HR dataset using pandas's \"read_csv\" function.**","f478caf4":"<a id=\"1.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.4 Project Structure & Tasks<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n#### 1. Exploratory Data Analysis\n- Importing Modules\n- Loading Dataset\n- Data Insigts\n\n#### 2. Data Visualization\n- Employees Left\n- Determine Number of Projects\n- Determine Time Spent in Company\n- Subplots of Features\n\n#### 3. Data Pre-Processing\n- Scaling\n- Label Encoding\n\n#### 4. Cluster Analysis\n- Find the optimal number of clusters (k) using the elbow method for for K-means.\n- Determine the clusters by using K-Means then Evaluate predicted results.\n\n#### 5. Model Building\n- Split Data as Train and Test set\n- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data\n- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data\n- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data\n\n#### 6. Model Deployement\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl ","473b2011":"<a id=\"8.5.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.6 Random Forest Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","52fca86f":"<a id=\"1.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**You can describe 10 attributes (features) in detail as:**\n\n- ***satisfaction_level:*** It is employee satisfaction point, which ranges from 0-1.\n- ***last_evaluation:*** It is evaluated performance by the employer, which also ranges from 0-1.\n- ***number_projects:*** How many of projects assigned to an employee?\n- ***average_monthly_hours:*** How many hours in averega an employee worked in a month?\n- ***time_spent_company:*** time_spent_company means employee experience. The number of years spent by an employee in the company.\n- ***work_accident:*** Whether an employee has had a work accident or not.\n- ***promotion_last_5years:*** Whether an employee has had a promotion in the last 5 years or not.\n- ***Departments:*** Employee's working department\/division.\n- ***Salary:*** Salary level of the employee such as low, medium and high.\n- ***left:*** Whether the employee has left the company or not.","8e5109ac":"<a id=\"1.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.3 What The Problem Is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFirst of all, to observe the structure of the data, outliers, missing values and features that affect the target variable, you must use exploratory data analysis and data visualization techniques. \n\nThen, you must perform data pre-processing operations such as ***Scaling*** and ***Label Encoding*** to increase the accuracy score of Gradient Descent Based or Distance-Based algorithms. you are asked to perform ***Cluster Analysis*** based on the information you obtain during exploratory data analysis and data visualization processes. \n\nThe purpose of clustering analysis is to cluster data with similar characteristics. You are asked to use the ***K-means*** algorithm to make cluster analysis. However, you must provide the K-means algorithm with information about the number of clusters it will make predictions. Also, the data you apply to the K-means algorithm must be scaled. In order to find the optimal number of clusters, you are asked to use the ***Elbow method***. Briefly, try to predict the set to which individuals are related by using K-means and evaluate the estimation results.\n\nOnce the data is ready to be applied to the model, you must ***split the data into train and test***. Then build a model to predict whether employees will churn or not. Train your models with your train set, test the success of your model with your test set. \n\nTry to make your predictions by using the algorithms ***Gradient Boosting Classifier***, ***K Neighbors Classifier***, ***Random Forest Classifier***. You can use the related modules of the ***scikit-learn*** library. You can use scikit-learn ***Confusion Metrics*** module for accuracy calculation. You can use the ***Yellowbrick*** module for model selection and visualization.\n\nIn the final step, you will deploy your model using Streamlit tool.","14e5fad7":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6) DATA PRE-PROCESSING<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n### Tasks in this section\n**- Label Encoding**<br>\n**- Scaling**    ","acf8865b":"### categorical variables","47915d3c":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7) CLUSTER ANALYSIS<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- Find the optimal number of clusters (k) using the elbow method for for K-means.**\n**- Determine the clusters by using K-Means then Evaluate predicted results.**\n---------\n\n**- Cluster analysis or clustering** is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.\n\n    \n**For a better understanding and more information, please refer to [SOURCE 1](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis) and [SOURCE 2](https:\/\/realpython.com\/k-means-clustering-python\/)**    ","3017ca11":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8) MODEL BUILDING<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- Split Data as Train and Test set**<br>\n**- Built Gradient Boosting Classifier, Evaluate Model Performance and Predict Test Data**<br>\n**- Built K Neighbors Classifier and Evaluate Model Performance and Predict Test Data**<br>\n**- Built Random Forest Classifier and Evaluate Model Performance and Predict Test Data**<br>\n---------","ef4a2fb5":"<a id=\"0\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">PREFACE<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nWelcome to \"***Employee Churn Analysis Project***\". This is the second project of Capstone Project Series, which you will be able to build your own classification models for a variety of business settings. \n\nAlso you will learn what Employee Churn is?, How it is different from customer churn, Exploratory Data Analysis (EDA) and visualization of employee churn dataset using ***matplotlib*** and ***seaborn***, ***plotly express*** model building and evaluation using python ***scikit-learn*** package. \n\nYou will be able to implement classification techniques in Python. Using Scikit-Learn allowing you to successfully make predictions with the Random Forest, Gradient Descent Boosting , KNN algorithms.\n\nAt the end of the project, you will have the opportunity to deploy your model using *Streamlit*.\n\nBefore diving into the project, please take a look at the determines and project structure.\n\n- NOTE: This project assumes that you already know the basics of coding in Python and are familiar with model deployement as well as the theory behind K-Means, Gradient Boosting , KNN, Random Forest, and Confusion Matrices. You can try more models and methods beside these to improve your model metrics.","c6a76ecf":"**- Looking at the 'time_spent_company' values, there is an increase in turnover in the 3rd working year, but this increase gradually decreases until the 6th working year.** ","1e2f9c15":"<a id=\"8.5.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","af646e84":"<a id=\"8.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3 - Gradient Boosting Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","951ec6fd":"***According to the basic examinations on the dataset;***\n\n**- There has been a classification problem.**<br>\n**- We are going to make classification on the target variable \"left\".**<br>\n**- So a model will be built to get the best classification on the \"left\" column.**<br>\n**- Moreover, being balanced of \"left\" column or not will be take into consideration meticolously.**<br>\n**- The dataset has 10 columns and 11991 observations after dropping of duplicated observations.**<br>\n**- 8 columns contain numerical values and 2 columns contain categorical values.**<br> \n**- There seems to be no missing value.** ","edb3a172":"<a id=\"11\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">11) REFERENCES<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","e07c70dd":"<a id=\"8.4.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","a3720f1f":"**The 'left' column, which is the target column, was moved from where it is to the end. In this way, it will make more comfortable the analysis for the evaluation of analysis.**","90160a9b":"**- Looking at the 'average_montly_hours' values, there is a local increase in turnover in the 125-160 month working hours range and 210-290 monthly working hours.**<br> \n**- Those who work more than 290 hours per month are more likely to quit their jobs than those who do not.** ","028d5adf":"<a id=\"10.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.2 - Save and Export Variables as .pkl<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5e4a7c6b":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">1.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**\"Analyze employee churn. Find out why employees are leaving the company, and learn to predict who will leave the company..\" [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/predicting-employee-churn-python)**\n\nEmployee turn-over (also known as \"employee churn\") is a costly problem for companies. The true cost of replacing an employee can often be quite large. A study by the Center for American Progress found that companies typically pay about one-fifth of an employee\u2019s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. In other words, the cost of replacing employees for most employers remains significant. This is due to the amount of time spent to interview and find a replacement, sign-on bonuses, and the loss of productivity for several months while the new employee gets accustomed to the new role.\n\nIn the past, most of the focus on the \"rates\" such as attrition rate and retention rates. HR Managers compute the previous rates try to predict the future rates using data warehousing tools. These rates present the aggregate impact of churn, but this is the half picture. Another approach can be the focus on individual records in addition to aggregate.\n\nThere are lots of case studies on customer churn are available. In customer churn, you can predict who and when a customer will stop buying. Employee churn is similar to customer churn. It mainly focuses on the employee rather than the customer. Here, you can predict who, and when an employee will terminate the service. Employee churn is expensive, and incremental improvements will give significant results. It will help us in designing better retention plans and improving employee satisfaction.","296e3242":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2) LIBRARIES NEEDED IN THE STUDY<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","4a134273":"### 'left' Column-Target Column","d51b7f39":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 - Scalling<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Some machine learning algorithms are sensitive to feature scaling while others are virtually invariant to it. Machine learning algorithms like linear regression, logistic regression, neural network, etc. that use gradient descent as an optimization technique require data to be scaled. Also distance algorithms like KNN, K-means, and SVM are most affected by the range of features. This is because behind the scenes they are using distances between data points to determine their similarity.**\n\n### Scaling Types & Selecting Scaler:\n    \n  - **Min-Max Scaler (Normalization)** : Normalization is a scaling technique in which values are shifted and rescaled so that they end up ranging between 0 and 1. This Scaler responds well if the standard deviation is small and when a distribution is not Gaussian. This Scaler is sensitive to outliers. It is also known as Normalization.\n  \n  - **Standard Scaler** : Standardization is another scaling technique where the values are centered around the mean with a unit standard deviation. This means that the mean of the attribute becomes zero and the resultant distribution has a unit standard deviation. Assumes that data is normally distributed within each feature and scales them such that the distribution centered around 0, with a standard deviation of 1.\n  \n  - **Max Abs Scaler** : Scale each feature by its maximum absolute value. This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set is 1.0. It does not shift\/center the data and thus does not destroy any sparsity. On positive-only data, this Scaler behaves similarly to Min Max Scaler and, therefore, also suffers from the presence of significant outliers.\n\n  - **Robust Scaler** : Robust to outliers. If our data contains many outliers, scaling using the mean and standard deviation of the data won\u2019t work well. This Scaler removes the median and scales the data according to the quantile range. The centering and scaling statistics of this Scaler are based on percentiles and are therefore not influenced by a few numbers of huge marginal outliers.\n\n  - **Quantile Transformer Scaler** : Transform features using quantiles information. This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is, therefore, a robust pre-processing scheme. The cumulative distribution function of a feature is used to project the original values. Note that this transform is non-linear and may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable. This is also sometimes called as Rank scaler.\n  \n  - **Power Transformer Scaler** : The power transformer is a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to the variability of a variable that is unequal across the range (heteroscedasticity) or situations where normality is desired. The power transform finds the optimal scaling factor in stabilizing variance and minimizing skewness through maximum likelihood estimation. Currently, Sklearn implementation of PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood. Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.\n\n  - **Unit Vector Scaler** : Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector (L2 Norm). In some applications (e.g., histogram features), it can be more practical to use the L1 norm of the feature vector. Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n\n\n[Click here for more on scaling.](https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35)    ","279a42c9":"<a id=\"8.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2 - Best Model Scores With PyCaret<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**PyCaret** is an open source low-code machine learning library in Python that aims to reduce the hypothesis to insights cycle time in a ML experiment. It enables data scientists to perform end-to-end experiments quickly and efficiently. In comparison with the other open source machine learning libraries, PyCaret is an alternate low-code library that can be used to perform complex machine learning tasks with only few lines of code. PyCaret is simple and easy to use. All the operations performed in PyCaret are automatically stored in a custom Pipeline that is fully orchestrated for deployment. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, Microsoft LightGBM, spaCy and many more.\n    \n**For a better understanding and more information, please refer to [SOURCE 1](https:\/\/pycaret.org\/about\/), [SOURCE 2](https:\/\/machinelearningmastery.com\/pycaret-for-machine-learning\/), [SOURCE 3](https:\/\/www.analyticsvidhya.com\/blog\/2020\/05\/pycaret-machine-learning-model-seconds\/), [SOURCE 1](https:\/\/pycaret.org\/about\/) & [SOURCE 4](https:\/\/towardsdatascience.com\/every-data-scientist-should-use-pycaret-45a1e8e984be)**","7f194793":"### Evaluating Model Performance\n\n**- Confusion Matrix:** You can use scikit-learn metrics module for accuracy calculation. A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making.\n\n**For a better understanding and more information, please refer to [Confusion Matrix](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/)**  ","80e2aa9b":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4) DATA CLEANING & EXPLORATORY DATA ANALYSIS (EDA)<p>\n\n**Exploratory Data Analysis is an initial process of analysis, in which you can summarize characteristics of data such as pattern, trends, outliers, and hypothesis testing using descriptive statistics and visualization.**","e1ab6954":"<a id=\"4.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.2 - The Examination of Features<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**In the given dataset, we have two types of employee one who stayed and another who left the company. So, we can divide data into two groups and compare their characteristics. Here, we can find the average of both the groups using groupby() and mean() function.**    ","2b373fd3":"### Does Cluster make sense for our data?\n\n&emsp; 1. Does our data already have a potential class label?\n\n&emsp; Using the existing class label in our data is often better than trying to create a new label for your data from clustering. If we have the option, supervised machine learning almost always outperforms unsupervised learning in classification tasks. If we have data but have no way to organize the data into meaningful groups, then clustering makes sense. But if we already have an intuitive class label in your data set, then the labels created by a clustering analysis may not perform as well as the original class label.\n\n&emsp; 2. Is our data categorical or continuous?\n\n&emsp; Many clustering algorithms (like DBSCAN or K-Means) use a distance measurement to calculate the similarity between observations. Because of this, certain clustering algorithms will perform better with continuous attributes. However, if we have categorical data, you can one-hot encode the attributes or use a clustering algorithm built for categorical data, such as K-Modes. It should be noted that it does not make a lot of sense to calculate distance between binary variables. Knowing how different clustering algorithms perform on different data types is essential for deciding if clustering makes sense for our data. \n\n&emsp; 3. What does our data look like?\n\n&emsp; A simple visualization of your data with a scatter plot can provide insights into whether your data is well suited for clustering. If visualization reveals that our data has no amount of separation or distinct groups, then clustering may not be appropriate.\n\n&emsp; 4. Do we have a way to validate our clustering algorithm?\n\n&emsp; In order to trust the clustering algorithm results, we must have a method for measuring the algorithm's performance. Clustering algorithm performance can be validated with either internal or external validation metrics.\n\n  - An example of **internal validation is the silhouette score**, a way to measure how well each observation is clustered (1.0 is perfect, 0.0 is worst). \n\n  - An example of **external validation** is when the class label for a data set is already known, but we want to test how well a particular clustering algorithm performs on predicting the existing classes. A noteworthy caveat to the external validation approach is that there is not a huge use case for clustering if the data already has class labels! To have confidence in our machine learning model, we must have a consistent metric for measuring model performance. Clustering is no different. We must have a way to quantitatively assess how well the model is clustering the data. \n  - Before conducting a clustering analysis, we consider which type of validation and which metric makes the most sense for our data. Some algorithms may perform deceivingly well with certain validation metrics, so we may need to use a combination of performance metrics to negate this issue. If we consistently achieve poor model performance, then clustering is not a good fit for your data.\n   \n&emsp; 5. Does clustering provide any new insight into the data?\n\n&emsp; Let\u2019s say that we meet all the above considerations: \n   - We have continuous data with no class label\n   - We visualize the data and there is some separation\n   - We choose a validation metric that makes sense for our analysis\n   - We run a clustering algorithm on the data and obtain a reasonably high silhouette score\n\nour work is not done. After performing a clustering analysis, it is crucial to examine the observations in the individual clusters. This step allows us to assess whether or not the clusters provide any new insight into the data. Did the algorithm really find similar groups of observations and maximize intraclass similarity while minimizing between cluster similarity? An easy way to examine clusters is to calculate simple statistics for the observations in each cluster, such as the mean. If clustering fails to produce any new or useful insights into our data, then our data is not well suited for clustering.\n\n[Click here for more on 'When Clustering Doesn\u2019t Make Sense'.](https:\/\/towardsdatascience.com\/when-clustering-doesnt-make-sense-c6ed9a89e9e6)","0b59d3f8":"<a id=\"8.5.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","f32915dc":"<a id=\"8.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.1 - Spliting Data as Train & Test<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f97d6f14":"### 'salary' Column","bdc896be":"<a id=\"9\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9) THE COMPARISON OF MODELS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","8c635d7c":"**- 'promotion_last_5years' column has binary type values.**<br>\n**- Receiving a promotion in the last 5 working years is not determinant in terms of leaving or continuing to work.**<br> \n**- However, the percentage of those who receive promotions, even if it is small, are higher than those who do not.**","daa842d8":"<a id=\"8.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4 - KNeighbors Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","0d9c48db":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5) DATA VISUALIZATION<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**- Employees Left**<br>\n**- Determine Number of Projects**<br>\n**- Determine Time Spent in Company**<br>\n**- Subplots of Features**<br>\n----------\nYou can search for answers to the following questions using data visualization methods. Based on these responses, you can develop comments about the factors that cause churn.\n\n**- How does the promotion status affect employee churn?**<br>\n**- How does years of experience affect employee churn?**<br>\n**- How does workload affect employee churn?**<br>\n**- How does the salary level affect employee churn?**<br>    ","234d3dcc":"<a id=\"5.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2 - Number of Projects<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Similarly, you can also plot a bar graph to count the number of employees deployed on how many projects?**","31bbde9c":"### 'satisfaction_level' Column","bd09f457":"<a id=\"8.3.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.4 Gradient Boosting Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","db575912":"<a id=\"8.6.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.6 CatBoost Classifier ROC (Receiver Operating Curve) and AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","87c1d5b8":"<a id=\"8.6.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.5 CatBoost Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","2127d1e5":"### The Elbow Method\n\n**- \"Elbow Method\"** can be used to find the optimum number of clusters in cluster analysis. The elbow method is used to determine the optimal number of clusters in k-means clustering. The elbow method plots the value of the cost function produced by different values of k. If k increases, average distortion will decrease, each cluster will have fewer constituent instances, and the instances will be closer to their respective centroids. However, the improvements in average distortion will decline as k increases. The value of k at which improvement in distortion declines the most is called the elbow, at which we should stop dividing the data into further clusters.\n\n\n**For a better understanding and more information, please refer to [The Elbow Method SOURCE 1](https:\/\/en.wikipedia.org\/wiki\/Elbow_method_(clustering)), [The Elbow Method SOURCE 2](https:\/\/medium.com\/@mudgalvivek2911\/machine-learning-clustering-elbow-method-4e8c2b404a5d) and [KMeans SOURCE 1](https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1)**    \n\n**Let's find out the groups of employees who left. You can observe that the most important factor for any employee to stay or leave is satisfaction and performance in the company. So let's bunch them in the group of people using cluster analysis.**","add878fe":"**Now, let's examine crosstab outputs for each variable.**","232b2e37":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/\n- https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3\n- https:\/\/analyticsindiamag.com\/why-data-scaling-is-important-in-machine-learning-how-to-effectively-do-it\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/01\/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning\/\n- https:\/\/www.kaggle.com\/kadirduran\/military-power-clustering\n- https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/\n- https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/\n- https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=Early%20stopping%20is%20a%20method,deep%20learning%20neural%20network%20models.\n- https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1556086415306043\n- https:\/\/medium.com\/machine-learning-t%C3%BCrkiye\/crm-analizi-rfm-analizi-ve-cltv-m%C3%BC%C5%9Fteri-ya%C5%9Fam-boyu-de%C4%9Feri-36e5c3a232b1\n- https:\/\/medium.com\/capillary-data-science\/rfm-analysis-an-effective-customer-segmentation-technique-using-python-58804480d232\n- https:\/\/towardsdatascience.com\/rfm-analysis-using-bigquery-ml-bfaa51b83086\n- https:\/\/www.ticimax.com\/blog\/kohort-analizi-hakkinda-bilmeniz-gerekenler\n- https:\/\/www.ibm.com\/docs\/en\/spss-modeler\/18.2.2?topic=node-rfm-analysis-settings\n- https:\/\/www.kaggle.com\/vijayuv\/onlineretail\n- https:\/\/www.putler.com\/rfm-analysis\/\n- https:\/\/clevertap.com\/blog\/rfm-analysis\/\n- https:\/\/towardsdatascience.com\/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n- https:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n- https:\/\/medium.com\/swlh\/cohort-analysis-using-python-and-pandas-d2a60f4d0a4d\n- https:\/\/en.wikipedia.org\/wiki\/Feature_engineering#:~:text=Feature%20engineering%20is%20the%20process,competitions%20and%20machine%20learning%20projects.\n- https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114\n- https:\/\/www.kdnuggets.com\/2018\/12\/feature-engineering-explained.html\n- https:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/\n- https:\/\/www.analyticsvidhya.com\/blog\/2021\/03\/step-by-step-process-of-feature-engineering-for-machine-learning-algorithms-in-data-science\/\n- https:\/\/clevertap.com\/blog\/cohort-analysis\/","e43db1d1":"<a id=\"8.3.7\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.7 Prediction<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","2a4330db":"<a id=\"8.6.1\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.1 Model Building<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","aa3f30c2":"<a id=\"12\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">12) FURTHER READINGS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3e422041":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3) ANALYSIS<p>","7057ded8":"<a id=\"8.5.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.4 Random Forest Classifier Cross Validation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","acdd87a7":"<a id=\"8.3.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.3 Feature Importance for Gradient Boosting Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","7bc4d55b":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1 - A General Look at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Let's check how many employees were left?**<br>\n**Here, you can plot a bar graph using Matplotlib. The bar graph is suitable for showing discrete variable counts.**","05e4fef0":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10) MODEL DEPLOYMENT<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n- Save and Export the Model as .pkl\n- Save and Export Variables as .pkl \n---------\n\nYou cooked the food in the kitchen and moved on to the serving stage. The question is how do you showcase your work to others? Model Deployement helps you showcase your work to the world and make better decisions with it. But, deploying a model can get a little tricky at times. Before deploying the model, many things such as data storage, preprocessing, model building and monitoring need to be studied. Streamlit is a popular open source framework used by data scientists for model distribution.\n\nDeployment of machine learning models, means making your models available to your other business systems. By deploying models, other systems can send data to them and get their predictions, which are in turn populated back into the company systems. Through machine learning model deployment, can begin to take full advantage of the model you built.\n\nData science is concerned with how to build machine learning models, which algorithm is more predictive, how to design features, and what variables to use to make the models more accurate. However, how these models are actually used is often neglected. And yet this is the most important step in the machine learning pipline. Only when a model is fully integrated with the business systems, real values \u200b\u200bcan be extract from its predictions.\n\nAfter doing the following operations in this notebook, jump to new .py file and create your web app with Streamlit.","2e47f482":"<a id=\"8.3.6\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.6 Gradient Boosting Classifier ROC (Receiver Operating Curve) & AUC (Area Under Curve)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","ff5b5cb9":"**- There is a local increase between 0.45-0.6 and 0.8-1 in the 'last_evaluation' values, as in 'satisfaction_level' values. There is an increase in the number of people who quit their jobs in these intervals.** ","04ed2927":"<a id=\"8.3.2\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.3.2 Evaluating Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","43c7a63d":"<a id=\"5.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.4 - Subplots of Features<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**You can use the methods of the matplotlib.**","81510295":"<a id=\"8.4.4\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.4.4 Elbow Method for Choosing Reasonable K Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","c704b0d1":"**- Although it is expected that there should be a linear relationship between 'satisfaction_level' and 'left', it does not seem clearly on the graph due to the type of \"left\" feature.**<br> \n**- Those with a 'satisfaction_level' value of around 0.1 are more likely to be 'left'.**<br> \n**- There is a significant increase in the number of those labeled 'left' whose 'satisfaction_level' value is between 0.55. In fact, the number of those who left their companies exceeds that of the ones who not.**<br> \n**- When the 'satisfaction_level' value is around between 0.7 and 0.9, there is an increase in the number of those who left.**<br> \n**- Except for these intervals, no matter how low the satisfaction_level was, the employees continued their work.** ","a0008aa4":"<a id=\"8.5.5\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.5.5 Random Forest Classifier GridSearchCV<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","5e9c8e4e":"### Based on the examinations made above;\n\n**- There is no multicollinearity problem among the features.**<br>\n**- We have weak level correlation between the numerical features and the target column.**<br>\n**- Also there is weak level correlation between the columns.**<br>\n**- Target variable demonstrates a slight negative correlation with the variables of \"satisfaction_level\", \"promotion_last_5years\" and 'work_accident',**<br> \n**- Target variable of \"left\" demonstrates weak correlation with the variables of 'last_evaluation', 'number_project', 'average_montly_hours', 'time_spend_company', 'department' and 'salary'.**<br>\n**- \"satisfaction_level\", even if it's quite small, has more influence on the decision to leave work with a negative value than other features.**<br>\n**- The increase in spending time at workplace positively affects the decision to leave with the company.** ","7412c928":"### 'average_montly_hours' Column","09a8015e":"<a id=\"8.6.3\"><\/a>\n#### <p style=\"background-color:#19D1D1; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6.3 Feature Importance for CatBoost Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#79FAF4\" data-toggle=\"popover\">Table of Contents<\/a>","34d3b574":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#B61151; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nIn this project you have HR data of a company. A study is requested from you to predict which employee will churn by using this data.\n\nThe HR dataset has 14,999 samples with various information about the employees. In the given dataset, you have two types of employee one who stayed and another who left the company. This given dataset will be used to predict when employees are going to quit by understanding the main drivers of employee churn.\n\n**For a better understanding and more information, please refer to [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/predicting-employee-churn-python) and [Kaggle Website](https:\/\/www.kaggle.com\/kadirduran\/hr-dataset)**","93b793fb":"### 'number_project' Column","9660f81a":"<a id=\"5.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.3 - Time Spent in the Company<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Similarly, you can also plot a bar graph to count the number of employees have based on how much experience?**","cc6b0ddb":"<a id=\"8.6\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.6 - CatBoost Classifier<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","598a267e":"**How does workload affect employee churn?**","7a4aa486":"### What will be done to deploy our Model\n\n**1) Save you model using pickle (We did it in section 10.1).**\n\n**2) Create a folder.**\n\n**3) Inside the folder, create a .py extesion file which includes related codes for deployment and templates icluding index.htmml file if you will use flask for deployment(in our case we will use streamlit so we do NOT need it).**\n\n**4) Put all related files including requirements.txt, all model files saved and images you will use in your .py file for deployment.**\n\n**5) Sign in your account at https:\/\/aws.amazon.com\/ and run your instance under ec2.**\n\n**6) Open your Visual Studio Code IDE.**\n\n**7) Make sure your Public IPv4 address at Open SSH configuratin file is the same as the one in your instance if you restart it again (If you change it do NOT forget to save).**\n\n**8) Make a remote connection by connecting to Host via selecting related instance (Before doing this, if you wish, you can also publish the model at your local IP.).**\n\n**9) After connecting the remote, put your folder by either \"Open Folder\" window or just drag and drop the folder.**\n**10) Find and change your directory where your folder is by pwd, ls and cd commands.**\n\n**11) Just run your .py file via streamlit run \".......\".py (in our case streamlit run appN.py) (If there are NOT some libraries in your environment needed for deoployment or you need additional libraries for deployment please install them. Moreover, you have to need exact versions of libraries which you built your models with and needed for deployment).**\n\n**12) Now you can see your deployment at External URL by clicking it.**\n\n**13) However, you need a last step to be activated if you want it to be streamed even if you close your Visual Studio Code IDE.**\n\n**14) Open and create a new tmux session via tmux and tmux new -s session_name, respectively (If you do NOT have Tmux package install it viaa sudo apt install tmux).**\n\n**15) That's it! Now people enjoy your deployment and you can HAVE FUN...**","0dbb957d":"**- It is not observed that the departments which require the employees to work alone have an effect on the decision of \"left\".**<br> \n**- It is seen that the left percentages of the departments are quite similar.** "}}