{"cell_type":{"bace2821":"code","12c5a13b":"code","08ad016a":"code","1ab54e13":"code","7e89809d":"code","87abd388":"code","c7b55107":"code","e244016d":"code","e123cb7f":"code","360ad40e":"code","e25a8458":"code","7d351157":"code","4694d9ad":"code","8825ce5b":"code","64094bac":"code","a83f8c22":"code","3c2c7caa":"code","0529ecdd":"code","6173fab6":"code","efb15fda":"code","9cadafde":"code","05172952":"code","45658b26":"code","f9dff43e":"code","f7706a93":"code","20092eb2":"code","12c507f0":"code","cf7cee66":"code","d1fe720b":"code","a45da62b":"code","9e726b36":"code","a09033de":"code","dc513d2b":"code","c9e350a6":"code","132d9d80":"code","989ff28f":"code","c72a5ad2":"code","29f45d46":"markdown","0fbc3fd3":"markdown","2691c230":"markdown","cafa861b":"markdown","e46fd59c":"markdown","89751c0b":"markdown","8a3b6868":"markdown","9b2effd5":"markdown","17474f46":"markdown","8a46a2af":"markdown","a16a3d70":"markdown","714a2e0c":"markdown","de91a4b5":"markdown","19d3d1fc":"markdown","c3246527":"markdown","a9078953":"markdown","63d14e1e":"markdown","dea87449":"markdown","98fe556e":"markdown","13b97921":"markdown","1270e75c":"markdown","b8c4d107":"markdown","dbfd8e36":"markdown","f8880272":"markdown","b8f7662f":"markdown","abd52e8b":"markdown","17121b88":"markdown","2dbff900":"markdown","795109f2":"markdown","deda38eb":"markdown","cb1c641a":"markdown","f4f2fa3d":"markdown"},"source":{"bace2821":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport string, re\nimport requests\nfrom string import punctuation\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport nltk\nfrom nltk.corpus import stopwords\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.preprocessing import text, sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc","12c5a13b":"# Load data\n\nfake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","08ad016a":"fake.head()","1ab54e13":"real.head()","7e89809d":"print(fake.info())\nprint('='*25)\nprint(real.info())","87abd388":"real['label'] = 1\nfake['label'] = 0\n\ndf = real.append(fake)\ndf.shape","c7b55107":"fig = go.Figure(go.Bar(\n    x=['fake', 'real'], y = [len(df[df['label']==0]), len(df[df['label']==1])], \n    text = [len(df[df.label==0]), len(df[df.label==1])], textposition='auto', marker_color = ['crimson', 'teal']\n))\nfig.update_layout(title_text = 'Count of fake & real news articles', height=500, width=400)\nfig.show()","e244016d":"df.subject.value_counts()","e123cb7f":"plt.figure(figsize=(8,6))\nsns.countplot(df['subject'], hue=df['label'])\nplt.xticks(rotation=45)\nplt.title('Count of news articles by subject')","360ad40e":"df['text'] = df['title'] + ' ' + df['text']\ndel df['subject']\ndel df['title']\ndel df['date']","e25a8458":"df.head()","7d351157":"stop = set(stopwords.words('english'))\npunc = string.punctuation\nstop.update(punc)              ","4694d9ad":"# parse html\ndef strp_html(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    return soup.get_text()\n\n# remove sqaure brackets\ndef remove_bw_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# remove urls\ndef remove_urls(text):\n    return re.sub(r'http\\S+', '', text)\n\n# remove stopwords\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n# function which calls all above functions\ndef denoise_text(text):\n    text = strp_html(text)\n    text = remove_bw_square_brackets(text)\n    text = remove_urls(text)\n    text = remove_stopwords(text)\n    return text\n\ndf['text'] = df['text'].apply(denoise_text)","8825ce5b":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(20,8)\n\nwc_real = WordCloud(max_words=200, height = 1000, width = 1500, stopwords=STOPWORDS).generate(\" \".join(df[df['label']==1].text))\nax[0].imshow(wc_real, interpolation='bilinear')\nax[0].set_xticks([])\nax[0].set_yticks([])\nax[0].set_title('Wordcloud for real news', fontsize=18)\n\nwc_fake = WordCloud(max_words=200, height = 1000, width = 1500, stopwords=STOPWORDS).generate(\" \".join(df[df['label']==0].text))\nax[1].imshow(wc_fake, interpolation='bilinear')\nax[1].set_xticks([])\nax[1].set_yticks([])\nax[1].set_title('Wordcloud for fake news', fontsize=18)","64094bac":"fig = make_subplots(rows=1, cols=2)\n\nreal_chars = df[df['label']==1]['text'].str.len()\nfake_chars = df[df['label']==0]['text'].str.len()\n\ntraces = [go.Histogram(x=real_chars, xbins=dict(start=0,end=18000,size=1000), marker_color='dark green', name='Real news char count'),\n          go.Histogram(x=fake_chars, xbins=dict(start=0,end=18000,size=1000), marker_color='brown', name='Fake news char count')]\n\nfig.add_trace(traces[0], row=1, col=1)\nfig.add_trace(traces[1], row=1, col=2)\n\nfig.update_layout(title_text = 'Distribution of character length', bargap=0.1)\nfig.show()","a83f8c22":"fig = make_subplots(rows=1, cols=2)\n\nreal_words = df[df['label']==1]['text'].str.split().map(lambda x: len(x))\nfake_words = df[df['label']==0]['text'].str.split().map(lambda x: len(x))\n\ntraces = [go.Histogram(x=real_words, xbins=dict(start=0,end=1200,size=100), marker_color='#0E5164', name='Real news word count'),\n          go.Histogram(x=fake_words, xbins=dict(start=0,end=1200,size=100), marker_color='#99401C', name='Fake news word count')]\n\nfig.add_trace(traces[0], row=1, col=1)\nfig.add_trace(traces[1], row=1, col=2)\n\nfig.update_layout(title_text = 'Distribution of word count', bargap=0.1)\nfig.show()","3c2c7caa":"fig = make_subplots(rows=1, cols=2)\nword_len_real = df[df.label==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nword_len_fake = df[df.label==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\n\ntraces = [go.Histogram(x=word_len_real.apply(lambda x: np.mean(x)), xbins=dict(start=0,end=20,size=0.1), marker_color='#157F47', name='Real news avg word length'),\n          go.Histogram(x=word_len_fake.apply(lambda x: np.mean(x)), xbins=dict(start=0,end=20,size=0.1), marker_color='#881244', name='Fake news avg word length')]\n\nfig.add_trace(traces[0], row=1, col=1)\nfig.add_trace(traces[1], row=1, col=2)\n\nfig.update_layout(title_text = 'Distribution of avg word length', bargap=0.1)\nfig.show()","0529ecdd":"def get_corpus(text):\n    words = []\n    for i in text:\n        for j in i.split():\n            words.append(j.strip())\n    return words\n\ncorpus = get_corpus(df.text)\nprint(corpus[:5])\nprint(len(corpus))","6173fab6":"top_10_words = Counter(corpus).most_common(10)\ntop_10_words","efb15fda":"def get_top_ngrams(corpus, n, g):\n    \"\"\"\n    This function takes corpus, no. of grams, computes and returns top n grams \n    \"\"\"\n    vec = CountVectorizer(ngram_range=(g,g)).fit(corpus)\n    word_matrix = vec.transform(corpus)\n    sum_words = word_matrix.sum(axis=0)\n    word_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    return word_freq[:n]","9cadafde":"plt.figure(figsize=(10,6))\nmost_common = get_top_ngrams(corpus, 10, 1)\nmost_common = dict(most_common)\nsns.set(font_scale=1.2)\nsns.barplot(x=list(most_common.values()), y=list(most_common.keys()))\nplt.xlabel('Count')\nplt.ylabel('Unigrams')","05172952":"plt.figure(figsize=(10,6))\nmost_common_bi = get_top_ngrams(corpus, 10, 2)\nmost_common_bi = dict(most_common_bi)\nsns.set(font_scale=1.2)\nsns.barplot(x=list(most_common_bi.values()), y=list(most_common_bi.keys()))\nplt.xlabel('Count')\nplt.ylabel('Bigrams')","45658b26":"x_train, x_test, y_train, y_test = train_test_split(df['text'], df['label'], shuffle=True, \n                                                    test_size=0.3, random_state=42, stratify=df['label'])","f9dff43e":"max_features = 10000\nmaxlen = 300","f7706a93":"tokenizer = text.Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(x_train)\n\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)\n\ntokenized_test = tokenizer.texts_to_sequences(x_test)\nx_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","20092eb2":"EMBEDDING_FILE = \"..\/input\/glove-twitter\/glove.twitter.27B.100d.txt\"","12c507f0":"# This function converts words and their vectors in text file into a dictionary \n# with words as keys and arrays of vectors as values\ndef word_vec(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(word_vec(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","cf7cee66":"stacked_embeds = np.stack(embedding_index.values())\nemb_mean, emb_std = stacked_embeds.mean(), stacked_embeds.std()\n\n# initialize embedding matrix with values from normal distribution\n# having mean and std computed on word vectors \nembedding_matrix = np.random.normal(emb_mean, emb_std, size=(max_features, stacked_embeds.shape[1]))\n\n# insert embedding vector of a word in the embedding matrix at token position\nword_index = tokenizer.word_index\nfor word, token in word_index.items():\n    if token > max_features:\n        continue\n    else:\n        emb_vec = embedding_index.get(word)\n        if emb_vec is not None:\n            embedding_matrix[token-1] = emb_vec","d1fe720b":"model = Sequential()\nmodel.add(Embedding(input_dim = max_features, output_dim=embedding_matrix.shape[1], input_length=maxlen, weights = [embedding_matrix],\n                   trainable=False))\nmodel.add(Bidirectional(LSTM(units=256, recurrent_dropout=0.3, dropout=0.3, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(units=128, recurrent_dropout=0.1, dropout=0.1)))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))","a45da62b":"model.summary()","9e726b36":"model.compile(optimizer=keras.optimizers.Adam(lr=0.005), loss = 'binary_crossentropy', metrics = ['accuracy'])","a09033de":"batch_size = 256\nlr_reduce = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2, min_lr=0.000001, verbose=1)","dc513d2b":"history = model.fit(x_train, y_train, validation_data=(x_test, y_test), \n                        batch_size=batch_size, epochs=15, callbacks=[lr_reduce])","c9e350a6":"print(f'Accuracy of model on training data: {np.round(model.evaluate(x_train, y_train)[1]*100, 2)} %')\nprint(f'Accuracy of model on test data: {np.round(model.evaluate(x_test, y_test)[1]*100, 2)} %')","132d9d80":"fig, ax = plt.subplots(1,2)\nfig.set_size_inches(20,6)\n\nepochs = [i for i in range(1,16)]\ntrain_loss = history.history['loss']\ntrain_accuracy = history.history['accuracy']\nval_loss = history.history['val_loss']\nval_accuracy = history.history['val_accuracy']\n\nax[0].plot(epochs, train_loss, 'bo-', label='Training Loss')\nax[0].plot(epochs, val_loss, 'ro-', label='Validation Loss')\nax[0].set_xlabel('Epochs', fontsize=18)\nax[0].set_ylabel('Loss', fontsize=18)\nax[0].set_title('Training & Validation Loss', fontsize=20)\nax[0].legend()\n\nax[1].plot(epochs, train_accuracy, 'bo-', label='Training Accuracy')\nax[1].plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')\nax[1].set_xlabel('Epochs', fontsize=18)\nax[1].set_ylabel('Accuracy', fontsize=18)\nax[1].set_title('Training & Validation Accuracy', fontsize=20)\nax[1].legend()","989ff28f":"y_pred = model.predict_classes(x_test)\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize = (8,6))\nsns.heatmap(cm,cmap= \"Reds\", linecolor = 'black' , linewidth = 1 , annot = True, \n            fmt='' , xticklabels = ['Fake','Real'] , yticklabels = ['Fake','Real'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")","c72a5ad2":"fpr, tpr, thresholds = roc_curve(y_test,y_pred)\nroc_auc = auc(fpr,tpr)\n\nplt.figure(figsize=(8,6))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.4f'% roc_auc)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.04,1.0])\nplt.ylim([-0.04,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc='lower right')\nplt.show()","29f45d46":"# Training the model\n\n* The model will have one Embedding layer, followed by two bi-directional LSTM layers and three Dense layers.\n* Embedding layer is used to compress higher dimensional data into lower dimensional one. Keras Embedding layer has three important paramaters which have to be defined: input_dim which is the size of vocabulary, output_dim which is no. of dimensions of the output - 100 in this case, input_length is size in words for each example.\n* Weights of the Embedding layer will be set to GloVe weights which are pre-trained on a large set of data. Therefore these weights will not be trained again. ","0fbc3fd3":"# Using GLoVe word vector representations for mapping text to vector","2691c230":"Distribution of character length is different for the datasets. There are 10685 fake news articles with character length in the range 1000 and 1999 as compared to 6622 real news articles in the same range.","cafa861b":"# Tokenize train, test data. Set text length","e46fd59c":"There is no overlap of news subject for real and fake news. Hence this variable is not helpful for learning decision boundary.\nJoin text in one column and drop date, subject.","89751c0b":"# Import necessary libraries","8a3b6868":"**Helper functions for text preprocessing**","9b2effd5":"# n-gram analysis","17474f46":"**News Subject**","8a46a2af":"Bigrams","a16a3d70":"Unigrams","714a2e0c":"There are no NaN values in either fake or real datasets","de91a4b5":"**Count of fake and real news articles**","19d3d1fc":"**Average word length**","c3246527":"# Fake & Real news prediction","a9078953":"# Splitting data into train and test sets","63d14e1e":"**Confusion matrix**","dea87449":"# Exploratory Data Analysis","98fe556e":"Distribution of word length is also somewhat different in the datasets.","13b97921":"![fake-news-vs-facts.png](attachment:fake-news-vs-facts.png)","1270e75c":"Avg word length distribution is positively skewed in fake news, in real news it is close to normal","b8c4d107":"**Character length**","dbfd8e36":"**Word count**","f8880272":"# Plot Receiver Operating Characteristic curve","b8f7662f":"# Performance Analysis","abd52e8b":"# Corpus of words","17121b88":"**Wordcloud**","2dbff900":"The objective of this exercise is to develop a machine learning program to identify when a news article might be fake.\n\n**Data Description**\n\nThe data is given in two csv files - fake and true containing fake and true news respectively.\n\n**fake.csv:** \n* Title: Title of news article\n* Text: Body of news article\n* Subject: News subject\/area\n* Date: Date published\n \n**true.csv** contains the same fields for real news.","795109f2":"**Stopwords**\n\nStopwords are set of words which does not add much meaning to a sentence (For example: of, the, a, above, in are some stopwords in English language). They can safely be ignored without sacrificing the meaning of sentence and this will save memory and processing time. NLTK's corpus contains stopwords for 16 languages including English. Using NLTK corpus to fetch stopwords.","deda38eb":"There are 11M words in this dataset after preprocessing. This corpus includes repetitive words. Let's see the top 10 words which have the highest frequency.","cb1c641a":"Label fake and true news and concatenate the tables","f4f2fa3d":"Set text features"}}