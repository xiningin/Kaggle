{"cell_type":{"e3fb81a1":"code","7cbe98a3":"code","a46bf753":"code","4a20e82f":"code","8bbf6a8d":"code","22dd30b1":"code","46190d1d":"code","1e6ea609":"code","607b2cca":"code","f553849b":"code","0f480c9e":"code","73e38810":"code","df5fcf38":"code","5f4854e8":"code","815fad10":"code","c9cd245e":"code","ed265caa":"code","723a7a27":"code","46f7e361":"code","addd4948":"markdown","807678d3":"markdown","3c490dd2":"markdown","f5ff3ab7":"markdown","cfbf301c":"markdown","1dae021c":"markdown","41bef995":"markdown","8d7bdaa8":"markdown","56b589a3":"markdown","0730145a":"markdown","0076e6c9":"markdown"},"source":{"e3fb81a1":"import pandas as pd","7cbe98a3":"!pip install -q transformers","a46bf753":"import soundfile as sf\ntarget=\"..\/input\/medicinedata\/train\/\"\nlocation=target+\"Adalimumab.wav\"\nspeech_array, sampling_rate=sf.read(location)\n","4a20e82f":"print(speech_array)\nprint(\"The Sampling rate is:\",sampling_rate)","8bbf6a8d":"import librosa\nimport IPython.display as ipd\n\n# Loading and listening to the audio file\nexample_file = '..\/input\/medicinedata\/train\/Adapalene.wav'\naudio, sample_rate = librosa.load(example_file)\n\nipd.Audio(example_file, rate=sample_rate)\n","22dd30b1":"import numpy as np\nimport matplotlib.pyplot as plt\n#Getting spectrogra, using Librosa's Short Time Fourier Transform (stft)\n\nspec=np.abs(librosa.stft(audio))\nspec_db=librosa.amplitude_to_db(spec, ref=np.max)\n\n#Using the log scale to view the frequencies\nimport librosa.display\n\nlibrosa.display.specshow(spec_db, y_axis='log', x_axis='time')\nplt.colorbar()\nplt.title('Audio Spectrogram');","46190d1d":"#PLotting the mel spectrogram of the sample we aquired from the dataset\nmel_spec=librosa.feature.melspectrogram(audio, sr=sample_rate)\nmel_spec_db=librosa.power_to_db(mel_spec, ref=np.max)\n\nlibrosa.display.specshow(\n    mel_spec_db, x_axis='time', y_axis='mel'\n)\nplt.colorbar()\nplt.title('Mel Spectrogram')","1e6ea609":"import torch\nfrom transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n\nimport soundfile as sf\n\n","607b2cca":"model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook\/s2t-small-librispeech-asr\")\nprocessor = Speech2TextProcessor.from_pretrained(\"facebook\/s2t-small-librispeech-asr\")\ninputs = processor(array_speech[0], sampling_rate=16_000, return_tensors=\"pt\")\ngenerated_ids = model.generate(input_ids=inputs[\"input_features\"], attention_mask=inputs[\"attention_mask\"])\ntranscription = processor.batch_decode(generated_ids)","f553849b":"print(transcription)","0f480c9e":"from IPython.display import Audio\nfrom scipy.io import wavfile\nimport numpy as np","73e38810":"file_name = '..\/input\/medicinedata\/train\/Aspirin.wav'","df5fcf38":"Audio(file_name)","5f4854e8":"data = wavfile.read(file_name)\nframerate = data[0]\nsounddata = data[1]\ntime = np.arange(0,len(sounddata))\/framerate\nprint('The sample rate is:',framerate,'Hz')\nprint('The total time of the track is:',len(sounddata)\/framerate,'s')","815fad10":"import soundfile as sf\nimport librosa\nimport torch\nfrom transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer","c9cd245e":"tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook\/wav2vec2-base-960h\")\nmodel = Wav2Vec2ForCTC.from_pretrained(\"facebook\/wav2vec2-base-960h\")","ed265caa":"input_audio, _ = librosa.load(file_name, \n                              sr=16000)","723a7a27":"input_values = tokenizer(input_audio, return_tensors=\"pt\").input_values\nlogits = model(input_values).logits\npredicted_ids = torch.argmax(logits, dim=-1)\ntranscription = tokenizer.batch_decode(predicted_ids)[0]","46f7e361":"print(transcription)","addd4948":"## Importing the important libraries","807678d3":"## Defining the model and the processor. Also, here we convereted the audion files to 16000 Hz frequency","3c490dd2":"## Importing the soundfile","f5ff3ab7":"## The Speech2Text model was not able to recogninze the speech. ","cfbf301c":"# Model Name: wav2vec2-base-960h","1dae021c":"#### The sample rate of the audio files is 22050Hz. Hence, we will need it to convert to 16000Hz","41bef995":"## Here, we will use the Wav2Vec model and will try to compare it with Speech2Text model","8d7bdaa8":"## Plotting the audio spectrogram and Mel Spectrogram","56b589a3":"## We can clearly observe that the Wav2Vec2 model is almost able to recognize aspirin. So, we will be continuing with Wav2Vec2 model. Since, these models were still not able to accurately recognize medicine name. Hence, we will need to fine tune these models on a medicine names data. Currently, we don't have much data to fine-tune these models.","0730145a":"### Installing the transformers library","0076e6c9":"## Importing torch, Speech2TextProcessor, Speech2TexForConditionalGeneration models for speech recognition."}}