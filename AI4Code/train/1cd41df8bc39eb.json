{"cell_type":{"8d637c60":"code","a632469a":"code","013bfa62":"code","4097884d":"code","91df3181":"code","dbdabba9":"code","1688cf1c":"code","948d42f1":"code","a323fc95":"code","f67acfb0":"code","da51f039":"code","5f1aa495":"code","2f8cb3d8":"code","5aaec296":"code","2cfe0fca":"code","f258cdcd":"code","51be3950":"code","f2ff0363":"code","489031fc":"code","7b4e99eb":"code","804a1035":"code","5f5c7f40":"code","c5b57ae2":"code","553ad7d8":"code","194d636f":"code","8a8c46f9":"code","97bfc33d":"code","db866ac7":"code","fe7df27b":"code","b21a4cd5":"code","645c2e92":"code","acb3bc22":"code","4981ea41":"code","42d83730":"code","728d976d":"code","7dce2bad":"code","eb272fcc":"code","8dae2f40":"code","687b1ddc":"code","02fe1135":"code","2353e53b":"code","c47ea0cb":"code","52bd19fd":"code","a558b9e7":"code","4776aaf2":"code","c0e8dddf":"code","8662d6d9":"code","20812c41":"code","ac241605":"code","5445ebfa":"code","21db2935":"code","c9ff6333":"code","52eaba25":"code","741edd41":"code","ca3bc77c":"code","84d43aea":"markdown","8c54d928":"markdown","7ef30947":"markdown","eac5ba5b":"markdown","2b2b3f76":"markdown","9e88640e":"markdown","f9683bbb":"markdown","f27ea8b0":"markdown","891bfda3":"markdown","4a379b0c":"markdown","886f6f50":"markdown","d1bbad5f":"markdown","ceb3a07e":"markdown","2d457668":"markdown","9ba5f4db":"markdown","fd3d6494":"markdown","8143a961":"markdown","05a10ee2":"markdown","10514bda":"markdown","278c6eac":"markdown","d2f07a0f":"markdown","bf6ad142":"markdown","e8db16a7":"markdown","15270433":"markdown","9e6d22f4":"markdown","ff4c121e":"markdown","d9cfbb39":"markdown","36d8ee91":"markdown","b3bb60f7":"markdown","e6285fa5":"markdown","5fd4a681":"markdown","b1a3b0c4":"markdown","0515c62a":"markdown","81808e53":"markdown","3e536108":"markdown","10bfd1d6":"markdown","73664755":"markdown","dc8ec454":"markdown","362feb37":"markdown","e6b7e07b":"markdown","9f27687d":"markdown","32809191":"markdown","ff73a8ac":"markdown","298b658b":"markdown","9c4fc1a3":"markdown","b1c56bbe":"markdown","5b8a31fe":"markdown","98b2f041":"markdown","0c444305":"markdown","6c64c71b":"markdown","30ab0d8a":"markdown","9e9cc8c9":"markdown","78c578ea":"markdown","5e17848f":"markdown","39cb4b8e":"markdown","1e937bd6":"markdown","8e27989d":"markdown","c5c1b0c7":"markdown","2ee20f99":"markdown","9466ad3f":"markdown","8beea10c":"markdown","eb13cfe6":"markdown","9ac673a9":"markdown"},"source":{"8d637c60":"from IPython.display import Image\nImage(filename=\"..\/input\/covid19-images\/NGC.PNG\", width=500, height=500)","a632469a":"from IPython.display import Image\nImage(filename=\"..\/input\/covid19-images\/covid-19.jpg\", width=500, height=500)","013bfa62":"from IPython.display import Image\nImage(filename=\"..\/input\/covid19-images\/BERT.png\", width=500, height=500)","4097884d":"from IPython.display import Image\nImage(filename=\"..\/input\/covid19-images\/Bert_STS.PNG\", width=500, height=500)","91df3181":"#Installs needed\n!pip install langdetect\n!pip install semantic-text-similarity\n\n#Libraries needed\nimport pandas as pd \nimport glob\nimport json\nimport re \nimport numpy as np\nimport copy \nimport torch \nimport matplotlib.pyplot as plt\nfrom langdetect import detect\nfrom semantic_text_similarity.models import ClinicalBertSimilarity\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nprint(\"Done\")","dbdabba9":"#Read in the metadata\nprint(\"Reading in metadata.\")\nmetadata_path = f'..\/input\/cord1933k\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nprint(\"Metadata read.\")\nprint()","1688cf1c":"#Get all file names \nall_json = glob.glob(f'..\/input\/cord1933k\/**\/*.json', recursive=True)\nprint(str(len(all_json))+\" total file paths fetched.\")\nprint()","948d42f1":"#Reader class for files \nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n        return ","a323fc95":"#Read in all text files and store in a Pandas data frame \nprint(\"Reading in text files.\")\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'title': [], 'authors': [], 'journal': [], 'url': []}\nfor idx, entry in enumerate(all_json[:1000]):\n    print(f'Processing index: {idx} of {len(all_json)}', end='\\r')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    dict_['title'].append(meta_data.iloc[0]['title'])\n    dict_['authors'].append(meta_data.iloc[0]['authors'])\n    dict_['journal'].append(meta_data.iloc[0]['journal'])\n    dict_['url'].append(meta_data.iloc[0]['url'])\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'title', 'authors', 'journal', 'url'])\ndict_ = None\nprint(\"Text files read.\")\nprint()","f67acfb0":"#Get a count for the number of words in articles and abstracts \nprint(\"Getting abstract and body word counts.\")\ndf_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\nprint(\"Counts computed.\")\nprint()","da51f039":"#Saving the dataframe \nprint(\"Saving the dataframe.\")\ndf_covid.to_csv('covidData.csv') \nprint(\"Dataframe saved.\")","5f1aa495":"#Read in the saved data \nprint(\"Loading the dataframe.\")\ndf_covid = pd.read_csv('..\/input\/cord19cleaneddata\/covidData.csv')\nprint(\"Dataframe loaded.\")\nprint()\ndf_covid = df_covid.head(1000)","2f8cb3d8":"#Remove all articles that have fewer than the number of words specified \nmin_word_count = 1000\nprint(\"Removing all articles with fewer than \"+str(min_word_count)+\" words.\")\nindexNames = df_covid[df_covid['body_word_count'] < min_word_count].index\ndf_covid = df_covid.drop(indexNames)\ndf_covid = df_covid.reset_index(drop=True)\nprint(\"Articles cleaned.\")\nprint()","5aaec296":"#Remove all non-English articles\nprint(\"Removing all non-English articles\")\nindex = 0\nindexNames = []\nwhile(index < len(df_covid)):\n    print(f'Processing index: {index} of {len(df_covid)}', end='\\r')\n    language = detect(df_covid.iloc[index]['body_text'])\n    if(language != 'en'):\n        indexNames.append(index)\n    index += 1\ndf_covid = df_covid.drop(indexNames)\ndf_covid = df_covid.reset_index(drop=True)\nprint(\"All non-English articles removed. Total article count is now: \"+str(len(df_covid)))\nprint()","2cfe0fca":"#Save the cleaned dataset \nprint(\"Saving the dataframe.\")\ndf_covid.to_csv('covidDataCleaned.csv') \nprint(\"Dataframe saved.\")","f258cdcd":"#Read in the saved data \nprint(\"Loading the dataframe.\")\ndf_covid = pd.read_csv('..\/input\/cord19cleaneddata\/covidDataCleaned.csv')\nprint(\"Dataframe loaded.\")\nprint()\ndf_covid = df_covid.head(5000)","51be3950":"#Get the article text and pre-process by converting to lowercase and removing weird characters \nprint(\"Pre-processing articles by converting to lowercase and removing random characters.\")\narticleTexts = df_covid.drop([\"paper_id\", \"abstract\", \"title\", \"authors\", \"journal\", \"url\", \"abstract_word_count\", \"body_word_count\"], axis=1)\narticleTexts['body_text'] = articleTexts['body_text'].apply(lambda x: x.lower())\narticleTexts['body_text'] = articleTexts['body_text'].apply(lambda x: re.sub('[^a-z0-9.!?\\s]','',x))\ntext_list = list(articleTexts['body_text'])\nprint(\"Pre-processing complete.\")\nprint()","f2ff0363":"#Extract all sentences - gensim expects a sequence of sentences as input\n#Example: sentences = [['first', 'sentence'], ['second', 'sentence']]\nprint(\"Tokenizing articles into sentences and words.\")\nsentences = []\nfor index, text in enumerate(text_list):\n    sentenceList = sent_tokenize(text)\n    for sentence in sentenceList:\n        wordList = word_tokenize(sentence)\n        sentences.append(wordList)\n    print(\"Processing article \"+str(index)+\" of \"+str(len(text_list)), end=\"\\r\")\nprint(\"A total of \"+str(len(sentences))+\" sentences have been tokenized for word2vec training.\")\nprint()","489031fc":"#Train & save the word2vec model \nprint(\"Training word2vec.\")\nmodel = Word2Vec(sentences, size=100, window=5, min_count=10, workers=4)\nprint(\"Word count:\", len(list(model.wv.vocab)))\nmodel.save(\"word2vec.model\")\nprint(\"Finished training and saving word2vec.\")","7b4e99eb":"#Load the trained word2vec model \nprint(\"Loading the pre-trained word2vec model.\")\nmodel = Word2Vec.load(\"word2vec.model\")\nprint(\"Model loaded.\")\nprint()","804a1035":"#From: https:\/\/methodmatters.github.io\/using-word2vec-to-analyze-word\/\n#Define the function to compute the dimensionality reduction and then produce the biplot  \ndef tsne_plot(model, words):\n    \"Creates a TSNE model and plots it\"\n    labels = []\n    tokens = []\n    \n    print(\"Getting embeddings.\")\n    for word in model.wv.vocab:  \n        if(word in words):\n            tokens.append(model[word])\n            labels.append(word)\n    print(\"Embeddings extracted.\")\n    print()\n        \n    print(\"Performing dimensionality reduction with t-sne.\")\n    tsne_model = TSNE(perplexity=5, n_components=2, init='pca', n_iter=2500, verbose=0)\n    new_values = tsne_model.fit_transform(tokens)\n    print(\"Dimensioanlity reduction complete.\")\n    print()\n    \n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(8, 8))\n    for i in range(len(x)):\n        if(labels[i] in words):\n            plt.scatter(x[i],y[i])\n            plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n    plt.show()\n    return ","5f5c7f40":"#List of words to visualize in the plot\nwords = ['china', 'italy', 'taiwan', 'india', 'japan', 'france', \n         'spain', 'canada', 'infection', 'disease', 'pathogen', \n         'organism', 'bacteria', 'virus', 'covid19', 'coronavirus', \n         'healthcase', 'doctor', 'nurse', 'specialist', 'hospital', \n         'novel', 'human', 'sars', 'covid', 'wuhan', 'case', \n         'background', 'dynamic', 'pneumonia', 'outbreak', 'pandemic', \n         'syndrome', 'contact', 'wash', 'hands', 'cough', \n         'respiratory', 'case', 'fear', 'spike', 'curve', \n         'transmission', 'seasonal', 'genome', 'dna', 'testing', \n         'asymptomatic', 'global', 'spread', 'diagnosis']\n  \n#Call the function on our dataset  \ntsne_plot(model, words)","c5b57ae2":"#Word to compare against and number of similar words to print out \nword = 'facemask'\nsimilarCount = 3","553ad7d8":"#Get and print the results \nresults = model.wv.most_similar(positive=word, topn=similarCount)\nprint(\"Input word:\", word)\nprint(\"Top \"+str(similarCount)+\" similar words are:\")\nfor index, word in enumerate(results):\n    print(str(index+1)+\". \"+word[0]+\" --- Score: \"+str(word[1]))","194d636f":"#Words to compute cosine similarity over  \nword1 = 'china'\nword2 = 'wuhan'\n\n#Get the word embeddings \nembedding1 = model.wv[word1]\nembedding2 = model.wv[word2]","8a8c46f9":"#Compute the cosine similarity and print the results \ncosineSimilarity = np.sum(embedding1*embedding2) \/ (np.sqrt(np.sum(np.square(embedding1)))*np.sqrt(np.sum(np.square(embedding2))))\nprint(\"Word1: \"+word1+\" --- Word2: \"+word2)\nprint(\"Cosine similarity: \"+ str(cosineSimilarity))","97bfc33d":"#Set the GPU device\ndevice = 0\ntorch.cuda.set_device(device)","db866ac7":"#Read in the saved data \nprint(\"Loading the dataframe.\")\ndf_covid = pd.read_csv('..\/input\/cord19cleaneddata\/covidDataCleaned.csv')\nprint(\"Dataframe loaded.\")\nprint()\ndf_covid = df_covid.head(500)","fe7df27b":"#Variable to store the batch size\nbatchSize = 500 ","b21a4cd5":"#Load the model\nprint(\"Loading BERT semantic similarity model.\")\nmodel = ClinicalBertSimilarity(device='cuda', batch_size=batchSize) #defaults to GPU prediction\nprint(\"Model loaded.\")\nprint()","645c2e92":"#The primary questions that attempt to be answered  \nprimaryQuestions = [\n    \"What is known about transmission, incubation, and environmental stability of coronavirus\"\n    #\"What do we know about coronavirus risk factors\"\n    #\"What do we know about coronavirus genetics, origin, and evolution\"\n    #\"What do we know about vaccines and therapeutics for coronavirus\"\n    #\"What has been published about coronavirus medical care\"\n    #\"What do we know about non-pharmaceutical interventions for coronavirus\"\n    #\"What do we know about diagnostics and surveillance of coronavirus\"\n    #\"In what ways does geography affects virality\"\n    #\"What has been published about ethical and social science considerations regarding coronavirus\"\n    #\"What has been published about information sharing and inter-sectoral collaboration\"\n]","acb3bc22":"#Extract the text from the articles \narticleTexts = df_covid.drop([\"paper_id\", \"abstract\", \"title\", \"authors\", \"journal\", \"url\", \"abstract_word_count\", \"body_word_count\"], axis=1)\ntext_list = list(articleTexts['body_text'])","4981ea41":"#Get the index of where the prediction ranks among the current best predictions \ndef computeScoreIndex(predictionScore, answerScores):\n    index = 0\n    while(index < len(answerScores)):\n        if(predictionScore > answerScores[index]):\n            break\n        index += 1\n    return index ","42d83730":"#Function to make a batch to send through the semantic similarity model \ndef makeBatch(query, sentences):\n    batch = []\n    index = 0\n    while(index < len(sentences)):\n        batch.append((query, sentences[index]))\n        index += 1\n    return batch ","728d976d":"#Set scores to strings for saving \ndef convertScores(answers):\n    for key in answers.keys():\n        for scoreIndex, score in enumerate(answers[key]['scores']):\n            answers[key]['scores'][scoreIndex] = str(answers[key]['scores'][scoreIndex])\n    return answers","7dce2bad":"#Dictionary to store answers and variable to specify the number of answers to store \nanswerCount = 10\nquestionResponses = {'titles': [], 'sentences': [], 'scores': []}\nanswers = {}\nfor query in primaryQuestions:\n    answers[query] = copy.deepcopy(questionResponses)","eb272fcc":"#Function to update the most relevant answers based on model predictions \ndef updateAnswersSentences(query, title, sentences, predictions, answers, answerCount):\n    #Get the top answerCount prediction scores \n    topIndices = predictions.argsort()[-answerCount:][::-1]\n    for index in topIndices:\n        #Case where lists are empty\n        if(len(answers[query]['scores']) == 0):\n            answers[query]['titles'].append(title)\n            answers[query]['sentences'].append(sentences[index])\n            answers[query]['scores'].append(predictions[index])\n        #Case where lists have length shorter than answerCount \n        elif(len(answers[query]['scores']) < answerCount):\n            scoreIndex = computeScoreIndex(predictions[index], answers[query]['scores'])\n            answers[query]['titles'].insert(scoreIndex, title)\n            answers[query]['sentences'].insert(scoreIndex, sentences[index])\n            answers[query]['scores'].insert(scoreIndex, predictions[index])\n        #Case where lists are full \n        else:\n            scoreIndex = computeScoreIndex(predictions[index], answers[query]['scores'])\n            #Check to see if an item should be bumped out of the list \n            if(scoreIndex < answerCount):\n                answers[query]['titles'].insert(scoreIndex, title)\n                answers[query]['sentences'].insert(scoreIndex, sentences[index])\n                answers[query]['scores'].insert(scoreIndex, predictions[index])\n                answers[query]['titles'] = answers[query]['titles'][:answerCount]\n                answers[query]['sentences'] = answers[query]['sentences'][:answerCount]\n                answers[query]['scores'] = answers[query]['scores'][:answerCount]\n    return answers ","8dae2f40":"#Loop through all queries   \nfor queryIndex, query in enumerate(primaryQuestions):\n    #Loop through all articles \n    for textIndex, text in enumerate(text_list):\n        #Tokenize the article to sentences and loop through all sentences computing prediction scores for each \n        sentences = sent_tokenize(text)\n        batchIndex = 0\n        while(batchIndex*batchSize < len(sentences)):\n            batchSentences = None\n            batch = None\n            #Check to make sure the batch size won't go out of bounds in regard to the sentences \n            if((batchIndex*batchSize)+batchSize > len(sentences)):\n                batchSentences = sentences[batchIndex*batchSize:len(sentences)]\n                batch = makeBatch(query, batchSentences)\n            else:\n                batchSentences = sentences[batchIndex*batchSize:(batchIndex*batchSize)+batchSize]\n                batch = makeBatch(query, batchSentences)\n            predictions = model.predict(batch)\n            answers = updateAnswersSentences(query, df_covid.iloc[textIndex][\"title\"], batchSentences, predictions, answers, answerCount)\n            batchIndex += 1\n        print(\"Processing query \"+str(queryIndex)+\" of \"+str(len(primaryQuestions))+\" --- Article \"+str(textIndex)+\" of \"+str(len(text_list)), end='\\r')\n    print()","687b1ddc":"#Save the results\nanswers = convertScores(answers)\njsonData = json.dumps(answers)\nf = open(\"topSentences.json\",\"w\")\nf.write(jsonData)\nf.close()","02fe1135":"#Read in the json file of results \nfilename = \"topSentences.json\"\nanswers = None\nwith open(filename, 'r') as myfile:\n    answers = json.load(myfile)","2353e53b":"#Print the results \nfor query in answers.keys():\n    print(\"Query: \"+query)\n    resultCount = len(answers[query]['scores'])\n    index = 0\n    while(index < resultCount):\n        print(str(index+1)+\". Article: \"+str(answers[query]['titles'][index]))\n        print(\"Sentence: \"+answers[query]['sentences'][index])\n        print(\"Score: \"+answers[query]['scores'][index])\n        print()\n        index += 1","c47ea0cb":"#Dictionary to store answers and variable to specify the number of answers to store \nanswerCount = 10\nquestionResponses = {'titles': [], 'scores': []}\nanswers = {}\nfor query in primaryQuestions:\n    answers[query] = copy.deepcopy(questionResponses)","52bd19fd":"#Function to update the most relevant answers based on model predictions \ndef updateAnswersArticles(query, title, articleScore, answers, answerCount):\n    #Case where lists are empty\n    if(len(answers[query]['scores']) == 0):\n        answers[query]['titles'].append(title)\n        answers[query]['scores'].append(articleScore)\n    #Case where lists have length shorter than answerCount \n    elif(len(answers[query]['scores']) < answerCount):\n        scoreIndex = computeScoreIndex(articleScore, answers[query]['scores'])\n        answers[query]['titles'].insert(scoreIndex, title)\n        answers[query]['scores'].insert(scoreIndex, articleScore)\n    #Case where lists are full \n    else:\n        scoreIndex = computeScoreIndex(articleScore, answers[query]['scores'])\n        #Check to see if an item should be bumped out of the list \n        if(scoreIndex < answerCount):\n            answers[query]['titles'].insert(scoreIndex, title)\n            answers[query]['scores'].insert(scoreIndex, articleScore)\n            answers[query]['titles'] = answers[query]['titles'][:answerCount]\n            answers[query]['scores'] = answers[query]['scores'][:answerCount]\n    return answers ","a558b9e7":"#Loop through all queries   \nfor queryIndex, query in enumerate(primaryQuestions):\n    #Loop through all articles \n    for textIndex, text in enumerate(text_list):\n        #Tokenize the article to sentences and loop through all sentences computing prediction scores for each \n        #Create a variable to store all score values \n        sentences = sent_tokenize(text)\n        sentenceScores = np.array([])\n        batchIndex = 0\n        while(batchIndex*batchSize < len(sentences)):\n            batchSentences = None\n            batch = None\n            #Check to make sure the batch size won't go out of bounds in regard to the sentences \n            if((batchIndex*batchSize)+batchSize > len(sentences)):\n                batchSentences = sentences[batchIndex*batchSize:len(sentences)]\n                batch = makeBatch(query, batchSentences)\n            else:\n                batchSentences = sentences[batchIndex*batchSize:(batchIndex*batchSize)+batchSize]\n                batch = makeBatch(query, batchSentences)\n            predictions = model.predict(batch)\n            sentenceScores = np.append(sentenceScores, predictions)\n            batchIndex += 1\n        articleScore = np.mean(sentenceScores)\n        answers = updateAnswersArticles(query, df_covid.iloc[textIndex][\"title\"], articleScore, answers, answerCount)\n        print(\"Processing query \"+str(queryIndex)+\" of \"+str(len(primaryQuestions))+\" --- Article \"+str(textIndex)+\" of \"+str(len(text_list)), end='\\r')\n    print()","4776aaf2":"#Save the results\nanswers = convertScores(answers)\njsonData = json.dumps(answers)\nf = open(\"topArticles.json\",\"w\")\nf.write(jsonData)\nf.close()","c0e8dddf":"#Read in the json file of results \nfilename = \"topArticles.json\"\nanswers = None\nwith open(filename, 'r') as myfile:\n    answers = json.load(myfile)","8662d6d9":"#Print the results \nfor query in answers.keys():\n    print(\"Query: \"+query)\n    resultCount = len(answers[query]['scores'])\n    index = 0\n    while(index < resultCount):\n        print(str(index+1)+\". Article: \"+str(answers[query]['titles'][index]))\n        print(\"Score: \"+answers[query]['scores'][index])\n        print()\n        index += 1","20812c41":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_0.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_0.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","ac241605":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_11.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_11.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","5445ebfa":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_12.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_12.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","21db2935":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_13.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_13.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","c9ff6333":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_15.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_15.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","52eaba25":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_16.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_16.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","741edd41":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_17.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_17.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","ca3bc77c":"#Load in pandas and set options \nimport pandas as pd \nimport numpy as np\npd.set_option('display.max_colwidth', 10000)\npd.set_option('display.max_rows', 100)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topSentences_3.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results)\n\n#Read in the saved data \nresults = pd.read_csv('..\/input\/cord19results\/topArticles_3.csv')\nresults = results.drop(['Unnamed: 0'], axis=1)\nresults.index = np.arange(1, len(results)+1)\ndisplay(results.head(10))","84d43aea":"## 3. Loading CORD-19 Data\n\nThe first step before doing any data processing will be to load in all the libraries that will be used throughout this notebook. As you can see a variety of these are used; pandas for storing data frames, nltk for tokenization, torch for BERT model description\/architecture, etc. ","8c54d928":"### Cons\n\n1. Compute Time: BERT is a massive model with millions of parameters; thus it takes 7-8 hours to compute score values for one query across all sentences using a GPU\n2. Hardware Requirements: GPU hardware is critical for fast computation time   \n3. Classification Context Level: Currently sentences are classified by themselves in regard to a score; in the future looking at computing scores across sets of sentences or paragraphs to ensure proper context could be beneficial\n4. Query Wording: Wording can impact results\n\nNote that in terms of the first con (currently the most problematic) there are several obvious solutions that could be implemented going forward to resolve this issue. The first solution would be to use a technique similar to  [Sentence-BERT](https:\/\/arxiv.org\/abs\/1908.10084) where sentence embeddings are pre-computed across the corpus. At runtime the only computation requirement is then computing a query embedding along with similarity scores. Thus, instead of sending all the sentences through the model for each query, we only send the query. If this system were to be implemented in the real world this would be a requirement, otherwise the computation time constraints are simply too large. ","7ef30947":"After tokenizing the text word2vec is trained. The embedding size used is 100 with a window size of 5 and minimum word count of 10. For potentially more expressive embeddings the vector size can be increased to 200, 300, etc. Note that it is not recommended to go above 500 due to the limited size of the corpus. We save the learned word vectors for use in other downstream tasks. ","eac5ba5b":"Next article text is extracted from the pandas data frame and placed in a list. ","2b2b3f76":"Next we perform computation by looping through all queries and articles, tokenizing sentences, and computing score values for each while maintaining the top results. ","9e88640e":"## 2. Approach Overview\n\nThe approach implemented has various pros to it as well as a number of cons. In future iterations these cons can be addressed enabling the system to be faster as well as derive more powerful results. ","f9683bbb":"Finally we save the data frame for quicker access later (for example if we want to change the pre-processing steps) as the computations above take some time for completion.","f27ea8b0":"Here text is tokenized into a list of sentences with each element in the list containing a list of words. This is the format [gensim](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html) expects as input for learning word2vec embeddings. ","891bfda3":"Compute the results.","4a379b0c":"#### Query: What is the physical science of coronavirus such as the charge distribution, adhesion to hydrophilic\/phobic surfaces, and environmental survival to inform decontamination efforts for affected areas? (Task 1 - Subtask)","886f6f50":"Here we create a function to update the dictionary of top sentence results based on new score values from our BERT model. ","d1bbad5f":"Since the actual text is contained within four separate folders of json files (biorxiv_medrxiv, comm_use_subset, custom_license, and noncomm_use_subset) this code goes through all these folders and extracts file names for article loading. ","ceb3a07e":"Create a function to compare a computed score value to a list of current best scores. This function will later be used in determining when to add new sentences\/articles to the best results list. ","2d457668":"## 1. Background Information\n\nBefore diving into the code and results there are a number of key concepts that will be covered as they are used throughout the notebook. For those well-versed in the field of NLP this background knowledge can be skipped. ","9ba5f4db":"Next we get the top N similar words to some input based on the learned word vectors. Here we get the 3 most similar words to 'facemask'. Note that the variables below can be changed to get different count values for other words (given that they are present in the corpus). ","fd3d6494":"Save the results.","8143a961":"The top sentence results are now saved to a json file. ","05a10ee2":"Read in the pre-processed and cleaned data frame. \n\nThe code below can be modified to load the file saved at the end of section 4 if edits are made to it. Otherwise the pre-computed data frame can be loaded from \"cord19cleaneddata\".\n\n<b><u>Currently we only load in and use the first 500 articles due to runtime. To use the entire dataset remove line 6 that reads<\/u><\/b>\n\n\"df_covid = df_covid.head(500)\"","10514bda":"Print the results; this returns the most relevant articles in regard to our query!","278c6eac":"Next we extract the text body from articles and do a little more pre-processing by converting all text to lowercase and removing non-alphanumeric characters and punctuation. ","d2f07a0f":"Last we will compute the cosine similarity between two words using our word vectors. Cosine similarity is a distance measurement that ranges from -1 to 1, higher values represent words more similar in meaning. \n\nFirst we set the words and get the embedding value for each. The variables 'word1' and 'word2' can be changed to compute the cosine similarity between other sets of words. ","bf6ad142":"As another pre-processing step we remove non-English articles.","e8db16a7":"To aid in pre-processing and data cleaning we compute the number of words in both the abstract and text body of articles and add these to the pandas data frame. ","15270433":"#### Query: What is the persistence of coronavirus on surfaces of different material such as copper, stainless steel, and plastic? (Task 1 - Subtask)","9e6d22f4":"Source: [Image](http:\/\/www.sci-news.com\/medicine\/guidelines-management-icu-patients-covid-19-08295.html)\n\n## Project Overview \nThe goal of this project is to utilize state-of-the-art NLP techniques for text mining and information retrieval using transformer-based LMs. This is accomplished by feeding a pre-trained BERT model with sentences and queries while computing similarity scores. By using this technique we extract the most relevant sentences for each task question. Hopefully this tool provides helpful information to medical personnel and other researchers as they seek to quickly get answers to many questions about the deadly COVID-19 virus.  \n\nNote that this notebook was created using an older iteration of the dataset, about 33,000 total articles; thus the stages for reading in the data may need to be adjusted when working with the latest release. This was done due to the time constraint of the challenge as well as the computation requirement for using BERT. As this is the case the notebook has been loaded with the older dataset under the \"cord1933k\" folder. \n\nAlso note that the additional data used is <b><u>only<\/u><\/b> for runtime speedup purposes and does not pull any information other than that contained in CORD-19. The data in the \"cord19cleaneddata\" folder is the original data saved in csv files for quick loading to avoid data pre-processing steps. Likewise the \"cord19results\" folder contains the results from running this model over various queries across the corpus using outside GPU hardware resources. \n\nOne final thing to mention is that in various parts of this notebook the dataset has been condensed with slicing. This is only for runtime speedup purposes and thus the answers derived from BERT and word2vec may be affected. <b><u>For the best results change the code where stated to compute over ALL data<\/u><\/b>.\n\nTwo useful sources while developing this code base were [COVID-19 Literature Clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering) and [BERT Semantic Text Similarity](https:\/\/github.com\/AndriyMulyar\/semantic-text-similarity)","ff4c121e":"#### Query: What is the prevalence of asymptomatic shedding and transmission of SARS-CoV-2 particularly in children? (Task 1 - Subtask)","d9cfbb39":"#### Query: How long are people contagious with SARS-CoV-2 even after recovery? (Task 1 - Subtask)","36d8ee91":"# Information Retrieval with Semantic Similarity and BERT\n\nIn this notebook the powerful pre-trained [BERT](https:\/\/arxiv.org\/abs\/1810.04805) Language Model is used to compute semantic similarity scores over all sentences in the dataset in regard to some of the tasks presented (queries). Essentially this is a form of fine-grained information retrieval where the most relevant sentences are returned for each task (in contrast to high-level information retrieval where an entire article is returned). Before using BERT for semantic similarity computation, word vectors are learned over the dataset using the famous [word2vec](https:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) model. Although these word embeddings aren't used throughout the remainder of the notebook they may prove beneficial to other researchers and challenge participants as they try to tackle downstream tasks.","b3bb60f7":"## 6. Visualizing Learned Word Vectors, Computing Cosine Similarity between Words, and Extracting the Most Similar Words\n\nUsing the learned word vectors we will now perform a number of different analytics over them. First we will visualize some of the more prominent words using matplotlib for plotting and t-sne for dimensionality reduction. T-Distributed Stochastic Neighbor Embedding, or t-sne, maps 100 dimensional vectors down to a 2-dimensional space. \n\nLoad in the saved word vectors (skip this step if running the notebook sequentially as these will already be in RAM).","e6285fa5":"## 8. Results\n\nResults of using BERT for information retrieval and semantic similarity are as follows. Here we give the results in regard to the most relevant sentences for each query followed by the most relevant articles. <b><u>To view the results run the code blocks contained below. Results have been added to the dataset and computed beforehand on additional hardware (due to runtime constraints) over ALL ~33k articles in \"cord1933k\".<\/u><\/b> \n\nTo add context to answers we have added a window size of 2 around highly ranked sentences (2 sentences before and 2 after). Here we report all results (~100) for sentences; some are able to directly answer the query while others are slightly out of context. There are numerous ways to advance the techniques currently in use for better results. For discussion please contact the authors.\n\nFor articles only the 10 most relevant are reported.\n\n#### Query: What is known about transmission, incubation, and environmental stability of coronavirus? (Task 1)","5fd4a681":"### Pros\n\n1. Fine-Grained Results: Often contain a direct answer to the query posed\n2. Query Scaling: Works on any input and is not task specific \n3. Reporting Potential: Allows for the N most relevant sentences to be computed where a report can then be generated\n4. Dataset Reduction: Derive the N most relevant articles\n5. Leveraging Powerful Bert LM: Takes advantage of the current state-of-the-art NLP LM technique\n6. Literature Backing: Bert-based models set the bar in STS (Semantic Textual Similarity) Tasks","b1a3b0c4":"### Semantic Similarity\n\nSemantic similarity is a metric defined over a set of documents or terms where the idea of distance between them is based on the likeness of their meaning or semantic content as opposed to similarity which can be estimated regarding their syntactical representation (string format). In other words the goal of semantic textual similarity is to input two sentences \/ passages \/ etc. and output some score value with higher scores corresponding to sentences or passages more similar in meaning and lower scores vice-versa. BERT or BERT-based models such as [XLNet](https:\/\/arxiv.org\/abs\/1906.08237) and [RoBERTa](https:\/\/arxiv.org\/abs\/1907.11692) have essentially set the state-of-the-art on this task. To compute semantic similarity scores over sentences and task queries the following architecture is used:","0515c62a":"Create another function useful in making batches to send through our BERT semantic similarity model. ","81808e53":"The following function is used for performing dimensionality reduction and creating the plot. ","3e536108":"After setting up our environment by loading in all the necessary libraries we load the CORD-19 metadata. This provides a lot of relevant information that can be used for pre-processing and analysis. ","10bfd1d6":"Next we set the batch size or the number of sentences and queries to compute the semantic similarity score over in parallel. Using a batch size of 500 takes up about 10.6GB of RAM. For systems with fewer than 10GB of RAM available this number will need to be reduced to avoid a \"CUDA out of memory\" error. ","73664755":"Again we save the pre-processed data for faster computation of downstream tasks. ","dc8ec454":"### Word2Vec\n\nWord2vec is a combination of models that can be used to learn vectors \/ embeddings for all words present in a corpus. This algorithm was one of the major break-throughs that started the Deep Learning revolution in the field of NLP as these vector embeddings can be fed into neural networks to better accomplish downstream tasks. Embeddings are typically learned in one of two ways, first a single word is used to predict the context and other words around it (skip-gram); second the context around a word is used to predict the word itself (CBOW). These techniques allow words to be mapped to a vector space (typically 100, 200, or 300 dimensional) where similar words are mapped to similar locations in the space. \n\nFor an in-depth tutorial on this subject see [Word2vec](https:\/\/medium.com\/@zafaralibagh6\/a-simple-word2vec-tutorial-61e64e38a6a1).","362feb37":"And finally we print the results. This returns the most relevant sentences in regard to our initial input query!","e6b7e07b":"#### Query: What is the natural history of SARS-CoV-2 and its shedding from an infected person? (Task 1 - Subtask)","9f27687d":"Read in the results (skip if the results are already in RAM). ","32809191":"## 4. Data Pre-Processing & Cleaning\n\nBefore computing word vectors or using BERT for semantic similarity we first do a number of pre-processing steps to clean up the data. This consists of removing articles that don't meet certain criteria as well as non-English text. \n\nThe code below can be modified to load the file saved at the end of the section above if edits are made to it. Otherwise the pre-computed data frame can be loaded from \"cord19cleaneddata\". \n\n<b><u>Currently we only load in and use the first 1000 articles due to runtime. To use the entire dataset remove line 6 that reads<\/u><\/b>\n\n\"df_covid = df_covid.head(1000)\"","ff73a8ac":"### Top Sentences\n\nThis section is for computing the most relevant <b><u>sentences<\/u><\/b> in the corpus regarding some query. There are various code differences between getting sentences and articles, thus they are broken up into separate sections that can be executed in either order.  \n\nFirst a dictionary data structure is created to store the top N results (in this case 10) relative to each query. This dictionary has a key for each query; underneath that are keys for article titles, most similar sentences, and semantic similarity score values. Each of these are lists containing the top results in order from best to worst. \n\nNote that the first element within the 'titles' list corresponds to the title of the article that is first in the 'sentences' list. Likewise, this applies to 'scores' too. ","298b658b":"Similar to our process for getting the top sentences, we create a function to update the dictionary of top article results based on new score values from BERT. ","9c4fc1a3":"Next we create a class description for a json file reader used in loading articles. This class functions by loading the file and then extracting the paper id, article abstract, and text body. ","b1c56bbe":"## Notebook Setup \n\nThis notebook is partitioned into the following sections:\n1. Background Information (Word2Vec \/ BERT \/ Semantic Similarity)\n2. Approach Overview (Pros \/ Cons)\n3. Loading CORD-19 Data\n4. Data Pre-Processing & Cleaning \n5. Computing Word Vectors with Word2Vec\n6. Visualizing Learned Word Vectors, Computing Cosine Similarity Between Words, and Extracting the Most Similar Words\n7. Computing Most Relevant Sentences \/ Articles\n8. Results\n\nNote that all sections except for 5 and 6 are self-contained meaning that they aren't dependent on running earlier sections of code <b><u>given that the library imports in section 1 have been executed<\/u><\/b>. In other words if a viewer would just like to look at the results they can skip to section 8 and run the code blocks there, if they simply want to use BERT they can skip to section 7 and run all code sequentially within it, etc. To allow the notebook to function in this manner intermediate processing of the dataset has been saved in the \"cord19cleaneddata\" folder. Likewise all results have been stored under the \"cord19results\" folder since massive GPU hardware resources are required to get lots of answers quickly. \n\nTo run section 6, section 5 must first be executed. \n\n<b><u>Again, to view the results ONLY please go to section 8 and directly run the code blocks there skipping all other sections.<\/u><\/b>","5b8a31fe":"<b><u>DISCLAIMER<\/u><\/b>:  THIS SET OF CODE IS FOR USE IN A NON-CLINICAL RESEARCH SETTING.  IT IS NOT INTENDED FOR USE TO TREAT OR DIAGNOSE PATIENTS, DRIVE CLINICAL MANAGEMENT OR INFORM CLINICAL MANAGEMENT.\n\nThis software is provided on an \u201cas is\u201d basis without any warranty or liability whatsoever. Northrop Grumman Systems Corporation (NGSC) expressly disclaims, to the maximum extent permissible by applicable law, all warranties, express, implied and statutory, including without limitation any implied warranty of merchantability, fitness for a particular purpose, non-infringement, or arising from course of performance, dealing, usage, or trade. NGSC shall not be liable to the Government or any user of the software for any incidental, consequential, special or other damages, including loss of profits, revenue, or data, resulting, directly or indirectly, from use of the software.\n\n\u00a9 Northrop Grumman Systems Corporation ","98b2f041":"## 5. Computing Word Vectors with Word2Vec\n\nHere we learn word vectors over all tokens in the CORD-19 data set. Although these vectors are not directly used in the remainder of this notebook they may prove beneficial to other researchers\/groups in downstream tasks. \n\nFirst we read in the pre-processed data set. The code below can be modified to load the file saved at the end of the section above if edits are made to it. Otherwise the pre-computed data frame can be loaded from \"cord19cleaneddata\".  \n\n<b><u>Currently we only load in and use the first 5000 articles due to runtime. To use the entire dataset remove line 6 that reads<\/u><\/b>\n\n\"df_covid = df_covid.head(5000)\"","0c444305":"Here we create a variable to store the 'query' we want to search our corpus over. This variable expects a list, thus to compute similarities over multiple questions some of the queries below can be uncommented or you can add your own. Currently the code is setup to search for the most relevant sentences\/articles relative to the first task of the challenge: \"What is known about transmission, incubation, and environmental stability?\"\n\nOn another note we have added the word 'coronavirus' to many of the tasks to aid our semantic search capabilities in returning better results. ","6c64c71b":"Perform computation and get the top article results.  ","30ab0d8a":"Source: [BERT Explained](https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)\n\nAfter learning the LM in an unsupervised fashion it can be fine-tuned on various tasks. This has allowed the architecture to produce incredible scores as it already understands the concept of language extremely well. Thus, instead of having to learn both a language and a task during supervised training, the model only has to learn the task. For an in-depth overview of BERT see [Illustrated BERT](http:\/\/jalammar.github.io\/illustrated-bert\/).","9e9cc8c9":"Here we compute the cosine similarity. ","78c578ea":"To avoid training a model we load in a pre-trained BERT semantic similarity model from [BERT Semantic Similarity](https:\/\/github.com\/AndriyMulyar\/semantic-text-similarity). Thankfully this model has been pre-trained and fine-tuned on the [MED-STS](https:\/\/arxiv.org\/abs\/1808.09397) data set making it perfect to be used in this setting. <b><u>Change device to 'cpu' if no GPU is available system wide.<\/u><\/b>","5e17848f":"#### Query: What is the immune response and immunity to SARS-CoV-2? (Task 1 - Subtask)","39cb4b8e":"Specify another function helpful in saving results to a file. This function is used to convert score values from floats to strings that way results can be saved as json files. ","1e937bd6":"### Top Articles \n\nThis section is for computing the most relevant articles in the corpus.\n\nA dictionary data structure is created to store the top results. This dictionary has a key for each query; underneath that are keys for article titles and semantic similarity score values. Each of these are lists.","8e27989d":"#### Query: What do we know about vaccines and therapeutics for coronavirus? (Task 4)","c5c1b0c7":"Create the list of words to visualize and plot them. ","2ee20f99":"Next we perform data cleaning by removing all articles with a text body shorter than 1000 words. This number is arbitrary and can be modified. The thinking here is that this may help to remove less refined texts for better downstream results. ","9466ad3f":"### BERT\n\nBERT, or Bidirectional Encoder Representations from Transformers, was one of the recent major accomplishments in the field of NLP. At the time of its release in 2018 this architecture set the state-of-the-art on the GLUE benchmark encompassing numerous tasks such as question-answering, named-entity-recognition, and natural-language-inference. BERT built upon the [Transformer](https:\/\/arxiv.org\/pdf\/1706.03762.pdf) architecture by adding the concept of both left-to-right and right-to-left (bidirectional) processing. It also leveraged the concept of unsupervised learning by pre-training a massive Language Model (LM) on all of English Wikipedia using a [Masked Language Model](https:\/\/www.quora.com\/What-is-a-masked-language-model-and-how-is-it-related-to-BERT). ","8beea10c":"Read in the results (skip if the results are already in RAM). ","eb13cfe6":"Here we read json article files into a Python dictionary. From each article we extract the paper id, abstract, text body, title, authors, journal, and url. After reading in articles we convert the dictionary to a pandas data structure for more easy usage\/access. \n\n<b><u>Currently this is only done over the first 1000 articles due to runtime. To perform this computation over the entire dataset change line 4 from<\/u><\/b>\n\n\"for idx, entry in enumerate(all_json[:1000]):\"\n\n<b><u>to<\/u><\/b>\n\n\"for idx, entry in enumerate(all_json):\"","9ac673a9":"## 7. Computing Most Relevant Sentences \/ Articles\n\nTo use BERT for semantic similarity computation and information retrieval having a GPU is recommended (for faster computation speed). For multi-GPU systems the code below is used to specify the specific GPU device, this is particularly useful when running multiple jobs in parallel with each being mapped to separate hardware. <b><u>Note that for systems without a GPU the following code may need to be commented out.<\/u><\/b>  "}}