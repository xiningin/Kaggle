{"cell_type":{"6acda60a":"code","0e76aff6":"code","85d38c37":"code","8b6bafd9":"code","cc669698":"code","2fbe9028":"code","121ce81e":"code","33c6511c":"code","8177c771":"code","0d7a5feb":"code","41bf94e0":"code","16923226":"code","f55b39bb":"code","255ff846":"code","cdf348b9":"code","f1f1978c":"code","be58d11e":"code","66ff4e04":"code","9d99237f":"code","33f87f0c":"code","341b4609":"code","e3744282":"code","280a003a":"code","fd3512ed":"code","b210f349":"code","83bdd188":"code","be3c9ab8":"code","65e12d93":"code","83d5ae82":"code","ecd1305a":"code","115da32c":"code","dcf68237":"code","7cf347d2":"code","9923b4c2":"code","2d9ac650":"code","f1556441":"code","37f526a7":"code","226efdf6":"code","eab64fa2":"code","780b2c74":"code","499d7aef":"code","ba64ad47":"code","73dacc91":"code","e438fc4f":"code","c29d6630":"code","525b659f":"code","a7e933ec":"code","525499f2":"code","5ce440a2":"code","244bd06b":"code","d9356f3f":"code","2b501f02":"code","f65c3096":"code","a7006eca":"markdown","890e7b12":"markdown","cee9c345":"markdown","c53ce83a":"markdown","da08b237":"markdown","ed1b7dd9":"markdown","36718302":"markdown","a0e0c5df":"markdown","7899c858":"markdown","7f2263f8":"markdown","c2e8bf7b":"markdown","eae95faa":"markdown","7a0b123c":"markdown","c6a265cc":"markdown","ae6a25df":"markdown","11508184":"markdown","ccb431bf":"markdown","cdcbe817":"markdown","afbdc889":"markdown","72ca01a4":"markdown","929360e9":"markdown","753ad001":"markdown","d0610e18":"markdown","a8c347bc":"markdown","bb748657":"markdown","445b5458":"markdown","2b938e33":"markdown","990abb0d":"markdown","e7162f74":"markdown","6cbfc94e":"markdown","84f66f10":"markdown","2f5057ad":"markdown","3bc4c417":"markdown","2608dcba":"markdown","e5b5ec4c":"markdown","25e950d9":"markdown","6ecfe45f":"markdown","85c37cce":"markdown","01fccdf7":"markdown","749c887d":"markdown","7ebc73d0":"markdown","d226c10c":"markdown","43a01e71":"markdown","cf4ec87e":"markdown","3c986dbe":"markdown","edae036d":"markdown","ffcab7aa":"markdown","b3318555":"markdown","2f06f710":"markdown"},"source":{"6acda60a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport sklearn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport numpy\nimport seaborn as sns;\nsns.set(style=\"ticks\", color_codes=True)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.","0e76aff6":"Tweet= pd.read_csv(\"\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv\")\nTweet.head()","85d38c37":"(len(Tweet)-Tweet.count())\/len(Tweet)\n","8b6bafd9":"del Tweet['tweet_coord']\ndel Tweet['airline_sentiment_gold']\ndel Tweet['negativereason_gold']","cc669698":"\n\nax=Tweet['airline_sentiment_confidence'].plot(kind='hist', bins=50, fontsize=14, ,figsize=(12,7))\nax.set_title('Article Length in Words\\n', fontsize=20)\nax.set_ylabel('Frequency', fontsize=18)\nax.set_xlabel('Number of Words', fontsize=18);","2fbe9028":"\n\nax=Tweet['negativereason_confidence'].plot(kind='hist', bins=50, fontsize=14, figsize=(12,7))\nax.set_title('Article Length in Words\\n', fontsize=20)\nax.set_ylabel('Frequency', fontsize=18)\nax.set_xlabel('Number of Words', fontsize=18);","121ce81e":"Best_confidence = Tweet[Tweet['airline_sentiment_confidence'] > .80]\nprint('you have dropped :'+ str((round((1 - Best_confidence.shape[0]\/Tweet.shape[0]) * 100 )))+ '% of your data which has less that 80% confidence')\nTweet = Best_confidence","33c6511c":"Tweet['uniq_wds'] = Tweet['text'].str.split().apply(lambda x: len(set(x)))\nTweet['uniq_wds'].mean()","8177c771":"ax=Tweet['uniq_wds'].plot(kind='hist', bins=50, fontsize=14, figsize=(12,7))\nax.set_title('Unique Words Per Article\\n', fontsize=20)\nax.set_ylabel('Frequency', fontsize=18)\nax.set_xlabel('Number of Unique Words', fontsize=18);","0d7a5feb":"def plotBars(col):\n    sns.countplot(x=col, hue=col, data=Tweet ,palette=\"ch:.25\")\n","41bf94e0":"plt.figure(figsize=(12,7))\nplotBars('airline_sentiment')","16923226":"plt.figure(figsize=(12,7))\nplotBars('airline')","f55b39bb":"#sns.catplot( row=\"airline\" ,col=\"airline_sentiment\" ,kind=\"count\", palette=\"ch:.25\", data=Tweet);\nplt.figure(figsize=(12,7))\nsns.countplot(x=\"airline\", hue=\"airline_sentiment\",palette=\"ch:.25\", data=Tweet)\n","255ff846":"NR_Count=dict(Tweet['negativereason'].value_counts(sort=False))\n","cdf348b9":"NR_Count","f1f1978c":"def NR_Count(Airline):\n    if Airline=='All':\n        df=Tweet\n    else:\n        df=Tweet[Tweet['airline']==Airline]\n    count=dict(df['negativereason'].value_counts())\n    Unique_reason=list(Tweet['negativereason'].unique())\n    Unique_reason=[x for x in Unique_reason if str(x) != 'nan']\n    Reason_frame=pd.DataFrame({'Reasons':Unique_reason})\n    Reason_frame['count']=Reason_frame['Reasons'].apply(lambda x: count[x])\n    return Reason_frame","be58d11e":"NR_Count('All')","66ff4e04":"g = sns.catplot(x=\"Reasons\", y=\"count\", palette=\"ch:.25\" ,kind=\"bar\",data=NR_Count('All'))\ng.set_xticklabels(rotation=90)\n","9d99237f":"from wordcloud import WordCloud,STOPWORDS\ndf=Tweet[Tweet['airline_sentiment']=='negative']\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","33f87f0c":"from wordcloud import WordCloud,STOPWORDS\ndf=Tweet[Tweet['airline_sentiment']=='positive']\nwords = ' '.join(df['text'])\ncleaned_word = \" \".join([word for word in words.split()\n                            if 'http' not in word\n                                and not word.startswith('@')\n                                and word != 'RT'\n                            ])\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(cleaned_word)\nplt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","341b4609":"import re\nimport nltk\nfrom nltk.corpus import stopwords","e3744282":"def tweet_to_words(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return( \" \".join( meaningful_words )) ","280a003a":"def clean_tweet_length(raw_tweet):\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \",raw_tweet) \n    words = letters_only.lower().split()                             \n    stops = set(stopwords.words(\"english\"))                  \n    meaningful_words = [w for w in words if not w in stops] \n    return(len(meaningful_words)) ","fd3512ed":"Tweet['sentiment']=Tweet['airline_sentiment'].apply(lambda x: 0 if x=='negative' else 1)\nTweet['clean_tweet']=Tweet['text'].apply(lambda x: tweet_to_words(x))\nTweet['Tweet_length']=Tweet['text'].apply(lambda x: clean_tweet_length(x))","b210f349":"from sklearn.model_selection import train_test_split\ntrain,test = train_test_split(Tweet,test_size=0.2,random_state=42)","83bdd188":"train_clean_tweet=[]\nfor tweet in train['clean_tweet']:\n    train_clean_tweet.append(tweet)\ntest_clean_tweet=[]\nfor tweet in test['clean_tweet']:\n    test_clean_tweet.append(tweet)","be3c9ab8":"from sklearn.feature_extraction.text import CountVectorizer\nv = CountVectorizer(analyzer = \"word\")\ntrain_features= v.fit_transform(train_clean_tweet)\ntest_features=v.transform(test_clean_tweet)","65e12d93":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score","83d5ae82":"Classifiers = [\n    LogisticRegression(C=0.000000001,solver='liblinear',max_iter=200),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=200),\n    AdaBoostClassifier(),\n    GaussianNB()]","ecd1305a":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['sentiment'])\n        pred = fit.predict(test_features)\n    except Exception:\n        fit = classifier.fit(dense_features,train['sentiment'])\n        pred = fit.predict(dense_test)\n    accuracy = accuracy_score(pred,test['sentiment'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    print('Accuracy of '+classifier.__class__.__name__+'is '+str(accuracy))    ","115da32c":"from sklearn.model_selection import cross_val_score\nclassifier = RandomForestClassifier(n_estimators=200)\nall_accuracies = cross_val_score(estimator=classifier, X=train_features, y=train['sentiment'], cv=5)\n","dcf68237":"print('Acc',all_accuracies.mean())\nprint('Std',all_accuracies.std())\n","7cf347d2":"import numpy as np\nimport pandas as pd\nimport re\n#from sentiment_utils import *\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.initializers import glorot_uniform\nfrom keras.utils import np_utils\nfrom keras.callbacks import EarlyStopping\nfrom nltk.corpus import stopwords\n#np.random.seed(1)\nfrom sklearn.model_selection import train_test_split","9923b4c2":"def remove_stopwords(input_text):\n    '''\n    Function to remove English stopwords from a Pandas Series.\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    stopwords_list = stopwords.words('english')\n    # Some words which might indicate a certain sentiment are kept via a whitelist\n    whitelist = [\"n't\", \"not\", \"no\"]\n    words = input_text.split() \n    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n    return \" \".join(clean_words)\n\ndef remove_mentions(input_text):\n    '''\n    Function to remove mentions, preceded by @, in a Pandas Series\n    \n    Parameters:\n        input_text : text to clean\n    Output:\n        cleaned Pandas Series \n    '''\n    return re.sub(r'@\\w+', '', input_text)","2d9ac650":"Tweet.head()","f1556441":"#cleaning Data\ntrain_df = Tweet[['text', 'airline_sentiment']]\ntrain_df.text = Tweet.text.apply(remove_mentions)\ntrain_df.loc[:,'sentiment'] = train_df.airline_sentiment.map({'negative':0,'neutral':1,'positive':2})\ntrain_df = train_df.drop(['airline_sentiment'], axis=1)\n","37f526a7":"#This step is to find the maximun length of the input string so as to fed the neural net with same length\nraw_docs_train = train_df[\"text\"].values\nsentiment_train = train_df['sentiment'].values\n\nmaxLen = len(max(raw_docs_train, key=len).split())\nmaxLen","226efdf6":"#For our model we need to split our training dataset into test dataset. This is actually dev set for getting the loss\nX_train, X_test, Y_train, Y_test = train_test_split(raw_docs_train, sentiment_train, \n                                                  stratify=sentiment_train, \n                                                  random_state=42, \n                                                  test_size=0.1, shuffle=True)\nprint('Train data samples:', X_train.shape)\nprint('Test data samples:', X_test.shape)\nassert X_train.shape[0] == Y_train.shape[0]\nassert X_test.shape[0] == Y_test.shape[0]","eab64fa2":"\n#Y_oh_train = convert_to_one_hot(Y_train, C = num_labels)\nnum_labels = len(np.unique(sentiment_train))\nY_oh_train = np_utils.to_categorical(Y_train, num_labels)\nY_oh_test = np_utils.to_categorical(Y_test, num_labels)\nprint(Y_oh_train.shape)","780b2c74":"\n# load the GloVe vectors in a dictionary:\n\ndef read_glove_vecs(glove_file):\n    with open(glove_file, encoding=\"utf8\") as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\n\n\nword_to_index, index_to_word, word_to_vec_map = read_glove_vecs('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')","499d7aef":"word = \"bad\"\nindex = 289846\nprint(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\nprint(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])","ba64ad47":"\ndef sentences_to_indices(X, word_to_index, max_len):\n    \"\"\"\n    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n    \n    Arguments:\n    X -- array of sentences (strings), of shape (m, 1)\n    word_to_index -- a dictionary containing the each word mapped to its index\n    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n    \n    Returns:\n    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n    \"\"\"\n    \n    m = X.shape[0]                                   # number of training examples\n    \n    # Initialize X_indices as a numpy matrix of zeros and the correct shape (\u2248 1 line)\n    X_indices = np.zeros((m,max_len))\n    \n    for i in range(m):                               # loop over training examples\n        \n        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n        sentence_words =[word.lower().replace('\\t', '') for word in X[i].split(' ') if word.replace('\\t', '') != '']\n        \n        # Initialize j to 0\n        j = 0\n        \n        # Loop over the words of sentence_words\n        for w in sentence_words:\n            # Set the (i,j)th entry of X_indices to the index of the correct word.\n            try:\n                X_indices[i, j] = word_to_index[w]\n            except: 0\n            # Increment j to j + 1\n            j = j+1\n    \n    return X_indices","73dacc91":"# Create Keras Embedding layer\ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    \"\"\"\n    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n    \n    Arguments:\n    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    embedding_layer -- pretrained layer Keras instance\n    \"\"\"\n    \n    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n    \n    ### START CODE HERE ###\n    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n    emb_matrix = np.zeros((vocab_len,emb_dim))\n    \n    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n\n    # Define Keras embedding layer with the correct output\/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n    ### END CODE HERE ###\n\n    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n    embedding_layer.build((None,))\n    \n    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","e438fc4f":"\ndef ltsm_model(input_shape, word_to_vec_map, word_to_index):\n    \"\"\"\n    Function creating the ltsm_model model's graph.\n    \n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n\n    Returns:\n    model -- a model instance in Keras\n    \"\"\"\n    \n    ### START CODE HERE ###\n    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n    sentence_indices =  Input(shape=input_shape, dtype='int32')\n    \n    # Create the embedding layer pretrained with GloVe Vectors (\u22481 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)   \n    \n    # Propagate the embeddings through an LSTM layer with 512-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(512, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.3\n    X = Dropout(0.3)(X)\n     # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    X = LSTM(256, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.1\n    X = Dropout(0.1)(X)\n    \n    # Propagate X trough another LSTM layer with 512-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(512, return_sequences=False)(X)\n    # Add dropout with a probability of 0.3\n    X = Dropout(0.3)(X)\n    \n    \n    # Propagate X through a Dense layer with softmax activation to get back a batch of 3-dimensional vectors.\n    X = Dense(3, activation=None)(X)\n    # Add a softmax activation\n    X = Activation('softmax')(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=[sentence_indices], outputs=X)\n    \n    ### END CODE HERE ###\n    \n    return model","c29d6630":"model = ltsm_model((maxLen,), word_to_vec_map, word_to_index)\nmodel.summary()","525b659f":"from keras.optimizers import Adam,Adagrad,rmsprop\nadam = Adam(lr=0.01, epsilon=None, decay=0.0)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n","a7e933ec":"X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\nX_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\nprint(X_train_indices.shape)","525499f2":"earlystop = EarlyStopping(monitor='val_loss', restore_best_weights = True ,min_delta=0, patience=30, verbose=0, mode='auto')\n\nmodel.fit(X_train_indices, y=Y_oh_train, batch_size=512, epochs=120, \n          verbose=1, validation_data=(X_test_indices, Y_oh_test), callbacks=[earlystop])","5ce440a2":"def TestModel(stringList):\n        classess = ['negative','neutral','positive']\n        results=[]\n        if len(stringList) == 1:\n            x_test = np.array(stringList)\n            X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n            index = np.argmax(model.predict(X_test_indices))\n            return x_test[0] +' is a '+  classess[index]+' word'\n        for i,x in enumerate(stringList):\n            x = [x]\n            x_test = np.array(x)\n            X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n            index = np.argmax(model.predict(X_test_indices))\n            results.append(x_test[0] +' is a '+  classess[index]+' word')\n        return results\n            \n        ","244bd06b":"TestModel(['Nice AirPlance Service I love Them ', # Positive\n           'bad AirPlance', # negative\n           'What The Fuckk Plane' , # negative\n           'my Flight is so late and its about 1 hour why ?', # negative\n           'okay its fine flight , thank you ',# Postive\n           'I lost my Bag and and They Back it to me So Thanks ', # Negative\n           'okay' # neutral\n          ])","d9356f3f":"story = 'The flight was delayed at first but I felt so bad because I waited for more than three hours, the flight crew that he give us with\\\nfree meals during the flight that was awesome , I know we paid for the meal before but in fact they were more than \\\nfive meals  and i feel so But I am quite satisfied with the trip , My best trip ever but I hate them ..\\\nIm kidding I loved that service very much I hope you try it' \n# Negative Story + Kidding + Positive and Recommendation Can our model find it ?","2b501f02":"TestModel([story])","f65c3096":"TestModel(story.split(','))","a7006eca":"-- GloVe is you here ?\n","890e7b12":"3 - in this step we start proccessing data between negative and positive but in numeric way becuase our machine not know how to dail with strings just with numbers that representation of unique classess","cee9c345":"Words of thanks and gratitude and some keywords that are repeated in very large quantities may express positive in dealing with the company and recommendations to deal with in the future","c53ce83a":"11 - at first we define earlystopping that observe our validation data lose and get restore best weights after that if there is any improve of val_loss on maximum 30 epoch it will start and our mode is auto\n12- we fit our model with train data after that we run it on 512 batch size with 120 epoch ","da08b237":"7- Define LSTM Model ","ed1b7dd9":"Unfortunately, as you can see, he cannot distinguish us as human beings as to what was a good event and became a bad event or a bad event and became a good thing.\nAt each interval we will consider it as a separate sentence and we will predict the person's condition\nFor example, if the percentage is less than 40% of the total sentences, we will contact the author of the tweet to solve the problem immediately\nin next example we had more 50% 2 positive and 2 negative of 1 stroy so , we made our filter before at 40% of positive label so thats positive and at the next story splited with comma we have 50% pos acc so its positive ","36718302":"6 - Create Keras Embedding layer","a0e0c5df":"\n[GloVe](https:\/\/nlp.stanford.edu\/projects\/glove\/) is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\n\n4 - Now we need Glove Vectors for Word which is available online ","7899c858":"Helper Function :\n- Remove Function\n- Remove Tag Function","7f2263f8":"Note that the prevalence when multiplied by 100% is very low, but 80% is not an appropriate ratio at all, so you can try to use some other strategies like GridSearch with Cross Validation Using Random Forest Classifer !\n\n","c2e8bf7b":"8000 negative tweets and more than 3000 neutral tweets and also more than 2000 Positive Tweets <br> \nLets Visualize Air Lines  Data :","eae95faa":" we\u2019ll add the result as a new column that contains the number of unique words in each article.","7a0b123c":"Here we will create our First Helper Function plotBars wich help us to visualize unique class counts foreach column :","c6a265cc":"# NLP - Data Proccessing ","ae6a25df":"Hieghst Columns Missing Values Percentage  :\n- negativereason_gold             99.8%\n- airline_sentiment_gold          99.8%\n- tweet_coord                     93%\n\nand in next step we will remove it we didn't need it anymore\n             \n\n","11508184":"Well, the random forest is the best algorithm so far and performs almost 80%. Well that's an acceptable figure as a first step But we'll improve it Using Cross Validation","ccb431bf":"7\/7 That's Awesome and here we are very thanks and Let's Try a tweet contains a small story:","cdcbe817":" ## Machine Learning Lab:","afbdc889":"so i see we can put a threshold right there :\n- %80 for airline sentiment confidence \n- and if you want to make it classification problem with negative reasons just put another threshold at 65%<br><br>\nif confidence is lower than threshold drop all rows why ? because its called outlier and mabye make your model explode on your face","72ca01a4":"importing and get First 5 Rows of the data","929360e9":"1 - Importing Tools :","753ad001":"At this point, it might be helpful for us visualize a distribution of the article sentiment confidence and negative  confidence to see how skewed our average might be by outliers. Let's generate another plot to take a look:","d0610e18":"1- we trying to made Function to Convert Tweets to Words if it's not in stopword <br>\n- what is stopword ?\nin natural language processing, useless words (data), are referred to as stop words. Stop Words: A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query","a8c347bc":"10 - convert sentences to indices","bb748657":"\n6 - We have a clean data here but we need to make it to be Vectorizerd by eachword \n \n - Vectorized by eachword : Convert a collection of text documents to a matrix of token counts\n - Override the string tokenization step while preserving the preprocessing and n-grams generation steps.\n\n","445b5458":"Here we Visualize the biggest reasons that may lead to negative views and we will conclude that the biggest problems are customer service and problems in the flight process and delays and bad leadership.\nWell we can summarize the problem in that the password for success is customer service because it is possible where there is a tweet reported that there are problems because of what but also added that the customer service was bad","2b938e33":"13 - That's Test Model make testing appoarch on any data model ","990abb0d":"![](http:\/\/)5 - Apply Clean Tweet Function","e7162f74":"1- Define max length of all tweet","6cbfc94e":"How Much Accuracy and Variance in our Model :\n","84f66f10":"4- Let's apply Train and Test Split with 80% for training and 20% for testing and select tweets randomly","2f5057ad":"Here we will Visualize Each Airlane sentiment distubation in it's 3 classes\n","3bc4c417":"Let's Visualize Airline Sentiment :","2608dcba":"2- Splitting data ","e5b5ec4c":"in This step we try to measure what's columns have the higest column has missed values ","25e950d9":"Well, the way we deal with data in the case of deep learning will be a little different because we will use Recurrent neural networks based on iterative patterns, sequence data , whether in all sentences or in subjects or in the whole tweets, so we have to simplify it in another way\n\n-","6ecfe45f":"# Bouns Idea :","85c37cce":"Glove : Loaii why are you get me out from stanford ?<br>\nLoaii : we have some work with some tweets ...<br>\nGlove : Tweets agaaaain !!<br>\n","01fccdf7":"# Deep Learning Lab :","749c887d":"8 - Summary our code","7ebc73d0":"we can observe what is more and lowest data","d226c10c":"5- This function convert array of strings into array of Indices of word in the voacab","43a01e71":"2 - Here we trying to make it cleanTweet and That's mean we need it to be just in english language ","cf4ec87e":"## Trip Advisor System\n\n\nThis project will work on a practical solution for the classification of negative and positive tweets for airlines and will work on how we can and how we will use this model in the future we will carry out some analyzes on tweets and establish a small recommendation to users on the best companies and customer recommendation","3c986dbe":"That way we will present some words that are repeated in most tweets and we will notice that all things are related to either time management or personal belongings or ask for help from the community\nIn fact, I believe that the word help is being used to ask to expose the company and the greater the number of retweets, the more it becomes tragic\n\nWell we have a simple conclusion here that the method of complaint is by exposing the company and present the position of the evaluation to the public and this means that we need to pay attention to the tweets with large numbers of re-tweet because it may contain some negative words, especially if they are largely negative","edae036d":"## Exploraty data analysis","ffcab7aa":"# Testing Our Model :","b3318555":"3 - Convert the labels to One hot encoding vector for softmax for neural network","2f06f710":"9 - Compiling our ooptimizers :"}}