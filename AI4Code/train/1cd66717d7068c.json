{"cell_type":{"f9908558":"code","8ed10c91":"code","9d722a34":"code","9d6f5d3b":"code","7d6c1ce0":"code","62e8f6f9":"code","0438ef45":"code","28918f71":"code","ab55cd2a":"code","3fbef32b":"code","97bb086e":"code","d3fff76e":"code","5ed8da61":"code","2767e287":"code","df68248c":"code","8f99dd0d":"code","f2d8ba70":"code","f8240638":"markdown","ff3baab8":"markdown","d9da38ac":"markdown","acff4081":"markdown","7915a3b0":"markdown"},"source":{"f9908558":"!pip uninstall --y kaggle\n!pip install --upgrade pip\n!pip install kaggle==1.5.6","8ed10c91":"!mkdir -p ~\/.kaggle\n!cp kaggle.json ~\/.kaggle\n!ls -lha kaggle.json\n!chmod 600 ~\/.kaggle\/kaggle.json","9d722a34":"!kaggle competitions download -c 2020-ai-termproject-18011817","9d6f5d3b":"!unzip 2020-ai-termproject-18011817.zip","7d6c1ce0":"import pandas as pd\nimport numpy as np\nimport random\n\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision.datasets as data\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.preprocessing import MinMaxScaler","62e8f6f9":"device = torch.device('cuda')\n\ntorch.manual_seed(777)\nrandom.seed(777)\ntorch.cuda.manual_seed_all(777)\n\nlearning_rate = 0.01\ntraining_epochs = 6000\nbatch_size = 60","0438ef45":"xy_train = pd.read_csv('train_seoul_grandpark.csv', header = None, skiprows=1, usecols=range(1, 8))\n\nx_data = xy_train.iloc[: , 1:-1]\ny_data = xy_train.iloc[: , [-1]]\n\nx_data = np.array(x_data)\ny_data = np.array(y_data)\n\nscaler = MinMaxScaler()\nx_data = scaler.fit_transform(x_data)\n\nx_train = torch.FloatTensor(x_data)\ny_train = torch.FloatTensor(y_data)\n\nx_train.shape","28918f71":"train_dataset = TensorDataset(x_train, y_train)\ndata_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = batch_size, \n                                           shuffle = True, \n                                           drop_last = True)","ab55cd2a":"class MishFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_variables[0]\n        sigmoid = torch.sigmoid(x)\n        tanh_sp = torch.tanh(F.softplus(x)) \n        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        return MishFunction.apply(x)\n\ndef to_Mish(model):\n    for child_name, child in model.named_children():\n        if isinstance(child, nn.ReLU):\n            setattr(model, child_name, Mish())\n        else:\n            to_Mish(child)","3fbef32b":"linear1 = torch.nn.Linear(5, 8,bias=True)\nlinear2 = torch.nn.Linear(8, 8,bias=True)\nlinear3 = torch.nn.Linear(8, 8,bias=True)\nlinear4 = torch.nn.Linear(8, 8,bias=True)\nlinear5 = torch.nn.Linear(8, 1,bias=True)\n#dropout = torch.nn.Dropout(p=drop_prob)\nmish = Mish()","97bb086e":"torch.nn.init.kaiming_normal_(linear1.weight)\ntorch.nn.init.kaiming_normal_(linear2.weight)\ntorch.nn.init.kaiming_normal_(linear3.weight)\ntorch.nn.init.kaiming_normal_(linear4.weight)\ntorch.nn.init.kaiming_normal_(linear5.weight)\n\nmodel = torch.nn.Sequential(linear1,mish,\n                            linear2,mish,\n                            linear3,mish,\n                            linear4,mish,\n                            linear5).to(device)","d3fff76e":"\ubaa8\ub378 \ud559\uc2b5","5ed8da61":"loss = torch.nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\nlosses = []\nmodel_history = []\nerr_history = []\n\ntotal_batch = len(data_loader)\n\nfor epoch in range(training_epochs + 1):\n  avg_cost = 0\n  #model.train()\n  \n  for X, Y in data_loader:\n    X = X.to(device)\n    Y = Y.to(device)\n\n    optimizer.zero_grad()\n    hypothesis = model(X)\n    cost = loss(hypothesis, Y)\n    cost.backward()\n    optimizer.step()\n\n    avg_cost += cost \/ total_batch\n    \n  model_history.append(model)\n  err_history.append(avg_cost)\n  \n  if epoch % 100 == 0:  \n    print('Epoch:', '%d' % (epoch + 1), 'Cost =', '{:.9f}'.format(avg_cost))\n  losses.append(cost.item())\nprint('Learning finished')","2767e287":"best_model = model_history[np.argmin(err_history)]","df68248c":"xy_test = pd.read_csv('test_seoul_grandpark.csv', header = None, skiprows=1, usecols = range(1, 7))\nx_data = xy_test.iloc[:, 1:]\nx_data = np.array(x_data)\nx_data = scaler.transform(x_data)\nx_test = torch.FloatTensor(x_data).to(device)\n\nwith torch.no_grad():\n    model.eval()     \n    predict = best_model(x_test)\n","8f99dd0d":"submit = pd.read_csv('submit_sample.csv')\nsubmit['Expected'] = submit['Expected'].astype(float)\nfor i in range(len(predict)):\n  submit['Expected'][i] = predict[i]\nsubmit.to_csv('submit.csv', mode = 'w', index = False, header = True)\nsubmit","f2d8ba70":"!kaggle competitions submit -c 2020-ai-termproject-18011817 -f submit.csv -m \"14010974_\uc774\uae30\ud0dd\"","f8240638":"> Import \ub77c\uc774\ube0c\ub7ec\ub9ac\n","ff3baab8":"> Train , Test \ub370\uc774\ud130 \ub85c\ub4dc","d9da38ac":"> Mish Activation Class - [Mish \ub17c\ubb38 \ub9c1\ud06c](https:\/\/arxiv.org\/abs\/1908.08681)","acff4081":"> \ubaa8\ub378 \uac80\uc99d","7915a3b0":"> \ubaa8\ub378 \uc124\uacc4"}}