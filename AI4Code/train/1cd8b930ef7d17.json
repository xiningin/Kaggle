{"cell_type":{"523eff27":"code","88d9f702":"code","55289dab":"code","c3c6923d":"code","51aec7e2":"code","8fedce3f":"code","406b5e8d":"code","fdfafe12":"code","ca2373ef":"code","ce23558d":"code","64b297d2":"code","81af145b":"code","4239f206":"code","6c6baca0":"code","6806071b":"code","0dc955c4":"code","09201b00":"code","858bd1c8":"code","b10f746a":"code","5d519a70":"code","e8198275":"code","5cc6e13a":"code","ece3de83":"code","a89874a9":"code","908c400f":"code","9b035290":"markdown","b17595a9":"markdown","972152ee":"markdown","1b525b6e":"markdown","6ebb1add":"markdown","2d5dbba2":"markdown","23e40fef":"markdown","e5a46496":"markdown","780e75ef":"markdown","f9807e2a":"markdown","efb2f123":"markdown","68ec1678":"markdown","a4852e3d":"markdown","ed1105ff":"markdown","7bc0c1a0":"markdown","63ae34dc":"markdown","845711a5":"markdown","ae3e3eda":"markdown","273b5fa3":"markdown","9fff991f":"markdown","f21295f3":"markdown","e6ffc4ab":"markdown","3b5758ed":"markdown"},"source":{"523eff27":"import numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.impute import SimpleImputer\nimport missingno as msno\n\nfrom category_encoders.leave_one_out import LeaveOneOutEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nfrom imblearn.combine import SMOTEENN\nfrom collections import Counter","88d9f702":"train = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntest = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\")\ntrain.head()","55289dab":"train.info()","c3c6923d":"train.isnull().sum() \/ len(train)","51aec7e2":"msno.matrix(train, figsize=(14, 6), fontsize=10)","8fedce3f":"msno.heatmap(train, figsize=(14, 6), fontsize=10)","406b5e8d":"cities=train.groupby([\"city\"]).count().sort_values(by=\"target\", ascending=False)[:10].index.to_list()\n\nplt.figure(figsize=(12,4))\nsns.countplot(x=\"city\", hue='target', data=train[train[\"city\"].isin(cities)])\nplt.title('Top 10 City vs Target')\nplt.show()","fdfafe12":"categories = [\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\"]\ntitles = [\"Gender\", \"Relevent Experience\", \"Enrolled University\", \"Education Level\", \"Major Discipline\", \"Experience\", \"Company Size\", \"Company Type\", \"Last New Job\"]\nfor cat,tit in zip(categories, titles):\n    plt.figure(figsize=(12,4))\n    sns.countplot(x=cat, hue='target', data=train)\n    plt.title(tit + ' vs Target')\n    plt.show()","ca2373ef":"train[\"gender\"] = train[\"gender\"].fillna('Other')\ntrain[\"enrolled_university\"] = train[\"enrolled_university\"].fillna('Other')\ntrain[\"education_level\"] = train[\"education_level\"].fillna('Other')\ntrain[\"major_discipline\"] = train[\"major_discipline\"].fillna('Other')\ntrain[\"experience\"] = train[\"experience\"].fillna('-1')\ntrain[\"company_size\"] = train[\"company_size\"].fillna('0')\ntrain[\"company_type\"] = train[\"company_type\"].fillna('Other')\ntrain[\"last_new_job\"] = train[\"last_new_job\"].fillna('-1')\n\nprint(train.isnull().sum())","ce23558d":"test[\"gender\"] = test[\"gender\"].fillna('Other')\ntest[\"enrolled_university\"] = test[\"enrolled_university\"].fillna('Other')\ntest[\"education_level\"] = test[\"education_level\"].fillna('Other')\ntest[\"major_discipline\"] = test[\"major_discipline\"].fillna('Other')\ntest[\"experience\"] = test[\"experience\"].fillna('-1')\ntest[\"company_size\"] = test[\"company_size\"].fillna('0')\ntest[\"company_type\"] = test[\"company_type\"].fillna('Other')\ntest[\"last_new_job\"] = test[\"last_new_job\"].fillna('-1')\n\nprint(test.isnull().sum())","64b297d2":"train[\"education_level\"] = train[\"education_level\"].replace({'Other': \"0\", 'Primary School': \"2\", 'High School': \"4\", 'Graduate': \"8\", 'Masters': \"13\", 'Phd': \"20\"})\ntrain[\"experience\"] = train[\"experience\"].replace({'>20': \"21\", '<1': \"0\"})\n\ntrain[\"company_size\"] = train[\"company_size\"].replace({'10000+': \"10000-10000\", '10\/49': \"10-49\", '<10': \"10-10\", '0': \"0-0\"})\ntrain['company_size'] = train['company_size'].apply(lambda x: int((int(x.split('-')[0]) + int(x.split('-')[1])) \/ 2))\n\ntrain[\"last_new_job\"] = train[\"last_new_job\"].replace({'>4': \"5\", 'never': \"0\"})\ntrain[\"relevent_experience\"] = train[\"relevent_experience\"].replace({'No relevent experience': \"0\", 'Has relevent experience': \"1\"})\n\nLABELS = [\"relevent_experience\",\"education_level\",\"experience\",\"last_new_job\"]\n\nint_label = lambda x: x.astype('int64')\ntrain[LABELS] = train[LABELS].apply(int_label, axis=0)","81af145b":"test[\"education_level\"] = test[\"education_level\"].replace({'Other': \"0\", 'Primary School': \"2\", 'High School': \"4\", 'Graduate': \"8\", 'Masters': \"13\", 'Phd': \"20\"})\ntest[\"experience\"] = test[\"experience\"].replace({'>20': \"21\", '<1': \"0\"})\n\ntest[\"company_size\"] = test[\"company_size\"].replace({'10000+': \"10000-10000\", '10\/49': \"10-49\", '<10': \"10-10\", '0': \"0-0\"})\ntest['company_size'] = test['company_size'].apply(lambda x: int((int(x.split('-')[0]) + int(x.split('-')[1])) \/ 2))\n\ntest[\"last_new_job\"] = test[\"last_new_job\"].replace({'>4': \"5\", 'never': \"0\"})\ntest[\"relevent_experience\"] = test[\"relevent_experience\"].replace({'No relevent experience': \"0\", 'Has relevent experience': \"1\"})\n\ntest[LABELS] = test[LABELS].apply(int_label, axis=0)","4239f206":"X = train.drop(columns=['enrollee_id', 'target'])\ny = train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","6c6baca0":"pd.options.mode.chained_assignment = None  # default='warn'\n\nencoder = LeaveOneOutEncoder(return_df=True)\nX_train['city'] = encoder.fit_transform(X_train['city'], y_train)\n\nX_test['city'] = encoder.transform(X_test['city'])\ntest['city'] = encoder.transform(test['city'])","6806071b":"columns = ['gender', 'enrolled_university', 'major_discipline', 'company_type']\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[columns]), columns=OH_encoder.get_feature_names(columns))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[columns]), columns=OH_encoder.get_feature_names(columns))\ncols_test = pd.DataFrame(OH_encoder.transform(test[columns]), columns=OH_encoder.get_feature_names(columns))\n\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\ncols_test.index = test.index\n\ntemp_X_train = X_train.drop(columns, axis=1)\ntemp_X_test = X_test.drop(columns, axis=1)\ntemp_test = test.drop(columns, axis=1)\n\nOH_X_train = pd.concat([temp_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([temp_X_test, OH_cols_test], axis=1)\nOH_test = pd.concat([temp_test, cols_test], axis=1) ","0dc955c4":"counter = Counter(y_train)\nprint('Before :', counter)\nsmt = SMOTEENN(random_state=42)\n\nX_train_sm, y_train_sm = smt.fit_resample(OH_X_train, y_train)\ncounter = Counter(y_train_sm)\nprint('After :', counter)","09201b00":"rf = RandomForestClassifier(n_estimators=80,max_features=6,max_samples=0.1,random_state=42)\nrf.fit(X_train_sm, y_train_sm)\ny_pred = rf.predict(OH_X_test)\nprint(classification_report(y_test, y_pred))\nprint('Roc auc score :',roc_auc_score(y_test, rf.predict_proba(OH_X_test)[:, 1]))","858bd1c8":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = rf.predict_proba(OH_X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\n#plt.figure(figsize=(8,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","b10f746a":"# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train_sm.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen', figsize=(12,8))\nplt.title('Features Importances')\nplt.show()","5d519a70":"gbc = GradientBoostingClassifier(n_estimators=50,learning_rate=0.1,max_features='sqrt',min_samples_leaf=12, random_state=42)\ngbc.fit(X_train_sm, y_train_sm)\ny_pred = gbc.predict(OH_X_test)\nprint(classification_report(y_test, y_pred))\nprint('Roc auc score :',roc_auc_score(y_test, gbc.predict_proba(OH_X_test)[:, 1]))","e8198275":"# Compute predicted probabilities: y_pred_prob\ny_pred_prob = gbc.predict_proba(OH_X_test)[:,1]\n\n# Generate ROC curve values: fpr, tpr, thresholds\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\n#plt.figure(figsize=(8,8))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","5cc6e13a":"# Create a pd.Series of features importances\nimportances = pd.Series(data=gbc.feature_importances_,\n                        index= OH_X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen', figsize=(12,8))\nplt.title('Features Importances')\nplt.show()","ece3de83":"test_answers = np.load('..\/input\/job-change-dataset-answer\/jobchange_test_target_values.npy')","a89874a9":"rf_pred = rf.predict(OH_test.drop(columns=['enrollee_id']))\nprint(classification_report(test_answers, rf_pred))\nprint('Roc auc score :',roc_auc_score(test_answers, rf.predict_proba(OH_test.drop(columns=['enrollee_id']))[:, 1]))","908c400f":"gbc_pred = gbc.predict(OH_test.drop(columns=['enrollee_id']))\nprint(classification_report(test_answers, gbc_pred))\nprint('Roc auc score :',roc_auc_score(test_answers, gbc.predict_proba(OH_test.drop(columns=['enrollee_id']))[:, 1]))","9b035290":"# Encoding categorical variables","b17595a9":"##### Some columns have a significant amount of missing data. Let's check if there is a pattern between these features by visualizing the missing data.","972152ee":"## Task Details\n\nThis dataset designed to understand the factors that lead a person will work for the company(leaving current job) ,and the goal of this task is building model(s) that uses the current credentials,demographics,experience to predict the probability of a candidate looking for a new job or will work for the company.\n\n\n## Note:\n\n* The dataset is imbalanced so it might affect your result if you dont handle it\n* Most features are categorical (Nominal, Ordinal, Binary), some with high cardinality so encoding methods and techniques will help to boost models performance\n* Missing imputation strategy might affect the results so it can be a part of your pipeline as well.","1b525b6e":"# **Predict the probability of a candidate looking for a new job**","6ebb1add":"# Split the data","2d5dbba2":"# Handling missing values","23e40fef":"# Handling imbalanced dataset","e5a46496":"# RandomForestClassifier","780e75ef":"We do the same operations for test data.","f9807e2a":"# Load the data","efb2f123":"# Preprocessing data","68ec1678":"In order to prevent data leakage, encoding categorical variable operations are done after data is separated as train and test data. First of all, the LeaveOneOutEncoder method was used here because the city variable is a high cardinality categorical variable. Then, the OneHotEncoder method was used, as variables such as gender, enrolled_university, major_discipline, company_type were nominal variables.","a4852e3d":"I used the SMOTEENN method to solve the Imbalanced dataset issue. SMOTE  can generate noisy samples by interpolating new points between marginal outliers and inliers. This issue can be solved by cleaning the space resulting from over-sampling. We can do this by using the SMOTEENN method.","ed1105ff":"## Import required packages","7bc0c1a0":"Here we will first start with the training dataset. Many methods can be used when handling missing data. Some of these are the following methods\n\n* Deleting rows. \n* Deleting columns. \n* Predicting missing values. \n* Assign a unique category\n\nWe will use the assign a unique category method here, using the knowledge we have gained from Visual EDA.","63ae34dc":"# Visual EDA","845711a5":"# Visualize the missing data","ae3e3eda":"## Features\n\n* enrollee_id : Unique ID for candidate\n* city: City code\n* city_ development _index : Developement index of the city (scaled)\n* gender: Gender of candidate\n* relevent_experience: Relevant experience of candidate\n* enrolled_university: Type of University course enrolled if any\n* education_level: Education level of candidate\n* major_discipline :Education major discipline of candidate\n* experience: Candidate total experience in years\n* company_size: No of employees in current employer's company\n* company_type : Type of current employer\n* lastnewjob: Difference in years between previous job and current job\n* training_hours: training hours completed\n* target: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change","273b5fa3":"Since the education_level property is an ordinal category, I implemented an appropriate method for it. To make the difference between the categories more obvious, I increased the difference between them numerically. Likewise, in the company_size property, I created the categories by calculating the average values. Apart from these, I made some replacement operation and data type corrections.","9fff991f":"# TEST","f21295f3":"* A value close to -1 means that when one variable appears, the other variable is most likely missing.\n* A value close to 0 means that there is no dependency between the occurrence of missing values of the two variables.\n* A value close to 1 means that when one variable is present, the other variable is very likely to be present.\n\nIt is seen that there is a higher correlation between company_size and company_type variables and between major_discipline and education_level variables compared to other variables.","e6ffc4ab":"## Introduction\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because **it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates**. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\nThis dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials,demographics,experience data we will **predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision**.","3b5758ed":"# GradientBoostingClassifier"}}