{"cell_type":{"dee9d03d":"code","a5a9dd34":"code","1ef7a242":"code","0157719a":"code","8a22a9f4":"code","5504f058":"code","5e809bd3":"code","bb58e28c":"code","e9f31c09":"code","9a771ee7":"markdown","8bb5dbdd":"markdown","7fe0c79b":"markdown","a311df63":"markdown","42e52a38":"markdown","96327bb8":"markdown","00349dd4":"markdown","64ce32ce":"markdown","ba8c64aa":"markdown","2698cbbf":"markdown","d5654c41":"markdown","3e6add29":"markdown","ba43655f":"markdown","392ffb17":"markdown"},"source":{"dee9d03d":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings\nfrom collections import Counter\n\nimport warnings\n#warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 100)\npd.set_option('display.max_rows', 50)\npd.set_option('display.width', 5000)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.float_format', lambda x: '%.4f' % x)\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nimport plotly\nimport plotly.offline as pyo\nimport plotly.express as px\nimport plotly.graph_objs as go\npyo.init_notebook_mode()\nimport plotly.figure_factory as ff\nimport missingno as msno\n\n# Generating Random Floating Point Values\nfrom numpy.random import seed\nimport random\nfrom random import randrange\n\n# System Operations\nimport sys\nimport os\n\npd.options.mode.chained_assignment = None\n\n# Data Pre-processing Libraries\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Modelling Libraries\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, \\\n    AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor, StackingRegressor\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV,RepeatedStratifiedKFold","a5a9dd34":"df = pd.read_csv('\/kaggle\/input\/hitters\/Hitters.csv')\ndf.head()","1ef7a242":"def eda_version():\n\n    def grab_col_names(dataframe, cat_th=10, car_th=20):\n        cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n        num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                       dataframe[col].dtypes != \"O\"]\n        cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                       dataframe[col].dtypes == \"O\"]\n        cat_cols = cat_cols + num_but_cat\n        cat_cols = [col for col in cat_cols if col not in cat_but_car]\n        num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n        num_cols = [col for col in num_cols if col not in num_but_cat]\n        return cat_cols, num_cols, cat_but_car\n\n    def replace_with_thresholds(dataframe, column, q1=0.20, q3=0.80):\n        quartile1 = dataframe[column].quantile(q1)\n        quartile3 = dataframe[column].quantile(q3)\n        interquantile_range = quartile3 - quartile1\n        up_limit = quartile3 + 1.5 * interquantile_range\n        low_limit = quartile1 - 1.5 * interquantile_range\n        dataframe.loc[(dataframe[column] < low_limit), column] = low_limit\n        dataframe.loc[(dataframe[column] > up_limit), column] = up_limit\n\n    def label_encoder(dataframe, binary_col):\n        labelencoder = LabelEncoder()\n        dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n        return dataframe\n\n    global df\n    df = df[(df['Salary'] < 1500) & (df['Salary'].notnull())]\n    #from 322 to 251 rows\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    for col in num_cols:\n        replace_with_thresholds(df, col)\n    df.loc[:, 'new1'] = 0\n    df.loc[(df['CHmRun'] > 100) & (df['AtBat'] > 400), 'new1'] = 1\n    df.loc[:, 'new2'] = 0\n    df.loc[(df['Years'] < 4), 'new2'] = 1\n    df.loc[:, 'new3'] = 0\n    df.loc[(df['CHmRun'] < 25) & (df['Walks'] < 40), 'new3'] = 1\n    df.loc[:, 'new4'] = 0\n    df.loc[(df['AtBat'] > 450) & (df['Walks'] > 70), 'new4'] = 1\n    df.loc[:, 'new5'] = 0\n    df.loc[(df['RBI'] < 50) & (df['CHmRun'] < 25), 'new5'] = 1\n    df['new6'] = df['AtBat'] * df['RBI']\n    df['new7'] = df['Walks'] * df['Years']\n    df['new8'] = df['AtBat'] \/ df['Hits']\n    df['new9'] = df['AtBat'] \/ df['Runs']\n    df['new10'] = df['Hits'] \/ df['Runs']\n    df['new11'] = df['HmRun'] \/ df['RBI']\n    df['new12'] = df['Runs'] \/ df['RBI']\n    df['new13'] = df['Runs'] \/ df['RBI']\n    df['new14'] = df['Years'] \/ df['CAtBat']\n    df['new15'] = df['Years'] \/ df['CHits']\n    df['new16'] = df['Years'] \/ df['CHmRun']\n    df['new17'] = df['Years'] \/ df['CRuns']\n    df['new18'] = df['Years'] \/ df['CRBI']\n    df['new19'] = df['CAtBat'] \/ df['CHits']\n    df['new20'] = df['CAtBat'] \/ df['CRuns']\n    df['new21'] = df['CAtBat'] \/ df['CRBI']\n    df['new22'] = df['CAtBat'] \/ df['CWalks']\n    df['new23'] = df['CHits'] \/ df['CRuns']\n    df['new24'] = df['CHits'] \/ df['CRBI']\n    df['new25'] = df['CHits'] \/ df['CWalks']\n    df['new26'] = df['CHmRun'] \/ df['CRuns']\n    df['new27'] = df['CHmRun'] \/ df['CRBI']\n    df['new28'] = df['CHmRun'] \/ df['CWalks']\n    df['new29'] = df['CRuns'] \/ df['CRBI']\n    df['new30'] = df['CRuns'] \/ df['CWalks']\n    df['new31'] = df['CHmRun'] \/ df['CRBI']\n    df.replace([np.inf, -np.inf], 0, inplace=True)\n\n    for col in ['League', 'Division', 'NewLeague']:\n        df = label_encoder(df, col)\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n    num_cols.remove('Salary')\n    for col in num_cols:\n        transformer = RobustScaler().fit(df[[col]])\n        df[col] = transformer.transform(df[[col]])\n\n    global X,y\n    y = df[\"Salary\"]\n    X = df.drop([\"Salary\"], axis=1)\n\n    global X_train, X_test, y_train, y_test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)\n\n    print(\"EDA Finished.\")\n","0157719a":"eda_version()","8a22a9f4":"def run_single_model():\n    ######################################################\n    # Base Models\n    ######################################################\n\n    models = [('LR', LinearRegression()),\n              (\"Ridge\", Ridge()),\n              (\"Lasso\", Lasso()),\n              (\"ElasticNet\", ElasticNet()),\n              ('KNN', KNeighborsRegressor()),\n              ('CART', DecisionTreeRegressor()),\n              ('RF', RandomForestRegressor()),\n              ('SVR', SVR()),\n              ('GBM', GradientBoostingRegressor()),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n              (\"LightGBM\", LGBMRegressor()),\n              (\"CatBoost\", CatBoostRegressor(verbose=False))\n              , (\"AdaBoost\", AdaBoostRegressor())\n              , (\"Bagging\", BaggingRegressor())\n              , (\"ExtraTrees\", ExtraTreesRegressor())\n              , (\"HistGradient\", HistGradientBoostingRegressor())]\n\n    global output_df\n    output_df = pd.DataFrame(models, columns=[\"MODEL_NAME\", \"MODEL_BASE\"])\n    output_df.drop('MODEL_BASE',axis=1,inplace=True)\n    for name, regressor in models:\n        rmse_cv = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE: {round(rmse_cv, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_BASE_CV_ALL\"] = rmse_cv\n\n        regressor.fit(X_train, y_train)\n        # Train Hatas\u0131\n        y_pred_train = regressor.predict(X_train)\n        rmse_train = np.sqrt(mean_squared_error(y_pred_train, y_train))\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_BASE_TRAIN\"] = rmse_train\n\n        # Test Hatas\u0131\n        y_pred_test = regressor.predict(X_test)\n        rmse_test = np.sqrt(mean_squared_error(y_pred_test, y_test))\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_BASE_TEST\"] = rmse_test\n\n\n\n    ######################################################\n    # Automated Hyperparameter Optimization\n    ######################################################\n\n    cart_params = {'max_depth': range(1, 20),\n                   \"min_samples_split\": range(2, 30)}\n\n    rf_params = {\"max_depth\": [5, 15, 20, None],\n                 \"max_features\": [5, 7, 9,  \"auto\"],\n                 \"min_samples_split\": [6, 8, 15],\n                 \"n_estimators\": [150, 200, 300]}\n\n    xgboost_params = {\"learning_rate\": [0.15], #0.01, 0.05, 0.1,\n                      \"max_depth\": [5], #3, 5, 8\n                      \"n_estimators\": [200], # 100, 200, 300\n                      \"colsample_bytree\": [0.5] #0.3, 0.5, 0.8\n                     }\n\n    lightgbm_params = {\"learning_rate\": [0.001, 0.01, 0.1],\n                       \"n_estimators\": [100, 300, 500],\n                       \"colsample_bytree\": [0.1, 0.3, 0.7, 1]}\n\n    extraTrees_params = {\n                        'n_estimators': [10, 50, 100],\n                        'max_depth': [2, 16, 50],\n                        'min_samples_split': [2, 6],\n                        'min_samples_leaf': [1, 2],\n                        'max_features': ['auto', 'sqrt', 'log2'],\n                        'bootstrap': [True, False],\n                        'warm_start': [True, False],\n                        }\n\n    HistGradient_params = {\"learning_rate\": [0.01, 0.05],\n                       \"max_iter\": [20, 100],\n                       \"max_depth\": [None, 25],\n                       \"l2_regularization\": [0.0, 1.5],\n                           }\n\n    regressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n                  (\"RF\", RandomForestRegressor(), rf_params),\n                  ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n                  ('LightGBM', LGBMRegressor(), lightgbm_params),\n                  ('ExtraTrees', ExtraTreesRegressor(), extraTrees_params),\n                  ('HistGradient', HistGradientBoostingRegressor(), HistGradient_params)\n                  ]\n    global best_models\n    best_models = {}\n\n    for name, regressor, params in regressors:\n        print(f\"########## {name} ##########\")\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_BASE_CV_ALL_Before_Tune\"] = rmse\n\n\n        gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X_train, y_train)\n        final_model = regressor.set_params(**gs_best.best_params_)\n        final_model.fit(X_train, y_train)\n\n        #Train evaluation\n        y_pred_train = final_model.predict(X_train)\n        rmse = np.sqrt(mean_squared_error(y_pred_train, y_train))\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_TUNED_CV_TRAIN\"] = rmse\n\n        #Test evaluation\n        y_pred_test = regressor.predict(X_test)\n        rmse = np.sqrt(mean_squared_error(y_pred_test, y_test))\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_TUNED_CV_TEST\"] = rmse\n\n        #CV TO GS\n        gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = regressor.set_params(**gs_best.best_params_)\n        rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"RMSE_TUNED_CV_ALL\"] = rmse\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        output_df.loc[output_df['MODEL_NAME'] == name, \"BEST_PARAMS\"] = str(gs_best.best_params_)\n\n        best_models[name] = final_model","5504f058":"run_single_model()","5e809bd3":"def run_multiple_model():\n    ######################################################\n    # # Stacking & Ensemble Learning\n    ######################################################\n    best_models\n\n    voting_reg_RF_LGBM = VotingRegressor(estimators=[('RF', best_models[\"RF\"]), ('LightGBM', best_models[\"LightGBM\"])])\n    voting_reg_RF_LGBM.fit(X, y)\n    voting_reg_rmse = np.mean(np.sqrt(-cross_val_score(voting_reg_RF_LGBM, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (voting_reg): {round(voting_reg_rmse, 4)} \")\n    global output_df\n    output_df = output_df.append({'MODEL_NAME': \"voting_reg_RF_LGBM\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"voting_reg_RF_LGBM\", \"RMSE_TUNED_CV_ALL\"] = voting_reg_rmse\n\n\n    estimators = [('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"])]\n    stacking_reg = StackingRegressor(estimators=estimators, final_estimator=best_models[\"LightGBM\"])\n    stacking_reg.fit(X, y)\n    stacking_reg_rmse = np.mean(np.sqrt(-cross_val_score(stacking_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (stacking_reg): {round(stacking_reg_rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"stacking_reg_rmse\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"stacking_reg_rmse\", \"RMSE_TUNED_CV_ALL\"] = stacking_reg_rmse\n\n\n    voting_reg_RF_XG_EXTRA = VotingRegressor(estimators=[('RF', best_models[\"RF\"]), ('XGBoost', best_models[\"XGBoost\"]), ('ExtraTrees', best_models[\"ExtraTrees\"])])\n    voting_reg_RF_XG_EXTRA.fit(X, y)\n    voting_reg_rmse = np.mean(np.sqrt(-cross_val_score(voting_reg_RF_XG_EXTRA, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (voting_reg_RF_XG_EXTRA): {round(voting_reg_rmse, 4)} \")\n    output_df = output_df.append({'MODEL_NAME': \"voting_reg_RF_XG_EXTRA\"}, ignore_index=True)\n    output_df.loc[output_df['MODEL_NAME'] == \"voting_reg_RF_XG_EXTRA\", \"RMSE_TUNED_CV_ALL\"] = voting_reg_rmse","bb58e28c":"run_multiple_model()","e9f31c09":"output_df.sort_values(['RMSE_TUNED_CV_ALL'])","9a771ee7":"![8-4-2021%202-20-32%20PM_Moment.jpg](attachment:8-4-2021%202-20-32%20PM_Moment.jpg)","8bb5dbdd":"Major League Baseball is one of the most watched sports, especially in America. In recent years, the performance of a player and his team is an important parameter that determines the salary. It is therefore important to determine whether a player's performance really matches his worth. Therefore, how to evaluate the salaries of players has always been a hot topic. In addition to the statistical performance of the players on the field, some new variables have also been proposed that may affect the salary of the players. It is well known that there are many studies on the salary of major league baseball, and there are many reasons for the impact of salary. In order to accurately determine the annual salary increase of the players, a classification prediction model is made using some machine learning methods. The results of these studies are useful to increase the accuracy of the model by creating new variables.\n    \n    \n* Problem\n    \nThis dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n\n* Feature Description\n\n    * A data frame with 322 observations of major league players on the following 20 variables.\n    * AtBat Number of times at bat in 1986\n    * Hits Number of hits in 1986\n    * HmRun Number of home runs in 1986\n    * Runs Number of runs in 1986\n    * RBI Number of runs batted in in 1986\n    * Walks Number of walks in 1986\n    * Years Number of years in the major leagues\n    * CAtBat Number of times at bat during his career\n    * CHits Number of hits during his career\n    * CHmRun Number of home runs during his career\n    * CRuns Number of runs during his career\n    * CRBI Number of runs batted in during his career\n    * CWalks Number of walks during his career\n    * League A factor with levels A and N indicating player\u2019s league at the end of 1986\n    * Division A factor with levels E and W indicating player\u2019s division at the end of 1986\n    * PutOuts Number of put outs in 1986\n    * Assists Number of assists in 1986\n    * Errors Number of errors in 1986\n    * Salary 1987 annual salary on opening day in thousands of dollars\n    * NewLeague A factor with levels A and N indicating player\u2019s league at the beginning of 1987\n\n\n* Purpose\n\nYou are expected to estimate these values. Carrying out a machine learning project that predicts hitters salary with minimum error on the data set we have.\n","7fe0c79b":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">3. Machine Learning Model<\/h1>","a311df63":"* At Hitters data we predict salary of the players. Using different models and algorithms and with feature engineering etc we try to build our model. As usual we start with EDA. I have tried many different models from sklearn, ligthgbm, catboost, xgboost. I have applied Voting and Stacking Models to improve the model predictions.Many can be done on top of it. We may select Voting Regressor with RandomForest and XGBoost and ExtraTrees for deployment.","42e52a38":"* The data used in the experiment will be processed using a combination of preprocessing methods to improve the prediction accuracy. Also, some factors will be added to the local dataset to examine the relationship between these factors and the selling price in Iowa.","96327bb8":"* Feature selection helps to interpret data, reduce computational requirements, and reduce the impact of the curse of dimensionality, thereby improving the performance of the prediction model.\n\n* Many feature selection methods are available. The main purpose of feature selection is to select the most suitable features as input variables to reduce the number of features and improve the prediction accuracy of machine learning. In this study, it was investigated whether feature selection has an effect on model prediction performance.","00349dd4":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">5. References<\/h1>","64ce32ce":"![8-4-2021%202-54-27%20PM.jpg](attachment:8-4-2021%202-54-27%20PM.jpg)","ba8c64aa":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">2. Explatory Data Analysis and Feature Engineering<\/h1>","2698cbbf":"<a id=\"toc\"><\/a>\n\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h3>\n    \n* [1. Introduction](#0) \n\n* [2. Explatory Data Analysis and Feature Engineering](#0) \n    * [Feature Enginnering](#0) \n    * [Producing New Columns for Feature Engineering](#0) \n    * [Feature Extractions](#0) \n    * [Outliers](#0) \n    * [Missing Values](#0) \n        \n* [3. Machine Learning Model](#0) \n    * [Modeling](#0) \n    * [Hyper Parameters Tunning](#0)\n\n* [4. Conclusions](#0) \n* [5. References](#0) \n","d5654c41":"* Any observations which had missing values were also removed from the dataset. Below are a few feature engineering processes which were done  to cleanse the dataset:\n","3e6add29":"* [[1] Huang, M.L. and Li, Y.Z., 2021. Use of Machine Learning and Deep Learning to Predict the Outcomes of Major League Baseball Matches. Applied Sciences, 11(10), p.4499.](#0)\n* [[2] https:\/\/www.baseball-reference.com\/](#0)\n* [[3] Bierig, B., Hollenbeck, J. and Stroud, A., 2017. Understanding career progression in baseball through machine learning. arXiv preprint arXiv:1712.05754.](#0)\n* [[4] Alfiyatin, A.N., Febrita, R.E., Taufiq, H. and Mahmudy, W.F., 2017. Modeling house price prediction using regression analysis and particle swarm optimization. International Journal of Advanced Computer Science and Applications, 8(10), pp.323-326.](#0)\n* [[5] Truong, Q., Nguyen, M., Dang, H. and Mei, B., 2020. Housing price prediction via improved machine learning techniques. Procedia Computer Science, 174, pp.433-442.](#0)\n","ba43655f":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">4. Conclusions<\/h1>","392ffb17":"<h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">1. Introduction<\/h1>"}}