{"cell_type":{"d94ab799":"code","525d8660":"code","e790896a":"code","9258445a":"code","528859f4":"code","0503a3b8":"code","97ef01a6":"code","bdc307bb":"code","5fb1e968":"code","b1603d13":"code","dc0449ba":"code","85e34e6b":"code","2870391e":"code","7173b945":"code","ab321496":"code","b0ade5c7":"code","b8d2b8a3":"code","7128cfbf":"code","c51f59e7":"code","301c803e":"code","fc9de587":"code","6be83295":"code","35676a71":"code","e8b71bad":"code","e1c919d3":"code","2767e822":"code","f9afa754":"code","eb71c42a":"code","9dff0980":"code","5a6ee2ce":"code","eda65c80":"code","f09a0638":"code","e6e19784":"code","5566d4cd":"code","e6b17e9e":"code","dbdf22e7":"code","da36a202":"code","aeec3089":"code","abd02bda":"code","e377d009":"code","1c75773b":"code","ca0e1a94":"code","36f5b492":"code","4f34ac9d":"code","295610ec":"markdown","9c5db4bc":"markdown","19de35d2":"markdown","c6722683":"markdown","cf029a21":"markdown","838cce13":"markdown","df54311e":"markdown","269f754c":"markdown","ddfd85e7":"markdown","3a6271d8":"markdown","3df71bf2":"markdown","534f7c8c":"markdown","ed662517":"markdown","c7869d92":"markdown","91d3a36f":"markdown","56096d25":"markdown","f0d9aea5":"markdown"},"source":{"d94ab799":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\npd.set_option(\"display.max_rows\", None)\npd.set_option(\"display.max_columns\", None)","525d8660":"data_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')","e790896a":"df_train = data_train.copy()\ndf_test = data_test.copy()","9258445a":"df_train.head()","528859f4":"def describe_columns(df):\n    desc = {'Columns':[],\n            'Type':[],\n           'NaN Count':[],\n           'NaN Frequency':[],\n           'Number of unique values':[]}\n    for col in df.columns:\n        desc['Columns'].append(col)\n        desc['Type'].append(df[col].dtype)\n        desc['NaN Count'].append(df[col].isna().sum())\n        desc['NaN Frequency'].append(df[col].isna().mean())\n        \n        lenght = len(df[col].unique())\n        \n        if desc['NaN Count'][-1] > 0:\n            lenght -= 1\n        desc['Number of unique values'].append(lenght)\n        \n    return pd.DataFrame(desc).set_index('Columns')","0503a3b8":"def move_column(df, column_name, column_place):\n    rearranged_columns = list(df.columns)\n    rearranged_columns.remove(column_name)\n    rearranged_columns = rearranged_columns[:column_place] + [column_name] + rearranged_columns[column_place:]\n    return df[rearranged_columns]","97ef01a6":"describe_columns(df_train)","bdc307bb":"describe_columns(df_test)","5fb1e968":"df_train['Survived'].value_counts(normalize=True)","b1603d13":"sns.pairplot(df_train, hue='Survived')","dc0449ba":"class Preprocessing:\n    \n    def __init__(self):\n        pass\n    \n    def fit(self, df):\n        df = df.drop(['Cabin','Ticket', 'Name'], axis=1)\n        desc_df = describe_columns(df)\n        self.categ_columns = list(desc_df[desc_df['Number of unique values'] < 4].index)\n        self.num_columns = list(desc_df[desc_df['Number of unique values'] >= 4].index)\n        \n        self.dict_most_common = {}\n        self.dict_median = {}\n        \n        for col in self.categ_columns:\n            self.dict_most_common[col] = df[col].value_counts().index[0]\n        for col in self.num_columns:\n            self.dict_median[col] = df[col].median()\n       \n    def transform(self, df):\n        df = df.drop(['Cabin','Ticket', 'Name'], axis=1)\n        \n        for col in df.columns:\n            if col in self.categ_columns:\n                df[col] = df[col].fillna(self.dict_most_common[col])\n            else:\n                df[col] = df[col].fillna(self.dict_median[col])\n        df['Number Related'] = df['SibSp'] + df['Parch']\n        df = move_column(df, 'Number Related', list(df.columns).index('Parch')+1)\n        \n        for col in self.categ_columns:\n            if col == 'Sex':\n                continue\n            liste = list(df[col].unique())\n            liste.sort(reverse=True)\n            for classe in liste:\n                df[f'{col}_{classe}'] = (df[col] == classe).astype('float')\n                df = move_column(df, f'{col}_{classe}', list(df.columns).index(col)+1) \n                \n            del df[col]\n        \n            \n        df['Sex'] = df['Sex'].map({'male':0, 'female':1})\n        df['Fare'] = df['Fare'].replace({0.0:0.1})\n        df['Fare'] = np.log(df['Fare'])\n        self.encoded_columns = list(df.columns)\n        return df\n    \n    def fit_transform(self, df):\n        self.fit(df)\n        return self.transform(df)","85e34e6b":"prep_train = Preprocessing()\nprep_train.fit(df_train.drop(['Survived', 'PassengerId'], axis=1))","2870391e":"df_train_encoded = prep_train.transform(df_train.drop(['Survived', 'PassengerId'], axis=1))\ndf_train_encoded['Survived'] = df_train['Survived']\ndf_train_encoded['PassengerId'] = df_train['PassengerId']\ndf_train_encoded = move_column(df_train_encoded, 'Survived', 0)\ndf_train_encoded = move_column(df_train_encoded, 'PassengerId', 0)","7173b945":"df_train_encoded.head()","ab321496":"plt.figure(figsize=(20,10))\nsns.heatmap(df_train_encoded.corr(), annot=True)","b0ade5c7":"class Scale:\n    \n    def __init__(self, q1=25, q2=75):\n        self.q1 = q1\n        self.q2 = q2\n    \n    def fit(self, X):\n        self.dict_perc = {}\n        for i in range(X.shape[1]):\n            if len(pd.Series(X[:,i]).unique()) != 2:\n                perc1 = np.percentile(X[:,i], self.q1)\n                perc2 = np.percentile(X[:,i], self.q2)\n                \n                if perc1 == perc2:\n                    perc1 = X[:,i].min()\n                    perc2 = X[:,i].max()\n                    \n                self.dict_perc[i] = (perc1, perc2)\n                \n    def transform(self, X):\n        for i, value in self.dict_perc.items():\n            X[:,i] = (X[:,i] - value[0])\/(value[1] - value[0])\n            \n        return X","b8d2b8a3":"from sklearn.model_selection import train_test_split","7128cfbf":"df_tr, df_te = train_test_split(df_train, test_size=0.2, random_state=0)","c51f59e7":"X_train = df_tr.drop(['Survived', 'PassengerId'], axis=1)\nX_test = df_te.drop(['Survived', 'PassengerId'], axis=1)\ny_train = df_tr['Survived'].values\ny_test = df_te['Survived'].values","301c803e":"prep = Preprocessing()\nprep.fit(X_train)","fc9de587":"X_train = prep.transform(X_train).values\nX_test = prep.transform(X_test).values","6be83295":"scaler = Scale(q1=10, q2=90)\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","35676a71":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report","e8b71bad":"params_grid = {'n_neighbors':range(1,21),\n              'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n              'weights':['uniform', 'distance'],\n              'leaf_size':[20,30,40]}\nclf1 = GridSearchCV(KNeighborsClassifier(), params_grid, cv=3, scoring='f1')","e1c919d3":"clf1.fit(X_train_scaled, y_train)","2767e822":"model = KNeighborsClassifier(**clf1.best_params_)\nmodel.fit(X_train_scaled, y_train)","f9afa754":"y_pred_test = model.predict(X_test)\ny_pred_train = model.predict(X_train)","eb71c42a":"print('Train Resutlts:')\nsns.heatmap(confusion_matrix(y_test, y_pred_test), annot=True)","9dff0980":"print('Test Resutlts:')\nsns.heatmap(confusion_matrix(y_train, y_pred_train), annot=True)","5a6ee2ce":"print(classification_report(y_test, y_pred_test))","eda65c80":"print('Score Train: ' + str(model.score(X_train_scaled, y_train)))\nprint('Score Test: ' + str(model.score(X_test_scaled, y_test)))","f09a0638":"from sklearn.ensemble import RandomForestClassifier","e6e19784":"params_grid = {'n_estimators':[50, 100, 150],\n              'min_samples_split':[2, 3, 4, 5],\n              'criterion':['gini', 'entropy'],\n              'random_state':[0]}\nclf2 = GridSearchCV(RandomForestClassifier(), params_grid, cv=3, scoring='f1')","5566d4cd":"clf2.fit(X_train_scaled, y_train)","e6b17e9e":"model2 = RandomForestClassifier(**clf2.best_params_)\nmodel2.fit(X_train_scaled, y_train)","dbdf22e7":"y_pred_test2 = model2.predict(X_test)\ny_pred_train2 = model2.predict(X_train)","da36a202":"print('Train Resutlts:')\nsns.heatmap(confusion_matrix(y_train, y_pred_train2), annot=True)","aeec3089":"print('Test Resutlts:')\nsns.heatmap(confusion_matrix(y_test, y_pred_test2), annot=True)","abd02bda":"print(classification_report(y_test, y_pred_test2))","e377d009":"print('Score Train: ' + str(model2.score(X_train_scaled, y_train)))\nprint('Score Test: ' + str(model2.score(X_test_scaled, y_test)))","1c75773b":"X_final_test = prep.transform(df_test.drop('PassengerId', axis=1)).values","ca0e1a94":"X_final_test_scaled = scaler.transform(X_final_test)","36f5b492":"df_submission = pd.DataFrame({'PassengerId':df_test['PassengerId'],\n                'Survived':model2.predict(X_final_test_scaled)})\ndf_submission.head(10)","4f34ac9d":"#df_submission.to_csv('rf_submission.csv', index=False)","295610ec":"### Pairplot","9c5db4bc":"## Training process","19de35d2":"## Exploration","c6722683":"## Modeling with RandomForestClassifier","cf029a21":"### Results for KNeighborsClassifier","838cce13":"## Modeling with KNeighborsClassifier","df54311e":"#### Exemple of use for the train set:","269f754c":"### Preprocessing and feature engineering class\n\nCreating a preprocessing class to keep the same changes for all datasets\n\n**Steps:**\n\n- Deleting 'Cabin','Ticket', 'Name' columns.\n- Distinguish numerical and categorical columns. Categorical columns are all 'object' columns and columns that have less than 4 unique values.\n- Categorical columns will be encoded like a OneHotEncoder (excepting 'Sex' will become 0 and 1)\n- 'Fare'  ->  log('Fare')\n- New column 'Number Related' which is 'SibSp' + 'Parch'","ddfd85e7":"### Splitting the train set","3a6271d8":"Will change the 'Fare' column with log in the preprocessing","3df71bf2":"### Robust Scaler custom class\n\nVery similar class than the RobustScaler but work only for non-binary columns.\nThere is also a MinMaxScaler backup if perc1 and perc2 are equals.","534f7c8c":"## Submission","ed662517":"### Correlation Matrix","c7869d92":"# Titanic - Machine Learning from Disaster\n\nNotebook containing very simple analyzes and models to predict the survival of the titanic disaster using a custom preprocessing class.\n\n## Importations","91d3a36f":"## Data importations","56096d25":"### Preprocessing and Scaling datas","f0d9aea5":"### Results for RandomForestClassifier"}}