{"cell_type":{"5b7bf161":"code","b88f2ac1":"code","0b5ed5ba":"code","59e9d4cb":"code","bae3afdc":"code","0d01aa4a":"code","6a4f8268":"code","bb836128":"code","7ae621f5":"code","2fc193a5":"code","aa7f8f5f":"code","8964bcb3":"code","f23c4af0":"code","0dcdc8c8":"code","03acb397":"code","13197ff3":"code","ea50bdef":"code","334275c1":"code","ff6bf13a":"code","c1fb744d":"code","48eb3eb1":"code","dad43864":"code","64d6795a":"code","d49c353f":"code","350c25ce":"code","38f6de3a":"code","4873eefa":"code","da0545a1":"code","d86f7333":"code","4883767a":"code","4834886d":"code","032bde71":"code","8e5844f3":"code","b184800c":"markdown","271e0695":"markdown","d7ad429e":"markdown","4392e0b3":"markdown","51c49381":"markdown","bd90797a":"markdown","532d2a82":"markdown","4a8be2c0":"markdown","0403502a":"markdown","6c35da0e":"markdown","7e743845":"markdown","3453a27c":"markdown","94e22a89":"markdown","ced7455b":"markdown","93363461":"markdown","d04b6919":"markdown","de9a7b13":"markdown","e5f1a893":"markdown","a7cd6cf4":"markdown","b537e45c":"markdown","595dd9ab":"markdown","f36de86f":"markdown","a8423250":"markdown","61996194":"markdown","fee145fc":"markdown","4010a902":"markdown","c32ac831":"markdown","f929bd6c":"markdown"},"source":{"5b7bf161":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b88f2ac1":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.base import TransformerMixin, BaseEstimator\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nimport seaborn as sns\n!pip install miceforest\nimport miceforest as mf","0b5ed5ba":"df_train = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')\nprint(\"Train has {} rows\" .format(len(df_train)))\nprint(\"Test has {} rows\" .format(len(df_test)))","59e9d4cb":"from IPython.display import display\npd.options.display.max_columns = None\ndisplay(df_train.head(5))\ndisplay(df_test.head(5))","bae3afdc":"def drop_unnecessary_columns(df, column_name):\n    \"\"\"\n    Function to delete the list of columns \n    Parameters\n    ----------\n    df : dataframe\n            pass in full dataframe\n    column_name : list\n            pass in list of full column\n    ----------\n    Returns: Dataframe\n    \"\"\"\n    \n    df = df.drop(column_name, axis=1)\n    return df\n\ndef print_unique_values(df):\n    \"\"\"\n    Function to print unique values in categorical datatypes \n    Parameters\n    ----------\n    df : dataframe\n            pass in full dataframe\n    ----------\n    Returns: None\n    \"\"\"\n    \n    print(\"unique values\\n\")\n    for col in df.columns:\n        if df[col].dtypes=='object':\n            if len(df[col].unique())>5:\n                print('{:>15s} \\t more than 5 unique'.format(col))\n            else:\n                print('{:>15s} \\t {}'.format(col,df[col].unique() ))\n\ndef split_categ_numer(df):\n    \"\"\"\n    Function to split dataframe into two, one having categorical columns and another having numerical columns\n    Parameters\n    ----------\n    df : dataframe\n            pass in full dataframe\n    ----------\n    Returns: \n        dataframe with categorical columns\n        dataframe with numerical columns\n    \"\"\"\n    categorical_col = []\n    numerical_col = []\n    for c in df.columns:\n        if df[c].dtype =='object':\n            categorical_col.append(c)\n        else:\n            numerical_col.append(c)\n    return df[categorical_col], df[numerical_col]\n","0d01aa4a":"print(\"number of duplicate records in train - {}\".format(df_train.duplicated().sum()))\nprint(\"number of duplicate records in test - {}\".format(df_test.duplicated().sum()))","6a4f8268":"df_train.isnull().sum()","bb836128":"df_test.isnull().sum()","7ae621f5":"df_train.info()","2fc193a5":"print_unique_values(df_train)","aa7f8f5f":"approved = len(df_train[df_train['Loan_Status']=='Y'])\nrejected = len(df_train[df_train['Loan_Status']=='N'])\nprint(\"proportion of 'No' vs 'Yes' {:>3.2f}%\".format(rejected\/approved*100))","8964bcb3":"# preparing the train set\ndf_train = drop_unnecessary_columns(df_train, ['Loan_ID'])\n# # preparing the test set\nx_test = drop_unnecessary_columns(df_test, ['Loan_ID'])","f23c4af0":"x = df_train.drop(['Loan_Status'], axis=1)\ny = df_train['Loan_Status']","0dcdc8c8":"df_train.head(2)","03acb397":"sns.countplot(x='Loan_Status', data=df_train)","13197ff3":"sns.countplot(x='Loan_Status', data=df_train, hue='Gender')","ea50bdef":"sns.countplot(x='Loan_Status', data=df_train, hue='Education')","334275c1":"sns.boxplot(x='Gender', y='LoanAmount', data=df_train)","ff6bf13a":"fg = sns.FacetGrid(df_train, col='Gender')\nfg.map(sns.barplot, 'Loan_Status', 'LoanAmount' )","c1fb744d":"fg = sns.FacetGrid(df_train, col='Education')\nfg.map(sns.barplot, 'Loan_Status', 'LoanAmount' )","48eb3eb1":"categorical_transformer = Pipeline(steps = [('simple_imputer',SimpleImputer(strategy='most_frequent')),\n                                            ('one_hot_encodr', OneHotEncoder(sparse=False))\n                                           ])\n\nnumerical_transformer = Pipeline(steps = [('iterative_imputer', IterativeImputer())])\n\n","dad43864":"# creating categorical train dataset and numerical train dataset\ncat_train_df, numeri_train_df = split_categ_numer(x)\n\n# extracting the categroical column names and numerical column names\ncat_train_features = cat_train_df.columns\nnum_train_features = numeri_train_df.columns","64d6795a":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_train_features),\n        ('cat', categorical_transformer, cat_train_features)\n        ])","d49c353f":"x_train,x_test, y_train, y_test = train_test_split(x,y, train_size=70, random_state=42)\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ny_train = pd.Series(lb.fit_transform(y_train))\ny_test = pd.Series(lb.transform(y_test))","350c25ce":"rf = Pipeline(steps = [('preprocessor',preprocessor),\n                      ('classifier', RandomForestClassifier() )])","38f6de3a":"rf.fit(x_train, y_train)\ny_pred = rf.predict(x_test)\nprint('Train set score : ', rf.score(x_train, y_train))\nprint('Test set score : ', accuracy_score(y_test,y_pred))","4873eefa":"classifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\n\nscores= []\nfor classifier in classifiers:\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('classifier', classifier)])\n    pipe.fit(x_train, y_train)   \n    scores.append(pipe.score(x_test, y_test))","da0545a1":"ml_model = ['knn','decision tree', 'random forest', 'ada boost', 'gradient boost']\ndf_x = pd.DataFrame(list(zip(ml_model,scores)), columns=['models','scores'])\nprint(df_x)\nsns.barplot(x='models', y='scores', data=df_x.sort_values(by='scores'), )\nplt.xticks(rotation=90)\nplt.show()","d86f7333":"classifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier()\n    ]\n\nknn_param_grid = {\n                'classifier__n_neighbors':[2,3,4,5]\n                }\n\ndt_param_grid = { \n    'classifier__max_features': ['sqrt', 'log2'],\n    'classifier__max_depth' : [4,5,6,7,8],\n        }\n\nrf_param_grid = { \n    'classifier__n_estimators': [200, 500],\n    'classifier__max_features': ['auto', 'sqrt', 'log2'],\n    'classifier__max_depth' : [4,5,6,7,8],\n    'classifier__criterion' :['gini', 'entropy']\n    }\n\n\nada_param_grid = {\n         'classifier__n_estimators':[200, 500]#,\n        }\n\ngbc_param_grid = {\n              \"classifier__learning_rate\": [0.1,0.01,0.001]\n        }\n\ngrids = [knn_param_grid, dt_param_grid, rf_param_grid, ada_param_grid, gbc_param_grid]\nscores = []\nbest_params= []\nfor i, model in enumerate(classifiers):\n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('classifier', model)])\n    CV = GridSearchCV(pipe, grids[i], n_jobs= 1)\n    CV.fit(x_train, y_train)    \n    best_params.append(CV.best_params_)\n    scores.append(CV.best_score_)","4883767a":"ml_model = ['knn','decision tree', 'random forest', 'ada boost', 'gradient boost']\ndf_x = pd.DataFrame(list(zip(ml_model,scores)), columns=['models','scores'])\nprint(df_x)\nsns.barplot(x='models', y='scores', data=df_x.sort_values(by='scores'))\nplt.xticks(rotation=90)\nplt.show()","4834886d":"random_forest_param = best_params[3]\nprint(random_forest_param)","032bde71":"rf = Pipeline(steps = [('preprocessor',preprocessor),\n                      ('classifier', RandomForestClassifier(n_estimators= 200))])\nrf.fit(x, y)\ny_pred = rf.predict(df_test)","8e5844f3":"y_pred_series = pd.Series(y_pred)\nd = pd.concat([df_test, pd.DataFrame(y_pred_series)], axis=1)\nd","b184800c":"* The data set is balanced with almost 45% of \"N\" values compared to 55% of \"Y\" values","271e0695":"* Graduates tend to ask more loan than non graduates","d7ad429e":"## Modelling ","4392e0b3":"* Here we find that the Gradient boosting is performing better with accuracy of 73.71%","51c49381":"* By experience, we can say that the loan_id is not an useful attribute for our classification problem\n* hence we remove the \"loan_id\" from both train and test ","bd90797a":"* The loan acceptance and rejection rate is balanced\n* The dataset is balanced dataset so we don't need undersampling or oversampling","532d2a82":"## Preliminary data analysis","4a8be2c0":"> Defining pipelines","0403502a":"* Male applicants are obiously more than female applicants","6c35da0e":"* we take random forest as our baseline model,\n* The accuracy of the mode is 72.97%\n* our objective is to find a model that give better accuracy than this baseline model\n* hence we try other models with and without grid search","7e743845":"## Import necessary libraries","3453a27c":"## Production model","94e22a89":"### **Lets Dive into finding the best model for production**\n\n*** Short description of different steps followed ***\n   \n\n    1. Understanding dataset.\n\n    2. Exploratory data analysis.\n      * Most importantly, plot a countplot of the target variable, this will reveal if the dataset is imbalanced.\n      *  Since this is classification problem, we need to check if there is imbalance in the dataset \n      *  if the dataset is imbalanced we need to perform either undersampling or oversampling \n    3. Creating Baseline model \n    4. Gridsearch and Pipeline\n      * we define series of 5 models and define a pipeline to run all these 5 models through gridsearch \n      * The model with highest accuracy is noted, Its hyper-parameters are noted \n\n    5. Define production model with the found best parameter\n    ","ced7455b":"### Helper function","93363461":"* we see from the below preliminary analysis that \n\n    * There are no duplicate row\n    * There are some nan's across columns in both train and test\n    * The dataset is balanced\n    ","d04b6919":"### without Grid_search","de9a7b13":"* Male's median loan amount is more than Females","e5f1a893":"## Baseline model","a7cd6cf4":"### With grid search","b537e45c":"**<span style=\"color:crimson;\">Kindly upvote if you like the pipeline \ud83d\ude03<\/span>** ","595dd9ab":"* However using grid search we find that the random forest preforms much better with acc of 78.57% than the baseline model of acc 72.97%","f36de86f":"* Males tend to ask more loan than females","a8423250":"## Data preprocessing","61996194":"* Since the acc of tuned Random forest model is better than any model, we will use it to estimate the loan_status of the test set","fee145fc":"* We see below that the categorical variables are not more sparse, except for Loan_id which will be removed later**","4010a902":"*** Summary of Interesting Finding from Exploratory data analysis**\n\n    *  The loan acceptance and rejection rate is balanced\n    *  The dataset is balanced dataset so we don't need undersampling or oversampling\n    *  Male applicants are obiously more than female applicants\n    *  More graudates are given loan than the non graudates\n    *  Male's median loan amount is more than Females\n    *  Males tend to ask more loan than females\n    *  Graduates tend to ask more loan than non graduates\n\n","c32ac831":"## Exploratory analysis","f929bd6c":"* More graudates are given loan than the non graudates"}}