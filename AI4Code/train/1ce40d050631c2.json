{"cell_type":{"d7ca77e4":"code","514c53ce":"code","3af07a3d":"code","b60154c9":"code","bbbf644f":"code","aaa6e1d1":"code","15eb56f9":"code","c271288b":"code","391200b8":"code","d785c9d2":"code","bfd129d2":"code","ca1f20b0":"code","071f99b1":"code","930e77d0":"code","0f093a85":"code","2265a44a":"code","a58b0bad":"code","842ef9c8":"code","a872a7f5":"code","b4bea45b":"code","bf7e5d18":"code","6a268c44":"code","a2bb3632":"code","53e6c16c":"code","b14ff5ac":"code","bb357373":"code","a9179d1b":"code","d83d5661":"code","4ac6f8c9":"code","bc8acb15":"code","a0769356":"code","f9d5f874":"code","3d8e8ddd":"code","bec79148":"code","5217ec93":"code","d18edef1":"code","d5a1ed23":"code","24a19ffe":"code","129d6e66":"code","f6516c35":"code","7160d228":"code","b8070eef":"code","008a81a4":"code","4c6ce725":"code","68bd957e":"code","60da527d":"code","2e259588":"code","730f4c97":"code","19a125c9":"code","b969eafa":"code","307217b4":"code","671a2da9":"code","aa5c401d":"code","9d793627":"code","e509823c":"code","7f9eb3bd":"code","d900ae1f":"code","f6abca9a":"code","603e62c8":"code","504505fb":"code","65ea4fbb":"code","fea87fb1":"code","e7211522":"code","6319db1d":"code","122de6d7":"markdown","339e5352":"markdown","d34bc139":"markdown","438cec1c":"markdown","410493ed":"markdown","c9c38ac3":"markdown","aef212d1":"markdown","f962a68b":"markdown","912eb429":"markdown","77e8e8b0":"markdown","8852d82d":"markdown","bc7375f6":"markdown","7db27a36":"markdown","211f4444":"markdown","002964fc":"markdown","db48825c":"markdown","bad7fdf4":"markdown","521b22e7":"markdown","a6112b02":"markdown","53ff8d98":"markdown","f9fb8129":"markdown","9307a31f":"markdown","5ac1f670":"markdown","d14e8b5c":"markdown","c5b0f85e":"markdown","d55e13fb":"markdown","148a7693":"markdown","405f2995":"markdown","bade3193":"markdown","4f3d9e07":"markdown","d20b786c":"markdown","059eddc3":"markdown","d96db00d":"markdown","3df8bcc8":"markdown","e971ddef":"markdown","5d0e828d":"markdown","686f0770":"markdown","8e1ef334":"markdown","57f4c9b2":"markdown","8042fd21":"markdown","6c016b9e":"markdown","1697aeac":"markdown","062785d7":"markdown","9e1f5d73":"markdown","2a0e2fc8":"markdown","c4d16cc9":"markdown","294d2749":"markdown","f5d46057":"markdown","03dba619":"markdown","827ed7d3":"markdown","a2468ac3":"markdown","55dab661":"markdown","4ddbb047":"markdown","02ed13d6":"markdown","863ecd65":"markdown","7ac26d8c":"markdown","e0d7c6ab":"markdown","3aaac005":"markdown","d6aa9caf":"markdown","1af531ce":"markdown","9839439a":"markdown","335e4fc3":"markdown","571006a4":"markdown","be2a45c4":"markdown","7882ac49":"markdown","8e01b65e":"markdown","41894f3c":"markdown","d036944e":"markdown","3ac8e952":"markdown"},"source":{"d7ca77e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","514c53ce":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport itertools\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import resample\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.cluster import KMeans\nfrom sklearn.svm import SVR\nfrom pprint import pprint\nfrom matplotlib import pyplot\nimport time\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor","3af07a3d":"df  = pd.read_csv('\/kaggle\/input\/concrete-compressive-strength\/concrete.csv')\ndf.head(10)","b60154c9":"rows_count, columns_count = df.shape\nprint('Total Number of rows :', rows_count)\nprint('Total Number of columns :', columns_count)\n","bbbf644f":"df.info()","aaa6e1d1":"sns.heatmap(df.isna(), yticklabels=False, cbar=False, cmap='viridis')","15eb56f9":"df.apply(lambda x: sum(x.isnull()))","c271288b":"df_transpose = df.describe().T\ndf_transpose","391200b8":"concrete_df = df.copy()","d785c9d2":"plt.figure(figsize=(12,6))\nsns.boxplot(data=concrete_df, orient=\"h\", palette=\"Set2\", dodge=False)","bfd129d2":"sns.pairplot(concrete_df,markers=\"h\", diag_kind = 'kde')\nplt.show()","ca1f20b0":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['cement'],ax=ax1)\nax1.tick_params(labelsize=15)\nax1.set_xlabel('cement', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\n\nsns.boxplot(concrete_df['cement'],ax=ax2)\nax2.set_title(\"Box Plot\")\nax2.set_xlabel('cement', fontsize=15)","071f99b1":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['slag'],ax=ax1)\nax1.set_xlabel('Slag', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['slag'],ax=ax2)\nax2.set_xlabel('Slag', fontsize=15)\nax2.set_title(\"Box Plot\")\n","930e77d0":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['ash'],ax=ax1)\nax1.set_xlabel('Ash', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['ash'],ax=ax2)\nax2.set_xlabel('Ash', fontsize=15)\nax2.set_title(\"Box Plot\")\n","0f093a85":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['water'],ax=ax1)\nax1.set_xlabel('Water', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['water'],ax=ax2)\nax2.set_xlabel('Water', fontsize=15)\nax2.set_title(\"Box Plot\")\n\n","2265a44a":"outlier_columns = []\n\nQ1 =  concrete_df['water'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['water'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_water = Q1 - 1.5 * IQR   # lower bound \nUTV_water = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('water <',LTV_water ,'and >',UTV_water, ' are outliers')\nprint('Numerber of outliers in water column below the lower whisker =', concrete_df[concrete_df['water'] < (Q1-(1.5*IQR))]['water'].count())\nprint('Numerber of outliers in water column above the upper whisker =', concrete_df[concrete_df['water'] > (Q3+(1.5*IQR))]['water'].count())\n\n# storing column name and upper-lower bound value where outliers are presense \noutlier_columns.append('water')\nupperLowerBound_Disct = {'water':UTV_water}","a58b0bad":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['superplastic'],ax=ax1)\nax1.set_xlabel('superplastic', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['superplastic'],ax=ax2)\nax2.set_xlabel('Superplastic', fontsize=15)\nax2.set_title(\"Box Plot\")\n","842ef9c8":"Q1 =  concrete_df['superplastic'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['superplastic'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_superplastic = Q1 - 1.5 * IQR   # lower bound \nUTV_superplastic = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('superplastic <',LTV_superplastic ,'and >',UTV_superplastic, ' are outliers')\nprint('Numerber of outliers in superplastic column below the lower whisker =', concrete_df[concrete_df['superplastic'] < (Q1-(1.5*IQR))]['superplastic'].count())\nprint('Numerber of outliers in superplastic column above the upper whisker =', concrete_df[concrete_df['superplastic'] > (Q3+(1.5*IQR))]['superplastic'].count())\n\n# storing column name and upper-lower bound value where outliers are presense\noutlier_columns.append('superplastic')\nupperLowerBound_Disct['superplastic'] = UTV_superplastic","a872a7f5":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['coarseagg'],ax=ax1)\nax1.set_xlabel('Coarseagg', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['coarseagg'],ax=ax2)\nax2.set_xlabel('Coarseagg', fontsize=15)\nax2.set_title(\"Box Plot\")","b4bea45b":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['fineagg'],ax=ax1)\nax1.set_xlabel('Fineagg', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['fineagg'],ax=ax2)\nax2.set_xlabel('Fineagg', fontsize=15)\nax2.set_title(\"Box Plot\")\n","bf7e5d18":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['age'],ax=ax1)\nax1.set_xlabel('Age', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\nsns.boxplot(concrete_df['age'],ax=ax2)\nax2.set_xlabel('Age', fontsize=15)\nax2.set_title(\"Box Plot\")","6a268c44":"Q1 =  concrete_df['age'].quantile(0.25) # 1\u00ba Quartile\nQ3 =  concrete_df['age'].quantile(0.75) # 3\u00ba Quartile\nIQR = Q3 - Q1                      # Interquartile range\n\nLTV_age = Q1 - 1.5 * IQR   # lower bound \nUTV_age = Q3 + 1.5 * IQR   # upper bound\n\nprint('Interquartile range = ', IQR)\nprint('age <',LTV_age ,'and >',UTV_age, ' are outliers')\nprint('Numerber of outliers in age column below the lower whisker =', concrete_df[concrete_df['age'] < (Q1-(1.5*IQR))]['age'].count())\nprint('Numerber of outliers in age column above the upper whisker =', concrete_df[concrete_df['age'] > (Q3+(1.5*IQR))]['age'].count())\n\n# storing column name and upper-lower bound value where outliers are presense\noutlier_columns.append('age')\nupperLowerBound_Disct['age'] = UTV_age","a2bb3632":"fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (13, 5))\nfig.set_size_inches(20,6)\nsns.distplot(concrete_df['strength'],ax=ax1)\nax1.tick_params(labelsize=15)\nax1.set_xlabel('strength', fontsize=15)\nax1.set_title(\"Distribution Plot\")\n\n\nsns.boxplot(concrete_df['strength'],ax=ax2)\nax2.set_title(\"Box Plot\")\nax2.set_xlabel('strength', fontsize=15)","53e6c16c":"print('These are the columns which have outliers : \\n\\n',outlier_columns)\nprint('\\n\\n',upperLowerBound_Disct)","b14ff5ac":"concrete_df_new = concrete_df.copy()","bb357373":"for col_name in concrete_df_new.columns[:-1]:\n    q1 = concrete_df_new[col_name].quantile(0.25)\n    q3 = concrete_df_new[col_name].quantile(0.75)\n    iqr = q3 - q1\n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    \n    concrete_df_new.loc[(concrete_df_new[col_name] < low) | (concrete_df_new[col_name] > high), col_name] = concrete_df_new[col_name].median()","a9179d1b":"plt.figure(figsize=(15,8))\nsns.boxplot(data=concrete_df_new, orient=\"h\", palette=\"Set2\", dodge=False)","d83d5661":"concrete_df_new.shape","4ac6f8c9":"concrete_df_new.corr()","bc8acb15":"mask = np.zeros_like(concrete_df_new.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(15,7))\nplt.title('Correlation of Attributes', y=1.05, size=19)\nsns.heatmap(concrete_df_new.corr(),vmin=-1, cmap='plasma',annot=True,  mask=mask, fmt='.2f')","a0769356":"cluster_range = range( 2, 6 )   # expect 3 to four clusters from the pair panel visual inspection hence restricting from 2 to 6\ncluster_errors = []\nfor num_clusters in cluster_range:\n  clusters = KMeans( num_clusters, n_init = 5)\n  clusters.fit(concrete_df_new)\n  labels = clusters.labels_\n  centroids = clusters.cluster_centers_\n  cluster_errors.append( clusters.inertia_ )\nclusters_df = pd.DataFrame( { \"num_clusters\":cluster_range, \"cluster_errors\": cluster_errors } )\nclusters_df[0:15]","f9d5f874":"# Elbow plot\n\nplt.figure(figsize=(12,6))\nplt.plot( clusters_df.num_clusters, clusters_df.cluster_errors, marker = \"o\" )","3d8e8ddd":"cluster = KMeans( n_clusters = 3, random_state = 2354 )\ncluster.fit(concrete_df_new)\n\nprediction=cluster.predict(concrete_df_new)\nconcrete_df_new[\"GROUP\"] = prediction     # Creating a new column \"GROUP\" which will hold the cluster id of each record\n\nconcrete_df_new_copy = concrete_df_new.copy(deep = True)  # Creating a mirror copy for later re-use instead of building repeatedly","bec79148":"centroids = cluster.cluster_centers_\ncentroids","5217ec93":"# All variables are on same scale, hence we can omit scaling.\n# But to standardize the process we will do it here\nXScaled = concrete_df_new.apply(zscore)\nXScaled.head()","d18edef1":"plt.figure(figsize=(12,6))\nsns.boxplot(data=XScaled, orient=\"h\", palette=\"Set2\", dodge=False)","d5a1ed23":"y_set = XScaled[['strength']]\nX_set = XScaled.drop(labels= \"strength\" , axis = 1)","24a19ffe":"y_set = XScaled[['strength']]\nX_set = XScaled.drop(labels= \"strength\" , axis = 1)\n\n# data spliting using 80:20 train test data ratio and randon seeding 7\nX_model_train, X_test, y_model_train, y_test = train_test_split(X_set, y_set, test_size=0.20, random_state=7)","129d6e66":"print('---------------------- Data----------------------------- \\n')\nprint('x train data {}'.format(X_model_train.shape))\nprint('y train data {}'.format(y_model_train.shape))\nprint('x test data  {}'.format(X_test.shape))\nprint('y test data  {}'.format(y_test.shape))\n","f6516c35":"# data spliting using 70:30 train test data ratio and randon seeding 7\nX_train, X_validate, y_train, y_validate = train_test_split(X_model_train, y_model_train, test_size=0.30, random_state=7)","7160d228":"print('---------------------- Data----------------------------- \\n')\nprint('x train data {}'.format(X_train.shape))\nprint('y train data {}'.format(y_train.shape))\nprint('x test data  {}'.format(X_validate.shape))\nprint('y test data  {}'.format(y_validate.shape))\n","b8070eef":"# Defining the kFold function for the cross validation\nn_split = 10\nrandon_state = 7\nkfold = KFold(n_split, random_state = randon_state)\nlinear_model = []\nlinear_model_score = []\nlinear_model_RMSE = []\nlinear_model_R_2 = []\nModel = []\nRMSE = []\nR_sq = []","008a81a4":"regression_model = LinearRegression()\nregression_model.fit(X_train, y_train)\nlinear_model.append('Linear Regression')\n# coefficients for each of the independent attributes\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))\n\nintercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))\n\nlr_score = regression_model.score(X_validate, y_validate)\nlinear_model_score.append(lr_score)\nprint(\"Linear Regression Model Score:\",lr_score)\n\nlr_rmse = np.sqrt((-1) * cross_val_score(regression_model, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Linear Regression Model RMSE :\", lr_rmse)\n\nlinear_model_RMSE.append(lr_rmse)\n\nlr_r2 = cross_val_score(regression_model, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Linear Regression Model R-Square Value :\",lr_r2)\n\n\nlinear_model_R_2.append(lr_r2)","4c6ce725":"poly = PolynomialFeatures(degree=2, interaction_only=True)\nX_train_ = poly.fit_transform(X_train)\nX_test_ = poly.fit_transform(X_validate)\nprint(\"Shape\", X_train_.shape)\nlinear_model.append('Polynomial Features - 2D')\n\npoly_clf = LinearRegression()\n\npoly_clf.fit(X_train_, y_train)\n\npf_score = poly_clf.score(X_test_, y_validate)\nprint(\"2D Polynomial Model Score:\",pf_score)\nlinear_model_score.append(pf_score)\n\npf_rmse = np.sqrt((-1) * cross_val_score(poly_clf, X_train_, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"2D Polynomial Model RMSE :\", pf_rmse)\nlinear_model_RMSE.append(pf_rmse)\n\npf_r2 = cross_val_score(poly_clf, X_train_, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nlinear_model_R_2.append(pf_r2)\nprint(\"2D Polynomial Model R-Square Value :\", pf_r2)","68bd957e":"poly = PolynomialFeatures(degree=3, interaction_only=True)\nX_train__ = poly.fit_transform(X_train)\nX_test__ = poly.fit_transform(X_validate)\nprint(\"Shape\", X_train__.shape)\nlinear_model.append('Polynomial Features - 3D')\n\npoly_clf_3d = LinearRegression()\n\npoly_clf_3d.fit(X_train__, y_train)\n\npf3_score = poly_clf_3d.score(X_test__, y_validate)\nprint(\"3D Polynomial Model Score:\",pf3_score)\nlinear_model_score.append(pf3_score)\n\npf3_rmse = np.sqrt((-1) * cross_val_score(poly_clf_3d, X_train__, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"3D Polynomial Model RMSE :\",pf3_rmse)\nlinear_model_RMSE.append(pf3_rmse)\n\npf3_r2 = cross_val_score(poly_clf_3d, X_train__, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"3D Polynomial Model R-Square Value :\",pf3_r2)\nlinear_model_R_2.append(pf3_r2)","60da527d":"ridge = Ridge(alpha=.3)\nridge.fit(X_train,y_train)\nlinear_model.append('Ridge - with general data')\nprint (\"Coefficients of the Ridge model\",ridge.coef_)\nrid_score = ridge.score(X_validate, y_validate)\nlinear_model_score.append(rid_score)\nprint(\"Ridge Model Score:\", rid_score)\nrig_rmse = np.sqrt((-1) * cross_val_score(ridge, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Ridge Model RMSE :\", rig_rmse)\nlinear_model_RMSE.append(rig_rmse)\nrid_r2 = cross_val_score(ridge, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Ridge Model R-Square Value :\",rid_r2)\nlinear_model_R_2.append(rid_r2)\n\n\nridge_pf2 = Ridge(alpha=.3)\nridge_pf2.fit(X_train_,y_train)\nlinear_model.append('Ridge - with 2d Polynomial features')\nprint(\"Coefficients of the Ridge Model - with 2d Polynomial features\")\nprint (ridge_pf2.coef_)\nrid_score = ridge_pf2.score(X_test_, y_validate)\nlinear_model_score.append(rid_score)\nprint(\"Ridge Model (2d Polynomial features) Score:\",rid_score)\nrig_rmse = np.sqrt((-1) * cross_val_score(ridge_pf2, X_train_, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Ridge Model (2d Polynomial features) RMSE :\", rig_rmse)\nlinear_model_RMSE.append(rig_rmse)\nrid_r2 = cross_val_score(ridge_pf2, X_train_, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Ridge Model (2d Polynomial features) R-Square Value :\",rid_r2)\nlinear_model_R_2.append(rid_r2)","2e259588":"lasso = Lasso(alpha=.3)\nlasso.fit(X_train,y_train)\nlinear_model.append('Lasso - with general data')\nprint(\"Coefficients of the Lasso model\")\nprint (lasso.coef_)\nlasso_score = lasso.score(X_validate, y_validate)\nlinear_model_score.append(lasso_score)\nprint(\"Lasso Model Score:\", lasso_score)\nlasso_rmse = np.sqrt((-1) * cross_val_score(lasso, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Lasso Model RMSE :\", lasso_rmse)\nlinear_model_RMSE.append(lasso_rmse)\nlasso_r2 = cross_val_score(lasso, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Lasso Model R-Square Value :\",lasso_r2)\nlinear_model_R_2.append(lasso_r2)\n\n\nlasso_pf2 = Lasso(alpha=.3)\nlasso_pf2.fit(X_train_,y_train)\nlinear_model.append('Lasso - with 2d Polynomial features')\nprint(\"Coefficients of the Lasso Model - with 2d Polynomial features\")\nprint (lasso_pf2.coef_)\nlasso_pf2_score = lasso_pf2.score(X_test_, y_validate)\nlinear_model_score.append(lasso_pf2_score)\nprint(\"Lasso Model (2d Polynomial features) Score:\",lasso_pf2_score)\nlasso_pf2_rmse = np.sqrt((-1) * cross_val_score(lasso_pf2, X_train_, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Lasso Model (2d Polynomial features) RMSE :\", lasso_pf2_rmse)\nlinear_model_RMSE.append(lasso_pf2_rmse)\nlasso_pf2_r2 = cross_val_score(lasso_pf2, X_train_, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Lasso Model (2d Polynomial features) R-Square Value :\", lasso_pf2_r2)\nlinear_model_R_2.append(lasso_pf2_r2)","730f4c97":"compare_lr_model_df = pd.DataFrame({'Model': linear_model,\n                           'Score': linear_model_score,\n                           'RMSE': linear_model_RMSE,\n                           'R Squared': linear_model_R_2})\nprint(\"BELOW ARE THE TRAINING SCORES: \")\ncompare_lr_model_df","19a125c9":"compare_lr_model_df[(compare_lr_model_df['RMSE'] == compare_lr_model_df['RMSE'].min()) & (compare_lr_model_df['R Squared'] == compare_lr_model_df['R Squared'].max())]","b969eafa":"rfTree = RandomForestRegressor(n_estimators=100)\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\",rfTree_train_score)\n\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\", rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_model_df","307217b4":"print(\"Random Forest Regressor Model Test Data Set Score:\")\nrfTree_test_score = rfTree.score(X_test, y_test)\nprint(rfTree_test_score)","671a2da9":"rf = RandomForestRegressor(random_state = 7)\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","aa5c401d":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10 , stop = 100, num = 3)]   # returns evenly spaced 10 numbers\n# Number of features to consider at every split\nmax_features = ['auto', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 10, num = 2)]  # returns evenly spaced numbers can be changed to any\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)","9d793627":"rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 5, scoring='neg_mean_absolute_error', \n                              cv = kfold, verbose=2, random_state=7, n_jobs=-1,\n                              return_train_score=True)\n# Fit the random search model\nrf_random.fit(X_train, y_train.values.ravel());","e509823c":"# best ensemble model (with optimal combination of hyperparameters)\nrfTree = rf_random.best_estimator_\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\",rfTree_train_score)\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\",rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_random_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_random_model_df","7f9eb3bd":"rfTree_test_score = rfTree.score(X_test, y_test)\nprint(\"Random Forest Regressor Model Test Data Set Score:\", rfTree_test_score)","d900ae1f":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [10],\n    'max_features': ['log2'],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [5,10],\n    'n_estimators': np.arange(50, 71)\n}\nrfg = RandomForestRegressor(random_state = 7)\n\ngrid_search = GridSearchCV(estimator = rfg, param_grid = param_grid, \n                          cv = kfold, n_jobs = 1, verbose = 0, return_train_score=True)\n\ngrid_search.fit(X_train, y_train.values.ravel());\ngrid_search.best_params_","f6abca9a":"# best ensemble model (with optimal combination of hyperparameters)\nrfTree = grid_search.best_estimator_\nrfTree.fit(X_train, y_train.values.ravel())\nprint('Random Forest Regressor')\nrfTree_train_score = rfTree.score(X_train, y_train)\nprint(\"Random Forest Regressor Model Training Set Score:\", rfTree_train_score)\n\nrfTree_score = rfTree.score(X_validate, y_validate)\nprint(\"Random Forest Regressor Model Validation Set Score:\",rfTree_score)\n\nrfTree_rmse = np.sqrt((-1) * cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='neg_mean_squared_error').mean())\nprint(\"Random Forest Regressor Model RMSE :\", rfTree_rmse)\n\nrfTree_r2 = cross_val_score(rfTree, X_train, y_train.values.ravel(), cv=kfold, scoring='r2').mean()\nprint(\"Random Forest Regressor Model R-Square Value :\", rfTree_r2)\n\nrfTree_random_model_df = pd.DataFrame({'Trainng Score': [rfTree_train_score],\n                           'Validation Score': [rfTree_score],\n                           'RMSE': [rfTree_rmse],\n                           'R Squared': [rfTree_r2]})\nrfTree_random_model_df","603e62c8":"def input_scores(name, model, x, y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=kfold, \n                                               scoring='neg_mean_squared_error').mean()))\n    R_sq.append(cross_val_score(model, x, y, cv=kfold, scoring='r2').mean())\n#Comment: Above function uses to append the cross validation scores of the algorithms.","504505fb":"from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                              AdaBoostRegressor)\n\nnames = ['Linear Regression', 'Ridge Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor', \n         'Random Forest Regressor', 'Gradient Boosting Regressor',\n         'Adaboost Regressor', 'Support Vector Regressor']\nmodels = [LinearRegression(), Ridge(),\n          KNeighborsRegressor(), DecisionTreeRegressor(),\n          RandomForestRegressor(n_estimators=100), GradientBoostingRegressor(), \n          AdaBoostRegressor(), SVR(gamma= \"auto\")]\n\n#Running all algorithms\nfor name, model in zip(names, models):\n    input_scores(name, model, X_train, y_train.values.ravel())","65ea4fbb":"compare_model_df = pd.DataFrame({'Model': Model,\n                           'RMSE': RMSE,\n                           'R Squared': R_sq})\nprint(\"BELOW ARE THE TRAINING SCORES: \")\ncompare_model_df","fea87fb1":"# configure bootstrap\nvalues = XScaled.values\n\n\n\nn_iterations = 1000              # Number of bootstrap samples to create\nn_size = int(len(XScaled) * 1)    # size of a bootstrap sample\n\n# run bootstrap\nstats = list()   # empty list that will hold the scores for each bootstrap iteration\n\nfor i in range(n_iterations):\n\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement\n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n\n    # fit model\n    rfTree = RandomForestRegressor(n_estimators=50)  \n    rfTree.fit(train[:,:-1], train[:,-1])   # fit against independent variables and corresponding target values\n\n    rfTree.fit(train[:,:-1], train[:,-1])   # fit against independent variables and corresponding target values\n    y_test = test[:,-1]    # Take the target column for all rows in test set\n\n    # evaluate model\n    predictions = rfTree.predict(test[:, :-1])   # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_test)\n\n    stats.append(score)\n","e7211522":"# plot scores\npyplot.hist(stats)\npyplot.show()\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","6319db1d":"rfTree_random_model_df","122de6d7":"## We will be  creating create 3 part of our dataset. We'll be working working test and validation data. And one part of data will be kept for Test the final score of our models.","339e5352":"- From the above we have come to the conclusion that RandomForestRegressor is giving the good accuracy score. \n- I have also tested with validation data which is also giving better result with RandomForestRegressor. \n- Hence we can proceed with the RandomForestRegressor to  modeling of strength of high performance concrete.","d34bc139":"### Shape of the data :- ","438cec1c":"## Coarseagg :-","410493ed":"# ::--------------------------- Exploratory Data Analysis -------------------------------- ::","c9c38ac3":"###  Data type of each attribute :-","aef212d1":"<b> Dataset : <\/b> Concrete Compressive Strength\n\n<b> Domain : <\/b>Material manufacturing\n\n<b>Description: <\/b> The actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not\nscaled).The data has 8 quantitative input variables, and 1 quantitative output\nvariable, and 1030 instances (observations).\n\n<b> Objective : <\/b>Modeling of strength of high performance concrete using Machine Learning\n\n\n<b>Steps : <\/b>This project involved feature exploration and selection to predict the strength of high-performance concrete. Used Regression models like Decision tree regressors to find out the most important features and predict the strength. Cross-validation techniques and Grid search were used to tune the parameters for best model performance.\n\n<b>Skills and Tools :<\/b> Regression, Decision trees, feature engineering","f962a68b":"## Separating Independent and Dependent :-","912eb429":"## Random Forest Regressor ::-","77e8e8b0":"<b>Comment: <\/b> From above I can see Lasso model decreases the features number to 3. But that effecting the model performace as well a quite.","8852d82d":"<b>Insight : <\/b>From above we can see that there are outliers in Fineagg column and there are two peaks in distribution plot and there is right skewness because long tail is at right side(mean>median).","bc7375f6":"## Ridge ::-","7db27a36":"## Strength :-","211f4444":"<b>Comment:<\/b> Here I have read the Concrete dataset using read_csv() function of pandas. df is a dataframe. I have used head() funtion to display first 10 records of the dataset.\n\n<b> Features(attributes) Understanding from the above dataframe :- <\/b> \n- <b>Cement <\/b> measured in kg in a m3 mixture\n- <b>Blast <\/b> measured in kg in a m3 mixture\n- <b>Fly ash <\/b> measured in kg in a m3 mixture\n- <b>Water <\/b> measured in kg in a m3 mixture\n- <b>Superplasticizer <\/b> measured in kg in a m3 mixture\n- <b>Coarse Aggregate <\/b> measured in kg in a m3 mixture\n- <b>Fine Aggregate <\/b> measured in kg in a m3 mixture\n- <b>Age <\/b> day (1~365)\n\n<b>Concrete compressive strength:-<\/b> measured in MPa\n\n","002964fc":"### Copying Dataframe :-\n- Before doing any manipulation with the dataframe it is better to copy the dataframe into another dataframe and keep the original dataframe as it is.","db48825c":"### Ash :-","bad7fdf4":"## Lasso ::-","521b22e7":"<b>Insight : <\/b>From above we can see that there are outliers in Age column and there are many peaks in distribution plot and there is left skewness because long tail is at left side(mean<median).","a6112b02":"<b>Comment:<\/b> Shape of the dataframe is (1030, 9).\nThere are 1030 rows and 9 columns in the dataset. ","53ff8d98":"## Checking the presence of outliers :-","f9fb8129":"# :::::::::::::::::::::Comparing performances of all the models::::::::::::::::::::::: ","9307a31f":"## Standardization Independent Varaibles :-","5ac1f670":"## Linear Regression :-","d14e8b5c":"<b>Comment :<\/b> From above we can see that all three groups are at same lavel, I dont find any difference so we will not be proceding with the clustering.","c5b0f85e":"# ::-------------------------------------- Data Visualization ------------------------------------::","d55e13fb":"## ---------------------------------------- Correlation using Heatmap --------------------------------------------","148a7693":"<b>Observation : <\/b> From above heatmap we can see that there is no missing values are present.","405f2995":"## KMeans Clustering :-","bade3193":"#### GridsearchCV :","4f3d9e07":"<b>Observation : <\/b> From the above pair plot we can infer the association among the attributes and target column as follows:\n- No high correlation between any two features\n- Strength have some possitive linear relation with cement and some with superplastic that means if the quantity of cement or superplastic is more then concrete is having more strength.\n- More strength is between 20-150 days aprox.\n- Strength is again decreasing again after 250 days approx.\n- Also It is quite visible multiple gaussian slag,ash,water, superplastic, age.\n- slag, cement and ash also have a tendency to create linear relation but it's not prominant.\n- Rest of the relation between other individual attributes are mostly formed cloud shape or symmetrical shape.","d20b786c":"## ----------------------------------------------- Fixing Outliers -----------------------------------------------------\n\n- As we have seen above outlier are presence in the given dataset.\n- There are multiple ways to deal with outliers but I mostly prefer either to drop the outliers or repalce it with median\/mean.\n- Here I am going to replace the outliers with median becase if we drop then ther may be chance to loose some important information.\n- I have also shown the number of outliers presence in each column in below code.","059eddc3":"## Polynomial Regression - Degree 3 ::-","d96db00d":"## Creating and view the correlation matrix :-","3df8bcc8":"### Slag :-","e971ddef":"<b>Insight : <\/b>From above we can see that there are outliers in water column and there is right skewness because long tail is at the right side.\n####  As ouliers are there in water so we will check how many outliers are there in the water.","5d0e828d":"### Water :-","686f0770":"#### After fixing outliers shape of dataframe:","8e1ef334":"<b>Insight: <\/b>The elbow plot confirms our visual analysis that there are likely 3 good clusters.","57f4c9b2":"<b>Insight: <\/b>From above boxplot we can see that there are outliers in slug values lies between range 100 to 200. 400 is the higest slug value.","8042fd21":"## Superplastic :-","6c016b9e":"## Polynomial Regression - Degree 2 ::-","1697aeac":"### Here I am comparing Linear and polynomial features","062785d7":"<b>Insight : <\/b>From above we can see that there are no outliers in strength column and there are two peaks in distribution plot and there is right skewness because long tail is at right side(mean>median)","9e1f5d73":"## Conclusion ::-","2a0e2fc8":"###  Descriptive Statistics :-","c4d16cc9":"<b>Observation : <\/b> From the above boxplot we can see that there are outliers in some columns. From the above ploting I can say slag, water, superplastic, fineagg and age column are having clear outliers. Let see these plots separately. I will be finding the outliers counts in individual attributes analysis and <b>fixing the outliers<\/b> after visualization and analysis of each attribute.","294d2749":"<b>Observations : <\/b> From above we can see that Mean and the median is nearly same for the Cement, Water, Superplastic, Coarseagg, Fineagg, Strength so we can say it is approximately normally distributed. Slag, Ash, Age are having much values at the maximum portion so we can say it is skewed towards right side.","f5d46057":"<b>Comment : <\/b> From the above we see the Ridge - with 2d Polynomial features scores the best with lowest RMSE and High R Squared values. So let's check the Random Forest Regressor model in details.","03dba619":"<b>Important : <\/b> Now we can see in boxplot that most of the outliers are replaced with their median in dataframe. We have seen that outliers most of the outliers are removed but because of the gaussian by replacing it with median value, the attributes raised with new outliers which we can ignore. ","827ed7d3":"## Hyper-tuning Random Forest Regressor ::-\nRandomSearchCV","a2468ac3":"<b>Correlation Insight :<\/b>- From above correlation matrix we can see that there are many features which are  correlated. if we see carefully then ash and cement are having corelation of -0.40. superplastic and water are having corelation of -0.66 . fineagg and water are having corelation of -0.45. Age and strenght have correlation of 0.50.\nLittle correlation of ~0.6 between Superplasticizer and Water (which is negative as evident from scatter matrix), but lets move forward as it is.\n\n'cement' has the highest correlation with the area of 'concrete_compressive_strength'(which is a positive correlation), followed by 'superplasticizer', which is also a positive correlation, 'ash' has the least correlation.","55dab661":"<b>Insight : <\/b>From above we can see that there are no outliers in coarseagg and there is a right skewness because long tail is at right side(mean>median).","4ddbb047":"### Bootstrapping_Confidence_Level :-","02ed13d6":"<b> Insight : <\/b>From above we can see that there are outliers in superplastic column and there is right skewness because long tail is at right side(mean>median).\n\n<b>As outliers are there in superplastic so  we will check how many outliers are there in the superplastic.<\/b>","863ecd65":"### Checking the presence of missing values :-","7ac26d8c":"<b>Comment :<\/b> Here we can see that all the variables are numerical.","e0d7c6ab":"<b>Insight :<\/b> From above we can see that there are no outliers in ash column. We can see a tall tower at range of 0 to 20 which indicates if slug value is between 100 and 200.","3aaac005":"###  Pair plot that includes all the columns of the data frame :-","d6aa9caf":"<b>Comment: <\/b> So the model is our Random Forest Regressor model. After executing the model I found. \n - Training Data Score : 0.974034\n - Validation Data Score : 0.893274\n - Test Data Score : 0.8571364194016591\nWe need to tune our model further and need to check if the model score in test data can be improvised or not.","1af531ce":"Defination of the function to comparing models","9839439a":"## Age :-","335e4fc3":"## Analysis of each attributes with the help of plots :-\n\n### Cement  :-","571006a4":"# :::::::::::::::::::::::::::::::::::::::: Model Building :::::::::::::::::::::::::::::::::::::::::","be2a45c4":"<b>Comment: <\/b> I found the Random Forest Regressor is having lowest Root Mean Square Error (RMSE) and Higest R Square value. So, I can say it is the best model to execute our model. ","7882ac49":"## Import the necessary libraries :","8e01b65e":"<b>Comment :<\/b> Here I have used numpy, pandas, matplotlib, seaborn, scipy for EDA and Data Visualization. Also used sklearn for data spliting, model building and for confusion matrix. ","41894f3c":"<b>Insight: <\/b> From above graph we see that the result of standardization(Z-score) is that the features are rescaled and their properties of a standard normal distribution changed to \u03bc=0 and \u03c3=1.","d036944e":"## Fineagg :-","3ac8e952":"<b>Insight : <\/b>From above we can see that there are no outliers in cement column and it's looks like normally distributed. Cemennt values lies between range 100 to 500. "}}