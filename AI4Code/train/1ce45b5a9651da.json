{"cell_type":{"eeb49fb8":"code","794fe625":"code","66bf7a9a":"code","f2ea5f92":"code","5771f5ba":"code","62615130":"code","9b836159":"code","96f7ea51":"code","1ca4c71d":"code","117ee97d":"code","69a433cb":"code","8803272e":"code","62804bf6":"code","672555a2":"code","dee8667e":"code","2f6538ea":"code","5644ff11":"code","7458380d":"code","c8be41a6":"code","37b18ca0":"code","395a54f6":"code","433a5db3":"code","295f8539":"code","1802f5e8":"code","eb722a2b":"code","3539858b":"code","2e8074be":"code","e316a243":"code","66d5a68c":"code","2db7f186":"code","d5f169d8":"code","d3cd9a37":"code","5fe370e5":"code","3a14e920":"code","fafb8e49":"code","7daa6ac4":"code","1ea42326":"code","e6da6b2f":"code","c5fbd22e":"code","84a647f0":"code","6e29d54e":"code","481004fc":"code","ca8b01e6":"code","2e143fdf":"code","04433d31":"code","8122d01d":"code","b37ad895":"code","1b39b586":"markdown","74d6e111":"markdown","6169ce4e":"markdown","7bfdbf59":"markdown","094e403b":"markdown","4981a5b3":"markdown","451938eb":"markdown","cad348be":"markdown","6e544cd0":"markdown","0e43e51f":"markdown","c41e875a":"markdown","cd789433":"markdown","7a7550a3":"markdown","5ddc7368":"markdown","3641bdc7":"markdown","42bbca06":"markdown","ddb40410":"markdown","05c62d54":"markdown","1f24eca0":"markdown","686ba021":"markdown","4887469f":"markdown","f93b4818":"markdown","91f66695":"markdown","cdcc6e00":"markdown","fbdc29ee":"markdown","e064a17c":"markdown","765fcf22":"markdown","f124c41c":"markdown","7f47572a":"markdown","ea8da9f1":"markdown","73f00685":"markdown"},"source":{"eeb49fb8":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#visualization libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt, numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nfrom IPython.display import Image\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport plotly.offline as pyo\nfrom plotly import tools\nimport seaborn as sns\n\nimport missingno as msno #to visualize missing data\n\nfrom imblearn.over_sampling import SMOTE\nimport itertools\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score,recall_score,roc_auc_score,f1_score,plot_confusion_matrix,plot_roc_curve,roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.preprocessing import LabelEncoder #label encoding for categorical columns\n\npyo.init_notebook_mode()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","794fe625":"data = pd.read_csv(\"..\/input\/credit-card-approval-prediction\/application_record.csv\", encoding = 'utf-8') \nrecord = pd.read_csv(\"..\/input\/credit-card-approval-prediction\/credit_record.csv\", encoding = 'utf-8') ","66bf7a9a":"print(\"Number of datapoints for application records: {}\".format(len(data)))\nprint(\"Number of unique clients in dataset: {}\".format(len(data.ID.unique())))\ndata.head()","f2ea5f92":"print(\"Number of datapoints for credit records: {}\".format(len(record)))\nprint(\"Number of unique clients in dataset: {}\".format(len(record.ID.unique())))\nrecord.head()","5771f5ba":"len(set(record['ID']).intersection(set(data['ID']))) # checking to see how many records match in two datasets","62615130":"plt_missing_1 = msno.matrix(data)\n\nplt_missing_1.set_title(\"Missing Data for application records dataset\",fontsize=20)","9b836159":"plt_missing_2 = msno.matrix(record)\n\nplt_missing_2.set_title(\"Missing Data for credit records dataset\",fontsize=20)","96f7ea51":"unique_counts = pd.DataFrame.from_records([(col, data[col].nunique()) for col in data.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nunique_counts","1ca4c71d":"unique_counts = pd.DataFrame.from_records([(col, record[col].nunique()) for col in record.columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\nunique_counts","117ee97d":"sns.set_context(\"notebook\",font_scale=.7,rc={\"grid.linewidth\": 0.1,'patch.linewidth': 0.0,\n    \"axes.grid\":True,\n    \"grid.linestyle\": \"-\",\n    \"axes.titlesize\" : 13,                                       \n    \"figure.autolayout\":True})\n                \npalette_1 = ['#FF5E5B','#EC9B9A','#00CECB','#80DE99','#C0E680','#FFED66']\n\nsns.set_palette(sns.color_palette(sns.color_palette(palette_1)))\n\n","69a433cb":"plt.figure(figsize=(10,10))\n\ncols_to_plot = [\"CNT_CHILDREN\",\"AMT_INCOME_TOTAL\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\"]\ndata[cols_to_plot].hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)","8803272e":"fig, axes = plt.subplots(1,2)\n\ng1=sns.countplot(y=data.NAME_INCOME_TYPE,linewidth=1.2, ax=axes[0])\ng1.set_title(\"Customer Distribution by Income Type\")\ng1.set_xlabel(\"Count\")\n\ng2=sns.countplot(y=data.NAME_FAMILY_STATUS,linewidth=1.2, ax=axes[1])\ng2.set_title(\"Customer Distribution by Family Status\")\ng2.set_xlabel(\"Count\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\n\nplt.show()","62804bf6":"fig, axes = plt.subplots(1,2)\n\ng1= sns.countplot(y=data.NAME_HOUSING_TYPE,linewidth=1.2, ax=axes[0])\ng1.set_title(\"Customer Distribution by Housing Type\")\ng1.set_xlabel(\"Count\")\ng1.set_ylabel(\"Housing Type\")\n\ng2= sns.countplot(y=data.NAME_EDUCATION_TYPE, ax=axes[1])\ng2.set_title(\"Customer Distribution by Education\")\ng2.set_xlabel(\"Count\")\ng2.set_ylabel(\"Education Type\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\nplt.show()","672555a2":"fig, axes = plt.subplots(1,3)\n\ng1= data['CODE_GENDER'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=[\"#76B5B3\",\"#EC9B9A\"],textprops = {'fontsize':12}, ax=axes[0])\ng1.set_title(\"Customer Distribution by Gender\")\n\ng2= data['FLAG_OWN_CAR'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=[\"#80DE99\",\"#00CECB\"],textprops = {'fontsize':12}, ax=axes[1])\ng2.set_title(\"Car Ownership\")\n\ng3= data['FLAG_OWN_REALTY'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=[\"#76B5B3\",\"#00CECB\"],textprops = {'fontsize':12}, ax=axes[2])\ng3.set_title(\"Realty Ownership\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()\n\nplt.show()","dee8667e":"data = data.drop_duplicates('ID', keep='last') #remove duplicate values and keep the last entry of the ID if its repeated.\ndata.drop('OCCUPATION_TYPE', axis=1, inplace=True) #the occupation type has missing values, we dropped them.","2f6538ea":"object_columns = data.columns[data.dtypes =='object'].tolist() #object columns in dataset\n\nunique_counts = pd.DataFrame.from_records([(col, data[object_columns][col].nunique()) for col in data[object_columns].columns],\n                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])\n\nunique_counts #unique counts for object columns ","5644ff11":"#renaming columns\n\ndata.rename(columns={\"CODE_GENDER\":\"Gender\",\"FLAG_OWN_CAR\":\"Own_Car\",\"FLAG_OWN_REALTY\":\"Own_Realty\",\n                     \"CNT_CHILDREN\":\"Children_Count\",\"AMT_INCOME_TOTAL\":\"Income\",\"NAME_EDUCATION_TYPE\":\"Education\",\n                     \"NAME_FAMILY_STATUS\":\"Family_Status\",\"NAME_HOUSING_TYPE\":\"Housing_Type\",\"DAYS_BIRTH\":\"Birthday\",\n                     \"DAYS_EMPLOYED\":\"Employment_Date\",\"FLAG_MOBIL\":\"Own_Mobile\",\"FLAG_WORK_PHONE\":\"Own_Work_Phone\",\n                     \"FLAG_PHONE\":\"Own_Phone\",\"FLAG_EMAIL\":\"Own_Email\",\"CNT_FAM_MEMBERS\":\"Family_Member_Count\",\n                    \"NAME_INCOME_TYPE\":\"Income_Type\"},inplace=True)","7458380d":"#all users account open month\nopen_month=pd.DataFrame(record.groupby([\"ID\"])[\"MONTHS_BALANCE\"].agg(min))\nopen_month=open_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) \ncustomer_data=pd.merge(data,open_month,how=\"left\",on=\"ID\") #merge to record data\n\n#convert categoric features into numeric\n\ncustomer_data[\"Gender\"] =  customer_data['Gender'].replace(['F','M'],[0,1])\ncustomer_data[\"Own_Car\"] = customer_data[\"Own_Car\"].replace([\"Y\",\"N\"],[1,0])\ncustomer_data[\"Own_Realty\"] = customer_data[\"Own_Realty\"].replace([\"Y\",\"N\"],[1,0])\ncustomer_data[\"Is_Working\"] = customer_data[\"Income_Type\"].replace([\"Working\",\"Commercial associate\",\"State servant\",\"Pensioner\",\"Student\"],[1,1,1,0,0])\n\ncustomer_data[\"In_Relationship\"] = customer_data[\"Family_Status\"].replace([\"Civil marriage\",\"Married\",\"Single \/ not married\",\n                                                                          \"Separated\",\"Widow\"],[1,1,0,0,0])\n\nhousing_type = {'House \/ apartment' : 'House \/ apartment',\n                   'With parents': 'With parents',\n                    'Municipal apartment' : 'House \/ apartment',\n                    'Rented apartment': 'House \/ apartment',\n                    'Office apartment': 'House \/ apartment',\n                    'Co-op apartment': 'House \/ apartment'}\n\ncustomer_data[\"Housing_Type\"] = customer_data['Housing_Type'].map(housing_type)\n\nfamily_status = {'Single \/ not married':'Single',\n                     'Separated':'Single',\n                     'Widow':'Single',\n                     'Civil marriage':'Married',\n                    'Married':'Married'}\n\ncustomer_data[\"Family_Status\"] = customer_data[\"Family_Status\"].map(family_status)\n\neducation_type = {'Secondary \/ secondary special':'secondary',\n                     'Lower secondary':'secondary',\n                     'Higher education':'Higher education',\n                     'Incomplete higher':'Higher education',\n                     'Academic degree':'Academic degree'}\n\ncustomer_data[\"Education\"] = customer_data[\"Education\"].map(education_type)\n\nincome_type = {'Commercial associate':'Working',\n                  'State servant':'Working',\n                  'Working':'Working',\n                  'Pensioner':'Pensioner',\n                  'Student':'Student'}\n\ncustomer_data[\"Income_Type\"] = customer_data[\"Income_Type\"].map(income_type)\n\ncustomer_data[\"Household_Size\"] = customer_data[\"Children_Count\"] + customer_data[\"In_Relationship\"].apply(lambda x: 2 if x==1 else 1)\n\ncustomer_data[\"Age\"] = round((customer_data.Birthday\/365)*-1)\n\ncustomer_data[\"Experience\"] = customer_data.Employment_Date\/365\ncustomer_data['Experience']=customer_data['Experience'].apply(lambda v : int(v*-1) if v <0 else 0)\n\ncustomer_data=customer_data.drop(columns=['Employment_Date','Birthday','Children_Count'])\n\ncustomer_data= pd.get_dummies(customer_data, columns=['Income_Type', 'Education','Family_Status',\"Housing_Type\"])","c8be41a6":"customer_data.head()","37b18ca0":"other_numerical_cols = [\"Income\",\"Age\",\"Experience\",\"Household_Size\"]\n\nfig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\",\n                   subplot_titles=(\"Income\", \"Age\", \"Experience\", \"Family Member Count\"))\n\nfig.add_trace(go.Box(x=customer_data.Income, name='Income',boxmean=True),row=1,col=1)\nfig.add_trace(go.Box(x=customer_data.Age, name='Age', boxmean=True), row=1, col=2)\nfig.add_trace(go.Box(x=customer_data.Experience, name='Experience', boxmean=True), row=2, col=1)\nfig.add_trace(go.Box(x=customer_data.Household_Size, name=\"Family Member Count\", boxmean=True),row=2, col=2)\n\nfig.show()","395a54f6":"def calculate_z_scores(df, cols):\n    for col in cols:\n        df[col+\"_z_score\"] = (df[col] - df[col].mean())\/df[col].std()\n    return df\n\ndf_2 = calculate_z_scores(df = customer_data, cols = [\"Income\",\"Experience\",\"Household_Size\"])\n\n\n#removing outliers\nfilter_2 = df_2.Household_Size_z_score.abs() <= 3.5\nfilter_3 = df_2.Experience_z_score.abs() <= 3.5\nfilter_4 = df_2.Income_z_score.abs() <= 3.5\n\ncustomer_apps = df_2[filter_2 & filter_3 & filter_4]\n\ncustomer_apps.drop(columns= [\"Income_z_score\",\"Experience_z_score\",\"Household_Size_z_score\"],inplace=True)","433a5db3":"other_numerical_cols = [\"Income\",\"Age\",\"Experience\",\"Family_Member_Count\"]\n\nfig = make_subplots(rows=2, cols=2, start_cell=\"bottom-left\",\n                   subplot_titles=(\"Income\", \"Age\", \"Experience\", \"Family Member Count\"))\n\nfig.add_trace(go.Box(x=customer_apps.Income, name='Income',boxmean=True),row=1,col=1)\nfig.add_trace(go.Box(x=customer_apps.Age, name='Age', boxmean=True), row=1, col=2)\nfig.add_trace(go.Box(x=customer_apps.Experience, name='Experience', boxmean=True), row=2, col=1)\nfig.add_trace(go.Box(x=customer_apps.Household_Size, name=\"Family Member Count\", boxmean=True),row=2, col=2)\n\nfig.show()","295f8539":"record['dep_value'] = None\nrecord['dep_value'][record['STATUS'] =='2']='Yes' \nrecord['dep_value'][record['STATUS'] =='3']='Yes' \nrecord['dep_value'][record['STATUS'] =='4']='Yes' \nrecord['dep_value'][record['STATUS'] =='5']='Yes' \n\nrecord_count=record.groupby('ID').count()\nrecord_count['dep_value'][record_count['dep_value'] > 0]='Yes' \nrecord_count['dep_value'][record_count['dep_value'] == 0]='No' \nrecord_count = record_count[['dep_value']]","1802f5e8":"# Data frame to analyze length of time since initial approval of credit card\n# Shows number of past dues, paid off and no loan status.\ngrouped = record.groupby('ID')\n\npivot_tb = record.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')\npivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min()\npivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max()\npivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month']\npivot_tb['window'] += 1 # Adding 1 since month starts at 0.\n\n#Counting number of past dues, paid offs and no loans.\npivot_tb['paid_off'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'C'].count(axis = 1)\npivot_tb['pastdue_1-29'] = pivot_tb[pivot_tb.iloc[:,0:61] == '0'].count(axis = 1)\npivot_tb['pastdue_30-59'] = pivot_tb[pivot_tb.iloc[:,0:61] == '1'].count(axis = 1)\npivot_tb['pastdue_60-89'] = pivot_tb[pivot_tb.iloc[:,0:61] == '2'].count(axis = 1)\npivot_tb['pastdue_90-119'] = pivot_tb[pivot_tb.iloc[:,0:61] == '3'].count(axis = 1)\npivot_tb['pastdue_120-149'] = pivot_tb[pivot_tb.iloc[:,0:61] == '4'].count(axis = 1)\npivot_tb['pastdue_over_150'] = pivot_tb[pivot_tb.iloc[:,0:61] == '5'].count(axis = 1)\npivot_tb['no_loan'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'X'].count(axis = 1)\n#Setting Id column to merge with app data.\npivot_tb['ID'] = pivot_tb.index","eb722a2b":"pivot_tb.head()","3539858b":"target = pd.DataFrame()\ntarget['ID'] = pivot_tb.index\ntarget['paid_off'] = pivot_tb['paid_off'].values\ntarget['#_of_pastdues'] = pivot_tb['pastdue_1-29'].values+ pivot_tb['pastdue_30-59'].values + pivot_tb['pastdue_60-89'].values +pivot_tb['pastdue_90-119'].values+pivot_tb['pastdue_120-149'].values +pivot_tb['pastdue_over_150'].values\ntarget['no_loan'] = pivot_tb['no_loan'].values\ncustomer_apps_1 = customer_apps.merge(target, how = 'inner', on = 'ID')\n\ncustomer_apps_2=pd.merge(customer_apps_1,record_count,how='inner',on='ID')\ncustomer_apps_2['target']=customer_apps_2['dep_value']\ncustomer_apps_2.loc[customer_apps_2['target']=='Yes','target']=1\ncustomer_apps_2.loc[customer_apps_2['target']=='No','target']=0\n\ncustomer_apps_2.drop(columns=[\"dep_value\"],inplace=True)","2e8074be":"matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n\nf, ax = plt.subplots(figsize=(15,15))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\ncorr = customer_apps_2.drop(columns=[\"Own_Mobile\"]).corr().round(1)\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","e316a243":"customer_apps_2['target'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=['#FF5E5B', '#C0E680'],textprops = {'fontsize':7}).set_title(\"Target distribution\")\n\nplt.show()","66d5a68c":"sns.set_context(\"notebook\",font_scale=.7,rc={\"grid.linewidth\": 0.1,'patch.linewidth': 0.0,\n    \"axes.grid\":True,\n    \"grid.linestyle\": \"-\",\n    \"axes.titlesize\" : 13,                                       \n    'figure.figsize':(15,15)})\n                \npalette_1 = ['#FF5E5B','#EC9B9A','#00CECB','#80DE99','#C0E680','#FFED66']\n\nsns.set_palette(sns.color_palette(sns.color_palette(palette_1)))","2db7f186":"fig, axes = plt.subplots(1,3)\n\ng1=sns.boxenplot(x='target', y='Income', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[0])\ng1.set_title(\"Income-Target\")\ng2=sns.boxenplot(x='target', y='Age', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[1])\ng2.set_title(\"Age-Target\")\ng3=sns.boxenplot(x='target', y='Experience', data=customer_apps_2,palette=['#FF5E5B', '#C0E680'], ax=axes[2])\ng3.set_title(\"Work Experience-Target\")\n\nfig.set_size_inches(14,5)\n\nplt.tight_layout()","d5f169d8":"sns.displot(data=customer_apps_2, x='Income', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='Age', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='Experience', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='begin_month', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\n","d3cd9a37":"sns.displot(data=customer_apps_2, x='no_loan', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='#_of_pastdues', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])\nsns.displot(data=customer_apps_2, x='paid_off', hue=\"Is_Working\", col='target', kind=\"kde\", height=4, facet_kws={'sharey': False, 'sharex': False},palette=['#C70039','#80DE99'])","5fe370e5":"customer_apps_2.head()","3a14e920":"# Calculate information value\ndef calc_iv(df, feature, target, pr=False):\n    \n    lst = []\n\n    df[feature] = df[feature].fillna(\"NULL\")\n\n    for i in range(df[feature].nunique()):\n        val = list(df[feature].unique())[i]\n        lst.append([feature,                                                        # Variable\n                    val,                                                            # Value\n                    df[df[feature] == val].count()[feature],                        # All\n                    df[(df[feature] == val) & (df[target] == 0)].count()[feature],  # Good (think: Fraud == 0)\n                    df[(df[feature] == val) & (df[target] == 1)].count()[feature]]) # Bad (think: Fraud == 1)\n\n    data = pd.DataFrame(lst, columns=['Variable', 'Value', 'All', 'Good', 'Bad'])\n\n    data['Share'] = data['All'] \/ data['All'].sum()\n    data['Bad Rate'] = data['Bad'] \/ data['All']\n    data['Distribution Good'] = (data['All'] - data['Bad']) \/ (data['All'].sum() - data['Bad'].sum())\n    data['Distribution Bad'] = data['Bad'] \/ data['Bad'].sum()\n    data['WoE'] = np.log(data['Distribution Good'] \/ data['Distribution Bad'])\n\n    data = data.replace({'WoE': {np.inf: 0, -np.inf: 0}})\n\n    data['IV'] = data['WoE'] * (data['Distribution Good'] - data['Distribution Bad'])\n\n    data = data.sort_values(by=['Variable', 'Value'], ascending=[True, True])\n    data.index = range(len(data.index))\n\n    if pr:\n        print(data)\n        print('IV = ', data['IV'].sum())\n\n    iv = data['IV'].sum()\n\n    return iv, data","fafb8e49":"features = customer_apps_2.columns.tolist()[:-1]\niv_list = []\nfor feature in features:\n    iv, data = calc_iv(customer_apps_2, feature, 'target')\n    iv_list.append(round(iv,4))\n\nwoe_df = pd.DataFrame(np.column_stack([features, iv_list]), \n                      columns=['Feature', 'iv'])\nwoe_df","7daa6ac4":"x=customer_apps_2.loc[:, customer_apps_2.columns != 'target']\ny=customer_apps_2.iloc[:,-1:]\n\nX=x","1ea42326":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(x)\n\nX = pd.DataFrame(scaler.transform(x), columns=[x.columns])","e6da6b2f":"y = y.astype('int')\nX_balance,Y_balance = SMOTE().fit_resample(X,y)\nX_balance = pd.DataFrame(X_balance, columns = X.columns)","c5fbd22e":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\ncols = customer_apps_2.loc[:, customer_apps_2.columns != 'target'].columns.tolist()\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model, 15)\nfit = rfe.fit(X_balance, Y_balance)\nrfe_features = pd.DataFrame({\"Feature\":cols,\n              \"Support_LogisticRegression\":fit.support_,\n              \"Feature_Rank_logisticRegression\":fit.ranking_})\nrfe_features","84a647f0":"from sklearn.ensemble import ExtraTreesClassifier\n\nmodel = ExtraTreesClassifier(n_estimators=10)\nmodel.fit(X_balance, Y_balance)\nfeature_importances = pd.DataFrame({\"Feature\":cols,\n              \"Feature_Importance_ExtratreeClassifier\":model.feature_importances_})","6e29d54e":"df1=pd.merge(woe_df, feature_importances, on=[\"Feature\"])\nfeature_selection_df = pd.merge(df1, rfe_features, on=[\"Feature\"])\nfeature_selection_df","481004fc":"selected_features = [\"paid_off\",\"begin_month\",\"#_of_pastdues\",\"no_loan\",\"Income\",\"Experience\",\n                     \"Education_Higher education\",\"Education_secondary\",\"Own_Realty\",\n                     \"Family_Status_Single\",\"Family_Member_Count\",\"Is_Working\",\n                     \"Own_Car\",\"Age\"]\n\nX_balance= X_balance[selected_features]","ca8b01e6":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_balance, Y_balance, random_state=100, test_size=0.3)\nprint(X_train.shape)","2e143fdf":"classifiers = {\n    \"LogisticRegression\" : LogisticRegression(),\n    \"KNeighbors\" : KNeighborsClassifier(),\n    \"SVC\" : SVC(C = 0.8,kernel='linear',probability=True),\n    \"DecisionTree\" : DecisionTreeClassifier(),\n    \"RandomForest\" : RandomForestClassifier(n_estimators=250,max_depth=12,min_samples_leaf=16),\n    \"XGBoost\" : XGBClassifier(max_depth=12,\n                              n_estimators=250,\n                              min_child_weight=8, \n                              subsample=0.8, \n                              learning_rate =0.02,    \n                              seed=42),\n    \"CatBoost\" : CatBoostClassifier(iterations=250,\n                           learning_rate=0.2,\n                           od_type='Iter',\n                           verbose=25,\n                           depth=16,\n                           random_seed=42)\n}\n\nresult_table = pd.DataFrame(columns=['classifiers','accuracy','presicion','recall','f1_score','fpr','tpr','auc'])\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    y_predict = classifier.predict(X_test)\n    \n    yproba = classifier.predict_proba(X_test)[::,1]\n    \n    fpr, tpr, _ = roc_curve(y_test,  yproba)\n    auc = roc_auc_score(y_test, yproba)\n    \n    conf_matrix = confusion_matrix(y_test,y_predict)\n    \n    result_table = result_table.append({'classifiers':key,\n                                        'accuracy':accuracy_score(y_test, y_predict),\n                                        'presicion':precision_score(y_test, y_predict, average='weighted'),\n                                        'recall':recall_score(y_test, y_predict, average='weighted'),\n                                        'f1_score':f1_score(y_test, y_predict, average='weighted'),\n                                        'fpr':fpr, \n                                        'tpr':tpr, \n                                        'auc':auc\n                                         }, ignore_index=True)\n        \nresult_table.set_index('classifiers', inplace=True)","04433d31":"fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15,10))\n\nfor cls, ax in zip(list(classifiers.values()), axes.flatten()):\n    plot_confusion_matrix(cls, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap='Blues')\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()  \nplt.show()","8122d01d":"fig = plt.figure(figsize=(8,6))\n\nfor i in result_table.index:\n    plt.plot(result_table.loc[i]['fpr'], \n             result_table.loc[i]['tpr'], \n             label=\"{}, AUC={:.3f}\".format(i, result_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color='orange', linestyle='--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"Flase Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop={'size':13}, loc='lower right')\n\nplt.show()","b37ad895":"result_table.iloc[:,:4]","1b39b586":"As seen above, there are some outliers values in children count, family member count, income and employment rate columns\n\n* We need to remove these outliers to make sure they do not affect our model results.\n* We will now remove these outliers by using z scores.","74d6e111":"**Calculate Information Value**","6169ce4e":"# 4. Feature Selection","7bfdbf59":"# 3 - Data Preprocessing & Feature Engineering","094e403b":"**RFE (Recursive Feature Elimination)**","4981a5b3":"The following attributes were selected according to the table above.","451938eb":"**Credit Card Applications and the problems associated with it**\n\nBanks receive a lot of applications for issuance of credit cards. Many of them rejected for many reasons, like high-loan balances, low-income levels, or too many inquiries on an individual\u2019s credit report. Manually analyzing these applications is error-prone and a time-consuming process. This task can be automated with the power of machine learning, In this project, we will be build an automatic credit card approval predictor using machine learning techniques, just like the real banks do. ","cad348be":"We notice in the value counts above that label types are now balanced\nthe problem of oversampling is solved now\nwe will now implement different models to see which one performs the best","6e544cd0":"Logistic Regression, K-Nearest Neighbors, Support Vector Machine (SVM), Decision Tree, Random Forest, XGBoost and CatBoost algorithms performed.\n\nTo briefly mention these algorithms,\n\n**Logistic Regression** \nUnlike regression which uses Least Squares, the model uses Maximum Likelihood to fit a sigmoid-curve on the target variable distribution. It uses a logistic function, and most commonly used when the data in question has binary output.\n\n**K-Nearest Neighbors**\nK-Nearest Neighbor (KNN) algorithm predicts based on the specified number (k) of the nearest neighboring data points. Here, the pre-processing of the data is significant as it impacts the distance measurements directly. Unlike others, the model does not have a mathematical formula, neither any descriptive ability. \n\n**Support Vector Machine**\nIn the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well. Support vector machine is highly preferred by many as it produces significant accuracy with less computation power.\n\n**Decision Tree**\nIn this method a set of training examples is broken down into smaller and smaller subsets while at the same time an associated decision tree get incrementally developed. At the end of the learning process, a decision tree covering the training set is returned.\n\n**Random Forest**\nA Random Forest is a reliable ensemble of multiple Decision Trees (or CARTs); though more popular for classification, than regression applications. Here, the individual trees are built via bagging (i.e. aggregation of bootstraps which are nothing but multiple train datasets created via sampling of records with replacement) and split using fewer features. The resulting diverse forest of uncorrelated trees exhibits reduced variance; therefore, is more robust towards change in data and carries its prediction accuracy to new data. It works well with both continuous & categorical data.\n\n**XGBoost**\nIt is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Execution speed and high performance are the main reasons to use XGBoost.\n\n**CatBoost**\nCatBoost is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data.","0e43e51f":"# 6. Results","c41e875a":"In this project, we\u2019ll be using Credit Card Approval Dataset. The structure of our project will be as follows;\n\n* To get a basic introduction of our project & What\u2019s the business problem associated with it ?\n* We\u2019ll start by loading and viewing the dataset.\n* To manipulate data, if there are any missing entries in the dataset.\n* To perform exploratory data analysis (EDA) on our dataset.\n* To pre-process data before applying machine learning model to the dataset.\n* To apply machine learning models that can predict if an individual\u2019s application for a credit card will be accepted or not.","cd789433":"# 5. Modelling","7a7550a3":"# 1. Importing Data & EDA","5ddc7368":"**Unique counts**","3641bdc7":"**Feature Scaling**","42bbca06":"The number of unique ids in the two datasets is not equal. There are fewer customers than applications in the credit record dataset. The intersection is 36,457 customers.","ddb40410":"Feature scaling is essential for machine learning algorithms that calculate distances between data. The ML algorithm is sensitive to the \u201crelative scales of features,\u201d which usually happens when it uses the numeric values of the features rather than say their rank.In many algorithms, when we desire faster convergence, scaling is a must.","05c62d54":"Seaborn Plot Styling","1f24eca0":"Unique clients and rows are not equal,which means there are duplicates.","686ba021":"**Missing Values**","4887469f":"We will look at numeric columns to see if there is anything that needs to be changed.","f93b4818":"**SMOTE (Synthetic Minority Oversampling Technique) to Balance Dataset**\n\nA problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary. \n\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model. An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class.\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nWe use Synthetic Minority Over-Sampling Technique(SMOTE) to overcome sample imbalance problem.\n\n","91f66695":"# 2 - Data Visualization","cdcc6e00":"**Task**\n\nBuild a machine learning model to predict if an applicant is 'good' or 'bad' client, different from other tasks, the definition of 'good' or 'bad' is not given. You should use some techique, such as vintage analysis to construct you label. Also, unbalance data problem is a big problem in this task.","fbdc29ee":"**ExtraTreesClassifier**\n\nThe purpose of the ExtraTreesClassifier is to fit a number of randomized decision trees to the data, and in this regard is a from of ensemble learning. Particularly, random splits of all observations are carried out to ensure that the model does not overfit the data.\n\nEach Decision Tree in the Extra Trees Forest is constructed from the original training sample. Then, at each test node, Each tree is provided with a random sample of k features from the feature-set from which each decision tree must select the best feature to split the data based on some mathematical criteria (typically the Gini Index). This random sample of features leads to the creation of multiple de-correlated decision trees.\n\n","e064a17c":"Recursive feature elimination (RFE) is a feature selection method that fits a model and removes the weakest feature (or features) until the specified number of features is reached. Features are ranked by the model\u2019s coef_ or feature_importances_ attributes, and by recursively eliminating a small number of features per loop, RFE attempts to eliminate dependencies and collinearity that may exist in the model.","765fcf22":"We have checked the null values for records data, and all good here. ","f124c41c":"There are outliers in 2 columns.\n\n* CNT_CHILDREN\n* AMT_INCOME_TOTAL","7f47572a":"Now we will merge all importance scores from different feature selection methods","ea8da9f1":"We have filtered the columns that have non numeric values to see if they are useful. We will convert them numeric. ","73f00685":"The weight of evidence tells the predictive power of an independent variable in relation to the dependent variable.\n\nThe weight of evidence tells the predictive power of a single feature concerning its independent feature. If any of the categories\/bins of a feature has a large proportion of events compared to the proportion of non-events, we will get a high value of WoE which in turn says that that class of the feature separates the events from non-events."}}