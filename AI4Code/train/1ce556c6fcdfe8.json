{"cell_type":{"ba9fc349":"code","9ac938c1":"code","8d739c42":"code","00c33420":"code","befa8bdb":"code","737e1859":"code","06946ced":"code","bf19256f":"code","83493fd6":"code","666983ff":"code","de10059a":"code","92630c3d":"code","0e67d069":"code","1a802794":"code","e557e195":"code","3d93092b":"code","c3532b4c":"code","acb18a4f":"code","a60dc131":"code","ebfc756f":"code","e2dcb484":"code","4538a7f2":"code","77b4bbaf":"code","05c9452c":"code","026e7e33":"code","6ed6bae1":"code","c14a621d":"code","b2a01a47":"code","6e3ade78":"code","783efb06":"code","a27bc971":"code","4a20c796":"code","e3b25e68":"code","d43ba438":"code","e64213f5":"code","47249d6d":"code","cfeebb76":"code","16c0723b":"code","68f232e2":"code","a809fdab":"code","829ddaea":"code","c0c426fd":"code","4bfd3de3":"code","688497d6":"code","560cfa1b":"code","38bb04ee":"markdown","7feb7726":"markdown","080f9d0f":"markdown","70601cec":"markdown","c1440888":"markdown","854728d6":"markdown","876a1629":"markdown"},"source":{"ba9fc349":"#import library\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom functools import partial\nimport optuna\nfrom xgboost import XGBRegressor\n\n\nfrom sklearn import feature_extraction,linear_model,model_selection,preprocessing\n\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score,accuracy_score,log_loss\n","9ac938c1":"df_train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndf_test =pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","8d739c42":"df_train.describe()","00c33420":"#Ratio of disaster or not.\n\nax =sns.countplot(x='target',data=df_train)\nax.set_xticklabels(['0: Not Disaster','1: Disaster'],ha=\"center\")\nplt.title(\"Disaster or Not\")\nplt.style.use(\"seaborn-whitegrid\")\ntotal= len(df_train.target)\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() \/ total:.1f}%\\n'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height()\n    ax.annotate(percentage, (x, y), ha='center', va='center')","befa8bdb":"#not disaster example\ndf_train[df_train['target'] ==0].tail(100)","737e1859":"#disaster example\ndf_train[df_train['target']==1].head(100)\n    ","06946ced":"#word_count\n\ndf_train['word_count'] =df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] =df_test['text'].apply(lambda x: len(str(x).split()))\n\n#unique_word_count\n\ndf_train['unique_word_count'] =df_train['text'].apply(lambda x:len(set(str(x).split())))\ndf_test['unique_word_count'] =df_test['text'].apply(lambda x:len(set(str(x).split())))\n\n#stop_word_count\n\nfrom nltk.corpus import stopwords\nstopwords_en=set(stopwords.words('english'))\n\ndf_train['stop_word_count'] =df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords_en]))\ndf_test['stop_word_count'] =df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords_en]))\n\n\n#mean_word_length\n\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n\n#char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] =df_test['text'].apply(lambda x: len(str(x)))\n\n#punctuation_count\nfrom string import punctuation\n\npunctuations = set(punctuation)\n\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x:len([c for c in str(x) if c in punctuations]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x:len([c for c in str(x) if c in punctuations]))","bf19256f":"new_features=['word_count','unique_word_count','stop_word_count','mean_word_length','char_count','punctuation_count']","83493fd6":"from sklearn import preprocessing\nscaler= preprocessing.MinMaxScaler()\ndf_train[new_features] = scaler.fit_transform(df_train[new_features])\ndf_test[new_features] = scaler.transform(df_test[new_features])","666983ff":"#keywords\nprint(\"keywords:\" + str(len(df_train.keyword.unique())))\nprint(\"Total tweets:\" + str(len(df_train)) +\"\\n\")\n      \nprint(str(df_train.keyword.unique()))\n","de10059a":"#location\nprint(\"keywords:\" + str(len(df_train.location.unique())))\nprint(\"Total tweets:\" + str(len(df_train)) +\"\\n\")\n\nprint(str(df_train.location.unique()))\n","92630c3d":"df_train_withkeywords = df_train.dropna(subset=['keyword'],axis=0)","0e67d069":"df_train_withkeywords.head(5)\n","1a802794":"df_train_withkeywords['target_mean'] = df_train_withkeywords.groupby('keyword')['target'].transform('mean')","e557e195":"df_train_withkeywords","3d93092b":"#This code is from Referred to Basic NLP with NLTK(https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk\/notebook)\n# as mentioned below.\n\n\nfig =plt.figure(figsize=(12,72),dpi=100)\n\nsns.countplot(y=df_train_withkeywords.sort_values(by='target_mean',ascending=False)['keyword'],\n             hue=df_train_withkeywords.sort_values(by='target_mean',ascending=False)['target'])\n\nplt.tick_params(axis='x',labelsize=15)\nplt.tick_params(axis='y',labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\n","c3532b4c":"g= df_train_withkeywords.groupby(by='keyword')['target_mean'].apply(lambda x:list(np.unique(x)))\n","acb18a4f":"disaster_keywords=[]\nnon_disaster_keywords=[]\n\nfor word in df_train_withkeywords.keyword.unique():\n    if g[word][0] >0.7:\n        disaster_keywords.append(word)\n    if g[word][0] <0.3:\n        non_disaster_keywords.append(word)\n        ","a60dc131":"#Informative keywords list\n\n\ndef disaster(keyword):\n    if keyword in disaster_keywords:\n        return 1\n    return 0\ndef nondisaster(keyword):\n    if keyword in non_disaster_keywords:\n        return 1\n    return 0\n    \ndf_train['is_disaster_keyword'] =df_train['keyword'].apply(disaster)\ndf_train['is_non_disaster_keyword'] =df_train['keyword'].apply(nondisaster)\n\ndf_test['is_disaster_keyword'] =df_test['keyword'].apply(disaster)\ndf_test['is_non_disaster_keyword'] =df_test['keyword'].apply(nondisaster)\n","ebfc756f":"#example\n\n#from nltk import sent_tokenize, word_tokenize\n#text = df_train.text[0]\n#for sent in sent_tokenize(text):\n#    print([word.lower() for word in word_tokenize(sent)])","e2dcb484":"#text0_list = list(map(str.lower, word_tokenize(text)))\n\n#remove stopwords(non-content words)\n\n#from nltk.corpus import stopwords\n#stopwords_en=set(stopwords.words('english'))\n#remove punctuations\n\n#from string import punctuation\n#stopwords_en_withpunct = stopwords_en.union(set(punctuation))\n\n\n#text0_list1=[word for word in text0_list if word not in stopwords_en_withpunct]\n#print(text0_list1)","4538a7f2":"#from nltk.stem import PorterStemmer\n#porter=PorterStemmer()\n#for word in text0_list1:\n#    print(porter.stem(word))","77b4bbaf":"#def preprocessing(text):\n#    li1 = [word.lower() for word in text.split()]\n#    li2 =  [word for word in li1 if word not in stopwords_en_withpunct]\n#    li3= [porter.stem(word) for word in li2]\n#    return ' '.join(li3)","05c9452c":"#df_train.text[2]","026e7e33":"#preprocessing(df_train.text[2])","6ed6bae1":"#df_train['cleaned_text'] = df_train['text'].apply(preprocessing)\n#df_test['cleaned_text'] =df_test['text'].apply(preprocessing)","c14a621d":"#df_train.head(100)","b2a01a47":"count_vectorizer = feature_extraction.text.CountVectorizer()\n\nexample_train_vectors = count_vectorizer.fit_transform(df_train['text'][0:5])","6e3ade78":"print(example_train_vectors[0])","783efb06":"print(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())","a27bc971":"train_vectors = count_vectorizer.fit_transform(df_train[\"text\"])\ntest_vectors = count_vectorizer.transform(df_test['text'])","4a20c796":"#making dataframe from the vector\n\nvect_train_df= pd.DataFrame(train_vectors.todense(),columns=count_vectorizer.get_feature_names())\nvect_test_df =pd.DataFrame(test_vectors.todense(),columns=count_vectorizer.get_feature_names())\n\n","e3b25e68":"vect_train_df['is_disaster_keyword'] = df_train['is_disaster_keyword']\nvect_train_df['is_nondisaster_keyword'] =df_train['is_non_disaster_keyword']\nvect_test_df['is_disaster_keyword'] = df_test['is_disaster_keyword']\nvect_test_df['is_nondisaster_keyword'] =df_test['is_non_disaster_keyword']","d43ba438":"vect_train_df=vect_train_df.join(df_train[new_features])\nvect_test_df=vect_test_df.join(df_train[new_features])","e64213f5":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train,y_test = train_test_split(vect_train_df,df_train.target,test_size=0.3,random_state=1)","47249d6d":"X_train.shape","cfeebb76":"from tensorflow import keras\nfrom tensorflow.keras import layers, callbacks\n\nearly_stopping = callbacks.EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=10, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.2),\n   # keras.layers.Dense(300, activation='relu'),\n   # keras.layers.Dropout(0.5),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dropout(0.6),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dropout(0.4),\n    keras.layers.Dense(2, activation=\"softmax\")\n])\n\n","16c0723b":"model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])","68f232e2":"X_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\nX_test = np.asarray(X_test)\ny_test = np.asarray(y_test)\n\nvect_test=np.asarray(vect_test_df)","a809fdab":"history = model.fit(\nX_train,y_train,\nvalidation_data =(X_test,y_test),\nbatch_size=512,\nepochs =1000,\ncallbacks=[early_stopping],\n)","829ddaea":"len(model.predict(X_test))\nlen(X_test)","c0c426fd":"y_pred = model.predict(X_test)\ny_pred = [a.argmax() for a in y_pred]\n\n\nprint(mean_squared_error(y_test,y_pred,squared=False))\nprint(roc_auc_score(y_test,y_pred))\n\n\n#0.4333917065178553\n#0.7976782008772676\n","4bfd3de3":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","688497d6":"vect_test= [a.argmax() for a in model.predict(vect_test)]\nsample_submission[\"target\"] =vect_test","560cfa1b":"sample_submission.to_csv(\"submission.csv\",index=False)","38bb04ee":"**Using keywords to make new feature**\n\nToo many unique values in the location column to use them as features compared with keywords.\nLet's see which keywords are useful to predict if a predict is a disaster or not.","7feb7726":"# **New Featuer Creations**\nI did not have any good ideas how to make new features from the data, but I found a really good notebook to refer for that. NLP with Disaster Tweets - EDA, Cleaning and BERT(https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert). \nI am going to use some ideas from  this notebook.","080f9d0f":"# **Building Vectors**\n\nUse sckit-learn's countvectorizer to count the words in each tweet and turn them into data that machine learning model can process.","70601cec":"# **Create Model**","c1440888":"# **Explore the Data**","854728d6":"# **My First Notebook of NLP**\n\nReferred to NLP Getting Started Tutorial(https:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial) and Natural Language Processing courses(https:\/\/www.kaggle.com\/learn\/natural-language-processing).\n\n\n\n\n","876a1629":"# **Text Cleaning**\n\nReferred to Basic NLP with NLTK(https:\/\/www.kaggle.com\/alvations\/basic-nlp-with-nltk\/notebook)."}}