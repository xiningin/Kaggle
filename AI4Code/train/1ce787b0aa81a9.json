{"cell_type":{"c6c9189a":"code","3cedb0be":"code","c3ca5052":"code","dac0b70b":"code","90c903ce":"code","4066d4ee":"code","71a55f08":"code","e75e136b":"code","a15d0666":"code","16f50274":"code","0dc7be24":"code","cbdad8ad":"code","d8afcefe":"code","6ff4f085":"code","aba313ea":"code","5a73bcef":"code","d287642f":"code","40f4740f":"code","958d2298":"code","62111a43":"code","4afcb971":"code","30aa1485":"code","553b1072":"code","9c901330":"code","753ed90f":"code","9769f012":"code","10f70a2a":"code","e753643c":"code","5e9bad81":"code","9f3d9ded":"code","f2d7d72d":"code","d5933a2d":"code","ccb5c029":"code","95fbae92":"code","f66fcf6f":"code","04701203":"code","8e4e036d":"code","86543e7c":"code","6bdf4e84":"code","fd73289d":"code","e231d314":"code","d6facefa":"code","298e1c30":"code","02f88059":"code","ae49a473":"code","65ee2733":"code","552c38cd":"code","2a512c5d":"code","bae23716":"code","c087f4e9":"code","1beb5fc9":"code","163337a1":"code","4c0f2787":"markdown","43091eeb":"markdown","b70ac4fe":"markdown","e31abae3":"markdown","70b7832e":"markdown","9f471c33":"markdown","6152783b":"markdown","facdaf17":"markdown","688a3684":"markdown","13141a35":"markdown","03eb3dd0":"markdown","b81ea985":"markdown","80b62e72":"markdown","efba72d4":"markdown","98479467":"markdown","d79ced97":"markdown","60b2e588":"markdown","da400a37":"markdown","039e9569":"markdown","f51e3302":"markdown","a300aba0":"markdown","12e6e6a0":"markdown","c26b1076":"markdown","2080ef49":"markdown","cf5c573c":"markdown","6d26d9f5":"markdown","c5d12271":"markdown","32f37d8a":"markdown","756abde7":"markdown","7c82f49d":"markdown","2af43d7b":"markdown","949552dd":"markdown"},"source":{"c6c9189a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\nimport missingno as msno\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.dates as mdates\nimport scipy.stats\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport pylab\nsns.set(style='white')\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport lightgbm as lgb","3cedb0be":"df=pd.read_csv('..\/input\/nifty50-stock-market-data\/RELIANCE.csv')","c3ca5052":"df.head()","dac0b70b":"df['Date']=pd.to_datetime(df['Date'])\ndf.set_index(['Date'],inplace=True)","90c903ce":"df.head()","4066d4ee":"df.describe()","71a55f08":"df.shape","e75e136b":"df.drop(['Trades','Deliverable Volume','%Deliverble'],axis=1,inplace=True)","a15d0666":"fig = go.Figure([go.Scatter(x=df.index, y=df['VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP over time',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","16f50274":"sns.kdeplot(df['VWAP'],shade=True)","0dc7be24":"fig = go.Figure([go.Scatter(x=df.loc['2019', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2019',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\n\nfig.show()","cbdad8ad":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'VWAP'].index,y=df.loc['2020', 'VWAP'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    title='VWAP in 2020',\n    template=\"simple_white\",\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"VWAP\")\nfig.show()","d8afcefe":"cols_plot = ['Open', 'Close', 'High','Low']\naxes = df[cols_plot].plot(figsize=(15, 15), subplots=True)\nfor ax in axes:\n    ax.set_ylabel('Daily trade')","6ff4f085":"fig = go.Figure([go.Scatter(x=df.index, y=df['Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume over time'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","aba313ea":"fig = go.Figure([go.Scatter(x=df.loc['2020', 'Volume'].index,y=df.loc['2020', 'Volume'])])\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=500,\n    template='simple_white',\n    title='Volume in 2020'\n)\nfig.update_xaxes(title=\"Date\")\nfig.update_yaxes(title=\"Volume\")\nfig.show()","5a73bcef":"scipy.stats.probplot(df.VWAP,plot=pylab)\npylab.show()","d287642f":"def dicky_fuller_test(x):\n    result = adfuller(x)\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n    if result[1]>0.05:\n        print(\"[-] Fail to reject the null hypothesis (H0), the data is non-stationary\")\n    else:\n        print(\"[+] Reject the null hypothesis (H0), the data is stationary.\")\n        \ndicky_fuller_test(df['VWAP'])","40f4740f":"from statsmodels.tsa.seasonal import seasonal_decompose\nfrom dateutil.parser import parse\n\nplt.rcParams.update({'figure.figsize': (10,10)})\ny = df['VWAP'].to_frame()\n\n\n# Multiplicative Decomposition \nresult_mul = seasonal_decompose(y, model='multiplicative',period = 52)\n\n# Additive Decomposition\nresult_add = seasonal_decompose(y, model='additive',period = 52)\n\n# Plot\nplt.rcParams.update({'figure.figsize': (15,15)})\nresult_mul.plot().suptitle('Multiplicative Decompose', fontsize=22)\nresult_add.plot().suptitle('Additive Decompose', fontsize=22)\nplt.show()","958d2298":"sm.graphics.tsa.plot_acf(df['VWAP'].iloc[1:], lags=40,title='auto correlation of VWAP',zero=False)\nplt.show()","62111a43":"df['vwap_diff']=df['VWAP']-df['VWAP'].shift(1)\nsm.graphics.tsa.plot_acf(df['vwap_diff'].iloc[7:], lags=40,title='auto correlation of difference VWAP',zero=False)\nplt.show()","4afcb971":"sm.graphics.tsa.plot_pacf(df['VWAP'].iloc[1:], lags=40,title='partial auto correlation of VWAP',zero=False)\nplt.show()","30aa1485":"sm.graphics.tsa.plot_pacf(df['vwap_diff'].iloc[1:], lags=40,title='partial autocorrelation of difference VWAP  ',zero=False)\nplt.show()","553b1072":"df.head()","9c901330":"df=df.reset_index()","753ed90f":"lag_features = [\"High\", \"Low\", \"Volume\", \"Turnover\",\"Close\"]\nwindow1 = 3\nwindow2 = 7\nwindow3 = 30\n\ndf_rolled_3d = df[lag_features].rolling(window=window1, min_periods=0)\ndf_rolled_7d = df[lag_features].rolling(window=window2, min_periods=0)\ndf_rolled_30d = df[lag_features].rolling(window=window3, min_periods=0)\n\ndf_mean_3d = df_rolled_3d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_7d = df_rolled_7d.mean().shift(1).reset_index().astype(np.float32)\ndf_mean_30d = df_rolled_30d.mean().shift(1).reset_index().astype(np.float32)\n\ndf_std_3d = df_rolled_3d.std().shift(1).reset_index().astype(np.float32)\ndf_std_7d = df_rolled_7d.std().shift(1).reset_index().astype(np.float32)\ndf_std_30d = df_rolled_30d.std().shift(1).reset_index().astype(np.float32)\n\nfor feature in lag_features:\n    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3d[feature]\n    df[f\"{feature}_mean_lag{window2}\"] = df_mean_7d[feature]\n    df[f\"{feature}_mean_lag{window3}\"] = df_mean_30d[feature]\n    \n    df[f\"{feature}_std_lag{window1}\"] = df_std_3d[feature]\n    df[f\"{feature}_std_lag{window2}\"] = df_std_7d[feature]\n    df[f\"{feature}_std_lag{window3}\"] = df_std_30d[feature]\n\ndf.fillna(df.mean(), inplace=True)\n\ndf.set_index(\"Date\", drop=False, inplace=True)","9769f012":"\ndf.Date = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\ndf[\"month\"] = df.Date.dt.month\ndf[\"week\"] = df.Date.dt.week\ndf[\"day\"] = df.Date.dt.day\ndf[\"day_of_week\"] = df.Date.dt.dayofweek\n","10f70a2a":"df.head()","e753643c":"\ndf_train = df[df.Date < \"2019\"]\ndf_valid = df[df.Date >= \"2019\"]\n\nexogenous_features = [\"High_mean_lag3\", \"High_std_lag3\", \"Low_mean_lag3\", \"Low_std_lag3\",\n                      \"Volume_mean_lag3\", \"Volume_std_lag3\", \"Turnover_mean_lag3\",\n                      \"Turnover_std_lag3\",\"High_mean_lag7\", \"High_std_lag7\", \"Low_mean_lag7\", \"Low_std_lag7\",\n                      \"Volume_mean_lag7\", \"Volume_std_lag7\", \"Turnover_mean_lag7\",\n                      \"Turnover_std_lag7\",\"High_mean_lag30\", \"High_std_lag30\", \"Low_mean_lag30\", \"Low_std_lag30\",\n                      \"Volume_mean_lag30\", \"Volume_std_lag30\", \"Turnover_mean_lag30\",\n                      \"Close_mean_lag3\", \"Close_mean_lag7\",\"Close_mean_lag30\",\"Close_std_lag3\",\"Close_std_lag7\",\"Close_std_lag30\",\n                      \"Turnover_std_lag30\",\"month\",\"week\",\"day\",\"day_of_week\"]\n","5e9bad81":"df_valid['Date'].describe()","9f3d9ded":"!pip install pmdarima","f2d7d72d":"from pmdarima import auto_arima","d5933a2d":"model = auto_arima(df_train.VWAP, exogenous=df_train[exogenous_features], trace=True, error_action=\"ignore\", suppress_warnings=True)\nmodel.fit(df_train.VWAP, exogenous=df_train[exogenous_features])\n\nforecast = model.predict(n_periods=len(df_valid), exogenous=df_valid[exogenous_features])\ndf_valid[\"Prediction_ARIMAX\"] = forecast","ccb5c029":"model.summary()","95fbae92":"df_valid[[\"VWAP\", \"Prediction_ARIMAX\"]].plot(figsize=(15, 10))","f66fcf6f":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_ARIMAX)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_ARIMAX))","04701203":"from fbprophet import Prophet\n\nmodel_fbp = Prophet()\nfor feature in exogenous_features:\n    model_fbp.add_regressor(feature)\n\nmodel_fbp.fit(df_train[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\", \"VWAP\": \"y\"}))\n\nforecast = model_fbp.predict(df_valid[[\"Date\", \"VWAP\"] + exogenous_features].rename(columns={\"Date\": \"ds\"}))\ndf_valid[\"Prediction_Prophet\"] = forecast.yhat.values","8e4e036d":"model_fbp.plot_components(forecast)","86543e7c":"df_valid[[\"VWAP\", \"Prediction_ARIMAX\", \"Prediction_Prophet\"]].plot(figsize=(15, 10))","6bdf4e84":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_Prophet)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_Prophet))","fd73289d":"params = {\"objective\": \"regression\"}\n\ndtrain = lgb.Dataset(df_train[exogenous_features], label=df_train.VWAP.values)\ndvalid = lgb.Dataset(df_valid[exogenous_features])\n\nmodel_lgb = lgb.train(params, train_set=dtrain)\n\nforecast = model_lgb.predict(df_valid[exogenous_features])\ndf_valid[\"Prediction_LightGBM\"] = forecast","e231d314":"df_valid[[\"VWAP\", \"Prediction_ARIMAX\", \"Prediction_Prophet\", \"Prediction_LightGBM\"]].plot(figsize=(15, 10))","d6facefa":"print(\"RMSE of Auto ARIMAX:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_ARIMAX)))\nprint(\"RMSE of Prophet:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_Prophet)))\nprint(\"RMSE of LightGBM:\", np.sqrt(mean_squared_error(df_valid.VWAP, df_valid.Prediction_LightGBM)))\nprint(\"\\nMAE of Auto ARIMAX:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_ARIMAX))\nprint(\"MAE of Prophet:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_Prophet))\nprint(\"MAE of LightGBM:\", mean_absolute_error(df_valid.VWAP, df_valid.Prediction_LightGBM))","298e1c30":"from sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom keras.callbacks import EarlyStopping\nfrom math import sqrt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras","02f88059":"fig = plt.figure(figsize = (17,25))\nax = fig.gca()\nhist=df.hist(ax = ax)","ae49a473":"dataset = df[exogenous_features].values\ndataset = dataset.astype('float32')\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\ntrain_size = int(len(dataset) * 0.80)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\nX_train = train[:,1:]\ny_train = train[:,0]\nX_test = test[:,1:]\ny_test = test[:,0]","65ee2733":"batch_size = 1\ntimesteps = 1\nunits = 100\nnb_epoch = 70","552c38cd":"X_train = X_train.reshape(X_train.shape[0],timesteps,X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0],timesteps,X_test.shape[1])","2a512c5d":"model = Sequential()\nmodel.add(LSTM(units,batch_input_shape=(batch_size, X_train.shape[1], X_train.shape[2]), stateful=True))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nhistory=model.fit(X_train, y_train,epochs=nb_epoch,batch_size=batch_size, validation_data=(X_test, y_test),callbacks=[EarlyStopping(monitor='val_loss', patience=21)],verbose=0,shuffle=False)","bae23716":"model.summary()","c087f4e9":"yhat = model.predict(X_test, batch_size=batch_size)\nrmse = sqrt(mean_squared_error(y_test, yhat))\nmae=mean_absolute_error(y_test, yhat)\nprint('rmse:{} MAE:{}'.format(rmse,mae))","1beb5fc9":"plt.figure(figsize=(8,4))\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Test Loss')\nplt.title('LSTM model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(loc='upper right')\nplt.show();","163337a1":"plt.figure(figsize=(8,4))\nplt.plot(y_test, marker='.', label=\"actual\")\nplt.plot(yhat, label=\"prediction\")\nplt.tick_params(left=False, labelleft=True)\nplt.tight_layout()\nsns.despine(top=True)\nplt.subplots_adjust(left=0.07)\nplt.ylabel('Vwap', size=15)\nplt.xlabel('points', size=15)\nplt.legend(fontsize=15)\nplt.show();","4c0f2787":"## VWAP in 2019","43091eeb":"### Stationarity\n\nIn the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time. It does not mean that the series does not change over time, just that the way it changes does not itself change over time. The algebraic equivalent is thus a linear function, perhaps, and not a constant one; the value of a linear function changes as \ud835\udc99 grows, but the way it changes remains constant \u2014 it has a constant slope; one value that captures that rate of change.\n\n### Dicky-Fuller Test\n\nThe Augmented Dickey-Fuller test is a type of statistical test called a unit root test.\n\nThe intuition behind a unit root test is that it determines how strongly a time series is defined by a trend\nIt uses an autoregressive model and optimizes an information criterion across multiple different lag values.\n\nThe null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.\n\n**Null Hypothesis (H0)**: If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n\n**Alternate Hypothesis (H1)**: The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n\nWe interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).\n\np-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\np-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","b70ac4fe":"Comparing all three models we can clearly see that Facebook Prophet performs the best in prediction!","e31abae3":"## VWAP in 2020","70b7832e":"# Features:\n1. Series: Here EQ stands for equity series of stock market.\n2. Prev Close: The closing price of the stock for the day before.\n3. Open,High, Low, Last, Close: The opening price, highest price, lowest price, last price and closing price of ICICI shares on the current day.\n4. **VWAP**: Volume Weighted Average Price,the **target variable** to predict. VWAP is a trading benchmark used by traders that gives the average price the stock has traded at throughout the day, based on both volume and price.\n5. Volume: Volume of shares traded on the current day.\n6. Turnover: It is a measure of stock liquidity calculated by dividing the total number of shares traded over a period by the average number of shares outstanding for the period. \n7. Trades: total number of trades on the current day.\n8. Deliverable Volume:  is the quantity of shares which actually move from one set of people to another set of people.\n9. Deliverable(%): Deliverable volume in percentage.","9f471c33":"Lets take a look at our model formed","6152783b":"## Plotting Auto Correlation Factor and Partial Auto Correlation Factor ","facdaf17":"## 3) LightGBM\nTime series problems are popularly converted into a tabular i.i.d. structure and fed into boosting models like [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/) and [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/).\n\nThere is loss of information in terms of knowing the order of data points in the time series but it can be circumvented by the datetime features to capture this information to some extent.\n\nNote that the default parameters are used for LightGBM. They can be tuned to improve the results.","688a3684":"* We can see pattern that after March of every year VWAP decreases\n\n* And after April of every year there is a steady increase","13141a35":"## Seasonal Decompose","03eb3dd0":"A **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.\n\nA **partial autocorrelation** is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.\n\nThe autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.\n\nIt is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.","b81ea985":"## Checking for Stationarity ","80b62e72":"## Plotting VWAP(Volume Weighted Average Price) over time","efba72d4":"<font size=\"+3\" color='#780404'><b> Feature Engineering <\/b><\/font>","98479467":"Let's see VWAP prices from 2019, 2020 and 2021","d79ced97":"<font size=\"+3\" color='#780404'><b>Exploratory Data Analysis<\/b><\/font>","60b2e588":"## Volume over Time","da400a37":"## Q-Q plot of VWAP \n\nIt is used to determine whether dataset is distributed a certain way ","039e9569":"## 1) Auto - ARIMA Model","f51e3302":"Adding lag values of High, Low, Volume,Turnover, will use three sets of lagged values, one previous day, \n\none looking back 7 days and another looking back 30 days as a proxy for last week and last month metrics.","a300aba0":"Dicky Fuller Test results in the data being non stationary","12e6e6a0":"## 2) Facebook Prophet\nProphet is an open-source time series model developed by Facebook. It was released in early 2017. An exerpt from the homepage:\n\n> Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.\n\nRead more about Prophet: https:\/\/facebook.github.io\/prophet\/\n\nI also shared a starter code [Prophet's Prophecy](https:\/\/www.kaggle.com\/rohanrao\/ashrae-prophet-s-prophecy) for using Prophet in the ASHRAE competition on Kaggle.\n\nNote that the default parameters are used for Prophet. They can be tuned to improve the results.","c26b1076":"**Autocorrelation** and **partial autocorrelation** plots are heavily used in time series analysis and forecasting.\n\nThese are plots that graphically summarize the strength of a relationship with an observation in a time series with observations at prior time steps.\n\n**Statistical correlation** summarizes the strength of the relationship between two variables.\n\nWe can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a **serial correlation, or an autocorrelation.**\n\nA plot of the autocorrelation of a time series by lag is called the AutoCorrelation Function, or the acronym ACF. This plot is sometimes called a **correlogram or an autocorrelation plot**.","2080ef49":"## Volume in 2020","cf5c573c":"# Models","6d26d9f5":"## Open,Close,High,Low prices over time ","c5d12271":"We can see that there is a steady increase in price till 2008 after that it fell and also fell again in 2009-10","32f37d8a":"## 4) LSTM","756abde7":"All type of stock prices follow the same pattern","7c82f49d":"* We can see that data is not normally distributed.\n\n* However this is what we usually expect from time-series ","2af43d7b":"### Removing Missing Colomns","949552dd":"We can see a cyclic pattern is shown in every 30 days (monthly)"}}