{"cell_type":{"92a94199":"code","bcc013dc":"code","8bc3ad56":"code","4aea5bc8":"code","8306630c":"code","7edc7c76":"code","f37276b6":"code","59056ffe":"code","49c92c5b":"code","8d3f44d7":"code","eb2772ef":"code","4f505e24":"code","d81eb684":"code","62998ca4":"code","99f0fad0":"code","649cfb82":"code","1f05ff7a":"code","5bf50139":"code","4585adfa":"code","491dbcf7":"code","7b18fa68":"code","e8ebf9d0":"code","0471015b":"code","5c93a9a3":"code","22b01b6f":"code","6af66c3c":"code","29037479":"code","a07c4a75":"code","b4517387":"code","e28bbd40":"code","2526da92":"code","68575a80":"code","23fd6970":"code","6505f926":"code","9558d12b":"code","3882a217":"code","a5c42377":"code","e4310309":"code","3ecb798d":"code","d3f69ea5":"code","66ee55e4":"code","c10d10cf":"code","c9db6c0f":"code","535746d6":"code","362d7469":"code","8942bad4":"code","1ced2a26":"code","0ce15e8e":"code","e56e421b":"code","abd0db2d":"code","1486e4ed":"code","79fb723f":"markdown","e15d425b":"markdown","8596e2a5":"markdown","91e1257d":"markdown","a9495e03":"markdown","5d212d95":"markdown","82972733":"markdown","7ddd2c8d":"markdown","960ca2ce":"markdown","d6d64ecf":"markdown","d965054c":"markdown","1e9abe38":"markdown","7a07e3c6":"markdown","71504d03":"markdown","ed0d0f11":"markdown","4a4013d7":"markdown","3367d7f6":"markdown","51c24f9d":"markdown"},"source":{"92a94199":"# Importing initail library for data exploratory\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n","bcc013dc":"# importing dataset\ndf = pd.read_csv(\"..\/input\/heart-diseases-prediction-classification\/heart.csv\")\ndf.head()","8bc3ad56":"df.info()","4aea5bc8":"# Let's see the target variable data\ndf.target.value_counts()","8306630c":"sns.countplot(x = \"target\", data=df, palette=\"bwr\")\nplt.xlabel(\"Heart Disease (0 = No, 1= Yes)\")\nplt.show()","7edc7c76":"# let's see the percentage of the taget value counts\nno_disease = len(df[df.target == 0])\nhave_disease = len(df[df.target == 1])\nprint(\"Percentage of Patient Haven't Infected : {:.2f}%\".format((no_disease \/ (len(df.target))*100)))\nprint(\"Percentage of Patient has been Infected : {:.2f}%\".format((have_disease \/ (len(df.target))*100)))","f37276b6":"df.age.value_counts()[:10]\n","59056ffe":"# Let's check the Age of data\npd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","49c92c5b":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","8d3f44d7":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","eb2772ef":"categorical_val = []\ncontinous_val = []\nfor column in df.columns:\n    print('==============================')\n    print(f\"{column} : {df[column].unique()}\")\n    if len(df[column].unique()) <= 10:\n        categorical_val.append(column)\n    else:\n        continous_val.append(column)","4f505e24":"categorical_val","d81eb684":"plt.figure(figsize=(15, 15))\n\nfor i, column in enumerate(categorical_val, 1):\n    plt.subplot(3, 3, i)\n    df[df[\"target\"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)\n    df[df[\"target\"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","62998ca4":"plt.figure(figsize=(15, 15))\n\nfor i, column in enumerate(continous_val, 1):\n    plt.subplot(3, 2, i)\n    df[df[\"target\"] == 0][column].hist(bins=35, color='blue', label='Have Heart Disease = NO', alpha=0.6)\n    df[df[\"target\"] == 1][column].hist(bins=35, color='red', label='Have Heart Disease = YES', alpha=0.6)\n    plt.legend()\n    plt.xlabel(column)","99f0fad0":"df.describe()","649cfb82":"# let's check in heatmap\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(16,9))\nsns.heatmap(df[top_corr_features].corr(), annot=True, cmap=\"RdYlGn\")","1f05ff7a":"# Checking null value\ndf.isnull().sum().sum()","5bf50139":"df.isnull().sum()","4585adfa":"df.info()","491dbcf7":"df.head()","7b18fa68":"# Before creating any other features, firstly copy original dataset into other dataframe\ndf1 = df.copy()\ndf1.head()","e8ebf9d0":"df2 = pd.get_dummies(df1, columns=['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal'], dummy_na=False, drop_first=True)\ndf2.head()","0471015b":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndf2[columns_to_scale] = ss.fit_transform(df2[columns_to_scale])","5c93a9a3":"df2.head()","22b01b6f":"X = df2.drop(['target'], axis=1)\nY = df2.target.values","6af66c3c":"print(X.shape)\nprint(Y.shape)","29037479":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size = 0.2,random_state=42)","a07c4a75":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b4517387":"x_test.T","e28bbd40":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred = clf.predict(x_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(x_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","2526da92":"from sklearn.linear_model import LogisticRegression\n\n\n\nlr_clf = LogisticRegression()\nlr_clf.fit(x_train,y_train)\nlr_score = lr_clf.score(x_test,y_test)*100\n\n\nprint(\"Test Accuracy {:.2f}%\".format(lr_score))\nprint_score(lr_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(lr_clf, x_train, y_train, x_test, y_test, train=False)","68575a80":"y_train.reshape(1,-1)","23fd6970":"test_score = accuracy_score(y_test, lr_clf.predict(x_test)) * 100\nresults_df = pd.DataFrame(data=[[\"Logistic Regression\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df","6505f926":"lr_clf.predict(x_test)\n","9558d12b":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_clf = KNeighborsClassifier(n_neighbors = 2)\nknn_clf.fit(x_train, y_train)\nknn_score = knn_clf.score(x_test,y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(knn_score))\n\n\nprint_score(knn_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(knn_clf, x_train, y_train, x_test, y_test, train=False)","3882a217":"test_score = accuracy_score(y_test, knn_clf.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"KNeighborsClassifier\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","a5c42377":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train, y_train)\n    scoreList.append(knn2.score(x_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","e4310309":"from sklearn.svm import SVC\n\n\nsvm_clf = SVC(kernel='rbf', gamma=0.1, C=1.0)\nsvm_clf.fit(x_train, y_train)\nsvm_score = svm_clf.score(x_test,y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(svm_score))\n\nprint_score(svm_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(svm_clf, x_train, y_train, x_test, y_test, train=False)\n","3ecb798d":"test_score = accuracy_score(y_test, svm_clf.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"Support Vector Machine\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","d3f69ea5":"from sklearn.tree import DecisionTreeClassifier\n\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(x_train, y_train)\ntree_score  = tree_clf .score(x_test,y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(tree_score ))\n\nprint_score(tree_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(tree_clf, x_train, y_train, x_test, y_test, train=False)\n","66ee55e4":"test_score = accuracy_score(y_test, tree_clf.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"Decision Tree Classifier\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","c10d10cf":"from sklearn.ensemble import RandomForestClassifier\n\n\nrf_clf = RandomForestClassifier(n_estimators=1000, random_state=42)\nrf_clf.fit(x_train, y_train)\nrf_score  = rf_clf.score(x_test,y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(rf_score ))\n\nprint_score(rf_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(rf_clf, x_train, y_train, x_test, y_test, train=False)","c9db6c0f":"test_score = accuracy_score(y_test, rf_clf.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"Random Forest Classifier\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","535746d6":"from xgboost import XGBClassifier\n\nxgb_clf = XGBClassifier()\nxgb_clf.fit(x_train, y_train)\nxgb_score  = xgb_clf.score(x_test,y_test)*100\nprint(\"Test Accuracy {:.2f}%\".format(xgb_score ))\n\nprint_score(xgb_clf, x_train, y_train, x_test, y_test, train=True)\nprint_score(xgb_clf, x_train, y_train, x_test, y_test, train=False)","362d7469":"test_score = accuracy_score(y_test, nb.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"XGB Classifier\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","8942bad4":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\n\nnb_score = nb.score(x_test,y_test)*100\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(nb_score))\n\nprint_score(nb, x_train, y_train, x_test, y_test, train=True)\nprint_score(nb, x_train, y_train, x_test, y_test, train=False)","1ced2a26":"test_score = accuracy_score(y_test, nb.predict(x_test)) * 100\nresults_df2 = pd.DataFrame(data=[[\"Naive Bias Classifier\", test_score]], \n                          columns=['Model','Testing Accuracy %'])\nresults_df = results_df.append(results_df2, ignore_index=True)\nresults_df","0ce15e8e":"accuracies = {}\naccuracies['Logistic Regression'] = lr_clf\naccuracies['KNeighbors Classifier'] = knn_clf\naccuracies['Support Vector Machine'] = svm_clf \naccuracies['Decision Tree'] = tree_clf\naccuracies['Random Forest'] = rf_clf \naccuracies['XGB Classifier'] = xgb_clf\naccuracies['Naive Bayes'] = nb\n\ncolors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\",\"red\"]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","e56e421b":"# Predicted values\ny_head_lr = lr_clf.predict(x_test)\ny_head_knn = knn_clf.predict(x_test)\ny_head_svm = svm_clf.predict(x_test)\ny_head_nb = nb.predict(x_test)\ny_head_dtc = tree_clf.predict(x_test)\ny_head_rf =  rf_clf.predict(x_test)\ny_head_XGB =  xgb_clf.predict(x_test)\n","abd0db2d":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rf = confusion_matrix(y_test,y_head_rf)\ncm_xgb = confusion_matrix(y_test,y_head_XGB)","1486e4ed":"plt.figure(figsize=(24,15))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(3,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(3,3,7)\nplt.title(\"XGB Classifier Confusion Matrix\")\nsns.heatmap(cm_xgb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\n\nplt.show()\n\n","79fb723f":"- trestbps : resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n- chol {serum cholestoral in mg\/dl} : above 200 is cause for concern.\n- thalach {maximum heart rate achieved} : People how acheived a maximum more than 140 are more likely to have heart disease.\n- oldpeak ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more","e15d425b":"- cp {Chest Pain} : People with cp equl to 1, 2, 3 are more likely to have heart disease than people with cp equal to 0.\n- restecg {resting electrocardiographic results} : People with value 1 (signals non-normal heart beat, can range from mild symptoms to severe problems) are more likely to have heart disease.\n- exang {exercise induced angina} : People with value 0 (No ==> exercice induced angina) have heart disease more than people with value 1 (Yes ==> exercice induced angina)\n- slope {the slope of the peak exercise ST segment} : People with slope value equal to 2 (Downslopins: signs of unhealthy heart) are more likely to have heart disease than people with slope value equal to 0 (Upsloping: better heart rate with excercise) or 1 (Flatsloping: minimal change (typical healthy heart)).\n- ca {number of major vessels (0-3) colored by flourosopy} : the more blood movement the better so people with ca equal to 0 are more likely to have heart disease.\n- thal {thalium stress result} : People with thal value equal to 2 (fixed defect: used to be defect but ok now) are more likely to have heart disease.","8596e2a5":"## XGBoost Classifer","91e1257d":"## Split data for Training & Testing\n\nWe will split our data. 80% of our data will be train data and 20% of it will be test data.","a9495e03":"## Random Forest","5d212d95":"Data Contain ;\n\n- age - age in years\n- sex - (1 = male; 0 = female)\n- cp - The chest pain experienced (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic)\n- trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n- chol - serum cholestoral in mg\/dl\n- fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n- restecg - esting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria)\n- thalach - maximum heart rate achieved\nexang - exercise induced angina (1 = yes; 0 = no)\n- oldpeak - ST depression induced by exercise relative to rest ('ST' relates to positions on the ECG plot.)\n- slope - the slope of the peak exercise ST segment (Value 1: upsloping, Value 2: flat, Value 3: downsloping)\n- ca - number of major vessels (0-3) colored by flourosopy\n- thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n- target - have disease or not (1=yes, 0=no)\n\n**As we can clearly see that -Target- is our dependant variable.**","82972733":"## Confusion Matrix","7ddd2c8d":"## K-nearest neighbors","960ca2ce":"## Feature Scaling\nLet's normalized the imbalaced values of variable","d6d64ecf":"## Statistics & Null Value","d965054c":"## Comparison Model","1e9abe38":"## Naive Bayes Algorithm","7a07e3c6":"## Logistic Regression","71504d03":"## Program to Show the Model Accuracy Chart","ed0d0f11":"# Heart Disease Prediction - All Classifications Model Classified\n\nWe are going to learn about all the classifications alogorithms uses scalability along with percentage of accuracy strength.\n\nWe will be going to analyse Heart Diseases Dataset to predict wheather the Patient has certain diseases or not.\n\n","4a4013d7":"## Creating Dummy Variable for categorical variable","3367d7f6":"## Decision Tree Classifier","51c24f9d":"## Support Vector machine"}}