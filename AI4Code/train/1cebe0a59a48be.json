{"cell_type":{"3ceb13c2":"code","18ba024d":"code","864cf26e":"code","b7456a33":"code","f3972cdd":"code","31483db4":"code","51fc284b":"code","3bb189c2":"code","ebd46df9":"code","274ea821":"code","0677194f":"code","2959bbc6":"code","b7f3fe66":"code","0d089235":"code","4c4f79fc":"code","8962dcc9":"code","4cb2c4a3":"code","6fddfaaa":"code","4711d5ce":"code","5413dc52":"code","007b2b01":"code","dd16265d":"code","0bd43956":"code","54e72d57":"code","25f0ad29":"code","a22181db":"code","9a205d7a":"code","57e6e2c7":"code","fc343921":"code","f27af34f":"code","ad3dc9fd":"code","79550f83":"code","34574dbb":"code","d861a034":"code","c7d201f2":"markdown","0c59b603":"markdown","460da831":"markdown","f644c1a0":"markdown","9e957b42":"markdown","a17cf54f":"markdown","14ae88d3":"markdown"},"source":{"3ceb13c2":"import spacy\n\nimport pandas as pd\nimport os\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\n# fix random seed for reproducibility\nnp.random.seed(7)\n\n## For basic data preproc: https:\/\/realpython.com\/python-keras-text-classification\/\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.utils.np_utils import to_categorical # https:\/\/stackoverflow.com\/questions\/36927025\/how-to-use-keras-multi-layer-perceptron-for-multi-class-classification\n\nfrom keras.models import Sequential\nfrom keras import layers\n\nfrom keras.layers import Embedding,GlobalMaxPool1D,Dense, Conv1D, LSTM, Dropout, SpatialDropout1D, Input, merge, Multiply, Dot\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Bidirectional, MaxPooling1D\nfrom keras import regularizers, optimizers\n\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.initializers import Constant\nfrom keras.models import Model\nfrom keras import optimizers\n\npd.set_option('max_colwidth',400)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport re\n\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\n#nltk.download('stopwords')\nfrom sklearn import metrics\n\n## https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\nfrom keras.layers import Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nprint(os.listdir(\"..\/input\"))","18ba024d":"TARGET_COL = \"l1\"\n\nmaxlen = 80\nEMBEDDING_DIR = \"conceptnet-numberbatch-vectors\"\nEMBEDDING_FILE = \"numberbatch-en-17.06.txt\"\nEMBEDDING_DIM = 200\n\n# TRAIN_FILE = os.path.join(\"..\/input\",\"dbpedia_train.csv\")\n# VAL_FILE = os.path.join(\"..\/input\",\"dbpedia_val.csv\")\n\nTRAIN_FILE = \"..\/input\/dbpedia-classes\/DBPEDIA_train.csv\"\nVAL_FILE = \"..\/input\/dbpedia-classes\/DBPEDIA_val.csv\"","864cf26e":"## https:\/\/realpython.com\/python-keras-text-classification\/\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","b7456a33":"### https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\nclass Position_Embedding(Layer):\n    \n    def __init__(self, size=None, mode='sum', **kwargs):\n        self.size = size\n        self.mode = mode\n        super(Position_Embedding, self).__init__(**kwargs)\n        \n    def call(self, x):\n        if (self.size == None) or (self.mode == 'sum'):\n            self.size = int(x.shape[-1])\n        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n        position_j = 1. \/ K.pow(10000., \\\n                                 2 * K.arange(self.size \/ 2, dtype='float32' \\\n                               ) \/ self.size)\n        position_j = K.expand_dims(position_j, 0)\n        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 \n        position_i = K.expand_dims(position_i, 2)\n        position_ij = K.dot(position_i, position_j)\n        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n        if self.mode == 'sum':\n            return position_ij + x\n        elif self.mode == 'concat':\n            return K.concatenate([position_ij, x], 2)\n        \n    def compute_output_shape(self, input_shape):\n        if self.mode == 'sum':\n            return input_shape\n        elif self.mode == 'concat':\n            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n\n\n'''\noutput dimention: [batch_size, time_step, nb_head*size_per_head]\nevery word can be represented as a vector [nb_head*size_per_head]\n'''\nclass Attention(Layer):\n\n    def __init__(self, nb_head, size_per_head, **kwargs):\n        self.nb_head = nb_head\n        self.size_per_head = size_per_head\n        self.output_dim = nb_head*size_per_head\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.WQ = self.add_weight(name='WQ', \n                                  shape=(input_shape[0][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WK = self.add_weight(name='WK', \n                                  shape=(input_shape[1][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV', \n                                  shape=(input_shape[2][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        super(Attention, self).build(input_shape)\n        \n    def Mask(self, inputs, seq_len, mode='mul'):\n        if seq_len == None:\n            return inputs\n        else:\n            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n            mask = 1 - K.cumsum(mask, 1)\n            for _ in range(len(inputs.shape)-2):\n                mask = K.expand_dims(mask, 2)\n            if mode == 'mul':\n                return inputs * mask\n            if mode == 'add':\n                return inputs - (1 - mask) * 1e12\n                \n    def call(self, x):\n        if len(x) == 3:\n            Q_seq,K_seq,V_seq = x\n            Q_len,V_len = None,None\n        elif len(x) == 5:\n            Q_seq,K_seq,V_seq,Q_len,V_len = x\n        Q_seq = K.dot(Q_seq, self.WQ)\n        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n        K_seq = K.dot(K_seq, self.WK)\n        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n        V_seq = K.dot(V_seq, self.WV)\n        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) \/ self.size_per_head**0.5\n        A = K.permute_dimensions(A, (0,3,2,1))\n        A = self.Mask(A, V_len, 'add')\n        A = K.permute_dimensions(A, (0,3,2,1))    \n        A = K.softmax(A)\n        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n        O_seq = self.Mask(O_seq, Q_len, 'mul')\n        return O_seq\n        \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], input_shape[0][1], self.output_dim)","f3972cdd":"train = pd.read_csv(TRAIN_FILE,usecols=[\"text\",TARGET_COL]).sample(frac=0.12) # downsample for speed\nval =  pd.read_csv(VAL_FILE,usecols=[\"text\",TARGET_COL])\nprint(train.shape)\nprint(val.shape)\ntrain.tail()","31483db4":"N_CLASSES = train[TARGET_COL].nunique()","51fc284b":"encoder = LabelEncoder()\ny_train = encoder.fit_transform(train[TARGET_COL])\ny_val = encoder.transform(val[TARGET_COL]) # transform, not fit! \n\n\n\n# # onehot encode ### fix these if changing from train_labels to y_train .. \n# onehot_encoder = OneHotEncoder(sparse=False)\n# integer_encoded = integer_encoded.reshape(len(train_labels), 1)\n# onehot_encoded = onehot_encoder.fit_transform(train_labels)","3bb189c2":"## https:\/\/stackoverflow.com\/questions\/36927025\/how-to-use-keras-multi-layer-perceptron-for-multi-class-classification\n\n# y_train = to_categorical(train[TARGET_COL])\n# y_val = to_categorical(val[TARGET_COL])\n\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)","ebd46df9":"# lb = LabelBinarizer()\n# y_train = lb.fit_transform(train[TARGET_COL])\n\n# y_val = lb.transform(val[TARGET_COL])","274ea821":"sentences_train = train[\"text\"].values\nsentences_val = val[\"text\"].values","0677194f":"## https:\/\/realpython.com\/python-keras-text-classification\/\n\ntokenizer = Tokenizer(num_words=251234)\ntokenizer.fit_on_texts(sentences_train)","2959bbc6":"X_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(sentences_train[2])\nprint(X_train[2])","b7f3fe66":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","0d089235":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n","4c4f79fc":"embeddings_index = {}\n\nf = open(os.path.join(EMBEDDING_DIR, EMBEDDING_FILE))\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:(EMBEDDING_DIM+1)], dtype='float32') # take first 100 dims only\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","8962dcc9":"## At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","4cb2c4a3":"## define pretrained embedding layer, not trainable\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)","6fddfaaa":"## set up callbacks and optimizer defaults\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=3, min_lr=0.004)\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5,restore_best_weights=True)\n# model.fit(X_train, Y_train, callbacks=[reduce_lr])\n\n\nadam = optimizers.Adam(lr=0.07,decay=0.0005)","4711d5ce":"#### https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\n\nconfig = {\n    \"trainable\": False,\n    \"max_len\": maxlen,\n    \"max_features\": 95000,\n    \"embed_size\": EMBEDDING_DIM,\n    \"units\": 128,\n    \"num_heads\": 8,\n    \"dr\": 0.1,\n    \"epochs\": 2,\n    \"model_checkpoint_path\": \"best_weights\",\n}\ndef build_model(config):\n    inp = Input(shape = (config[\"max_len\"],))\n    \n    x = embedding_layer(inp)\n    x = Position_Embedding()(x)\n    x = Attention(config[\"num_heads\"], config[\"units\"])([x, x, x])  #output: [batch_size, time_step, nb_head*size_per_head]\n    x = GlobalAveragePooling1D()(x)\n#     x = Dropout(config[\"dr\"])(x)\n    \n    x = Dense(1, activation='sigmoid')(x)\n    x = Dense(N_CLASSES, activation='softmax')(x)\n    \n    # # model.add(layers.Dense(N_CLASSES, activation='relu'))\n# model.add(layers.Dense(N_CLASSES, activation='softmax')) # Instead of 1 sigmoid output (binary classification)\n# model.compile(optimizer='adam',\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(\n        loss = \"categorical_crossentropy\", \n        #optimizer = Adam(lr = config[\"lr\"], decay = config[\"lr_d\"]), \n        optimizer = \"adam\",\n        metrics = [\"accuracy\"])\n    \n    return model\n\nmodel = build_model(config)\n\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=4,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=64)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","5413dc52":"## attention based biLSTM:\n### https:\/\/www.kaggle.com\/danofer\/different-embeddings-with-attention-fork\n## https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-672-lb\n## use GRU for now\n\n\n# https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n    \n\ninp = Input(shape=(maxlen,))\nx = embedding_layer(inp)\nx = Bidirectional(GRU(100, return_sequences=True))(x)\nx = Bidirectional(GRU(50, return_sequences=True))(x)\nx = Attention(maxlen)(x)\nx = Dense(64, activation=\"relu\")(x)\nx = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs = inp, outputs = x)\nmodel.compile(\n    loss = \"categorical_crossentropy\", \n    #optimizer = Adam(lr = config[\"lr\"], decay = config[\"lr_d\"]), \n    optimizer = \"adam\",\n    metrics = [\"accuracy\"])\n\n\n# model = build_model(config)\n\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=4,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=64)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n","007b2b01":"maxlen=70","dd16265d":"print (keras.__version__)","0bd43956":"### https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention\n\n\n## https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention\n\ndef attention_3d_block(inputs, name):\n    # inputs.shape = (batch_size, time_steps, input_dim)\n    print(\"inputs.shape[1].value\",inputs.shape[1].value)\n    TIME_STEPS = inputs.shape[1].value\n    SINGLE_ATTENTION_VECTOR = False\n    \n    input_dim = int(inputs.shape[2])\n    print(\"input_dim.shape\",input_dim.shape)\n    a = Permute((2, 1))(inputs)\n    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n    a = Dense(TIME_STEPS, activation='softmax')(a)\n    if SINGLE_ATTENTION_VECTOR:\n        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n        a = RepeatVector(input_dim)(a)\n    a_probs = Permute((2, 1), name=name)(a)\n    output_attention_mul = Multiply()([inputs, a_probs])\n    return output_attention_mul\n\n\ndef model1(init):\n    x = init\n    x = Conv1D(64, 3,strides=2,padding='same',activation='relu')(x)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x) #CuDNN\n    x = attention_3d_block(x, 'attention_vec_1')\n    x = Bidirectional(LSTM(64, return_sequences=True))(x) #CuDNN\n    x = attention_3d_block(x, 'attention_vec_2')\n    x = GlobalMaxPool1D()(x)\n    out = Dense(64, activation=\"relu\")(x)\n    return out\n\ndef m2_block(init, filter, kernel, pool):\n    x = init\n    \n    x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n    skip = x\n    x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n    x = Conv1D(filter, kernel, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = Add()([x, skip])\n    x = Activation('elu')(x) # vs relu\n    x = MaxPooling1D(pool)(x)\n    \n    x = Flatten()(x)\n    x = BatchNormalization()(x)\n    \n    return x\n\ndef model2(init):\n    #init = Reshape((maxlen, embed_size, 1))(init)\n    \n    # pool = maxlen - filter + 1\n    x0 = m2_block(init, 32, 1, maxlen - 1 + 1)\n    x1 = m2_block(init, 32, 2, maxlen - 2 + 1)\n    x2 = m2_block(init, 32, 3, maxlen - 3 + 1)\n    x3 = m2_block(init, 32, 5, maxlen - 5 + 1)\n    \n    x = concatenate([x0, x1, x2, x3])\n    x = Dropout(0.1)(x)  # vs 0.5\n    out = Dense(64, activation=\"relu\")(x)\n    return out\n\n\ndef get_model():\n    inp = Input(shape=(maxlen, ))\n    #x = Embedding(max_features, embed_size)(inp)\n#     x = Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False)(inp)\n    x = embedding_layer(inp)\n    \n    out1 = model1(x)\n    out2 = model2(x)\n    \n    conc = concatenate([out1, out2])\n    \n    #conc = out1\n    x = Dropout(0.15)(conc)  # vs 0.5\n    x = Dense(64, activation='elu')(x) # relu\n    x = Reshape((x.shape[1].value, 1))(x)\n#     x = CuDNNLSTM(32)(x)\n    x = LSTM(32)(x)\n\n    x = Dense(64, activation=\"elu\")(x) # relu\n    outp = Dense(N_CLASSES, activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['acc'])    \n\n    return model\n\n# model = get_model()\n\n#################\ninp = Input(shape=(maxlen, ))\n#x = Embedding(max_features, embed_size)(inp)\n#     x = Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False)(inp)\nx = embedding_layer(inp)\n\nout1 = model1(x)\nout2 = model2(x)\n\nconc = concatenate([out1, out2])\n\n#conc = out1\nx = Dropout(0.15)(conc)  # vs 0.5\nx = Dense(64, activation='elu')(x) # relu\nx = Reshape((x.shape[1].value, 1))(x)\n#     x = CuDNNLSTM(32)(x)\nx = LSTM(32)(x)\n\nx = Dense(64, activation=\"elu\")(x) # relu\noutp = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs=inp, outputs=outp)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])    \n#################\n\n\nmodel.summary()\n\n\n\nhistory = model.fit(X_train, y_train,\n                    epochs=2,\n                    verbose=1,\n#                     validation_split=0.1,\n                    batch_size=128)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n","54e72d57":"# ### huge speed difference if embedding is not trainable..\n\n# # embedding_dim = 80\n\n# model = Sequential()\n# # model.add(layers.Dropout(0.2))\n# model.add(embedding_layer)\n# model.add(layers.GlobalMaxPool1D())\n# model.add(layers.Flatten()) # gives error\n# # model.add(layers.Dropout(0.35))\n# model.add(layers.Dense(128, activation='relu'))\n# # model.add(layers.Dense(N_CLASSES, activation='relu'))\n# model.add(layers.Dense(N_CLASSES, activation='softmax')) # Instead of 1 sigmoid output (binary classification)\n# model.compile(optimizer='adam',\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n# model.summary()","25f0ad29":"# ### model train + history + run\n\n# history = model.fit(X_train, y_train,\n#                     epochs=5,\n#                     verbose=1,\n# #                     validation_data=(X_test, y_test),\n#                     validation_split=0.1,\n#                     batch_size=128)\n# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n# loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n# plot_history(history)\n\n\n# score = model.evaluate(X_val, y_val,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])","a22181db":"## basic 1D CNN:\n\n# embedding_dim = 80\n\nmodel = Sequential()\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen,\n#                           trainable=False))\nmodel.add(embedding_layer)\n\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Conv1D(128, 5, activation='relu'))\n# model.add(layers.GlobalMaxPooling1D())# this being activated gives bug..\n# model.add(layers.Conv1D(32, 5, activation='relu')) # added\nmodel.add(layers.GlobalMaxPooling1D()) # added\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(32, activation='relu'))\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Dense(1, activation='sigmoid')) #orig\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=9,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[reduce_lr,earlystop])\n\nmodel.summary()\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","9a205d7a":"## Another  1D CNN:\n##TODO: use pretrained embeddings : https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py\n\n\n# embedding_dim = 50\n\nmodel = Sequential()\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen\n#                           ,trainable=False\n#                           ))\n\nmodel.add(embedding_layer)\n\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Conv1D(128, 5, activation='relu'))\n\nmodel.add(MaxPooling1D(2))\n# model.add(layers.Dropout(0.1))\n\nmodel.add(layers.Conv1D(64, 5, activation='relu'))\n\n# model.add(MaxPooling1D(2))\n# model.add(layers.Dropout(0.1))\n\n# model.add(layers.Conv1D(64, 5, activation='relu'))\n\n# model.add(layers.GlobalMaxPooling1D())# this being activated gives bug..\n# model.add(layers.Conv1D(32, 5, activation='relu')) # added\nmodel.add(layers.GlobalMaxPooling1D()) # added\n# model.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(32, activation='relu'))\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Dense(1, activation='sigmoid')) #orig\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[reduce_lr,earlystop])\n\nmodel.summary()\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","57e6e2c7":"## Another  1D CNN:\n##TODO: use pretrained embeddings : https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py\n\n\n\nmodel = Sequential()\n# model.add(layers.Dropout(0.1))\nmodel.add(embedding_layer)\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Conv1D(128, 4, activation='relu',padding='valid'))\n\n# model.add(MaxPooling1D(2))\n# model.add(layers.Dropout(0.1))\n\nmodel.add(layers.Conv1D(64, 6, activation='relu'))\n\n# model.add(MaxPooling1D(2))\n# # model.add(layers.Dropout(0.1))\n\n# model.add(layers.Conv1D(64, 4, activation='relu'))\n# # model.add(MaxPooling1D(2))\n\n# model.add(layers.Conv1D(32, 5, activation='relu')) # added\n\n# model.add(layers.GlobalMaxPooling1D()) # added\n\nmodel.add(layers.Flatten())\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(256, activation='relu'))\n# model.add(layers.Dense(128, activation='relu'))\n# model.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=25,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=64\n                   , callbacks=[reduce_lr,earlystop])\n\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","fc343921":"## imdb cnn-lstm example\n## https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_cnn_lstm.py\n","f27af34f":"## bi-lstm\n##https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_bidirectional_lstm.py\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(50)))\n# model.add(Dropout(0.2))\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\n\n# try using different optimizers and different optimizer configs\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n\nmodel.summary()\n\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=15,\n                    verbose=1,\n                    validation_split=0.1,\n                    batch_size=64\n                   , callbacks=[reduce_lr,earlystop])\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n","ad3dc9fd":"## defaultish network https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(128, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = MaxPooling1D(5)(x)\nx = Conv1D(128, 5, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)  # global max pooling\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\npreds = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, verbose=1,\n                    validation_split=0.1,\n          epochs=8, batch_size=128)\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n","79550f83":"maxlen","34574dbb":"# #### attention \n# ## https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n### older keras version, update to use merge\/multiply layer\n\n# inputs = Input(shape=(input_dim,))\n\n# # ATTENTION PART STARTS HERE\n# attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n# attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n# # ATTENTION PART FINISHES HERE\n\n# attention_mul = Dense(64)(attention_mul)\n# output = Dense(1, activation='sigmoid')(attention_mul)\n# model = Model(input=[inputs], output=output)\n\n\n\n### https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n# Here merged is actually a layer so first you're creating a Multiply object and then calling it. It would be equivalent to this:\nimport keras\nmultiply_layer = keras.layers.Multiply()\n# multiply_layer = keras.layers.dot()\n# merged = multiply_layer([layer1, layer2])\n\n\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nseq_v = keras.layers.Flatten()(embedded_sequences)\nattention_probs = Dense(10500, activation='softmax', name='attention_vec')(seq_v)\n# attention_mul = merge([embedded_sequences, attention_probs], output_shape=maxlen, name='attention_mul', mode='mul')\n# https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# attention_mul = multiply_layer([embedded_sequences, attention_probs])\n# attention_mul = keras.layers.dot([seq_v, attention_probs],axes=0,normalize=False)\n\nattention_mul = multiply_layer([seq_v, attention_probs])\n\n\n# attention_mul = Flatten(attention_mul)\nattention_mul = Dense(64)(attention_mul)\n\npreds = Dense(N_CLASSES, activation='softmax')(attention_mul)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()\n\nmodel.fit(X_train, y_train, verbose=1,\n                    validation_split=0.1,\n          epochs=3, batch_size=128)\n","d861a034":"# #### attention \n# ## https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n### older keras version, update to use merge\/multiply layer\n\n# inputs = Input(shape=(input_dim,))\n\n# # ATTENTION PART STARTS HERE\n# attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n# attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n# # ATTENTION PART FINISHES HERE\n\n# attention_mul = Dense(64)(attention_mul)\n# output = Dense(1, activation='sigmoid')(attention_mul)\n# model = Model(input=[inputs], output=output)\n\n\n\n### https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n# Here merged is actually a layer so first you're creating a Multiply object and then calling it. It would be equivalent to this:\nimport keras\nmultiply_layer = keras.layers.Multiply()\n# multiply_layer = keras.layers.dot()\n# merged = multiply_layer([layer1, layer2])\n\n\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nseq_v = embedding_layer(sequence_input)\n# seq_v = keras.layers.Flatten()(seq_v)\nattention_probs = Dense(maxlen, activation='softmax', name='attention_vec')(seq_v)\n# attention_mul = merge([embedded_sequences, attention_probs], output_shape=maxlen, name='attention_mul', mode='mul')\n# https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# attention_mul = multiply_layer([embedded_sequences, attention_probs])\n# attention_mul = keras.layers.dot([seq_v, attention_probs],axes=0,normalize=False)\n\nattention_mul = multiply_layer([seq_v, attention_probs])\n\n\n# attention_mul = Flatten(attention_mul)\nattention_mul = Dense(64)(attention_mul)\n\npreds = Dense(N_CLASSES, activation='softmax')(attention_mul)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel.summary()\n\nmodel.fit(X_train, y_train, verbose=1,\n                    validation_split=0.1,\n          epochs=3, batch_size=128)\n","c7d201f2":"## Models\n\n* start with fastText like:\n\n* use early stopping and maybe LR plateau : https:\/\/keras.io\/callbacks\/#reducelronplateau","0c59b603":"#### not attention models: ","460da831":"#### DBPedia classification\n\n* Note: needs adding of pretrained word vectors\n* Kernel uses downsampled data by default\n* not  all code snippets working\n\n### external resources:\n* https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n* XLNet\n* BERT\n* Keras examples on IMDB: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_cnn_lstm.py , https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_bidirectional_lstm.py\n* more imdb: https:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n\n\n* Example \"Wide anddeep\" model (e.g. categorical variables, or OHE BOW text + embeddings learn). : https:\/\/colab.research.google.com\/github\/sararob\/keras-wine-model\/blob\/master\/keras-wide-deep.ipynb\n\n\n* naive Attention (lstm, dense): https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n\n* https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\n* attention based biLSTM:\n    * https:\/\/www.kaggle.com\/danofer\/different-embeddings-with-attention-fork\n    * https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-672-lb\n\n* https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention # not working here? ","f644c1a0":"### callbacks and hyperparams: ","9e957b42":"## Pretrained embeddings\n* code from : https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n* start with conceptnet en : https:\/\/github.com\/commonsense\/conceptnet-numberbatch\nhttps:\/\/conceptnet.s3.amazonaws.com\/downloads\/2017\/numberbatch\/numberbatch-en-17.06.txt.gz","a17cf54f":"#### Process target col + tokenize + pad sequences\n* https:\/\/realpython.com\/python-keras-text-classification\/\n* Or use labelBinarizer for multiclass? (seems simpler):\n    * https:\/\/stackoverflow.com\/a\/50502803\/1610518","14ae88d3":"## Transform text data\n* Preprocess\n*max seq_len\n"}}