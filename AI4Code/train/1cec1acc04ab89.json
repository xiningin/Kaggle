{"cell_type":{"29917b89":"code","6d6d0821":"code","7cca0311":"code","a1e89947":"code","a95f691e":"code","f0b540c1":"code","7e1944ee":"code","3e9af46e":"code","a313387a":"code","81044765":"code","0609bd36":"code","e53657cb":"code","9f7ed12c":"code","a87762e5":"code","d3040712":"code","42015892":"code","98d1725d":"code","f41193fc":"code","694f92fe":"code","16c6c6eb":"code","1d2d3c16":"code","b7bacb8e":"code","af39f9f9":"code","abe11d81":"code","b35ae234":"markdown","9f8729fc":"markdown","cfb37ec8":"markdown","5d4d09db":"markdown","710ac29f":"markdown","5969c516":"markdown"},"source":{"29917b89":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6d6d0821":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression","7cca0311":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', header = 0, dtype={'Age': np.float64})\ntest  = pd.read_csv('\/kaggle\/input\/titanic\/test.csv' , header = 0, dtype={'Age': np.float64})\nfull_data = [train, test]","a1e89947":"train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean()","a95f691e":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=True).mean()","f0b540c1":"# create new feature called Family Size\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ntrain[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=True).mean()","7e1944ee":"# new feature called IsAlone to check if being single has any effect on the survival\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\ntrain[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=True).mean()","3e9af46e":"# fill the NaN with some values for 'Embarked'Randomly\nfor dataset in full_data:\n    choice = random.choice(['C','S','Q'])\n    dataset['Embarked'] = dataset['Embarked'].fillna(choice)\n    \nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","a313387a":"# Pclass vs Fare is it connsistent? NO\n# so lets not use Fare as a feature.\ngrouped = train[['Pclass', 'Fare']].groupby(['Pclass'], as_index=True)\n# for name, group in grouped:\n#     print(name)\n#     print(group)\n","81044765":"# fill the Fare NaN's using Pclass\ndataset_name = ['train', 'test']\n# print(test[test['Fare'].isnull()].index)  # only one index in test and 0 in train\n\nfor name, dataset in zip(dataset_name, full_data):\n    grouped = dataset[['Pclass', 'Fare']].groupby(['Pclass'], as_index=False)\n    ndex = dataset[dataset['Fare'].isnull()].index.tolist()\n    print('Dataset - \"{}\" Index with Null Value - {}'.format(name, ndex))\n    for i in ndex:\n        print(\"Current Value - \", dataset.loc[i, 'Fare'])\n        pclass = dataset.loc[i, 'Pclass']\n        dataset.loc[i, 'Fare'] = grouped.get_group(pclass).mean()['Fare']\n        print(\"Updated Value - \", dataset.loc[i, 'Fare'])","0609bd36":"# Fill in Age with values within Gaussian distribution and Categorize the age data\n# first fill the NaN's and then convert the age to int\nfor dataset in full_data:\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    fill_age_value = np.random.randint(age_avg-age_std, age_avg+age_std, size=age_null_count)\n    dataset.loc[dataset['Age'].isnull(), 'Age'] = fill_age_value\n    dataset['Age'] = dataset['Age'].astype(int)","e53657cb":"# convert to catergorical data and do impact analysis on Survivals\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\ntrain[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=True).mean()","9f7ed12c":"# With Name we can find the title of people\ndef get_title(name):\n    title_search = re.search('([A-Za-z]+)\\.', name) \n    if title_search:\n        return title_search.group(1) # group1 is just the (value within this) without the .\n    return \"\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)","a87762e5":"pd.crosstab(train['Title'], train['Sex'])","d3040712":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","42015892":"train[['Title', 'Survived']].groupby(['Title'], as_index=True).mean()","98d1725d":"train.head(2)","f41193fc":"for dataset in full_data:\n    dataset['Sex'] = dataset['Sex'].map({'female':0, 'male':1}).astype(int)\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n","694f92fe":"drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'FamilySize', 'Fare']\ntrain = train.drop(drop_elements, axis = 'columns')\ntest_passengerId = test['PassengerId']\ntest = test.drop(drop_elements, axis='columns')\ntrain = train.drop(['CategoricalAge'], axis = 'columns')","16c6c6eb":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n\tAdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression() ]\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog \t = pd.DataFrame(columns=log_cols)","1d2d3c16":"X = train.drop(['Survived'], axis='columns')\ny = train['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","b7bacb8e":"for clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(X_train, y_train)\n    train_predictions = clf.predict(X_test)    \n    acc = accuracy_score(y_test, train_predictions)\n    log = log.append({'Classifier':name, 'Accuracy':acc}, ignore_index=True)\n\nplt.title('Classifier Accuracy')\nplt.xlabel('Accuracy')\n# sns.set_color_codes(\"muted\")\nsns.barplot(x=\"Accuracy\", y=\"Classifier\", data=log, color='g')","af39f9f9":"clf = GradientBoostingClassifier()\nclf.fit(X_train, y_train)\npredictions = clf.predict(test)","abe11d81":"output = pd.DataFrame({'PassengerID': test_passengerId, 'Survived': predictions })\noutput.to_csv('my_submissions.csv', index=False)\nprint(\"Your submission was successfully saved!\")","b35ae234":"This notebook is written in Python\nReference - \"Titanic best working Classifier\" - https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier.","9f8729fc":"### Real Test Prediction","cfb37ec8":"### Feature Selection","5d4d09db":"### Classifier Comparision","710ac29f":"### Feature Engineering","5969c516":"## Data Cleaning"}}