{"cell_type":{"ab982715":"code","09500711":"code","5f6a3d8a":"code","2088e638":"code","3163d732":"code","70f7f6f0":"code","b39ed0b3":"code","dd77bc7e":"code","e2895850":"code","ac42281f":"code","77167953":"code","572995e1":"code","82d03f28":"code","f550eaef":"code","dbe9118b":"code","cff4fa14":"code","390e610c":"code","9feb8da5":"code","9ca440e9":"code","49dfcb80":"code","7f14578b":"code","724ab356":"markdown","61498ced":"markdown","3d995f5d":"markdown","acdb5919":"markdown","2fcead9c":"markdown","7d468dfd":"markdown","5c218089":"markdown","407b1fa0":"markdown","82a80c09":"markdown","1aba5ef0":"markdown","c9276d03":"markdown","86c83c49":"markdown"},"source":{"ab982715":"import nltk\nnltk.download(['punkt', 'wordnet'])\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline,FeatureUnion\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer","09500711":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5f6a3d8a":"def load_data():\n    df = pd.read_csv('..\/input\/corporate-messaging\/corporate_messaging.csv',encoding='latin-1')\n    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n    X = df.text.values\n    y = df.category.values\n    return X, y","2088e638":"url_regex = 'http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'","3163d732":"def tokenize(text):\n    # get list of all urls using regex\n    detected_urls = re.findall(url_regex, text)\n    \n    # replace each url in text string with placeholder\n    for url in detected_urls:\n        text = text.replace(url, \"urlplaceholder\")\n\n    # tokenize text\n    tokens = word_tokenize(text)\n    \n    # initiate lemmatizer\n    lemmatizer = WordNetLemmatizer()\n\n    # iterate through each token\n    clean_tokens = []\n    for tok in tokens:\n        \n        # lemmatize, normalize case, and remove leading\/trailing white space\n        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n        clean_tokens.append(clean_tok)\n\n    return clean_tokens","70f7f6f0":"# test out function\nX, y = load_data()\nfor message in X[:5]:\n    tokens = tokenize(message)\n    print(message)\n    print(tokens, '\\n')","b39ed0b3":"# Load data\nX, y = load_data()\n\n#Perform train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y)","dd77bc7e":"# Instantiate transformers and classifier\nvect = CountVectorizer(tokenizer=tokenize)\ntfidf = TfidfTransformer()\nclf = RandomForestClassifier()\n\n# Fit and\/or transform each to the data\nX_train_counts = vect.fit_transform(X_train)\nX_train_tfidf = tfidf.fit_transform(X_train_counts)\nclf.fit(X_train_tfidf, y_train)","e2895850":"# Transform test data\nX_test_counts = vect.transform(X_test)\nX_test_tfidf = tfidf.transform(X_test_counts)\n\n#Predict\ny_pred = clf.predict(X_test_tfidf)","ac42281f":"labels = np.unique(y_pred)\nconfusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\naccuracy = (y_pred == y_test).mean()\nprint(\"Labels:\", labels)\nprint(\"Confusion Matrix:\\n\", confusion_mat)\nprint(\"Accuracy:\", accuracy)","77167953":"def display_results(y_test, y_pred):\n    labels = np.unique(y_pred)\n    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n    accuracy = (y_pred == y_test).mean()\n\n    print(\"Labels:\", labels)\n    print(\"Confusion Matrix:\\n\", confusion_mat)\n    print(\"Accuracy:\", accuracy)\n\n\ndef old_main():\n    # insert steps 1 through 3 here\n    X, y = load_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    vect = CountVectorizer(tokenizer=tokenize)\n    tfidf = TfidfTransformer()\n    clf = RandomForestClassifier()\n\n    # train classifier\n    X_train_counts = vect.fit_transform(X_train)\n    X_train_tfidf = tfidf.fit_transform(X_train_counts)\n    clf.fit(X_train_tfidf, y_train)\n\n    # predict on test data\n    X_test_counts = vect.transform(X_test)\n    X_test_tfidf = tfidf.transform(X_test_counts)\n    y_pred = clf.predict(X_test_tfidf)\n    \n    # display results\n    display_results(y_test, y_pred)\n    \n","572995e1":"# run program\nold_main()","82d03f28":"def main():\n    X, y = load_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    # build pipeline\n    pipeline = Pipeline([\n        ('vect', CountVectorizer()),\n        ('tfidf', TfidfTransformer()),\n        ('clf', RandomForestClassifier()),\n    ])\n\n    # train classifier\n    pipeline.fit(X_train, y_train)\n    \n    # predict on test data\n    y_pred = pipeline.predict(X_test)\n    \n    # display results\n    display_results(y_test, y_pred)","f550eaef":"main()","dbe9118b":"class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n\n    def starting_verb(self, text):\n        sentence_list = nltk.sent_tokenize(text)\n        for sentence in sentence_list:\n            pos_tags = nltk.pos_tag(tokenize(sentence))\n            first_word, first_tag = pos_tags[0]\n            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n                return True\n        return False\n\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, X):\n        X_tagged = pd.Series(X).apply(self.starting_verb)\n        return pd.DataFrame(X_tagged)","cff4fa14":"def model_pipeline():\n    pipeline = Pipeline([\n        ('features', FeatureUnion([\n\n            ('text_pipeline', Pipeline([\n                ('vect', CountVectorizer(tokenizer=tokenize)),\n                ('tfidf', TfidfTransformer())\n            ])),\n\n            ('starting_verb', StartingVerbExtractor())\n        ])),\n\n        ('clf', RandomForestClassifier())\n    ])\n    return pipeline","390e610c":"def fu_main():\n    X, y = load_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    model = model_pipeline()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    display_results(y_test, y_pred)","9feb8da5":"fu_main()","9ca440e9":"def build_model():\n    pipeline = Pipeline([\n        ('features', FeatureUnion([\n            \n            ('text_pipeline', Pipeline([\n                ('vect', CountVectorizer(tokenizer=tokenize)),\n                ('tfidf', TfidfTransformer())\n            ])),\n\n            ('starting_verb', StartingVerbExtractor())\n        ])),\n    \n        ('clf', RandomForestClassifier())\n    ])\n\n    # specify parameters for grid search\n    parameters = {\n        'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n        'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n        'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n        'features__text_pipeline__tfidf__use_idf': (True, False),\n        'clf__n_estimators': [50, 100, 200],\n        'clf__min_samples_split': [2, 3, 4],\n        'features__transformer_weights': (\n            {'text_pipeline': 1, 'starting_verb': 0.5},\n            {'text_pipeline': 0.5, 'starting_verb': 1},\n            {'text_pipeline': 0.8, 'starting_verb': 1},\n        )\n    }\n\n    # create grid search object\n    cv = GridSearchCV(pipeline, param_grid=parameters)\n    \n    return cv","49dfcb80":"def main():\n    X, y = load_data()\n    X_train, X_test, y_train, y_test = train_test_split(X, y)\n\n    model = build_model()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    display_results(model, y_test, y_pred)\n","7f14578b":"main()","724ab356":"### Building pipeline to have this structure:\n- Pipeline\n    - feature union\n        - text pipeline\n            - count vectorizer\n            - TFIDF transformer\n        - starting verb extractor\n    - classifier","61498ced":"# Grid Search\nMost machine learning algorithms have a set of parameters that need tuning. Grid search is a tool that allows you to define a \u201cgrid\u201d of parameters, or a set of values to check. Your computer automates the process of trying out all possible combinations of values. Grid search scores each combination with cross validation, and uses the cross validation score to determine the parameters that produce the most optimal model.\n\nRunning grid search on your pipeline allows you to try many parameter values thoroughly and conveniently, for both your data transformations and estimators.\n\nAnd again, although you can also run grid search on just a single classifier, running it on your whole pipeline helps you test multiple parameter combinations across your entire pipeline. This accounts for interactions among parameters not just in your model, but data preparation steps as well.","3d995f5d":"Try to grid search some parameters in your data transformation steps as well as those for your classifier!","acdb5919":"# Train classifier\n* Fit and transform the training data with `CountVectorizer`. Hint: You can include your tokenize function in the `tokenizer` keyword argument!\n* Fit and transform these word counts with `TfidfTransformer`.\n* Fit a classifier to these tfidf values.","2fcead9c":"# Feature Union\n* FEATURE UNION: Feature union is a class in scikit-learn\u2019s Pipeline module that allows us to perform steps in parallel and take the union of their results for the next step.\n* A pipeline performs a list of steps in a linear sequence, while a feature union performs a list of steps in parallel and then combines their results.\n* In more complex workflows, multiple feature unions are often used within pipelines, and multiple pipelines are used within feature unions.","7d468dfd":"# Predict on test data\n* Transform (no fitting) the test data with the same CountVectorizer and TfidfTransformer\n* Predict labels on these tfidf values.","5c218089":"# Display results\nDisplay a confusion matrix and accuracy score based on the model's predictions.","407b1fa0":"#### For step 1, the regular expression to detect a url is given below","82a80c09":"# Load Data and perform a train test split","1aba5ef0":"# Corporate Messaging Case Study\nThis corporate message data is from one of the free datasets provided on the [Figure Eight Platform](https:\/\/appen.com\/resources\/datasets\/), licensed under a [Creative Commons Attribution 4.0 International License](https:\/\/creativecommons.org\/licenses\/by\/4.0\/).\n\nThis notebook is a part of my learning journey which I've been documenting from Udacity's Natural Language Processing Nanodegree program, which helped me a lot to learn and excel advanced NLP stuff such as PySpark. Thank you so much Udacity for providing such quality content. \n\n\n## Tokenization\nBefore we can classify any posts, we'll need to clean and tokenize the text data. Use what you remember from the last lesson on NLP to implement the function `tokenize`. This function should perform the following steps on the string, `text`, using nltk:\n\n1. Identify any urls in `text`, and replace each one with the word, `\"urlplaceholder\"`.\n2. Split `text` into tokens.\n3. For each token: lemmatize, normalize case, and strip leading and trailing white space.\n4. Return the tokens in a list!\n\nFor example, this:\n```python\ntext = 'Barclays CEO stresses the importance of regulatory and cultural reform in financial services at Brussels conference  http:\/\/t.co\/Ge9Lp7hpyG'\n\ntokenize(text)\n```\nshould return this:\n```txt\n['barclays', 'ceo', 'stress', 'the', 'importance', 'of', 'regulatory', 'and', 'cultural', 'reform', 'in', 'financial', 'service', 'at', 'brussels', 'conference', 'urlplaceholder']\n```","c9276d03":"custom transformer, `StartingVerbExtractor`, add a feature union to your pipeline to incorporate a feature that indicates with a boolean value whether the starting token of a post is identified as a verb.","86c83c49":"# Implementing ML Pipeline"}}