{"cell_type":{"b452beee":"code","1fabdda4":"code","3f7e1236":"code","b1c96941":"code","630ba721":"code","2c1d0a47":"code","3366a7ad":"code","9158393f":"code","b4000b1a":"code","0f496792":"code","9aa745dd":"code","91bcbc80":"code","0853fb00":"code","dee9d021":"code","ac3ad994":"code","ff0784be":"code","1ecb8a1f":"code","d7951013":"code","18dde752":"code","7f87b51f":"code","f6c044c8":"code","19280600":"code","1931ba6f":"code","f6f9ab43":"code","7a430c17":"code","81cf2576":"code","1b0996db":"code","9e27afab":"code","6aa84816":"code","35a6af82":"code","5fdfe24b":"code","c5a9f929":"code","5c9c346f":"code","0290d7c1":"code","f160395e":"code","1d3a3f74":"code","44c8ffd8":"code","f1a5c684":"code","cfd15b2c":"code","8d72f307":"code","dc0dc422":"markdown","52d77635":"markdown","494da218":"markdown","3ad39e3a":"markdown","37504443":"markdown"},"source":{"b452beee":"# Import necessary libraries and make necessary arrangements\nimport time\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\n\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\nwarnings.filterwarnings('ignore')","1fabdda4":"# HELPER FUNCTIONS (UTILS)\n\n# Check dataframe\ndef check_df(dataframe, head=5, tail=5, quan=False):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(tail))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n\n    if quan:\n        print(\"##################### Quantiles #####################\")\n        print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n# Date Features\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    return df\n\n# Random Noise\ndef random_noise(dataframe):\n    return np.random.normal(scale=1.6, size=(len(dataframe),))\n\n# Lag\/Shifted Features\ndef lag_features(dataframe, lags):\n    for lag in lags:\n        dataframe['sales_lag_' + str(lag)] = dataframe.groupby([\"store\", \"item\"])['sales'].transform(\n            lambda x: x.shift(lag)) + random_noise(dataframe)\n    return dataframe\n\n# Rolling Mean Features\ndef roll_mean_features(dataframe, windows):\n    for window in windows:\n        dataframe['sales_roll_mean_' + str(window)] = dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                                                          transform(\n            lambda x: x.shift(1).rolling(window=window, min_periods=10, win_type=\"triang\").mean()) + random_noise(\n            dataframe)\n    return dataframe\n\n# Exponentially Weighted Mean Features\ndef ewm_features(dataframe, alphas, lags):\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales'].transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\n# Custom Cost Function\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds - target)\n    denom = np.abs(preds) + np.abs(target)\n    smape_val = (200 * np.sum(num \/ denom)) \/ n\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    labels = train_data.get_label()\n    smape_val = smape(np.expm1(preds), np.expm1(labels))\n    return 'SMAPE', smape_val, False\n\n# Feature Importance\ndef plot_lgb_importances(model, plot=False, num=10):\n\n    gain = model.feature_importance('gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name(),\n                             'split': model.feature_importance('split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        \n# Kaggle input part\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3f7e1236":"########################\n# Loading the data\n########################\ntrain = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)","b1c96941":"# Let's check the time periods of train and test sets\ndf[\"date\"].min(), df[\"date\"].max()  ","630ba721":"train[\"date\"].min(), train[\"date\"].max()   ","2c1d0a47":"test[\"date\"].min(), test[\"date\"].max()  ","3366a7ad":"check_df(train)","9158393f":"check_df(test)","b4000b1a":"check_df(df)","0f496792":"# Distribution of sales\ndf[\"sales\"].describe([0.10, 0.30, 0.50, 0.70, 0.80, 0.90, 0.95, 0.99])","9aa745dd":"# Number of stores\ndf[[\"store\"]].nunique()","91bcbc80":"# Number of items\ndf[[\"item\"]].nunique()","0853fb00":"# Number of unique items for each store\ndf.groupby([\"store\"])[\"item\"].nunique()","dee9d021":"# Sales distribution per store and item\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\"]})","ac3ad994":"# Sales statistics per store and item\ndf.groupby([\"store\", \"item\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","ff0784be":"########################\n# Date Features\n########################\ndf = create_date_features(df)\ncheck_df(df)","1ecb8a1f":"df.groupby([\"store\", \"item\", \"month\"]).agg({\"sales\": [\"sum\", \"mean\", \"median\", \"std\"]})","d7951013":"########################\n# Lag\/Shifted Features\n########################\n# Below sort_values() is so important!\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)","18dde752":"df = lag_features(df, [91, 98, 105, 112, 119, 126, 182, 364, 546, 728])","7f87b51f":"check_df(df)","f6c044c8":"########################\n# Rolling Mean Features\n########################\ndf = roll_mean_features(df, [365, 546])\ndf.tail()","19280600":"########################\n# Exponentially Weighted Mean Features\n########################\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\ncheck_df(df)","1931ba6f":"########################\n# One-Hot Encoding\n########################\ndf = pd.get_dummies(df, columns=['store', 'item', 'day_of_week', 'month'])","f6f9ab43":"########################\n# Converting sales to log(1+sales)\n########################\ndf['sales'] = np.log1p(df[\"sales\"].values)","7a430c17":"########################\n# Time-Based Validation Sets\n########################\n# Train set till the beginning of 2017\ntrain = df.loc[(df[\"date\"] < \"2017-01-01\"), :]\n\n# Validation set including first 3 months of 2017 (as we will forecast the first 3 months of 2018)\nval = df.loc[(df[\"date\"] >= \"2017-01-01\") & (df[\"date\"] < \"2017-04-01\"), :]","81cf2576":"cols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]","1b0996db":"# Define dependent variable and independent variables \nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]","9e27afab":"# Observe the shapes\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","6aa84816":"########################\n# LightGBM Model\n########################\n# LightGBM parameters\nlgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'num_boost_round': 1000,\n              'early_stopping_rounds': 200,\n              'nthread': -1}","35a6af82":"lgbtrain = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=X_val, label=Y_val, reference=lgbtrain, feature_name=cols)","5fdfe24b":"model = lgb.train(lgb_params, lgbtrain,\n                  valid_sets=[lgbtrain, lgbval],\n                  num_boost_round=lgb_params['num_boost_round'],\n                  early_stopping_rounds=lgb_params['early_stopping_rounds'],\n                  feval=lgbm_smape,\n                  verbose_eval=100)","c5a9f929":"y_pred_val = model.predict(X_val, num_iteration=model.best_iteration)","5c9c346f":"smape(np.expm1(y_pred_val), np.expm1(Y_val))","0290d7c1":"########################\n# Feature importance\n########################\nplot_lgb_importances(model, num=30, plot=True)","f160395e":"########################\n# Final Model\n########################\ntrain = df.loc[~df.sales.isna()]\nY_train = train['sales']\nX_train = train[cols]\n\ntest = df.loc[df.sales.isna()]\nX_test = test[cols]","1d3a3f74":"lgb_params = {'metric': {'mae'},\n              'num_leaves': 10,\n              'learning_rate': 0.02,\n              'feature_fraction': 0.8,\n              'max_depth': 5,\n              'verbose': 0,\n              'nthread': -1,\n              \"num_boost_round\": model.best_iteration}","44c8ffd8":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=X_train, label=Y_train, feature_name=cols)","f1a5c684":"model = lgb.train(lgb_params, lgbtrain_all, num_boost_round=model.best_iteration)","cfd15b2c":"test_preds = model.predict(X_test, num_iteration=model.best_iteration)","8d72f307":"########################\n# Submission\n########################\nsubmission_df = test.loc[:, ['id', 'sales']]\nsubmission_df['sales'] = np.expm1(test_preds)\nsubmission_df['id'] = submission_df.id.astype(int)\n\nsubmission_df.to_csv('submission_demand.csv', index=False)\nsubmission_df.head(20)","dc0dc422":"#### **REFERENCES**\n* Data Science and Machine Learning Bootcamp, 2021, https:\/\/www.veribilimiokulu.com\/","52d77635":"### **DEMAND FORECASTING**\n\n##### Mission is to create a 3-month demand forecasting model for the relevant store chain using the following time series and machine learning techniques:\n* Random Noise\n* Lag Shifted Features\n* Rolling Mean Features\n* Exponentially Weighted Mean Features\n* Custom Cost Function\n* Model Validation with LightGBM \n\nDataset is here: https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only","494da218":"#### **EXPLORATORY DATA ANALYSIS**","3ad39e3a":"#### **FEATURE ENGINEERING**","37504443":"#### **MODEL**\n\n* MAE: mean absolute error\n* MAPE: mean absolute percentage error\n* SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)"}}