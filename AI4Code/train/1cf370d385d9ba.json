{"cell_type":{"f0bcdac4":"code","f4358e02":"code","554a5aff":"code","3e5857db":"code","3a3c5592":"code","420c4339":"code","ab46d47e":"code","03b87096":"code","20010484":"code","4cc7cbc5":"code","7ee11781":"code","e0a0f755":"code","6d9748f7":"code","6bce8067":"code","f8aaf269":"code","3ebfc80c":"code","42236eb4":"code","abcab2f0":"code","56742250":"code","e5c7d0f6":"code","b8d5271e":"code","76e05af6":"code","0ae43c81":"code","12867dc1":"code","1417210b":"code","0511fc58":"code","b1f709d8":"code","25c8c496":"code","87ab0fa5":"code","69c157f3":"code","6d143d9c":"code","9ebaf02f":"code","fd4816af":"code","1b91ab38":"code","9af00bde":"code","6c00e810":"code","1e4a13d9":"markdown","1fd32efa":"markdown"},"source":{"f0bcdac4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm.auto import tqdm\nfrom glob import glob\nimport time, gc\nimport cv2\n\n\n\n\n\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f4358e02":"train_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/test.csv')\nclass_map_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/class_map.csv')\nsample_df = pd.read_csv('\/kaggle\/input\/bengaliai-cv19\/sample_submission.csv')\n","554a5aff":"train_df.head()","3e5857db":"test_df.head()","3a3c5592":"class_map_df.head()","420c4339":"sample_df.head()","ab46d47e":"print('shape of the train_df is : ',train_df.shape)\nprint('shape of the test_df is : ',test_df.shape)\nprint('shape of the class_map_df is : ',class_map_df.shape)\nprint('shape of the sample_df is : ',sample_df.shape)","03b87096":"print(f'Number of unique grapheme roots: {train_df[\"grapheme_root\"].nunique()}')\nprint(f'Number of unique vowel diacritic: {train_df[\"vowel_diacritic\"].nunique()}')\nprint(f'Number of unique consonant diacritic: {train_df[\"consonant_diacritic\"].nunique()}')\n","20010484":"train_df.groupby('grapheme')['vowel_diacritic'].agg('sum').head()","4cc7cbc5":"for i in range(5):\n    print(train_df['image_id'][i])","7ee11781":"train_df = train_df.drop(['grapheme'], axis=1, inplace=False)\ntrain_df","e0a0f755":"train_df.dtypes","6d9748f7":"train_df.dtypes","6bce8067":"from keras.models  import Sequential\nfrom keras.models import Model\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPool2D,Input\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization","f8aaf269":"IMG_SIZE = 64","3ebfc80c":"inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=128, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = MaxPool2D(pool_size=(2, 2))(model)\nmodel = Conv2D(filters=256, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\nmodel = BatchNormalization(momentum=0.15)(model)\nmodel = Dropout(rate=0.3)(model)\n","42236eb4":"model = Flatten()(model)\nmodel = Dense(1024, activation = \"relu\")(model)\nmodel = Dropout(rate=0.3)(model)\ndense = Dense(512, activation = \"relu\")(model)\n","abcab2f0":"head_root = Dense(168, activation = 'softmax')(dense)\nhead_vowel = Dense(11, activation = 'softmax')(dense)\nhead_consonant = Dense(7, activation = 'softmax')(dense)\n\nmodel = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])\n","56742250":"model.summary()","e5c7d0f6":"model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","b8d5271e":"train_df.shape","76e05af6":"import matplotlib.pyplot as plt\nhistories = []\nfor i in range(4):\n    train_df_ = pd.merge(pd.read_parquet(f'\/kaggle\/input\/bengaliai-cv19\/train_image_data_{i}.parquet'), train_df, on='image_id').drop(['image_id'], axis=1)\n    \n    # Visualize few samples of current training dataset\n   # fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n    #count=0\n    #for row in ax:\n       # for col in row:\n        #    col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(-1).reshape(IMG_SIZE, IMG_SIZE).astype(np.float64))\n         #   count += 1\n    #plt.show()\n    \n    #X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\n    #X_train = resize(X_train)\/255\n    \n    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n    #X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n    ","0ae43c81":"train_df_.shape","12867dc1":"X_train = train_df_.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1)\nX_train.shape","1417210b":"len(X_train)","0511fc58":"def resize(df, size=64, need_progress_bar=True):\n    resized = {}\n    resize_size=64\n    if need_progress_bar:\n        for i in tqdm(range(df.shape[0])):\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    else:\n        for i in range(df.shape[0]):\n            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n            image=df.loc[df.index[i]].values.reshape(137,236)\n            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n\n            idx = 0 \n            ls_xmin = []\n            ls_ymin = []\n            ls_xmax = []\n            ls_ymax = []\n            for cnt in contours:\n                idx += 1\n                x,y,w,h = cv2.boundingRect(cnt)\n                ls_xmin.append(x)\n                ls_ymin.append(y)\n                ls_xmax.append(x + w)\n                ls_ymax.append(y + h)\n            xmin = min(ls_xmin)\n            ymin = min(ls_ymin)\n            xmax = max(ls_xmax)\n            ymax = max(ls_ymax)\n\n            roi = image[ymin:ymax,xmin:xmax]\n            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n            resized[df.index[i]] = resized_roi.reshape(-1)\n    resized = pd.DataFrame(resized).T\n    return resized","b1f709d8":"X_train = resize(X_train)\/255\nX_train.shape","25c8c496":"X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\nprint(X_train.shape)\nY_train_root = pd.get_dummies(train_df_['grapheme_root']).values\nY_train_vowel = pd.get_dummies(train_df_['vowel_diacritic']).values\nY_train_consonant = pd.get_dummies(train_df_['consonant_diacritic']).values\n\nprint(f'Training images: {X_train.shape}')\nprint(f'Training labels root: {Y_train_root.shape}')\nprint(f'Training labels vowel: {Y_train_vowel.shape}')\nprint(f'Training labels consonants: {Y_train_consonant.shape}')\n","87ab0fa5":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nx_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n\ndel train_df\ndel X_train\ndel Y_train_root, Y_train_vowel, Y_train_consonant\n","69c157f3":"from tensorflow import keras\nclass ImageDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n\n    def flow(self,\n             x,\n             y=None,\n             batch_size=32,\n             shuffle=True,\n             sample_weight=None,\n             seed=None,\n             save_to_dir=None,\n             save_prefix='',\n             save_format='png',\n             subset=None):\n\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n        for output, target in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n\n        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n                                         shuffle=shuffle):\n            target_dict = {}\n            i = 0\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i: i + target_length]\n                i += target_length\n\n            yield flowx, target_dict","6d143d9c":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.15, # Randomly zoom image \n        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\n    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\ndatagen.fit(x_train)\n\nimport gc\ngc.collect()","9ebaf02f":"classifier = []\nbatch_size = 256\nepochs = 50\n\n\n\nhistory = model.fit_generator(datagen.flow(x_train, {'dense_3': y_train_root, 'dense_4': y_train_vowel, 'dense_5': y_train_consonant}, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n                              steps_per_epoch=x_train.shape[0] \/\/ batch_size )\n \nhistories.append(history)\n\ndel x_train\ndel x_test\ndel y_train_root\ndel y_test_root\ndel y_train_vowel\ndel y_test_vowel\ndel y_train_consonant\ndel y_test_consonant\ngc.collect()\n\n","fd4816af":"%matplotlib inline\ndef plot_loss(cls,epoch,title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0,epoch),cls.history['loss'],label = 'train_loss')\n    plt.plot(np.arange(0,epoch),cls.history['dense_3_loss'],label = 'train_root_loss')\n    plt.plot(np.arange(0,epoch),cls.history['dense_4_loss'],label = 'train_vowel_loss')\n    plt.plot(np.arange(0,epoch),cls.history['dense_5_loss'],label = 'train_consonant_loss')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_3_loss'],label = 'val_train_root_loss')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_4_loss'],label = 'val_train_vowel_loss')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_5_loss'],label = 'val_train_consonant_loss')\n    plt.title(title)\n    plt.legend(loc='upper right')\n    plt.xlabel('Number of Epoch ')\n    plt.ylabel('Loss')\n    plt.show()\n    \n    \n    \ndef plot_accuracy(cls,epoch,title):\n    plt.style.use('ggplot')\n    plt.figure()\n    plt.plot(np.arange(0,epoch),cls.history['dense_3_accuracy'],label = 'train_root_accuracy')\n    plt.plot(np.arange(0,epoch),cls.history['dense_4_accuracy'],label = 'train_vowel_accuracy')\n    plt.plot(np.arange(0,epoch),cls.history['dense_5_accuracy'],label = 'train_comnsonant_accuracy')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_3_accuracy'],label = 'val_train_root_accuracy')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_4_accuracy'],label = 'val_train_vowel_accuracy')\n    plt.plot(np.arange(0,epoch),cls.history['val_dense_5_accuracy'],label = 'val_train_consonant_accuracy')\n    plt.title(title)\n    plt.xlabel('Number of Epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='upper right')\n    plt.show()","1b91ab38":"for dataset in range(1):\n    plot_loss(histories[dataset], epochs, f'Training Dataset: {dataset}')\n    plot_accuracy(histories[dataset], epochs, f'Training Dataset: {dataset}')","9af00bde":"pred_dict = {\n    'grapheme_root': [],\n    'vowel_diacritic': [],\n    'consonant_diacritic': []\n}","6c00e810":"components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\ntarget=[] # model predictions placeholder\nrow_id=[] # row_id place holder\nfor i in range(4):\n    df_test_img = pd.read_parquet('\/kaggle\/input\/bengaliai-cv19\/test_image_data_{}.parquet'.format(i)) \n    df_test_img.set_index('image_id', inplace=True)\n\n    X_test = resize(df_test_img, need_progress_bar=False)\/255\n    X_test = X_test.values.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n    \n    preds = model.predict(X_test)\n\n    for i, p in enumerate(pred_dict):\n        pred_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k,id in enumerate(df_test_img.index.values):  \n        for i,comp in enumerate(components):\n            id_sample=id+'_'+comp\n            row_id.append(id_sample)\n            target.append(pred_dict[comp][k])\n    del df_test_img\n    del X_test\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target':target\n    },\n    columns = ['row_id','target'] \n)\ndf_sample.to_csv('submission.csv',index=False)\ndf_sample.head()","1e4a13d9":"train_df[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']] = train_df[['grapheme_root', 'vowel_diacritic', 'consonant_diacritic']].astype('uint8')\n","1fd32efa":"# Exploratory Data Analysis\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods."}}