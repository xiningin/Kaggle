{"cell_type":{"954f3bd1":"code","ac6fea01":"code","19ee1ac0":"code","6b6919a3":"code","3e74c531":"code","218ff709":"code","7d4634bc":"code","baffb5d1":"code","1da6acf3":"code","42841585":"code","1a5fdd30":"code","b19d111e":"code","e937318e":"code","3bfab0f8":"code","7269b5bd":"code","e447208b":"code","9ae67af9":"code","9cf470ee":"code","81a5002f":"code","7bcb2fc9":"markdown","4a2731f2":"markdown","03feffe9":"markdown","96c41ebe":"markdown","bc025f46":"markdown","c469d824":"markdown","0a518674":"markdown","146530db":"markdown","4ff18158":"markdown","c177246a":"markdown","d9daa72b":"markdown","2bc832dc":"markdown","95e5ed69":"markdown","9ef580eb":"markdown"},"source":{"954f3bd1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os","ac6fea01":"df = pd.read_csv('\/kaggle\/input\/waves-measuring-buoys-data-mooloolaba\/Coastal Data System - Waves (Mooloolaba) 01-2017 to 06 - 2019.csv')\ndf.head()","19ee1ac0":"# Deleting NaN values\ndf.replace(-99.90, np.nan, inplace=True)\ndf.drop('Date\/Time', axis=1, inplace=True)\ndf.dropna(inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.head()","6b6919a3":"df_graph = df.loc[0:100]\n\nplt.figure(figsize=(15,22))\nplt.subplot(6,2,1)\nplt.plot(df_graph['Hs'], color='blue')\nplt.title('Significant Wave Height')\n\nplt.subplot(6,2,2)\nplt.plot(df_graph['Hmax'], color='red')\nplt.title('Maximum Wave Height')\n\nplt.subplot(6,2,3)\nplt.plot(df_graph['Tz'], color='orange')\nplt.title('Zero Upcrossing Wave Period')\n\nplt.subplot(6,2,4)\nplt.plot(df_graph['Tp'], color='brown')\nplt.title('The Peak Energy Wave Period')\n\nplt.subplot(6,2,5)\nplt.plot(df_graph['Peak Direction'], color='purple')\nplt.title('Direction Related to True North')\n\nplt.subplot(6,2,6)\nplt.plot(df_graph['SST'], color='green')\nplt.title('Sea Surface Temperature')\nplt.show();","3e74c531":"print(df.info())","218ff709":"df.describe()","7d4634bc":"plt.figure(figsize=(7,7))\nsns.heatmap(df.corr(), linewidth=.1, annot=True, cmap='YlGnBu')\nplt.title('Correlation Matrix')\nplt.show();","baffb5d1":"from sklearn.preprocessing import MinMaxScaler\n\n# Scaling all the values between 0 and 1\nscaler = MinMaxScaler(feature_range=(0,1))\ndata = scaler.fit_transform(df)\nprint('Shape of the scaled data matrix: ', data.shape)","1da6acf3":"# Separete data into 2 groups for train and test\ntrain = data[:42000,]\ntest = data[42000: ,]\n\n# Shapes of our datasets\nprint('Shape of train data: ', train.shape)\nprint('Shape of test data: ', test.shape)","42841585":"# Separete every 30 samples as the input and get the 31st sample as the output.\ndef prepare_data(data):\n    databatch = 30\n    x_list = []\n    y_list = []\n    \n    for i in range(len(data)-databatch-1):\n        x_list.append(data[i:i+databatch])\n        y_list.append(data[i+databatch+1])\n        \n    X_data = np.array(x_list)\n    X_data = np.reshape(X_data, (X_data.shape[0], X_data.shape[2], X_data.shape[1]))\n    y_data = np.array(y_list)\n    \n    return X_data, y_data","1a5fdd30":"# Executing the separation\nX_train, y_train = prepare_data(train)\nX_test, y_test = prepare_data(test)\nprint('X_train Shape : ', X_train.shape, 'y_train shape :', y_train.shape)\nprint('X_test Shape  : ', X_test.shape, ' y_test shape  :', y_test.shape)","b19d111e":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\n\ndef lstm_model(x_data, y_data, num_epochs, batch_size, learning_rate):\n    # Creating the model\n    model = Sequential()\n    # Adding the first layer\n    model.add(LSTM(32, input_shape=(x_data.shape[1], x_data.shape[2]), return_sequences=True))\n    # Adding the second layer \n    model.add(LSTM(16, return_sequences=True))\n    # Adding a dropout value in order to prevent overfiting\n    model.add(Dropout(0.2))\n    # Adding the third layer\n    model.add(LSTM(10))\n    # Adding the output layer. 6 nodes are selected because the data has 6 features\n    model.add(Dense(6))\n    \n    # Choosing the optimizer\n    optimizer = Adam(lr=learning_rate)\n    \n    # Compiling the model\n    model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n    \n    # Fitting the model\n    history = model.fit(x_data, y_data, validation_split=0.25, epochs=num_epochs, batch_size=batch_size)\n    \n    return model, history","e937318e":"history = lstm_model(X_train, y_train, num_epochs=15, batch_size=200, learning_rate=.001)","3bfab0f8":"plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.plot(history[1].history['accuracy'], color='blue', label='Train accuracy')\nplt.plot(history[1].history['val_accuracy'], color='red', label='Validation accuracy')\nplt.title('Train vs Validation Accuracy')\nplt.xlabel('Number of Epochs')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(history[1].history['loss'], color='blue', label='Train Loss')\nplt.plot(history[1].history['val_loss'], color='red', label='Validation Loss')\nplt.title('Train vs Validation Loss')\nplt.xlabel('Number of Epochs')\nplt.legend()\nplt.show();","7269b5bd":"# Defining function to predict datas\ndef predicting(data, y_real):\n    predicted_data = history[0].predict(data)\n    # Invert scaling process to get the normal values range for the features \n    predicted_data = scaler.inverse_transform(predicted_data)\n    y_real = scaler.inverse_transform(y_real)\n    \n    return predicted_data, y_real","e447208b":"# Executing predictions\ntrain_prediction, y_train = predicting(X_train, y_train)\ntest_prediction, y_test = predicting(X_test, y_test)","9ae67af9":"# Defining function to investigate the root of mean squared errors (RMSE) between predicted and real data\n\nimport math\nfrom sklearn.metrics import mean_squared_error\n\ndef examine_rmse(y_data, predicted_data):\n    Score_Hs = math.sqrt(mean_squared_error(y_data[:,0], predicted_data[:,0]))\n    Score_Hmax = math.sqrt(mean_squared_error(y_data[:,1], predicted_data[:,1]))\n    Score_Tz = math.sqrt(mean_squared_error(y_data[:,2], predicted_data[:,2]))\n    Score_Tp = math.sqrt(mean_squared_error(y_data[:,3], predicted_data[:,3]))\n    Score_Dir = math.sqrt(mean_squared_error(y_data[:,4], predicted_data[:,4]))\n    Score_SST = math.sqrt(mean_squared_error(y_data[:,5], predicted_data[:,5]))\n    \n    print('RMSE_Hs       : ', Score_Hs)\n    print('RMSE_Hmax     : ', Score_Hmax)\n    print('RMSE_Tz       : ', Score_Tz)\n    print('RMSE_Tp       : ', Score_Tp)\n    print('RMSE_Direction: ', Score_Dir)\n    print('RMSE_SST      : ', Score_SST)","9cf470ee":"# Executing the RMSE comparison\nprint('Trainin Data Errors')\nprint(examine_rmse(y_train, train_prediction),'\\n')\nprint('Test Data Errors')\nprint(examine_rmse(y_test, test_prediction))","81a5002f":"plt.figure(figsize=(17,25))\n\n\nplt.subplot(6,2,1)\nplt.plot(test_prediction[1300:,0], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,0], color='blue', alpha=0.5, label='real')\nplt.title('Significant Wave Height')\nplt.legend()\nplt.grid(b=True, axis='y')\n\nplt.subplot(6,2,2)\nplt.plot(test_prediction[1300:,1], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,1], color='blue', alpha=0.5, label='real')\nplt.title('Maximum Wave Height')\nplt.legend()\nplt.grid(b=True, axis='y')\n\nplt.subplot(6,2,3)\nplt.plot(test_prediction[1300:,2], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,2], color='blue', alpha=0.5, label='real')\nplt.title('Zero Upcrossing Wave Period')\nplt.legend()\nplt.grid(b=True, axis='y')\n\nplt.subplot(6,2,4)\nplt.plot(test_prediction[1300:,3], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,3], color='blue', alpha=0.5, label='real')\nplt.title('Peak Energy Wave Period')\nplt.legend()\nplt.grid(b=True, axis='y')\n\nplt.subplot(6,2,5)\nplt.plot(test_prediction[1300:,4], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,4], color='blue', alpha=0.5, label='real')\nplt.title('Direction Related to True North')\nplt.legend()\nplt.grid(b=True, axis='y')\n\nplt.subplot(6,2,6)\nplt.plot(test_prediction[1300:,5], color='red', alpha=0.7, label='prediction')\nplt.plot(y_test[1300:,5], color='blue', alpha=0.5, label='real')\nplt.title('Sea Surface Temperature')\nplt.legend()\nplt.grid(b=True, axis='y')\nplt.show();","7bcb2fc9":"## Evaluating the Model Success","4a2731f2":"## Conclusion\n\nLSTM model is capable of predicting future values by looking at a defined batch of samples. 30 samples are used as a batch for the model in order to predict 31st sample. \n\n### After the training process LSTM model reached:\n#### a. 0.85 training model accuracy and 0.93 validation model accuracy\n#### b. 0.0023 training loss value and 0.0015 validation loss value\n#### c. 0.09 - 0.26 meters of error for predicting wave height (RMSE Hs and RMSE Hmax)\n#### d. 0.3 - 1.96 seconds of error for predicting wave period (RMSE Tz and RMSE Tp)\n#### e. App. 15 degrees of error for predicting the wave direction (RMSE Direction)\n#### f. 0.2 degrees celcius of error for predicting the sea surface temperature (RMSE SST)","03feffe9":"## Train and Test Split","96c41ebe":"## Initial Statistics","bc025f46":"**The wave data used in this study contains following features:**\n* Date\/Time: Date and 30 minute wave record\n* Hs : The significant wave height (in metres), defined as the average of the highest one-third of wave heights in a 30 minute wave record.\n* Hmax: The height (in metres) of the highest single wave in a wave record.\n* Tz: The average of the zero up-crossing wave periods (in seconds) in a wave record.\n* Tp: This is the wave period (in seconds) of those waves that are producing the most energy in a wave record.\n* Peak Direction: The direction that peak waves are coming from, shown in degrees from true north.\n* SST: The sea surface temperature at the wave monitoring buoy, in degrees Celsius.\n\nwave_graph![image.png](attachment:image.png)","c469d824":"## Creating the LSTM Model","0a518674":"I hope you learned something new and enjoyed reading this study. \n\nPlease write your ideas or your questions on the comment section, and don't forget to upvote.","146530db":"## Visualization of the Real and Predicted Values\n\nFor the ease of understanding the model, the real and predicted datas are limited by 123 samples (Samples from 1300-1423). ","4ff18158":"## Visualization of the Features","c177246a":"## Predictions","d9daa72b":"## Feature Scaling","2bc832dc":"## Loading Data","95e5ed69":"## Visualization of the Learning","9ef580eb":"# Ocean Wave Prediction Model Using RNN-LSTM\n\nPredicting wave behaviors is an important tool for the safety of ship navigation and offshore operations. \n\nIn order to predict wave behaviors we need to collect wave data. Wave monitoring buoys are used for collecting the data. Wave monitoring buoys continuously measures the wave height, wave period and wave direction. \n\nAs the wave monitoring buoy floats up and down each passing wave, its motion is measured and electronically processed. Data from the wave monitoring buoys are transmitted to a nearby receiver station as a radio signal.\n\nMeasured and derived wave data in this study is collected by oceanographic wave measuring buoys anchored at Mooloolaba\/Queensland\/Australia. \n\nMy aim is to create a Long Short Time Memory (LSTM) deep learning model to predict future wave behaviors. In order to achive this I will divide the data set into two part (train and test). Every 30 samples from the training data will be investigated and the 31st sample features will be predicted by the model. Once the model is properly trained, test data will be used as real time wave data. And lastly I will compare these real data samples and my model's predictions to evaluate the model's success.\n\nwave_monitoring_buoy![image.png](attachment:image.png)\n\nReferences: \n1. https:\/\/www.qld.gov.au\/environment\/coasts-waterways\/beach\/monitoring\/waves \n2. https:\/\/www.data.qld.gov.au\/dataset\/coastal-data-system-waves-mooloolaba"}}