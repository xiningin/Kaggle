{"cell_type":{"16360c67":"code","72537e62":"code","75dac5a6":"code","ad2ed8e5":"code","a4b4d0b1":"code","771a8d89":"code","dfff3524":"code","b55187cc":"code","3dfdc8c6":"code","ee464269":"code","f6272180":"code","3ada3c3c":"code","f674de9e":"code","45e77f93":"code","d6934d4d":"code","332ebb7e":"code","08ecd75f":"code","1a58a4be":"code","4d824aaf":"code","a7ee800e":"code","06af86f2":"code","89b4ca9b":"code","c753f5b3":"code","0fb243b0":"code","9025cc92":"code","0c9b4310":"code","fe327d87":"code","3ceb4de2":"code","d478e8b6":"code","b7dabe20":"code","d11d0f24":"code","7af34e27":"code","10559ce0":"code","51b05005":"code","7725b858":"code","f272dd70":"code","70146869":"code","c8886f46":"code","6ef83078":"code","d175fd5a":"code","8370bb52":"code","19219446":"code","6d0aa7d3":"code","c813fc43":"code","8734bb90":"code","e6d58910":"code","d1e8e0cd":"code","be455617":"code","8fd42a15":"code","4993c789":"code","3870d916":"code","48143699":"code","8596c91b":"code","a9329018":"code","9eeea76f":"code","cd07f637":"code","cef61d28":"code","e7de0876":"code","6da2d6a3":"code","be07ed49":"markdown","d01370e5":"markdown","3ab6d665":"markdown","c136ebc7":"markdown","e391bd61":"markdown","7f3d25b8":"markdown","66b7204e":"markdown","080da3c3":"markdown","ffece932":"markdown","df6b1fdd":"markdown","7a05b2fe":"markdown","8824a28f":"markdown","38fb1eb3":"markdown","45e3af85":"markdown","638b2bc2":"markdown","91177beb":"markdown","20d2d8b9":"markdown","60ab78bd":"markdown","62331e95":"markdown","71602328":"markdown","b894efd5":"markdown","5c3aa4ae":"markdown","9000ea5b":"markdown","168ffceb":"markdown","a50b1962":"markdown","723d64e9":"markdown","ecf6afd4":"markdown","a93d61b1":"markdown","bae0919c":"markdown","6c7ecf6c":"markdown","6e6844e5":"markdown","4a6778b8":"markdown","96d60e16":"markdown"},"source":{"16360c67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport joblib\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72537e62":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, r2_score ","75dac5a6":"original_dataset = pd.read_csv(\"\/kaggle\/input\/prediction-of-asteroid-diameter\/Asteroid_Updated.csv\", delimiter = ',', low_memory=False)\ndf = original_dataset.copy()","ad2ed8e5":"df.shape","a4b4d0b1":"percent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\nmissing_value_df.sort_values('percent_missing', ascending=False)","771a8d89":"# dropped rows in which 'diameter','data_arc' or 'albedo' value is missing\ndf=df.dropna(subset = ['diameter', 'albedo', 'data_arc'])","dfff3524":"print(df['H'].skew())\ndf['H'].hist(alpha=0.5, figsize=(10, 8))","b55187cc":"# In case of continuous skewed data \"median\" represents the central tendency best \ndf['H'] = df['H'].fillna(df['H'].median())","3dfdc8c6":"dropCols=missing_value_df[\"column_name\"][missing_value_df[\"percent_missing\"]>97]\ndropCols","ee464269":"df.drop(columns =dropCols, inplace=True)","f6272180":"convertDict = {'diameter' : float, 'condition_code': int}\ndf = df.astype(convertDict)","3ada3c3c":"df.dtypes","f674de9e":"df.shape #dropping the na values leaves us with this much data rows","45e77f93":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnum_data = df.select_dtypes(include=numerics)  #numerical data\ncorr_matrix= num_data.corr()\n\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr_matrix,annot=True,annot_kws={'size':10})","d6934d4d":"corr_matrix[\"diameter\"].abs().sort_values(ascending=False).head(7)","332ebb7e":"corr_matrix[\"diameter\"].abs().sort_values(ascending=True).head()","08ecd75f":"sns.set(style='darkgrid')\nsns.scatterplot(df['diameter'], df['H'], data=df)","1a58a4be":"sns.scatterplot(df['H'], df['albedo'], data=df)","4d824aaf":"sns.scatterplot(df['data_arc'], df['H'], data=df)","a7ee800e":"df.shape   #No we are left with ideal number of parameters. This will prevent the data from Overfitting.","06af86f2":"numericData = df.select_dtypes(include=numerics)  \nnumericData.shape","89b4ca9b":"categoricalData = df.select_dtypes(include=['object']).copy()\ncategoricalData.head(5)","c753f5b3":"print(categoricalData['neo'].value_counts(),\"\\n\")\nprint(categoricalData['pha'].value_counts(),\"\\n\")\nprint(categoricalData['class'].value_counts(),\"\\n\")","0fb243b0":"ohe_categoricalData= pd.get_dummies(categoricalData, columns=['class','neo','pha'], drop_first=True)\nohe_categoricalData","9025cc92":"cleanDf = pd.concat([numericData,ohe_categoricalData],axis = 1)\ncleanDf.head()","0c9b4310":"from scipy import stats\nnumOfOutliers= len(cleanDf)- len(cleanDf[(np.abs(stats.zscore(cleanDf)) < 3).all(axis=1)])\ncleanDf=pd.DataFrame(cleanDf[(np.abs(stats.zscore(cleanDf)) < 3).all(axis=1)])","fe327d87":"numOfOutliers  #num of outliers removed","3ceb4de2":"from sklearn.preprocessing import StandardScaler\n\ncleanDf=pd.DataFrame(StandardScaler().fit_transform(cleanDf), columns = cleanDf.columns)","d478e8b6":"cleanDf.head()","b7dabe20":"sm_Df=cleanDf.sample(10000)\nsm_Df.shape","d11d0f24":"# separate the variables into features and target\nX= cleanDf.drop(['diameter'],axis = 1)\ny = cleanDf['diameter']\n\nX_sm = sm_Df.drop(['diameter'],axis = 1)\ny_sm = sm_Df['diameter']","7af34e27":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n\nX_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_sm, y_sm, test_size = 0.3, random_state = 0)","10559ce0":"from sklearn.ensemble import RandomForestRegressor\n\nRFReg= RandomForestRegressor(n_estimators=400, max_features=3, max_depth=6)  #used RandomSearchCV to come up with these Hyper-params\nRFReg.fit(X_train, y_train)","51b05005":"print(f\"Training score: {RFReg.score(X_train, y_train)}\")\nprint(f\"Testing score: {RFReg.score(X_test, y_test)}\")\n\ny_pred= RFReg.predict(X_test)\nprint(f'Mean Squared Error: {mean_squared_error(y_test, y_pred)}')","7725b858":"from sklearn.model_selection import RandomizedSearchCV\ndef algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid,n_candidates, cv=3, scoring_fit=None,\n                       do_probabilities = False,verbose=1):\n    rs = RandomizedSearchCV(\n        estimator=model,\n        param_distributions=param_grid, \n        n_iter=n_candidates,\n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=verbose\n    )\n    fitted_model = rs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","f272dd70":"from xgboost import XGBRegressor\nXGBReg = XGBRegressor()\n\neval_set=[(X_test,y_test)]\nXGBReg.fit(X_train, y_train,eval_metric=\"error\",eval_set=eval_set, early_stopping_rounds=10, verbose=0)","70146869":"print(f\"Training score: {XGBReg.score(X_train, y_train)}\")\nprint(f\"Testing score: {XGBReg.score(X_test, y_test)}\")\n\ny_pred= XGBReg.predict(X_test)\nprint(f'\\nMean Squared Error: {mean_squared_error(y_test, y_pred)}')","c8886f46":"import lightgbm as lgb\n\nLGBReg = lgb.LGBMRegressor()\n\nLGBReg.fit(X_train,y_train,eval_set=[(X_test,y_test)],verbose=0,eval_metric='mse', early_stopping_rounds=10)","6ef83078":"print(f\"Training score: {LGBReg.score(X_train, y_train)}\")\nprint(f\"Testing score: {LGBReg.score(X_test, y_test)}\")\n\ny_pred= LGBReg.predict(X_test)\nprint(f'\\nMean Squared Error: {mean_squared_error(y_test, y_pred)}')","d175fd5a":"lgb_tuned = lgb.LGBMRegressor(verbose=-2)\nparam_rf = {\n#     number of boosting iterations (trees to build) ->more trees may give more accuracy but also increases chance for overfitting\n    'n_estimators': [400,600,700,800],\n#     controls depth of the tree. Higher value leads to overfitting of the model.\n    'max_depth': [5,6,7],\n#     Controls the complexity of the model. Higher value leads to overfitting of the model.\n    'num_leaves': [15,20,30],\n#     controls fraction of features available at the beginning of constructing each tree.\n    'feature_fraction': [0.5,0.6, 0.7],\n#     fraction of data used for creating Bootstrapped dataset for training DT\n    'bagging_fraction': [0.7,0.8],\n#     the learning_rate parameter is used to control the weighting of new trees added to the model.\n    'learning_rate':[0.07,0.1]\n}\nlgb_tuned=joblib.load(\"..\/input\/lightgbm-tuned\/lgb_tuned.pkl\")\n# lgb_tuned, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, lgb_tuned, param_rf, n_candidates=100)","8370bb52":"# best_score_: Mean cross-validated score of the best_estimator-> for every candidate 3 fits is done\nprint(f\"Best Score {lgb_tuned.best_score_}\\n\")\nprint(f\"Best Params: {lgb_tuned.best_params_}\\n\")","19219446":"lgb_tuned.score(X_test,y_test)","6d0aa7d3":"# Let's export this model \n# joblib.dump(lgb_tuned, \"lgb_tuned.pkl\")","c813fc43":"from catboost import CatBoostRegressor\n\n# build model\ncat_model = CatBoostRegressor()\n# Fit model \ncat_model.fit(X_train, y_train,\n               eval_set=(X_test, y_test), silent=True)","8734bb90":"print(f\"Training score: {cat_model.score(X_train, y_train)}\")\nprint(f\"Testing score: {cat_model.score(X_test, y_test)}\")\n\ny_pred= cat_model.predict(X_test)\nprint(f'\\nMean Squared Error: {mean_squared_error(y_test, y_pred)}')","e6d58910":"cb_tuned = CatBoostRegressor(silent=True)\nparam_rf = {\n    \"n_estimators\": [400,500,600],\n    \"learning_rate\": [0.1,0.2,0.25],\n    \"max_depth\": [5,6,7],\n}\n\ncb_tuned=joblib.load(\"..\/input\/catboost\/cb_tuned.pkl\")\n# cb_tuned, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, \n#                                     cb_tuned, param_rf, n_candidates=10,scoring_fit=\"r2\")","d1e8e0cd":"cb_tuned.score(X_test,y_test)","be455617":"print(f\"Best Score {cb_tuned.best_score_}\\n\")\nprint(f\"Best Params: {cb_tuned.best_params_}\\n\")","8fd42a15":"#Let's save this model\n# joblib.dump(cb_tuned, \"cb_tuned.pkl\")","4993c789":"import tensorflow as tf\nfrom tensorflow import keras","3870d916":"# Creating model using the Sequential in tensorflow\ndef build_model_using_sequential():\n  ANN= keras.Sequential([\n#input layer\n    keras.layers.Dense(60, input_dim=X_train.shape[1],activation=\"relu\"),\n#hidden layers\n    keras.layers.Dropout(0.4), \n    keras.layers.Dense(40,activation=\"relu\"),\n    keras.layers.Dropout(0.4), \n    keras.layers.Dense(20,activation=\"relu\"),\n    keras.layers.Dropout(0.3),  \n#output layer\n    keras.layers.Dense(1, activation='linear')\n])\n  return ANN\n# build the model\nANN = build_model_using_sequential()","48143699":"ANN.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"mse\"])","8596c91b":"# The model weights will be updated after each batch of 64 samples.\n# ANN.fit(X_train, y_train, epochs=10, batch_size=64,validation_split=0.2)\nANN= keras.models.load_model(\"..\/input\/ann-model\/ANN.h5\")\n# ANN.summary()","a9329018":"ANN.evaluate(X_test,y_test)","9eeea76f":"y_pred=ANN.predict(X_test)\nr2_score(y_test,y_pred)","cd07f637":"# ANN.save(\".\/ANN.h5\")\n# ANN = keras.models.load_model('..\/input\/ann-trained\/ANN.h5')\n# ANN.summary()","cef61d28":"from sklearn.neural_network import MLPRegressor","e7de0876":"mlpReg=MLPRegressor()\nmlpReg.fit(X_train,y_train)","6da2d6a3":"print(f\"Training score: {mlpReg.score(X_train, y_train)}\")\nprint(f\"Testing score: {mlpReg.score(X_test, y_test)}\")\n\ny_pred= mlpReg.predict(X_test)\nprint(f'\\nMean Squared Error: {mean_squared_error(y_test, y_pred)}')","be07ed49":"**selecting Numerical data**","d01370e5":"**'om', 'w', 'ma', 'per_y' and 'per'** have low correlation.","3ab6d665":"### Tuning HyperParams for 'CatBoostRegressor' model \n","c136ebc7":"**Not much improvement**","e391bd61":"## Scaling data\n\n**StandardScaler() will normalize the features i.e. each column of X, INDIVIDUALLY, so that each column\/feature will have \u03bc = 0 and \u03c3 = 1.**","7f3d25b8":"**The below alogirthm will be used for hyper paramater tuning**","66b7204e":"## RandomForestRegressor:","080da3c3":"**Dropping columns which has more than 97% of missing values**.","ffece932":"**Poor performance**","df6b1fdd":"## Light GBM","7a05b2fe":"**This feature is skewed left**","8824a28f":"Completed Prepocessing lets concat the OHE_categorical and Numeric Data","38fb1eb3":"Significant improvement in **Test score**, and **train score**.","45e3af85":"## Time for Model training\n- RandomForestRegressor\n- XGboost regressor\n- LighGBM\n- Catboost\n- Keras Artificial Neural Nets\n- Multi Layered Perceptron","638b2bc2":"Ref: [Asteroid Orbit Classes](https:\/\/pdssbn.astro.umd.edu\/data_other\/objclass.shtml)","91177beb":"**Let's One Hot Encode the variables as it allows the representation of categorical data to be more expressive.**","20d2d8b9":"## Remove Outliers\n\n**The code below will remove all rows that have outliers in at least one column**\n\n If the values fall out of the 3rd standard deviation those values are here considered as Outliers and are removed.","60ab78bd":"## Neural Nets","62331e95":"**Slight Improvement with CatBoost**","71602328":"### MLP Regressor","b894efd5":"## XGBoost\n\nJust like unextreme gradient boost **XGBoost fits regression trees to the residuals**. However, unlike unextreme Gradient Boost which uses ordinary regression trees, XGBoost uses **unique Regression Trees called XGBoost Tree.**","5c3aa4ae":"Since our dataset is really large it'll take a lot of time while experimenting stuff with different models and hyperparams.\nSo I have decided to pick random **10,000 samples** and experiment with it, depending upon the performance we will use the entire data set for the training of best model and hyper params","9000ea5b":"Now let's work on categorical data","168ffceb":"## CatBoost","a50b1962":"**Evaluating XGBRegressor**","723d64e9":"## Handling Missing Data\n**Lots of NA data present in our dataset**","ecf6afd4":"From the above matrix we can conclude that **['H', 'data_arc', 'n_obs_used', 'moid', 'q', 'n']** are the few variables which have strong relation with diameter.","a93d61b1":"### Tuning HyperParams for 'LGBMRegressor' model \n**using the algorithm pipeline I created earlier**","bae0919c":"## Split data into train and test data","6c7ecf6c":"Slight improvement as compared to XGBoost. Also it's **extremely FAST!!.**","6e6844e5":"\n**1Astronomical unit = 1.496e+11 Meter**\n\n- 'name':'asteroid_name', \n- 'a':'semi-major_axis', \n- 'e':'eccentricity', \n- 'i':'Inclination with respect to x-y ecliptic plane(deg)', \n- 'om':'Longitude of the ascending node', \n- 'w':'argument of perihelion',\n- 'q':'perihelion-distance', \n- 'ad':'aphelion distance(au)', \n- 'per_y':'sidereal_orrbital_period_years', \n- 'data_arc':'data arc-span',\n- 'condition_code':'Orbit condition code',\n- 'n_obs_used':'number of observations used',\n- 'H':'Absolute Magnitude parameter',\n- 'neo':'near_earth_object',\n- 'pha':'potentialy_hazardous_asteroid',\n- 'diameter':'Diameter of asteroid(Km)', \n- 'extent':'Object bi\/tri axial ellipsoid dimensions(Km)',\n- 'albedo': 'geometric_albedo', \n- 'rot_per':'rotation_period', \n- 'GM':'standard_gravitation_constant', \n- 'BV':'color-index_BV_magnitude_diff', \n- 'UB':'color-index_UB_magnitude_diff',\n- 'IR':'color-index_IR_magnitude_diff', \n- 'spec_B':'Spectral taxonomic type(SMASSII)', \n- 'spec_T':'Spectral taxonomic type(Tholen)', \n- 'G':'magnitude-slope_params',\n- 'moid':'Earth Minimum orbit Intersection Distance(au)', \n- 'class': 'asteroid orbit class' , \n- 'n':'mean_motion', \n- 'per':'sidereal_orrbital_period(d)', \n- 'ma':'mean_anomaly'","4a6778b8":"## Correlation Matrix \n\n**It will help us find *NUMERICAL* variables that have strong Multicollinearity**","96d60e16":"**Evaluating Light GBM**"}}