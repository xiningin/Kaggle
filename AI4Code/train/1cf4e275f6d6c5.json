{"cell_type":{"020bc4fe":"code","5968069e":"code","1d0f2293":"code","8912ec3a":"code","4d03bb58":"code","9edb24a2":"code","bdb60c7d":"code","d089fe68":"code","bd458086":"code","6f3d1583":"code","ebfa678f":"code","fe85ae6b":"code","82c3cde7":"code","a73adecd":"code","6c85114c":"code","61b39fd2":"code","05896514":"code","70e6ec4f":"code","63781fc7":"code","5018537d":"code","8972757c":"code","69dfe601":"code","0cd31795":"code","30fe24da":"code","e91b71eb":"code","1ac56e45":"code","f7f100ff":"code","604b8d0a":"code","b0613950":"code","1a170f99":"markdown","c665f85e":"markdown","b347540f":"markdown","8a9aa0a3":"markdown","c4ca4061":"markdown","a3066c28":"markdown","3111dd75":"markdown","cdbe6874":"markdown"},"source":{"020bc4fe":"import datetime\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split, KFold\nfrom sklearn.linear_model import LinearRegression, ElasticNetCV\nfrom sklearn.ensemble import RandomForestRegressor, VotingRegressor, StackingRegressor\nfrom xgboost import XGBRegressor","5968069e":"import os \nos.chdir('..\/input\/solar-radiation-prediction')","1d0f2293":"train = pd.read_csv('train_set.csv')\ntest = pd.read_csv('test_set.csv')","8912ec3a":"train.head()","4d03bb58":"train.describe()","9edb24a2":"(train.isnull().sum()).sum()","bdb60c7d":"train.hist(figsize=[8,8])\nplt.show()","d089fe68":"train.corr()[\"Radiation\"].sort_values(ascending = False)","bd458086":"f, ax = plt.subplots(figsize=(6,6))\nsns.distplot(train['Radiation'])\nplt.xlim([1.13,1602])","6f3d1583":"def get_date(s):\n    l = s.split()\n    return l[0]","ebfa678f":"datetime_local = (train['Data'].apply(get_date) + ' ' + train['Time']).apply(pd.Timestamp)","fe85ae6b":"datetime_utc = train['UNIXTime'].apply(datetime.datetime.utcfromtimestamp)","82c3cde7":"(datetime_utc - datetime_local).value_counts()","a73adecd":"def time_to_seconds(s):\n    dt = datetime.datetime.strptime(s, \"%H:%M:%S\") - datetime.datetime(1900,1,1)\n    return dt.total_seconds()","6c85114c":"train[\"Time\"] = train[\"Time\"].apply(time_to_seconds)\ntrain[\"TimeSunRise\"] = train[\"TimeSunRise\"].apply(time_to_seconds)\ntrain[\"TimeSunSet\"] = train[\"TimeSunSet\"].apply(time_to_seconds)\ntrain['TimeFromSunRise'] = train['Time'] - train['TimeSunRise']\ntrain['TimeFromSunSet'] = train['Time'] - train['TimeSunSet']\ntrain.drop(columns=['Time', 'TimeSunRise', 'TimeSunSet', 'Data'], inplace=True)","61b39fd2":"X_train, X_test, y_train, y_test = train_test_split(train.drop(columns='Radiation'), train['Radiation'], test_size = 0.2, random_state = 42)\nregressors = [LinearRegression(),\n              XGBRegressor(),\n              RandomForestRegressor()]\n\nfor model in regressors:\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    error = mean_squared_error(y_test, predictions)\n    print(f'MSE {type(model).__name__}: {error}')","05896514":"random_forest = RandomForestRegressor()\nrandom_forest.fit(train.drop(columns='Radiation'), train['Radiation'])\nrandom_forest.feature_importances_ ","70e6ec4f":"xgb = XGBRegressor()\nxgb.fit(train.drop(columns='Radiation'), train['Radiation'])\nxgb.feature_importances_","63781fc7":"features = train.drop(columns=['Radiation', 'WindDirection(Degrees)', 'Speed'])\ntarget = train['Radiation']","5018537d":"features.head()","8972757c":"#RANDOM FOREST TUNING:\nparam_grid = {'n_estimators': np.arange(500, 700, 10),\n              'max_depth': np.arange(4, 21),\n              'min_samples_split': [0.001, 0.01, 0.1, 2],\n              'min_samples_leaf': [0.001, 0.01, 0.1, 1],\n              'max_features': np.arange(3, 7), \n              'n_jobs': [-1]\n              }\n\nrf = RandomForestRegressor()\nrandom_search = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=25, scoring='neg_mean_squared_error', n_jobs=-1, refit=False)\nrandom_search.fit(features, target)","69dfe601":"# XGB TUNING:\nparam_grid = {'learning_rate': np.arange(0.01, 0.1, 0.01),\n              'n_estimators': np.arange(2, 500, 10), \n              'subsample': [0.7, 0.8, 0.9, 1.0],\n              'max_depth': np.arange(4,21), \n              'min_samples_split': [0.001, 0.01, 0.1, 2],\n              'min_samples_leaf': [0.001, 0.01, 0.1, 1],\n              'n_jobs': [-1]\n              }\n\nxgb = XGBRegressor()\nrandom_search = RandomizedSearchCV(xgb, param_distributions=param_grid, n_iter=30, scoring='neg_mean_squared_error', n_jobs=-1, refit=False)\nrandom_search.fit(features, target)","0cd31795":"# Tuned Models:\nrf = RandomForestRegressor(n_estimators = 550,\n                           max_depth = 19,\n                           max_features = 3,\n                           random_state = 42,\n                           n_jobs = -1)\n\nxgb = XGBRegressor(learning_rate = 0.06, \n                   estimators = 2, \n                   max_depth = 11, \n                   min_samples_leaf = 1, \n                   min_samples_split = 0.001, \n                   subsample = 0.7)","30fe24da":"estimators = [('rf', rf), ('xgb', xgb)]\nkf = KFold(shuffle=True, random_state=42)","e91b71eb":"for _, estimator in estimators:\n    scores = cross_val_score(estimator, features, target, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1)\n    print(f'{type(estimator).__name__}: scores: {scores}, avg: {np.mean(scores)}')","1ac56e45":"elastic_net = ElasticNetCV()\nensemble_models = [VotingRegressor(estimators, n_jobs=-1),\n                   StackingRegressor(estimators, cv=kf, n_jobs=-1),\n                   StackingRegressor(estimators, elastic_net, cv=kf, n_jobs=-1)]\n\nfor model in ensemble_models:\n    scores = cross_val_score(model, features, target, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1)\n    print(f'{type(model).__name__}: scores: {scores}, avg: {np.mean(scores)}')","f7f100ff":"voting = VotingRegressor(estimators, n_jobs=-1)\nvoting.fit(features, target)","604b8d0a":"test[\"Time\"] = test[\"Time\"].apply(time_to_seconds)\ntest[\"TimeSunRise\"] = test[\"TimeSunRise\"].apply(time_to_seconds)\ntest[\"TimeSunSet\"] = test[\"TimeSunSet\"].apply(time_to_seconds)\ntest['TimeFromSunRise'] = test['Time'] - test['TimeSunRise']\ntest['TimeFromSunSet'] = test['Time'] - test['TimeSunSet']\ntest.drop(columns = ['Time', 'TimeSunRise', 'TimeSunSet', 'Data', 'WindDirection(Degrees)', 'Speed'], inplace=True)","b0613950":"prediction = voting.predict(test)\ntest_target = pd.read_csv('test_target.csv')\nmean_squared_error(prediction, test_target)","1a170f99":"Given the smaller MSE we got with XGBoost and Random Forest, we proceeded with the tuning of these two models. However, from the output below, we realised that the fifth and sixth features corresponding to WindDirection(Degrees) and Speed have significantly lower impact on the result compared to the other features. We inferred that wind data may not be that relevant in estimating solar radiation, hence we dropped those columns.","c665f85e":"The models will be trained on this dataset:","b347540f":"# Model Selection and Tuning\n\nWe performed a preliminary evaluation of some of the possible models to get a benchmark and tree-based models seemed a suitable choice for the problem.","8a9aa0a3":"# Final Model\n\nWe evaluated three ensemble estimators: \n\n*   Voting Regressor, averaging the predictions of base estimators\n*   Stacking Regressor, with Ridge Regression as final estimator\n*   Stacking Regressor, with Elastic Net Regression as final estimator\n \nFinal results were satisfactory: this additional layer of ensemble led to a significant improvement with respect to individual models. The difference among the three was minimal and we chose Voting Regressor for our final submission. ","c4ca4061":"We realized that all the measurement of time were taken in a timezone 10 hours ahead of the utc time, which corresponds to the Hawaiian time. ","a3066c28":"# Data Exploration","3111dd75":"# Performance Evaluation\n\nThe evaluation of the performance was carried out through 5-fold cross-validation. The tuned models performed similarly on average, but slightly differently on individual folds. We resorted to ensemble models to reduce such variance. ","cdbe6874":"# Feature Engineering\n\nWe noticed that time was represented with several columns in the dataset. In order to be able to use the Time measurement, the TimeSunRise and TimeSunSet, we converted all those time columns to the seconds elapsed from the midnight of the current day.\nWe kept UNIXTime as an absolute representation and we extracted a relative representation, with respect to sunrise and sunset. Hence, we created a new column called TimeFromSunRise. The data of this column represents the spread in seconds between the data collection and the sunrise, a negative value represents an event before sunrise and a positive value represents an event happened after it. We did the same with TimeFromSunSet. We then dropped the original columns, because all the relevant information is contained in the two newly created columns."}}