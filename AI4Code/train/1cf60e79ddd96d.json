{"cell_type":{"81c3b035":"code","0d091108":"code","172cdc6f":"code","104e0dad":"code","671b72a9":"code","0d0eec67":"code","eb02ea3b":"code","563ad01e":"code","5b986046":"code","722c44bf":"code","96c579f1":"code","a9cd43f9":"code","d90fdfe3":"code","ca650a2a":"code","a6103425":"code","f79a5bea":"code","46737ce7":"code","9da18268":"code","81d17fc4":"code","6f798cfe":"code","19832223":"code","e5ddbe37":"code","8d86f5b5":"code","987cbd1c":"code","6badb2f2":"code","a3ef84a4":"code","e55fcc79":"code","6899e078":"code","c8c4e20c":"code","a50cb20c":"markdown","86c56042":"markdown","e7caf8be":"markdown","69f2d656":"markdown","682d5322":"markdown","1f609ed9":"markdown","c47cde9c":"markdown","43458ca2":"markdown","4c6270d3":"markdown","882935b8":"markdown","eab60b39":"markdown","677d6147":"markdown","cff86001":"markdown","c5aafd66":"markdown","0a3a4f2e":"markdown","1ebb78b3":"markdown","81153506":"markdown","c99be8ae":"markdown","7da4281a":"markdown","54eca82a":"markdown","f89de003":"markdown","15461b8c":"markdown","e0d6203c":"markdown","d475c87e":"markdown","6b1763ac":"markdown","740203b5":"markdown","eb2629a0":"markdown","5c1a6082":"markdown","7afe943a":"markdown","eedf65cd":"markdown","01e32d4f":"markdown","e644285f":"markdown","3e3862a6":"markdown","fc8175fa":"markdown","2dfc0ccb":"markdown","d4324a07":"markdown","e1abb057":"markdown","49200671":"markdown","b5ea9bee":"markdown","cf71740e":"markdown","6976d433":"markdown","9935c780":"markdown","4936c610":"markdown"},"source":{"81c3b035":"%matplotlib inline\nfrom sklearn.datasets import make_blobs\nfrom sklearn.datasets import make_moons\nfrom sklearn.datasets import load_digits\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import SpectralClustering\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.manifold import TSNE\n\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom scipy.stats import mode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","0d091108":"plt.style.use('ggplot')","172cdc6f":"X, y_true = make_blobs(n_samples=500, centers=4, cluster_std=0.3, random_state=3)\nplt.scatter(X[:, 0], X[:, 1])","104e0dad":"kmeans = KMeans(n_clusters=4)\nkmeans.fit(X)\ny_result = kmeans.predict(X)\nplt.scatter(X[:, 0], X[:, 1], cmap='cividis', c=y_result)","671b72a9":"center = kmeans.cluster_centers_\n\nplt.scatter(X[:, 0], X[:, 1], cmap='cividis', c=y_result)\nplt.scatter(center[:, 0], center[:, 1], cmap='cividis', c='black')","0d0eec67":"cov1 = [[1, 0],[0, 1]]\ncov2 = [[1, 0.5],[0.5, 1]]","eb02ea3b":"X_1 = np.random.multivariate_normal([0, 10], cov1, (100,))\nX_2 = np.random.multivariate_normal([5, 15], cov2, (100,))\nX_3 = np.random.multivariate_normal([5, 10], cov2, (100,))\nX = np.append(np.append(X_1, X_2, axis=0), X_3, axis=0)\n\nplt.scatter(X[:,0], X[:,1])","563ad01e":"kmeans_3 = KMeans(n_clusters=3)\nkmeans_3.fit(X)\ny_result_3 = kmeans_3.predict(X)\n\nkmeans_4 = KMeans(n_clusters=4)\nkmeans_4.fit(X)\ny_result_4 = kmeans_4.predict(X)","5b986046":"plt.figure(figsize=(15,5))\nplt.subplot(1, 2, 1)\nplt.scatter(X[:, 0], X[:, 1], cmap='cividis', c=y_result_3)\nplt.subplot(1, 2, 2)\nplt.scatter(X[:, 0], X[:, 1], cmap='cividis', c=y_result_4)","722c44bf":"sil_3 = silhouette_score(X, y_result_3)\nsil_4 = silhouette_score(X, y_result_4)\nprint(\"Silhouette score for 3 clusters: {:.4f} | Silhouette score for 4 clusters: {:.4f}\".format(sil_3, sil_4))","96c579f1":"shs = []\nx = []\nfor i in range(2,11):\n    kmeans_i = KMeans(n_clusters=i)\n    kmeans_i.fit(X)\n    y_result_i = kmeans_i.predict(X)\n    x.append(i)\n    shs.append(silhouette_score(X, y_result_i))\n    \nplt.plot(x, shs)\nplt.xlabel(\"Silhouette score\")\nplt.ylabel(\"N\u00famero de clusters\")","a9cd43f9":"X, y = make_moons(n_samples=300, noise=0.05, random_state=3)\nplt.scatter(X[:,0], X[:,1])","d90fdfe3":"kmeans_moons = KMeans(n_clusters=2, n_init=20)\nkmeans_moons.fit(X)\ny_result_moons = kmeans_moons.predict(X)\nplt.scatter(X[:, 0], X[:, 1], cmap='cividis', c=y_result_moons)","ca650a2a":"spc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\ny_spc = spc.fit_predict(X)\nplt.scatter(X[:, 0], X[:, 1], cmap='viridis', c=y_spc)","a6103425":"X, y = make_moons(n_samples=300, noise=0.09, random_state=3)\nplt.scatter(X[:,0], X[:,1])","f79a5bea":"spc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', assign_labels='kmeans')\ny_spc = spc.fit_predict(X)\nplt.scatter(X[:, 0], X[:, 1], cmap='viridis', c=y_spc)","46737ce7":"digits = load_digits()\nkmeans = KMeans(n_clusters=10, random_state=3)\ny_result = kmeans.fit_predict(digits.data)","9da18268":"kmeans.cluster_centers_.shape","81d17fc4":"fig, ax = plt.subplots(2,5,figsize=(8,3))\ncenters = kmeans.cluster_centers_.reshape(10,8,8)\nfor axi, center in zip(ax.flat, centers):\n    axi.set(xticks=[], yticks=[])\n    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)","6f798cfe":"labels = np.zeros_like(y_result)\nfor i in range(10):\n    mask = (y_result == i)\n    labels[mask] = mode(digits.target[mask])[0]","19832223":"acc = accuracy_score(digits.target, labels)\n\nprint(\"Accuracy score:  {}\".format(acc))","e5ddbe37":"mx = confusion_matrix(digits.target, labels).max()\nplt.figure(figsize=(8,8))\nsns.heatmap(confusion_matrix(digits.target, labels)\/mx, cmap='viridis', annot=True, fmt='.2f')\nplt.xlabel(\"Label predito\")\nplt.ylabel(\"Label verdadeiro\")","8d86f5b5":"spc = SpectralClustering(n_clusters=10, affinity='nearest_neighbors', assign_labels='kmeans')\ny_result = spc.fit_predict(digits.data)\nlabels = np.zeros_like(y_result)\n\nfor i in range(10):\n    mask = (y_result == i)\n    labels[mask] = mode(digits.target[mask])[0]\n    \nacc = accuracy_score(digits.target, labels)\n\nprint(\"Accuracy score:  {}\".format(acc))","987cbd1c":"mx = confusion_matrix(digits.target, labels).max()\nplt.figure(figsize=(8,8))\nsns.heatmap(confusion_matrix(digits.target, labels)\/mx, cmap='viridis', annot=True, fmt='.2f')\nplt.xlabel(\"Label predito\")\nplt.ylabel(\"Label verdadeiro\")","6badb2f2":"agg = AgglomerativeClustering(n_clusters=10)\ny_result = agg.fit_predict(digits.data)\nlabels = np.zeros_like(y_result)\n\nfor i in range(10):\n    mask = (y_result == i)\n    labels[mask] = mode(digits.target[mask])[0]\n    \nacc = accuracy_score(digits.target, labels)\nprint(\"Accuracy score:  {}\".format(acc))","a3ef84a4":"mx = confusion_matrix(digits.target, labels).max()\nplt.figure(figsize=(8,8))\nsns.heatmap(confusion_matrix(digits.target, labels)\/mx, cmap='viridis', annot=True, fmt='.2f')\nplt.xlabel(\"Label predito\")\nplt.ylabel(\"Label verdadeiro\")","e55fcc79":"tsne = TSNE(init='pca', random_state=3)\nd_proj = tsne.fit_transform(digits.data)\nkmeans = KMeans(n_clusters=10, random_state=3)\ny_result = kmeans.fit_predict(d_proj)\n\nlabels = np.zeros_like(y_result)\n\nfor i in range(10):\n    mask = (y_result == i)\n    labels[mask] = mode(digits.target[mask])[0]\n    \nacc = accuracy_score(digits.target, labels)\nprint(\"Accuracy score:  {}\".format(acc))","6899e078":"mx = confusion_matrix(digits.target, labels).max()\nplt.figure(figsize=(8,8))\nsns.heatmap(confusion_matrix(digits.target, labels)\/mx, cmap='viridis', annot=True, fmt='.2f')\nplt.xlabel(\"Label predito\")\nplt.ylabel(\"Label verdadeiro\")","c8c4e20c":"X = digits.data\ny = digits.target\n\ntsne = TSNE(init='pca', random_state=3)\nX_plot = tsne.fit_transform(X)\ntarget_ids = range(len(digits.target_names))\n\nplt.figure(figsize=(10, 10))\ncolors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple'\nfor i, c, l in zip(target_ids, colors, digits.target_names):\n    plt.scatter(X_plot[y == i, 0], X_plot[y == i, 1], c=c, label=l)\nplt.legend()\nplt.show()","a50cb20c":"Aplicando o K-Means com 4 clusters:","86c56042":"Agora, iremos aplicar o K-Means em um problema pr\u00e1tico e intuitivo: reconhecimento de d\u00edgitos.\n\nSabemos o seu funcionamento, os problemas relacionados, poss\u00edveis alternativas. Falta us\u00e1-lo em alguma situa\u00e7\u00e3o!","e7caf8be":"## 3) Spectral Clustering","69f2d656":"Vemos que nesse caso, escolher 3 clusters foi razo\u00e1vel (sabiamos que seria o ideal), mas j\u00e1 com 4 clusters, o *label* dos dados do cluster em azul mudou parcialmente para amarelo (sendo que amarelo passou para bege claro). Ou seja, identificou o n\u00famero de clusters que quer\u00edamos, mas por nossa escolha incorreta, encontrou clusters que n\u00e3o correspondem aos dados.","682d5322":"Calculando as m\u00e9tricas para a classifica\u00e7\u00e3o:","1f609ed9":"### Silhouette score","c47cde9c":"Agora tivemos uma melhora! Apesar de ainda termos problemas no d\u00edgito 9, o erro do d\u00edgito 1 foi mitigado.","43458ca2":"O Spectral Clustering se deu melhor em rela\u00e7\u00e3o ao K-Means, tendo uma previs\u00e3o de por volta de 82-83%. Analisando a *confusion matrix* correspondente:","4c6270d3":"O algoritmo do K-Means \u00e9 relativamente simples. \nDado um n\u00famero k de clusters inicial (veremos porque isso pode ser um problema) selecionados aleatoriamente, s\u00e3o calculados as dist\u00e2ncias de cada ponto aos k pontos (centros) escolhidos e o ponto recebe a *label* do cluster mais pr\u00f3ximo, re-calculamos os centros com base na m\u00e9dia do cluster, at\u00e9 que os clusters n\u00e3o mudem.","882935b8":"Aqui, podemos ver um caso claro no qual o K-Means falha em reconhecer os dois clusters. \n\nUma alternativa para casos como esse \u00e9 aplicar Spectral Clustering, que por baixo dos panos projeta o conjunto de dados em uma dimens\u00e3o menor, baseado na constru\u00e7\u00e3o de um grafo de similaridade, para ent\u00e3o aplicar o K-Means (pode ser algum outro algoritmo de clustering), na premissa de separar os clusters de uma maneira mais precisa em compara\u00e7\u00e3o a apenas aplicar o K-Means.","eab60b39":"O resultado foi bem melhor! O algoritmo conseguiu captar a forma dos dois clusters, mesmo com seu formato irregular.\n\nMas claro, como nada \u00e9 uma panaceia, mesmo Spectral Clustering falha em certos casos. Portanto, \u00e9 necess\u00e1rio testar par\u00e2metros, e eventualmente utilizar outro algoritmo de clustering, dado as caracter\u00edsticas dos dados e o que o modelo assume.","677d6147":"Vemos que o KMeans conseguiu selecionar bem cada um dos clusters. Mas \u00e9 evidente que nesse caso eles est\u00e3o claramente bem separados, al\u00e9m disso, sabiamos pelo *plot* que seriam necess\u00e1rios 4 clusters.\n\nDe qualquer forma, vemos que ele foi capaz de separar cada um deles.","cff86001":"Refer\u00eancias:\n\nhttps:\/\/developers.google.com\/machine-learning\/clustering\/algorithm\/advantages-disadvantages\n\nhttps:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/05.11-k-means.html\n\nhttps:\/\/datasciencemadesimpler.wordpress.com\/2016\/03\/05\/why-k-means-is-not-always-a-good-idea\/","c5aafd66":"## 1) O que \u00e9 K-Means, para que serve e como podemos utiliz\u00e1-lo.\n","0a3a4f2e":"Para praticarmos, vejamos como ele se comporta com um dataset simples, utilizando a fun\u00e7\u00e3o *make_blobs* do *sklearn*.","1ebb78b3":"### Importando as bibliotecas","81153506":"Podemos *plotar* os dados junto com os centr\u00f3ides obtidos pelo algoritmo.","c99be8ae":"## 2) Problemas relacionados e m\u00e9tricas de compara\u00e7\u00e3o.","7da4281a":"Iremos usar o estilo de plots como o *ggplot* (\u00e9 mais bonito ^.^)","54eca82a":"Como vimos anteriormente, o n\u00famero de clusters pode influenciar nos resultados do K-Means, e ter de escolher esse valor de antem\u00e3o pode ser problem\u00e1tico. Iremos verificar a diferen\u00e7a entre escolher 3 ou 4 clusters.","f89de003":"Aplicando o K-Means no dataset do make_moons, obtemos o seguinte resultado:","15461b8c":"Al\u00e9m do problema relacionado ao n\u00famero de clusters, existem outros casos nos quais o K-Means n\u00e3o se comporta bem. A seguir, veremos um conjunto cujo formato \u00e9 bastante irregular, e difere dos clusters que se parecem normais (no sentido estat\u00edstico).","e0d6203c":"Podemos verificar visualmente qual \u00e9 o centro de cada um dos 10 clusters obtidos","d475c87e":"Como todo algoritmo, o K-Means tem alguns problemas, explorados abaixo no c\u00f3digo. S\u00e3o eles:\n- o n\u00famero de clusters \u00e9 dado inicialmente, e nem sempre sabemos o n\u00famero correto\n- o uso da m\u00e9dia para re-calcular os centros pode ser problem\u00e1tico se considerarmos outliers (dado que a m\u00e9dia \u00e9 fortemente influenciada por esses valores)\n- dependendo da geometria dos clusters, pode ser necess\u00e1rio mapearmos os dados para uma dimens\u00e3o maior antes de aplicar o K-Means, dado que ele n\u00e3o ser\u00e1 capaz, sem nenhum processo adicional, de separar os clusters adequadamente.","6b1763ac":"Uma m\u00e9trica utilizada para avaliar algoritmos n\u00e3o supervisionados \u00e9 o Silhouette Score. Seu c\u00e1lculo \u00e9 dado por:\n\n$$\nSH = \\frac{dist. entre clusters - dist. intra cluster}{max(dist. entre clusters, dist. intra cluster)}\n$$\n\nEssa medida por ser entendida como uma interpreta\u00e7\u00e3o da vari\u00e2ncia presente nos dados, calculada pela dist\u00e2ncia entre clusters e a dist\u00e2ncia intra cluster (entre os pontos de um dado cluster). E veremos asseguir, como ela se aplica ao nosso pequeno dataset.","740203b5":"A ideia do uso do K-Means para classifica\u00e7\u00e3o \u00e9 basicamente:\n\nD\u00edgitos s\u00e3o como pontos, mas em uma dimens\u00e3o mais alta. Mesmo n\u00e3o sabendo o label deles, podemos (ou queremos) separ\u00e1-los em clusters. Sabendo os clusters, podemos comparar a previs\u00e3o do K-Means (o label atribu\u00eddo) ao label real (que indica qual d\u00edgito \u00e9).","eb2629a0":"## 4) K-Means aplicado ao *load_digits()* (subconjunto do MNIST) ","5c1a6082":"O dataset que iremos utilizar cont\u00e9m 500 pontos (x, y), separados em 4 clusters.","7afe943a":"## Principais pontos do tutorial\n\n#### 1. O que \u00e9 K-Means, para que serve e como podemos utiliz\u00e1-lo.\n    Como funciona e o que \u00e9.\n\n#### 2. Problemas relacionados e m\u00e9tricas de compara\u00e7\u00e3o.\n    Como avaliar um modelo no cen\u00e1rio n\u00e3o supervisionado?\n\n#### 3. Spectral Clustering\n    Comparar o K-Means com outro algorimto de clustering.\n\n#### 4. K-Means aplicado ao *load_digits()* (subconjunto do MNIST)\n    Aplicar o K-Means no reconhecimento de d\u00edgitos e comparar com outros algoritmos de clustering","eedf65cd":"Iremos utilizar o dataset do *load_digits()*, que \u00e9 basicamente o MNIST reduzido, e analisar como o K-Means se comporta na classifica\u00e7\u00e3o de d\u00edgitos por meio de clustering. (Mas o K-Means n\u00e3o \u00e9 apenas para aprendizado n\u00e3o supervisionado? Veremos a seguir!). Como iremos analisar apenas como o algoritmo se comporta, n\u00e3o iremos separar o dataset da forma correta: treino e teste. Iremos utilizar todo o dataset para treino e exemplificar o uso do algoritmo. \n\nAqui, o n\u00famero de clusters $n\\_clusters=10$, pois temos d\u00edgitos no intervalo de 0-9.","01e32d4f":"Abaixo, veremos visualmente o que isso significa!","e644285f":"Acima, vemos um caso que mesmo Spectral Clustering n\u00e3o reconhece os 2 clusters da forma desejada.","3e3862a6":"Acima, vimos que comparando o silhouette score entre 3 e 4 clusters, o score \u00e9 maior para 3 clusters, um indicativo de que $k=3$ \u00e9 uma melhor op\u00e7\u00e3o, dado o nosso conjunto de dados.\n\nPodemos testar de forma sistem\u00e1tica diferentes k's, e dentre eles, verificar qual tem o maior Silhouette score. No nosso caso, 3 clusters \u00e9 a op\u00e7\u00e3o ideal.","fc8175fa":"Podemos mais uma vez testar um algoritmo de clustering diferente, nesse caso, Agglomerative Clustering (ou Clustering Hier\u00e1rquico):","2dfc0ccb":"# Tutorial K-Means","d4324a07":"Existem diversos algoritmos de clustering, cada um deles com caracter\u00edsticas diferentes. Para termos uma base de compara\u00e7\u00e3o, iremos observar o comportamento do Spectral Clustering em rela\u00e7\u00e3o ao K-Means. Aqui a ideia \u00e9 dar uma intui\u00e7\u00e3o sobre as diferen\u00e7as e limita\u00e7\u00f5es do K-Means comparado a outros algoritmos, e n\u00e3o em se aprofundar em Spectral Clustering :))","e1abb057":"Para analisar o caso de classifica\u00e7\u00e3o, devemos verificar os labels que o K-Means atribuiu as imagens, para posteriormente comparar com o label real.","49200671":"A ideia central do algoritmo K-Means \u00e9 clusteriza\u00e7\u00e3o, ou seja, dado um dataset, conseguir separar os dados em clusters, de forma a diferenci\u00e1-los com labels diferentes. \u00c9 um algoritmo n\u00e3o supervisionado, pois n\u00e3o necessita de um valor real ou classifica\u00e7\u00e3o par construir os clusters.","b5ea9bee":"Podemos observar que o K-Means falha mais no reconhecimento do d\u00edgito 8, prevendo em seu lugar os d\u00edgitos 1 ou 9. Podemos avaliar o desempenho do Spectral Clustering:","cf71740e":"Apesar de ter uma melhor acur\u00e1cia, a previs\u00e3o do d\u00edgito 9 foi completamente errada, ao passo que a do d\u00edgito 1 continua ruim. No nosso caso, estamos apenas comparando os algoritmos, mas seria necess\u00e1rio avaliar com mais cautela os par\u00e2metros do Spectral Clustering para mitigar esse erro.","6976d433":"Cada uma das imagens \u00e9 8x8, portanto, cada centr\u00f3ide do cluster tem 64 dimens\u00f5es.","9935c780":"Podemos observar que o K-Means fez um bom trabalho na previs\u00e3o dos d\u00edgitos, tendo 78-79% de acur\u00e1cia. A seguir, podemos verificar a *confusion matrix* correspondente:","4936c610":"Para fins de teste, iremos analisar um caso com 3 clusters, com os dados $X \\sim Normal$ multivariada.\nAs m\u00e9dias e covari\u00e2ncias s\u00e3o dadas na c\u00e9lula abaixo."}}