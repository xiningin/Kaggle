{"cell_type":{"24a8b902":"code","86e99784":"code","4cbd3fe3":"code","6bcce25d":"code","ecf06b3d":"code","9210e6ce":"code","3557517d":"code","a6e4aeae":"code","d01dd8b9":"code","6b095c89":"code","d52029be":"code","5b045e66":"code","619c0d1e":"code","b0a5cae9":"code","fc6b2ea2":"code","ea7fb5f1":"code","35eab6a2":"code","93ccc34b":"code","dbfa09e4":"code","304e9e1b":"code","0804ec5f":"code","098cee98":"code","2302d398":"code","83411a84":"code","b946a3be":"code","92b0ec10":"code","deb6a5f5":"code","444612e9":"code","90d9e788":"code","a8395285":"code","d03ee3a4":"code","5ef5e65d":"code","dc2ebe60":"code","24686bfb":"code","8275c334":"code","018c4623":"code","19511d1d":"code","27acac64":"code","01b21c90":"markdown","be662016":"markdown","514860f6":"markdown","55b51f45":"markdown","67c3b286":"markdown","75f4b94c":"markdown"},"source":{"24a8b902":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport random\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.sparse import hstack\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","86e99784":"true_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ntrue_data.head()","4cbd3fe3":"true_data.shape","6bcce25d":"fake_data = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake_data.head()","ecf06b3d":"fake_data.shape","9210e6ce":"random_true_news = random.randint(0,true_data.shape[0])\nrandom_fake_news = random.randint(0,fake_data.shape[0])","3557517d":"true_data['title'][random_true_news]","a6e4aeae":"true_data['text'][random_true_news]","d01dd8b9":"fake_data['title'][random_fake_news]","6b095c89":"fake_data['text'][random_fake_news]","d52029be":"fake_data['target'] = 'fake'\ntrue_data['target'] = 'true'","5b045e66":"news = pd.concat([fake_data, true_data]).reset_index(drop = True)\nnews.head()","619c0d1e":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\n'''Function to expand commonly occuring test'''\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","b0a5cae9":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","fc6b2ea2":"# Combining all the above stundents \ndef preprocessTextData(dataToProcess):\n    \"\"\"This function do the preprocessing of the column text data in essay and title\"\"\"\n    processedData = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(dataToProcess):\n        lowersent = sentance.lower()\n        sent = decontracted(lowersent)\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in sent.split() if e not in stopwords)\n        processedData.append(sent.strip())\n    return processedData","ea7fb5f1":"news['processed title'] = preprocessTextData(news['title'].values)","35eab6a2":"news['processed text'] = preprocessTextData(news['text'].values)","93ccc34b":"news['title'][random_fake_news]","dbfa09e4":"news['processed title'][random_fake_news]","304e9e1b":"news['text'][random_fake_news]","0804ec5f":"news['processed text'][random_fake_news]","098cee98":"x_train,x_test,y_train,y_test = train_test_split(news[['processed title', 'processed text', 'subject']], news.target, test_size=0.2, random_state=42)","2302d398":"#fitting categorical data\ndef fitCatogarizedData(dataToProcess, vocab = None):\n    if vocab is None :\n        vectorizer = CountVectorizer()\n    else:\n        vectorizer = CountVectorizer(vocabulary=vocab, lowercase=False, binary=True)\n    vectorizer.fit(dataToProcess)\n    return vectorizer\n\n#transforming categorical data\ndef transformCatogarizedData(dataToProcess, vectorizer):\n    categories_one_hot = vectorizer.fit_transform(dataToProcess)\n    print(vectorizer.get_feature_names())\n    print(\"Shape of matrix after one hot encodig \",categories_one_hot.shape)\n    return categories_one_hot","83411a84":"train_vector = fitCatogarizedData(x_train['subject'].values)\nx_train_cat = transformCatogarizedData(x_train['subject'].values, train_vector)\nx_test_cat = transformCatogarizedData(x_test['subject'].values, train_vector)","b946a3be":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\nglove_vectors = '\/kaggle\/input\/donors-chose\/glove_vectors'\nwith open(glove_vectors, 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","92b0ec10":"# average Word2Vec\n# compute average word2vec for each review.\ndef fitAvgW2V(dataToProcess):\n    avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n    for sentence in tqdm(dataToProcess): # for each review\/sentence\n        vector = np.zeros(300) # as word vectors are of zero length\n        cnt_words =0; # num of words with a valid vector in the sentence\/review\n        for word in sentence.split(): # for each word in a review\/sentence\n            if word in glove_words:\n                vector += model[word]\n                cnt_words += 1\n        if cnt_words != 0:\n            vector \/= cnt_words\n        avg_w2v_vectors.append(vector)\n\n    print(len(avg_w2v_vectors))\n    print(len(avg_w2v_vectors[0]))\n    return avg_w2v_vectors","deb6a5f5":"#creating avgw2v essay and title vectors\navgw2v_title_train = fitAvgW2V(x_train['processed title'])\navgw2v_text_train = fitAvgW2V(x_train['processed text'])\navgw2v_title_test = fitAvgW2V(x_test['processed title'])\navgw2v_text_test = fitAvgW2V(x_test['processed text'])","444612e9":"x_train.drop(['subject'], axis = 1, inplace=True)\nx_test.drop(['subject'], axis = 1, inplace=True)","90d9e788":"#function to do batch prediction\ndef batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your tr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    if data.shape[0]%1000 !=0:\n        y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","a8395285":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef find_best_threshold(threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    return t\n\ndef predict_with_best_t(proba, threshould):\n    predictions = []\n    for i in proba:\n        if i>=threshould:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","d03ee3a4":"#function to do grid search cross validation\ndef doGridSearch(X_tr, y_train, dense=False):\n    neigh = KNeighborsClassifier(n_jobs=-1)\n    parameters = {'n_neighbors':[11, 21, 31, 41, 51]}\n    clf = GridSearchCV(neigh, parameters, cv=3, scoring='roc_auc',return_train_score=True, n_jobs=-1, verbose=10)\n    clf.fit(X_tr, y_train)\n\n    results = pd.DataFrame.from_dict(clf.cv_results_)\n    results = results.sort_values(['param_n_neighbors'])\n\n    train_auc= results['mean_train_score']\n    train_auc_std= results['std_train_score']\n    cv_auc = results['mean_test_score'] \n    cv_auc_std= results['std_test_score']\n    K =  results['param_n_neighbors']\n\n    plt.plot(K, train_auc, label='Train AUC')\n\n    plt.plot(K, cv_auc, label='CV AUC')\n\n    plt.scatter(K, train_auc, label='Train AUC points')\n    plt.scatter(K, cv_auc, label='CV AUC points')\n\n\n    plt.legend()\n    plt.xlabel(\"K: hyperparameter\")\n    plt.ylabel(\"AUC\")\n    plt.title(\"Hyper parameter Vs AUC plot\")\n    plt.grid()\n    plt.show()\n    \n    return clf","5ef5e65d":"#function to plot auc and heat maps of confusion matrix\ndef plotAucAndHeatmap(neighbors, X_tr, X_te, y_train, y_test):\n    neigh = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=-1)\n    neigh.fit(X_tr, y_train)\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_train_pred = batch_predict(neigh, X_tr)    \n    y_test_pred = batch_predict(neigh, X_te)\n\n    train_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\n    test_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\n    plt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\n    plt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\n    plt.legend()\n    plt.xlabel(\"FPR\")\n    plt.ylabel(\"TPR\")\n    plt.title(\"ERROR PLOTS\")\n    plt.grid()\n    plt.show()\n    \n    print(\"=\"*100)\n    best_t = find_best_threshold(tr_thresholds, train_fpr, train_tpr)\n    print(\"Train confusion matrix\")\n    print(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)))\n    print(\"Test confusion matrix\")\n    print(confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n    print(\"=\"*100)\n\n    \n    plotheatMap(confusion_matrix(y_train, predict_with_best_t(y_train_pred, best_t)), confusion_matrix(y_test, predict_with_best_t(y_test_pred, best_t)))\n    return str(auc(train_fpr, train_tpr)), str(auc(test_fpr, test_tpr))","dc2ebe60":"# subplot seaborn : https:\/\/stackoverflow.com\/a\/41384984\/8363466\n#confusion matrix heat map : https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n#plot confusion matrix of test and train\ndef plotheatMap(confusion_matrix_train, confusion_matrix_test):\n    fig, (ax1, ax2) = plt.subplots(1,2)\n    fig.set_figheight(5)\n    fig.set_figwidth(15)\n    \n    confusion_train_bow = pd.DataFrame(confusion_matrix_train)\n    sns.heatmap(confusion_train_bow, annot=True, fmt='d', ax=ax1)\n    ax1.set_title(\"Train Confusion matrix\")\n    ax1.set(xlabel='Actual', ylabel='Predicted')\n    \n    confusion_test_bow = pd.DataFrame(confusion_matrix_test)\n    sns.heatmap(confusion_test_bow, annot=True, fmt='d', ax=ax2)\n    ax2.set_title(\"Test Confusion matrix\")\n    ax2.set(xlabel='Actual', ylabel='Predicted')","24686bfb":"#creating train and test data for KNN brute force on AVG W2V\nX_tr = hstack((avgw2v_title_train, avgw2v_text_train, x_train_cat)).tocsr()\nX_te = hstack((avgw2v_title_test, avgw2v_text_test, x_test_cat)).tocsr()\n\nprint(X_tr.shape, y_train.shape)\nprint(X_te.shape, y_test.shape)\nprint(\"=\"*100)","8275c334":"%%time\n#do grid search to find best K\navgw2v_clf = doGridSearch(X_tr, y_train)","018c4623":"#randomizedsearchcv sklearn: https:\/\/www.youtube.com\/watch?v=Gol_qOgRqfA\nprint(avgw2v_clf.best_score_)\nprint(avgw2v_clf.best_params_)","19511d1d":"print(\"Number of neighbours as per GridSearchCV : \",avgw2v_clf.best_params_['n_neighbors'])","27acac64":"#plot auc and confusion matrix\navgw2v_train_auc, avgw2v_test_auc = plotAucAndHeatmap(avgw2v_clf.best_params_['n_neighbors'], X_tr, X_te, y_train, y_test)","01b21c90":"## Peeping into data","be662016":"We can do some preprocessing to do some text cleaning","514860f6":"## Importing data","55b51f45":"## Importing libraries","67c3b286":"## Handling the catagorical data `subject`","75f4b94c":"## Spliting to train and test data"}}