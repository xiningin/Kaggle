{"cell_type":{"aa494ad3":"code","103a8ed5":"code","60eb1ff4":"code","1dacaae8":"code","3e0ad015":"code","c9dd5312":"code","d597f895":"code","7bfbec6a":"code","0d1dbb52":"code","5df84fdd":"code","bca03c38":"code","b8a55477":"code","2f4b1551":"code","8a81075b":"code","f7d65e9a":"code","f19a5025":"code","6507a126":"code","a925b74f":"code","9b91655e":"code","e57a9963":"code","491edadc":"markdown","9333b8ce":"markdown","5ac730d2":"markdown"},"source":{"aa494ad3":"import os\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer","103a8ed5":"os.chdir(\"..\/input\/facialrecognitiont10\/\")  \n%pwd\nfrom triplet_loss import TripletLossLayer\nfrom lfw_preprocessor import LfwDataGenerator","60eb1ff4":"os.chdir(\"models\/face_recognition\")\nfrom align import AlignDlib\nfrom model import create_model\nos.chdir(\"..\/..\")","1dacaae8":"in_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\nmodel_sm = create_model()\n\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\nfacial_rec_model = Model([in_a, in_p, in_n], triplet_loss_layer)\nfacial_rec_model.load_weights(\"epoch097_loss0.176.hdf5\")\nfacial_rec_model.summary()\n\nfacial_rec_base_model = facial_rec_model.layers[3]","3e0ad015":"def set_base_embeddings():\n    embeddings = np.empty((49, 128))\n    for i in range(49):\n        print(\"now processing: face_recognition_dataset\/\" + str(i) + \".png\" )\n        img = load_image('face_recognition_dataset\/' + str(i) + \".png\")\n        img = align_image(img)\n        img = img.astype('float32')\n        img = img \/ 255.0\n        img = np.expand_dims(img, axis=0)\n        embeddings[i] = facial_rec_base_model.predict(img)\n        \n    return embeddings\n\nimport csv\nfrom numpy import asarray\ndef name_mapping():\n    id_names = []\n    with open('face_recognition_dataset\/person_id_name_mapping.csv') as id_name_map_csv:\n        csv_dict_reader = csv.DictReader(id_name_map_csv)\n        for row in csv_dict_reader:\n            id_names.append({\n                \"id\": int(row[\"person_id\"]),\n                \"name\": row[\"person_name\"]})\n    return id_names\n\ndef preprocess(img, box):\n    face = img.crop((int(box[0]), int(box[1]), int(box[2]), int(box[3])))\n    face = asarray(face)\n    face = face[...,::-1]\n    face = align_image(face)\n    face = face.astype('float32')\n    face = face \/ 255.0\n    face = np.expand_dims(face, axis=0)\n    return face\n\ndef distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef infer(face):\n\n    face_embed = facial_rec_base_model.predict(face)\n    minDistance = distance(face_embed, embeddings[0])\n    minIndex = 0\n\n\n    for i in range(1, 49):\n        if(distance(face_embed, embeddings[i]) < minDistance):\n            minDistance = distance(face_embed, embeddings[i]) \n            minIndex = i\n    return next(item for item in names if item[\"id\"] == minIndex)[\"name\"]\n\ndef align_image(img):\n    alignment = AlignDlib('models\/landmarks.dat')\n    bb = alignment.getLargestFaceBoundingBox(img)\n    if bb is None:\n        return cv2.resize(img, (96,96))\n    else:\n        return alignment.align(96, \n                               img, \n                               bb,\n                               landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]","c9dd5312":"os.chdir('detr')","d597f895":"import argparse\nimport random\nfrom pathlib   import Path\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\nimport PIL.Image\nimport util.misc as utils\nos.chdir('..')\nimport detr.models\nfrom detr.models import build_model\n!pip install pycocotools\nos.chdir('detr')\n%pwd \n%ls \nimport pycocotools\nfrom main import get_args_parser","7bfbec6a":"parser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\n#This now loads the newly trained face detection weights\nargs.resume = 'checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\nargs.distributed = False\nprint(args)","0d1dbb52":"face_detection_model, criterion, postprocessors = build_model(args)\ndevice = torch.device(args.device)\nface_detection_model.to(device)","5df84fdd":"os.chdir('..')","bca03c38":"output_dir = Path(args.output_dir)\nif args.resume:\n    if args.resume.startswith('https'):\n        checkpoint = torch.hub.load_state_dict_from_url(\n            args.resume, map_location='cpu', check_hash=True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n    face_detection_model.load_state_dict(checkpoint[\"model\"], strict=True)","b8a55477":"CLASSES = ['none', 'person']\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n","2f4b1551":"def detect(im, model, transform):\n    img = transform(im).unsqueeze(0)\n    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each size'\n\n    outputs = face_detection_model(img)\n\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled","8a81075b":"def plot_results(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n        text =f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor = 'yellow', alpha=0.5))\n\n    plt.axis('off')\n    plt.show()","f7d65e9a":"def plot_predictions(pil_img, prob, boxes, classes, matched_people):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c, mp in zip(prob, boxes.tolist(), COLORS * 100, matched_people):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=c, linewidth=3))\n        text = mp\n        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor = 'yellow', alpha=0.5))\n\n    plt.axis('off')\n    plt.show()","f19a5025":"def predict_names(boxes, img_letter):\n  #Below are 2 arrays recording the corresponding id for each bounding box and \n  #their distance\n    prediction_names = [None] * len(boxes)\n    prediction_distances = [None] * len(boxes)\n\n    for i in range(len(base_embeddings)):\n        face_embed = base_embeddings[i]\n        distances = []\n        for j in range(len(pp_face_embeds)):\n            distances.append(distance(face_embed, pp_face_embeds[j]))\n        while min(distances) != 999:\n            dist = min(distances)\n            closestIndex = distances.index(dist)\n            if prediction_distances[closestIndex] == None or prediction_distances[closestIndex][0] > dist:\n                if prediction_distances[closestIndex] != None:\n                    del submission_dict[img_letter + \"_\" + str(prediction_distances[closestIndex][1])]\n                prediction_distances[closestIndex] = (dist, i)\n                prediction_names[closestIndex] = next(item for item in names if item[\"id\"] == i)[\"name\"]\n                submission_dict[img_letter + \"_\" + str(i)] = boxes[closestIndex].detach().numpy()\n                break\n            distances[closestIndex] = 999\n    return prediction_names","6507a126":"names = name_mapping()\n#line below will take a while, that's normal\nbase_embeddings = set_base_embeddings()","a925b74f":"url1 = '..\/c\/2021-spring-coml-face-recognition-competition\/a.jpg'\nurl2 = '..\/c\/2021-spring-coml-face-recognition-competition\/b.jpg'\nurl3 = '..\/c\/2021-spring-coml-face-recognition-competition\/c.jpg'\nurl4 = '..\/c\/2021-spring-coml-face-recognition-competition\/d.jpg'\nplot_classes = [\"person\"]\n\nsubmission_dict = {}\nthe_image = PIL.Image.open(url1)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\npredictions = predict_names(boxes, \"a\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\nthe_image = PIL.Image.open(url2)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"b\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\n\nthe_image = PIL.Image.open(url3)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"c\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)\n\n\n\n\n\n\nthe_image = PIL.Image.open(url4)\nscores, boxes = detect(the_image, face_detection_model, transform)\npp_face_embeds = []\nfor box in boxes:\n    pp_face_embeds.append(facial_rec_base_model.predict(preprocess(the_image, box)))\n# predictions = []\n\n# for box in boxes:\n#   face_embeding = preprocess(the_image, box)\n#   predictions.append(infer(face_embeding))\npredictions = predict_names(boxes, \"d\")\nplot_predictions(the_image, scores, boxes, plot_classes, predictions)","9b91655e":"import csv\n\nos.chdir(\"..\/..\/working\")\nwith open('..\/input\/c\/2021-spring-coml-face-recognition-competition\/kaggle_sample_submission.csv') as sample_submission_csv:\n    with open(\"submission.csv\", \"w\", newline = \"\") as submission_csv:\n        sample_csv_reader = csv.DictReader(sample_submission_csv)\n        open('submission.csv', 'w').close()\n        submission_csv_writer = csv.DictWriter(submission_csv, sample_csv_reader.fieldnames)\n        submission_csv_writer.writeheader()\n        for row in sample_csv_reader:\n            if row[\"id\"] in submission_dict:\n                submission_csv_writer.writerow({\"id\": row[\"id\"], \n                                                \"xmin\": submission_dict[row[\"id\"]][0], \n                                                \"xmax\": submission_dict[row[\"id\"]][1], \n                                                \"ymin\": submission_dict[row[\"id\"]][2], \n                                                \"ymax\": submission_dict[row[\"id\"]][3]})\n            else:\n                submission_csv_writer.writerow({\"id\": row[\"id\"], \n                                                \"xmin\": 0, \n                                                \"xmax\": 0, \n                                                \"ymin\": 0, \n                                                \"ymax\": 0})","e57a9963":"ls ..","491edadc":"# DETR Model","9333b8ce":"# Facial Recognition (RUN ALL)","5ac730d2":"# Driver (RUN ALL)\n"}}