{"cell_type":{"ebe0e87f":"code","1fc4ac01":"code","9ae6b24f":"code","b37a2465":"code","c1713051":"code","4b6a477c":"code","8c1e34f1":"code","89956f0f":"code","ba66fe37":"code","114a2457":"code","402a7a29":"code","a03c1793":"code","79126609":"code","baf91d0b":"code","f8b8e452":"code","14a0eb48":"code","336fbf9a":"code","155663b5":"code","c99438c3":"code","1a4d09d5":"code","f26dd68f":"code","3decff75":"code","6a5842b2":"code","13746e25":"code","6682b175":"code","12038166":"code","f3d1ca04":"code","efea5e8f":"code","ff7e01fc":"code","64ccf9e0":"code","2157fec7":"code","4ac17049":"code","9cb03fd4":"markdown","fbe8e527":"markdown","39091f14":"markdown","d8e4aa61":"markdown","015ddf7f":"markdown","c7f7d548":"markdown","f530f16d":"markdown","48bc4ecf":"markdown","67f217ba":"markdown","95220acc":"markdown","ee390527":"markdown","be8dad53":"markdown","6e3a30b5":"markdown","5b2a71f0":"markdown","08bf0ea6":"markdown","b09dec36":"markdown","f0efa32d":"markdown","7fc0cfe2":"markdown","f6bd7b84":"markdown","92049cb1":"markdown","f4fff608":"markdown","a2a2ebb8":"markdown","0a7ca332":"markdown","ef0cbc34":"markdown","be92f3f6":"markdown","4fa8556e":"markdown","7de661fd":"markdown","c62a698e":"markdown","3275641a":"markdown","e863435b":"markdown","060ced86":"markdown","03044066":"markdown","02e6d73f":"markdown","dd7bdd8b":"markdown","47acc7bc":"markdown"},"source":{"ebe0e87f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n%matplotlib inline","1fc4ac01":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error","9ae6b24f":"df = pd.read_csv('..\/input\/BlackFriday.csv')","b37a2465":"print(df.info())","c1713051":"print('<Contain NaNs?>')\nprint(df.isnull().any())\nmissing_ser_percentage = (df.isnull().sum()\/df.shape[0]*100).sort_values(ascending=False)\nmissing_ser_percentage = missing_ser_percentage[missing_ser_percentage!=0].round(2)\nmissing_ser_percentage.name = 'missing values %'\nprint('\\n<NaN ratio>')\nprint(missing_ser_percentage)","4b6a477c":"df.fillna(0,inplace=True)","8c1e34f1":"for col in df.columns:\n    print('{} unique element: {}'.format(col,df[col].nunique()))","89956f0f":"plt.figure(figsize=(10,5))\nplt.hist(df['Purchase'],bins=50, alpha=0.8)\nplt.xlabel('Purchase',fontsize=14)\nplt.ylabel('Count',fontsize=14)","ba66fe37":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.countplot('User_ID',data=df,alpha = 0.8)\nplt.xlabel('User ID',fontsize=14)\nplt.ylabel('')\nplt.title('Total count',fontsize=14)\nplt.xticks([])\nplt.subplot(122)\nsns.countplot('Product_ID',data=df,alpha = 0.8)\nplt.xlabel('Product ID',fontsize=14)\nplt.ylabel('')\nplt.title('Total count',fontsize=14)\nplt.xticks([])","114a2457":"df_Apurchase_by_UID_Gender = df.groupby(['User_ID','Gender']).agg({'Purchase':np.mean}).reset_index()\ndf_Apurchase_by_UID_Age = df.groupby(['User_ID','Age']).agg({'Purchase':np.mean}).reset_index()\nage_order = ['0-17','18-25','26-35','36-45','46-50','51-55','55+']\n\nplt.figure(figsize=(20,5))\nsns.lmplot('User_ID','Purchase',data=df_Apurchase_by_UID_Gender,fit_reg=False,hue='Gender',aspect=2.5)\nplt.xticks([])\nplt.xlabel('User ID',fontsize=14)\nplt.ylabel('')\nplt.title('Average purchase by Gender',fontsize=14)\nplt.figure(figsize=(20,5))\nsns.lmplot('User_ID','Purchase',data=df_Apurchase_by_UID_Age,fit_reg=False,hue='Age',hue_order=age_order,aspect=2.5)\nplt.xticks([])\nplt.xlabel('User ID',fontsize=14)\nplt.ylabel('')\nplt.title('Average purchase by Age',fontsize=14)","402a7a29":"df_Apurchase_by_PID_Gender = df.groupby(['Product_ID','Gender']).agg({'Purchase':np.mean}).reset_index()\nle_P_ID = LabelEncoder()\ndf_Apurchase_by_PID_Gender['Product_ID'] = le_P_ID.fit_transform(df_Apurchase_by_PID_Gender['Product_ID'])\ndf_Apurchase_by_PID_Age = df.groupby(['Product_ID','Age']).agg({'Purchase':np.mean}).reset_index()\ndf_Apurchase_by_PID_Age['Product_ID'] = le_P_ID.fit_transform(df_Apurchase_by_PID_Age['Product_ID'])\n\nplt.figure(figsize=(20,5))\nsns.lmplot('Product_ID','Purchase',data=df_Apurchase_by_PID_Gender,fit_reg=False,hue='Gender',aspect=2.5)\nplt.xticks([])\nplt.xlabel('Product_ID',fontsize=14)\nplt.ylabel('')\nplt.title('Average purchase by Gender',fontsize=14)\nplt.figure(figsize=(20,5))\nsns.lmplot('Product_ID','Purchase',data=df_Apurchase_by_PID_Age,fit_reg=False,hue='Age',hue_order=age_order,aspect=2.5)\nplt.xticks([])\nplt.xlabel('Product_ID',fontsize=14)\nplt.ylabel('')\nplt.title('Average purchase by Age',fontsize=14)","a03c1793":"plt.figure(figsize=(15,5))\nplt.subplot(131)\nsns.countplot('Age',order=age_order,hue='Gender',data=df,alpha = 0.8)\nplt.xlabel('Age',fontsize=14)\nplt.ylabel('')\nplt.xticks(rotation=70)\nplt.title('Number of customers',fontsize=14)\nplt.legend(['Female','Male'],frameon=True,fontsize=14)\nplt.tick_params(labelsize=15)\nplt.subplot(132)\ndf_Tpurchase_by_Age = df.groupby(['Age','Gender']).agg({'Purchase':np.sum}).reset_index()\nsns.barplot('Age','Purchase',hue='Gender',data=df_Tpurchase_by_Age,alpha = 0.8)\nplt.xlabel('Age',fontsize=14)\nplt.ylabel('')\nplt.xticks(rotation=70)\nplt.title('Total purchase',fontsize=14)\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)\nplt.subplot(133)\ndf_Apurchase_by_Age = df.groupby(['Age','Gender']).agg({'Purchase':np.mean}).reset_index()\nsns.barplot('Age','Purchase',hue='Gender',data=df_Apurchase_by_Age,alpha = 0.8)\nplt.xlabel('Age',fontsize=14)\nplt.ylabel('')\nplt.xticks(rotation=70)\nplt.title('Average purchase',fontsize=14)\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)","79126609":"city_order = ['A','B','C']\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nsns.countplot('City_Category',order=city_order,hue='Occupation',data=df,alpha = 0.8)\nplt.xlabel('City',fontsize=14)\nplt.ylabel('')\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)\nplt.title('Number of customers',fontsize=14)\nplt.subplot(132)\ndf_Tpurchase_by_City = df.groupby(['City_Category','Occupation']).agg({'Purchase':np.sum}).reset_index()\nsns.barplot('City_Category','Purchase',hue='Occupation',data=df_Tpurchase_by_City,alpha = 0.8)\nplt.title('Total purchase',fontsize=14)\nplt.xlabel('City',fontsize=14)\nplt.ylabel('')\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)\nplt.subplot(133)\ndf_Apurchase_by_City = df.groupby(['City_Category','Occupation']).agg({'Purchase':np.mean}).reset_index()\nsns.barplot('City_Category','Purchase',hue='Occupation',data=df_Apurchase_by_City,alpha = 0.8)\nplt.title('Average purchase',fontsize=14)\nplt.xlabel('City',fontsize=14)\nplt.ylabel('')\nplt.legend(title='Occupation',frameon=True,fontsize=10,bbox_to_anchor=(1,0.5), loc=\"center left\")\nplt.tick_params(labelsize=15)","baf91d0b":"df['Marital_Status_label']=np.where(df['Marital_Status'] == 0,'Single','Married')\ndf_Tpurchase_by_City_Marital = df.groupby(['City_Category','Marital_Status_label']).agg({'Purchase':np.sum}).reset_index()\ndf_Tpurchase_by_City_Stay = df.groupby(['City_Category','Stay_In_Current_City_Years']).agg({'Purchase':np.sum}).reset_index()\nfig = plt.figure(figsize=(12,5))\nfig.suptitle('Total purchase',fontsize=20)\nplt.subplot(121)\nsns.barplot('City_Category','Purchase',hue='Marital_Status_label',data=df_Tpurchase_by_City_Marital,alpha = 0.8)\nplt.xlabel('City',fontsize=14)\nplt.ylabel('')\nplt.legend(frameon=True,fontsize=14)\nplt.tick_params(labelsize=15)\nplt.subplot(122)\nsns.barplot('City_Category','Purchase',hue='Stay_In_Current_City_Years',data=df_Tpurchase_by_City_Stay,alpha = 0.8)\nplt.xlabel('City',fontsize=14)\nplt.ylabel('')\nplt.legend(title='Residency duration',frameon=True,fontsize=12,loc=2)\nplt.tick_params(labelsize=15)\ndf.drop('Marital_Status_label',axis=1,inplace=True)","f8b8e452":"df_Tpurchase_by_PC1_Age = df.groupby(['Product_Category_1','Age']).agg({'Purchase':np.sum}).reset_index()\nfig = plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.countplot('Product_Category_1',hue='Age',data=df,alpha = 0.8,hue_order=age_order)\nplt.title('Item count',fontsize=14)\nplt.xlabel('Product category 1',fontsize=14)\nplt.ylabel('')\nplt.legend(title='Age group',frameon=True,fontsize=12)\nplt.tick_params(labelsize=15)\nplt.subplot(122)\nsns.barplot('Product_Category_1','Purchase',hue='Age',data=df_Tpurchase_by_PC1_Age,alpha = 0.8)\nplt.title('Total purchase',fontsize=14)\nplt.xlabel('Product category 1',fontsize=14)\nplt.ylabel('')\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)\n\ndf_Tpurchase_by_PC1_Gender = df.groupby(['Product_Category_1','Gender']).agg({'Purchase':np.sum}).reset_index()\nfig = plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.countplot('Product_Category_1',hue='Gender',data=df,alpha = 0.8)\nplt.title('Item count',fontsize=14)\nplt.xlabel('Product category 1',fontsize=14)\nplt.ylabel('')\nplt.legend(['Female','Male'],frameon=True,fontsize=12)\nplt.tick_params(labelsize=15)\nplt.subplot(122)\nsns.barplot('Product_Category_1','Purchase',hue='Gender',data=df_Tpurchase_by_PC1_Gender,alpha = 0.8)\nplt.title('Total purchase',fontsize=14)\nplt.xlabel('Product category 1',fontsize=14)\nplt.ylabel('')\nplt.legend().set_visible(False)\nplt.tick_params(labelsize=15)","14a0eb48":"le_U_ID = LabelEncoder()\ndf['User_ID'] = le_U_ID.fit_transform(df['User_ID'])\nle_P_ID = LabelEncoder()\ndf['Product_ID'] = le_P_ID.fit_transform(df['Product_ID'])\ndf['Gender'] = np.where(df['Gender']=='M',1,0) # Female: 0, Male: 1\ndf_Age = pd.get_dummies(df.Age)\ndf_CC = pd.get_dummies(df.City_Category)\ndf_SIC = pd.get_dummies(df.Stay_In_Current_City_Years)\ndf_encoded = pd.concat([df,df_Age,df_CC,df_SIC],axis=1)\ndf_encoded.drop(['Age','City_Category','Stay_In_Current_City_Years'],axis=1,inplace=True)","336fbf9a":"df_frac = df_encoded.sample(frac=0.02,random_state=100)\nX = df_frac.drop(['Purchase'], axis=1)\ny = df_frac['Purchase']\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=100)\n\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","155663b5":"param_grid = {'n_estimators':[1,3,10,30,100,150,300]}\ngrid_rf = GridSearchCV(RandomForestRegressor(),param_grid,cv=3,scoring='neg_mean_squared_error').fit(X_train_scaled,y_train)\nplt.figure()\nplt.plot(list(param_grid.values())[0],(-1*grid_rf.cv_results_['mean_test_score'])**0.5)\nplt.xlabel('Number of trees')\nplt.ylabel('3-fold CV RMSE')\nprint('Best parameter: {}'.format(grid_rf.best_params_))\nprint('Best score: {:.2f}'.format((-1*grid_rf.best_score_)**0.5))","c99438c3":"param_grid = {'n_estimators':[1,3,10,30,100,150,300],'max_depth':[1,3,5,7,9]}\ngrid_rf = GridSearchCV(RandomForestRegressor(),param_grid,cv=3,scoring='neg_mean_squared_error').fit(X_train_scaled,y_train)","1a4d09d5":"print('Best parameter: {}'.format(grid_rf.best_params_))\nprint('Best score: {:.2f}'.format((-1*grid_rf.best_score_)**0.5))","f26dd68f":"train_sizes, train_scores, valid_scores = learning_curve(RandomForestRegressor(max_depth=7, n_estimators=150), X_train_scaled, y_train, cv=3, scoring='neg_mean_squared_error')","3decff75":"train_scores = (-1*train_scores)**0.5\nvalid_scores = (-1*valid_scores)**0.5\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes,valid_scores_mean,label='valid')\nplt.plot(train_sizes,train_scores_mean,label='train')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3,color=\"g\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,valid_scores_mean + valid_scores_std, alpha=0.3, color=\"b\")\nplt.xlabel('Number of samples')\nplt.ylabel('RMSE')\nplt.legend()","6a5842b2":"rf = RandomForestRegressor(max_depth=7, n_estimators=150).fit(X_train_scaled,y_train)\nf_im = rf.feature_importances_.round(3)\nser_rank = pd.Series(f_im,index=X.columns).sort_values(ascending=False)\n\nplt.figure()\nsns.barplot(y=ser_rank.index,x=ser_rank.values,palette='deep')\nplt.xlabel('relative importance')","13746e25":"X = df_encoded.drop(['Purchase'], axis=1)\ny = df_encoded['Purchase']\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=100)\n\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nrf = RandomForestRegressor(max_depth=7, n_estimators=150).fit(X_train_scaled,y_train)\ny_predicted = rf.predict(X_test_scaled)\nprint('Test set RMSE: {:.3f}'.format(mean_squared_error(y_test,y_predicted)**0.5))","6682b175":"df_ocup = pd.get_dummies(df.Occupation)\ndf_encoded = pd.concat([df_encoded,df_ocup],axis=1)\ndf_encoded.drop(['Occupation'],axis=1,inplace=True)","12038166":"df_frac = df_encoded.sample(frac=0.02,random_state=100)\nX = df_frac.drop(['Purchase','User_ID','Product_ID','Product_Category_1','Product_Category_2','Product_Category_3'], axis=1)\ny = df_frac['Purchase']\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=100)","f3d1ca04":"param_grid = {'n_estimators':[1,3,10,30,100,150,300],'max_depth':[1,3,5,7,9]}\ngrid_rf = GridSearchCV(RandomForestRegressor(),param_grid,cv=3,scoring='neg_mean_squared_error').fit(X_train,y_train)","efea5e8f":"print('Best parameter: {}'.format(grid_rf.best_params_))\nprint('Best score: {:.2f}'.format((-1*grid_rf.best_score_)**0.5))","ff7e01fc":"train_sizes, train_scores, valid_scores = learning_curve(RandomForestRegressor(max_depth=3, n_estimators=300), X_train, y_train, cv=3, scoring='neg_mean_squared_error')","64ccf9e0":"train_scores = (-1*train_scores)**0.5\nvalid_scores = (-1*valid_scores)**0.5\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes,valid_scores_mean,label='valid')\nplt.plot(train_sizes,train_scores_mean,label='train')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3,color=\"g\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,valid_scores_mean + valid_scores_std, alpha=0.3, color=\"b\")\nplt.xlabel('Number of samples')\nplt.ylabel('RMSE')\nplt.legend()","2157fec7":"rf = RandomForestRegressor(max_depth=3, n_estimators=300).fit(X_train,y_train)\nf_im = rf.feature_importances_.round(3)\nser_rank = pd.Series(f_im,index=X.columns).sort_values(ascending=False)\n\nplt.figure()\nsns.barplot(y=ser_rank.index,x=ser_rank.values,palette='deep')\nplt.xlabel('relative importance')","4ac17049":"y_predicted = rf.predict(X_test)\nprint('Test set RMSE: {:.3f}'.format(mean_squared_error(y_test,y_predicted)**0.5))","9cb03fd4":"This dataset has 537577 entries with 12 columns. It is a lot of data (~half million) that contains customer-specific (`User_ID`) and product-specific (`Product_ID`,`Product_Category_1`,`Product_Category_2`,`Product_Category_3`) information. The unique `User_ID` and `Product_ID` are ~5900 and ~3600, respectively. They are quite small compared to the total number of entries, therefore, it can be inferred that a great portion of `User_ID` and `Product_ID` repeat many times and they may contain crucial information. **However**, these information are only available when the target variable, `Purchase`, is determined at the cashier. Therefore, training the data using these information may cause undersirable **data leakage** depending on the goal of the project. The store may want to predict `Purchase` soley based on the low-level information (i.e. `Gender`, `Age`, etc) so that they can reflect those in their marketing strategies that target new customers. \n\nIn this project, both cases will be considered and two separate models will be trained for comparison.\n* Case 1: Training with all features (including customer- and product- specific information)\n* Case 2: Training with low-level features only\n\nA brief investigation revealed that ~31% and ~69% data are NaNs in ***Product_Category_2*** and ***Product_Categroy_3***, respectively. As the datatype of `Product_Category_1` is *int*, these NaNs were filled with 0.\n\nLastly, it should be noted that most features are categorical.","fbe8e527":"In this specific random grid Search,`max_depth` = 7 and `n_estimators` = 150 were found to be optimal. With the two parameters optimized, however, the 3-fold CV score is still quite low. \nNext, the learning curve can be considered to understand the performance of the model better.\n\n#### Model investigation by the learning curve","39091f14":"# Quick overview of the data\n#### Dataset structure","d8e4aa61":"Only the number of trees used in the ensemble (`n_estimators`) was roughly scanned for the initial attempt.\nAs the number of tree in RF increased, the average CV RMSE decreased. While it seemed to be saturated at `n_estimators`=~100, it was difficult to tell why the model was still suffering with high RMSE.\n\nBy considering `max_depth`, more optimized parameters can be searched up.","015ddf7f":"#### Hyperparameter selection through 3-fold cross-validation: RMSE","c7f7d548":"####  Purchase by City by Marrital status and Residency duration","f530f16d":"#### Unique element in each column","48bc4ecf":"This tells that the current model heavily relies on `Product_Category_1` (followed by `Product_ID` and `User_ID`) to make prediction on `Purchase`. This was somewhat expected from EDA as they are directly related to the amount of `Purchase` that the customers make. I interpret this as a strong sign of **data leakage**. Therefore in Case 2, these suspicious features will be removed in training the second model.\n\nAnyways, this can be trained with the entire dataset available to mitigate the high variance issue and its performance can be evaluated on the test set.","67f217ba":"# ML model (Random Forest) fitting (Case 1):\n\nSince most features are discrete, *Random Forest Regressor* is expected to fit the data well. Since the given dataset contains ~half million entries, using all of them may cause running-time issue on my machine when trying to do some iterative works like generating the learning curve. Therefore, only the fraction (1\/50) of its data (~10k) will be randomly sampled for initial ML model fitting attempts.\n#### Random sampling and train\/test split","95220acc":"#### Filling the missing values","ee390527":"####  Count\/Purchase by City by Occupation","be8dad53":"From the learning curve, it seems like utilizing more training set would not help much. With 1\/50th of the dataset, this model achieves RMSE of ~4876.105 on the test set.","6e3a30b5":"#### Utilize the entire dataset for evalution on test set: RMSE","5b2a71f0":"In this project, the blackfriday dataset has been explored. While EDA revealed some interesting relationships among different features, for the purpose of predicting customer's ***Purchase***, the top three features useful included ***Product_Category_1***, ***Product_ID*** and ***User_ID***. With a roughly tuned Random Forest model with `max_depth` = 9 and `n_estimators` = 300, the ***RMSR*** for predicting ***Purchase*** of this model was shown to be ~***2911***. \n\nHowever, if the goal is to predict `Purchase` for the new customer who have never been in the store, those customer-specific and product-specific information cannot be utilized. In this case, only the low-level features can be utilized to train the model. In this case, the model perfornace of the test set was ~***4876***.\n\nThe results are quite dissapointing. However, at the same time, it looks quite reasonable for this synthetically generated dataset where the given low-level features showed to have very weak relationships with the target variable. \n\nFor further optimization of the model, the parameters of RF can be tuned more intensively (by exploring more\/deeper trees) or other more sophisticated ML models, such as Gradient Boosted Decision Trees and neural network, can be investigated on this dataset to overcome the bias problem.","08bf0ea6":"In both gender, the age group that purchased most was [26-35]. The total number of customers who made transactions and their total purchase amount were very strongly correlated. This was well reflected in the thrid subplot where the average purchase appeared to be very similiar in all different age groups. In other words, the total purchase of [26-35] was the most simply because there were more customers in that age group. This may mean that ***Age*** may not be a strong predictor for ***Purchase***. However, ***Gender*** may be helpful as it was clear that Male spent more than Female.\n\nAssuming the Gender ratio to be simliar in each city, different city may have different occupation distribution. If that is the case, the ***city*** that the customer comes from may be a strong predictor for ***Purchase*** as customers with certain occupations may spend more during shopping. ","b09dec36":"The target varaible is slightly right skewed, but looks more or less like normal.\n#### Distributions of `User_ID` and `Product_ID`","f0efa32d":"# Feature engineering for model fitting\n\nWhile the ordinal categorical features can be simply encoded with integers, the nominal categorical features need to be one-hot-encoded. In this case, however, the number of unique entires for `User_ID` and `Product_ID` is too big and one-hot-encoding these features will unnecessarily increase the data dimension and therefore cardinality. In real case, some other technique like *feature hashing* may be considered, but for the scope of this project where these two are being considered to be the source of data leakage, thses two features will be simply encoded with integers. Except for `Gender` and `Marital_Status` which are binary, all other nominal categorical data will be, however, one-hot-encoded.","7fc0cfe2":"#### Model investigation by the learning curve","f6bd7b84":"# EDA","92049cb1":"Considering only the low-level features, now it seems like the model suffers from high bias issue. Knowing that this dataset is artificial, it could possibly be that the given low-level features do not simply have enough predictive power for `Purchase`.\n#### Feature importance","f4fff608":"Certainly, specific users and products are contributing more in terms of `Purchase`. Based on these figures, `Product_ID` seems to have stronger predictive power than `User_ID`. ","a2a2ebb8":"The dataset used in this project, *BlackFriday.csv*, was downloaded from [Kaggle](https:\/\/www.kaggle.com\/mehdidag\/black-friday).\n\n# Background\nThis dataset contains samples of transactions made in a retail store on Black Friday. The main goal of this project is to train a model that can predict ***Purchase*** given (old or new) customers' information. Such information is invaluable for stores as it can serve as a solid foundation for more effective future sales\/marketing strategies to maximize their profits. \n\nFirst, the entire dataset will be explored through EDA. Then, it is going be a straightforward regression problem where the model performance is evaluated by root mean squared error (RMSE). This project will focus on a step-by-step Random Forest model development process, rather than intensely seaching for optimized hyperparameters and better training models.\n\n#### Import dataset and libraries","0a7ca332":"However, in each city, the occupation distribution was quite similar. As observed in the age distribution above, the number of customers and total purchase amounts from customers in different occupations in each city showed strong correlation. This was also well reflected in the last subplot where the average purchase was similiar in all different groups of consideration. One thing to be noted was that the average purchase of customers with *Occupation #8* from *City A* showed distinctively high average purchase.\n\nOther features can also be investigated in terms of `Purchase`.","ef0cbc34":"#### Average purchase by `Product_ID`","be92f3f6":"#### Purchase distribution","4fa8556e":"#### Evalution on test set: RMSE","7de661fd":"From the learning curve, it is shown that the RF model with `max_depth` = 7 and `n_estimators` = 300 is suffering with a variance problem. We may consider investigating the feature importance to remove some features. \n#### Feature importance","c62a698e":"These distributions shows that if the model is trained for these customer- and product-specific information, it will help predicting `Purchase`. This can be seen in more detail in the figures below.\n#### Average purchase by `User_ID`","3275641a":"#### Checking for missing values","e863435b":"It was shown that unmarried customers spent more than the married. Customers who lived in their city for 1 year tent to spend more than other groups.   \n\nAlso, it would be interesting to find the ***Product_Category_1*** that was the most famous. \n####  Count\/Purchase by Product_Category_1 by Age and Gender","060ced86":"# Conclusion","03044066":"#### Hyperparameter selection through 3-fold cross-validation: RMSE","02e6d73f":"####  Count\/Purchase by Age by Gender","dd7bdd8b":"# ML model (Random Forest) fitting (Case 2):\n#### Data cleaning - removing leackage-causing features &  Random sampling and train\/test split","47acc7bc":"In general, \n* Male shopped more than Female\n* Single shopped more than Married\n* Customers from *City B* shopped the most\n* Customers who has resided in their city for 1 year shopped the most \n* *Product_category_1 #1,5,8* were the most selling \n* *Product_category_1 #1* made the most profit\n\nThese relationships between different features can be investigated further to set the new marketing strategies to maximize the profit of the retail store (possibly for the future black Fridays). For example, the retail store may consider doing more advertisements targetting their unmarried male customers in *City B* on *product_category_1 #1*."}}