{"cell_type":{"2d746d9b":"code","bafdbec0":"code","827ddbcb":"code","953df0fa":"code","461def12":"code","83198141":"code","cbd0d4e1":"code","3c4547dd":"code","9e7bdad0":"code","b9cf9f65":"code","c138727f":"code","21ab32ff":"code","19c0e666":"code","1be034ef":"code","6cd5b9c4":"code","87738911":"code","1afab3d2":"code","c4f604b3":"code","b30cb6e0":"code","b33a1abf":"code","8f79261d":"code","5310eddc":"code","d5c2bcfa":"code","4ee8cfdf":"code","eede580d":"code","10a16086":"code","db0d2526":"code","00ae2415":"code","91029879":"code","cdbc2eda":"markdown","de5231fe":"markdown","06a2ed8d":"markdown","420f1e52":"markdown","012ab5bc":"markdown","3caae0b4":"markdown","fe8a39e7":"markdown","f7731329":"markdown","0e942110":"markdown"},"source":{"2d746d9b":"import pickle\nimport operator\n\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom itertools import chain\nfrom os.path import isfile\nfrom PIL import Image as pil_image\nfrom tqdm import tqdm_notebook, tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom torch.utils import data\nfrom torch.optim.lr_scheduler import StepLR\nfrom torchvision import models\nfrom torchvision import transforms\n\n# Avoid pil error\npil_image.MAX_IMAGE_PIXELS = None\n\n# For reproducibility purpose\ntorch.manual_seed(42)\nnp.random.seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","bafdbec0":"train_df_path = '..\/input\/whale-categorization-playground\/train.csv'\ntest_df_path = '..\/input\/whale-categorization-playground\/sample_submission.csv'\nTRAIN = '..\/input\/whale-categorization-playground\/train\/train\/'\nTEST = '..\/input\/whale-categorization-playground\/test\/test\/'","827ddbcb":"rotate_path = '..\/input\/humpback-whale-identification-model-files\/rotate.txt'\nexclude_path = '..\/input\/humpback-whale-identification-model-files\/exclude.txt'\nbboxes_path = '..\/input\/humpback-whale-identification-model-files\/bounding-box.pickle'","953df0fa":"train_df = pd.read_csv(train_df_path, header=0)\ntest_df = pd.read_csv(test_df_path)","461def12":"def expand_path(p):\n    '''Takes image name and returns full path'''\n    if isfile(TRAIN + p):\n        return TRAIN + p\n    if isfile(TEST + p):\n        return TEST + p\n    return p\n\n\ndef show_whale(imgs, per_row=2):\n    n = len(imgs)\n    rows = (n + per_row - 1) \/\/ per_row\n    cols = min(per_row, n)\n    fig, axes = plt.subplots(rows, cols, figsize=(24 \/\/ per_row * cols, 24 \/\/ per_row * rows))\n    \n    for ax in axes.flatten(): \n        ax.axis('off')\n    for i, (img, ax) in enumerate(zip(imgs, axes.flatten())): \n        ax.imshow(img.convert('RGB'))\n","83198141":"NORMALIZATION_MEAN = [0.485, 0.456, 0.406]\nNORMALIZATION_STD = [0.229, 0.224, 0.225]","cbd0d4e1":"# Load array of images to rotate\n# https:\/\/www.kaggle.com\/martinpiotte\/humpback-whale-identification-model-files\nwith open(rotate_path, 'rt') as f: \n    rotate = set(f.read().split('\\n')[:-1])\n    \n# Load array of images to exclude\n# https:\/\/www.kaggle.com\/martinpiotte\/humpback-whale-identification-model-files\nwith open(exclude_path, 'rt') as f: \n    exclude = f.read().split('\\n')[:-1]   \n    \n# Load bounding boxes data\n# https:\/\/www.kaggle.com\/martinpiotte\/humpback-whale-identification-model-files\nwith open(bboxes_path, 'rb') as f:\n    bboxes = pickle.load(f)","3c4547dd":"show_whale([pil_image.open(expand_path(img)) for img in exclude[14:29]], 5)\ntrain_df = train_df[~train_df['Image'].isin(exclude)]\nprint('Excluded images: ')","9e7bdad0":"show_whale([pil_image.open(expand_path(img)) for img in rotate], 5)\nprint('Rotated whale tales: ')","b9cf9f65":"train_df = train_df[train_df['Id'] != 'new_whale'].reset_index(drop=True)\ntrain_df.shape","c138727f":"# The margin added around the bounding box to compensate for bounding box inaccuracy\nIMAGE_SIZE = (224, 224)\ncrop_margin  = 0.05 \nresize = transforms.Resize(IMAGE_SIZE)\n\ndef load_image(image_name):\n    image = pil_image.open(expand_path(image_name)).convert('RGB')\n    width, height = image.size\n\n    # Crop bounding box with respect to crop margin\n    x0, y0, x1, y1 = bboxes[image_name]\n    dx = x1 - x0\n    dy = y1 - y0\n    x0 -= dx * crop_margin\n    x1 += dx * crop_margin + 1\n    y0 -= dy * crop_margin\n    y1 += dy * crop_margin + 1\n    if (x0 < 0):\n        x0 = 0\n    if (x1 > width):\n        x1 = width\n    if (y0 < 0):\n        y0 = 0\n    if (y1 > height):\n        y1 = height\n    try:\n        # A few images have incorrect bounding boxes\n        image = image.crop((x0, y0, x1, y1))\n    except:\n        pass\n    # Rotate whale tails which are upside-down\n    if image_name in rotate:\n        image = image.rotate(180)\n        \n    return resize(image)","21ab32ff":"cache = {}\n\nfor image_name in tqdm_notebook(train_df['Image']):\n    cache[image_name] = load_image(image_name)\n    \ndef get_image(image_name):\n    '''Returns cropped and resized image either from cache or from disk'''\n    return cache.get(image_name) or load_image(image_name)","19c0e666":"class WhaleDataset(data.Dataset):\n    '''\n    PyTorch class for Whales' tails\n    Link: https:\/\/www.kaggle.com\/c\/whale-categorization-playground\/data\n\n    Data issues mentioned:\n    - some bboxes are incorrect\n    '''\n\n    def __init__(self, images_data, scope='train', augment=False):\n        self.image_data = images_data\n        self.scope = scope\n\n        if augment:\n            self.transform = transforms.Compose([\n                transforms.ColorJitter(brightness=0, contrast=0.05, saturation=0.05),\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.ToTensor(),\n                transforms.Normalize(NORMALIZATION_MEAN, NORMALIZATION_STD)\n            ])\n\n        else:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(NORMALIZATION_MEAN, NORMALIZATION_STD)\n            ])\n\n        print('Images: {}. Augmentation: {}. Scope: {}.'.format(len(self.image_data), augment, scope))\n\n    def __getitem__(self, idx):\n\n        '''\n        For train and validation triplets are required, for prediction - only images;\n        '''\n        image_mode = 'RGB'\n        row = self.image_data.iloc[idx]\n        anchor_name = row['Image']\n        anchor = get_image(anchor_name)\n\n        anchor = self.transform(anchor)\n        if self.scope == 'train' or self.scope == 'val':\n            anchor_id = row['Id']\n\n            positive_candidates = list(self.image_data[self.image_data['Id'] == anchor_id]['Image'])\n            positive_candidates = [x for x in positive_candidates if x != anchor_name]\n\n            if len(positive_candidates) == 0:\n                positive_name = anchor_name\n            else:\n                positive_name = np.random.choice(positive_candidates)\n\n            negative_candidates = list(\n                self.image_data[(self.image_data['Id'] != anchor_id)]['Image']\n            )\n            negative_name = np.random.choice(negative_candidates)\n\n            positive = get_image(positive_name)\n            negative = get_image(negative_name)\n\n            positive = self.transform(positive)\n            negative = self.transform(negative)\n\n            return {'name': anchor_name,\n                    'anchor': anchor,\n                    'positive': positive,\n                    'negative': negative\n                    }\n        else:\n            return {'name': anchor_name, 'anchor': anchor}\n    \n\n    def __len__(self):\n        return len(self.image_data)","1be034ef":"# Group images by id\ngrouped = train_df.groupby('Id')\n\nvalidation_indexes = []\n\nfor group in grouped.groups.items():\n    indexes = group[1]\n    # Take only one image from class which has at least 3 images\n    if (len(indexes) > 2):\n        validation_indexes.append(indexes[0])\n        \nvalidation_df = train_df.iloc[validation_indexes].reset_index(drop=True)\ntrain_df = train_df[~train_df.index.isin(validation_indexes)].reset_index(drop=True)\n\nvalidation_df.shape, train_df.shape","6cd5b9c4":"train_dataset = WhaleDataset(train_df, augment=True)\ntrain_dataloader = data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers = 4, drop_last=False)","87738911":"validation_df = WhaleDataset(validation_df)\nvalidation_dataloader = data.DataLoader(validation_df, batch_size=32, shuffle=False, num_workers = 4, drop_last=False)","1afab3d2":"EMBEDDINGS_SIZE = 500\n\nclass ResNet34(nn.Module):\n    '''\n    Last fully connected layer changed to ouput EMBEDDINGS_SIZE-dim vector.\n    '''\n    \n    def __init__(self):\n        super(ResNet34, self).__init__()\n        self.model = models.resnet34(pretrained=True)\n        self.model.fc = nn.Linear(in_features=512, out_features=EMBEDDINGS_SIZE, bias=True)\n\n    def forward(self, image):\n        features = self.model(image)\n        return features\n\n\nmodel = ResNet34().cuda()","c4f604b3":"class TripletLossCosine(nn.Module):\n    def __init__(self):\n        super(TripletLossCosine, self).__init__()\n        self.MARGIN = 0.7\n            \n    def forward(self, anchor, positive, negative):\n        dist_to_positive = 1 - F.cosine_similarity(anchor, positive)\n        dist_to_negative = 1 - F.cosine_similarity(anchor, negative)\n        loss = F.relu(dist_to_positive - dist_to_negative + self.MARGIN)\n        loss = loss.mean()\n        return loss\n    \nloss_func = TripletLossCosine()","b30cb6e0":"optimizer = torch.optim.SGD(params=model.parameters(), lr=0.007, momentum=0.9)\nlr_scheduler = StepLR(optimizer, step_size=10, gamma=0.4)","b33a1abf":"def validate(model): \n    model.eval()\n    batch_losses = []\n\n    with torch.no_grad():\n        for sample in validation_dataloader:\n            for key in ['anchor','positive','negative']:\n                sample[key] = sample[key].cuda()\n\n        anchor_embed = model(sample['anchor'])\n        positive_embed = model(sample['positive'])\n        negative_embed = model(sample['negative'])\n        loss = loss_func(anchor_embed, positive_embed, negative_embed) \n\n        batch_losses.append(loss.item())\n    return batch_losses","8f79261d":"train_losses = []\nvalidation_losses = []\n\nfor epoch in range(1, 12):\n    batch_losses = []\n\n    for sample in tqdm_notebook(train_dataloader):\n        model.train()\n        optimizer.zero_grad()\n\n        for key in ['anchor','positive','negative']:\n            sample[key] = sample[key].cuda()\n\n        anchor_embed = model(sample['anchor'])\n        positive_embed = model(sample['positive'])\n        negative_embed = model(sample['negative'])\n        loss = loss_func(anchor_embed, positive_embed, negative_embed)  \n        loss.backward()\n        \n        optimizer.step()\n        if epoch == 10:\n            optimizer = torch.optim.SGD(params=model.parameters(), lr=0.0005, momentum=0.9)\n\n#         lr_scheduler.step()\n        batch_losses.append(loss.item())\n        \n    train_losses.append(np.mean(batch_losses))\n    \n    val_loss = validate(model)\n    val_loss_mean = np.mean(val_loss)\n    validation_losses.append(val_loss_mean)\n\n    print('====Epoch {}. Train loss: {}. Val loss: {}'.format(epoch,  np.mean(batch_losses),  val_loss_mean))\n","5310eddc":"epochs = list(range(1, len(train_losses)+1))\nfig, ax = plt.subplots(figsize=(18,7))\n\nsns.lineplot(x=epochs, y=train_losses, palette=\"tab10\", linewidth=2.5, ax=ax);\nsns.lineplot(x=epochs, y=validation_losses, palette=\"tab10\", linewidth=2.5, ax=ax);\nax.legend(['Training loss', 'Validation loss']);\nax.set_xlabel('Epochs')\nax.set_ylabel('Loss');","d5c2bcfa":"all_images = pd.DataFrame({'Image': pd.concat([train_df['Image'], test_df['Image']])}).reset_index(drop=True)\n\nembed_dataset = WhaleDataset(all_images, scope='embed')\nembed_dataloader = data.DataLoader(embed_dataset, batch_size=32, shuffle=False, num_workers = 4, drop_last=False)\nembeddings_dict = {}\n\nmodel.eval()\nwith torch.no_grad():\n    for sample in tqdm_notebook(embed_dataloader):\n        anchors = sample['anchor'].cuda()\n        embeds = model(anchors)\n        \n        for image_name, embed in zip(sample['name'], embeds):\n            embeddings_dict[image_name] = embed.cpu().numpy()","4ee8cfdf":"assert len(embeddings_dict) == len(all_images)","eede580d":"train_embeds = np.zeros((len(train_df), EMBEDDINGS_SIZE))\ntest_embeds = np.zeros((len(test_df), EMBEDDINGS_SIZE))\n\nfor index, image_name in enumerate(train_df['Image']):\n    train_embeds[index] = embeddings_dict[image_name]\n    \nfor index, image_name in enumerate(test_df['Image']):\n    test_embeds[index] = embeddings_dict[image_name]","10a16086":"similarities = cosine_similarity(test_embeds, train_embeds)","db0d2526":"def find_top_k(sims, k):\n    top_sims = sims.argsort()[::-1]\n    top_klasses = set(['new_whale'])\n    for sim in top_sims:\n        klass = train_df.iloc[sim]['Id']\n        top_klasses.add(klass)\n        if len(top_klasses) == k:\n            break\n    return ' '.join(top_klasses)\n\n\nfind_top_k(similarities[666], 5)\n","00ae2415":"for index in tqdm_notebook(range(len(test_df))):\n    test_df.iloc[index]['Id'] = find_top_k(similarities[index], 5)","91029879":"test_df.to_csv('submission.csv', index=False)","cdbc2eda":"### Calculate all embeddings","de5231fe":"### Cache training images to speedup train dataloader","06a2ed8d":"### Find top 5 classes and submit","420f1e52":"### Pretrained ResNet34  with 500 embeddings","012ab5bc":"### Training","3caae0b4":"#### Siamese NN validation is a little bit tricky. We will take only images from classes with more than 3 images per class.","fe8a39e7":"### Training \/ Validation datasets","f7731329":"### Image loading","0e942110":"### Define Triplet loss with margin 0.7"}}