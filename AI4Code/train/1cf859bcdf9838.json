{"cell_type":{"c017fbfe":"code","2806523a":"code","856f7cc0":"code","fcba44a4":"code","d991df2f":"code","2c0386c9":"code","1e414b49":"code","8b3968ba":"code","025bf1ad":"code","ddd8f46c":"code","27629550":"code","ad481117":"code","c42f9d40":"code","6aa64df4":"code","aedf8068":"code","deaa5566":"code","f4c12d06":"code","2088f8d6":"code","d9b32798":"code","f85cf2d6":"code","3590bb49":"code","3ed08593":"code","7fc0e1b0":"code","e6208989":"code","5fb46b6c":"code","bd624047":"code","73f8bedc":"code","2f58cb6c":"code","892b4328":"code","c858d38b":"code","11da073e":"code","2fd3e215":"code","bb06a40f":"code","cfcbb053":"code","b33cd3b5":"code","01bd25ea":"code","e6b212a0":"markdown","fff8ce28":"markdown","41165b62":"markdown","fc3b0622":"markdown","f7159a4b":"markdown","32f8b388":"markdown","caf9e83a":"markdown","c289b155":"markdown","7652a538":"markdown","02f4af9a":"markdown","1163fe98":"markdown","d6fb5cb1":"markdown","53944be4":"markdown","0cac5449":"markdown","ff0f806c":"markdown","c535015c":"markdown","7ae27dbe":"markdown","64d3e06d":"markdown","72472362":"markdown","6cb63939":"markdown","b2f73462":"markdown","aa2525f8":"markdown","8dfe3db2":"markdown","2527442e":"markdown","831b9be2":"markdown","67086875":"markdown","ef0c9ba5":"markdown","3f45f000":"markdown","c710ee9e":"markdown","e9561e69":"markdown","fff73003":"markdown","0cc02e15":"markdown"},"source":{"c017fbfe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2806523a":"# load training dataset\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","856f7cc0":"# load testing dataset\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","fcba44a4":"# get the total number of rows and columns and data type in each column\nprint(train_data.shape)\ntrain_data.dtypes","d991df2f":"# Separate target from predictors\ny=train_data.Survived\nX=train_data.drop(['Survived'], axis=1)\n\n# Divide data into training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)","2c0386c9":"# get the number of missing data points per column\nmissing_values_count = train_data.isnull().sum()\n\n# only show the columns with missing values\nmissing_values_count[missing_values_count>0]","1e414b49":"# drop four columns\ndrop_cols=['PassengerId', 'Cabin', 'Name', 'Ticket']\nreduced_X_train=X_train_full.drop(drop_cols, axis=1)\nreduced_X_valid=X_valid_full.drop(drop_cols, axis=1)","8b3968ba":"# fill the missing values with the value in next row in Embarked\nreduced_X_train.Embarked=reduced_X_train.Embarked.fillna(method='bfill', axis=0).fillna(method='ffill',axis=0)\nreduced_X_valid.Embarked=reduced_X_valid.Embarked.fillna(method='bfill', axis=0).fillna(method='ffill',axis=0)\n\n# double check if all missing values in Embarked are fixed\nprint(reduced_X_train.Embarked.isnull().any())\nprint(reduced_X_valid.Embarked.isnull().any())","025bf1ad":"# get the statistics of column Age group by survived or not\nprint(train_data.groupby('Survived').Age.describe())\n\n# have a look at the age distribution\nax=sns.displot(x='Age', hue='Survived', data=train_data)\nax.set(ylabel='Number of people')\nplt.title('Histogram of Ages, by Survived')","ddd8f46c":"# Calculate correlation efficient: Pearson's r\ntrain_data.Age.corr(train_data.Survived)","27629550":"# have a look at the statistics after grouping by sex\nreduced_X_train.groupby('Sex').Age.describe()","ad481117":"# find the median age of female and male respectively\nage_median_train=reduced_X_train.groupby('Sex').Age.median()\n\n# define a function to fill missing values according to column Sex\ndef imputeAge(row, age_median):\n    if np.isnan(row.Age):\n        if row.Sex=='female':\n            row.Age=age_median['female']\n        else:\n            row.Age=age_median['male']\n    return row\n\n# apply function to dateframe\nfilled_X_train=reduced_X_train.apply(lambda row: imputeAge(row, age_median_train), axis=1)\n\n# double-check missing values in training subset\nfilled_X_train.isnull().any()","c42f9d40":"# repeat the process for validation data\nage_median_valid=reduced_X_valid.groupby('Sex').Age.median()\n# apply the existing function directly\nfilled_X_valid=reduced_X_valid.apply(lambda row: imputeAge(row, age_median_valid), axis=1)\n# double-check missing values in validation subset\nfilled_X_valid.isnull().any()","6aa64df4":"# have a look at unique values in two categorical columns\nprint(filled_X_train['Sex'].unique())\nprint(filled_X_train['Embarked'].unique())","aedf8068":"# transform categorical data into numerical data\ncat_cols=['Sex', 'Embarked']\ntrans_X_train=pd.get_dummies(filled_X_train, columns=cat_cols)\n# repeat the same process for validation data\ntrans_X_valid=pd.get_dummies(filled_X_valid, columns=cat_cols)\n# have a look at output\ntrans_X_valid.head()","deaa5566":"# Calculate survival rate of female\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)","f4c12d06":"# Calculate survival rate of male\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)","2088f8d6":"# survival rate of kids <= 18 years old\nkids_18=train_data.loc[train_data.Age<=18]['Survived']\nrate_kids_18=sum(kids_18)\/len(kids_18)\nprint('% of kids <=18 who survived:', rate_kids_18)\n\n# survival rate of kids <=5 years old\nkids_5=train_data.loc[train_data.Age<=5]['Survived']\nrate_kids_5=sum(kids_5)\/len(kids_5)\nprint('% of kids <=5 who survived:', rate_kids_5)","d9b32798":"kids_18_c1=train_data.loc[(train_data.Age<=18)&(train_data.Pclass==1)]['Survived']\nrate_kids_18_c1=sum(kids_18_c1)\/len(kids_18_c1)\nprint('% of kids in first class who survived:', rate_kids_18_c1)","f85cf2d6":"train_data['Passenger']=train_data.Sex.copy()\ndef FindKid(row):\n    if row.Age<=18:\n        row.Passenger='kid'\n    return row\ntrain_data=train_data.apply(FindKid, axis=1)\ntemp_df=train_data.groupby(['Pclass', 'Passenger']).Survived.agg([len, sum])\ntemp_df['Rate']=temp_df['sum']\/temp_df['len']\ntemp_df.columns=['Total','Survived','Survival_Rate']\nprint(temp_df)","3590bb49":"# visualization\nax=sns.catplot(x=\"Pclass\",hue='Passenger', y=\"Survived\", data=train_data, ci=None,\n            kind='bar')\nax.set(xlabel='Passenger Class', ylabel='Survival Rate', xticklabels=['First','Second','Third'],\n       title='Survival Rate of Male, Female and Kid, by Passenger Class')","3ed08593":"# get the statistics of Fare column\nprint(train_data.Fare.describe())\n\n# Detecting Outliers:\n# Mathematical Method: calculate IQR score and find how many values are outliers\nFare_Q1=train_data.Fare.quantile(0.25)\nFare_Q3=train_data.Fare.quantile(0.75)\nFare_IQR=Fare_Q3-Fare_Q1\nprint('Q1 =',Fare_Q1,'Q3 =',Fare_Q3,'IQR =',Fare_IQR)\nprint(train_data.Fare[(train_data.Fare<(Fare_Q1-1.5*Fare_IQR))|(train_data.Fare>(Fare_Q3+1.5*Fare_IQR))].count())\n\n# Visualization Method: box plot, outliers will be points \nsns.boxplot(x=train_data.Fare)\nplt.title('Box Plot of Ticke Fare')","7fc0e1b0":"# group by passenger class\nprint(train_data.groupby('Pclass').Fare.describe())\n\n# visualization, only show fare <= 300\nax=sns.boxplot(x='Pclass', y='Fare', hue='Passenger',data=train_data)\nax.set(xlabel='Passenger Class', ylim=[0,300], xticklabels=['First', 'Second','Third'])\nplt.title('Box Plot of Fare, by Passenger Class')","e6208989":"# group by passenger class\nprint(train_data.groupby('Embarked').Fare.describe())\n\n# visualization, only show fare <= 300\nax=sns.boxplot(x='Embarked', y='Fare', data=train_data)\nax.set(xlabel='Embarked', xticklabels=['Southampton','Cherbourg','Queenstown'],ylim=[0,150])\nplt.title('Box Plot of Fare, by Embarked')","5fb46b6c":"# add a new column to show fare level\ntrain_data['Fare_Level']=train_data.Fare.copy()\ndef FareLevel(row):\n    if row.Fare<10:\n        row.Fare_Level='0-10'\n    elif row.Fare<30:\n        row.Fare_Level='10-30'\n    elif row.Fare<100:\n        row.Fare_Level='30-100'\n    elif row.Fare>=100:\n        row.Fare_Level='>100'\n    return row\ntrain_data=train_data.apply(FareLevel, axis=1)","bd624047":"ax=sns.countplot(data=train_data.sort_values(by='Fare_Level'), \n                 x='Embarked', hue='Fare_Level')\nax.set(xticklabels=['Southampton','Queenstown','Cherbourg'], ylabel='Number of People')\nplt.title('Number of People at Different Fare Level, by Embarked')","73f8bedc":"ax=sns.countplot(data=train_data.sort_values(by='Fare_Level'), x='Survived', hue='Fare_Level')\nax.set(ylabel='Number of People', xlabel='', xticklabels=['Not Survived','Survived'])\nplt.title('Number of People at Different Fare Level, by Survived')","2f58cb6c":"alone=train_data.loc[(train_data.SibSp==0)&(train_data.Parch==0)].Survived.agg([sum,len])\nrate_alone=alone['sum']\/alone['len']\nprint('% of people who travel alone survived:', rate_alone)\nwith_family=train_data.loc[(train_data.SibSp>0)|(train_data.Parch>0)].Survived.agg([sum,len])\nrate_with_family=with_family['sum']\/with_family['len']\nprint('% of people who travel with family survived:', rate_with_family)","892b4328":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# build and validate model with one set of parameters\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(trans_X_train, y_train)\npred=model.predict(trans_X_valid)\naccuracy_score(y_valid, pred)","c858d38b":"# function\ndef BuildValidModel(n_esti, max_depth):\n    model = RandomForestClassifier(n_estimators=n_esti, max_depth=max_depth, random_state=1)\n    model.fit(trans_X_train, y_train)\n    pred=model.predict(trans_X_valid)\n    accu=accuracy_score(y_valid, pred)\n    return accu\n\n# parameters I want to test\nn_esti=list(range(25,201,25))\nmax_depth=[3,4,5,6,7,8]\n\n# run function several times and save results in a data frame\naccuracy=dict()\nfor i in range(6):\n    a=[]\n    for j in range(8):\n        a.append(BuildValidModel(n_esti[j],max_depth[i]))\n    accuracy['max_depth='+str(max_depth[i])]=a\naccu_data=pd.DataFrame(accuracy,index=n_esti)\n\n# plot results\nax=sns.lineplot(data=accu_data)\nax.set(xlabel='n_estimators', ylabel='Accuracy')\nplt.title('Test of Different Parameters')","11da073e":"print('Best n_estimator:',50)\nprint('Best max_depth:', 8)\nprint('Highest Prediction Accuracy:',BuildValidModel(50,8))","2fd3e215":"# Select categorical columns\ncategorical_cols = ['Sex','Embarked']\n\n# Select numerical columns\nnumerical_cols = ['Pclass','Age','SibSp','Parch','Fare']\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()","bb06a40f":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","cfcbb053":"# Build model using the best parameters I found above\nmodel=RandomForestClassifier(n_estimators=50, max_depth=8, random_state=1)\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)","b33cd3b5":"from sklearn.model_selection import cross_val_score\n\n# cv is number of folds, 'accuracy'= sklearn.metrics.accuracy_score\nscores = cross_val_score(my_pipeline, X, y,\n                              cv=5,\n                              scoring='accuracy')\n\nprint(\"Accuracy scores:\", scores)\nprint('Average score:', scores.mean())","01bd25ea":"# build model\nmodel = RandomForestClassifier(n_estimators=50, max_depth=8, random_state=1)\nmodel.fit(trans_X_train, y_train)\n\n# need to preprocessing X_test\nreduced_X_test=test_data.drop(drop_cols, axis=1)\nreduced_X_test.Embarked=reduced_X_test.Embarked.fillna(method='bfill', axis=0).fillna(method='ffill',axis=0)\nage_median_test=reduced_X_test.groupby('Sex').Age.median()\nfilled_X_test=reduced_X_test.apply(lambda row: imputeAge(row, age_median_test), axis=1)\nfilled_X_test.Fare=filled_X_test.Fare.fillna(filled_X_test.Fare.median(), axis=0)\ntrans_X_test=pd.get_dummies(filled_X_test, columns=cat_cols)\n\n# double check\uff0cthen found a NaN in Fare, so I add a line of code above\ntrans_X_test.isnull().sum()\n\npredictions = model.predict(trans_X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","e6b212a0":"I then deal with the missing values in Embarked, which is only two. I apply the imputation method that fill the missing value with the value in next row. ","fff8ce28":"The missing values in column 'Age' is less than 20%. I prefer not dropping this column. More importantly, age may be relevant to the survial prediction. I need to have a look at the two columns, age and survival. ","41165b62":"No matter in which class, both females and kids have much higher survival rate than males. Females and kids in first and second classes have higher survival rate than those in third class. Males in first class have the highest survival rate among all three classes. \n\nNext, I want to have a look at the Fare column and detect outliers.","fc3b0622":"Then, I generate a count plot to show the number of people at different fare levels group by Embarked. ","f7159a4b":"In order to find the best parameters (n_estimators and max_depth), I change the above code into a function and test it with different set of parameters. ","32f8b388":"# Introduction\n\nTitanic dataset is one the most widely-used datasets for new starters. In this notebook, I am going to explore this dataset and go through the prediction process using random forest. The aim is to explore data set, practice data cleaning skills and be familiar with prediction model. \n\nThe main steps include:\n1. Data Loading: load data, check data types and split data set\n2. Data Cleaning: deal with missing values and categorical data\n3. Data Exploration: explore patterns in data\n4. Building Model and Validation: build and valid a random forest classifier model using training data set\n6. Prediction and Submission: predict using testing data set and submit to compete\n\nLet's dive into data!","caf9e83a":"Note that the above step also can be done by using SimpleImputer to fill the missing values with the mean, median or mode value along the column. \n\nNext, I need to change categorical into numerical data. \n\nScikit-learn has two classes, LabelEncoder and OneHotEncoder, to transform categorical into numerical data. In general, one-hot coding has a better performance than label encoding on prediction, and one-hot coding is more reasonable for categorical data that has no clear ordering. However, one-hot coding is not suitable for categorical data that includes many unique values due to the large amount of adding entries.","c289b155":"# Step 1: Data Loading","7652a538":"Based on outputs of both methods, we can see there are a quite large amount of outliers in Fare column. However, it is reasonable if the ticket price of first class is much higher than that of third class according to reality. I need to analyse ticket fare with passenger class together. ","02f4af9a":"# Step 5: Prediction and Submission\nI am going to submit the model I built before using pipeline and cross-validation.","1163fe98":"The missing values in column 'Cabin' is about 77%. I then drop this column given it don't have much information. Additionally, column 'PassengerId', 'Name' and 'Ticket' are not relevant based on data description (https:\/\/www.kaggle.com\/c\/titanic\/data). So I drop them together. ","d6fb5cb1":"We can see the mean and median ages are slightly different for female and male. I decided to fill the missing values with the median age of female and male respectively.","53944be4":"# Step 2: Data Cleaning\n\nI still use the whole training data set when exploring data. When I do data cleaning, e.g., dropping columns, imputation and transforming categorical data, I separately process the splitted traing subset and validation subset to prevent train-test contamination problem.\n\nFirstly, I need to handle missing values. ","0cac5449":"Considering the number of unique values in two columns are two and three respectively, which are very small, I decide to use the one-hot coding method to transform them. An easier way to implement this than using the OneHotEncoder class is to use get_dummies function in pandas. ","ff0f806c":"I limit the maximum value of y-axis to 300 to exclude some outliers.\n\nThe distribution of ticket fare with passenger class is the same as my expectation. Unexpectedly, kids in all three classes has the higher median fare than adults. \n\nTicket price also depends on traveling distance. Thus I think there may be a correlation between Fare and Embarked that means where the passenger got on board. ","c535015c":"I use random forest classifier in this work. ","7ae27dbe":"Only half of kids survived. However, for kids who are less than or equal to 5 years old, 70% of them survived. ","64d3e06d":"I then apply cross-validation to get a more accurate measure of my model. ","72472362":"Next, define preprocessing steps: \n* For numerical values, fill the missing values with the 'median' along the column;\n* For categorical values, fill the missing values with the 'most_freqent', then apply one-hot encoding to categorical data. ","6cb63939":"Women has a higher survival rate than men. ","b2f73462":"Next, I need to prepare the training and validation subsets of data through splitting the 'train_data'.","aa2525f8":"Define model and pipeline:","8dfe3db2":"People who travel with family has a higher survival rate than people who travel alone. \n\n# Step 4: Building Model and Validation","2527442e":"I limit the maximum value of y-axis to 150 to exclude some outliers. We can see that the ticket price at Cherbourg is the highest, and Queenstown has the lowest ticket price. \n\nTo eliminate the effect of outliers without dropping rows, I added a new column that shows the ticket level.","831b9be2":"Before proceeding to next step, I want to apply pipeline and cross-validation to clean up my code and get a more accurate measure of my model. Note that in order to simplify preprocessing, I use SimpleImputer to fill the missing values. \n\n# Extra: pipeline and cross-validation\nThe first step is to prepare data sets. I keep the same columns as in trans_train_data. Note that at this stage, missing values and categorical values have not been processed yet. ","67086875":"# Conclusion\nIn this work, I went through some main steps of preparing data and making a prediction. As I said in Introduction, the aim of this work is to brush up skills and to get familiar with the process. Hope you enjoy reading and get some useful skills as I did!\n\nPS: I am still new in this area, so please feel free to tell me if I did anything wrong. I will be really appreciated for your comments!\nThanks in advance!","ef0c9ba5":"From above steps, we can see that the impact of age on survival prediction is very insignificant. However, the figure shows an interesting point that it looks like more kids survived than dead. I will explore this point later. For now, in order to practice skills, I keep this column and fill the missing values.","3f45f000":"Even only half of kids survived, but most kids in first class are survived.\n\nNext, I want to have a look at the survival rate of female, male and kids in different class. Both female and male who are under 18 are counted as kids. ","c710ee9e":"# Step 3: Data Exploration\n\nIn this step, I will explore patterns in this data set. I am curious about the following questions:\n* What is the difference of survival rate between female and male?\n* Do kids have a higher survival rate? How about kids in first class?\n* Does passenger class have an impact on survival?\n* Is there any outlier in the ticket fare column? If there is, is the value possible?\n* Is ticket fare related to passenger class or embarked?\n* How about the survival rate of people with family and people who travel alone?\n...\n\nThere are too many questions we can ask.\n\nI start with the survival rate of female and male.","e9561e69":"Most people got on board at Southampton, and fewest people got on board at Queenstown. Most people who got on board at Queenstown spent less than 30 for their tickets. ","fff73003":"Most people who spent less than 30 are dead. On the other hand, most people who spent more than 30 are survived. ","0cc02e15":"Show above data in a figure. "}}