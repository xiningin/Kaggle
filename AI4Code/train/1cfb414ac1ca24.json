{"cell_type":{"45f64462":"code","d9a675e4":"code","7217441d":"code","b7855d9e":"code","3b8e9a9f":"code","31793a01":"code","af8680e9":"code","41ecff5d":"code","271e4b35":"code","7f29f7fd":"code","765a50b4":"code","22a82b56":"code","bb118461":"code","c2924951":"code","72c1155e":"code","b5492809":"code","c3eb032f":"code","05c11afc":"code","4f296d9f":"code","9b1db647":"code","a125480e":"code","c5933e4e":"code","478fd2ae":"code","c4b24f56":"code","536e3c51":"code","a0ae242a":"code","ab41d000":"code","aa86adc0":"code","462f7b1d":"code","8228e7dd":"code","e08d03ad":"code","b5bf92c7":"code","8564840e":"markdown","ec481e30":"markdown","7511443c":"markdown","68d0e0ec":"markdown","74c348d7":"markdown","e5b9c937":"markdown"},"source":{"45f64462":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d9a675e4":"import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport time\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\nfrom sklearn import metrics\nfrom sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7217441d":"train = pd.read_csv(\"..\/input\/train.csv\") # readingthe training set\ntest = pd.read_csv(\"..\/input\/test.csv\") # reading the test set\nsub = pd.read_csv(\"..\/input\/sample_submission.csv\")\nstructures = pd.read_csv(\"..\/input\/structures.csv\")","b7855d9e":"train.head()","3b8e9a9f":"test.head()","31793a01":"print(f'There are {train.shape[0]} rows in train data.')\nprint(f'There are {test.shape[0]} rows in test data.')\n\nprint(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\nprint(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\nprint(f\"There are {train['atom_index_0'].nunique()} unique atoms.\")\nprint(f\"There are {train['type'].nunique()} unique types.\")","af8680e9":"len(structures)","41ecff5d":"\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain = map_atom_info(train, 0)\ntrain = map_atom_info(train, 1)\n\ntest = map_atom_info(test, 0)\ntest = map_atom_info(test, 1)\n\n","271e4b35":"train.head()","7f29f7fd":"train.describe()","765a50b4":"# checking some variable unique couunt\nprint(train['atom_index_0'].nunique())\nprint(train['atom_index_1'].nunique())\nprint(train['id'].nunique())","22a82b56":"train_p_0 = train[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test[['x_1', 'y_1', 'z_1']].values\n\ntrain['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)\ntrain['dist_x'] = (train['x_0'] - train['x_1']) ** 2\ntest['dist_x'] = (test['x_0'] - test['x_1']) ** 2\ntrain['dist_y'] = (train['y_0'] - train['y_1']) ** 2\ntest['dist_y'] = (test['y_0'] - test['y_1']) ** 2\ntrain['dist_z'] = (train['z_0'] - train['z_1']) ** 2\ntest['dist_z'] = (test['z_0'] - test['z_1']) ** 2","bb118461":"train.isnull().sum() # no missing value","c2924951":"# features from groupby\ntrain['molecule_name_unique'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['molecule_name'].nunique())\ntest['molecule_name_unique'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['molecule_name'].nunique())\ntrain['molecule_name_type'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['type'].nunique())\ntest['molecule_name_type'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['type'].nunique())\ntrain['molecule_dist_mean'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['dist'].mean())\ntest['molecule_dist_mean'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['dist'].mean())\ntrain['molecule_dist_sum'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['dist'].sum())\ntest['molecule_dist_sum'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['dist'].sum())\ntrain['molecule_dist_min'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['dist'].min())\ntest['molecule_dist_min'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['dist'].min())\ntrain['molecule_atom_count'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['atom_1'].count())\ntest['molecule_atom_count'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['atom_1'].count())\ntrain['molecule_atom_u'] = train['molecule_name'].map(train.groupby(train['molecule_name'])['atom_1'].nunique())\ntest['molecule_atom_u'] = test['molecule_name'].map(test.groupby(test['molecule_name'])['atom_1'].nunique())\n# by type\ntrain['type_unique'] = train['type'].map(train.groupby(train['type'])['type'].nunique())\ntest['type_unique'] = test['type'].map(test.groupby(test['type'])['type'].nunique())\ntrain['type_dist_mean'] = train['type'].map(train.groupby(train['type'])['dist'].mean())\ntest['type_dist_mean'] = test['type'].map(test.groupby(test['type'])['dist'].mean())\ntrain['type_dist_sum'] = train['type'].map(train.groupby(train['type'])['dist'].sum())\ntest['type_dist_sum'] = test['type'].map(test.groupby(test['type'])['dist'].sum())\ntrain['type_dist_min'] = train['type'].map(train.groupby(train['type'])['dist'].min())\ntest['type_dist_min'] = test['type'].map(test.groupby(test['type'])['dist'].min())\ntrain['type_atom_count'] = train['type'].map(train.groupby(train['type'])['atom_1'].count())\ntest['type_atom_count'] = test['type'].map(test.groupby(test['type'])['atom_1'].count())\ntrain['type_atom_u'] = train['type'].map(train.groupby(train['type'])['atom_1'].nunique())\ntest['type_atom_u'] = test['type'].map(test.groupby(test['type'])['atom_1'].nunique())","72c1155e":"train.head()","b5492809":"#train = train.drop(['id', 'type_atom_u'], axis=1)\n#test = test.drop(['id', 'type_atom_u'], axis=1)","c3eb032f":"object_data = train.dtypes[train.dtypes == 'object'].index","05c11afc":"train = train.drop(['atom_0', 'atom_1'], axis=1)\ntest = test.drop(['atom_0', 'atom_1'], axis=1)","4f296d9f":"object_data","9b1db647":"train['molecule_name'] = train['molecule_name'].astype('category').cat.codes\ntest['molecule_name'] = test['molecule_name'].astype('category').cat.codes","a125480e":"# dummies the remaing \ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)","c5933e4e":"train.head()","478fd2ae":"X = train.drop('scalar_coupling_constant', axis=1)\ny = train['scalar_coupling_constant']\nX_test = test","c4b24f56":"n_fold = 3\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=100)","536e3c51":"from xgboost import XGBRegressor ","a0ae242a":"params = {#'num_leaves': 12,\n          'min_child_samples': 3,\n          'objective': 'regression',\n          'max_depth': 7,\n          'learning_rate': 0.1,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 1,\n          \"subsample\": 0.9,\n          #\"bagging_seed\": 11,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'reg_alpha': 0.9,\n          'reg_lambda': 0.9,\n          'colsample_bytree': 0.8\n         }\nresult_dict_lgb = XGBRegressor(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='xgb', eval_metric='group_mae', plot_feature_importance=True,\n                                                      verbose=500, early_stopping_rounds=100, n_estimators=1000)\n\n","ab41d000":"model = XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=500, random_state=42)","aa86adc0":"from sklearn.model_selection import train_test_split","462f7b1d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)","8228e7dd":"model.fit(X_train, y_train)","e08d03ad":"pred = model.predict(test)","b5bf92c7":"sub['scalar_coupling_constant'] = pred\nsub.to_csv('chemistry.csv', index=False)\nsub.head()","8564840e":"**Create distance features**","ec481e30":"MOdelling","7511443c":"Droping redundant variable","68d0e0ec":"Import Libararies","74c348d7":"Basic feature engineering","e5b9c937":"Check the descriptive statistics of the variable"}}