{"cell_type":{"97251484":"code","314fcffc":"code","39661000":"code","c8c9de44":"code","e5e22548":"code","76c8e2c4":"code","dc055a25":"code","8d739282":"code","3c5c65d5":"code","895cb29b":"code","7482a4f0":"code","8632fdcf":"code","890ceede":"code","f2f98d9b":"code","8d1025ed":"code","8402d7f0":"code","18e662b1":"code","a9b8e80a":"code","794d5a08":"code","f6935244":"code","708b1a4c":"code","60b5e787":"code","fac138ca":"code","d3b1deef":"code","c8e90fa5":"code","f8698006":"code","3e4a1a75":"code","964261bb":"code","34ec0f41":"code","9d2e8abd":"code","7e66465b":"code","ef9a9f1e":"code","0ae03ac4":"code","e77070dc":"code","4decc0c8":"markdown","722da29e":"markdown","e29c12dc":"markdown","57568e91":"markdown","2dc2bf43":"markdown","beab73b1":"markdown","5c082351":"markdown","8b30e62d":"markdown"},"source":{"97251484":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","314fcffc":"path = '..\/input\/glass\/glass.csv'\ndf = pd.read_csv(path)\ndf.head()","39661000":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n%matplotlib inline","c8c9de44":"df.info() ## No null value","e5e22548":"df['Type'].value_counts()","76c8e2c4":"## Gives correlation of different parameters on target class\ndf.corr() ","dc055a25":"## checking for null values\ndf.isnull().sum()  ","8d739282":"sns.pairplot(df, hue = 'Type')","3c5c65d5":"plt.figure(figsize = (12,10))\nsns.heatmap(df.corr(), annot = True, cmap = 'Greens')","895cb29b":"cor = df.corr()['Type'].sort_values(ascending = False)\ncor","7482a4f0":"plt.figure(figsize = (12,6))\ncor.plot(kind = 'bar')","8632fdcf":"df.columns","890ceede":"X = df[['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']]\ny = df['Type']\nX","f2f98d9b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","8d1025ed":"from sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\nlg.fit(X_train, y_train)","8402d7f0":"pred1 = lg.predict(X_test)\nfrom sklearn.metrics import confusion_matrix, classification_report","18e662b1":"print(confusion_matrix(y_test, pred1))","a9b8e80a":"print(classification_report(y_test, pred1))","794d5a08":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)","f6935244":"y = df['Type']\nX = pd.DataFrame(X, columns = df.columns[:-1])\nX","708b1a4c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","60b5e787":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\npred2 = knn.predict(X_test)","fac138ca":"print(confusion_matrix(y_test, pred2))","d3b1deef":"print(classification_report(y_test, pred2))","c8e90fa5":"# for loop to predict values for different neighbours value\nerror = []\nfor i in range(1,30):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","f8698006":"plt.figure(figsize = (10,6))\nplt.plot(range(1,30), error, color = 'green', marker = 'o', markerfacecolor = 'blue')\nplt.title('Error rate vs K value')\nplt.xlabel('K-values')\nplt.ylabel('Error')","3e4a1a75":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\npred3 = dtree.predict(X_test)","964261bb":"print(confusion_matrix(y_test, pred3))","34ec0f41":"print(classification_report(y_test, pred3))","9d2e8abd":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train, y_train)\npred4 = rfc.predict(X_test)","7e66465b":"print(confusion_matrix(y_test, pred4))","ef9a9f1e":"print(classification_report(y_test, pred4))","0ae03ac4":"Accuracy = [66,57,62,80]\nModels = ['Logical Regression','K-means clustor','Decesion Tree','Random Forest']","e77070dc":"plt.figure(figsize = (8,4))\nplt.bar(Models, Accuracy, color = 'maroon')\nplt.title('Models vs Accuracy')","4decc0c8":"**Decesion Tree**","722da29e":"**KNN Model**","e29c12dc":"**Logical regression Model**","57568e91":"We can observe some improvements over the previous results","2dc2bf43":"Train test split","beab73b1":"Since this is multi class classification let us try other models","5c082351":"**Random Forest**","8b30e62d":"Here the error is increasing hence k = 1 is best choice"}}