{"cell_type":{"a156864b":"code","d6f4a04e":"code","256ed4a1":"code","e9988acb":"code","18e1c505":"code","cf5d065a":"code","de1e74a1":"code","7e2895ed":"code","30b3b27d":"code","28a1c174":"code","0694b099":"code","e444d30a":"code","6aa0d833":"code","2988857a":"code","84df99c7":"code","f594ce5f":"code","55db183e":"code","02e2d593":"code","3210dac7":"code","f7f101b9":"code","ade8b97f":"code","88828cf9":"code","12b4173e":"code","12d79ba3":"code","c5e49a00":"code","5220e50e":"code","d98bf4c6":"code","d7f74cb8":"code","e6fd4b85":"code","97b62afa":"code","5ca8fe36":"code","c904a574":"code","b05322b2":"code","a4009c75":"code","ce373b4e":"code","a20589c4":"code","f36b931e":"code","d7db689e":"code","06701ea2":"code","1f1ef059":"code","fb0991f9":"code","0f80a467":"code","df19bac6":"code","c5195ce6":"code","efc27965":"code","60a8ece9":"code","c5a5a4a8":"code","b9d39de7":"code","9daabf01":"code","22207db8":"code","0d14e38b":"code","509add7f":"code","a9901eea":"code","cabfcac3":"code","46206060":"code","69cd2670":"markdown","5f204eb6":"markdown","1139ee65":"markdown","f89b3b1f":"markdown","97f49395":"markdown","ad25e82f":"markdown","2061169e":"markdown","8162c46f":"markdown","6d9f98da":"markdown","252de6cc":"markdown","3f9333d9":"markdown","8ba5df8d":"markdown","01e007da":"markdown","7e52d220":"markdown","8b2e7d75":"markdown","3e3ef1a2":"markdown","34cb808d":"markdown","04d8e255":"markdown","3dd4cda0":"markdown","957e11fb":"markdown","c02baa41":"markdown","fa355126":"markdown","6c30ac4d":"markdown","6d3c4b57":"markdown","0a772f54":"markdown","daa8a372":"markdown","801feaa4":"markdown"},"source":{"a156864b":"#import necessary libraries\n\nimport pandas as pd\npd.set_option('display.max_rows', 500)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler","d6f4a04e":"pip install eli5","256ed4a1":"#ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions.\nfrom eli5.sklearn import PermutationImportance","e9988acb":"#import datasets\n\ntrain = pd.read_csv('..\/input\/Train.csv')\ntest = pd.read_csv('..\/input\/Test.csv')\n\n","18e1c505":"#Lets look at the train and test datasets\nprint(train.head(),test.head())","cf5d065a":"print(train.info(),test.info())","de1e74a1":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of Selling_Price\")\nax = sns.distplot(train[\"Selling_Price\"])","7e2895ed":"train['Selling_Price'] = np.log(train['Selling_Price'])","30b3b27d":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of Selling_Price after transformation\")\nax = sns.distplot(train[\"Selling_Price\"])","28a1c174":"sns.scatterplot('Item_Rating','Selling_Price',data=train)","0694b099":"combine = train.append(test)","e444d30a":"combine.head()","6aa0d833":"print(combine.info())","2988857a":"combine['Product']=combine['Product'].str.split('-', n=1, expand=True)[1]\ncombine['Product'] = combine['Product'].astype(int)\ncombine['Product'] = np.log(combine['Product'])\n","84df99c7":"combine['Product_Brand']=combine['Product_Brand'].str.split('-', n=1, expand=True)[1]\ncombine['Product_Brand'] = combine['Product_Brand'].astype(int)\ncombine['Product_Brand'] = np.log(combine['Product_Brand'])","f594ce5f":"combine.dtypes","55db183e":"cols = ['Item_Category','Subcategory_1','Subcategory_2']\nfor i in cols:\n  print(\"============================\")\n  print(\"Categories in\",i,\": \")\n  print(combine[i].value_counts())\n","02e2d593":"  for i in cols:\n    print(\"Total distinct categories in\",i, \":\",len(combine[i].unique()))","3210dac7":"combine['Subcategory_1'] = np.where((combine['Subcategory_1']=='unknown'),combine['Item_Category'],combine['Subcategory_1'])\ncombine['Subcategory_2'] = np.where((combine['Subcategory_2']=='unknown'),combine['Subcategory_1'],combine['Subcategory_2'])","f7f101b9":"enc_nom = (combine.groupby('Item_Category').size()) \/ len(combine)\nenc_nom\ncombine['Item_Category_encode'] = combine['Item_Category'].apply(lambda x : enc_nom[x])","ade8b97f":"enc_nom_1 = (combine.groupby('Subcategory_1').size()) \/ len(combine)\nenc_nom_1\ncombine['Subcategory_1_encode'] = combine['Subcategory_1'].apply(lambda x : enc_nom_1[x])","88828cf9":"enc_nom_2 = (combine.groupby('Subcategory_2').size()) \/ len(combine)\nenc_nom_2\ncombine['Subcategory_2_encode'] = combine['Subcategory_2'].apply(lambda x : enc_nom_2[x])","12b4173e":"from datetime import datetime\ncombine['Date'] = pd.to_datetime(combine['Date'])\ncombine['Month'] = [date.month for date in combine.Date]","12d79ba3":"combine.drop(['Item_Category','Subcategory_1','Subcategory_2','Date','Item_Rating'],axis=1,inplace = True)","c5e49a00":"combine.head()","5220e50e":"#Separating train and test datasets\ntrain=combine[combine['Selling_Price'].isnull()!=True]\ntest=combine[combine['Selling_Price'].isnull()==True]\n\ntest=test.drop(['Selling_Price'], axis=1)","d98bf4c6":"print(train.shape,test.shape)","d7f74cb8":"#train-test split\nY = train['Selling_Price']\nX = train.drop('Selling_Price',axis=1)\n\n\nX_train,X_val,Y_train,Y_val=train_test_split(X,Y,test_size=0.1,random_state=0)","e6fd4b85":"print(X_train.shape,Y_train.shape)","97b62afa":"pip install catboost\n","5ca8fe36":"from catboost import CatBoostRegressor\ncb = CatBoostRegressor(\n    n_estimators = 500,\n    learning_rate = 0.1,\n    loss_function = 'MAE',\n    eval_metric = 'RMSE')\n\ncb.fit(X_train,Y_train)","c904a574":"import eli5\nperm = PermutationImportance(cb,random_state=100).fit(X_val, Y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())","b05322b2":"from sklearn.metrics import mean_squared_error\nkf=KFold(n_splits=10, random_state=100, shuffle=True)\n\ny_test_predict=0\nmse = 0\nj=1\nresult={}\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    \n   Y_train, Y_valid = Y.iloc[train_index], Y.iloc[test_index]\n   X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n   \n   print( \"\\nFold \", j)\n   cb = CatBoostRegressor(\n    n_estimators = 1000,\n    learning_rate = 0.05,\n    max_depth = 6,\n    boosting_type = 'Ordered',\n    loss_function = 'RMSE',\n    eval_metric = 'RMSE',verbose = 0)\n   \n  #  xg=XGBRegressor(booster='gbtree', max_depth=5, learning_rate=0.05, reg_alpha=0,\n  #                 reg_lambda=1, n_jobs=-1, random_state=100, n_estimators=5000)\n    \n   model=cb.fit(X_train,Y_train)\n   pred = model.predict(X_valid)\n   \n   print(np.sqrt(mean_squared_error(Y_valid, np.abs(pred))))\n   mse+=np.sqrt(mean_squared_error(Y_valid,np.abs(pred)))\n    \n   #y_test_predict+=model.predict(test)  \n   #result[j]=model.predict(X_main_test)\n   j+=1\n\nresults=y_test_predict\/10\n\nprint(mse\/10)","a4009c75":"# model3=cb.fit(x,y)\n# pred_cb = model3.predict(test)\n# pred_cb = np.abs(pred_cb)\n# pred_cb","ce373b4e":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100,criterion='mse',\n                           \n                           min_samples_leaf=1, \n                           min_samples_split = 5, \n                           random_state=100)\nrf.fit(X_train,Y_train)\n","a20589c4":"import eli5\nperm = PermutationImportance(rf,random_state=100).fit(X_val, Y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())","f36b931e":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 300, stop = 500, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto','sqrt']\n#max_features = ['sqrt']\n# Maximum number of levels in tree\n#max_depth = [int(x) for x in np.linspace(6, 30, num = 5)]\n#max_depth.append(None)\n# Minimum number of samples required to split a node\n#min_samples_split = [2, 4,6]\nmin_samples_split = [4,5,6,7]\n# Minimum number of samples required at each leaf node\n#min_samples_leaf = [1, 2, 4]\nmin_samples_leaf = [1]\n# Method of selecting samples for training each tree\n#bootstrap = [True, False]\nbootstrap = [True]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               #'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","d7db689e":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, Y_train)","06701ea2":"rf_random.best_params_","1f1ef059":"# from sklearn.model_selection import GridSearchCV\n# # Create the parameter grid based on the results of random search \n# param_grid = {\n#     'bootstrap': [True],\n#     'max_features': [1,2,3],\n#     'min_samples_leaf': [1],\n#     'min_samples_split': [4,5],\n#     'n_estimators': [300,350,400]\n# }\n# # Create a based model\n# rf = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 2)","fb0991f9":"# grid_search.fit(X_train, Y_train)","0f80a467":"# grid_search.best_params_","df19bac6":"#Calsulating cv score using the best parameters\nkf=KFold(n_splits=10, random_state=100, shuffle=True)\n\ny_test_predict=0\nmse = 0\nj=1\nresult={}\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    \n   Y_train, Y_valid = Y.iloc[train_index], Y.iloc[test_index]\n   X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n   \n   print( \"\\nFold \", j)\n   \n   rf = RandomForestRegressor(n_estimators=455,\n                           max_features='sqrt',\n                           bootstrap='True',\n                           min_samples_leaf=1,\n                           min_samples_split=4,\n                           random_state=100)\n  #  xg=XGBRegressor(booster='gbtree', max_depth=5, learning_rate=0.05, reg_alpha=0,\n  #                 reg_lambda=1, n_jobs=-1, random_state=100, n_estimators=5000)\n    \n   model=rf.fit(X_train,Y_train)\n   pred = model.predict(X_valid)\n   \n   print(np.sqrt(mean_squared_error(Y_valid, np.abs(pred))))\n   mse+=np.sqrt(mean_squared_error(Y_valid,np.abs(pred)))\n    \n   #y_test_predict+=model.predict(test)  \n   #result[j]=model.predict(X_main_test)\n   j+=1\n\nresults=y_test_predict\/10\n\nprint(mse\/10)","c5195ce6":"x = X_train.append(X_valid)\ny = Y_train.append(Y_valid)\nx.head()\n","efc27965":"#Training the model on whole dataset\nmodel=rf.fit(x,y)\npred = model.predict(test)\npred = np.abs(pred)\npred","60a8ece9":"from xgboost import XGBRegressor\n#xgb = xgb.XGBRegressor(objective ='reg:squarederror',  learning_rate = 0.1,\n#                max_depth = 8, alpha = 10, n_estimators = 400)\n\nxgb=XGBRegressor(booster='gbtree', max_depth=6, learning_rate=0.1, reg_alpha=0,\n                  reg_lambda=1, n_jobs=-1, random_state=100, n_estimators=500)\nxgb.fit(X_train,Y_train)","c5a5a4a8":"import eli5\nperm = PermutationImportance(xgb,random_state=100).fit(X_val, Y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())","b9d39de7":"import warnings\nwarnings.filterwarnings('ignore')\nkf=KFold(n_splits=10, random_state=100, shuffle=True)\n\ny_test_predict=0\nmse = 0\nj=1\nresult={}\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    \n   Y_train, Y_valid = Y.iloc[train_index], Y.iloc[test_index]\n   X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n   \n   print( \"\\nFold \", j)\n   \n   xg=XGBRegressor(booster='gbtree', max_depth=6, learning_rate=0.06,\n                  n_jobs=-1, random_state=100, n_estimators=800)\n    \n   model=xg.fit(X_train,Y_train)\n   pred = model.predict(X_valid)\n   \n   print(np.sqrt(mean_squared_error(Y_valid, np.abs(pred))))\n   mse+=np.sqrt(mean_squared_error(Y_valid,np.abs(pred)))\n    \n   #y_test_predict+=model.predict(test)  \n   #result[j]=model.predict(X_main_test)\n   j+=1\n\nresults=y_test_predict\/10\n\nprint(mse\/10)","9daabf01":"model2=xg.fit(x,y)\npred_xg = model2.predict(test)\npred_xg = np.abs(pred_xg)\npred_xg","22207db8":"from lightgbm import LGBMRegressor\nlgb = LGBMRegressor(boosting_type='gbdt', objective='regression',metric = 'rmsle',\n                      max_depth=6, learning_rate=0.1, \n                      n_estimators=500, nthread=-1, silent=True)\nlgb.fit(X_train,Y_train)","0d14e38b":"from sklearn.metrics import mean_squared_error\npredictions = lgb.predict(X_val)\nnp.sqrt(mean_squared_error(Y_val, predictions))","509add7f":"import eli5\nperm = PermutationImportance(lgb,random_state=100).fit(X_val, Y_val)\neli5.show_weights(perm,feature_names=X_val.columns.tolist())","a9901eea":"import warnings\nwarnings.filterwarnings('ignore')\nkf=KFold(n_splits=10, random_state=100, shuffle=True)\n\ny_test_predict=0\nmse = 0\nj=1\nresult={}\n\nfor i, (train_index, test_index) in enumerate(kf.split(train)):\n    \n   Y_train, Y_valid = Y.iloc[train_index], Y.iloc[test_index]\n   X_train, X_valid = X.iloc[train_index,:], X.iloc[test_index,:]\n   \n   print( \"\\nFold \", j)\n   \n   lg = LGBMRegressor(boosting_type='gbdt', objective='regression',metric = 'rmsle',\n                      max_depth=8, learning_rate=0.025, \n                      n_estimators=750, nthread=-1, silent=True)\n    \n   model=lg.fit(X_train,Y_train)\n   pred = model.predict(X_valid)\n   \n   print(np.sqrt(mean_squared_error(Y_valid, np.abs(pred))))\n   mse+=np.sqrt(mean_squared_error(Y_valid,np.abs(pred)))\n    \n   #y_test_predict+=model.predict(test)  \n   #result[j]=model.predict(X_main_test)\n   j+=1\n\nresults=y_test_predict\/10\n\nprint(mse\/10)","cabfcac3":"model4=lg.fit(x,y)\npred_lg = model4.predict(test)\npred_lg = np.abs(pred_lg)\npred_lg","46206060":"Dataset_Submission=pd.read_excel('\/content\/Sample_Submission.xlsx')\nDataset_Submission['Selling_Price']=np.exp(pred)\nDataset_Submission.head(10)\n\nDataset_Submission.to_excel('submission15.xlsx', index=False)","69cd2670":"**Private Score(100% dataset): 0.63579**\n\n**Model- RandomForestRegressor**\n","5f204eb6":"# Model Building","1139ee65":"# Data Analysis","f89b3b1f":"*Let us now combine our dataset and perform some analysis\/feature engineering*","97f49395":"\n\n> No linear relationship can be seen whatsoever bw the two variables,hence 'Iten Rating' is not much of a use for us.\n\n","ad25e82f":"**Now let us look at probably the 3 most important and interesting features (Item_category,subcategory1 and subcategory2) in detail**","2061169e":"**Hackathon 5:E-commerce Price Prediction**\n\n**Platform**: Analytics India Magazine\n\n**Link**:[E-commerce Price Prediction](https:\/\/www.machinehack.com\/course\/e-commerce-price-prediction-weekend-hackathon-8\/)\n\n**Pulic Leaderboard Rank**:7\n\n**Private Leaderboard Rank**:9","8162c46f":"-As observed, there are a lot of distinct categories in all the three features above hence OHE will not make sense here.\n\n-We will be using frequency encoding for converting these features into numeric type\n\n**One major issue to Adress**\n\n\n\n> How to handle 'Unknown' category in subcategory1 and subcategory2?\n\n**SOL**: Here is a simple and elegant solution provided by a friend...\n\nAnd I quote......\n\n*'Unkown is subcategory1\/subcategory2 means that there is no hierarchial relationship bw Item_Category-Subcategory1-Subcategory2 which affects the selling price and hence if we find an unknown we can simply replace it by the category one level above'*\n\n\n","6d9f98da":"**Performing the above operations**","252de6cc":"## CatBoost","3f9333d9":"**Extracting the additional Month feature from Date as it may be useful**","8ba5df8d":"## LightGBM","01e007da":"**General Observations**\n\n-The features 'Product' and 'Product_Brand' though object type , can easily be converted into numeric by removing the char and separator to avoid encoding the data\n\n-Item Category and Sucategory 1 & 2 features seem to have a lot of unique categorical values.","7e52d220":"# Summary","8b2e7d75":"**Item Rating vs Selling Price**","3e3ef1a2":"**Target Variable Distribution**","34cb808d":"# Submission File","04d8e255":"## RandomForestRegressor","3dd4cda0":"**Let us have a look at the final dataset after dropping unnecessary columns**","957e11fb":"**Some Observations**:\n\n\n*   We have 6 features(1 float nd 5 object type)\n*   There are no null values in the dataset\n*   Date feature maybe used to create some additional features of interest\n\n\n\n\n","c02baa41":"# Remarks\n\n## <a>Do Upvote if you liked the approach and comment if you have any suggestions<\/a>\n\nTHANKS!!!","fa355126":"\n**Public Score(30% dataset)**\n\n**Metric-rmsle**\n*   Catboost : 0.698\n*   Random Forest: 0.646\n*   Light GBM : 0.687\n*   XGBoost: 0.66\n\n\n\n","6c30ac4d":"## Description : \n\nE-commerce platforms have been in existence for more than 2 decades now. The popularity and its preference as a common choice for buying and selling essential products have grown rapidly and exponentially over the past few years. E-commerce has impacted the lifestyle of common people to a huge extent. Many such platforms are competing over each other for dominance by providing consumer goods at a competitive price. In this hackathon, we challenge data science enthusiasts to predict the price of commodities on an e-commerce platform.\n\n## AIM:\n\n**Given are 7 distinguishing factors that can influence the price of a product on an e-commerce platform. Your objective as a data scientist is to build a machine learning model that can accurately predict the price of a product based on the given factors.**","6d3c4b57":"\n\n> Converting 'Product' and 'Product_Brand' to numeric type.\n\n","0a772f54":"## XGBOOST","daa8a372":"\n\n> The Distribution of our target variable is highly left skewed, hence we will be performing log-transformation to make it more normal.\n\n\n","801feaa4":"**I used 4 of the following tree based\/boosting algorithms for training purpose:**\n\n\n\n1.   CatBoost\n2.   RandomForestRegressor\n3.   XGBoost\n4.   LightGBM\n\n\n\n> Random Forest performed the best as a standalone model based on 10 fold CV score and public dataset results and hence hyper-parameter tuning was done only for it.\n\nScore summary is provided at the end for each model\n\n\n\n\n\n"}}