{"cell_type":{"8e7c5466":"code","697bb1d3":"code","f47b70a2":"code","777263b5":"code","38eb4f9d":"code","baef7a59":"code","2acff040":"code","6261f48f":"code","fe9b33ba":"code","32d16c53":"code","83db0a88":"code","c23412ff":"code","ebc5b5cf":"code","f9623a8c":"code","359b0592":"code","81731328":"code","cccea2df":"code","c9469536":"code","9cd5229e":"code","c2f6a7eb":"code","103c0d48":"code","55aa2751":"code","252053ca":"code","03e28aec":"code","1c4c8cb5":"code","198c8f8a":"code","2aa63d89":"code","abc7e6a7":"code","4b6e1bb1":"code","8a20ad31":"code","15ad8e44":"code","5ecfe289":"code","5a29d949":"code","9b99cba8":"code","a5d86c45":"code","3fd3279a":"code","6616408d":"code","6951a933":"code","5d0b22e5":"code","e22fcd32":"code","df0faa13":"code","85248a71":"code","24c37652":"code","c88c8c70":"code","b09309f2":"markdown","5f7d8fc9":"markdown","91258584":"markdown","15b7b482":"markdown","7167267a":"markdown","3a33b18e":"markdown","8a696013":"markdown","cdec12b1":"markdown","7e093268":"markdown","c31a5fa5":"markdown","97aa2c73":"markdown","242b9295":"markdown","184fd32e":"markdown","5abcaccc":"markdown","d70e8a97":"markdown","9e2fad9d":"markdown","e11790d9":"markdown","672a3a0a":"markdown"},"source":{"8e7c5466":"# Importing:\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","697bb1d3":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\nx_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nto_submission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","f47b70a2":"# Train data needs to be split into x and y\ny_train = train[\"label\"]\nx_train = train.drop(\"label\", axis = 1)","777263b5":"y_train.head()","38eb4f9d":"x_train.head() # we can see that the data is allready spread. Let's reshape this to be of 28 x 28","baef7a59":"x_test.head()","2acff040":"# We first change it to numpy\nx_train = x_train.to_numpy()\nx_test = x_test.to_numpy()","6261f48f":"# We will flatten this again once we use MLP, but for now just for the visualization\nx_train = x_train.reshape((-1,28,28))\nx_test = x_test.reshape((-1,28,28))","fe9b33ba":"x_train.max()","32d16c53":"x_train.min()","83db0a88":"# Let's look at how the data looks as an image:\nfig, ax = plt.subplots(4, 4, figsize=(16,10))\nfor i in range(16):\n    ax[i\/\/4, i%4].imshow(x_train[i])","c23412ff":"# Let's look at the distribution of the labels:\ntrain.label.value_counts().plot(kind = \"bar\")","ebc5b5cf":"# The labels are in numbers. Let's one hot encode it:\ny_train_one_hot = np.zeros([y_train.shape[0], 10])\ny_train_one_hot[np.arange(y_train.shape[0]), y_train] = 1","f9623a8c":"y_train_one_hot","359b0592":"# Preprocessing the image:\nx_train = x_train \/ 255.0\nx_test = x_test \/ 255.0","81731328":"# Convert them to tensors so that we can use them\ndef to_tensor(input):\n    return tf.cast(tf.convert_to_tensor(input), dtype=tf.float32)","cccea2df":"x_train = to_tensor(x_train)\ny_train_one_hot = to_tensor(y_train_one_hot)","c9469536":"x_train.dtype","9cd5229e":"# Let's flatten back our x_train data:\n\nx_flattened = tf.reshape(x_train, shape=[x_train.shape[0], -1])","c2f6a7eb":"x_flattened","103c0d48":"# Variables we need to initialize:\nn_classes = 10  # This is the number of classes we need to predict. We need 10 nodes for the output layer\nn_nodes_l1 = 265  # This is the number of nodes that we have in layer 1\nn_nodes_l2 = 128  # Layer 2 nodes\nn_nodes_l3 = 64  # Layer 3 nodes\nn_nodes_l4 = 32  # Layer 4 nodes\nn_nodes_l5 = 64  # Layer 5 nodes","55aa2751":"# Let's make our Dense Layer: We will be using tf.Module to inherit from.\n# tf.Module allows us to track the variables(later to be used in optimiziation)\nclass Dense(tf.Module):\n    def __init__(self, output_nodes, activation=\"relu\"):  # By default, we will use relu as activation function\n        super(Dense, self).__init__()\n        # We could define the variables here, but for our model's flexibility with the input, we will do the following:\n        self.build = False  # The weights and biases are not created during initialization\n        # This is so that we could have some \"flexibility\" of the shapes of the input data that we might expect to get\n        # If I initialized my weights and biases during this step, then I will have to have prior knowledge about the shape of the input\n        self.output_nodes = output_nodes  # Initialize to have output nodes equal to the number of nodes pre-defined\n        self.activation = activation  # Initialize the activation name\n\n    def __call__(self, input_data):  # Once it is called\n        if not self.build:  # If it has not been built yet. So this will run only upon calling the model the first time\n            # We declare our trainable variables here:\n            # The weights and biases:\n            self.w = tf.Variable(tf.random.normal([input_data.shape[-1], self.output_nodes], stddev=0.01), dtype=tf.float32, name=\"w\")\n            self.b = tf.Variable(tf.random.normal([self.output_nodes], stddev=0.01), dtype=tf.float32, name=\"b\")\n            self.build = True  # So that it is not run other than the first time\n        \n        # Computation:\n        node = tf.add(tf.matmul(input_data, self.w), self.b)  # The node will multiply the input with the weights, then add bias\n        if self.activation == \"relu\":  # If activation is \"relu\"\n            return tf.nn.relu(node)\n        elif self.activation == \"softmax\":  # If activation is \"softmax\"\n            return tf.nn.softmax(node)\n        # I know I could have written functions for these two activations... but it is worth it???","252053ca":"# Take the following example:\na = np.random.randint(0, 255, size=(512))\na[:10]","03e28aec":"# Create weights by picking numbers from a normal distribution of mean 0 and stddev = 1.0\n# This is the same as the default values of tf.random.normal()\nb = np.random.normal(loc = 0.0, scale = 1.0, size = (512,512))\nb[:10]","1c4c8cb5":"c = a@b\nc.max()","198c8f8a":"# Making the model:\nclass SimpleNeural(tf.Module):  # Also inherits from tf.Module\n    def __init__(self):\n        super(SimpleNeural, self).__init__()\n        self.layer_1 = Dense(n_nodes_l1)  # Initializing the layers with the number of nodes\n        self.layer_2 = Dense(n_nodes_l2)\n        self.layer_3 = Dense(n_nodes_l3)\n        self.layer_4 = Dense(n_nodes_l4)\n        self.layer_5 = Dense(n_nodes_l5)\n        self.layer_out = Dense(n_classes, activation=\"softmax\")  # The ouput layer with softmax activation\n\n    def __call__(self, input_data):  # once we call\n        # The weights and biases are now initialized with the shape of the input\n        l1_out = self.layer_1(input_data)\n        l2_out = self.layer_2(l1_out)\n        l3_out = self.layer_3(l2_out)\n        l4_out = self.layer_4(l3_out)\n        l5_out = self.layer_5(l4_out)\n        lo_out = self.layer_out(l5_out)\n        return lo_out","2aa63d89":"# The loss function that checks how far we are from the actual target\ndef loss_fn(actual, prediction):\n    return tf.reduce_mean(-tf.reduce_sum(actual * tf.math.log(tf.math.add(prediction, 0.00001)), axis=1))\n    # We add a very small amount to prediction so that if we get a prediction of zero, log won't be changed to inf\n    # and give us useless loss and gradients\n    # Note axis=1(in tf.reduce_sum) means to reduce along the second dim,\n    # this means that if we have a Tensor of 2x3, do summation across the 3,\n    # so moving across the columns.\n    # https:\/\/stackoverflow.com\/questions\/34240703\/what-are-logits-what-is-the-difference-between-softmax-and-softmax-cross-entrop","abc7e6a7":"np.log(1)","4b6e1bb1":"np.log(0)","8a20ad31":"np.log(1.00001)","15ad8e44":"np.log(0.00001)","5ecfe289":"# The training function:\ndef train(train_x, train_y):\n    with tf.GradientTape() as t:  # Use tensorflow's GradientTape to track variables\n        loss = loss_fn(train_y, model(train_x))  # We will use the loss function we made\n    gradients = t.gradient(loss, model.trainable_variables)  # Get the gradients\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables))  # Optimize using these gradients","5a29d949":"##########################################################################\n# Batching: (mini-batch where batch size is in between 1 and len(x_train))\n# Mini-batch or stochastic-batch takes more time than batch(the whole training dataset) since it fixes the weights,\n# in more steps than simply passing the dataset as one.\n##########################################################################\n\n# We could pass the dataset as one chunk of points to train. Inceasing number of epochs would increase accuracy\n# However, if we pass batches (smaller chunks) of data to trainm the model will perform better and take less epochs to train\n\nbatch_size = 50  # Batch should be divisors of len(x_train)\n# Tensorflow is very sensitive to shapes.\n# Below, we are reshaping our train data so that it has 28 * 28 pixels, grouped in number of batches, and how many groups we have.\n# Reordering the above statement => shape = (how many groups, of groups of batch size, of rows, of columns) \nx_batch = tf.reshape(x_train, shape=[-1, batch_size, 28, 28]) # If batch size is not divisor of len(x_train), we get shape error.\n# Flattening the batch:\nx_batch_flattened = tf.reshape(x_batch, shape=[x_batch.shape[0], x_batch.shape[1], -1])\ny_batch = tf.reshape(y_train_one_hot, shape=[-1, batch_size, 10])\n# Think of batch as the number of training samples used for updating and modifying the weights\n# https:\/\/machinelearningmastery.com\/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size\/\n\n##########################################################################\nprint(x_batch_flattened.shape)\nprint(y_batch.shape)","9b99cba8":"# Initialize the variables:\nepochs = 50\nlearn_rate = 0.001\nmodel = SimpleNeural()","a5d86c45":"# An epoch is one feed forward and back propagation training where weights are updated.\nmeanLoss = []  # For storing the mean of the losses calculated from each epoch\npatience = 0  # For counting a number of times we check if the model is not improving, so that we could modify learning rate","3fd3279a":"for epoch in range(epochs):  # For each epoch\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learn_rate)  # This is the optimizer that we are going to use\n    lossList = []  # A temporary list to create the losses from each batch and get the mean as average loss\n    for x, y in zip(x_batch_flattened, y_batch):  # Go over each batch \/\/ We use zip to get a tuple of (x, y)\n        # So that we could iterate over them\n        train(x, y)  # x is of shape=[50, 784], y is of shape[50,10]\n        currentLoss = loss_fn(y, model(x))\n        lossList.append(currentLoss)  # loss from each batch\n    avgLoss = np.array(lossList).mean()  # The average loss calculated from each epoch\n    print(\"Epoch : {}, loss : {}\".format(epoch, avgLoss))  # Print the average loss\n    # We will reduce the learning rate if our mdoel fails to improve several times:\n    if len(meanLoss) > 0 and meanLoss[-1] < avgLoss:  # Check if the list is not empty and if the previous loss is less than current loss. \n    # loss is our metric. If our error or gradient is low, which means if we get closer to the minimum, our loss will eventually decrease to zero. \n    # If, for some reason, our loss increases, it means we moved away from the minimum point. \n    # It could be that our gradient skipped the minimum due to high learning rate. We can't see this by using gradients, so we will use loss which is a metric and end-result of optimizing (learning rate * gradient) \n      patience += 1  # Increase number of patience\n      if patience > 3:  # If we can't be patient anymore\n        learn_rate *= 0.5  # Reduce by half\n        print(\"learn_rate has changed to {}\".format(learn_rate))  # Check\n        patience = 0  # Reset our patience\n    meanLoss.append(avgLoss)  #  Appending the loss to the list","6616408d":"# Plot the loss\nplt.plot([x for x in range(len(meanLoss))], meanLoss, c=\"b\")\nmeanLoss = np.array(meanLoss)\nprint(meanLoss.argmin())","6951a933":"len(x_train)","5d0b22e5":"# Getting accuracy of training:\ncount = 0\nresult = model(x_flattened)\nfor i in range(len(x_flattened)):\n  if result[i].numpy().argmax() == y_train_one_hot[i].numpy().argmax():\n    count+=1\nprint(count\/len(x_flattened))","e22fcd32":"# Predicting:\n\nx_test = to_tensor(x_test)\n# Flatten the test data:\nx_test_flattened = tf.reshape(x_test, shape=[x_test.shape[0], -1])","df0faa13":"# The following will give us a one hot encoded arrays\nprediction_one_hot = model(x_test_flattened)\nprediction_one_hot.numpy()[:10]","85248a71":"for i in range(x_test_flattened.shape[0]):\n    to_submission.Label[i] = prediction_one_hot[i].numpy().argmax()","24c37652":"to_submission.head()","c88c8c70":"#Saving it:\nto_submission.to_csv('submission_final.csv', index=False)","b09309f2":"`self.w = tf.Variable(tf.random.normal([input_data.shape[-1], self.output_nodes], stddev=0.01), dtype=tf.float32, name=\"w\")`\n* Thus, we change the standard deviation to be smaller so that our numbers do not explode.\n* This will be covered later, but as the number get higher and higher, our gradients wil vanish to become very small. This will be shown in the next few steps","5f7d8fc9":"## `4. Making the model:`","91258584":"* We can see that upon grouping the training data by batches of 50, we get 840 groups of batches","15b7b482":"### Additionally, the reason why we might have exploding or vanishing gradients is that gradients are partial derivatives and derivatives of log (natural log in this case) is:\n * `dy\/dx = f'(x) \/ f(x)` where y = ln(f(x))\n * our loss function is `-Summation(y * ln(y_pred))`\n * If our y_pred = inf, then dy\/dx = `(1\/inf) * derivative of y_pred w.r.t trainabe variable`\n * => ` 0 * derivative of y_pred w.r.t trainabe variable` = 0\n * This will make our gradient disappear.\n * One the other hand, if our y_pred is approximately equal to zero, then our gradients will explode.","7167267a":"### _Thus, we should not have our weights and biases initialized to a high value and we should add a very small value to the predicted outcome in loss function so that we don't get inf or 0._","3a33b18e":" * We could see that the labels arre distributed apprroximately equally. \n * Thus, we can use accuacy as our metric for estimating our model","8a696013":"## `2. EDA and Getting to know data`","cdec12b1":"`tf.reduce_mean(-tf.reduce_sum(actual * tf.math.log(tf.math.add(prediction, 0.00001)), axis=1))`\n* You might wonder why we added 0.00001 on the predicted outcome from softmax inside the log() \n* This is because sometiimes we get our softmax output to be 0 or 1. ","7e093268":"## `6. Model Evaluation and Prediction`","c31a5fa5":"## `3. Preprocessing Data`","97aa2c73":" * We can know that maximum value is 255 and min value is 0\n * This means that we could simply divide by 255 to do min-max scaling since min is zero","242b9295":"# MLP from scratch with Tensorflow\n\n * This notebook dives deeper into what's under the hood of a Multilayer perceptron so that beginners could understand how each layer works in a neural network in a more overt way.\n * Note: For the optimizer, I simply used the provided one. This notebook focuses more on the layer customization\n * Tensorfow provides good documentations about how to make layers and models, but I could find a complete one so I made this one for me","184fd32e":"* We could see that the numbers go right up to thousands.\n* If we have a deeper layers, it would be a matter of time that the numbers reach infinity","5abcaccc":"* Important note: Tensorflow documentation uses the default stddev of tf.random.normal However, this could cause exploding numbers as our models get deeper and deeper","d70e8a97":"### The workflow:\n * Get input data > multiply by weights > send it to hidden layer1 (activation function) > add bias >\n * multiply by weight > send to hidden layer 2 (activation function) > add bias >  ... output layer\n * This is Feed-Forward method\n***\n***\n\n * Then compare output from output layer to the target > cost_function() > optimization function() >\n * minimize cost\n * This is backpropagation method\n * `feed forward + backpropagation = 1 epoch`","9e2fad9d":"## `5. Training`","e11790d9":"* To prevent from our loss function to give us inf, -inf, or zero\n* We add a very small value so that it does not give us vanishing or exploding gradients","672a3a0a":"## `1. Importing and Data Collecting`"}}