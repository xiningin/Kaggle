{"cell_type":{"712d8e94":"code","da208976":"code","8a441ae4":"code","2391dc45":"code","a89ef7c9":"code","f5978d2e":"code","c2a55117":"code","5864d789":"code","9ac66e8b":"code","2f314c89":"code","d55b700f":"code","aaefc143":"code","aa040b0c":"code","731a4f43":"code","a6a87d99":"code","ee15048d":"code","e869f972":"code","4eccdbe7":"code","c9281d5a":"code","58333ef5":"code","032d22d4":"code","b995743a":"code","f08b8057":"code","111fee04":"code","2ca71b07":"code","dca264c2":"code","daaad3e5":"code","eef4e58b":"code","60bf8740":"code","352b4be1":"code","b71f86ed":"code","7c2bd500":"code","226519d5":"code","bcbeaafb":"code","849f957d":"code","da2b696b":"code","b1ac2eec":"code","6f8cdf53":"markdown","0d5fdb6e":"markdown","155a929c":"markdown","3fdd27dd":"markdown","7502cab3":"markdown","a8f80d98":"markdown","0e7d2ea8":"markdown","4f8d06f9":"markdown","80e81afc":"markdown","9eb10cdd":"markdown"},"source":{"712d8e94":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\npd.set_option('display.max_columns', 500)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da208976":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv') #1460 rows\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv') #1459","8a441ae4":"print(train_data.shape)\nprint(test_data.shape)","2391dc45":"full_data = pd.concat([train_data,test_data], axis = 0, sort= False)\nfull_data.shape","a89ef7c9":"#function to show the missing values count and percentage\ndef missing_data(df):\n    #find the columns having null values\n    missing_values = df.isnull().sum().sort_values(ascending = False)\n    missing_values_df = pd.DataFrame(missing_values[missing_values>0])\n\n    #percentage of null values in descnding order\n    missing_percent = missing_values\/len(df) * 100\n    missing_percent_df = pd.DataFrame(missing_percent[missing_percent>0])\n\n    missing_df =  pd.concat([missing_values_df,missing_percent_df],axis = 1)\n    missing_df.columns = ['missing_counts','missing_percent']\n    \n    return missing_df","f5978d2e":"missing_data(full_data)","c2a55117":"corr_matrix = full_data.corr()\nfilter = np.abs(corr_matrix['SalePrice']) > 0.5\ncorr_features = corr_matrix.columns[filter].tolist()\nsns.clustermap(train_data[corr_features].corr(), annot=True, fmt=\".2f\")\nplt.show()","5864d789":"def impute_missing_values(df):\n# We can drop 'PoolQC','MiscFeature','Alley','Fence','FireplaceQu' columns as the percentage of null values present are very high\n    df = df.drop(['PoolQC','MiscFeature','Alley','Fence','FireplaceQu'],axis = 1)\n    bsmt_str_cols= ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n    #filling missing values with NA as we can understand from the data that there is no basment for these houses\n    df[bsmt_str_cols] = df[bsmt_str_cols].fillna('NA') \n\n    Gar_str_cols= ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n    #filling missing values with NA as we can understand from the data that there is no garage for these houses\n    df[Gar_str_cols] = df[Gar_str_cols].fillna('NA') \n\n\n    df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n    df[\"MasVnrType\"]= df[\"MasVnrType\"].fillna(\"None\")\n\n    df[\"MasVnrArea\"]= df[\"MasVnrArea\"].fillna(0)\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(0)\n    df['BsmtFullBath'] = df['BsmtFullBath'] .fillna(0)\n    df['BsmtHalfBath'] = df['BsmtHalfBath'] .fillna(0)\n    df['TotalBsmtSF'] = df['TotalBsmtSF'] .fillna(0)\n    df['BsmtUnfSF'] = df['BsmtUnfSF'] .fillna(0)\n    df['BsmtFinSF1'] = df['BsmtFinSF1'] .fillna(0)\n    df['BsmtFinSF2'] = df['BsmtFinSF2'] .fillna(0)\n    df['GarageCars'] = df['GarageCars'] .fillna(0)\n    df['GarageArea'] = df['GarageArea'] .fillna(0)\n\n    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode()[0])\n    df['Functional'] = df['Functional'] .fillna(df['Functional'].mode()[0])\n    df['MSZoning'] = df['MSZoning'] .fillna(df['MSZoning'].mode()[0])\n    df['SaleType'] = df['SaleType'] .fillna(df['SaleType'].mode()[0])\n    df['Exterior1st'] = df['Exterior1st'] .fillna(df['Exterior1st'].mode()[0])\n    df['Exterior2nd'] = df['Exterior2nd'] .fillna(df['Exterior2nd'].mode()[0])\n    df['KitchenQual'] = df['KitchenQual'] .fillna(df['KitchenQual'].mode()[0])\n    df['Utilities'] = df['Utilities'] .fillna(df['Utilities'].mode()[0])\n    \n    return df\n# # We can drop Utilites as it contains only one value \n# train_data = train_data.drop(['Utilities'],axis = 1)\n\n","9ac66e8b":"full_data = impute_missing_values(full_data)","2f314c89":"# we can ignore SalePrice as it is target variable and the missing values are coming from test dataset\nmissing_data(full_data)","d55b700f":"full_data.head()","aaefc143":"data_type = pd.DataFrame(full_data.dtypes,columns = ['data_type'])\ndata_type['count_unique_values'] = [full_data[col].nunique() for col in full_data.columns]\ndata_type['unique_values_examples'] = [full_data[col].unique()[0:10] for col in full_data.columns]","aa040b0c":"data_type","731a4f43":"cat_cols = data_type[data_type['data_type']=='object'].index\nnum_cols = data_type[data_type['data_type']!='object'].index\nnum_cols = num_cols.drop(['SalePrice','Id'])\nnum_cols, cat_cols","a6a87d99":"cat_to_num = pd.get_dummies(full_data[cat_cols], drop_first=True)\ncat_to_num.head()","ee15048d":"full_data = pd.concat([full_data,cat_to_num], axis = 1)\nfull_data.drop(cat_cols, axis=1, inplace = True)","e869f972":"#now divide it into original train and test dataset\ntrain = full_data[:1460].copy()\ntest = full_data[1460:].copy()\ntest.drop('SalePrice', axis = 1, inplace =True)\nprint(train.shape)\nprint(test.shape)","4eccdbe7":"y_train = train['SalePrice']\nX_train = train.drop(['SalePrice','Id'], axis = 1)","c9281d5a":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_train.head()","58333ef5":"#highly correlated values\ncorrelated_features = set()\ncorrleation_matrix = X_train.corr()\n\nfor i in range(len(corrleation_matrix.columns)):\n    for j in range(i):\n        if abs(corrleation_matrix.iloc[i,j]) > 0.8:\n            colname = corrleation_matrix.columns[i]\n            correlated_features.add(colname)\ncorrelated_features_lis = list(correlated_features)","032d22d4":"#drop highly correlated features\nprint(X_train.shape)\nX_train.drop(correlated_features_lis, axis=1, inplace = True)\nprint(X_train.shape)","b995743a":"X_train.head()","f08b8057":"from sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold","111fee04":"lm = LinearRegression()\nlm.fit(X_train, y_train)","2ca71b07":"rfecv = RFECV(estimator=lm, step = 1, cv = 10, scoring='r2')\nrfecv.fit(X_train, y_train)","dca264c2":"print(rfecv.n_features_)\nrfecv_cols = X_train.columns[rfecv.support_]","daaad3e5":"rfecv_cols","eef4e58b":"drop_cols = ['GarageCond_TA','RoofMatl_CompShg','RoofMatl_Metal','RoofMatl_Roll','RoofMatl_WdShake','GarageCond_Gd','RoofMatl_Membran','RoofStyle_Shed','Condition2_RRAe','GarageQual_Po','GarageCond_Po','GarageQual_Fa','RoofMatl_Tar&Grv','Exterior2nd_Other','Functional_Typ','GarageQual_TA','GarageQual_Gd','Condition2_PosA','Utilities_NoSeWa','LotConfig_FR3','HeatingQC_Po','Exterior2nd_Stone','Exterior1st_AsphShn','Foundation_Wood','GarageCond_Fa','Exterior1st_ImStucc','SaleType_Con','HouseStyle_2.5Fin','Exterior2nd_ImStucc','Condition1_RRAe']","60bf8740":"import statsmodels.api as sm\nX_train_rfe = X_train[rfecv_cols.drop(drop_cols)]\nX_train_lm = sm.add_constant(X_train_rfe)\n\nlr = sm.OLS(y_train, X_train_lm).fit()","352b4be1":"print(lr.summary())","b71f86ed":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7c2bd500":"y_train_pred = lr.predict(X_train_lm)","226519d5":"test.head()","bcbeaafb":"X_test = test.drop('Id', axis = 1)\nX_test[num_cols] = scaler.transform(X_test[num_cols])\nX_test.head()","849f957d":"X_test_lm = sm.add_constant(X_test[X_train_rfe.columns])\ny_test_pred = lr.predict(X_test_lm)\ny_test_pred.head()","da2b696b":"submission = pd.concat([test['Id'], y_test_pred], axis = 1)\nsubmission.columns = ['Id', 'SalePrice']\nsubmission.to_csv('submission.csv', index = False)","b1ac2eec":"submission.head()","6f8cdf53":"#### Impute Missing Values ","0d5fdb6e":"## Data Preparation","155a929c":"Find the data types of each column its unique values(10 samples) and count of unique values","3fdd27dd":"## Model Building","7502cab3":"#### One hot encoding ","a8f80d98":"## Apply the model on Test data set","0e7d2ea8":"#### Concatenate Train and Test dataset","4f8d06f9":"## Train Data","80e81afc":"### RFECV (Recursive Feature Elimination Cross Validation)\n1. It is used to select the best n features for the mode.\n2. You can also use RFE in which you would have to specify the number of features you want for building the model","9eb10cdd":"### StatsModel\nFurther reduce the features by checking their p-value and vif.\nSteps:\n1. Using the statsmodel check the statistical significance(coef, p-value, r2, etc) of the 35 features selected by rfecv.\n2. Then drop the feature one by one whose p-value and vif is greater than 0.05 (5%).\n3. Repeat the above steps till all the features has p-value and vif less than 0.05."}}