{"cell_type":{"07fd95bf":"code","b157a6d3":"code","e34bedc2":"code","d4b86396":"code","96f8b682":"code","716b6420":"code","37c3ce45":"code","66650a67":"code","d1cf6947":"code","03e98b1a":"code","aee57a92":"code","36660efd":"code","8d91e1d6":"code","add16867":"code","72e99e1d":"code","7c5356eb":"code","7a850443":"code","092ed7de":"code","e6b6cc9b":"code","d4837a3d":"code","5e3b8aca":"code","0b5ca042":"code","22b1cd77":"code","d3f5000b":"code","988d40aa":"code","233e7d69":"code","5764c971":"code","2c082b5a":"code","4fb5a49e":"code","721252c7":"code","dac87534":"code","f151ffc7":"code","76c95675":"code","4dd521e9":"code","ff359ba2":"code","c2131cb1":"code","c8c98b27":"code","8fc8a9d1":"code","66d03283":"code","a65c9dca":"code","70325122":"code","0cc71b12":"code","34ec2463":"code","a3c8be1f":"code","5ea8f667":"code","516b1a64":"code","da463107":"code","f4177cb3":"code","b19dcf1d":"code","04e0c04a":"code","fb504a6b":"markdown","b22a4f16":"markdown","6f052d60":"markdown","ab360975":"markdown","3c0eefee":"markdown","fe2baead":"markdown","d820380b":"markdown","4fcc46e4":"markdown","62acea03":"markdown","c531c2a5":"markdown","8882166e":"markdown","033f030e":"markdown","c8be3b3b":"markdown","9dd0e551":"markdown","8b0c4b0c":"markdown","49388eef":"markdown","5ee49f62":"markdown","15ed53ba":"markdown","2342b0e3":"markdown","09d3d495":"markdown","effe3e52":"markdown","8d221819":"markdown","c9d0a8e9":"markdown","ae0510ab":"markdown","ec864bb2":"markdown","01469432":"markdown","01c92c50":"markdown","bbb11153":"markdown","0acbd1ad":"markdown","7e91d9b8":"markdown","80bef1df":"markdown","49de7fd7":"markdown","5027d7eb":"markdown","c52cb310":"markdown","7cb09d42":"markdown","f1088cbb":"markdown","8af28228":"markdown","e3f436de":"markdown","d3c96174":"markdown","bb29f7a3":"markdown","2a099046":"markdown","92336420":"markdown","40c04b56":"markdown","0876db41":"markdown","873cac5f":"markdown","829ea202":"markdown","aff37163":"markdown","1f87fd9e":"markdown","07714ee9":"markdown","add33507":"markdown","812a02c3":"markdown"},"source":{"07fd95bf":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","b157a6d3":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","e34bedc2":"train_df.info()","d4b86396":"train_df.describe()","96f8b682":"train_df.head(8)","716b6420":"total = train_df.isnull().sum().sort_values(ascending=False)\npercent_1 = train_df.isnull().sum()\/train_df.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head(5)","37c3ce45":"train_df.columns.values","66650a67":"survived = 'survived'\nnot_survived = 'not survived'\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nprint(women,type(women))\nprint(men)","d1cf6947":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","03e98b1a":"FacetGrid = sns.FacetGrid(train_df, row='Embarked', size=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex' )\nFacetGrid.add_legend()","aee57a92":"sns.barplot(x='Pclass', y='Survived', data=train_df)","36660efd":"grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","8d91e1d6":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\n    \ntrain_df['not_alone'].value_counts()","add16867":"axes = sns.factorplot('relatives','Survived', \n                      data=train_df, aspect = 2.5, )","72e99e1d":"train_df = train_df.drop(['PassengerId'], axis=1)","7c5356eb":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n#     print(dataset['Cabin'],'........')\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n# we can now drop the cabin feature\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","7a850443":"data = [train_df, test_df]\n\nfor dataset in data:\n    mean = train_df[\"Age\"].mean()\n    std = test_df[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_df[\"Age\"].astype(int)\ntrain_df[\"Age\"].isnull().sum()","092ed7de":"train_df['Embarked'].describe()","e6b6cc9b":"common_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","d4837a3d":"# Converting Features:\n\ntrain_df.info()","5e3b8aca":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","0b5ca042":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","22b1cd77":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","d3f5000b":"train_df['Ticket'].describe()","988d40aa":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","233e7d69":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","5764c971":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ntrain_df['Age'].value_counts()","2c082b5a":"train_df.head(10)","4fb5a49e":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n    dataset['Fare'] = dataset['Fare'].astype(int)","721252c7":"data = [train_df, test_df]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age']* dataset['Pclass']","dac87534":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']\/(dataset['relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# Let's take a last look at the training set, before we start training the models.\ntrain_df.head(10)","f151ffc7":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()","76c95675":"sgd = linear_model.SGDClassifier(max_iter=5)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nprint('sgd_score: ',sgd.score(X_train, Y_train))\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(acc_sgd)","4dd521e9":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(acc_random_forest)","ff359ba2":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(acc_log)","c2131cb1":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(acc_knn)","c8c98b27":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(acc_gaussian)","8fc8a9d1":"perceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(acc_perceptron)","66d03283":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(acc_linear_svc)","a65c9dca":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(acc_decision_tree)","70325122":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","0cc71b12":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","34ec2463":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","a3c8be1f":"importances.plot.bar()","5ea8f667":"train_df  = train_df.drop(\"not_alone\", axis=1)\ntest_df  = test_df.drop(\"not_alone\", axis=1)\n\ntrain_df  = train_df.drop(\"Parch\", axis=1)\ntest_df  = test_df.drop(\"Parch\", axis=1)","516b1a64":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","da463107":"param_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\nclf.fit(X_train, Y_train)\nclf.best_params_","f4177cb3":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 5, \n                                       min_samples_split = 2,   \n                                       n_estimators=700, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","b19dcf1d":"Y_prediction","04e0c04a":"output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': Y_prediction})\noutput.to_csv('titanic_submission.csv', index=False)\nprint(\"Your output was successfully saved!\")","fb504a6b":"K-Fold Cross Validation:\n    \nK-Fold Cross Validation randomly splits the training data into K subsets called folds. Let\u2019s image we would split our data into 4 folds (K = 4). Our random forest model would be trained and evaluated 4 times, using a different fold for evaluation everytime, while it would be trained on the remaining 3 folds.\nEvery row represents one training + evaluation process. In the first row, the model get\u2019s trained on the first, second and third subset and evaluated on the fourth. In the second row, the model get\u2019s trained on the second, third and fourth subset and evaluated on the first. K-Fold Cross Validation repeats this process till every fold acted once as an evaluation fold.\n\nThe result of our K-Fold Cross Validation example would be an array that contains 4 different scores. We then need to compute the mean and the standard deviation for these scores.\nThe code below perform K-Fold Cross Validation on our random forest model, using 10 folds (K = 10). Therefore it outputs an array with 10 different scores.","b22a4f16":"Building Machine Learning Models","6f052d60":"Ticket:","ab360975":"Since the Ticket attribute has 681 unique tickets, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset.","3c0eefee":"Creating new Features\n\nI will add two new features to the dataset, that I compute out of other features.\n\n1. Age times Class","fe2baead":"survival:    Survival \nPassengerId: Unique Id of a passenger. \npclass:    Ticket class     \nsex:    Sex     \nAge:    Age in years     \nsibsp:    # of siblings \/ spouses aboard the Titanic     \nparch:    # of parents \/ children aboard the Titanic     \nticket:    Ticket number     \nfare:    Passenger fare     \ncabin:    Cabin number     \nembarked:    Port of Embarkation\n","d820380b":"# Training Rnadom Forest again","4fcc46e4":"5. SibSp and Parch:\n\n\nSibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that sows if someone is not alone.","62acea03":"# K Nearest Neighbor:","c531c2a5":"# Linear Support Vector Machine:","8882166e":"model has a average accuracy of 82% with a standard deviation of 4 %. The standard deviation shows us, how precise the estimates are .\nThis means in our case that the accuracy of our model can differ + \u2014 4%.","033f030e":"Age:\n\nNow we can tackle the issue with the age features missing values. I will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","c8be3b3b":"Data Exploration\/Analysis","9dd0e551":"# Logistic Regression:","8b0c4b0c":"# Conclusion:\nnot_alone and Parch doesn\u2019t play a significant role in our random forest classifiers prediction process. Because of that I will drop them from the dataset and train the classifier again. We could also remove more or less features, but this would need a more detailed investigation of the features effect on our model. But I think it\u2019s just fine to remove only Alone and Parch.","49388eef":"You can see that men have a high probability of survival when they are between 18 and 30 years old, which is also a little bit true for women but not fully. For women the survival chances are higher between 14 and 40.\nFor men the probability of survival is very low between the age of 5 and 18, but that isn\u2019t true for women. Another thing to note is that infants also have a little bit higher probability of survival.\nSince there seem to be certain ages, which have increased odds of survival and because I want every feature to be roughly on the same scale, I will create age groups later on.","5ee49f62":"Sex:\n\nConvert \u2018Sex\u2019 feature into numeric.","15ed53ba":"The Embarked feature has only 2 missing values, which can easily be filled. It will be much more tricky, to deal with the \u2018Age\u2019 feature, which has 177 missing values. The \u2018Cabin\u2019 feature needs further investigation, but it looks like that we might want to drop it from the dataset, since 77 % of it are missing.","2342b0e3":"The training-set has 891 examples and 11 features + the target variable (survived). 2 of the features are floats, 5 are integers and 5 are objects. Below I have listed the features with a short description:","09d3d495":"# Gaussian Naive Bayes:","effe3e52":"2. Fare per Person","8d221819":"Above you can see the 11 features + the target variable (survived). What features could contribute to a high survival rate ?\nTo me it would make sense if everything except \u2018PassengerId\u2019, \u2018Ticket\u2019 and \u2018Name\u2019 would be correlated with a high survival rate.","c9d0a8e9":"4. Pclass","ae0510ab":"3. Embarked, Pclass and Sex:\n","ec864bb2":"# Decision Tree","01469432":"Embarked:\n\nSince the Embarked feature has only 2 missing values, we will just fill these with the most common one.","01c92c50":"Above you can see that \u2018Fare\u2019 is a float and we have to deal with 4 categorical features: Name, Sex, Ticket and Embarked. Lets investigate and transfrom one after another.\n\nFare:\n\nConverting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas provides:","bbb11153":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.","0acbd1ad":"Embarked:\n\nConvert \u2018Embarked\u2019 feature into numeric.","7e91d9b8":"Missing Data:\n\nCabin:\n\nwe have to deal with Cabin (687), Embarked (2) and Age (177). First I thought, we have to delete the \u2018Cabin\u2019 variable but then I found something interesting. A cabin number looks like \u2018C123\u2019 and the letter refers to the deck. Therefore we\u2019re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero. In the picture below you can see the actual decks of the titanic, ranging from A to G.","80bef1df":"Embarked seems to be correlated with survival, depending on the gender.\nWomen on port Q and on port S have a higher chance of survival. The inverse is true, if they are at port C. Men have a high survival probability if they are on port C, but a low probability if they are on port Q or S.\nPclass also seems to be correlated with survival. We will generate another plot of it below.","49de7fd7":"figuring Missing values","5027d7eb":"# Stochastic Gradient Descent (SGD):","c52cb310":"# Random Forest:","7cb09d42":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.","f1088cbb":"# Selecting best model","8af28228":"From the table above, we can note a few things. First of all, that we need to convert a lot of features into numeric ones later on, so that the machine learning algorithms can process them. Furthermore, we can see that the features have widely different ranges, that we will need to convert into roughly the same scale. We can also spot some more features, that contain missing values (NaN = not a number), that wee need to deal with.\n","e3f436de":"As we can see, the Random Forest classifier goes on the first place. But first, let us check, how random-forest performs, when we use cross validation.","d3c96174":"# predicting on test data","bb29f7a3":"1. Age and Sex","2a099046":"# Hyperparameter Tuning","92336420":"Getting the data","40c04b56":"Fare:\n\nFor the \u2018Fare\u2019 feature, we need to do the same as with the \u2018Age\u2019 feature. But it isn\u2019t that easy, because if we cut the range of the fare values into a few equally big categories, 80% of the values would fall into the first category. Fortunately, we can use sklearn \u201cqcut()\u201d function, that we can use to see, how we can form the categories.","0876db41":"Creating Categories:\n\nWe will now create categories within the following features:\n\nAge:\n\nNow we need to convert the \u2018age\u2019 feature. First we will convert it from float into integer. Then we will create the new \u2018AgeGroup\u201d variable, by categorizing every age into a group. Note that it is important to place attention on how you form these groups, since you don\u2019t want for example that 80% of your data falls into group 1.","873cac5f":"# Perceptron","829ea202":"Importing the libraries","aff37163":"Name:\n\nWe will use the Name feature to extract the Titles from the Name, so that we can build a new feature out of that.\n","1f87fd9e":"Above we can see that 38% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.4 to 80. On top of that we can already detect some features, that contain missing values, like the \u2018Age\u2019 feature.","07714ee9":"# Feature Importance","add33507":"Here we can see that you had a high probabilty of survival with 1 to 3 realitves, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives).","812a02c3":"Data Preprocessing\n\ndroping \u2018PassengerId\u2019 from the train set"}}