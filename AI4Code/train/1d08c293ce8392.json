{"cell_type":{"f8129617":"code","ea21af05":"code","b84f74ab":"code","0f79672b":"code","89772727":"code","82681923":"code","59a84004":"code","f27359c7":"code","7cac50ee":"code","13a7e8ca":"code","d43abbcc":"code","58fc1e6b":"code","3499111a":"code","dc1c1e19":"code","b828d063":"code","0f67f066":"code","63c1265a":"code","60f5c4b4":"code","da521329":"code","5acbb5d1":"code","f69f0faf":"code","7e88111e":"code","95f2f56b":"code","e25c9b80":"code","3750d5f2":"code","8b96c29a":"code","5efe8134":"code","84b12aac":"code","bb8198d7":"markdown","6e8fcb48":"markdown","edd52284":"markdown","de8bc55a":"markdown","15caad2a":"markdown","703532f9":"markdown","7dc402bf":"markdown","01345e8f":"markdown","c89f7004":"markdown"},"source":{"f8129617":"import numpy as np\nimport pandas as pd \npd.set_option('display.max_columns', 2000)\nimport datetime\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nfrom tensorflow.keras import optimizers, utils\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GlobalMaxPooling2D, Dense, Flatten, Dropout, PReLU\nfrom tensorflow.keras.applications import EfficientNetB1\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nimport tensorflow as tf\n\nfrom scipy.special import erfinv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc","ea21af05":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\")\ntargetCols=[i for i in train.columns if \"target\" in i]\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/test.csv\")\nsamSub = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv\")\npslb = pd.read_csv(\"..\/input\/tps-lightautoml-baseline-with-pseudolabels\/lightautoml_with_pseudolabelling_kernel_version_14.csv\")","b84f74ab":"train[\"date_time\"] = train.date_time.astype(\"datetime64\")\ntest[\"date_time\"] = test.date_time.astype(\"datetime64\")\npslb[\"date_time\"] = pslb.date_time.astype(\"datetime64\")","0f79672b":"test = test.merge(right=pslb, on = \"date_time\")\ntest[targetCols] = test[targetCols].round(1)","89772727":"targetCols.append(\"date_time\")\nyTr=train[targetCols]\nyTe=test[targetCols]\ntargetCols=targetCols[:-1]","82681923":"def SaisonalComponents(DF1, DF2, periods=24*2):\n    DF12 = pd.concat([DF1, DF2])\n    DF = DF12.copy()\n    for i in DF12.columns[1:]:\n        result = seasonal_decompose(DF12[f\"{i}\"], model='additive', period=periods)\n        DF12[f\"S{i}\"] = result.seasonal\n        DF12[f\"S{i}\"] = DF12[f\"S{i}\"].fillna(DF12[f\"S{i}\"].mean())\n        #result.plot()\n    return DF, DF12","59a84004":"gap=24*2\nOrig, SeasAdj = SaisonalComponents(train, test, periods=gap)","f27359c7":"def FeatEng(DF, lags=range(2,29)):\n    \n    DF=DF.copy()\n    \n    DF[\"weekday\"] = np.sin(DF.date_time.dt.weekday \/ 7 * np.pi\/2)\n    DF[\"hour\"] = np.sin(DF.date_time.dt.hour \/ 24 * np.pi\/2)\n    DF[\"day\"] = DF.date_time.dt.day\n    DF[\"dayOfYear\"] = DF.date_time.dt.dayofyear\n    DF[\"month\"] = DF.date_time.dt.month\n    DF[\"working_hours\"] =  DF[\"hour\"].isin(list(range(7, 22, 1))).astype(\"int\")\n    DF[\"SMC\"] = np.log1p(DF[\"absolute_humidity\"] * 100) - np.log1p(DF[\"relative_humidity\"])\n    DF[\"Elapsed\"] = np.sin(DF.date_time.dt.dayofyear \/ 365 * np.pi \/ 2)\n    \n    DF[\"sensor_1sq\"] = DF[\"sensor_1\"]**2\n    DF[\"sensor_3sq\"] = DF[\"sensor_2\"]**2\n    DF[\"sensor_3sq\"] = DF[\"sensor_3\"]**2\n    DF[\"sensor_4sq\"] = DF[\"sensor_4\"]**2\n    DF[\"relative_humiditysq\"] = DF[\"relative_humidity\"]**2\n    DF[\"absolute_humiditysq\"] = DF[\"absolute_humidity\"]**2\n    DF[\"deg_Csq\"] = DF[\"deg_C\"]**2\n    \n    for l in lags:\n        for v in DF.columns[1:12]:\n            \n            m=DF[f\"{v}\"].mean()\n            s=DF[f\"{v}\"].std()\n            mx=DF[f\"{v}\"].mean()+DF[f\"{v}\"].std()\n            mi=DF[f\"{v}\"].mean()-DF[f\"{v}\"].std()\n\n            DF[\"mean{0}L{1}\".format(v,l)] = DF[f\"{v}\"].rolling(window=l, center=True).mean().fillna(m).round(3)\n            DF[\"sd{0}L{1}\".format(v,l)] = DF[f\"{v}\"].rolling(window=l, center=True).std().fillna(s).round(3)\n            DF[\"max{0}L{1}\".format(v,l)] = DF[f\"{v}\"].rolling(window=l, center=True).max().fillna(mx).round(3)\n            DF[\"min{0}L{1}\".format(v,l)] = DF[f\"{v}\"].rolling(window=l, center=True).min().fillna(mi).round(3)\n            DF[\"lagDelta{0}L{1}\".format(v,l)] = DF[f\"{v}\"] - DF[f\"{v}\"].rolling(window=l).mean().fillna(m).round(3)\n            gc.collect()\n\n    DF.dropna(inplace=True)\n\n    return DF","7cac50ee":"%%time\ntrainTest = FeatEng(Orig)\ntrainTest.rename(columns={\"target_carbon_monoxide\": \"target_carbon_monoxideN\", \n                          \"target_benzene\": \"target_benzeneN\", \n                          \"target_nitrogen_oxides\": \"target_nitrogen_oxidesN\"\n                         },\n                 inplace=True\n                )\ntrainTest.shape","13a7e8ca":"length=train.shape[0]\n\ntrain = pd.concat([trainTest.iloc[:length,:], SeasAdj.iloc[:length,12:]], axis=1)\ntest = pd.concat([trainTest.iloc[length:,:], SeasAdj.iloc[length:,12:]], axis=1)\n\ntrain = pd.concat([train, test]).reset_index(drop=True)\nYs = pd.concat([yTr, yTe]).reset_index(drop=True)\n\ntrain.shape","d43abbcc":"X = train.loc[train.date_time < datetime.datetime(2011,1,1,12,0,0)]\nx = train.loc[train.date_time > datetime.datetime(2011,1,1,12,0,0) + datetime.timedelta(hours=gap)]\n\nprint(\"tr shape: \" + str(X.shape))\nprint(\"val shape: \" + str(x.shape))\nimgSze = int(np.sqrt(X.shape[1]-1))\nprint(\"square root: \" + str(imgSze))","58fc1e6b":"def rg(DF1, DF2, e, Vars):\n    \n    DF1=DF1.copy()\n    length = DF1.shape[0]\n    DF2=DF2.copy()\n    \n    DF12 = pd.concat([DF1[Vars], DF2[Vars]])\n    \n    for i in Vars:\n        r = DF12[i].rank()\n        Range = (r\/r.max()-0.5)*2\n        Range = np.clip(Range, a_max = 1-e, a_min = -1+e)\n        rg = erfinv(Range)\n        rg = rg * 2**0.5\n        DF1[i] = rg[:length]\n        DF2[i] = rg[length:]\n        \n    return DF1, DF2","3499111a":"X, x = rg(X, x, 0.000001, train.columns[1:])","dc1c1e19":"Y=Ys.loc[Ys.date_time < datetime.datetime(2011,1,1,12,0,0)].drop(columns=[\"date_time\"]).values\ny=Ys.loc[Ys.date_time > datetime.datetime(2011,1,1,12,0,0) + datetime.timedelta(hours=gap)].drop(columns=[\"date_time\"]).values\nprint(y.shape)\n\nX=np.reshape(X.drop(columns=[\"date_time\"]).to_numpy(),(-1, imgSze, imgSze, 1))\nx=np.reshape(x.drop(columns=[\"date_time\"]).to_numpy(),(-1, imgSze, imgSze, 1))\nprint(x.shape)","b828d063":"XD = np.ndarray(shape=(X.shape[0], X.shape[1], X.shape[2], 3), dtype= np.uint8)\nXD[:, :, :, 0] = X[:,:,:,0]\nXD[:, :, :, 1] = X[:,:,:,0]\nXD[:, :, :, 2] = X[:,:,:,0]\n\nxD = np.ndarray(shape=(x.shape[0], x.shape[1], x.shape[2], 3), dtype= np.uint8)\nxD[:, :, :, 0] = x[:,:,:,0]\nxD[:, :, :, 1] = x[:,:,:,0]\nxD[:, :, :, 2] = x[:,:,:,0]","0f67f066":"train, test = rg(train, test, 0.000001, train.columns[1:])","63c1265a":"plt.figure(figsize=(5,5), dpi= 100)\nfig, p = plt.subplots(4, 6, figsize=(25,20))\n\nr=0\nc=0\n\nfor i in range(0, 288, 12):\n \n    p[r, c].imshow(x[i], interpolation='nearest')\n    \n    p[r, c].set_xlabel(test.iloc[i,0])\n    \n    if c == 0:\n        p[r, c].set_ylabel('heigth')\n    \n    if r == 0:\n        p[r, c].set_title('width')\n    \n    if c < 5:\n        c+=1\n    else:\n        c=0\n        r+=1\n        \nplt.show()   ","60f5c4b4":"Ys=Ys.drop(columns=[\"date_time\"]).values\n\ntrain=np.reshape(train.drop(columns=[\"date_time\"]).to_numpy(),(-1, imgSze, imgSze, 1))\ntest=np.reshape(test.drop(columns=[\"date_time\"]).to_numpy(),(-1, imgSze, imgSze, 1))\nprint(test.shape)","da521329":"trainD = np.ndarray(shape=(train.shape[0], train.shape[1], train.shape[2], 3), dtype= np.uint8)\ntrainD[:, :, :, 0] = train[:,:,:,0]\ntrainD[:, :, :, 1] = train[:,:,:,0]\ntrainD[:, :, :, 2] = train[:,:,:,0]\n\ntestD = np.ndarray(shape=(test.shape[0], test.shape[1], test.shape[2], 3), dtype= np.uint8)\ntestD[:, :, :, 0] = test[:,:,:,0]\ntestD[:, :, :, 1] = test[:,:,:,0]\ntestD[:, :, :, 2] = test[:,:,:,0]","5acbb5d1":"M = EfficientNetB1(\n    include_top=False, \n    input_shape=(imgSze, imgSze, 3),\n    weights='imagenet'\n    )\n\nmodel=Sequential()\nmodel.add(M)\nmodel.add(GlobalMaxPooling2D(name=\"pool\"))\nmodel.add(Dropout(rate=0.1))\nmodel.add(Dense(3, PReLU()))\n\nM.trainable = True","f69f0faf":"def rmsle(y_pred, y_true):\n    y_pred = tf.cast(y_pred, dtype=\"float32\")\n    y_true = tf.cast(y_true, dtype=\"float32\")\n    r = tf.sqrt(tf.keras.backend.mean(tf.square(tf.math.log(y_pred+1) - tf.math.log(y_true+1))))\n    return r","7e88111e":"lrReducer = ReduceLROnPlateau (    \n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=2,\n    verbose=1,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0.000001,\n    )","95f2f56b":"def lr_schaker(epoch, lr):\n    if epoch == 20:\n        lr = lr*1.1\n    elif epoch == 40:\n        lr = lr*1.1\n    return lr","e25c9b80":"model.compile(\n  optimizer=optimizers.SGD(\n      lr=0.2,\n      decay = 0.0001, \n      momentum = 0.5,\n      nesterov = True,\n      clipvalue=20\n      ),\n  loss=rmsle,\n  metrics=\"mae\",\n)\n\nmodel.summary()","3750d5f2":"history = model.fit(\n  trainD,\n  Ys,\n  batch_size=128,\n  epochs=40,\n  validation_data=(xD, y),\n  verbose=1,\n  use_multiprocessing=True,\n  workers=4, \n  callbacks=[lrReducer, LearningRateScheduler(lr_schaker, verbose=0)] \n)","8b96c29a":"predBx = model.predict(testD)\npredBx = pd. DataFrame(np.reshape(predBx, (test.shape[0], 3))) ","5efe8134":"samSub[targetCols] = predBx.values\nsamSub.to_csv(\"Submission.csv\",index=False)","84b12aac":"samSub.describe()","bb8198d7":"Credits to [Alexander Ryzhkov](https:\/\/www.kaggle.com\/alexryzhkov) for his [great lightAutoMl notebook](https:\/\/www.kaggle.com\/alexryzhkov\/tps-lightautoml-baseline-with-pseudolabels)!","6e8fcb48":"# Normalize","edd52284":"In this notebook, I have converted the tabular data into image data by feature engineering to be able to use pre-trained models. In this case I use an EfficientNetB7:\n\n![](https:\/\/1.bp.blogspot.com\/-Cdtb97FtgdA\/XO3BHsB7oEI\/AAAAAAAAEKE\/bmtkonwgs8cmWyI5esVo8wJPnhPLQ5bGQCLcBGAs\/s640\/image4.png)\n\nThe EfficientNetB7 performs a grid search to find the relationship between different scaling dimensions of the baseline network. The effectiveness of the scaling relies heavily on the baseline model. Therefore, an additional architecture search was developed. ","de8bc55a":"# Feature Engineering\nHere, so many features are created that the square root of the resulting columns results in an even number - after throwing out the variables that are not needed.","15caad2a":"To use pre-trained models, it is important to create three channels. ","703532f9":"# Test Validation Data\n\nAt this point it is important to set the gap between the training and validation data set as large as the largest delay - used at dhe feature engineering part.","7dc402bf":"# Image Visualization","01345e8f":"# The Model","c89f7004":"# Train Test Data"}}