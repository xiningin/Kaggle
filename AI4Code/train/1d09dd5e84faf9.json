{"cell_type":{"9c0e2b79":"code","2c9d2bab":"code","dbea5813":"code","20ff775d":"code","c0f225cb":"code","8955ac8f":"code","729d308d":"code","a57da042":"code","13e7aa41":"code","c0d71f9d":"code","2200030d":"code","e5e5de13":"code","9cc35c17":"code","07772ba4":"code","789b5f70":"code","78dedb4a":"code","9e46de8b":"code","a675f7f8":"code","ecaffb87":"code","0046bbf4":"code","802a4033":"code","286ac00a":"code","6b21736e":"code","04a7849a":"code","cffff54c":"code","07782b5d":"code","31e1c177":"code","3e2900f8":"code","0ba98c5e":"code","4d10de82":"code","eccd35b4":"code","6fba2d0c":"code","497a370c":"code","6409e337":"code","5e5a4873":"code","f235366b":"code","f87d7702":"code","1a5ae096":"code","402fa790":"code","579783b0":"code","687637ad":"code","725697fb":"code","7589cc82":"code","cb0c2f84":"code","c52fb5be":"code","0153b8da":"markdown","abb78924":"markdown","9a0409aa":"markdown","e8a1af22":"markdown","3fec24c6":"markdown","c1004e7c":"markdown","ba9b4c88":"markdown","518ccdc0":"markdown","e693e900":"markdown","5b1c8c5e":"markdown","0fcbba20":"markdown","5dc3af36":"markdown","16a9868f":"markdown","1e58ffe5":"markdown","4587abcd":"markdown","679fbb4a":"markdown","cbbb8138":"markdown","c7f58599":"markdown","522a5eb9":"markdown","91044042":"markdown","f0ff2ebd":"markdown","aea9b2e4":"markdown","158a7f25":"markdown","097bc27f":"markdown","2f28a405":"markdown","11601c7c":"markdown","83bf567e":"markdown"},"source":{"9c0e2b79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_selection import RFE\nimport sklearn.metrics as metrics\nimport scipy.stats as ss\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2c9d2bab":"df=pd.read_csv(\"..\/input\/travel insurance.csv\")\ndf1=df\ndf.head(5)","dbea5813":"df.info()","20ff775d":"missingno.matrix(df)","c0f225cb":"df['Gender'].isnull().sum()","8955ac8f":"df.fillna('Not Specified',inplace=True)","729d308d":"df.isnull().sum()","a57da042":"df_numerical=df._get_numeric_data()\ndf_numerical.info()","13e7aa41":"for i, col in enumerate(df_numerical.columns):\n    plt.figure(i)\n    sns.distplot(df_numerical[col])","c0d71f9d":"df['Duration'].describe()","2200030d":"df10=df['Duration']<0\ndf10.sum()","e5e5de13":"df.loc[df['Duration'] < 0, 'Duration'] = 49.317","9cc35c17":"df6= df['Net Sales']<df['Commision (in value)']\ndf6.sum()","07772ba4":"df.loc[df['Net Sales'] == 0.0, 'Commision (in value)'] = 0","789b5f70":"def cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))","78dedb4a":"categorical=['Agency', 'Agency Type', 'Distribution Channel', 'Product Name',  'Destination','Gender','Claim']\ncramers=pd.DataFrame({i:[cramers_v(df[i],df[j]) for j in categorical] for i in categorical})\ncramers['column']=[i for i in categorical if i not in ['memberid']]\ncramers.set_index('column',inplace=True)\n\n#categorical correlation heatmap\n\nplt.figure(figsize=(10,7))\nsns.heatmap(cramers,annot=True)\nplt.show()","9e46de8b":"test=[(df[df['Gender']=='Not Specified']['Claim'].value_counts()\/len(df[df['Gender']=='Not Specified']['Claim']))[1],(df[df['Gender']=='M']['Claim'].value_counts()\/len(df[df['Gender']=='M']['Claim']))[1],\n      (df[df['Gender']=='F']['Claim'].value_counts()\/len(df[df['Gender']=='F']['Claim']))[1]]\ntest","a675f7f8":"fig, axes=plt.subplots(1,3,figsize=(24,9))\nsns.countplot(df[df['Gender']=='Not Specified']['Claim'],ax=axes[0])\naxes[0].set(title='Distribution of claims for null gender')\naxes[0].text(x=1,y=30000,s=f'% of 1 class: {round(test[0],2)}',fontsize=16,weight='bold',ha='center',va='bottom',color='navy')\nsns.countplot(df[df['Gender']=='M']['Claim'],ax=axes[1])\naxes[1].set(title='Distribution of claims for Male')\naxes[1].text(x=1,y=6000,s=f'% of 1 class: {round(test[1],2)}',fontsize=16,weight='bold',ha='center',va='bottom',color='navy')\nsns.countplot(df[df['Gender']=='F']['Claim'],ax=axes[2])\naxes[2].set(title='Distribution of claims for Female')\naxes[2].text(x=1,y=6000,s=f'% of 1 class: {round(test[2],2)}',fontsize=16,weight='bold',ha='center',va='bottom',color='navy')\nplt.show()","ecaffb87":"pd.crosstab(df['Agency'],df['Agency Type'],margins=True)","0046bbf4":"table1=pd.crosstab(df['Agency'],df['Claim'],margins=True)\n\ntable1.drop(index=['All'],inplace=True)\ntable1=(table1.div(table1['All'],axis=0))*100\n\ntable1['mean commision']=df.groupby('Agency')['Commision (in value)'].mean()\ntable1","802a4033":"table1.columns","286ac00a":"fig,ax1=plt.subplots(figsize=(18,9))\nsns.barplot(table1.index,table1.Yes,ax=ax1)\nplt.xticks(rotation=90)\nax1.set(ylabel='Acceptance %')\nax2=ax1.twinx()\nsns.lineplot(table1.index,table1['mean commision'],ax=ax2,linewidth=3)","6b21736e":"table2=pd.crosstab(df['Product Name'],df['Claim'],margins=True)\ntable2=(table2.div(table2['All'],axis=0))*100\n\ntable2['mean commision']=df.groupby('Product Name')['Commision (in value)'].mean()\ntable2.drop(index=['All'],inplace=True)\ntable2","04a7849a":"fig,ax1=plt.subplots(figsize=(20,11))\nsns.barplot(table2.index,table2.Yes,ax=ax1)\nplt.xticks(rotation=90)\nax1.set(ylabel='Acceptance %')\nax2=ax1.twinx()\nsns.lineplot(table2.index,table2['mean commision'],ax=ax2,linewidth=3)","cffff54c":"tests=df.copy()\ntests['Duration_label']=pd.qcut(df['Duration'],q=35)\ntable3=pd.crosstab(tests['Duration_label'],tests['Claim'],normalize='index')\ntable3","07782b5d":"table3.columns\n","31e1c177":"plt.figure(figsize=(10,7))\nsns.barplot(table3.index,table3.Yes)\nplt.xticks(rotation=90)","3e2900f8":"table4=pd.crosstab(df['Destination'],df['Claim'],margins=True,normalize='index')\ntable4\n","0ba98c5e":"table4 = table4.sort_values(by=['Yes'], ascending=[False])\ntable4","4d10de82":"sns.countplot(df['Claim'])","eccd35b4":"from scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, df):\n        self.df = df\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = ss.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)","6fba2d0c":"X = df.drop(['Claim'], axis=1)\nct = ChiSquare(df)\nfor c in X.columns:\n    ct.TestIndependence(c, 'Claim')","497a370c":"df.drop(columns=['Distribution Channel','Agency Type'],axis=1,inplace=True)","6409e337":"df.info()","5e5a4873":"y=df['Claim']\n","f235366b":"x=df\nx.drop(columns='Claim',axis=1,inplace=True)","f87d7702":"x_dummy=pd.get_dummies(x,columns=['Agency','Gender','Product Name','Destination'],drop_first=True)","1a5ae096":"lr = LogisticRegression()\nrfe = RFE(estimator=lr, n_features_to_select=10, verbose=3)\nrfe.fit(x_dummy, y)\nrfe_df1 = rfe.fit_transform(x_dummy, y)","402fa790":"print(\"Features sorted by their rank:\")\nprint(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), x_dummy.columns)))","579783b0":"X=x_dummy[['Agency_EPX','Agency_TST','Gender_Not Specified','Product Name_2 way Comprehensive Plan','Product Name_24 Protect','Product Name_Basic Plan','Product Name_Comprehensive Plan','Product Name_Premier Plan','Product Name_Travel Cruise Protect','Product Name_Value Plan']]","687637ad":"X.head(5)","725697fb":"\n\nsmote = SMOTE(random_state=7)\nX_ov, y_ov = smote.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X_ov, y_ov, train_size=0.7, random_state=7)","7589cc82":"from sklearn.svm import LinearSVC\nalgo_dict = {'Random Forest Classifier':RandomForestClassifier(),'DecisionTreeClassifier':DecisionTreeClassifier(),'Linear SVC':LinearSVC()}\n\n\n                \nalgo_name=[]\nfor i in algo_dict:\n    algo_name.append(i)\n\nfor i in algo_dict.keys():\n      \n          \n        algo = algo_dict[i]\n        model = algo.fit(X_train, y_train)\n        y_pred = model.predict(X_test)        \n        print('Classification report'+'\\n',classification_report(y_test, y_pred))\n        print('***'*30)\n          \n        ","cb0c2f84":"from sklearn.ensemble import GradientBoostingClassifier\nlearning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in learning_rates:\n    gb = GradientBoostingClassifier(n_estimators=20, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n    gb.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate)\n    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_test, y_test)))\n    print()","c52fb5be":"gb = GradientBoostingClassifier(n_estimators=20, learning_rate = 0.5, max_features=2, max_depth = 2, random_state = 0)\ngb.fit(X_train, y_train)\npredictions = gb.predict(X_test)\n\n#print(\"Confusion Matrix:\")\n#print(confusion_matrix(y_test, predictions))\n#print()\nprint(\"Classification Report\")\nprint(classification_report(y_test, predictions))","0153b8da":"**Observation :**\nFemale Gender have highest number of Claims approved. Though the numbers aren't that descriptive to conclude to a decision","abb78924":"There is no null values now","9a0409aa":"So, there are 5 negative values in Duration column. I am gonna replace those with the mean value","e8a1af22":"Now, lets check which columns have the null values","3fec24c6":"Previously we have checked that some columns have low Net Sales but High Commison but thats not possible\n\nLets see how many such columns we have here","c1004e7c":"****Now lets look at the spread of the numerical data****","ba9b4c88":"So, there are 4 numerical columns and 7 categorical columns","518ccdc0":"Lets see which features are important for the prediction using **Chi Square Test**","e693e900":"45107\/63326 are null values, nearly 71.2% data in the column are null values.","5b1c8c5e":"**Lets see how many negative values we have in Duration column**","0fcbba20":"**Some of the products with high commission have a high ratio of claims acceptance. The plans with zero commission generally have low acceptance**","5dc3af36":"According to the **Chi Square Test** Distribution Channel is not important thus I am dropping the column. Also as discussed above \"Agency Type\" is gonna dropped","16a9868f":"**From the graph we can conclude:**\n\n*Duration*: Data in this column is highly right skewed.\n\n*Net Sales and Commison*: These both column seems to related but the graph plot shows disparency as low net sales     shows high commison which is not pratically possible.\n\n*Age*: Age is random so its distribution can be random.","1e58ffe5":"We have negative values in this Duration column but can time be negative? **NO**","4587abcd":"First make an another dataframe which consist of only numerical columns from df","679fbb4a":"From the above graph we can say that there is high imbalance in the target variable. We will see how to deal with that a little later","cbbb8138":"**On varying the value of bins, we found that Durations>364 have a high percentage of acceptance compared to the rest.**","c7f58599":"**Does the duration of the trip have an impact on claims acceptance**","522a5eb9":"**Lets Check how Gender is related to the Claim column**","91044042":"\"Black\" in the data depicts the column is fill with data and \"White\" depicts they have null values in that particular area\n\nSo, we can conclude that only \"Gender\" have the null values and seems quite much","f0ff2ebd":"**Observation:**\n\nWe can see the Co-relation between the Categorical columns.\n\nCan coclude that the cloumn \"Agency Type' have high corelation with some of the columns like \"Agency\",\"Product Name\" thus we can drop \"Agency Type\".","aea9b2e4":"**Lets check the data in \"Duration\" column**","158a7f25":"We gonna make all comission value 0 where net sales is 0.","097bc27f":"**Does Claim % depends on Agency Type**","2f28a405":"I have replaced the null values with another category called \"Not Specified\"","11601c7c":"Lets see whats the number of null values in the \"Gender\" column","83bf567e":"**X axis= Agency Name\nY axis=Acceptance %\nline plot= Commision\nSo we can see certain agencies have higher % of acceptance, C2B, LWC, TTW**"}}