{"cell_type":{"3becc99d":"code","888f2709":"code","f3ff1b10":"code","7612740c":"code","63b51d9f":"code","a0ac7665":"code","df88ec9b":"code","f6b86992":"code","dc04fa62":"code","09f08927":"code","f70ebde2":"code","5857e9f4":"code","382e7bc0":"code","6c552566":"code","a23d7dc3":"code","43409b1d":"markdown","ee4da668":"markdown","bab89dba":"markdown","9c54ae31":"markdown"},"source":{"3becc99d":"import os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch import optim\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport gc\ngc.enable()","888f2709":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 256\nEVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\nROBERTA_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nTOKENIZER_PATH = \"..\/input\/clrp-roberta-base\/clrp_roberta_base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42","f3ff1b10":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\n    \nset_random_seed(SEED)    \n","7612740c":"# train_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/train.csv\")\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nfolds_df = pd.read_csv('..\/input\/train-val-split\/kfold.csv')","63b51d9f":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)","a0ac7665":"def convert_examples_to_features(text, tokenizer, max_len):\n    tok = tokenizer.encode_plus(\n        text, \n        max_length=max_len, \n        truncation=True,\n        padding='max_length',\n    )\n    return tok\n\nclass CLRPDataset(Dataset):\n    def __init__(self, data, tokenizer, is_test=False):\n        self.data = data\n        self.excerpts = self.data.excerpt.tolist()\n        if not is_test:\n            self.targets = self.data.target.tolist()\n            \n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.max_len = MAX_LEN\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, item):\n        if not self.is_test:\n\n            excerpt = self.excerpts[item]\n            label = self.targets[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n                'label':torch.tensor(label, dtype=torch.float),\n            }\n        else:\n            excerpt = self.excerpts[item]\n            features = convert_examples_to_features(\n                excerpt, self.tokenizer, self.max_len\n            )\n            return {\n                'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n            }","df88ec9b":"class AttentionHead(nn.Module):\n    def __init__(self, hidden_dim=512):\n        super().__init__()\n        self.W = nn.Linear(768, 512)\n        self.V = nn.Linear(512, 1)\n        \n    def forward(self, features):\n        att = torch.tanh(self.W(features))\n        score = self.V(att)\n        attention_weights = torch.softmax(score, dim=1)\n        context_vector = attention_weights * features\n        context_vector = torch.sum(context_vector, dim=1)\n\n        return context_vector\n\nclass CLRPModel(nn.Module):\n    def __init__(self,transformer,config):\n        super(CLRPModel,self).__init__()\n        self.h_size = config.hidden_size\n        self.transformer = transformer\n        self.head = AttentionHead(self.h_size)\n        self.linear = nn.Linear(self.h_size, 1)\n              \n    def forward(self, input_ids, attention_mask):\n        transformer_out = self.transformer(input_ids, attention_mask)\n        x = self.head(transformer_out.last_hidden_state)\n        x = self.linear(x)\n        return x\n\n    \ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return optim.AdamW(parameters)","f6b86992":"def eval_mse(model, data_loader):\n    model.eval()            \n    mse_sum = 0\n\n    with torch.no_grad():\n        for batch_num, batch in enumerate(data_loader):\n            input_ids, attention_mask, target = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['label'].to(DEVICE)\n\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)                        \n            target = target.to(DEVICE)           \n            \n            pred = model(input_ids, attention_mask)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n                \n\n    return mse_sum \/ len(data_loader.dataset)","dc04fa62":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, batch in enumerate(data_loader):\n            input_ids, attention_mask = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE)\n\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n                        \n            pred = model(input_ids, attention_mask)                        \n\n            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n            index += pred.shape[0]\n\n    return result","09f08927":"def rmse_loss(y_true,y_pred):\n    return torch.sqrt(nn.functional.mse_loss(y_true,y_pred))\n","f70ebde2":"def train(model, model_path, train_loader, val_loader,\n          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n    losses_info = {\n        'train_loss': [],\n        'val_loss': [],\n    }\n    for epoch in range(num_epochs):                           \n        val_rmse = None         \n\n        for batch_num, batch in enumerate(train_loader):\n            model.train()\n            \n            input_ids, attention_mask, target = batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE), batch['label'].to(DEVICE)\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)            \n            target = target.to(DEVICE)                        \n\n            optimizer.zero_grad()\n            \n\n            pred = model(input_ids, attention_mask)\n                                                        \n#             mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n            mse = rmse_loss(pred, target.unsqueeze(1))\n#             print(f'{epoch+1}#[{step+1}\/{len(train_loader)}]: train loss - {mse.item()}')\n\n            mse.backward()\n            losses_info['train_loss'].append((step, math.sqrt(mse)))\n            \n            optimizer.step()\n#             if scheduler:\n            scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n                losses_info['val_loss'].append((step, val_rmse))\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break                               \n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n#                     torch.save(model.state_dict(), model_path)\n                    torch.save(model, model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")                                    \n                    \n                start = time.time()\n                                            \n            step += 1\n                        \n    \n    return best_val_rmse, losses_info","5857e9f4":"gc.collect()\n\nlist_val_rmse = []\n\nfor fold in range(5):\n    print(f\"\\nFold {fold + 1}\/{NUM_FOLDS}\")\n    model_path = f\"model_{fold + 1}.pth\"\n        \n#     set_random_seed(SEED + fold)\n    train_dataset = CLRPDataset(folds_df[folds_df.fold!=fold], tokenizer=tokenizer)    \n    val_dataset = CLRPDataset(folds_df[folds_df.fold==fold], tokenizer=tokenizer) \n  \n        \n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              drop_last=True, shuffle=True, num_workers=2)    \n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False, num_workers=2)    \n        \n#     set_random_seed(SEED + fold)    \n    \n    config = AutoConfig.from_pretrained(ROBERTA_PATH)\n    config.update({\n            \"hidden_dropout_prob\": 0.0,\n            \"layer_norm_eps\": 1e-7\n            }) \n\n    transformer = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n    \n    model = CLRPModel(transformer, config).to(DEVICE)\n    \n    optimizer = create_optimizer(model)                        \n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_training_steps=NUM_EPOCHS * len(train_loader),\n        num_warmup_steps=50)    \n    \n    val_rmse, losses_info = train(model, model_path, train_loader,\n                               val_loader, optimizer, scheduler=scheduler)\n    list_val_rmse.append(val_rmse)\n\n    del model\n    gc.collect()\n    \n    \n    steps, train_losses = list(zip(*losses_info['train_loss']))\n    plt.plot(steps, train_losses, label='train_loss')\n    steps, val_losses = list(zip(*losses_info['val_loss']))\n    plt.plot(steps, val_losses, label='val_loss')\n    plt.legend()\n    plt.show()\n    \n    print(\"\\nPerformance estimates:\")\n    print(list_val_rmse)\n    print(\"Mean:\", np.array(list_val_rmse).mean())\n    ","382e7bc0":"all_predictions = np.zeros((5, len(test_df)))\n\ntest_dataset = CLRPDataset(test_df, tokenizer, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                         drop_last=False, shuffle=False, num_workers=2)\nfor index in range(5):            \n    model_path = f\".\/model_{index+1}.pth\"\n    print(f\"\\nUsing {model_path}\")\n#     transformer = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n     \n#     model = CLRPModel(transformer, config)\n#     model.load_state_dict(torch.load(model_path))    \n    model = torch.load(f'.\/model_{index+1}.pth')\n    model.to(DEVICE)\n#     torch.save(model, f'model_full_{index}.pth')\n    \n    all_predictions[index] = predict(model, test_loader)\n    torch.save(model, f'model_{index+1}.pth')\n    del model\n    gc.collect()","6c552566":"print(all_predictions)\npredictions = all_predictions.mean(axis=0)\nsubmission_df.target = predictions\nprint(submission_df)\nsubmission_df.to_csv(\"submission.csv\", index=False)","a23d7dc3":"folds_df[folds_df.fold==4].head()","43409b1d":"# Inference","ee4da668":"# Model\nThe model is inspired by the one from [Maunish](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","bab89dba":"# Dataset","9c54ae31":"# Overview\nThis is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https:\/\/www.kaggle.com\/andretugan\/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https:\/\/www.kaggle.com\/maunish\/clrp-roberta-base).\n\nAcknowledgments: some ideas were taken from kernels by [Torch](https:\/\/www.kaggle.com\/rhtsingh) and [Maunish](https:\/\/www.kaggle.com\/maunish)."}}