{"cell_type":{"0fcdee6a":"code","d83789c5":"code","468b05a2":"code","80116747":"code","4758303c":"code","3432b747":"code","473fde22":"code","f64664dc":"code","74314dfe":"markdown","dff1a0c7":"markdown","e73bf3f0":"markdown","d61a99cb":"markdown","021f8965":"markdown","8387f7ad":"markdown","2ae1d697":"markdown","7863f2ab":"markdown","b171fe10":"markdown"},"source":{"0fcdee6a":"import pandas as pd\nimport numpy as np\n\n# import datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')","d83789c5":"for dataset in [train,test]:\n    \n    # Title : Gender \/ Married\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.')\n    dataset['Title'] = dataset['Title'].replace(\n        ['Capt', 'Col', 'Countess', 'Don','Dona', 'Dr', 'Jonkheer','Lady','Major', 'Rev', 'Sir'], 'Other')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')    \n    dataset['Title'] = dataset['Title'].astype(str)\n    \n    # Family: 0,1,2\n    dataset[\"Family\"] = dataset[\"Parch\"] + dataset[\"SibSp\"]\n    dataset['Family'] = dataset['Family'].astype(int)\n    dataset.loc[ dataset['Family'] >=  2, 'Family'] = 2\n    \n    # Embarked : Classification by marina\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    dataset['Embarked'] = dataset['Embarked'].astype(str)\n    \n    # Convert text to number.\n    dataset[\"Title\"] = dataset[\"Title\"].map({'Mr':0, 'Mrs':1, 'Miss':2, 'Master':3, 'Other':4})\n    dataset[\"Sex\"] = dataset[\"Sex\"].map({'male':0, 'female':1})\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].map({'Q':0, 'S':1, \"C\":2})\n       \n    # Fare\n    dataset.loc[(dataset['Fare'] <= 7.854), 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.854) & (dataset['Fare'] <= 10.5), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 21.679), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 39.688), 'Fare']   = 3\n    dataset.loc[ dataset['Fare'] > 39.688, 'Fare'] = 4\n    dataset['Fare'] = dataset['Fare'].fillna(5)\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    \n    # Age\n    dataset.loc[(dataset['Age'] <=20), 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 40), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 60), 'Age']   = 2\n    dataset.loc[ dataset['Age'] > 60, 'Age'] = 3\n    dataset['Age'] = dataset['Age'].fillna(4)\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain = train[['Survived',\"Title\", \"Sex\",\"Age\",\"Pclass\",\"Embarked\",\"Family\",\"Fare\"]]\ntest = test[[\"Title\", \"Sex\",\"Age\",\"Pclass\",\"Embarked\",\"Family\",\"Fare\"]]\ntrain","468b05a2":"# The variables to be used only have the following values.\nfitures = [\"Title\", \"Sex\",\"Pclass\",\"Embarked\",\"Family\",\"Fare\",\"Age\"]\n\nfor fiture in fitures :\n    white = \" \"*(10 - len(fiture))+\":\"\n    print(fiture,white,train[fiture].unique())","80116747":"# Splite The train dataset into train data and validation data.\nfitures = [\"Title\", \"Sex\",\"Pclass\",\"Embarked\",\"Family\",\"Fare\",\"Age\"]\n\nnum = int(len(train)*(7\/10)) \nX_train,X_valid = train[fitures][:num],   train[fitures][num:]\ny_train,y_valid = train['Survived'][:num],train['Survived'][num:]\nprint(f\"train data : {len(y_train)}\\nvalid data : {len(y_valid)}\")","4758303c":"import time\nimport pandas as pd\nfrom tqdm.notebook   import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_predict\n\ndef train_report(*models,dataset=(X_train,y_train,X_valid, y_valid)):\n    columns = [\"Name\", \"Time(sec)\",\"accuracy(%)\", \"precision(%)\",\"recall(%)\",\"f1-score(%)\",\"confusion\" ]\n    df = pd.DataFrame(columns=columns)\n    \n    X_train,y_train,X_valid,y_valid = dataset\n\n    for model in tqdm(models) :\n        model_name = str(model.__class__.__name__)\n        print(model_name, end=\"...\")\n        \n        # Time measurement\n        start = time.time()\n        \n        # Trainning start\n        model.fit(X_train,y_train)\n        \n        # report\n        y_pred     = cross_val_predict(model, X_valid, y_valid, cv=3)     \n        clf_report = classification_report(y_valid,y_pred, output_dict =True)\n        \n        accuracy   = clf_report[\"accuracy\"]                # accuracy\n        precision  = clf_report['macro avg']['precision']  # precision\n        recall     = clf_report['macro avg']['recall']     # recall\n        f1_score   = clf_report['macro avg']['f1-score']\n        confusion  = confusion_matrix(y_valid, y_pred)     # confusion_matrix\n        \n        accuracy,precision,recall,f1_score = [round(100*x,2) for x in [accuracy,precision,recall,f1_score]]\n        \n        train_time = round(time.time() - start,2)\n\n        # save data\n        new_row = {f\"{columns[0]}\":model_name, # name\n                   f\"{columns[1]}\":train_time, # training time\n                   f\"{columns[2]}\":accuracy,   # accuracy\n                   f\"{columns[3]}\":precision,  # precision\n                   f\"{columns[4]}\":recall,     # recall \n                   f\"{columns[5]}\":f1_score,   # f1_score \n                   f\"{columns[6]}\":confusion,  # confusion_matrix \n                  }\n        \n        df = df.append(new_row,ignore_index=True)    \n        df = df.drop_duplicates([\"Name\"],keep=\"last\")\n        print(\"complite..!\")\n    return df","3432b747":"from sklearn.svm          import SVC\nfrom sklearn.naive_bayes  import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors    import KNeighborsClassifier\nfrom sklearn.tree         import DecisionTreeClassifier\nfrom sklearn.ensemble     import RandomForestClassifier\nfrom sklearn.ensemble     import ExtraTreesClassifier\nfrom sklearn.ensemble     import BaggingClassifier\nfrom sklearn.ensemble     import VotingClassifier\n\nrand = 1234\n\nsvm_clf = SVC()\ngnb_clf = GaussianNB()\nsgd_clf = SGDClassifier()\nlog_clf = LogisticRegression()\nknn_clf = KNeighborsClassifier(n_neighbors = 15)\nrdf_clf = RandomForestClassifier(n_estimators=100)\next_clf = ExtraTreesClassifier(n_estimators=5,random_state=rand)\n\nbag_clf=BaggingClassifier(\n    DecisionTreeClassifier(random_state=rand),n_estimators=20,\n    max_samples=50,bootstrap=True,n_jobs=-1,random_state=rand)\n\nvoting_clf=VotingClassifier(estimators=[\n    ('svm',svm_clf),\n    ('knn',knn_clf),\n    ('rdf',rdf_clf),\n    ('ext',ext_clf)\n], voting='hard')\n\nmodels =  [svm_clf, gnb_clf, sgd_clf, log_clf, knn_clf, rdf_clf, ext_clf, bag_clf, voting_clf]","473fde22":"clf_data= train_report(*models)\nclf_data","f64664dc":"X = train[[\"Title\", \"Sex\",\"Pclass\",\"Embarked\",\"Family\",\"Fare\",\"Age\"]]\ny = train['Survived']\n\nfor model in tqdm(models):\n    model.fit(X,y)\n    a = pd.DataFrame({\"PassengerId\":range(892,1310),\"Survived\":model.predict(test)})\n    a = a.set_index(\"PassengerId\")\n    a.to_csv(f\"{model.__class__.__name__}.csv\")","74314dfe":"# 1. Preprocessing\n- Title : Gender \/ Married\n- Family: alone, with one, with 2 or more  \n    The number of people who did not come alone is small, more than two is treated as two.\n- Embarked : Classification by marina  \n    There are Two NAN data. They are likely to be S,the largest among SQC.  \n    Even if they are not S, I hope the large number would cancle the error.  \n- Convert text to number.  \n    A large number is assigned to the classification with the highest survival rate.\n\n    ","dff1a0c7":"# A Beginner's Classifier Comparison\n1. Data Preprocessing\n2. Check Dataset\n   - 2.1. Check input data\n   - 2.2. Split train, validation data\n3. Definition\n    - 3.1. Custom function\n    - 3.2. Simple classifiers\n4. Report\n5. Output scv\n\nBased on the following materials.  \n1. [insight from sinakhorami](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier)\n1. [insight from cyc1am3n](https:\/\/github.com\/cyc1am3n\/kaggle_study\/blob\/master\/01_titanic\/01_titanic.ipynb)\n\n## Survival rate\n- Sex: Male (18.9%) < Female (74.2%\n- Embarked: Q < S < C)\n- Pclass: class3 < class2 < class1\n- SibSp : The proportion of people who survived on a boat with two or more siblings or spouses was large\n- Parch : I survived a little more when I was on a boat with two or more parents or children,Otherwise, the proportion of people who survived was small.","e73bf3f0":"## 2.2 Split train, validation data","d61a99cb":"# 5. Output csv","021f8965":"# 2. Check Dataset \n## 2.1. Check input data","8387f7ad":"- [pairplot](https:\/\/datascienceschool.net\/view-notebook\/4c2d5ff1caab4b21a708cc662137bc65\/)\n- [seaborn](https:\/\/chancoding.tistory.com\/12)\n- [panda](https:\/\/3months.tistory.com\/292)","2ae1d697":"## 3.2. Simple classifiers","7863f2ab":"# 3. Definition\n## 3.1. Custom function","b171fe10":"# 4. Report"}}