{"cell_type":{"44781b78":"code","8ba31bd2":"code","cd6255b2":"code","19ed3f09":"code","bcca3a1c":"code","e677bbdf":"code","3ff9e547":"code","7e5be137":"code","85afcca7":"code","643afc43":"code","4c112ebf":"code","bffe9a5b":"markdown","e29f84a1":"markdown","f6695487":"markdown","53155c2c":"markdown","24e6ebea":"markdown","519f0586":"markdown","5f3cabf1":"markdown","80b9a651":"markdown","03747782":"markdown","808f21b3":"markdown","f507cb63":"markdown"},"source":{"44781b78":"import numpy as np # linear algebra\nfrom tqdm import tqdm_notebook\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom tqdm import tqdm_notebook","8ba31bd2":"# !pip install transformers","cd6255b2":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","19ed3f09":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","bcca3a1c":"\ndef get_features(data, batch_size=2500):\n    # Use DistilBERT as feature extractor:\n    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n    # Load pretrained model\/tokenizer\n    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n    model = model_class.from_pretrained(pretrained_weights)\n     model.to(device)\n    \n    # tokenize,padding and masking\n    tokenized = data[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n\n    attention_mask = np.where(padded != 0, 1, 0)\n    \n    last_hidden_states=[]\n    no_batch = data.shape[0]\/\/batch_size\n    start_index=0\n    end_index=1\n    for i in tqdm_notebook(range(1,no_batch+2)):\n\n        if  data.shape[0]>batch_size*i:\n                end_index=batch_size*i\n        else:\n            end_index=train.shape[0]\n\n        input_ids = torch.tensor(padded[start_index:end_index])  \n        batch_attention_mask = torch.tensor(attention_mask[start_index:end_index])\n\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n\n        with torch.no_grad():\n            batch_hidden_state = model(input_ids, attention_mask=batch_attention_mask)\n#             import pdb\n#             pdb.set_trace()\n            print(\"Batch {} is completed sucessfully\".format(i))\n            last_hidden_states.append(batch_hidden_state[0])\n\n        start_index=batch_size*i\n        end_index=batch_size*i\n    fin_features = torch.cat(last_hidden_states,0)\n    clf_features = fin_features[:,0,:].numpy()\n    return clf_features","e677bbdf":"gc.collect()\nfeatures = get_features(train,batch_size=2500)\ntest_features = get_features(test,batch_size=2500)","3ff9e547":"## Use features from previous modle and train a Logistic regression model\nlabels = train[\"target\"]\n# train model\nlr_clf = LogisticRegression()\nlr_clf.fit(features, labels)","7e5be137":"# train_features, test_features, train_labels, test_labels = train_test_split(clf_features, labels,random_state=420)","85afcca7":"lr_clf = LogisticRegression()\nlr_clf.fit(features, labels)","643afc43":"test_pred = lr_clf.predict(test_features)","4c112ebf":"\nsubmission['target'] = test_pred\nsubmission.to_csv('submission.csv', index=False)\n","bffe9a5b":"## Training a Logistic Regression model using features from DistilBERT","e29f84a1":"The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.","f6695487":"This notebook approach is derived and inspired from http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/","53155c2c":"If we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:","24e6ebea":"Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.","519f0586":"To DO\nThis notebook is still in progress. \n* Pre-processing of text\n* Currenty, only using *text* column. Encode other information such as keywords, location etc.","5f3cabf1":"![image.png](attachment:image.png)\n\nSource: http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/","80b9a651":"Prepare dataset\nSince Kaggle only allow 16 GB RAM and it would not be enough for processing all training data through it. We need to process the train dataset in few batches \n","03747782":"## Create a submission file\n","808f21b3":"## Feature Extraction using DistilBERT","f507cb63":"After tokenization, tokenized is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."}}