{"cell_type":{"13697783":"code","ada16190":"code","3ae9c528":"code","f389b319":"code","2dc2fb85":"code","e4a56b7e":"code","96cec5fe":"code","45cc813c":"code","be7d9f30":"code","ae2d5574":"code","ccb93cfe":"code","c2eff29c":"code","357bfbf0":"code","34ffe2a1":"code","258adf8f":"code","e8213a07":"code","be71ebdb":"code","6261ea09":"code","1f7a2de3":"code","5137e37a":"code","d2874cd1":"code","92012f12":"code","55e6ccc3":"code","50ef5ba5":"code","854526a9":"code","46c1b24d":"code","d825de73":"code","a6bef6b9":"code","b01a8068":"code","22a73a61":"code","d77676f7":"code","f2a50913":"code","8e7e3207":"code","81389074":"code","10056eef":"code","bd7d9732":"code","7e94ef56":"code","3f176609":"code","617fd2d3":"code","d7985b7f":"code","950d35cb":"code","5fb11625":"code","d79d8c5a":"code","647dd0b2":"code","25a755c7":"code","1c949062":"code","075df6d3":"code","4ca1fd23":"code","bc1887fc":"code","c85db475":"code","acc10a51":"code","727ca0a6":"markdown","db96f9af":"markdown","5300988f":"markdown","7c264e5b":"markdown","f4da3730":"markdown","9fdfd5bd":"markdown","90736f68":"markdown","3a91cd73":"markdown","68694dbb":"markdown","167686d5":"markdown","b0f25f7f":"markdown","6d1d6446":"markdown","1912068f":"markdown","03bf7c98":"markdown","72914e41":"markdown","ff8c2984":"markdown","51eb998b":"markdown","b55f0ae4":"markdown","fb26b761":"markdown","522045e6":"markdown","ed7a514b":"markdown"},"source":{"13697783":"# Display full output in Jupyter notebook cell (not only final statement)\nfrom __future__ import print_function\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","ada16190":"# torch from pytorch (huggingface)\nimport torch\n#from transformers import *\nimport pandas as pd\nimport numpy as np\nimport os\nimport pickle\nfrom pathlib import Path\n\nimport collections\nfrom tqdm import tqdm\nimport pprint\n\ntqdm.pandas(desc=\"my bar!\")","3ae9c528":"# if using GPU, test GPU is working\n\ncuda = torch.device('cuda')     # Default CUDA device\ntorch.__version__\ntorch.cuda.get_device_name(0)","f389b319":"# set display options\npd.options.display.max_rows\npd.set_option('display.max_colwidth', -1)\nfrom IPython.display import display, HTML\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)","2dc2fb85":"%%capture\n# clone sentence-transformers code and examples, and install\n# install sentence-transformers and download repo to perform fine-tuning steps. pip install will not give you access to all the examples\n\n!git clone https:\/\/github.com\/UKPLab\/sentence-transformers.git\nos.chdir(\"\/kaggle\/working\/sentence-transformers\")\n!pip install -e .","e4a56b7e":"%%capture\n# Install spacy and scispacy, and scispacy language model\n!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_sm-0.2.4.tar.gz\n","96cec5fe":"import scispacy\nimport spacy\nimport en_core_sci_sm\nnlp = en_core_sci_sm.load()\nnlp.max_length=100000000 # for extra long documents","45cc813c":"#!pip install sentence-transformers\n#import os\n#os.chdir(\"\/content\/sentence-transformers\")\nfrom sentence_transformers import SentenceTransformer","be7d9f30":"\n%%capture\n#!pip install faiss-cpu\nuseGPU=True\n\n!pip install faiss-gpu\nimport faiss","ae2d5574":"# define some generic functions\n\n# define sentence_embedding function\ndef embed_sentence_list(model,list_of_sents):\n    sentence_embeddings=model.encode(list_of_sents)\n    doc_matrix=np.asarray(sentence_embeddings,dtype=np.float32)\n    return(doc_matrix)\n\n\n# we need to serialise faiss index to save it to output\n#https:\/\/gist.github.com\/mdouze\/e30e8f57a98ed841c082cc68baa14b4a\n\ndef serialize_index(index):\n    \"\"\" convert an index to a numpy uint8 array  \"\"\"\n    writer = faiss.VectorIOWriter()\n    faiss.write_index(index, writer)\n    return faiss.vector_to_array(writer.data)\n\n\ndef deserialize_index(data):\n    reader = faiss.VectorIOReader()\n    faiss.copy_array_to_vector(data, reader.data)\n    return faiss.read_index(reader)","ccb93cfe":"model_load_path='\/kaggle\/input\/bertcovidbasicnlists\/training_nli_sts_covid-bert-base-2020-04-01_00-26-48'\n\nif useGPU:\n    model=torch.load(model_load_path) \n\nelse: # not working\n    #model=torch.load(model_load_path,map_location=torch.device('cpu')) \n    device = torch.device('cpu')\n    model=SentenceTransformer()\n    model.load_state_dict(torch.load(model_load_path, map_location=device) # not working\n# type(model) #  model is of type \"SentenceTransformer\"","c2eff29c":"# Load pre-built files containing dataframes, where each row represents one paper.\n# Columns are [paper_id, abstract_text, body_text,ents]\n# The \"ents\" column represents entities present in the abstract (or if not provided, the body text.) \n# The \"ents\" column was generate using code in Part B, but was run on Google Colab as described below. \ndef read_full_cord19_df():\n    df_covid1=pd.read_csv(\"\/kaggle\/input\/cord19-df-with-entities\/1.csv\",index_col=0)\n    df_covid2=pd.read_csv(\"\/kaggle\/input\/cord19-df-with-entities\/2.csv\",index_col=0)\n    df_covid3=pd.read_csv(\"\/kaggle\/input\/cord19-df-with-entities\/3.csv\",index_col=0)\n    df_covid4=pd.read_csv(\"\/kaggle\/input\/cord19-df-with-entities\/4.csv\",index_col=0)\n    df_covid5=pd.read_csv(\"\/kaggle\/input\/cord19-df-with-entities\/5.csv\",index_col=0)\n\n    # Concatenate individual dfs to one df\n    df_covid_ents=pd.concat([df_covid1,df_covid2,df_covid3,df_covid4,df_covid5])\n    return df_covid_ents","357bfbf0":"# Read in processed Document dataframe \ndf_covid_ents=read_full_cord19_df()","34ffe2a1":"# Load pre-built faiss index from input\n# This contains an embedding for each sentence, for each document\n# The sentence embedding model has been trained as per Part A, below. \n\ndef read_full_faiss_index():\n    filepath=Path(\"\/kaggle\/input\/faiss-index-file-full-d\") \/ \"index_faiss_file_all\"\n    file=open(filepath, \"rb\")\n    data=pickle.load(file)\n    faiss_index=deserialize_index(data)\n\n    # convert index from cpu to gpu\n    gpu_index=None\n    if useGPU==True:\n        res = faiss.StandardGpuResources()  # use a single GPUres = faiss.StandardGpuResources()  # use a single GPU\n        gpu_index = faiss.index_cpu_to_gpu(res, 0, faiss_index)\n    \n    # Load pre-built index dictionary\n    # format, for each index in faiss index (key), values are the paper_id, and sentence_id, of the embedded sentence at that index locaiton in the faiss index. \n\n    filepath = Path(\"\/kaggle\/input\/faiss-index-to-doc-sent-ids-dict\") \/ \"faiss_index_ids_dict_all\"\n    #filepath=\"\/content\/drive\/My Drive\/kaggle\/covid19\/input\/faiss_index_ids_dict_all\"\n\n    infile=open(filepath, \"rb\")\n    ids_dict=pickle.load(infile)\n    \n    return gpu_index, ids_dict, faiss_index","258adf8f":"gpu_index, ids_dict, faiss_index=read_full_faiss_index()","e8213a07":"# Let us run a test queries\nquery_list=[]\nquery_list.append(\"nurse to patient transmission in aged care facilities\")\n\n# create a numpy array, and normalise\nxq=embed_sentence_list(model,query_list) # model is instantiated previously from sentence-transformers\nfaiss.normalize_L2(xq)","be71ebdb":"top_k = gpu_index.search(xq[0,:].reshape(1,-1), 3) # sanity check\ntop_k","6261ea09":"j=0 # first result\n\n#  return faiss index values from matches from second element in result list (at top_k[1]).\n# Once in list, get first row (query), and j'th column (j'th best match ) (at ...to_list()[0][j])\nindex_tmp=top_k[1].tolist()[0][j] \n\n# Retrieve original sentence text from best match \npaper_id, sent_id=ids_dict[index_tmp][0] # use dictionary to retrieve original paper, and sentence location \npaper_row=df_covid_ents[df_covid_ents['paper_id']==paper_id] # get row in document by filtering for unique paper id \n\ndoc=nlp(paper_row.iloc[0]['body_text']) # convert text to sentences \n\n\n# get sentence - need to re-execute spacy pipeline to retrieve sentences since this is not stored. \nlist_of_sents=[sent.text for sent in doc.sents]\nsent=list_of_sents[sent_id] # retrieve sentence id\n\n\nprint(sent)","1f7a2de3":"list_of_spans=[sent for sent in doc.sents]\n\nspan_start=list_of_spans[sent_id].start\nspan_end=list_of_spans[sent_id].end\nif span_start < 100:\n    span_start = 0\nelse:\n    span_start -=100\nif (span_end + 100) > len(doc):\n    span_end = len(doc)\nelse:\n    span_end += 100\n    \nspan_results=doc[span_start: span_end]\n\n\nlist_of_spans[sent_id] # original sentence \nspan_results # full span ","5137e37a":"score_tmp=top_k[0].tolist()[0][j] # return faiss scores from first element in result list (at top_k[0])\nscore_tmp # faiss inner product score\n\n# check the score manually, to make sure it is what we expect \ndoc_matrix=embed_sentence_list(model,list_of_sents)\nfaiss.normalize_L2(doc_matrix)\nfrom scipy.spatial.distance import cosine\n1-cosine(xq[0,:],doc_matrix[sent_id,:]) # first query (this is a cosine distance, not a cosine similarity)  # agreement! \n\nnp.inner(xq[0,:],doc_matrix[sent_id,:]) # agreement! ","d2874cd1":"pp = pprint.PrettyPrinter(indent=4)\n\ndef create_gpu_index(df_covid_tmp):\n    \"\"\" \n    used to generate a smaller faiss index, limited by keyword\n    \"\"\"\n    tqdm.pandas(desc=\"my bar!\")\n\n    print(\"creating new gpu index of size\", df_covid_tmp.shape[0])\n    \n    # init faiss index\n    d=768 # sentence transformer embedding length\n    res = faiss.StandardGpuResources()  # use a single GPU\n    index = faiss.IndexIDMap(faiss.IndexFlatIP(d)) # IP is inner product. Data must be normalised first\n    gpu_index_tmp = faiss.index_cpu_to_gpu(res, 0, index)\n\n    # init dict\n    ids_dict_tmp = collections.defaultdict(list)\n\n    ids_next=0\n    i=0\n    for row in tqdm(range(df_covid_tmp.shape[0])):\n        doc=nlp(df_covid_tmp.iloc[row]['body_text'])\n        paper_id=df_covid_tmp.iloc[row]['paper_id']\n        \n        list_of_sents=[sent.text for sent in doc.sents]\n        #if len(list_of_sents) > 800:\n        #    list_of_sents=list_of_sents[:800] # just taking first 800 sentences for memory reasons. \n            \n        doc_matrix=embed_sentence_list(model,list_of_sents)\n        faiss.normalize_L2(doc_matrix)\n\n        # ids in faiss index begin after last idx (last document processed, last sentence)\n        custom_ids = np.array(range(ids_next, ids_next+len(doc_matrix))) # from last postion (range add +1) to new position\n        ids_next=ids_next+len(doc_matrix) # increment by current document length. Current doc lenght = num sentences in current doc             \n        gpu_index_tmp.add_with_ids(doc_matrix, custom_ids)\n        for sent_idx, faiss_ids_val in enumerate(custom_ids): # sentence_idx is sentence id within the 1 document, faiss_ids_val is the faiss index value\n            items=(paper_id, sent_idx)\n            ids_dict_tmp[faiss_ids_val].append(items)\n            \n    del doc_matrix\n    del doc\n    gc.collect()\n    return(gpu_index_tmp,ids_dict_tmp)","92012f12":"# to  do - add in function to process search response\n# combine with absolute match result to ensure sentence contain keywords\npp = pprint.PrettyPrinter(indent=4)\nimport gc\ndef search(model,query, k=3, gpu_index_default=None,ids_dict_default=None,keyword=None,df_covid_ents_default=None):\n    \"\"\"\n    returns results based on keyword, the gpu_index, and the ids_dict\n    \n    :param model, query, k, keyword, gpu_index_default, ids_dict_default \n    :type\n    :param query\n    :type \n    \"\"\"\n         \n    query_list=[query]\n    xq=embed_sentence_list(model,query_list) # model is instantiated previously from sentence-transformers\n    faiss.normalize_L2(xq)\n\n    if df_covid_ents_default is None:\n      df_covid_ents=read_full_cord19_df() #df_covid_ents\n    else:\n      df_covid_ents=df_covid_ents_default\n\n    if gpu_index_default==None or ids_dict_default==None:\n\n      if keyword is not None:\n        df_covid_tmp=df_covid_ents[df_covid_ents['ents'].str.contains(keyword, na=False) ].copy() # filter on keyword\n        print(\"Create new faiss index based on\", df_covid_tmp.shape[0], \"documents\")\n        gpu_index_tmp,ids_dict_tmp = create_gpu_index(df_covid_tmp)  # generate new index, based on filtered dataframe\n\n      else: # no keyword but still need to create index from scratch\n        gpu_index_tmp, ids_dict_tmp = read_full_faiss_index() #gpu_index_default # if no keyword, use original artifacts\n        df_covid_tmp=df_covid_ents #df_covid_ents\n\n    else: \n        print(\"using custom index and dict - ignoring any keywords\") #ignore keyword\n        gpu_index_tmp=gpu_index_default\n        ids_dict_tmp=ids_dict_default\n        df_covid_tmp=df_covid_ents\n  \n    # get top k best matches through index look_up\n    top_k = gpu_index_tmp.search(xq, k)  \n    # >>>\n    pp.pprint(\"end faiss search\")\n    # >>>\n      \n    # init output results dataframe \n    colnames=[\"query\",\"sentence\",\"score\", \"span\",\"paper_id\"]    \n    results=pd.DataFrame(columns=colnames)\n      \n    for i, _id in tqdm(enumerate(top_k[1].tolist()[0])): # for each result in the top k, element zero since only one query (could do batch queries)\n        paper_id, sent_id=ids_dict_tmp[_id][0]\n          \n        ## retrieve sent value   \n        # get row in document by filtering for unique paper id \n        paper_row=df_covid_tmp[df_covid_tmp['paper_id']==paper_id]\n        \n        # convert text to sentences \n        doc=nlp(paper_row.iloc[0]['body_text'])\n\n        # get sentence - need to re-execute spacy pipeline to retrieve sentences since this is not stored. \n        list_of_sents=[sent.text for sent in doc.sents]\n        sentence_result=list_of_sents[sent_id] # retrieve original sentence text       \n\n        # retrieve spans\n        list_of_spans=[sent for sent in doc.sents]\n        \n        span_start=list_of_spans[sent_id].start\n        span_end=list_of_spans[sent_id].end\n\n        if span_start < 100:\n          span_start = 0\n        else:\n          span_start -= 100\n                \n        if (span_end + 100) > len(doc):\n          span_end = len(doc)\n        else:\n          span_end += 100\n                \n        span_results=doc[span_start: span_end]        \n\n        # get score\n        score_tmp=top_k[0].tolist()[0][i] # score for i'th match\n            \n        #title=paper_row['title'] # to do\n        #authors=paper_row['authors'] # to \n        #abstract=paper_row['abstract'] # to do\n            \n        tmp=pd.DataFrame( [pd.Series([query,sentence_result, score_tmp, span_results, paper_id],index=colnames)] )\n        results=results.append(tmp, ignore_index=True, sort=False)\n              \n    return(results, gpu_index_tmp, ids_dict_tmp)\n","55e6ccc3":"# if GPU is working, run search \n\nresults=search(model=model,\n               query=\"nurses transmit to patients in hospitals\", \n               k=6)\ndf,index_,dict_=results\n\n","50ef5ba5":"df=pd.read_csv(\"\/kaggle\/input\/results-output\/full_dataset_results_transmission.csv\") # pregenerate results\n\ndf[[\"paper_id\",\"sentence\",\"score\",\"span\"]].head() ","854526a9":"# works if Kaggle GPU is working\nresults=search(model=model,\n               query=\"timing intervals of treatment delivered\",\n               k=3, \n               gpu_index_default=None, \n               ids_dict_default=None,\n                 keyword=\"hydroxychloroquine\", \n)\ndf_results_hcq, gcu_index_hcq, ids_dict_hcq=results\n","46c1b24d":"# read in results compiled on Google Colab manually for now (since Kaggle GPU unavailable)\ndf_results_hcq=pd.read_csv(\"\/kaggle\/input\/results-output\/df_results_hcq.csv\")\ndf_results_hcq[['paper_id','sentence','score','span']]","d825de73":"# NOTE - we could also run, passing in a pre-compiled index for a given keyword (\"hydroxychloroquine\") in this case. \nresults=search(model=model,\n               query=\"the treatment was delivered at timing intervals \",\n               k=3, \n               gpu_index_default=gcu_index_hcq, \n               ids_dict_default=ids_dict_hcq,\n               keyword=None, df_covid_ents_default=df_covid_ents\n)","a6bef6b9":"df_results_hcq_timing, _, _ = results\ndf_results_hcq_timing[['paper_id','sentence','score','span']]","b01a8068":"# import sentence_transformer functions \n# See fine-tuning example at - https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/examples\/training_nli_bert.py\nos.chdir(\"\/kaggle\/working\/sentence-transformers\")\n\nfrom sentence_transformers import models, losses\nfrom sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\nfrom sentence_transformers.readers import *\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nfrom sentence_transformers.datasets import *\n\nfrom torch.utils.data import TensorDataset, DataLoader, SequentialSampler\nfrom torch.utils.data import DataLoader\nimport math\nimport logging\nfrom datetime import datetime","22a73a61":"# set up SentenceTransformer model, using pooled averaging. \n# start with covid_bert_base model, rather than CORD-19 task\n\nword_embedding_model = models.BERT(\"deepset\/covid_bert_base\")\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=False,\n                               pooling_mode_max_tokens=False)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","d77676f7":"# Now fine-tune model further on NLI and STS tasks to get natural language and semantic tones included in embeddings\n\n!python examples\/datasets\/get_data.py # downloads AllNLI.zip and STSbenchmark.zip datasets ","f2a50913":"os.chdir(\".\/examples\") # switch to examples directory where output is stored\nnli_reader = NLIDataReader('datasets\/AllNLI')\ntrain_data = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\nmodel_name = 'covid-bert-base'\nbatch_size = 16\nnli_reader = NLIDataReader('datasets\/AllNLI')\nsts_reader = STSDataReader('datasets\/stsbenchmark')\ntrain_num_labels = nli_reader.get_num_labels()\nmodel_save_path = 'kaggle\/working\/training_nli_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntrain_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n\nlogging.info(\"Read STSbenchmark dev dataset\")\ndev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\ndev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\nevaluator = EmbeddingSimilarityEvaluator(dev_dataloader)\n\n# Configure the training\nnum_epochs = 1\n\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs \/ batch_size * 0.1) #10% of train data for warm-up\nlogging.info(\"Warmup-steps: {}\".format(warmup_steps))\n\n# Train the model\nmodel.fit(train_objectives=[(train_dataloader, train_loss)],\n          evaluator=evaluator,\n          epochs=num_epochs,\n          evaluation_steps=1000,\n          warmup_steps=warmup_steps,\n          output_path=model_save_path\n          )","8e7e3207":"# save model \nmodel_save_name='training_nli_covid_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\nfilename=Path(\"\/kaggle\/working\/\" ) \/ model_save_name \noutfile = open(filename,'wb')\ntorch.save(model, outfile)","81389074":"#https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool\n# papers_df only got stuff from biox...\n# we need to get the full data set \n\n#https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool\n    \n#import matplotlib.pyplot as plt\n#plt.style.use('ggplot')\nimport glob\nimport json\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\n#meta_df.head()","10056eef":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","bd7d9732":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            if 'abstract' in content.keys():\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            else:\n                self.abstract.append('')\n            # Body text\n            if 'body_text' in content.keys():\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            else:\n                self.body_text.append('')\n                \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n\n            # Extend Here\n            #\n            #\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)\n\n\n","7e94ef56":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\n#df_covid.head()","3f176609":"# https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_covid.head()","617fd2d3":"df_covid1=df_covid.drop( [\"ents\"],axis=1)","d7985b7f":"# export index\nfrom pathlib import Path\nfilepath = Path(\"\/kaggle\/working\") \/\"df_covid\"\noutfile=open(filepath, \"wb\")\npickle.dump(df_covid,outfile)","950d35cb":"# Download model. Upload to datasets\nos.chdir(\"\/kaggle\/working\")\nfrom IPython.display import FileLink\nFileLink(r'df_covid')","5fb11625":"os.chdir(\"\/kaggle\/working\/sentence-transformers\")\n\nfrom sentence_transformers import SentenceTransformer\nos.chdir(\"\/kaggle\/working\/\")","d79d8c5a":"# Load model from input \nmodel_load_path='\/kaggle\/input\/bertcovidbasicnlists\/training_nli_sts_covid-bert-base-2020-04-01_00-26-48'\nmodel=torch.load(model_load_path) \n# type(model) #  model is of type \"SentenceTransformer\"","647dd0b2":"# init faiss index\nimport faiss # in case not already loaded \nd=768 # sentence transformer embedding length\nres = faiss.StandardGpuResources()  # use a single GPU\nindex = faiss.IndexIDMap(faiss.IndexFlatIP(d)) # IP is inner product. Data must be normalised first\ngpu_index = faiss.index_cpu_to_gpu(res, 0, index)","25a755c7":"# load covid_df\n# import dictionary\nfrom pathlib import Path\nfilepath = Path(\"\/kaggle\/input\/covid-docs-processed-dataframe\") \/ \"df_covid_ents\"\ninfile=open(filepath, \"rb\")\ndf_covid=pickle.load(infile)","1c949062":"df_covid.shape","075df6d3":"# embed documents and add them to faiss index \n# maintain a dictionary which stores, for each index in faiss, details of the paper (from papers_df)\n\n# index - ids, paper_id, sent_idx\n\nids_dict = collections.defaultdict(list)\n\nids_next=0\n\n#for row in range(papers_df.shape[0]):\nfor row in range(df_covid.shape[0]):\n\n    # doc=nlp(papers_df.iloc[row]['text'])\n    doc=nlp(df_covid.iloc[row]['body_text'])\n\n    # paper_id=papers_df.iloc[row]['paper_id']\n    paper_id=df_covid.iloc[row]['paper_id']\n\n    list_of_sents=[sent.text for sent in doc.sents]\n    doc_matrix=embed_sentence_list(model,list_of_sents)\n    faiss.normalize_L2(doc_matrix)\n\n    # ids in faiss index begin after last idx (last document processed, last sentence)\n    custom_ids = np.array(range(ids_next, ids_next+len(doc_matrix))) # from last postion (range add +1) to new position\n    ids_next=ids_next+len(doc_matrix) # increment by current document length. Current doc lenght = num sentences in current doc             \n    gpu_index.add_with_ids(doc_matrix, custom_ids)\n    for sent_idx, faiss_ids_val in enumerate(custom_ids): # sentence_idx is sentence id within the 1 document, faiss_ids_val is the faiss index value\n        items=(paper_id, sent_idx)\n        ids_dict[faiss_ids_val].append(items)\n","4ca1fd23":"# serialize index for output \ncpu_index=faiss.index_gpu_to_cpu(gpu_index)\nindex_ser=serialize_index(cpu_index)","bc1887fc":"# export index\nfrom pathlib import Path\nfilepath = Path(\"\/kaggle\/working\") \/\"index_faiss_file\"\noutfile=open(filepath, \"wb\")\npickle.dump(index_ser,outfile)\n\n","c85db475":"# export dictionary\nfrom pathlib import Path\nfilepath = Path(\"\/kaggle\/working\") \/\"faiss_index_ids_dict\"\noutfile=open(filepath, \"wb\")\npickle.dump(ids_dict,outfile)","acc10a51":"# Download model. Upload to datasets\nos.chdir(\"\/kaggle\/working\")\nos.listdir()\nfrom IPython.display import FileLink\nFileLink(r'index_faiss_file')\nFileLink(r'faiss_index_ids_dict')","727ca0a6":"### Part B - Create df_covid \n\nThis part is divided into Part B.1 and Part B.2\n\nPart B.1 was run on Kaggle kernel. It generates a dataframe with columns paper_id, abstract and body_text.\n\nPart B.2 was run on Google Colab, which connects to a GCP VM, with 16 cores to allow efficient multiprocessing of dataframe batches. . I included the code here to view. This part takes the dataframe from B.1, and uses scispacy to extract scientific entities, and creates a new column \"ents\" in the dataframe. ","db96f9af":"\n## Example: Simple Semantic Search \n\nLets now do a query on all the data (full index) for sentences that shed light on nurse transmission to patients in hospitals. ","5300988f":"### Second, load our processed CORD-19 dataset (see Part B)\nFormat of dataframe:\n\n- [paper_id, abstract, body_text, ents]\n\npaper_id - paper_id of the original paper\nabstract - text of the abstract\nbody_text - text of the body \nents - a list of entities extracted from the abstract (or body_text, if abstract missing) with scipy entity recognition\n\nSee Part B details. Work largely cut+paste cells from: https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering","7c264e5b":"### Part C\nPart C involves embedding the CORD-19 dataset using our fine-tuned SentenceTranformer model (from Part A), and setting up the faiss  index to store the embeddings.","f4da3730":"### Step 2 - Generate Top K Semantic Search Results\nWe can now generate the top K semantic matches. This returns the top k best sentences, with the closest semantic meaning to our query sentence. \n\nThe object \"top_k\" is returned as a list with 2 elements. The first element contains the cosine simmilarity scores of the matches, and the second element contains the ids (row number) of the match in the faiss index. \n\nIt returns one row per query. Within each row, columns are the 1..k'th best match for the query.","9fdfd5bd":"### Third, read in our pre-trained gpu index, and associated dictionary\n","90736f68":"## Test Faiss Index Search with Step by Step Query \n\n### Step 1: Create and Embed Input Query\nGenerate a test query for semantic similarity search.\n\n\nThe faiss engine does an inner product on the query embeddings against the indexed embeddings, so we must normalise the query embeddings first. (The indexed embeddings were normalised upon creation. \n","3a91cd73":"### First,** load our fine-tuned SentenceTransformer model (created in Part A below)\n","68694dbb":"* ### Step 3 Examine the Results\n\nWe can examine the scores and index of the top sentence matches, returned from the search of faiss. \nHowever, we still need to retrieve details of the original sentence, to see whether our match makes any sense. ","167686d5":"## References \n\n#### Kaggle Kernels \n\n-  https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n- https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n\n\nThese kernels provided code cells cut+pasted into this notebook. They have been to used generate a clean dataset through parsing json files. Many thanks. \n\n\n#### Spacy\n\n- https:\/\/spacy.io\/\n\nSpacy is used to perform nlp pipeline functions such as tokenization, sentence segmentation and span retrieval\n\n#### deepset\/covid-bert-base\n\n- https:\/\/huggingface.co\/deepset\/covid_bert_base\n- https:\/\/github.com\/deepset-ai\/FARM\/blob\/master\/examples\/lm_finetuning.py \n\nDeepsetAI script showing how to fine-tune BERT language model with a language modeling task. From what I can gather, they used script lm_finetuning.py in their FARM tools using the CORD-19 dataset to fine-tune the model. \n\n#### SentenceTransformers\n- https:\/\/github.com\/UKPLab\/sentence-transformers\n- https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/examples\/training_nli_bert.py\n\nAfter loading the covid-bert model, we continued to fine-tune the sentence-transformer model to perform well of natural language tasks, since we will be performing these sorts of tasks when runnning our searches on the embeddings-index. \n\n#### Faiss\n- https:\/\/gist.github.com\/mdouze\/e30e8f57a98ed841c082cc68baa14b4a\nThis provides code to serialise and deserialise the index so it can be pickled. ","b0f25f7f":"### Example - Keyword Filtered Document Set - Semantic Similarity Search \nNow, lets do a refined search. Lets limit the search to only those documents that contain \"hydroxychloroquine\" within the extracted list of \"entities\". This will give < 100 documents. We can then compile a limited faiss index from this shortlist of documents. **","6d1d6446":"### Setup and Installation  ","1912068f":"Deepset have finetuned a BERT language model on the CORD-19 dataset (see references above). \n\nhttps:\/\/huggingface.co\/deepset\/covid_bert_base\n\nWe load this model. \nThe following code is otherwise lifted from \"https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/examples\/training_nli_bert.py\" to complete the remainder of the fine-tuning. \n    ","03bf7c98":"### Part A - SentenceTransformer model fine-tuning ","72914e41":"### Step 5, Double-check cosine similiarity score manually \n\nLet's check the score returned by faiss manually. It should be the cosine between the embedded query matrix, and the result vector. \n","ff8c2984":"#### Step 4, Generate Sentence \"Context\"","51eb998b":"That's great. The sentence looks relevant. \n\nBut we might need more context to know whether that sentence is relevant to our query. We use spacy spans to grab the context, i.e. text around the sentence. ","b55f0ae4":"## Report: Semantic Search for limited documents \n\nThis section contains the report, and assumes Part A, B and C have been run. We can load pre-built objects from those sections from the input directory.\n","fb26b761":"#### Create an Embeddings Index\n\nThis Script produces an embedding index that allows a researcher to perform a semantic similarity search on the cord-19 dataset. It has a few features\n\n- You can search an index (Faiss) for sentences with semantic similarity to an input query. \n    - The sentences are constructued using sentence embeddings (SentenceTransformer library)\n    - SentenceTransformer is initialised with covid-bert-base, and then fine-tuned with NLI and STS tasks so it adds semantic components to the underlying covid based language model. \n    \n- A search filter function allows you to limit the documents compiled into the semantic index, by keyword; \n    - If the keyword appears in the documents precompiled \"list of entities\", the document is inclued in the index. \n    - Entities(e.g. compounds, treatments, protocols) are detected with scipacy,\n\nBriefly, the following steps are performed.  . \n\n- Report \n\n- Part A\n    - Step 1: Initialise SentenceTransfomer model with covid_bert_base (https:\/\/huggingface.co\/deepset\/covid_bert_base)\n    - Step 2: Fine-tune SentenceTransformer model with natural language inference (sentence entailment) and semantic similarity (sts) tasks\n \n\n - Part B \n\n    - This part is divided into Part B.1 and Part B.2\n\n    - Part B.1 is run on Kaggle kernel. It generates a dataframe with columns paper_id, abstract and body_text.\n\n    - Part B.2 was run on Google Colab, which connects to a GCP VM, with 16 cores to allow efficient multiprocessin- g of dataframe batches. . I included the code here to view. This part takes the dataframe from B.1, and uses scispacy to extract scientific entities, and creates a new column \"ents\" in the dataframe. \n\n-  Part C \n -   Create Faiss Index for each sentence within corpus \n  \n\n## TO DO\nstill missing some abstracts which means filtering should look at first seciton of body_text for ents...\n\nsearch function should be refactored, nlp(doc) should be factored out so its only performed once for each doc \n\nmemory usage is too high. Need to rethink use of garbage collector, or more memory efficient table storage\n\n\n\n","522045e6":"### End of Setup\n","ed7a514b":"### Functions to generate new index based on a subset of documents\n#### Filtered by Keyword\n\nNow, we might need to create our own index, based on a limited set of keywords. \nThis is a bit time-consuming, and ideally I would shift this to a GCP VM, with an API where it might run more quickly."}}