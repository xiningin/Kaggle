{"cell_type":{"fe2a1fba":"code","19bf112f":"code","ece83d66":"code","43870f70":"code","1cf7bf0f":"code","9bb43c34":"code","a9d71642":"code","cf5b0693":"code","78caa9a9":"code","d2c3531c":"code","0a43a88d":"code","f25f40fa":"code","91ddf6eb":"code","80dd6591":"code","bb02611f":"code","7b193868":"code","b3f79bbb":"code","dfb6aa78":"code","bed8c62f":"code","092a1379":"code","1b4fb0f6":"code","632189b4":"code","7e209bb4":"code","c208c994":"code","270461f4":"code","b0cb062b":"code","dccd96af":"code","65af3079":"code","de14df65":"code","e58f5874":"code","9b5ae4fb":"markdown","47015827":"markdown","fb400898":"markdown","736c7b47":"markdown","c8de6669":"markdown","ae4790da":"markdown","046467a2":"markdown","d26a7a4b":"markdown","a51d2f51":"markdown","89ba9a2d":"markdown","3b3d3ebf":"markdown"},"source":{"fe2a1fba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt \nimport seaborn as sns\nsns.set(color_codes = True)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","19bf112f":"from sklearn.linear_model import LinearRegression, Ridge\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom sklearn.pipeline import make_pipeline","ece83d66":"# initialise some random values. \n\nX  = np.random.rand(15,5)\nY = np.random.rand(15,1)\nTheta = np.zeros((X.shape[1],1))\n\n# We dont need to standarise these values as they np.random.rand intitialises uniform distribution between 0 & 1. ","43870f70":"from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n X, Y, test_size=0.20, random_state=42)","1cf7bf0f":"reg_1 =LinearRegression().fit(X_train,y_train)","9bb43c34":"#Stats for scklearn LR.\n\n#high is better\nprint(reg_1.score(X_train,y_train))\n#Low is better\nprint(mean_absolute_error(reg_1.predict(X_test),y_test))","a9d71642":"#Our model:\n\nalpha = 0.03\nm = len(X_train)\nJ = []\nfor i in range(1,15000):\n    hx  = np.dot(X_train,Theta)                          # m,1  \n    error = hx - y_train                                 # m,1\n    grad = np.dot(error.T,X_train)                       # 1,m * m,n = 1,n\n    Theta = Theta - ((alpha\/m) * grad.T)\n    J.append(1\/(2*m) * (sum((error)**2)))          # 1,1   \n    ","cf5b0693":"v = sum(((y_train - np.mean(y_train))**2))\nu = sum((error)**2)\n\nprint(f'this is our model score {1 - u\/v}')\n\n# predict cost on test set\nm = len(X_test)\nhx  = np.dot(X_test,Theta)                          # m,1  \nprint(f'this is our mae {mean_absolute_error(hx,y_test)}') ","78caa9a9":"X = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")","d2c3531c":"X.describe()","0a43a88d":"X = X.drop_duplicates()\nX.describe()","f25f40fa":"X.plot(kind='box', subplots=True, layout=(2,2),\nsharex=False, sharey=False, figsize=(10,10))\nplt.show()\n\n","91ddf6eb":"X.hist(figsize=(10,10))","80dd6591":"sns.pairplot(X)","bb02611f":"# somkers are charged more\n\nX[X.smoker==\"yes\"].hist(figsize=(7,7))\nX[X.smoker==\"no\"].hist(figsize=(7,7))","7b193868":"#smoker has strong correlation\nplt.figure(figsize=(10,10))\nc = pd.get_dummies(X).corr()\nsns.heatmap(c,cmap=\"BrBG\",annot=True)","b3f79bbb":"X = pd.get_dummies(X,drop_first=True)\nY = X.charges\nX = X.drop([\"charges\"],1)","dfb6aa78":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nX_scaled = scaler.fit_transform(X)\nX = pd.DataFrame(X_scaled, columns = X.columns)\n\n","bed8c62f":"X_train, X_test, y_train, y_test = train_test_split(\n X, Y, test_size=0.2, random_state=42)","092a1379":"from sklearn.linear_model import Ridge\nmodel = Ridge()\nmodel.fit(X_train, y_train)\nimportance = model.coef_\nfeat_importances = pd.Series(model.coef_, index=X.columns)\nfeat_importances.plot(kind='barh')\nplt.show()","1b4fb0f6":"feat_importances","632189b4":"#apply SelectKBest class to extract top 10 best features\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nbestfeatures = SelectKBest(score_func=f_regression, k=6)\nfit = bestfeatures.fit(X_train,y_train)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(8,'Score'))  #print 10 best features","7e209bb4":"## check using p.values\n\nfrom sklearn import datasets, linear_model\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom scipy import stats\n\nX2 = sm.add_constant(X_train)\n\nest = sm.OLS(y_train.values, X2.values)\nest2 = est.fit()\nprint(est2.summary())\n","c208c994":"X2","270461f4":"relevent_features = ['age','bmi','children','smoker_yes']\n\nX_train = X_train[relevent_features]\nX_test = X_test[relevent_features]","b0cb062b":"model_score = []\nR2_score = []\nmae = []\nfor i in range(1,8):\n    polyreg=make_pipeline(PolynomialFeatures(i,include_bias=True),LinearRegression())\n    polyreg.fit(X_train,y_train)\n    model_score.append(polyreg.score(X_train,y_train))\n    R2_score.append(r2_score(polyreg.predict(X_test),y_test))\n    mae.append(mean_absolute_error(polyreg.predict(X_test),y_test))\n    \n    ","dccd96af":"print(f'this is the r2_score of test set: {R2_score}')\nprint(f'This is mean absolute error of the test set: {mae}')\nprint(f'this is training model score: {model_score}')","65af3079":"\n\nfrom sklearn.model_selection import validation_curve, learning_curve\n\ndef draw_learning_curve(model, x, y):\n    train_sizes,train_scores, test_scores = learning_curve(model, x, y,train_sizes=[50, 100, 300, 500, 700, 800,900], cv=10)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    \n    plt.plot(train_sizes, train_scores_mean, color='blue', label='Train score')\n    plt.plot(train_sizes, test_scores_mean, color='red', label='Cross-validation score')\n    \n    plt.legend(loc='best')\n    plt.xlabel('Training size')\n    plt.ylabel('score')\n\n","de14df65":"for i in range(1,5):\n    polyreg=make_pipeline(PolynomialFeatures(i,include_bias=True),LinearRegression())\n    draw_learning_curve(polyreg,X_train, y_train)\n    plt.title(f\"Learning curve for {i}-degree poly Regressor\")\n    plt.show()","e58f5874":"for i in range(1,8):\n\n    polyreg=make_pipeline(PolynomialFeatures(i,include_bias=True),LinearRegression())\n    polyreg.fit(X_train,y_train)\n    y_pred_pr = polyreg.predict(X_test)\n\n    predTest = pd.DataFrame({\"prediction\": y_pred_pr, \"observed\": y_test})\n    plt.scatter(predTest['prediction'], predTest['observed'])\n    plt.title(\"Polynomial Regressor: Prediction Vs Actual Data\")\n    plt.xlabel(\"Predicted Medical Charges\") \n    plt.ylabel(\"Observed Medical Charges\")\n    plt.show()\n","9b5ae4fb":"## Upvote if you like !! Cheers!!","47015827":"## Lets Implement Multivariate Linear regression on insurance data-set","fb400898":"# conclusion\n\n1. Scklearn gives slightly better results. \n2. It gives more stable results.\n3. Is faster than  becasue our model uses loop for gradient descent. I suspect, scklearn is using normal equation for weight updates. You can check the source code for conformation. \n4. Hence proven that using scklearn linear-regression is a better choice then your own implementaion unless you get better results in the test phase. ","736c7b47":"# Lets Build the model;","c8de6669":"Conclusion:\n\n1. Its clear as the degree of polynomial increases our train model overfits --> high varience. Then performs worse on test set. \n2. Quadratic polynomial linear regression fits our data best. As R2  is highest and MAE is also econd highest for the respective test sets. ","ae4790da":"### This confirms that, only four features are important, rest we will drop so that our model does not overfit. ","046467a2":"## Now, you can visualise and see it yourself which is the best n-degree polynomial function. ","d26a7a4b":"## Seems sex and region are not directly correlated with charges. Lets see what are their coeficients inorder to check how important they are.  ","a51d2f51":"### The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant. \n\n### 1. This implies features with Coeffiecients near 0. Are not that important while fitting our model and can be avoided, inorder to keep our hx(theta) general and not let it overfit features which dont compensate much.","89ba9a2d":"### Lets first check how does our simple multivaraiate linear regression performs in comparision to scikitlearn linear regression. ","3b3d3ebf":"### Feature Selection"}}