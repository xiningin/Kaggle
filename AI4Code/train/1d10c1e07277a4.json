{"cell_type":{"abfd29cd":"code","4bb73491":"code","337f427c":"code","2b4638a4":"code","7e560850":"code","9a6717ff":"code","b929632d":"code","d1e0812b":"code","548b17bc":"code","f87c779e":"code","dcb0977f":"code","52bb2171":"code","573c7848":"code","b1d7e9f5":"code","3e76ee87":"code","06b89aa9":"code","d450e123":"code","143d8657":"code","42f1b3f5":"code","9ed628da":"code","f18e15e3":"code","6057057c":"code","e870dc24":"code","6fe8fadc":"code","bc5d9cd2":"code","68f5d59b":"code","2a01e471":"code","22a7a132":"code","38c87998":"code","a5349aa1":"code","76bb0aa5":"code","5c88c82d":"code","52cb585c":"code","e57d0c9a":"code","64a59b4c":"code","8ced5a5f":"code","f235bbe3":"code","946bdf53":"code","5f6c78eb":"code","12d68b30":"code","62d77d17":"code","2e3f6395":"code","c764c005":"code","997461a4":"code","5578dee7":"code","3ae10ba2":"code","e7ae44ae":"code","6157267f":"code","33ac898c":"code","15d97f18":"code","06b46971":"code","4ddc7c52":"code","8cab0c46":"code","cfaa61a4":"code","c0daab6a":"code","c74f3cd1":"code","c61361c0":"code","f2cdbcf8":"code","7565f4a6":"code","5789f31f":"code","824b331a":"code","da363f40":"code","21dd0c16":"code","5752a47b":"code","7ea4c7bb":"code","6fcf3a19":"code","2ee053b1":"code","ada1026f":"code","f1c64c3b":"markdown","76fdc5cd":"markdown","6cedc220":"markdown","bb1bee98":"markdown","4606939e":"markdown","b5fd4080":"markdown","8402d054":"markdown","649c4fb5":"markdown","4f204e4f":"markdown","b899c75b":"markdown","e9138321":"markdown","85d7d0eb":"markdown","a5891ba2":"markdown","0f31bee5":"markdown","f3b669f8":"markdown","b057c5f3":"markdown","38d9bdd9":"markdown","676b6c7b":"markdown","ffe64983":"markdown","71cf909b":"markdown","38683727":"markdown","a04b4965":"markdown","b0febc88":"markdown","05dd75d4":"markdown","772f99cb":"markdown","e9f71b6d":"markdown","2af395d2":"markdown","243c1029":"markdown","c8f8b1b4":"markdown","de2ec554":"markdown","762e15f0":"markdown","f7ccf89b":"markdown","334d5c6d":"markdown","c2e44402":"markdown","2d988fee":"markdown","09868a66":"markdown","360f8950":"markdown","55ded253":"markdown","15ffe4c2":"markdown"},"source":{"abfd29cd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler","4bb73491":"import warnings\nwarnings.filterwarnings('ignore')","337f427c":"ls ..\/input","2b4638a4":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","7e560850":"train.head(5)","9a6717ff":"test.head(5)","b929632d":"train.shape, test.shape","d1e0812b":"train.columns","548b17bc":"# train.info()","f87c779e":"train['SalePrice'].describe()","dcb0977f":"corr_matrix = train.corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(corr_matrix, cmap=plt.cm.RdBu_r)","52bb2171":"k = 10\ncols = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\nk_corr_matrix = train[cols].corr()\nplt.figure(figsize=(12, 8))\nsns.heatmap(k_corr_matrix, annot=True, cmap=plt.cm.RdBu_r)","573c7848":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], height=2)","b1d7e9f5":"plt.figure(figsize=(16, 30))\nfor idx, f in enumerate(['OverallQual', 'GrLivArea', 'GarageArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', \n                         'FullBath', 'TotRmsAbvGrd','YearBuilt']):\n    plt.subplot(9, 2, 2*idx+1)\n    sns.distplot(train[f])\n    plt.subplot(9, 2, 2*idx+2)\n    sns.scatterplot(x=f, y='SalePrice', data=train)","3e76ee87":"plt.figure(figsize=(8, 12))\ntrain.corr()['SalePrice'].sort_values().plot(kind='barh')","06b89aa9":"# distribution histogram for ourt target: SalePrice\nsns.distplot(train['SalePrice'], fit=norm)\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n\n# normal probability plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","d450e123":"#skewness and kurtosis\nprint('mu: %.2f, sigma: %.2f' % (mu, sigma))\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","143d8657":"# applying log transformation\ntrain['SalePriceLog'] = np.log1p(train['SalePrice'])","42f1b3f5":"# distribution histogram and normal probability plot\n(mu, sigma) = norm.fit(train['SalePriceLog'])\n\nsns.distplot(train['SalePriceLog'], fit=norm)\nplt.legend(['Normal dist ($\\mu=${:.2f}, $\\sigma=${:.2f})'.format(mu, sigma)])\n\nfig = plt.figure()\nstats.probplot(train['SalePriceLog'], plot=plt)\nplt.show()","9ed628da":"#skewness and kurtosis\nprint('mu: %.2f, sigma: %.2f' % (mu, sigma))\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","f18e15e3":"sale_price_scaled = StandardScaler().fit_transform(train['SalePrice'][:, np.newaxis])\n\nsns.distplot(sale_price_scaled, fit=norm)\n\nlow_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[:10]]\nhigh_range = sale_price_scaled[sale_price_scaled[:, 0].argsort()[-10:]]\nprint(f'outer range (low) of the distribution: \\n{low_range}')\nprint(f'outer range (high) of the distribution: \\n{high_range}')","6057057c":"train.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000), xlim=(0, 6000))","e870dc24":"train = train[train['GrLivArea'] < 4000]","6fe8fadc":"train.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000), xlim=(0, 6000))","bc5d9cd2":"quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nquantitative.sort()\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']\nqualitative.sort()","68f5d59b":"# continuous variable\nquantitative","2a01e471":"# categorical variable\nqualitative","22a7a132":"train.reset_index(drop=True, inplace=True)\ny_train = train['SalePriceLog']\nX_train = train.drop(['SalePrice', 'SalePriceLog'], axis=1)\nX_test = test","38c87998":"all_data = pd.concat([X_train, test], axis=0, sort=False)\nall_data.drop(['Id'], axis=1, inplace=True)\nall_data.shape","a5349aa1":"all_data.head(5)","76bb0aa5":"na_total = all_data.isnull().sum().sort_values(ascending=False)\nna_ratio = (all_data.isnull().sum() \/ all_data.shape[0]).sort_values(ascending=False)\nmissing_data = pd.concat([na_total, na_ratio], axis=1, keys=['Total', 'Ratio'])\nmissing_data.head(50)","5c88c82d":"# Most value of these 4 features are missing and they have no pattern , just delete them\nplt.figure(figsize=(16, 12))\nfor idx, f in enumerate(['PoolQC', 'Utilities', 'Street', 'MiscFeature']):\n    plt.subplot(2, 2, idx+1)\n    sns.scatterplot(x='SalePrice', y=f, data=train)\n\nall_data.drop(['PoolQC', 'Utilities', 'Street', 'MiscFeature', ], axis=1, inplace=True)","52cb585c":"all_data['Alley'].fillna('None', inplace=True)\nall_data['Fence'].fillna('None', inplace=True)\nall_data['FireplaceQu'].fillna('None', inplace=True)\n\nall_data['GarageQual'].fillna('None', inplace=True)\nall_data['GarageFinish'].fillna('None', inplace=True)\nall_data['GarageCond'].fillna('None', inplace=True)\nall_data['GarageType'].fillna('None', inplace=True)\n\nall_data['BsmtExposure'].fillna('None', inplace=True)\nall_data['BsmtCond'].fillna('None', inplace=True)\nall_data['BsmtQual'].fillna('None', inplace=True)\nall_data['BsmtFinType2'].fillna('None', inplace=True)\nall_data['BsmtFinType1'].fillna('None', inplace=True)","e57d0c9a":"all_data['MasVnrType'].fillna('None', inplace=True)\nall_data['HasMasVnr'] = all_data['MasVnrType'].apply(lambda x: 0 if x == 'None' else 1)\n\nall_data['MSZoning'] = all_data.groupby(['MSSubClass'])['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nall_data['Functional'].fillna('Typ', inplace=True)\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","64a59b4c":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","8ced5a5f":"all_data['GarageYrBlt'] = (all_data['YearBuilt'] + all_data['YearRemodAdd']) \/2","f235bbe3":"sns.scatterplot(x='SalePrice', y='MasVnrArea',hue='MasVnrType', data=train, legend=None)\nall_data['MasVnrArea'] = all_data.groupby(['MasVnrType'])['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","946bdf53":"print(all_data[all_data['GarageCars'].isnull()][['GarageArea', 'GarageCars', 'GarageType', 'GarageYrBlt', 'GarageQual']])\nall_data['GarageArea'].fillna(0, inplace=True)\nall_data['GarageCars'].fillna(0, inplace=True)","5f6c78eb":"print(all_data[all_data['TotalBsmtSF'].isnull()][\n    ['TotalBsmtSF', 'BsmtQual', 'BsmtCond', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFinSF1', 'BsmtFullBath','BsmtHalfBath']])\nall_data['TotalBsmtSF'].fillna(0, inplace=True)\nall_data['BsmtUnfSF'].fillna(0, inplace=True)\nall_data['BsmtFinSF1'].fillna(0, inplace=True)\nall_data['BsmtFinSF2'].fillna(0, inplace=True)\nall_data['BsmtFullBath'].fillna(0, inplace=True)\nall_data['BsmtHalfBath'].fillna(0, inplace=True)","12d68b30":"# sns scatter plot might be very usefull to see the data distribution with different categories\n# sns.scatterplot(x='SalePrice', y='MasVnrArea',hue='MasVnrType', data=train, legend=None)","62d77d17":"all_data['YrBltAndRemod']=all_data['YearBuilt']+all_data['YearRemodAdd']\nall_data['TotalSF']=all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\nall_data['TotalSqrFootage'] = (all_data['BsmtFinSF1'] + all_data['BsmtFinSF2'] +\n                                 all_data['1stFlrSF'] + all_data['2ndFlrSF'])\n\nall_data['TotalBathrooms'] = (all_data['FullBath'] + (0.5 * all_data['HalfBath']) +\n                               all_data['BsmtFullBath'] + (0.5 * all_data['BsmtHalfBath']))\n\nall_data['TotalPorchSF'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] +\n                              all_data['EnclosedPorch'] + all_data['ScreenPorch'] +\n                              all_data['WoodDeckSF'])","2e3f6395":"all_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","c764c005":"# Some of the non-numeric predictors are stored as numbers; we convert them into strings \nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","997461a4":"all_data = pd.get_dummies(all_data).reset_index(drop=True)","5578dee7":"all_data.isnull().sum().sort_values(ascending=False)","3ae10ba2":"from sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom mlxtend.regressor import StackingCVRegressor","e7ae44ae":"X_train = all_data.iloc[:len(y_train), :]\nX_test = all_data.iloc[len(y_train):, :]","6157267f":"X_train.shape, y_train.shape, X_test.shape","33ac898c":"def rmse_cv(model):\n    mse = cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv=5)\n    rmse = np.sqrt(-mse)\n    print(f'{model.__class__.__name__} score: {rmse.mean():.4f}, {rmse.std():.4f}')\n    #return(rmse)","15d97f18":"lasso = Lasso()\nlasso_search = GridSearchCV(lasso, {'alpha': np.logspace(-4, -3, 5)}, cv=5, scoring=\"neg_mean_squared_error\")\nlasso_search.fit(X_train, y_train)\nlasso_search.best_estimator_","06b46971":"lasso_model = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nrmse_cv(lasso_model)","4ddc7c52":"ridge = Ridge()\nridge_search = GridSearchCV(ridge, {'alpha': np.linspace(10, 30, 10)}, cv=5, scoring=\"neg_mean_squared_error\")\nridge_search.fit(X_train, y_train)\nridge_search.best_estimator_","8cab0c46":"ridge_model = make_pipeline(RobustScaler(), Ridge(alpha=19))\nrmse_cv(ridge_model)","cfaa61a4":"enet = ElasticNet()\nenet_search = GridSearchCV(enet, {'alpha': np.linspace(0.0001, 0.001, 10), 'l1_ratio':np.linspace(0.5, 1.5, 10)}, cv=5, scoring=\"neg_mean_squared_error\")\nenet_search.fit(X_train, y_train)\nenet_search.best_estimator_","c0daab6a":"enet_model = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0004, l1_ratio=1.4, random_state=3))\nrmse_cv(enet_model)","c74f3cd1":"gbdt_model = GradientBoostingRegressor(learning_rate=0.05, min_samples_leaf=5, min_samples_split=10, max_depth=4, n_estimators=3000)\nrmse_cv(gbdt_model)","c61361c0":"rf_model = RandomForestRegressor(min_samples_leaf=4, min_samples_split=8)\nrmse_cv(rf_model)","f2cdbcf8":"svr_model = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.005, gamma=0.0003))\nrmse_cv(svr_model)","7565f4a6":"xgb_model = XGBRegressor(learning_rate=0.01, max_depth=5, n_estimators=3000, \n                         n_thread=-1, n_jobs=-1, objective='reg:squarederror')\nrmse_cv(xgb_model)","5789f31f":"lgb_model = LGBMRegressor(objective='regression',\n                    learning_rate=0.01, max_depth=5, num_leaves=4, \n                    n_estimators=3000)\nrmse_cv(lgb_model)","824b331a":"stack_model = StackingCVRegressor([lasso_model, ridge_model, enet_model, gbdt_model, rf_model, svr_model, xgb_model, lgb_model], \n                                  meta_regressor=lgb_model,\n                                  use_features_in_secondary=True)\n# rmse_cv(stack_model)","da363f40":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","21dd0c16":"lasso_model = lasso_model.fit(X_train, y_train)\nridge_model = ridge_model.fit(X_train, y_train)\nenet_model = enet_model.fit(X_train, y_train)\ngbdt_model = gbdt_model.fit(X_train, y_train)\nrf_model = rf_model.fit(X_train, y_train)\nsvr_model = svr_model.fit(X_train, y_train)\nxgb_model = xgb_model.fit(X_train, y_train)\nlgb_model = lgb_model.fit(X_train, y_train)\nstack_model = stack_model.fit(np.array(X_train), np.array(y_train))","5752a47b":"def combine_models_predict(X):\n    return ((0.1 * enet_model.predict(X)) + \\\n            (0.1 * ridge_model.predict(X)) + \\\n            (0.1 * lasso_model.predict(X)) + \\\n            (0.15 * gbdt_model.predict(X)) + \\\n            (0.15 * xgb_model.predict(X)) + \\\n            (0.1 * lgb_model.predict(X)) + \\\n            (0.075 * rf_model.predict(X)) + \\\n            (0.075 * svr_model.predict(X)) + \\\n            (0.15 * stack_model.predict(np.array(X)))\n           )","7ea4c7bb":"print('RMSLE score on train data:')\nprint(rmsle(y_train, combine_models_predict(X_train)))","6fcf3a19":"log_result = combine_models_predict(X_test)\nresult = np.expm1(log_result)","2ee053b1":"sub = pd.DataFrame()\nsub['Id'] = test['Id']\nsub['SalePrice'] = result\nsub.head()","ada1026f":"sub.to_csv('submission.csv',index=False)","f1c64c3b":"What we can find from the `Pairplot` figure above about variables relationship?\n\n1. `TotalBsmtSF` and `GrLivArea`: dots drawing a linear line like a upperbound, means that most of the basement area is not bigger than above ground living area, this make sense. It is a house, not a bunker\n2. `GrLivArea ~ SalePrice` and `TotalBsmtSF ~ SalePrice` is linear related, those 2 continuous variable is both about **area square feet** and they are highly related to house `SalePrice`\n3. `YearBuilt ~ SalePrice` dot clouds appears to be like an exponential function\n\nAnd the categorical varibles `OverallQual`, `GarageCars`, `FullBath` is also positive correlated to `SalePrice`.","76fdc5cd":"The most correlated features with `SalePrice`:\n\n- OverallQual: **Rates** the overall material and finish of the house\n- GrLivArea: Above grade (ground) living **area square feet**\n- GarageCars and GarageArea: the most strongly correlated features, twin brothers of **Garage**\n- TotalBsmtSf and 1stFlrSF: **square feet** of basement and 1st floor\n- FullBath: Full **bathrooms** above grade, for urgency of ...\n- TotRmsAbvGrd: **Total rooms** above grade (does not include bathrooms)\n- YearBuilt: Original construction date decides the **years** of house\n\nRates, area square feet, Garage, bathrooms, total rooms, years, etc. These features is highly related to house price when taken into our real life, the correlation heatmap is really make sence.","6cedc220":"The left skew is corrected and the data appears more normally distributed.","bb1bee98":"There are 2 highlighted red block get to my first sight. (Red means high correlation here)\n\n- TotalBsmtSF and 1stFlrSF: the square feet of basement(TotalBsmtSF) area is probably very similiar to the 1st floor(1stFlrSF)\n- GarageCars and GarageArea: the number of cars(GarageCars) in Garage is decided by its size(GarageArea)\n\nI think we don't care about the most negative correlated features in the highlighted blue blocks here. In current topic, the target is **Linear Regression**, pick out the most significantly related variables is more important.","4606939e":"## Submission","b5fd4080":"- Low range is within 2 standard deviations ($-2\\sigma$)\n- High range like the 7.x values are really out of range\n\nAt least, the 2 points with value greater than 7 should be considered as an outlier.","8402d054":"The `SalePrice` is not rightly normal. Shows deflecting to left, positive \"skewness\", and not follow the diagonal line.\n\n**Statistics: in case of positive skewness, log transformations works well**","649c4fb5":"## Outliers","4f204e4f":"## Read Data","b899c75b":"Merge mutiple related or same kind of categorical features to creat a new one","e9138321":"> #### SVR","85d7d0eb":"> #### XGBoost","a5891ba2":"### Scatter Plot","0f31bee5":"### Concat all data\n\nConcatenate the train and test data to analyze","f3b669f8":"> #### Light GBM","b057c5f3":"Boston House Price is a very good dataset to analyze Regression problem.\n\nStart from the EXPLORATION of Multivariate Data Analysis:\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\nAnd refer to the wonderful solution:\n\nhttps:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1\n\nNow we begin our Exploratory Data Analysis","38d9bdd9":"According to the `data_description`, value NA means \"None\" for these `categorical features`, Fiil NA with **None** for them","676b6c7b":"> #### ElasticNet, hybrid of Lasso and Ridge","ffe64983":"> #### Ridge","71cf909b":"### Pairplot\n\nAnother wonderful tool in @seaborn","38683727":"## Prediction","a04b4965":"### SalePrice\n\nAnalyze the target variable first. Does it a normal distribution?","b0febc88":"### Correlation Matrix of 'SalePrice'\n\nReduce the Scope of Heatmap","05dd75d4":"### Missing Data","772f99cb":"### Pearson Correlation Matrix of Features\n\nHeatmap of correlation is the best way to overview the data features.","e9f71b6d":"Simplified features","2af395d2":"> #### Gradient Boost Regressor","243c1029":"### GridSearchCV to tune Model parameters\n\nUse `GridSearchCV` to find the best parameters","c8f8b1b4":"> #### Lasso","de2ec554":"> #### Random Forest","762e15f0":"### Define a cross validation strategy\n\nUse `cross_val_score` to get the **root mean square error**, which is the score method for current regression problem\n","f7ccf89b":"## Preprocessing","334d5c6d":"## Feature Engineering","c2e44402":"For the `categorical features` without what NA means, fill the NA with the **mode**, \nwhich means most categorical type of the feature","2d988fee":"The 2 points in the bottom right are outside of the crowd and definetly outliers.\n\nThe 2 points in the upper right greater than 4000 in x-axis could also be considered as outliers.","09868a66":"### Quantitative and Qualitative","360f8950":"### SalePrice Distribution\n\nStandardize the data and see if there're any outlier points. \n\nStandardization means converting data values to be with mean of 0 and standard deviation of 1 ($x \\sim \\mathcal{N}(0, 1)$). \n\n$$x=\\frac{x-\\mu}{\\sigma}$$","55ded253":"## Models","15ffe4c2":"> #### Stacked Models"}}