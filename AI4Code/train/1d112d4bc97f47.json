{"cell_type":{"59fdea1a":"code","7dd0bf01":"code","98e6bb9b":"code","07da8573":"code","13907f26":"code","42a983ca":"code","a2c5529f":"code","f6319722":"code","3b3e7af0":"code","752674c7":"code","19b47c14":"code","3732b298":"code","0bc0cecf":"code","15b486e9":"code","d10dcbe2":"code","dd4b08a9":"code","a170d734":"code","93424223":"code","cd41bc0c":"code","234555ff":"code","e28c0abe":"code","22b8a471":"code","93fa3f13":"code","335779f6":"code","98788f7c":"code","b125b22f":"code","2c25adf1":"code","36e2a068":"code","40bac821":"code","30341b0d":"code","9e4da7cd":"code","c3500b4e":"code","9373d850":"code","a6256f26":"code","974ff041":"code","7a49759d":"code","5029e36e":"code","09b4099f":"code","9e94b300":"code","e0941766":"code","8417745f":"code","5ebfc061":"code","ecfb811e":"code","08970a7f":"code","32202e90":"code","bc1493d9":"code","a456dd8e":"code","7ad7a7ad":"code","c521cd33":"code","edfb1842":"code","ff1aa76a":"code","3ebef8db":"code","e7a68d2e":"code","e7d314ae":"code","c040b922":"code","2cbc7ee0":"code","67537ea4":"code","ab08fd8e":"code","305cd64b":"code","ef972b64":"code","64db7303":"code","36994085":"code","0235f258":"code","89ce58a8":"code","30830643":"code","dbb7f179":"markdown","03fd12b4":"markdown","288d4cae":"markdown","00a01f98":"markdown","a40eec8f":"markdown","6b253a21":"markdown","11354b6a":"markdown","e114ec24":"markdown"},"source":{"59fdea1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\nfrom imblearn.metrics import sensitivity_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import make_classification\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score, ShuffleSplit, cross_validate\n\nfrom collections import Counter\nplt.style.use(\"seaborn-muted\")\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7dd0bf01":"#upload dataset\ndf = pd.read_csv(\"\/kaggle\/input\/ckdisease\/kidney_disease.csv\") ","98e6bb9b":"#info about dataset\ndf.info() ","07da8573":"#first five rows of dataset\ndf.head(10) ","13907f26":"#drop id column\ndf.drop([\"id\"],axis=1,inplace=True) ","42a983ca":"#convert to numeric data type\ndf.pcv = pd.to_numeric(df.pcv, errors='coerce')\ndf.wc = pd.to_numeric(df.wc, errors='coerce')\ndf.rc = pd.to_numeric(df.rc, errors='coerce')","a2c5529f":"#statistical information of the features used in the data set\ndf.describe()","f6319722":"#correlation between the features used in the data set\ndf.corr()","3b3e7af0":"#correlation map\nf,ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(df.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)\nplt.show()","752674c7":"#detect outliers\ndef detect_outliers(df,features):\n    outlier_indices=[]\n    \n    for c in features:\n        Q1=np.percentile(df[c],25) #1st quartile\n        Q3=np.percentile(df[c],75) #3rd quartile\n        IQR=Q3-Q1                  #IQR\n        outlier_step=IQR*1.5       #Outlier step\n        outlier_list_col=df[(df[c]<Q1-outlier_step) | (df[c]>Q3 + outlier_step)].index #Detect outlier and their indeces\n        outlier_indices.extend(outlier_list_col) #Store indeces\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers=list(i for i,v in outlier_indices.items() if v>2)\n    \n    return multiple_outliers","19b47c14":"#check if I have outliers\ndf.loc[detect_outliers(df,[\"age\",\"bp\",\"sg\",\"al\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\"])]","3732b298":"#number of missing values in features\ndf.isnull().sum()","0bc0cecf":"#another way to show missing data\n\n#sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n#plt.grid()\n#plt.title(\"Number of Missing Values\")","15b486e9":"#show missing data\nimport missingno as msno\n\nmsno.matrix(df)\nplt.show()","d10dcbe2":"#show missing data\nmsno.bar(df)\nplt.show()","dd4b08a9":"#how missing data in age \ndf[df[\"age\"].isnull()]","a170d734":"#fill missing data with mean value\ndf[\"bgr\"]= df[\"bgr\"].fillna(np.mean(df[\"bgr\"]))\ndf[\"bu\"]= df[\"bu\"].fillna(np.mean(df[\"bu\"]))\ndf[\"sc\"]= df[\"sc\"].fillna(np.mean(df[\"sc\"]))\ndf[\"sod\"]= df[\"sod\"].fillna(np.mean(df[\"sod\"]))\ndf[\"pot\"]= df[\"pot\"].fillna(np.mean(df[\"pot\"]))\ndf[\"hemo\"]= df[\"hemo\"].fillna(np.mean(df[\"hemo\"]))\ndf[\"pcv\"]= df[\"pcv\"].fillna(np.mean(df[\"pcv\"]))\ndf[\"wc\"]= df[\"wc\"].fillna(np.mean(df[\"wc\"]))\ndf[\"rc\"]= df[\"rc\"].fillna(np.mean(df[\"rc\"]))","93424223":"#The number \"1\" is indicated by \"ckd\" (the condition of kidney disease) and the number \n#\"0\" is indicated by \"notckd\" (the state of the absence of kidney disease).\ndf[\"classification\"] = [1 if i == \"ckd\" else 0 for i in df[\"classification\"]]","cd41bc0c":"sns.countplot(df.classification)\nplt.xlabel('Chronic Kidney Disease')\nplt.title(\"Classification\",fontsize=15)\nplt.show()","234555ff":"#sns.pairplot(df)\n#plt.show()","e28c0abe":"#blood pressure-frequency graph\nsns.factorplot(data=df, x='bp', kind= 'count',size=6,aspect=2)","22b8a471":"#density-frequency graph\nsns.factorplot(data=df, x='sg', kind= 'count',size=6,aspect=2)","93fa3f13":"#albumin-frequency graph\nsns.factorplot(data=df, x='al', kind= 'count',size=6,aspect=2)","335779f6":"#sugar-frequency graph\nsns.factorplot(data=df, x='su', kind= 'count',size=6,aspect=2)","98788f7c":"df['dm'] = df['dm'].replace(to_replace={'\\tno':'no','\\tyes':'yes',' yes':'yes'})\ndf['cad'] = df['cad'].replace(to_replace='\\tno',value='no')","b125b22f":"#Check the bar graph of categorical data using factorplot\nsns.factorplot(data=df, x='rbc', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='pc', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='pcc', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='ba', kind= 'count',size=4,aspect=2)\n\nsns.factorplot(data=df, x='pcv', kind= 'count',size=6,aspect=2)\nsns.factorplot(data=df, x='wc', kind= 'count',size=10,aspect=2)\nsns.factorplot(data=df, x='rc', kind= 'count',size=6,aspect=2)\n\nsns.factorplot(data=df, x='htn', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='dm', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='cad', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='appet', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='pe', kind= 'count',size=4,aspect=2)\nsns.factorplot(data=df, x='ane', kind= 'count',size=4,aspect=2)","2c25adf1":"def hist_plot(variable):\n    plt.figure(figsize=(9,3))\n    plt.hist(df[variable],bins=50)\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"Age Distribution with Histogram\")\n    plt.show()","36e2a068":"numericVar = [\"age\"]\nfor n in numericVar:\n    hist_plot(n)","40bac821":"plt.figure(figsize=(70,25))\nplt.legend(loc='upper left')\ng = sns.countplot(data = df, x = 'age', hue = 'classification')\ng.legend(title = 'Kidney Disease', loc='upper left', bbox_to_anchor=(0.1, 0.5), ncol=1)\ng.tick_params(labelsize=20)\nplt.setp(g.get_legend().get_texts(), fontsize='32')\nplt.setp(g.get_legend().get_title(), fontsize='42')\ng.axes.set_title('Graph of the number of patients with chronic kidney disease by age',fontsize=50)\ng.set_xlabel('Age',fontsize=40)\ng.set_ylabel(\"Count\",fontsize=40)","30341b0d":"g = sns.FacetGrid(df,col=\"classification\")\ng.map(sns.distplot,\"age\", bins=25)\nplt.show()","9e4da7cd":"sns.factorplot(x=\"classification\",y=\"age\",data=df,kind=\"box\")\nplt.show()","c3500b4e":"age_corr = ['age', 'classification']\nage_corr1 = df[age_corr]\nage_corr_y = age_corr1[age_corr1['classification'] == 1].groupby(['age']).size().reset_index(name = 'count')\nage_corr_y.corr()","9373d850":"sns.regplot(data = age_corr_y, x = 'age', y = 'count').set_title(\"Correlation graph for Age vs chronic kidney disease patient\")","a6256f26":"age_corr_n = age_corr1[age_corr1['classification'] == 0].groupby(['age']).size().reset_index(name = 'count')\nage_corr_n.corr()","974ff041":"sns.regplot(data = age_corr_n, x = 'age', y = 'count').set_title(\"Correlation graph for Age vs healthy patient\")","7a49759d":"df2 = df.loc[:,[\"bp\",\"bgr\",\"sod\",\"pot\",\"pcv\"]]\ndf2.plot()","5029e36e":"df2.plot(subplots = True)\nplt.show()","09b4099f":"g = sns.jointplot(\"age\", \"classification\", data=df, size=7,ratio=3, color=\"r\")","9e94b300":"g = sns.jointplot(df.age, df.classification, kind=\"kde\", size=7)\n#pearsonr shows the correlation between two features, 1 if positive , -1 if negative, 0 if no correlation.\n","e0941766":"pal = sns.cubehelix_palette(2, rot=-.5, dark=.3)\nsns.violinplot(data=df2, palette=pal, inner=\"points\")\nplt.show()","8417745f":"sns.boxplot(x=\"sg\", y=\"age\", hue=\"classification\",data=df, palette=\"PRGn\")\nplt.show()","5ebfc061":"g = sns.FacetGrid(df,col=\"classification\",row=\"sg\")\ng.map(plt.hist,\"age\", bins=25)\ng.add_legend()\nplt.show()","ecfb811e":"#I assigned the value 0 and 1 to the nominal features\ndf['rbc'] = df.rbc.replace(['normal','abnormal'], ['1', '0'])\ndf['pc'] = df.pc.replace(['normal','abnormal'], ['1', '0'])\ndf['pcc'] = df.pcc.replace(['present','notpresent'], ['1', '0'])\ndf['ba'] = df.ba.replace(['present','notpresent'], ['1', '0'])\ndf['htn'] = df.htn.replace(['yes','no'], ['1', '0'])\ndf['dm'] = df.dm.replace(['yes','no'], ['1', '0'])\ndf['cad'] = df.cad.replace(['yes','no'], ['1', '0'])\ndf['appet'] = df.appet.replace(['good','poor'], ['1', '0'])\ndf['pe'] = df.pe.replace(['yes','no'], ['1', '0'])\ndf['ane'] = df.ane.replace(['yes','no'], ['1', '0'])\ndf.head()","08970a7f":"#then I converted them to numeric data type\ndf.rbc = pd.to_numeric(df.rbc, errors='coerce')\ndf.pc = pd.to_numeric(df.pc, errors='coerce')\ndf.pcc = pd.to_numeric(df.pcc, errors='coerce')\ndf.ba = pd.to_numeric(df.ba, errors='coerce')\ndf.htn = pd.to_numeric(df.htn, errors='coerce')\ndf.dm = pd.to_numeric(df.dm, errors='coerce')\ndf.cad = pd.to_numeric(df.cad, errors='coerce')\ndf.appet = pd.to_numeric(df.appet, errors='coerce')\ndf.pe = pd.to_numeric(df.pe, errors='coerce')\ndf.ane = pd.to_numeric(df.ane, errors='coerce')","32202e90":"df.info()","bc1493d9":"#I used the knnimputer method for the remaining missing values\n#because some features have specific values that's why I didn't get the mean value.\nimputer = KNNImputer(n_neighbors=2)\ndf_filled = imputer.fit_transform(df)","a456dd8e":"df_filled.tolist()","7ad7a7ad":"#When we use the knnimputer method, we obtained an array\n#so I turned it back into a dataframe.\ndf2 = pd.DataFrame(data = df_filled)","c521cd33":"#now I have filled all the features\ndf2.info()","edfb1842":"df2.head()","ff1aa76a":"#these variables will be used to show the algorithm name and its successes.\nscore=[] \nalgorithms=[] \nprecision=[]\nsensitivity=[]\nrecall=[]\nf1score=[]","3ebef8db":"from sklearn.tree import DecisionTreeClassifier\n\ny=df2[24].values\nx_data=df2.drop([24],axis=1)\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.3)\n\ndt = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 5, random_state=1)\ndt.fit(x_train,y_train)\n\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=dt.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Decision Tree Confusion Matrix\")\nplt.show()\n\nprint(\"Decision Tree accuracy =\",dt.score(x_test,y_test)*100)\nscore.append(dt.score(x_test,y_test)*100)\nalgorithms.append(\"Decision Tree\")\n\nprint(\"Decision Tree precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Decision Tree sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Decision Tree recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Decision Tree f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)\n\n\n\n#y_pred_prob = dt.predict_proba(x_test)[:,1]\n#fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n#roc_auc = auc(fpr, tpr)\n## Plot ROC curve\n#plt.figure()\n#plt.plot([0, 1], [0, 1], color='orange', linestyle='--')\n#plt.plot(fpr, tpr)\n#plt.xlabel('False Positive Rate')\n#plt.ylabel('True Positive Rate')\n#plt.title('Receiver operating characteristic')\n#plt.legend(loc=\"lower right\")\n#plt.show()","e7a68d2e":"from IPython.display import Image\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\nimport os\nos.environ[\"PATH\"] += os.pathsep + 'C:\\Program Files (x86)\/Graphviz2.38\/bin\/'\n\nfeatures = list(df.columns[1:])\nfeatures\n\ndot_data = StringIO()\nexport_graphviz(dt, out_file = dot_data,feature_names = features,filled = True,rounded=True)\n\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","e7d314ae":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100, random_state=1)\nrf.fit(x_train,y_train)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=rf.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Random Forest Confusion Matrix\")\nplt.show()\nprint(\"Random Forest accuracy =\",rf.score(x_test,y_test)*100)\nscore.append(rf.score(x_test,y_test)*100)\nalgorithms.append(\"Random Forest\")\n\nprint(\"Random Forest precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Random Forest sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Random Forest recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Random Forest f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)","c040b922":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n\ny=df2[24].values\nx_data=df2.drop([24],axis=1)\n\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=1,test_size=0.3)\n\nknn=KNeighborsClassifier(n_neighbors=3)\nknn.fit(x_train,y_train)\nknn.predict(x_test)\n\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=knn.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\" KNN Confusion Matrix\")\nplt.show()\n\nprint(\"KNN accuracy =\",knn.score(x_test,y_test)*100)\nscore.append(knn.score(x_test,y_test)*100)\nalgorithms.append(\"KNN\")\n\nprint(\"KNN precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"KNN sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"KNN recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"KNN f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)","2cbc7ee0":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=svm.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Support Vector Machine Confusion Matrix\")\nplt.show()\n\nprint(\"SVM accuracy =\",svm.score(x_test,y_test)*100)\nscore.append(svm.score(x_test,y_test)*100)\nalgorithms.append(\"Support Vector Machine\")\n\nprint(\"SVM precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"SVM sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"SVM recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"SVM f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)","67537ea4":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n#Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ny_pred=nb.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Naive Bayes Confusion Matrix\")\nplt.show()\n\nprint(\"Naive Bayes accuracy =\",nb.score(x_test,y_test)*100)\nscore.append(nb.score(x_test,y_test)*100)\nalgorithms.append(\"Naive Bayes\")\n\nprint(\"Naive Bayes precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Naive Bayes sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Naive Bayes recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Naive Bayes f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)","ab08fd8e":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression()\nlr.fit(x_train,y_train)\n\n#Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ny_pred=lr.predict(x_test)\ny_true=y_test\ncm=confusion_matrix(y_true,y_pred)\n#Confusion Matrix on Heatmap\nf,ax=plt.subplots(figsize=(5,5))\nsns.heatmap(cm,annot=True,linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.title(\"Logistic Regression Confusion Matrix\")\nplt.show()\n\nprint(\"Logistic Regression accuracy =\",lr.score(x_test,y_test)*100)\nscore.append(lr.score(x_test,y_test)*100)\nalgorithms.append(\"Logistic Regression\")\n\nprint(\"Logistic Regression precision =\",precision_score(y_true, y_pred,average = 'macro')*100)\nprecision.append(precision_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Logistic Regression sensitivity =\",sensitivity_score(y_true, y_pred,average = 'macro')*100)\nsensitivity.append(sensitivity_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Logistic Regression recall =\",recall_score(y_true, y_pred,average = 'macro')*100)\nrecall.append(recall_score(y_true, y_pred,average = 'macro')*100)\n\nprint(\"Logistic Regression f1 score =\",f1_score(y_true, y_pred,average = 'binary')*100)\nf1score.append(f1_score(y_true, y_pred,average = 'binary')*100)","305cd64b":"tuned_parameters = [{'n_estimators':[7,8,9,10,11,12,13,14,15,16],'max_depth':[2,3,4,5,6,None],\n                     'class_weight':[None,{0: 0.33,1:0.67},'balanced'],'random_state':[42]}]\nclf = GridSearchCV(RandomForestClassifier(), tuned_parameters, cv=10,scoring='f1')\nclf.fit(x_train, y_train)\n\n\nprint(\"Detailed classification report:\")\ny_true, lr_pred = y_test, clf.predict(x_test)\nprint(classification_report(y_true, lr_pred))\n\nconfusion = confusion_matrix(y_test, lr_pred)\nprint('Confusion Matrix:')\nprint(confusion)\n\n# Determine the false positive and true positive rates\n#fpr,tpr,roc_auc = auc_scorer(clf, x_test, y_test, 'RF')\n\nprint('Best parameters:')\nprint(clf.best_params_)\nclf_best = clf.best_estimator_","ef972b64":"plt.figure(figsize=(12,3))\nfeatures = x_test.columns.values.tolist()\nimportance = clf_best.feature_importances_.tolist()\nfeature_series = pd.Series(data=importance,index=features)\nfeature_series.plot.bar()\nplt.title('Feature Importance')","64db7303":"trace1 = {\n  'x': algorithms,\n  'y': score,\n  'name': 'score',\n  'type': 'bar'\n    \n}\n\ndata = [trace1];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of Accuracy of Classification Algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","36994085":"trace2 = {\n  'x': algorithms,\n  'y': precision,\n  'name': 'score',\n  'type': 'bar'\n    \n}\n\ndata = [trace2];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of Precision of Classification Algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","0235f258":"trace3 = {\n  'x': algorithms,\n  'y': sensitivity,\n  'name': 'score',\n  'type': 'bar'\n    \n}\n\ndata = [trace3];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of Sensitivity of Classification Algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","89ce58a8":"trace4 = {\n  'x': algorithms,\n  'y': recall,\n  'name': 'score',\n  'type': 'bar'\n    \n}\n\ndata = [trace4];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of Recall of Classification Algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","30830643":"trace5 = {\n  'x': algorithms,\n  'y': f1score,\n  'name': 'score',\n  'type': 'bar'\n    \n}\n\ndata = [trace5];\nlayout = {\n  'xaxis': {'title': 'Classification Algorithms'},\n  'title': 'Comparison of F1 Scores of Classification Algorithms'\n};\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","dbb7f179":"<a id=\"6\"> <\/a>\n## MACHINE LEARNING ALGORITHMS","03fd12b4":"# CONTENT\n1. [Introduction](#1)\n2. [Load and Check Data](#2)\n3. [Outlier Detection](#3)\n4. [Fill Missing Value](#4)\n5. [Data Visualization](#5)\n6. [Machine Learning Algorithms](#6)\n7. [Results](#7)\n\n\n\n","288d4cae":"<a id=\"4\"> <\/a>\n## FILL MISSING VALUE","00a01f98":"<a id=\"7\"> <\/a>\n## RESULTS \n\n* In this study, 24 data recording information of 400 people such as age, blood pressure, density, diabetes were used as attributes. Clinical records were examined to determine whether chronic kidney disease was present or not, and provided a high accuracy rate with machine learning methods.\n\n* Chronic kidney disease is a disease that hinders the normal functions of the kidney and damages the kidneys. It is one of the common diseases in the world and the prediction of the disease is one of the basic issues in medical diagnosis. Chronic kidney disease is one of the leading causes of death worldwide. Early detection of this disease is very important in terms of health and treatment costs. Many machine learning algorithms have been used in the literature to predict the disease.\n\n* In the study, six different classifiers were utilized in determining the targeted chronic kidney disease and the best performing classifier was tried to be found. These algorithms were compared on the basis of accuracy, sensitivity, sensitivity, recall and f1 score. When the results were evaluated with the data used in this study, it was seen that the random forest method (with an accuracy of 99.16%) performed better than other classification algorithms.\n\n* Machine learning tools can be used for timely and accurate diagnosis of chronic kidney disease, helping doctors confirm their diagnostic findings in a relatively short time, thereby helping a doctor to look and diagnose more patients in less time. In future studies, it may be possible to use different algorithms, such as deep learning methods, to predict chronic kidney disease.","a40eec8f":"<a id=\"3\"> <\/a>\n## OUTLIER DETECTION","6b253a21":"<a id=\"1\"> <\/a>\n## INTRODUCTION\n\n* Chronic kidney disease (CKD) is an important public health problem worldwide, especially for underdeveloped countries. Chronic kidney disease means that the kidney is not working as expected and cannot filter blood properly. Approximately 10% of the world's population suffers from this disease and millions die every year. Recently, the number of patients who have reached renal insufficiency is increasing, which necessitates kidney transplant or dialysis. CKD does not show any symptoms in its early stages. The only way to find out if the patient has kidney disease is by testing. Early detection of CKD in its early stages can help the patient receive effective treatment. \n* The aim of this study is to analyze the methods and compare their accuracy values by using 6 different machine learning methods.","11354b6a":"<a id=\"5\"> <\/a>\n## DATA VISUALIZATION","e114ec24":"<a id=\"2\"> <\/a>\n## LOAD AND CHECK DATA"}}