{"cell_type":{"c6d32abb":"code","11cc2f35":"code","22b9f118":"code","fa30059d":"code","163341c2":"code","1289f562":"code","3b0a4e02":"code","84ce2fc3":"code","f6a81995":"markdown","530e2cd0":"markdown","e18183c1":"markdown","15ed68ab":"markdown","75e26aa2":"markdown","876460c0":"markdown","53b3acf2":"markdown","e7307bca":"markdown","002b4c9e":"markdown","4500bce6":"markdown"},"source":{"c6d32abb":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, BatchNormalization, Dropout, LeakyReLU, Activation\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd","11cc2f35":"# Load CIFAR-10 dataset & check data dimension\n# Ref: Homepage: https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\n\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data() \n\nprint(\"X_train:\",X_train.shape)\nprint(\"y_train:\",y_train.shape)\nprint(\"X_test: \",X_test.shape)\nprint(\"y_test: \",y_test.shape)","22b9f118":"\nclass_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\nN_classes = len(class_names)\n\nX_train = X_train \/255.0\nX_test = X_test \/ 255.0\n\ny_train_cat = to_categorical (y_train)\ny_test_cat = to_categorical (y_test)\n\nprint(\"X_train:\",X_train.shape)\n# print(\"y_train:\",y_train.shape)\nprint(\"y_train_cat:\",y_train_cat.shape)\nprint(\"X_test: \",X_test.shape)\n# print(\"y_test: \",y)\nprint(\"y_test_cat: \",y_test_cat.shape)","fa30059d":"\nNpics=8\nfig, ax = plt.subplots(nrows=1, ncols=Npics, figsize=(Npics*3,4), sharey=True)\nfig.suptitle('CIFAR-10')\nfor i in range(Npics):\n  rnd = np.random.randint(1000)\n  ax[i].imshow(X_train[rnd,:,:,:])\n  ax[i].set_title (str(\"True class: \"+class_names[y_train[rnd][0]]))\n","163341c2":"# seed = 100\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32,kernel_size=(3,3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64,kernel_size=(3,3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(N_classes, activation='softmax'))\n\n# model.summary()","1289f562":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate = 0.003, \n    decay_steps = 100000, \n    decay_rate = 0.96, \n    staircase=True\n)\n\nmyoptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmyLoss = \"categorical_crossentropy\" \nmymetrics = [\"accuracy\"] #, Precision(name='precision'), Recall(name='recall')]\n\nmodel.compile(\n    loss = myLoss,\n    optimizer = myoptimizer,\n    metrics = mymetrics,\n)","3b0a4e02":"history = model.fit(X_train, y_train_cat, \n                epochs=15, \n                shuffle=True, \n                validation_data=(X_test, y_test_cat),\n                batch_size = 32, \n                )","84ce2fc3":"# note: using adam learning rate=0.0001 and batch=32\n\nNfigs = len(mymetrics)+1\nH = pd.DataFrame(history.history)\nfig, ax = plt.subplots(nrows=1, ncols=Nfigs, figsize=(Nfigs*5,3))\nfig.suptitle('CIFAR-10 evaluation')\n\nfor i in range(Nfigs):\n    H.iloc[:,i].plot(ax=ax[i], label='Training ') \n    H.iloc[:,i+Nfigs].plot(ax=ax[i], label='Validation ')\n    ax[i].set_title(H.columns[i])\n    ax[i].legend()\n    ax[i].grid(True)\n    ax[i].set_xlabel('epochs')\n\ntraining = model.evaluate(X_train, y_train_cat)\nevaluation = model.evaluate(X_test, y_test_cat)\n","f6a81995":"# Model definition","530e2cd0":"# CNN on CIFAR-10 - Main characteristics \n- non dimensionalization of datasets + hot encoding of labels (classes)\n- 2 conv (32,64 filters, 3x3), relu, BN, dropout 25%\n- 2 dense hidden layers (256, 128 neurones) , relu, BN, dropout 25%\n- softmax\n- loss=\"Categorical_crossentropy\", optimizer=\"adam\"\n\nACCURACY: 74% ","e18183c1":"# Define classes and print a couple of pictures for fun_test.shape","15ed68ab":"# import modules","75e26aa2":"## Obviously not great with generalization...  \n\n## check my next attempt at getting a better classification using a VGG-10 model:\nhttps:\/\/www.kaggle.com\/olivier2008\/cifar-10-vgg10","876460c0":"# Model attributes definition","53b3acf2":"# Evaluating model","e7307bca":"# Load CIFAR-10 dataset & check data dimension\n### Ref: Homepage: https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html","002b4c9e":"# non-dimensionalisation of training and test datasets, define classes","4500bce6":"# Running model"}}