{"cell_type":{"e1d9ed68":"code","ba486dbe":"code","59ed7621":"code","e4337a8d":"code","1c257b5b":"code","962c205a":"code","94bc6a72":"code","4e04938a":"code","a6e453d0":"code","511c0600":"code","45b2c0eb":"code","80f2d2dc":"code","e4f0ae73":"code","c99f85e9":"code","5e4873c0":"code","6aecbe4c":"code","bf3c7c8e":"code","402b973a":"code","272fc6e0":"code","ca2ef675":"code","fd6a4fcd":"code","a3fdb62e":"code","2a962d4f":"code","dd7f15e0":"code","c4e164ec":"code","b136c684":"code","e4d625ba":"code","da76089c":"code","f20ae5e5":"code","e7b29bfe":"code","ae1db1cc":"code","4ce41059":"code","ebc08653":"code","14a3d6d3":"code","7fbe7676":"code","1c08c5df":"code","682e5496":"code","b8ca2a79":"code","cf9aadb5":"code","90e81608":"code","7022d625":"code","1a8d3e49":"code","cd1c900e":"code","e28db7b0":"code","41079975":"code","50a706e2":"code","76845533":"code","5a64b66a":"code","90eb6d9c":"code","b8eccd9d":"code","f30fd518":"code","a7058e10":"code","1c5d0cee":"code","efcf9569":"code","5012693a":"code","d42a9927":"code","d9dd2b01":"code","8b65e146":"code","dde5bc0a":"code","8f414b8d":"code","9aff2cf2":"code","253cf3d2":"code","c6a5d8d8":"code","581c866b":"code","ca4bf29f":"code","c0eafe4e":"code","77d42efc":"code","356c0bb6":"code","7e1c29e0":"code","52edb413":"code","48e20aa7":"code","1dabb651":"code","466c41dd":"code","b83c7637":"code","854c0865":"code","163de31c":"code","f1b8c0c6":"code","f50cc8c0":"code","92acb643":"code","42113a87":"code","d0ed1894":"code","0a3d78a1":"markdown","50781224":"markdown","ec85ca89":"markdown","f3a59e9c":"markdown","f79b76a1":"markdown","1b8b381b":"markdown","21bd965a":"markdown","6cffbdf9":"markdown","2a7815ad":"markdown","ad9165ef":"markdown","9f6b7b57":"markdown","4267f6d4":"markdown","2efe7ee8":"markdown","2717a075":"markdown","271f1625":"markdown","80a01a95":"markdown","4ed53a57":"markdown","d982c3d1":"markdown","63c5ee37":"markdown","939faed3":"markdown","543aec89":"markdown","d20568e0":"markdown","e90e2c2a":"markdown","9aade695":"markdown","45e8a2a9":"markdown","ccc9b72a":"markdown","8ea61257":"markdown","d30b33c9":"markdown","65daa3af":"markdown","5bd304cc":"markdown","4eb3249a":"markdown","a40ffd1a":"markdown","95492d96":"markdown","50a1bf37":"markdown","f2978d21":"markdown","abfb5126":"markdown","9a170291":"markdown","eceb800f":"markdown","73970f54":"markdown","16b204f2":"markdown","d745e824":"markdown","79c59345":"markdown","f27c5b03":"markdown","0176e218":"markdown"},"source":{"e1d9ed68":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom datetime import datetime,timedelta\nimport seaborn as sns\n%matplotlib inline\nwarnings.filterwarnings('ignore')","ba486dbe":"\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","59ed7621":"train_df = reduce_mem_usage(pd.read_csv(\"..\/input\/train.csv\",parse_dates=[\"first_active_month\"]))\ntest_df = reduce_mem_usage(pd.read_csv(\"..\/input\/test.csv\",parse_dates=[\"first_active_month\"]))\nhistorical_df = reduce_mem_usage(pd.read_csv(\"..\/input\/historical_transactions.csv\",parse_dates=[\"purchase_date\"]))\nnew_merchants_df = reduce_mem_usage(pd.read_csv(\"..\/input\/new_merchant_transactions.csv\",parse_dates=[\"purchase_date\"]))\nmerchants_df = reduce_mem_usage(pd.read_csv(\"..\/input\/merchants.csv\"))\n# tmp_train_df = pd.merge(train_df,pd.concat([new_merchants_df,historical_df]),on=\"card_id\")\n# tmp_test_df =  pd.merge(test_df,pd.concat([new_merchants_df,historical_df]),on=\"card_id\")","e4337a8d":"train_df.head()","1c257b5b":"train_df.first_active_month.describe()","962c205a":"fig, axes = plt.subplots(figsize=(15,10))\naxes.set_title(\"First Active Month\")\naxes.set_ylabel(\"#\")\naxes.set_xlabel(\"years\")\ntrain_df.first_active_month.value_counts().plot()","94bc6a72":"train_df.card_id.describe()","4e04938a":"fig,axes = plt.subplots(1,3,figsize=(15,8),sharey=True)\naxes[0].set_ylabel(\"#\")\naxes[1].set_ylabel(\"#\")\naxes[2].set_ylabel(\"#\")\n# axes.set_xlabel(\"feature\")\ntrain_df[\"feature_1\"].value_counts().plot(kind=\"bar\",ax = axes[0],title=\"feature_1\",rot=0,color=\"tan\")\ntrain_df[\"feature_2\"].value_counts().plot(kind=\"bar\",ax=axes[1],title=\"feature_2\",rot=0,color=\"teal\")\ntrain_df[\"feature_3\"].value_counts().plot(kind=\"bar\",ax=axes[2],title=\"feature_3\",rot=0,color=\"gray\")","a6e453d0":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,10))\naxes.set_title(\"target boxplot\")\naxes.set_ylabel(\"target\")\naxes.boxplot(list(train_df[\"target\"].values),showmeans=True)","511c0600":"merchants_df.head()","45b2c0eb":"merchants_df.columns","80f2d2dc":"fig,axes = plt.subplots(1,3,figsize=(15,8))\nnp.log(merchants_df[\"merchant_group_id\"].value_counts()[:10]).plot(kind=\"bar\",ax = axes[0],title=\"Log(#(merchant_group_id))\",rot=0,color=\"tan\")\nmerchants_df[\"merchant_category_id\"].value_counts()[:10].plot(kind=\"bar\",ax=axes[1],title=\"#(merchant_category_id)\",rot=0,color=\"teal\")\nmerchants_df[\"subsector_id\"].value_counts()[:10].plot(kind=\"bar\",ax=axes[2],title=\"#(subsector_id)\",rot=0,color=\"blue\")\n","e4f0ae73":"pd.concat([merchants_df[\"numerical_1\"],merchants_df[\"numerical_2\"]],axis=1).describe()","c99f85e9":"fig,axes = plt.subplots(1,3,figsize=(21,7))\nmerchants_df[\"category_1\"].value_counts().plot(kind=\"pie\",explode=(0,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[0])\nmerchants_df[\"category_2\"].value_counts().plot(kind=\"pie\",explode=(0,0.01,0.02, 0.03, 0.04),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[1])\nmerchants_df[\"category_4\"].value_counts().plot(kind=\"pie\",explode=(0,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[2])","5e4873c0":"fig,axes = plt.subplots(1,2,figsize=(16,8))\nmerchants_df[\"most_recent_purchases_range\"].value_counts().plot(kind=\"pie\",autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[0])\nmerchants_df[\"most_recent_sales_range\"].value_counts().plot(kind=\"pie\",autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[1])\n","6aecbe4c":"#city_id && #state_id\nfig,axes = plt.subplots(2,1,figsize=(16,8))\nnp.log(merchants_df[\"city_id\"].value_counts())[:20].plot(kind=\"bar\",title=\"Log(#(city_id))\",rot=0,ax=axes[0],color=\"b\")\nnp.log(merchants_df[\"state_id\"].value_counts()).plot(kind=\"bar\",title=\"Log(#(state_id))\",rot=0,ax=axes[1],color=\"b\")","bf3c7c8e":"merchants_df[[\"avg_sales_lag3\",\"avg_sales_lag6\",\"avg_sales_lag12\"]].describe()","402b973a":"merchants_df[[\"avg_purchases_lag3\",\"avg_purchases_lag6\",\"avg_purchases_lag12\"]].describe()","272fc6e0":"historical_df.head()","ca2ef675":"historical_df.columns","fd6a4fcd":"fig,axes = plt.subplots(2,2,figsize=(8,8))\nhistorical_df[\"authorized_flag\"].value_counts().plot(kind=\"pie\",explode=(0,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[0][0])\nhistorical_df[\"category_1\"].value_counts().plot(kind=\"pie\",explode=(0,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[0][1])\nhistorical_df[\"category_3\"].value_counts().plot(kind=\"pie\",explode=(0,0.1,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[1][0])\nhistorical_df[\"category_2\"].value_counts().plot(kind=\"pie\",explode=(0,0.1,0.1,0.1,0.1),autopct='%1.1f%%',shadow=False, startangle=90,ax=axes[1][1])","a3fdb62e":"historical_df[\"purchase_amount\"].describe()","2a962d4f":"np.percentile(historical_df[\"purchase_amount\"].values,q=99)","dd7f15e0":"fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\naxes.set_title(\"purchase amount violin plot (removing top 1%)\")\naxes.set_ylabel(\"purchase_amount\")\nax = sns.violinplot(y=list(historical_df[historical_df[\"purchase_amount\"] < np.percentile(historical_df[\"purchase_amount\"],99)][\"purchase_amount\"]),showmeans=True,showmedians=True, palette=\"muted\")","c4e164ec":"fig,axes = plt.subplots(1,1,figsize=(8,8))\naxes.set_title(\"log(#(installments))\")\naxes.set_ylabel(\"#\")\naxes.set_xlabel(\"records\")\nnp.log(historical_df.installments.value_counts()).plot(kind=\"bar\",color=\"b\")","b136c684":"historical_df[\"purchase_date\"].head(5)","e4d625ba":"\"first_day:'{}' and last_day:'{}'\".format(np.min(historical_df[\"purchase_date\"]), np.max(historical_df[\"purchase_date\"]))","da76089c":"start_date = datetime(2017, 1, 1)\nend_date = datetime(2018, 2, 1)\ndef daterange(start_date, end_date):\n    for n in range(int((end_date - start_date).days)):\n        yield start_date + timedelta(n)\ndates_month = []\nfor single_date in daterange(start_date, end_date):\n    dates_month.append(single_date.strftime(\"%Y-%m\"))\ndates_month = list(set(dates_month))\n\ntmp_churn_df = pd.DataFrame()\ntmp_churn_df[\"date\"] = historical_df[\"purchase_date\"]\ntmp_churn_df[\"yaer\"] = pd.DatetimeIndex(tmp_churn_df[\"date\"]).year\ntmp_churn_df[\"month\"] =pd.DatetimeIndex(tmp_churn_df[\"date\"]).month\ntmp_churn_df[\"merchant_id\"] = historical_df[\"merchant_id\"]\ntmp_churn_df.head()","f20ae5e5":"\"distinct merchants who have bought on the website on 2017-01 are:'{}'merchants\".format(len(set(historical_df[(tmp_churn_df.yaer == 2017) & (tmp_churn_df.month == 1) ][\"merchant_id\"])))","e7b29bfe":"target_intervals_list = [(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7),(2017,8),(2017,9),(2017,10),(2017,11),(2017,12),(2018,1),(2018,2)]\nintervals_visitors = []\nfor tmp_tuple in target_intervals_list:\n    intervals_visitors.append(tmp_churn_df[(tmp_churn_df.yaer == tmp_tuple[0]) & (tmp_churn_df.month == tmp_tuple[1]) ][\"merchant_id\"])\n\"Size of intervals_visitors:{} \".format(len(intervals_visitors))\n\ntmp_matrix = np.zeros((14,14))\n\nfor i in range(0,14):\n    k = False\n    tmp_set = []\n    for j in range(i,14): \n        if k:\n            tmp_set = tmp_set & set(intervals_visitors[j])\n        else:\n            tmp_set = set(intervals_visitors[i]) & set(intervals_visitors[j])\n        tmp_matrix[i][j] = len(list(tmp_set))\n        k = True\nxticklabels = [\"interval 1\",\"interval 2\",\"interval 3\",\"interval 4\",\"interval 5\",\"interval 6\",\"interval 7\",\"interval 8\",\n              \"interval 9\",\"interval 10\",\"interval 11\", \"interval 12\",\"interval 13\",\"interval 14\"]\nyticklabels = [(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7),(2017,8),(2017,9),(2017,10),(2017,11),(2017,12),(2018,1),(2018,2)]\nfig, ax = plt.subplots(figsize=(14,14))\nax = sns.heatmap(np.array(tmp_matrix,dtype=int), annot=True, cmap=\"RdBu_r\",xticklabels=xticklabels,fmt=\"d\",yticklabels=yticklabels)\nax.set_title(\"Churn-rate using merchant_id\")\nax.set_xlabel(\"intervals\")\nax.set_ylabel(\"months\")","ae1db1cc":"import gc;gc.collect()","4ce41059":"start_date = datetime(2017, 1, 1)\nend_date = datetime(2018, 2, 1)\ndef daterange(start_date, end_date):\n    for n in range(int((end_date - start_date).days)):\n        yield start_date + timedelta(n)\ndates_month = []\nfor single_date in daterange(start_date, end_date):\n    dates_month.append(single_date.strftime(\"%Y-%m\"))\ndates_month = list(set(dates_month))\n\ntmp_churn_df = pd.DataFrame()\ntmp_churn_df[\"date\"] = historical_df[\"purchase_date\"]\ntmp_churn_df[\"yaer\"] = pd.DatetimeIndex(tmp_churn_df[\"date\"]).year\ntmp_churn_df[\"month\"] =pd.DatetimeIndex(tmp_churn_df[\"date\"]).month\ntmp_churn_df[\"card_id\"] = historical_df[\"card_id\"]\ntmp_churn_df.head()\n\n\"distinct cards who have bounght on the website on 2017-01 are:'{}'merchants\".format(len(set(historical_df[(tmp_churn_df.yaer == 2017) & (tmp_churn_df.month == 1) ][\"merchant_id\"])))\ntarget_intervals_list = [(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7),(2017,8),(2017,9),(2017,10),(2017,11),(2017,12),(2018,1),(2018,2)]\nintervals_visitors = []\nfor tmp_tuple in target_intervals_list:\n    intervals_visitors.append(tmp_churn_df[(tmp_churn_df.yaer == tmp_tuple[0]) & (tmp_churn_df.month == tmp_tuple[1]) ][\"card_id\"])\n\"Size of intervals_visitors:{} \".format(len(intervals_visitors))\n\ntmp_matrix = np.zeros((14,14))\n\nfor i in range(0,14):\n    k = False\n    tmp_set = []\n    for j in range(i,14): \n        if k:\n            tmp_set = tmp_set & set(intervals_visitors[j])\n        else:\n            tmp_set = set(intervals_visitors[i]) & set(intervals_visitors[j])\n        tmp_matrix[i][j] = len(list(tmp_set))\n        k = True\nxticklabels = [\"interval 1\",\"interval 2\",\"interval 3\",\"interval 4\",\"interval 5\",\"interval 6\",\"interval 7\",\"interval 8\",\n              \"interval 9\",\"interval 10\",\"interval 11\", \"interval 12\",\"interval 13\",\"interval 14\"]\nyticklabels = [(2017,1),(2017,2),(2017,3),(2017,4),(2017,5),(2017,6),(2017,7),(2017,8),(2017,9),(2017,10),(2017,11),(2017,12),(2018,1),(2018,2)]\nfig, ax = plt.subplots(figsize=(14,14))\nax = sns.heatmap(np.array(tmp_matrix,dtype=int), annot=True, cmap=\"RdBu_r\",xticklabels=xticklabels,fmt=\"d\",yticklabels=yticklabels)\nax.set_title(\"Churn-rate using card_ids\")\nax.set_xlabel(\"intervals\")\nax.set_ylabel(\"months\")","ebc08653":"del tmp_churn_df\nimport gc;gc.collect()","14a3d6d3":"city_state_df = historical_df[[\"city_id\" , \"state_id\"]]\n\ntmp_df = city_state_df.groupby(by=\"city_id\").count().reset_index()\ntmp_df = tmp_df.rename(index=str,columns={\"state_id\" : \"count\"})\ntmp_df.sort_values(ascending=False,by=\"count\",inplace=True)\n# tmp_df = tmp_df.head(100)\nlables=list(tmp_df.city_id)\nsizes=list(tmp_df[\"count\"])\n\ntmp_df=city_state_df.groupby(by=\"state_id\").count().reset_index()\ntmp_df = tmp_df.rename(index=str,columns={\"city_id\" : \"count\"})\ntmp_df.sort_values(ascending=False,by=\"count\",inplace=True)\nlabels_gender = list(tmp_df.state_id)\nsizes_gender = list(tmp_df[\"count\"])\n\nfig, ax = plt.subplots(figsize=(15,15))\nplt.pie(sizes, labels=lables, startangle=90,frame=True)\nplt.pie(sizes_gender,radius=0.75,startangle=90)\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.show()\nprint()","7fbe7676":"del city_state_df\ndel tmp_df","1c08c5df":"historical_df[\"year\"] = historical_df[\"purchase_date\"].dt.year\nhistorical_df[\"month\"] = historical_df[\"purchase_date\"].dt.month\nhistorical_df[\"day\"] = historical_df[\"purchase_date\"].dt.day\nhistorical_df[\"hour\"] = historical_df[\"purchase_date\"].dt.hour\n#  you can also use single line commands\n# historical_df = pd.concat(\n#     [\n#         historical_df,\n#         historical_df[\"purchase_date\"].apply(lambda x:\n#                                              pd.Series({\n#                                                  'year':x.year,\n#                                                  'month':x.month,\n#                                                  'day':x.day\n#                                              }))\n#     ]\n#     ,axis=1) ","682e5496":"fig,axes = plt.subplots(4,2,figsize=(10,30))\naxes[0][0].set_title(\"yearly histogram\")\naxes[0][1].set_title(\"monthly histogram\")\naxes[1][0].set_title(\"daily histogram\")\naxes[1][1].set_title(\"hourly histogram\")\naxes[2][0].set_title(\"state_id histogram\")\naxes[2][1].set_title(\"city_id histogram\")\naxes[3][0].set_title(\"subsector_id histogram\")\naxes[3][1].remove()\n\nhistorical_df.year.hist(ax=axes[0][0],normed=True,bins=2)\nhistorical_df.month.hist(ax=axes[0][1],normed=True,bins=12)\nhistorical_df.day.hist(ax=axes[1][0],normed=True,bins=30)\nhistorical_df.hour.hist(ax=axes[1][1],normed=True,bins=24)\nhistorical_df[\"state_id\"].hist( bins=30,normed=True,ax=axes[2][0])\nhistorical_df[\"city_id\"].hist( bins=30,normed=True,ax=axes[2][1])\nhistorical_df[\"subsector_id\"].hist( bins=30,normed=True,ax=axes[3][0])","b8ca2a79":"import gc; gc.collect()","cf9aadb5":"test_df.describe()","90e81608":"test_df.shape","7022d625":"# test_df = test_df.dropna()","1a8d3e49":"datetimes_df = pd.concat([train_df[[\"first_active_month\"]].groupby(by=[\"first_active_month\"],axis=0).size(),test_df[[\"first_active_month\"]].groupby(by=[\"first_active_month\"],axis=0).size()],axis=1)\n\nfig, ax1 = plt.subplots(figsize=(20,10))\nt = datetimes_df.index\ns1 = datetimes_df.iloc[:,0]\nax1.plot(t, s1, 'b-')\nax1.set_xlabel('date')\n# Make the y-axis label, ticks and tick labels match the line color.\nax1.set_ylabel('train', color='b')\nax1.tick_params('y', colors='b')\n\nax2 = ax1.twinx()\ns2 = datetimes_df.iloc[:,1]\nax2.plot(t, s2, 'r--')\nax2.set_ylabel('test', color='r')\nax2.tick_params('y', colors='r')\nfig.tight_layout()","cd1c900e":"fig,axes = plt.subplots(1,2,figsize=(10,5))\naxes[0].set_title(\"train card_id distru\")\naxes[1].set_title(\"test card_id distru\")\naxes[0].hist(train_df[[\"card_id\"]].groupby(by=[\"card_id\"]).size(),bins=3)\naxes[1].hist(test_df[[\"card_id\"]].groupby(by=[\"card_id\"]).size(),bins=3)","e28db7b0":"# calling gargage collector\ndel datetimes_df\nimport gc; gc.collect()","41079975":"card_merchant_df = historical_df[[\"card_id\",\"merchant_id\",\"purchase_amount\"]]\ncard_merchant_df.head()","50a706e2":"merchants_loyality_and_revenue = card_merchant_df.groupby(by=[\"merchant_id\"]).agg({\"card_id\":\"count\",\"purchase_amount\":\"sum\"})","76845533":"merchants_loyality_and_revenue = merchants_loyality_and_revenue.reset_index().rename(index=str , columns={\"purchase_amount\" : \"amount\" , \"card_id\" : \"count\"})\nmerchants_loyality_and_revenue[\"avg\"] = merchants_loyality_and_revenue[\"amount\"] \/ merchants_loyality_and_revenue[\"count\"]\nmerchants_loyality_and_revenue.head()","5a64b66a":"# who are disloyal merchants\nmerchants_loyality_and_revenue.sort_values(by=\"count\",ascending=False).head(5)","90eb6d9c":"#who are the most affordable merchants\nmerchants_loyality_and_revenue.sort_values(by=\"avg\",ascending=False).head(5)","b8eccd9d":"del merchants_loyality_and_revenue","f30fd518":"cards_loyality_and_revenue = card_merchant_df.groupby(by=[\"card_id\"]).agg({\"merchant_id\":\"count\",\"purchase_amount\":\"sum\"})\ncards_loyality_and_revenue = cards_loyality_and_revenue.reset_index().rename(index=str , columns={\"purchase_amount\" : \"amount\" , \"merchant_id\" : \"count\"})\ncards_loyality_and_revenue[\"avg\"] = cards_loyality_and_revenue[\"amount\"] \/ cards_loyality_and_revenue[\"count\"]","a7058e10":"cards_loyality_and_revenue.head()","1c5d0cee":"# who are disloyal cards\ncards_loyality_and_revenue.sort_values(by=\"count\",ascending=False).head(5)","efcf9569":"#who are the most affordable cards\ncards_loyality_and_revenue.sort_values(by=\"avg\",ascending=False).head(5)","5012693a":"import gc; gc.collect()","d42a9927":"historical_df.head(1)","d9dd2b01":"#extracting information from historical_df\nimport gc;gc.collect()\nhistorical_df['authorized_flag'] = historical_df['authorized_flag'].map({'Y':1, 'N':0})\ndef aggregate_historical_transactions(history):\n    history.loc[:, 'purchase_date'] = pd.DatetimeIndex(history['purchase_date']).astype(np.int64) * 1e-9\n    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'state_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'purchase_date': [np.ptp],\n        'month_lag': ['min', 'max'],\n        'year': ['nunique'],\n        'month': ['nunique'],\n        'day': ['nunique'],\n        'hour': ['nunique'],\n        'merchant_category_id': ['nunique'],\n        }\n    agg_history = history.groupby(['card_id']).agg(agg_func)\n    agg_history.columns = ['hist_' + '_'.join(col).strip() \n                           for col in agg_history.columns.values]\n    agg_history.reset_index(inplace=True)\n    \n    df = (history.groupby('card_id')\n          .size()\n          .reset_index(name='hist_transactions_count'))\n    \n    agg_history = pd.merge(df, agg_history, on='card_id', how='left')\n    \n    return agg_history\n\nnew_history = aggregate_historical_transactions(historical_df)","8b65e146":"new_history.head(1)","dde5bc0a":"new_merchants_df.head(1)","8f414b8d":"new_merchants_df['authorized_flag'] = new_merchants_df['authorized_flag'].map({'Y':1, 'N':0})\ndef aggregate_new_transactions(new_trans):    \n    agg_func = {\n        'authorized_flag': ['sum', 'mean'],\n        'merchant_id': ['nunique'],\n        'city_id': ['nunique'],\n        'purchase_amount': ['sum', 'median', 'max', 'min', 'std'],\n        'installments': ['sum', 'median', 'max', 'min', 'std'],\n        'month_lag': ['min', 'max'],\n        'subsector_id':['nunique'],\n        'state_id':['nunique']\n        }\n    agg_new_trans = new_trans.groupby(['card_id']).agg(agg_func)\n    agg_new_trans.columns = ['new_' + '_'.join(col).strip() \n                           for col in agg_new_trans.columns.values]\n    agg_new_trans.reset_index(inplace=True)\n    \n    df = (new_trans.groupby('card_id')\n          .size()\n          .reset_index(name='new_transactions_count'))\n    \n    agg_new_trans = pd.merge(df, agg_new_trans, on='card_id', how='left')\n    \n    return agg_new_trans\n\nnew_merchants = aggregate_new_transactions(new_merchants_df)","9aff2cf2":"new_merchants.head(1)","253cf3d2":"train_df['elapsed_time'] = (datetime(2018, 2, 1) - train_df['first_active_month']).dt.days\ntest_df['elapsed_time'] = (datetime(2018, 2, 1) - test_df['first_active_month']).dt.days","c6a5d8d8":"train_df = pd.merge(train_df, new_history, on='card_id', how='left')\ntest_df = pd.merge(test_df, new_history, on='card_id', how='left')\n\ntrain_df = pd.merge(train_df, new_merchants, on='card_id', how='left')\ntest_df = pd.merge(test_df, new_merchants, on='card_id', how='left')","581c866b":"train_df.shape, test_df.shape","ca4bf29f":"set(train_df.columns)-set(test_df.columns)","c0eafe4e":"# categorizing feature_1 & feature_2 & feature_3\nfrom sklearn.preprocessing import LabelEncoder\nfor col in [\"feature_1\", \"feature_2\",\"feature_3\"]:\n#     print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","77d42efc":"exclude_features = ['card_id', 'first_active_month']","356c0bb6":"train_df = train_df.loc[:,train_df.columns[~train_df.columns.isin(exclude_features)]]","7e1c29e0":"from sklearn.model_selection import train_test_split\n_train, _eval = train_test_split(train_df, test_size=0.2, random_state=42)","52edb413":"_train.shape","48e20aa7":"import lightgbm as lgb\nparams = {\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"num_leaves\": 30,\n    \"min_child_samples\": 100,\n    \"learning_rate\": 0.1,\n    \"bagging_fraction\": 0.7,\n    \"feature_fraction\": 0.5,\n    \"bagging_frequency\": 5,\n    \"bagging_seed\": 2018,\n    \"verbosity\": -1\n}\nlgb_train = lgb.Dataset(_train.loc[:,_train.columns[~_train.columns.isin([\"target\"])]], _train[\"target\"])\nlgb_eval = lgb.Dataset(_eval.loc[:,_eval.columns[~_eval.columns.isin([\"target\"])]], _eval[\"target\"], reference=lgb_train)\ngbm = lgb.train(params, lgb_train, num_boost_round=2000, valid_sets=[lgb_eval], early_stopping_rounds=100,verbose_eval=100)","1dabb651":"exclude_features = [\"first_active_month\",\"card_id\"]\ntest_df = test_df.loc[:,test_df.columns[~test_df.columns.isin(exclude_features)]]\n\npredicted_target = gbm.predict(test_df, num_iteration=gbm.best_iteration)\npredicted_target[predicted_target < 0] = 0 \n\nsub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = predicted_target\n\nsub_df.to_csv(\"submission_raw.csv\", index=False)","466c41dd":"test_df.shape, sub_df.shape","b83c7637":"fig, ax = plt.subplots(figsize=(12,18))\nlgb.plot_importance(gbm, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()","854c0865":"target = train_df['target']\ndel train_df['target']\n","163de31c":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\nlgb_params = {\"objective\" : \"regression\", \"metric\" : \"rmse\", \n               \"max_depth\": 7, \"min_child_samples\": 20, \n               \"reg_alpha\": 1, \"reg_lambda\": 1,\n               \"num_leaves\" : 64, \"learning_rate\" : 0.001, \n               \"subsample\" : 0.8, \"colsample_bytree\" : 0.8, \n               \"verbosity\": -1}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_lgb = np.zeros(len(train_df))\npredictions_lgb = np.zeros(len(test_df))\n\nfeatures_lgb = list(train_df.columns)\nfeature_importance_df_lgb = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_df)):\n    trn_data = lgb.Dataset(train_df.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx], label=target.iloc[val_idx])\n\n    print(\"LGB \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    clf = lgb.train(lgb_params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=500, early_stopping_rounds = 2000)\n    oof_lgb[val_idx] = clf.predict(train_df.iloc[val_idx], num_iteration=clf.best_iteration)\n\n    fold_importance_df_lgb = pd.DataFrame()\n    fold_importance_df_lgb[\"feature\"] = features_lgb\n    fold_importance_df_lgb[\"importance\"] = clf.feature_importance()\n    fold_importance_df_lgb[\"fold\"] = fold_ + 1\n    feature_importance_df_lgb = pd.concat([feature_importance_df_lgb, fold_importance_df_lgb], axis=0)\n    predictions_lgb += clf.predict(test_df, num_iteration=clf.best_iteration) \/ FOLDs.n_splits\n    \n\nprint(np.sqrt(mean_squared_error(oof_lgb, target)))","f1b8c0c6":"import xgboost as xgb\n\nxgb_params = {'eta': 0.001, 'max_depth': 7, 'subsample': 0.8, 'colsample_bytree': 0.8, \n          'objective': 'reg:linear', 'eval_metric': 'rmse', 'silent': True}\n\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=1989)\n\noof_xgb = np.zeros(len(train_df))\npredictions_xgb = np.zeros(len(test_df))\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(train_df)):\n    trn_data = xgb.DMatrix(data=train_df.iloc[trn_idx], label=target.iloc[trn_idx])\n    val_data = xgb.DMatrix(data=train_df.iloc[val_idx], label=target.iloc[val_idx])\n    watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n    print(\"xgb \" + str(fold_) + \"-\" * 50)\n    num_round = 2000\n    xgb_model = xgb.train(xgb_params, trn_data, num_round, watchlist, early_stopping_rounds=100, verbose_eval=500)\n    oof_xgb[val_idx] = xgb_model.predict(xgb.DMatrix(train_df.iloc[val_idx]), ntree_limit=xgb_model.best_ntree_limit+50)\n\n    predictions_xgb += xgb_model.predict(xgb.DMatrix(test_df), ntree_limit=xgb_model.best_ntree_limit+50) \/ FOLDs.n_splits\n\nnp.sqrt(mean_squared_error(oof_xgb, target))","f50cc8c0":"print('lgb', np.sqrt(mean_squared_error(oof_lgb, target)))\nprint('xgb', np.sqrt(mean_squared_error(oof_xgb, target)))","92acb643":"total_sum = 0.5 * oof_lgb + 0.5 * oof_xgb\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(total_sum, target)**0.5))","42113a87":"cols = (feature_importance_df_lgb[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\n\nbest_features = feature_importance_df_lgb.loc[feature_importance_df_lgb.feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","d0ed1894":"sub_df = pd.read_csv('..\/input\/sample_submission.csv')\nsub_df[\"target\"] = 0.5 * predictions_lgb + 0.5 * predictions_xgb\nsub_df.to_csv(\"submission.csv\", index=False)","0a3d78a1":"test_df contains a null in first_active_month in record number #11578","50781224":"<a id=\"15\"><\/a> <br>\n**C. History_transactions.csv**","ec85ca89":"states -1, 1 and 2 have more than half of buy records. ","f3a59e9c":"<a id=\"4\"><\/a> <br>\n4. TARGET","f79b76a1":"<a id=\"1\"><\/a> <br>\n#  1-FEATURES AT A GLANCE\n\nIn the first paragraph we will take a glance to our feature spaces.  We will try to data feature by feature.\n\nLets import necessary packages ;-).","1b8b381b":"<a id=\"24\"><\/a> <br>\n2.  STATUS OF TEST SET ?!","21bd965a":"<a id=\"4\"><\/a> <br>\n2. CARD_ID","6cffbdf9":"<a id=\"12\"><\/a> <br>\n5. CITY_ID & STATE_ID\n\n*city_id: City identifier (anonymized )*\n\n*state_id: State identifier (anonymized)*","2a7815ad":"There is a huge difference between 75% percentile and maximum. You can conclude that there is a peak which may not be related to normal activeities. Lets get 99% percentile to check this. ","ad9165ef":"<a id=\"11\"><\/a> <br>\n4. MOST_RECENT_SALES_RANGE & MOST_RECENT_PURCHASE_RANGE\n\n*most_recent_sales_range: Range of revenue (monetary units) in last active month --> A > B > C > D > E *\n\n*most_recent_purchases_range: Range of quantity of transactions in last active month --> A > B > C > D > E*","9f6b7b57":"Although daily histogram reveals that there is a uniform distribution for historical data but hourly histogram represents that it could be usefull data for our competition goal. city id, state id and also subsector id have acceptable entropy and they may have considerable potential for enhacing the accuracy of model.","4267f6d4":"\n**In progress ...**\n\n**Be in touch to get last commits ...**\n\n**I'll try to complete it as soon as possible**\n\n**Your upvote will be motivation for me for continuing the kernel ;-)**","2efe7ee8":"<a id=\"29\"><\/a> <br>\n**B. SIMPLE TRAINING**","2717a075":"<a id=\"26\"><\/a> <br>\n4.  WHO ARE DISLOYAL CARDS & WHO ARE AFFORDABLE CARDS  ?!","271f1625":"<a id=\"22\"><\/a> <br>\n**D. ANSWERING BASIC QUESTIONS ABOUT DATASETS. **\n\n<a id=\"23\"><\/a> <br>\n1.  WHICH FEATURES COULD BE HELPFUL\n","80a01a95":"<a id=\"28\"><\/a> <br>\n**A. PREPROCESSING**","4ed53a57":"<a id=\"25\"><\/a> <br>\n3.  WHO ARE DISLOYAL MERCHANTS & WHO ARE AFFORDABLE MERCHANTS  ?!","d982c3d1":"99 percent of purchase amounts are less than 1.22. Lets assume the remaining 1 percent is outlier and move to visualize it.","63c5ee37":"Considering the memory limitations, we need to reduce the size of historical_transactions. It is the biggest dataset existed in this kernel. \nInspiring form [Fabien Kernel](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world), we decrease the size of our data to prevent probable mermory overflow.","939faed3":"<a id=\"10\"><\/a> <br>\n3. CATEGORY_1 & CATEGORY_2 & CATEGORY_4\n\n*category_1: anonymized category*\n\n*category_2: anonymized category*\n\n*category_4: anonymized category*","543aec89":"<a id=\"17\"><\/a> <br>\n2.  PURCHASE_AMOUNT\n","d20568e0":"You will get 3.92 score if use the output for submission.\n\nIn next steps we will use some methods for improving the performance of the regressioner. ","e90e2c2a":"<a id=\"20\"><\/a> <br>\n5.  CHURN_RATE WITH CARD_ID\n","9aade695":"<a id=\"30\"><\/a> <br>\n**C. FEATURE IMPORTANCE**","45e8a2a9":"<a id=\"13\"><\/a> <br>\n6. AVG_SALES_LAG3 & AVG_SALES_LAG6 & AVG_SALES_LAG12\n\n*avg_sales_lag3: Monthly average of revenue in last 3 months divided by revenue in last active month*\n\n*avg_sales_lag6: Monthly average of revenue in last 6 months divided by revenue in last active month*\n\n*avg_sales_lag12: Monthly average of revenue in last 12 months divided by revenue in last active month*","ccc9b72a":"<a id=\"31\"><\/a> <br>\n**D. IMPROVING PERFORMANCE USING BOOSTING METHODS**\n\n\nNow, lets improve our accuracy by using boosting approaches. We have used [This kernel](https:\/\/www.kaggle.com\/youhanlee\/hello-elo-ensemble-will-help-you) because of easy and straight forward road map it has. Thanks to @youhanlee.","8ea61257":"Now, the accuracy have been increased thanks to boosting methods ;-).","d30b33c9":"<a id=\"27\"><\/a> <br>\n#  2-TRAINING\n\nNow, lets move on training steps. In the first poit we will try to do simple regression on our data.","65daa3af":"<a id=\"2\"><\/a> <br>\n**A. Train.csv**","5bd304cc":"<a id=\"8\"><\/a> <br>\n1. MERCHANT_GROUP_ID & MERCHANT_CATEGORY_ID & SUBSCTOR_ID\n\n*merchant_group_id: Merchant group (anonymized )*\n\n*merchant_category_id: Unique identifier for merchant category (anonymized )*\n\n*subsector_id: Merchant category group (anonymized )*","4eb3249a":"<a id=\"9\"><\/a> <br>\n2. NUMERICAL_1 & NUMERICAL_2\n\n*numerical_1: anonymized measure*\n\n*numerical_2: anonymized measure*","a40ffd1a":"<a id=\"14\"><\/a> <br>\n7. AVG_PURCHASES_LAG3 & AVG_PURCHASES_LAG6 & AVG_PURCHASES_LAG12\n\n*avg_purchases_lag3: Monthly average of transactions in last 3 months divided by transactions in last active month*\n\n*avg_purchases_lag6: Monthly average of transactions in last 6 months divided by transactions in last active month*\n\n*avg_purchases_lag12: Monthly average of transactions in last 12 months divided by transactions in last active month*","95492d96":"<a id=\"16\"><\/a> <br>\n1. AUTHORIZED_FLAG & CATEGORY_1 & CATEGORY_2 & CATEGORY_3\n","50a1bf37":"As you can see merchant_id 'M_ID_00a6ca8a8a' has the most interactions with card_ids.","f2978d21":"<a id=\"21\"><\/a> <br>\n5.  NESTED PIE CHART FOR STATE_ID AND CITY_ID\n","abfb5126":"Comparing two diagram represents that in overall number of buys have been increased and the system have satisfied churn rate. We will go deeper to churn rates and conversion rates in next sections.","9a170291":"<a id=\"7\"><\/a> <br>\n**B. Merchants.csv**","eceb800f":"<a id=\"0\"><\/a> <br>\n## Kernel Headlines\n1. [Features at a Glance](#1)\n    1. [Train.csv](#2)\n    \n        1. [first_active_month](#3)\n        2. [card_id](#4)\n        3. [features_#](#5)\n        4. [target](#6)\n        \n\t2. [merchants.csv](#7)\n\t\n\t    1. [merchant_group_id & merchant_category_id & subsector_id](#8)\n\t\t2. [numerical_1 & numerical_2](#9)\n\t\t3. [category_1 & category_2 & category_4 ](#10)\n\t\t4. [most_recent_sales_range & most_recent_purchases_range](#11)\n        5. [city_id & state_id](#12)\n        6. [avg_sales_lag3 & avg_sales_lag6 & avg_sales_lag12](#13)\n        7. [avg_purchases_lag3 & avg_purchases_lag6 & avg_purchases_lag12](#14)\n\t\n    3. [Historical_transactions_df.csv](#15)\n\t\n\t\t1. [authorized_flag & catgory_1 & category_2 & category_3](#16)\n\t\t2. [purchase_amount](#17)\n\t\t3. [installments](#18)\n\t\t4. [churn rate using merchant_id](#19)\n\t\t5. [churn rate using card_id](#20)\n\t\t6. [nested_pie_chart historical state_id & city_id](#21)\n\t\n\t4. [Answering Basic Question about Datasets](#22)\n\t\t1. [Which features could be helpful ?! ](#23)\n\t\t2. [Status of Test Set](#24)\n\t\t3. [Who are disloyal merchants and who are affordable merchants](#25)\n\t\t4. [Who are disloyal cards and who are affordable cards](#26)\n       \n2. [Training](#27)\n      1. [Preprocessing ](#28)\n      2. [SimpleTraining ](#29)\n      3. [Feature Importances](#30)\n      4. [Improving accuracy using boosting methods](#31)","73970f54":"Test sets are completeltely in the same distribution the train data are.","16b204f2":"<a id=\"18\"><\/a> <br>\n3.  INSTALLMENTS\n","d745e824":"<a id=\"5\"><\/a> <br>\n3. FEATURES_1 & FEATURES_2 & FEATURES_3","79c59345":"   There is only one card_id in both of the train and test datasets.","f27c5b03":"<a id=\"19\"><\/a> <br>\n4.  CHURN_RATE WITH MERCHANT_ID\n","0176e218":"<a id=\"3\"><\/a> <br>\n1. FIRST_ACTIVE_MONTH"}}