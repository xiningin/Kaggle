{"cell_type":{"8218116e":"code","2de62c00":"code","3b5d25dd":"code","9230d0b7":"code","1ddd4e25":"code","2a44f3d6":"code","41fa3225":"code","e66cb293":"code","c5256c0e":"code","467c324c":"code","ae430d22":"code","9b932745":"code","371fc330":"code","39f9b729":"code","2d193c3d":"code","830aab53":"code","1212f3b9":"code","15c0260d":"code","2593e009":"code","81e87c19":"code","34397c08":"code","aab2bed4":"code","ed29f80e":"code","b83326ae":"code","ebb79779":"code","177370a0":"markdown","cce37596":"markdown","9490bbd2":"markdown","c478028d":"markdown","2f684e69":"markdown","2ee5d33b":"markdown","19d4a623":"markdown","450a0e97":"markdown","37dad585":"markdown","59c02621":"markdown","d06f5d1b":"markdown","9d7f55ab":"markdown","9e098dc7":"markdown","56135118":"markdown"},"source":{"8218116e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2de62c00":"# Import Necessary Libraries\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision.utils import make_grid\nfrom torch.optim import lr_scheduler\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\n%matplotlib inline\n\nimport torchvision.transforms as T\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport itertools\nimport time\nimport os\nimport copy\n\n\n# Get pwd to the two classes of cells \nprint('Classes of cells in dataset: ', os.listdir(\"..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images\"))\n","3b5d25dd":"\n# Defining the show plot method\ndef show_plots(history, plot_title=None, fig_size=None):\n    \"\"\" Useful function to view plot of loss values & accuracies across the various epochs\n        Works with the history object returned by the train_model(...) call \"\"\"\n    \n    assert type(history) is dict\n    # NOTE: the history object should always have loss & acc (for training data), but MAY have\n    # val_loss & val_acc for validation data\n    loss_vals = history['loss']\n    val_loss_vals = history['val_loss'] if 'val_loss' in history.keys() else None\n    epochs = range(1, len(history['loss']) + 1)\n\n    f, ax = plt.subplots(nrows=1, ncols=2, figsize=((16, 4) if fig_size is None else fig_size))\n        # plot losses on ax[0]\n    ax[0].plot(epochs, loss_vals, color='navy', marker='o', linestyle=' ', label='Training Loss')\n    if val_loss_vals is not None:\n        ax[0].plot(epochs, val_loss_vals, color='firebrick', marker='*', label='Validation Loss')\n        ax[0].set_title('Training & Validation Loss')\n        ax[0].legend(loc='best')\n    else:\n        ax[0].set_title('Training Loss')\n\n    ax[0].set_xlabel('Epochs')\n    ax[0].set_ylabel('Loss')\n    ax[0].grid(True)\n    # plot accuracies\n    acc_vals = history['acc']  if 'acc' in history.keys() else None\n    val_acc_vals = history['val_acc'] if 'val_acc' in history.keys() else None\n    if acc_vals is not None:\n        ax[1].plot(epochs, acc_vals, color='navy', marker='o', ls=' ', label='Training Accuracy')\n    if val_acc_vals is not None:\n        ax[1].plot(epochs, val_acc_vals, color='firebrick', marker='*', label='Validation Accuracy')\n        ax[1].set_title('Training & Validation Accuracy')\n        ax[1].legend(loc='best')\n    else:\n        ax[1].set_title('Training Accuracy')\n\n    ax[1].set_xlabel('Epochs')\n    ax[1].set_ylabel('Accuracy')\n    ax[1].grid(True)\n    \n    if plot_title is not None:\n        plt.suptitle(plot_title)\n\n    plt.show()\n    plt.savefig(\"metrics.png\")\n    plt.close()\n\n    # delete locals from heap before exiting (to save some memory!)\n    del loss_vals, epochs\n    if val_loss_vals is not None:\n        del val_loss_vals","9230d0b7":"# Create the path dir\nimport shutil\npath = '\/dataset'\nshutil.os.mkdir(path)\nprint('Current path:',path)","1ddd4e25":"import shutil\npath = '\/dataset\/train'\nshutil.os.mkdir(path)\nprint('Current path:',path)","2a44f3d6":"path = '\/dataset\/train\/'\ndata_dir = '..\/input\/cell-images-for-detecting-malaria\/cell_images\/cell_images\/'\ndef copytree(src, dst, symlinks=False, ignore=None):\n    for item in os.listdir(src):\n        s = os.path.join(src, item)\n        d = os.path.join(dst, item)\n        if os.path.isdir(s):\n            shutil.copytree(s, d, symlinks, ignore)\n        else:\n            shutil.copy2(s, d)\n\ncopytree(data_dir, path)\nprint ('Data dir:',data_dir)\nprint ('Path:',path)","41fa3225":"print(os.listdir(path))","e66cb293":"from sklearn.model_selection import train_test_split\n\n# Top level data directory. Here we assume the format of the directory conforms\n#   to the ImageFolder structure\ndata_dir = '\/dataset\/train\/'\n \n# Models to choose from [densenet, vgg, mobilenet, resnet], but implemeting resnet first.\nmodel_name = \"resnet\"\n\n# Number of classes in the dataset\nnum_classes = 2\n\n# Batch size for training\nbatch_size = 64\n\n# Number of epochs to train for\nnum_epochs = 15\n\n\n# Flag for feature extracting. When False, we finetune the whole model,\n#   when True we only update the reshaped layer params\nfeature_extract = True\n\nprint('Dataset directory:', data_dir)\nprint('Current Model:',model_name)","c5256c0e":"batch_size = 64\ndata_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                           transforms.RandomResizedCrop(224),\n                                           transforms.RandomHorizontalFlip(),\n                                           transforms.ToTensor(),\n                                           transforms.Normalize([0.485, 0.456, 0.406],\n                                                                [0.229, 0.224, 0.225])])    \n\ndataset = datasets.ImageFolder(data_dir, transform=data_transforms)\nprint('Dataset Details: \\n', dataset)","467c324c":"print('Size of dataset is:',len(dataset))","ae430d22":"\n#Split dataset into training and validation sets\ntrain_set, val_set = torch.utils.data.random_split(dataset, [20000, 7558])\n\n# Load the dataset using DataLoader\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(val_set, batch_size, shuffle=True)\ndataloaders_dict = {'train':train_loader, 'val':valid_loader}\n\nprint('Training set size is:',len(train_set))\nprint('Validation set size is:',len(val_set))","9b932745":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders_dict['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\n#imshow(out, title=[num_classes[x] for x in classes])","371fc330":"def set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False","39f9b729":"def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n    # Initialize these variables which will be set in this if statement. Each of these\n    #   variables is model specific.\n    model_ft = None\n    input_size = 0\n\n    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n    elif model_name == \"mobilenet\":\n        \"\"\" mobilenet_v2\n        \"\"\"\n        model_ft = models.mobilenet_v2(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[1].in_features\n        model_ft.classifier[1] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n    elif model_name == \"vgg\":\n        \"\"\" VGG11_bn\n        \"\"\"\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier[6].in_features\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n        input_size = 224\n    elif model_name == \"densenet\":\n        \"\"\" Densenet\n        \"\"\"\n        model_ft = models.densenet121(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n    else:\n        print(\"Invalid model name, exiting...\")\n        exit()\n\n    return model_ft, input_size","2d193c3d":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n    \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\" \n        for b in self.dl:\n            yield to_device(b, self.device)\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","830aab53":"# check if CUDA is available\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint('Working device:',device)","1212f3b9":"# Initialize the model for this run\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n\n# Print the model we just instantiated\nprint(model_ft)","15c0260d":"model_ft = model_ft.to(device)\n\n# Gather the parameters to be optimized\/updated in this run. If we are\n#  finetuning we will be updating all parameters. However, if we are\n#  doing feature extract method, we will only update the parameters\n#  that we have just initialized, i.e. the parameters with requires_grad\n#  is True.\n\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif feature_extract:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)    \n            \n# Observe that all parameters are being optimized\n#optimizer_ft = optim.Adam(params_to_update, lr=0.001)\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","2593e009":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']","81e87c19":"def train_model(model, dataloaders, criterion, max_lr, num_epochs=15,\n                weight_decay=0,opt_func=torch.optim.SGD, grad_clip=None, is_inception=False):\n    since = time.time()\n\n    val_acc_history = []\n    val_loss_history = []\n    \n    train_acc_history = []\n    train_loss_history = []\n    lrs = []\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.CyclicLR(optimizer, 1e-3, max_lr,\n                                                step_size_up=len(dataloaders['train']), cycle_momentum=False)\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:                \n                inputs = inputs.to(device)\n                labels = labels.to(device)                 \n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    # Get model outputs and calculate loss\n                    # Special case for inception because in training it has an auxiliary output. In train\n                    # mode we calculate the loss by summing the final output and the auxiliary output\n                    # but in testing we only consider the final output.\n                    if is_inception and phase == 'train':\n                        # From https:\/\/discuss.pytorch.org\/t\/how-to-optimize-inception-model-with-auxiliary-classifiers\/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)                    \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        # Record & update learning rate\n                        lrs.append(get_lr(optimizer))\n                        sched.step()\n                                        \n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                val_loss_history.append(epoch_loss)\n            if phase == 'train':\n                train_acc_history.append(epoch_acc)\n                train_loss_history.append(epoch_loss)\n        print()\n        \n        \n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    history = {'loss':train_loss_history, 'acc':train_acc_history, 'val_loss':val_loss_history, 'val_acc':val_acc_history, 'lr':lrs}\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model,history","34397c08":"epochs = 15\nmax_lr = 0.01\ngrad_clip = None  #0.1\nweight_decay = 0  #1e-4\nopt_func = torch.optim.Adam\ncriterion = F.cross_entropy\n# Train and evaluate\nmodel, history = train_model(model_ft, dataloaders_dict, criterion, max_lr, \n                             epochs,weight_decay, opt_func, grad_clip=None)","aab2bed4":"#getting all the predictions on the valid set\ndef eval_predict(model,dataloaders_dict, phase):\n    model.eval()\n    all_predict = torch.tensor([], device=device)\n    all_labels = torch.tensor([], device=device)\n    with torch.no_grad():\n        for inputs, labels in dataloaders_dict[phase]:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            all_predict = torch.cat((all_predict, preds), 0)\n            all_labels = torch.cat((all_labels, labels), 0)\n    return all_predict, all_labels\n\nphase = 'val'\ny_pred,y_true = eval_predict(model, dataloaders_dict, phase)\n\ny_pred = y_pred.cpu().numpy()\ny_true = y_true.cpu().numpy()\n\nprint('Y pred:', y_pred)\nprint('Y true:', y_true)","ed29f80e":"from sklearn.metrics import classification_report\ndef CLASS_Report(y_test, y_pred, labels):\n    class_report = classification_report(y_test, y_pred, target_names=labels)\n    return class_report\n\ndef extractClass(testXpred, classes):\n    Test = [] \n    for i in range(len(testXCount)):      \n        instance = testXCount[i]\n        label = testXpred[i]\n        instance.append(label)\n        Test.append(instance)\n    clusters = separate_by_class_pop_class(Test)   \n    testXpred  = []\n    for x in testXCount:\n        for clss in classes:\n            if clss in clusters:\n              if x in clusters[clss]:\n                  kmClusters = clusters[clss]                  \n        maxima = []\n        maximumLabelDict = {}\n        for value in kmClusters:\n            label = np.argmax(value)\n            maximum = value[label]\n            maxima.append(maximum)\n            maximumLabelDict[maximum] = label\n        Predictlabel = maximumLabelDict[max(maxima)]\n        testXpred.append(Predictlabel)\n    return testXpred","b83326ae":"labels = [\"Parasitized\", \"Uninfected\"]\nclass_report = CLASS_Report(y_true, y_pred, labels)\nprint(class_report)","ebb79779":"show_plots(history, plot_title=None, fig_size=None)\n","177370a0":"# Validate for other Metrics","cce37596":"# Importing Libraries","9490bbd2":"# Data Preprocessing","c478028d":"# Send the model to GPU","2f684e69":"# Run Epoch","2ee5d33b":"# Create path for data","19d4a623":"# Split Dataset into train\/test","450a0e97":"# Download the pretrained model","37dad585":"# Visualize train samples","59c02621":"# Set GPU","d06f5d1b":"# Getting Predictions on Validation Set","9d7f55ab":"# Initialize Model","9e098dc7":"# Training process","56135118":"# Define Feature Extraction"}}