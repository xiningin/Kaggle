{"cell_type":{"d4fe11d3":"code","a18ed137":"code","26b4d4ba":"code","b47e970a":"code","f9ca34c6":"code","e29da952":"code","bd68460e":"code","34c7dcb5":"code","e8e49318":"code","5bb39512":"code","666aa969":"code","7b95c2a3":"code","1e0e1810":"code","587d3d59":"code","79fec179":"code","8e40d906":"code","07371951":"markdown","df4bf522":"markdown","b1b98e6c":"markdown","0dc03e10":"markdown","6b8dd7f0":"markdown","fb22fd9c":"markdown","bf1bb05d":"markdown","e0c57316":"markdown","28266a90":"markdown","3b2b2abc":"markdown","0d740774":"markdown","19f3a67f":"markdown","952346d5":"markdown","b531f6b1":"markdown","bc4c1376":"markdown","ced1cf54":"markdown"},"source":{"d4fe11d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a18ed137":"import zipfile\n\nzip_files = ['test1', 'train']\nfor zip_file in zip_files:\n    with zipfile.ZipFile(\"\/kaggle\/input\/dogs-vs-cats\/{}.zip\".format(zip_file),\"r\") as z:\n        z.extractall(\".\")\n        print(\"{} unzipped\".format(zip_file))","26b4d4ba":"image_path = \".\/train\"\nfile_names = os.listdir(image_path)","b47e970a":"os.path.join(image_path, file_names[0])","f9ca34c6":"file_names[0].split(\".\")","e29da952":"targets = []\ntrain = []\n\nfor file_name in file_names:\n    target = file_name.split(\".\")[0]\n    trains = os.path.join(\".\/train\", file_name)\n    train.append(trains)\n    targets.append(target)\n    \n\ndata = pd.DataFrame()\ndata[\"image\"] = train\ndata[\"target\"] = targets\n\ndata.head()","bd68460e":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(data, test_size = 0.2)\n\n\n\n\nfrom keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True,\n                                   fill_mode = \"nearest\")\n\ntrain_generator = train_datagen.flow_from_dataframe(dataframe = train_data,\n                                                    x_col = \"image\",\n                                                    y_col = \"target\",\n                                                    target_size = (150, 150),\n                                                    class_mode = \"binary\",\n                                                    batch_size = 100)","34c7dcb5":"from keras.preprocessing import image\nimport matplotlib.pyplot as plt\n\nfnames = [os.path.join(image_path, fname) for fname in os.listdir(image_path)]\n\nimg_path = fnames[2]\n\nimg = image.load_img(img_path, target_size = (150, 150))\n\nx = image.img_to_array(img)\nx = x.reshape((1, ) + x.shape)\ni = 0\n\nfor batch in train_datagen.flow(x, batch_size = 1):\n    plt.figure()\n    imgplot = plt.imshow(image.array_to_img(batch[0]))\n    i += 1\n    if i % 4 == 0:\n        break\n\nplt.show()","e8e49318":"test_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_generator = test_datagen.flow_from_dataframe(dataframe = test_data,\n                                                 x_col = \"image\",\n                                                 y_col = \"target\",\n                                                 target_size = (150, 150),\n                                                 class_mode = \"binary\",\n                                                 batch_size = 100)","5bb39512":"from keras import layers\nfrom keras import models\nfrom keras import optimizers\n\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = (150, 150, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(512, activation = \"relu\"))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(loss = \"binary_crossentropy\",\n             optimizer = optimizers.RMSprop(lr = 1e-4),\n             metrics = [\"acc\"])\n\nmodel.summary()","666aa969":"history = model.fit_generator(train_generator,\n                             steps_per_epoch = 200,\n                             epochs = 100,\n                             validation_data = test_generator,\n                             validation_steps = 50)","7b95c2a3":"import matplotlib.pyplot as plt\n\nacc = history.history[\"acc\"]\nval_acc = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\n\nepochs = range(1, len(acc) + 1)\n\nplt.figure(figsize = (15, 6))\n\nplt.plot(epochs, acc, \"bo\", label = \"E\u011fitim Ba\u015far\u0131s\u0131\")\nplt.plot(epochs, val_acc, \"b\", label = \"Test Ba\u015far\u0131s\u0131\")\nplt.title(\"E\u011fitim ve Test Ba\u015far\u0131s\u0131\")\nplt.legend()\n\nplt.figure(figsize = (15, 6))\n\nplt.plot(epochs, loss, \"bo\", label = \"E\u011fitim Kayb\u0131\")\nplt.plot(epochs, val_loss, \"b\", label = \"Do\u011frulama Kayb\u0131\")\nplt.legend()\n\nplt.show()","1e0e1810":"os.path.join(image_path, file_names[0])","587d3d59":"img_path = \".\/train\/cat.1650.jpg\"\n\nimg = image.load_img(img_path, target_size = (150, 150))\nimg_tensor = image.img_to_array(img)\nimg_tensor = np.expand_dims(img_tensor, axis = 0)\nimg_tensor \/= 255\n\nplt.imshow(img_tensor[0])\nplt.show()\n","79fec179":"layer_output = [layer.output for layer in model.layers[:8]]\nactivation_model = models.Model(inputs = model.input, outputs = layer_output)\nactivations = activation_model.predict(img_tensor)\n\nfirst_layer_activation = activations[0]\n\nplt.matshow(first_layer_activation[0, :, :, 4], cmap = \"viridis\")","8e40d906":"layer_names = []\nfor layer in model.layers[:8]:\n    layer_names.append(layer.name)\n\nimages_per_row = 16\n\n\nfor layer_name, layer_activation in zip(layer_names, activations):\n   \n    n_features = layer_activation.shape[-1]\n\n    \n    size = layer_activation.shape[1]\n\n    \n    n_cols = n_features \/\/ images_per_row\n    display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n    \n    for col in range(n_cols):\n        for row in range(images_per_row):\n            channel_image = layer_activation[0,\n                                             :, :,\n                                             col * images_per_row + row]\n            \n            channel_image -= channel_image.mean()\n            channel_image \/= channel_image.std()\n            channel_image *= 64\n            channel_image += 128\n            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n            display_grid[col * size : (col + 1) * size,\n                         row * size : (row + 1) * size] = channel_image\n\n    \n    scale = 1. \/ size\n    plt.figure(figsize=(scale * display_grid.shape[1],\n                        scale * display_grid.shape[0]))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    \nplt.show()","07371951":"**CNN \u0130LE ANLA\u015eMAK**","df4bf522":"Daha sonra train datas\u0131n\u0131 liste \u015feklinde alal\u0131m ileride bu listeyi etiketleri belirken kullanaca\u011f\u0131z.","b1b98e6c":"Evet veri \u00e7e\u015fitlendirmesi yaparak modeli overfit etmekted kurtarm\u0131\u015f gibi g\u00f6z\u00fck\u00fcyoruz. Do\u011fruluk oran\u0131 train ve test setimiz i\u00e7inde %90'lara geldi. Burada \u00f6nemli bir nokta hem veri \u00e7e\u015fitlili\u011fi yap\u0131lmas\u0131 hemde model kurulurken maxpooling gibi parametre azaltma bir nevi modelin g\u00f6r\u00fcnt\u00fc \u00fczerinde daha geni\u015f \u00e7al\u0131\u015fmas\u0131n\u0131 sa\u011flama gibi etkenler oldu\u011fu a\u00e7\u0131k.\n\n\n\n\u015eimdi evri\u015fimli sinir a\u011flar\u0131 bir kara kutu mu de\u011fil mi buna bakal\u0131m. Pek \u00e7ok insan evri\u015fimli sinir a\u011flar\u0131n\u0131n arkas\u0131nda mant\u0131\u011f\u0131n bir sihir gibi g\u00f6r\u00fcyor fakat asl\u0131nda konu \u00f6yle de\u011fil tek bir filtre \u00e7\u0131kt\u0131s\u0131 \u00fczerinden eimden geldi\u011fince anlatmaya \u00e7al\u0131\u015faca\u011f\u0131m.","0dc03e10":"Evet datam\u0131z yukar\u0131dak gibi haz\u0131rland\u0131 ve bu kadar. G\u00f6rd\u00fc\u011f\u00fcn\u00fcz gibi resim \u00fczerine \u00e7al\u0131\u015f\u0131yor olsak da datam\u0131z asl\u0131nda resimlerin kaynaklar\u0131 yani nerede olduklar\u0131 bilgisi ve bu resimlerin ne olduklar\u0131 bilgisinden ibaret.\n\n\u0130kinci a\u015famada train i\u00e7indeki resimleri train ve validation olarak b\u00f6lme i\u015flemine geldi. Burada klasik makine \u00f6\u011frenmesinden i\u015flemi uyguluyorum ard\u0131ndan train datam\u0131n i\u00e7indeki resimleri \u00e7e\u015fitlendirmek i\u00e7in onlar\u0131 d\u00f6nd\u00fcrme, b\u00fcy\u00fctme, kayd\u0131rma gibi i\u015flemleri tabi tutuyorum.","6b8dd7f0":"Datay\u0131 haz\u0131rlamadan \u00f6nce datay\u0131 nas\u0131l \u00e7\u0131karaca\u011f\u0131z buna bakal\u0131m.\n\nData zip dosyas\u0131 i\u00e7inde bu nedenle \u00f6nce zip dosyas\u0131ndan \u00e7\u0131karmak gerekiyor. \u00c7\u0131kan zip dosyas\u0131 \u00e7al\u0131\u015fma sayfas\u0131ndaki output'un alt\u0131nda working klas\u00f6r\u00fc i\u00e7inde olacakt\u0131r.","fb22fd9c":"Modeli \u00e7al\u0131\u015ft\u0131rmak.","bf1bb05d":"Foto\u011fraf lisesinin bir \u00f6rne\u011fi","e0c57316":"\u015eimdi yukar\u0131daki ilk resim kedinin orjinal resmi ikinci resim ise ilk katmandaki 4. kanal\u0131n \u00e7\u0131kt\u0131s\u0131 burada bakmam\u0131z gereken yer ikinci resimdeki a\u00e7\u0131k ye\u015fil olan yerler bunlar asl\u0131nda birer dedekt\u00f6r. Konu asl\u0131nda \u015fu: a\u00e7\u0131k ye\u015fik yerlere bakt\u0131\u011f\u0131n\u0131z zaman g\u00f6rd\u00fc\u011f\u00fcn\u00fcz yerler kulaklar, burun \u00e7evresi, kedilerin etraf\u0131. Asl\u0131nda evri\u015fimli sinir a\u011flar\u0131nda herhangi bir sihir yok konu \u00f6zellikle ilk katmanlar i\u00e7in tamamen kenar k\u00f6\u015fe bulma i\u015fi. \u015eunu unutmamak gerekir bilgisayar hi\u00e7bir zaman resimleri bizim gibi g\u00f6remez resimleri onun i\u00e7in RGB kanallar\u0131ndaki piksel de\u011ferlerinden ibaret yani o resim de\u011fil say\u0131 g\u00f6r\u00fcyor! \n\nPeki say\u0131 g\u00f6rerek nas\u0131l kedi k\u00f6pek tespiti yapabiliryor?\n\n\u015eimdi yukar\u0131daki ilk resme bak\u0131n resmin b\u00fcy\u00fck \u00e7o\u011funlu\u011funu kediler kapl\u0131yor de\u011fil mi ve bu kediler asl\u0131nda birbirlerine benzer yani g\u00f6r\u00fcn\u00fc\u015fleri, boyutlar\u0131, hatta renkleri i\u015fte sihir(!) burada ba\u015fl\u0131yor. \u00d6rne\u011fin bilgisayar\u0131n resimdeki renk de\u011ferlerini (say\u0131lar\u0131) yukar\u0131dan a\u015fa\u011f\u0131ya okudu\u011funu varsayal\u0131m en yukar\u0131 beyaz bir renk var muhtemelen bu bir kanepe veya yast\u0131k yani bir obje haliyle renk de\u011ferleride ayn\u0131 resmin biraz a\u015fa\u011f\u0131s\u0131na indi\u011finde kediler ile kar\u015f\u0131la\u015f\u0131yor yani ba\u015fka bir obje ama bilgisayar onlar\u0131 g\u00f6rerek farketmiyor renk de\u011ferlerinin de\u011fi\u015fimi ile \u015f\u00f6yle bir yorum yap\u0131yor: \"Hmmm san\u0131r\u0131m buraya kadar ki her \u015fey ayn\u0131yd\u0131 fakat \u015fimdi buradan sonras\u0131 de\u011fi\u015fmeye ba\u015flad\u0131.\" Evet de\u011fi\u015fiyor \u00e7\u00fcnk\u00fc renk de\u011fi\u015fiyor, say\u0131lar de\u011fi\u015fiyor ve bilgisayar buna burada ba\u015fka bir nesne var diyerek cevap veriyor bu y\u00fczden kedilerin kulaklar\u0131 ve etraflar\u0131 a\u00e7\u0131k ye\u015fil yani dedekt\u00f6re tak\u0131l\u0131yor. Bunun gibi binlerce resim olunda bilgisayar bunlar\u0131 kolayca s\u0131n\u0131fland\u0131rabiliyor.\n\nDedekt\u00f6re tak\u0131lma yerleri, b\u00fcy\u00fckl\u00fckleri, kaba \u015fekilleri vs gibi. Onun g\u00fcn\u00fcn sonunda kedi veya k\u00f6pek bulmak gibi bir derdi yok onun derdi sadece birbirine benzer objeleri ayn\u0131 s\u0131n\u0131fa atmak bu kadar.","28266a90":"Resim \u00e7e\u015fitlendirmenin ne gibi bir faydas\u0131 var?\n\n> A\u015f\u0131r\u0131 uydurma, \u00f6\u011frenilecek \u00e7ok az veri oldu\u011funda modeli e\u011fitmenin yeni verilere genelle\u015ftirmemesini olarak ortaya \u00e7\u0131kar. Sonsuz veri oldu\u011funda modeliniz var olan t\u00fcm veri da\u011f\u0131l\u0131mlar\u0131n\u0131 ortaya \u00e7\u0131karabilir ve asla a\u015f\u0131r\u0131 uydurmaz. Veri seti \u00e7e\u015fitlendirme eldeki verilerin birtak\u0131m d\u00f6\u00fcn\u00fc\u015f\u00fcmler ile ger\u00e7e\u011fe yak\u0131n daha fazla e\u011fitim verisi olu\u015fturur. Ama\u00e7, e\u011fitim esnas\u0131nda modelinizi ayn\u0131 resmi ikinci kez g\u00f6rmemesidir. Bu modelinizin daha \u00e7ok \u00f6r\u00fcnt\u00fc aramas\u0131na ve daha iyi genelle\u015ftirmesini sa\u011flar\n                                                        \n                                                        Francois Chollet, Deep Learning with Python sf147\n","3b2b2abc":"T\u00fcm katmanlar\u0131n g\u00f6r\u00fcnt\u00fcs\u00fc\nKaynak:\n* [https:\/\/nbviewer.jupyter.org\/github\/fchollet\/deep-learning-with-python-notebooks\/blob\/master\/5.4-visualizing-what-convnets-learn.ipynb](http:\/\/)","0d740774":"Umar\u0131m bu \u00e7al\u0131\u015fma sizin i\u00e7in faydal\u0131 olmu\u015ftur.\n\nTekrar g\u00f6r\u00fc\u015fmek \u00fczere!","19f3a67f":"Orjinal Resim","952346d5":"Model i\u00e7in 4 katl\u0131 Conv2D haz\u0131rlad\u0131m. Her katmandan sonra maxpooling i\u015flemi yap\u0131yorum. Katmanalardaki filtreler 3x3'l\u00fck activasyon fonksiyonu relu'yu tercih ettim. Son dense katman\u0131nda tahmin de\u011ferlerim 0 ile 1 aras\u0131nda de\u011ferler alaca\u011f\u0131 i\u00e7in sigmoid fonksiyonunu kulland\u0131m. Compile a\u015famas\u0131nda kay\u0131p de\u011feri i\u00e7in ikili s\u0131n\u0131f oldu\u011fu i\u00e7in binarty_crossentropy kulland\u0131m. Model \u00e7\u0131kt\u0131s\u0131nda yakla\u015f\u0131k 3.5 mio parametre var.","b531f6b1":"\u015eimdi datay\u0131 haz\u0131rlamak i\u00e7in target ve train diye iki bo\u015f liste olu\u015fturuyorum. Burada targets'lara foto\u011fraf\u0131n kedi veya k\u00f6pek oldu\u011fu bilgisini verece\u011fim bunun i\u00e7in yukar\u0131daki file_names de\u011fi\u015fkeninden yararlanaca\u011f\u0131m. Train i\u00e7in ise sadece train klas\u00f6r\u00fc ve pe\u015fine resim ismini ekleyece\u011fim bu kadar.","bc4c1376":"train_generator isimli de\u011fi\u015fkende asl\u0131nda train datam\u0131n ne oldu\u011fu bu data i\u00e7irisindeki ba\u011f\u0131ml\u0131 ve ba\u011f\u0131ms\u0131z de\u011fi\u015fkenin ne oldu\u011funu resimlerin boyutlar\u0131n\u0131 ve bu resimlerin sadece iki \u00e7\u0131kt\u0131s\u0131 oldu\u011funu s\u00f6yl\u00fcyorum.\n\nBatch_size olarak 100 kulland\u0131m bu 20.000 elemanl\u0131 data \u00fczerinden 100'er gruplar halinde data al demek.Neden datan\u0131n tamam\u0131 de\u011filde sadece 100 tane? Asl\u0131dan datan\u0131n tamam\u0131n\u0131 kullanaca\u011f\u0131z fakat 100'l\u00fck gruplar halinde bu e\u011fitim i\u015flemimizin daha k\u0131sa s\u00fcrede ger\u00e7ekle\u015fmesini sa\u011flayacak elbette b\u00fcy\u00fck batch_size ile \u00e7al\u0131\u015fmak daha iyi model kurmam\u0131za yarar sa\u011flayabilir fakat performans olarak bu i\u015flem uzun s\u00fcrmesi muhtemeldir.\n\nResimlerde yapt\u011f\u0131m\u0131z de\u011fi\u015fikli\u011fi g\u00f6rmek i\u00e7in bir \u00f6rnek \u00e7izdirelim.","ced1cf54":"Merhaba, bu yaz\u0131mda kaggle i\u00e7erisinde me\u015fhur bir data olan Dogs vs Cats ile evri\u015fimli sinir a\u011flar\u0131na ait bir \u00f6rnek yapaca\u011f\u0131m.\n\nBurada hem bir evri\u015fimli sinir a\u011f\u0131 i\u00e7in data nas\u0131l haz\u0131rlan\u0131r hemde evri\u015fimli sinir a\u011flar\u0131 nas\u0131l \u00e7al\u0131\u015f\u0131r bunlar\u0131 inceleyece\u011fim.\n\nAyn\u0131 zamanda evri\u015fimli sinir a\u011flar\u0131n\u0131n bir kara kutu olup olmad\u0131\u011f\u0131n\u0131 elimden geldi\u011fince anlatmaya \u00e7al\u0131\u015faca\u011f\u0131m."}}