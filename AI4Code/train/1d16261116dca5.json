{"cell_type":{"902f3a51":"code","58f083c1":"code","1aee48ea":"code","43b9b0f3":"code","501df5ed":"code","eb8a2498":"code","b415d9e8":"code","1cb02920":"code","50692ef0":"code","acbbb2a4":"code","edce24ce":"code","bb04f26c":"markdown","cb6afda7":"markdown","c31411c8":"markdown","9e131021":"markdown","6659494f":"markdown","6dfbc46b":"markdown"},"source":{"902f3a51":"import math\nimport itertools\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import boxcox, kurtosis, skew\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error \nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.arima_model import ARIMAResults\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nimport plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight') \nplt.rcParams['xtick.labelsize'] = 20\nplt.rcParams['ytick.labelsize'] = 20\n\n%matplotlib inline","58f083c1":"df = pd.read_csv('..\/input\/candy_production.csv')\ndf = df.rename(columns={'observation_date': 'date', 'IPG3113N':'production'})\ndf.index = pd.DatetimeIndex(data= df.date)\ndf = df.drop(columns=['date'])\ndf.head()","1aee48ea":"df.plot(figsize=(20, 10), fontsize=20)\nplt.title('Candy Production from 1972-2017', fontsize=30)\nplt.show()","43b9b0f3":"df['bc_production'], lamb = boxcox(df.production)\ndf['df1_production'] = df['bc_production'].diff()\ndf['df_production'] = df['production'].diff()\nfig = plt.figure(figsize=(20,40))\n\nbc = plt.subplot(411)\nbc.plot(df.bc_production)\nbc.title.set_text('Box-Cox Transform')\ndf1 = plt.subplot(412)\ndf1.plot(df.df1_production)\ndf1.title.set_text('First-Order Transform w\/ Box-Cox')\ndf2 = plt.subplot(413)\ndf2.plot(df.df_production)\ndf2.title.set_text('First-Order Transform w\/o Box-Cox')\n\ndf.bc_production.dropna(inplace=True)\ndf.df1_production.dropna(inplace=True)\ndf.df_production.dropna(inplace=True)\n\nprint(f'Lambda Value {lamb}')","501df5ed":"f_acf = plot_acf(df['df_production'], lags=50)\nf_pacf = plot_pacf(df['df_production'], lags=50, method='ols')\nf_acf.set_figheight(10)\nf_acf.set_figwidth(15)\nf_pacf.set_figheight(10)\nf_pacf.set_figwidth(15)\nplt.show()","eb8a2498":"split_date = '2008-12-01'\ntrain = df['production'].loc[:split_date]\ntest = df['production'].loc[split_date:]\ntrain.plot(figsize=(20, 10), fontsize=20)\nplt.title('Candy Production Train\/Test Split', fontsize=30)\ntest.plot()\nplt.show()","b415d9e8":"model = SARIMAX(train, freq='MS', order=(1, 1, 1), seasonal_order=(1, 0, 0, 12))\nmodel_fit = model.fit(disp=False) ","1cb02920":"fcast_len = len(test)\nfcast = model_fit.forecast(fcast_len)\nmse = mean_squared_error(test, fcast)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(test, fcast)\nplt.figure(figsize=(20, 10))\nplt.title('Candy Production Forecast', fontsize=30)\nplt.plot(train, label='Train')\nplt.plot(fcast, label='Forecast')\nplt.plot(test, label='Test')\n\nprint(f'Mean Squared Error: {mse}')\nprint(f'Root Mean Squared Error: {rmse}')\nprint(f'Mean Absolute Error: {mae}')\nplt.legend(fontsize=25)\nplt.show()","50692ef0":"def rolling_forecast(train, test, order, season):\n    history = [x for x in train]\n    model = SARIMAX(history, order= order, seasonal_order= season)\n    model_fit = model.fit(disp=False)\n    predictions = []\n    results = {}\n    yhat = model_fit.forecast()[0]\n\n    predictions.append(yhat)\n    history.append(test[0])\n    for i in range(1, len(test)):\n        model = SARIMAX(history, order= order, seasonal_order= season)\n        model_fit = model.fit(disp=False)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        obs = test[i]\n        history.append(obs)\n    mse = mean_squared_error(test, predictions)\n    mae = mean_absolute_error(test, predictions)\n    rmse = math.sqrt(mse)\n    predictions = pd.Series(predictions, index=test.index)\n    results['predictions'] = predictions\n    results['mse'] = mse\n    results['rmse'] = rmse\n    results['mae'] = mae\n    return results","acbbb2a4":"rolling_fcast = rolling_forecast(train, test, (1, 1, 1), (1, 0, 0, 12))","edce24ce":"plt.figure(figsize=(20, 10))\nplt.title('Candy Production Rolling Forecast', fontsize=30)\nplt.plot(train, label='Train')\nplt.plot(rolling_fcast['predictions'], label='Forecast')\nplt.plot(test, label='Test')\n\nprint(f'Mean Squared Error: {rolling_fcast[\"mse\"]}')\nprint(f'Root Mean Squared Error: {rolling_fcast[\"rmse\"]}')\nprint(f'Mean Absolute Error: {rolling_fcast[\"mae\"]}')\nplt.legend(fontsize=25)\nplt.show()","bb04f26c":"<a id='measurement'><\/a>\n## Model Measurements\n---\n#### Scale-dependent Errors\nScale dependent errors are errors on the same scale as the data. \n    - Mean Squared Error (MAE): Popular and easy to understand\n    - Root Mean Squared Error (RMSE)\n    \n#### Percentage Errors\nPercentage errors are unit free and are frequently used to compare forecast performace between data sets. The percentage erros are typically some form of the estimated_value\/ true_value. The downside to percentage errors is that they can lead to infinite or undefined values when the true value is zero. Also, when data is without a meaningful zero, like temperature, the combination of division and then absolute value, like in MAPE, can lead to errors that don't capture the true difference. \n    - Mean absolute percentage error (MAPE)\n    - Symmetric MAPE: Deals with MAPE's tendency to put a heavier penalty on negative errors\n\n#### Scaled Errors\nScaled errors are an attempt to get around some of the problems with percentage errors. \n    - Mean absolute scaled error (MASE)","cb6afda7":"<a id='sarima'><\/a>\n# SARIMA\n---\n## Seasonality (S)\nIn our last kernel() we broke down the various parts of an ARIMA model. Now, we introduce *seasonality* with the addition of *S* in SARIMA. Seasonality is a common feature and deterministic source of non-stationarity that we have to account for in real life data. Where our ARIMA model had only three parameters to deal with, modelling seasonality will require 4 parameters by itself. \n\nThey are: \n1. P: Seasonal Autoregressive order\n2. D: Seasonal difference order\n3. Q: Seasonal moving average order\n4. m: The number of time steps for a single seasonal period. \n\nSpecified as $SARIMA(p,d,q)(P,D,Q)[m]$\n\nTo determine the parameter values for Seasonality we will again use the ACF\/PACF plots. Choosing values for each parameter is not a precise exercise, these are rules of thumb and it will take iterations to get the correct model parameters. \n\n#### Seasonal Period (m)\nOur seasonal period m, is the number of periods in each season. We can identify this from the ACF and PACF plot where our value m is equal to the lag with the greatest autocorrelation coefficient. The 0 value is always 1 because it has perfect correlation with the current timestep. If our data is seasonal we'd expect that the next most correlated value to the current timestep would be the equivalent point in the season exactly one season ago. The seasonal period value will also help us determine P and Q. \n\n#### Seasonal Autoregressive Order (P)\nOur seasonal autoregressive order can be thought about just like our autoregressive order except instead of finding the order of past timesteps that influence the value at the current timestep, we are looking for past timesteps on the order of seasons of m, the seasonal period. This is why the mth lag is also used to determine the value of P. If the lag m has a positive value then P should be >= 1. Otherwise P should be 0. We can fit the model with a value of 1 and increment as we see fit. \n\n#### Seasonal Difference Order (D)\nThe rule of thumb for our D parameter is that our series differencing and seasonal differencing should not be greater than 2. If our seasonal pattern is stable overtime then we can set D=1 and set D=0 if the seasonal pattern seems unstable. \n\n#### Seasonal Moving Average Order (Q)\nWe determine Q much like we determine P. If the lag m, is negative then Q >= 1- the opposite of how we fit P. We generally do not want P+Q to exceed 2. We want to keep our parameter values low because there is a high risk to overfit as we use increasingly complex models. ","c31411c8":"---\n# Modelling Seasonality in Candy Production with SARIMAX\n**[Nicholas Holloway](https:\/\/github.com\/nholloway)**\n\n---\n\n### Mission \nSeasonality is a deterministic source of non-stationarity common in real world data. We will build on the ARMA class of time series models by introducing SARIMA, a model that adds a seasonal-period based autoregressive and moving average term to model time series data with seasonality. We will go over each parameter and how each relates to their corresponding parameter in the ARIMA model. We will then use the *Box-Jenkins method* to fit our SARIMA model and introduce a *rolling forecast* to test our model accuracy. \n\n### Table of Contents:\n1. [SARIMA](#sarima)\n2. [Data Exploration](#exploration)\n3. [Parameter Estimation](#estimation)\n4. [Model Measurement](#measurement)\n5. [Model Evaluation](#evaluation)\n\n### Time Series Kernels: 3 of 4 \n* [Stationarity, Smoothing, and Seasonality](https:\/\/www.kaggle.com\/nholloway\/stationarity-smoothing-and-seasonality)\n* [Deconstructing ARIMA](https:\/\/www.kaggle.com\/nholloway\/deconstructing-arima)\n* [Seasonality and SARIMAX](https:\/\/www.kaggle.com\/nholloway\/seasonality-and-sarimax)\n* [Volatility Clustering and GARCH](https:\/\/www.kaggle.com\/nholloway\/volatility-clustering-and-garch)","9e131021":"<a id='evaluation'><\/a>\n## Model Evaluation\n---\nUnfortunately our model is not fitting the trend very well. The problem with the candy forecast data is that it has an unstable trend. What we could do next is decompose the trend from the rest of the time series with a Hodrick-Prescott Filter, a bandpass filter that was created for dealing with business cycles in economic data. \n\nInstead, we'll use a **rolling forecast**. A rolling forecast is where we forecast one step ahead and then refit our model on the new data, adding data from the test set. It's expensive because the model is refit every timestep but it allows us to forecast where a bad step will add to the overal error without effecting future forecasts. This means that early deviations from the time series because of the trend won't hurt our ability to forecast future steps. ","6659494f":"<a id='estimation'><\/a>\n## Parameter Estimation\n---\nAfter the box-cox transformation and differencing we no longer have a trend but the seasonality is still present and irregular. Becuase our lambda value is close to 1 (no transform) and the box-cox transformation didn't appear to make an important difference in the series, I've decided to drop it and the ACF and PACF plots reflect an untransformed series. We can use the ACF and PACF plots to determine the correct parameters for our SARIMA model. \n\nFirst, looking at the ACF we have a m value of 12, given that our greatest autocorrelation is at lag number 12, which makes since given our original plot and that this is monthly data. \n\nGiven that m is positive that would indicate that P = 1, and Q = 0. Looking at the ACF and PACF plots both have their first significant lag at 1. So p = 1, and q = 1. Because we differenced the series d will also be 1. So we have parameters of: \n`SARIMA(1,1,1)(1,0,0)[12]`\n\nWe will split our data into a training and test set and then fit our SARIMA model.","6dfbc46b":"<a id='exploration'><\/a>\n## Data Exploration \n---\nLooking at our time series there is an interesting trend that may be fixed with a square root transform if not for the large dip around 2007-2008. This trend is the result of business cycles and other exongenous variables that effect economic data. We can use a box-cox transformation to remove the trend. Next we can dif the time series to deal with trend and plot both that result, and the resulting ACF and PACF graph that it creates. \n\n#### Box Cox Transformation\nThe box cox transformation makes non-normal data normally distributed. We can pass it an argument for lambda to automatically perform a log transform, square root transform, or reciprocal transform. If we pass no argument for lambda it will tune automatically and return a lambda value. "}}