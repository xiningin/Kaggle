{"cell_type":{"5a66c542":"code","f299fa37":"code","7cc89c5f":"code","2def09e3":"code","c251a6a9":"code","c44dd328":"code","8c62cf48":"code","23b2b2da":"code","ba2df692":"code","e4a22669":"code","90580a15":"code","ef91f1d7":"code","80efa580":"code","f18ffee6":"code","54da1067":"code","b97ff8eb":"code","a80a9d41":"code","4a56d29a":"code","d6316794":"code","4792e3bc":"code","ad2c71f6":"code","59ecac6d":"code","f3f22653":"code","71c4a108":"code","d85755d5":"code","584c9526":"code","0418366f":"code","27055db2":"markdown","4e5a858b":"markdown","ccfc54bb":"markdown","0a5af3c9":"markdown","fba6cb75":"markdown","467147bf":"markdown","b7215c00":"markdown","0d5cb084":"markdown","c992ea41":"markdown","89d17ce4":"markdown","940eaf93":"markdown","9f8ac0ab":"markdown","a67d27f2":"markdown","2bf716dd":"markdown","d09449a6":"markdown","38f3866a":"markdown","6862d935":"markdown","04c47d6e":"markdown","fd08178e":"markdown","c215c91f":"markdown","bde01c12":"markdown","3c7256ce":"markdown","ce3d7fea":"markdown","cafa437b":"markdown","dd5980c4":"markdown"},"source":{"5a66c542":"#import libraries\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f299fa37":"#Read the data\n\ntrain = pd.read_csv('..\/input\/airline-passenger-satisfaction\/train.csv')","7cc89c5f":"#drop unnecesary column\n\ntrain = train.drop(columns = ['Unnamed: 0'] ,  axis = 1)","2def09e3":"#count number of targets\n\ntrain['satisfaction'].value_counts()   #Binary balanced classification","c251a6a9":"#Check number of missing values in numerical\n\nN_total = train.isnull().sum().sort_values(ascending = False)\nN_total # 310 NAs in [Arrival Delay in Minutes]","c44dd328":"#drop rows with na\ntrain = train.dropna().copy()\n\n#Checking\ntrain.isnull().sum()","8c62cf48":"#Create a sample\n\ndf1 = train.sample(frac = 0.1, random_state = 21).reset_index().drop(columns = ['index'], axis = 1)\ndf2 = train.sample(frac = 0.1, random_state = 22).reset_index().drop(columns = ['index'], axis = 1)\ndf3 = train.sample(frac = 0.1, random_state = 23).reset_index().drop(columns = ['index'], axis = 1)\ndf4 = train.sample(frac = 0.1, random_state = 24).reset_index().drop(columns = ['index'], axis = 1)\ndf5 = train.sample(frac = 0.1, random_state = 25).reset_index().drop(columns = ['index'], axis = 1)\ndf6 = train.sample(frac = 0.1, random_state = 26).reset_index().drop(columns = ['index'], axis = 1)\ndf7 = train.sample(frac = 0.1, random_state = 27).reset_index().drop(columns = ['index'], axis = 1)\ndf8 = train.sample(frac = 0.1, random_state = 28).reset_index().drop(columns = ['index'], axis = 1)\ndf9 = train.sample(frac = 0.1, random_state = 29).reset_index().drop(columns = ['index'], axis = 1)\ndf10 = train.sample(frac = 0.1, random_state = 30).reset_index().drop(columns = ['index'], axis = 1)","23b2b2da":"#function to add missing values\n\ndef add_nan(df,n_rows):\n    new_row = [np.nan for x in range(0,len(df.columns))]\n    for i in range(len(df),len(df) + n_rows):\n        df.loc[i] = new_row\n    \n    return df","ba2df692":"#Apply the function\n\ndf1 = add_nan(df1,50)\ndf2 = add_nan(df2,500)\ndf3 = add_nan(df3,250)\ndf4 = add_nan(df4,1230)\ndf5 = add_nan(df5,20)\ndf6 = add_nan(df6,120)\ndf7 = add_nan(df7,640)\ndf8 = add_nan(df8,2340)\ndf9 = add_nan(df9,5550)\ndf10 = add_nan(df10,710)","e4a22669":"#Calculate missing values as a percentage\n\nnan_1 = df1['Flight Distance'].isnull().sum()\/df1['Flight Distance'].count()\nnan_2 = df2['Flight Distance'].isnull().sum()\/df2['Flight Distance'].count()\nnan_3 = df3['Flight Distance'].isnull().sum()\/df3['Flight Distance'].count()\nnan_4 = df4['Flight Distance'].isnull().sum()\/df4['Flight Distance'].count()\nnan_5 = df5['Flight Distance'].isnull().sum()\/df5['Flight Distance'].count()\nnan_6 = df6['Flight Distance'].isnull().sum()\/df6['Flight Distance'].count()\nnan_7 = df7['Flight Distance'].isnull().sum()\/df7['Flight Distance'].count()\nnan_8 = df8['Flight Distance'].isnull().sum()\/df8['Flight Distance'].count()\nnan_9 = df9['Flight Distance'].isnull().sum()\/df9['Flight Distance'].count()\nnan_10 = df10['Flight Distance'].isnull().sum()\/df10['Flight Distance'].count()\n\n#To a pandas series\n\nmissing = pd.Series([nan_1,nan_2,nan_3,nan_4,nan_5,nan_6,nan_7,nan_8,nan_9,nan_10])","90580a15":"#Create a visualization\nplt.figure(figsize=(9, 5))\n\nviz = sns.lineplot(data = missing)\nviz.axhline(0.25, color='r')\n\nplt.xlabel('Measurements')\nplt.ylabel('Missing values')\nplt.title('Stadistical Control of missing values in Flight Distance')\nplt.show()","ef91f1d7":"#Define the variable to be monitorized\n\nFD_1 = df1['Flight Distance'].dropna()\nFD_2 = df2['Flight Distance'].dropna()\nFD_3 = df3['Flight Distance'].dropna()\nFD_4 = df4['Flight Distance'].dropna()\nFD_5 = df5['Flight Distance'].dropna()\nFD_6 = df6['Flight Distance'].dropna()\nFD_7 = df7['Flight Distance'].dropna()\nFD_8 = df8['Flight Distance'].dropna()\nFD_9 = df9['Flight Distance'].dropna()\nFD_10 = df10['Flight Distance'].dropna()","80efa580":"#function to add outliers\n\ndef add_outliers(variable,n_rows,outlier):\n    for i in range(len(variable),len(variable) + n_rows):\n        variable.loc[i] = variable.max()*outlier\n    return variable","f18ffee6":"#Add outliers\n\nFD_1 = add_outliers(FD_1,15,2)\nFD_2 = add_outliers(FD_2,15,2)\nFD_3 = add_outliers(FD_3,15,1)\nFD_4 = add_outliers(FD_4,15,2)\nFD_5 = add_outliers(FD_5,15,1)\nFD_6 = add_outliers(FD_6,15,2)\nFD_7 = add_outliers(FD_7,15,3)\nFD_8 = add_outliers(FD_8,15,2)\nFD_9 = add_outliers(FD_9,15,3)\nFD_10 = add_outliers(FD_10,15,3)\n\n#Calculate difference\n\ndif_1 = abs(FD_1.mean() - FD_1.median())\ndif_2 = abs(FD_2.mean() - FD_2.median())\ndif_3 = abs(FD_3.mean() - FD_3.median())\ndif_4 = abs(FD_4.mean() - FD_4.median())\ndif_5 = abs(FD_5.mean() - FD_5.median())\ndif_6 = abs(FD_6.mean() - FD_6.median())\ndif_7 = abs(FD_7.mean() - FD_7.median())\ndif_8 = abs(FD_8.mean() - FD_8.median())\ndif_9 = abs(FD_9.mean() - FD_9.median())\ndif_10 = abs(FD_10.mean() - FD_10.median())\n\n#Concatenate the results\n\noutliers_difference = [dif_1,dif_2,dif_3,dif_4,dif_5,dif_6,dif_7,dif_8,dif_9,dif_10]","54da1067":"#Create a visualization\nplt.figure(figsize=(9, 5))\n\nviz = sns.lineplot(data = outliers_difference)\n\nplt.xlabel('Measurements')\nplt.ylabel('Outliers detection')\nplt.title('Stadistical Control of outliers in Flight Distance')\nplt.show()","b97ff8eb":"df_1 = df1['Flight Distance'].dropna()\ndf_2 = df2['Flight Distance'].dropna()\ndf_3 = df3['Flight Distance'].dropna()\ndf_4 = df4['Flight Distance'].dropna()\ndf_5 = df5['Flight Distance'].dropna()\ndf_6 = df6['Flight Distance'].dropna()\ndf_7 = df7['Flight Distance'].dropna()\ndf_8 = df8['Flight Distance'].dropna()\ndf_9 = df9['Flight Distance'].dropna()\ndf_10 = df10['Flight Distance'].dropna()","a80a9d41":"def Kolmogorov(var1,var2):\n    \n    #Control the lenght error\n    a = len(var1)-len(var2)\n    \n    if a != 0:\n        raise ValueError(\"The length of the variables must be the same\")\n    \n    stat_test = stats.ks_2samp(var1, var2)\n    \n    p_value = stat_test[1]\n    \n    \n    return p_value","4a56d29a":"#Apply the function\n\np_value_1 = Kolmogorov(df_1,df_2)\np_value_2 = Kolmogorov(df_2,df_3)\np_value_3 = Kolmogorov(df_3,df_4)\np_value_4 = Kolmogorov(df_4,df_5)\np_value_5 = Kolmogorov(df_5,df_6)\np_value_6 = Kolmogorov(df_6,df_7)\np_value_7 = Kolmogorov(df_7,df_8)\np_value_8 = Kolmogorov(df_8,df_9)\np_value_9 = Kolmogorov(df_9,df_10)\n\n#Concatenate the results\n\np_values = pd.Series([p_value_1,p_value_2,p_value_3,p_value_4,p_value_5,p_value_6,p_value_7,p_value_8,p_value_9])","d6316794":"#plot the results to control the data\n\nplt.figure(figsize=(9, 5))\n\nviz = sns.lineplot(data = p_values)\nviz.axhline(0.05, color='r')\n\nplt.xlabel('Measurements')\nplt.ylabel('p values')\nplt.title('Stadistical Control of data distribution in Flight Distance')\nplt.show()","4792e3bc":"#Divide the data\n\nX = train.drop('satisfaction', axis=1)\nY = train['satisfaction']\n\n#dummies\n\nX['Gender'] = pd.get_dummies(X['Gender'])\nX['Customer Type'] = pd.get_dummies(X['Customer Type'])\nX['Type of Travel'] = pd.get_dummies(X['Type of Travel'])\nX['Class'] = pd.get_dummies(X['Class'])\n\n#Split the data\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)","ad2c71f6":" \n#Classifier\nrfc=RandomForestClassifier(random_state=65)\n\n#fit\nrfc.fit(X_train, Y_train)\n\n#prediction\npred=rfc.predict(X_test)","59ecac6d":"#Accuracy\n\nprint(\"Accuracy for Random Forest data: \",accuracy_score(Y_test,pred))","f3f22653":"#install Boruta library\n\n!pip install boruta","71c4a108":"from boruta import BorutaPy\n\n# Defining parameters of boruta object for feature selection\nfeature_selection = BorutaPy(rfc, n_estimators = 5, verbose = 1, \n                             random_state = 16, )\n\n# Get relevant features\nfeature_selection.fit(np.array(X), np.array(Y))","d85755d5":"# print the results based on the above picture\ngreen_area = X.columns[feature_selection.support_].to_list()\nblue_area = X.columns[feature_selection.support_weak_].to_list()\nprint('features in the green area:', green_area)\nprint('\\n')\nprint('features in the blue area:', blue_area)","584c9526":"# Creating list of varibale name, ranks, and final decision using zip\nfeature_rankings = list(zip(X.columns,\n                           feature_selection.ranking_,\n                           feature_selection.support_))","0418366f":"#Show the relevant variables\n\nfeature_rankings","27055db2":"![Boruta.jpg](attachment:8564e55e-adeb-4af0-8c33-1ed10f95a16e.jpg)","4e5a858b":"### Kolmogorov-Smirnov (K-S) test","ccfc54bb":"### Monitor missing values","0a5af3c9":"**Boruta** is an all relevant selection method which means that it tries to find all features carrying information usable for prediction. Other methods choose a subset of features on which some model has a minimal error and this means that your output depends on the model that you select rather than the useful information from your features.","fba6cb75":"It is correct to assume that the company wants to know if their passengers are satisfied with high accuracy, but, don't you think that they also want to know what are the variables that makes the model classify like that? The company wants to improve their service and it has decided to give you the responsibility of studying this problem and you not only are going to create a high quality model but you are also going to give them what are the model saying for each variable. This topic can be dealt with **Feature Importance**.\n\nIn machine learning we call **Feature Importance** to the score assignation to input features based on the importance to predict the output.","467147bf":"## Methods to control the data quality","b7215c00":"When the algorithm is applied, the features are divided into rejected, tentative and accepted like in this graphic:","0d5cb084":"# How to control your data and meet your client's requeriments \ud83e\udd1c\ud83c\udffb\ud83e\udd1b\ud83c\udffb","c992ea41":"This is the full version code of one article that I've written. If you want to read it, just follow this [link](https:\/\/medium.com\/@paalfer96\/how-to-control-your-data-and-meet-your-clients-requeriments-436401e6ee6b)","89d17ce4":"**Data drift:** In Machine Learning in Production we call data drift to a change in the distribution of the data **over the time**. As you can imagine, if your data is reaching data drift, your model must detect it and if it is possible, should deal with it. So, What can we do to detect data drift?","940eaf93":"Boruta algorithm is based on the classification of a RandomForest so it is relevant to ensure that this model classifies data correctly. In our example, we've reached 96% of accuracy with the test set, so we can confirm that RandomForest is a good model for this problem.","9f8ac0ab":"Imagine we are developing a model, which can be a .py file for example, and we want that our model works long time. Our main problem might be not to have good data quality **over the time**. These last words are our main issue to tackle because if we cannot ensure good data quality, our model is not bound to work.","a67d27f2":"To reduce the output size, I am going to center the study in the variable 'Flight Distance'.","2bf716dd":"Are different data being added to your model? Are your model studying data from the same flights?","d09449a6":"As we can see we haven't got any tentative feature (blue area). This implies that our features are accepted features or rejected features. So, let's make a ranking in order to check this information:","38f3866a":"This simple visualization helps to monitorize the data quality of the column 'Flight Distance' **over the time**.","6862d935":"This method takes more importance when we are trying to study if one variable is statistically identical to other. It is a test which gives the null hypothesis that both variables have the same data distribution, Thus, if we reject the null hypothesis we can conclude that there is a drift in the data.","04c47d6e":"The above visualization is telling us that the statistical properties of these variables, which are the same measured **over the time**, are identical. We can conclude that they were ingested correctly.","fd08178e":"In this subsection, we want to check if the variable Flight Distance presents outliers **over the time**. One idea can be to check if the difference between the median and the mean is increasing.","c215c91f":"### Monitor outliers","bde01c12":"In a real world data problem, we generally have to deal with loads of data but no all of this data are equally important for the prediction. It implies, in this business case, that we can be capable of telling the company what are the most important aspects which leads the model to predict that a passenger is satisfied with the services. ","3c7256ce":"Can your model automatically find out if the data contains information? How many misinformation are you going to permit?","ce3d7fea":"### Feature Importance: Boruta algorithm","cafa437b":"What we discover with this plot is that our data was measured good till the sixth and eighth measurement. Thus, our model would be assessing other flights and the output should not be considered as a result.","dd5980c4":"Let's assume that this dataset was measured **over the time**. So, we create a sample of it where each sample might be one run of our model or one measurement:"}}