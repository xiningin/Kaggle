{"cell_type":{"7ed69051":"code","e8b4b29f":"code","1d991eee":"code","c96ba31f":"code","4b324999":"code","7b9f7a21":"code","ff8eb785":"code","b0244332":"code","f7fcbfe8":"code","9eeaf1b9":"code","f20005f4":"code","8da48cd1":"code","f91f5a6f":"code","9087f0c1":"code","599b988f":"code","7d251bb6":"code","09e46ff3":"code","9bd37a62":"code","e4de825d":"code","27e21649":"code","589ddc30":"code","fb11c146":"code","b411a9e8":"code","9e7dac83":"code","13d92faa":"code","241146b5":"code","d29b3c8a":"code","bdbc9561":"code","4d35ffd2":"markdown","8b4ac14d":"markdown","6d2a2d22":"markdown","2a14b07e":"markdown","1488a42d":"markdown","3aa1d28e":"markdown","f5f5df89":"markdown","17557a6e":"markdown","516586b7":"markdown","39d0b2a2":"markdown","0d161844":"markdown","0155fea8":"markdown","6e49cd54":"markdown","c5bf1696":"markdown","d56935ed":"markdown","43e773c3":"markdown","fe9d93bc":"markdown","eb83d3fd":"markdown"},"source":{"7ed69051":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nfrom scipy import special #comb, factorial\nfrom scipy.stats import uniform\nfrom matplotlib import pyplot as plt\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest,chi2,f_classif\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler,LabelEncoder\nfrom scipy.stats import kstest\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, make_scorer, plot_confusion_matrix, confusion_matrix, accuracy_score,f1_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8b4b29f":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","1d991eee":"df['Class'] = df['Class'].map({1: 'Fraudulent',\n                               0: 'Legitimate'})","c96ba31f":"df.info()","4b324999":"nulls = df.isnull().sum()\nnulls[nulls>0]","7b9f7a21":"df['Class'].value_counts()","ff8eb785":"df.describe()[['Time','Amount']]","b0244332":"plt.style.use('seaborn-whitegrid')","f7fcbfe8":"dataframe = df\nfeature = 'Amount'\nsns.set_style('ticks')\nplt.figure(figsize=(10,7))\ndataframe[feature].hist()\nplt.title(f\"Distribution of the feature `{feature}`\",fontsize=25)\nplt.show()\n","9eeaf1b9":"dataframe = df\nfeature_1 = 'Amount'\nplt.figure(figsize=(7,7))\nsns.boxplot(y=feature_1, data=dataframe)\nplt.title(\"Distribution of `Amount`\",fontsize=20)\nplt.show()","f20005f4":"minnum = df['Amount'].min()\nprint(f'The minimum value in the column `Amount` is: {minnum}')","8da48cd1":"log_tr = np.log(df['Amount']+1)\nsq_root_tr = df['Amount']**0.5\nsq_cube_tr = df['Amount']**(1\/3)\nrecip_root = -1\/((df['Amount']+1)**0.5)\n","f91f5a6f":"fig, ax = plt.subplots(2,2,figsize=(10,10))\n\nax = ax.flatten()\n\n\nsns.distplot(log_tr,ax=ax[0])\nax[0].set_title('Logarithmic transormation of `Amount`', fontsize=15)\n\nsns.distplot(sq_root_tr,ax=ax[1])\nax[1].set_title('Square root transormation of `Amount`', fontsize=15)\n\nsns.distplot(sq_cube_tr,ax=ax[2])\nax[2].set_title('Cube root transormation of `Amount`', fontsize=15)\n\nsns.distplot(recip_root,ax=ax[3])\nax[3].set_title('Reciprocal root transormation of `Amount`', fontsize=15)\nplt.show()","9087f0c1":"index = ['Log','Square Root','Cube root','Inverse square']\nd_value = [kstest(log_tr,'norm')[0],\n            kstest(sq_root_tr,'norm')[0],\n           kstest(sq_cube_tr,'norm')[0],\n           kstest(recip_root,'norm')[0],\n          ]\n\npd.DataFrame(data={'D value': d_value},index=index)","599b988f":"df['Amount'] = -1\/((df['Amount']+1)**0.5)","7d251bb6":"fig, ax = plt.subplots(1,2,figsize=(12,7))\n\ndataframe = df\nfeature_1 = 'Class'\nfeature_2 = 'Amount'\n\nsns.boxplot(x=feature_1, y=feature_2, data=dataframe,ax=ax[0])\nax[0].set_title(f\"Distribution of `{feature_2}` conditional on `{feature_1}` (boxplot)\",fontsize=10)\n\n\ncat_feat = 'Class'\ncont_feat = 'Amount'\nfor value in df[cat_feat].unique():\n    sns.distplot(df[df[cat_feat] == value][cont_feat], label=value,ax=ax[1])\nplt.legend()\nax[1].set_title(f\"Distribution of `{feature_2}` conditional on `{feature_1}` (hist)\",fontsize=10)\nplt.show()\n","09e46ff3":"amount_x = df['Amount'].values.reshape(-1,1)\nclass_y = df['Class']\n\nprint('Null: Features `Amount` and `Class` independent. \\nThreshold: 0.05')\nprint(f'P-value: {f_classif(amount_x,class_y)[1][0]} ')\nprint(f'The p-value is smaller than 0.05: {(f_classif(amount_x,class_y)[1] < 0.05)[0]}')","9bd37a62":"#Histogram\ndataframe = df\nfeature = 'Time'\nsns.set_style('ticks')\nplt.figure(figsize=(10,7))\ndataframe[feature].hist()\nplt.title(f\"Distribution of the feature `{feature}`\",fontsize=25)\nplt.show()","e4de825d":"dataframe = df\ncat_feat = 'Class'\ncont_feat = 'Time'\nplt.figure(figsize=(7,7))\nfor value in df[cat_feat].unique():\n    sns.distplot(df[df[cat_feat] == value][cont_feat], label=value)\nplt.legend()\nplt.title(f\"Distribution of `{cont_feat}` conditional on `{cat_feat}`\")\nplt.show()\n","27e21649":"cols = df.drop(['Time','Class','Amount'],axis=1).columns\nfig, ax = plt.subplots(len(cols),1,figsize=(20,150))\nax = ax.flatten()\n\nfor i,feature in enumerate(cols):\n    sns.distplot(df[feature],ax=ax[i])\n    ax[i].set_title(f\"Distribution of {feature}\")\n    \n","589ddc30":"X,y = df.drop(['Class'],axis=1), df['Class']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3, random_state=11)\n\nsc = StandardScaler()\n\nX_train[['Time','Amount']] = sc.fit_transform(X_train[['Time','Amount']])\nX_test[['Time','Amount']] = sc.transform(X_test[['Time','Amount']])\n\n","fb11c146":"nb_clf = GaussianNB().fit(X_train,y_train)\nprint(classification_report(y_true=y_test, y_pred=nb_clf.predict(X_test)))\nplot_confusion_matrix(nb_clf, X_test, y_test)","b411a9e8":"log_random_state = None\nlog_clf = LogisticRegression(random_state=log_random_state).fit(X_train, y_train)\nprint(classification_report(y_true=y_test, y_pred=log_clf.predict(X_test)))\nplot_confusion_matrix(log_clf, X_test, y_test)","9e7dac83":"rf_clf = RandomForestClassifier(n_estimators=10, max_depth=10, random_state=13).fit(X_train, y_train)\n\n\nprint(classification_report(y_true=y_test, y_pred=rf_clf.predict(X_test)))\nplot_confusion_matrix(rf_clf, X_test, y_test)","13d92faa":"svm_clf = SVC(C=30).fit(X_train,y_train)\nprint(classification_report(y_true=y_test, y_pred=svm_clf.predict(X_test)))\nplot_confusion_matrix(svm_clf, X_test, y_test)","241146b5":"f1_sc = make_scorer(f1_score,label_pos = 'Fraudulent')\nmodels = np.array(['NB','Logistic','RF','SVM'])\nscore = np.array([f1_score(y_true=y_test,y_pred=nb_clf.predict(X_test),pos_label='Fraudulent'),\n        f1_score(y_true=y_test,y_pred=log_clf.predict(X_test),pos_label='Fraudulent'),\n        f1_score(y_true=y_test,y_pred=rf_clf.predict(X_test),pos_label='Fraudulent'),\n        f1_score(y_true=y_test,y_pred=svm_clf.predict(X_test),pos_label='Fraudulent')]\n)\n\n\nindex = np.argsort(score)\nmodels = models[index]\nscore = score[index]","d29b3c8a":"models1 = np.array(['NB','Logistic','RF','SVM'])\nscore1 = np.array([f1_score(y_true=y_test,y_pred=nb_clf.predict(X_test),pos_label='Fraudulent',average='macro'),\n        f1_score(y_true=y_test,y_pred=log_clf.predict(X_test),pos_label='Fraudulent',average='macro'),\n        f1_score(y_true=y_test,y_pred=rf_clf.predict(X_test),pos_label='Fraudulent',average='macro'),\n        f1_score(y_true=y_test,y_pred=svm_clf.predict(X_test),pos_label='Fraudulent',average='macro')]\n)\n\n\nindex = np.argsort(score1)\nmodels1 = models1[index]\nscore1 = score1[index]","bdbc9561":"fig, ax = plt.subplots(1,2,figsize=(15,4))\n\nplt.figure(figsize=(7,7))\ngraph = sns.barplot(models,score,ax=ax[0])\ngraph.set(ylim=(0, 1.1),title='f1 score (Positive label: Fraudulent)')\nfor p in graph.patches:\n    graph.annotate(round(p.get_height(),2), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')\n\n    \n    \ngraph = sns.barplot(models1,score1,ax=ax[1])\ngraph.set(ylim=(0, 1.1),title='f1 score (macro)')\nfor p in graph.patches:\n    graph.annotate(round(p.get_height(),2), (p.get_x()+0.4, p.get_height()),\n                   ha='center', va='bottom',\n                   color= 'black')","4d35ffd2":"# Training","8b4ac14d":"# kstest test for normality for each transformation","6d2a2d22":"Checking nulls","2a14b07e":"# Feature: Time","1488a42d":"That's a pretty extreme label disbalance","3aa1d28e":"Since `Time` and `Amount` are the only features that were not PCA-transformed, we will take a closer look at them.","f5f5df89":"We have just seen that the `Amount` is heavily skewed to the right. If we don't do anything with the shape, it might negatively affect performance of our models. Hence now we will try couple of trasformations. First let's check what is the minimum value","17557a6e":"# ANOVA test for independence between `Amount` and `Class`","516586b7":"Let's check what type of data we got in our dataset","39d0b2a2":"Checking the label distribution","0d161844":"# Taking look at the distributions of the PCA-transformed feaures","0155fea8":"We will remap the label for clarity.","6e49cd54":"# Normalizing feature `Amount`","c5bf1696":"# Let's start with amount","d56935ed":"We see that a lot of transformed features have quite long tails. Ideally, we'd want to reduce the skew. But since we have no idea what those features represent, we will just leave them unchanged.","43e773c3":"Let's check the conditional distributions of `Amount` based on `Class`","fe9d93bc":"# Import relevant libraries","eb83d3fd":"We see that inverse square transform minimizes $D$ value, hence this will be the transformation we will use."}}