{"cell_type":{"7ae60490":"code","659adb4b":"code","756aff02":"code","34b541f5":"code","6ec29c95":"code","3a18a0f6":"code","b2dcb1cd":"code","7c54b49f":"code","8db30011":"code","579804a6":"code","10d83bcb":"code","894d7bdf":"code","7f4fe3e4":"code","20dffa45":"code","1da99f3d":"code","f19e193f":"code","84dfef08":"code","e967def3":"code","49e4b1d2":"code","721532ae":"code","c7ad2092":"code","3984791e":"markdown","3b7f93d8":"markdown","aad0507a":"markdown","f1dca277":"markdown","8b3a9c65":"markdown","bd624f3e":"markdown","74df400e":"markdown","1a904b95":"markdown","0834d518":"markdown","adb0174d":"markdown","c0271c70":"markdown","2fb14180":"markdown","0e85ee1a":"markdown","919816ab":"markdown","3b3cd0af":"markdown","1c68be19":"markdown","276db420":"markdown","c00ee302":"markdown","61ccf444":"markdown","1f32436b":"markdown","26d7afd9":"markdown","3451529f":"markdown","5b2ded3f":"markdown","2e1083d3":"markdown","9e2ec344":"markdown","53f3e41d":"markdown","885b8f46":"markdown"},"source":{"7ae60490":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n","659adb4b":"df = pd.read_csv(\"..\/input\/pulsar_stars.csv\")","756aff02":"df.info()","34b541f5":"df.head()","6ec29c95":"x_data = df.drop([\"target_class\"], axis = 1)\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\ny = df.target_class.values\n\ncompareScore = []","3a18a0f6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n","b2dcb1cd":"def initializeWeightBias(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w, b\n\ndef sigmoidFunc(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head\n    ","7c54b49f":"def fwPropagation(w, b, x_train, y_train):\n    #forward\n    z = np.dot(w.T, x_train.T) + b\n    y_head = sigmoidFunc(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[0]\n    \n    #backward\n    deriWeight = (np.dot(x_train.T,((y_head-y_train.T).T)))\/x_train.shape[0] \n    deriBias = np.sum(y_head-y_train.T)\/x_train.shape[0]\n    graDescents = {\"deriWeight\" : deriWeight, \"deriBias\" : deriBias}\n    \n    return graDescents, cost\n    ","8db30011":"def update(iterNumber, w, b, x_train, y_train, learningRate) :\n    costList = []\n    index = []\n    for i in range(iterNumber + 1):\n        graDescents, cost = fwPropagation(w, b, x_train, y_train)\n        w = w - learningRate*graDescents[\"deriWeight\"]\n        b = b - learningRate*graDescents[\"deriBias\"]\n        \n        if i % 10 == 0:\n            costList.append(cost)\n            index.append(i)\n            print(\"Cost after {} iteration = {}\".format(i, cost))\n            \n    parameters = {\"weight\" : w, \"bias\" : b}\n    \n    return parameters, costList, index     ","579804a6":"def plotGraph(index, costList):\n    plt.plot(index, costList)\n    plt.ylabel(\"Cost\")\n    plt.show()\n    ","10d83bcb":"def predict(w, b, x_test):\n    z = np.dot(w.T, x_test.T) + b\n    y_head = sigmoidFunc(z)\n    yPrediction = np.zeros((1,x_test.shape[0]))\n    \n    for i in range(y_head.shape[1]):\n        if y_head[0,i] <= 0.5:\n            yPrediction[0,i] = 0\n        else:\n            yPrediction[0,i] = 1\n            \n    return yPrediction","894d7bdf":"def logisticRegression(x_train, y_train, x_test, y_test, iterNumber, learningRate):\n    dimension = x_train.shape[1]\n    w, b = initializeWeightBias(dimension)\n    parameters, costList, index = update(iterNumber, w, b, x_train, y_train, learningRate)\n    \n    predictionTest = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    \n    #printing errors\n    print(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(predictionTest - y_test)) * 100))\n    \n    plotGraph(index, costList)\n    \nlogisticRegression(x_train, y_train, x_test, y_test, iterNumber = 30, learningRate = 0.5)","7f4fe3e4":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\n\nlrScore = lr.score(x_test, y_test) * 100\ncompareScore.append(lrScore)\n\nprint(\"Test accuracy: {} %\".format(lrScore))","20dffa45":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=4)\n\nknn.fit(x_train, y_train)\nprint(\"Test accuracy: {}%\".format(knn.score(x_test,y_test)*100))\n","1da99f3d":"scoreList = []\nn = 15\nfor i in range(1,n):\n    knn2 = KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(x_train, y_train)\n    scoreList.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,n), scoreList)\nplt.ylabel(\"Accuracy rate\")\nplt.show()","f19e193f":"knn = KNeighborsClassifier(n_neighbors=8)\n\nknn.fit(x_train, y_train)\n\nknnScore = knn.score(x_test,y_test)*100\ncompareScore.append(knnScore)\n\nprint(\"Test accuracy: {}%\".format(knnScore))","84dfef08":"from sklearn.svm import SVC\nsvm = SVC(random_state=42, gamma = \"scale\")\n\nsvm.fit(x_train, y_train)\n\nsvmScore = svm.score(x_test,y_test)*100\ncompareScore.append(svmScore)\n\nprint(\"Test accuracy: {}%\".format(svmScore))","e967def3":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\n\nnb.fit(x_train, y_train)\n\nnbScore = nb.score(x_test,y_test)*100\ncompareScore.append(nbScore)\n\nprint(\"Test accuracy: {}%\".format(nbScore))","49e4b1d2":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state=42, max_depth=4)\n\ndt.fit(x_train, y_train)\n\ndtScore = dt.score(x_test, y_test)*100\ncompareScore.append(dtScore)\n\nprint(\"Test accuracy: {}%\".format(dtScore))","721532ae":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(random_state=42, n_estimators=10)\n\nrf.fit(x_train, y_train)\n\nrfScore = rf.score(x_test, y_test)*100\ncompareScore.append(rfScore)\n\nprint(\"Test accuracy: {}%\".format(rfScore))","c7ad2092":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nalgoList = [\"LogisticRegression\", \"KNN\", \"SVM\", \"NaiveBayes\", \"DecisionTree\", \"RandomForest\"]\ncomparison = {\"Models\" : algoList, \"Accuracy\" : compareScore}\ndfComparison = pd.DataFrame(comparison)\n\nnewIndex = (dfComparison.Accuracy.sort_values(ascending = False)).index.values\nsorted_dfComparison = dfComparison.reindex(newIndex)\n\n\ndata = [go.Bar(\n               x = sorted_dfComparison.Models,\n               y = sorted_dfComparison.Accuracy,\n               name = \"Scores of Models\",\n               marker = dict(color = \"rgba(116,173,209,0.8)\",\n                             line=dict(color='rgb(0,0,0)',width=1.0)))]\n\nlayout = go.Layout(xaxis= dict(title= 'Models',ticklen= 5,zeroline= False))\n\nfig = go.Figure(data = data, layout = layout)\n\niplot(fig)","3984791e":"<a id=\"6\"><\/a> <br>\n**Decision Tree Classification**","3b7f93d8":"* We can plot the graph for seeing decrease of cost if we will.","aad0507a":"<a id=\"2\"><\/a> <br>\n* Now, we'll do it with sklearn. ","f1dca277":"* Initializing parameters and creating sigmoid function","8b3a9c65":"* Train test split process","bd624f3e":"<a id=\"3\"><\/a> <br>\n**K-Nearest Neighbour (KNN) Classification**","74df400e":"* Comparison of models' accuracy","1a904b95":"<a id=\"8\"><\/a> <br>\n## Conclusion\nIn a nutshell, we wrote a bunch of supervised machine learning algorithms and we made really good predictions. Please upvote if you like and leave a comment.","0834d518":"* Updating parameters.  (Learning)","adb0174d":"This was a very good prediction too.","c0271c70":"%90 accuracy, not bad.","2fb14180":"## Introduction\n\nMost neutron stars are observed as pulsars. Pulsars are rotating neutron stars observed to have pulses of radiation at very regular intervals that typically range from milliseconds to seconds. Pulsars have very strong magnetic fields which funnel jets of particles out along the two magnetic poles. These accelerated particles produce very powerful beams of light.(NASA)\n\nIn this kernel we will try to write supervised machine learning algorithms to predict the class of pulsars.\n\n\n","0e85ee1a":"* And now, we are at beautiful part :)","919816ab":"1. [Logistic Regression](#1)\n1. [Logistic Regression with sklearn](#2)\n1. [KNN](#3)\n1. [SVM](#4)\n1. [Naive Bayes](#5)\n1. [Decision Tree](#6)\n1. [Random Forest](#7)\n1. [Conclusion](#8)\n","3b3cd0af":"<a id=\"7\"><\/a> <br>\n**Random Forest Classification**","1c68be19":"We got 98% which is the best by far. ","276db420":"<a id=\"5\"><\/a> <br>\n**Naive Bayes Classification**","c00ee302":"* Forward and backward propagation","61ccf444":"As you can see above, best value for best accuracy is n_neighbours = 8.","1f32436b":"* Predict process","26d7afd9":"* Setting x and y and normalize data.","3451529f":"<a id=\"4\"><\/a> <br>\n**Support Vector Machine (SVM) Classification**","5b2ded3f":"<a id=\"1\"><\/a> <br>\n**Logistic Regression**","2e1083d3":"* Importing libraries and data.","9e2ec344":"We got a good rate but we don't know whether this n_neighbours value(currently 4) providing best accuracy or not. \n\nSo we should figure it out.","53f3e41d":"This was a good accuracy too but this is a bit lower than the others. That means this classification method is not convenient for this dataset so much. ","885b8f46":"And %97 accuracy with sklearn logistic regression. It is very good rate. Let's look at other supervised learning algorithms, try to predict classes and see the accuracy of predictions."}}