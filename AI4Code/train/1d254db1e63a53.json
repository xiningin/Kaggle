{"cell_type":{"f78aa916":"code","336cfd6a":"code","5761f32f":"code","64e26803":"code","c158af9e":"code","aaf23a78":"code","cde9af9c":"code","7636f233":"code","137fa876":"code","e8b59327":"code","2f66065e":"code","9587056e":"code","6edc664d":"code","59eebc23":"code","0ebe9688":"code","3b7505cf":"code","cf3f8fd0":"code","70329eb4":"code","e77be67a":"code","2391a4ca":"code","ff664bb0":"code","6e9cf308":"code","95685fde":"code","05587694":"code","af8293be":"code","81e86fe6":"code","4b063e05":"code","343d1c1e":"code","8d362228":"code","80cc2da3":"code","ce5b94b0":"code","bee7774b":"code","b444eb56":"code","717aab95":"code","df1ea5d0":"code","25f598ff":"code","3746d65b":"code","ed5696ef":"code","49c5fb5e":"code","94625cab":"code","7cb005de":"code","a5b56ffd":"code","3fdf01ab":"code","c7ea81f6":"code","6a88a5aa":"code","4d5a5939":"code","ea39ec0c":"code","a4a6db16":"code","30ec7d36":"code","663e4dd0":"code","6c1126da":"code","1946ae21":"code","ee8684ef":"code","04b7eae6":"code","639414f8":"code","0892238b":"markdown","9f37d7c2":"markdown","15dbb8e7":"markdown","eeeca12b":"markdown","02051b87":"markdown","3ca6a783":"markdown","27ae7147":"markdown","0e7161fd":"markdown","fffc8d39":"markdown","f1df61f2":"markdown","22d6c8cc":"markdown","1b74de0e":"markdown","d37a6aea":"markdown","fbb93ce6":"markdown","3898ceb8":"markdown","87a537d2":"markdown","b2889fa2":"markdown","da57465d":"markdown","d47e2269":"markdown","2771594d":"markdown","3909039b":"markdown","10ae8da2":"markdown","d2434ad3":"markdown","85df38f5":"markdown","d4c74b19":"markdown","3898368a":"markdown","1dc2b6da":"markdown","716ba0c5":"markdown","a1f4e009":"markdown","987056f1":"markdown","ba50a774":"markdown","ed143902":"markdown","5c06cf24":"markdown","cb004388":"markdown","d4eefd9c":"markdown","c57dc41b":"markdown","da7d4658":"markdown","d8f1d25c":"markdown","ff3a15dc":"markdown","c33d2558":"markdown","4189c3ec":"markdown","9b325e82":"markdown","6337553b":"markdown","81d3eced":"markdown"},"source":{"f78aa916":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl > \/dev\/null 2>&1","336cfd6a":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom mlens.ensemble import SuperLearner\n\nfrom sklearn.manifold import TSNE\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom sklearn.metrics import roc_auc_score\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nfrom bokeh.layouts import row, column\nfrom bokeh.models import ColumnDataSource, CustomJS, Label,Range1d, Slider, Span\nfrom bokeh.plotting import figure, output_notebook, show\n\noutput_notebook()\n\n# sys.path.append('..\/input\/pytorchtabnet\/')\n# !pip install '..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl'\n\n# from pytorch_tabnet.tab_model import TabNetClassifier","5761f32f":"%%time\nfolder_path = '..\/input\/jane-street-market-prediction\/'\ntrain_data = dt.fread(folder_path + 'train.csv').to_pandas()\nfeatures = dt.fread(folder_path + 'features.csv').to_pandas()\nsample = dt.fread(folder_path + 'example_sample_submission.csv').to_pandas()\ntest_data = dt.fread(folder_path + 'example_test.csv').to_pandas()","64e26803":"print(\"{0}Number of rows in train data: {1}{2}\\n{0}Number of columns in train data: {1}{3}\".format(y_,r_,train_data.shape[0],train_data.shape[1]))\nprint(\"{0}Number of rows in test data: {1}{2}\\n{0}Number of columns in test data: {1}{3}\".format(m_,r_,test_data.shape[0],test_data.shape[1]))\nprint(\"{0}Number of rows in features data: {1}{2}\\n{0}Number of columns in features data: {1}{3}\".format(b_,r_,features.shape[0],features.shape[1]))\nprint(\"{0}Number of rows in sample data: {1}{2}\\n{0}Number of columns in sample data: {1}{3}\".format(c_,r_,sample.shape[0],sample.shape[1]))","c158af9e":"train_data.head()","aaf23a78":"features.head()","cde9af9c":"sample.head()","7636f233":"test_data.head()","137fa876":"print(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","e8b59327":"features = [f\"feature_{x}\" for x in range(130)]\nnull_count = train_data[features].isna().sum().sort_values(ascending=True)\nnull_count = null_count[null_count >0]\nfig = px.bar(y=null_count.index,x=null_count.values)\nfig.show()","2f66065e":"train_data.fillna(train_data.mean(),inplace=True)\nprint(\"Number of features with null values:\",np.sum(train_data.isna().sum()>0))","9587056e":"def distribution1(feature,color,df=train_data):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    sns.distplot(df[feature],color=color)\n    plt.subplot(122)\n    sns.violinplot(df[feature])\n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,df[feature].max(),g_,feature,r_,df[feature].min(),b_,feature,r_,df[feature].mean(),m_,feature,r_,df[feature].std()))","6edc664d":"distribution1('weight',color='yellow')","59eebc23":"print(\"Number of rows with weight zero:\",np.sum(train_data.weight==0))\nprint(\"Percentage of rows with weight zero\",np.sum(train_data.weight==0)\/len(train_data))","0ebe9688":"distribution1('resp',color='green')","3b7505cf":"print(\"Number of rows with resp zero:\",np.sum(train_data.resp==0))\nprint(\"Percentage of rows with resp zero\",np.sum(train_data.resp==0)\/len(train_data))","cf3f8fd0":"def utility_score(df,score=0.5,testing=False,true_percentage=0.8):\n    if testing:\n        df['action'] = 0\n        \n        false_percentage = 1 - true_percentage\n        mask1 = df['resp'] >= 0\n        mask2 = df['resp'] < 0\n        \n        df.loc[mask1,'action'] = score\n        \n        #make incorrect prediction on the positive resp \n        change_idx = df[mask1].sample(frac=false_percentage).index\n        df.loc[change_idx,'action'] = 0\n        \n        #make incorrect prediction on the negative resp \n        change_idx = df[mask2].sample(frac=false_percentage).index\n        df.loc[change_idx,'action'] = score\n        \n        df['pj'] = df['action'] * df['resp'] * df['weight']\n        pi = df[['date','pj']].groupby('date').agg({'pj':['sum']})\n        pi.columns = ['pi']\n        pi['pi_sq'] = pi['pi']**2\n        t = np.sum(pi['pi'])\/np.sqrt(np.sum(pi['pi_sq'])) * np.sqrt(250\/len(pi['pi'])) \n        u = min(max(t,0),6) * np.sum(pi['pi'])\n        \n        return u\n    \n    else:\n        df['pj'] = df['action'] * df['resp'] * df['weight']\n        pi = df[['date','pj']].groupby('date').agg({'pj':['sum']})\n        pi.columns = ['pi']\n        pi['pi_sq'] = pi['pi']**2\n        t = np.sum(pi['pi'])\/np.sqrt(np.sum(pi['pi_sq'])) * np.sqrt(250\/len(pi['pi'])) \n        u = min(max(t,0),6) * np.sum(pi['pi'])\n        \n        return u","70329eb4":"train_data['action'] =0\ntrain_data.loc[train_data['resp']>0.0,'action'] = 1\nfeatures = [f\"feature_{x}\" for x in range(130)]","e77be67a":"train_data.head()","2391a4ca":"scores = list()\nfor i in np.arange(0,1.05,0.05):\n    if i == 0:\n        score = utility_score(train_data,score=0.01,testing=True) \n    else:\n        score = utility_score(train_data,score=i,testing=True)\n    scores.append(score)\npx.scatter(x=np.arange(0.0,1.05,.05),y=scores)","ff664bb0":"scores = list()\nfor i in np.arange(0,1.05,0.05):\n    score = utility_score(train_data,score=1,testing=True,true_percentage=i)\n    scores.append(score)\n    \npx.scatter(x=np.arange(0.0,1.05,.05),y=scores)","6e9cf308":"train_data['pj'] = train_data['action'] * train_data['resp'] * train_data['weight']\npi = train_data[['date','pj']].groupby('date').agg({'pj':['sum']})\npi.columns = ['pi']\ndistribution1('pi','blue',df=pi)\nprint(\"sum of pi is\",np.sum(pi['pi']))","95685fde":"corr = train_data[features+['resp','resp_1','resp_2','resp_3','resp_4']].corr()\nfig = px.imshow(corr)\nfig.show()","05587694":"plt.figure(figsize=(10,10))\nsns.pairplot(train_data.loc[:10000,[f\"feature_{x}\" for x in range(18,39)]]);","af8293be":"def pca_plot1(features,n_components,target,nrows=10**4):\n    pca = PCA(n_components=n_components)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n\n    fig = px.scatter_matrix(\n        train_g_pca,\n        dimensions=range(n_components),\n        labels=labels,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values\n    )\n\n    fig.update_traces(diagonal_visible=True,opacity=0.5)\n    fig.show()","81e86fe6":"pca_plot1([f\"feature_{x}\" for x in range(0,130)],4,'resp')","4b063e05":"def pca_plot_3d(features,target,nrows=10**4):\n    pca = PCA(n_components=3)\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    train_g_pca = pca.fit_transform(train_d[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n\n    fig = px.scatter_3d(\n        train_g_pca,x=0,y=1,z=2,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=train_d[target].values,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n    )\n\n    fig.show()","343d1c1e":"pca_plot_3d([f\"feature_{x}\" for x in range(0,130)],'resp')","8d362228":"def plot_exp_var(features,nrows=10**4):\n    pca = PCA()\n    train_d = train_data.sample(n=nrows).fillna(train_data.mean())\n    pca.fit(train_d[features])\n    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\n    fig = px.area(\n        x=range(1, exp_var_cumul.shape[0] + 1),\n        y=exp_var_cumul,\n        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n    )\n    fig.show()","80cc2da3":"plot_exp_var([f\"feature_{x}\" for x in range(0,130)])","ce5b94b0":"distribution1('date','red');","bee7774b":"plt.figure(figsize=(15,10))\nplt.subplot(511)\nsns.kdeplot(train_data['resp_1'],color='#4285F4',shade=True,alpha=1)\nplt.subplot(512)\nsns.kdeplot(train_data['resp_2'],color='#EA4335',shade=True,alpha=1)\nplt.subplot(513)\nsns.kdeplot(train_data['resp_3'],color='#FBBC05',shade=True,alpha=1)\nplt.subplot(514)\nsns.kdeplot(train_data['resp_4'],color='#34A853',shade=True,alpha=1)\nplt.subplot(515)\nsns.kdeplot(train_data['resp'],color='#4285F4',shade=True,alpha=1);","b444eb56":"def distribution2(feature):\n    sns.distplot(train_data.loc[train_data['resp']>0,feature],label='negative resp')\n    sns.distplot(train_data.loc[train_data['resp']<0,feature],label='positive resp')\n    plt.legend()","717aab95":"temp = train_data.groupby('action')[[f\"feature_{x}\" for x in range(130)]].mean()\ndiff_mean = (temp.iloc[1] - temp.loc[0]).abs()\nhighest_diff_in_mean = diff_mean.sort_values(ascending=False)[:10].index.tolist()","df1ea5d0":"plt.figure(figsize=(20,15))\nfor i,feature in enumerate(highest_diff_in_mean):\n    plt.subplot(2,5,i+1)\n    distribution2(feature)","25f598ff":"px.bar(train_data['action'].value_counts(),color=['yellow','blue'],labels=['negative resp','positive resp'])","3746d65b":"distribution2('weight')","ed5696ef":"feature_df = train_data[features+['date']].groupby('date').mean()\ncolors = ['#8ECAE6','#219EBC','#023047','#023047','#023047','#0E402D','#023047','#023047','#F77F00','#D62828']\n\nplt.figure(figsize=(20,20))\nfor i in range(0,10):\n    plt.subplot(10,1,i+1)\n    ax = sns.lineplot(x=feature_df.index,y=feature_df[f'feature_{i}'],color=colors[i],lw=1)\nplt.show()","49c5fb5e":"plt.figure(figsize=(15,5))\nresp_date = train_data[['resp','date']].groupby('date').mean()\nsns.lineplot(x=[0,500],y=[0,0],lw=1.5)\nsns.lineplot(x=resp_date.index,y=resp_date['resp'],color='#dc2f02',lw=1);","94625cab":"resp_date['rolling'] = resp_date['resp'].rolling(10).mean()\nplt.figure(figsize=(15,5))\nsns.lineplot(x=[0,500],y=[0,0],lw=1.5)\nsns.lineplot(x=resp_date.index,y=resp_date['rolling'],color='#dc2f02',lw=1);","7cb005de":"dates = np.random.randint(low=0,high=500,size=10)\ncolors = ['#8ECAE6','#219EBC','#023047','#023047','#023047','#0E402D','#023047','#023047','#F77F00','#D62828']\nplt.figure(figsize=(20,20))\nfor i,date in enumerate(dates):\n    plt.subplot(10,1,i+1)\n    sns.lineplot(data=train_data.loc[train_data.date==date,'resp'],lw=1,color=colors[i])\n    plt.ylabel(date)\nplt.show()","a5b56ffd":"plt.style.use('ggplot')\nsample = train_data.resp[:10000]\nplt.figure(figsize=(15,7))\npd.plotting.autocorrelation_plot(sample,lw=1);","3fdf01ab":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups \/\/ n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]","c7ea81f6":"features = [f'feature_{x}' for x in range(130)] \n\ntarget_feature = 'action'\n\ntrain_mean = train_data.mean()\n\ntrain_data.fillna(train_mean,inplace=True)\ntrain_data['action'] = 0\ntrain_data.loc[train_data['resp']>0.0,'action'] = 1\n\ndates = train_data['date'].to_numpy()\n\ntarget = train_data[target_feature].to_numpy()\ntrain_data = train_data[features].to_numpy()","6a88a5aa":"params = {\n    'objective': 'binary',\n    'metrics':['auc'],\n}\n\nnfolds=3\ngroup_gap = 31","4d5a5939":"kfold = PurgedGroupTimeSeriesSplit(n_splits=nfolds,group_gap=group_gap)\nlgb_models = list()\n\nfor k ,(train_idx,valid_idx) in enumerate(kfold.split(train_data,target.ravel(),dates.ravel())): \n    \n    lgb_train = lgb.Dataset(train_data[train_idx],target[train_idx].ravel())\n    lgb_valid = lgb.Dataset(train_data[valid_idx],target[valid_idx].ravel())\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets = [lgb_train,lgb_valid],\n        num_boost_round = 10000,\n        verbose_eval = 50,\n        early_stopping_rounds = 10,\n    )\n    \n    lgb_models.append(model)","ea39ec0c":"def plot_feature_importance(model):\n    feature_importance = pd.DataFrame({\"feature\":features,\"importance\":model.feature_importance(importance_type='gain')})\n    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n    \n    plt.figure(figsize=(20,20))\n    sns.barplot(data=feature_importance,x='importance',y='feature')\n    plt.title(\"Importance type Gain\")\n    \n    for idx, v in enumerate(feature_importance.importance):\n            plt.text(v, idx, \"  {:.2e}\".format(v))","a4a6db16":"plot_feature_importance(lgb_models[0])","30ec7d36":"predictions = np.zeros(test_data.shape[0])\n\nfor model in  lgb_models:\n    predictions += model.predict(test_data[features])\n    \npredictions \/= len(lgb_models)\nsns.distplot(predictions);","663e4dd0":"del train_data, target\ngc.collect()","6c1126da":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","1946ae21":"%%time\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    prediction = 0\n    for model in lgb_models:\n        prediction += model.predict(test_df[features])[0]\n    \n    prediction \/= len(lgb_models)\n    prediction = prediction > 0.5\n    sample_prediction_df.action = prediction.astype(int)\n    env.predict(sample_prediction_df)","ee8684ef":"# env.predict(sample)","04b7eae6":"submission = pd.read_csv('.\/submission.csv')\nsubmission.head()","639414f8":"sns.countplot(submission.action);","0892238b":"## 4.4 Understanding Metrics \ud83d\udccf\ud83d\udcd0\n\nOne thing is clear that negative resp values contribute lowers the utility score and vice-verca.<br\/>\nso we need to make a model that gives resp values for the feature but<br\/>\nwhat should be the action value for the negative and positive value of resp let's try to understand that<br\/>\n\nLets make a function which make n% correct prediction i.e score 'v' for n% positive resp values and score 0 for <br\/>\nn% negative resp values and see what are utility scores.","9f37d7c2":"Even though these features have highest difference in mean of their resp.<br\/>\nit is difficult to see any difference in their graphs so it is difficult to seprate<br\/>\ndata using few features so we will need many features to make model<br\/>\n\nIn all the dataset that I have seen there is atleast one feature that you can see its distplot <br\/>\nand tell that there is a difference but this dataset is different.\n\nAnd almost every feature have mean value around zero which I think makes it easier for us<br\/>\nas data would not require any extra preprocessing.","15dbb8e7":"### 4.6 Pairplot for highly related features (18 to 39)","eeeca12b":"### 4.7.2 Ploting explained variance ","02051b87":"## 1. Given Data \ud83d\udcbd\n\nWe are provided with 4 files \n\n1. example_sample_submission.csv\n2. features.csv\n3. example_test.csv\n4. train.csv\n\ntrain.csv is the main file which contains feature_0 to feature_129 for each trade which is market data which we will use for making prediction and each trade has associated weight and resp which together represents a return on the trade.<br\/>\n\nIt also contain date column for day of trade. <br\/>\nts_id is for time ordering.<br\/>\n\nIt also has resp_1,2,3,4 for returns over different time horizons, but resps are not present in test data.\n\nfeatures.csv contains metadata of features bu no information about metadata is provided.\n\nexample_test shows how private test data will look like.\n\nexample_sample_submission is example of how submission file should look like.\n\nWe have to use api provided by kaggle to make the prediction.","3ca6a783":"### 4.1 Checking for Null Values","27ae7147":"## 4. Exploratory Data Analysis \ud83d\udcca\ud83d\udcc8\ud83d\udcc9\ud83d\udc40","0e7161fd":"## 4.11 Count of neg and pos resps in train_data","fffc8d39":"There are too many features with null values right now we will just fill those values with mean values.","f1df61f2":"### 4.4.2 plot utility score vs Accuracy","22d6c8cc":"## LGBM Model\n\nWe are going to make a baseline model which is simple classification model.<br\/>","1b74de0e":"## 4.15 Rolling mean of Mean Value of resp features over time","d37a6aea":"## Importing Libraries \ud83d\udcd7.","fbb93ce6":"## 3. Loading Data and Stats\ud83d\udcbd\n\ndf.fread().to_pandas() is faster than pd.read_csv()","3898ceb8":"## 4.12 Distplot of weight with neg and pos resps","87a537d2":"## 4.13 Mean Value of some features over time","b2889fa2":"## 4.17 White Noise Time Series Analysis\n\n**What is white noise ? **<br\/>\nwhite noise in time series is random number which can not be predicted.<br\/>\nWe can make model for the data if data is not white noise.<br\/>\n\nEvery signal consist of some form of white noise.<br\/>\ny(t) = signal + noise.\n\nWhite noise are series of variable which are independent and they have same variance.<br\/>\nthat means the value in present has no relation with value of past or future.<br\/>\n\nIf your signal is just white noise it means you can not model it they are just random numbers.<br\/>\n\nWhen a model makes prediction all the errors made by models are just white noise which means.<br\/>\nmodel has used all the signal available to make the prediction and all that left is white noise.<br\/>\n\nTime series is not white noise if following conditions are satisfied.<br\/>\n1. Mean is not-zero or near zero.\n2. Mean change over time\n3. Variance change over time\n4. Values correlate with lag values\n","da57465d":"Number of datapoints in negative and positive resps are almost same which is a good news<br\/>\nwe do not have to worry about training on unbalanced dataset. ","d47e2269":"## 4.16 Value of resp over 10 random days","2771594d":"### 4.5 Heatmap of features.","3909039b":"It is good to see that there is not much disturbance in number of trades per day<br\/>\nwe do see that number of transaction at the start from 0 to 50 is high.<br\/>\nThere might be some reason for that.<br\/>\nBut there is no information about the month or year the data was produced so it is hard to tell the reason<br\/>","10ae8da2":"## 4.14 Mean Value of resp features over time","d2434ad3":"\ud83d\ude06\ud83d\ude06\ud83d\ude06\nOh man EDA is fun.\nI mean look at it, It looks like those optical illusion images<br\/>","85df38f5":"You will only get a utility score if your prediction accuracy is higher than 50%.","d4c74b19":"One might find this one thing mentioned in the [data](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data) section is confusing.<br\/>\n\n\"Trades with weight = 0 were intentionally included in the dataset for completeness, although such trades will not contribute towards the scoring evaluation.\"<br\/>\n\nAs you can see there are many rows that has weight 0<br\/>\n\nDo we have to remove those rows while training ? \ud83e\udd37\u200d\u2640\ufe0f<br\/>\n\nAnswer: \nMaybe we can use it while training because we are predicting resp (or action) value.<br\/>\nbut while calculating utility score for validation we should be carefull that<br\/>\nthere are not so many values with weight zero because as we see that in metrics <br\/>\npi is \u2211j(weightij\u2217respij\u2217actionij) so 0 weight will not<br\/>\ncontribute in utility score.","3898368a":"## 4.8 Distribution of date \n\nDistribution of transaction per day","1dc2b6da":"## 2. Metrics \ud83d\udcd0\n\nFor each row in the test set we have to predict an action value 1 to make the trade and 0 to pass on it.\n\neach trade has associated weight and resp score which will be used to calculate utility score which is the final score.\n\nFor each i\n\npi= \u2211j(weightij\u2217respij\u2217actionij)\n\nt= \u2211pi\/\u2211p**2i * \u221a250|i|\n\ni is no of unique dates in test set.\n\nutitily = min(max(t,0),6)\u2211pi.","716ba0c5":"We can see that there is positive linear relation ship between action and score which is obvious given the linear metrics<br\/>\n\n**Note: this graph is just to see relationship of action vs score.<br\/>\nWe can only use values 0 or 1 for the action.**","a1f4e009":"## 4.7 Let's play with PCA","987056f1":"## PurgedGroupTimeSeriesSplit\u00b6\n","ba50a774":"### 4.2 Distribution of weights ","ed143902":"Threre are many features which are highly correlated<br\/>\nI think we should look into some highly correlated features to see their distribution and relation.\n\nI used plotly so that you can zoom into the feature.<br\/>\nYou can see that most of high correlated feature are near the center diagonal line<br\/>\nMost important patter is in the bottom right corner if you zoom into it you will see that<br\/>\nThere is alternate high and low correlations<br\/>\n\nAnother interesing one is 18 to 36 feature<br\/>\n\nBut there is no feature which is highly correlated to the resp value.","5c06cf24":"### 4.3 Distribution of resp","cb004388":"### 4.7.1 How good are 4 components of pca at seprating data","d4eefd9c":"## 4.9 Distribution of Resps","c57dc41b":"## 4.10 Distplot of some features\n\nI am going to plot distribution plot of those 10 features which has highest difference in mean<br\/>\nof data with negative resps and positive resps.","da7d4658":"### 4.1.1 Count of Null values in features \n\nzoom in to see the hidden features","d8f1d25c":"## Feature Importance","ff3a15dc":"### 4.17.1 Correlation with lag values.\n\nTo check that the values are correlated with lag values we will use pandas autocorrelation function.","c33d2558":"Acctually looking at it. I looks like white noise.<br\/>\nstraight grey line is 95% confidence bar.<br\/>\ndashed grey line is 99% confidence bar.<br\/>\n\nThere are few points which are crossing that line.<br\/>\nSo is the data just white noise or did I missed something.<br\/>","4189c3ec":"## Jane Streek Market Price Prediction \ud83d\udcc8\n\n![image](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23304\/logos\/thumb76_76.png?t=2020-11-16-17-41-20)\n\nToday's Trading system is lot depended on use of technology, market is evolving everyday and inorder to survive this dynamic nature of market one needs to use all the tools available to him\/her. <br\/>\n\nThe use of machine learning in market price prediction has been increased as machine learning models are very good at understading patterns and making predictions.\n\nMachine learning model along with human knowledge can help you to make proper market predictions.\n\n## what is this competition about \ud83d\udca1?\n\nHere the challange is to make a model which can predict wether to accept a trade(1) or reject a trade(0)<br\/>\ngiven the data. \n\nSo it is a classification problem.<br\/>\nbut not exactly as we can not submit probabilities.","9b325e82":"### 4.4.3 Distribution of pi - (sum(weight * action * resp)) if action is 0.5","6337553b":"### 4.4.1 Utility Score vs action with 80% accuracy for train data","81d3eced":"## Make Prediction"}}