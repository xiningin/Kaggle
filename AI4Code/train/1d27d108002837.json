{"cell_type":{"2002f623":"code","93e25695":"code","ceb0233f":"code","ad578260":"code","179d48cc":"code","0ee905a6":"code","aaba7934":"code","b9f7e0a9":"code","0154b0fd":"code","8979c535":"code","ee848fe9":"code","487ddedb":"code","62de8e4a":"code","1db00ab8":"code","ee0d7b77":"code","de1aa460":"markdown","2387baf4":"markdown","8b0ff05c":"markdown"},"source":{"2002f623":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nfrom sklearn import preprocessing\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93e25695":"fnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")","ceb0233f":"df.head()","ad578260":"df.shape","179d48cc":"labels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nlabels_df.head()","0ee905a6":"labels_df.shape","aaba7934":"df['target'] = 0","b9f7e0a9":"df.loc[df.Id.isin(labels_df.Id), 'target'] = 0\ndf.loc[~df.Id.isin(labels_df.Id), 'target'] = 1\ndf.head()","0154b0fd":"features = df.columns[1:-1]\ntrain = df[features].values\ntarget = df['target'].values","8979c535":"train, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()","ee848fe9":"train = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)","487ddedb":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.05,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 56,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","62de8e4a":"num_round = 2000\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","1db00ab8":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","ee0d7b77":"feature_imp.sort_values(by=\"Value\", ascending=False).head(20)","de1aa460":"The AUC of 0.72 is far from random, and seems that despite very close CV\/LB congreunce, there is still a nontrivial difference between the train and test sets. \n\nLet us now take a look at the most important features ","2387baf4":"Here it would appear that the worst \"culprits\" are the IC features. Let's again list the top 20 most important ones, at least according to this measure.","8b0ff05c":"So far in this competition there has been a farily good, albeit not perfect, concistency between the local CV scores and teh LB scores. It probably helps that the dataset seems to have been shuffled randomly. Nonetheless, it would be interesting to see if there are any significant differences between the train set and the test set. To that end, we'll resort to Adversarial Validation - a technique where we build a model to predict whether any given data point has come from the train or the test set.  "}}