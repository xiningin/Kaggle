{"cell_type":{"13c81c1a":"code","efcddebb":"code","ee0f15f5":"code","31c44a31":"code","7b4118a7":"code","045daeef":"code","181df190":"code","1199d60a":"code","f9ea67ec":"code","3c145697":"code","d6282c20":"code","f98ac425":"code","e4ba6225":"code","996995a4":"code","c5c25daa":"code","13acb431":"code","9f89a070":"code","f95732ac":"code","a3af61d6":"code","ba979086":"markdown","b393e8e0":"markdown","7ae79622":"markdown","4b66afa8":"markdown","eaacb4d0":"markdown","7a09e499":"markdown","56082ea3":"markdown","defaf3fc":"markdown","c9975c1f":"markdown","f00b7df2":"markdown","b82b5ce3":"markdown","05a3990f":"markdown","32cfa910":"markdown","71fea01f":"markdown","bb05330e":"markdown","c749b57a":"markdown","8ee3eb12":"markdown","47603601":"markdown"},"source":{"13c81c1a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","efcddebb":"def sigmoid(z, prime = None):\n    sigmoid = lambda t: 1.0\/(1.0 + np.exp(-t))\n    sigmoid_func = np.vectorize(sigmoid)\n    if not prime:\n        return sigmoid_func(z)\n    prime = lambda t: sigmoid_func(t) * (sigmoid_func(t) + -1)\n    prime_func = np.vectorize(prime)\n    return prime_func(z)","ee0f15f5":"def relu(z, prime = None):\n    relu = lambda t: t if t >= 0.0 else 0.0\n    relu_func = np.vectorize(relu)\n    if not prime:\n        return relu_func(z)\n    prime = lambda t: 1.0 if t >= 0.0 else 0.0\n    prime_vectorized = np.vectorize(prime)\n    return prime_vectorized(z)","31c44a31":"def linear(z, prime = None):\n    if not prime:\n        return z\n    return 1","7b4118a7":"def tanh(z, prime = None):\n    if not prime:\n        return np.tanh(z)\n    x = 1.0 - np.tanh(z)**2\n    return x","045daeef":"def leaky_relu(z, prime = None):\n    relu = lambda t: t if t > 0.0 else 0.01 * t\n    relu_func = np.vectorize(relu)\n    if not prime:\n        return relu_func(z)\n    prime = lambda t: 1.0 if t > 0.0 else 0.01\n    prime_func = np.vectorize(prime)\n    return prime_func(z)","181df190":"def L1(z, target = None, action = None): # Absolute error\n    if not action:\n        return z\n    elif action == \"prime\":\n        prime = lambda y, t: -1.0 if y < t else 1.0\n        prime_vectorized = np.vectorize(prime)\n        return prime_vectorized(z, target)\n    elif action == \"loss\":\n        return np.sum(np.absolute(z - target))\ndef L2(z, target = None, action = None): # Squared error\n    if not action:\n        return z\n    elif action == \"prime\":\n        return z - target\n    elif action == \"loss\":\n        return np.sum(np.square(z - target))","1199d60a":"class Layer:\n    def __init__(self, height, width, activation):\n        if activation == relu or activation == leaky_relu:\n            self.weight_matrix = np.random.rand(height, width) * np.sqrt(2\/width)\n        elif activation == tanh:\n            self.weight_matrix = np.random.rand(height, width) * np.sqrt(1\/width)\n        else:\n            self.weight_matrix = np.random.rand(height,width)\n        self.bias_matrix = np.zeros((height, width))\n        self.activation = activation\n    def forward_feed(self, input_matrix):\n        return self.activation(\n            np.dot(self.weight_matrix, input_matrix)) + np.sum(self.bias_matrix, axis=1)\n    def derivative(self, left_input, right_input):\n        return (self.weight_matrix.T * self.activation(np.sum(right_input, axis=0), 1)).T * left_input.T\n    def derivative_out(self, left_input, right_input, target_input):\n        return (self.weight_matrix.T * self.activation(right_input, target_input, \"prime\").T).T * left_input.T\n    def update_weights(self, derivative_matrix, learning_rate):\n        self.weight_matrix -= learning_rate * derivative_matrix\n        self.bias_matrix -= learning_rate * derivative_matrix","f9ea67ec":"class Network:\n    # Initialization function\n    def __init__(self, input_size, output_size):\n        self.layers = []\n        self.input_size = input_size\n        self.output_size = output_size\n        \n    # Add Layers\n    def add_out_layer(self, loss_function = L1):\n        l = Layer(self.output_size, self.layers[-1].weight_matrix.shape[0], loss_function)\n        self.layers.append(l)\n        \n    def add_layer(self, height, activation = relu):\n        if not self.layers:\n            width = self.input_size\n        else:\n            width = self.layers[-1].weight_matrix.shape[0]\n        input_height = self.layers\n        l = Layer(height, width, activation)\n        self.layers.append(l)\n    # Predict an input\n    def predict(self, input_matrix, return_all = False):\n        last_out = input_matrix\n        out_list = [input_matrix]\n        for layer in self.layers:\n            last_out = layer.forward_feed(last_out)\n            if return_all:\n                out_list.append(last_out)\n        if return_all:\n            return out_list\n        return last_out\n    # back_prop an (input, target) pair and update weights\n    def back_prop(self, input_matrix, target_matrix):\n        out_list = self.predict(input_matrix, 1)\n        derivative_matrix = self.layers[-1].derivative_out(out_list[-2], out_list[-1], target_matrix)\n        self.layers[-1].update_weights(derivative_matrix, self.learning_rate)\n        i = -3\n        for layer in reversed(self.layers[:-1]):\n            derivative_matrix = layer.derivative(out_list[i], derivative_matrix)\n            layer.update_weights(derivative_matrix, self.learning_rate)\n            i -= 1\n        return self.layers[-1].activation(out_list[-1], target_matrix, \"loss\")\n    def train(self, input_list, target_list, learning_rate = 0.1):\n        batch_loss = 0.0\n        self.learning_rate = learning_rate\n        for input_matrix, target_matrix in zip(input_list, target_list):\n            batch_loss += self.back_prop(input_matrix, target_matrix)\n        return batch_loss\n    def batch_train(self, input_list, target_list, learning_rate = 0.1):\n        self.learning_rate = learning_rate\n        loss_before = self.loss(input_list, target_list)\n        best_layer_model = self.layers\n        best_loss = self.layers[-1].activation(self.predict(input_list[0]), target_list[0], \"loss\")\n        \n        for input_matrix, target_matrix in zip(input_list[1:], target_list[1:]):\n            current_loss = self.back_prop(input_matrix, target_matrix)\n            if best_loss < current_loss:\n                self.layers = best_layer_model\n            else:\n                best_loss = current_loss\n                best_layer_model = self.layers\n            return loss_before - self.loss(input_list, target_list)\n    def loss(self, input_list, target_list):\n        loss = 0.0\n        for x, y in zip(input_list, target_list):\n            loss += self.layers[-1].activation(self.predict(x), y, \"loss\")\n        return loss","3c145697":"test_data = pd.read_csv(\"..\/input\/testSimple.csv\")\ntrain_data = pd.read_csv(\"..\/input\/trainSimple.csv\")","d6282c20":"n = Network(6, 2)","f98ac425":"n.add_layer(24, leaky_relu)\nn.add_layer(24, tanh)\nn.add_out_layer(L1)","e4ba6225":"from sklearn.utils import shuffle\nlearning_rate = 0.001\nbatch_size = 12\nepoch = 20\nprevious_total_loss = n.loss(train_data.values[:, :6], train_data.values[:, 6:])\nfor i in range(epoch):\n    x = shuffle(train_data.values)\n    improvement = 0\n    for batch in np.array_split(x, len(x)\/batch_size):\n        improvement += n.batch_train(batch[:, :6], batch[:, 6:], learning_rate)\n    epoch_loss = n.loss(train_data.values[:, :6], train_data.values[:, 6:])\n    print(\"MAE loss for epoch:\" + str(i) + \" = \" + str(epoch_loss))\n    print(\"Improvement = \" + str(improvement))\ncurrent_total_loss =n.loss(train_data.values[:, :6], train_data.values[:, 6:])\nprint(\"Current Total Loss = \" + str(current_total_loss))\nprint(\"Total Improvement(MAE) = \" + str(previous_total_loss - current_total_loss))","996995a4":"sample_data = shuffle(train_data)\ntrain_inputs = sample_data.iloc[:, :6].values\ntrain_targets = sample_data.iloc[:, 6:].values\ndata_number = 0\nfor train_input, train_target in zip(train_inputs[:10], train_targets[:10]):\n    data_number += 1\n    print(\"Sample Data \" + str(data_number))\n    print(\"Prediction:\")\n    prediction = n.predict(train_input)\n    print(prediction)\n    print(\"Target:\")\n    print(train_target)\n    print(\"L1 loss:\")\n    print(L1(prediction, train_target, \"loss\"))","c5c25daa":"test = pd.read_csv(\"..\/input\/testSimple.csv\")","13acb431":"test.tail()","9f89a070":"predicted_list = []\nfor row in test.values:\n    predicted_list.append(np.append(row[0], n.predict(row[1:])))\npredicted_df = pd.DataFrame(predicted_list, columns = ['ID', 'A', 'B'])\npredicted_df['ID'] = predicted_df['ID'].astype(int)","f95732ac":"predicted_df.head()","a3af61d6":"predicted_df.to_csv('submission.csv', index=False)","ba979086":"## Storing Training Data to Dataframes","b393e8e0":"## Activation Functions","7ae79622":"### Leaky Relu\n* Pros\n    * Never saturate\n    * Computationally efficient\n    * Acceleration is quick\n    * Never die\n* Cons\n    * Relu is generraly more preferrable, but if you have lots of negative valued data, then you may use leaky Relu.","4b66afa8":"### Sigmoid\n* Pros\n    * Derivative is simple: z * (1 - z)\n    * For artificial networks it is preferable\n* Cons\n    * Not zero centered\n    * Saturate and kill gradients at either 0 or 1\n    * Computationally expensive because of exp()\n    * Not generally prefered in deep neural networks\n    ","eaacb4d0":"### Network Class\n* The method: \"init\" method is used to initalize a network. You should enter input_size and output_size that matches your data\n* The method: \"add_out_layer\" is used to add the output layer. You can chose either L1 or L2 loss.\n* The method: \"add_layer\" is used to add an hidden layer to network provided the node size(height). You can use any activation function provided above.\n* The method: \"predict\" is used to run the network for a given input. Returns the predicted out. If the argument: \"return_all\" is true then returns [input + hidden outs + out] as a list\n* The method: \"back_prop\" is used to run back_propagation algorithm provided the input and desired output. Automaticaly updates the weights.\n* The method: \"train\" is just calls back_prop for every item in lists \"input_list\" and \"target_list\"\n* The method: \"batch_train\" is same as train, but only updates the best loss.\n* The method: \"loss\" returns the loss of a input and target pair.","7a09e499":"* L1 loss = | predicted - target |\n* L2 loss = (predicted - target)^2","56082ea3":"## Lets use our trained network on test data","defaf3fc":"### Rectified Linear Unit\n* Pros\n    * Computationally efficient\n    * Acceleration is quick\n    * \n* Cons\n    * Can irreversibly die during training\n    * Not zero centered\n    * Very popular on deep neural networks","c9975c1f":"### Hyperbolic Tangent\n* Pros\n    * Zero centered\n* Cons\n    * Kills gradient at -1 or 1","f00b7df2":"## Initializing the Network\n6 is the input size and 2 is the output size","b82b5ce3":"### Layer Class\nLayer class will be used to initialize hidden layers & output layer in the network.\n    \n* The variable: \"weight_matrix\" is a [node size X input size] numpy array. Every node has number of weights that is equal to input size(fully connected layer).\n* The method  : \"forward_feed\" returnes the output of a layer by using the input_matrix.\n* The method  : \"derivative\" used to calculate derivative of a layer by using the previous layers output and next layers derivative.\n* The method  : \"derivative_out\" is same as derivative but only used for outlayer.\n* The method  : \"update_weights\" is used to update the weight of a layer by using the derivative_matrix calculated by the method \"derivative or derivative_out\" and learning rate.","05a3990f":"## Training the Network\nWhat that code does is that;\n    \n* Runs train algorithm on whole data for 20 times(epoch).\n* Every epoch data is a randomly shuffled train data.\n* For every 12 [input, target] pair(batches), only the best loss provided by the backpropagation algorithm is used to update weights.","32cfa910":"### Linear\n* Pros\n    * Can be used for real valued outputs(no restriction)\n* Cons\n    * Cannot be used to make complex networks","71fea01f":"## Network Components","bb05330e":"## Analyzing Predictions on 10 random data","c749b57a":"## Lets download the predictions on test data","8ee3eb12":"## Adding layers\nYou can layers as you like but dont forget to add an out layer(L1 or L2 loss may be used. The format is : add_layer( node size, activation function)","47603601":"## Loss Functions"}}