{"cell_type":{"4e7f1363":"code","920375da":"code","83c91153":"code","83f34cfc":"code","a569cd6a":"code","f3505738":"code","0130647c":"code","82f9329e":"code","e78d3816":"code","992bce49":"code","adc0922d":"markdown"},"source":{"4e7f1363":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndf = pd.read_csv(\n    '..\/input\/price-volume-data-for-all-us-stocks-etfs\/Stocks\/a.us.txt',\n    usecols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume'],\n    index_col = 'Date', \n    parse_dates = [0]\n)\n\ndisplay(df)","920375da":"for i in {1,3,7}:\n    df.loc[:, 'Value before %i days' % i] = df.loc[:, 'Close'].shift(i)\n    df.loc[:, 'Diff since %i days' % i] = df.loc[:, 'Close'].diff(i)\n\ndf = df.dropna()\n\ndisplay(df)\n\ny_column = 'Close'\n\nX_train = df[:'2010'].drop([y_column, 'Open', 'Low', 'High'], axis = 1)\ny_train = df.loc[:'2010', y_column]\n\nX_test = df['2011':].drop([y_column, 'Open', 'Low', 'High'], axis = 1)\ny_test = df.loc['2011':, y_column]\n\ndisplay(X_train, y_train, X_test, y_test)","83c91153":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(X_train)\n\nX_train_unscaled = X_train.copy()\nX_test_unscaled = X_test.copy()\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","83f34cfc":"from sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import PassiveAggressiveRegressor\nfrom sklearn.linear_model import SGDRegressor\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import TimeSeriesSplit\n\nimport matplotlib.pyplot as plt\n\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('NN', MLPRegressor(solver = 'lbfgs', max_iter = 10000)))  #neural network\nmodels.append(('KNN', KNeighborsRegressor())) \nmodels.append(('RF', RandomForestRegressor(n_estimators = 10))) # Ensemble method - collection of many decision trees\nmodels.append(('SVR', SVR(gamma='auto'))) # kernel = linear# Evaluate each model in turn\nmodels.append(('SGD', SGDRegressor(shuffle = False)))\nmodels.append(('PA', PassiveAggressiveRegressor()))\n\nresults = []\nnames = []\n\nfor name, model in models:\n # TimeSeries Cross validation\n tscv = TimeSeriesSplit(n_splits=10)\n    \n cv_results = cross_val_score(model, X_train, y_train, cv=tscv, scoring='r2')\n results.append(cv_results)\n names.append(name)\n print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n    \n# Compare Algorithms\nplt.boxplot(results, labels=names)\nplt.title('Algorithm Comparison')\nplt.show()","a569cd6a":"from sklearn.metrics import make_scorer\n\ndef rmse(actual, predict):\n    \n    predict = np.array(predict)\n    actual = np.array(actual)\n    \n    distance = predict - actual\n    square_distance = distance ** 2\n    mean_square_distance = square_distance.mean()\n    \n    score = np.sqrt(mean_square_distance)\n    \n    return score\n\nrmse_score = make_scorer(rmse, greater_is_better = False)","f3505738":"import sklearn.metrics as metrics\n\ndef regression_results(y_true, y_pred):\n    \n    # Regression metrics\n    explained_variance=metrics.explained_variance_score(y_true, y_pred)\n    mean_absolute_error=metrics.mean_absolute_error(y_true, y_pred) \n    mse=metrics.mean_squared_error(y_true, y_pred) \n    mean_squared_log_error=metrics.mean_squared_log_error(y_true, y_pred)\n    median_absolute_error=metrics.median_absolute_error(y_true, y_pred)\n    r2=metrics.r2_score(y_true, y_pred)\n    \n    print('explained_variance: ', round(explained_variance,4))    \n    print('mean_squared_log_error: ', round(mean_squared_log_error,4))\n    print('r2: ', round(r2,4))\n    print('MAE: ', round(mean_absolute_error,4))\n    print('MSE: ', round(mse,4))\n    print('RMSE: ', round(np.sqrt(mse),4))","0130647c":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\n\nmodel = RandomForestRegressor()\nparam_search = { \n    'n_estimators': [20, 50, 100],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [i for i in range(5,15)]\n}\n\ntscv = TimeSeriesSplit(n_splits = 10)\ngsearch = GridSearchCV(estimator = model, cv = tscv, param_grid = param_search, scoring = rmse_score, n_jobs = 4)\n\ngsearch.fit(X_train, y_train)\nbest_score_rf = gsearch.best_score_\nbest_model_rf = gsearch.best_estimator_\n\ndisplay(gsearch.cv_results_)\n\ny_true = y_test.values\ny_pred_rf = best_model_rf.predict(X_test)\nregression_results(y_true, y_pred_rf)","82f9329e":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\n\nmodel = SGDRegressor(shuffle = False)\nparam_search = {\n    'alpha': 10.0 ** -np.arange(1, 7),\n    'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n    'penalty': ['l2', 'l1', 'elasticnet'],\n    'learning_rate': ['constant', 'optimal', 'invscaling'],\n}\n\ntscv = TimeSeriesSplit(n_splits = 10)\ngsearch = GridSearchCV(estimator = model, cv = tscv, param_grid = param_search, scoring = rmse_score, n_jobs = 4)\n\ngsearch.fit(X_train, y_train)\nbest_score_sgd = gsearch.best_score_\nbest_model_sgd = gsearch.best_estimator_\n\ndisplay(gsearch.cv_results_)\n\ny_true = y_test.values\ny_pred_sgd = best_model_sgd.predict(X_test)\nregression_results(y_true, y_pred_sgd)","e78d3816":"plt.title('error RF & SGD')\nplt.plot(y_test.index, y_true - y_pred_rf, color = 'g', alpha = 0.5, label='RF')\nplt.plot(y_test.index, y_true - y_pred_sgd, color = 'r', alpha = 0.5, label='SGD')\nplt.show()\n\nplt.title('prediction RF vs truth')\nplt.plot(y_test.index, y_true, color = 'g', alpha = 0.5, label='RF')\nplt.plot(y_test.index, y_pred_rf, color = 'r', alpha = 0.5, label='SGD')\nplt.show()\n\nplt.title('error RF')\ny_pret_rf = best_model_rf.predict(X_train)\nplt.plot(y_train.index, y_train.values - y_pret_rf, color = 'b', label='SGD train')\nplt.plot(y_test.index, y_true - y_pred_rf, color = 'r', label='SGD test')\nplt.show()\n\nplt.title('error SGD')\ny_pret_sgd = best_model_sgd.predict(X_train)\nplt.plot(y_train.index, y_train.values - y_pret_sgd, color = 'b', label='SGD train')\nplt.plot(y_test.index, y_true - y_pred_sgd, color = 'r', label='SGD test')\nplt.show()","992bce49":"imp_rf = best_model_rf.feature_importances_\nimp_sgd = best_model_sgd.coef_\nindices_rf = np.argsort(imp_rf)\nindices_sgd = np.argsort(imp_sgd)\n\nfeatures = X_train_unscaled.columns\n\nplt.title('Feature Importances RF')\nplt.barh(range(len(indices_rf)), imp_rf[indices_rf], color='b', alpha = 0.5, align='center')\nplt.yticks(range(len(indices_rf)), [features[i] for i in indices_rf])\nplt.xlabel('Relative Importance')\nplt.show()\n\nplt.title('Feature Importances SGD')\nplt.barh(range(len(indices_sgd)), imp_sgd[indices_sgd], color='r', alpha = 0.5, align='center')\nplt.yticks(range(len(indices_sgd)), [features[i] for i in indices_sgd])\nplt.xlabel('Relative Importance')\nplt.show()\n\n","adc0922d":"https:\/\/towardsdatascience.com\/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1"}}