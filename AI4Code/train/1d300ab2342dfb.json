{"cell_type":{"9e439347":"code","6cb7deb1":"code","cbdf2645":"code","e2cecc03":"code","6e5fbe0e":"code","8912220d":"code","3c85899a":"code","b4d57320":"code","c184c640":"code","9f1e6656":"code","a90bcd96":"code","16f51a50":"code","8bb3affe":"code","78c1bdef":"code","ec6dd95e":"code","059f4b05":"code","4d79b3dd":"code","d46fc699":"code","2cfd62b6":"code","75646100":"code","45cf1fec":"code","3b9ea0a4":"code","4c0238e3":"code","9bb481d7":"code","5c32a8d3":"code","f4079e0c":"code","b98bed4c":"code","27e0106d":"code","0022f847":"code","495075ba":"code","c817c908":"code","3cd4c72f":"code","fbd58bcf":"code","1ec53f5b":"code","24345b90":"code","61ed14b4":"code","f115f5b9":"code","7ab598a0":"code","982388ae":"code","5fa3f710":"code","1ab3f134":"code","522ca920":"code","9819c603":"code","496293b2":"code","0439a620":"code","e973e31a":"code","78c2424e":"code","f99aa39f":"code","1bdc81d0":"code","b503ca36":"code","e2a5a79b":"code","be40c515":"code","67790962":"code","a2b45ade":"code","a92ab114":"code","90bc03cd":"code","c67d8bda":"code","774cf16c":"code","6468a6cf":"code","c846005e":"code","6c4ca4b4":"code","776036a0":"code","127a8d52":"code","fae5955c":"code","e70bab28":"code","32c9001e":"code","b0ca4c9a":"code","ffdbb589":"code","3db647c2":"code","8dbc7865":"code","1db5c265":"code","9ecc88b5":"code","9ebb7e18":"code","b7206188":"code","f914340e":"code","7b01ec91":"code","086217fe":"code","4620c141":"code","8a1b2134":"code","312b1268":"code","4cff766d":"code","4d491fa5":"code","13c05bda":"code","a0d8e33b":"code","83cf8e89":"markdown","87762099":"markdown","56f18887":"markdown","10f260b8":"markdown","03b4a16c":"markdown","78dcfe12":"markdown","ab5ce797":"markdown","40223c13":"markdown","7d9cd0bc":"markdown","aefc660a":"markdown","0e96ed47":"markdown","9d5d8a3e":"markdown","9b0dd62e":"markdown","95486b6c":"markdown","bdff5df4":"markdown","bbead868":"markdown","14788929":"markdown","a825175e":"markdown","6ff93f0c":"markdown","750d61ee":"markdown","9619b41c":"markdown","8b7191b1":"markdown","eb7cd238":"markdown","feaeccf9":"markdown","172038d8":"markdown","f2caf583":"markdown","cea19b7d":"markdown","d68d60a8":"markdown","f5d54406":"markdown","344e6570":"markdown","de5bc63c":"markdown","5725e4b5":"markdown","343dfe33":"markdown","c8276da2":"markdown","7a8a3146":"markdown","b20c6b7a":"markdown","5371831a":"markdown","7bc41788":"markdown","4ea47c96":"markdown","ca7a35fa":"markdown","a9ddb83d":"markdown","e5b52061":"markdown","8521a728":"markdown","1e31b849":"markdown","4e90514d":"markdown","6f986fb1":"markdown","5e93425e":"markdown","6636bc14":"markdown","46a2dd55":"markdown","9885505a":"markdown","048916bd":"markdown","f7fa9dcf":"markdown","9081193d":"markdown","2781f6aa":"markdown","ddfec3d1":"markdown","c6789a10":"markdown","204de161":"markdown","40dcf5f5":"markdown","15b059c0":"markdown","e9cad1b3":"markdown","fb0e13c1":"markdown","b6d1d95f":"markdown","d0383ba6":"markdown","2cc38a60":"markdown","04155327":"markdown","202f1eb1":"markdown","4d3a5a01":"markdown","aa2b5237":"markdown","7a0f8f9a":"markdown","4bafaf98":"markdown","020d1c11":"markdown","6a980efe":"markdown","aef51ffd":"markdown","741d6a4a":"markdown","276a2d12":"markdown","c6b1062b":"markdown","1f5cde0b":"markdown","fd39593c":"markdown"},"source":{"9e439347":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns            #For plots\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n%matplotlib inline","6cb7deb1":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","cbdf2645":"train.shape, test.shape","e2cecc03":"train.head(5)","6e5fbe0e":"test.head(5)","8912220d":"#target = train[\"target\"]","3c85899a":"#train = train.drop([\"target\"], axis=1)","b4d57320":"train.isnull().sum()","c184c640":"test.isnull().sum()","9f1e6656":"train.describe()","a90bcd96":"test.describe()","16f51a50":"train_vis = train.iloc[:, 1:13]","8bb3affe":"train_vis.head(2)","78c1bdef":"# PROVIDE CITATIONS TO YOUR CODE IF YOU TAKE IT FROM ANOTHER WEBSITE.\n# https:\/\/matplotlib.org\/gallery\/pie_and_polar_charts\/pie_and_donut_labels.html#sphx-glr-gallery-pie-and-polar-charts-pie-and-donut-labels-py\n\n\ny_value_counts = train['target'].value_counts()\nprint(\"Number of people transacted the money in future \", y_value_counts[1], \", (\", (y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\nprint(\"Number of people not transacted the money in future  \", y_value_counts[0], \", (\", (y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0]))*100,\"%)\")\n#above codes will give the%age of approved and not approved project\n\nfig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(aspect=\"equal\"))\nrecipe = [\"transacted\", \"not transacted\"]\n\ndata = [y_value_counts[1], y_value_counts[0]]\n\nwedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\nbbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\nkw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n          bbox=bbox_props, zorder=0, va=\"center\")\n\nfor i, p in enumerate(wedges):\n    ang = (p.theta2 - p.theta1)\/2. + p.theta1\n    y = np.sin(np.deg2rad(ang))\n    x = np.cos(np.deg2rad(ang))\n    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n    connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n    ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                 horizontalalignment=horizontalalignment, **kw)\n\nax.set_title(\"Number of people transacted money or not\")\n\nplt.show()","ec6dd95e":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html\nplt.close()  #Closing all open window\nsns.set_style(\"whitegrid\");\nsns.pairplot(train_vis, hue=\"target\", height=3);\nplt.show()","059f4b05":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(10,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(10,10,i)\n        sns.distplot(df1[feature], hist=False,label=label1)\n        sns.distplot(df2[feature], hist=False,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","4d79b3dd":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]","d46fc699":"features = train.columns.values[2:102]\nplot_feature_distribution(t0, t1, '0', '1', features)","2cfd62b6":"features = train.columns.values[102:202]\nplot_feature_distribution(t0, t1, '0', '1', features)","75646100":"train_5000 = train.head(5000)\ny =train_5000[\"target\"]\nx = train_5000.iloc[:,2:202].values","45cf1fec":"# https:\/\/github.com\/pavlin-policar\/fastTSNE you can try this also, this version is little faster than sklearn \n#reference: aaic tsne\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn import datasets\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ntsne = TSNE(n_components=2, perplexity=30, learning_rate=200)\n\nX_embedding = tsne.fit_transform(x)\n# if x is a sparse matrix you need to pass it as X_embedding = tsne.fit_transform(x.toarray()) , .toarray() will convert the sparse matrix into dense matrix\n\nfor_tsne = np.vstack((X_embedding.T, y)).T#y.reshape(-1,1)\nfor_tsne_df = pd.DataFrame(data=for_tsne, columns=['Dim_1','Dim_2','label'])\n# Ploting the result of tsne\nsns.FacetGrid(for_tsne_df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.title(\"Visualise tsne \")\nplt.show()","3b9ea0a4":"features_train = train.columns.values[2:202]\nfeatures_test = test.columns.values[1:201]\nrow_mean_train = train[features_train].mean(axis=1)\ntrain[\"row_mean\"] =row_mean_train\nrow_mean_test = test[features_test].mean(axis=1)\ntest[\"row_mean\"] = row_mean_test\n","4c0238e3":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_mean\")\\\n             .add_legend()\nplt.title(\"Histogram of mean\")\nplt.ylabel(\"Density of mean\")\nplt.plot()","9bb481d7":"#reference : aaic haberman","5c32a8d3":"row_median_train = train[features_train].median(axis=1)\ntrain[\"row_median\"] =row_median_train\nrow_median_test = test[features_test].median(axis=1)\ntest[\"row_median\"] = row_median_test\n","f4079e0c":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_median\")\\\n             .add_legend()\nplt.title(\"Histogram of median\")\nplt.ylabel(\"Density of median\")\nplt.plot()","b98bed4c":"row_std_train = train[features_train].std(axis=1)\ntrain[\"row_std\"] =row_std_train\nrow_std_test = test[features_test].std(axis=1)\ntest[\"row_std\"] = row_std_test\n","27e0106d":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_std\")\\\n             .add_legend()\nplt.title(\"Histogram of std\")\nplt.ylabel(\"Density of std\")\nplt.plot()","0022f847":"row_min_train = train[features_train].min(axis=1)\ntrain[\"row_min\"] =row_min_train\nrow_min_test = test[features_test].min(axis=1)\ntest[\"row_min\"] = row_min_test\n","495075ba":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_min\")\\\n             .add_legend()\nplt.title(\"Histogram of min\")\nplt.ylabel(\"Density of min\")\nplt.plot()","c817c908":"row_max_train = train[features_train].max(axis=1)\ntrain[\"row_max\"] =row_max_train\nrow_max_test = test[features_test].max(axis=1)\ntest[\"row_max\"] = row_max_test\n","3cd4c72f":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_max\")\\\n             .add_legend()\nplt.title(\"Histogram of max\")\nplt.ylabel(\"Density of max\")\nplt.plot()","fbd58bcf":"row_skew_train = train[features_train].skew(axis=1)\ntrain[\"row_skew\"] =row_skew_train\nrow_skew_test = test[features_test].skew(axis=1)\ntest[\"row_skew\"] = row_skew_test\n","1ec53f5b":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_skew\")\\\n             .add_legend()\nplt.title(\"Histogram of skew\")\nplt.ylabel(\"Density of skew\")\nplt.plot()","24345b90":"row_kurt_train = train[features_train].kurtosis(axis=1)\ntrain[\"row_kurt\"] =row_kurt_train\nrow_kurt_test = test[features_test].kurtosis(axis=1)\ntest[\"row_kurt\"] = row_kurt_test\n","61ed14b4":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_kurt\")\\\n             .add_legend()\nplt.title(\"Histogram of kurt\")\nplt.ylabel(\"Density of kurt\")\nplt.plot()","f115f5b9":"row_sum_train = train[features_train].sum(axis=1)\ntrain[\"row_sum\"] =row_sum_train\nrow_sum_test = test[features_test].sum(axis=1)\ntest[\"row_sum\"] = row_sum_test\n","7ab598a0":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"row_sum\")\\\n             .add_legend()\nplt.title(\"Histogram of sum\")\nplt.ylabel(\"Density of sum\")\nplt.plot()","982388ae":"#https:\/\/www.kaggle.com\/hjd810\/keras-lgbm-aug-feature-eng-sampling-prediction\nrow_ma_train = train[features_train].apply(lambda x: np.ma.average(x), axis=1)\ntrain[\"ma\"] = row_ma_train\nrow_ma_test = test[features_test].apply(lambda x: np.ma.average(x), axis=1)\ntest[\"ma\"] = row_ma_test\n","5fa3f710":"#https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\n#https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.ma.average.html\nsns.FacetGrid(train, hue = \"target\", height = 5)\\\n             .map(sns.distplot, \"ma\")\\\n             .add_legend()\nplt.title(\"Histogram of ma\")\nplt.ylabel(\"Density of ma\")\nplt.plot()","1ab3f134":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]","522ca920":"#reference: aaic haberman\ncounts, bin_edges=np.histogram(t0[\"row_mean\"], bins=10, density=True)\npdf=counts\/(sum(counts))\nprint(pdf);    #this will return 10 values\nprint(bin_edges);  #this will return 11 values\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label=\"mean0pdf\");\nplt.plot(bin_edges[1:], cdf, label=\"mean0pdf\");\n\ncounts, bin_edges=np.histogram(t1[\"row_mean\"], bins=10, density=True)\npdf=counts\/(sum(counts))\nprint(pdf);    #this will return 10 values\nprint(bin_edges);  #this will return 11 values\ncdf=np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label=\"mean1pdf\");\nplt.plot(bin_edges[1:], cdf, label=\"mean1pdf\");\nplt.legend()\nplt.title(\"Pdf & Cdf of year\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"percentage\")","9819c603":"#reference aaic haberman\nsns.boxplot(x=\"target\", y=\"row_sum\", data=train)\nplt.title(\"Boxplot for row_sum\")\nplt.plot()","496293b2":"sns.boxplot(x=\"target\", y=\"row_mean\", data=train)\nplt.title(\"Boxplot for mean\")\nplt.plot()","0439a620":"#create a function which makes the plot:\n#https:\/\/www.kaggle.com\/sicongfang\/eda-feature-engineering\nfrom matplotlib.ticker import FormatStrFormatter\ndef visualize_numeric(ax1, ax2, ax3, df, col, target):\n    #plot histogram:\n    df.hist(column=col,ax=ax1,bins=200)\n    ax1.set_xlabel('Histogram')\n    \n    #plot box-whiskers:\n    df.boxplot(column=col,by=target,ax=ax2)\n    ax2.set_xlabel('Transactions')\n    \n    #plot top 10 counts:\n    cnt = df[col].value_counts().sort_values(ascending=False)\n    cnt.head(10).plot(kind='barh',ax=ax3)\n    ax3.invert_yaxis()  # labels read top-to-bottom\n#     ax3.yaxis.set_major_formatter(FormatStrFormatter('%.2f')) #somehow not working \n    ax3.set_xlabel('Count')","e973e31a":"##https:\/\/www.kaggle.com\/sicongfang\/eda-feature-engineering\nfor col in list(train.columns[10:20]):\n    fig, axes = plt.subplots(1, 3,figsize=(10,3))\n    ax11 = plt.subplot(1, 3, 1)\n    ax21 = plt.subplot(1, 3, 2)\n    ax31 = plt.subplot(1, 3, 3)\n    fig.suptitle('Feature: %s'%col,fontsize=5)\n    visualize_numeric(ax11,ax21,ax31,train,col,'target')\n    plt.tight_layout()","78c2424e":"train.head(2)","f99aa39f":"train.to_csv(\"..\/train_sant.csv\")","1bdc81d0":"test.to_csv(\"..\/test_sant.csv\")","b503ca36":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport time\nimport warnings\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom scipy.sparse import hstack\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold \nfrom collections import Counter, defaultdict\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nimport math\nfrom sklearn.metrics import normalized_mutual_info_score\nfrom sklearn.ensemble import RandomForestClassifier\nwarnings.filterwarnings(\"ignore\")\n\nfrom mlxtend.classifier import StackingClassifier\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\n","e2a5a79b":"train = pd.read_csv(\"..\/train_sant.csv\")\n#test = pd.read_csv(\"\/content\/drive\/My Drive\/test_santander.csv\")","be40c515":"train.head(2)","67790962":"#target values\ntarget = train[\"target\"].values","a2b45ade":"#imp features from column 3 to 212\ntrain = train.iloc[:,3:212]","a92ab114":"train.shape","90bc03cd":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\nfrom sklearn.model_selection import train_test_split\ntrain, test, y_train, y_test = train_test_split(train, target, test_size=0.4)\n","c67d8bda":"train.head(2)","774cf16c":"test.head(2)","6468a6cf":"#https:\/\/stackoverflow.com\/questions\/26414913\/normalize-columns-of-pandas-data-frame\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler() \nscaler = scaler.fit(train) \ntrain = scaler.transform(train)\ntest = scaler.transform(test)","c846005e":"def batch_predict(clf, data):\n    # roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n    # not the predicted outputs\n\n    y_data_pred = []\n    tr_loop = data.shape[0] - data.shape[0]%1000\n    # consider you X_tr shape is 49041, then your cr_loop will be 49041 - 49041%1000 = 49000\n    # in this for loop we will iterate unti the last 1000 multiplier\n    for i in range(0, tr_loop, 1000):\n        y_data_pred.extend(clf.predict_proba(data[i:i+1000])[:,1])\n    # we will be predicting for the last data points\n    y_data_pred.extend(clf.predict_proba(data[tr_loop:])[:,1])\n    \n    return y_data_pred","6c4ca4b4":"#From facebook recommendation applied this code is taken and modified according to use\nfrom sklearn.metrics import confusion_matrix\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    \n    TN = C[0,0]       \n    FP = C[0,1]  \n    FN = C[1,0]\n    TP = C[1,1]\n    print(\"True Positive\",TP)\n    print(\"False Negative\",FN)\n    print(\"False Positive\",FP)\n    print(\"True Negative\",TN)\n    \n    \n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    \n    B =(C\/C.sum(axis=0))\n    plt.figure(figsize=(30,6))\n    \n    labels = [0,1]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"Navy\", as_cmap=True)#https:\/\/stackoverflow.com\/questions\/37902459\/seaborn-color-palette-as-matplotlib-colormap\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    \n    plt.show()","776036a0":"# we are writing our own function for predict, with defined thresould\n# we will pick a threshold that will give the least fpr\ndef predict(proba, threshould, fpr, tpr):\n    \n    t = threshould[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    \n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","127a8d52":"#As mentioned in logistic regression assignment I am changing alpha to log to plot a goog graph\nimport numpy as np\ndef log_alpha(al):\n    alpha=[]\n    for i in al:\n        a=np.log(i)\n        alpha.append(a)\n    return alpha    ","fae5955c":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\nlg = SGDClassifier(loss='log', class_weight='balanced', penalty=\"l2\")\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(lg, parameters, cv=3, scoring='roc_auc', n_jobs=-1, return_train_score=True,)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\nalpha = log_alpha(alpha)\n\n\nbest_alpha = clf.best_estimator_.alpha\n#best_split = clf.best_estimator_.min_samples_split\n\nprint(best_alpha)\n#print(best_split)\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","e70bab28":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV\n\nlg = SGDClassifier(loss='log', alpha=best_alpha, penalty=\"l2\", class_weight=\"balanced\")\n#lg.fit(train_1, project_data_y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\nsig_clf = CalibratedClassifierCV(lg, method=\"isotonic\")\nlg = sig_clf.fit(train, y_train)\n\n\ny_train_pred = lg.predict_proba(train)[:,1]   \ny_test_pred = lg.predict_proba(test)[:,1] \n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","32c9001e":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","b0ca4c9a":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","ffdbb589":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import roc_auc_score\n\nsvm = SGDClassifier(loss='hinge', class_weight='balanced', penalty=\"l2\")\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(svm, parameters, cv=3, scoring='roc_auc', n_jobs=-1, return_train_score=True,)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\nalpha = log_alpha(alpha)\n\n\nbest_alpha = clf.best_estimator_.alpha\n#best_split = clf.best_estimator_.min_samples_split\n\nprint(best_alpha)\n#print(best_split)\n\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score'] \ncv_auc_std= clf.cv_results_['std_test_score']\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","3db647c2":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.calibration import CalibratedClassifierCV\n\nsvm = SGDClassifier(loss='hinge', alpha=best_alpha, penalty=\"l2\", class_weight=\"balanced\")\n#svm.fit(train_1, project_data_y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\nsig_clf = CalibratedClassifierCV(svm, method=\"isotonic\")\nsvm = sig_clf.fit(train, y_train)\n\n\ny_train_pred = svm.predict_proba(train)[:,1]   \ny_test_pred = svm.predict_proba(test)[:,1] \n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","8dbc7865":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","1db5c265":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","9ecc88b5":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.MultinomialNB.html\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nnaive = MultinomialNB(fit_prior=False)\nalpha=[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\nparameters = {'alpha':alpha}\nclf = GridSearchCV(naive, parameters, cv=3, scoring='roc_auc', return_train_score=True)\nclf.fit(train, y_train)\n\nprint(\"Model with best parameters :\\n\",clf.best_estimator_)\n\ntrain_auc= list(clf.cv_results_['mean_train_score'])\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = list(clf.cv_results_['mean_test_score']) \ncv_auc_std= clf.cv_results_['std_test_score']\n\nbest_alpha=clf.best_estimator_.alpha\n\nalpha = log_alpha(alpha)\n\nplt.plot(alpha, train_auc, label='Train AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,train_auc - train_auc_std,train_auc + train_auc_std,alpha=0.2,color='darkblue')\n\nplt.plot(alpha, cv_auc, label='CV AUC')\n# this code is copied from here: https:\/\/stackoverflow.com\/a\/48803361\/4084039\nplt.gca().fill_between(alpha,cv_auc - cv_auc_std,cv_auc + cv_auc_std,alpha=0.2,color='darkorange')\n\nplt.scatter(alpha, train_auc, label='Train AUC points')\nplt.scatter(alpha, cv_auc, label='CV AUC points')\n\n\nplt.legend()\nplt.xlabel(\"alpha and l1\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","9ebb7e18":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nnaive = MultinomialNB(alpha=best_alpha, fit_prior=False)\nnaive.fit(train, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n\ny_train_pred = naive.predict_proba(train)[:,1]    \ny_test_pred = naive.predict_proba(test)[:,1]\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\" hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.grid()\nplt.show()","b7206188":"print('Train confusion_matrix')\nplot_confusion_matrix(y_train,predict(y_train_pred, tr_thresholds, train_fpr, train_fpr))\n","f914340e":"print('Test confusion_matrix')\nplot_confusion_matrix(y_test,predict(y_test_pred, tr_thresholds, train_fpr, train_fpr))","7b01ec91":"train = pd.read_csv(\"..\/train_sant.csv\")\ntest = pd.read_csv(\"..\/test_sant.csv\")","086217fe":"#from sklearn.model_selection import train_test_split\n#train, test, y_train, y_test = train_test_split(train, target, test_size=0.4)","4620c141":"#taking all the columns except idcode, target, unnamed0\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target',\"Unnamed:0\"]]\ntarget = train['target']","8a1b2134":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","312b1268":"#setting parameters\nparam = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.4,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1\n}\n","4cff766d":"#making 10 folds\nfolds = StratifiedKFold(n_splits=10, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","4d491fa5":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\n#plotting a bar plot where y represents features and x represents its importance.\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","13c05bda":"sub_df = pd.DataFrame({\"ID_code\":test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions\nsub_df.to_csv(\"submission.csv\", index=False)","a0d8e33b":"#http:\/\/zetcode.com\/python\/prettytable\/\nfrom prettytable import PrettyTable\n\nx = PrettyTable()\nx.field_names =[\"Models\",\"Test auc\"]\nx.add_row([\"Logistic \",0.862])\nx.add_row([\"SVM \",0.863])\nx.add_row([\"Naive \",0.854])\nx.add_row([\"LightGbm\",0.90])\n\nprint(x)","83cf8e89":"**so from the above plot we can see tha  test auc is 0.863**","87762099":"**Dividing train into train and test**","56f18887":"**->from the above we can conclude that data follows different distribution**\\\n**->from the boxplot we can assume that for var_11 50% of its values lies with -8 to 0. and for for var_10 50% of its value lie within -5 to 5 and like wise for others we can conclude from boxplot**\\\n**->from the above count plot we can see that maximum number of count of some particular value is variable in nature. **","10f260b8":"# Naive Bayes","03b4a16c":"# Note: Here i am working only on train data dropping the labels for test as aaic till naive bayes for better visualization","78dcfe12":"**from the above pdf we can see that when median>6 and median<7 , the probability of target==1 is high**","ab5ce797":"# Important features in decending order.","40223c13":"# Light GbM\nhttps:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction","7d9cd0bc":"**from the above tsne plot we can see that label 1 is not much seperable when we visualise it in 2d plot**","aefc660a":"# Visualising by tsne","0e96ed47":"**it is clear from the above pdf that when  max>35 and min<45 probability of target==1 is high**","9d5d8a3e":"**working only on train datasets just to see how well my model is doing.**","9b0dd62e":"**as we can see from above that I have successfully added feature engineered features in train data**","95486b6c":"**Reference:**https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction \\\nhttps:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment \\\nhttps:\/\/www.kaggle.com\/sicongfang\/eda-feature-engineering","bdff5df4":"# SVM","bbead868":"**distribution according to box plot is also same.**","14788929":"**sum**","a825175e":"**from this few features only we can see that both traget is easily seperable using any of the two features.**\\\nalthough the data is imbalanced but easily seperable","6ff93f0c":"**Submission**","750d61ee":"# Visualising with all the features is quite difficult so I am choosing 10 var columns to visualise as pair plot","9619b41c":"**we can see that there is no null value in train**","8b7191b1":"**from the above plot we can say that test auc is 0.854**","eb7cd238":"**from this train_vis we are going to visualize  11 features as pair plot because visualizing all at one takes a lot of time**","feaeccf9":"At Santander our mission is to help people and businesses prosper. We are always looking for ways to help our customers understand their financial health and identify which products and services might help them achieve their monetary goals.\n\nOur data science team is continually challenging our machine learning algorithms, working with the global data science community to make sure we can more accurately identify new ways to solve our most common challenge, binary classification problems such as: is a customer satisfied? Will a customer buy this product? Can a customer pay this loan?\n\nIn this challenge, we invite Kagglers to help us identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data we have available to solve this problem.","172038d8":"**min**","f2caf583":"# Steps Done:\n1. Importing the necessary libraries.\n2. Visualizing the train and test data.\n3. Checking for null values in train and test data if any.\n4. Describing the data\n5.Since pairplot for all the data was not possible so I did it for random 10 data\n6. Analysis of train data where we find out that data is purely unbaanced.\n7. Visualizing the pair plots.\n8. Pdf for all the features from 2 to 202(here we find out that there is some corelations between some of the data.)\n9. Visualising by tsne.\n10. Visualizing mean.\n11. visualising median\n12. visualising dtd\n13. visualising min\n14. visualising max\n15. visualising kurtosis\n16. visualizing skew\n17. visualising moving average.\n18. Visualizing by kde\n19. Visualizing by boxplot\n20. puting all the features to to dataframe.\n21. importing necessary libraries\n22. importing the new train data.\n23. Splitting data into train and test.\n24. Applying different models like naive bayes, logistic regression, svm, lightgbm\n25. Feature importance\n","cea19b7d":"**From the above plot it is clearly visible that when alpha=0.0001 we have maximum auc.**","d68d60a8":"**Skew**","f5d54406":"# PrettyTable","344e6570":"**importing necessary libraries**","de5bc63c":"# PAIR PLOT only for first 10 var_0 to var_10\n**we can visualize relationship between two varioables with this**","5725e4b5":"# Data Analysis on train data","343dfe33":"**metric used = auc(since data is not balanced)**","c8276da2":"**We can see that there is no null values in test data**","7a8a3146":"**it is clear from the above pdf that when std>9.2 and std<10.2 probability of target==1 is high.**","b20c6b7a":"# Conclusion:\n**Lightgbm is giving best results than any other models.**","5371831a":"**Pdf and cdf using kde(Kernal Distribution Estimation)**","7bc41788":"**importing necessary libraries**","4ea47c96":"# from features 100 -200","ca7a35fa":"**it is clear from the above pdf that when  ma>6.2 and ma<7 probability of target==1 is high**","a9ddb83d":"**from the above pdf and cdf we can say that 90 % of data lie below 7.5**","e5b52061":"# Logistic Regression","8521a728":"# Logistic","1e31b849":"# first 100","4e90514d":"**Pdf gives the probabily of points lying in a certain range**","6f986fb1":"**std**","5e93425e":"**importing the necessary libraries**","6636bc14":"**it is clear from the above pdf that when  min<-20 and min>-50 probability of target==1 is high**","46a2dd55":"# Visualizing mean, median, std, kurtosis, skew, add, min, max, moving average of train and simultaneously doing feature engineering","9885505a":"# Lets describe test","048916bd":"**from the above pdf we can say that when mean>6.2 and mean<7 then it is clear that probability of target=1 is high.**","f7fa9dcf":"# Visualize var_13 to var_17","9081193d":"**it is clear from the above pdf that when  kurt>2 and kurt<4 probability of target==1 is high**","2781f6aa":"**it is clear from the above pdf that when  sum>1250 and sum<1450 probability of target==1 is high**","ddfec3d1":"**here I am distributing dataset label wise**","c6789a10":"# Now saving all the feature engineered data to train_santander.csv","204de161":"# Making final models with best alpha and penalty","40dcf5f5":"# Confusion Matrix with using map","15b059c0":"**As we can see from above that train and test, both contains  0.2 million rows.**","e9cad1b3":"# Box_plot","fb0e13c1":"**Checking missing values if any**","b6d1d95f":"**Defining necessary functions.**","d0383ba6":"**So from the above plot we can observe that the number of people transected the money os about 10% of the total data only.**\\\n*this is a purely imbalanced data.*","2cc38a60":"**from the above auc plot it is clearly visible that when auc alpha = 0.0001 , we have maximum auc**","04155327":"**Source:**https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction","202f1eb1":"# Now applying different ML algorithms","4d3a5a01":"# Pdf for all features\nhttps:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction","aa2b5237":"# Confusion Matrix with using map","7a0f8f9a":"**max**","4bafaf98":"**kurtosis**","020d1c11":"**adding median**","6a980efe":"# Making final models with best alpha and penalty","aef51ffd":"**moving sum mean**","741d6a4a":"# Confusion Matrix using heat map","276a2d12":"**So the maximum auc here is 0.862**","c6b1062b":"**As we can see from above pdf that there is a lot of different distribution**\\\n**and for most of the data where label=1 and label=0 follows same distribution**\\\n**var_10 ,var_11, var_8,  var_65, var_84  ect. follows same distribution like gaussian**\\\n**var_70, var_60, var_85 ect follows similar distribution**\\\n**var_80, var_86 etc follows similar distribution.**\\\nsimilarly we can see from feature 102 to 202.","1f5cde0b":"# Lets  describe train","fd39593c":"**from the above we can say taht lightgbm has performed well than all the other models. auc reaching to 0.90**"}}