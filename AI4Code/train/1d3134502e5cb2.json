{"cell_type":{"484fb8c9":"code","6158dc53":"code","ab9b6713":"code","9e80d981":"code","128c2479":"code","df5e6466":"code","68512c44":"code","4648023a":"code","d74e8b08":"code","e04e638d":"code","12206794":"code","019d0f48":"code","b19e8342":"code","fb4b54d6":"code","429bbd94":"code","f364edc0":"code","a9f49483":"code","c25d79ed":"code","7da911d0":"code","e14cd927":"code","ea2cc144":"code","28e9323b":"code","7b89ff52":"code","912be40e":"code","cecfbb8d":"code","a0ab7ab9":"code","4bb4b0a6":"code","c7fa4682":"code","9737c4f8":"code","75e6245b":"code","3954bd59":"code","e82d9ea3":"code","b9a291e8":"code","aaa6fba2":"code","08e46463":"code","4784c222":"code","1483f8ff":"code","957905e0":"code","19c33e10":"code","03bbfa53":"code","870b7619":"code","edba4364":"code","b0df8fac":"code","0dd87c52":"code","9cd31769":"code","dec6902a":"code","68f1c66f":"code","dd162cad":"code","cfc2c095":"code","a27adf39":"code","3eca0885":"code","fb477a9a":"code","df000f7b":"code","782fcfa0":"code","edb9b21e":"code","434fc651":"code","89497999":"code","5faed11c":"code","0a927d7b":"code","cff5cde3":"code","641e9280":"code","529057c7":"code","9951d46e":"code","76b502a8":"code","5adf3d9b":"code","7be4906d":"code","4532422e":"code","abbb5659":"code","5b67450b":"code","6587e5a8":"code","5aadcfab":"code","a07d0c04":"code","be399c9f":"code","d19bd8ab":"code","8e7f4097":"code","174587c8":"code","855382a7":"code","2b7c278d":"code","26bbe0f4":"code","1a4d281d":"code","4d802d11":"code","5209bcbf":"code","f4ec9e7f":"markdown","1ce1e10e":"markdown","f7fe4cdf":"markdown","a3899db1":"markdown","9b02de64":"markdown","81c4d415":"markdown","28127df1":"markdown","05618b2c":"markdown","fbf30718":"markdown","9f46df5d":"markdown","75d5b321":"markdown","ba266bb0":"markdown","369d8686":"markdown","4f39f8fd":"markdown","052d6852":"markdown","562bb0c3":"markdown","8ece0ae0":"markdown","3212ecbb":"markdown","926ab4b8":"markdown","08dbd0c7":"markdown","a509db15":"markdown","3d77ae10":"markdown","6db1ed18":"markdown","6f3c9698":"markdown","df08525f":"markdown","ad60ce3b":"markdown","36b36265":"markdown","e086b2b7":"markdown","9c012552":"markdown","a23fd65c":"markdown","23841617":"markdown","6cceedb1":"markdown","1027411a":"markdown"},"source":{"484fb8c9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6158dc53":"import pandas as pd","ab9b6713":"runs = pd.read_csv(\"..\/input\/hkracing\/runs.csv\")\nruns.head()","9e80d981":"races = pd.read_csv('..\/input\/hkracing\/races.csv')\nraces.head()","128c2479":"runs_data = runs[['race_id', 'won', 'horse_age', 'horse_country', 'horse_type', 'horse_rating',\n       'horse_gear', 'declared_weight', 'actual_weight', 'draw', 'win_odds',\n       'place_odds', 'horse_id']]\nruns_data.head()","df5e6466":"races_data = races[['race_id', 'venue', 'config', 'surface', 'distance', 'going', 'race_class', 'date']]\nraces_data.head()","68512c44":"# merge the two datasets based on race_id column\ndf = pd.merge(runs_data, races_data)\ndf.head()","4648023a":"df.isnull().any()","d74e8b08":"df.horse_country.isnull().value_counts(ascending=True)","e04e638d":"df.horse_type.isnull().value_counts(ascending=True)","12206794":"df.place_odds.isnull().value_counts(ascending=True)","019d0f48":"df.shape","b19e8342":"df = df.dropna()\ndf.shape","fb4b54d6":"df.date = pd.to_datetime(df.date)\ndf.date.dtype","429bbd94":"min(df.date), max(df.date)\n# 8-year duration","f364edc0":"start_time = min(df.date).strftime('%d %B %Y')\nend_time = max(df.date).strftime('%d %B %Y')\nno_of_horses = df.horse_id.nunique()\nno_of_races = df.race_id.nunique()\n\nprint(f'The dataset was collected from {start_time} to {end_time}, which contains information about {no_of_horses} horses and {no_of_races} races. ')","a9f49483":"# drop the unnecessary columns\ndf = df.drop(columns=['horse_id', 'date'])\ndf.head()","c25d79ed":"df.columns","7da911d0":"df.horse_gear.value_counts(ascending=False)","e14cd927":"df.horse_gear.nunique()","ea2cc144":"def horse_gear_impute(cols):\n    if cols == '--':\n        return 0\n    else: \n        return 1","28e9323b":"df.horse_gear = df.horse_gear.apply(horse_gear_impute)","7b89ff52":"df.horse_gear.value_counts()","912be40e":"df = pd.get_dummies(df, drop_first=True)\ndf.head()","cecfbb8d":"df.columns","a0ab7ab9":"from time import time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nfrom sklearn.metrics import precision_score, classification_report, confusion_matrix","4bb4b0a6":"last_raceid = max(df.race_id)\nlast_raceid","c7fa4682":"# split the last race data for deployment & save it in last_race variable\nlast_race = df[df.race_id == last_raceid]\nlast_race","9737c4f8":"new_data = df[:75696]   # drop the last race data for modeling\nnew_data = new_data.drop(columns='race_id')   # drop the unnecessary race_id column\nnew_data.tail()","75e6245b":"new_data.shape","3954bd59":"plt.figure(figsize=(6,4))\nsns.countplot(data=new_data, x='won')\nplt.title('Number of Labels by Class')","e82d9ea3":"X = new_data.drop(columns='won')\ny = new_data['won']","b9a291e8":"# extermely skewed data\ny.value_counts()","aaa6fba2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)","08e46463":"k_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    # precision ratio: tp \/ (tp + fp), aiming at minimize fp (predict: win, actual: lose)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","4784c222":"# find the highest precision score of the positive class (1)\nimport operator\nmax(scores.items(), key=operator.itemgetter(1))","1483f8ff":"plt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('Original Data')","957905e0":"start = time()\n\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","19c33e10":"print(classification_report(y_test, y_pred))","03bbfa53":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","870b7619":"rus = RandomUnderSampler(random_state=0)\nX_rus, y_rus = rus.fit_sample(X_train, y_train)\n\nk_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_rus, y_rus)\n    y_pred = knn.predict(X_test)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","edba4364":"max(scores.items(), key=operator.itemgetter(1))","b0df8fac":"plt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('RUS Data')","0dd87c52":"start = time()\n\nknn_rus = KNeighborsClassifier(n_neighbors=8)\nknn_rus.fit(X_rus, y_rus)\ny_pred = knn_rus.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","9cd31769":"print(classification_report(y_test, y_pred))","dec6902a":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","68f1c66f":"sm = SMOTE(random_state=0)\nX_sm, y_sm = sm.fit_sample(X_train, y_train)\n\nk_range = range(1,10)\nscores = {}\nscores_list = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_sm, y_sm)\n    y_pred = knn.predict(X_test)\n    scores[k] = precision_score(y_test, y_pred)\n    scores_list.append(precision_score(y_test, y_pred))","dd162cad":"max(scores.items(), key=operator.itemgetter(1))","cfc2c095":"# SMOTE data\nplt.plot(k_range, scores_list)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Precision Score of the positive class (1)')\nplt.title('SMOTE Data')","a27adf39":"start = time()\n\nknn_sm = KNeighborsClassifier(n_neighbors=2)\nknn_sm.fit(X_sm, y_sm)\ny_pred = knn_sm.predict(X_test)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","3eca0885":"print(classification_report(y_test, y_pred))","fb477a9a":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","df000f7b":"start = time()\n\nd_train = lgb.Dataset(X_train, label = y_train)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","782fcfa0":"#Prediction\ny_pred = clf.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.0995:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","edb9b21e":"print(classification_report(y_test, y_pred))","434fc651":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","89497999":"# plot model\u2019s feature importances (original data)\nlgb.plot_importance(clf, max_num_features=10)","5faed11c":"# convert array data into dataframe with column names, and feed into lgb model\nX_rus = pd.DataFrame(X_rus, columns=list(X_train))\nX_rus.head()","0a927d7b":"start = time()\n\nd_train = lgb.Dataset(X_rus, label = y_rus)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf_rus = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","cff5cde3":"#Prediction\ny_pred = clf_rus.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.55:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","641e9280":"print(classification_report(y_test, y_pred))","529057c7":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","9951d46e":"# plot model\u2019s feature importances (Random Under-sampling)\nlgb.plot_importance(clf_rus, max_num_features=10)","76b502a8":"# convert array data into dataframe with column names, and feed into lgb model\nX_sm = pd.DataFrame(X_sm, columns=list(X_train))\nX_sm.head()","5adf3d9b":"start = time()\n\nd_train = lgb.Dataset(X_sm, label = y_sm)\nparams = {}\nparams['learning_rate'] = 0.003\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['metric'] = 'binary_logloss'\nparams['sub_feature'] = 0.5\nparams['num_leaves'] = 100\nparams['min_data'] = 500\nparams['max_depth'] = 100\nclf_sm = lgb.train(params, d_train, 100)\n\nend = time()\nrunning_time = end - start\nprint('time cost: %.5f sec' %running_time)","7be4906d":"#Prediction\ny_pred = clf_sm.predict(X_test)\n#convert into binary values\nfor i in range(15140):\n    if y_pred[i] >= 0.5:       # setting threshold \n        y_pred[i] = 1\n    else:  \n        y_pred[i] = 0","4532422e":"print(classification_report(y_test, y_pred))","abbb5659":"labels = ['lose', 'win']\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')","5b67450b":"# plot model\u2019s feature importances (SMOTE)\nlgb.plot_importance(clf_sm, max_num_features=10)","6587e5a8":"# data that never been seen by the models\nlast_race","5aadcfab":"# drop unnecessary columns & define data and labels\nX_deploy = last_race.drop(columns=['race_id', 'won'])\ny_deploy = last_race.won","a07d0c04":"predictions = knn.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","be399c9f":"predictions = knn_rus.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","d19bd8ab":"import numpy as np\n\ndata = confusion_matrix(y_deploy, predictions)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='RdBu')\n\nfor (i, j), z in np.ndenumerate(data):\n    ax.text(j, i, '{}'.format(z), ha='center', va='center',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n    \nplt.title('Confusion matrix of kNN_rus', y=1.1)\nfig.colorbar(cax)\nlabels = ['lose', 'win']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","8e7f4097":"predictions = knn_sm.predict(X_deploy)\nprint(classification_report(y_deploy, predictions))","174587c8":"predictions = clf.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions[i] >= 0.0995:       # setting threshold \n        predictions[i] = 1\n    else:  \n        predictions[i] = 0","855382a7":"predictions_rus = clf_rus.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions_rus[i] >= 0.55:       # setting threshold \n        predictions_rus[i] = 1\n    else:  \n        predictions_rus[i] = 0","2b7c278d":"predictions_sm = clf_sm.predict(X_deploy)\n#convert into binary values\nfor i in range(14):\n    if predictions_sm[i] >= 0.5:       # setting threshold \n        predictions_sm[i] = 1\n    else:  \n        predictions_sm[i] = 0","26bbe0f4":"print(classification_report(y_deploy, predictions))","1a4d281d":"print(classification_report(y_deploy, predictions_rus))","4d802d11":"print(classification_report(y_deploy, predictions_sm))","5209bcbf":"data = confusion_matrix(y_deploy, predictions)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(data, cmap='RdBu')\n\nfor (i, j), z in np.ndenumerate(data):\n    ax.text(j, i, '{}'.format(z), ha='center', va='center',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='0.3'))\n    \nplt.title('Confusion matrix of LightGBM models', y=1.1)\nfig.colorbar(cax)\nlabels = ['lose', 'win']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Prediction')\nplt.ylabel('Actual')","f4ec9e7f":"* By processing a lot of data, kNN model trained with over-sampled data took the longest time, while LightGBM model trained with under-sampled data took the shortest time. \n* kNN models performed relatively worse with low precision score and f1-score of the positive class (1). \n* Training models aimed at minimize False Positive (predict: win, actual: lose), but it seems True Positive and False Positive are correlated. Same as gambling and investment, you have the chance to win and the risk to lose at the same time.\n* File sizes of LightGBM models are incredibly small and the time spent on training models is really quick.","1ce1e10e":"## Predictions of the LightGBM models","f7fe4cdf":"## Load kNN model trained with original data","a3899db1":"## Extract the last race data for model testing","9b02de64":"## kNN Classifier (original data)","81c4d415":"## Check missing values","28127df1":"## kNN Classifier (under-sampling)","05618b2c":"Only class 0 (lose) can be predicted. ","fbf30718":"For horse_gear column, we dicided to impute the data into 1 and 0 (with gear and no gear), rather than one-hot labeling (which will lead to numerous features). ","9f46df5d":"## Impute feature","75d5b321":"# MODELING","ba266bb0":"## Features explanation:\nwon - whether horse won (1) or otherwise (0)<br\/>\nhorse_age - current age of this horse at the time of the race<br\/>\nhorse_rating - rating number assigned by HKJC to this horse at the time of the race<br\/>\nhorse_gear - string representing the gear carried by the horse in the race. An explanation of the codes used may be found on the HKJC website.<br\/>\ndeclared_weight - declared weight of the horse and jockey, in lbs<br\/>\nactual_weight - actual weight carried by the horse, in lbs<br\/>\ndraw - post position number of the horse in this race<br\/>\nwin_odds - win odds for this horse at start of race<br\/>\nplace_odds - place (finishing in 1st, 2nd or 3rd position) odds for this horse at start of race<br\/>\nsurface - a number representing the type of race track surface: 1 = dirt, 0 = turf<br\/>\ndistance - distance of the race, in metres<br\/>\nrace_class - a number representing the class of the race<br\/>\nhorse_country - country of origin of this horse<br\/>\nhorse_type - sex of the horse, e.g. 'Gelding', 'Mare', 'Horse', 'Rig', 'Colt', 'Filly'<br\/>\nvenue - a 2-character string, representing which of the 2 race courses this race took place at: ST = Shatin, HV = Happy Valley<br\/>\nconfig - race track configuration, mostly related to the position of the inside rail. For more details, see the HKJC website.<br\/>\ngoing - track condition. For more details, see the HKJC website.<br\/>","369d8686":"Confusion matrix plot code reference from [Stack Overflow](https:\/\/stackoverflow.com\/questions\/20998083\/show-the-values-in-the-grid-using-matplotlib) user Joe Kington. Thank you for sharing your experience! =]","4f39f8fd":"# DEPLOY MODELS","052d6852":"## Basic information of the data","562bb0c3":"LightGBM code reference from Medium [article](https:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc) by Pushkar Mandot. Thank you for sharing your experience! =]","8ece0ae0":"## Select features for modeling","3212ecbb":"## One-hot encoding for categorical features","926ab4b8":"## LightGBM (under-sampling)","08dbd0c7":"# SELECT FEATURES & DATA CLEANING","a509db15":"## Conclusions:\nFor KNeighborsClassifier, only model trained with under-sampled data can predict both class 0 and class 1 (with one False Positive error). The original data model and over-sampling model can only predict class 0.  <br\/>\nLightGBM models can predict all data correctly, even using the model trained with skewed dataset (by tuning the threshold value). ","3d77ae10":"## Load kNN model trained with under-sampled data","6db1ed18":"## LightGBM (over-sampling)","6f3c9698":"Only class 0 (lose) can be predicted.","df08525f":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data.","ad60ce3b":"## Load LightGBM models & Set threshold values same as the training models","36b36265":"All LightGBM models can achieve 100% accuracy rate. ","e086b2b7":"## kNN Classifier (over-sampling)","9c012552":"## Load kNN model trained with over-sampled data","a23fd65c":"## Distribution of labels","23841617":"The amount of rows for missing values is relatively small, therefore we decided to drop these rows. ","6cceedb1":"kNN model trained with under-sampled data can predict the winning horse. However, there is also one False Positive in the prediction. ","1027411a":"## LightGBM (original data)"}}