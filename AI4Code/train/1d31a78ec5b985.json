{"cell_type":{"1f0e1190":"code","ed6c42c1":"code","9a7eba3b":"code","911e04fe":"code","c828fa16":"code","fe8f0e5b":"code","ecdf4d51":"code","08975793":"code","9110437b":"code","e5002c51":"code","4483baef":"markdown","503f46ed":"markdown","ce67db01":"markdown","2682ccd9":"markdown","7b068042":"markdown","3781243f":"markdown","f50c46ff":"markdown","40d1fe52":"markdown","f6d2791b":"markdown","df45343c":"markdown"},"source":{"1f0e1190":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ed6c42c1":"instances = 1000\ngroups = 3\nfeatures = 2\n\n# Randomly generates multicategory data for classification\n# random state ensures same data each time so we can compare runs\nX, y = make_blobs(n_samples=instances, centers=groups, n_features=features, random_state=99)\n\n# Extractin the input information and putting it into a dataframe for manipulations\ndf_X = pd.DataFrame()\ndf_X['bias'] = [1]*instances\ndf_X['feat1'] = X[:,0]\ndf_X['feat2'] = X[:,1]\nprint(df_X.head())\n\n# bringing the y values into a dataframe for manipulations\ndf_y = pd.DataFrame()\ndf_y['label'] = y\nprint(df_y.head())\n\n# PLotting the three groups according to the two features\nplt.scatter(df_X.loc[df_y['label'] == 0, 'feat1'], df_X.loc[df_y['label'] == 0, 'feat2'],\n            c='r', label = 'Cat. 0')\nplt.scatter(df_X.loc[df_y['label'] == 1, 'feat1'], df_X.loc[df_y['label'] == 1, 'feat2'],\n            c='m', label = 'Cat. 1')\nplt.scatter(df_X.loc[df_y['label'] == 2, 'feat1'], df_X.loc[df_y['label'] == 2, 'feat2'],\n            c='b', label = 'Cat. 2')\nplt.title('The three categories')\nplt.xlabel('feat 1')\nplt.ylabel('feat 2')\nplt.legend()\nplt.show()","9a7eba3b":"# Splitting the y-values into test and train\ndf_X_test = df_X[int(instances * 0.8):]\ndf_X = df_X[:int(instances * 0.8)]\n\n# Creating the dummy variables. Ie chaging the three category label to three binary categories\ndummies = pd.get_dummies(df_y['label'])\ndf_y = pd.concat((df_y, dummies), axis = 1)\n\n# Splitting the y-values into test and train\ndf_y_test = df_y[int(instances * 0.8):]\ndf_y = df_y[:int(instances * 0.8)]\n\nprint(df_y.head())","911e04fe":"# The fucntion we get our probability of yes or no (0 or 1)\ndef log_func(X, theta):\n    return 1\/(1+np.exp(-np.matmul(X, theta)))\n\n# The quantification of how good our parameters are\ndef cost_func(X, y, theta):\n    mm = len(X)\n    return (-1\/mm) * np.sum((y * np.log(log_func(X, theta))) + ((1-y) * (np.log(1-log_func(X, theta)))))\n\n# Adjusts the weights according to the partial derivatives of the cost function\ndef gradient_descent(X, y, theta, alpha):\n    mm = len(y)\n    return theta - ((alpha*mm)*np.matmul(np.transpose(X), (log_func(X, theta))-y))","c828fa16":"# Feature scaling to aid convergence\ndf_X['feat1'] = df_X['feat1'] \/ (df_X['feat1'].max() - df_X['feat1'].min())\ndf_X['feat2'] = df_X['feat2'] \/ (df_X['feat2'].max() - df_X['feat2'].min())\n\n# Adding polynomial terms as there are groups which are not so easily split by linear\ndf_X['feat1^2'] = df_X['feat1'] ** 2\ndf_X['feat2^2'] = df_X['feat2'] ** 2\n\nprint(df_X.head())\n\n# Obtaining our feature inputs (same as before)\nX = df_X.values \n# Our optimization parameter\nalpha = 1e-6\n# The number of loops we are using to converge\nloops = 100000","fe8f0e5b":"# Loops the parameter update over and over to get the best possible parameters\ndef get_params(target, alpha, loops):\n    y = df_y[target].values\n    # Our inital guess of the parameters, this time in vector form\n    theta = np.random.randint(0, 2, X.shape[1])\n\n    print(f'Initial parameters are {theta}')\n\n    # This will be used to track the cost over the iterations to check it is decreasing\n    cost_tracker = []\n    for ii in range(loops):\n        cost_tracker.append(cost_func(X, y, theta))\n        theta = gradient_descent(X, y, theta, alpha)\n\n    print(f'Final parameters are {theta}')\n    \n    #PLotting the cost as a function of its the iteration number\n    plt.plot(cost_tracker)\n    plt.xlabel('iteration number')\n    plt.ylabel('Cost')\n    plt.title(f'Category {target}')\n    plt.show()\n    \n    return theta\n\nprint('Category 0\\n')\ntheta_0 = get_params(0, alpha, loops)\nprint('Category 1\\n')\ntheta_1 = get_params(1, alpha, loops)\nprint('Category 2\\n')\ntheta_2 = get_params(2, alpha, loops)","ecdf4d51":"# Finding the value of the logistic function for a single pair of values of the features\ndef find_probability(weights, x1, x2):\n    return weights[0] + weights[1]*x1 + weights[2]*x2 + weights[3]*x1*x1 + weights[4]*x2*x2\n\n# Finds the probability of being a certain category for many points\ndef find_grid(grid, weights):\n    prob = []\n    for row in grid:\n        prob.append(1\/(1+np.exp(-find_probability(weights, row[0], row[1]))))\n    return np.array(prob)\n\n# Creating the 2d area we will plot our decision line in\nx1 = np.linspace(0, 1.1, 10)\nx2 = np.linspace(-1.0, 0.5, 10)\nax1, ax2 = np.meshgrid(x1,x2)\ngrid = np.c_[ax1.ravel(), ax2.ravel()]","08975793":"# PLots out probabilities in 2d space in order to visialise them\ndef plot_prob_grid(theta, name='Category'):\n    # gets the probability array\n    prob_grid = find_grid(grid, theta).reshape(10,10)\n\n    fig, ax = plt.subplots(dpi = 100)\n\n    # Plots the probabilities as a contour plot\n    contour = ax.contourf(ax1, ax2, prob_grid)\n    ax_c = fig.colorbar(contour)\n    ax_c.set_label(f\"$P(y = Cat. {name})$\")\n    ax_c.set_ticks([0, .25, .5, .75, 1])\n\n    ax = plt.scatter(df_X.loc[df_y['label'] == 0, 'feat1'], df_X.loc[df_y['label'] == 0, 'feat2'],\n                c='r', label = 'Cat. 0', s=1)\n    ax = plt.scatter(df_X.loc[df_y['label'] == 1, 'feat1'], df_X.loc[df_y['label'] == 1, 'feat2'],\n                c='m', label = 'Cat. 1', s=1)\n    ax = plt.scatter(df_X.loc[df_y['label'] == 2, 'feat1'], df_X.loc[df_y['label'] == 2, 'feat2'],\n                c='b', label = 'Cat. 2', s=1)\n    plt.title(f'Decision boundary for category {name}')\n    plt.xlabel('feat 1')\n    plt.ylabel('feat 2')\n    plt.legend()\n\n    plt.show()\n    \nplot_prob_grid(theta_0, name = '0')\nplot_prob_grid(theta_1, name = '1')\nplot_prob_grid(theta_2, name = '2')","9110437b":"# Calculating the probability of each category for all the points in our train\ndf_y['prob_0'] = log_func(X, theta_0)\ndf_y['prob_1'] = log_func(X, theta_1)\ndf_y['prob_2'] = log_func(X, theta_2)\n\n# Fidning the highest value of our probabilites\ndf_y['Pred_label'] = df_y[['prob_0','prob_1', 'prob_2']].idxmax(axis=1)\ndf_y['Pred_label'] = df_y['Pred_label'].replace({\"prob_0\": 0,\n                                                 \"prob_1\": 1,\n                                                 \"prob_2\": 2})\n\nprint(df_y.head())\nprint('\\nPearson R correlation - ', df_y['label'].corr(df_y['Pred_label']))","e5002c51":"print(df_X_test.head())\n# Feature scaling for the test values\ndf_X_test['feat1'] = df_X_test['feat1'] \/ (df_X_test['feat1'].max() - df_X_test['feat1'].min())\ndf_X_test['feat2'] = df_X_test['feat2'] \/ (df_X_test['feat2'].max() - df_X_test['feat2'].min())\n\n# Getting the polynomial terms for the test\ndf_X_test['feat1^2'] = df_X_test['feat1'] ** 2\ndf_X_test['feat2^2'] = df_X_test['feat2'] ** 2\n\n# Extracting the input data to calculate probabilities from\nX_test = df_X_test.values\n\n# Calculating probabilities of each category using the parameters we calculated earlier\ndf_y_test['prob_0'] = log_func(X_test, theta_0)\ndf_y_test['prob_1'] = log_func(X_test, theta_1)\ndf_y_test['prob_2'] = log_func(X_test, theta_2)\n\n# Fidning the highest value of our probabilites\ndf_y_test['Pred_label'] = df_y_test[['prob_0','prob_1', 'prob_2']].idxmax(axis=1)\ndf_y_test['Pred_label'] = df_y_test['Pred_label'].replace({\"prob_0\": 0,\n                                                 \"prob_1\": 1,\n                                                 \"prob_2\": 2})\n\nprint(df_y_test.head())\nprint('\\nPearson R correlation - ', df_y_test['label'].corr(df_y_test['Pred_label']))","4483baef":"Here we see the probability arrays. yellow is a high probability of being a certain category and blue is low. The line between them at $P(y=1) = 0.5$ is called the decision boundary. \n\nWe can see a clear and consistnet divide between category 1 and not category 1. However, the decision boundary between category 0 and 2 shows many data points with a higher probability of the wrong category assignment. This was inevitable however given our features and in a real life system we would need more features to split these two more accurately.","503f46ed":"We split our data into train and test. We will now continue with the train data and go back to the test data when we have made our model.\n\nWe have converted our three category target label from $y \\in \\{0,1,3\\}$ to label_0 $\\in \\{0,1\\}$, label_1 $\\in \\{0,1\\}$ and label_2 $\\in \\{0,1\\}$. This turns out multiclass classification into three binary classification problems. From here we can just implement usual logistic regression three times to gain a probability of each instance being in category 0, category 1 and category 2.","ce67db01":"We now do the same process with our test data. This means scaling the features and creating new polynomial features. This was done separately with the training data set to avoid any information from the test set being included in the training. The new input parameters are extracted and are used with our trained parameters to give probabilites for each instance.\n\nThe final correlation is actually higher than for the training dataset. This is pretty unusual but is likely just that the test dataset happened to be a bit neater with a little less overlap.","2682ccd9":"We have used only two features in order to be able to visualise the data and the decision boundaries we will calculate. However, this can be used for any number of variables and only in the extremely large cases ($> 10^{5}$) would we need to use more sophisticated algorithms to help convergence.\n\nThe data is divided into three categories. We can see substantial overlap between categories 0 and 2 which will undoubtedly cause some errors in our final model as there are plenty of data points that could easily be in either category. We can see that it would be impossible to split all three categories with a straight line. A split down the middle of category 0 and 2 would no exclude points from category 2. We will therefore need to use polynomial terms. However, it is a relatively simple spread so we will avoid overfitting by only using feat$^{2}$ terms.\n\nWe have included the bias as a pseudo feature with all its values set to 1. This means that the weighting will be added to every single instance and so will act as a bias.","7b068042":"# Multiclass multivariable logistic regression\n\nThis notebook should sere as a guide in how to implement multiclass logistic regression for simple systems so that you can scale this up for your problems. (Note: much of what has been coded explicitly for the three categories would likely need to be changed to a loop for large number of features. I have not in this notebook in order to make it as clear as possible what is going on). \n\nThere is an explanation of the mathematics behind logistic regression for a binary classification problem in here but it is not in as much detail as my other [notebook on logistic regression for binary classification](https:\/\/www.kaggle.com\/kieranfox\/logistic-regression-tutorial-from-scratch-in-py). Please see this guide for a more detailed explanation and instructions on how to implement it in python.\n \nHere we use the sklearn function 'make_blobs' to generate data with a number of categories so we can test multiclass regression.","3781243f":"Here we have scaled down our features to be between -1 and 1. This will aid gradient descent as no particular parameter will have a much large gradient than another. They are then squared to give us our polynomial terms. We can make a linear relationship of the feature squared to give non-linear shape to the decision boundary.\n\nWe then set our alpha value as the highest in can be while still maintaining consistent decline of the cost function. This has been altered after checking the cost function plot.","f50c46ff":"Here we see our inital value for the weighting parameters changed to more accurately fit the data. The cost function decreases consistently for all three categories. We see a slightly slower descent for category 0 suggesting a more complicated shape. Finally we see that the lowest final cost is for category 1 which is expected as this is the only category with no overlap with another category.","40d1fe52":"Find probability gives us the probability of being a certain category for a certain combination of feature values. The weights we input determine which category we are looking for. Find grid then uses this function to find the values of a 2d array of feature combinations. This will be used for plotting the decision boundary.","f6d2791b":"We calculate theprobablity of each instance being each category and then take the highest probability as being most likely to be the target. Our correlation shows that our odel fits the data well but is not perfect. This is becasue of the overlap between categories 0 and 2.","df45343c":"The log function takes in input data of our features and weights with which to scale our features, and outputs a probability $P(y=1)$ from 0 to 1. This can be used for each instance to give a probability of each category.\n\nThe cost function gives us an indication of how good our parameters are at predicting the right categories. If our probability value is far from the actual value of 0 or 1, we get a large cost and vice versa. Minimizing this ensures we get a good fit to our target values.\n\nThe gradient descent is the partial derivative of the cost function. The slope of the cost function for each parameter indivdually tells us how to alter that particular parameter in order to reduce the cost function. We control the descent with the learning rate ($\\alpha$) so that we dont overshoot the minimum and increase up the other side of the cost function slope."}}