{"cell_type":{"fdad9abb":"code","6dc6164d":"code","43a410f1":"code","f0933b79":"code","35c9f792":"code","40875ca3":"code","83e26283":"code","785366f2":"code","6d1fa7fa":"code","3ee80626":"code","675fb977":"code","b4a0224a":"code","a2260be6":"code","1689ac95":"code","8478ac12":"code","c253ec78":"code","7cf0b570":"code","f115524e":"code","78c8bd34":"code","59eb6c2e":"code","8bf169d4":"code","974e88ff":"code","ba5e712e":"markdown","1418c53d":"markdown","98e7b8f9":"markdown","e59f8675":"markdown","9d811a03":"markdown","1423aad2":"markdown","980b172a":"markdown","86607715":"markdown","65e4fc84":"markdown","c6835fe2":"markdown","de3b5060":"markdown","875a0f46":"markdown","ff08727d":"markdown"},"source":{"fdad9abb":"#! \/bin\/env python3\n\nimport json\nimport os\nimport pprint\nfrom dataclasses import dataclass\nfrom typing import List\nfrom enum import Enum\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.lancaster import LancasterStemmer\n\n\nclass EntityType(Enum):\n    CELL_LINE = 'CellLine'\n    MUTATION = 'Mutation'\n    SPECIES = 'Species'\n    GENUS = 'Genus'\n    STRAIN = 'Strain'\n    GENE = 'Gene'\n    DOMAIN_MOTIF = 'DomainMotif'\n    CHEMICAL = 'Chemical'\n    DISEASE = 'Disease'\n\nclass SectionType(Enum):\n    TITLE = 'title'\n    ABSTRACT = 'abstract'\n    INTRODUCTION = 'introduction'\n    METHODS = 'methods'\n    CONCLUSION = 'conclusion'\n    RESULTS = 'results'\n    DISCUSSION = 'discussion'\n    REFERENCES = 'references'\n    OTHER = 'other'\n\n@dataclass\nclass Location:\n    offset: int\n    length: int\n\n@dataclass\nclass Entity:\n    type: EntityType\n    text: str\n    location: List[Location]\n    source: SectionType\n\n@dataclass\nclass Section:\n    text: str\n    section_type: SectionType\n\n@dataclass\nclass Paper:\n    id: str\n    _id: str\n    entities: List[Entity]\n    sections: List[Section]\n    \n\ndef resolveSectionType(section_str: str) -> SectionType:\n    for st in SectionType:\n        if section_str.lower().find(st.value):\n            return st\n    return SectionType.OTHER\n\n        \npaper_limit = float(\"inf\")\ncurrent_index = 0\n\nprocessed_papers:List[Paper] = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/cord19-named-entities\/entities\/pmcid'):\n    for filename in filenames:\n        current_index += 1\n        if current_index > paper_limit:\n            break\n        with open(os.path.join(dirname, filename), 'r') as f:\n            data = json.load(f)\n            title = abstract = None\n            entities = []\n            sections = []\n            for passages in data['passages']:\n                section_type = resolveSectionType(passages['infons']['section'])\n                sections.append(\n                    Section(\n                        passages['text'],\n                        section_type,\n                    )\n                )\n                for extracted_entity in passages['annotations']:\n                    entities.append(\n                        Entity(\n                            EntityType(extracted_entity['infons']['type']),\n                            extracted_entity['text'],\n                            [Location(l['offset'], l['length']) for l in extracted_entity['locations']],\n                            section_type,\n                        )\n                    )\n            processed_papers.append(\n                Paper(\n                    data['id'],\n                    data['_id'],\n                    entities,\n                    sections,\n                )\n            )\n\nprint('--data loaded--')\nprint('# of papers: ', len(processed_papers))\nentity_number = sum([\n    len(paper.entities)\n    for paper in processed_papers\n])\nprint('# of entities: ', entity_number)","6dc6164d":"# Filter for papers that specifically mention model organisms\nmodel_organism_keywords = ['model organism', 'animal model']\nmodel_organism_papers = [\n    paper\n    for paper in processed_papers\n    if any(\n        any(\n            phrase in section.text\n            for phrase in model_organism_keywords\n        )\n        for section in paper.sections\n    )\n]\nprint(len(model_organism_papers), 'papers mention model organisms')\n","43a410f1":"# Stem and tokenize words\n# Stemming is a bit over aggressive... (e.g. 'human' -> 'hum')\n# But it's alright because we maintain the same stemming to\n# generate the word2vec model later, and we create\n# species_lookup to map back later.\ndef normalize_animal(ls, animal):\n    return ' '.join([ls.stem(word.lower()) for word in word_tokenize(animal)])\n    \n\nentities_by_paper = [\n    set([(e.text, e.type) for e in entities])\n    for entities in [\n        paper.entities\n        for paper in model_organism_papers\n    ]\n]\nspecies = [\n    entity[0]\n    for entity_set in entities_by_paper \n    for entity in entity_set\n    if entity[1] == EntityType.SPECIES\n]\n\n\nls = LancasterStemmer()\nnormalized_species = [normalize_animal(ls, a) for a in species]\nspecies_lookup = {\n    normalize_animal(ls, a): a\n    for a in species\n}\ncount_lookup = {\n    animal: normalized_species.count(animal)\n    for animal in set(normalized_species)\n}\ncounts = [\n    (a, species_lookup[a], count_lookup[a])\n    for a in set(normalized_species)\n]\nmost_common_species = sorted(\n    counts, \n    key=lambda s: s[2], \n    reverse=True\n)\n\nprint('Total of', len(species), 'species mentions tallied')\nmost_common_species[:20]","f0933b79":"import re\n\n# This is somewhat clumsy ... :^\/, unfortunately it's the best I've got\ndef filter_non_model_animals(species):\n    filtered_species = []\n    for animal_tuple in species:\n        normalized_name, name, count = animal_tuple\n        # Filter out acronyms (e.g. \"HIV\", \"MERS\")\n        if re.search('[A-Z]{2}', name):\n            continue\n        # Filter out viruses\n        if 'virus' in name.lower():\n            continue\n        # Filter out names containing a digit (e.g. \"H5N1\", \"50s\")\n        if re.search('\\d', name):\n            continue\n        # Filter out names containing a \"-\" (e.g. \"Mers-Cov\", \"HCoV-HKU1\")\n        if '-' in normalized_name:\n            continue\n        if name in ['NiV', 'SeV', 'CoV', 'HeV', 'Mtb']:\n            continue\n        filtered_species.append(animal_tuple)\n    return filtered_species\n  \n# Take the top 1K\nmost_common_species = filter_non_model_animals(most_common_species)[0:1000]\nmost_common_species[0:20]","35c9f792":"with open('..\/input\/whotherapeutics\/therapeutics.json') as json_data:\n    therapeutic_data = json.load(json_data)\ntherapeutics = []\nfor dictionary in therapeutic_data:\n    therapeutics.extend(dictionary['product_type'])\ntherapeutics[:10]\n","40875ca3":"# !pip install ipywidgets matplotlib pandas spacy tqdm\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz","83e26283":"%%bash -e\nif ! [[ -f .\/xyz2mol.py ]]; then\n  wget https:\/\/raw.githubusercontent.com\/jensengroup\/xyz2mol\/master\/xyz2mol.py\nfi","785366f2":"!pip install py3Dmol\n!pip install -U chembl_webresource_client\nimport sys\n!conda install --yes --prefix {sys.prefix} -c rdkit rdkit","6d1fa7fa":"import glob\nimport json\nimport pandas as pd\nimport pickle\nimport spacy\nfrom spacy import displacy\nfrom spacy.matcher import Matcher\nfrom tqdm import tqdm\nimport en_ner_bc5cdr_md\nimport os\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom chembl_webresource_client.new_client import new_client\nimport rdkit\nfrom rdkit import Chem\nfrom rdkit.Chem import Draw\nimport py3Dmol # Amazing library for 3D visualization\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom ipywidgets import interact, interactive, fixed\nfrom IPython.display import Image\nimport cv2\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import decomposition\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\nimport sys\n","3ee80626":"def doi_to_url(doi):\n    if isinstance(doi, float):\n        return None\n    elif doi.startswith('http'):\n        return str(doi)\n    elif doi.startswith('doi'):\n        return 'https:\/\/' + str(doi)\n    else:\n        return 'https:\/\/doi.org\/' + str(doi)\n\ndf_meta = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\ndf_meta['url'] = df_meta.doi.apply(doi_to_url)","675fb977":"data_path = '..\/input\/CORD-19-research-challenge'\njson_files = glob.glob(f'{data_path}\/**\/**\/*.json', recursive=True)\n# Limit the JSON files, 208388 is too many...\njson_files = json_files[:50000]\n\ndef to_covid_json(json_files):\n    jsonl = []\n    for file_name in tqdm(json_files):\n        row = {\"doc_id\": None, \"title\": None, \"abstract\": '', \"body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n            \n            abstract = ''\n            if 'abstract' in data:\n                abstract_list = [\n                    abst['text']\n                    for abst in data['abstract']\n                ]\n                abstract = \"\\n\".join(abstract_list)\n                row['abstract'] = abstract\n\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n\".join(body_list)\n            row['body'] = body\n            \n        jsonl.append(row)\n    \n    return jsonl\n    \n\ndef get_data():\n    try:\n        with open('df_cache.pickle', 'rb') as f:\n            unpickler = pickle.Unpickler(f)\n            # if file is not empty scores will be equal\n            # to the value unpickled\n            df = unpickler.load()\n    except (FileNotFoundError, EOFError):\n        df = pd.DataFrame(to_covid_json(json_files))\n        with open('df_cache.pickle', 'wb') as f:\n            pickle.dump(df, f)\n    return df\n\ndf = get_data()\nprint(df.shape)\ndf.head(3)","b4a0224a":"df.describe()","a2260be6":"def no_title(row):\n    return not row.title.strip()\n\ndef no_abstract(row):\n    return not row.abstract or not row.abstract.strip()\n\ndef no_body(row):\n    return not row.body.strip()\n\ndef no_title_abstract_body(row):\n    return no_title(row) and no_abstract(row) and no_body(row)\n\nmask = df.apply(no_title_abstract_body, axis=1)\nprint('Number of articles that have no text data at all:', df.loc[mask].shape[0])","1689ac95":"# insert missing values for empty strings\ndf.loc[df.apply(no_title, axis=1), 'title'] = None\ndf.loc[df.apply(no_abstract, axis=1), 'abstract'] = None\ndf.loc[df.apply(no_body, axis=1), 'body'] = None\ndf.head(3)","8478ac12":"print('Missing value counts by column')\nlen(df) - df.count()","c253ec78":"import gc\ngc.collect()\n\ndf = df.dropna(subset=['abstract'])\n","7cf0b570":"covid19_names = {\n    'COVID19',\n    'COVID-19',\n    '2019-nCoV',\n    '2019-nCoV.',\n#     'novel coronavirus',  # too ambiguous, may mean SARS-CoV\n    'coronavirus disease 2019',\n    'Corona Virus Disease 2019',\n    '2019-novel Coronavirus',\n    'SARS-CoV-2',\n    'covid',\n    'coronavirus',\n}\n\ndef has_covid19(text):\n    for name in covid19_names:\n        if text and name.lower() in text.lower():\n            return True\n    return False\n\ndf['title_has_covid19'] = df.title.apply(has_covid19)\ndf['abstract_has_covid19'] = df.abstract.apply(has_covid19)\n# df['body_has_covid19'] = df.body.apply(has_covid19)\ndf_covid19 = df[df.title_has_covid19 | df.abstract_has_covid19]\n\n\ndf_covid19['all'] = df_covid19[['title','abstract']].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n","f115524e":"\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gensim\n\n\nfrom gensim.models import word2vec\nfrom gensim import corpora, models, similarities\n\n\nfrom sklearn.cluster import KMeans\nimport scipy.cluster.hierarchy as sch\nimport re\n\nimport nltk\nfrom nltk.corpus import brown\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\n","78c8bd34":"data = df_covid19['all'] #13344 total\n\ntexts_tokenized = [[word.lower() for word in word_tokenize(document)] for document in data ]\n\n# remove stopwords\nenglish_stopwords = stopwords.words('english')\ntexts_filtered_stopwords = [[word for word in document if not word in english_stopwords] for document in texts_tokenized]\n\n# remove punctuations\npunctuations = [',','.',':',';','?','(',')','[',']','&','!','*','@','#','$','%']\ntexts_filtered = [[word for word in document if not word in punctuations] for document in texts_filtered_stopwords]\n\n# stem\nst = LancasterStemmer()\ntexts_stemmed = [[st.stem(word) for word in docment] for docment in texts_filtered]\n\n#size is the dimensionality of the feature vectors.\n#window is the maximum distance between the current and predicted word within a sentence.\n#min_count = ignore all words with total frequency lower than this.\n#workers = use this many worker threads to train the model (=faster training with multicore machines).\n#iter = number of iterations (epochs) over the corpus. Default is 5.\nmodel = gensim.models.Word2Vec(texts_filtered, size=1000, window=10, min_count=5, workers=5, iter=20)","59eb6c2e":"def clean_input_tensor(model, tensor):\n    tensor = {\n        ' '.join([r.lower() for r in row])\n        for row in tensor\n        if all(r.lower() in model.wv.vocab for r in row)\n    }\n    return [row.split(' ') for row in tensor]\n\ndef get_similarity_matrix(model, rows, columns, row_name):\n    rows = clean_input_tensor(model, rows)\n    columns = clean_input_tensor(model, columns)\n    matrix = [\n        [' '.join(row)] + [\n            model.wv.n_similarity(row, col)\n            for col in columns\n        ]\n        for row in rows\n    ]\n    return pd.DataFrame(matrix, columns = [row_name] + [' '.join(c) for c in columns])  \n\ncolumns = [['effective'], ['preventive'],['successful'],['controlled']]\nrows = [[t] for t in therapeutics]\ntherapeutic_similarties_df = get_similarity_matrix(model, rows, columns, 'therapeutic')\ntherapeutic_similarties_df.sort_values(by='effective', ascending=False)","8bf169d4":"# Pick out the organisms with the highest similarity to \"model organism\" to filter irrelevant organisms\nprocessed_species = [species[0].lower().split(' ') for species in most_common_species]\nmodel_organism_measure = [\n    (animal_tokens, model.wv.n_similarity(animal_tokens, ['animal', 'model', 'organism']))\n    for animal_tokens in processed_species\n    if all(token in model.wv.vocab for token in animal_tokens)\n]\nmodel_organisms = sorted(\n    model_organism_measure, \n    key=lambda s: s[1], \n    reverse=True\n)[0:200]\n# I'm not sure  how well this filter works...\nmodel_organisms[0:5]","974e88ff":"\ncolumns = [[t] for t in therapeutics]\nrows = [animal[0] for animal in model_organisms]\norganism_matrix_df = get_similarity_matrix(model, rows, columns, 'animal')\n\n\ndef create_sorted_matrix(organism_matrix_df, species_lookup, proximity_threshold=0.2):\n    animal_lookup = None\n    sorted_animal_models = {}\n    for column, rows in organism_matrix_df.iteritems(): \n        if column == 'animal':\n            animal_lookup = rows\n            continue\n        sorted_model_organisms = sorted(\n            [\n                (r[1], animal_lookup[r[0]])\n                for r in enumerate(rows)\n            ], \n            key=lambda s: s[0], \n            reverse=True\n        )[0:20]\n        sorted_animal_models[column] = [\n            species_lookup[o[1]] if o[0] > proximity_threshold else ''\n            for o in sorted_model_organisms \n        ]\n    return pd.DataFrame.from_dict(sorted_animal_models)\n\ncreate_sorted_matrix(organism_matrix_df, species_lookup)\n","ba5e712e":"\nLoad and Clean Data","1418c53d":"## Verification of Results\n\nA quick literature search shows that most results are meaningful and can be read directly as such and such drug has been tested with such and such animal :): \n\n|Drug|Model Organism|Source|\n|---|---|---|\n|Ritanovar|Marmosets|[source](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/26198719)|\n|ifn-\u03b1|Ascaris|[source](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3080889\/)|\n|ifn-\u03b2|Ascaris|[source](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3080889\/)|\n|Hydroxychloroquine|Mice|[source](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC6417644\/)| \n|Corticosteroids|Humans|[source](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/29607554)|\n|Kaletra|Mice|[source](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4524660\/)|\n\n## Further Improvement:\n\n* Drugs which have yet to be animal tested (such as ([CR3022](https:\/\/science.sciencemag.org\/content\/early\/2020\/04\/02\/science.abb7269?rss=1)) seem to be overly greedily matched with animals (such as goats, masked palm civets, bank voles, and red foxes).\n* Drugs which share names with certain diseases (such as MERS-4) have similarity scores clsoe to the animals from which said diseases originated ([camels in the case of MERs](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/24896817))\n\n","98e7b8f9":"## Continuation\/TODOs:\n* Return the sets of papers that study each animal-therapeutic combination. This could be an important tool to aid researchers.\n* Find a better way to filter model organisms from the full list of all animals. Currently we rely on clumsy filters: only including articles that mention \"animal models\" or \"model organisms\", and regexes to filter acronyms, virus names, names with digits, etc. We've also implemented filtering model organism names that are not close to [\"model\", \"organism\"] in our word2vec embedding but it doesn't seem very effective...\n* Cluster species names (e.g. \"patient\" and \"human\" should be clustered together). However we have found that using the embedding trained from CORD-19 does not produce sensible clusters by AffinityPropagation or t-SNE. Using a different more general embedding may be necessary.\n","e59f8675":"## 2. Import Therapeutics from WHO's [Public Appendix](https:\/\/www.who.int\/blueprint\/priority-diseases\/key-action\/Table_of_therapeutics_Appendix_17022020.pdf)","9d811a03":"## 3. Load & pre-format CORD-19 JSON ([source](****https:\/\/www.kaggle.com\/maria17\/cord-19-explore-drugs-being-developed))\n\nImport the necessary libraries.  ","1423aad2":"## Goals\nThis notebook primarily strives to help us better understand which animal models are being used to test which therapeutics. This question is relevant to these 3 subtasks under the task \"What do we know about vaccines and therapeutics?\"\n* Exploration of use of best animal models and their predictive value for a human vaccine.\n* Efforts to develop animal models and standardize challenge studies\n* Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics] \n\n## Contribution\nThe majority of Kaggle contributions for therapeutics have been hyper focused on explicitly looking at the effects on humans. We believe there's value in looking at potential animal models as the body of research grows and more treatments are taken to the clinical trial stage. To take a look at the distribution animal models first you need a list of animals and a list of therapeutics. This notebook not only uses data from CORD-19 but also at a high quality publicly available knowledge graph extracted from CORD-19, and the WHO's published appendix of therapeutics. It is our belief that these two datasources can be extremely generalizable and powerful to tasks beyond the scope of this notebook, and hope for their wider adoption.\n\n## Acknowledgements\n\n1. Species Named Entities scraped from the CORD-19 dataset within a larger Knowledge Graph built by Qingyun Wang, Heng Ji, Jiawei Han, Shih-Fu Chang, and Kyunghyun Cho as a collaboration between UIUC, Columbia, and NYU. The full knowledge graph was originally published to the [Illinois CS website](http:\/\/blender.cs.illinois.edu\/covid19\/) and now made available on Kaggle as a [dataset](https:\/\/www.kaggle.com\/yitongtseo\/cord19-named-entities) with permission of the authors.\n2. Therapeutic appendix from WHO. Publicly available as a [PDF](https:\/\/www.who.int\/blueprint\/priority-diseases\/key-action\/Table_of_therapeutics_Appendix_17022020.pdf), and made available on Kaggle as a well formatted [JSON dataset](https:\/\/www.kaggle.com\/yitongtseo\/whotherapeutics)\n3. Code for CORD-19 JSON loading, and data pre-formatting written by [Maria and Gtteixeira](****https:\/\/www.kaggle.com\/maria17\/cord-19-explore-drugs-being-developed)\n\n\n","980b172a":"## 4. Train Word2Vec model from CORD-19 Titles & Abstracts","86607715":"# RESULTS!\n\n## 5. Therapeutics by Effectiveness\nBy calculating each therapeutic's similarity with positive words (\"effective\", \"preventive\", \"successful\", \"controlled\") in our word2vec embedding, we can score their perceived effectiveness.","65e4fc84":"All document IDs are unique, nothing to tidy up. But there seem to be missing titles, abstracts and possibly missing bodies.","c6835fe2":"# RESULTS!\n## 6. Model Organisms by Therapeutic\n### Which therapeutics have been tested with which model organisms?\n\nBy calculating the similarity between each therapeutic embedding and animal model embedding in our word2vec embedding, we can determine which drugs are associated with which animal models. And hopefully establish which drugs have been tested with which animals :) ","de3b5060":"Load JSON Data","875a0f46":"We will be working with abstracts. They provide an appropriate level of detail for the question at hand. Thus, we will drop all documents that do not have an abstract.","ff08727d":"## 1. Import [Extracted Species Entities](http:\/\/blender.cs.illinois.edu\/covid19\/) from CORD-19"}}