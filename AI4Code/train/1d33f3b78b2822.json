{"cell_type":{"3b05ef6e":"code","43d6b909":"code","f13af6ba":"code","0f0a5041":"code","df339263":"code","067f5b7b":"code","c62d7f34":"code","41f36b62":"code","c6e46618":"code","737ed020":"code","41398848":"code","4a41b7c0":"code","2540c1f0":"code","5ff51f3a":"code","16f6128a":"code","65de0a3c":"code","0665fd48":"code","55a4c952":"code","bed2656e":"code","01889d18":"code","eb1894f5":"code","426dc4ba":"code","e47caa92":"code","25ac7e89":"code","a3103733":"code","e508be16":"code","533dc7e0":"code","06574128":"code","ff1412c9":"code","bce388fe":"code","9bd97319":"code","cc0c2fb0":"code","d55cdf52":"code","edf7905b":"code","4604cc35":"code","a92284f9":"code","b42c414f":"code","9be8b22e":"code","8f5a6234":"code","ad2c4b72":"markdown","de8e3501":"markdown","40ef3d23":"markdown","5098e807":"markdown","76ed2a05":"markdown","9c523afd":"markdown","60ba498c":"markdown","049e408d":"markdown","e64b58ba":"markdown","02d03985":"markdown","d6316dd6":"markdown","03f10242":"markdown","ed16e1e3":"markdown","27b53308":"markdown","2f78368c":"markdown","5913e10b":"markdown","772b0421":"markdown","c637ac5a":"markdown","c28f4763":"markdown","95e9c894":"markdown","2065a6c1":"markdown","e2f36b81":"markdown","d4dc84c6":"markdown","75721c8d":"markdown","dbde0689":"markdown","31e42a38":"markdown","c694c2fb":"markdown","47292f49":"markdown","ecad3846":"markdown","66eafbf8":"markdown","fb123d02":"markdown","488e843d":"markdown","307114a4":"markdown","165b600a":"markdown","6b29911e":"markdown","daddb46e":"markdown","0b737f59":"markdown","35737532":"markdown","afecaacd":"markdown","8d9bad22":"markdown","65c20a64":"markdown","86f07699":"markdown","62951984":"markdown","d8df4c95":"markdown","06b3d37b":"markdown","492a0f95":"markdown","bad804c6":"markdown","c91b4669":"markdown","63c7824e":"markdown","fba1ffa3":"markdown","6f346b11":"markdown","03482005":"markdown","3576cbb7":"markdown","4f1548c7":"markdown","effbe6de":"markdown","9f5b9459":"markdown","07471f72":"markdown","05e74d21":"markdown"},"source":{"3b05ef6e":"import pandas as pd\ndata = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\bossa_nova_dataset.plk')\ndata.head()\n","43d6b909":"a = 1\nfor i in data.columns:\n    print(f'Coluna {a}: {i}')\n    a += 1","f13af6ba":"data.columns = ['nome_musica', 'artista', 'letra_musica', 'compositor', 'idioma']\ndata.head()","0f0a5041":"freq_artistas = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\freq_artistas.plk')\nfreq_artistas.head(5)","df339263":"fre_compositor = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\freq_compositor.plk')\nfre_compositor.head(5)","067f5b7b":"freq_idioma = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\freq_idioma.plk')\nfreq_idioma.head()","c62d7f34":"data_pt = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\bossa_nova_dataset_pt.plk')\ndata_pt.columns = ['nome_musica', 'artista', 'letra_musica', 'compositor', 'idioma']\ndata_pt.head(5)\ndata_pt.dropna(inplace=True)\ndata_pt.reset_index(drop=True, inplace=True)\ndata_pt.columns = data_pt.columns.str.strip()\ndata_pt","41f36b62":"data_en = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\bossa_nova_dataset_en.plk')\ndata_en.head()","c6e46618":"import nltk","737ed020":"tokens = []\nfor i in data_pt.letra_musica:\n    tokens.append(nltk.tokenize.word_tokenize(i))\ndata_pt.letra_musica = tokens","41398848":"data_pt.head()","4a41b7c0":"tokens_sem_sinais = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\tokens_sem_sinais.plk')\ntokens_sem_sinais.head()","2540c1f0":"data_pt.letra_musica = tokens_sem_sinais.Tokens\ndata_pt.head()","5ff51f3a":"lista_normalizada = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\tokens_normalizados.plk')\nlista_normalizada.head()","16f6128a":"data_pt.letra_musica = lista_normalizada.tokens\ndata_pt.head()","65de0a3c":"songs_lyrics_artista = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\songs_lyrics_artista.plk')\nsongs_lyrics_artista.index = songs_lyrics_artista.artistas\nsongs_lyrics_artista.drop(columns='artistas', inplace=True)\nsongs_lyrics_artista","0665fd48":"from sklearn.feature_extraction.text import CountVectorizer","55a4c952":"matriz_data_pt = pd.read_pickle(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\base_dados\\matriz_vetorizada.plk')\nmatriz_data_pt.head()","bed2656e":"matriz_data_pt = pd.DataFrame(matriz_data_pt.groupby(by=matriz_data_pt.index).sum())\nmatriz_data_pt.head()","01889d18":"# matriz_data_pt = matriz_data_pt.transpose() \n    # Comentei essa liha acima para n\u00e3o correr os risco de acidentalmente transpor de novo!\n    \nmatriz_data_pt.head()","eb1894f5":"top_dict= {}\n\na = 0\nfor i in matriz_data_pt.columns:\n    top = matriz_data_pt[i].sort_values(ascending=False).head(30)\n    top_dict[i] = list(zip(top.index, top.values))\n \n\nfor artista, palavra in top_dict.items():\n    print(artista + '\\n')\n    print(', '.join([str(i) for i in palavra[:14]]))\n    print('\\n _____________________________________________________________________________')\n    a = a+1\n    if a == 3:\n        break\n        \n# Reduzi a quantidade de dados imprimidos para facilitar a leitura do notebook.\n\n    ","426dc4ba":"from collections import Counter","e47caa92":"palavras_comuns = []\nfor artista in matriz_data_pt.columns:\n    top = [palavra for (palavra, contagem) in top_dict[artista]]\n    for t in top:\n        palavras_comuns.append(t)\npalavras_comuns[:10]","25ac7e89":"Counter(palavras_comuns).most_common()[:10]","a3103733":"stopwords = nltk.corpus.stopwords.words('portuguese')\n\nadd_stopword = [palavra for palavra, contagem in Counter(palavras_comuns).most_common() if contagem > 40]\nadd_stopword.remove('amor')","e508be16":"stop_words = nltk.corpus.stopwords.words('portuguese')","533dc7e0":"add_stopword.extend(['pra', 'oh', 'quero', 'mim', 'vai', 'vou', 'ai', 'assim', 'tudo', 'ser', 'vem', 'bem', 'sei', 'gente',\n                 'tudo', 'l\u00e1', 'assim', 'mim', 'vem', 't\u00e3o', 'pra', 'boom', 'bom', 'chic', 'bim', 'dizer', 'ter',\n                 'mundo', 'nad', 'porque', 'ai', 'd\u00e1', 'gente', 'vida', 'lembro', 'refr\u00e3o', 'ser', 'pif', 'noite', 'canto',\n                 'faz', 't\u00ea', 'favor', 'trazer', 'fa\u00e7a', 'ficar', 'boa', 'novo', 'rumo', 'jeito', 'deixo','dia', 'tudo', 'lado',\n                 'dar'])\nstop_words.extend(add_stopword)\nstop_words[:10]","06574128":"import matplotlib.pyplot as plt\nfrom PIL import Image\nfrom scipy.ndimage import gaussian_gradient_magnitude\nimport numpy as np\nfrom wordcloud import WordCloud, ImageColorGenerator","ff1412c9":"texto = songs_lyrics_artista.lyrics.values\ntexto[0]","bce388fe":"def word_cloud_visualization(stop_words, dados, nome_arquivo, qnt_palavras):\n    \n\n    \n    \n    mask_color = np.array(Image.open(r\"C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\mascaras\\mask-image.png\"))\n    mask_color = mask_color[::3, ::3]\n    mask_image = mask_color.copy()\n    mask_image[mask_image.sum(axis=2) == 0] = 255\n    \n\n    edges = np.mean([gaussian_gradient_magnitude(mask_color[:, :, i]\/255., 2) for i in range(3)], axis=0)\n    mask_image[edges > .08] = 255\n    \n    wc = WordCloud(max_words=qnt_palavras, stopwords = stop_words, background_color ='white',\n                   max_font_size=150, random_state = 42, width=1200, height=600, mask=mask_image)\n\n    \n\n\n    wc.generate(str(texto))\n    image_colors = ImageColorGenerator(mask_color)\n    wc.recolor(color_func=image_colors)\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    \n    plt.figure(dpi = 300)\n    pasta = r\"C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\word_cloud\\\\\"\n    wc.to_file(pasta[:-1] + nome_arquivo + \".png\")\n    plt.show()","9bd97319":"word_cloud_visualization(stop_words, texto, 'analise_0', 100)","cc0c2fb0":"add_stopwords_II = ['fa\u00e7o', 'contigo', 'nada', 'cada', 'ver', 'momento', 'onde', 'ent\u00e3o', 'olhar', 'todo', 'consegue', \n                   'enquanto', 'chama', 'falar', 'lugar', 'pode', 'podia', 'anda', 'maior', 'fazer', 'sempre', \n                   'tal', 'pro', 'tanto', 'possa', 'final', 'ningu\u00e9m']\nstop_words.extend(add_stopwords_II)\nword_cloud_visualization(stop_words, texto, 'analise_1', 100)","d55cdf52":"add_stopwords_III = ['pipi', 'vire', 'controlar', 'corpo', 'falo', 'aqui', 'entro', 'tarde', 'zero', 'dure', 'casa', 'antes',\n                    'procure', 'm\u00e3o', 'outro', 'vazia', 'algu\u00e9m', 'ch\u00e3o', 'total', 'andam', 'saio', 'posto', 'teto', 'engra\u00e7ada',\n                    'penico', 'n\u00famero', 'v\u00ea', 'arrume']\nstop_words.extend(add_stopwords_III)\nword_cloud_visualization(stop_words, texto, 'analise_2', 100)","edf7905b":"add_stopwords_IV = [\"zero'\", 'ali', 'caindo', 'hoje', 'raz\u00e3o','nela', 'valor', \"voc\u00ea'\", 'perde', 'lugares', 'feita', 'deixou',\n                   'ouvi', 'acabou', 'virem', 'quis', 'trazem', 'diz', 'ouvia', 'rua', 'outra', 'face', 'viver', 'claro',\n                   \"dure'\", 'dormir']\nstop_words.extend(add_stopwords_IV)\nword_cloud_visualization(stop_words, texto, 'analise_3', 100)","4604cc35":"add_stopwords_V = ['nesses', 'dizem', 'maciel', 'mesmos', 'denovo', 'forma', 'parece', 't\u00e1', 'v\u00e3o', 'aparecer', 'ms', 'ou\u00e7a',\n                  'fico', 'c\u00e1', 'bobos', 'atoa']\nstop_words.extend(add_stopwords_V)\nword_cloud_visualization(stop_words, texto, 'analise_4', 100)","a92284f9":"add_stopwords_VI = [\"seu'\", 'voltar', 'alguns', 'deixei', 'ficava', 'feitos', 'bons', 'atr\u00e1s', 'toda', 'tava', 'existe']\nstop_words.extend(add_stopwords_VI)\nword_cloud_visualization(stop_words, texto, 'analise_5', 100)","b42c414f":"add_stopwords_VII = ['consegui', \"lugar'\"]\nstop_words.extend(add_stopwords_VII)\nword_cloud_visualization(stop_words, texto, 'analise_6', 100)","9be8b22e":"add_stopwords_VIII = ['restou', 'queria', 'entrar', 'ainda', 'apesar', 'fez', 'dizia', 'trouxeram', 'sabe']\nstop_words.extend(add_stopwords_VIII)\nword_cloud_visualization(stop_words, texto, 'analise_7', 100)","8f5a6234":"import matplotlib.image as mpimg\nresultado_analise = mpimg.imread(r'C:\\Users\\mikei\\OneDrive\\Documentos\\analise_bossa_nova\\word_cloud\\analise_7.png')\nplt.figure(dpi = 300)\nplt.axis('off')\nplt.imshow(resultado_analise, interpolation='bilinear')","ad2c4b72":"Vamos remover os simbolos agora:","de8e3501":"Vamos tentar encontrar as palavras mais frequentes de cada m\u00fasica:","40ef3d23":"Como foi falado acima, a bossa nova visava a internacionaliza\u00e7\u00e3o da m\u00fasica brasileira. Aparentemente o feito foi parcialmente atingido, 67% das m\u00fasicas eram em portugu\u00eas, 11% em ingl\u00eas e 9% em espanhol! Grande feito.","5098e807":"Leitura de um dataset contendo as colunas: [nome_musica, artista, letra_musica, compositor, idioma] das musicas da Bossa nova para realizar as seguintes an\u00e1lises atrav\u00e9s da bibliota *pandas*:\n\n   * Realizamos contagem de frequ\u00eancias para descobrir os interpretes mais influentes;\n   * Realizamos contagem de frequ\u00eancias para descobrir os compositores mais influentes;\n   * Realizamos contagem de frequ\u00eancias para descobrir os idiomas mais comuns;\n\nFizemos procedimentos de NLP (natural language processing) utilizando a bicliota *nltk*, *sklearn* e *collections*:\n   \n   * Tokeniza\u00e7\u00e3o das letras;\n   * Vetoriza\u00e7\u00e3o de tokens utilizando sklearn;\n   * Descobrir a frequ\u00eancia de palavras por artistas;\n   * Determinar stop words, ou seja, palavras que n\u00e3o agregam nada em nossa an\u00e1lise;\n\nPara nossa visualiza\u00e7\u00e3o de dados n\u00f3s utilizamos as bibliotecas *matplotlib*, *numpy*, *wordcloud* e *PIL* para que pudessemos:\n   \n   * Definimos uma fun\u00e7\u00e3o para gerar nossa visaliza\u00e7\u00e3o;\n   * Definimos requisitos de remo\u00e7\u00e3o dessas palavras;\n   * Com base nas visualiza\u00e7\u00f5es geradas n\u00f3s adicionamos novas stopwords;","76ed2a05":"Agora que tokenizamos as letras, vamos verificar como est\u00e1 nosso dataset:","9c523afd":"Para a constru\u00e7\u00e3o dessa an\u00e1lise foi utilizada as seguintes bibliotecas:","60ba498c":"* pandas\n* nlkt\n* matplotlib\n* pickle\n* wordcloud\n* sklearn","049e408d":"Curioso saber que 20% das m\u00fasicas feitas no g\u00eanero bossa nova tem um compositor n\u00e3o conhecido. Outro fato curioso \u00e9 que Tom Jobim e Vinicius de moraes tanto tiveram bastante trabalhos individuais na composi\u00e7\u00e3o quanto tiveram juntos. Vamos guardar essa informa\u00e7\u00e3o para mais tarde!","e64b58ba":"Feito isso, vamos transpor nossa matriz para que os artistas fiquei nas colunas e as palavras na linha:","02d03985":"Vamos verificar quais colunas estamos trabalhando:","d6316dd6":"Github: [https:\/\/github.com\/xpcosmos](https:\/\/github.com\/xpcosmos)","03f10242":"Com o acontecimento do fato hist\u00f3rico da ditadura militar, notou-se haver uma maior presen\u00e7a de temas sociais nas m\u00fasicas, o que por sua vez significou a extin\u00e7\u00e3o  por volta de 1966, extin\u00e7\u00e3o essa te\u00f3rica, mas n\u00e3o pr\u00e1tica. A musica que procedeu a bossa nova foi a **MPB**. Esse g\u00eanero deixou ainda mais claro a instisfa\u00e7\u00e3o da classe art\u00edstica com o regime militar.","ed16e1e3":"Primeiro passo \u00e9 criarmos um DataFrame somente com as letras das m\u00fasicas e os artistas:","27b53308":"Perceba que temos muitas frequ\u00eancia com 'Vinicius de Moraes'. Para resolucionar esse nosso problema, vamos agrupar o DataFrame pelo index:","2f78368c":"Nesse projeto fizemos:","5913e10b":"### Visualiza\u00e7\u00e3o de dados e limpeza complementar","772b0421":"Visando entender mais sobre esse ritmo e sobre a import\u00e2ncia dele para o cen\u00e1rio musical brasileiro, utlizaremos uma base de dados contendo m\u00fasicas de bossa nova. Vamos extrair a _data_ utilizando t\u00e9cnicas de NLTK _(processamento de linguagem natural)_ e tentaremos trazer os insights atrav\u00e9s de uma biblioteca de _WordCloud_.","c637ac5a":"LinkdIn: [https:\/\/www.linkedin.com\/in\/mikeias-o-5a4b2a184\/](https:\/\/www.linkedin.com\/in\/mikeias-o-5a4b2a184\/)","c28f4763":"## Proposta","95e9c894":"Vamos criar um print() com todas as primeias 10 palavras e suas respectivas frequ\u00eancia dentro da matriz:","2065a6c1":"Olha que bacana! Apararentemente Vinicius de Moraes foi o nome mais not\u00e1vel da bossa nova tendo interpretado 317 m\u00fasicas, incr\u00edveis 7% de outros 102 outros artistas ","e2f36b81":"# Bossa nova: Um advento da m\u00fasica brasileira","d4dc84c6":"## Entendendo a base de dados","75721c8d":"### Tokeniza\u00e7\u00e3o de dados","dbde0689":"### Vetoriza\u00e7\u00e3o de dados","31e42a38":"Agora que removemos os sinais, vamos atribuir as novas letras ao nosso DataFrame e visulizar:","c694c2fb":"Podemos ver palavras com peso positivo como \"feliz\", \"rir\", \"prazer\" etc, assim com palavras com peso positivo, como \"solid\u00e3o\" e \"ang\u00fastia\".","47292f49":"Conclu\u00edmos que as m\u00fasicas da bossa nova eram para o grande p\u00fablico, com palavras de f\u00e1ceis compreens\u00e3o e muitos temas retratando experi\u00eancias amorosas boas e ruins, por\u00e9m as experi\u00eancias amorosas retradas em sua imensa maioria s\u00e3o experi\u00eancias boas no ponto de vis\u00e3o do eu l\u00edrico.","ecad3846":"Vamos dar uma verificada nos resultados que podemos encontrar com as palavras que est\u00e3o em nossos tokens. Para isso irei utilizar as bibliotecas matplotlib e WordCloud, PIL, numpy e scipy para trabalharmos com imagens e aplicarmos m\u00e1scaras interessantes.","66eafbf8":"Vamos eliminar as palavras mais comuns: ","fb123d02":"Conseguimos nossa primeira plotagem, mas ainda h\u00e1 alguma limpezas para realizar. H\u00e1 palavras que n\u00e3o s\u00e3o \u00fateis para nossa an\u00e1lise predominando. Vamos para a fase 2 da limepeza:","488e843d":"### Informa\u00e7\u00f5es para contato:","307114a4":"Podemos observar tamb\u00e9m a presen\u00e7a das palavras solid\u00e3o, infelizmente, ag\u00fastica, que podemos concluir que derivam da paix\u00e3o talvez n\u00e3o correspondida do eu l\u00edrico.","165b600a":"Vou mesclar todas as m\u00fasicas pois o que nos interessa aqui \u00e9 o g\u00eanero bossa nova como um todo.","6b29911e":"Vimos que bastante das m\u00fasicas retratam temas bem comuns como amor, paix\u00f5es, desejo e felicidade.","daddb46e":"O resultado do nosso estudo foi esse:","0b737f59":"Primeiro vamos importar nossa bilbioteca e entender de quais dados estaremos trabalhando.","35737532":"## Conclus\u00e3o","afecaacd":"Vemos tamb\u00e9m a palavra \"bares\" bem destacada. \u00c9 poss\u00edvel que haja uma tentativa de al\u00edvio do sofrimento amoroso atrav\u00e9s do \u00e1lcool, assim como a utiliza\u00e7\u00e3o do meio social em pares na tentiva de encontrar um romance.","8d9bad22":"A base de dados foi extra\u00edda de um site que dispobiliza variadas bases de dados, chamado [Kaggle](https:\/\/www.kaggle.com\/). A base de dados pode ser baixada e acessada [clicando aqui](https:\/\/www.kaggle.com\/mcarujo\/bossa-nova-lyrics)","65c20a64":"### Visualiza\u00e7\u00e3o de dados","86f07699":"## Bibliotecas utilizadas","62951984":"Vamos verificar algumas frequ\u00eancias aqui nesses dados:","d8df4c95":"Vamos agorar gerar nossa wordcloud para que possamos verificar as palavras que fazem sentido para nossa an\u00e1lise, caso n\u00e3o fa\u00e7a, devemos adicionar elas a lista e remover. Vamos manter um registro de todas as palavras em vari\u00e1veis, para caso voltemos atr\u00e1s, seja f\u00e1cil para remover.","06b3d37b":"Nosso objetivo principal por agora \u00e9 o entendimento da bossa nova no cen\u00e1rio brasileiro, mas seria interessante an\u00e1lise tamb\u00e9m aspectos da bossa nova na lingua inglesa. Vamos deixar um dataset para o portugu\u00eas e outro para o ingl\u00eas para um poss\u00edvel posterior an\u00e1lise.","492a0f95":"## Limpeza de dados","bad804c6":"\u00c9 interessante que criemos um loop para ver a quantidade de palavras mais comuns dentre as dos artistas:","c91b4669":"## Prepara\u00e7\u00e3o de dados","63c7824e":"Sabemos agora os maiores nomes desse g\u00eanero! Vamos tentar trabalhar com eles para facilitar nossa an\u00e1lise.","fba1ffa3":"Vamos tamb\u00e9m importar as chamadas _'stopwords'_ da biblioteca da nltk e mesclar com algumas palavras que n\u00e3o ir\u00e3o agregar nada em nossa an\u00e1lise.","6f346b11":"Vamos agora atribuir novamente essas lista normalizada para nosso DataFrame e visualizar:","03482005":"Eu optei por eliminar as que aparecem em at\u00e9 40 artistas em comum, mas voc\u00ea pode verificar qual se encaixa melhor no seu DataFrame. Vai depende do volume de dados e o tipo de an\u00e1lise que est\u00e1 fazendo!","3576cbb7":"Vamos importar a biblioteca _'CountVectorizer'_  para criar uma matriz com as palavras, assim, conseguiremos contabilizar melhor.","4f1548c7":"Temos cinco colunas contendo o t\u00edtulo da m\u00fasica, o nome do artista, a letra da m\u00fasica, o compositor e a linguagem da m\u00fasica. J\u00e1 da de fazer algumas coisas interessantes com essas informa\u00e7\u00f5es, mas antes, que tal traduzirmos para facilitar a legibilidade?","effbe6de":"Ap\u00f3s termos feito a remo\u00e7\u00e3o dos sinais, vamos padronizar nossas palavras em letras min\u00faculas","9f5b9459":"Note que optei por n\u00e3o eliminar 'Amor', pois acho que podemos tirar conclus\u00f5es do uso dessa palavra.","07471f72":"Vamos importar a fun\u00e7\u00e3o 'Counter' para nos ajudar com a contagem das palavras:","05e74d21":"A bossa nova \u00e9 sem vias de d\u00favidas um dos momentos mais importantes para a cultura social brasileira. O ritmo surgiu logo ap\u00f3s a segunda guerra mundial, por volta de 1958. Naquela \u00e9poca era visado entre m\u00fasicos de classe m\u00e9dia alta a internacionaliza\u00e7\u00e3o da m\u00fasica brasileira. Por meio de um canto mais falado e com menos vocais proeminentes, nasceu a **bossa nova**."}}