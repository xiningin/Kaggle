{"cell_type":{"82f492d2":"code","ebb6a8a4":"code","62a833e7":"code","8dcfa2d7":"code","8509b4f9":"code","d52861a7":"code","a87f6678":"code","61112dd1":"code","167c32e8":"code","f3c3dea9":"code","6713b263":"code","e592333a":"code","c4acd46d":"code","591aaecc":"code","ff805cba":"code","b8e3b771":"code","e420f162":"code","a4bfd60c":"markdown"},"source":{"82f492d2":"import os \nimport pandas as pd\nimport torch\nfrom torch import nn as nn\nimport torchvision\nimport time\nimport pickle\nimport torch.optim as opt\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.model_selection import train_test_split","ebb6a8a4":"data_folder='..\/input\/lish-moa'\ntrain=data_folder+str('\/train_features.csv')\nlabel=data_folder+str('\/train_targets_scored.csv')\ntest=data_folder+str('\/test_features.csv')\n\ndf=pd.read_csv(train)\ntraining_labels=pd.read_csv(label)\ndf_gen=pd.read_csv(test)\n\ndf=df.drop(columns={'sig_id','cp_type','cp_time','cp_dose'},axis=1)\ntraining_labels=training_labels.drop('sig_id',axis=1)\nnames=df_gen['sig_id']\ndf_gen=df_gen.drop(['sig_id','cp_type','cp_time','cp_dose'],axis=1)","62a833e7":"df_train,df_test,training_labels,validation_labels=train_test_split(df,training_labels,test_size=0.25)","8dcfa2d7":"class Dataset(torch.utils.data.Dataset):\n  def __init__(self,df,t):\n    self.df=df\n    self.t=t\n\n  def __len__(self):\n    return len(self.df)\n\n  def __getitem__(self,idx):\n    if(torch.is_tensor(idx)):\n      idx=idx.tolist()\n    x=torch.Tensor(self.df.iloc[idx].values)\n    y=torch.Tensor(self.t.iloc[idx].values)\n    return x,y","8509b4f9":"trainset=Dataset(df_train,training_labels)\ntestset=Dataset(df_test,validation_labels)\ntrainloader=torch.utils.data.DataLoader(trainset,batch_size=100)\ntestloader=torch.utils.data.DataLoader(testset,batch_size=100)","d52861a7":"class model(nn.Module):\n  def __init__(self):\n    super(model,self).__init__()\n    self.model=nn.Sequential(nn.Linear(872,598),nn.BatchNorm1d(598),nn.Dropout(0.5),nn.PReLU(),\n                             nn.Linear(598,423),nn.BatchNorm1d(423),nn.Dropout(0.5),nn.PReLU(),\n                             nn.Linear(423,295),nn.BatchNorm1d(295),nn.Dropout(0.5),nn.PReLU(),\n                             nn.Linear(295,206),nn.Softmax(dim=1))\n\n    if(type(self.model)==nn.Linear):\n      nn.init.kaiming_normal_(self.model)\n\n  def forward(self,x):\n    x=self.model(x)\n    return x\n\nm=model()\nm=m.cuda()","a87f6678":"training_epochs=300\nop=opt.SGD(m.parameters(),lr=0.05,momentum=0.9)\ncriterion=nn.BCELoss()\n\ntraining_loss={}\nvalidation_loss={}\nl={0:[],1:[],2:[],3:[]}\n\nearly_stopping_tolerance=6\npath='model_dropout_sig_drop.pt'\ntorch.save({'episode': 1,'model_state_dict': m.state_dict(),'op': op.state_dict()}, path)","61112dd1":"def validationloss(m,testloader):\n  m.eval()\n  loss=0\n  with torch.no_grad():\n    for x,y in testloader:\n      x=x.cuda()\n      y=y.cuda()\n      out=m(x)\n      l=criterion(out,y)\n      loss+=len(x)*l.item()\n  loss\/=len(testset)\n  m.train()\n  return loss","167c32e8":"early_stopping_buffer=0\nbest_val_loss=100\n\ncheck=torch.load(path)\nepisode=check['episode']\nif(episode!=1):\n  m.load_state_dict(check['model_state_dict'])\n  op.load_state_dict(check['op'])\n\nwhile episode <= training_epochs:\n  start=time.time()\n  running_loss=0\n  \n  for x,y in trainloader:\n    x=x.cuda()\n    y=y.cuda()\n    out=m(x)\n    loss=criterion(out,y)\n    op.zero_grad()\n    loss.backward()\n    op.step()\n    running_loss+=len(x)*loss.item()\n\n  loss=running_loss\/len(trainset)\n  training_loss[episode]=loss\n  valloss=validationloss(m,testloader)\n  validation_loss[episode]=valloss\n\n  #early stopping section \n  if(valloss<best_val_loss):\n    best_val_loss=valloss\n    early_stopping_buffer=0\n    torch.save({'episode': episode,'model_state_dict': m.state_dict(),'op': op.state_dict()}, path)\n  \n  if(valloss>=best_val_loss):\n    early_stopping_buffer=early_stopping_buffer+1\n\n  print('Done {} in time {}'.format(episode,time.time()-start))\n  \n  if(early_stopping_buffer>early_stopping_tolerance):\n    print('stopped on episode {}'.format(episode))\n    break\n\n  #tracking gradients \n  if(episode%5==0):\n    for i in range(4):\n      l[i].append(m.model[4*i].weight.grad.norm(2))\n  \n  episode=episode+1","f3c3dea9":"training_file='training_loss_history_dp_s_drp.pickle'\nf=open(training_file,'wb')\npickle.dump(training_loss,f)\nvalidation_file='validation_loss_history_dp_s_drp.pickle'\np=open(validation_file,'wb')\npickle.dump(validation_loss,p)","6713b263":"print(validation_loss[len(validation_loss)-1])","e592333a":"plt.plot(list(training_loss.keys()),list(training_loss.values()),label='Training loss')\nplt.plot(list(validation_loss.keys()),list(validation_loss.values()),label='Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","c4acd46d":"#plot gradients\nfor i in range(4):\n  plt.plot(range(0,5*len(l[i]),5),l[i],label='layer {}'.format(i))\nplt.ylabel('Gradient Norm')\nplt.xlabel('Epochs')\nplt.legend()\nplt.show()","591aaecc":"m_test=model()\nm_test=m_test.cuda()\ncheckpoint=torch.load(path)\nm_test.load_state_dict(checkpoint['model_state_dict'])","ff805cba":"data=torch.Tensor(df_gen.values)\ndata=data.cuda()\nm_test.eval()\noutput=m_test(data).cpu().detach().numpy()\n\nsubmission=pd.DataFrame(data=output,columns=training_labels.columns)\nsubmission=pd.concat([names,submission],axis=1)","b8e3b771":"print(submission)","e420f162":"submission.to_csv('submission.csv',index=False)","a4bfd60c":"Dropout,Dropping columns\n"}}