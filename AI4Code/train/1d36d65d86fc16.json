{"cell_type":{"076dec93":"code","435efadc":"code","d9ced01e":"code","07b512d2":"code","099c0db4":"code","c1d9950f":"code","742ff4d2":"code","5b3a6587":"code","0e95eb81":"code","5530f7e3":"code","77257b55":"code","2302eccb":"code","d393a530":"code","51ffa44f":"code","dbc3c738":"code","d7b7e9f1":"code","81434665":"code","11c0bbf1":"code","46ddfc1d":"code","e765c2f5":"code","00866605":"code","9f2bcde4":"code","4d3dc1b6":"code","325df335":"code","c52b793c":"code","93b2ee0f":"markdown","fb2623db":"markdown","73fb6ef0":"markdown","44dd75d7":"markdown","74a13113":"markdown","35ed38bf":"markdown"},"source":{"076dec93":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nimport gresearch_crypto\nimport gc\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npd.set_option('display.max_columns', None)\n\nDEBUG = True","435efadc":"train = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv').set_index(\"timestamp\")\nassets = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\n#for assets sorting \nassets_order = pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv').Asset_ID[:14]\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\n\nif DEBUG:\n    #train = train[1000000:12000000]\n    train = train[10000000:]\n\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']] = \\\ntrain[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP','Target']].astype(np.float32)\n\ntrain['Target'] = train['Target'].fillna(method=\"ffill\")","d9ced01e":"# VWAP column has -inf and inf values. VWAP_max and VWAP_min will be used for replacement\n    \nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\nprint(VWAP_max, \"\\n\", VWAP_min)\n\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)","07b512d2":"# Get the series of the 'real' record ids for futher matching\n    \ndf = train[['Asset_ID', 'Target']].copy()\n\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\n\ndel df","099c0db4":"def add_features(df):\n    df['Upper_Shadow'] = df['High'] - np.maximum(df['Close'], df['Open'])\n    df['Lower_Shadow'] = np.minimum(df['Close'], df['Open']) - df['Low']\n    \n    df['spread'] = df['High'] - df['Low']\n    df['mean_trade'] = df['Volume']\/df['Count']\n    df['log_price_change'] = np.log(df['Close']\/df['Open'])\n    return df\n\ntrain=add_features(train)\ntrain.shape","c1d9950f":"scale_features = train.columns.drop(['Asset_ID','Target'])\nRS = RobustScaler()\ntrain[scale_features] = RS.fit_transform(train[scale_features])","742ff4d2":"# Filling gaps\n\nind = train.index.unique()\n\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\n\ntrain=train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ntrain.shape","5b3a6587":"# Matching records and marking generated rows as 'non-real'\n\ntrain['group_num'] = train.index.map(times)\ntrain = train.dropna(subset=['group_num'])\ntrain['group_num'] = train['group_num'].astype('int')\n\ntrain['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n\ntrain['is_real'] = train.id.isin(ids)*1\ntrain = train.drop('id', axis=1)","0e95eb81":"# Features values for 'non-real' rows are set to zeros\n\nfeatures = train.columns.drop(['Asset_ID','group_num','is_real'])\ntrain.loc[train.is_real==0, features]=0.","5530f7e3":"# Sorting assets according to their order in the 'supplemental_train.csv'\n\ntrain['asset_order'] = train.Asset_ID.map(assets_order) \ntrain=train.sort_values(by=['group_num', 'asset_order'])\ntrain.head(20)","77257b55":"targets = train['Target'].to_numpy().reshape(-1, 14)\n#targets = np.expand_dims(targets, axis=1)\n\nfeatures = train.columns.drop(['Asset_ID', 'Target', 'group_num','is_real'])\ntrain = train[features]\n\ntrain=np.array(train)\ntrain = train.reshape(-1,14,train.shape[-1])\n#train = np.expand_dims(train, axis=1)\ntrain.shape","2302eccb":"# timeseriesgenerator-like class, except it using target from the last timestep insteed of last+1\nclass sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.size = len(x_set)\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if end_ind <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1])\n\n        return np.array(batch_x), np.array(batch_y)","d393a530":"#last 10% of the data are used as validation set\nX_train, X_test = train[:-len(train)\/\/10], train[-len(train)\/\/10:]\ny_train, y_test = targets[:-len(train)\/\/10], targets[-len(train)\/\/10:]","51ffa44f":"BATCH_SIZE=2**10\ntrain_generator = sample_generator(X_train, y_train, length=15, batch_size=BATCH_SIZE)\nval_generator = sample_generator(X_test, y_test, length=15, batch_size=BATCH_SIZE)\n\nprint(f'Sample shape: {train_generator[0][0].shape}')\nprint(f'Target shape: {train_generator[0][1].shape}')","dbc3c738":"#https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37495\ndef MaxCorrelation(y_true,y_pred):\n    \"\"\"Goal is to maximize correlation between y_pred, y_true. Same as minimizing the negative.\"\"\"\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return -tf.math.abs(tfp.stats.correlation(y_true_masked,y_pred_masked, sample_axis=None, event_axis=None))\n\ndef Correlation(y_true,y_pred):\n    return tf.math.abs(tfp.stats.correlation(y_pred,y_true, sample_axis=None, event_axis=None))\n\ndef masked_mse(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_squared_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_mae(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.mean_absolute_error(y_true = y_true_masked, y_pred = y_pred_masked)\n\ndef masked_cosine(y_true, y_pred):\n    mask = tf.math.not_equal(y_true, 0.)\n    y_true_masked = tf.boolean_mask(y_true, mask)\n    y_pred_masked = tf.boolean_mask(y_pred, mask)\n    return tf.keras.losses.cosine_similarity(y_true_masked, y_pred_masked)\n\ndef get_model(n_assets=14):  \n    x_input = keras.Input(shape=(train_generator[0][0].shape[1], n_assets, train_generator[0][0].shape[-1]))\n\n    branch_outputs = []\n        \n    for i in range(n_assets):\n            # Slicing the ith asset:\n        a = layers.Lambda(lambda x: x[:,:, i])(x_input)\n        a = layers.Masking(mask_value=0.,)(a)\n        a = layers.BatchNormalization()(a)\n        a = layers.LSTM(units=32, return_sequences=True)(a)\n        a = layers.LSTM(units=16)(a)\n        branch_outputs.append(a)\n    \n    x = layers.Concatenate()(branch_outputs)\n    x = layers.Dense(units=128)(x)\n    out = layers.Dense(units=n_assets)(x)\n    \n    model = keras.Model(inputs=x_input, outputs=out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3), \n                  #loss = 'mse',\n                  #loss = 'cosine_similarity',\n                  loss = masked_cosine,\n                  metrics=[Correlation]\n                 )\n    \n    return model \n    \nmodel=get_model()\n#model.summary()","d7b7e9f1":"#example with 3 assets for visibility\ntf.keras.utils.plot_model(get_model(n_assets=3), show_shapes=True)","81434665":"tf.random.set_seed(10)\n\nestop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min',restore_best_weights=True)\nscheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5*len(X_train)\/BATCH_SIZE), 1e-3)\nlr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n    \nhistory = model.fit(train_generator, validation_data = (val_generator),\n              epochs = 30, callbacks = [lr])","11c0bbf1":"fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nhistories = pd.DataFrame(history.history)\n\nepochs = list(range(1,len(histories)+1))\nloss = histories['loss']\nval_loss = histories['val_loss']\nCorrelation = histories['Correlation']\nval_Correlation = histories['val_Correlation']\n\nax[0].plot(epochs, loss, label = 'Train Loss')\nax[0].plot(epochs, val_loss, label = 'Val Loss')\nax[0].set_title('Losses')\nax[0].set_xlabel('Epoch')\nax[0].legend(loc='upper right')\n\nax[1].plot(epochs, Correlation, label = 'Train Correlation')\nax[1].plot(epochs, val_Correlation, label = 'Val Correlation')\nax[1].set_title('Correlations')\nax[1].set_xlabel('Epoch')\nax[1].legend(loc='upper right')\n\nfig.show()","46ddfc1d":"gc.collect()","e765c2f5":"predictions = model.predict(val_generator)\npredictions.shape","00866605":"print('Asset:    Corr. coef.')\nprint('---------------------')\nfor i in range(14):\n    #drop first 14 values in the y_test, since they are absent in val_generator labels\n    y_true = np.squeeze(y_test[14:,i])\n    y_pred = np.squeeze(predictions[:,i])\n    \n    #get non-zero targets (assuming that zeros indicates 'non-real' rows)\n    real_target_ind = np.argwhere(y_true!=0)\n    asset_id = list(assets_order.keys())[i]\n    asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n    print(f\"{asset_name}: {np.corrcoef(y_pred[real_target_ind].flatten(), y_true[real_target_ind].flatten())[0,1]:.4f}\") ","9f2bcde4":"#test_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\n#sample_prediction_df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_sample_submission.csv')\n\n#test_df = test_df.iloc[2:12]\n#sample_prediction_df=sample_prediction_df[sample_prediction_df.row_id.isin(test_df.row_id)]\n\n#scale_features = test_df.columns.drop(['Asset_ID','group_num','row_id'])#test_df.head(3)\n#test_df.head()","4d3dc1b6":"#placeholder for first 15 samples\nsup=pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv')[:15*14]\nplaceholder=add_features(sup)\nplaceholder[scale_features] = RS.transform(placeholder[scale_features])\nplaceholder['asset_order'] = placeholder.Asset_ID.map(assets_order) \ntest_sample = np.array(placeholder[features])\ntest_sample = test_sample.reshape(-1,14,test_sample.shape[-1])\ntest_sample = np.expand_dims(test_sample, axis=0)\ntest_sample.shape","325df335":"# for test gap filling\nexample = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')[:14]\nexample['asset_order'] = example.Asset_ID.map(assets_order) \nexample = example[['Asset_ID','asset_order']]\nexample","c52b793c":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor (test_df, sample_prediction_df) in iter_test:\n        \n    test_df['VWAP'] =np.nan_to_num(test_df.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n    test_df=add_features(test_df)\n    test_df[scale_features] = RS.transform(test_df[scale_features])\n    \n    test_data = test_df.merge(example, how='outer', on='Asset_ID').sort_values('asset_order')\n        \n    test = np.array(test_data[features].fillna(method=\"ffill\"))\n    test = test.reshape(-1,1,14,test.shape[-1])\n        #get test sample, shape (1, 15, 14, test.shape[-1])\n    test_sample = np.hstack([test_sample, test])[:,-15:]\n    y_pred = model.predict(test_sample).squeeze().reshape(-1, 1).squeeze()\n    \n    test_data['Target'] = y_pred\n        \n    #test_df = test_df.merge(test_data[['row_id', 'Target']], how='left', on='row_id')\n    for _, row in test_df.iterrows(): \n        try:\n            sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = test_data.loc[test_data['row_id'] == row['row_id'], 'Target'].item()\n        except:\n            sample_prediction_df.loc[sample_prediction_df['row_id'] == row['row_id'], 'Target'] = 0\n       \n    #sample_prediction_df['Target'] = test_df['Target']\n    #display(test_df)\n    #display(test_data)\n    #display(sample_prediction_df)    \n    env.predict(sample_prediction_df)","93b2ee0f":"### Intro\n\nHere is an attempt to implement an idea of using the parallel RNN branches for each asset and use learned information to compute the vector of targets at each timestep.\n\nSteps:\n* Fill gaps in the training set like it describer in the competition tutorial notebook;\n* Use `TimeseriesGenerator` like generator class to build the dataset;\n* Build the model. Simplified  structure:\n    - Lambda layer needed for assets separation;\n    - Masking layer. Generated records (filled gaps) has zeros as features values, so they are not used in the computations;\n    - LSTM or GRU layer;\n    - Concatanate layer.\n\n![\u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435.png](attachment:99932047-786c-45c1-afb0-14c19c3b6183.png)\n","fb2623db":"### Load and prepare the data","73fb6ef0":"### Training","44dd75d7":"### Dataset creation\n\nSamples with a duration of 15 records (minutes) will be formed from the `train` array. Each sample has a target vector corresponding to the 15th record. \n\n![data.JPG](attachment:364826a2-fe7e-4ac8-89cb-5d1217ae2d02.JPG)","74a13113":"### Submission","35ed38bf":"### The correlation coefficients by asset for the validation data"}}