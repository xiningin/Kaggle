{"cell_type":{"913ce810":"code","23dbe48a":"code","762d85f4":"code","84c7a867":"code","5258c308":"code","26567705":"code","6476fb69":"code","44651f4a":"code","fbef1671":"code","286b0258":"code","ced9bcde":"code","e0bc992c":"code","f96a7f9b":"code","fe465353":"code","65a334c8":"code","98fb750e":"code","9283afdc":"code","48cb545c":"code","dad86dad":"code","c8d9f742":"code","c615a162":"code","8484032c":"code","77c706b0":"code","9a8fdb58":"code","094ba816":"code","d965808d":"code","ae176ac2":"code","7e84fc4b":"markdown","97908c3d":"markdown","74a47cec":"markdown","df84cb8a":"markdown","496ee33e":"markdown","488862db":"markdown","9e1dba8c":"markdown","ee9bfc3d":"markdown","f4edfe63":"markdown","dff67a53":"markdown"},"source":{"913ce810":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns","23dbe48a":"# Preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline","762d85f4":"# Performance Measurement\nfrom sklearn.metrics import mean_absolute_error #MAE\nfrom sklearn.metrics import mean_squared_error #MSE","84c7a867":"# Create test\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split","5258c308":"# algorithms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost import XGBRegressor","26567705":"train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\ntrain.drop('id',axis=1,inplace=True)","6476fb69":"missingValues = [col for col in train.columns if train[col].isnull().sum() > 0]\nprint(\"Your Train Dataframe has ** \"+str(train.shape[1])+\" ** Columns and ** \"+str(train.shape[0])+\" ** Rows.\\n\"\n                 \"There are ** \"+str(len(missingValues))+\n                  \" ** columns that have missing values. \\n\")","44651f4a":"missingValues = [col for col in test.columns if test[col].isnull().sum() > 0]\nprint(\"Your Test Dataframe has ** \"+str(test.shape[1])+\" ** Columns and ** \"+str(test.shape[0])+\" ** Rows.\\n\"\n                 \"There are ** \"+str(len(missingValues))+\n                  \" ** columns that have missing values. \\n\")","fbef1671":"temp = pd.DataFrame(train.select_dtypes('object').nunique()).rename(columns={0:'Unique Values'})\nfig = px.pie(names = temp.index,values = temp['Unique Values'],\n       title = 'Unique values of each categorical columns')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","286b0258":"uniqueValues = dict()\ncolsCat = train.select_dtypes('object').columns.tolist()\nfor typeCol in ['train','test']:\n    uniqueValues[typeCol]= dict()\n    for cal in ['count','percentage']:\n        uniqueValues[typeCol][cal]=dict()\n        for col in colsCat:\n            if cal == 'count':\n                uniqueValues[typeCol][cal][col] = dict(globals()[typeCol][col].value_counts())\n            else:\n                uniqueValues[typeCol][cal][col] = dict(globals()[typeCol][col].value_counts()\/ len(train))","ced9bcde":"pd.DataFrame(uniqueValues['train']['percentage']).style.background_gradient(cmap='Greens')","e0bc992c":"valueCountsTrain = pd.DataFrame(uniqueValues['train']['count']).fillna(0).add_suffix('_train').reset_index()\nvalueCountsTest  = pd.DataFrame(uniqueValues['test']['count']).fillna(0).add_suffix('_test').reset_index()\nvalueCountsMerged= valueCountsTrain.merge(valueCountsTest,how='left',on=['index']).set_index('index')\nvalueCountsMerged = valueCountsMerged.T.reset_index().rename(columns  = {'index':'category'})\ncols = ['A', 'B', 'C', 'D', 'I', 'H', 'E', 'G', 'F', 'L', 'K', 'N', 'J', 'O','M']\nvalueCountsMerged.plot(x=\"category\", y=cols, kind=\"bar\",figsize=(18,8))","f96a7f9b":"# Create function for frequentcy\ndef find_frequent_labels(df, variable, tolerance):\n    temp = df[variable].value_counts()  \/ len(df)\n    frequent = [x for x in temp.loc[temp>tolerance].index.values]\n    return frequent","fe465353":"frequents = dict()\ntolerances = {'cat0':0,'cat1':0,'cat2':0,\n              'cat3':0.11,'cat4':0.01,'cat5':0.07,\n              'cat6':0.03,'cat7':0.05,'cat8':0.09,\n              'cat9':0.06}\nfor key,values in tolerances.items():\n    locals()['frequent_'+str(key)] = find_frequent_labels(train, key,values)\n    frequents[str(key)] = 'frequent_'+str(key)","65a334c8":"for key,values in frequents.items():\n    train[key] = np.where(train[key].isin(globals()[values]),\n                         train[key],'Rare')\n    test[key] = np.where(test[key].isin(globals()[values]),\n                         test[key],'Rare')","98fb750e":"temp = pd.DataFrame(train.select_dtypes('object').nunique()).rename(columns={0:'Unique Values'})\nfig = px.pie(names = temp.index,values = temp['Unique Values'],\n       title = 'Unique values of each categorical columns After processing')\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.show()","9283afdc":"# Ordinal Encoding\ncatOrdinal = ['cat0','cat1','cat2',\n              'cat3','cat4','cat6',\n              'cat7']\nordinal_encoder = OrdinalEncoder()\ntrain[catOrdinal] = ordinal_encoder.fit_transform(train[catOrdinal])\ntest[catOrdinal] = ordinal_encoder.transform(test[catOrdinal])","48cb545c":"# One-Hot Encoding\nOH_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train[['cat5','cat8','cat9']]))\nOH_cols_test  = pd.DataFrame(OH_encoder.transform(test[['cat5','cat8','cat9']]))\n\nOH_cols_train.index = train.index\nOH_cols_test.index = test.index\n\ntrain.drop(['cat5','cat8','cat9'],axis=1,inplace=True)\ntest.drop(['cat5','cat8','cat9'],axis=1,inplace=True)\n\ntrain = pd.concat([train,OH_cols_train],axis=1)\ntest  = pd.concat([test,OH_cols_test],axis=1)","dad86dad":"corr = train.corr()\nmask = np.triu(np.ones_like(corr,dtype=bool))\nplt.figure(figsize=(18,16))\nplt.title('Correlation matrix for Train data')\nsns.heatmap(corr,mask=mask,annot=True,linewidths=0.2,\n            square=True,cbar_kws={\"shrink\": .60})\nplt.show()","c8d9f742":"X = train.drop('target',axis=1)\ny = train.target","c615a162":"X","8484032c":"reg1 = make_pipeline(StandardScaler(),\n            LinearRegression())","77c706b0":"scores = []\ni = 0\ncv = KFold(n_splits=10,random_state=42,shuffle=True)\nfor train_index,valid_index in cv.split(X):\n    i+=1\n    X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n    y_train, y_valid = y.loc[train_index], y.loc[valid_index]\n    reg1.fit(X_train,y_train)\n    predict = reg1.predict(X_valid)\n    print(f'Fold {i},','rmse : ',np.sqrt(mean_squared_error(predict,y_valid)))\n    scores.append(np.sqrt(mean_squared_error(predict,y_valid)))\nprint(f'''Algorithm :Linear regression \\n performance measurement RMSE \\n mean fold {np.mean(scores)}''')","9a8fdb58":"reg2 = make_pipeline(StandardScaler(),\n                    SGDRegressor(max_iter=1000, tol=1e-3))","094ba816":"scores = []\ni = 0\ncv = KFold(n_splits=10,random_state=42,shuffle=True)\nfor train_index,valid_index in cv.split(X):\n    i+=1\n    X_train, X_valid = X.loc[train_index], X.loc[valid_index]\n    y_train, y_valid = y.loc[train_index], y.loc[valid_index]\n    reg2.fit(X_train,y_train)\n    predict = reg2.predict(X_valid)\n    print(f'Fold {i},','rmse : ',np.sqrt(mean_squared_error(predict,y_valid)))\n    scores.append(np.sqrt(mean_squared_error(predict,y_valid)))\nprint(f'''Algorithm :Stochastic Gradient Descent \\n performance measurement RMSE \\n mean fold {np.mean(scores)}''')","d965808d":"X_train.shape","ae176ac2":"predict = reg1.predict(test.drop('id',axis=1))\npredictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"target\"] = predict\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","7e84fc4b":"## Stochastic Gradient Descent","97908c3d":"Percentage of observations per category of [cat0 to cat9]","74a47cec":"## Linear regression","df84cb8a":"# Exploratory Data Analysis (EDA)","496ee33e":"# Load DataSet","488862db":"# Prepare Data for ML Algorithms","9e1dba8c":"For test model I use K Fold Cross Validation","ee9bfc3d":"If we consider as rare those labels present in less than 9% of the observations in cat8, then B,D,G,F are rare categories","f4edfe63":"# Algorithms","dff67a53":"OK, this seems better    \ncat9 has more unique value, and it's not logical to use one hot encoding or ordinal, so this is my road map for categorical columns    \n1. I use label encoder [cat0 to cat4,cat6,cat7,] \n2. I use one hot encoding  [cat5,cat8,cat9]  "}}