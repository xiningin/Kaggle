{"cell_type":{"ee155ea6":"code","a3bc241e":"code","f6e85471":"code","86083b8c":"code","3089ba8d":"code","8ab3cf1e":"code","8448e84c":"code","968ce525":"code","30ec789a":"code","e25ea24c":"code","7bad8a11":"code","2f7c0ccb":"code","9d8f5f35":"code","4603a717":"code","7a425dd5":"code","ecea5c40":"code","ca430195":"code","67ae1362":"code","cd4a9fe5":"markdown","511edb04":"markdown","76cbe41d":"markdown","ef14c6bc":"markdown","80db8dbe":"markdown"},"source":{"ee155ea6":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\n\nimport numpy as np\nimport nltk\nimport re\nimport datetime\nimport matplotlib.pyplot as plt\n# %% [code]\nFIGSIZE = (16, 9)\nFONT = {\"family\": \"Share Tech Mono\", \"weight\": \"normal\", \"size\": 16}\ntds = \"#0073F1\"\nweek = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n#UrlLib for http handlings\nimport urllib.request  \nimport bs4 as BeautifulSoup\nimport nltk\n\n#from string import punctuation\n#from nltk.corpus import stopwords\n#from nltk.tokenize import word_tokenize\n#from nltk.tokenize import sent_tokenize\n\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport urllib.request  \nimport bs4 as BeautifulSoup\nimport nltk\nfrom string import punctuation\n\n#For the next version \nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n!pip install pyshorteners\n#!pip install xlrd\n\nimport pyshorteners","a3bc241e":"df = pd.read_excel('\/kaggle\/input\/ai-ds-coursera-trainings\/CS_DS_Free_and_FreeToAudit_Jair_Ribeiro.xlsx')","f6e85471":"courses = df\ncourses","86083b8c":"def summarization_courses(i):\n    # Data collection from the links using web scraping(using Urllib library)\n    course_url = courses['URL'].tolist()\n    course_url = course_url[i]\n    #course_url = \"\"\n\n\n    text = urllib.request.urlopen(course_url)\n    text\n\n    course_summary = text.read()\n    course_summary\n    \n    # Parsing the URL content \n    course_parsed = BeautifulSoup.BeautifulSoup(course_summary,'html.parser')\n    \n    # Returning <p> tags\n    paragraphs = course_parsed.find_all('p')\n    \n    # To get the content within all poaragrphs loop through it\n    course_content = ''\n    for p in paragraphs:  \n        course_content += p.text\n    \n    \n    # 3) Tokenization & Data clean up    \n    tokens = word_tokenize(course_content)\n    stop_words = []\n    stopwords  = []\n    punctuation = \" \"\n    stopwords = set(STOPWORDS)\n    punctuation = punctuation + '\\n'\n    punctuation\n    \n    word_frequencies = {}\n    for word in tokens:    \n        if word.lower() not in stop_words:\n            if word.lower() not in punctuation:\n                if word not in word_frequencies.keys():\n                    word_frequencies[word] = 1\n                else:\n                    word_frequencies[word] += 1\n    #word_frequencies\n    \n    max_frequency = max(word_frequencies.values())\n    #print(max_frequency)\n    \n    for word in word_frequencies.keys():\n        word_frequencies[word] = word_frequencies[word]\/max_frequency\n       #print(word_frequencies)\n    \n    sent_token = sent_tokenize(course_content)\n    #print(sent_token)\n    \n    \n    sentence_scores = {}\n    for sent in sent_token:\n        sentence = sent.split(\" \")\n        for word in sentence:        \n            if word.lower() in word_frequencies.keys():\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word.lower()]\n                else:\n                    sentence_scores[sent] += word_frequencies[word.lower()]\n    #sentence_scores\n    \n    from heapq import nlargest\n    select_length = int(len(sent_token)*0.20) # Set the summary to 8% of the whole article.\n    select_length = 1 # Set the summary to 1 sentence.\n    \n    summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n    final_summary = [word for word in summary]\n    summary = ' '.join(final_summary)\n    \n    return summary\n","3089ba8d":"s = pyshorteners.Shortener()\nprint(s.tinyurl.short(df[\"URL\"][2])) # just checking if it works!","8ab3cf1e":"courses.head()","8448e84c":"print(\"Summary of the AI & Data Science Courses\")\nprint()\ni=0\nn = int(courses[\"URL\"].count())\nfor i in range(n):\n    print(\"-------------------------------------\")\n            \n    \n    type_of_training = courses[\"Type\"][i]\n    if type_of_training == \"Free\":\n        print(\" \")\n        print(courses[\"Course Title\"][i] + \" (Free)\")\n        \n    else:\n        print(\" \")\n        print(courses[\"Course Title\"][i] + \" (Free to Audit)\")\n    \n    print(\" \")\n    print(summarization_courses(i))\n\n    tiny = str(courses[\"URL\"][i])\n    print(\" \")\n    print(\"University: \" + courses[\"University Name\"][i])\n    print(\"Domain: \" + courses[\"Subject Subdomain\"][i])\n    print(\"Rating: \" + str(courses[\"Rating\"][i]))\n    print(\"Link: \"+ s.tinyurl.short(tiny))\n    print(\" \")\n    \n    i=i+1","968ce525":"df","30ec789a":"#Connections Plotting Functions\n\n\ndef plot_bar_column(courses, col):\n    fnames = courses[col].value_counts().head(30)\n    plot_fnames(fnames,col)\n\n\ndef plot_nlp_cv(courses):\n    tfidf = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n    cleaned_positions = list(courses[\"Course Title\"].fillna(\"\"))\n    res = tfidf.fit_transform(cleaned_positions)\n    res = res.toarray().sum(axis=0)\n\n    fnames = pd.DataFrame(\n        list(sorted(zip(res, tfidf.get_feature_names())))[-30:],\n        columns=[\"Course Title by Words Freq\", \"Words\"]\n    )[::-1] \n    plot_fnames(fnames, \"Course Title by Words Freq\", \"Words\")\n\n\ndef plot_fnames(fnames, col, index=\"index\"):\n    fnames = fnames.reset_index()\n\n    fig, ax = plt.subplots(figsize=FIGSIZE)\n\n    plt.bar(\n        x=fnames.index,\n        height=fnames[col],\n        color=tds,\n        alpha=0.5\n    )\n\n    plt.title(\"{} distribution\".format(col), fontdict=FONT, y=1.2)\n    plt.xticks(\n        fnames.index,\n        fnames[index].str.capitalize(),\n        rotation=65,\n        ha=\"right\",\n        size=FONT[\"size\"],\n    )\n\n    plt.ylabel(\"Nb occurences\", fontdict=FONT)\n    plt.yticks()#[0, 5, 10, 15, 20])\n    ax.set_frame_on(False)\n    plt.grid(True)\n\n    plt.show()","e25ea24c":"courses.head()","7bad8a11":"plot_nlp_cv(courses)","2f7c0ccb":"plot_bar_column(courses, \"University Name\")","9d8f5f35":"plot_bar_column(courses, \"Subject Subdomain\")","4603a717":"plot_bar_column(courses, \"Type\")","7a425dd5":"#Top 10 Universities\nuniversities = courses['University Name'].value_counts().head(10)\nuniversities\n\nax = universities.plot(kind='bar',\n                                    figsize=(14,8),\n                                    title=\"University providing trainings\")\nax.set_xlabel(\"University\")\nax.set_ylabel(\"Number of Courses\")","ecea5c40":"#Top 10 Domains\ndomains = courses['Subject Subdomain'].value_counts().head(10)\ndomains\n\n\nax = domains.plot(kind='bar',\n                                    figsize=(14,8),\n                                    title=\"Top 10 Domains\")\nax.set_xlabel(\"Domains\")\nax.set_ylabel(\"Number of courses\")","ca430195":"courses.head()","67ae1362":"courses['Subject Subdomain'].value_counts().plot(kind='bar');","cd4a9fe5":"# Let's do some plottings","511edb04":"#  Summarization","76cbe41d":"Plotting functions","ef14c6bc":"Here I've created a loop that find the number of articles: n = int(df[\"URL\"].count()) and run the funcion \"summarization_links()\" for each article in the dataset.","80db8dbe":"Using tinyUrl\n\nTo simplify the copy\/paste from the summary I wanted to uniform all the links using tinyUrl so I've build a function to do it here:\n\n"}}