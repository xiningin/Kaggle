{"cell_type":{"f29be701":"code","df83302e":"code","b85881f7":"code","c525ef5e":"code","3095f208":"code","5b29904f":"code","86f8ba3f":"code","4965c649":"code","7f03c872":"code","6200391c":"code","dc55ade8":"code","86fad465":"code","48487737":"markdown","6ed9e714":"markdown","14500579":"markdown"},"source":{"f29be701":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport nltk\nfrom sklearn.preprocessing import LabelEncoder\n## ADD STOPWORDS\nstop = set(list(stop) + [\"http\",\"https\", \"s\", \"nt\", \"m\"])\nDropout_num = 0\n","df83302e":"test_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","b85881f7":"def load_training(training_path=\"\/kaggle\/input\/nlp-getting-started\/train.csv\"):\n    df = pd.read_csv(training_path)\n    \n    print(df.head(10))\n    return df\n\ndf = load_training()","c525ef5e":"\ndef clean_df(df):\n    def remove_stopwords(text):\n        if text is not None:\n            tokens = [x for x in word_tokenize(text) if x not in stop]\n            return \" \".join(tokens)\n        else:\n            return None\n    \n    #TMP: TRY TO USE DEFAULT STRING FOR NONE. TODO: USE ROW[\"KEYWORDS\"]\n    #df['hashtag'] =df['hashtag'].apply(lambda x : \"NO\" if x is None else x)\n    \n    df[\"text\"] = df['text'].apply(lambda x : x.lower())\n    #df[\"hashtag\"] = df['hashtag'].apply(lambda x : x.lower())\n    \n    #df['text'] =df['text'].apply(lambda x : remove_stopwords(x))\n    #df['hashtag'] =df['hashtag'].apply(lambda x : remove_stopwords(x))\n    \n    \n    \n\n\n    def remove_URL(text):\n        url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n        return url.sub(r'',text)\n\n    df['text']=df['text'].apply(lambda x : remove_URL(x))\n    def remove_html(text):\n        html=re.compile(r'<.*?>')\n        return html.sub(r'',text)\n\n    df['text']=df['text'].apply(lambda x : remove_html(x))\n    # Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n    def remove_emoji(text):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', text)\n\n    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n    def remove_punct(text):\n        table=str.maketrans('','',string.punctuation)\n        return text.translate(table)\n\n    df['text']=df['text'].apply(lambda x : remove_punct(x))\n    \n    df.text = df.text.replace('\\s+', ' ', regex=True)\n    return df\ndf = clean_df(df)\nprint(\"-- Word distrig Positive Class\")\n\n","3095f208":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ndf.loc[df['id'].isin(ids_with_target_error),'target'] = 0\ndf[df['id'].isin(ids_with_target_error)]","5b29904f":"def read_test(test_path=\"\/kaggle\/input\/nlp-getting-started\/test.csv\"):\n    \n    my_df = pd.read_csv(test_path)\n    \n    res_df = my_df[['id']]\n    my_df = my_df[['text']]\n    \n    \n    return my_df, res_df\n\ndef dump_preds(res_df, preds, out=\"default\"):\n    res_df['target'] = None\n    \n    for i, p in  enumerate(preds):\n        res_df.ix[i, 'target'] = p\n    \n    res_df.to_csv(out, index = False)\n    \n\ndef split_data(df, _t=True):\n    X = df.text\n    if _t:\n        Y = df.target\n        le = LabelEncoder()\n        Y = le.fit_transform(Y)\n        Y = Y.reshape(-1,1)\n        return X, Y\n    else:\n        return X\n\n    ","86f8ba3f":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense, Dropout, Bidirectional,  Conv1D, MaxPooling1D\nfrom tensorflow.keras.models import Model\nimport tensorflow_hub as hub\n\nimport tokenization\n\n\n","4965c649":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\n","7f03c872":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model\n\n","6200391c":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","dc55ade8":"\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input = bert_encode(df.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test_df.text.values, tokenizer, max_len=160)\ntrain_labels = df.target.values\n\nmodel = build_model(bert_layer, max_len=160)\nmodel.summary()\n\n\n","86fad465":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')\n\n ","48487737":"# Data Cleaning\nHere we are gonna clean the DF.\nSpecifically, we clean:\n- stopwords (Kept cause removing them cause drop of performances)\n- URL \n- HTML \n- emoji \n- punctuation","6ed9e714":"# Utils for models","14500579":"# BERT TfHub"}}