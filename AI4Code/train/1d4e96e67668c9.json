{"cell_type":{"df59f759":"code","79ef5782":"code","369c7a7e":"code","f5fc65ee":"code","7d7c85f9":"code","cd69c5a4":"code","3f4c9af2":"code","8b03aaf6":"code","d2ea444f":"code","bdc3d655":"code","849a3d77":"code","fd148430":"code","eb129ae1":"markdown","5a6cf979":"markdown","2181e834":"markdown","e725672f":"markdown","e25a5806":"markdown","84c5a09a":"markdown","22f74494":"markdown","9cb6f2f4":"markdown","acf7304b":"markdown","7ef9fc25":"markdown","6dfff9d0":"markdown","a72c42d4":"markdown","69a02a63":"markdown","23a1473e":"markdown"},"source":{"df59f759":"import sys\nimport warnings\n\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.stats import skew\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\n\nprint(\"Imports have been set\")\n\n# Disabling warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","79ef5782":"# Reading the training\/val data and the test data\nX = pd.read_csv('..\/input\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/test.csv', index_col='Id')\n\n# Rows before:\nrows_before = X.shape[0]\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\nrows_after = X.shape[0]\nprint(\"\\nRows containing NaN in SalePrice were dropped: \" + str(rows_before - rows_after))\n\n# Logarithming target variable in order to make distribution better\nX['SalePrice'] = np.log1p(X['SalePrice'])\n\ny = X['SalePrice'].reset_index(drop=True)\ntrain_features = X.drop(['SalePrice'], axis=1)\n\n# concatenate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, X_test]).reset_index(drop=True)\nprint('\\nFeatures size:', features.shape)","369c7a7e":"nan_count_table = (features.isnull().sum())\nnan_count_table = nan_count_table[nan_count_table > 0].sort_values(ascending=False)\nprint(\"\\nColums containig NaN: \")\nprint(nan_count_table)\n\ncolumns_containig_nan = nan_count_table.index.to_list()\nprint(\"\\nWhat values they contain: \")\nprint(features[columns_containig_nan])","f5fc65ee":"for column in columns_containig_nan:\n\n    # populating with 0\n    if column in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                  'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'TotalBsmtSF',\n                  'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold',\n                  'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea']:\n        features[column] = features[column].fillna(0)\n\n    # populate with 'None'\n    if column in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \"PoolQC\", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                  'BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu', 'Fence', 'MiscFeature']:\n        features[column] = features[column].fillna('None')\n\n    # populate with most frequent value for cateforic\n    if column in ['Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'RoofStyle',\n                  'Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'RoofMatl', 'ExterQual', 'ExterCond',\n                  'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']:\n        features[column] = features[column].fillna(features[column].mode()[0])\n\n# MSSubClass: Numeric feature. Identifies the type of dwelling involved in the sale.\n#     20  1-STORY 1946 & NEWER ALL STYLES\n#     30  1-STORY 1945 & OLDER\n#     40  1-STORY W\/FINISHED ATTIC ALL AGES\n#     45  1-1\/2 STORY - UNFINISHED ALL AGES\n#     50  1-1\/2 STORY FINISHED ALL AGES\n#     60  2-STORY 1946 & NEWER\n#     70  2-STORY 1945 & OLDER\n#     75  2-1\/2 STORY ALL AGES\n#     80  SPLIT OR MULTI-LEVEL\n#     85  SPLIT FOYER\n#     90  DUPLEX - ALL STYLES AND AGES\n#    120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n#    150  1-1\/2 STORY PUD - ALL AGES\n#    160  2-STORY PUD - 1946 & NEWER\n#    180  PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n#    190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n# Stored as number so converted to string.\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures[\"MSSubClass\"] = features[\"MSSubClass\"].fillna(\"Unknown\")\n# MSZoning: Identifies the general zoning classification of the sale.\n#    A    Agriculture\n#    C    Commercial\n#    FV   Floating Village Residential\n#    I    Industrial\n#    RH   Residential High Density\n#    RL   Residential Low Density\n#    RP   Residential Low Density Park\n#    RM   Residential Medium Density\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# LotFrontage: Linear feet of street connected to property\n# Groupped by neighborhood and filled in missing value by the median LotFrontage of all the neighborhood\n# TODO may be 0 would perform better than median?\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# LotArea: Lot size in square feet.\n# Stored as string so converted to int.\nfeatures['LotArea'] = features['LotArea'].astype(np.int64)\n# Alley: Type of alley access to property\n#    Grvl Gravel\n#    Pave Paved\n#    NA   No alley access\n\n# So. If 'Street' made of 'Pave', so it would be reasonable to assume that 'Alley' might be 'Pave' as well.\nfeatures['Alley'] = features['Alley'].fillna('Pave')\n# MasVnrArea: Masonry veneer area in square feet\n# Stored as string so converted to int.\nfeatures['MasVnrArea'] = features['MasVnrArea'].astype(np.int64)","7d7c85f9":"features['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# If area is not 0 so creating new feature looks reasonable\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","cd69c5a4":"nan_count_train_table = (features.isnull().sum())\nnan_count_train_table = nan_count_train_table[nan_count_train_table > 0].sort_values(ascending=False)\nprint(\"\\nAre no NaN here now: \" + str(nan_count_train_table.size == 0))","3f4c9af2":"numeric_columns = [cname for cname in features.columns if features[cname].dtype in ['int64', 'float64']]\nprint(\"\\nColumns which are numeric: \" + str(len(numeric_columns)) + \" out of \" + str(features.shape[1]))\nprint(numeric_columns)\n\ncategoric_columns = [cname for cname in features.columns if features[cname].dtype == \"object\"]\nprint(\"\\nColumns whice are categoric: \" + str(len(categoric_columns)) + \" out of \" + str(features.shape[1]))\nprint(categoric_columns)\n\nskewness = features[numeric_columns].apply(lambda x: skew(x))\nprint(skewness.sort_values(ascending=False))\n\nskewness = skewness[abs(skewness) > 0.5]\nfeatures[skewness.index] = np.log1p(features[skewness.index])\nprint(\"\\nSkewed values: \" + str(skewness.index))","8b03aaf6":"# Kind of One-Hot encoding\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\n# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\nprint('Features size for train(X,y) and test(X_test):')\nprint('X', X.shape, 'y', y.shape, 'X_test', X_test.shape)","d2ea444f":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n# check maybe 10 kfolds would be better\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=14, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# Gradient Boosting for regression\ngboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)\n\n# LightGBM regressor.\nlgbm = lgb.LGBMRegressor(objective='regression', num_leaves=4,\n                         learning_rate=0.01, n_estimators=5000,\n                         max_bin=200, bagging_fraction=0.75,\n                         bagging_freq=5, feature_fraction=0.2,\n                         feature_fraction_seed=7, bagging_seed=7, verbose=-1)\n\nxgb_r = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                     max_depth=3, min_child_weight=0,\n                     gamma=0, subsample=0.7,\n                     colsample_bytree=0.7,\n                     objective='reg:squarederror', nthread=-1,\n                     scale_pos_weight=1, seed=27,\n                     reg_alpha=0.00006)\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gboost, xgb_r, lgbm),\n                                meta_regressor=xgb_r,\n                                use_features_in_secondary=True)\n\nsvr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))","bdc3d655":"print('\\n\\nFitting our models ensemble: ')\nprint('Elasticnet is fitting now...')\nelastic_model = elasticnet.fit(X, y)\nprint('Lasso is fitting now...')\nlasso_model = lasso.fit(X, y)\nprint('Ridge is fitting now...')\nridge_model = ridge.fit(X, y)\nprint('XGB is fitting now...')\nxgb_model = xgb_r.fit(X, y)\nprint('Gradient Boosting regressor is fitting now...')\ngboost_model = gboost.fit(X, y)\nprint('LGBMRegressor is fitting now...')\nlgbm_model = lgbm.fit(X, y)\nprint('stack_gen is fitting now...')\nstack_gen_model = stack_gen.fit(X, y)\nprint('SVR is fitting now...')\nsvr_model = svr.fit(X, y)\n\n\n# model scoring and validation function\n# def cv_rmse(the_model, x):\n#     return np.sqrt(-cross_val_score(the_model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n\n\n# print('\\n\\nModels evaluating: ')\n# score = cv_rmse(ridge_model, X)\n# print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(lasso_model, X)\n# print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(elastic_model, X)\n# print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(xgb_model, X)\n# print(\"xgb_r score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(gboost_model, X)\n# print(\"Gradient boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(lgbm_model, X)\n# print(\"LGB score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(stack_gen_model, X)\n# print(\"Stack gen score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n#\n# score = cv_rmse(svr_model, X)\n# print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n","849a3d77":"def blend_models(x):\n    return ((0.1* elastic_model.predict(x)) +\n            (0.1 * lasso_model.predict(x)) +\n            (0.05 * ridge_model.predict(x)) +\n            (0.1 * svr_model.predict(x)) +\n            (0.1 * gboost_model.predict(x)) +\n            (0.15 * xgb_model.predict(x)) +\n            (0.1 * lgbm_model.predict(x)) +\n            (0.3 * stack_gen_model.predict(np.array(x))))\n            \n\ndef rmsle(y_actual, y_pred):\n    return np.sqrt(mean_squared_error(y_actual, y_pred))\n\n# RMSLE score on train data:\n# 0.07450991123905183\nprint('\\nRMSLE score on train data:')\nprint(rmsle(y, blend_models(X)))","fd148430":"submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.iloc[:, 1] = np.expm1(blend_models(X_test))\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file is formed\")","eb129ae1":"<h1>\n    Checking for NaNs and printing them\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> printing NaN-containing columns names <\/li>\n        <li> printing NaN-containing columns values for clarity<\/li>\n    <\/ul>\n<\/span>","5a6cf979":"<h1>\n    Input data handling\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> removing rows with NaN in Sale Price <\/li>\n        <li> logarithm SalePrice (and when model predicts I use np.expm1 function to return value)  <\/li>\n        <li> splitting X to target variable y and train_features <\/li>\n        <li> joining X_test and train_features to process all features together <\/li>\n    <\/ul>\n<\/span>","2181e834":"<h1>\n    Fixing skewed values\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Checking skewness of all the numeric features and logarithm it if more than 0.5 <\/li>\n    <\/ul>\n<\/span>","e725672f":"<h1>\n    ML part (models initialization)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) as encoder which returns kind of One-Hot encoded categoric features<\/li>\n        <li> Splitted to X and X_test by y length<\/li>\n    <\/ul>\n<\/span>","e25a5806":"<h1>\n    Imports\n<\/h1>","84c5a09a":"<h1>\n    Let's check if we filled all the gaps\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Just printing True or False if all the gaps are filled <\/li>\n    <\/ul>\n<\/span>","22f74494":"<h1 align=\"center\">  DISCLAIMER <\/h1>\n<p>Guys, everything that you see in the kernel you can freely take, copy, modify and create your own amazing solutions!<\/p>\n<p>If you don't want to waste your time - just read 'briefly' section I made for you.<\/p>\n<p>Do not forget upvote if the kernel was useful for you.<\/p>\n<img src=\"https:\/\/i.gifer.com\/Be.gif\">","9cb6f2f4":"<h1>\n    ML part (models ensembling)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> The weighted sum of models on the basis of which the solution is assembled<\/li>\n        <li> There is score in comment to each row which explains coefficient to model<\/li>\n    <\/ul>\n<\/span>","acf7304b":"<h1>TODO-list<\/h1>\n<ul>\n    <li>Add more new features<\/li>\n    <li>Remove outliers<\/li>\n    <li>Try to stack some models or just investigate how to do it<\/li>\n<\/ul>","7ef9fc25":"<h1>\n    ML part (models fitting)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> One-by-one all models fitting<\/li>\n        <li> Printing models scores (might be commented for quicker work) <\/li>\n    <\/ul>\n<\/span>","6dfff9d0":"<h1>\n    Categoric features encoding and splitting to train and test data\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) which returns kind of One-Hot encoded categoric features<\/li>\n        <li> Splitted to X and X_test by y length<\/li>\n    <\/ul>\n<\/span>","a72c42d4":"<h1>\n    Adding new features\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> YrBltAndRemod means overall sum of years <\/li>\n        <li> Separating to the other features overall squares<\/li>\n        <li> Separating to the other features presence\/absence of a garage and so on<\/li>\n    <\/ul>\n<\/span>","69a02a63":"<h1>\n    Feature engineering\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Filling with 0 numeric columns <\/li>\n        <li> Filling with 'None' categoric columns where 'NA' meant 'other' value<\/li>\n        <li> Filling with the most frequent values categoric columns where 'NA' meant 'nothing is here'<\/li>\n        <li> Turning to 'str' columns which are actually categoric <\/li>\n        <li> Turning to 'int' columns which are actually numeric <\/li>\n    <\/ul>\n<\/span>","23a1473e":"<h1>Model improvements history to see how various changes affect the performance of the model<\/h1>\n<span>\n<ul>\n    <li>**1. Initial start only with XGBoost**:\n        <ul>\n            <li>RMSLE: 0.13901<\/li>\n        <\/ul>\n    <\/li>\n    <li>**2. Added 'SalePrice' logarithming**:\n        <ul>\n            <li>RMSLE: 0.13984 \u2197<\/li>\n            <li>Review: For now it made performance worse. Reasons are unknown.<\/li>\n        <\/ul>\n    <\/li>\n    <li>**3. Added huge amount of feature engineering**:\n        <ul>\n            <li>RMSLE: 0.13490 \u2198<\/li>\n            <li>Review: Obviously this makes sense<\/li>\n        <\/ul>\n    <\/li>\n    <li>**4. Used Label encoder instead of get_dummies()**:\n        <ul>\n            <li>RMSLE: 0.13630 \u2197<\/li>\n            <li>Review: get_dummies performs better by it's kind of OH-encoding<\/li>\n        <\/ul>\n    <\/li>\n    <li>**5. Aded skew-features fix**:\n        <ul>\n            <li>RMSLE: 0.13494 \u2197<\/li>\n            <li>Review: It is just a bit better than 0.13490 (launch after feature engineering). I will comment it and uncomment a bit after<\/li>\n        <\/ul>\n    <\/li>\n    <li>**6. Created models ensemble: ridge, lasso, elasticnet, gbr, xgboost **:\n        <ul>\n            <li>RMSLE: 0.11759 \u2198<\/li>\n            <li>Review: Sure. Models ensemble would make magic.<\/li>\n        <\/ul>\n    <\/li>\n    <li>**7. Added KernelRidge, Gradient Boosting and LGBMRegressor to ensemble and sum made weighted**:\n        <ul>\n            <li>RMSLE: 0.11638 \u2198<\/li>\n            <li>Review: Very nice! Keep going!<\/li>\n        <\/ul>\n    <\/li>\n     <li>**8. Added SVR and Stack Gen**:\n        <ul>\n            <li>RMSLE: 0.11534 \u2198<\/li>\n            <li>Review: Nice as well, but here comes question with coefficients.<\/li>\n        <\/ul>\n    <\/li>\n<\/ul>\n<\/span>"}}