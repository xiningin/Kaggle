{"cell_type":{"48a2b778":"code","0b531272":"code","b0869208":"code","8ee8ff6d":"code","0e1104aa":"code","8f999c5f":"code","dfeaa865":"code","eda8d1bd":"code","5d9a7e46":"code","15b0e1e1":"markdown","43be4523":"markdown","20aa948c":"markdown","ee735109":"markdown","b1d20dd8":"markdown","96185fc1":"markdown","33e64b92":"markdown","a8f45b29":"markdown","465a78c1":"markdown","26b68f84":"markdown"},"source":{"48a2b778":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b531272":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/alzheimer\/Alzheimer - Sheet1.csv\")\ndf = df.drop(columns = \"id\")\ndf = df.dropna(axis = 0, how = \"any\")\ndf.head()\n\n","b0869208":"X = df.drop(columns = \"Subject\")\ny = df[\"Subject\"]","8ee8ff6d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 97)","0e1104aa":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, classification_report\nmodel = xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\nprint(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\nprint(classification_report(y_test, model.predict(X_test)))","8f999c5f":"import numpy as np\nimport random\n\n\n\n\nrandom.seed(723)\nnp.random.seed(723)\n\ndef initilialize_poplulation(numberOfParents):\n    learningRate = np.empty([numberOfParents, 1])\n    nEstimators = np.empty([numberOfParents, 1], dtype = np.uint8)\n    maxDepth = np.empty([numberOfParents, 1], dtype = np.uint8)\n    minChildWeight = np.empty([numberOfParents, 1])\n    gammaValue = np.empty([numberOfParents, 1])\n    subSample = np.empty([numberOfParents, 1])\n    colSampleByTree =  np.empty([numberOfParents, 1])\n\n    for i in range(numberOfParents):\n        print(i)\n        learningRate[i] = round(random.uniform(0.01, 1), 2)\n        nEstimators[i] = random.randrange(10, 1500, step = 25)\n        maxDepth[i] = int(random.randrange(1, 10, step= 1))\n        minChildWeight[i] = round(random.uniform(0.01, 10.0), 2)\n        gammaValue[i] = round(random.uniform(0.01, 10.0), 2)\n        subSample[i] = round(random.uniform(0.01, 1.0), 2)\n        colSampleByTree[i] = round(random.uniform(0.01, 1.0), 2)\n    \n    population = np.concatenate((learningRate, nEstimators, maxDepth, minChildWeight, gammaValue, subSample, colSampleByTree), axis= 1)\n    return population\n\n   \n\ndef fitness_accuracy_score(y_true, y_pred):\n    fitness = round((accuracy_score(y_true, y_pred)), 4)\n    return fitness\n\ndef train_population(population, dMatrixTrain, dMatrixtest, y_test):\n    aScore = []\n    for i in range(population.shape[0]):\n        param = { 'objective':'binary:logistic',\n              'learning_rate': population[i][0],\n              'n_estimators': population[i][1], \n              'max_depth': int(population[i][2]), \n              'min_child_weight': population[i][3],\n              'gamma': population[i][4], \n              'subsample': population[i][5],\n              'colsample_bytree': population[i][6],\n              'seed': 24}\n        num_round = 100\n        xgbT = xgb.train(param, dMatrixTrain, num_round)\n        preds = xgbT.predict(dMatrixtest)\n        preds = preds>0.5\n        aScore.append(fitness_accuracy_score(y_test, preds))\n    return aScore\n\n\n\ndef new_parents_selection(population, fitness, numParents):\n    selectedParents = np.empty((numParents, population.shape[1])) \n    \n\n    for parentId in range(numParents):\n        bestFitnessId = np.where(fitness == np.max(fitness))\n        bestFitnessId  = bestFitnessId[0][0]\n        selectedParents[parentId, :] = population[bestFitnessId, :]\n        fitness[bestFitnessId] = -1 \n    return selectedParents\n        \n\ndef crossover_uniform(parents, childrenSize):\n    \n    crossoverPointIndex = np.arange(0, np.uint8(childrenSize[1]), 1, dtype= np.uint8)\n    crossoverPointIndex1 = np.random.randint(0, np.uint8(childrenSize[1]), np.uint8(childrenSize[1]\/2)) \n    crossoverPointIndex2 = np.array(list(set(crossoverPointIndex) - set(crossoverPointIndex1))) \n    \n    children = np.empty(childrenSize)\n    \n    \n    for i in range(childrenSize[0]):\n        \n        parent1_index = i%parents.shape[0]\n        parent2_index = (i+1)%parents.shape[0]\n        children[i, crossoverPointIndex1] = parents[parent1_index, crossoverPointIndex1]\n        children[i, crossoverPointIndex2] = parents[parent2_index, crossoverPointIndex2]\n    return children\n    \n\n\ndef mutation(crossover, numberOfParameters):\n    \n\n    minMaxValue = np.zeros((numberOfParameters, 2))\n    \n    minMaxValue[0:] = [0.01, 1.0] \n    minMaxValue[1, :] = [10, 2000] \n    minMaxValue[2, :] = [1, 15] \n    minMaxValue[3, :] = [0, 10.0] \n    minMaxValue[4, :] = [0.01, 10.0] \n    minMaxValue[5, :] = [0.01, 1.0] \n    minMaxValue[6, :] = [0.01, 1.0] \n \n    \n    mutationValue = 0\n    parameterSelect = np.random.randint(0, 7, 1)\n    print(parameterSelect)\n    if parameterSelect == 0: \n        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n    if parameterSelect == 1: \n        mutationValue = np.random.randint(-200, 200, 1)\n    if parameterSelect == 2:\n        mutationValue = np.random.randint(-5, 5, 1)\n    if parameterSelect == 3: \n        mutationValue = round(np.random.uniform(5, 5), 2)\n    if parameterSelect == 4: \n        mutationValue = round(np.random.uniform(-2, 2), 2)\n    if parameterSelect == 5: \n        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n    if parameterSelect == 6: \n        mutationValue = round(np.random.uniform(-0.5, 0.5), 2)\n  \n    \n    for idx in range(crossover.shape[0]):\n        crossover[idx, parameterSelect] = crossover[idx, parameterSelect] + mutationValue\n        if(crossover[idx, parameterSelect] > minMaxValue[parameterSelect, 1]):\n            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 1]\n        if(crossover[idx, parameterSelect] < minMaxValue[parameterSelect, 0]):\n            crossover[idx, parameterSelect] = minMaxValue[parameterSelect, 0]    \n    return crossover","dfeaa865":"xgDMatrix = xgb.DMatrix(X_train, y_train) \nxgbDMatrixTest = xgb.DMatrix(X_test, y_test)\n\n\n\n\nnumberOfParents = 64 \nnumberOfParentsMating = 32 \nnumberOfParameters = 7 \nnumberOfGenerations = 10\n\n\n\npopulationSize = (numberOfParents, numberOfParameters)\n\n\npopulation = initilialize_poplulation(numberOfParents)\n\n\nfitnessHistory = np.empty([numberOfGenerations+1, numberOfParents])\n\n\npopulationHistory = np.empty([(numberOfGenerations+1)*numberOfParents, numberOfParameters])\n\n\npopulationHistory[0:numberOfParents, :] = population\n\nfor generation in range(numberOfGenerations):\n    print(\"This is number %s generation\" % (generation))\n    \n    \n    fitnessValue = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\n    fitnessHistory[generation, :] = fitnessValue\n    \n    \n    print('Best Accuracy score in the this iteration = {}'.format(np.max(fitnessHistory[generation, :])))\n\n    \n    parents = new_parents_selection(population=population, fitness=fitnessValue, numParents=numberOfParentsMating)\n    \n\n    children = crossover_uniform(parents=parents, childrenSize=(populationSize[0] - parents.shape[0], numberOfParameters))\n    \n    \n    children_mutated = mutation(children, numberOfParameters)\n    \n    \n    \n    population[0:parents.shape[0], :] = parents \n    population[parents.shape[0]:, :] = children_mutated \n    \n    populationHistory[(generation+1)*numberOfParents : (generation+1)*numberOfParents+ numberOfParents , :] = population \n    \n\n\n\nfitness = train_population(population=population, dMatrixTrain=xgDMatrix, dMatrixtest=xgbDMatrixTest, y_test=y_test)\nfitnessHistory[generation+1, :] = fitness\n\n\nbestFitnessIndex = np.where(fitness == np.max(fitness))[0][0]\n\n\nprint(\"Best fitness is =\", fitness[bestFitnessIndex])\n\n\nprint(\"Best parameters are:\")\nprint('learning_rate', population[bestFitnessIndex][0])\nprint('n_estimators', population[bestFitnessIndex][1])\nprint('max_depth', int(population[bestFitnessIndex][2])) \nprint('min_child_weight', population[bestFitnessIndex][3])\nprint('gamma', population[bestFitnessIndex][4])\nprint('subsample', population[bestFitnessIndex][5])\nprint('colsample_bytree', population[bestFitnessIndex][6])","eda8d1bd":"clf = xgb.XGBClassifier(learning_rate = 0.51, n_estimators = 155, max_depth = 4, min_child_weight = 2.2, gamma = 0.32, subsample = 0.78, colsample_bytree = 0.09, seed = 24)\nclf.fit(X_train, y_train)\nprint(\"Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\nprint(classification_report(y_test, clf.predict(X_test)))\n","5d9a7e46":"import shap\nexplainer = shap.Explainer(clf)\nshap_values = explainer(X)\nshap.plots.bar(shap_values)\nplt.show()","15b0e1e1":"# Optimizing Model\nAbove, we see our best fitness(accuracy) score from the genetic algorithim. We also see the parameters for the XGBoost that yielded the best fitness score. In the next cell, we run the XGBoost with new paramters and print the accuracy score and classificaiton report of the predictions. \n","43be4523":"# Hyperparameter tuning XGBoost through Evolutionary Computation\nNow lets optimize our XGBoost model. To do so, we will use evolutionary computation, specifically genetic algorithim. A genetic algorithm is an hyperparameter tuning technique that takes inspiration from Charles Darwin's theory of natural selection. It can be used to optimize the parameters of a machine learning model to improve model performance using concepts such as mutation, crossover, and selection. The genetic algorithm starts with an initial population of parents. Each parent has certain traits(parameters). The initial population is then tested to see how well the parents perform based on a fitness score. This fitness score can be any model performance metric, such as accuracy or F1-score. In this case, we choose the accuracy score as the fitness score for the genetic algorithm. Based on fitness scores, the top-performing parents are selected; this is similar to survival of the fittest. The parents that were selected(\"survived\") now \"mate\" to \"produce offspring\". This involves crossover\/recombination and mutation. In crossover, the machine learning parameters from the survived parents are combined to produce offspring with each offspring inheriting some genes(parameters) from each parent. In addition, the genetic algorithm introduces mutation. In mutation, some of the parameter values are altered to maintain genetic diversity. Mutation allows the genetic algorithm to have a higher fitness score. This entire process repeats as top performing offspring are selected which then mate and produce new offspring with higher fitness. This will continue for a certain number of generations at which the algorithm will terminate and the best fitness(accuracy score) will be yielded.The next two cells feature the code for developing a genetic algorithim for hyperparameter tuning XGBoost. We will have 64 parents, 32 of which will mate to produce offspring. We will use all 7 parameters of XGBoost and will have 10 generations of individuals. ","20aa948c":"# Feature Importance through Shapley Additive Explanations\nNow, we will analyze feature Importance Using Shapley Additive Explanations, a game theory approach to explaining how each feature contributes to the predictions made by the model. \n","ee735109":"# Loading the Dataset Into Kaggle\nFirstly, we will load the Alzheimer's biomarker csv into our Kaggle notebook. After downloading the Alzheimer csv in the drive, go to \"File\" then click on \"Add or Upload Data\". From there select, \"Upload\" and then \"Browse files\" from your computer. Select \"Alzheimer.csv\" downloaded by your computer. Name data as \u201calzheimer\u201d. Lastly, click \"Create\".\n\nThen, click on Plus Icon to Add New Cell. Add the code in the cell below into your new cell. The code lodes the alzheimer csv into data. ","b1d20dd8":"# Gradient Boosting and XGBoost\nBoosting algorithms are a type of ML algorithms used for classification. Boosting algorithims ensembles weak learner to yield a stronger learner, reducing overfitting and bias. XGBoost, a boosting algorithim, is one of the most powerful machine learning tools for tabular data known for its tendancy to yeild accurate classifications. It improves upon the gradient boosting framework through optimization, regularization, built-in cross validation, and many other enhancements. The weak learner for XGBoost algorithm is Decision Tree. Below is the code for the XGBoost algorithim. \n\n","96185fc1":"# Train Test Split\nFor machine learning classification, we further split data into train and test. Train data is the data that the machine learning model uses to learn from. We evaluate our machine learning model using test data.20% of our data will go towards testing while the remaining 80% will be used to train the model. \n","33e64b92":"# Splitting Data\nFor machine learning classification, we have to split our data into features(inputs) and labels(outputs). Features will be in the X dataframe, and labels will be in the y dataframe. Our features are Age, Gender, and Biomarkers. Our label is Class(Healthy or Alzheimer\u2019s). \n","a8f45b29":"# Loading and Wrangling the Data\nAfter importing the essential libraries, load the data using Pandas's \"read_csv\" function. Then we drop the \"id\" column and rows with values. We then output the head of our transformed dataset to give us an idea of how it looks like. ","465a78c1":"# Interpreting Results \nThe cocentrations of PGJ2 contribute most to whether or not a person has Alzheimer's. Research has shwon that PGJ2 is neurotoxic and induces processes relevant to Alzheimer\u2019s such as promoting the formation of amyloid beta plaques. TNF concentrations are the next important feature in the dataset for predicting Alzheimer's. This alligns with previous research, which has shown that TNF signaling worsens amyloid beta plaques and phosphorylated tau proteins, which are hallmarks of Alzheimer\u2019s. The third most important feature is age. Age is considered the greatest risk factor for Alzheimer's disease. Symptoms and risk of this neurodegenerative disease worsens as a person ages. ","26b68f84":"# Machine Learning\nMachine learning involves computer algorithms that learn from data. These algorithims make decisions and predictions without explicitly being programmed to do so. One major category of machine learning is supervised learning, the mathematical modeling out of data that contains structured inputs and outputs.One type of supervised learning is classification, which assigns a label to an observation based on its features. For example, supervised learning can be used to assign an email as \u201cspam\u201d or \u201cnot-spam\u201d based on certain words in the email or diagnose a patient based on certain characteristics.\n"}}