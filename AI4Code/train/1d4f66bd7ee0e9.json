{"cell_type":{"9ee908ef":"code","a4d4fdc2":"code","dcbf21c8":"code","fa34804c":"code","2c8dcf86":"code","986a371a":"code","7bcde731":"code","69c95872":"code","174232f9":"code","4c3bd330":"code","f5f734fb":"code","654c9b66":"code","c2c0afc5":"code","aa6c9e54":"code","f118c684":"code","f9f64dad":"code","bfdbc97f":"code","7e77984f":"code","78e5622b":"code","2620a1a7":"code","dc2c89bd":"code","c7935c59":"code","25e7da6d":"code","fd5f84db":"code","0aac5306":"code","3d5df2c7":"code","6518d726":"code","21bb768c":"code","8bb6a537":"code","596fb94e":"code","e1dd7b86":"code","8889e2bd":"code","d1a7e175":"code","e03da371":"code","7e26baec":"markdown","65597e99":"markdown","42438f0b":"markdown","a9274fa3":"markdown","a420c8bb":"markdown","16ad9da8":"markdown","989ca92e":"markdown","2cf1af77":"markdown","f976fc93":"markdown","d16a574c":"markdown","2718df85":"markdown","51cc496b":"markdown","a80e4a22":"markdown","32b91b43":"markdown","4aeff978":"markdown","2b3bb6f0":"markdown","be84f438":"markdown","03d3dc3c":"markdown","a0b90291":"markdown","caef1c7e":"markdown"},"source":{"9ee908ef":"import gc\nimport time\n# Data\nimport numpy as np\nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nimport random\nrandom.seed(1337)\nnp.random.seed(1337)\n\n# Credit for this method here: https:\/\/www.kaggle.com\/rejasupotaro\/effective-feature-engineering\ndef reload():\n    gc.collect()\n    df = pd.read_csv('..\/input\/train_V2.csv')\n    invalid_match_ids = df[df['winPlacePerc'].isna()]['matchId'].values\n    df = df[-df['matchId'].isin(invalid_match_ids)]\n    return df","a4d4fdc2":"df = reload()\ndf.head()","dcbf21c8":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df\ndf = reduce_mem_usage(df)","fa34804c":"df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')","2c8dcf86":"df['killsNorm'] = df['kills']*((100-df['playersJoined'])\/100 + 1)\ndf['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])\/100 + 1)\ndf['maxPlaceNorm'] = df['maxPlace']*((100-df['playersJoined'])\/100 + 1)\ndf['matchDurationNorm'] = df['matchDuration']*((100-df['playersJoined'])\/100 + 1)\ndf = reduce_mem_usage(df)\ndf.head()","986a371a":"# Dropping columns with too many categories or unique values as well as the target column.\ntarget = 'winPlacePerc'\ndrop_cols = ['Id', 'groupId', 'matchId', target]\nselect = [x for x in df.columns if x not in drop_cols]\nX = df.loc[:, select]\nX.head()","7bcde731":"# Now one-hot encode the remaining category column (matchType)\nX = pd.get_dummies(X)\nX.head()","69c95872":"from sklearn.decomposition import PCA","174232f9":"pca2 = PCA(n_components=2)\npca2.fit(X)","4c3bd330":"print(sum(pca2.explained_variance_ratio_))\nP2 = pca2.transform(X)","f5f734fb":"plt.scatter(P2[:100000, 0], P2[:100000, 1])\nplt.show()","654c9b66":"pca3 = PCA(n_components=3)\npca3.fit(X)\nprint(sum(pca3.explained_variance_ratio_))\nP3 = pca3.transform(X)","c2c0afc5":"from mpl_toolkits.mplot3d import Axes3D","aa6c9e54":"fig_p3 = plt.figure()\nax = Axes3D(fig_p3, elev=48, azim=134)\nax.scatter(P3[:100000, 0], P3[:100000, 1], P3[:100000, 2])\nfig_p3.show()","f118c684":"from sklearn.cluster import KMeans","f9f64dad":"kms = KMeans(n_clusters=2).fit(P2)","bfdbc97f":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms.labels_[:100000])\nplt.show()","7e77984f":"kms3 = KMeans(n_clusters=3).fit(P2)\nkms4 = KMeans(n_clusters=4).fit(P2)\nkms5 = KMeans(n_clusters=5).fit(P2)","78e5622b":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms3.labels_[:100000])\nplt.show()","2620a1a7":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms4.labels_[:100000])\nplt.show()","dc2c89bd":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms5.labels_[:100000])\nplt.show()","c7935c59":"kms6 = KMeans(n_clusters=6).fit(P2)\nkms7 = KMeans(n_clusters=7).fit(P2)\nkms8 = KMeans(n_clusters=8).fit(P2)","25e7da6d":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms6.labels_[:100000])\nplt.show()","fd5f84db":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms7.labels_[:100000])\nplt.show()","0aac5306":"plt.scatter(P2[:100000, 0], P2[:100000, 1], c=kms8.labels_[:100000])\nplt.show()","3d5df2c7":"def cluster_features(df, model, pca):\n    P = pca.transform(df)\n    new_df = pd.DataFrame()\n    new_df['cluster'] = model.predict(P)\n    one_hot = pd.get_dummies(new_df['cluster'], prefix='cluster')\n    new_df = new_df.join(one_hot)\n    new_df = new_df.drop('cluster', axis=1)\n    new_df = new_df.fillna(0)\n    return new_df\n    \ndef centroid_features(df, model, pca):\n    P = pd.DataFrame(pca.transform(df))\n    new_df = pd.DataFrame()\n    cluster = 0\n    for centers in model.cluster_centers_:\n        new_df['distance_{}'.format(cluster)] = np.linalg.norm(P[[0, 1]].sub(np.array(centers)), axis=1)\n        cluster += 1\n    return new_df","6518d726":"def norm_features(df):\n    df['playersJoined'] = df.groupby('matchId')['matchId'].transform('count')\n    df['killsNorm'] = df['kills']*((100-df['playersJoined'])\/100 + 1)\n    df['damageDealtNorm'] = df['damageDealt']*((100-df['playersJoined'])\/100 + 1)\n    df['maxPlaceNorm'] = df['maxPlace']*((100-df['playersJoined'])\/100 + 1)\n    df['matchDurationNorm'] = df['matchDuration']*((100-df['playersJoined'])\/100 + 1)\n    df = reduce_mem_usage(df)\n    return df\n\ndef one_hot_encode(df):\n    return pd.get_dummies(df, columns=['matchType'])\n\ndef remove_categories(df):\n    target = 'winPlacePerc'\n    drop_cols = ['Id', 'groupId', 'matchId', 'matchType', target]\n    select = [x for x in df.columns if x not in drop_cols]\n    return df.loc[:, select]","21bb768c":"def kmeans_5_clusters(df):\n    return df.join(cluster_features(remove_categories(one_hot_encode(norm_features(df))), kms5, pca2))\n    \ndef kmeans_5_centroids(df):\n    return df.join(centroid_features(remove_categories(one_hot_encode(norm_features(df))), kms5, pca2))\n\ndef kmeans_3_clusters(df):\n    return df.join(cluster_features(remove_categories(one_hot_encode(norm_features(df))), kms3, pca2))\n    \ndef kmeans_3_centroids(df):\n    return df.join(centroid_features(remove_categories(one_hot_encode(norm_features(df))), kms3, pca2))\n\ndef kmeans_4_clusters(df):\n    return df.join(cluster_features(remove_categories(one_hot_encode(norm_features(df))), kms4, pca2))\n    \ndef kmeans_4_centroids(df):\n    return df.join(centroid_features(remove_categories(one_hot_encode(norm_features(df))), kms4, pca2))","8bb6a537":"def train_test_split(df, test_size=0.1):\n    match_ids = df['matchId'].unique().tolist()\n    train_size = int(len(match_ids) * (1 - test_size))\n    train_match_ids = random.sample(match_ids, train_size)\n\n    train = df[df['matchId'].isin(train_match_ids)]\n    test = df[-df['matchId'].isin(train_match_ids)]\n    \n    return train, test","596fb94e":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\n\ndef run_experiment(preprocess):\n    df = reload()    \n\n    df = preprocess(df)\n    df.fillna(0, inplace=True)\n    \n    target = 'winPlacePerc'\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'matchType', target]\n    cols_to_fit = [col for col in df.columns if col not in cols_to_drop]\n    train, val = train_test_split(df, 0.1)\n    \n    model = LinearRegression()\n    model.fit(train[cols_to_fit], train[target])\n    \n    y_true = val[target]\n    y_pred = model.predict(val[cols_to_fit])\n    return mean_absolute_error(y_true, y_pred)\n\ndef run_experiments(preprocesses):\n    results = []\n    for preprocess in preprocesses:\n        start = time.time()\n        score = run_experiment(preprocess)\n        execution_time = time.time() - start\n        results.append({\n            'name': preprocess.__name__,\n            'score': score,\n            'execution time': f'{round(execution_time, 2)}s'\n        })\n        gc.collect()\n        \n    return pd.DataFrame(results, columns=['name', 'score', 'execution time']).sort_values(by='score')","e1dd7b86":"def original(df):\n    return df\n\ndef items(df):\n    df['items'] = df['heals'] + df['boosts']\n    return df\n\ndef players_in_team(df):\n    agg = df.groupby(['groupId']).size().to_frame('players_in_team')\n    return df.merge(agg, how='left', on=['groupId'])\n\ndef total_distance(df):\n    df['total_distance'] = df['rideDistance'] + df['swimDistance'] + df['walkDistance']\n    return df\n\ndef headshotKills_over_kills(df):\n    df['headshotKills_over_kills'] = df['headshotKills'] \/ df['kills']\n    df['headshotKills_over_kills'].fillna(0, inplace=True)\n    return df\n\ndef killPlace_over_maxPlace(df):\n    df['killPlace_over_maxPlace'] = df['killPlace'] \/ df['maxPlace']\n    df['killPlace_over_maxPlace'].fillna(0, inplace=True)\n    df['killPlace_over_maxPlace'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef walkDistance_over_heals(df):\n    df['walkDistance_over_heals'] = df['walkDistance'] \/ df['heals']\n    df['walkDistance_over_heals'].fillna(0, inplace=True)\n    df['walkDistance_over_heals'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef walkDistance_over_kills(df):\n    df['walkDistance_over_kills'] = df['walkDistance'] \/ df['kills']\n    df['walkDistance_over_kills'].fillna(0, inplace=True)\n    df['walkDistance_over_kills'].replace(np.inf, 0, inplace=True)\n    return df\n\ndef teamwork(df):\n    df['teamwork'] = df['assists'] + df['revives']\n    return df\n\ndef min_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId','groupId'])[features].min()\n    return df.merge(agg, suffixes=['', '_min'], how='left', on=['matchId', 'groupId'])\n\ndef max_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].max()\n    return df.merge(agg, suffixes=['', '_max'], how='left', on=['matchId', 'groupId'])\n\ndef sum_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].sum()\n    return df.merge(agg, suffixes=['', '_sum'], how='left', on=['matchId', 'groupId'])\n\ndef median_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].median()\n    return df.merge(agg, suffixes=['', '_median'], how='left', on=['matchId', 'groupId'])\n\ndef mean_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n    return df.merge(agg, suffixes=['', '_mean'], how='left', on=['matchId', 'groupId'])\n\ndef rank_by_team(df):\n    cols_to_drop = ['Id', 'groupId', 'matchId', 'winPlacePerc']\n    features = [col for col in df.columns if col not in cols_to_drop]\n    agg = df.groupby(['matchId', 'groupId'])[features].mean()\n    agg = agg.groupby('matchId')[features].rank(pct=True)\n    return df.merge(agg, suffixes=['', '_mean_rank'], how='left', on=['matchId', 'groupId'])","8889e2bd":"run_experiments([\n    original,\n    items,\n    players_in_team,\n    total_distance,\n    headshotKills_over_kills,\n    killPlace_over_maxPlace,\n    walkDistance_over_heals,\n    walkDistance_over_kills,\n    teamwork\n])","d1a7e175":"run_experiments([\n    original,\n    kmeans_3_clusters, \n    kmeans_3_centroids,\n    kmeans_4_clusters, \n    kmeans_4_centroids,\n    kmeans_5_clusters, \n    kmeans_5_centroids\n])","e03da371":"run_experiments([\n    original,\n    min_by_team,\n    max_by_team,\n    sum_by_team,\n    median_by_team,\n    mean_by_team,\n    rank_by_team,\n])","7e26baec":"Nice! You can see two pretty clear clusters in the data with two components! It's too early to tell whether this will be useful for building a model, but it's helpful to see nonetheless. Now lets try it with three dimensions to see if clusters show up there as well!","65597e99":"Wow! That's almost all of the variance explained in only three components! That's wild! Lets see if there's any obvious clusters that pop out when we plot it... Once again, we're only plotting the first 100k in order to save time and memory.","42438f0b":"### 1.3 PCA With Two Components\n\nNow we're going to try PCA with two components and see if we can see any clear clusters on the graph.**","a9274fa3":"### 3.1 Linear Experiment\n\nI'm going to lift some code from the Kernel I mentioned earlier to run and prepare experiments from preprocess methods.","a420c8bb":"### 3.3 Run the Experiments\n\nTime to run the experiments alongside the other features. Remember with mean absolute error, a lower error is better! We're using a validation split on the match id, so anything that's lower than the original on the validation set is promising. Only trying it on a tree based model will really give us a good idea though, because linear regression (what we're using for the simple model) is highly sensitive to how we treat categorical data like the `clusters` features. ","16ad9da8":"Well that's unfortunate. It seems K-means doesn't find those two clusters. This is a setback because it means I'll need to experiment with more clustering methods but lets see if we can't get K-means to converge on at least part of the upper and lower clusters.","989ca92e":"### 1.1 Normalize the Data\n\nIn my first version of this Kernel I didn't do any normalization. I'm changing that this iteration. I'm going to normalize the `kills`, `damageDealt`, `maxPlace` and `matchDuration` with the number of players in the match. Inspired by [this kernel](https:\/\/www.kaggle.com\/carlolepelaars\/pubg-data-exploration-rf-funny-gifs) by Carlo.","2cf1af77":"Interesting. Sadly it doesn't look as if we got additional clusters from this data with three components, but it does reinforce what we saw on the earlier clusters: that there are two rather distinct clusters. We'll have to see if they're useful for creating a model later on!","f976fc93":"So it doesn't look like K-means is going to give us exactly what we want. None-the-matter, the 5-cluster K-means produced some interesting clusters, so we can use those to generate some features. We'll explore a few more clustering algorithms later on. For now, lets look at some feature engineering using K-means clusters!","d16a574c":"#### 2.1.1 Feature Generation from K-Means\n\nWe're going to generate features in a few different ways: we're going to calculate distances to cluster centroids and predicted clusters. We're going to create a function that takes in a data frame and returns the processed one.","2718df85":"Since our features used for the clustering were normalized, we need to normalize them prior to feeding them into our experimenter. This method normalizes them and the following two functions are experiments our processor can take.","51cc496b":"Interesting! So it's stubornly not converging on the top and bottom \"tails\", and keeps creating clusters closer to the heads. Lets see if 6, 7 or 8 clusters will separate out those tails. If not, we may need to explore another clustering method.","a80e4a22":"### 3.2 Features to Compare With\n\nThe feature generation we did with K-means won't mean much until we compare it with other possible features. I'm going to use some features from the earlier Kernel I referenced: \"[Effective Feature Engineering](https:\/\/www.kaggle.com\/rejasupotaro\/effective-feature-engineering)\" by rejasupotaro.","32b91b43":"## 3. Baseline Model\n\nIn order to determine how useful the clusters are for building a model, we need to first try to fit a model to the data as is, so we can get a baseline to compare with later.\n\nI'm going to base some of my code off of suggestions from [rejasupotaro's kernel](https:\/\/www.kaggle.com\/rejasupotaro\/effective-feature-engineering) including to split by match id and to use a simple linear regressor to test some experiments.","4aeff978":"## 2. Clustering Time\n\nSo now that we have reduced the dimensionality of the data, we can actually perform some clustering in order to get some engineered features out of the data. While we can see two pretty clear clusters at first glance, computers aren't that smart without some coaching.\n\n### 2.1 K-Means\n\nThe first clustering method we're going to use is K-means. I'm hoping it will converge on two clusters, the top and bottom clusters we can see in the scatter plots.","2b3bb6f0":"**Wowza!** That's a lot of variance explained using only two components, which is nice because it'll graph extrermely well in two dimensions!\n\nBecause we have several million datapoints, it would take forever for matplotlib to graph them all. So we'll just graph the first 100k, the distribution is the same (I checked!) and all of our math will still be done on the whole data.","be84f438":"### 1.2 Preprocess the Non-Numerical Columns\n\nThis dataset has some non-numerical columns and PCA can only take numerical values. Consequently, we need to one-hot encode or drop things we're not using for this. We're also going to drop the `winPlacePerc` since it's our target variable, we obviously don't want it in included in the PCA.\n\nAdditionally, while the -1 values for `rankPoints` should probably be considered NaN values according to the data description, PCA can't take NaN values, so I'm going to leave them as -1 for now.\n","03d3dc3c":"## 1. Dimensionality Reduction\n\nClustering is **highly** sensitive to dimensionality, and also hard to visualize outside of 3-Dimensions. One thing I like to do when clustering is to use PCA to reduce the dimensionality of the data. You can see the example I used to build off of [here](http:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py) but I'll try to explain what I'm doing as I go.","a0b90291":"# A Quick Exploration of Clustering in the PUBG Dataset\n\nClustering can be a powerful tool, both for EDA and for feature engineering. I wanted to take a look at the PUBG data and see what kind of clusters were hiding in the data.","caef1c7e":"### 1.4 PCA With Three Components\n\nAlthough our first PCA with two components explained quite a bit of the variance, it still fell short of the 80% variance rule of thumb, so I'm going to do some PCA again, but with three components this time! Similar steps to what we did last time, but we expect it to explain more of the variance due to the addition of another dimension."}}