{"cell_type":{"3d8a6702":"code","5ada9dc4":"code","c83d9230":"code","08432048":"code","2da664ef":"code","ad8ae1ee":"code","77aee6de":"code","6f10371d":"code","2d8e0c68":"code","f1e09819":"code","2a9cac32":"code","818d69ec":"code","a790b09d":"code","1ab1bd60":"markdown","bef6f789":"markdown","a7327da1":"markdown","110bce38":"markdown","5c66f048":"markdown","ffd927e6":"markdown","257fb6b2":"markdown","09664c67":"markdown","26690f80":"markdown"},"source":{"3d8a6702":"import pandas as pd \nfrom bs4 import BeautifulSoup  \nimport re\nfrom nltk.corpus import stopwords # Import the stop word list","5ada9dc4":"train = pd.read_csv(\"..\/input\/popcorn\/labeledTrainData.tsv\", header=0,delimiter=\"\\t\", quoting=3)","c83d9230":"print(\"Number of rows: \",train.shape[0])\nprint(\"Number of columns: \",train.shape[1])\nprint(\"List of columns: \",train.columns)","08432048":"train.head(5)","2da664ef":"print(train[\"review\"][0])","ad8ae1ee":"def review_to_words( raw_review ):\n    # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043d\u0435\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u043e\u0433\u043e \u043e\u0431\u0437\u043e\u0440\u0430 \u0432 \u0441\u0442\u0440\u043e\u043a\u0443 \u0441\u043b\u043e\u0432\n    # \u0412\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0441\u043e\u0431\u043e\u0439 \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443 (\u043d\u0435\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0439 \u043e\u0431\u0437\u043e\u0440 \u0444\u0438\u043b\u044c\u043c\u0430), \u0438\n    #\u0432\u044b\u0432\u043e\u0434 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443 (\u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u043d\u044b\u0439 \u043e\u0431\u0437\u043e\u0440 \u0444\u0438\u043b\u044c\u043c\u0430)\n    #\n    # 1. \u0423\u0434\u0430\u043b\u0438\u0442\u044c HTML\n    review_text = BeautifulSoup(raw_review).get_text() \n    #\n    # 2. \u0423\u0434\u0430\u043b\u0438\u0442\u044c \u043d\u0435-\u0431\u0443\u043a\u0432\u044b     \n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n    #\n    # 3. \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442\u0440, \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430\n    words = letters_only.lower().split()                             \n    #\n    # 4. \u0412 Python \u043f\u043e\u0438\u0441\u043a \u043d\u0430\u0431\u043e\u0440\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442\u0441\u044f \u043d\u0430\u043c\u043d\u043e\u0433\u043e \u0431\u044b\u0441\u0442\u0440\u0435\u0435, \u0447\u0435\u043c \u043f\u043e\u0438\u0441\u043a \u043f\u043e \u0441\u043f\u0438\u0441\u043a\u0443, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0439\u0442\u0435 \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430 \u0432 \u043d\u0430\u0431\u043e\u0440\n    stops = set(stopwords.words(\"english\"))                  \n    # \n    # 5. \u0423\u0434\u0430\u043b\u0438\u0442\u044c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\n    meaningful_words = [w for w in words if not w in stops]   \n    #\n    # 6. \u0421\u043e\u0435\u0434\u0438\u043d\u0438\u0442\u0435 \u0441\u043b\u043e\u0432\u0430 \u043e\u0431\u0440\u0430\u0442\u043d\u043e \u0432 \u043e\u0434\u043d\u0443 \u0441\u0442\u0440\u043e\u043a\u0443, \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u043d\u0443\u044e \u043f\u0440\u043e\u0431\u0435\u043b\u043e\u043c, \u0438 \u0432\u0435\u0440\u043d\u0438\u0442\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442.\n    return( \" \".join( meaningful_words ))   ","77aee6de":"# \u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043e\u0442\u0437\u044b\u0432\u043e\u0432 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0444\u0440\u0435\u0439\u043c\u0430 \u0434\u0430\u043d\u043d\u044b\u0445\nnum_reviews = train[\"review\"].size\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0434\u043b\u044f \u0445\u0440\u0430\u043d\u0435\u043d\u0438\u044f \u0447\u0438\u0441\u0442\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439\nclean_train_reviews = []\n \nprint(\"Cleaning and parsing the training set movie reviews...\\n\")\nclean_train_reviews = []\nfor i in range( 0, num_reviews ):\n    # \u0415\u0441\u043b\u0438 \u0438\u043d\u0434\u0435\u043a\u0441 \u0440\u0430\u0432\u043d\u043e\u043c\u0435\u0440\u043d\u043e \u0434\u0435\u043b\u0438\u0442\u0441\u044f \u043d\u0430 1000, \u0432\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0441\u043e\u043e\u0431\u0449\u0435\u043d\u0438\u0435\n    if( (i+1)%10000 == 0 ):\n        print(\"Review %d of %d completed\\n\" % ( i+1, num_reviews ))                                                                    \n    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))\nprint(\"Data cleaning completed\\n\")","6f10371d":"print(\"Creating the bag of words...\\n\")\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#\u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043e\u0431\u044a\u0435\u043a\u0442 \"CountVectorizer\", \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0438\u043d\u0441\u0442\u0440\u0443\u043c\u0435\u043d\u0442\u043e\u043c scikit-\u0438\u0437\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430\u0431\u043e\u0440\u0430 \u0441\u043b\u043e\u0432.  \nvectorizer = CountVectorizer(analyzer = \"word\",   \\\n                             tokenizer = None,    \\\n                             preprocessor = None, \\\n                             stop_words = None,   \\\n                             max_features = 5000) \n\n# fit_transform() \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442 \u0434\u0432\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438: \u0432\u043e-\u043f\u0435\u0440\u0432\u044b\u0445, \u043e\u043d \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u0435\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0438\u0437\u0443\u0447\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c; \n# \u0432\u043e-\u0432\u0442\u043e\u0440\u044b\u0445, \u043e\u043d \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u0442 \u043d\u0430\u0448\u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u0412\u0445\u043e\u0434\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u043b\u044f \n# fit_transform \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c \u0441\u043f\u0438\u0441\u043e\u043a \u0441\u0442\u0440\u043e\u043a.\ntrain_data_features = vectorizer.fit_transform(clean_train_reviews)\n\n# \u0421 \u043c\u0430\u0441\u0441\u0438\u0432\u0430\u043c\u0438 \u043b\u0435\u0433\u043a\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u044b\u0432\u0430\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432 \u043c\u0430\u0441\u0441\u0438\u0432\ntrain_data_features = train_data_features.toarray()\nprint(\"done\\n\")","2d8e0c68":"train_data_features.shape","f1e09819":"# \u0412\u0437\u0433\u043b\u044f\u043d\u0435\u043c \u043d\u0430 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435\nvocab = vectorizer.get_feature_names()\nprint(vocab)","2a9cac32":"import numpy as np\n\n# \u0421\u0443\u043c\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\ndist = np.sum(train_data_features, axis=0)\n\n# \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u043d\u0438\u0445 \u0432\u044b\u0432\u0435\u0434\u0438\u0442\u0435 \u0441\u043b\u043e\u0432\u0430\u0440\u043d\u043e\u0435 \u0441\u043b\u043e\u0432\u043e \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0437, \n# \u043a\u043e\u0433\u0434\u0430 \u043e\u043d\u043e \u043f\u043e\u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0435\u043c \u043d\u0430\u0431\u043e\u0440\u0435\n\nfor tag, count in zip(vocab, dist):\n    print(count, tag)","818d69ec":"print(\"Training the random forest...\")\nfrom sklearn.ensemble import RandomForestClassifier\n\n# \u0418\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f Random Forest classifier \u0438\u0437 100 \u0434\u0435\u0440\u0435\u0432\u044c\u0435\u0432\nforest = RandomForestClassifier(n_estimators = 200) \n\n# \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043b\u0435\u0441\u0430 \u0432 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u0439 \u043d\u0430\u0431\u043e\u0440, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044f \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u043e\u0432 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \n# \u0444\u0443\u043d\u043a\u0446\u0438\u0439 \u0438 \u043c\u0435\u0442\u043a\u0438 \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0439 \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043e\u0442\u0432\u0435\u0442\u0430\n\nforest = forest.fit( train_data_features, train[\"sentiment\"] )\nprint('done')","a790b09d":"# \u0421\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 test data\ntest = pd.read_csv(\"..\/input\/popcorn\/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n\n# \u0423\u0431\u0435\u0436\u0434\u0430\u0435\u043c\u0441\u044f, \u0447\u0442\u043e \u0438\u043c\u0435\u0435\u0442\u0441\u044f 25 000 \u0441\u0442\u0440\u043e\u043a \u0438 2 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\nprint(test.shape)\n\n# \u0421\u043e\u0437\u0434\u0430\u0451\u043c \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0438 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u0447\u0438\u0441\u0442\u044b\u0435 reviews \u043e\u0434\u0438\u043d \u0437\u0430 \u0434\u0440\u0443\u0433\u0438\u043c\nnum_reviews = len(test[\"review\"])\nclean_test_reviews = [] \n\nprint(\"Cleaning and parsing the test set movie reviews...\\n\")\nfor i in range(0,num_reviews):\n    if( (i+1) % 10000 == 0 ):\n        print(\"Review %d of %d completed\\n\" % (i+1, num_reviews))\n    clean_review = review_to_words( test[\"review\"][i] )\n    clean_test_reviews.append( clean_review )\n\n# \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043d\u0430\u0431\u043e\u0440 \u0441\u043b\u043e\u0432 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u043d\u0430\u0431\u043e\u0440\u0430 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0435\u0435\u043c \u0435\u0433\u043e \u0432 \u043c\u0430\u0441\u0441\u0438\u0432 numpy\ntest_data_features = vectorizer.transform(clean_test_reviews)\ntest_data_features = test_data_features.toarray()\n\n# \u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u0439 \u043b\u0435\u0441 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u043e\u043a \u043d\u0430\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0439\nresult = forest.predict(test_data_features)\n\n# \u041a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0432 \u0444\u0440\u0435\u0439\u043c \u0434\u0430\u043d\u043d\u044b\u0445 pandas \u0441\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u043c \"id\" \n# \u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u043c \"sentiment\".\noutput = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n\noutput.to_csv( \"submission.csv\", index=False, quoting=3 )\nprint(\"Output data ready for submission\\n\")","1ab1bd60":"\u0421\u0447\u0438\u0442\u044b\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","bef6f789":"# # **Creating a Submission**","a7327da1":"\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043e\u0431\u0437\u043e\u0440\u0430 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445","110bce38":"# # **\u0427\u0442\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445**","5c66f048":"\n\u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","ffd927e6":"# # **Classification using Random Forest**","257fb6b2":"\u0418\u0437\u0443\u0447\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","09664c67":"# # **\u041e\u0447\u0438\u0441\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430**","26690f80":"# # **Creating Features from a Bag of Words**"}}