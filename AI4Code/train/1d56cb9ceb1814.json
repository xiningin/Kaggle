{"cell_type":{"13bafa46":"code","ff82787a":"code","d26bc71a":"code","63fcc523":"code","61292059":"code","4167c7af":"code","f09e1890":"code","b4c01524":"code","c04e9ff6":"code","4647c3bb":"code","fe65be53":"code","e59c799b":"code","91aae2d6":"code","5bd581d2":"code","99bdf794":"code","4c7a55bb":"code","26e67cb0":"code","136b7b5c":"code","dfc0f976":"code","f84c243b":"code","02559962":"code","04f90fef":"code","311bab75":"code","e08db735":"code","86aab23d":"code","52c3f6ea":"code","086445bb":"code","13cb7bd2":"code","a3acfc18":"code","1bd355ba":"code","500ec09c":"code","b5894059":"code","2693a77c":"code","df99a1d1":"code","e0e9ba8f":"code","27435d66":"code","1ff8b573":"code","f31c907c":"code","4cadf42e":"code","f752a52d":"code","98f88c25":"code","c6b2f9af":"code","7a4f0337":"code","3efcd978":"code","7907294f":"code","d30ff77e":"code","b1fcbf21":"code","79ac31e8":"code","38926ca7":"code","15466e4c":"code","80c58ad2":"code","8f3a20fd":"code","03307be3":"code","4efb2146":"code","856e85aa":"code","1276dc1b":"code","7e1914b9":"code","5b8c30af":"code","85634fd1":"code","c6d06172":"code","950ad919":"code","fcab6825":"code","43ae76c8":"code","99769839":"code","828711f1":"code","7450543a":"code","0c98152f":"code","e78910f1":"code","e42f8e25":"code","1782b681":"code","12853f9d":"code","410e4e72":"code","1a89c613":"code","e3e50fb5":"code","b63afdfa":"code","d5f1d99b":"code","ac7b789a":"code","1eefa065":"code","9c45d642":"code","e47864c8":"code","a0fc8ec2":"code","ff20a3ed":"code","73f1e587":"code","c6cee2cc":"code","a9dc72f9":"code","45affbdd":"code","56403f7e":"code","1aad6c12":"code","a6e5cb18":"code","e8c0082b":"code","b08b28cf":"code","ef46bca8":"code","019f5349":"markdown","c6a769e2":"markdown","3c3b4668":"markdown","381aee27":"markdown","51a276fc":"markdown","9295e26e":"markdown","d3b2f7c0":"markdown","80c0981f":"markdown","c4ea9858":"markdown","d532b34b":"markdown","97aebb74":"markdown","53dec500":"markdown","bc21c1cb":"markdown","a3b5547c":"markdown","48f76e2c":"markdown","5a712616":"markdown","b382fbe8":"markdown","d66bf5c7":"markdown","027e3c0e":"markdown","bbfcdb2c":"markdown","198673d2":"markdown","a76b58dc":"markdown","ba749a82":"markdown","b23163dc":"markdown","77216153":"markdown","8bbe555e":"markdown","84d357ca":"markdown","eae1ce6d":"markdown","067869b3":"markdown","9f887e7f":"markdown","2e7d0b16":"markdown","db21bb8b":"markdown","9b7a96d5":"markdown","c5d6a9d2":"markdown","61f9c663":"markdown","f7994000":"markdown","6324d12b":"markdown","324ced01":"markdown","0c0f95df":"markdown","ac1aeda5":"markdown","14b99399":"markdown","14431410":"markdown","01da65cb":"markdown","a4afd162":"markdown","ff3cb85c":"markdown","8cdd97d7":"markdown"},"source":{"13bafa46":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport seaborn as sns\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ff82787a":"plt.style.available","d26bc71a":"a =[1,2,3,4]\nplt.plot(a)\nplt.show()","63fcc523":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_PassengerId = test_df[\"PassengerId\"]","61292059":"train_df.columns","4167c7af":"train_df.head()","f09e1890":"train_df.describe()","b4c01524":"train_df.info()","c04e9ff6":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Sex\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature \n    var = train_df[variable]\n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()\n    \n    \n    #visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,  varValue))","4647c3bb":"category1 = [\"Survived\", \"Sex\", \"Pclass\", \"Embarked\", \"SibSp\", \"Parch\"] #variable olarak goruyor.\nfor c in category1:\n    bar_plot(c)","fe65be53":"# we seperated that part because we can confuse to name,cabin and ticket number these are special and we dont understand in vizualize tool. We just run counts.\ncategory2 = [\"Cabin\", \"Name\", \"Ticket\"]\nfor c in category2:\n    print(\"{} \\n\".format(train_df[c].value_counts()))","e59c799b":"def plot_hist(variable): # bu degiskenleri for i in metodunda fare, age ve passengerId dondurerek degerleri al\u0131p yazmam\u0131z\u0131 sagl\u0131yor (variable)\n    plt.figure(figsize = (9,3))\n    plt.hist(train_df[variable], bins = 50) #default 10 dur s\u0131kl\u0131k.\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()","91aae2d6":"numericVar = [\"Fare\", \"Age\", \"PassengerId\"] # python bu k\u0131sm\u0131 variable olarak goruyor esitlememize gerek kalm\u0131yor. num variable gibi categorical da ayni sekilde.\nfor n in numericVar: #feature lar\u0131m plotta yazd\u0131r\u0131l\u0131yor.\n    plot_hist(n)","5bd581d2":"# Pclass vs Survived\ntrain_df[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index = False).mean().sort_values(by=\"Survived\",ascending= False) #grupla ve ortalamas\u0131n\u0131 goster. Hayatta kalma oran\u0131m\u0131z. S\u0131n\u0131flara gore bakt\u0131\u011f\u0131m\u0131zda 1. s\u0131n\u0131fta olanlar hayatta kalma oran\u0131 daha fazla.","99bdf794":"# Sex vs Survived\ntrain_df[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index = False).mean().sort_values(by=\"Survived\",ascending= False) # prediction 1. s\u0131n\u0131f ve kad\u0131nsa hayatta kalma olas\u0131l\u0131\u011f\u0131 \u00e7ok y\u00fcksektir diyebiliriz. ML'imiz de buna bakacakt\u0131r.","4c7a55bb":"# Sibsp vs Survived\ntrain_df[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index = False).mean().sort_values(by=\"Survived\",ascending= False) #2 den fazla kisi olursa hayatta kalma oran\u0131 dusuyor. ","26e67cb0":"# Parch vs Survived #groupby buna gore grupla dmeek\ntrain_df[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index = False).mean().sort_values(by=\"Survived\",ascending= False) #anlamam\u0131z gereken iliskiler ve s\u0131n\u0131fland\u0131rmalar icin s\u0131n\u0131fland\u0131rmam gereklidir. Diger feature lar icinde siz yapabilirsiniz bakabilir ve yorumlayabilirsiniz.","136b7b5c":"#df.c() #hangi feature \u0131n ","dfc0f976":"def detect_outliers(df,features): #icersine outlier ve feature lar\u0131m\u0131z\u0131 alacak.\n    outlier_indices = []\n    \n    for c in features:\n        #1st quartile\n        Q1 = np.percentile(df[c],25)\n        #3rd quartile\n        Q3 = np.percentile(df[c],75)\n        #IQR\n        IQR = Q3-Q1\n        #outlier step\n        outlier_step = IQR * 1.5\n        #detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step)  | (df[c] > Q3 + outlier_step )].index\n        #store indeces\n        outlier_indices.extend(outlier_list_col)#outlier list column lar\u0131 topluyorum.\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v>2 )  #outlier lar 2 den fazlaysa \u00e7\u0131kart. 1 taneyse \u00e7\u0131kartmama gerek yok demek\n    \n    return multiple_outliers","f84c243b":"train_df.loc[detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\" ])]","02559962":"a = [\"a\",\"a\",\"a\",\"a\",\"b\",\"b\"]\nCounter(a) #ornek","04f90fef":"#drop outliers\ntrain_df = train_df.drop(detect_outliers(train_df,[\"Age\",\"SibSp\",\"Parch\",\"Fare\" ]), axis = 0).reset_index(drop = True)","311bab75":"test_df.head(10)","e08db735":"train_df_len= len(train_df) #train_df in ilk 998 boyutuda kaydetmek istemiyorum bunun icinde tutucam. 3-4 kere s\u00fcrekli run etmemek laz\u0131m. T\u00fcm variable lar reset section a bas\u0131ca. Yormamak i\u00e7in.\ntrain_df = pd.concat ([train_df, test_df], axis = 0 ).reset_index(drop = True) #yukar\u0131dan asag\u0131ya kaydediyorum.","86aab23d":"train_df.head()","52c3f6ea":"train_df.columns[train_df.isnull().any()] #train_df missing value var m\u0131 bak\u0131caz. Hangi feature da oldu\u011funu g\u00f6rece\u011fiz. Survived normal, ","086445bb":"train_df.isnull().sum() #missing value bul ve topla","13cb7bd2":"train_df[train_df[\"Embarked\"].isnull()]","a3acfc18":"train_df.boxplot(column =\"Fare\",by =\"Embarked\")\nplt.show()","1bd355ba":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"C\")\ntrain_df[train_df[\"Embarked\"].isnull()]","500ec09c":"train_df[train_df[\"Fare\"].isnull()]","b5894059":"np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"])","2693a77c":"train_df[train_df[\"Pclass\"] == 3][\"Fare\"]","df99a1d1":"train_df[\"Fare\"]= train_df[\"Fare\"].fillna(np.mean(train_df[train_df[\"Pclass\"] == 3][\"Fare\"]))","e0e9ba8f":"train_df[train_df[\"Fare\"].isnull()]","27435d66":"list1 = [\"SibSp\",\"Parch\",\"Age\",\"Fare\", \"Survived\"]\nsns.heatmap(train_df[list1].corr(), annot= True, fmt= \".2f\") # if false we dont see number.\nplt.show()","1ff8b573":"g = sns.factorplot( x= \"SibSp\", y= \"Survived\", data = train_df, kind = \"bar\", size=6)\ng.set_ylabels(\"Survived Probability\")\nplt.show","f31c907c":"g= sns.factorplot(x=\"Parch\", y= \"Survived\", kind= \"bar\", data=train_df, size=6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()\n\n#black lines are mid of ratio its changable","4cadf42e":"g= sns.factorplot(x= \"Pclass\", y=\"Survived\", data = train_df, kind=\"bar\", size=6)\ng.set_ylabels=(\"Survived Probability\")\nplt.show()","f752a52d":"g= sns.FacetGrid(train_df, col=\"Survived\")\ng.map(sns.distplot, \"Age\", bins=25)\nplt.show()","98f88c25":"g = sns.FacetGrid(train_df, col=\"Survived\", row=\"Pclass\", size=2)\ng.map(plt.hist, \"Age\", bins=25)\ng.add_legend()\nplt.show()","c6b2f9af":"g = sns.FacetGrid(train_df, row= \"Embarked\",size=2)\ng.map(sns.pointplot, \"Pclass\", \"Survived\", \"Sex\")\ng.add_legend()\nplt.show()","7a4f0337":"g = sns.FacetGrid(train_df, row = \"Embarked\", col= \"Survived\", size=2.3)\ng.map(sns.barplot, \"Sex\", \"Fare\")\ng.add_legend()\nplt.show()","3efcd978":"train_df[train_df[\"Age\"].isnull()] # two train_df dont show me false values. Do not forget that. You can show just true what we need (isnull= mean ) we need totally 256\n","7907294f":"sns.factorplot(x= \"Sex\", y= \"Age\", data=train_df, kind=\"box\")\nplt.show()","d30ff77e":"sns.factorplot(x= \"Sex\", y= \"Age\", hue= \"Pclass\", data=train_df, kind=\"box\")\nplt.show()","b1fcbf21":"sns.factorplot(x= \"Parch\", y= \"Age\", data=train_df, kind=\"box\")\nsns.factorplot(x= \"SibSp\", y= \"Age\", data=train_df, kind=\"box\")\nplt.show()","79ac31e8":"train_df[\"Sex\"] = [1 if i == \"male\" else 0 for i in train_df[\"Sex\"]] # sex is object(str) male and female. We change 1 and 0 like survive rate","38926ca7":"sns.heatmap(train_df[[\"Age\", \"Sex\", \"SibSp\", \"Parch\", \"Pclass\"]].corr(), annot=True)\nplt.show()","15466e4c":"index_nan_age = list(train_df[\"Age\"][train_df[\"Age\"].isnull()].index)\nfor i in index_nan_age:\n    age_pred = train_df[\"Age\"][((train_df[\"SibSp\"]== train_df.iloc[i][\"SibSp\"]) & (train_df[\"Parch\"]== train_df.iloc[i][\"Parch\"]) & (train_df[\"Pclass\"]== train_df.iloc[i][\"Pclass\"]))].median()\n    age_med = train_df[\"Age\"].median()\n    if not np.isnan(age_pred):\n        train_df[\"Age\"].iloc[i]= age_pred\n    else:\n        train_df[\"Age\"].iloc[i]= age_med  \n        \n# Burada anlad\u0131\u011f\u0131m; Age in isnull k\u0131s\u0131mlar\u0131n\u0131 gez, \u00f6rne\u011fin Age'in 500. indexi bo\u015f. O de\u011fere ait; SibSp, ParCh,Pclass lar\u0131n\u0131n 500. indexindeki de\u011ferlerin hepsini bul. \n# Bu de\u011ferlere e\u015fit indexleri bul ve age lerini kaydet. Bunlar\u0131nda median lar\u0131n\u0131 al. Demek","80c58ad2":"train_df[\"Name\"].head(10)","8f3a20fd":"name = train_df[\"Name\"]\ntrain_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]\n# this method is seperate one name to two parts. Because we can see Mr. and Mrs. Master. Etc. so we can predict title and survive corr.\n","03307be3":"#another step is Johnson, Mrs. we choose 0. index. But we need just title. Thats why we split , and we take last index (title) I merge two methods in one...\ntrain_df[\"Title\"].head(10)","4efb2146":"train_df[\"Title\"].head(10)","856e85aa":"# I want to see all title one visualize tool\nsns.countplot(x=\"Title\", data=train_df)\nplt.xticks(rotation =45)\nplt.show()","1276dc1b":"# convert to categorical\ntrain_df[\"Title\"] = train_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\"],\"other\")\ntrain_df[\"Title\"] = [0 if i == \"Master\" else 1 if i == \"Miss\" or i == \"Ms\" or i == \"Mlle\" or i == \"Mrs\" else 2 if i == \"Mr\" else 3 for i in train_df[\"Title\"]]\ntrain_df[\"Title\"].head(20)","7e1914b9":"sns.countplot(x=\"Title\", data = train_df)\nplt.xticks(rotation = 45)\nplt.show()","5b8c30af":"g = sns.factorplot(x = \"Title\", y= \"Survived\", data=train_df, kind= \"bar\")\ng.set_xticklabels([\"Master\",\"Mrs\",\"Mr\",\"Other\"])\ng.set_ylabels(\"Survival Probability\")\nplt.show()","85634fd1":"train_df.drop(labels = [\"Name\"], axis = 1, inplace = True)","c6d06172":"train_df.head()","950ad919":"train_df = pd.get_dummies(train_df,columns =[\"Title\"])\ntrain_df.head()","fcab6825":"train_df[\"Fsize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1","43ae76c8":"train_df.head()","99769839":"g = sns.factorplot( x= \"Fsize\", y= \"Survived\", data= train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","828711f1":"#We decided threshold 5.\ntrain_df[\"family_size\"] = [1 if i<5 else 0 for i in train_df[\"Fsize\"]]","7450543a":"sns.countplot(x= \"family_size\", data=train_df)\nplt.show()","0c98152f":"g = sns.factorplot(x= \"family_size\", y= \"Survived\", data= train_df, kind = \"bar\")\ng.set_ylabels(\"Survival\")\nplt.show()","e78910f1":"train_df = pd.get_dummies(train_df, columns = [\"family_size\"])\ntrain_df.head()","e42f8e25":"train_df[\"Embarked\"].head()","1782b681":"sns.countplot(x=\"Embarked\", data= train_df)\nplt.show()","12853f9d":"train_df = pd.get_dummies(train_df, columns = [\"Embarked\"])\ntrain_df.head()","410e4e72":"train_df[\"Ticket\"].head(15)","1a89c613":"a = \"A\/5. 2151\"\na.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0]","e3e50fb5":"tickets = []\nfor i in list(train_df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ntrain_df[\"Ticket\"] = tickets","b63afdfa":"train_df[\"Ticket\"].head(20)","d5f1d99b":"train_df.head()","ac7b789a":"train_df = pd.get_dummies(train_df, columns= [\"Ticket\"], prefix = \"T\")\ntrain_df.head(10)\n","1eefa065":"sns.countplot(x= \"Pclass\", data= train_df)\nplt.show()","9c45d642":"train_df[\"Pclass\"] = train_df[\"Pclass\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns= [\"Pclass\"])\ntrain_df.head()","e47864c8":"train_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\ntrain_df = pd.get_dummies(train_df, columns=[\"Sex\"])\ntrain_df.head()","a0fc8ec2":"train_df.drop(labels = [\"PassengerId\", \"Cabin\"], axis = 1, inplace = True)","ff20a3ed":"train_df.columns","73f1e587":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","c6cee2cc":"train_df_len","a9dc72f9":"test = train_df[train_df_len:]\ntest.drop(labels = [\"Survived\"],axis = 1, inplace = True)","45affbdd":"train = train_df[:train_df_len]\nX_train = train.drop(labels = \"Survived\", axis = 1)\ny_train = train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.33, random_state = 42)\nprint(\"X_train\",len(X_train))\nprint(\"X_test\",len(X_test))\nprint(\"y_train\",len(y_train))\nprint(\"y_test\",len(y_test))\nprint(\"test\",len(test))","56403f7e":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nacc_log_train = round(logreg.score(X_train, y_train)*100,2) \nacc_log_test = round(logreg.score(X_test,y_test)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_test))","1aad6c12":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,500,20),\n                \"max_depth\": range(1,20,2)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [0.001, 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,200,300,1000]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid]","a6e5cb18":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","e8c0082b":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\"]})\n#We took results in cv_result parameters. We must add ML models with step by step if we mixed some values name, we didnt understand or we didnt predict logical\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","b08b28cf":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\nvotingC = votingC.fit(X_train, y_train)\nprint(accuracy_score(votingC.predict(X_test),y_test))","ef46bca8":"test_survived = pd.Series(votingC.predict(test), name = \"Survived\").astype(int)\nresults = pd.concat([test_PassengerId, test_survived],axis = 1)\nresults.to_csv(\"titanic.csv\", index = False)\n","019f5349":"<a id = \"24\"> <\/a><br>\n## Embarked","c6a769e2":"<a id = \"18\"> <\/a><br>\n## Embarked -- Sex -- Pclass -- Survived","3c3b4668":"<a id = \"27\"> <\/a><br>\n## Sex","381aee27":"Fare feature seems to have correlation with survived feature \"0.26\" . ","51a276fc":"* Pclass is important feature for model training.","9295e26e":"<a id = \"11\"> <\/a><br>\n# Visualization","d3b2f7c0":"<a id = \"20\"> <\/a><br>\n## Fill Missing: Age Feature","80c0981f":"Age is not coreelated with sex but it is correlated with Parch, SibSp and Pclass","c4ea9858":"<a id = \"28\"> <\/a><br>\n## Drop Passenger ID and Cabin","d532b34b":"* SibSp and ParCh can be used for new feature extraction with th= 3 trashold\n* Small families have more chance to survive.\n* There is std in survival of passenger with Parch =3","97aebb74":"<a id = \"14\"> <\/a><br>\n## Parch -- Survived","53dec500":"Small families have more chance than big families","bc21c1cb":"<a id = \"12\"> <\/a><br>\n## Correlation Between SibSp -- Parch -- Age -- Fare -- Survived","a3b5547c":"<a id = \"33\"><\/a><br>\n## Ensemble Modeling","48f76e2c":"<a id = \"22\"> <\/a><br>\n## Name -- Title","5a712616":"* Female passengers have much better survival rate than males.\n* Males have better survival rates in pclass 3 in C\n* Embarked and sex will be used in training.","b382fbe8":"<a id = \"10\"> <\/a><br>\n\n## Fill Missing Value\n* Embarked has 2 missing value\n* Fare has only 1","d66bf5c7":"First class passengers are older than 2nd class and 2nd is older than 3rd class(james is 3th clas)\n","027e3c0e":"<a id = \"17\"> <\/a><br>\n## Pclass -- Survived -- Age","bbfcdb2c":"<a id = \"2\"> <\/a><br>\n# Veriable Describtion\n1. PassengerId: Unique ID number to each passenger.\n1. Survived: Passenger survive(1) or died(0)\n1. Pclass: Passenger Class\n1. Name\n1. Sex: Gender of Passenger\n1. Age\n1. SibSp: Number of siblings\/spouses\n1. Parch: Number of parents\/children\n1. Ticket:Ticket number\n1. Fare: Amount of money s\n1. Cabin: Cabin Category\n1. Embarked: Port where passenger embarked ( C= Cherbourg, Q= Queenstown, S= Southampton","198673d2":"Sex is not informative for age prediction, age distribution seems to be same.**","a76b58dc":"<a id = \"3\"> <\/a><br>\n# Univariate Variable Analysis\n\n* Categorical Veriable: Survived, Sex, Pclass, Embarked, Cabin, Name, Ticket, Sibsp and Parch\n* Numerical Veriable:Fare, Age and PassengerId","ba749a82":"<a id = \"16\"> <\/a><br>\n## Age -- Survived","b23163dc":"## Train - Test Split","77216153":"<a id = \"15\"> <\/a><br>\n## Pclass -- Survived","8bbe555e":"<a id = \"5\"> <\/a><br>\n## Numerical Veriable","84d357ca":"* Passengers who pay higher fare have better survival. Fare can be used as a categorical for training\n  ","eae1ce6d":"<a id = \"6\"> <\/a><br>\n\n# Basic Data Analysis\n* Pclass - Survived\n* Sex - Survived\n* Sibsb - Survived\n* Parch - Survived","067869b3":"<a id = \"8\"> <\/a><br>\n# Missing Value\n* Find Missing Value\n* Fill Missing Value","9f887e7f":"# Modeling","2e7d0b16":"## Simple Logistic Regression","db21bb8b":"<a id = \"9\"> <\/a><br>\n\n## Find Missing Value","9b7a96d5":"<a id = \"19\"> <\/a><br>\n## Embarked -- Sex -- Fare -- Survived","c5d6a9d2":"![](http:\/\/)\n# Introduction\n \n The sinking of Titanic is one of the most bad famous shipwrecks in the history. In 1912, during her voyage, the titanic sank after colliding with an iceberg.Killing 1502 out of 2224 passengers and crew\n\n<font color = \"blue\">\n    \n Contenct:\n \n 1. [Load and check Data](#1)\n 1. [Veriable Describtion](#2)\n     * [Univariate Veriable Analysis](#3)\n          * [Categorical Veriable ](#4) \n          * [Numerical Veriable ****](#5) \n 1. [Basic Data Analysis](#6)\n 1. [Outlier Detection](#7)\n 1. [Missing Value](#8)\n     * [Find Missing Value](#9)\n     * [Fill Missing Value](#10)\n 1. [Visualization](#11)\n     * [Correlation Between Sibsp -- Parch -- Age -- Fare -- Survived](#12)\n     * [SibSp -- Survived](#13)\n     * [Parch -- Survived](#14)\n     * [Pclass -- Survived](#15)\n     * [Age -- Survived](#16)\n     * [Pclass -- Survived -- Age](#17)\n     * [Embarked -- Sex -- Pclass -- Survived](#18)\n     * [Embarked -- Sex -- Fare -- Survived](#19)\n     * [Fill Missing: Age Feature](#20)\n 1. [Feature Engineering](#21)\n     * [Name -- Title](#22)\n     * [Family Size](#23)\n     * [Embarked](#24)\n     * [Ticket](#25)\n     * [Pclass](#26)\n     * [Sex](#27)\n     * [Drop Passenger ID and Cabin](#28)\n     \n 1. [Modeling](#29)\n     * [Train - Test Split](#30)\n     * [Simple Logistic Regression](#31)\n     * [Hyperparameter Tuning -- Grid Search -- Cross Validation](#32) \n     * [Ensemble Modeling](#33)\n     * [Prediction and Submission](#34)\n     ","61f9c663":"* Having a lot od SibSp have less chance to survive\n* If SibSp == 0 or 1 or 2, passenger has more chance to survive\n* We consider a new feature describing these categories.","f7994000":"<a id = \"7\"> <\/a><br>\n# Outlier Detection - Ayk\u0131r\u0131","6324d12b":"<a id = \"23\"> <\/a><br>\n## Family Size","324ced01":"<a id = \"1\"> <\/a><br>\n\n# Load and Check Data","0c0f95df":"<a id = \"21\"> <\/a><br>\n# Feature Engineering\n","ac1aeda5":"<a id = \"34\"><\/a><br>\n## Prediction and Submission\n","14b99399":"* float64(2): Fare and Age\n* int64(5): Pclass, sibsp, parch, passengerId and survived\n* object(5): Name,sex, Cabin, embarked,ticket","14431410":"<a id = \"25\"> <\/a><br>\n## Ticket","01da65cb":" * Age <= 10 has a high survival rate\n * Oldest passenger (80) survived.\n * Large number of 20 years old did not survive.\n * Most passengers are in 15-35 age range, \n * Use age feature in training\n * Use age distrubition for missing values of ages.","a4afd162":"<a id = \"4\"> <\/a><br>\n## Categorical Veriable","ff3cb85c":"<a id = \"13\"> <\/a><br>\n## SibSp -- Survived","8cdd97d7":"<a id = \"26\"> <\/a><br>\n## Pclass"}}