{"cell_type":{"bb844340":"code","a1f6852e":"code","dd6b18b5":"code","ed916723":"code","8272aa32":"code","8816aa6b":"code","cef8931c":"code","da0591dc":"code","3558373b":"code","22a422e2":"code","fc7d6e6d":"code","86dd9dbd":"code","6fd84fac":"code","58dc18a1":"code","6e2d35df":"code","6a55ba86":"code","87280084":"code","a57f193a":"code","a796a6f4":"code","bc91b09b":"code","9f45c0c8":"code","805484b8":"code","0e8903d0":"code","16022531":"code","0ab2c039":"code","b8b081b7":"code","9672dbc8":"code","c6a5a1db":"code","31e06f74":"code","dcbf0177":"code","ea3286fe":"code","f95bc9ff":"code","df4ad0de":"code","86efeefa":"code","de367ed8":"code","8383b100":"code","d2f2b55b":"code","cd05f4e2":"code","6c364931":"code","f8592d8c":"code","05fb7c03":"code","a36e26b8":"code","ca6af370":"code","9d19dec7":"code","867eebce":"code","7e142161":"code","61f2124b":"markdown","5c598a82":"markdown","65b4320a":"markdown","c3ec207b":"markdown","faa80ed7":"markdown","4bf1a353":"markdown","570112e8":"markdown","b01bcbd0":"markdown","6b0cab54":"markdown","352ca10f":"markdown","3cd941e3":"markdown","9d152458":"markdown","ee5d05c3":"markdown","56865b7a":"markdown","af5c7eec":"markdown","3ce7d91a":"markdown","0dda777d":"markdown","102cdd53":"markdown","bdcc2175":"markdown","eb88dff9":"markdown","dd8ee76b":"markdown","9b504be5":"markdown","f3047b91":"markdown","e5a257f1":"markdown","a4c86afe":"markdown","da80f12d":"markdown","12b38f96":"markdown"},"source":{"bb844340":"#Importing the Required Python Packages\nimport shutil\nimport string\nimport numpy as np\nimport pandas as pd\nimport pickle\nfrom tqdm import tqdm\nimport ast\nfrom sklearn import utils\nfrom gensim.models import Doc2Vec\nfrom gensim.models.doc2vec import TaggedDocument\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\npd.set_option('display.max_colwidth', None)","a1f6852e":"# Lets load the train dataset.\ntrain = pd.read_csv('Train\/train_final.csv')\ntrain.head(3)","dd6b18b5":"# Lets load validation and test datasets as well\nvalid = pd.read_csv('Validation\/valid_final.csv')\ntest = pd.read_csv('Test\/test_final.csv')","ed916723":"valid.head(3)","8272aa32":"test.head(3)","8816aa6b":"    abbrev = list(train['ABV'].unique())\n    valid = valid[valid['ABV'].isin(abbrev)]\n    test = test[test['ABV'].isin(abbrev)]\n    labels = list(train['LABEL'].unique())\n    valid = valid[valid['LABEL'].isin(labels)]\n    test = test[test['LABEL'].isin(labels)]\n","cef8931c":"train.drop(columns='ABV', inplace = True)\nvalid.drop(columns='ABV', inplace = True)\ntest.drop(columns='ABV', inplace = True)","da0591dc":"# Convert TOKEN column from string to list\ntrain['TOKEN'] = train['TOKEN'].apply(lambda x: ast.literal_eval(x))\nvalid['TOKEN'] = valid['TOKEN'].apply(lambda x: ast.literal_eval(x))\ntest['TOKEN'] = test['TOKEN'].apply(lambda x: ast.literal_eval(x))","3558373b":"train_tagged = train.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\nvalid_tagged = valid.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)\ntest_tagged = test.apply(lambda x: TaggedDocument(words = x['TOKEN'], tags = [x['LABEL']]), axis=1)","22a422e2":"train_tagged.values[:5]","fc7d6e6d":"vectorize = Doc2Vec(dm=0, vector_size=100, min_count=2, window = 2)\nvectorize.build_vocab(train_tagged.values)","86dd9dbd":"vectorize.train(train_tagged.values, total_examples=len(train_tagged.values), epochs=30)","6fd84fac":"def vec_for_learning(model, tagged_docs):\n    sents = tagged_docs.values\n    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=30)) for doc in sents])\n    return targets, regressors","58dc18a1":"y_train, X_train = vec_for_learning(vectorize, train_tagged)","6e2d35df":"param_grid = {'C':[0.001, 0.01, 0.1, 1, 10, 100]}\ngrid_model = GridSearchCV(LogisticRegression(n_jobs=-1), param_grid)","6a55ba86":"grid_model.fit(X_train, y_train)","87280084":"### Best parameters for the Grid Search\ngrid_model.best_params_","a57f193a":"### Accuracy Score\ngrid_model.best_score_","a796a6f4":"logreg = LogisticRegression(n_jobs=-1, C=1)\nlogreg.fit(X_train, y_train)","bc91b09b":"### Apply the above Model on Validation Set\ny_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\ny_pred_valid = logreg.predict(X_valid)","9f45c0c8":"print('Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\nprint('Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","805484b8":"### Apply the above Model on Test Set\ny_test, X_test = vec_for_learning(vectorize, test_tagged)\ny_pred_test = logreg.predict(X_test)","0e8903d0":"accuracy = accuracy_score(y_test, y_pred_test)\nf1_scr = f1_score(y_test, y_pred_test, average='weighted')\nprint('Test Accuracy:', accuracy)\nprint('Test F1-Score:', f1_scr)","16022531":"y_unique = list(set(y_test))\ncr = classification_report(y_test, y_pred_test, target_names=y_unique)\nprint(cr)","0ab2c039":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [1, 0.1, 0.01, 0.001, 0.0001], 'kernel': ['rbf']}\ngrid_svm = GridSearchCV(SVC(), param_grid)","b8b081b7":"grid_svm.fit(X_train, y_train)","9672dbc8":"### Best parameters for the Grid Search\ngrid_svm.best_params_","c6a5a1db":"### Accuracy Score\ngrid_svm.best_score_","31e06f74":"svcModel = SVC(C=10, gamma=0.01, kernel='rbf')\nsvcModel.fit(X_train, y_train)","dcbf0177":"### Apply the above Model on Validation Set\ny_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\ny_pred_valid = svcModel.predict(X_valid)","ea3286fe":"print('SVM Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\nprint('SVM Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","f95bc9ff":"### Apply the above Model on Test Set\ny_test, X_test = vec_for_learning(vectorize, test_tagged)\ny_pred_test = svcModel.predict(X_test)","df4ad0de":"accuracy = accuracy_score(y_test, y_pred_test)\nf1_scr = f1_score(y_test, y_pred_test, average='weighted')\nprint('SVM Test Accuracy:', accuracy)\nprint('SVM Test F1-Score:', f1_scr)","86efeefa":"y_unique = list(set(y_test))\ncr = classification_report(y_test, y_pred_test, target_names=y_unique)\nprint(cr)","de367ed8":"param_grid = {'n_estimators':[100, 500, 1000], 'max_depth':[5, 6, 7], 'min_child_weight': [3, 5, 8]}","8383b100":"unique = list(set(y_train))\nX_train = pd.DataFrame(X_train)\ny_train = np.asarray(y_train)","d2f2b55b":"XGBgrid = GridSearchCV(XGBClassifier(learning_rate= 0.1, gamma= 0, objective= 'multi:softmax', num_classes= len(unique), seed= 27), param_grid)","cd05f4e2":"XGBgrid.fit(X_train, y_train)","6c364931":"### Best parameters for the Grid Search\nXGBgrid.best_params_","f8592d8c":"### Accuracy Score\ngrid_svm.best_score_","05fb7c03":"XGBModel = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=7,\n min_child_weight=4,\n gamma=0,\n objective= 'multi:softmax',\n seed=27)   \nXGBModel.fit(X_train, y_train)","a36e26b8":"### Apply the above Model on Validation Set\ny_valid, X_valid = vec_for_learning(vectorize, valid_tagged)\nX_valid = pd.DataFrame(X_valid)\ny_valid = np.asarray(y_valid)\ny_pred_valid = XGBModel.predict(X_valid)","ca6af370":"print('XGBoost Validation Accuracy:', accuracy_score(y_valid, y_pred_valid))\nprint('XGBoost Validation F1-Score:', f1_score(y_valid, y_pred_valid, average='weighted'))","9d19dec7":"### Apply the above Model on Test Set\ny_test, X_test = vec_for_learning(vectorize, test_tagged)\nX_test = pd.DataFrame(X_test)\ny_test = np.asarray(y_test)\ny_pred_test = XGBModel.predict(X_test)","867eebce":"accuracy = accuracy_score(y_test, y_pred_test)\nf1_scr = f1_score(y_test, y_pred_test, average='weighted')\nprint('XGBoost Test Accuracy:', accuracy)\nprint('XGBoost Test F1-Score:', f1_scr)","7e142161":"y_unique = list(set(y_test))\ncr = classification_report(y_test, y_pred_test, target_names=y_unique)\nprint(cr)","61f2124b":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.71\n2. Average Recall: 0.71","5c598a82":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.69\n2. Average Recall: 0.70","65b4320a":"### Lets calculate some Performance Metrics on the Test predictions.","c3ec207b":"# Abbreviation Disambiguation in Medical Texts - Data Modeling\n\nThis Notebook is in continuation of the notebook- 'Step 2- Data Preprocessing' and lists down:\n\n1. Modeling Preprocessed data using: GridSearchCV on Logistic Regression, SVM and XG Boost.\n2. Testing the models using Test set.\n3. Comparing the models and identifying the Next Steps","faa80ed7":"## Step# 2: Apply Doc2vec vectorizer on the Dataset","4bf1a353":"### Lets keep only relevant records in Valid and test set.","570112e8":"## We can see that a basic Logistic Classification implementation gives 70% Accurate results hence, for next steps we can:\n1. Try tuning the Doc2Vec vectorizer's Hyperparameters.\n2. Try some other Classification Algorithms like SVN, Random Forrest and compare results.\n3. Present model has been trained to disambiguate 20 'Medical Abbreviations' but this same model can be generalized to be used in other fields as well. Some including Scientific Researches and Internet Slags.","b01bcbd0":"## Model# 2: SVM","6b0cab54":"### Lets tag every Token List with its Label","352ca10f":"### As per the above analysis of validation set, it can be seen that the Logistic Classification model gives\n1. F1- Score of: 0.68\n2. Accuracy of: 69%\n\nHence, lets apply this model to our Test set and check its performance metrics.","3cd941e3":"### Lets perform a Grid Search to get the best possible combination of Hyperparameters for Logistic Regression Model","9d152458":"### Train a XGBoost Classifier","ee5d05c3":"### Apply the best parameters to Logistic Regression and train the model.","56865b7a":"## Model# 3: XGBoost","af5c7eec":"# Next Steps","3ce7d91a":"### Lets calculate some Performance Metrics on the Test predictions.","0dda777d":"### Lets drop 'ABV' column","102cdd53":"### As per the above analysis of validation set, it can be seen that the SVC Classification model gives\n1. F1- Score of: 0.71\n2. Accuracy of: 70%\n\nHence, lets apply this model to our Test set and check its performance metrics.","bdcc2175":"### Lets perform a Grid Search to get the best possible combination of Hyperparameters for SVM's","eb88dff9":"### Lets calculate some Performance Metrics on the Test predictions.","dd8ee76b":"## Step# 1: Loading Dataset","9b504be5":"### Thus, from the above Report it can be seen:\n1. Average Precision: 0.59\n2. Average Recall: 0.59","f3047b91":"### As per the above analysis of validation set, it can be seen that the XGBoost Classification model gives\n1. F1- Score of: 0.58\n2. Accuracy of: 59.1%\n\nHence, lets apply this model to our Test set and check its performance metrics.","e5a257f1":"### Lets create a parameter grid for XGBoost Model","a4c86afe":"### Apply the best parameters to SVC and train the model","da80f12d":"## Model# 1: Logistic Classifier","12b38f96":"### Building the Final Vector Feature Classifier"}}