{"cell_type":{"86f87f6c":"code","ab3681ed":"code","150e3f1a":"code","8a97c8e5":"code","971db664":"code","40e551bc":"code","19a54679":"code","d0b6a00e":"code","1378dd9a":"code","0a4bc034":"code","4ce937e0":"code","3ade461f":"code","53d3d9a4":"code","a7fcf571":"code","e4d00ba3":"code","c0c7f1a0":"code","354032c3":"code","c04db344":"code","cf7be75a":"markdown","f6497144":"markdown","55a2e7f1":"markdown","48c50f88":"markdown","7e22095f":"markdown","ba26f1da":"markdown","8d742b42":"markdown","0f8dc96f":"markdown","942f8808":"markdown","9917e50b":"markdown","62dcbe5d":"markdown","26971e76":"markdown"},"source":{"86f87f6c":"!pip install 'kaggle-environments>=0.1.6' > \/dev\/null\n!pip install git+https:\/\/github.com\/openai\/baselines > \/dev\/null","ab3681ed":"import numpy as np\nimport base64\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch import optim\n\nimport gym\nfrom gym import spaces\nfrom gym.spaces.box import Box\n\nfrom baselines.common.vec_env.subproc_vec_env import SubprocVecEnv\n\nfrom kaggle_environments import evaluate, make\nfrom kaggle_environments.envs.connectx.connectx import is_win","150e3f1a":"NUM_PROCESSES = 16\nNUM_ADVANCED_STEP = 5\nGAMMA = 0.99\n\nTOTAL_MOVES = 3e5\nNUM_UPDATES = int(TOTAL_MOVES \/ NUM_ADVANCED_STEP \/ NUM_PROCESSES)","8a97c8e5":"# A2C loss\nvalue_loss_coef = 0.5\nentropy_coef = 0.01\nmax_grad_norm = 0.5\n\n# RMSprop\nlr = 7e-4\neps = 1e-5\nalpha = 0.99","971db664":"class ConnectX(gym.Env):\n    def __init__(self, switch_prob=0.5, opponent='random'):\n        self.env = make('connectx')\n        self.pair = [None, opponent]\n        self.trainer = self.env.train(self.pair)\n        self.switch_prob = switch_prob\n        \n        self.rows = self.env.configuration.rows\n        self.columns = self.env.configuration.columns\n        self.action_space = spaces.Discrete(self.columns)\n        self.observation_space = spaces.Box(low=0, high=1,\n                                            shape=(3, self.rows, self.columns), dtype=np.uint8)\n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n    \n    def observation(self, observation):\n        obs = observation.board\n        if observation.mark == 2:\n            obs = [3 - x if x != 0 else 0 for x in obs]\n        \n        obs = np.array(obs).reshape(self.rows, self.columns)\n        obs = np.eye(3)[obs].transpose(2, 0, 1)\n        return obs\n\n    def step(self, action):\n        obs, reward, done, info = self.trainer.step(int(action))\n        \n        if reward == 1: # Won\n            reward = 1\n        elif reward == 0: # Lost\n            reward = -1\n        else:\n            reward = 0\n            \n        return self.observation(obs), reward, done, info\n    \n    def reset(self):\n        if np.random.random() < self.switch_prob:\n            self.switch_trainer()\n        obs = self.trainer.reset()\n        return self.observation(obs)","40e551bc":"def weighted_random(obs, config):\n    from kaggle_environments.envs.connectx.connectx import is_win\n    from random import choices\n    from scipy.stats import norm\n    \n    columns = [c for c in range(config.columns) if obs.board[c] == 0]\n    for mark in [obs.mark, 3 - obs.mark]:\n        for column in columns:\n            if is_win(obs.board, column, mark, config, False):\n                return column\n\n    return choices(columns, weights=norm.pdf(columns, 3, 1))[0]","19a54679":"def make_env():\n    def _thunk():\n        env = ConnectX(opponent=weighted_random)\n        return env\n\n    return _thunk","d0b6a00e":"class RolloutStorage(object):\n    def __init__(self, num_steps, num_processes, obs_shape):\n        self.observations = torch.zeros(\n            num_steps + 1, num_processes, *obs_shape).cuda()\n        self.masks = torch.ones(num_steps + 1, num_processes, 1).cuda()\n        self.rewards = torch.zeros(num_steps, num_processes, 1).cuda()\n        self.actions = torch.zeros(\n            num_steps, num_processes, 1).long().cuda()\n\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1).cuda()\n        self.index = 0\n\n    def insert(self, current_obs, action, reward, mask):\n        self.observations[self.index + 1].copy_(current_obs)\n        self.masks[self.index + 1].copy_(mask)\n        self.rewards[self.index].copy_(reward)\n        self.actions[self.index].copy_(action)\n\n        self.index = (self.index + 1) % NUM_ADVANCED_STEP\n\n    def after_update(self):\n        self.observations[0].copy_(self.observations[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value):\n        self.returns[-1] = next_value\n        for ad_step in reversed(range(self.rewards.size(0))):\n            self.returns[ad_step] = self.returns[ad_step + 1] * \\\n                GAMMA * self.masks[ad_step + 1] + self.rewards[ad_step]","1378dd9a":"def init(module, gain):\n    nn.init.orthogonal_(module.weight.data, gain=gain)\n    nn.init.constant_(module.bias.data, 0)\n    return module\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass Net(nn.Module):\n    def __init__(self, n_out):\n        super(Net, self).__init__()\n\n        def init_(module): return init(\n            module, gain=nn.init.calculate_gain('relu'))\n\n        self.conv = nn.Sequential(\n            init_(nn.Conv2d(3, 24, kernel_size=3, padding=1)),\n            nn.ReLU(),\n            init_(nn.Conv2d(24, 48, kernel_size=3)),\n            nn.ReLU(),\n            init_(nn.Conv2d(48, 48, kernel_size=3)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(48 * 2 * 3, 80)),\n            nn.ReLU()\n        )\n\n        def init_(module): return init(module, gain=1.0)\n        # Critic\n        self.critic = init_(nn.Linear(80, 1))\n\n        def init_(module): return init(module, gain=0.01)\n        # Actor\n        self.actor = init_(nn.Linear(80, n_out))\n\n        self.train()\n\n    def forward(self, x):\n        conv_output = self.conv(x)\n        critic_output = self.critic(conv_output)\n        actor_output = self.actor(conv_output)\n\n        return critic_output, actor_output\n\n    def act(self, x):\n        value, actor_output = self(x)  \n\n        for i in range(x.size(0)):\n            for j in range(x.size(-1)):\n                if x[i][0][0][j] != 1:\n                    actor_output[i][j] = -1e7\n        \n        probs = F.softmax(actor_output, dim=1)\n        action = probs.multinomial(num_samples=1)\n\n        return action\n\n    def get_value(self, x):\n        value, actor_output = self(x)\n        return value\n\n    def evaluate_actions(self, x, actions):\n        value, actor_output = self(x)\n        \n        log_probs = F.log_softmax(actor_output, dim=1)\n        action_log_probs = log_probs.gather(1, actions)\n\n        probs = F.softmax(actor_output, dim=1)\n        dist_entropy = -(log_probs * probs).sum(-1).mean()\n\n        return value, action_log_probs, dist_entropy","0a4bc034":"class Brain(object):\n    def __init__(self, actor_critic):\n        self.actor_critic = actor_critic\n        self.optimizer = optim.RMSprop(\n            actor_critic.parameters(), lr=lr, eps=eps, alpha=alpha)\n\n    def update(self, rollouts):\n        obs_shape = rollouts.observations.size()[2:]\n        num_steps = NUM_ADVANCED_STEP\n        num_processes = NUM_PROCESSES\n\n        values, action_log_probs, dist_entropy = self.actor_critic.evaluate_actions(\n            rollouts.observations[:-1].view(-1, *obs_shape),\n            rollouts.actions.view(-1, 1))\n\n        values = values.view(num_steps, num_processes, 1)\n        action_log_probs = action_log_probs.view(num_steps, num_processes, 1)\n\n        advantages = rollouts.returns[:-1] - values\n        value_loss = advantages.pow(2).mean()\n\n        action_gain = (advantages.detach() * action_log_probs).mean()\n\n        total_loss = (value_loss * value_loss_coef -\n                      action_gain - dist_entropy * entropy_coef)\n\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_grad_norm)\n        self.optimizer.step()","4ce937e0":"class Trainer(object):\n    def __init__(self, config):\n        self.config = config\n    \n    def checkmate(self, board, mark):    \n        columns = [c for c in range(self.config.columns) if board[c] == 0]\n        for mark in [mark, 3 - mark]:\n            for column in columns:\n                if is_win(board, column, mark, self.config, False):\n                    return column\n\n    def train(self):\n        seed_num = 1\n        torch.manual_seed(seed_num)\n        torch.cuda.manual_seed(seed_num)\n\n        torch.set_num_threads(seed_num)\n        envs = [make_env() for i in range(NUM_PROCESSES)]\n        envs = SubprocVecEnv(envs)\n\n        n_out = envs.action_space.n\n        actor_critic = Net(n_out).cuda()\n        global_brain = Brain(actor_critic)\n\n        obs_shape = envs.observation_space.shape\n        rollouts = RolloutStorage(\n            NUM_ADVANCED_STEP, NUM_PROCESSES, obs_shape)\n        episode_rewards = torch.zeros([NUM_PROCESSES, 1])\n        final_rewards = torch.zeros([NUM_PROCESSES, 1])\n\n        obs = envs.reset()\n        obs = torch.from_numpy(obs).float()\n        rollouts.observations[0].copy_(obs)\n        \n        complete_count = 0\n\n        for i in tqdm(range(NUM_UPDATES)):\n            for step in range(NUM_ADVANCED_STEP):\n                with torch.no_grad():\n                    action = actor_critic.act(rollouts.observations[step])\n\n                cpu_actions = action.squeeze(1).cpu().numpy()\n\n                for j in range(NUM_PROCESSES):\t\n                    board = rollouts.observations[step][j].cpu().numpy().argmax(axis=0).reshape(-1)\t\n                    forced_action = self.checkmate(board, 1)\n                    if forced_action is not None:\n                        cpu_actions[j] = forced_action\n\n                obs, reward, done, info = envs.step(cpu_actions)\n\n                reward = np.expand_dims(np.stack(reward), 1)\n                reward = torch.from_numpy(reward).float()\n                episode_rewards += reward\n\n                masks = torch.FloatTensor(\n                    [[0.0] if done_ else [1.0] for done_ in done])\n\n                final_rewards *= masks\n                final_rewards += (1 - masks) * episode_rewards\n\n                episode_rewards *= masks\n\n                masks = masks.cuda()\n\n                obs = torch.from_numpy(obs).float()\n                rollouts.insert(obs, action.data, reward, masks)\n\n            with torch.no_grad():\n                next_value = actor_critic.get_value(\n                    rollouts.observations[-1]).detach()\n\n            rollouts.compute_returns(next_value)\n\n            global_brain.update(rollouts)\n            rollouts.after_update()\n\n            if i % 125 == 0:\n                print(\"finished moves {}, mean\/median reward {:.2f}\/{:.2f}, min\/max reward {:.2f}\/{:.2f}\".\n                      format(i*NUM_PROCESSES*NUM_ADVANCED_STEP,\n                             final_rewards.mean(),\n                             final_rewards.median(),\n                             final_rewards.min(),\n                             final_rewards.max()))\n            \n            complete_count = complete_count + 1 if final_rewards.mean() >= 0.75 else 0\t            \t\n            if complete_count == 10:\t\n                print(\"finished training\")\t\n                break\n\n        torch.save(global_brain.actor_critic.state_dict(), 'weight.pth')","3ade461f":"config = make(\"connectx\").configuration\ntrainer = Trainer(config)\ntrainer.train()","53d3d9a4":"%%writefile submission.py\n\nimport numpy as np\nimport io\nimport base64\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom kaggle_environments.envs.connectx.connectx import is_win\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\nclass Net(nn.Module):\n    def __init__(self, n_out):\n        super(Net, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 24, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(24, 48, kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(48, 48, kernel_size=3),\n            nn.ReLU(),\n            Flatten(),\n            nn.Linear(48 * 2 * 3, 80),\n            nn.ReLU()\n        )\n        self.critic = nn.Linear(80, 1)\n        self.actor = nn.Linear(80, n_out)\n\n    def forward(self, x):\n        conv_output = self.conv(x)\n        critic_output = self.critic(conv_output)\n        actor_output = self.actor(conv_output)\n\n        return critic_output, actor_output\n\n    def act(self, x):\n        value, actor_output = self(x)  \n\n        for i in range(x.size(0)):\n            for j in range(x.size(-1)):\n                if x[i][0][0][j] != 1:\n                    actor_output[i][j] = -1e7\n        \n        probs = F.softmax(actor_output, dim=1)\n        action = probs.multinomial(num_samples=1)\n\n        return action","a7fcf571":"with open('weight.pth', 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes)\n\ntemplate = f\"\"\"\nactor_critic = Net({config.columns})\ndecoded = base64.b64decode({encoded_weights})\nbuffer = io.BytesIO(decoded)\nactor_critic.load_state_dict(torch.load(buffer, map_location='cpu'))\n\"\"\"\n\nwith open('submission.py', 'a') as f:\n    f.write(template)","e4d00ba3":"%%writefile -a submission.py\n\ndef my_agent(obs, config):    \n    board = obs.board\n    columns = [c for c in range(config.columns) if board[c] == 0]\n    for mark in [obs.mark, 3 - obs.mark]:\n        for column in columns:\n            if is_win(board, column, mark, config, False):\n                return column\n    \n    if obs.mark == 2:\n        board = [3 - x if x != 0 else 0 for x in board]\n    board = np.array(board).reshape(config.rows, config.columns)\n    board = np.eye(3)[board].transpose(2, 0, 1)\n    board = torch.from_numpy(board).view([1, 3, config.rows, config.columns]).float()\n    \n    with torch.no_grad():\n        action = actor_critic.act(board)\n    action = action.item()\n\n    return action","c0c7f1a0":"%run submission.py","354032c3":"env = make(\"connectx\", debug=True)\nenv.reset()\nenv.run([my_agent, weighted_random])\nenv.render(mode=\"ipython\", width=500, height=450)","c04db344":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Weighted Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, weighted_random], num_episodes=100)))\nprint(\"Weighted Random Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [weighted_random, my_agent], num_episodes=100)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))\nprint(\"Negamax Agent vs My Agent:\", mean_reward(evaluate(\"connectx\", [\"negamax\", my_agent], num_episodes=10)))","cf7be75a":"# Import Libraries","f6497144":"# Test the Agent","55a2e7f1":"# Set Parameters","48c50f88":"# Evaluate the Agent","7e22095f":"## Model","ba26f1da":"## ConnectX Environment","8d742b42":"## Memory","0f8dc96f":"# Training","942f8808":"## Opponent","9917e50b":"# Create an Agent and Write Submission File","62dcbe5d":"## Brain","26971e76":"# Define Classes and Functions"}}