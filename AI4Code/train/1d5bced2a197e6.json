{"cell_type":{"530d2940":"code","02291fb1":"code","eb3c5f20":"code","743ad904":"code","9e56497b":"code","0b9317f4":"code","19e36e74":"code","c399d930":"code","7273c389":"code","483293e5":"code","e9f0e89b":"code","797be0a9":"code","71b3ba65":"code","f98f72f4":"code","cca8fcab":"code","2b95ef31":"code","3182436c":"code","84e3a67d":"code","2300a81a":"code","72cd1508":"code","39f1bc1e":"code","72682321":"code","38b2b7cb":"code","dad3a274":"code","bad67d7b":"markdown","e2f1b3d2":"markdown","17a375ea":"markdown","59822dcb":"markdown","0e314eeb":"markdown","d0ed39ba":"markdown","8252caa2":"markdown","31aefd7b":"markdown","3e45aa33":"markdown","ff6983b9":"markdown","55ddcd24":"markdown","03d898b3":"markdown","bd758b7a":"markdown","8e0c5dc8":"markdown","804491cd":"markdown","f227c9f2":"markdown","73c39f71":"markdown","79b36425":"markdown","6baf7509":"markdown","64ee4fc9":"markdown","3240e80a":"markdown","e603b1c6":"markdown","58ba54f3":"markdown","0e7a946d":"markdown","f881ed07":"markdown","02c7a630":"markdown","95b3bbe3":"markdown","fa5fcd26":"markdown"},"source":{"530d2940":"import numpy\n\nx = numpy.array([1, 2, 3, 4, 5])\ny = numpy.array([4, 6, 7, 9, 10])","02291fb1":"import matplotlib.pyplot as pyplot\n%matplotlib inline\n\nfig1 = pyplot.figure()\naxes1 = pyplot.axes(title='Vizualization of the data')\nscatter1 = axes1.scatter(x, y)","eb3c5f20":"def Predict(x, w, b):\n    return w * x + b ","743ad904":"w = 0\nb = 0","9e56497b":"yp = Predict(x, w, b)\naxes1.plot(x, yp, color='red')\nfig1","0b9317f4":"def Loss(x, y, w, b):\n    yp = Predict(x, w, b)\n    J = (yp - y)**2      \n    loss = numpy.average(J)\n    return loss    ","19e36e74":"ws, bs = numpy.meshgrid(numpy.linspace(0, 5, 20), numpy.linspace(0, 5, 20))\nws, bs = ws.ravel(), bs.ravel()\nyp = numpy.outer(ws, x) + bs.reshape(-1, 1)\nlosses = numpy.average((yp - y)**2, axis=1).ravel()","c399d930":"from plotly.offline import iplot\nimport plotly.graph_objs as go\n\ntrace0 = go.Mesh3d(x=ws, y=bs, z=losses, opacity=0.5)\nlayout = dict(scene=dict(xaxis=dict(title='w'), yaxis=dict(title='b'), zaxis=dict(title='loss')))\nfig2 = go.Figure(data=[trace0], layout=layout)\niplot(fig2)","7273c389":"idx = numpy.where(losses == numpy.amin(losses))\nw, b, loss = ws[idx][0], bs[idx][0], losses[idx][0]\nprint('w:', w, 'b:', b, 'loss:', loss)","483293e5":"trace1 = go.Scatter3d(x=(w,), y=(b,), z=(loss,), marker=dict(size=5, color='cyan'))\nfig3 = go.Figure(data=[trace0, trace1], layout=layout)\niplot(fig3)","e9f0e89b":"yp = Predict(x, w, b)\naxes1.plot(x, yp, color='cyan')\nfig1","797be0a9":"def Fit(x, y):\n    xavg = numpy.average(x)\n    yavg = numpy.average(y)\n\n    xyavg = numpy.average(x * y)\n    x2avg = numpy.average(x**2) \n\n    w = (xyavg - xavg * yavg) \/ (x2avg - xavg**2)\n    b = yavg - w * xavg\n\n    loss = Loss(x, y, w, b)\n    \n    return w, b, loss","71b3ba65":"w, b, loss = Fit(x, y)\nprint('w:', w, 'b:', b, 'loss:', loss)","f98f72f4":"trace2 = go.Scatter3d(x=(w,), y=(b,), z=(loss,), marker=dict(size=5, color='green'))\nfig4 = go.Figure(data=[trace0, trace1, trace2], layout=layout)\niplot(fig4)","cca8fcab":"yp = Predict(x, w, b)\naxes1.plot(x, yp, color='green')\nfig1","2b95ef31":"def Gradient(x, y, w, b):\n    yp = Predict(x, w, b)\n    dLdw = 2 * numpy.average((yp - y) * x)\n    dLdb = 2 * numpy.average(yp - y)    \n    return dLdw, dLdb","3182436c":"def GradientDescent(x, y, gradient, alpha, max_steps, goal):\n    w = 0\n    b = 0\n    loss = Loss(x, y, w, b)\n    ws=[w]; bs=[b]; losses=[loss]\n    for i in range(max_steps):\n        dLdw, dLdb = gradient(x, y, w, b)\n        w = w - alpha * dLdw\n        b = b - alpha * dLdb\n        loss = Loss(x, y, w, b)\n        ws.append(w); bs.append(b); losses.append(loss)\n        if loss < goal:\n            break\n    return ws, bs, losses","84e3a67d":"ws, bs, losses = GradientDescent(x, y, Gradient, alpha=0.01, max_steps=10000, goal=0.06)\nw, b, loss = ws[-1], bs[-1], losses[-1]\nprint('w:', w, 'b:', b, 'loss:', loss, 'steps:', len(losses)-1)","2300a81a":"trace3 = go.Scatter3d(x=ws, y=bs, z=losses, marker=dict(size=2, color='blue'))\nfig5 = go.Figure(data=[trace0, trace1, trace2, trace3], layout=layout)\niplot(fig5)","72cd1508":"yp = Predict(x, w, b)\naxes1.plot(x, yp, color='blue')\nfig1","39f1bc1e":"def NumGradient(x, y, w, b):\n    eps = 1E-12         \n    loss = Loss(x, y, w, b)\n    dLdw = (Loss(x, y, w + eps, b) - loss) \/ eps\n    dLdb = (Loss(x, y, w, b + eps) - loss) \/ eps\n    return dLdw, dLdb","72682321":"ws, bs, losses = GradientDescent(x, y, NumGradient, alpha=0.01, max_steps=10000, goal=0.06)\nw, b, loss = ws[-1], bs[-1], losses[-1]\nprint('w:', w, 'b:', b, 'loss:', loss, 'steps:', len(losses)-1)","38b2b7cb":"trace4 = go.Scatter3d(x=ws, y=bs, z=losses, marker=dict(size=2, color='purple'))\nfig6 = go.Figure(data=[trace0, trace1, trace2, trace3, trace4], layout=layout)\niplot(fig6)","dad3a274":"yp = Predict(x, w, b)\naxes1.plot(x, yp, color='purple')\nfig1","bad67d7b":"## Train the model\nWe need to train the model to better predict the data.  The goal of the training is to find the value of (w, b) that would minimize the prediction error.\n\n### Define a loss function\nThe optimum solution can be found directly by defining a \"**squared error**\" loss function and then minimizing that loss function.\n\n$L = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2 = \\frac{1}{m} \\sum_{i=1}^{m} (w x_i + b - y_i)^2$\n\n","e2f1b3d2":"Define a function that implements the gradient descent algorithm.","17a375ea":"The gradient can also be estimated numerically using $\\frac{\\delta L}{\\delta w} \\approx \\frac{L(w + \\epsilon, b) - L(w, b)}{\\epsilon}$ and $\\frac{\\delta L}{\\delta b} \\approx \\frac{L(w, b + \\epsilon) - L(w, b)}{\\epsilon}$  in $\\lim \\epsilon \\to 0$ .\n<br>\nDefine a function to estimate the gradient numerically.","59822dcb":"## Choose a model\nIt looks like the data is fairly linear, so let's use a linear model as the \"brain\" that will be trained to predict this data.\n\n$y = w x + b$","0e314eeb":"# Part I - Coding the ML ourselves ","d0ed39ba":"Add a marker at that point on the plot of the loss function.","8252caa2":"Plot the trained model.","31aefd7b":"Plot the loss function.","3e45aa33":"Search the graph points to find the point of minimum loss (brute force search).","ff6983b9":"## Summary\n\nThe model has been trained by using the data to find the best choice for the \"network parameters\" (w, b) such that the model predicts the data with minimum error.\n\nThe minimum steps were the following:\n* Collect data\n* Visualize the data (optional)\n* Choose a model and write the corresponding Predict() function\n* Choose a loss function and write the corresponding Loss() function\n* Use the GradientDescent() function to train the model\n\nNote that if you use Keras or TensorFlow, then most of the coding is done for you automatically, as shown in [Intro to ML - Part II](https:\/\/www.kaggle.com\/tagoodr\/intro-to-ml-part-ii).\n","55ddcd24":"Call the function to get the fit.","03d898b3":"Add a marker at that point on the plot of the loss function.","bd758b7a":"### Minimize the loss function using calculus\nTo minimize the loss function we set the partial derivitives of the loss function with respect to w and b equal to zero...\n$L = \\frac{1}{m} \\sum_{i=1}^{m} (w x_i + b - y_i)^2$\n\n$\\frac{\\delta L}{\\delta w} = \\frac{2}{m} \\sum_{i=1}^{m} (w x_i + b - y_i) x_i = 0$\n<br><br>\n$\\frac{\\delta L}{\\delta b} = \\frac{2}{m} \\sum_{i=1}^{m} (w x_i + b - y_i) = 0$\n<br><br>\nand then solve the resulting system of two equations in two unknowns to get w and b:\n<br><br>\n$w = \\frac{(\\frac{1}{m} \\sum_{i=1}^{m} x_i y_i) - (\\frac{1}{m} \\sum_{i=1}^{m} x_i) (\\frac{1}{m} \\sum_{i=1}^{m} y_i)}\n      {(\\frac{1}{m} \\sum_{i=1}^{m} x_i^2) - (\\frac{1}{m} \\sum_{i=1}^{m} x_i)^2}$\n<br><br>\n$b = (\\frac{1}{m} \\sum_{i=1}^{m} y_i) - w (\\frac{1}{m} \\sum_{i=1}^{m} x_i)$ \n<br><br>\nThese equations for w and b can be simplified by recognizing that each summation is an average of some quantity.\n<br><br>\n$w = \\frac{\\text{avg}(x y) \\, - \\, \\text{avg}(x) \\, \\text{avg}(y)}{\\text{avg}(x^2) \\, - \\, \\text{avg}(x)^2}\n= \\frac{(24.6) - (3)(7.2)}{(11) - (3)^2} = 1.5$\n<br><br>\n$b = \\text{avg}(y) - w \\, \\text{avg}(x) = (7.2) - (1.5)(3) = 2.7$\n\n$x$ | $y$ | $x y$ | $x^2$\n--- | --- | ----- | ----:\n1   | 4   | 4     | 1\n2   | 6   | 12    | 4\n3   | 7   | 21    | 9\n4   | 9   | 36    | 16\n5   | 10  | 50    | 25","8e0c5dc8":"### Minimize the loss function using an iterative numerical technique\n\nThe most common optimization technique used in ML is called \"**Gradient Descent**\".  The idea is to start anywhere on the loss surface and walk down-hill until you find the minimum point.  The **gradient** of a function is denoted using the $\\nabla$ symbol, and is defined as a vector describing the slope of the function along each axis at a given point.\n\n$\\nabla L = (\\frac{\\delta L}{\\delta w}, \\frac{\\delta L}{\\delta b})$\n\n$\\nabla L = (\\frac{2}{m} \\sum_{i=1}^{m} (w x_i + b - y_i) x_i, \\frac{2}{m} \\sum_{i=1}^{m} (w x_i + b - y_i))$\n\n$\\nabla L = (\\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) x_i, \\frac{2}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i))$\n\nDefine a function to compute the gradient.","804491cd":"## Visualize the data\nPlot the raw data.","f227c9f2":"Plot the history of the gradient descent.","73c39f71":"Call the function to get the fit.","79b36425":"## Collect data\n\nDefine some (x, y) data.\n\nx | y\n- | -:\n1 | 4\n2 | 6\n3 | 7\n4 | 9\n5 | 10","6baf7509":"Plot the untrained model.","64ee4fc9":"Plot the trained model.","3240e80a":"We don't yet know the values for the parameters (w, b), so initially let them be zero.","e603b1c6":"Define a function to compute the exact minimum point by using the calculus solution.","58ba54f3":"### Find the minimum loss visually\nCompute the loss function over a range of w and b values.","0e7a946d":"There are several available techniques to find the minimum of a function.  We will explore three ways:\n* Find the approximate minimum visually using a graph\n* Find the exact minimum using calculus\n* Find the approximate minimum using an iterative numerical technique","f881ed07":"Call the GradientDescent function again (but using NumGradient) to get the fit.","02c7a630":"Plot the trained model.","95b3bbe3":"Plot the trained model.","fa5fcd26":"Plot the history of the gradient descent."}}