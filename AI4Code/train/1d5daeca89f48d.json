{"cell_type":{"3e1c0671":"code","7dd7174d":"code","12b44199":"code","ae5e2669":"code","c2311f45":"code","193aa323":"code","42405316":"code","5ce1bdca":"code","7c6499da":"markdown","9546ee94":"markdown","c7ba4579":"markdown","b7e2b5c4":"markdown","09bf57a0":"markdown","0ec9ec04":"markdown","768715e4":"markdown","21305a83":"markdown","8cd22735":"markdown","594292c0":"markdown","6949df8f":"markdown","09d3b78f":"markdown","d4f16169":"markdown","5892b1dc":"markdown","6d9504f8":"markdown","6cf1145a":"markdown"},"source":{"3e1c0671":"from IPython.display import YouTubeVideo      \nYouTubeVideo('vEmm9fZJuuM')","7dd7174d":"import pandas as pd\nimport sklearn as sk\nimport math #Tocalculate IDF\n\nfirst= 'Deep Learning is fascinating'\nsecond= 'I am loving Deep Learning'\n\n#split spring into words\nfirst = first.split(\" \")\nsecond= second.split(\" \")\n\n#to remove duplicate words\ntotal= set(first).union(set(second))\nprint(total)\n\nwordDictA = dict.fromkeys(total, 0) \nwordDictB = dict.fromkeys(total, 0)\nfor word in first:\n    wordDictA[word]+=1\n    \nfor word in second:\n    wordDictB[word]+=1\n#Output \npd.DataFrame([wordDictA, wordDictB])","12b44199":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nsent = ['Deep Learning is is fascinating', \n        'am loving Deep Learning lot'\n       ]\nvect = CountVectorizer(analyzer= 'word')\nsent_vt = vect.fit_transform(sent)\n\ncount_tokens = vect.get_feature_names()\ndf_countvec = pd.DataFrame(data = sent_vt.toarray(),index = ['sentence1', 'sentence2'], columns = count_tokens)\nprint(df_countvec)","ae5e2669":"#To calculate Term Freguency\ndef computeTF(wordDict, bow):\n    tfDict = {}\n    bowCount = len(bow)\n    for word, count in wordDict.items():\n        tfDict[word] = count\/float(bowCount)\n    return tfDict\n#running our sentences through the tf function:\ntfFirst = computeTF(wordDictA, first)\ntfSecond = computeTF(wordDictB, second)\n#Converting to dataframe for visualization\ntf_df= pd.DataFrame([tfFirst, tfSecond])","c2311f45":"tf_df","193aa323":"def computeIDF(docList):\n    idfDict = {}\n    N = len(docList)\n    \n    idfDict = dict.fromkeys(docList[0].keys(), 0)\n    for doc in docList:\n        for word, val in doc.items():\n            if val > 0:\n                idfDict[word] += 1\n    \n    for word, val in idfDict.items():\n        idfDict[word] = math.log10(N \/ float(val))\n        \n    return idfDict\n#inputing our sentences in the log file\nidfs = computeIDF([wordDictA, wordDictB])\n#The actual calculation of TF*IDF from the table above:\ndef computeTFIDF(tfBow, idfs):\n    tfidf = {}\n    for word, val in tfBow.items():\n        tfidf[word] = val*idfs[word]\n    return tfidf\n#running our two sentences through the IDF:\nidfFirst = computeTFIDF(tfFirst, idfs)\nidfSecond = computeTFIDF(tfSecond, idfs)\n#putting it in a dataframe\ntfidf= pd.DataFrame([idfFirst, idfSecond])","42405316":"tfidf","5ce1bdca":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nsent = ['Deep Learning is fascinating', \n        'i am loving Deep  Learning'\n       ]\n    \n#vect = TfidfVectorizer(norm = False, smooth_idf = False, analyzer= 'word')\nvect = TfidfVectorizer(use_idf = True, smooth_idf = False,vocabulary=None,input='content',norm='l2',\n    lowercase=True,preprocessor=None,\n    tokenizer=None\n                      )\nsent_vt = vect.fit_transform(sent)\ntfid_tokens = vect.get_feature_names()\ndf_tfidvec = pd.DataFrame(data = sent_vt.toarray(),index = ['sentence1', 'sentence2'], columns = tfid_tokens)\nsent_vect = vect.fit_transform(sent)\nprint(df_tfidvec)","7c6499da":"sent = ['Deep Learning is fascinating', \n        'I am loving Deep Learning'\n       ]\n\nTotal Docuemnt = 2 \n\nSentence 1=> Total words = 4\nSentence 2=> Total words = 5\n\nTF =>  how frequently a term occurs in a document\n\nFormula:\nTF(t) = No.of times a terms t occurs in the document\/ total no.of terms in a document\n\nSentence 1:\nTF(Deep) = 1\/4 = 0.25\nTF(is)   = 1\/4 = 0.25\n\nSentence 2: \nTF(Deep) = 1\/5 = 0.2\n\nIDF => which measures, how important a term by providing lower weights to very common terms\n\nFormula:\nIDF(t) = log(total no. of documents \\total no of documents with term t in it)\nIDF(Deep) = 2\/2 = 1, \nlog(IDF(Deep))=> log(1) = 0\nIDF(is)   = 2\/1 = 2, \nlog(IDF(is))=> log(2) = 0.313\n \nTF-idf(t) = TF(t)*IDF(t) \nTF-idf(is) = TF(is)*log(IDF(is))\nTF-idf(is) = 0.25*0.3 = 0.075\n\n\nTF-idf(Deep) = TF(Deep)*log(IDF(Deep))\nTF-idf(Deep) = 0.25*0  = 0","9546ee94":"## Types of Word Embeddings\n\n=> Frequecy based Embedding\n       1. Count Vector\n       2. TF-IDF Vector\n       3. Co-Occurence Vector\n=> prediction based Embedding\n       1. CBOW(Continuos Bag of Words)\n       2. Skip - Gram Model","c7ba4579":"## Challenges\/Limitation in Tf-IDF\n\n==> context \n==> Sequence of word is completley lost\n\nEx: Movie is fantastically bad","b7e2b5c4":"##  After vectorization\n\n    Numeric data\n    Tabular format\n","09bf57a0":"# Conclussion\n\n  To overcome the context problem, we use differnt technigues in the upcoming session","0ec9ec04":"# Word Embeddings\n\nTo represent words as vector (Count Vectoizer to Word2Vec)","768715e4":"# Term Frequency- Inverse Document Frequency(TF-IDF) \n\nWeight is used to evaluate how important a word in corpus","21305a83":"## Challgenes\/Limitation in Count Vector\n\n1. Increase in size\n2. Many 0's\n3. no meaningful information","8cd22735":"# Count Vector\n\n  Simplest form of text representation in numbers","594292c0":"![TP.PNG](attachment:TP.PNG)","6949df8f":"![e1.PNG](attachment:e1.PNG) ","09d3b78f":"## Why do we need word embeddings","d4f16169":"## Calculation using Python","5892b1dc":"## Calculation using Python","6d9504f8":"## Using sklearn","6cf1145a":"## How to calculate Tf-idf weight"}}