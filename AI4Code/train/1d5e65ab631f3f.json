{"cell_type":{"cde49747":"code","43ee5d55":"code","e89b323d":"code","6fb92ed6":"code","67ff2999":"code","e58a4577":"code","00374320":"code","73a6b5b1":"code","408b20e8":"markdown","20050df8":"markdown","3b0562e7":"markdown","dc68136b":"markdown","f7992721":"markdown","fb13dea4":"markdown","6b4dd3ec":"markdown"},"source":{"cde49747":"#\n# Set up: imports\n#\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F","43ee5d55":"class KaggleMNIST(Dataset):\n    \"\"\"A custom dataset to be used with pytorch.\n    Even when pytorch has the MNIST dataset, to participate in the Kaggle\n    competition we better use the Kaggle dataset.\n    This class only loads the train dataset (images + labels).\n    The digits have to be tensors with shape [1, 28, 28] in order to be inputs\n    that the CNN can use.\n    \"\"\"\n    \n    def __init__(self):\n        data = np.loadtxt('..\/input\/digit-recognizer\/train.csv', delimiter=',', skiprows=1, dtype=np.float32)\n        self._size = len(data)\n        self._digits = (torch.from_numpy(data[:, 1:]) \/ 256).view(self._size, 1, -1, 28)                                   \n        self._labels = torch.from_numpy(data[:, 0]).type(torch.long)\n        \n        print('Training dataset with MNIST digits loaded.')\n        print('Digits dataframe has shape:', self._digits.shape)\n        print('Labels dataframe has shape:', self._labels.shape)                                          \n        \n    def __getitem__(self, idx):\n        return (self._digits[idx], self._labels[idx])\n    \n    def __len__(self):\n        return self._size\n\n\nclass KaggleMNIST_test(Dataset):\n    \"\"\"A custom dataset to be used with pytorch.\n    Even when pytorch has the MNIST dataset, to participate in the Kaggle\n    competition we better use the Kaggle dataset.\n    This class only loads the test dataset (images).\n    The digits have to be tensors with shape [1, 28, 28] in order to be inputs\n    that the CNN can use.\n    \"\"\"\n    \n    def __init__(self):\n        data = np.loadtxt('..\/input\/digit-recognizer\/test.csv', delimiter=',', skiprows=1, dtype=np.float32)\n        self._size = len(data)\n        self._digits = (torch.from_numpy(data) \/ 255).view(self._size, 1, -1, 28)\n        print('Testing dataset with MNIST digits loaded.')\n        print('Digits dataframe has shape:', self._digits.shape)\n        \n    def __getitem__(self, idx):\n        return self._digits[idx]\n    \n    def __len__(self):\n        return self._size\n\n#\n# The samplers. Will play a minor role in this notebook since there is split of\n# training data.\n#\n    \ndef create_samplers(size, train_prop):\n    \"\"\"A function that creates 2 subsets from a training dataset, one to be\n    use din training and another to be used in validation.\n    \n    Parameters\n    ----------\n    size : Numeric, integer.\n        The number of elements in the dataset to be split into a train set and\n        a validation set.\n    train_prop : Numeric, float.\n        A number between 0 and 1 that will determine the proportion of elements\n        that will be included in the validation set.\n\n    Returns\n    -------\n    A tuple with 2 SubsetRandomSampler objects, the first to be used with the\n    training DataLoader, and the second with the validation DataLoader.\n    \"\"\"\n    cut_point = int(size * train_prop)\n    shuffled = np.random.permutation(size)\n    train_sampler = SubsetRandomSampler(shuffled[:cut_point])\n    validation_sampler = SubsetRandomSampler(shuffled[cut_point:])\n    return train_sampler, validation_sampler","e89b323d":"class Convoluted_Layer_Description:\n    \"\"\"A helper objet to make easier the calculations of the convoluted\n    layers.\n    The only missing thing is the input size, which must be provided\n    later using the inject_input_size() method, since this will depend\n    on the dataset used (for the input layer) and the previous layers.\n    In all the tuples, by convention, please put first the width and\n    then the height.\n    The object does not check that the input is correct, but will convert\n    a single int into a tuple.\n    \"\"\"\n        \n    def _single_to_tuple(val):\n        \"\"\"\n        Helper function to turn single values into tuples.\n        Raises an error if val is neither a single int nor a tuple or list\n        with 2 elements.\n\n        Parameters\n        ----------\n        val : int or tuple (or list) with 2 components.\n            DESCRIPTION.\n\n        Returns\n        -------\n        tuple\n            (val, val) if val is int, or val if it a dim 2 tuple.\n\n        \"\"\"\n        if type(val) == int:\n            return (val, val)\n        if len(val) == 2:\n            return val\n        # This will cuase problems ahead...\n        return None\n    \n    def __init__(self, kernels, kernels_size, padding, stride, pooling):\n        \"\"\"\n        \n        Parameters\n        ----------\n        kernels : int\n            The number of kernels (filters) that the convoluted layers will\n            have.\n        kernels_size : tuple of int\n            The dimensions of the kernels, in pixels.\n        padding : tuple of int\n            Extra blank spaces that will be added around the image during the\n            convolution.\n        stride : tuple of int\n            Distance to advance in each application of the kernels.\n        pooling : tuple of int\n            Reduction factor that will be applied in the polling layer.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        self._input_size = None\n        self._kernels = kernels\n        self._kernels_size = Convoluted_Layer_Description._single_to_tuple(kernels_size)\n        self._padding = Convoluted_Layer_Description._single_to_tuple(padding)\n        self._stride = Convoluted_Layer_Description._single_to_tuple(stride)\n        self._pooling = Convoluted_Layer_Description._single_to_tuple(pooling)\n        \n    @property\n    def kernels(self):\n        return self._kernels\n    \n    @property\n    def kernels_size(self):\n        return self._kernels_size\n    \n    @property\n    def padding(self):\n        return self._padding\n    \n    @property\n    def stride(self):\n        return self._stride\n    \n    @property\n    def pooling(self):\n        return self._pooling\n    \n    def inject_input_size(self, size):\n        self._input_size = Convoluted_Layer_Description._single_to_tuple(size)\n        \n    def output_dimensions(self):\n        result_H = ((self._input_size[0] - self._kernels_size[0] + 2 * self._padding[0]) \/ self._stride[0] + 1) \/ self._pooling[0]\n        if result_H % 1 > 0:\n            raise ValueError('The combination of values for the width produce a non integer output size.')\n        result_V = ((self._input_size[1] - self._kernels_size[1] + 2 * self._padding[1]) \/ self._stride[1] + 1) \/ self._pooling[1]\n        if result_V % 1 > 0:\n            raise ValueError('The combination of values for the height produce a non integer output size.')\n        return [self._kernels, int(result_H), int(result_V)]\n    \n    def output_size(self):\n        dims = self.output_dimensions()\n        return int(dims[0] * dims[1] * dims[2])\n        \n    def __str__(self):\n        dims = self.output_dimensions()\n        return '\\n'.join([f'Kernels: {self._kernels} {self._kernels_size}. Padding: {self._padding}. Stride: {self._stride}. Pooling: {self._pooling}.' ,\n                          f'Input {self._input_size[0]} x {self._input_size[1]} => output {dims[1]} x {dims[2]}',\n                          f'Flattened output is {self.output_size()}.'])","6fb92ed6":"class CNN(nn.Module):\n    \"\"\"A not very complex CNN that will have three convolutional layers and\n    one fully connected layer, and the expected output layer.\n    \"\"\"\n\n    def __init__(self, input_size, conv1, conv2, conv3, hidden1, hidden2, output_size):\n        \"\"\"\n        \n        Parameters\n        ----------\n        input_size: the size of the images that will be processed. Note that\n            the input size is not needed for the net to work properly, it is\n            on\u00f1y needed to properly calculate the output that will be produced\n            by the last convolutional layer.\n            \n        conv1 : Convoluted_Layer_Description object.\n        conv2 : Convoluted_Layer_Description object.\n        conv3 : Convoluted_Layer_Description object.\n\n        hidden1 : The size of the fully connected hidden layer.\n        \n        output_size: the number of classes to b predicted.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        super().__init__()\n        \n        self._input_size = input_size # The side of the square for this dataset\n        \n        self._conv1 = conv1\n        self._conv1.inject_input_size(self._input_size)\n        self._conv2 = conv2\n        self._conv2.inject_input_size(self._conv1.output_dimensions()[1])\n        self._conv3 = conv3\n        self._conv3.inject_input_size(self._conv2.output_dimensions()[1])\n        \n        self._hidden1 = hidden1\n        self._hidden2 = hidden2\n        \n        self._classes = output_size # The output classes for this dataset\n\n        print(f'Input image size: {self._input_size} x {self._input_size}.\\n')\n        print(f'First convolution layer description:\\n{self._conv1}')\n        print(f'\\nSecond convolution layer description:\\n{self._conv2}')\n        print(f'\\nThird convolution layer description:\\n{self._conv3}')\n        print(f'\\nHidden layer 1 has size {self._hidden1}.')\n        print(f'Hidden layer 2 has size {self._hidden2}.')\n        print(f'Output layer has size {self._classes}.')\n\n        self.train_loader = None\n        self.validation_loader = None\n        self.test_loader = None\n        self._optimizer_fn = torch.optim.Adam\n        self._loss_fn = F.cross_entropy\n        dropout = 0.25\n        \n        self._net = nn.Sequential(\n            # Convolutional layer 1.\n            nn.Conv2d(1, # This dataset has only 1 channel.\n                      self._conv1.kernels,\n                      kernel_size=self._conv1.kernels_size,\n                      padding=self._conv1.padding, stride=self._conv1.stride),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=self._conv1.pooling),\n            nn.Dropout(dropout),\n            # Convolutional layer 2.\n            nn.Conv2d(self._conv1.kernels,\n                      self._conv2.kernels,\n                      kernel_size=self._conv2.kernels_size,\n                      padding=self._conv2.padding, stride=self._conv2.stride),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=self._conv2.pooling),\n            nn.Dropout(dropout),\n            # Convolutional layer 3.\n            nn.Conv2d(self._conv2.kernels,\n                      self._conv3.kernels,\n                      kernel_size=self._conv3.kernels_size,\n                      padding=self._conv3.padding, stride=self._conv3.stride),\n            nn.LeakyReLU(),\n            nn.MaxPool2d(kernel_size=self._conv3.pooling),\n            nn.Dropout(dropout),\n            # Fully connected layer 1.\n            nn.Flatten(),\n            nn.Linear(self._conv3.output_size(), self._hidden1),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout),\n            # Fully connected layer 2.\n            nn.Flatten(),\n            nn.Linear(self._hidden1, self._hidden2),\n            nn.LeakyReLU(),\n            # Output layer\n            nn.Linear(self._hidden2, self._classes))\n        \n    def forward(self, batch):\n        return self._net(batch)\n    \n    def define_loaders(self, train, validation, test):\n        self.train_loader = train\n        self.validation_loader = validation\n        self.test_loader = test\n            \n    def _accuracy(self, outputs, labels):\n        preds = torch.max(outputs, dim=1)[1]\n        return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n                \n    def _validation_with_batch(self, batch):\n        images, labels = batch \n        preds = self(images)\n        loss = self._loss_fn(preds, labels, reduction='sum')\n        acc = self._accuracy(preds, labels) * len(labels)\n        return {'loss': loss, 'accuracy': acc}\n        \n    def _evaluate(self):\n        with torch.no_grad():\n            outputs = [self._validation_with_batch(batch) for batch in self.validation_loader]\n        # Put together the results of the different batches.\n        samples = len(self.validation_loader.sampler.indices)\n        batch_losses = [x['loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).sum() \/ samples\n        batch_accuracies = [x['accuracy'] for x in outputs]\n        epoch_accuracy = torch.stack(batch_accuracies).sum() \/ samples\n        return {'loss': epoch_loss.item(), 'accuracy': epoch_accuracy.item()}\n    \n    def predict_submission(self):\n        outputs = []\n        with torch.no_grad():\n            for batch in self.test_loader:\n                outputs.extend(torch.max(self(batch), dim=1)[1])\n        outputs = [x.item() for x in outputs]\n        outputs = zip(range(1, len(outputs)+1), outputs)\n        return pd.DataFrame(outputs, columns=['ImageId', 'Label'])\n\n    def fit(self, epochs, learning_rate, verbose=False):\n        if verbose:\n            print(f'Training the model for {epochs} epochs with learning rate {learning_rate}.')\n        optim = self._optimizer_fn(self.parameters(), lr=learning_rate)\n        history = []\n        for epoch in range(epochs):\n            # Training Phase \n            for batch in self.train_loader:\n                images, labels = batch \n                loss = self._loss_fn(self(images), labels)\n                loss.backward()\n                optim.step()\n                optim.zero_grad()\n            # Evaluate epoch with validation data set, if defined.\n            if self.validation_loader:\n                result = self._evaluate()\n                if verbose:\n                    print(\"Epoch [{}], loss: {:.4f}, accuracy: {:.4f}\".format(epoch, result['loss'], result['accuracy']))\n                history.append(result)\n        return history ","67ff2999":"batch_size = 80\ntrain_pct = 0.95\n\n# Load and prepare data.\n\ntrain_set = KaggleMNIST()\ntest_set = KaggleMNIST_test()\n\ntrain_sampler, validation_sampler = create_samplers(len(train_set), train_pct)\n\nprint(f'\\nSize of training set: {len(train_sampler.indices)}.')\nprint(f'Size of validation set: {len(validation_sampler.indices)}.\\n')\n\ntrain_loader = DataLoader(train_set, batch_size, sampler = train_sampler)\nvalidation_loader = DataLoader(train_set, batch_size, sampler = validation_sampler)\ntest_loader = DataLoader(test_set, batch_size)","e58a4577":"\n# Creating the convolutional layers descriptions.\n\nconv_layer1 = Convoluted_Layer_Description(kernels=48, kernels_size=3, padding=1, stride=1, pooling=2)\nconv_layer1.inject_input_size(28)\nprint(conv_layer1)\nconv_layer2 = Convoluted_Layer_Description(kernels=220, kernels_size=3, padding=1, stride=1, pooling=2)\nconv_layer2.inject_input_size(conv_layer1.output_dimensions()[1])\nprint(conv_layer2)\nconv_layer3 = Convoluted_Layer_Description(kernels=160, kernels_size=4, padding=0, stride=1, pooling=1)\nconv_layer3.inject_input_size(conv_layer2.output_dimensions()[1])\nprint(conv_layer3)","00374320":"# Model creation and training\n\nmodel = CNN(28, conv_layer1, conv_layer2, conv_layer3, 260, 140, 10)\n\nmodel.define_loaders(train_loader, validation_loader, test_loader)\n","73a6b5b1":"print('Starting training...')\nresult = model.fit(4,  0.0002, verbose=True)\nresult = model.fit(5,  0.00001, verbose=True)\nresult = model.fit(10, 0.000001, verbose=True)\nresult = model.fit(18, 0.0000001, verbose=True)\nprint('Training completed.')\n\n\nsubmission = model.predict_submission()\nsubmission.to_csv(\"submission.csv\", index=False)","408b20e8":"# Convolutional Neural Network\n\nThis is my second attempt at digit recognition; this time, using a CNN. The pattern I follow is quite simmilar to my previous code using a simple MLP.\n\nBefore writing this notebook I have done several tests in Spyder, trying different combinations: 2 and 3 convolutional layers with 1 or 2 hidden linear layers, with kernels sized 3 to 5, and number of kernels from 16 to 160, and hidden layers from 180 to 60 neurons.\nBasically, all the combinations seemed to stall at about 99% (at the moment of writing thisnotebook my best accuracy is 99.15%).\n\nI have been using from 80% to 95% of the train cases for actual training, and the rest for validation. Using a bigger part of the training set  for validation resulted in less accuracy overall (both in validation and in the competition).","20050df8":"### Layers definition","3b0562e7":"### Model creation and training","dc68136b":"## Data loaders","f7992721":"### The definition of the arquictecture\nI'll be using 3 convoluted layers and 2 hidden layers.","fb13dea4":"## Helper class: description of a convoluted layer\nDuring the training process, I found myself calculating by hand the compatible parameters (kernel size, padding and stride) that could be stacked to create the convoluted layers. In the end, I wrote this clas that takes the convoleuted layer characteristics and calculates what the outpust size will be, warning if certain combination produces an invalid output size, which is basically an odd-sized output grid that is to be pooled by a number that is not a divisor of the dimension. Although the dataset uses a square grid, this class can work with rectangular grids.","6b4dd3ec":"## The actual training\n### Data loading"}}