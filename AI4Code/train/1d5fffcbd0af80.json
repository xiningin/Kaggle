{"cell_type":{"aae292ed":"code","998f3abe":"code","ae4a1418":"code","77ac8053":"code","34cbb9e5":"code","ac0d29a4":"code","50dff009":"code","84b0aa29":"code","c560c631":"code","5ecedf70":"code","8972d5e7":"code","4083c9d1":"code","b0b6df00":"markdown","ff43f754":"markdown","9d4da4bd":"markdown","5a700aec":"markdown","f70dddb9":"markdown","4e7d800f":"markdown","d51c324f":"markdown","37ec7167":"markdown"},"source":{"aae292ed":"%pwd","998f3abe":"import pandas as pd\nimport numpy as np","ae4a1418":"def entropy(target_col):\n    elements,counts = np.unique(target_col,return_counts = True)\n    entropy = np.sum([(-counts[i]\/np.sum(counts))*np.log2(counts[i]\/np.sum(counts)) for i in range(len(elements))])\n    return entropy","77ac8053":"def InfoGain(data,split_attribute_name,target_name=\"class\"):\n \n    #Calculate the entropy of the total dataset\n    total_entropy = entropy(data[target_name])\n    \n    ##Calculate the entropy of the dataset\n    \n    #Calculate the values and the corresponding counts for the split attribute \n    vals,counts= np.unique(data[split_attribute_name],return_counts=True)\n    \n    #Calculate the weighted entropy\n    Weighted_Entropy = np.sum([(counts[i]\/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])\n    \n    #Calculate the information gain\n    Information_Gain = total_entropy - Weighted_Entropy\n    return Information_Gain","34cbb9e5":"def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):\n    \n    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#\n    \n    #If all target_values have the same value, return this value\n    if len(np.unique(data[target_attribute_name])) <= 1:\n        #print(data[target_attribute_name])\n        return np.unique(data[target_attribute_name])[0]\n    \n    #If the dataset is empty, return the mode target feature value in the original dataset\n    elif len(data)==0:\n        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]\n    \n    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that\n    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence\n    #the mode target feature value is stored in the parent_node_class variable.\n    \n    elif len(features) ==0:\n        return parent_node_class\n    \n    #If none of the above holds true, grow the tree!\n    \n    else:\n        #Set the default value for this node --> The mode target feature value of the current node\n        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]\n        \n        #Select the feature which best splits the dataset\n        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset\n        best_feature_index = np.argmax(item_values)\n        best_feature = features[best_feature_index]\n        \n        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information\n        #gain in the first run\n        tree = {best_feature:{}}\n        \n        \n        #Remove the feature with the best inforamtion gain from the feature space\n        features = [i for i in features if i != best_feature]\n        \n        #Grow a branch under the root node for each possible value of the root node feature\n        \n        for value in np.unique(data[best_feature]):\n            value = value\n            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets\n            sub_data = data.where(data[best_feature] == value).dropna()\n            \n            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!\n            subtree = ID3(sub_data,originaldata,features,target_attribute_name,parent_node_class)\n            \n            #Add the sub tree, grown from the sub_dataset to the tree under the root node\n            tree[best_feature][value] = subtree\n            \n        return(tree)","ac0d29a4":"dataset = pd.read_csv('..\/input\/Zoo_New.csv',\n                      names=['animal_name','hair','feathers','eggs','milk',\n                                                   'airbone','aquatic','predator','toothed','backbone',\n                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals\n#We drop the animal names since this is not a good feature to split the data on\ndataset=dataset.drop('animal_name',axis=1)","50dff009":"dataset.head()","84b0aa29":"def predict(query,tree,default = 1):\n    \n    \n    #1.\n    for key in list(query.keys()):\n        if key in list(tree.keys()):\n            #2.\n            try:\n                result = tree[key][query[key]] \n            except:\n                return default\n  \n            #3.\n            result = tree[key][query[key]]\n            #4.\n            if isinstance(result,dict):\n                return predict(query,result)\n            else:\n                return result\n        ","c560c631":"dataset.shape","5ecedf70":"def train_test_split(dataset):\n    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index\n    #starting form 0, because we do not want to run into errors regarding the row labels \/ indexes\n    testing_data = dataset.iloc[80:].reset_index(drop=True)\n    return training_data,testing_data\ntraining_data = train_test_split(dataset)[0]\ntesting_data = train_test_split(dataset)[1] \n\n","8972d5e7":"def test(data,tree):\n    #Create new query instances by simply removing the target feature column from the original dataset and \n    #convert it to a dictionary\n    queries = data.iloc[:,:-1].to_dict(orient = \"records\")\n    print(len(queries))\n    #Create a empty DataFrame in whose columns the prediction of the tree are stored\n    predicted = pd.DataFrame(columns=[\"predicted\"]) \n    \n    #Calculate the prediction accuracy\n   # print(data)\n    for i in range(len(data)):\n        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0) \n       # print(predict(queries[i],tree,1.0) )\n    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])\/len(data))*100,'%')","4083c9d1":"tree = ID3(training_data,training_data,training_data.columns[:-1])\nprint(tree)\ntest(testing_data,tree)","b0b6df00":"#### Predict function to find the class Label","ff43f754":"#### Splitting the datasets into train and test","9d4da4bd":"#### Function to Predict the class of test data","5a700aec":"#### Importing Required Packages","f70dddb9":"### Decision tree","4e7d800f":"The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree. In ZeroR model there is no predictor, in OneR model we try to find the single best predictor, naive Bayesian includes all predictors using Bayes' rule and the independence assumptions between predictors but decision tree includes all predictors with the dependence assumptions between predictors.","d51c324f":"The objective of this experiment is to understand Decision Tree.\n\nAs the name says all about it, it is a tree which helps us by assisting us in decision-making. Used\nfor both classification and regression, it is a very basic and important predictive learning algorithm.\n\n    1. It is different from others because it works intuitively i.e., taking decisions one-by-one.\n    2. Non-Parametric: Fast and efficient.\n\nIt consists of nodes which have parent-child relationships","37ec7167":"#### Loading Dataset"}}