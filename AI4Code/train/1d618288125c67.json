{"cell_type":{"b1ecbb56":"code","d957eed2":"code","a1d605f1":"code","4b8c61db":"code","7c136300":"code","ce5af3e0":"code","8d49464e":"code","d5db7f8e":"code","1bd9bc12":"code","e3a7dabc":"code","fa5b05bc":"code","7601fadf":"code","4fef9569":"code","a2377772":"code","08aa803a":"markdown","73acbc4d":"markdown","9b4db818":"markdown","cabcb791":"markdown","dfcd1d58":"markdown","0bc3388f":"markdown","ab7ad4d4":"markdown"},"source":{"b1ecbb56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d957eed2":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv1D, MaxPool1D\nfrom tensorflow.keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","a1d605f1":"cancer = pd.read_csv('..\/input\/breastcancer-dataset\/data.csv')\ncancer.head()","4b8c61db":"X = cancer.drop(['diagnosis','Unnamed: 32'],axis=1)\ny = cancer['diagnosis']","7c136300":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ny = lb.fit_transform(y)\ny","ce5af3e0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)","8d49464e":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d5db7f8e":"X_train.shape, X_test.shape","1bd9bc12":"X_train = X_train.reshape(455,31,1)\nX_test = X_test.reshape(114,31,1)","e3a7dabc":"epochs = 50\nmodel = Sequential()\nmodel.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(31,1)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))","fa5b05bc":"model.summary()","7601fadf":"model.compile(optimizer=Adam(lr=0.00005), loss='binary_crossentropy', metrics=['accuracy'])","4fef9569":"history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=1)","a2377772":"import warnings\nwarnings.filterwarnings('ignore')\n\n#plotting training and validation accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Val'], loc='upper_left')\nplt.show()\n\n#plotting training and validation loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Val'], loc='upper_left')\nplt.show()","08aa803a":"## Designing the CNN model","73acbc4d":"# Some useful links.Hope it helps to understand the basic\n\nYou can learn more about keras Conv1D from [here](https:\/\/keras.io\/api\/layers\/convolution_layers\/convolution1d\/)\n\nAnd for batchnormalization and dropout layers you can use this [link](https:\/\/keras.io\/api\/layers\/convolution_layers\/convolution1d\/)","9b4db818":"# Reshaping the dataset for training into CNN","cabcb791":"# Importing of necessary libraries.","dfcd1d58":"# Graph for showing the accuracy and loss between test and train","0bc3388f":"# Taking the labeled features to y","ab7ad4d4":"# Scaling the X part of dataset for better fitting."}}