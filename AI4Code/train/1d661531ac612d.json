{"cell_type":{"89ac19cd":"code","22c7b0fb":"code","bf67b286":"code","b932a8f6":"code","b3539360":"code","333d8a6e":"code","e14f05b8":"code","0566dad9":"code","39003fb7":"code","452c71d2":"code","f720c5ee":"code","f5930d4f":"code","b8a42ee2":"code","a991e605":"markdown","19c7413d":"markdown","f0bf12cb":"markdown","101397a2":"markdown","8e435b1e":"markdown","caa387b3":"markdown"},"source":{"89ac19cd":"import os, sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport skimage.io\nfrom skimage.transform import resize\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","22c7b0fb":"path_to_train = '..\/input\/train\/'\ndata = pd.read_csv('..\/input\/train.csv')\n\ntrain_dataset_info = []\nfor name, labels in zip(data['Id'], data['Target'].str.split(' ')):\n    train_dataset_info.append({\n        'path':os.path.join(path_to_train, name),\n        'labels':np.array([int(label) for label in labels])})\ntrain_dataset_info = np.array(train_dataset_info)","bf67b286":"class data_generator:\n    \n    def create_train(dataset_info, batch_size, shape, augument=True):\n        assert shape[2] == 3\n        while True:\n            random_indexes = np.random.choice(len(dataset_info), batch_size)\n            batch_images = np.empty((batch_size, shape[0], shape[1], shape[2]))\n            batch_labels = np.zeros((batch_size, 28))\n            for i, idx in enumerate(random_indexes):\n                image = data_generator.load_image(\n                    dataset_info[idx]['path'], shape)   \n                if augument:\n                    image = data_generator.augment(image)\n                batch_images[i] = image\n                batch_labels[i][dataset_info[idx]['labels']] = 1\n            yield batch_images, batch_labels\n            \n    \n    def load_image(path, shape):\n        image_red_ch = skimage.io.imread(path+'_red.png')\n        image_yellow_ch = skimage.io.imread(path+'_yellow.png')\n        image_green_ch = skimage.io.imread(path+'_green.png')\n        image_blue_ch = skimage.io.imread(path+'_blue.png')\n\n        image_red_ch += (image_yellow_ch\/2).astype(np.uint8) \n        image_green_ch += (image_yellow_ch\/2).astype(np.uint8)\n\n        image = np.stack((\n            image_red_ch, \n            image_green_ch, \n            image_blue_ch), -1)\n        image = resize(image, (shape[0], shape[1]), mode='reflect')\n        return image\n                \n            \n    def augment(image):\n        augment_img = iaa.Sequential([\n            iaa.OneOf([\n                iaa.Affine(rotate=0),\n                iaa.Affine(rotate=90),\n                iaa.Affine(rotate=180),\n                iaa.Affine(rotate=270),\n                iaa.Fliplr(0.5),\n                iaa.Flipud(0.5),\n            ])], random_order=True)\n        \n        image_aug = augment_img.augment_image(image)\n        return image_aug","b932a8f6":"# create train datagen\ntrain_datagen = data_generator.create_train(\n    train_dataset_info, 5, (299,299,3), augument=True)","b3539360":"images, labels = next(train_datagen)\n\nfig, ax = plt.subplots(1,5,figsize=(25,5))\nfor i in range(5):\n    ax[i].imshow(images[i])\nprint('min: {0}, max: {1}'.format(images.min(), images.max()))","333d8a6e":"from keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, Conv3D, MaxPooling3D, Reshape, BatchNormalization, MaxPooling2D\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import metrics\nfrom keras.optimizers import Adam \nfrom keras import backend as K\nimport keras\n\ndef create_model(input_shape, n_out):\n    \n    '''pretrain_model = InceptionResNetV2(\n        include_top=False, \n        weights='imagenet', \n        input_shape=input_shape)'''\n    \n    model = Sequential()\n    model.add(Reshape((299,299,3,1), input_shape=input_shape))\n    model.add(Conv3D(4, kernel_size=(4,4,2), strides=(1,1,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n    model.add(Dropout(0.5))\n    model.add(Conv3D(8, kernel_size=(4,4,2), strides=(1,1,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n    model.add(Dropout(0.5))\n    model.add(Conv3D(16, kernel_size=(4,4,1), strides=(1,1,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n    model.add(Dropout(0.5))\n    model.add(Reshape((34,34,16)))\n    model.add(Conv2D(32, kernel_size=(4,4), strides=(1,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.5))\n    model.add(Conv2D(64, kernel_size=(4,4), strides=(1,1)))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n    model.add(Dropout(0.5))\n    model.add(Flatten())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1024))\n    model.add(Activation('relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(n_out))\n    model.add(Activation('sigmoid'))\n    return model\n#input_shape=(299,299,3)","e14f05b8":"from keras.callbacks import EarlyStopping, ModelCheckpoint\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","0566dad9":"keras.backend.clear_session()\n\nmodel = create_model(\n    input_shape=(299,299,3), \n    n_out=28)\n\nmodel.compile(\n    loss='categorical_crossentropy', \n    optimizer=Adam(),\n    metrics=['acc', f1])\n\nmodel.summary()","39003fb7":"epochs = 100; batch_size = 32\ncheckpointer = ModelCheckpoint(\n    '.\/model.hdf5', \n    verbose=1, \n    save_best_only=True, monitor='val_f1', mode='max')\n\n# split and suffle data \nnp.random.seed(2018)\nindexes = np.arange(train_dataset_info.shape[0])\nnp.random.shuffle(indexes)\ntrain_indexes = indexes[:27500]\nvalid_indexes = indexes[27500:]\n\n# create train and valid datagens\ntrain_generator = data_generator.create_train(\n    train_dataset_info[train_indexes], batch_size, (299,299,3), augument=True)\nvalidation_generator = data_generator.create_train(\n    train_dataset_info[valid_indexes], 100, (299,299,3), augument=False)\n\n# train model\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=100,\n    validation_data=next(validation_generator),\n    epochs=epochs, \n    verbose=1,\n    callbacks=[checkpointer])","452c71d2":"fig, ax = plt.subplots(1, 2, figsize=(15,5))\nax[0].set_title('loss')\nax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\nax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\nax[1].set_title('acc')\nax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\nax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\nax[0].legend()\nax[1].legend()","f720c5ee":"submit = pd.read_csv('..\/input\/sample_submission.csv')","f5930d4f":"%%time\npredicted = []\nfor name in tqdm(submit['Id']):\n    path = os.path.join('..\/input\/test\/', name)\n    image = data_generator.load_image(path, (299,299,3))\n    score_predict = model.predict(image[np.newaxis])[0]\n    label_predict = np.arange(28)[score_predict>=0.5]\n    str_predict_label = ' '.join(str(l) for l in label_predict)\n    predicted.append(str_predict_label)","b8a42ee2":"submit['Predicted'] = predicted\nsubmit.to_csv('submission.csv', index=False)","a991e605":"### Load dataset info","19c7413d":"### Create datagenerator","f0bf12cb":"### Create submit","101397a2":"\n### Show data","8e435b1e":"### Create model","caa387b3":"### Train model"}}