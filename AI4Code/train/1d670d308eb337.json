{"cell_type":{"ae0c5c0a":"code","ed5aa8e1":"code","a4840d20":"code","109ca7b9":"code","960b4939":"code","81270453":"code","2682c0ef":"code","68d5e7d8":"code","6d27978d":"code","1e817cf0":"code","c0327ceb":"code","a18d2e2b":"code","6518f4d8":"code","2ad09320":"code","e647a8e9":"code","b55efb40":"code","bc22b8ca":"code","18bd9428":"code","93d42ecf":"code","641df614":"code","d136766a":"code","84a075d8":"code","5b84c552":"code","749100a9":"code","904f35fc":"code","9bbc6351":"code","73f862cc":"code","b532e5cb":"code","66b41720":"code","2d1eb88a":"code","c05fe806":"code","d6ddf327":"code","9f2d6de6":"code","94174fe3":"code","d5ebf150":"markdown","be74ca7d":"markdown"},"source":{"ae0c5c0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ed5aa8e1":"train = pd.read_csv('..\/input\/training.csv', delimiter = \";\")\nval = pd.read_csv('..\/input\/validation.csv', delimiter = \";\")","a4840d20":"train.head()","109ca7b9":"train.describe()","960b4939":"train.info()","81270453":"total = pd.concat([train, val], axis=0)","2682c0ef":"total.shape\ntrain.shape\nval.shape","68d5e7d8":"total['variable2'] = total['variable2'].str.replace(',','.').astype(np.float64)\ntotal['variable3'] = total['variable3'].str.replace(',','.').astype(np.float64)\ntotal['variable8'] = total['variable8'].str.replace(',','.').astype(np.float64)","6d27978d":"total[:train.shape[0]].info()","1e817cf0":"total.head()","c0327ceb":"total.drop('variable18', axis = 1, inplace = True)","a18d2e2b":"total[['variable2']].boxplot()","6518f4d8":"total['variable2'] = total['variable2'].fillna(total['variable2'].median())\ntotal['variable3'] = total['variable3'].fillna(total['variable3'].median())\ntotal['variable8'] = total['variable8'].fillna(total['variable8'].median())\ntotal['variable14'] = total['variable14'].fillna(total['variable14'].median())\ntotal['variable17'] = total['variable17'].fillna(total['variable17'].median())","2ad09320":"total.info()","e647a8e9":"total.columns","b55efb40":"for i in total.columns:\n    if total[i].dtypes == \"O\":\n        total[i] = total[i].fillna(total[i].mode()[0])","bc22b8ca":"total.info()","18bd9428":"total.head()","93d42ecf":"total['classLabel'].unique()","641df614":"total[total['classLabel'] ==  'no.'].shape\nsampling_data = total[total['classLabel'] ==  'no.']","d136766a":"total = pd.concat([total, sampling_data, sampling_data, sampling_data], axis=0)","84a075d8":"X = pd.get_dummies(total.iloc[:, total.columns != 'classLabel'])","5b84c552":"X_train = X[ :train.shape[0] + (2 * sampling_data.shape[0])]\nX_test = X[train.shape[0]: ]\ny_train = total['classLabel'][ :train.shape[0]+ (2 * sampling_data.shape[0])]\ny_test = total['classLabel'][train.shape[0]: ]","749100a9":"X_train.shape ,  X_test.shape , y_train.shape , y_test.shape","904f35fc":"y_train = y_train.values.reshape(-1, 1)\ny_test = y_test.values.reshape(-1, 1)","9bbc6351":"from sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_train_sc = sc_X.fit_transform(X_train)\nX_test_sc = sc_X.transform(X_test)","73f862cc":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier","b532e5cb":"logi_clf = LogisticRegression(solver='lbfgs', max_iter=500)\nlogi_parm = {\"C\": [0.1, 0.5, 1, 5, 10, 50],\n            'random_state': [0,1,2,3,4,5]}\n\nsvm_clf = SVC(probability=True)\nsvm_parm = {'kernel': ['rbf', 'poly'], \n            'C': [1, 5, 50, 100, 500, 1000], \n            'degree': [3, 5, 7], \n            'gamma': ['auto', 'scale'],\n           'random_state': [0,1,2,3,4,5]}\n\ndt_clf = DecisionTreeClassifier()\ndt_parm = {'criterion':['gini', 'entropy'],\n          'random_state': [0,1,2,3,4,5]}\n\nknn_clf = KNeighborsClassifier()\nknn_parm = {'n_neighbors':[5, 10, 15, 20], \n            'weights':['uniform', 'distance'], \n            'p': [1,2]}\n\ngnb_clf = GaussianNB()\ngnb_parm = {'priors':['None']}\n\nclfs = [logi_clf, svm_clf, dt_clf, knn_clf]\nparams = [logi_parm, svm_parm, dt_parm, knn_parm] \nclf_names = ['logistic', 'SVM', 'DT', 'KNN', 'GNB']","66b41720":"y_train","2d1eb88a":"import sklearn.ensemble as ens\n\nclf = ens.RandomForestClassifier()\nparam = {'n_estimators':[10,50,100,500,100],\n         'criterion': ['gini', 'entropy'],}\nRF = RandomizedSearchCV(clf, param, cv=5)\nRF.fit(X_train_sc, y_train)\nRF.best_score_","c05fe806":"from sklearn.metrics import accuracy_score, confusion_matrix","d6ddf327":"accuracy_score(y_test, RF.predict(X_test))","9f2d6de6":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train_sc, y_train)\n\ny_pred = xgb_model.predict(X_test_sc)\n\nprint(confusion_matrix(y_test, y_pred))","94174fe3":"accuracy_score(y_test, y_pred)","d5ebf150":"# <center>  Focus on the next 4 cells ","be74ca7d":"## <center> Focus more on the next cell"}}