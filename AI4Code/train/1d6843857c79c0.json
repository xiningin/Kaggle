{"cell_type":{"bf2c6c57":"code","7126d34c":"code","4c34504a":"code","cc057878":"code","5720ac33":"code","70f0d5bd":"code","4fb40f2d":"code","bb803d5f":"code","d9283f88":"code","148800a9":"code","7713d96d":"code","4285c77a":"code","77df5995":"code","0bcbc00b":"code","39fe6a61":"code","e2e5bd2a":"markdown","b4e6ff27":"markdown","4a0561dd":"markdown","ae099469":"markdown","9b39dd07":"markdown","e41e6e42":"markdown","45178e13":"markdown","92030e04":"markdown","3c1d21a3":"markdown","78225b2e":"markdown","627f9fb4":"markdown","c8610fb0":"markdown","42f9df36":"markdown","baf9fb46":"markdown","f9cd2b57":"markdown","da7808aa":"markdown"},"source":{"bf2c6c57":"import tensorflow as tf\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom PIL import Image","7126d34c":"IMG_CHANNELS, IMG_WIDTH, IMG_HEIGHT = 3, 512, 512","4c34504a":"X = next(os.walk('..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images'))[2]\ny = next(os.walk('..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/masks'))[2]","cc057878":"X_ids = X[:-10]\ny_ids = y[:-10]","5720ac33":"X_train = np.zeros((len(X_ids), 256, 256, 3), dtype=np.float32)\ny_train = np.zeros((len(y_ids), 256, 256, 1), dtype=np.bool)\n\nfor n, id_ in enumerate(X_ids):\n    image = tf.keras.preprocessing.image.load_img(f'..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images\/{id_}', target_size=(IMG_HEIGHT, IMG_WIDTH))\n    input_arr = tf.keras.preprocessing.image.img_to_array(image)[90:450,150:406]\n    image = tf.keras.preprocessing.image.array_to_img(input_arr, ).resize((256, 256))\n    X_train[n] = np.array(image)\n\nfor n, id_ in enumerate(y_ids):\n    image = tf.keras.preprocessing.image.load_img(f'..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/masks\/{id_}', \n                                                  target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode=\"grayscale\")\n    input_arr = tf.keras.preprocessing.image.img_to_array(image)[90:450,150:406]\n    image = tf.keras.preprocessing.image.array_to_img(input_arr).resize((256, 256))\n    y_train[n] = np.array(image)[:, :, np.newaxis]","70f0d5bd":"plt.imshow(tf.keras.preprocessing.image.array_to_img(X_train[1000]))","4fb40f2d":"plt.imshow(tf.keras.preprocessing.image.array_to_img(y_train[1000]))","bb803d5f":"inputs = tf.keras.layers.Input((256, 256, 3))\ns = tf.keras.layers.Lambda(lambda x: x \/ 255)(inputs)\n\n#Contraction path\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\nc1 = tf.keras.layers.Dropout(0.1)(c1)\nc1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\np1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\nc2 = tf.keras.layers.Dropout(0.1)(c2)\nc2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\np2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n \nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\nc3 = tf.keras.layers.Dropout(0.2)(c3)\nc3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\np3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n \nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\nc4 = tf.keras.layers.Dropout(0.2)(c4)\nc4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\np4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n \nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\nc5 = tf.keras.layers.Dropout(0.3)(c5)\nc5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n#Expansive path \nu6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\nu6 = tf.keras.layers.concatenate([u6, c4])\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\nc6 = tf.keras.layers.Dropout(0.2)(c6)\nc6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n \nu7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\nu7 = tf.keras.layers.concatenate([u7, c3])\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\nc7 = tf.keras.layers.Dropout(0.2)(c7)\nc7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n \nu8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\nu8 = tf.keras.layers.concatenate([u8, c2])\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\nc8 = tf.keras.layers.Dropout(0.1)(c8)\nc8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n \nu9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\nu9 = tf.keras.layers.concatenate([u9, c1], axis=3)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\nc9 = tf.keras.layers.Dropout(0.1)(c9)\nc9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n \noutputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n \nmodel = tf.keras.Model(inputs=[inputs], outputs=[outputs])\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","d9283f88":"results = model.fit(X_train, y_train, validation_split=0.1, batch_size=16, epochs=25)","148800a9":"# summarize history for accuracy\nplt.plot(results.history['accuracy'])\nplt.plot(results.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(results.history['loss'])\nplt.plot(results.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7713d96d":"import random\n\ntest_id = random.choice(X_ids[-10:])\nprint(test_id)","4285c77a":"img = tf.keras.preprocessing.image.load_img(f\"..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images\/{test_id}\", target_size=(256, 256))\ninput_array = tf.keras.preprocessing.image.img_to_array(img)\ninput_array = np.array([input_array])  # Convert single image to a batch.\npredictions = model.predict(input_array)","77df5995":"Image.open(f\"..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images\/{test_id}\").resize((256, 256))","0bcbc00b":"plt.imshow(tf.keras.preprocessing.image.array_to_img(np.squeeze(predictions)[:, :, np.newaxis]))","39fe6a61":"Image.open(f\"..\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/masks\/{test_id}\").resize((256 ,256))","e2e5bd2a":"### MODEL ARCH:\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Artem_Sevastopolsky\/publication\/324859814\/figure\/fig1\/AS:621316675731456@1525144847236\/Res-U-Net-architecture-a-basic-block-of-the-Stack-U-Net-model-Another-possible-basic.png\" width=\"650\" height=\"350\">","b4e6ff27":"<img src='https:\/\/media0.giphy.com\/media\/USCdT944ENuRtIpJHm\/giphy.gif' width=150 height=150>","4a0561dd":"# Prediction:","ae099469":"# Importing The libraries","9b39dd07":"![image.png](attachment:image.png)","e41e6e42":"## The Different Types of Image Segmentation:\n\n![image.png](attachment:image.png)\n\n* In image 1, every pixel belongs to a particular class (either background or person). Also, all the pixels belonging to a particular class are represented by the same color (background as black and person as pink). This is an example of semantic segmentation\n\n* Image 2 has also assigned a particular class to each pixel of the image. However, different objects of the same class have different colors (Person 1 as red, Person 2 as green, background as black, etc.). This is an example of instance segmentation\n","45178e13":"Image view after pre-processing.","92030e04":"<img src=\"https:\/\/miro.medium.com\/proxy\/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width=\"300\" height=\"300\">","3c1d21a3":"# MODEL:","78225b2e":"Getting name of image files and appending it to a list.","627f9fb4":"## PREDICTED IMAGE:","c8610fb0":"## UNet model for image segmentation [Semantic].\n\n[Paper](https:\/\/arxiv.org\/abs\/1505.04597)\n","42f9df36":"Plotting history:","baf9fb46":"# Pre-processing:\n\n* Assigning intital values to X_train and y_train, iterating over file name.\n* Loading image using keras's load_img.\n* converting img to np array using img_to_array and slicing(cropping)\n* converting array to PIL format to resize it to (256, 256){INPUT SIZE}\n* and then replacing zeros in X_train by the Image array values\n\n\n* in the case of y_train, adding an ***extra dimension*** using ***np.newaxis***","f9cd2b57":"# Training the model.","da7808aa":"#### UPVOTE if you like\n##### Peace \ud83d\udd4a"}}