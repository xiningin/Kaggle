{"cell_type":{"130045b4":"code","ea0387c1":"code","a241570b":"code","f8c696f2":"code","5ae00394":"code","4eea45db":"code","604602b9":"code","e0164e48":"code","34a6d2e8":"code","5c1962e8":"code","5143ce27":"code","a067fd56":"code","61b8f7c2":"code","417153cf":"code","bb5b429e":"code","443be55b":"code","26364b96":"code","ded8eed5":"code","b3b9ebb7":"code","1dfbc38f":"code","701399b2":"code","a015b4fc":"code","7879140c":"code","fb368fa0":"code","53dd18cd":"code","e5f7bde4":"code","159cb042":"code","32f155ad":"code","dc42660b":"code","5ed4b4b5":"code","f80ebb75":"code","79144e3b":"code","cbd8b1a4":"code","50c6c055":"code","8faf1170":"code","f8673aef":"code","881ba81f":"code","b9543020":"code","3a0da353":"code","16c27f18":"code","e733bff9":"code","6713df8e":"code","eb49117e":"code","a9abd6e9":"code","f4245e6d":"code","e3fdec3c":"code","e4a5fe5c":"code","9084e5ee":"code","0f1ce7ca":"code","d69be4a9":"code","14182491":"code","f4d9f7e2":"code","30dc8f29":"code","172565f2":"code","4804fdfd":"code","1a9f3f3a":"code","d214b57c":"code","cfaf769b":"code","6aa27327":"code","8e7d747f":"code","3b2091f2":"code","86d00ea4":"code","6dee699c":"code","f4601181":"code","04f34f8c":"code","76e8093d":"code","8fa2dc32":"code","3407b545":"code","a9518f57":"markdown","5c49bd83":"markdown","93fb2dd9":"markdown","689f74d5":"markdown","6d934dc9":"markdown","0e543ff6":"markdown","e6c45762":"markdown","ab44e184":"markdown","5db6ddde":"markdown","38812745":"markdown","bc60e668":"markdown","9d0cf041":"markdown","bcc63584":"markdown","4100b2bf":"markdown","f3a173f2":"markdown"},"source":{"130045b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea0387c1":"#test='test'\ntest=\"prod\"","a241570b":"! ls \/kaggle\/input\/ktrain-library\/ktrain","f8c696f2":"!pip install ktrain --no-index --find-links=file:\/\/\/kaggle\/input\/ktrain-library\/ktrain","5ae00394":"#!pip install ktrain","4eea45db":"from tqdm.notebook import tqdm","604602b9":"import tensorflow as tf","e0164e48":"import seaborn as sns","34a6d2e8":"import ktrain","5c1962e8":"#df=pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\")\n#df=pd.read_csv(\"\/kaggle\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n#\/kaggle\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv\n#\/kaggle\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\n\n#\/kaggle\/input\/d\/julian3833\/jigsaw-toxic-comment-classification-challenge\/test.csv\n\ndf=pd.read_csv(\"\/kaggle\/input\/jigsaw-score-augumentation\/jigsaw_train_hate_annotationprob.csv\",index_col=0)","5143ce27":"df.columns","a067fd56":"df_valid=pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\",index_col=0)","61b8f7c2":"comments_to_score=pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\",index_col=0)","417153cf":"#undersampled_df[undersampled_df[\"toxic\"] >0].head(20)","bb5b429e":"# # #df['score']=2*df['toxic']+4*df['severe_toxic']+df['obscene']+4*df['threat']+df['insult']+4*df['identity_hate']+df['predicted_score']\n# # #df['score']=df['toxic']+2*df['severe_toxic']+df['obscene']+df['threat']+df['insult']+df['identity_hate']+0.5*df['predicted_score']\n# # #df['score']=df['toxic']+2*df['severe_toxic']+df['obscene']+df['threat']+df['insult']+df['identity_hate']\n# # df['score']=0.32*df['toxic']+1.5*df['severe_toxic']+0.16*df['obscene']+1.5*df['threat']+0.64*df['insult']+1.5*df['identity_hate']\n# # #df['score']=2*(df['toxic']+2*df['severe_toxic']+0.5*df['obscene']+df['threat']+0.5*df['insult']+df['identity_hate'])+df['predicted_score']\n# # df['score']=1*df['toxic']+3*df['severe_toxic']+1.5*df['obscene']+6*df['threat']+2*df['insult']+1.2*df['identity_hate']\n# # df['score']=\n# undersampled_df['predicted_score_v1']=0.32*undersampled_df['toxic']+1.5*undersampled_df['severe_toxic']+0.16*undersampled_df['obscene']+1.5*undersampled_df['threat']+0.64*undersampled_df['insult']+1.5*undersampled_df['identity_hate']\n# #ndersampled_df['predicted_score_v2']=undersampled_df['toxic']+2*undersampled_df['severe_toxic']+undersampled_df['obscene']+undersampled_df['threat']+undersampled_df['insult']+undersampled_df['identity_hate']\n# undersampled_df['predicted_score_v3']=1*undersampled_df['toxic']+3*undersampled_df['severe_toxic']+1.5*undersampled_df['obscene']+2*undersampled_df['threat']+0.5*undersampled_df['insult']+1.2*df['identity_hate']\n# #ver1\n# #undersampled_df['predicted_score_v1']=(undersampled_df[\"toxicity_sum\"]+0.2)*undersampled_df[\"offensive_agreement_rating\"]+undersampled_df[\"hate_speech_agreement\"]*undersampled_df[\"numeric_class\"]\n# #ver2\n# undersampled_df['predicted_score_v2']=undersampled_df[\"proposed_score3\"]+undersampled_df[\"offensive_agreement_rating\"]*undersampled_df['numeric_class']+4*undersampled_df['hate_speech_agreement']*undersampled_df['numeric_class']\n# #ver3 if i'm right the score does not count accepted hate speech definitions so the score will be without hate speech\n# undersampled_df['predicted_score']=undersampled_df[\"proposed_score3\"]+4*undersampled_df[\"hate_speech_agreement\"]*undersampled_df['numeric_class']+undersampled_df[\"proposed_score2\"]\n# #ver4 if i'm right\n# #undersampled_df['predicted_score_v3']=(undersampled_df[\"proposed_score3\"]-1.5*undersampled_df[\"severe_toxic\"])+4*undersampled_df[\"hate_speech_agreement\"]*undersampled_df['numeric_class']","443be55b":"comments_to_score.head()","26364b96":"import shutil","ded8eed5":"import os","b3b9ebb7":"mkdir -p ..\/working\/models\/toxic","1dfbc38f":"shutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/toxic\/tf_model.h5\", \"..\/working\/models\/toxic\/tf_model.h5\")","701399b2":"shutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/toxic\/tf_model.preproc\", \"..\/working\/models\/toxic\/tf_model.preproc\")","a015b4fc":"mkdir -p ..\/working\/models\/hate\n","7879140c":"mkdir -p ..\/working\/models\/neutral\n","fb368fa0":"mkdir -p ..\/working\/models\/offensive","53dd18cd":"shutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/hate\/tf_model.preproc\", \"..\/working\/models\/hate\/tf_model.preproc\")\nshutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/hate\/tf_model.h5\", \"..\/working\/models\/hate\/tf_model.h5\")\nshutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/neutral\/tf_model.preproc\", \"..\/working\/models\/neutral\/tf_model.preproc\")\nshutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/neutral\/tf_model.h5\", \"..\/working\/models\/neutral\/tf_model.h5\")\nshutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/offensive\/tf_model.preproc\", \"..\/working\/models\/offensive\/tf_model.preproc\")\nshutil.copy(\"..\/input\/models-jigsaw-score-augumentation\/models\/offensive\/tf_model.h5\", \"..\/working\/models\/offensive\/tf_model.h5\")","e5f7bde4":"predictor = ktrain.load_predictor('..\/working\/models\/toxic')\n","159cb042":"predictor.predict_proba(\"text\")","32f155ad":"probabilitati_toxicitate = predictor.predict_proba(comments_to_score.text.values)","dc42660b":"len(probabilitati_toxicitate)","5ed4b4b5":"#del comments_to_score","f80ebb75":"comments_to_score.head()","79144e3b":"len(comments_to_score)","cbd8b1a4":"comments_to_score['probabilitati']=probabilitati_toxicitate.tolist()","50c6c055":"scores=pd.DataFrame(comments_to_score['probabilitati'].to_list(), columns=[ 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate'])","8faf1170":"scores.head()","f8673aef":"# comments_to_score['toxic']=scores['toxic'].values\n# comments_to_score['severe_toxic']=scores['severe_toxic'].values\n# comments_to_score['obscene']=scores['obscene'].values\n# comments_to_score['threat']=scores['threat'].values\n# comments_to_score['insult']=scores['insult'].values\n# comments_to_score['identity_hate']=scores['identity_hate'].values\ncolnames=scores.columns\nfor col in colnames:\n    comments_to_score[col]=scores[col].values\n    #print(rank)","881ba81f":"predictor_offensive = ktrain.load_predictor('..\/working\/models\/offensive')","b9543020":"predictor_offensive.predict_proba(\"text\")","3a0da353":"probabilitati_ofensive = predictor_offensive.predict_proba(comments_to_score.text.values)","16c27f18":"comments_to_score['ofensivitate']=probabilitati_ofensive.tolist()","e733bff9":"ofensiveness=pd.DataFrame(comments_to_score['ofensivitate'].to_list(), columns=[ 'ofensivitate'])","6713df8e":"comments_to_score['ofensivitate']=ofensiveness.values","eb49117e":"predictor_hate = ktrain.load_predictor('..\/working\/models\/hate')\npredictor_hate.predict_proba(\"text\")\nprobabilitati_hate = predictor_hate.predict_proba(comments_to_score.text.values)\ncomments_to_score['hate']=probabilitati_hate.tolist()\nhateeness=pd.DataFrame(comments_to_score['hate'].to_list(), columns=[ 'ofensivitate'])\ncomments_to_score['hate']=hateeness.values","a9abd6e9":"#comments_to_score['hate']=hateeness.values","f4245e6d":"predictor_neutral = ktrain.load_predictor('..\/working\/models\/neutral')\npredictor_neutral.predict_proba(\"text\")\nprobabilitati_neutral = predictor_neutral.predict_proba(comments_to_score.text.values)\ncomments_to_score['neutral']=probabilitati_neutral.tolist()\nneutralitate=pd.DataFrame(comments_to_score['neutral'].to_list(), columns=[ 'neutralitate'])\ncomments_to_score['neutral']=neutralitate.values","e3fdec3c":"#comments_to_score['neutral']=neutralitate.values","e4a5fe5c":"comments_to_score.head(20)","9084e5ee":"comments_to_score.describe()","0f1ce7ca":"ax=sns.distplot(comments_to_score.hate)","d69be4a9":"ax=sns.distplot(comments_to_score.ofensivitate)","14182491":"ax=sns.distplot(1-comments_to_score.neutral)","f4d9f7e2":"ax=sns.distplot(comments_to_score.neutral)","30dc8f29":"def cutoff(x):\n    if x>0.5:\n        return x\n    else:\n        return 0","172565f2":"# cut_df=comments_to_score","4804fdfd":"# cut_df.hate=cut_df.hate.apply(cutoff)\n# cut_df.ofensivitate=cut_df.ofensivitate.apply(cutoff)\n# cut_df.neutral=cut_df.neutral.apply(cutoff)\n# #df['col1'] = df['col1'].apply(complex_function)","1a9f3f3a":"# cut_df.describe()","d214b57c":"# cut_df.head()","cfaf769b":"# # df['score']=0.32*df['toxic']+1.5*df['severe_toxic']+0.16*df['obscene']+1.5*df['threat']+0.64*df['insult']+1.5*df['identity_hate']","6aa27327":"comments_to_score['score']=-1*comments_to_score['neutral']+0*comments_to_score['toxic']+0*comments_to_score['severe_toxic']+0.16*comments_to_score['obscene']+0.32*comments_to_score['threat']+1*comments_to_score['insult']+0.76*comments_to_score['identity_hate']+1*comments_to_score['ofensivitate']+1*comments_to_score['hate']\n","8e7d747f":"ax=sns.distplot(comments_to_score.score)","3b2091f2":"submission=comments_to_score[['score']]\nsubmission.to_csv('submission.csv')\nsubmission.head()","86d00ea4":"ax=sns.distplot(comments_to_score.score)","6dee699c":"# bins = [0, 0.15, 0.45, 0.70, 1]\n# labels = [0,0.33,0.66,1]\nbins = [-1,0, 0.15, 0.45, 0.70, 1,2]\nlabels = [0.00001,0,0.33,0.66,1,1.5]\ncomments_to_score['IAA_ofensivitate'] = pd.cut(comments_to_score['ofensivitate'], bins=bins, labels=labels)\n\ncomments_to_score['IAA_hate'] = pd.cut(comments_to_score['hate'], bins=bins, labels=labels)\n\ncomments_to_score['IAA_neutral'] = pd.cut(comments_to_score['neutral'], bins=bins, labels=labels)","f4601181":"comments_to_score.head(10)","04f34f8c":"comments_to_score.tail(10)","76e8093d":"comments_to_score['text'][504143017]","8fa2dc32":"comments_to_score['text'][504189084]","3407b545":"comments_to_score['text'][504145199]","a9518f57":"In singur test si ma opresc :)","5c49bd83":"Proposed score","93fb2dd9":"dont't do like this :) for countless milion reasons.","689f74d5":"Toxic classifications preds","6d934dc9":"Load comments to score","0e543ff6":"Samplig for testing purpose\n-to be commenteted for submission","e6c45762":"create models directories","ab44e184":"Load training dataset","5db6ddde":"!pip install facenet-pytorch --no-index --find-links=file:\/\/\/kaggle\/input\/facenet_pytorch\/ ","38812745":"Workaround for importing saved models from input to a directory","bc60e668":"https:\/\/nbviewer.org\/github\/amaiya\/ktrain\/blob\/master\/tutorials\/tutorial-A3-hugging_face_transformers.ipynb","9d0cf041":"Load evaluation dataset","bcc63584":"INTER-ANNOTATOR AGREEMENT\nplease note that for \"neutrality\" the original dataset is poor in samples","4100b2bf":"Predicting chaos","f3a173f2":"Offensive"}}