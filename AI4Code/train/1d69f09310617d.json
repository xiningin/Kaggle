{"cell_type":{"613e5778":"code","09228f32":"code","cfbb0ad5":"code","d7df6a8b":"code","5aae9215":"code","97680179":"code","b6021034":"code","8b4b95c8":"code","63031d27":"code","09971262":"code","f410296f":"code","ad3e9c0c":"code","a8d0df47":"code","147c0c03":"code","11ac44de":"code","8edf8a92":"code","c1a830f8":"code","476cbe52":"code","339e290a":"code","4f07c997":"code","345dc228":"code","c236943e":"code","8515dca2":"code","148e142b":"code","1d6a1276":"code","30952e5e":"code","23abdc9d":"code","3a5f9ed8":"code","7200a302":"code","76d614d5":"code","078bc5bb":"code","c17a52bc":"code","8aa6b323":"code","f84abcce":"code","3a1d7cb0":"code","0054f509":"code","365e0792":"code","f6226f1e":"code","d72383f3":"code","6045317c":"code","63636332":"code","a44628d7":"code","0be9d6d5":"code","d42ec5a5":"code","d22a3da7":"code","b3d5bf96":"code","3108b612":"code","cb82c712":"code","f1d7444f":"code","f45b1aeb":"code","b90a3492":"code","b56a22ce":"code","da8c1158":"code","98508c78":"code","fe99b259":"code","20f22f3f":"code","fc7dff78":"code","73b506c6":"code","72b3973b":"code","aaf0b275":"code","775887dc":"code","e2b4ffdc":"code","d68d0562":"code","07b41ed1":"code","5c8b8397":"code","46854263":"code","b7614b9d":"code","9390f4e7":"code","ab898191":"code","1959befe":"code","177eb6b4":"code","0055eb0e":"code","e81b9a9e":"code","67586529":"code","a799b8a2":"code","1973d53f":"code","03591512":"code","2401e757":"code","0b1d2d6d":"code","3486b28c":"code","81ebbc06":"code","bcd2e853":"code","0427310a":"code","d790fa67":"code","32cc206c":"code","15354faf":"markdown","b0c2648e":"markdown","577f0ff2":"markdown","fd9dda16":"markdown","c573f1d3":"markdown","e87583a1":"markdown","f07fb7c6":"markdown","ed4fd88a":"markdown","2c14ea42":"markdown","f3dbf27b":"markdown","3fe01a08":"markdown","71347555":"markdown","083aa682":"markdown","60db423c":"markdown","f2c57e14":"markdown","08dbd845":"markdown","aa15a3c8":"markdown","e4e482cc":"markdown","af4334d1":"markdown","c85ed10e":"markdown","4426485e":"markdown","a8349ecb":"markdown","5ea7f61e":"markdown","62136fa3":"markdown","1e165936":"markdown","0c703ba1":"markdown","f0432317":"markdown","a137828b":"markdown","764623f0":"markdown","9956ce30":"markdown","fcdf8708":"markdown","25b61010":"markdown","40c51e2d":"markdown","a4e046c3":"markdown","79e92fbc":"markdown","75a26d80":"markdown","8bf2115f":"markdown","06875d12":"markdown","1a01378c":"markdown","1f917ed1":"markdown","5a8f899a":"markdown","2adbd469":"markdown","fa8b40f2":"markdown","a11236b0":"markdown","dec38ac1":"markdown","a73c7570":"markdown","acb5fc7f":"markdown","aa93138a":"markdown","818e36e5":"markdown","0ad6b5d2":"markdown","8cf5dd16":"markdown","4ecd4bd3":"markdown"},"source":{"613e5778":"# Standard imports\nimport pandas as pd\nimport numpy as np\n\n# Visualization tools\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom imblearn.over_sampling import SMOTE\n\n# Modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, classification_report\n\n# Miscellaneous\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')","09228f32":"# Setting the default styling attributes for seaborn\nsns.set_theme(style='darkgrid')","cfbb0ad5":"# Loading in the dataset\ndf = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')","d7df6a8b":"df.head()","5aae9215":"df.columns","97680179":"df.info()","b6021034":"round(df.isna().sum() \/ len(df), 3)","8b4b95c8":"df.describe()","63031d27":"for col in df.columns:\n    print('\\n')\n    print(col)\n    print('-'*15)\n    print(df[col].value_counts(normalize=True).head())","09971262":"df.Date.value_counts()","f410296f":"df.Location.nunique()","ad3e9c0c":"df.RainToday = df.RainToday.map({'No': 0, 'Yes': 1})\ndf.RainToday.value_counts(normalize=True)","a8d0df47":"df.RainTomorrow = df.RainTomorrow.map({'No': 0, 'Yes': 1})\ndf.RainTomorrow.value_counts(normalize=True)","147c0c03":"fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(12, 18))\naxes = axes.reshape(-1)\n\ncontinuous = [col for col in df.columns if df[col].dtype != object]\nfor i, col in enumerate(continuous):\n    sns.histplot(df[col], ax=axes[i])\n\nfig.tight_layout(pad=2.0)\nplt.title('Histograms of Columns');","11ac44de":"sns.histplot(df.Rainfall)\nplt.xlim(0, 1);","8edf8a92":"df_rain_by_loc = df.groupby(by='Location').sum()\ndf_rain_by_loc = df_rain_by_loc[['RainToday']]\ndf_rain_by_loc.head()","c1a830f8":"plt.figure(figsize=(8, 12))\nsns.barplot(x='RainToday', \n            y=df_rain_by_loc.index, \n            data=df_rain_by_loc.sort_values('RainToday', ascending=False), \n            orient='h',\n            palette='crest'\n           )\nplt.xlabel('Number of Days')\nplt.title('Rainy Days by Location')\nplt.tight_layout();","476cbe52":"df.Location.value_counts()","339e290a":"df_seasonality = df.copy()\ndf_seasonality['month'] = df_seasonality.Date.apply(lambda x: int(str(x)[5:7]))\ndf_seasonality[['Date', 'month']].head()","4f07c997":"df_seasonality_grouped = df_seasonality.groupby('month').mean()\ndf_seasonality_grouped[['RainToday']]","345dc228":"sns.lineplot(data=df_seasonality_grouped, x=df_seasonality_grouped.index, y='RainToday')\nplt.title('Seasonality of Rainfall')\nplt.xlabel('Month of the Year')\nplt.ylabel('Percentage of Days it Rains')\nplt.tight_layout();","c236943e":"plt.figure(figsize=(14, 14))\nplt.title('Correlation Matrix')\n\n# Creating a mask to block the top right half of the heatmap (redundant information)\nmask = np.triu(np.ones_like(df.corr()))\n\n# Custom color map\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(df.corr(), mask=mask, annot=True, fmt='.2f', square=True, cmap=cmap)\nplt.tight_layout();","8515dca2":"df_imputed = df.dropna(axis=0, subset=['RainTomorrow'])\ndf_imputed.isna().sum()","148e142b":"cont_feats = [col for col in df_imputed.columns if df_imputed[col].dtype != object]\ncont_feats.remove('RainTomorrow')\ncont_feats","1d6a1276":"imputer = IterativeImputer(random_state=42)\ndf_imputed_cont = imputer.fit_transform(df_imputed[cont_feats])\ndf_imputed_cont = pd.DataFrame(df_imputed_cont, columns=cont_feats)\ndf_imputed_cont.head()","30952e5e":"df_imputed_cont.isna().sum()","23abdc9d":"cat_feats = [col for col in df_imputed.columns if col not in cont_feats]\ncat_feats.remove('RainTomorrow')\n\n# Also removing Date and Location since no values are missing\ncat_feats.remove('Date')\ncat_feats.remove('Location')\ncat_feats","3a5f9ed8":"df_imputed_cat = df_imputed[cat_feats]\n\nfor col in df_imputed_cat.columns:\n    values = df_imputed_cat.WindDir3pm.value_counts().reset_index()['index'].values\n    probs = df_imputed_cat[col].value_counts(normalize=True).values\n    df_imputed_cat[col].replace(np.nan, np.random.choice(a=values, p=probs), inplace=True)\n\ndf_imputed_cat.head()","7200a302":"df_imputed_cat.isna().sum()","76d614d5":"df_date_loc = df_imputed[['Date', 'Location']]\ndf_target = df_imputed.RainTomorrow\n\nprint(df_date_loc.shape)\nprint(df_imputed_cont.shape)\nprint(df_imputed_cat.shape)\nprint(df_target.shape)","078bc5bb":"df_imputed_final = pd.concat(objs=[df_date_loc.reset_index(drop=True), \n                                   df_imputed_cont.reset_index(drop=True), \n                                   df_imputed_cat.reset_index(drop=True), \n                                   df_target.reset_index(drop=True)\n                                  ], \n                             axis=1\n                            )\ndf_imputed_final.shape","c17a52bc":"df_imputed_final.head()","8aa6b323":"df_imputed_final.isna().sum()","f84abcce":"df_month = df_imputed_final.copy()\ndf_month.insert(1, 'Month', df_month.Date.apply(lambda x: int(str(x)[5:7])))\ndf_month.drop(columns='Date', inplace=True)\ndf_month.head()","3a1d7cb0":"categoricals = ['Month', 'Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm']\ndf_dummies = pd.get_dummies(df_month, columns=categoricals)\ndf_dummies.head()","0054f509":"df_dummies.columns","365e0792":"df_final = df_dummies.copy()\nX = df_final.drop(columns='RainTomorrow')\ny = df_final.RainTomorrow\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nprint('Train size:', X_train.shape[0])\nprint('Test size: ', X_test.shape[0])","f6226f1e":"logreg = LogisticRegression(solver='liblinear', random_state=42)\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred","d72383f3":"def conf_matrix(model, X_test, y_test, cmap='Blues'):\n    plot_confusion_matrix(model, X_test, y_test, cmap=cmap)\n    plt.grid()\n    plt.show()\n\ndef roc_curve_custom(model, X_test, y_test):\n    plot_roc_curve(model, X_test, y_test)\n    plt.plot([0, 1], [0, 1], color='black', linestyle='--')\n    plt.show()\n    \ndef evaluate(model, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, y_pred=y_pred):\n    # Confusion Matrix\n    print('Confusion Matrix')\n    print('-'*53)\n    conf_matrix(model, X_test, y_test)\n    print('\\n') \n    \n    # Classification Report\n    print('Classification Report') \n    print('-'*53)\n    print(classification_report(y_test, y_pred))\n    print('\\n')\n    \n    # ROC Curve\n    print('ROC Curve')\n    print('-'*53)\n    roc_curve_custom(model, X_test, y_test)\n    print('\\n')\n    \n    # Checking model fitness\n    print('Checking model fitness') \n    print('-'*53)\n    print('Train score:', round(model.score(X_train, y_train), 4))\n    print('Test score: ', round(model.score(X_test, y_test), 4))\n    print('\\n')\n    \nevaluate(logreg)","6045317c":"X_train_resampled, y_train_resampled = SMOTE().fit_resample(X_train, y_train)\n\nprint('Original')\nprint('-'*20)\nprint(y_train.value_counts())\nprint('\\n')\nprint('SMOTE')\nprint('-'*20)\nprint(pd.Series(y_train_resampled).value_counts())","63636332":"logreg_smote = LogisticRegression(solver='liblinear', random_state=42)\nlogreg_smote.fit(X_train_resampled, y_train_resampled)\ny_pred_smote = logreg_smote.predict(X_test)\ny_pred_smote","a44628d7":"evaluate(logreg_smote, X_train=X_train_resampled, y_train=y_train_resampled, y_pred=y_pred_smote)","0be9d6d5":"logreg_params = {\n    'C': [1, 1e8, 1e16],\n    'fit_intercept': [True, False],\n    'max_iter': [50, 100, 150],\n    'random_state': [42]\n}\n\nlogreg_gs = GridSearchCV(logreg, logreg_params, scoring='accuracy', n_jobs=-1, cv=3)\nlogreg_gs.fit(X_train, y_train)","d42ec5a5":"import os\nos.mkdir('saved_models')","d22a3da7":"ls","b3d5bf96":"joblib.dump(logreg_gs, 'saved_models\/logreg_gs.joblib')","3108b612":"logreg_gs = joblib.load('saved_models\/logreg_gs.joblib')","cb82c712":"logreg_gs.best_params_","f1d7444f":"round(logreg_gs.best_score_, 4)","f45b1aeb":"y_pred_logreg_gs = logreg_gs.predict(X_test)\ny_pred_logreg_gs","b90a3492":"evaluate(logreg_gs, y_pred=y_pred_logreg_gs)","b56a22ce":"clf = DecisionTreeClassifier(random_state=42)\nclf.fit(X_train, y_train)\ny_pred_tree = clf.predict(X_test)\ny_pred_tree","da8c1158":"evaluate(clf, y_pred=y_pred_tree)","98508c78":"params = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 7, 11],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 3, 5],\n    'random_state': [42]\n}\n\nclf_gs = GridSearchCV(clf, param_grid=params, scoring='accuracy', n_jobs=-1, cv=3)\nclf_gs.fit(X_train, y_train)","fe99b259":"joblib.dump(clf_gs, 'saved_models\/clf_gs.joblib')","20f22f3f":"clf_gs = joblib.load('saved_models\/clf_gs.joblib')","fc7dff78":"clf_gs.best_params_","73b506c6":"round(clf_gs.best_score_, 4)","72b3973b":"y_pred_tree_gs = clf_gs.best_estimator_.predict(X_test)\ny_pred_tree_gs","aaf0b275":"evaluate(clf_gs.best_estimator_, y_pred=y_pred_tree_gs)","775887dc":"rf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\ny_pred_rf","e2b4ffdc":"evaluate(rf, y_pred=y_pred_rf)","d68d0562":"rf_params = {\n    'n_estimators': [10, 35, 100],\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [3, 7, 11],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 3, 5],\n    'random_state': [42]\n}\n\nrf_gs = GridSearchCV(rf, param_grid=rf_params, scoring='accuracy', n_jobs=-1, cv=3)\nrf_gs.fit(X_train, y_train)","07b41ed1":"joblib.dump(rf_gs, 'saved_models\/rf_gs.joblib')","5c8b8397":"rf_gs = joblib.load('saved_models\/rf_gs.joblib')","46854263":"rf_gs.best_params_","b7614b9d":"round(rf_gs.best_score_, 4)","9390f4e7":"y_pred_rf_gs = rf_gs.predict(X_test)\ny_pred_rf_gs","ab898191":"evaluate(rf_gs, y_pred=y_pred_rf_gs)","1959befe":"xgb = XGBClassifier(random_state=42)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\ny_pred_xgb","177eb6b4":"evaluate(xgb, y_pred=y_pred_xgb)","0055eb0e":"xgb_params = {\n    'n_estimators': [10, 35, 100],\n    'max_depth': [5, 10, 15],\n    'learning_rate': [0.01, 0.1, 0.25]\n}\n\nxgb_gs = GridSearchCV(xgb, xgb_params, scoring='accuracy', n_jobs=-1, cv=3)\nxgb_gs.fit(X_train, y_train)","e81b9a9e":"joblib.dump(xgb_gs, 'saved_models\/xgb_gs.joblib')","67586529":"xgb_gs = joblib.load('saved_models\/xgb_gs.joblib')","a799b8a2":"xgb_gs.best_params_","1973d53f":"round(xgb_gs.best_score_, 4)","03591512":"y_pred_xgb_gs = xgb_gs.predict(X_test)\ny_pred_xgb_gs","2401e757":"evaluate(xgb_gs, y_pred=y_pred_xgb_gs)","0b1d2d6d":"best_xgb = xgb_gs.best_estimator_\n\nplt.figure(figsize=(8, 25))\nplt.barh(range(best_xgb.n_features_in_), best_xgb.feature_importances_)\nplt.yticks(np.arange(best_xgb.n_features_in_), X_train.columns.values)\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.title('Feature Importances of the XGBoost Model');","3486b28c":"feat_imp_df = pd.DataFrame(data={'Feature': df_final.columns.drop('RainTomorrow'),\n                                 'Importance': best_xgb.feature_importances_\n                                }\n                           )\nfeat_imp_df['Group'] = feat_imp_df.Feature.apply(lambda x: x.split('_')[0])\nfeat_imp_df","81ebbc06":"feat_imp_df.Group.value_counts()","bcd2e853":"feat_imp_df_grouped = feat_imp_df.groupby(by='Group').sum()\nfeat_imp_df_grouped.sort_values('Importance', ascending=False, inplace=True)\nfeat_imp_df_grouped","0427310a":"plt.figure(figsize=(7, 8))\nsns.barplot(y=feat_imp_df_grouped.index,\n            x=feat_imp_df_grouped.Importance,\n            orient='h',\n            color=sns.color_palette()[0]\n           )\nplt.title('Feature Importances for the XGBoost Model')\nplt.ylabel('Feature Group')\nplt.xlabel('Importance')\nplt.tight_layout();","d790fa67":"models = [logreg_gs, clf_gs, rf_gs, xgb_gs]\n\nsns.barplot(x=['Logistic\\nRegression', 'Decision\\nTree', 'Random\\nForest', 'XGBoost'],\n            y=[model.best_score_ for model in models]\n           )\nplt.xlabel('Model')\nplt.ylabel('Accuracy')\nplt.ylim(0.7, 1.0)\nplt.title('Comparison of Model Accuracies')\nplt.tight_layout();","32cc206c":"fig, ax = plt.subplots()\nfor model in models:\n    plot_roc_curve(model, \n                   X_test, \n                   y_test,\n                   name=type(model.best_estimator_).__name__,\n                   ax=ax\n                  )\nplt.plot([0, 1], [0, 1], color='black', linestyle='--')\nplt.title('Comparison of Model ROC Curves')\nplt.tight_layout();","15354faf":"**Observations:**\n- Nothing in this correlation heatmap is surprising\n- Features with strong correlations (either positive or negative) have intuitive reasons for being so","b0c2648e":"## Random Forest\n### Baseline","577f0ff2":"# Exploratory Data Analysis\n---","fd9dda16":"### Correlation Matrix","c573f1d3":"### Summary Info and Stats\nTaking a look at the dataframe info:","e87583a1":"Due to the amount of time it takes to run the grid search, I'll be using the `joblib` library to save it to a file for easy access in the future without having to rerun everything again.","f07fb7c6":"# Data Preprocessing\n---\n\n## Missing Values\nThe primary preprocessing need for this dataset is handling the missing values. Given the strong correlations between certain features, using a multivariate feature imputation method makes sense. While still experimental, the `IterativeImputer` module from `sklearn` is perfect for this use case and appears stable enough. This module...\n> \"...models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\"\n\nSource: [6.4.3. Multivariate feature imputation](https:\/\/scikit-learn.org\/stable\/modules\/impute.html#iterative-imputer)\n\nI do not want to impute values for the target variable (`RainTomorrow`) since this will detract from the ground truth and have potential negative effects on the model. To start, I'll drop rows in which the `RainTomorrow` value is missing.","ed4fd88a":"Saving the grid search to a file for easy access:","2c14ea42":"## Data Preview","f3dbf27b":"**Observations:**\n- Multiple columns have clear outliers (e.g., the max `Rainfall` value is 371.0 despite the 75th percentile being 0.8)\n- Not seeing any values that are immediate cause for concern (such as a negative value for minimum `Rainfall`)\n\nIn order to get a better feel for the data and catch any placeholder values that may not have shown up in the summary statistics, I also want to check the top five most frequent values for each column.","3fe01a08":"**Observations:**\n- The value counts of the `Date` column need further explored on a non-normalized basis\n- There's a disconnect between the `Rainfall` value counts and the `RainToday` \/ `RainTomorrow` value counts. While roughly 64% of observations had a value of 0 for `Rainfall`, about 77.5% of days did not have rainfall according to the latter two columns. This discrepency is likely due to differences in the number of missing values for each column\n- The `RainToday` and `RainTomorrow` columns should be converted to 0s and 1s for easier manipulation\n\nFurther exploring the `Date` column:","71347555":"### Hyperparameter Tuning","083aa682":"### Continuous Features\nFor the continuous features, I'll apply the `IterativeImputer`.","60db423c":"### Hyperparameter Tuning","f2c57e14":"### Hyperparameter Tuning","08dbd845":"**Observations:**\n- Highest accuracy score yet\n- Highest AUC yet\n- The model is decently fit","aa15a3c8":"Rainfall in Australia clearly has a degree of seasonality.","e4e482cc":"### Feature Importances\nSince this model achieved the best results, I want to explore the feature importances a bit more in depth.","af4334d1":"### Hyperparameter Tuning","c85ed10e":"These value counts align with the number of unique values for the categorical columns in the original dataframe (excluding `Month` which was engineered later), meaning the lambda function worked as expected.","4426485e":"**Observations:**\n- The accuracy score remained roughly the same while the F1 score decreased\n- Small increase in the AUC of the ROC curve\n- The tuned model has a much better fit than the baseline model","a8349ecb":"### Categorical Features\nFor the categorical features, I'll be replacing the missing values with a randomly chosen option from the unique values of each feature according to their probability distribution.","5ea7f61e":"# Conclusion\n---\n\n## Results\nThe best performing model is the hyperparameter-tuned XGBoost model with an accuracy of approximately 86%. The scores for both the training and testing data were similar, reducing concerns of the model being overfit. In terms of feature importances, `Humidity3pm` is the single most important feature. However, when grouping the features back into their original categories, the following groups have the most importance:\n- `Location`\n- `WindDir3pm`\n- `WindDir9am`\n- `WindGustDir`\n- `Month`\n- `Humidity3pm`\n\n## Next Steps\nWhile this model is a good starting point for rain prediction in Australia, there are several ways in which the model could be improved upon:\n- Further hyperparameter tuning\n- Engineering new features such as trailing amounts of rain or sunshine\n- Collecting additional data from nearby countries (for example, does rain originating in Indonesia or New Zealand have predictive power?)\n- Attempting to predict the *amount* of rainfall","62136fa3":"### Histograms","1e165936":"**Observations:**\n- Slight improvement in some metrics but largely the same\n- AUC remains the same\n- Model fitness slightly decreased\n- Overall, not much of an impact","0c703ba1":"### Seasonality\nRainfall exhibits seasonality in many areas of the world. Through grouping the data by month of the year, the percentage of days that it rains in a given month can be easily calculated. Any sort of trend would indicate that the month of the year is a valuable piece of information for modeling purposes.","f0432317":"**Observations:**\n- Slight improvements in precision and model fitness\n- Overall, not much improvement over the baseline logreg model","a137828b":"## Decision Tree\n### Baseline","764623f0":"The above chart is useful for a quick check on the differences between locations with regard to the number of rainy days but suffers from one key issue: the number of observations from each location is not exactly the same. Checking the value counts for each location (below) reveals that the locations of `Katherine`, `Nhil`, and `Uluru` should be ignored when analyzing the above plot. The remaining locations have value counts that are close enough to be properly comparable.","9956ce30":"### Correcting Class Imbalance\nA class imbalance currently exists for the target variable. Correcting for this may help improve model performance. To do so, I will resample the training data using `SMOTE`.","fcdf8708":"**Observations:**\n- Most features are normally distributed as expected\n- The `Rainfall` distribution needs further investigation as the large outlier is likely affecting the ability to plot the data\n- The `Sunshine` distribution is interesting but largely explainable:\n    - The high frequency of 0 values represents days where it is overcast all day\n    - The abrupt decline in frequency after around 11 hours is a reflection of the limited number of days of the year where it is light out for that many hours or longer\n- The `Humidity9am` distribution is particularly interesting due to the large spike in frequencies near 100%\n\nSince the summary statistics section showed that the 75th percentile for the `Rainfall` feature is only 0.8, the following plot shows the distribution of values betwen 0 and 1.","25b61010":"**Observations:**\n- `Evaporation`, `Sunshine`, `Cloud9am`, and `Cloud3pm` are all missing more than 35% of their values\n- Aside from `Date` and `Location`, all columns are missing at least some values\n- These missing values can be handled by either dropping certain columns\/rows, imputing the values, or a mix of both\n\nNext, taking a look at some summary statistics:","40c51e2d":"A quick check to ensure all missing values have been handled:","a4e046c3":"The maximum number of observations for a given date aligns with the number of unique locations within the dataset. This intuitively makes sense because each weather station at the different locations would be reporting their own data for a given day.\n\nAdjusting the `RainToday` and `RainTomorrow` columns:","79e92fbc":"# Modeling\n---","75a26d80":"### Rainy Days by Location","8bf2115f":"Although the dummy variables were necessary for modeling the data, they are not conducive to analyzing the feature importances. As a result, I need to regroup the data into their primary categories to aggregate their category-level importances.","06875d12":"## Dummy Variables\nAll categorical features now need transformed into dummy variables in order to be useable in the modeling section.","1a01378c":"## XGBoost\n### Baseline","1f917ed1":"## Extracting the Month\nAs seen in the EDA section, rainfall in Australia exhibits seasonality. Instead of using the full date from the `Date` column, extracting just the month is much more valuable.","5a8f899a":"**Observations:**\n- Good scores on the evaluation metrics\n- The model is a bit overfit","2adbd469":"## Column Definitions\nAccording to the author of the Kaggle dataset and the [\"Notes to accompany Daily Weather Observations\"](http:\/\/www.bom.gov.au\/climate\/dwo\/IDCJDW0000.shtml) published by the Australian Bureau of Meteorology, the meanings and units for each of the columns in the dataset are as follows:\n\n| **Column Name** | **Definition** | **Units** |\n| --------------- | -------------- | --------- |\n| `Date` | Date of the observation | N\/A |\n| `Location` | Location of the weather station | N\/A |\n| `MinTemp` | Minimum temperature in the 24 hours to 9am. Sometimes only known to the nearest whole degree | Degrees Celsius |\n| `MaxTemp` | Maximum temperature in the 24 hours to 9am. Sometimes only known to the nearest whole degree | Degrees Celsius |\n| `Rainfall` | Precipitation (rainfall) in the 24 hours to 9am. Sometimes only known to the nearest whole millimeter | Millimeters |\n| `Evaporation` | \"Class A\" pan evaporation in the 24 hours to 9am | Millimeters |\n| `Sunshine` | Bright sunshine in the 24 hours to midnight | Hours |\n| `WindGustDir` | Direction of the strongest wind gust in the 24 hours to midnight | 16 compass points |\n| `WindGustSpeed` | Speed of the strongest wind gust in the 24 hours to midnight | Kilometers per hour |\n| `WindDir9am` | Direction of the wind at 9am | 16 compass points |\n| `WindDir3pm` | Direction of the wind at 3pm | 16 compass points |\n| `WindSpeed9am` | Speed of the wind at 9am | Kilometers per hour |\n| `WindSpeed3pm` | Speed of the wind at 3pm | Kilometers per hour |\n| `Humidity9am` | Relative humidity at 9am | Percent |\n| `Humidity3pm` | Relative humidity at 3pm | Percent |\n| `Pressure9am` | Atmospheric pressure reduced to mean sea level at 9am | Hectopascals |\n| `Pressure3pm` | Atmospheric pressure reduced to mean sea level at 3pm | Hectopascals |\n| `Cloud9am` | Fraction of sky obscured by cloud at 9am | Eighths |\n| `Cloud3pm` | Fraction of sky obscured by cloud at 3pm | Eighths |\n| `Temp9am` | Temparature at 9am | Degrees Celsius |\n| `Temp3pm` | Temparature at 3am | Degrees Celsius |\n| `RainToday` | Did the current day receive precipitation exceeding 1mm in the 24 hours to 9am | Binary (0 = No, 1 = Yes) |\n| `RainTomorrow` | Did the next day receive precipitation exceeding 1mm in the 24 hours to 9am | Binary (0 = No, 1 = Yes) |","fa8b40f2":"**Observations:**\n- Despite a slight increase in the positive F1 score, the accuracy of this model sharply decreased\n- This model remains well fit but scores for both the train and test sets decreased\n- Contrary to my initial thoughts, using `SMOTE` actually had worse performance and will not be utilized in subsequent iterations","a11236b0":"## Model Comparisons","dec38ac1":"**Observations:**\n- The accuracy is lower than the tuned logisitic regression model\n- The model is overfit, given by the much higher score for the train data versus the test data","a73c7570":"## Logistic Regression\n### Baseline","acb5fc7f":"**Observations:**\n- Solid increases in the evaluation metrics\n- The tuned model is much better fit than the baseline model which showed overfitness","aa93138a":"**Observations:**\n- Decent performance for a baseline model\n- Recall is the weakest point, particularly for days where it *does* rain tomorrow\n- The model is well fit, with both the train and test scores approximately the same","818e36e5":"# Imports & Settings\n---","0ad6b5d2":"**Observations:**\n- The `Date` column needs converted to a datetime datatype\n- The datatypes for all other columns look good as is\n- There appears to be a large number of missing values across multiple columns\n\nLooking into the number of missing values per column as a percentage:","8cf5dd16":"### Concatenating\nNow that the missing values have been handled, I need to place all of the separated dataframes back together into one final dataframe.","4ecd4bd3":"## Exploration"}}