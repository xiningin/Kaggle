{"cell_type":{"289a54c1":"code","6cdcfaec":"code","c4f6d519":"code","04c4d0f2":"code","ee4c6334":"code","3885b364":"code","4384d04f":"code","207e1864":"code","96dea4b3":"code","97adec18":"code","bacc5bfb":"code","124d5537":"code","d15f91ec":"code","7915bae1":"code","48e60105":"code","5072b566":"code","61184b07":"code","36a75ff1":"code","456cc492":"code","d623c26f":"code","5029e875":"code","8f199602":"code","2ab930cd":"code","a45758c5":"code","f27a434e":"code","c7400552":"code","6cf1bf31":"code","c1f74c45":"code","83aec8c1":"code","5ef4744c":"code","a2a1f01e":"code","fd2b63cf":"code","22208ccf":"code","06522c25":"code","7092dc08":"code","6addb193":"code","b6c27b6a":"code","9a136bd8":"code","f92d54a5":"code","45369b41":"code","0f1e629f":"code","03e84de9":"markdown","30fee16d":"markdown","47a4391a":"markdown","4872b1fd":"markdown","8758b7b5":"markdown","1fa83619":"markdown","cbeb3722":"markdown","4af4c5f7":"markdown","f147a838":"markdown","e3f9644c":"markdown","f29638ae":"markdown","85892a06":"markdown","b3238c0e":"markdown","9a8ee3b7":"markdown","f24a13fb":"markdown","9bbbb7cf":"markdown","9565d2b2":"markdown","6fb12cad":"markdown","d418d619":"markdown","76840d4e":"markdown","9ec312eb":"markdown"},"source":{"289a54c1":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport random\nimport re\nfrom PIL import Image\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import *\nimport os\nimport sys\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import *\n\n\nfrom tensorflow.keras.applications.resnet50 import *\nfrom tensorflow.keras.models import *\n\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Cropping2D, Conv2D\nfrom tensorflow.keras.layers import Input, Add, Dropout, Permute, add\nfrom tensorflow.compat.v1.layers import conv2d_transpose\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","6cdcfaec":"print(\"Is there a GPU available: \"),\nprint(tf.test.is_gpu_available())\n","c4f6d519":"!wget http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamSeq01\/CamSeq01.zip","04c4d0f2":"cwd = os.getcwd()\ncwd","ee4c6334":"!mkdir data\n!mkdir data\/CamSeq01","3885b364":"!unzip CamSeq01.zip -d data\/CamSeq01","4384d04f":"def _read_to_tensor(fname, output_height=384, output_width=384, normalize_data=False):\n    '''Function to read images from given image file path, and provide resized images as tensors\n        Inputs: \n            fname - image file path\n            output_height - required output image height\n            output_width - required output image width\n            normalize_data - if True, normalize data to be centered around 0 (mean 0, range 0 to 1)\n        Output: Processed image tensors\n    '''\n    \n    # Read the image as a tensor\n    img_strings = tf.io.read_file(fname)\n    imgs_decoded = tf.image.decode_jpeg(img_strings)\n    \n    # Resize the image\n    output = tf.image.resize(imgs_decoded, [output_height, output_width])\n    \n    # Normalize if required\n    if normalize_data:\n        output = (output - 128) \/ 128\n    return output","207e1864":"img_dir = 'data\/CamSeq01\/'\n\n# Required image dimensions\noutput_height = 384\noutput_width = 384","96dea4b3":"def read_images(img_dir):\n    '''Function to get all image directories, read images and masks in separate tensors\n        Inputs: \n            img_dir - file directory\n        Outputs \n            frame_tensors, masks_tensors, frame files list, mask files list\n    '''\n    \n    # Get the file names list from provided directory\n    file_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n    \n    # Separate frame and mask files lists, exclude unnecessary files\n    frames_list = [file for file in file_list if ('_L' not in file) and ('txt' not in file)]\n    masks_list = [file for file in file_list if ('_L' in file) and ('txt' not in file)]\n    \n    frames_list.sort()\n    masks_list.sort()\n    \n    print('{} frame files found in the provided directory.'.format(len(frames_list)))\n    print('{} mask files found in the provided directory.'.format(len(masks_list)))\n    \n    # Create file paths from file names\n    frames_paths = [os.path.join(img_dir, fname) for fname in frames_list]\n    masks_paths = [os.path.join(img_dir, fname) for fname in masks_list]\n    \n    # Create dataset of tensors\n    frame_data = tf.data.Dataset.from_tensor_slices(frames_paths)\n    masks_data = tf.data.Dataset.from_tensor_slices(masks_paths)\n    \n    # Read images into the tensor dataset\n    frame_tensors = frame_data.map(_read_to_tensor)\n    masks_tensors = masks_data.map(_read_to_tensor)\n    \n    print('Completed importing {} frame images from the provided directory.'.format(len(frames_list)))\n    print('Completed importing {} mask images from the provided directory.'.format(len(masks_list)))\n    \n    return frame_tensors, masks_tensors, frames_list, masks_list\n\nframe_tensors, masks_tensors, frames_list, masks_list = read_images(img_dir)","97adec18":"# Make an iterator to extract images from the tensor dataset\n\nframe_batches = tf.compat.v1.data.make_one_shot_iterator(frame_tensors)  # outside of TF Eager, we would use make_one_shot_iterator\nmask_batches = tf.compat.v1.data.make_one_shot_iterator(masks_tensors)\n\n","bacc5bfb":"n_images_to_show = 5\n\nfor i in range(n_images_to_show):\n    \n    # Get the next image from iterator\n    fig,ax = plt.subplots(1,figsize=(5,5))\n    frame = frame_batches.next().numpy().astype(np.uint8)\n    mask = mask_batches.next().numpy().astype(np.uint8)\n    ax.imshow(frame)\n    ax.imshow(mask,alpha = 0.4)","124d5537":"DATA_PATH = 'data\/CamSeq01\/'\n\n# Create folders to hold images and masks\n\nfolders = ['train_frames\/train', 'train_masks\/train', 'val_frames\/val', 'val_masks\/val']\n\n\nfor folder in folders:\n    try:\n        os.makedirs(DATA_PATH + folder)\n    except Exception as e: print(e)","d15f91ec":"def generate_image_folder_structure(frames, masks, frames_list, masks_list):\n    '''Function to save images in the appropriate folder directories \n        Inputs: \n            frames - frame tensor dataset\n            masks - mask tensor dataset\n            frames_list - frame file paths\n            masks_list - mask file paths\n    '''\n    #Create iterators for frames and masks\n    frame_batches = tf.compat.v1.data.make_one_shot_iterator(frames)  # outside of TF Eager, we would use make_one_shot_iterator\n    mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks)\n    \n    #Iterate over the train images while saving the frames and masks in appropriate folders\n    dir_name='train'\n    for file in zip(frames_list[:-round(0.2*len(frames_list))],masks_list[:-round(0.2*len(masks_list))]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    #Iterate over the val images while saving the frames and masks in appropriate folders\n    dir_name='val'\n    for file in zip(frames_list[-round(0.2*len(frames_list)):],masks_list[-round(0.2*len(masks_list)):]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    print(\"Saved {} frames to directory {}\".format(len(frames_list),DATA_PATH))\n    print(\"Saved {} masks to directory {}\".format(len(masks_list),DATA_PATH))\n    \ngenerate_image_folder_structure(frame_tensors, masks_tensors, frames_list, masks_list)\n\n#generate_image_folder_structure(train_frames, train_masks, val_files, 'val')","7915bae1":"def parse_code(l):\n    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n    '''\n    if len(l.strip().split(\"\\t\")) == 2:\n        a, b = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), b\n    else:\n        a, b, c = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), c","48e60105":"label_codes, label_names = zip(*[parse_code(l) for l in open(img_dir+\"label_colors.txt\")])\nlabel_codes, label_names = list(label_codes), list(label_names)\nlabel_codes[:5], label_names[:5]","5072b566":"code2id = {v:k for k,v in enumerate(label_codes)}\nid2code = {k:v for k,v in enumerate(label_codes)}","61184b07":"name2id = {v:k for k,v in enumerate(label_names)}\nid2name = {k:v for k,v in enumerate(label_names)}","36a75ff1":"def rgb_to_onehot(rgb_image, colormap = id2code):\n    '''Function to one hot encode RGB mask labels\n        Inputs: \n            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n            colormap - dictionary of color to label id\n        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n    '''\n    num_classes = len(colormap)\n    shape = rgb_image.shape[:2]+(num_classes,)\n    encoded_image = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(colormap):\n        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n    return encoded_image\n\n\ndef onehot_to_rgb(onehot, colormap = id2code):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    return np.uint8(output)","456cc492":"# Normalizing only frame images, since masks contain label info\ndata_gen_args = dict(rescale=1.\/255)\nmask_gen_args = dict()\n\ntrain_frames_datagen = ImageDataGenerator(**data_gen_args)\ntrain_masks_datagen = ImageDataGenerator(**mask_gen_args)\nval_frames_datagen = ImageDataGenerator(**data_gen_args)\nval_masks_datagen = ImageDataGenerator(**mask_gen_args)\n\n# Seed defined for aligning images and their masks\nseed = 1","d623c26f":"def TrainAugmentGenerator(seed = 1, batch_size = 5):\n    '''Train Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    train_image_generator = train_frames_datagen.flow_from_directory(\n    DATA_PATH + 'train_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (384,384))\n\n    train_mask_generator = train_masks_datagen.flow_from_directory(\n    DATA_PATH + 'train_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (384,384))\n\n    while True:\n        X1i = train_image_generator.next()\n        X2i = train_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n\ndef ValAugmentGenerator(seed = 1, batch_size = 5):\n    '''Validation Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    val_image_generator = val_frames_datagen.flow_from_directory(\n    DATA_PATH + 'val_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (384,384))\n\n\n    val_mask_generator = val_masks_datagen.flow_from_directory(\n    DATA_PATH + 'val_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (384,384))\n\n\n    while True:\n        X1i = val_image_generator.next()\n        X2i = val_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n        \n","5029e875":"from tqdm import tqdm","8f199602":"from tqdm import tqdm_gui","2ab930cd":"def initial_block(inp):\n    inp1 = inp\n    conv = Conv2D(filters=13, kernel_size=3, strides=2,\n                  padding='same', kernel_initializer='he_normal')(inp)\n    pool = MaxPool2D(2)(inp1)\n    concat = concatenate([conv, pool])\n    return concat\n\n\ndef encoder_bottleneck(inp, filters, name, dilation_rate=2, downsample=False, dilated=False, asymmetric=False, drop_rate=0.1):\n    reduce = filters \/\/ 4\n    down = inp\n    kernel_stride = 1\n\n    # Downsample\n    if downsample:\n        kernel_stride = 2\n        pad_activations = filters - inp.shape.as_list()[-1]\n        down = MaxPool2D(2)(down)\n        down = Permute(dims=(1, 3, 2))(down)\n        down = ZeroPadding2D(padding=((0, 0), (0, pad_activations)))(down)\n        down = Permute(dims=(1, 3, 2))(down)\n\n    # 1*1 Reduce\n    x = Conv2D(filters=reduce, kernel_size=kernel_stride, strides=kernel_stride, padding='same',\n               use_bias=False, kernel_initializer='he_normal', name=f'{name}_reduce')(inp)\n    x = BatchNormalization(momentum=0.1)(x)\n    x = PReLU(shared_axes=[1, 2])(x)\n\n    # Conv\n    if not dilated and not asymmetric:\n        x = Conv2D(filters=reduce, kernel_size=3, padding='same',\n                   kernel_initializer='he_normal', name=f'{name}_conv_reg')(x)\n    elif dilated:\n        x = Conv2D(filters=reduce, kernel_size=3, padding='same', dilation_rate=dilation_rate,\n                   kernel_initializer='he_normal', name=f'{name}_reduce_dilated')(x)\n    elif asymmetric:\n        x = Conv2D(filters=reduce, kernel_size=(1, 5), padding='same', use_bias=False,\n                   kernel_initializer='he_normal', name=f'{name}_asymmetric')(x)\n        x = Conv2D(filters=reduce, kernel_size=(5, 1), padding='same',\n                   kernel_initializer='he_normal', name=name)(x)\n    x = BatchNormalization(momentum=0.1)(x)\n    x = PReLU(shared_axes=[1, 2])(x)\n\n    # 1*1 Expand\n    x = Conv2D(filters=filters, kernel_size=1, padding='same', use_bias=False,\n               kernel_initializer='he_normal', name=f'{name}_expand')(x)\n    x = BatchNormalization(momentum=0.1)(x)\n    x = SpatialDropout2D(rate=drop_rate)(x)\n\n    concat = Add()([x, down])\n    concat = PReLU(shared_axes=[1, 2])(concat)\n    return concat\n\n\ndef decoder_bottleneck(inp, filters, name, upsample=False):\n    reduce = filters \/\/ 4\n    up = inp\n\n    # Upsample\n    if upsample:\n        up = Conv2D(filters=filters, kernel_size=1, strides=1, padding='same',\n                    use_bias=False, kernel_initializer='he_normal', name=f'{name}_upsample')(up)\n        up = UpSampling2D(size=2)(up)\n\n    # 1*1 Reduce\n    x = Conv2D(filters=reduce, kernel_size=1, strides=1, padding='same',\n               use_bias=False, kernel_initializer='he_normal', name=f'{name}_reduce')(inp)\n    x = BatchNormalization(momentum=0.1)(x)\n    x = PReLU(shared_axes=[1, 2])(x)\n\n    # Conv\n    if not upsample:\n        x = Conv2D(filters=reduce, kernel_size=3, strides=1, padding='same',\n                   kernel_initializer='he_normal', name=f'{name}_conv_reg')(x)\n    else:\n        x = Conv2DTranspose(filters=reduce, kernel_size=3, strides=2, padding='same',\n                            kernel_initializer='he_normal', name=f'{name}_transpose')(x)\n    x = BatchNormalization(momentum=0.1)(x)\n    x = PReLU(shared_axes=[1, 2])(x)\n\n    # 1*1 Expand\n    x = Conv2D(filters=filters, kernel_size=1, strides=1, padding='same',\n               use_bias=False, kernel_initializer='he_normal', name=f'{name}_expand')(x)\n    x = BatchNormalization(momentum=0.1)(x)\n\n    concat = Add()([x, up])\n    concat = ReLU()(concat)\n\n    return concat\n\n","a45758c5":"def ENet(H=384, W=384, nclasses=32):\n    '''\n    '''\n\n    print('Loading ENet')\n    inp = Input(shape=(H, W, 3))\n    enc = initial_block(inp)\n\n    # Bottleneck 1.0\n    enc = encoder_bottleneck(enc, 64, name='enc1',\n                             downsample=True, drop_rate=0.001)\n\n    enc = encoder_bottleneck(enc, 64, name='enc1.1', drop_rate=0.001)\n    enc = encoder_bottleneck(enc, 64, name='enc1.2', drop_rate=0.001)\n    enc = encoder_bottleneck(enc, 64, name='enc1.3', drop_rate=0.001)\n    enc = encoder_bottleneck(enc, 64, name='enc1.4', drop_rate=0.001)\n    \n    enc = encoder_bottleneck(enc, 64, name='enc1.5', drop_rate=0.001)\n    enc = encoder_bottleneck(enc, 64, name='enc1.6', drop_rate=0.001)\n    enc = encoder_bottleneck(enc, 64, name='enc1.7', drop_rate=0.001)\n\n    # Bottleneck 2.0\n    enc = encoder_bottleneck(enc, 128, name='enc2.0', downsample=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.1')\n    enc = encoder_bottleneck(enc, 128, name='enc2.2',\n                             dilation_rate=2, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.3', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.4',\n                             dilation_rate=4, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.5', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.6',\n                             dilation_rate=6, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.7', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.8',\n                             dilation_rate=8, dilated=True)\n    \n    enc = encoder_bottleneck(enc, 128, name='enc2.9')\n    enc = encoder_bottleneck(enc, 128, name='enc2.10',\n                             dilation_rate=10, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.11', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.12',\n                             dilation_rate=12, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.13', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.14',\n                             dilation_rate=14, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.15', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc2.16',\n                             dilation_rate=16, dilated=True)\n\n    # Bottleneck 3.0\n    enc = encoder_bottleneck(enc, 128, name='enc3.0')\n    enc = encoder_bottleneck(enc, 128, name='enc3.1',\n                             dilation_rate=2, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.2', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.3',\n                             dilation_rate=4, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.4')\n    enc = encoder_bottleneck(enc, 128, name='enc3.5',\n                             dilation_rate=6, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.6')\n    enc = encoder_bottleneck(enc, 128, name='enc3.7',\n                             dilation_rate=8, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.8', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.9',\n                             dilation_rate=10, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.10', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.11',\n                             dilation_rate=12, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.12', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.13',\n                             dilation_rate=14, dilated=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.14', asymmetric=True)\n    enc = encoder_bottleneck(enc, 128, name='enc3.15',\n                             dilation_rate=16, dilated=True)\n\n\n\n\n    # Bottleneck 4.0\n    dec = decoder_bottleneck(enc, 64, name='dec4.0', upsample=True)\n    dec = decoder_bottleneck(dec, 64, name='dec4.1')\n    dec = decoder_bottleneck(dec, 64, name='dec4.2')\n    dec = decoder_bottleneck(dec, 64, name='dec4.3')\n    dec = decoder_bottleneck(dec, 64, name='dec4.4')\n\n    # Bottleneck 5.0\n    dec = decoder_bottleneck(dec, 16, name='dec5.0', upsample=True)\n    dec = decoder_bottleneck(dec, 16, name='dec5.1')\n    dec = decoder_bottleneck(dec, 16, name='dec5.2')\n    dec = decoder_bottleneck(dec, 16, name='dec5.3')\n\n    dec = Conv2DTranspose(filters=nclasses, kernel_size=2, strides=2,\n                          padding='same', kernel_initializer='he_normal', name='fullconv')(dec)\n    dec = Activation('softmax')(dec)\n\n    model = Model(inputs=inp, outputs=dec, name='Enet')\n    \n    return model","f27a434e":"model = ENet()\nmodel.summary()","c7400552":"\ndef tversky_loss(y_true, y_pred):\n    alpha = 0.5\n    beta  = 0.5\n    \n    ones = K.ones(K.shape(y_true))\n    p0 = y_pred      # proba that voxels are class i\n    p1 = ones-y_pred # proba that voxels are not class i\n    g0 = y_true\n    g1 = ones-y_true\n    \n    num = K.sum(p0*g0, (0,1,2,3))\n    den = num + alpha*K.sum(p0*g1,(0,1,2,3)) + beta*K.sum(p1*g0,(0,1,2,3))\n    \n    T = K.sum(num\/den) # when summing over classes, T has dynamic range [0 Ncl]\n    \n    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n    return Ncl-T\n\n\ndef tversky(y_true, y_pred, smooth=1):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)\/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\n\n\ndef focal_tversky_loss_r(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1-pt_1), gamma)","6cf1bf31":"@tf.function()\ndef dice_coef(y_true, y_pred):\n    mask =  tf.equal(y_true, 255)\n    mask = tf.logical_not(mask)\n    y_true = tf.boolean_mask(y_true, mask)\n    y_pred = tf.boolean_mask(y_pred, mask)\n    \n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)","c1f74c45":"smooth = 1.","83aec8c1":"model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[dice_coef,'accuracy'])\n","5ef4744c":"# tb = TensorBoard(log_dir='logs', write_graph=True)\n# mc = ModelCheckpoint(mode='max', filepath='camvid_model_vgg16_segnet_checkpoint.h5', monitor='accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n# # es = EarlyStopping(mode='min', monitor='val_loss', patience=4, verbose=1)\n# callbacks = [mc]","a2a1f01e":"batch_size = 5\nsteps_per_epoch = np.ceil(float(len(frames_list) - round(0.2*len(frames_list))) \/ float(batch_size))\nsteps_per_epoch","fd2b63cf":"validation_steps = (float((round(0.2*len(frames_list)))) \/ float(batch_size))\nvalidation_steps","22208ccf":"num_epochs = 40","06522c25":"# Train model\n\nbatch_size = 5\nresult = model.fit_generator(TrainAugmentGenerator(), steps_per_epoch=steps_per_epoch ,\n                validation_data = ValAugmentGenerator(), \n                validation_steps = validation_steps, epochs=num_epochs, verbose= 1)","7092dc08":"# Get actual number of epochs model was trained for\nN = len(result.history['loss'])\n\n#Plot the model evaluation history\nplt.style.use(\"ggplot\")\nfig = plt.figure(figsize=(20,8))\n\nfig.add_subplot(1,2,1)\nplt.title(\"Training Loss\")\nplt.plot(np.arange(0, N), result.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), result.history[\"val_loss\"], label=\"val_loss\")\nplt.ylim(0, 1)\n\nfig.add_subplot(1,2,2)\nplt.title(\"Training Accuracy\")\nplt.plot(np.arange(0, N), result.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), result.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.ylim(0, 1)\n\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()","6addb193":"training_gen = TrainAugmentGenerator()\ntesting_gen = ValAugmentGenerator()","b6c27b6a":"\nbatch_img,batch_mask = next(testing_gen)\npred_all= model.predict(batch_img)\nnp.shape(pred_all)\n","9a136bd8":"for i in range(0,np.shape(pred_all)[0]):\n    \n    fig,ax = plt.subplots(1,2,figsize=(10,8))\n    \n    ax[0].imshow(batch_img[i])\n    ax[0].title.set_text('Actual frame')\n    ax[0].grid(b=None)\n    ax[0].imshow(onehot_to_rgb(batch_mask[i],id2code),alpha = 0.6)\n    \n    ax[1].set_title('Predicted frames')\n    ax[1].imshow(batch_img[i])\n    ax[1].imshow(onehot_to_rgb(pred_all[i],id2code),alpha = 0.6)\n    ax[1].grid(b=None)\n    plt.show()","f92d54a5":"batch_img,batch_mask = next(training_gen)\npred_all= model.predict(batch_img)\nnp.shape(pred_all)","45369b41":"for i in range(0,np.shape(pred_all)[0]):\n    \n    fig,ax = plt.subplots(1,2,figsize=(10,8))\n    \n    ax[0].imshow(batch_img[i])\n    ax[0].title.set_text('Actual frame')\n    ax[0].grid(b=None)\n    ax[0].imshow(onehot_to_rgb(batch_mask[i],id2code),alpha = 0.6)\n    \n    ax[1].set_title('Predicted Train frames')\n    ax[1].imshow(batch_img[i])\n    ax[1].imshow(onehot_to_rgb(pred_all[i],id2code),alpha = 0.6)\n    ax[1].grid(b=None)\n    plt.show()","0f1e629f":"!rm -rf .\/*","03e84de9":"### Image directory and size parameters","30fee16d":"# Model Evaluation","47a4391a":"## Train and save the DEEPLABV3 model","4872b1fd":"### Reading frames and masks\n- Mask file names end in \"\\_L.png\"\n","8758b7b5":"### Creating folder structure common for Computer Vision problems","1fa83619":"## Data preparation - Importing, Cleaning and Creating structured directory ","cbeb3722":"### Define functions for one hot encoding rgb labels, and decoding encoded predictions","4af4c5f7":"### Function to import and process frames and masks as tensors","f147a838":"### Parse and extract label names and codes","e3f9644c":"### Saving frames and masks to correct directories","f29638ae":"with albummentation image mask augumentation  on training up to 150 epochs model can be improved very much","85892a06":"### Model evaluation historical plots","b3238c0e":"### Custom image data generators for creating batches of frames and masks","9a8ee3b7":"### Extract and display model frame, prediction and mask batch","f24a13fb":"### Function to parse the file \"label_colors.txt\" which contains the class definitions","9bbbb7cf":"# Creating custom Image data generators","9565d2b2":"### Defining data generators","6fb12cad":"# implementation of E-NET model\n\n\nhttps:\/\/arxiv.org\/pdf\/1606.02147.pdf","d418d619":"## Extract Target Class definitions","76840d4e":"### Displaying Images in the train dataset","9ec312eb":"## Defining dice co-efficients for model performance"}}