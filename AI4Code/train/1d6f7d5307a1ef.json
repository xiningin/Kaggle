{"cell_type":{"f1f8b4ed":"code","9c15f28f":"code","ccb1e060":"code","f2294352":"code","29a274d8":"code","ec26ec06":"code","2465bd20":"code","03117eef":"code","bdba7bdc":"code","0bbafc1f":"code","d009bf1f":"code","cd9534b6":"code","7d5cc225":"code","61c1a34d":"code","8b236976":"code","7ac901c1":"code","352630ea":"code","25d9048d":"code","07d916a8":"code","32ab1761":"code","34f4e4d5":"code","d043f3b9":"code","f21d30d4":"markdown","e7ba5565":"markdown","5db5f785":"markdown","2fd2eedb":"markdown","a5d69114":"markdown","e259ac0f":"markdown","07e2a914":"markdown","4ffe326c":"markdown"},"source":{"f1f8b4ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c15f28f":"data_train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")","ccb1e060":"features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\ndata_train[\"Soil_Count\"] = data_train[features_soil].apply(sum, axis=1)\ndata_train.head()","f2294352":"features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ndata_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(sum, axis=1)\ndata_train.Wilderness_Area.describe()","29a274d8":"data_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(np.argmax, axis=1)\n#data_train[\"Wilderness_Area\"] = data_train[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1])\ndata_train.Wilderness_Area.head()","ec26ec06":"features = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \"Cover_Type\"]\nsns.heatmap(data=data_train[features].corr(), annot=True, linecolor=\"w\", fmt=\".1\")\nplt.show()","2465bd20":"sns.distplot(data_train[\"Hillshade_Noon\"].apply(lambda x: x**4))\nplt.show()","03117eef":"def clear_dataset(dataset):\n    features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n    features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n    dataset[\"Soil_Type\"] = dataset[features_soil].apply(np.argmax, axis=1)\n    #dataset[\"Soil_Type\"] = dataset[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1]).astype(int)\n    dataset = dataset.drop([\"Soil_Type15\", \"Soil_Type7\"], axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[features_wilderness].apply(np.argmax, axis=1)\n    #dataset[\"Wilderness_Area\"] = dataset[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1]).astype(int)\n    #dataset = dataset.drop(features_wilderness, axis=1)\n    dataset[\"Hillshade_1\"] = (dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_1_sqrt\"] = np.sqrt(dataset[\"Hillshade_1\"])\n    dataset[\"Hillshade_2\"] = (dataset.Hillshade_3pm * dataset.Hillshade_9am)\n    dataset[\"Hillshade_2_sqrt\"] = np.sqrt(dataset[\"Hillshade_2\"])\n    dataset[\"Hillshade_3\"] = (dataset.Hillshade_3pm * dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_3_sqrt\"] = np.sqrt(dataset[\"Hillshade_3\"])\n    dataset.Hillshade_1 = dataset.Hillshade_1.astype(float)\n    dataset[\"DistanceToHydrology\"] = np.sqrt(dataset.Horizontal_Distance_To_Hydrology ** 2 + dataset.Vertical_Distance_To_Hydrology ** 2)\n    dataset[\"Horizontal_Distance_To_Roadways_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Roadways\"])\n    dataset[\"Horizontal_Distance_To_Fire_Points_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Fire_Points\"])\n    dataset[\"Slope_sqrt\"] = np.sqrt(dataset[\"Slope\"])\n    dataset[\"Hillshade_9am_cube\"] = dataset[\"Hillshade_9am\"].apply(lambda x: x**3)\n    dataset[\"Hillshade_Noon_cube\"] = dataset[\"Hillshade_Noon\"].apply(lambda x: x**3)\n    dataset[\"Aspect_Slope_cbrt\"] = np.cbrt(dataset.Aspect * dataset.Slope)\n    dataset[\"Elevation_Slope_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Slope)\n    dataset[\"Elevation_Aspect_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Aspect)\n    dataset[\"Elevation_sqrt\"] = np.sqrt(dataset.Elevation)\n    return dataset","bdba7bdc":"data_test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nfinal_test = clear_dataset(data_test)","0bbafc1f":"data_train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/\/train.csv\")\nfinal_train = clear_dataset(data_train)","d009bf1f":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","cd9534b6":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","7d5cc225":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.25, random_state=42)","61c1a34d":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=19, max_features=11,n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\n\ny_predicted = clf.predict(x_val)","8b236976":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","7ac901c1":"accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","352630ea":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","25d9048d":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","07d916a8":"data_test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nfinal_test = clear_dataset(data_test)","32ab1761":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","34f4e4d5":"y_predicted = clf.predict(x_train)\naccuracy, precision, recall, f1 = get_metrics(y_train, y_predicted)\nprint(\"train accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","d043f3b9":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_data, y_data)\ntest_preds = clf.predict(final_test.drop([\"Id\"], axis=1))\noutput = pd.DataFrame({'Id': data_test.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('rf_submission.csv', index=False)","f21d30d4":"#Again  I got AttributeError: 'int' object has no attribute 'split'. Then I commented the line in the middle of the snippet above.","e7ba5565":"#Above I got AttributeError: 'int' object has no attribute 'split'. Then I commented the line in the middle of the snippet above.","5db5f785":"#Your notebook tried to allocate more memory than is available. It has restarted.\n\n#Time to give up. I knew it. It was not Too Easy as I thought. ","2fd2eedb":"![](https:\/\/i.ytimg.com\/vi\/m_V6m4snTNM\/maxresdefault.jpg)youtube.com","a5d69114":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #87CEFA;\"><b style=\"color:#0000CD;\">Learn with other Kagglers. Come to our Playground.<\/b><\/h1><\/center>\n\nThat code was the first one that I copied in a Competition (Learn With Other Kaggle Users). Then I named my work: \"Adapt and Overcome\" (2019). For obvious reasons I didn't adapted anything. Just changed the file's names and commented some lines where I got errors that I couldn't fix. Overcome? I don't think so. Though my titles are still fun. \n\nThen I got that awful message that pursues me: \"Your notebook tried to allocate more memory than is available. It has restarted.\" That it's not fun even too easy.\n\nAnyway, thank you @TooEzy (Emrecan) for having shared this script in 2015.  See you all in the next Forest Cover Types. In the Roosevelt National Forest of Northern Colorado or anywhere else.","e259ac0f":"#I don't even know who is George Fisher, the author of the snippet above that Enrecan (@tooezy) has copied and gave credits.","07e2a914":"#Thanks to Enrecan @tooezy. Only the ones who share could be remembered. Even 6 years ago (2015).","4ffe326c":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcSKR1aGy-dBRtZxwqX1qk0Np170XF88yGvAHQ&usqp=CAU)coloradosun.com"}}