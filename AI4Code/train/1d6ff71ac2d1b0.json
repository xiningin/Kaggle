{"cell_type":{"aa86e6c5":"code","14f4f764":"code","eb4477e3":"code","89f93a8c":"code","7d004557":"code","7f9e850d":"code","694802a9":"code","cc44d49c":"code","a12bc20c":"code","c1735c88":"code","cda42661":"code","603e148c":"code","87271c22":"code","93de36a9":"code","9b4f4091":"code","a3d56db9":"code","ae486539":"code","9dcea99a":"code","b2c4ef0f":"code","e2b545c2":"code","a4457de9":"code","4b7a2de9":"code","80487fd7":"code","cd63d698":"code","bbed1150":"code","babbba40":"code","94641003":"code","f72a2f0c":"code","229911aa":"code","408fb2a9":"code","159518b3":"code","baecedec":"code","178206a5":"code","ee7946d0":"code","26543fe3":"code","96ee6726":"code","828cf5d1":"code","186eeb62":"code","756bf30b":"markdown","bf75e526":"markdown","a7d293b1":"markdown","40ebeacd":"markdown","1614a74d":"markdown","5e7c4ea9":"markdown","aa049d03":"markdown","e08f82fb":"markdown","a443c00f":"markdown","1db8da2b":"markdown","24985a2d":"markdown","23f7e801":"markdown","8095b8b1":"markdown","fba3f60f":"markdown","8eee2966":"markdown","cdb3e439":"markdown","d6d72eb5":"markdown","10e9048c":"markdown","93f53a35":"markdown","df5f5c52":"markdown","46fd6059":"markdown","45894587":"markdown"},"source":{"aa86e6c5":"#import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#list files and directories in workspace\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","14f4f764":"train_file_path = ('..\/input\/home-data-for-ml-course\/train.csv')\ntest_file_path = ('..\/input\/home-data-for-ml-course\/test.csv')\ntrain_raw = pd.read_csv(train_file_path)\ntest_raw = pd.read_csv(test_file_path)","eb4477e3":"print(train_raw.shape) #returns a tuple, showing (number_of_rows, number_of_cols)\nprint(train_raw.columns) #returns a list of all column names\nprint(train_raw.head()) #returns the head of the dataframe\nprint(train_raw.info()) #returns a DataFrame with column, number of valid entries, and dtype","89f93a8c":"col_description = {'SalePrice': \"the property's sale price in dollars. This is the target variable that you're trying to predict.\",\n                   'MSSubClass': \"The building class\",\n                   'MSZoning': \"The general zoning classification\",\n                   'LotFrontage': \"Linear feet of street connected to property\",\n                   'LotArea': \"Lot size in square feet\",\n                   'Street': \"Type of road access\",\n                   'Alley': \"Type of alley access\",\n                   'LotShape': \"General shape of property\",\n                   'LandContour': \"Flatness of the property\",\n                   'Utilities': \"Type of utilities available\",\n                   'LotConfig': \"Lot configuration\",\n                   'LandSlope': \"Slope of property\",\n                   'Neighborhood': \"Physical locations within Ames city limits\",\n                   'Condition1': \"Proximity to main road or railroad\",\n                   'Condition2': \"Proximity to main road or railroad (if a second is present)\",\n                   'BldgType': \"Type of dwelling\",\n                   'HouseStyle': \"Style of dwelling\",\n                   'OverallQual': \"Overall material and finish quality\",\n                   'OverallCond': \"Overall condition rating\",\n                   'YearBuilt': \"Original construction date\",\n                   'YearRemodAdd': \"Remodel date\",\n                   'RoofStyle': \"Type of roof\",\n                   'RoofMatl': \"Roof material\",\n                   'Exterior1st': \"Exterior covering on house\",\n                   'Exterior2nd': \"Exterior covering on house (if more than one material)\",\n                   'MasVnrType': \"Masonry veneer type\",\n                   'MasVnrArea': \"Masonry veneer area in square feet\",\n                   'ExterQual': \"Exterior material quality\",\n                   'ExterCond': \"Present condition of the material on the exterior\",\n                   'Foundation': \"Type of foundation\",\n                   'BsmtQual': \"Height of the basement\",\n                   'BsmtCond': \"General condition of the basement\",\n                   'BsmtExposure': \"Walkout or garden level basement walls\",\n                   'BsmtFinType1': \"Quality of basement finished area\",\n                   'BsmtFinSF1': \"Type 1 finished square feet\",\n                   'BsmtFinType2': \"Quality of second finished area (if present)\",\n                   'BsmtFinSF2': \"Type 2 finished square feet\",\n                   'BsmtUnfSF': \"Unfinished square feet of basement area\",\n                   'TotalBsmtSF': \"Total square feet of basement area\",\n                   'Heating': \"Type of heating\",\n                   'HeatingQC': \"Heating quality and condition\",\n                   'CentralAir': \"Central air conditioning\",\n                   'Electrical': \"Electrical system\",\n                   '1stFlrSF': \"First Floor square feet\",\n                   '2ndFlrSF': \"Second floor square feet\",\n                   'LowQualFinSF': \"Low quality finished square feet (all floors)\",\n                   'GrLivArea': \"Above grade (ground) living area square feet\",\n                   'BsmtFullBath': \"Basement full bathrooms\",\n                   'BsmtHalfBath': \"Basement half bathrooms\",\n                   'FullBath': \"Full bathrooms above grade\",\n                   'HalfBath': \"Half baths above grade\",\n                   'Bedroom': \"Number of bedrooms above basement level\",\n                   'Kitchen': \"Number of kitchens\",\n                   'KitchenQual': \"Kitchen quality\",\n                   'TotRmsAbvGrd': \"Total rooms above grade (does not include bathrooms)\",\n                   'Functional': \"Home functionality rating\",\n                   'Fireplaces': \"Number of fireplaces\",\n                   'FireplaceQu': \"Fireplace quality\",\n                   'GarageType': \"Garage location\",\n                   'GarageYrBlt': \"Year garage was built\",\n                   'GarageFinish': \"Interior finish of the garage\",\n                   'GarageCars': \"Size of garage in car capacity\",\n                   'GarageArea': \"Size of garage in square feet\",\n                   'GarageQual': \"Garage quality\",\n                   'GarageCond': \"Garage condition\",\n                   'PavedDrive': \"Paved driveway\",\n                   'WoodDeckSF': \"Wood deck area in square feet\",\n                   'OpenPorchSF': \"Open porch area in square feet\",\n                   'EnclosedPorch': \"Enclosed porch area in square feet\",\n                   '3SsnPorch': \"Three season porch area in square feet\",\n                   'ScreenPorch': \"Screen porch area in square feet\",\n                   'PoolArea': \"Pool area in square feet\",\n                   'PoolQC': \"Pool quality\",\n                   'Fence': \"Fence quality\",\n                   'MiscFeature': \"Miscellaneous feature not covered in other categories\",\n                   'MiscVal': \"$Value of miscellaneous feature\",\n                   'MoSold': \"Month Sold\",\n                   'YrSold': \"Year Sold\",\n                   'SaleType': \"Type of sale\",\n                   'SaleCondition': \"Condition of sale\",\n                  }","7d004557":"# Testing new column description dictionary.\nprint(col_description['Fence'])","7f9e850d":"# getting a summary for all the numerical columns:\ntrain_raw.select_dtypes(exclude = 'object').describe() #select_dtypes returns a df, filtered by dtype #describe returns a summary","694802a9":"# getting a summary for all the categorical and bool columns:\ntrain_raw.select_dtypes(include=['object','bool']).describe()","cc44d49c":"# getting value counts for specific columns:\nprint(train_raw.SaleCondition.value_counts())\nprint(train_raw.GarageType.value_counts(normalize=True)) # if normalize argument is set to True, relative frequencies are shown","a12bc20c":"#way one\ntrain_raw_top25 = train_raw.copy().sort_values(by='SalePrice', ascending=False).head(25)\n#way two\ntrain_raw_top25_2 = train_raw[train_raw['SalePrice'] > 400000]","c1735c88":"# to get a quick overview regarding the top25 we can show a simple mean summary statistc\nprint(train_raw_top25.mean()) # shows a df with the name of the columns and their mean\n\n# to compare specific colums we can also show mean values for specific columns by label subsetting and applying .mean() method\nprint('Lot Area enth\u00e4lt folgende Informationen: ' + col_description['LotArea'])\nprint(train_raw_top25['LotArea'].mean())\nprint(train_raw['LotArea'].mean())","cda42661":"# creating a heatmap plot with matplotlib\nimport matplotlib.pyplot as plt\n\ndf = train_raw.select_dtypes(exclude = 'object').drop('Id',axis=1)\nf = plt.figure(figsize=(19, 15))\nplt.matshow(df.corr(), fignum=f.number)\nplt.xticks(range(df.shape[1]), df.columns, fontsize=14, rotation=90)\nplt.yticks(range(df.shape[1]), df.columns, fontsize=14)\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)","603e148c":"#creating a coloured correlation matrix with pandas\nimport pandas as pd\nimport numpy as np\n\ndf = train_raw.select_dtypes(exclude = 'object').drop('Id',axis=1)\ncorr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n# 'RdBu_r' & 'BrBG' are other good diverging colormaps","87271c22":"# including all cols in the list, that are associated to SalePrice with an absolut correlation >.5\nsubset_cols_2 = [col for col in train_raw.select_dtypes(exclude = 'object') if abs(train_raw[col].corr(train_raw['SalePrice'])) >= 0.5]\n\n# creating drop list:\ncol_drop_list = set(train_raw.columns) - set(subset_cols_2)\n\n# dropping all elements except our subset_cols_2\ntrain_raw_sub = train_raw.drop(col_drop_list, axis = 1)","93de36a9":"train_raw_sub.corr().style.background_gradient(cmap = 'YlOrRd')\n","9b4f4091":"import matplotlib.pyplot as plt\n\n# create a figure and axes\nfig, ax = plt.subplots()\n\n# scatterplot of SalePrice and OverallQual\nax.scatter(train_raw_sub.OverallQual, # define x axis first\n           train_raw_sub.SalePrice) # define y axis then\nax.set_title('Scatterplot of SalePrice by OverallQual')\nax.set_xlabel('Overall Quality')\nax.set_ylabel('Sale Price')\n\n# if we want to add a linear regression curve, we can add:\nfrom sklearn.linear_model import LinearRegression\nlinear_regressor = LinearRegression()\nlinear_regressor.fit(train_raw_sub.OverallQual.values.reshape(-1, 1), \n                     train_raw_sub.SalePrice.values.reshape(-1, 1))\nSalePricePred = linear_regressor.predict(train_raw_sub.OverallQual.values.reshape(-1, 1))\n\nax.plot(train_raw_sub.OverallQual,\n       SalePricePred,\n       color = 'red')","a3d56db9":"### 1 ) Standardization ###\n# import standardization module\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# create StandardScaler object\nsc = StandardScaler()\n# create SimpleImputer object, impute by mean\nimp = SimpleImputer(strategy = 'mean')\n\n# create a copy of train_raw_sub to prevent overwriting\ndf = train_raw_sub.copy()\n# impute missing values\ndf_imp = imp.fit_transform(df)\n# standardize df with imputed values\ndf = pd.DataFrame(sc.fit_transform(df_imp))\n# create a copy to extract columns later\ndf2 = df.copy()\n# df is now ready for PCA\n\n### 2 )     PCA        ###\n# import PCA module\nfrom sklearn.decomposition import PCA\n\n# first we create a scree plot to investigate how many components we should include in the PCA\nscree_df = PCA().fit(df)\n\n# visualize a scree plot\n# import matplotlib for the graph\nimport matplotlib.pyplot as plt\n# import numpy to calculate cumulated sum\nimport numpy as np\n\nplt.figure()\nplt.plot(np.cumsum(scree_df.explained_variance_ratio_))\nplt.xlabel('Number of components')\nplt.ylabel('Variance (%)')\nplt.show()","ae486539":"# create PCA object with predefined components\npca = PCA(n_components=3)\n# create principal components by fitting the object to the preprocessed df\nprincipalComponents = pca.fit_transform(df2)\n# create a PCA dataframe\ndf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n# inspect amount of explained variance\nprint(pca.explained_variance_ratio_)\n\n#create a covariance matrix\nprint('NumPy covariance matrix: \\n%s' %np.cov(df.T))\n#create desciptive index:\nind = [col_description[el] for el in train_raw_sub.columns]\nprint(ind)\nprint(pca.components_)\npca_cov = pd.DataFrame(pca.components_.T, index = ind, columns = ['pc1','pc2','pc3'])\nprint(pca_cov)","9dcea99a":"# Preprocessing the whole dataset to be PCA'd\n\ndf = train_raw.copy()\ndf_num = df.select_dtypes(exclude = 'object')\ndf_cat = df.select_dtypes(include = 'object')\n\n# impute cat\nimp = SimpleImputer(strategy = 'most_frequent')\ndf_cat2 = pd.DataFrame(imp.fit_transform(df_cat))\ndf_cat2.columns = df_cat.columns\nprint(df_cat2.head(10))\n\n# Encode labels\nfrom sklearn.preprocessing import LabelEncoder\n\nLE = LabelEncoder()\n\n# create list of all columns that should be labelencoded\nenc_list = [col for col in df.select_dtypes(include = 'object')]\n\n# encode each column\nfor col in enc_list:\n    df[col] = LE.fit_transform(df_cat2[col])\n\ndf_prep = df.copy()\n    \n# check if everything worked right\nprint(df.head())\nprint(df.info())\nprint(df.describe())","b2c4ef0f":"# next we have to standardize all the columns to be able to conduct a PCA\n# standardize\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n# apply StandardScaler over whole dataset\ndf_std = sc.fit_transform(df_prep)\n# reattach columns\ndf_std = pd.DataFrame(df_std, columns = df_prep.columns)\n# drop ID column\ndf_std.drop('Id', axis=1, inplace=True)\n# drop still missing cols # WHY DO THEY MISS?\nmissing_cols = [col for col in df_std if df_std[col].isnull().any()]\ndf_std.drop(missing_cols, axis = 1, inplace = True)\n# inspect results\nprint(df_std)\nprint(df_std.describe())","e2b545c2":"# we are ready to conduct the PCA\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\n\npca = PCA()\npca.fit(df_std)\n\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.show()","a4457de9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n# starting by simple seaborn plots\nsns.relplot(x='YearBuilt', #relplot plots 'relationships' between two variables\n            y='SalePrice',\n            hue = 'FullBath', # hue (Farbton) allows to change the color by mapping to an independent variable\n            #style = 'FullBath', # style allows to change the marker of the datapoints, by mapping to a variable. it can help highlighting the hue variable or plot another variable instead\n            #size = 'FullBath',# size allows to change the size of the datapoints, by mapping to a variable\n            #alpha = 0.5, # alpha allows to make the points transparent (0 = invisible, 1 = full visibility)\n            data=train_raw_sub,\n            kind = 'scatter',) # the kind argument allows to specify the kind of plot we want to create\n","4b7a2de9":"graph1 = sns.relplot(x='YearBuilt',y='SalePrice',kind='line',data=train_raw_sub)\ngraph1.fig.autofmt_xdate() # by using the autoformat method, we can format the axis automatically to make it depict e.g. time data\n# as there are multiple measurements at each value of x, seaborn automatically plots the mean and the 95% confidence interval around it to depict the distribution\n# the confidence intervals are calculated by bootstrapping and can be disabled by the ci argument:\ngraph2 = sns.relplot(x='YearBuilt', y='SalePrice', kind = 'line', ci=False, data = train_raw_sub) #by passing ci=False, we inform seaborn to just depict the mean and not show the confidence intervals\ngraph2.fig.autofmt_xdate()\n# we can also plot the std instead of the CI's , which would be particularly interesting for this dataset:\ngraph3 = sns.relplot(x='YearBuilt', y='SalePrice', kind = 'line', ci = 'sd', data = train_raw_sub)","80487fd7":"# again it is also possible to easily split the graph by mapping to a third variable. Wen can use the same arguments as for scatter plots:\nsns.relplot(x = 'YearBuilt', y = 'SalePrice', kind = 'line', ci = 'sd', hue = 'FullBath', data = train_raw_sub)\n# as we can see, that the plot is a little overloaded now and it is difficult to extract useful information. facetting the plot is a good idea then:\nprint(col_description['FullBath'])\nsns.relplot(x = 'YearBuilt',\n            y = 'SalePrice', \n            kind = 'line', \n            ci = 'sd', \n            col = 'FullBath',\n            col_wrap = 2, # 'wraps' 2 column per row, so that after 2 columns, the rest of the figures are depicted onto the next row\n            data = train_raw_sub)","cd63d698":"sns.catplot(x = 'FullBath', y = 'SalePrice', kind = 'strip', hue = 'FullBath', alpha = 0.3, data = train_raw_sub)","bbed1150":"# we can also change the axis easily and implement a third variable, however just the hue argument is available, while style and size are not:\nsns.catplot(x = 'FullBath', \n            y = 'SalePrice',\n            kind = 'strip',\n            jitter = True,\n            alpha = 1,\n            hue = 'OverallQual', \n            palette = sns.color_palette(\"ch:2.5,-.2,dark=.3\", # defines a color palette that is used for the hue argument\n                                        n_colors = 10), # defines the number of elements in the palette gradient (should fit to the number of possible data values)\n            data = train_raw_sub)","babbba40":"# FullBath seems to be skew distributed, with varying frequencies in each category. \nfb1 = sns.catplot(x = 'FullBath', kind = 'count', palette = 'ch:.25', col = 'OverallQual', col_wrap = 4, data = train_raw)\n","94641003":"# rescale SalePrice column\nSalePriceK = train_raw['SalePrice'].copy()\/100000\n# rename pd.Series object name, that will be shown in the fig\nSalePriceK.name = 'SalePrice in 100K'\n# create fig\ndist = sns.distplot(SalePriceK, \n                    #kde = False, # would drop the kernel density estimation curve from the figure\n                    #hist = False, # would drop the histogram, leaving just the kde curve\n                   )\n","f72a2f0c":"# a jointplot adds histograms to a scatter plot:\nsns.jointplot(x = 'OverallQual', y = 'SalePrice', alpha = 0.5, data = train_raw_sub)","229911aa":"# a hexbin plot shows a bivariate histogram by depicting counts of obversations that fall within hexagonal bins\nwith sns.axes_style('white'):\n    sns.jointplot(x = 'LotArea', y = 'SalePrice', kind = 'hex', color = 'k', data = train_raw)","408fb2a9":"# so far so good. let's practive some relationshis, e.g. regression lines\nsns.regplot(x = 'LotArea', y = 'SalePrice', data = train_raw)","159518b3":"def reduce(x):\n    if x < 50000:\n        return x\n    else:\n        return np.nan\nTidyLotArea = pd.Series([reduce(row) for row in df['LotArea']], name = 'TidyLotArea')\n# now repeat the fig without the outliers in the LotArea:\nsns.regplot(x = TidyLotArea, y = 'SalePrice', data = train_raw)","baecedec":"# the same is possible for categorical x data\nsns.regplot(y = 'SalePrice', x = 'OverallQual', x_jitter = 0.3, data = train_raw)","178206a5":"# as the plot is quite messy, it might be helpful to change the points for the x data to estimators (mean + variance or ci)\nsns.regplot(x = 'OverallQual', \n            y = 'SalePrice',\n            data = train_raw, \n            x_jitter = 0.3, \n            x_estimator = np.mean)\n\n","ee7946d0":"sns.regplot(x = 'OverallQual', \n            y = 'SalePrice',\n           x_jitter = 0.3,\n           order = 3, # order specifys the integer that is passed to np.polyfit, which describes the fitting polynomial\n            x_estimator = np.mean,\n           data = train_raw)","26543fe3":"good_ovrl_qual = df['OverallQual'] >=8\ngood_ovrl_qual.name = 'Good overall Quality'\nsns.regplot(y = good_ovrl_qual, x = 'SalePrice', logistic = True, data = train_raw)","96ee6726":"good_ovrl_qual = df['OverallQual'] >=8\ngood_ovrl_qual.name = 'Good overall Quality'\nsns.residplot(y = good_ovrl_qual, x = 'SalePrice', data = train_raw)","828cf5d1":"f1 = sns.lmplot(x = 'YearBuilt', y = 'SalePrice', data = train_raw)\nsns.lmplot(x = 'YearBuilt', y = 'SalePrice', hue = 'FullBath', col = 'FullBath', col_wrap = 2, data = train_raw)","186eeb62":"f2 = sns.residplot(x = 'YearBuilt', y = 'SalePrice', data = train_raw)","756bf30b":"We can now conclude that the mean Lot size area of the top 25 sold houses was about 7000 square feet larger than the Lot size area of the whole dataset. Next, a correlation matrix would be interesting to inform ourselves about potential associations.","bf75e526":"**Importing data**","a7d293b1":"as we can see, the resulting plot doesn't scale well, so that it might make sense to drop the outliers from the data for the plot.","40ebeacd":"Now what if we wanted to perform the PCA over our whole dataset? Let's inspect if we can gather some new insights.","1614a74d":"Unfortunately the screeplot isn't very clear about how many components to include, but there is a slight break at the level of component 3, so we will go with 3 components.","5e7c4ea9":"But what if we wanted to show plots with categorical data?","aa049d03":"We can also plot a residual plot for our models, using the residplot function.","e08f82fb":"*\nNow what if we wanted to look at the top-priced (e.g. top 25) sold houses and their features?\nOne way could be to sort the df by SalePrice value and subset the top25.\nAnother way could be to define a threshold (e.g. 400k) and create a subset using a boolean condition.\n*","a443c00f":"We just created a simple but appealing scatter plot. But actually the YearBuilt variable could also be seen as a timevariable, so that we could do a lineplot.","1db8da2b":"We could also depict a logistig regression model, by passing logistic = True.","24985a2d":"**Exploratory data analysis**","23f7e801":"**PRINCIPAL COMPONENT ANALYSIS**\n\nPrincipial component is mostly used for 2 purposes:\n\nThe first purpose is, that it helps to understand the data and facilitates data visualization.\n\nThe second purpose is, that a PCA can help you speed up your machine learning algorithms, as it reduces the amount of features that are needed to feed the algorithm. This makes sense for large datasets.\n\n**How to do a PCA**\n\nAt first there are some things to remember:\n\n1) **standardization!** as a PCA can be effected by the scale of the variables, the data has to be standardized. we can do this by using the scikitlearn StandardScaler\n\n2) **inspect variance!** if the PCA model explains less than 85.0% of the variance in the full dataset, we should change or aboard it. we can inspect this by the pca.explained_variance_ratio_ attribute. \nAnother way to make sure that we keep enough variance is to conduct the PCA by using the PCA Class of sklean.decompositions and pass it the argument of the variance high, that the model need to retain, e.g. pca = PCA(.95). In this case, the model automatically tries to give us a model with at least 95% explained variance. We could then inspect the number of components that the model chose by applying the attribute pca.n_components_\n\n\n","8095b8b1":"*Diving deeper into exploratory analysis with a subset of variables, that are associated with SalePrice, but instead of creating a subset list by hand, we wish to include a subset of all the variables that are associated with SalePrice with an absolute correlation of at least \".5\" .*","fba3f60f":"What if we wanted to plot distributions?","8eee2966":"as we can see, this approach returns a scree plot that is garbage, because it shows a perfect curve. However we could still try to interpret a 2-component and 3-component PCA to understand correlations in the data. let's do some more visualization.","cdb3e439":"![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)","d6d72eb5":"*My first question is: What do the columns mean? Is there any description? Retrieving the description from the competition homepage gave me results, that now get stored in a dictionary \"col_description\" with column names as keys and column descriptions as values.*","10e9048c":"we can see that a linear model doesn't fit the association well, while a simple exponential function could fit it better. we can also model that in the regplot:\n","93f53a35":"**This is my data science learning notebook, using the data from the Housing Prices ML Competition**\n\nhelpful links:\nhttps:\/\/www.kaggle.com\/alexisbcook\/hello-seaborn\nhttps:\/\/mlcourse.ai\nhttps:\/\/pandas.pydata.org\/Pandas_Cheat_Sheet.pdf\n","df5f5c52":"We can see that the residuals are not evenly distributed around the y-axis which means, that a simple linear regression model doesn't fit to the data distribution well.\nIn this case we can see, that at first the Good overall Quality get's overestimated a little, while with increasing SalePrice the estimation underestimates increasingly. We could see this before, as a linear regression line didn't fit the data as good as an exponential line.\n\n\nIf we wanted to show a figure with regression lines that are faceted by a third variable, we can do this by using the lmplot function and the hue argument:","46fd6059":"Let's do some basic plotting!","45894587":"Let's create a new heatmap with our new subset!"}}